,html_url,number,labels,state,created_at,comments,title,body,rel
140,https://github.com/huggingface/transformers/issues/6368,6368,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-09 17:05:20+00:00,5,Can't load a saved tokenizer with AutoTokenizer.from_pretrained without saving Config as well,"### Environment info
- `transformers` version: master (https://github.com/huggingface/transformers/commit/6e8a38568eb874f31eb49c42285c3a634fca12e7)

### Who can help
 tokenizers: @mfuntowicz

### Information
When saving a tokenizer with .save_pretrained, it can be loaded with the class it was saved with but not with AutoTokenizer:
```
from transformers import BertTokenizer, AutoTokenizer
BertTokenizer.from_pretrained(""bert-base-cased"").save_pretrained(""."")

BertTokenizer.from_pretrained(""."") # works

AutoTokenizer.from_pretrained(""."") # throws exception
```
The error is:
```
Traceback (most recent call last):
  File ""/home/transformers/src/transformers/configuration_utils.py"", line 333, in get_config_dict
    local_files_only=local_files_only,
  File ""/home/transformers/src/transformers/file_utils.py"", line 684, in cached_path
    raise EnvironmentError(""file {} not found"".format(url_or_filename))
OSError: file ./config.json not found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/transformers/src/transformers/tokenization_auto.py"", line 205, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File ""/home/transformers/src/transformers/configuration_auto.py"", line 203, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File ""/home/transformers/src/transformers/configuration_utils.py"", line 346, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for '.'. Make sure that:

- '.' is a correct model identifier listed on 'https://huggingface.co/models'

- or '.' is the correct path to a directory containing a config.json file
```

If a configuration is saved as well, then loading with AutoTokenizer does work:
```
from transformers import BertTokenizer, BertConfig, AutoTokenizer
BertConfig.from_pretrained(""bert-base-cased"").save_pretrained(""."")
BertTokenizer.from_pretrained(""bert-base-cased"").save_pretrained(""."")

AutoTokenizer.from_pretrained(""."") # works
```

### Expected behavior
I'd expect that loading a tokenizer with AutoTokenizer would require the same files as a dedicated tokenizer class (e.g. BertTokenizer) requires.
",2
264,https://github.com/huggingface/transformers/issues/6632,6632,[],closed,2020-08-21 06:01:20+00:00,3,"Error on `PreTrainedTokenizerBase.batch_encode_plus` with `return_overflowing_tokens=True, truncation=True`","## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2 (master branch)
- Platform: macOS-10.14.6-x86_64-i386-64bit
- Python version: 3.8.1
- PyTorch version (GPU?): 1.6.0 (False)
- Tensorflow version (GPU?): 2.3.0 (False)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 examples/bert-loses-patience: @JetRunner
 tensorflow: @jplu
 examples/token-classification: @stefan-it
 documentation: @sgugger
 -->

 tokenizers: @mfuntowicz

## Information

Model I am using (Bert, XLNet ...): bert-base-uncased

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [x] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [x] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. Run the below code

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")
tokenizer.batch_encode_plus(
    [""foo"", ""bar "" * 1000], return_overflowing_tokens=True, truncation=True, padding=True
)
```

raises the following error:

```
Traceback (most recent call last):
  File ""foo.py"", line 4, in <module>
    tokenizer.batch_encode_plus(
  File ""/Users/user/work/transformers/src/transformers/tokenization_utils_base.py"", line 2121, in batch_encode_plus
    return self._batch_encode_plus(
  File ""/Users/user/work/transformers/src/transformers/tokenization_utils.py"", line 534, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
  File ""/Users/user/work/transformers/src/transformers/tokenization_utils.py"", line 606, in _batch_prepare_for_model
    batch_outputs = self.pad(
  File ""/Users/user/work/transformers/src/transformers/tokenization_utils_base.py"", line 2305, in pad
    assert all(
AssertionError: Some items in the output dictionnary have a different batch size than others.
```

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->

No error",2
720,https://github.com/huggingface/transformers/issues/7601,7601,[],closed,2020-10-06 03:58:28+00:00,1,Does tokenizer.from_pretrained tokenize text on CPU even a GPU is available?,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
For a tokenizer from `tokenizer = AutoTokenizer.from_pretrained(model_name)`, if `tokenizer.encode_plus(text)` works on CPUs even a GPU is available. I tried to run such code on a AWS GPU machine instance, but found GPUs are totally not used.
Thanks.

",2
1071,https://github.com/huggingface/transformers/issues/8306,8306,[],closed,2020-11-04 19:12:40+00:00,3,Tokenizers save_pretrained broken when defining vocab and merges file arguments (v3.1) ,"## Problem Information

Since the upgrade to transformers v3, it appears that issues arise when defining a RoBERTa or BART tokenizer using the vocab_file and merges_file arguments. 

Previously defining the tokenizer, by supplying these two arguments resulted in a correctly configured tokenizer and enabled the tokenizer to be saved using the standard .save_pretrained() function. However, since the upgrade to v3, it appears that using such arguments causes the tokenizer to default to incorrect ""init_kwargs"".

## Replicating the problem

```py
from transformers import RobertaTokenizer

#load the tokenizer using the standard method - results in correct init_kwargs
correct_tokenizer = RobertaTokenizer.from_pretrained(""roberta-large"")

#save the tokenizer so the vocab and merges file can be read in from disk
correct_tokenizer.save_pretrained(""file_path/roberta_tokenizer"")

#load the saved vocab and merges files to create a new tokenizer with the same properties
broken_tokenizer = RobertaTokenizer(
          vocab_file=""file_path/roberta_tokenizer/vocab.json"",
          merges_file = ""file_path/roberta_tokenizer/merges.txt""
          )

#attempt to save the second tokenizer using the same function as above
broken_tokenizer.save_pretrained(""file_path/saved_folder"")
```

In the above script, loading the tokenizer using the "".from_pretrained('roberta-large')"" method results in a tokenizer with the correct properties. The tokenizer's init_kwargs are as expected, and in the form of a dictionary as follows:

- merges_file: ""file/path/..."",
- model_max_length: 512,
- vocab_file: ""file/path/..."",

This tokenizer can be saved using the "".save_pretrained()"" function as intended.


However, when you load a tokenizer, while defining the ""vocab_file=vocab.json"" and ""merges_file=merges.txt"", as is the case with ""broken_tokenizer"", it appears that the init_kwargs defaults to incorrect properties. Unlike before the init_kwargs now present a dictionary with no mention of model configurations, but instead includes the following:

- bos_token: AddedToken(bos_token, lstrip=False, rstrip=False) 
- eos_token: AddedToken(eos_token, lstrip=False, rstrip=False)
- sep_token: AddedToken(sep_token, lstrip=False, rstrip=False)
- cls_token: AddedToken(cls_token, lstrip=False, rstrip=False)
- unk_token: AddedToken(unk_token, lstrip=False, rstrip=False) 
- pad_token: AddedToken(pad_token, lstrip=False, rstrip=False) 

## Error identification

The error in saving appears to arise when you reach the following section of the ""save_pretrained()"" function within tokenization_utils_base:

```py
tokenizer_config = copy.deepcopy(self.init_kwargs)
if len(self.init_inputs) > 0:
    tokenizer_config[""init_inputs""] = copy.deepcopy(self.init_inputs)
for file_id in self.vocab_files_names.keys():
    tokenizer_config.pop(file_id, None)

with open(tokenizer_config_file, ""w"", encoding=""utf-8"") as f:
    f.write(json.dumps(tokenizer_config, ensure_ascii=False))
```

In the scenario where the tokenizer has the correct kwargs, the vocab_file and merges_file are popped from ""tokenizer_config"", leaving a tokenizer config that can be saved in JSON format. 

However, where the incorrect kwargs are configured, as is the case with ""broken_tokenizer"" above, the resulting ""tokenizer_config"" contains AddedToken objects, and therefore results in the following error:
**""TypeError: Object of type AddedToken is not JSON serializable""**

## Versioning
Transformers = 3.1
Tokenizers = 0.8.1rc2",2
1289,https://github.com/huggingface/transformers/issues/8748,8748,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-11-24 01:27:57+00:00,5,"""AutoTokenizer.from_pretrained"" does not work when loading a pretrained Albert model","## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version:
- Platform: 5.4.0-53-generic #59~18.04.1-Ubuntu SMP Wed Oct 21 12:14:56 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Python version: 3.7.9
- PyTorch version (GPU?): 1.7.0
- Tensorflow version (GPU?): N/A
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @patrickvonplaten @TevenLeScao
 Blenderbot: @patrickvonplaten
 Bart: @patrickvonplaten
 Marian: @patrickvonplaten
 Pegasus: @patrickvonplaten
 mBART: @patrickvonplaten
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao
 RAG: @patrickvonplaten, @lhoestq
 FSMT: @stas00
 examples/seq2seq: @patil-suraj
 examples/bert-loses-patience: @JetRunner
 tensorflow: @jplu
 examples/token-classification: @stefan-it
 documentation: @sgugger
 -->

## Information

Model I am using (Bert, XLNet ...):

The problem arises when using:
* [x] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [ ] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. Install PyTorch from the official website as well as the transformers via pip.
2. Using the following pre-trained model:
```
from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained(""ckiplab/albert-tiny-chinese"")

model = AutoModelForMaskedLM.from_pretrained(""ckiplab/albert-tiny-chinese"")
```
3. Error:
```
Downloading: 100%|鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻坾 683/683 [00:00<00:00, 1.32MB/s]
Downloading: 100%|鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅| 112/112 [00:00<00:00, 215kB/s]
Downloading: 100%|鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅| 174/174 [00:00<00:00, 334kB/s]
Traceback (most recent call last):
  File ""/home/faith/torch_tutorials/torch_chatbot.py"", line 30, in <module>
    tokenizer = AutoTokenizer.from_pretrained(""ckiplab/albert-tiny-chinese"")
  File ""/home/faith/miniconda3/envs/torch/lib/python3.7/site-packages/transformers/tokenization_auto.py"", line 341, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File ""/home/faith/miniconda3/envs/torch/lib/python3.7/site-packages/transformers/tokenization_utils_base.py"", line 1653, in from_pretrained
    resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs
  File ""/home/faith/miniconda3/envs/torch/lib/python3.7/site-packages/transformers/tokenization_utils_base.py"", line 1725, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File ""/home/faith/miniconda3/envs/torch/lib/python3.7/site-packages/transformers/tokenization_albert.py"", line 149, in __init__
    self.sp_model.Load(vocab_file)
  File ""/home/faith/miniconda3/envs/torch/lib/python3.7/site-packages/sentencepiece.py"", line 367, in Load
    return self.LoadFromFile(model_file)
  File ""/home/faith/miniconda3/envs/torch/lib/python3.7/site-packages/sentencepiece.py"", line 177, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
TypeError: not a string
```

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
Expect to download this model correctly with error prompting.
",2
1594,https://github.com/huggingface/transformers/issues/9330,9330,[],closed,2020-12-28 12:24:23+00:00,2,Fail to reload tokenizer from save_pretrained method,"Hi,
to reproduce:
```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
tokenizer.save_pretrained(""."")
tokenizer = AutoTokenizer.from_pretrained(""."")
```
with error msg:
```
file ./config.json not found
Traceback (most recent call last):
  File ""/data/stars/user/jhou/Test/pytorch_test/huggingface/jc-hou_fork/transformers/src/transformers/configuration_utils.py"", line 389, in get_config_dict
    local_files_only=local_files_only,
  File ""/data/stars/user/jhou/Test/pytorch_test/huggingface/jc-hou_fork/transformers/src/transformers/file_utils.py"", line 1015, in cached_path
    raise EnvironmentError(""file {} not found"".format(url_or_filename))
OSError: file ./config.json not found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/data/stars/user/jhou/Test/pytorch_test/huggingface/jc-hou_fork/transformers/src/transformers/models/auto/tokenization_auto.py"", line 337, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File ""/data/stars/user/jhou/Test/pytorch_test/huggingface/jc-hou_fork/transformers/src/transformers/models/auto/configuration_auto.py"", line 341, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File ""/data/stars/user/jhou/Test/pytorch_test/huggingface/jc-hou_fork/transformers/src/transformers/configuration_utils.py"", line 401, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for '.'. Make sure that:

- '.' is a correct model identifier listed on 'https://huggingface.co/models'

- or '.' is the correct path to a directory containing a config.json file

```
Thanks.
transformers:4.1.0
 tokenizers: @mfuntowicz
 
",2
123,https://github.com/huggingface/transformers/issues/6330,6330,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-07 18:00:19+00:00,4,BertForPreTraining with NSP,"# 鉂?Questions & Help

## Details
I am trying to train BERT from scratch following a modification of https://huggingface.co/blog/how-to-train, where I use a BertTokenizer and BertForPreTraining. The [documentation for BertForPreTraining](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertForPreTraining) states that it has two heads on top for both pre-training processes (MLM and NSP), but [the example provided](https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L874-L884) only provides an example of MLM.

Based on [a comment](https://github.com/huggingface/transformers/issues/2693#issuecomment-580870278) provided by @LysandreJik in a previous issue, it seems that none of the provided datasets (i.e. LineByLineTextDataset) will handle the NSP objective and this objective is excluded because the RoBERTa paper has proven that the NSP objective was not particularly helpful.

@LysandreJik additionally noted that anyone who wants to implement the NSP objective can do so by changing the dataset/training loop, and I was wondering if there were any plans to add support for NSP for the sake of completeness? 

It seems that something similar to what is going on in a PR (https://github.com/huggingface/transformers/pull/6168) for Albert SOP can be done. Is this correct and can anyone provide me with some guidance moving forward?",1
197,https://github.com/huggingface/transformers/issues/6486,6486,[],closed,2020-08-14 13:23:55+00:00,7,from_pretrained() never works,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Linux
- Python version: 3.6
- PyTorch version (GPU?): 1.5.1 (yes)
- Tensorflow version (GPU?): 
- Using GPU in script?: not relevant
- Using distributed or parallel set-up in script?: no

### Who can help
@LysandreJik , @TevenLeScao , @mfuntowicz 
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 tensorflow: @jplu 
documentation: @sgugger
 -->

## Information

Model I am using (Bert, XLNet ...): any

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [X] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [X] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. `import transformers as pt`
2. `pt.AutoModelForSequenceClassification.from_pretrained(<any_valid_model_id>)`
3. Observe the error below

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

```python
>>> pt.AutoModelForSequenceClassification.from_pretrained('xlnet-base-cased')
I0814 15:00:47.832349 46912496391360 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /xxx/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6
I0814 15:00:47.832984 46912496391360 configuration_utils.py:300] Model config XLNetConfig {
  ""architectures"": [
    ""XLNetLMHeadModel""
  ],
  ""attn_type"": ""bi"",
  ""bi_data"": false,
  ""bos_token_id"": 1,
  ""clamp_len"": -1,
  ""d_head"": 64,
  ""d_inner"": 3072,
  ""d_model"": 768,
  ""dropout"": 0.1,
  ""end_n_top"": 5,
  ""eos_token_id"": 2,
  ""ff_activation"": ""gelu"",
  ""initializer_range"": 0.02,
  ""layer_norm_eps"": 1e-12,
  ""mem_len"": null,
  ""model_type"": ""xlnet"",
  ""n_head"": 12,
  ""n_layer"": 12,
  ""pad_token_id"": 5,
  ""reuse_len"": null,
  ""same_length"": false,
  ""start_n_top"": 5,
  ""summary_activation"": ""tanh"",
  ""summary_last_dropout"": 0.1,
  ""summary_type"": ""last"",
  ""summary_use_proj"": true,
  ""task_specific_params"": {
    ""text-generation"": {
      ""do_sample"": true,
      ""max_length"": 250
    }
  },
  ""untie_r"": true,
  ""vocab_size"": 32000
}

Traceback (most recent call last):
  File ""/xxx/.conda/envs/xxx/lib/python3.6/site-packages/transformers/modeling_utils.py"", line 655, in from_pretrained
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/xxx/.conda/envs/xxx/lib/python3.6/site-packages/transformers/modeling_auto.py"", line 1363, in from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
  File ""/xxx/.conda/envs/xxx/lib/python3.6/site-packages/transformers/modeling_utils.py"", line 662, in from_pretrained
    raise EnvironmentError(msg)
OSError: Can't load weights for 'xlnet-base-cased'. Make sure that:

- 'xlnet-base-cased' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'xlnet-base-cased' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.
```

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
A pretrained model should be loaded. This worked (and still works) great in `pytorch_transformers`. I switched to `transformers` because XLNet-based models stopped working in `pytorch_transformers`. But surprise surprise in `transformers` no model whatsoever works for me.",1
234,https://github.com/huggingface/transformers/issues/6566,6566,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-18 10:16:11+00:00,1,Can not convert the pytorch pretrained bert model to onnx model ,"When I convert the pytorch pretrained bert model to onnx model  as follows:

```
import os
import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel

model = BertModel.from_pretrained('bert-base-uncased')

input_1 = torch.LongTensor(1, 14)
input_2 = torch.LongTensor(1, 14)
input_3 = torch.LongTensor(1, 14)
base_path = os.path.dirname(__file__)
onnx_path = os.path.join(base_path, 'bert.onnx')
torch.onnx.export(model, (input_1, input_2, input_3), onnx_path)
```
**raise error**: 
`IndexError: index out of range in self`
     
- `transformers` version: 3.0.2
- Platform: pycharm
- Python version: 3.8
- PyTorch version (GPU?): 1.6.0 no GPU
- onnx version: 1.7.0
- pytorch-pretrained-bert version: 0.6.2

Thank you for the help :)


",1
274,https://github.com/huggingface/transformers/issues/6652,6652,[],closed,2020-08-21 19:41:07+00:00,5,"['encoder.version', 'decoder.version'] are unexpected when loading a pretrained BART model","Using an example from the bart doc:
https://huggingface.co/transformers/model_doc/bart.html#bartforconditionalgeneration

```
from transformers import BartTokenizer, BartForConditionalGeneration
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')
TXT = ""My friends are <mask> but they eat too many carbs.""

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')
input_ids = tokenizer([TXT], return_tensors='pt')['input_ids']
logits = model(input_ids)[0]

masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
probs = logits[0, masked_index].softmax(dim=0)
values, predictions = probs.topk(5)

print(tokenizer.decode(predictions).split())
```
gives:

```
Some weights of the model checkpoint at facebook/bart-large were not used 
when initializing BartForConditionalGeneration: 
['encoder.version', 'decoder.version']

- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
test:9: UserWarning: This overload of nonzero is deprecated:
        nonzero()
Consider using one of the following signatures instead:
        nonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1597302504919/work/torch/csrc/utils/python_arg_parser.cpp:864.)
  masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
['good', 'great', 'all', 'really', 'very']
```

well, there is one more issue of using a weird deprecated `nonzero()` invocation, which has to do with some strange undocumented requirement to pass the `as_tuple` arg, since pytorch 1.5 .https://github.com/pytorch/pytorch/issues/43425

we have `authorized_missing_keys`:
 `authorized_missing_keys = [r""final_logits_bias"", r""encoder\.version"", r""decoder\.version""]`
https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bart.py#L942
which correctly updates `missing_keys`  - should there be also an `authorized_unexpected_keys` which would clean up `unexpected_keys`?

(note: I re-edited this issue once I understood it better to save reader's time, the history is there if someone needs it)

And found another variety of it: for `['model.encoder.version', 'model.decoder.version']`
```
tests/test_modeling_bart.py::BartModelIntegrationTests::test_mnli_inference Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']
- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
PASSED
```",1
277,https://github.com/huggingface/transformers/issues/6657,6657,[],closed,2020-08-22 00:29:08+00:00,3,"Error while loading pretrained model with ""return_dict=True""","# 鉂?Questions & Help

torch: 1.6.0+cu101
Transformers: 3.0.2

**Error with ""return_dict=True""** 

```
from transformers import BertTokenizer, BertForPreTraining
import torch
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForPreTraining.from_pretrained('bert-base-uncased', return_dict=True)
```

```
TypeError                                 Traceback (most recent call last)
<ipython-input-3-5eca8cb45c88> in <module>()
      2 import torch
      3 tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
----> 4 model = BertForPreTraining.from_pretrained('bert-base-uncased', return_dict=True)

/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    670 
    671         # Instantiate model.
--> 672         model = cls(config, *model_args, **model_kwargs)
    673 
    674         if state_dict is None and not from_tf:

TypeError: __init__() got an unexpected keyword argument 'return_dict'
```

",1
413,https://github.com/huggingface/transformers/issues/6957,6957,[],closed,2020-09-04 21:26:28+00:00,2,PRETRAINED_INIT_CONFIGURATION for local model path,"Tokenizers have a special dict `PRETRAINED_INIT_CONFIGURATION`, which tells the tokenization_utils_base, which extra args to pass to the tokenizer's `__init__`, except it doesn't work for the local model, as the hash is for online s3 models.

I have:

```
PRETRAINED_INIT_CONFIGURATION = {
    ""stas/fsmt-wmt19-ru-en"": {
        ""langs"": [""ru"", ""en""],
    },
    ""stas/fsmt-wmt19-en-ru"": {
        ""langs"": [""en"", ""ru""],
    },
    ""stas/fsmt-wmt19-de-en"": {
        ""langs"": [""de"", ""en""],
    },
    ""stas/fsmt-wmt19-en-de"": {
        ""langs"": [""en"", ""de""],
    },
}

```

So in my own code I use

```
   if LOCAL:
        path = ""/code/huggingface/transformers-fair-wmt/data/fsmt-wmt19-ru-en/""
        mname = path
        mname_tok = f""stas/fsmt-wmt19-{src}-{tgt}""
        tokenizer = FSMTTokenizer.from_pretrained(mname_tok)
        model = FSMTForConditionalGeneration.from_pretrained(mname)
    else:
        # # s3 uploaded model
        mname = f""stas/fsmt-wmt19-{src}-{tgt}""
        tokenizer = FSMTTokenizer.from_pretrained(mname)
        model = FSMTForConditionalGeneration.from_pretrained(mname)
```

So `mname_tok` overrides to look up the dict above, since it fails to find that entry using local path.

This, however, doesn't work in tools that aren't under my control, `run_eval.py` in seq2seq for example.

**edit**, more to it - it doesn't pass `PRETRAINED_VOCAB_FILES_MAP` args either for the same reason - it fails to look up those entries for local path. I need to change them all.

Any suggestions on how to fix this problem?",1
449,https://github.com/huggingface/transformers/issues/7023,7023,[],closed,2020-09-09 08:30:20+00:00,3,PreTrained (custom) model not correctly initializing when using AutoModel methods,"## Environment info

- `transformers` version: 3.1.0
- Platform: Windows-10-10.0.18362-SP0
- Python version: 3.8.5
- PyTorch version (GPU?): 1.6.0 (True)
- Tensorflow version (GPU?): 2.3.0 (True)
- Using GPU in script?: yes
- Using distributed or parallel set-up in script?: ???

### Who can help
@LysandreJik 

## Information

Model I am using (Bert, XLNet ...): bert-base-uncased, roberta-base, albert-large-v2

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [X] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [X] my own task or dataset: (give details below)

## To reproduce

(Conceptual) steps to reproduce the behavior:

1) Derive a custom model from a ""PreTrainedModel"" like:

class TransformerLSTMDocClassifier(PreTrainedModel):

    def __init__(self, model_config: AutoConfig):
        
        super(TransformerLSTMDocClassifier, self).__init__(model_config)
        self.transformer = AutoModel.from_config(model_config)
        self.dropout = nn.Dropout(p=model_config.hidden_dropout_prob)
        self.lstm = LSTM(model_config.hidden_size, model_config.hidden_size)
        self.classifier = nn.Sequential(
                        nn.Dropout(p=model_config.hidden_dropout_prob),
                        nn.Linear(model_config.hidden_size, model_config.num_labels),
                        nn.Tanh()
                    )

2) Train (fine-tune) the custom model starting from a pre-trained (standard) model such as bert-base-uncased, roberta-base, albert-large-v2 (using AutoModel features). My objective is to easily exchange the 'transformer' part in the above model

3) Initializing the model class 'TransformerLSTMDocClassifier' via the 'from_pretrained' method with the fine-tuned model (output of step 2) results in the message:

Some weights of TransformerLSTMDocClassifier were not initialized from the model checkpoint at <MODEL_CHECKPOINT> and are newly initialized: [...]

The list of weights apparently includes ALL weights of the TransformerLSTMDocClassifier  (200+in case of using bert-base_uncased)

## Expected behavior

Properly initialized weights

## Addendum

Disclaimer: I am a novice in the ""transformers"" framework. The above described (unexpected) behavior might result from an incorrect use of 'transformers' features/methods at my end.

However, I was digging a bit in the code and made the following observations:

(1) in case of deriving the custom model from a specific class (e.g. BertPreTrainedModel in case of using bert-base-uncased) instead from the generic class 'PreTrainedModel', the custom model is correctly initialized.

(2) the 'cls.base_model_prefix' (module: modeling_utils; method: from_pretrained) is """" (empty string) in case the custom model is derived from ""PreTrainedModel"", resulting in 'has_prefix_module' being set to 'True' in line 925, finally resulting in an inappropriate 'start_prefix' (apparently preventing that weights are matched/loaded).
",1
565,https://github.com/huggingface/transformers/issues/7280,7280,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-09-21 07:34:43+00:00,2,"I want to use the Bert2GPT2 architecture,but my pretrained Bert and GPT2 have different vocabs,so what should I do for the vocabs? ","# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",1
694,https://github.com/huggingface/transformers/issues/7547,7547,[],closed,2020-10-02 22:14:59+00:00,6,Converting Tensorflow checkpoint to Pytorch not work for TF models downloaded using TFAutoModel.from_pretrained(),"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->
I took a try of converting tf checkpoint to pytorch and it works well on the model that in the links on your [page](https://huggingface.co/transformers/converting_tensorflow_models.html)
However, the conversion seems not working with models(bert, albert..) that downloaded using TFAutoModel.from_pretrained()
I am wondering if I miss anything or those models are not currently supported?

Thanks
<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",1
718,https://github.com/huggingface/transformers/issues/7594,7594,[],closed,2020-10-05 19:03:02+00:00,5,RagTokenForGeneration.from_pretrained fails while running demo script,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.3.1
- Platform: Linux-4.4.0-1113-aws-x86_64-with-debian-stretch-sid
- Python version: 3.7.9
- PyTorch version (GPU?): 1.3.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: yes
- Using distributed or parallel set-up in script?: no

### Who can help
@VictorSanh 
@patrickvonplaten 
@sshleifer
transformers/modeling_utils.py
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 examples/bert-loses-patience: @JetRunner
 tensorflow: @jplu
 examples/token-classification: @stefan-it
 documentation: @sgugger
 -->

## Information

Model I am using (Bert, XLNet ...): RAG

The problem arises when using:
* [x] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [x] an official GLUE/SQUaD task: (give the name)
* [ ] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. install new conda env py=3.7
2. install RAG requirements
3. run example code from https://huggingface.co/transformers/master/model_doc/rag.html
```python
Python 3.7.9 (default, Aug 31 2020, 12:42:55)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
>>> import torch
>>> tokenizer = RagTokenizer.from_pretrained(""facebook/rag-token-nq"")
>>> retriever = RagRetriever.from_pretrained(""facebook/rag-token-nq"", index_name=""exact"", use_dummy_dataset=True)
Using custom data configuration dummy.psgs_w100.nq.no_index
Reusing dataset wiki_dpr (/homes/thielk/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.no_index/0.0.0/14b973bf2a456087ff69c0fd34526684eed22e48e0dfce4338f9a22b965ce7c2)
Using custom data configuration dummy.psgs_w100.nq.exact
Reusing dataset wiki_dpr (/homes/thielk/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.exact/0.0.0/14b973bf2a456087ff69c0fd34526684eed22e48e0dfce4338f9a22b965ce7c2)
>>> model = RagTokenForGeneration.from_pretrained(""facebook/rag-token-nq"", retriever=retriever)
```

stack trace:

```python
Traceback (most recent call last):
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/tarfile.py"", line 187, in nti
    n = int(s.strip() or ""0"", 8)
ValueError: invalid literal for int() with base 8: 'del.embe'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/tarfile.py"", line 2289, in next
    tarinfo = self.tarinfo.fromtarfile(self)
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/tarfile.py"", line 1095, in fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/tarfile.py"", line 1037, in frombuf
    chksum = nti(buf[148:156])
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/tarfile.py"", line 189, in nti
    raise InvalidHeaderError(""invalid header"")
tarfile.InvalidHeaderError: invalid header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/site-packages/torch/serialization.py"", line 595, in _load
    return legacy_load(f)
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/site-packages/torch/serialization.py"", line 506, in legacy_load
    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/tarfile.py"", line 1593, in open
    return func(name, filemode, fileobj, **kwargs)
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/tarfile.py"", line 1623, in taropen
    return cls(name, mode, fileobj, **kwargs)
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/tarfile.py"", line 1486, in __init__
    self.firstmember = self.next()
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/tarfile.py"", line 2301, in next
    raise ReadError(str(e))
tarfile.ReadError: invalid header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/site-packages/transformers/modeling_utils.py"", line 927, in from_pretrained
    state_dict = torch.load(resolved_archive_file, map_location=""cpu"")
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/site-packages/torch/serialization.py"", line 426, in load
    return _load(f, map_location, pickle_module, **pickle_load_args)
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/site-packages/torch/serialization.py"", line 599, in _load
    raise RuntimeError(""{} is a zip archive (did you mean to use torch.jit.load()?)"".format(f.name))
RuntimeError: /homes/thielk/.cache/torch/transformers/06fe449ffe41cbe16aeb1f5976989313464a3c44a605e9a8b91bf6440dfa6026.696574d8c17eafbac08f43f01e951252057f8feb133b64a33b76d4c47d65367a is a zip archive (did you mean to use torch.jit.load()?)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/homes/thielk/miniconda3/envs/transformers-pytorch/lib/python3.7/site-packages/transformers/modeling_utils.py"", line 930, in from_pretrained
    ""Unable to load weights from pytorch checkpoint file. ""
OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.
```
<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior
be able to completely run example code from RAG documentation
May be related to #7583 
<!-- A clear and concise description of what you would expect to happen. -->
```python
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
import torch
tokenizer = RagTokenizer.from_pretrained(""facebook/rag-token-nq"")
retriever = RagRetriever.from_pretrained(""facebook/rag-token-nq"", index_name=""exact"", use_dummy_dataset=True)
# initialize with RagRetriever to do everything in one forward call
model = RagTokenForGeneration.from_pretrained(""facebook/rag-token-nq"", retriever=retriever)
input_dict = tokenizer.prepare_seq2seq_batch(""How many people live in Paris?"", ""In Paris, there are 10 million people."", return_tensors=""pt"")
input_ids = input_dict[""input_ids""]
outputs = model(input_ids=input_ids, labels=input_dict[""labels""])
# or use retriever seperately
model = RagTokenForGeneration.from_pretrained(""facebook/rag-token-nq"", use_dummy_dataset=True)
# 1. Encode
question_hidden_states = model.question_encoder(input_ids)[0]
# 2. Retrieve
docs_dict = retriever(input_ids.numpy(), question_hidden_states.detach().numpy(), return_tensors=""pt"")
doc_scores = torch.bmm(question_hidden_states.unsqueeze(1), docs_dict[""retrieved_doc_embeds""].float().transpose(1, 2)).squeeze(1)
# 3. Forward to generator
outputs = model(context_input_ids=docs_dict[""context_input_ids""], context_attention_mask=docs_dict[""context_attention_mask""], doc_scores=doc_scores, decoder_input_ids=input_dict[""labels""])
# or directly generate
generated = model.generate(input_ids=input_dict[""input_ids""])
generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)
```",1
798,https://github.com/huggingface/transformers/issues/7739,7739,[],closed,2020-10-12 16:09:28+00:00,1,Cannot load pretrained microsoft's layoutlm,"## Environment info
     
- `transformers` version: 3.3.1
- Platform: Linux-4.15.0-112-generic-x86_64-with-glibc2.10
- Python version: 3.8.5
- PyTorch version (GPU?): 1.4.0 (False)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No


### Who can help

@julien-c @sshleifer 

## Information

Model I am using: LayoutLM

The problem arises when using:
* [x] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [x] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

```
from transformers import LayoutLMTokenizer, LayoutLMForTokenClassification
import torch

tokenizer = LayoutLMTokenizer.from_pretrained('microsoft/layoutlm-base-uncased')
model = LayoutLMForTokenClassification.from_pretrained('microsoft/layoutlm-base-uncased')
```

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
~/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    926             try:
--> 927                 state_dict = torch.load(resolved_archive_file, map_location=""cpu"")
    928             except Exception:

~/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)
    526         if _is_zipfile(opened_file):
--> 527             with _open_zipfile_reader(f) as opened_zipfile:
    528                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)

~/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/serialization.py in __init__(self, name_or_buffer)
    223     def __init__(self, name_or_buffer):
--> 224         super(_open_zipfile_reader, self).__init__(torch._C.PyTorchFileReader(name_or_buffer))
    225 

RuntimeError: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /pytorch/caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /pytorch/caffe2/serialize/inline_container.cc:132)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7faec0a9f193 in /home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/lib/libc10.so)
frame #1: caffe2::serialize::PyTorchStreamReader::init() + 0x1f5b (0x7faec3c279eb in /home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/lib/libtorch.so)
frame #2: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::string const&) + 0x64 (0x7faec3c28c04 in /home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x6c1ef6 (0x7faf0bb54ef6 in /home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #4: <unknown function> + 0x295928 (0x7faf0b728928 in /home/user/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #5: PyCFunction_Call + 0x56 (0x5640b0549f76 in /home/user/anaconda3/envs/nlp/bin/python)
frame #6: _PyObject_MakeTpCall + 0x22f (0x5640b050785f in /home/user/anaconda3/envs/nlp/bin/python)
frame #7: <unknown function> + 0x18bfdc (0x5640b0555fdc in /home/user/anaconda3/envs/nlp/bin/python)
frame #8: PyVectorcall_Call + 0x71 (0x5640b0507041 in /home/user/anaconda3/envs/nlp/bin/python)
frame #9: <unknown function> + 0x18c92a (0x5640b055692a in /home/user/anaconda3/envs/nlp/bin/python)
frame #10: _PyObject_MakeTpCall + 0x1a4 (0x5640b05077d4 in /home/user/anaconda3/envs/nlp/bin/python)
frame #11: _PyEval_EvalFrameDefault + 0x4596 (0x5640b058ef56 in /home/user/anaconda3/envs/nlp/bin/python)
frame #12: _PyEval_EvalCodeWithName + 0x659 (0x5640b0554e19 in /home/user/anaconda3/envs/nlp/bin/python)
frame #13: _PyObject_FastCallDict + 0x20c (0x5640b055648c in /home/user/anaconda3/envs/nlp/bin/python)
frame #14: _PyObject_Call_Prepend + 0x63 (0x5640b0556733 in /home/user/anaconda3/envs/nlp/bin/python)
frame #15: <unknown function> + 0x18c8ca (0x5640b05568ca in /home/user/anaconda3/envs/nlp/bin/python)
frame #16: _PyObject_MakeTpCall + 0x1a4 (0x5640b05077d4 in /home/user/anaconda3/envs/nlp/bin/python)
frame #17: _PyEval_EvalFrameDefault + 0x475 (0x5640b058ae35 in /home/user/anaconda3/envs/nlp/bin/python)
frame #18: _PyEval_EvalCodeWithName + 0x2d2 (0x5640b0554a92 in /home/user/anaconda3/envs/nlp/bin/python)
frame #19: _PyFunction_Vectorcall + 0x1e3 (0x5640b0555943 in /home/user/anaconda3/envs/nlp/bin/python)
frame #20: <unknown function> + 0x10011a (0x5640b04ca11a in /home/user/anaconda3/envs/nlp/bin/python)
frame #21: _PyEval_EvalCodeWithName + 0x7df (0x5640b0554f9f in /home/user/anaconda3/envs/nlp/bin/python)
frame #22: <unknown function> + 0x18bd20 (0x5640b0555d20 in /home/user/anaconda3/envs/nlp/bin/python)
frame #23: <unknown function> + 0x10077f (0x5640b04ca77f in /home/user/anaconda3/envs/nlp/bin/python)
frame #24: _PyEval_EvalCodeWithName + 0x2d2 (0x5640b0554a92 in /home/user/anaconda3/envs/nlp/bin/python)
frame #25: PyEval_EvalCodeEx + 0x44 (0x5640b0555754 in /home/user/anaconda3/envs/nlp/bin/python)
frame #26: PyEval_EvalCode + 0x1c (0x5640b05e3edc in /home/user/anaconda3/envs/nlp/bin/python)
frame #27: <unknown function> + 0x24f083 (0x5640b0619083 in /home/user/anaconda3/envs/nlp/bin/python)
frame #28: <unknown function> + 0x140699 (0x5640b050a699 in /home/user/anaconda3/envs/nlp/bin/python)
frame #29: <unknown function> + 0xfeb84 (0x5640b04c8b84 in /home/user/anaconda3/envs/nlp/bin/python)
frame #30: _PyGen_Send + 0x149 (0x5640b054edc9 in /home/user/anaconda3/envs/nlp/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x49a3 (0x5640b058f363 in /home/user/anaconda3/envs/nlp/bin/python)
frame #32: _PyGen_Send + 0x149 (0x5640b054edc9 in /home/user/anaconda3/envs/nlp/bin/python)
frame #33: _PyEval_EvalFrameDefault + 0x49a3 (0x5640b058f363 in /home/user/anaconda3/envs/nlp/bin/python)
frame #34: _PyGen_Send + 0x149 (0x5640b054edc9 in /home/user/anaconda3/envs/nlp/bin/python)
frame #35: <unknown function> + 0x1701cd (0x5640b053a1cd in /home/user/anaconda3/envs/nlp/bin/python)
frame #36: <unknown function> + 0x10075e (0x5640b04ca75e in /home/user/anaconda3/envs/nlp/bin/python)
frame #37: _PyFunction_Vectorcall + 0x10b (0x5640b055586b in /home/user/anaconda3/envs/nlp/bin/python)
frame #38: <unknown function> + 0xfeb84 (0x5640b04c8b84 in /home/user/anaconda3/envs/nlp/bin/python)
frame #39: _PyFunction_Vectorcall + 0x10b (0x5640b055586b in /home/user/anaconda3/envs/nlp/bin/python)
frame #40: <unknown function> + 0x10075e (0x5640b04ca75e in /home/user/anaconda3/envs/nlp/bin/python)
frame #41: _PyEval_EvalCodeWithName + 0x2d2 (0x5640b0554a92 in /home/user/anaconda3/envs/nlp/bin/python)
frame #42: _PyFunction_Vectorcall + 0x1e3 (0x5640b0555943 in /home/user/anaconda3/envs/nlp/bin/python)
frame #43: <unknown function> + 0x18be79 (0x5640b0555e79 in /home/user/anaconda3/envs/nlp/bin/python)
frame #44: PyVectorcall_Call + 0x71 (0x5640b0507041 in /home/user/anaconda3/envs/nlp/bin/python)
frame #45: _PyEval_EvalFrameDefault + 0x1fdb (0x5640b058c99b in /home/user/anaconda3/envs/nlp/bin/python)
frame #46: _PyEval_EvalCodeWithName + 0x659 (0x5640b0554e19 in /home/user/anaconda3/envs/nlp/bin/python)
frame #47: <unknown function> + 0x18bd20 (0x5640b0555d20 in /home/user/anaconda3/envs/nlp/bin/python)
frame #48: <unknown function> + 0x10011a (0x5640b04ca11a in /home/user/anaconda3/envs/nlp/bin/python)
frame #49: <unknown function> + 0x215056 (0x5640b05df056 in /home/user/anaconda3/envs/nlp/bin/python)
frame #50: <unknown function> + 0x1847f3 (0x5640b054e7f3 in /home/user/anaconda3/envs/nlp/bin/python)
frame #51: <unknown function> + 0x140699 (0x5640b050a699 in /home/user/anaconda3/envs/nlp/bin/python)
frame #52: <unknown function> + 0xfeb84 (0x5640b04c8b84 in /home/user/anaconda3/envs/nlp/bin/python)
frame #53: _PyEval_EvalCodeWithName + 0x659 (0x5640b0554e19 in /home/user/anaconda3/envs/nlp/bin/python)
frame #54: _PyFunction_Vectorcall + 0x1e3 (0x5640b0555943 in /home/user/anaconda3/envs/nlp/bin/python)
frame #55: <unknown function> + 0x10075e (0x5640b04ca75e in /home/user/anaconda3/envs/nlp/bin/python)
frame #56: <unknown function> + 0x215056 (0x5640b05df056 in /home/user/anaconda3/envs/nlp/bin/python)
frame #57: <unknown function> + 0x1847f3 (0x5640b054e7f3 in /home/user/anaconda3/envs/nlp/bin/python)
frame #58: <unknown function> + 0x140699 (0x5640b050a699 in /home/user/anaconda3/envs/nlp/bin/python)
frame #59: <unknown function> + 0xfeb84 (0x5640b04c8b84 in /home/user/anaconda3/envs/nlp/bin/python)
frame #60: _PyEval_EvalCodeWithName + 0x659 (0x5640b0554e19 in /home/user/anaconda3/envs/nlp/bin/python)
frame #61: <unknown function> + 0x18bd20 (0x5640b0555d20 in /home/user/anaconda3/envs/nlp/bin/python)
frame #62: <unknown function> + 0xfeb84 (0x5640b04c8b84 in /home/user/anaconda3/envs/nlp/bin/python)
frame #63: <unknown function> + 0x215056 (0x5640b05df056 in /home/user/anaconda3/envs/nlp/bin/python)


During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
<ipython-input-6-29a9c0941587> in <module>
      3 
      4 tokenizer = LayoutLMTokenizer.from_pretrained('microsoft/layoutlm-base-uncased')
----> 5 model = LayoutLMForTokenClassification.from_pretrained('microsoft/layoutlm-base-uncased')
      6 

~/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    927                 state_dict = torch.load(resolved_archive_file, map_location=""cpu"")
    928             except Exception:
--> 929                 raise OSError(
    930                     ""Unable to load weights from pytorch checkpoint file. ""
    931                     ""If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. ""

OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. 
```
## Expected behavior

Load pretrained layoutlm
",1
877,https://github.com/huggingface/transformers/issues/7889,7889,[],closed,2020-10-18 20:04:58+00:00,4,from_pretrained incompatible with the models being downloaded,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 2.3.0
- Platform: Linux
- Python version: 3.7.7

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 -->
@LysandreJik 

## Information

Model I am using (Bert, XLNet ...):

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [x] my own modified scripts: (give details below)

```
import logging
logging.basicConfig(level=logging.DEBUG) 
from transformers import TFAlbertModel 
model = TFAlbertModel.from_pretrained('albert-xxlarge-v2') 
```

**Problem:**
The model does not load the weights. I assume due to some incompatibility. The output:
```
INFO:transformers.modeling_tf_utils:Layers of TFAlbertModel not initialized from pretrained model: ['pooler', 'encoder', 'embeddings']
INFO:transformers.modeling_tf_utils:Layers from pretrained model not used in TFAlbertModel: ['albert', 'predictions']
```

I suppose the model from https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v2-tf_model.h5 has been changed and is incompatible now. I cannot find the older version of the model anywhere.

## To reproduce

Steps to reproduce the behavior:

(See script above)
1. (Assume) Clean model cache
2. Enable full logging
3. Run TFAlbertModel.from_pretrained('albert-xxlarge-v2')


<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

It should load the weights correctly (or at least give an error/warning), not simply fail silently!

<!-- A clear and concise description of what you would expect to happen. -->

---

## Question

Is there a way to access older Albert models (for `transformers==2.3.0`)?
Those from https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v2-tf_model.h5 seem to be incompatible.

I really cannot modify the code now or upgrade `transformers` version, but it would totally save me if someone had available the older version of the model. Would it be possible to share (at least all Albert models)?
",1
959,https://github.com/huggingface/transformers/issues/8072,8072,[],closed,2020-10-26 22:35:23+00:00,4,`BartForConditionalGeneration.from_pretrained` suddenly fails,"I have been using the same `BartForConditionalGeneration` model and `transformers==3.0.2` for weeks, but today the same code threw a new error that has never happened before. It says I am passing an unexpected `output_past` parameter to `from_pretrained`. I am loading the `facebook/bart-large` model.

The line throwing the error is `BartForConditionalGeneration.from_pretrained(""facebook/bart-large"", output_past=True)`

```
/usr/local/lib/python3.6/dist-packages/qaeval/generation/model.py in __init__(self, vocab, model_name, max_decoding_steps, beam_size)
     59                  beam_size: int = 4) -> None:
     60         super().__init__(vocab)
---> 61         self.bart = BartForConditionalGeneration.from_pretrained(model_name, output_past=True)
     62         self.tokenizer = PretrainedTransformerTokenizer(model_name)
     63 

/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    670 
    671         # Instantiate model.
--> 672         model = cls(config, *model_args, **model_kwargs)
    673 
    674         if state_dict is None and not from_tf:

TypeError: __init__() got an unexpected keyword argument 'output_past'
```

I am running this code in a Jupyter Notebook. This morning the code ran (I have all of the output saved). When I make a copy of the notebook and rerun it now without making any changes, it fails with the above error. I see that the `output_past` parameter was removed [here](https://github.com/huggingface/transformers/pull/3632/files/904b387af42744f9141a6dc4be698a5815ce5bbd), but that does not explain why it was working up until just a few hours ago.

I can see in the saved output between the two notebooks that one of the files that transformers first downloads when loading the model used to be 1.26kb in size, but now it's 1.52k. I assume this is the config file for `facebook/bart-large`.

Did anything change to that file within the past few hours? I don't know where to look for the copy of that file to see what happened. I am quite perplexed by this issue.",1
1003,https://github.com/huggingface/transformers/issues/8174,8174,[],closed,2020-10-30 08:15:39+00:00,4,"Possible bug in ""trainer"" when training ""BertForPretraining.from_pretrained()""","## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
Environment is colab with GPU enabled.  modules are provided in the Jupyter notebook on Google Drive here:

[https://colab.research.google.com/drive/1UX6NMXA2cHGUtDJwh_U6LL-kyd8Gyt9y?usp=sharing](https://colab.research.google.com/drive/1UX6NMXA2cHGUtDJwh_U6LL-kyd8Gyt9y?usp=sharing)



### Who can help
@sgugger

<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 examples/bert-loses-patience: @JetRunner
 tensorflow: @jplu
 examples/token-classification: @stefan-it
 documentation: @sgugger
 -->

## Information

Model I am using BERT 
BertForPreTraining.from_pretrained(""bert-base-uncased"")

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [X] my own modified scripts: Error can be reproduced with the Notebook provided above.

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [X] my own task or dataset: (give details below)
I am attempting to ""fine-tune"" bert-base-uncased by training it on additional sentences.  I am unable to do this with Trainer - I get an error message shown in the notebook.

## To reproduce

Steps to reproduce the behavior:

1. Execute the notebook
2. First example succeeds with model BertLMHeadModel.from_pretrained(""bert-base-uncased"")
3. Second example fails at train() simply by changing the model to BertForPreTraining.from_pretrained(""bert-base-uncased"")

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

The bug is entirely reproduced on the linked Jupyter Notebook above, when run on Google Colab.  The error message is:

RuntimeError: grad can be implicitly created only for scalar outputs

## Expected behavior
The model ""BertForPretraining.from_pretrained(""bert-base-uncased"") should train on the two sentences provided.

<!-- A clear and concise description of what you would expect to happen. -->

If you know a work-around for this bug, I will appreciate it.

Sylvain - good to see you doing interesting work!! - Dana Ludwig (student of fast.ai course and owner of your book)
",1
1065,https://github.com/huggingface/transformers/issues/8292,8292,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-11-04 14:01:07+00:00,16,Fine Tune Bert Ner using TFBertForTokenClassification.from_pretrained,"Hey,

I am new to the transformers Bert Training world and am trying to fine tune Bert model for NER on a coNll like dataset. But its not training the model and giving the below error

ValueError: No gradients provided for any variable: ['tf_bert_for_token_classification_8/classifier/kernel:0', 'tf_bert_for_token_classification_8/classifier/bias:0'].


Below is my code

```py
tr_inputs = tf.convert_to_tensor(tr_inputs)
val_inputs = tf.convert_to_tensor(val_inputs)
tr_tags = tf.convert_to_tensor(tr_tags)
val_tags = tf.convert_to_tensor(val_tags)
tr_masks = tf.convert_to_tensor(tr_masks)
val_masks = tf.convert_to_tensor(val_masks)
tr_segs = tf.convert_to_tensor(tr_segs)
val_segs = tf.convert_to_tensor(val_segs)

input_features_dict = {""input_ids"":tr_inputs, ""attention_mask"":tr_masks, ""token_type_ids"":tr_segs, 'labels':tr_tags}
val_features_dict = {""input_ids"":val_inputs, ""attention_mask"":val_masks, ""token_type_ids"":val_segs, 'labels':tr_tags}
train_data = tf.data.Dataset.from_tensor_slices(input_features_dict)
batch_train_data = train_data.batch(batch_num)
valid_data = tf.data.Dataset.from_tensor_slices(val_features_dict)
batch_valid_data = valid_data.batch(batch_num)
modell = TFBertForTokenClassification.from_pretrained('bert-base-uncased',num_labels=len(tag2idx))
modell.layers[2].activation = tf.keras.activations.softmax
modell.layers[0].trainable = False
modell.compile(optimizer=optimizer, loss=loss, metrics=[metrics])
modell.fit(batch_train_data, epochs=epochs, validation_data=batch_val_data)
```

Not sure what needs to be done. Any advice/pointers on this would be highly helpful for me.",1
1177,https://github.com/huggingface/transformers/issues/8513,8513,[],closed,2020-11-13 06:49:24+00:00,1,Using Pretrained BERT model to add additional words that are not recognized by the model,"Hello,

I want some help regarding adding additional words in the existing BERT model. I have two quires kindly guide me:

I am working on NER task for a domain:

There are few words (not sure the exact numbers) that BERT recognized as [UNK], but those entities are required for the model to recognize. The pretrained model learns well (up to 80%) accuracy on ""bert-base-cased"" while providing labeled data and fine-tune the model but intuitively the model will learn better if it recognize all the entities.

1. Do i need to add those unknown entities in vocabs.txt and train the model again?


2. Do i need to train the BERT model on my data from Scratch?

Thanks... 
",1
1344,https://github.com/huggingface/transformers/issues/8864,8864,[],closed,2020-12-01 01:59:44+00:00,6,AttributeError: 'NoneType' object has no attribute 'from_pretrained',"This code was working yesterday but doesn't work today:

```py
from transformers import AutoTokenizer
AutoTokenizer(""Helsinki-NLP/opus-mt-en-fr"")
```",1
1523,https://github.com/huggingface/transformers/issues/9207,9207,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-12-19 04:39:18+00:00,2,Saving Pretrained Tokenizer,"I created a custom `tokenizers.Tokenizer` and saved it as follows

```
tokenizer.model.save(""./tokenizer"")
tokenizer.save(""./tokenizer.json"") 
```

This produces 3 files, merges.txt, vocab.json and tokenizer.json

Then I created a `transformers.RobertaTokenizerFast` and saved it to the same folder

```
tokenizer = RobertaTokenizerFast.from_pretrained(""./tokenizer"")  
tokenizer.save_pretrained(""./tokenizer"") 
```

This adds `special_tokens_map.json` and `tokenizer_config.json`

I then saved it to another folder to simulate what happens when I train my model

```
tokenizer.save_pretrained(""./model"") 
tokenizer = RobertaTokenizerFast.from_pretrained(""./model"")  
```

What I noticed was `tokenizer_config.json` contains a key `name_or_path` which still points to `./tokenizer`, so what seems to be happening is `RobertaTokenizerFast.from_pretrained(""./model"") ` is loading files from two places (./model and ./tokenizer)

Not sure if this is expected, it seems that the tokenizer_config.json should be updated in save_pretrained, and tokenizer.json should be saved with it?

Or perhaps this is just an issue because I'm training my tokenizer in a subdirectory of the model folder?
",1
2163,https://github.com/huggingface/transformers/issues/10368,10368,[],closed,2021-02-24 08:03:29+00:00,7,TFMarianModel from_pretrained can't load weights,"## Environment info
- `transformers` version: 4.3.2
- Platform: Windows-7-6.1.7601-SP1
- Python version: 3.6.6
- PyTorch version (GPU?): 1.5.1+cpu (False)
- Tensorflow version (GPU?): 2.3.0 (False)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help
@patrickvonplaten

## Information

Model I am using (Bert, XLNet ...): TFMarianMT

The problem arises when using:
* [+] the official example scripts: (give details below)

## To reproduce
```python
from transformers import MarianTokenizer, TFMarianModel
model = TFMarianModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')

```
Steps to reproduce the behavior:

1. Run the above code 
2. Get an error:

> Exception has occurred: OSError       (note: full exception trace is shown but execution is paused at: _run_module_as_main)
> Can't load weights for 'Helsinki-NLP/opus-mt-en-de'. Make sure that:
> 
> - 'Helsinki-NLP/opus-mt-en-de' is a correct model identifier listed on 'https://huggingface.co/models'
> 
> - or 'Helsinki-NLP/opus-mt-en-de' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.
> 
>   File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\site-packages\transformers\modeling_tf_utils.py"", line 1219, in from_pretrained
>     raise EnvironmentError(msg)
>   File ""C:\Users\FA.PROJECTOR-MSK\Google 袛懈褋泻\Colab Notebooks\PoetryTransformer\Unsupervised\translation\paraphrases_translation.py"", line 14, in <module>
>     model = TFMarianModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')
>   File ""C:\Users\FA.PROJECTOR-MSK\Google 袛懈褋泻\Colab Notebooks\PoetryTransformer\Unsupervised\translation\run_locally.py"", line 1, in <module>
>     from paraphrases_translation import run
>   File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\runpy.py"", line 85, in _run_code
>     exec(code, run_globals)
>   File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\runpy.py"", line 96, in _run_module_code
>     mod_name, mod_spec, pkg_name, script_name)
>   File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\runpy.py"", line 263, in run_path
>     pkg_name=pkg_name, script_name=fname)
>   File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\runpy.py"", line 85, in _run_code
>     exec(code, run_globals)
>   File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\runpy.py"", line 193, in _run_module_as_main (Current frame)
>     ""__main__"", mod_spec)
> 

## Expected behavior
No error
",1
64,https://github.com/huggingface/transformers/issues/6193,6193,[],closed,2020-08-01 23:07:00+00:00,8,Some weights not initialized in pre-trained RobertaForMaskedLM,"The bug is similar to #2202.

I am trying to evaluate MLM perplexity (without training/finetuning) using Roberta with `run_language_modeling.py` (from the [official example](https://github.com/huggingface/transformers/tree/master/examples/language-modeling)). However, some weights seems to be reinitialized instead of getting loading from the pretrained Roberta checkpoint.

## To Reproduce (~~with master branch~~):

```
import logging
logging.basicConfig(level=logging.INFO)
from transformers import RobertaForMaskedLM
_ = RobertaForMaskedLM.from_pretrained('roberta-base')
```

It gives the following warning message:
```
WARNING:transformers.modeling_utils:Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

The perplexities I get on direct evaluation on Wikitext-2/103 datasets are also much higher than the official Roberta implementation from fairseq. I suspect this could be the reason.",1
209,https://github.com/huggingface/transformers/issues/6513,6513,[],closed,2020-08-16 07:04:17+00:00,2,Longformer pretrained weights are not really pretrained?,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version:3.0.2
- Platform:Ubuntu 18.04
- Python version:3.7
- PyTorch version (GPU?):1.6 Yes
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No

 @patrickvonplaten


Model I am using (Bert, XLNet ...): Longformer

The problem arises when using:
* [V] my own modified scripts

The tasks I am working on is:
* [V] my own task or dataset: (give details below)

## To reproduce

As a preprocessing step in my pipeline I train a pre-trained model on a subset of Wikipedia dataset.
When using roBERTa the results of my fine-tuning steps are as follows (LM task):
![image](https://user-images.githubusercontent.com/31047807/90328740-71ccc880-dfa7-11ea-9df5-0163ae48533b.png)

Unfortunately, when simply plugging in longformer (and pointing the pretrained path to `allenai/longformer-base-4096`
![image](https://user-images.githubusercontent.com/31047807/90328734-5e216200-dfa7-11ea-888b-53e759fd2e29.png)
Which seems like the weights are simply random-initialized.
I both cases (roberta and longformer) I get the message stating the weights have been initialized.
What went wrong?
",0
261,https://github.com/huggingface/transformers/issues/6628,6628,[],closed,2020-08-21 00:59:57+00:00,6,PreTrainedModel's tie_weights invocation needs to be configurable,"`PreTrainedModel` defines `tie_weights` method and then in [one place suggests](https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_utils.py#L512)

> Takes care of tying weights embeddings afterwards if the model class has a :obj:`tie_weights()` method.

But since the super-class has it defined, it's always there. 

So the only way for a sub-class to avoid this ""tying"" is to override it with:
```
     def tie_weights(self): pass
```

if nothing else happens that comment needs to be edited to suggest a noop override in the sub-class.

But it took some hunting to get there, so a better solution is needed.

Most likely, currently, most (all?) models in transformers with encoder/decoder share token embed weights, hence the issue didn't come up. I'm working on porting a fairseq transformer and there the enc/dec token embeds aren't shared. 

I propose a solution which adds a new param to `PretrainedConfig`, say: `is_enc_dec_sharing_embeds=True` and let the subclass override those, then add at the start of `tie_weights` in `modeling_utils.py`

```
     def tie_weights(self):
         if not self.config.is_enc_dec_sharing_embeds:
             return
```
that way it's easy to quickly become aware that an action needs to be taken and set the desired behavior from within the subclass.

Thoughts?

If the proposed solution is agreeable, please, let me know which config param name it should be `is_enc_dec_sharing_embeds` - or different and I will submit a PR.

Thank you.

**edit:**

OK, having had a closer look:
```
grep -r -A 2  'def tie_weights' src/transformers | grep pass | wc -l
```
we have 5 sub-classes that override it with a no-op so only some rely on the default. Bad superclass, no cookies for you.
",0
298,https://github.com/huggingface/transformers/issues/6702,6702,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-25 01:08:49+00:00,1,Questions on the date of Wikipedia dumps for pretrained checkpoints (BERT and RoBERTa).,"# 鉂?Questions & Help

Hi, I really appreciate the Huggingface and the fantastic code for pretrained LMs.
I am trying to figure out the date of Wikipedia dumps used for pretrained checkpoints. (BERT and RoBERTa)



<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->
I'd like to know the date (month, year) of Wikipedia dumps that were used for the current pretrained checkpoints of BERT-base uncased and RoBERTa-base and large.
I am looking for an older version of pretrained checkpoints that were trained on a Wikipedia dump before 2019.
If available, is there a way to get the older version of pretrained checkpoints (before 2019)?
Thanks!

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",0
386,https://github.com/huggingface/transformers/issues/6901,6901,[],closed,2020-09-02 06:59:02+00:00,7,Relaxing `PreTrainedModel` requirement in _save,"# 馃殌 Feature request
It's great to see that `Trainer` is becoming flexible. Each functions seems to be more self contained now making inheritance easier. I've experimented with many custom models. For instance, 
```
class Model(nn.Module):
    def __init__(self, ..):
        self.encoder = AutoModel.from_pretrained(..)
        self.custom_modules = ..
    def forward(self, **kwargs):
        output = self.encoder(**kwargs)
        # some custom operations
```
Many users are required to create custom models if they just don't want simple `SequenceClassification` head. In all cases, I have to override `_save` method because of [this line](https://github.com/huggingface/transformers/blob/d822ab636b6a14ed50f7bca0797c1de42c19de61/src/transformers/trainer.py#L1097) which explicitly puts a restriction on `Trainer` to be used with models that inherit from `PreTrainedModel`. It would be good to relax this requirement and give a warning about not using `PreTrainedModel` instead.

## Your contribution
I'll open a PR if I get approval.
",0
428,https://github.com/huggingface/transformers/issues/6985,6985,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-09-07 09:13:00+00:00,2,Enhance a MarianMT pretrained model from HuggingFace with more training data,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**: https://stackoverflow.com/questions/63774619/enhance-a-marianmt-pretrained-model-from-huggingface-with-more-training-data",0
456,https://github.com/huggingface/transformers/issues/7036,7036,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-09-10 05:32:28+00:00,7,"""You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference"" when I am finetuning on distilert pretrained model, After printing this it is taking a lot of time and using only one CPU, how can we parallelized to all the cores in the system ( even I hve 8 GPU's but it is not using tht)","# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",0
822,https://github.com/huggingface/transformers/issues/7781,7781,[],closed,2020-10-14 14:53:35+00:00,2,`decoder_config` variable not defined in EncoderDecoderModel.from_encoder_decoder_pretrained,"https://github.com/huggingface/transformers/blob/890e790e16084e58a1ecb9329c98ec3e76c45994/src/transformers/modeling_encoder_decoder.py#L330

Using this function results in an error:

`UnboundLocalError: local variable 'decoder_config' referenced before assignment`

Suggest changing `decoder_config.add_cross_attention` to `kwargs_decoder[""config""].add_cross_attention`

",0
4,https://github.com/huggingface/transformers/issues/6083,6083,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-28 03:11:43+00:00,1,Error when using np.where() during squad tokenization ,"# 馃悰 Bug

## Information

Model I am using (Bert, XLNet ...):Bert

Language I am using the model on (English, Chinese ...):English

The problem arises when using:
* [x] the official example scripts: squad1.1
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [x] an official GLUE/SQUaD task: SQUaD task

* [ ] my own task or dataset: (give details below)

## To reproduce
 I'm confused about this line:
https://github.com/huggingface/transformers/blob/896300177bf9f35feac4698370212a80a5ab6138/src/transformers/data/processors/squad.py#L230

Should this be  
```
pad_token_indices = np.where(np.array(span[""input_ids""]) == tokenizer.pad_token_id)
```
Cause ""p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)"", and the padding positions should not be answers

Steps to reproduce the behavior:
```python
span[""input_ids""] = [1,1,3,0,0,0]
np.where([1,1,3,0,0,0] == tokenizer.pad_token_id)
# return (array([], dtype=int64),)

np.where(np.array([1,1,3,0,0,0]) == tokenizer.pad_token_id)
# return (array([3, 4, 5]),)
```

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
As described above.

## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 7a68d401388bc68f10dfeb591709352736a6c0b6
- Platform:
- Python version: python3.6.8
- PyTorch version (GPU?):1.5.0 CPU
- Tensorflow version (GPU?):
- Using GPU in script?: NO
- Using distributed or parallel set-up in script?:NO
",0
7,https://github.com/huggingface/transformers/issues/6088,6088,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-28 09:09:32+00:00,1,Finetuning German BERT for QA on biomedical domain ,"Hello there and thank you very much for this wonderful work. I am relatively new to this field, so please bear with my amateur question. I want to perform question-answering on a German Biomedical text. From what I understand up to now, I need to fine-tune German BERT on biomedical QA datasets. Is there any script/pipeline that I should be using for this?
I have also posted in StackOverflow and the huggingface forum, but to no avail as of now. 
Thank you very much in advance.",0
8,https://github.com/huggingface/transformers/issues/6090,6090,[],closed,2020-07-28 10:22:25+00:00,2,customize special tokens,"# 鉂?Questions & Help

## Details

I am trying to add more special tokens in order to train new language models without modifying the model architecture but simply modifying the input data. For example, adding more separators to involving more complex structure information. 

Is there a simple, clear, consistent way to add/customize special tokens. 
1. The function `add_special_tokens` can only add token string to token attribute specified in `SPECIAL_TOKENS_ATTRIBUTES`
2. I tried to extend this list and `add_special_tokens` works, but calling 
`tokenizer.get_special_tokens_mask` report that _XXX does not exist, where XXX is the new customized special token attribute

finally I found a solution: modifying `tokenizer._additional_special_tokens.extend` directly makes `tokenizer.get_special_tokens_mask` work. But I don't know how could this work and if there is any other side effect. 

Properties like `special_tokens_map` is also confusing, it seems like a @property implementation, which is not modifiable. And I don't know where this could be used to have what effect. 

I cost much time to read the source code but still lack a overview about the special token architecture in tokenizers. ",0
9,https://github.com/huggingface/transformers/issues/6092,6092,[],closed,2020-07-28 13:11:48+00:00,1,i dont know what Tranier`s Dataset is.,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->
![image](https://user-images.githubusercontent.com/32102558/88669501-8ccdba80-d116-11ea-8b29-8dda1a61ce42.png)
![image](https://user-images.githubusercontent.com/32102558/88669549-9bb46d00-d116-11ea-840c-ff661b0a2b0d.png)
 i thought its my customer dataset goes wrong, i dont konw what dataset it should return. the Trainer receive what dataset.
<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",0
10,https://github.com/huggingface/transformers/issues/6094,6094,[],closed,2020-07-28 14:07:02+00:00,1,5 Slow test failures,"traceback: [here](https://github.com/huggingface/transformers/runs/916838508?check_suite_focus=true)

```bash
=========================== short test summary info ============================
FAILED tests/test_modeling_auto.py::AutoModelTest::test_model_for_pretraining_from_pretrained
FAILED tests/test_modeling_auto.py::AutoModelTest::test_model_from_pretrained
FAILED tests/test_modeling_bart.py::BartModelIntegrationTests::test_bart_base_mask_filling
FAILED tests/test_modeling_bart.py::BartModelIntegrationTests::test_bart_large_mask_filling
FAILED tests/test_modeling_common.py::ModelUtilsTest::test_model_from_pretrained
==== 5 failed, 1423 passed, 489 skipped, 384 warnings in 1609.33s (0:26:49) ====
```

I'm investigating",0
11,https://github.com/huggingface/transformers/issues/6096,6096,[],closed,2020-07-28 14:25:21+00:00,1,mBART: incorrect <mask> token id,"# 馃悰 Bug

## Information

Model I am using: mBART

## To reproduce

```
from transformers import MBartTokenizer
tokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-cc25')
print(tokenizer.convert_tokens_to_ids(['<mask>', 'ar_AR']))
```
The output for the above code is `[250001, 250001]`  - two different special tokens are mapped to the same id.

## Expected behavior

As far as I can tell, `<mask>` token should be mapped to id 250026.

I've checked [fairseq implementation](https://github.com/pytorch/fairseq/blob/master/fairseq/tasks/multilingual_denoising.py) and it seems that `<mask>` token is added after all the language codes, so it should be the last token in the vocab.

Currently, when I try to use mBART to denoise text with `<mask>` tokens, it mostly just ignores them, but if I replace mask ids with 250026, the model actually generates new text in place of `<mask>` tokens:

```
from transformers import MBartTokenizer, BartForConditionalGeneration

tokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-cc25')
model = BartForConditionalGeneration.from_pretrained('facebook/mbart-large-cc25')

text = 'I highly recommend <mask> - it is one of the best <mask> ever read!'
inputs = tokenizer.prepare_translation_batch([text], src_lang='en_XX')

outputs = model.generate(inputs['input_ids'], decoder_start_token_id=tokenizer.lang_code_to_id['en_XX'], 
                         num_beams=5)
print(tokenizer.batch_decode(outputs)[0])
```

The output is:
```
en_XX<s> highly recommend - it is one of the best ever read!
```
Replacing mask ids:

```
where = (inputs['input_ids'] == 250001)
inputs['input_ids'][where] = 250026

outputs = model.generate(inputs['input_ids'], decoder_start_token_id=tokenizer.lang_code_to_id['en_XX'], 
                         num_beams=5)
print(tokenizer.batch_decode(outputs)[0])
```

The output is:
```
en_XX<s> highly recommend this book - it is one of the best books I have ever read!
```

(In both cases, the model also skips the first input token when generating output, as discussed in #5755.)

I've also noticed that fairseq is using [language code tokens](https://github.com/pytorch/fairseq/blob/108bb2560b1ec01524ba723bc7c69186875afa0a/fairseq/tasks/multilingual_denoising.py#L62) of the form `[en_XX]` rather than just `en_XX`, which can lead to different tokenization if words like  `en_XX` appear in the text, but that's a rather contrived case.


## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2


@sshleifer
",0
12,https://github.com/huggingface/transformers/issues/6105,6105,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-28 19:28:52+00:00,5,Recursive error calling generate in forward,"## System Info
Pop!_OS 20.04
Pytorch: 1.5.1
Transformers: 3.0.2
Python: 3.7.6

## Question
Here is the training loop: 
```python
def sd_data_collator(dataset_samples_list):
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='right')
    tokenizer.pad_token = tokenizer.eos_token

    encoded_results = tokenizer(dataset_samples_list, padding=True, truncation=True, return_tensors='pt', return_attention_mask=True)

    batch = {}
    batch['input_ids'] = torch.stack([result for result in encoded_results['input_ids']])
    batch['past'] = None
    batch['attention_mask'] = torch.stack([result for result in encoded_results['attention_mask']])
    batch['position_ids'] = None
    batch['head_mask'] = None
    batch['inputs_embeds'] = None
    batch['labels'] = None
    batch['use_cache'] = True
    return batch

sd_dataset = SDAbstractsDataset('/path/to/sd_samples_64.csv')

training_args = TrainingArguments(
    output_dir='/path/to/finetuned_gpt2',
    do_train=True,
    per_device_train_batch_size=4,
    learning_rate=1e-3,
    num_train_epochs=1
)

model = GPT2FinetunedWithNgrams.from_pretrained('gpt2')

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=sd_dataset,
    data_collator = sd_data_collator
)

trainer.train()
```
Here's the model class and its `forward` method:
```python
class GPT2FinetunedWithNgrams(GPT2LMHeadModel):
    def __init__(self, config):
        super().__init__(config)
        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='right')
        self.tokenizer.pad_token = self.tokenizer.eos_token

    def load_ngrams_model(self, ngrams_model_path):
        self.ngrams_model = NGrams(ngrams_model_path)

    def forward(
            self,
            input_ids=None,
            past=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            labels=None,
            use_cache=True,
    ):

        output = self.generate(input_ids=input_ids, max_length=474)
        decoded_output = self.tokenizer.decode(output[0], skip_special_tokens=True)
```
Here's the whole error. It's really lengthy and I cut out the repetitions:
```python
Some weights of GPT2FinetunedWithNgrams were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/16 [00:00<?, ?it/s]Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
.
.
.
      File ""/path/to/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py"", line 15, in decorate_context
    return func(*args, **kwargs)
  File ""/path/to/anaconda3/lib/python3.7/site-packages/transformers/generation_utils.py"", line 480, in generate
    model_specific_kwargs=model_specific_kwargs,
  File ""/path/to/anaconda3/lib/python3.7/site-packages/transformers/generation_utils.py"", line 520, in _generate_no_beam_search
    outputs = self(**model_inputs)
  File ""/path/to/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/path/to/ric-2020/text_gen_w_transformers/finetune_gpt2.py"", line 33, in forward
    
  File ""/path/to/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py"", line 15, in decorate_context
    return func(*args, **kwargs)
  File ""/path/to/anaconda3/lib/python3.7/site-packages/transformers/generation_utils.py"", line 350, in generate
    ""Setting `pad_token_id` to {} (first `eos_token_id`) to generate sequence"".format(eos_token_id)
.
.
.
  File ""/path/to/anaconda3/lib/python3.7/logging/__init__.py"", line 1390, in warning
    self._log(WARNING, msg, args, **kwargs)
  File ""/path/to/anaconda3/lib/python3.7/logging/__init__.py"", line 1514, in _log
    self.handle(record)
  File ""/path/to/anaconda3/lib/python3.7/logging/__init__.py"", line 1524, in handle
    self.callHandlers(record)
  File ""/path/to/anaconda3/lib/python3.7/logging/__init__.py"", line 1594, in callHandlers
    lastResort.handle(record)
  File ""/path/to/anaconda3/lib/python3.7/logging/__init__.py"", line 894, in handle
    self.emit(record)
  File ""/path/to/anaconda3/lib/python3.7/logging/__init__.py"", line 1025, in emit
    msg = self.format(record)
  File ""/path/to/anaconda3/lib/python3.7/logging/__init__.py"", line 869, in format
    return fmt.format(record)
  File ""/path/to/anaconda3/lib/python3.7/logging/__init__.py"", line 608, in format
    record.message = record.getMessage()
  File ""/path/to/anaconda3/lib/python3.7/logging/__init__.py"", line 360, in getMessage
    def getMessage(self):
RecursionError: maximum recursion depth exceeded while calling a Python object
```
My guess is the `self.generate()` being called within the model produces the recursion problem. I found this problematic because the `generate` method has some awesome functionality for beam search, greedy search, top-k, etc. To overcome this, I added a flag to `generate` called `is_finetuning_current_model`:
```python
@torch.no_grad()
    def generate(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        max_length: Optional[int] = None,
        min_length: Optional[int] = None,
        do_sample: Optional[bool] = None,
        early_stopping: Optional[bool] = None,
        num_beams: Optional[int] = None,
        temperature: Optional[float] = None,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
        repetition_penalty: Optional[float] = None,
        bad_words_ids: Optional[Iterable[int]] = None,
        bos_token_id: Optional[int] = None,
        pad_token_id: Optional[int] = None,
        eos_token_id: Optional[int] = None,
        length_penalty: Optional[float] = None,
        no_repeat_ngram_size: Optional[int] = None,
        num_return_sequences: Optional[int] = None,
        attention_mask: Optional[torch.LongTensor] = None,
        decoder_start_token_id: Optional[int] = None,
        use_cache: Optional[bool] = None,
        is_finetuning_current_model: Optional[bool] = None,
        **model_specific_kwargs
    ) -> torch.LongTensor:
```
propagated this down to the `num_beams` check:
```python
        if num_beams > 1:
            output = self._generate_beam_search(
                input_ids,
                cur_len=cur_len,
                max_length=max_length,
                min_length=min_length,
                do_sample=do_sample,
                early_stopping=early_stopping,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                no_repeat_ngram_size=no_repeat_ngram_size,
                bad_words_ids=bad_words_ids,
                pad_token_id=pad_token_id,
                eos_token_id=eos_token_id,
                batch_size=effective_batch_size,
                num_return_sequences=num_return_sequences,
                length_penalty=length_penalty,
                num_beams=num_beams,
                vocab_size=vocab_size,
                encoder_outputs=encoder_outputs,
                attention_mask=attention_mask,
                use_cache=use_cache,
                is_finetuning_current_model=is_finetuning_current_model,
                model_specific_kwargs=model_specific_kwargs
            )
        else:
            output = self._generate_no_beam_search(
                input_ids,
                cur_len=cur_len,
                max_length=max_length,
                min_length=min_length,
                do_sample=do_sample,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                no_repeat_ngram_size=no_repeat_ngram_size,
                bad_words_ids=bad_words_ids,
                pad_token_id=pad_token_id,
                eos_token_id=eos_token_id,
                batch_size=effective_batch_size,
                encoder_outputs=encoder_outputs,
                attention_mask=attention_mask,
                use_cache=use_cache,
                is_finetuning_current_model=is_finetuning_current_model,
                model_specific_kwargs=model_specific_kwargs
            )
```
updated `_generate_no_beam_search` and `_generate_beam_search` with the following:
```python
            if is_finetuning_current_model:
                outputs = self.generate_text_while_finetuning(**model_inputs)
            else:
                outputs = self(**model_inputs)
```
For my model class, I just added the `generate_text_while_finetuning` method and set the `is_finetuning_current_model`:
```python
class GPT2FinetunedWithNgrams(GPT2LMHeadModel):
    def __init__(self, config):
        super().__init__(config)
        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='right')
        self.tokenizer.pad_token = self.tokenizer.eos_token

    def load_ngrams_model(self, ngrams_model_path):
        self.ngrams_model = NGrams(ngrams_model_path)

    def generate_text_while_finetuning(self,
        input_ids=None,
        past=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,):

        transformer_outputs = self.transformer(
            input_ids,
            past=past,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )
        hidden_states = transformer_outputs[0]

        lm_logits = self.lm_head(hidden_states)

        outputs = (lm_logits,) + transformer_outputs[1:]
        return outputs  # (loss), lm_logits, presents, (all hidden_states), (attentions)

    def forward(
            self,
            input_ids=None,
            past=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            labels=None,
            use_cache=True,
    ):

        output = self.generate(input_ids=input_ids, max_length=474, is_finetuning_current_model=True)
        decoded_output = self.tokenizer.decode(output[0], skip_special_tokens=True)
```
This seems to resolve the recursive error and produces an expected `decoded_output` for me. My usecase is using GPT2 with a different loss function to finetune it on a particular domain corpus. I imagine other people would be doing something similar for GPT2 and other models, so I tested this approach just using `GPT2LMHeadModel` and got the same expected results. 

My question is do contributors think I should open up a bug report for this?",0
13,https://github.com/huggingface/transformers/issues/6106,6106,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-28 19:34:10+00:00,1,Weird Behavior on XLNetTokenizer after new tokens added,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: ubuntu
- Python version: 3.7.7
- PyTorch version (GPU?): 1.5.1 (GPU)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help
@mfuntowicz
 
## Information
I use XLNetTokenizer (pretrained) and added a new token in it. After that, the output from ```tokenizer.tokenize``` looks weird.
(I am not sure if the problem comes from ```transformers``` or ```tokenizers```, but I post here anyway.)

## To reproduce
```
from transformers import XLNetTokenizer
tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')

test = 'This is so awesome!!@username!'
out = tokenizer.tokenize(test)

# without new tokens added, @username is broken down as expected
out
>> ['鈻乀his', '鈻乮s', '鈻乻o', '鈻乤wesome', '!!', '@', 'user', 'name', '!']
```

```
from tokenizers import AddedToken

# introduce new tokens (with white-space on right only)
new_tokens = AddedToken(
    '@username', single_word = True, 
    lstrip = False, rstrip = True
)
tokenizer.add_tokens(new_tokens)

out = tokenizer.tokenize(test)

# weird result about the white-space around new tokens
out
>> ['鈻乀his', '鈻乮s', '鈻乻o', '鈻乤wesome', '!!', '@username', '鈻?, '!']
```

In here, there are two places weird:
1. new tokens ""@username"" has a white-space on the left, so """"!!@username"" should not be broken down into ""!!"", ""@username"" (I think it should be broken down into ""!!"", ""@"", ""user"", ""name"", ""!"")
2. I am a bit confused on why there is a white-space produced after ""@username"" token. (i.e. '@username', '鈻?, '!')

And oddly, when I encode and decode the sentence back, the white-space token after ""@username"" is not translated to actual whitespace. (Also note there is a white space added before ""@username"" which mean the new token is correctly identified to have a white-space on the left):
```
enc = tokenizer.encode(test, add_special_tokens = False)
dec = tokenizer.decode(enc)

# in encoding stage, the 2nd last token is whitespace
enc 
>> [122, 27, 102, 8729, 7675, 32000, 17, 136]
# in decoding stage, the white-space disappear

dec
>> This is so awesome!! @username!
```

",0
14,https://github.com/huggingface/transformers/issues/6107,6107,[],closed,2020-07-28 19:46:47+00:00,1,Where do the Masked Language Model perform mask on the input data,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->

I am trying to pre-train a bert from scratch with my own word set using only the Masked Language Model. 
I have a trouble finding where exactly the code masks 15% of the words and replaces it with 80% mask, 10% random, and 10% original. 

I noticed that the input ""labels"" kind of refers to the places where words are masked. Does it mean that when I preprocess the data, I need to masked it myself and then indicates the position of masks in the ""labels"" input? If so, is ""labels"" the only input that would be affect? Is there any other input variables, such as the ""masked_lm_positions"" and ""masked_lm_ids"" in google-bert that I need to take care of?


<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",0
15,https://github.com/huggingface/transformers/issues/6108,6108,[],closed,2020-07-28 20:05:14+00:00,1,allenai/longformer-large-4096 unavailable,"For some reason, I'm unable to download allenai/longformer-large-4096. Everything was working an hour ago, but all of a sudden I get the error included below. It's still listed on https://huggingface.co/models?search=allenai%2Flongformer-large-4096. I'm not sure what's up. Any ideas?
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py"", line 242, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/ptcc/run_glue.py"", line 246, in <module>
    main()
  File ""/ptcc/run_glue.py"", line 123, in main
    cache_dir=model_args.cache_dir,
  File ""/usr/local/lib/python3.6/dist-packages/transformers/configuration_auto.py"", line 203, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py"", line 251, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'allenai/longformer-large-4096'. Make sure that:

- 'allenai/longformer-large-4096' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'allenai/longformer-large-4096' is the correct path to a directory containing a config.json file


Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py"", line 242, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/ptcc/run_glue.py"", line 246, in <module>
    main()
  File ""/ptcc/run_glue.py"", line 123, in main
    cache_dir=model_args.cache_dir,
  File ""/usr/local/lib/python3.6/dist-packages/transformers/configuration_auto.py"", line 203, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py"", line 251, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'allenai/longformer-large-4096'. Make sure that:

- 'allenai/longformer-large-4096' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'allenai/longformer-large-4096' is the correct path to a directory containing a config.json file
```",0
16,https://github.com/huggingface/transformers/issues/6109,6109,[],closed,2020-07-28 20:29:57+00:00,0,StopIteration error in RobertaForMultipleChoice,"Hello,
I am trying to execute the line below for my `RobertaForMultipleChoice` model:
```python
# retrieve the resulting mc_loss
mc_loss = model(input_ids = input_ids, attention_mask = attention_mask, labels =  mc_labels)[0]
```
but this generates the following error:

```python
Traceback (most recent call last):
  File ""STAT946_final_project_code_v4.py"", line 623, in <module>
    success_rate_list_diag_normal = main_function_diag_normal('/home/ec2-user/test.txt', 'test_ans_num.txt', num_iter, log_interval)
  File ""STAT946_final_project_code_v4.py"", line 414, in main_function_diag_normal
    best_model_RobertaForMultipleChoice_diag_normal = train_loop(model_RobertaForMultipleChoice, tokenizer, optimizer_1, scheduler_1, log_interval, svi_diag_normal, guide_diag_normal, best_model_RobertaForMultipleChoice_diag_normal)
  File ""STAT946_final_project_code_v4.py"", line 341, in train_loop
    optimizer, scheduler, log_interval, svi, guide, epoch)
  File ""STAT946_final_project_code_v4.py"", line 236, in train_mc_head
    mc_loss = model(input_ids = input_ids, attention_mask = attention_mask, labels =  mc_labels)[0]
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/pyro/nn/module.py"", line 413, in __call__
    return super().__call__(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_roberta.py"", line 441, in forward
    output_hidden_states=output_hidden_states,
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/pyro/nn/module.py"", line 413, in __call__
    return super().__call__(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py"", line 732, in forward
    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py"", line 228, in get_extended_attention_mask
    extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py"", line 159, in dtype
    first_tuple = next(gen)
StopIteration
```

How can I get around this type of error? Thank you,",0
18,https://github.com/huggingface/transformers/issues/6113,6113,"[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",closed,2020-07-29 01:45:45+00:00,35,馃専 BigBird,"# 馃専 New model addition

## Model description

Paper : https://arxiv.org/pdf/2007.14062.pdf

Abstract : 
> Transformers-based models, such as BERT, have been one of the most successful deep learning
models for NLP. Unfortunately, one of their core limitations is the quadratic dependency
(mainly in terms of memory) on the sequence length due to their full attention mechanism.
To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this
quadratic dependency to linear. We show that BigBird is a universal approximator of
sequence functions and is Turing complete, thereby preserving these properties of the
quadratic, full attention model. Along the way, our theoretical analysis reveals some of the
benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence
as part of the sparse attention mechanism. The proposed sparse attention can handle
sequences of length up to 8x of what was previously possible using similar hardware. As
a consequence of the capability to handle longer context, BigBird drastically improves
performance on various NLP tasks such as question answering and summarization. We also
propose novel applications to genomics data.


## Open source status

* [ ] the model implementation is available: *No*
* [ ] the model weights are available: *No*
* [ ] who are the authors: *?*
",0
19,https://github.com/huggingface/transformers/issues/6114,6114,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-29 02:55:00+00:00,2,"namespace object has no attribute to ""enc_only""","# 鉂?Questions & Help
when I running the distillation.py then 
 File ""E:/transformers-master/examples/seq2seq/distillation.py"", line 370, in create_module
    elif args.enc_only:
AttributeError: 'Namespace' object has no attribute 'enc_only'
 how can I deal  with this problem??  thx a lot.

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",0
20,https://github.com/huggingface/transformers/issues/6115,6115,[],closed,2020-07-29 03:08:06+00:00,3,Usage of Pytorch Native AMP in place of apex (Pytorch 1.6) in Trainer,"# 馃殌 Feature request
It would be nice to remove the Apex dependency for `fp16` training and use the native [Pytorch's AMP methods](https://github.com/pytorch/pytorch/releases) in the `Trainer` method. Pytorch recommends Apex users to switch to it's native implementation, even [Apex does it so](https://github.com/NVIDIA/apex/issues/818). Moreover, it will eliminate the need for users to build apex by themselves. 

## Your contribution
I am happy to submit a PR if you think it would be a good addition. Please let me know.
",0
21,https://github.com/huggingface/transformers/issues/6116,6116,[],closed,2020-07-29 03:35:07+00:00,2,No button for creating new post at the forum.,"![image](https://user-images.githubusercontent.com/4702353/88753699-76b01080-d18f-11ea-80c3-905a006ad31a.png)
discuss.huggingface.co",0
22,https://github.com/huggingface/transformers/issues/6117,6117,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-29 03:44:35+00:00,3,Using control codes for finetuning,"Hi
I have a use case of style conditioned generation, where I ask the LM to generate a sentence based on the control code I provide. CTRL is pretty suitable for that task.
Can you tell me how to use control codes for fine-tuning as well as inference?  It should be the same as any CLM like GPT2 but I want to specifically know about the style and control code conditioning. How should be the data format and other stuffs",0
23,https://github.com/huggingface/transformers/issues/6118,6118,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-29 06:19:40+00:00,3,Is `guid` allowed to be None in `InputExample`?,"## Environment
-`transformers` version: 3.0.2
Omitted the rest because they most likely don't affect this issue.

## To reproduce

```py
from transformers.data.processors.utils import SingleSentenceClassificationProcessor

processor = SingleSentenceClassificationProcessor(labels=[""lbl1"", ""lbl2""])
processor.add_examples(texts_or_text_and_labels=[""example1"", ""example2""]) # There's a default ids=None
print(processor[0])
```
prints 
```InputExample(guid=None, text_a='example1', text_b=None, label=None)```

If `guid` is allowed to be None, that should be reflected in the type annotation(and documentation) of `InputExample`. If not, then `ids` should not be allowed to be `None`.

### Who can help 
@sgugger because it's a documentation issue, @thomwolf because of `git blame`.

",0
25,https://github.com/huggingface/transformers/issues/6120,6120,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-29 08:49:52+00:00,1,Don't see how to use correct padding with QA pipeline,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details

In some cases, when using a **QuestionAnsweringPipeline**, I get an error similar to the following:

    *** ValueError: expected sequence of length 384 at dim 1 (got 260)

I've traced the problem to line 1496 of pipelines.py, and the change to [this commit](https://github.com/huggingface/transformers/commit/896300177bf9f35feac4698370212a80a5ab6138).  The troubled line is:

    fw_args = {k: torch.tensor(v, device=self.device) for (k, v) in fw_args.items()}

Basically, when trying to convert this **v** to a tensor an error is thrown because not every array in **v** has the same length - because the padding strategy has been changed (if I comment out the padding strategy in the call to **squad_convert_examples_to_features** then the default value `""max_length""` takes effect and there is no problem).  I guess this was done as some sort of optimization, but I'm not really sure how to use it.  Every other argument to **squad_convert_examples_to_features** will use a *kwarg*, but this one does not.  Maybe it should use a *kwarg* like everything else, so if you need the padding (or don't want to have to deal with it) you can set the **padding_strategy** as you like?  Or am I missing something?

### Minimal code to reproduce:

    from transformers import QuestionAnsweringPipeline, AutoModelForQuestionAnswering, AutoTokenizer
    model = AutoModelForQuestionAnswering.from_pretrained('twmkn9/distilbert-base-uncased-squad2')
    tokenizer = AutoTokenizer.from_pretrained('twmkn9/distilbert-base-uncased-squad2')
    
    # I've omitted the context for brevity.  You can, for example, take the **plot** section from [the matrix](https://en.wikipedia.org/wiki/The_Matrix)
    context = """"""...""""""
    
    pipeline({""question"":""what was Neo's job?"", ""context"": context}) # error as described above

<!-- Description of your issue -->

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",0
36,https://github.com/huggingface/transformers/issues/6141,6141,[],closed,2020-07-29 18:41:26+00:00,1,Bug in language_modeling.py calling tokenizer.num_special_tokens_to_add,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Linux-4.4.0-1081-aws-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.6.0+cu101 (True)
- Tensorflow version (GPU?): 2.1.0 (True)
- Using GPU in script?: yes
- Using distributed or parallel set-up in script?: no

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 tokenizers: @mfuntowicz
 -->

## Information

Model I am using GPT-2

The class TextDataset call tokenizer.num_special_tokens_to_add(pair=False) but the correct argument is called is_pair. I assume the bugfix corresponds to transformers repo.

## To reproduce

https://github.com/huggingface/transformers/blob/e49393c3617e877f0370f7bad7c7e823808c5bfb/src/transformers/data/datasets/language_modeling.py#L27

https://github.com/huggingface/tokenizers/blob/master/bindings/python/tokenizers/implementations/base_tokenizer.py#L20


I'm using transformers 3.0.2 and tokenizers 0.8.1rc1 (try to update but says its incompatible)

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
",0
37,https://github.com/huggingface/transformers/issues/6144,6144,[],closed,2020-07-29 21:48:05+00:00,12,Question-Answering pipeline doesn't work anymore with long text,"Transformers version: 3.0.2

The question-answering models don't seem to work anymore with long text, any reason why this is happening? I have tried with the default model in `pipeline` as well as with specific models.

e.g

__Sample Code:__

```
from transformers import pipeline

nlp_qa = pipeline('question-answering') # 1st try
nlp_qa = pipeline('question-answering', model='deepset/roberta-base-squad2') # 2nd try

context = """"""
Coronaviruses are a large family of viruses which may cause illness in animals or humans.  
In humans, several coronaviruses are known to cause respiratory infections ranging from the 
common cold to more severe diseases such as Middle East Respiratory Syndrome (MERS) and Severe Acute Respiratory Syndrome (SARS). 
The most recently discovered coronavirus causes coronavirus disease COVID-19.
COVID-19 is the infectious disease caused by the most recently discovered coronavirus. 
This new virus and disease were unknown before the outbreak began in Wuhan, China, in December 2019. 
COVID-19 is now a pandemic affecting many countries globally.
The most common symptoms of COVID-19 are fever, dry cough, and tiredness. 
Other symptoms that are less common and may affect some patients include aches 
and pains, nasal congestion, headache, conjunctivitis, sore throat, diarrhea, 
loss of taste or smell or a rash on skin or discoloration of fingers or toes. 
These symptoms are usually mild and begin gradually. 
Some people become infected but only have very mild symptoms.
Most people (about 80%) recover from the disease without needing hospital treatment. 
Around 1 out of every 5 people who gets COVID-19 becomes seriously ill and develops difficulty breathing. 
Older people, and those with underlying medical problems like high blood pressure, heart and lung problems, 
diabetes, or cancer, are at higher risk of developing serious illness.  
However, anyone can catch COVID-19 and become seriously ill.  
People of all ages who experience fever and/or  cough associated with difficulty breathing/shortness of breath, 
chest pain/pressure, or loss of speech or movement should seek medical attention immediately. 
If possible, it is recommended to call the health care provider or facility first, 
so the patient can be directed to the right clinic.
People can catch COVID-19 from others who have the virus. 
The disease spreads primarily from person to person through small droplets from the nose or mouth, 
which are expelled when a person with COVID-19 coughs, sneezes, or speaks. 
These droplets are relatively heavy, do not travel far and quickly sink to the ground. 
People can catch COVID-19 if they breathe in these droplets from a person infected with the virus.  
This is why it is important to stay at least 1 meter) away from others. 
These droplets can land on objects and surfaces around the person such as tables, doorknobs and handrails.  
People can become infected by touching these objects or surfaces, then touching their eyes, nose or mouth.  
This is why it is important to wash your hands regularly with soap and water or clean with alcohol-based hand rub.
Practicing hand and respiratory hygiene is important at ALL times and is the best way to protect others and yourself.
When possible maintain at least a 1 meter distance between yourself and others. 
This is especially important if you are standing by someone who is coughing or sneezing.  
Since some infected persons may not yet be exhibiting symptoms or their symptoms may be mild, 
maintaining a physical distance with everyone is a good idea if you are in an area where COVID-19 is circulating. 
""""""


nlp_qa(context=context, question='What is a coronavirus ?')
```

__Error Message:__

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-15-ddac1f9cb68e> in <module>()
----> 1 nlp_qa(context=context, question='What is a coronavirus ?')

1 frames
/usr/local/lib/python3.6/dist-packages/transformers/pipelines.py in <listcomp>(.0)
   1314                         ),
   1315                     }
-> 1316                     for s, e, score in zip(starts, ends, scores)
   1317                 ]
   1318 

KeyError: 0
```

This used to work before version 3 I remember, would really appreciate some help on this.",0
39,https://github.com/huggingface/transformers/issues/6146,6146,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}, {'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",closed,2020-07-30 01:45:13+00:00,1,馃専 Mirostat decoding algorithm,"# 馃専 Mirostat: A Perplexity-Controlled Neural Text Decoding Algorithm

## Description

Paper : https://arxiv.org/pdf/2007.14966.pdf

Abstract : 

> [...] We use this analysis to design a feedbackbased adaptive top-k text decoding algorithm called mirostat that generates text (of any length) with a redetermined value of perplexity, and thereby high-quality text without any tuning. [...] Mirostat avoids both traps: experiments show that cross-entropy has a near-linear relation with repetition in generated text. This relation is almost independent of the sampling method but slightly dependent on the model used. Hence, for a given language model, control over perplexity also gives control over repetitions.

## Open source status

* [x] the model implementation is available: https://github.com/basusourya/mirostat
* [x] the model weights are available: _Not applicable_
* [x] who are the authors: @basusourya 
",0
40,https://github.com/huggingface/transformers/issues/6147,6147,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-30 02:44:55+00:00,2,the documents for transformer don't work,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

##   i find the new document for transformer can't work, https://huggingface.co/transformers/v2.5.0/model_doc/bert.html#bertmodel  ; the class link don't work 
<!-- Description of your issue -->

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",0
41,https://github.com/huggingface/transformers/issues/6148,6148,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-30 02:56:11+00:00,2,tokenize cache for examples/language-modeling,"# 馃殌 Feature request

I find transformer already has cache for tokenized result in examples/token-classification.
I think language-modeling which with a much larger dataset also need that 
",0
42,https://github.com/huggingface/transformers/issues/6150,6150,[],closed,2020-07-30 04:53:04+00:00,2,馃悰 T5 Tokenizer ignores \n \t characters and more than one whitespace together,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2 (master)
- Platform: Linux-4.9.0-12-amd64-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.5.1+cpu (False)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no

### Who can help

@patrickvonplaten @sshleifer

## Information
T5 Tokenizer is based out of SentencePiece and in sentencepiece Whitespace is treated as a basic symbol. But huggingface tokenizers just ignores more than one whitespace. 
Consider all the following examples tokenize to the same thing.
```
from transformers import T5Tokenizer
tokenizer = T5Tokenizer.from_pretrained(""t5-base"")

print(tokenizer.tokenize(""Hi there I'm good""))
>> ['鈻丠i', '鈻乼here', '鈻両', ""'"", 'm', '鈻乬ood']

print(tokenizer.tokenize(""Hi there     I'm good""))
>> ['鈻丠i', '鈻乼here', '鈻両', ""'"", 'm', '鈻乬ood']

print(tokenizer.tokenize(""Hi there    I'm good\n""))
>> ['鈻丠i', '鈻乼here', '鈻両', ""'"", 'm', '鈻乬ood']

print(tokenizer.tokenize(""Hi there \n   I'm good\n""))
>> ['鈻丠i', '鈻乼here', '鈻両', ""'"", 'm', '鈻乬ood']

print(tokenizer.tokenize(""Hi there \n     I'm good\n""))
>> ['鈻丠i', '鈻乼here', '鈻両', ""'"", 'm', '鈻乬ood']

print(tokenizer.tokenize(""Hi there \n  \t   I'm good\n""))
>> ['鈻丠i', '鈻乼here', '鈻両', ""'"", 'm', '鈻乬ood']

print(tokenizer.tokenize(""Hi there\nI'm good""))
>> ['鈻丠i', '鈻乼here', '鈻両', ""'"", 'm', '鈻乬ood']
```
All these examples should tokenize to different representations. Also ignoring newline outright means that all applications that use newlines fail.
Model I am using (Bert, XLNet ...): T5


<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
All the whitespaces have different tokenizations",0
44,https://github.com/huggingface/transformers/issues/6154,6154,[],closed,2020-07-30 14:52:16+00:00,4,Hidden State Embedding-Transformers,"Hi everybody, I want to use Bert model to get the embedding for a sentence after I fine-tuned it with raw texts. I was wondering if is that possible or not and anybody can help me with that? ",0
46,https://github.com/huggingface/transformers/issues/6159,6159,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-30 18:35:08+00:00,8,OSError: Unable to load weights from pytorch checkpoint file. ,"Hello,

When I am trying to load the `Roberta-large` pre-trained model, I get the following error:

```python
model_RobertaForMultipleChoice = RobertaForMultipleChoice.from_pretrained('roberta-large', output_hidden_states = True)

OUT:
OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. 
```

How can I solve this issue? Thank you,",0
47,https://github.com/huggingface/transformers/issues/6160,6160,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-30 20:10:19+00:00,4,run_squad.py eval metrics meaning,"I am having difficulty understanding what exactly the best_f1 and best_exact scores that are outputted in the run_squad.py evaluation mean. (The scores are computed in the squad_metrics script, found [here](https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py)). 
What are the ""scores"" the best calculations are working with, what do the metrics represent, and when/is the best_threshold value employed during training?

Thank you!",0
48,https://github.com/huggingface/transformers/issues/6161,6161,[],closed,2020-07-30 20:11:14+00:00,5,Padding Strategy Code missing an else case (maybe?),"## Environment info
- `transformers` version: 3.0.2
- Platform: macOS 10.15.5 
- Python version: 3.7
- PyTorch version (GPU?): 1.5 GPU-Yes
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help
 tokenizers: @mfuntowicz
 Summarization: @sshleifer
 T5: @patrickvonplaten
 
## Information

Model I am using (T5 via Autotokenizer):

The problem arises when using:
`tokenizer([line], max_length=max_length, padding='max_length' if pad_to_max_length else False,
                     truncation=True, return_tensors=return_tensors, **extra_kw)`

In batch encoding, the latest code decides on a padding strategy:
`_get_padding_truncation_strategies(
        self, padding=False, truncation=False, max_length=None, pad_to_multiple_of=None, verbose=True, **kwargs
    ):`

       ` elif padding is not False:
            if padding is True:
                padding_strategy = PaddingStrategy.LONGEST  # Default to pad to the longest sequence in the batch
            elif not isinstance(padding, PaddingStrategy):
                padding_strategy = PaddingStrategy(padding)`

While calling the tokenizer, instead of 'max_length' I first gave the actual PaddingStrategy.MAX_LENGTH Enum as argument,
but the above code throws an error as 'padding_strategy' is not defined.
## To reproduce
Call the tokenizer as:
`tokenizer([line], max_length=max_length, padding=PaddingStrategy.MAX_LENGTH if pad_to_max_length else False,
                     truncation=True, return_tensors=return_tensors, **extra_kw)`

## Expected behavior

The PaddingStrategy enum should be assigned no issue.

##Suggested Solution
               
                    ` elif padding is not False:
                              if padding is True:
                                 padding_strategy = PaddingStrategy.LONGEST  # Default to pad to the longest sequence in the batch
                      elif not isinstance(padding, PaddingStrategy):
                             padding_strategy = PaddingStrategy(padding)
                      else:
                          padding_strategy = padding`        
It's a one line fix basically, I can raise a PR for the same, unless PaddingStrategy wasn't designed to be used directly?
",0
49,https://github.com/huggingface/transformers/issues/6164,6164,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-30 22:32:05+00:00,2,RoBERTa ``tokenizer.decode`` does not produce the same sentence.,"## Environment info
- `transformers` version: 3.0.2
- Platform: Linux-4.15.0-74-generic-x86_64-with-glibc2.27
- Python version: 3.8.0
- PyTorch version (GPU?): 1.5.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: No.
- Using distributed or parallel set-up in script?: No.
     

### Who can help
@mfuntowicz
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 tensorflow: @jplu 
documentation: @sgugger
 -->

## Information

Model I am using (Bert, XLNet ...): RoBERTa

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [x] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [x] my own task or dataset: (give details below)

## To reproduce

This code example should reproduce the issue:

```python3
from transformers import RobertaTokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
s = """"""Meanwhile, Tucci's 'straight guy', the emphatic doctor Seger, is not developed into a more interesting character, like the fallible 'straight guys' Cuddy and Wilson.""""""
outputs = tokenizer(s)
input_ids = outputs['input_ids']
ss = tokenizer.decode(input_ids, skip_special_tokens=True)
print('s='+s)
print('ss='+ss)
```


<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior
I expect ``s`` and ``ss`` should be exactly the same. However, they are not. The outputs are:

```bash
s=Meanwhile, Tucci's 'straight guy', the emphatic doctor Seger, is not developed into a more interesting character, like the fallible 'straight guys' Cuddy and Wilson.

ss=Meanwhile, Tucci's'straight guy', the emphatic doctor Seger, is not developed into a more interesting character, like the fallible'straight guys' Cuddy and Wilson.
```
There are two spaces missing before ``'straight guy'``. 
I am not sure if this behavior is expected or it is a bug.
The thing is I want to use the sentence produced by the ``decode`` function and  I find the output is not exactly the same as the original sentence.

Thanks for the help!
<!-- A clear and concise description of what you would expect to happen. -->
",0
51,https://github.com/huggingface/transformers/issues/6172,6172,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-31 10:22:25+00:00,4,馃悰 Not adding `token_type_ids` when the model is `electra` (pytorch_lightning example),"### Who can help
@sshleifer (examples issue)


## Information

Model I am using (Bert, XLNet ...): `ELECTRA`

The problem arises when using:
* [X] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [X] an official GLUE/SQUaD task: (give the name)
* [ ] my own task or dataset: (give details below)

## About Issue

https://github.com/huggingface/transformers/blob/838dc06ff5a438159ac25f531d622e8f344476f5/examples/text-classification/run_pl_glue.py#L38-L39

As above, it seems that `token_type_ids` is not included if the `model_type=='electra'` (which also have `token_type_ids`)

I think this code should be changed in other way.",0
52,https://github.com/huggingface/transformers/issues/6173,6173,[],closed,2020-07-31 11:59:52+00:00,2,"My finetuned gpt2 model is taking wayy too long to generate samples, like 5-8 minutes","I fine tuned the gpt2 model using transformers, i trained it on a lyrics dataset, and after successful training, when i do model.generate(args), it takes like a hell lot of time to genrate results
What Should i do?
",0
53,https://github.com/huggingface/transformers/issues/6174,6174,[],closed,2020-07-31 16:14:28+00:00,0,t,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",0
54,https://github.com/huggingface/transformers/issues/6177,6177,[],closed,2020-07-31 20:49:29+00:00,4,RoBERTa for QuestionAnswering ,"I am trying to replicate the example in this link 
https://github.com/huggingface/transformers/pull/1502/files, but I get the following error : 
``
ValueError                                Traceback (most recent call last)
<ipython-input-23-823cc70a5d4f> in <module>
----> 1 token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]
      2 start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))
      3 all_tokens = tokenizer.convert_ids_to_tokens(input_ids)
      4 print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))

<ipython-input-23-823cc70a5d4f> in <listcomp>(.0)
----> 1 token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]
      2 start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))
      3 all_tokens = tokenizer.convert_ids_to_tokens(input_ids)
      4 print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))

ValueError: 102 is not in list
``
I see the same error discussed in https://github.com/huggingface/transformers/issues/2261

Any ideas how I could resolve this issue ?
Thanks in advance.
",0
55,https://github.com/huggingface/transformers/issues/6178,6178,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-31 20:52:09+00:00,1,Why are the `device()` and `dtype()` functions in `modelling_utils.py` needed?,"Hello,

For BERT and RoBERTa HuggingFace pre-trained models, why are the `device()` and `dtype()` functions in `modeling_utils.py` needed? 

See: https://github.com/huggingface/transformers/blob/8edfaaa81b9995cedea2f8805e4c18c2b6cb5bfc/src/transformers/modeling_utils.py#L158

https://github.com/huggingface/transformers/blob/8edfaaa81b9995cedea2f8805e4c18c2b6cb5bfc/src/transformers/modeling_utils.py#L177

Would it be possible for my RoBERTa model to function without an error, if I modify these  `device()` and `dtype()` functions in a way that they will always return `cpu` and `torch.float32` (or `torch.float64`), respectively? 

Also, while it is easy to modify the original code to do this, I am not sure on how to get my HuggingFace RoBERTa model to take those modified functions into effect. How can I do this?

Thank you (sorry for asking so many questions),
",0
56,https://github.com/huggingface/transformers/issues/6179,6179,[],closed,2020-07-31 21:05:02+00:00,2,HANS Dataset: Incorrect `label_list` and `label`.,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version:
- Platform: n/a
- Python version: n/a
- PyTorch version (GPU?): n/a
- Tensorflow version (GPU?): n/a
- Using GPU in script?: n/a
- Using distributed or parallel set-up in script?: n/a

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 tensorflow: @jplu 
documentation: @sgugger
 -->
@VictorSanh

## Information

Model I am using (Bert, XLNet ...):

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [ ] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1.
2.
3.

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
### Incorrect `label_list`
See line: https://github.com/huggingface/transformers/blob/master/examples/adversarial/utils_hans.py#L259
```python
    def get_labels(self):
        """"""See base class.""""""
        return [""contradiction"", ""entailment"", ""neutral""]
```
HANS dataset has only two labels, non-entailment, and entailment, but here three are given. Similarly, when mapping from text label to label-id, the label ""non-entailment"" (which exists in the task but not in the afore-defined labels), the below line is used. I'm curious if this is intentional? If so, would be great to add a warning/comment as those might cause subtle errors in the future.

https://github.com/huggingface/transformers/blob/master/examples/adversarial/utils_hans.py#L311
```python
label = label_map[example.label] if example.label in label_map else 0
```

### Incorrect `label` index
The below line uses the last column as the label. However, the HANS dataset uses the first column for `label`
https://github.com/huggingface/transformers/blob/master/examples/adversarial/utils_hans.py#L271
```python
            label = line[-1]
```",0
57,https://github.com/huggingface/transformers/issues/6181,6181,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-31 21:13:37+00:00,1,Failing ONNX Export test,"https://github.com/huggingface/transformers/runs/929962952?check_suite_focus=true

```
FAILED tests/test_onnx.py::OnnxExportTestCase::test_quantize_pytorch - TypeError
```",0
58,https://github.com/huggingface/transformers/issues/6182,6182,[],closed,2020-07-31 21:19:34+00:00,0,Failing XLMModelTest,"https://github.com/huggingface/transformers/runs/929962952?check_suite_focus=true

FAILED tests/test_modeling_xlm.py::XLMModelTest::test_inputs_embeds - RuntimeError


Failure introduced somewhere in here:
```
* d951c14a Sylvain Gugger: Model output test (#6155) -   (8 hours ago)
* 86caab1e Sylvain Gugger: Harmonize both Trainers API (#6157) -   (8 hours ago)
* 603cd81a Mehrdad Farahani: readme m3hrdadfi/albert-fa-base-v2 (#6153) -   (11 hours ago)
* 838dc06f Suraj Patil: parse arguments from dict (#4869) -   (13 hours ago)
* cf3cf304 Paul O'Leary McCann: Replace mecab-python3 with fugashi for Japanese tokenization (#6086) -   (13 hours ago)
* f250beb8 Stas Bekman: enable easy checkout switch (#5645) -   (13 hours ago)
* 7d50af4b kolk: Create README.md (#6169) -   (13 hours ago)
* 0034a1d2 Prajjwal Bhargava: Add Pytorch Native AMP support in Trainer (#6151) -   (13 hours ago)
* 7231f7b5 Funtowicz Morgan: Enable ONNX/ONNXRuntime optimizations through converter script (#6131) -   (14 hours ago)
* c0b93a1c Stas Bekman: correct the correction (#6163) -   (23 hours ago)
* a2f6d521 Stas Bekman: typos (#6162) -   (24 hours ago)
* f3065abd Sylvain Gugger: Doc tokenizer (#6110) -   (26 hours ago)
* e642c789 guillaume-be: Addition of a DialoguePipeline (#5516) -   (27 hours ago)
* ec026747 Lysandre Debut: Fix FlauBERT GPU test (#6142) -   (30 hours ago)
* 91cb9546 Sylvain Gugger: Switch from return_tuple to return_dict (#6138) -   (32 hours ago)
* 562b6369 Sylvain Gugger: Tf trainer cleanup (#6143) -   (32 hours ago)
* c127d055 Oren Amsalem: add another e.g. to avoid confusion (#6055) -   (32 hours ago)
* d24ea708 Oren Amsalem: Actually the extra_id are from 0-99 and not from 1-100 (#5967) -   (35 hours ago)
* 3212b885 Stas Bekman: [s2s] add support for overriding config params (#6149) -   (2 days ago)
* 54f9fbef Julien Plu: Rework TF trainer (#6038) -   (2 days ago)
* 3f94170a Lysandre Debut: [WIP] Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleC鈥?(#5614) -   (2 days ago)
* 8a8ae276 Sylvain Gugger: Use google style to document properties (#6130) -   (2 days ago)
* fc64559c Julien Plu: Fix TF CTRL model naming (#6134) -   (2 days ago)
* 641b873c Lysandre Debut: XLNet PLM Readme (#6121) -   (2 days ago)
* 8d157c93 Timo Moeller: add deepset/xlm-roberta-large-squad2 model card (#6128) -   (2 days ago)
* 6c002853 Funtowicz Morgan: Added capability to quantize a model while exporting through ONNX. (#6089) -   (2 days ago)
* 25de74cc Sylvain Gugger: Use FutureWarning to deprecate (#6111) -   (3 days ago)
* 640550fc Funtowicz Morgan: ONNX documentation (#5992) -   (3 days ago)
```


Any idea on this @sgugger or @LysandreJik  ? Otherwise I'll dig in.",0
59,https://github.com/huggingface/transformers/issues/6186,6186,[],closed,2020-08-01 10:24:56+00:00,1,Remove inconsistency between BertTokenizer and BertTokenizerFast ,"# 馃殌 Feature request
`BertTokenizerFast` has the option to specify `strip_accents=False`. The `BertTokenizer` does not have this option. This inconsistency should be removed by adding the `strip_accents` parameter to `BertTokenizer`.

## Motivation
Without adding this, the `BertTokenizer` can not be used for language models which are lowercase but have accents.

In case of a language model with lowercase and with accents you are forced to load the tokenizer by this:

```python
tokenizer = AutoTokenizer.from_pretrained(""<model_name_or_path>"", use_fast=True, strip_accents=False)
```

This will NOT work: `tokenizer = AutoTokenizer.from_pretrained(""<model_name_or_path>"")`

And even this would not work: `tokenizer = AutoTokenizer.from_pretrained(""<model_name_or_path>"", strip_accents=False)`

## Your contribution
With some hints I am willing to contribute.",0
60,https://github.com/huggingface/transformers/issues/6188,6188,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-01 15:34:30+00:00,4,taeminlee/kogpt2 not working,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- None: hosted inference API

### Who can help
@LysandreJik 
@julien-c 
@TevenLeScao


## Information

Model I am using (Bert, XLNet ...):

The problem arises when using:
* [O] the official example scripts: Hosted Inference API testing page
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [O] my own task or dataset: Text generation, or anything

## To reproduce

Steps to reproduce the behavior:

1. https://huggingface.co/taeminlee/kogpt2?text=鞝?鞚措鞚€+頇嶊父霃?
2. Or type any text
3. Model returns just the text

## Expected behavior

Generated text should be returned, but isn't.
",0
61,https://github.com/huggingface/transformers/issues/6190,6190,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-01 16:04:58+00:00,5,Add support for truncation argument when calling a Pipeline,"# 馃殌 Feature request

Currently, only the `padding` argument [is supported](https://github.com/huggingface/transformers/blob/a39dfe4fb122c11be98a563fb8ca43b322e01036/src/transformers/pipelines.py#L500) when calling a pipeline, and it's not possible to pass `truncation` argument. For example, running the following code sample would raise an error:

```python
import transformers as trf

model = trf.pipeline(task='feature-extraction', model='bert-base-cased')
output = model('a sample text', padding=False, truncation=True)
```

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request
     related to a problem? e.g., I'm always frustrated when [...]. If this is related
     to another GitHub issue, please link here too. -->

If toggling padding is supported, then why truncation shouldn't be?

## Your contribution

I think to achieve this, same as `padding`, only a `truncation` argument should be added to `_parse_and_tokenize` method and also when calling the tokenizer. If that's the case, I would be willing to work on a PR.",0
62,https://github.com/huggingface/transformers/issues/6191,6191,[],closed,2020-08-01 18:24:51+00:00,4,How to integrate the Pyro module with HuggingFace Transformers?,"Hello,
I am trying to convert the HuggingFace Transformer into a Bayesian neural network by using the `Pyro` module.
I provided my code below. Everything works well except I am stuck at the line `svi_loss = svi.step(input_ids = input_ids, attention_mask = attention_mask, labels = label)`. At that line an error is generated, because after converting the HuggingFace Transformer into a Pyro model, the new model does not have any set parameter (since it is a Bayesian model...so the weights for a Pyro model are not fixed, meaning the weights are sampled from a statistical distribution). Is there any way that I can get around this issue? I have also posted the similar question on Pyro forum. Thank you,

CODE: 

```python 
import torch
from torch import distributions
from transformers import RobertaTokenizer, RobertaForMultipleChoice, AdamW, get_constant_schedule
import pyro
import pyro.infer
import pyro.optim
import pyro.distributions as dist
import pyro.nn.module as module
import pyro.infer.autoguide.guides as guides
from torch import nn
from pyro.optim import Adam
from pyro.infer import SVI
from pyro.infer import Trace_ELBO
from pyro.infer import Predictive

# get the pre-trained HuggingFace RobertaForMultipleChoice and resize the token embeddings 
# after adding the special token
model_RobertaForMultipleChoice = RobertaForMultipleChoice.from_pretrained('Roberta-base')
        
# convert the HuggingFace model into a pyro model
module.to_pyro_module_(model_RobertaForMultipleChoice)

for m in model_RobertaForMultipleChoice.modules():
      for name, value in list(m.named_parameters(recurse=False)):
                setattr(m, name, module.PyroSample(prior=dist.Normal(0, 1)
                                             .expand(value.shape)
                                             .to_event(value.dim())))
        
        
# define parameters for training      
guide_delta = guides.AutoDelta(model_RobertaForMultipleChoice)
optimizer_2 = Adam({""lr"": 0.000000055}) 
scheduler_2 = pyro.optim.StepLR({'optimizer': optimizer_2, 'optim_args': {'lr': 0.000000055}})
svi_delta = SVI(model_RobertaForMultipleChoice, guide_delta, optimizer_2, loss=Trace_ELBO())

# training loop
for m in range(num_iter):

        # calculate the loss and take a gradient step for svi
        # ERRORS OCCUR HERE 
        svi_loss = svi.step(input_ids = input_ids, 
                          attention_mask = attention_mask, 
                          labels = label)

        # update the with the calculated loss 
        total_svi_loss = total_svi_loss + svi_loss
       
        if m % log_interval == 0 and m > 0:
            cur_svi_loss = total_svi_loss / log_interval
            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.9f} | 
                   loss {:5.4f} | ppl {:8.4f}'.format(
                    epoch, m, int(num_lines_train/4), scheduler.get_lr()[0], 
                    cur_svi_loss, math.exp(cur_svi_loss)))
                   
            total_svi_loss = 0 
```",0
63,https://github.com/huggingface/transformers/issues/6192,6192,[],closed,2020-08-01 19:01:41+00:00,20,GPT2 crashing at loss.backward(),"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Ubuntu
- Python version: 3.6
- PyTorch version (GPU?): 1.5.0
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: Yes

@LysandreJik 

## Information

Trying to finetune GPT2 model but the GPU is crashing after `loss.backward()`. I thought it might be just my code but I ran some different code involving finetuning GPT2 and that as well crashed in the same manner. 

Getting this warning as well. 
```
WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
``` 

A week or 2 back, everything was working fine but now the same code is crashing on `loss.backward()`. 

",0
65,https://github.com/huggingface/transformers/issues/6194,6194,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-02 04:10:00+00:00,4,longformertokenizerFast gives error,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version:
- Platform:
- Python version:
- PyTorch version (GPU?):
- Tensorflow version (GPU?):
- Using GPU in script?:
- Using distributed or parallel set-up in script?:

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 tensorflow: @jplu 
documentation: @sgugger
 -->

## Information

Model I am using (Bert, XLNet ...):

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [ X ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [ X ] my own task or dataset: (give details below)

## To reproduce

using LongformerTokenizerFast gives error. but using LongformerTokenizer works without any issues keeping everything same

```

---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-39-263240bbee7e> in <module>
----> 1 main()

<ipython-input-37-2c27a8a4db79> in main()
     99     )
    100 
--> 101     train_dataset = CustomDataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
    102     eval_dataset = CustomDataset(data_args, tokenizer=tokenizer, mode=""test"") if training_args.do_eval else None
    103 

<ipython-input-36-85278feb74ec> in __init__(self, args, tokenizer, limit_length, mode)
    184                     max_length=args.max_seq_length,
    185                     label_list=label_list,
--> 186                     output_mode=self.output_mode,
    187                 )
    188                 start = time.time()

/opt/conda/lib/python3.7/site-packages/transformers/data/processors/glue.py in glue_convert_examples_to_features(examples, tokenizer, max_length, task, label_list, output_mode)
     63         return _tf_glue_convert_examples_to_features(examples, tokenizer, max_length=max_length, task=task)
     64     return _glue_convert_examples_to_features(
---> 65         examples, tokenizer, max_length=max_length, task=task, label_list=label_list, output_mode=output_mode
     66     )
     67 

/opt/conda/lib/python3.7/site-packages/transformers/data/processors/glue.py in _glue_convert_examples_to_features(examples, tokenizer, max_length, task, label_list, output_mode)
    133         max_length=max_length,
    134         padding=""max_length"",
--> 135         truncation=True,
    136     )
    137 

/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in __call__(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_pretokenized, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   1918                 return_length=return_length,
   1919                 verbose=verbose,
-> 1920                 **kwargs,
   1921             )
   1922         else:

/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_pretokenized, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   2103             return_length=return_length,
   2104             verbose=verbose,
-> 2105             **kwargs,
   2106         )
   2107 

/opt/conda/lib/python3.7/site-packages/transformers/tokenization_gpt2.py in _batch_encode_plus(self, *args, **kwargs)
    385         )
    386 
--> 387         return super()._batch_encode_plus(*args, **kwargs)
    388 
    389     def _encode_plus(self, *args, **kwargs) -> BatchEncoding:

/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py in _batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_pretokenized, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
    378         else:
    379             encodings = self._tokenizer.encode_batch(
--> 380                 batch_text_or_text_pairs, add_special_tokens=add_special_tokens, is_pretokenized=is_pretokenized
    381             )
    382 

/opt/conda/lib/python3.7/site-packages/tokenizers/implementations/base_tokenizer.py in encode_batch(self, inputs, is_pretokenized, add_special_tokens)
    247             raise ValueError(""encode_batch: `inputs` can't be `None`"")
    248 
--> 249         return self._tokenizer.encode_batch(inputs, is_pretokenized, add_special_tokens)
    250 
    251     def decode(self, ids: List[int], skip_special_tokens: Optional[bool] = True) -> str:

Exception: Truncation error: Specified max length is too low to respect the various constraints
```
",0
66,https://github.com/huggingface/transformers/issues/6202,6202,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-02 15:12:47+00:00,2,Cannot fine tune my distilbart-cnn-12-6 model because of cuda memory,"I'm trying to fine tune my model like this:

```
import os
os.environ['PYTHONPATH'] += "":/content/transformers/examples""
%cd ""/content/transformers/examples""

!python /content/transformers/examples/seq2seq/finetune.py \
    --learning_rate=3e-5 \
    --fp16 \
    --gpus 1 \
    --do_train \
    --do_predict \
    --n_val 1000 \
    --val_check_interval 0.1 \
    --sortish_sampler \
    --data_dir '/content/dataset' \
    --train_batch_size=1\
    --eval_batch_size=1\
    --output_dir=distilbart_multi_news \
    --num_train_epochs 1 \
    --model_name_or_path /content/model/best_tfmr
```

But even with a batch size of 1 I get this error:
 File ""/content/transformers/examples/seq2seq/finetune.py"", line 344, in <module>
    main(args)
  File ""/content/transformers/examples/seq2seq/finetune.py"", line 322, in main
    logger=logger,
  File ""/content/transformers/examples/lightning_base.py"", line 330, in generic_train
    trainer.fit(model)
  File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 918, in fit
    self.single_gpu_train(model)
  File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 176, in single_gpu_train
    self.run_pretrain_routine(model)
  File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 1076, in run_pretrain_routine
    False)
  File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 279, in _evaluate
    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
  File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 452, in evaluation_forward
    output = model.validation_step(*args)
  File ""/content/transformers/examples/seq2seq/finetune.py"", line 136, in validation_step
    return self._generative_step(batch)
  File ""/content/transformers/examples/seq2seq/finetune.py"", line 163, in _generative_step
    generated_ids = self.model.generate(input_ids=source_ids, attention_mask=source_mask, use_cache=True,)
  File ""/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py"", line 15, in decorate_context
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/transformers/generation_utils.py"", line 459, in generate
    model_specific_kwargs=model_specific_kwargs,
  File ""/usr/local/lib/python3.6/dist-packages/transformers/generation_utils.py"", line 638, in _generate_beam_search
    outputs = self(**model_inputs)  # (batch_size * num_beams, cur_len, vocab_size)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/transformers/modeling_bart.py"", line 1005, in forward
    lm_logits = F.linear(outputs[0], self.model.shared.weight, bias=self.final_logits_bias)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py"", line 1676, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 11.17 GiB total capacity; 10.59 GiB already allocated; 91.81 MiB free; 10.66 GiB reserved in total by PyTorch)

Any idea what to do? 
",0
67,https://github.com/huggingface/transformers/issues/6203,6203,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-02 17:14:48+00:00,2,Issue with fp16_opt_level default,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Linux-4.15.0-1091-oem-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.5.1+cu101 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: Yes, 1 GPU
- Using distributed or parallel set-up in script?: No

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 tensorflow: @jplu 
documentation: @sgugger
 -->

## Information

Model I am using (Bert, XLNet ...): BART @sshleifer

The problem arises when using:
* [ ] the official example scripts: 
* [x] my own modified scripts: Running a modified version of finetune.py (via finetune.sh) that specifies ""bad word tokens"" that BART is prohibited from generating

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: 
* [x] my own task or dataset: Finetuning on a question understanding dataset

## To reproduce

Steps to reproduce the behavior:

1. Run finetune.sh using a GPU with fp16 flag and default settings, instructing the model to perform testing. See set-up below:
Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

2. At the start of testing, an error related to amp will be encountered:

```python
Traceback (most recent call last):
  File ""finetune_prohibition.py"", line 444, in <module>
    main(args)
  File ""finetune_prohibition.py"", line 433, in main
    trainer.test()
  File ""/home/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1281, in test
    results = self.__test_using_best_weights(ckpt_path, test_dataloaders)
  File ""/home/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1321, in __test_using_best_weights
    results = self.fit(model)
  File ""/home/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1003, in fit
    results = self.single_gpu_train(model)
  File ""/home/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 182, in single_gpu_train
    model, optimizers = model.configure_apex(amp, model, self.optimizers, self.amp_level)
  File ""/home/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py"", line 1006, in configure_apex
    model, optimizers = amp.initialize(model, optimizers, opt_level=amp_level)
  File ""/home/lib/python3.6/site-packages/apex/amp/frontend.py"", line 358, in initialize
    return _initialize(models, optimizers, _amp_state.opt_properties, num_losses, cast_model_outputs)
  File ""/home/lib/python3.6/site-packages/apex/amp/_initialize.py"", line 171, in _initialize
    check_params_fp32(models)
  File ""/home/lib/python3.6/site-packages/apex/amp/_initialize.py"", line 87, in check_params_fp32
    name, param.type()))
  File ""/home/lib/python3.6/site-packages/apex/amp/_amp_state.py"", line 32, in warn_or_err
    raise RuntimeError(msg)
RuntimeError: Found param model.model.shared.weight with type torch.cuda.HalfTensor, expected torch.cuda.FloatTensor.
When using amp.initialize, you do not need to call .half() on your model
before passing it, no matter what optimization level you choose.
```


<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
When using the new default of fp16_opt_level=O2, this error is encountered - using fp16_opt_level=O1 solves the issue. I'm unsure if this problem is just related to my machine, and would be interested to see whether others can recreate the problem. Although I used modified scripts, the modifications are simple and shouldn't interact with torch.cuda.HalfTensor classes differently. Let me know if you'd like me to try to recreate this issue on an official GLUE/SQUaD task or if you need any other information from me.",0
68,https://github.com/huggingface/transformers/issues/6204,6204,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}, {'id': 2139563322, 'node_id': 'MDU6TGFiZWwyMTM5NTYzMzIy', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/cleanup', 'name': 'cleanup', 'color': 'e7fc49', 'default': False, 'description': ''}]",closed,2020-08-02 18:38:15+00:00,4,QA Loss Cleanup,"This snippet appears a lot of places and could be factored out into a `calc_qa_loss(logits)`

Requires some care, because I'm not sure how good the test coverage is, and if it doesn't improve readability we shouldn't do it.


```python
        logits = self.qa_outputs(sequence_output)
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1)
        end_logits = end_logits.squeeze(-1)

        outputs = (start_logits, end_logits,) + outputs[2:]
        if start_positions is not None and end_positions is not None:
            # If we are on multi-GPU, split add a dimension
            if len(start_positions.size()) > 1:
                start_positions = start_positions.squeeze(-1)
            if len(end_positions.size()) > 1:
                end_positions = end_positions.squeeze(-1)
            # sometimes the start/end positions are outside our model inputs, we ignore these terms
            ignored_index = start_logits.size(1)
            start_positions.clamp_(0, ignored_index)
            end_positions.clamp_(0, ignored_index)

            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)
            start_loss = loss_fct(start_logits, start_positions)
            end_loss = loss_fct(end_logits, end_positions)
            total_loss = (start_loss + end_loss) / 2
```

@stas00 ",0
103,https://github.com/huggingface/transformers/issues/6291,6291,[],closed,2020-08-06 14:14:12+00:00,2,Why is the lm_head layer in GPT2LMHeadModel not a parameter?,"I loaded the model by 
```
from transformers import GPT2LMHeadModel
gpt2 = GPT2LMHeadModel.from_pretrained('distilgpt2')
```
doing `[n for n,p in gpt2.named_parameters()]` gives me:
```
['gpt2.transformer.wte.weight', 'gpt2.transformer.wpe.weight', 'gpt2.transformer.h.0.ln_1.weight', 'gpt2.transformer.h.0.ln_1.bias', 'gpt2.transformer.h.0.attn.c_attn.weight', 'gpt2.transformer.h.0.attn.c_attn.bias', 'gpt2.transformer.h.0.attn.c_proj.weight', 'gpt2.transformer.h.0.attn.c_proj.bias', 'gpt2.transformer.h.0.ln_2.weight', 'gpt2.transformer.h.0.ln_2.bias', 'gpt2.transformer.h.0.mlp.c_fc.weight', 'gpt2.transformer.h.0.mlp.c_fc.bias', 'gpt2.transformer.h.0.mlp.c_proj.weight', 'gpt2.transformer.h.0.mlp.c_proj.bias', 'gpt2.transformer.h.1.ln_1.weight', 'gpt2.transformer.h.1.ln_1.bias', 'gpt2.transformer.h.1.attn.c_attn.weight', 'gpt2.transformer.h.1.attn.c_attn.bias', 'gpt2.transformer.h.1.attn.c_proj.weight', 'gpt2.transformer.h.1.attn.c_proj.bias', 'gpt2.transformer.h.1.ln_2.weight', 'gpt2.transformer.h.1.ln_2.bias', 'gpt2.transformer.h.1.mlp.c_fc.weight', 'gpt2.transformer.h.1.mlp.c_fc.bias', 'gpt2.transformer.h.1.mlp.c_proj.weight', 'gpt2.transformer.h.1.mlp.c_proj.bias', 'gpt2.transformer.h.2.ln_1.weight', 'gpt2.transformer.h.2.ln_1.bias', 'gpt2.transformer.h.2.attn.c_attn.weight', 'gpt2.transformer.h.2.attn.c_attn.bias', 'gpt2.transformer.h.2.attn.c_proj.weight', 'gpt2.transformer.h.2.attn.c_proj.bias', 'gpt2.transformer.h.2.ln_2.weight', 'gpt2.transformer.h.2.ln_2.bias', 'gpt2.transformer.h.2.mlp.c_fc.weight', 'gpt2.transformer.h.2.mlp.c_fc.bias', 'gpt2.transformer.h.2.mlp.c_proj.weight', 'gpt2.transformer.h.2.mlp.c_proj.bias', 'gpt2.transformer.h.3.ln_1.weight', 'gpt2.transformer.h.3.ln_1.bias', 'gpt2.transformer.h.3.attn.c_attn.weight', 'gpt2.transformer.h.3.attn.c_attn.bias', 'gpt2.transformer.h.3.attn.c_proj.weight', 'gpt2.transformer.h.3.attn.c_proj.bias', 'gpt2.transformer.h.3.ln_2.weight', 'gpt2.transformer.h.3.ln_2.bias', 'gpt2.transformer.h.3.mlp.c_fc.weight', 'gpt2.transformer.h.3.mlp.c_fc.bias', 'gpt2.transformer.h.3.mlp.c_proj.weight', 'gpt2.transformer.h.3.mlp.c_proj.bias', 'gpt2.transformer.h.4.ln_1.weight', 'gpt2.transformer.h.4.ln_1.bias', 'gpt2.transformer.h.4.attn.c_attn.weight', 'gpt2.transformer.h.4.attn.c_attn.bias', 'gpt2.transformer.h.4.attn.c_proj.weight', 'gpt2.transformer.h.4.attn.c_proj.bias', 'gpt2.transformer.h.4.ln_2.weight', 'gpt2.transformer.h.4.ln_2.bias', 'gpt2.transformer.h.4.mlp.c_fc.weight', 'gpt2.transformer.h.4.mlp.c_fc.bias', 'gpt2.transformer.h.4.mlp.c_proj.weight', 'gpt2.transformer.h.4.mlp.c_proj.bias', 'gpt2.transformer.h.5.ln_1.weight', 'gpt2.transformer.h.5.ln_1.bias', 'gpt2.transformer.h.5.attn.c_attn.weight', 'gpt2.transformer.h.5.attn.c_attn.bias', 'gpt2.transformer.h.5.attn.c_proj.weight', 'gpt2.transformer.h.5.attn.c_proj.bias', 'gpt2.transformer.h.5.ln_2.weight', 'gpt2.transformer.h.5.ln_2.bias', 'gpt2.transformer.h.5.mlp.c_fc.weight', 'gpt2.transformer.h.5.mlp.c_fc.bias', 'gpt2.transformer.h.5.mlp.c_proj.weight', 'gpt2.transformer.h.5.mlp.c_proj.bias', 'gpt2.transformer.ln_f.weight', 'gpt2.transformer.ln_f.bias']
```
while running `print (gpt2)` gives me:
```
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
```

My question is why is the lm_head layer not included as the model's parameters? It bothers me at the moment because I am trying to only finetune the LM layer and realise I cant because doing something like `torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=lr, eps=1e-08)` will results in an error as the parameter list is empty
",0
104,https://github.com/huggingface/transformers/issues/6292,6292,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-06 14:45:02+00:00,1,inconclusive truncation strategies in encode_plus?,"# 鉂?Questions & Help

## Details

Not sure if this is a bug or missing feature or I just misunderstood something: specifically, I'm wondering whether a common truncation strategy is missing from [`encode_plus`](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__). More specifically, if you have a input sequence pair consisting of two texts `a` and `b` and invoke `encode_plus` as follows:

``` 
encode_plus(text=a, text_pair=b, max_length=100, truncation=whatever)
```

It seems that there is no truncation strategy that simply cuts of tokens from the end of the (internally concatenated) input sequence consisting of a and b. Instead the three options allow either to truncate from only a, from only b, or from the longest first (which could be either a or b, depending on the input). How can one remove token by token from the right until max_length is reached, e.g., also in the case that len(a)=200 and len(b)=2. In this example, no option seems to be suitable, e.g., ""longest first"" would remove only from a, ""only first"" likewise only from a (both of these remove from the first input a, but should in my case remove from end of the total sequence, e.g., first b then a if necessary), and ""only second"" only from b (which would be removed entirely, but since only second is defined, a will not be truncated so the total length is 200 and thus still longner than max length)

SO link: https://stackoverflow.com/questions/63280435/huggingface-transformers-truncation-strategy-in-encode-plus",0
105,https://github.com/huggingface/transformers/issues/6294,6294,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-06 14:52:13+00:00,1,How to get word and sentence level embeddings from T5-11b,Hi can someone please tell me how to get word and sentence level embeddings for a given sentence from T5-11b?,0
106,https://github.com/huggingface/transformers/issues/6295,6295,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-06 15:27:21+00:00,1,Fix/test convert_mbart.py,"from [forums](https://discuss.huggingface.co/t/how-can-i-convert-a-model-created-with-fairseq/564/10?u=sshleifer)


```

    Traceback (most recent call last):
      File ""./convert_mbart_original_checkpoint_to_pytorch.py"", line 7, in <module>
        from .convert_bart_original_pytorch_checkpoint_to_pytorch import remove_ignore_keys_
    ModuleNotFoundError: No module named '__main__.convert_bart_original_pytorch_checkpoint_to_pytorch'; '__main__' is not a package
```

After I change `from .convert_bart_original_pytorch_checkpoint_to_pytorch import remove_ignore_keys_` to `from convert_bart_original_pytorch_checkpoint_to_pytorch import remove_ignore_keys_` (just removing the dot), the script can run",0
107,https://github.com/huggingface/transformers/issues/6297,6297,[],closed,2020-08-06 15:47:34+00:00,1,Question about BERT model size (transformer block number) ,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
Hi,

Thank you for your interesting work! I have just started to learn BERT and distillation recently. I have some general questions regarding this topic.

1. I want to compare the performance of BERT with different model size (transformer block number). Is it necessary to do distillation? If I just train a BERT with 6 Layers without distillation, does the performance look bad?

2. Do you have to do pretraining every time you change the layer number of BERT? Is it possible to just remove some layers in an existing pre-trained model and finetune on tasks?

3. Why BERT has 12 blocks? Not 11 or 13 etc. ? I couldn't find any explanation.

Thanks,
ZLK",0
108,https://github.com/huggingface/transformers/issues/6301,6301,"[{'id': 1108649053, 'node_id': 'MDU6TGFiZWwxMTA4NjQ5MDUz', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Help%20wanted', 'name': 'Help wanted', 'color': '008672', 'default': False, 'description': 'Extra attention is needed, help appreciated'}]",closed,2020-08-06 17:37:00+00:00,1,Redundant code,"https://github.com/huggingface/transformers/blob/2f2aa0c89cab9a77560e6845578f917a61081c67/examples/text-classification/run_pl_glue.py#L57

This line is useless",0
109,https://github.com/huggingface/transformers/issues/6302,6302,[],closed,2020-08-06 19:15:36+00:00,0,Default value of `n_tpu_cores` in lightning_base.py,"https://github.com/huggingface/transformers/blob/2804fff8393dbda5098b8c9f5e36235e89c50023/examples/lightning_base.py#L294

The default setting ``0`` raise Error
``pytorch_lightning.utilities.exceptions.MisconfigurationException: `tpu_cores` can only be 1, 8 or [<1-8>]``
It should be replaced by ``None``.",0
110,https://github.com/huggingface/transformers/issues/6306,6306,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-06 20:02:39+00:00,4,solving `make quality` failures,"`make quality` currently and for a while now fails with 1 warning and 1 failure:

1. isort warning:
with `isort==4.3.21` or the required by the latest stable `pylint`, or `setup.py`'s current
`git+git://github.com/timothycrosley/isort.git@e63ae06ec7d70b06df9e528357650281a3d3ec22#egg=isort`

we get:
```
make style
```
```
/home/stas/anaconda3/envs/main/lib/python3.7/site-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  ""Distutils was imported before Setuptools. This usage is discouraged ""
[...]
```

If I install `isort==5.3.0` it now wants to reformat a whole bunch of imports:
```
ERROR: /mnt/nvme1/code/huggingface/transformers-unittests/examples/longform-qa/eli5_app.py Imports are incorrectly sorted and/or formatted.
ERROR: /mnt/nvme1/code/huggingface/transformers-unittests/examples/text-generation/pplm/run_pplm_discrim_train.py Imports are incorrectly sorted and/or formatted.
[...] some dozens of those
```

This version has deprecated the `--recursive` flag to `isort`, so once the code is re-formatted to appease to never-ending new rules we can:

1. require  `isort>=5.3.0` in `setup.py`'s `quality` section 
2. remove  the `--recursive` flag to `isort` in Makefile (I validated that just removing this deprecated flag won't change the configuration - it still checks the listed dirs recursively)

the only potential problem if we need to appease to `pylint`, which wants `isort==4.3.21`

----

2. and then older `flake8` can't handle `TYPE_CHECKING` - error

```
flake8 examples templates tests src utils

tests/test_tokenization_common.py:31:5: F401 'transformers.PretrainedConfig' imported but unused
tests/test_tokenization_common.py:31:5: F401 'transformers.PreTrainedModel' imported but unused
tests/test_tokenization_common.py:31:5: F401 'transformers.TFPreTrainedModel' imported but unused
src/transformers/pipelines.py:77:5: F401 '.modeling_utils.PreTrainedModel' imported but unused
src/transformers/pipelines.py:78:5: F401 '.modeling_tf_utils.TFPreTrainedModel' imported but unused
`
```

`flake8-3.8.3` doesn't complain about these.

Can we add:

```
diff --git a/setup.py b/setup.py
index 206c3e35..c33898b4 100644
--- a/setup.py
+++ b/setup.py
@@ -95,7 +95,7 @@ extras[""quality""] = [
     ""black"",
     # ""isort"",
     ""isort @ git+git://github.com/timothycrosley/isort.git@e63ae06ec7d70b06df9e528357650281a3d3ec22#egg=isort"",
-    ""flake8"",
+    ""flake8>=3.8.3"",
 ]
 extras[""dev""] = extras[""testing""] + extras[""quality""] + extras[""ja""] + [""scikit-learn"", ""tensorflow"", ""torch""]
```



",0
111,https://github.com/huggingface/transformers/issues/6308,6308,[],closed,2020-08-06 22:15:45+00:00,4,Debug flag to `run_language_modeling` triggers error,"## Environment info

- `transformers` version: 3.0.2
- Platform: Linux-5.4.0-42-generic-x86_64-with-glibc2.29
- Python version: 3.8.2
- PyTorch version (GPU?): 1.6.0+cu101 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: yes, run_language_modeling.py
- Using distributed or parallel set-up in script?: no

### Who can help
 
I'd guess @sgugger or @julien-c

## Information
I'm using [run_language_modeling.py](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) and turned on debug output to double check things were working as I expected. Unfortunately, [trainer.py](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L628) keys off that debug option to invoke `xm.master_print(...)` and `xm`/`torch_xla.core.xla_model` isn't loaded because I'm not working on a TPU-based system.

The problem arises when using:
* [X] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [X] my own task or dataset: (give details below)

## To reproduce

All steps should be run on a system with a GPU but no TPU. Steps to reproduce the behavior:

1. Run `run_language_modeling.py` with the debug flag:
```sh
python run_language_modeling.py \
  --output_dir ./output \
  --model_type gpt2 \
  --model_name_or_path gpt2 \
  --do_train \
  --train_data_file ./train.txt \
  --learning_rate 1e-4 \
  --num_train_epochs 1 \
  --save_total_limit 2 \
  --save_steps 200 \
  --do_eval \
  --eval_data_file ./eval.txt \
  --debug
```
2. Allow the script to run.

The command will error towards the end with this traceback:
```sh
Epoch:   0%|                                                                                                                                                   | 0/1 [54:06<?, ?it/s]
Traceback (most recent call last):
  File ""run_language_modeling.py"", line 281, in <module>
    main()
  File ""run_language_modeling.py"", line 245, in main
    trainer.train(model_path=model_path)
  File ""/home/user/project/env/lib/python3.8/site-packages/transformers/trainer.py"", line 570, in train
    xm.master_print(met.metrics_report())
NameError: name 'xm' is not defined
```
## Expected behavior

The script exits without error.
",0
112,https://github.com/huggingface/transformers/issues/6310,6310,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-06 23:01:42+00:00,2,collision between different cl arg definitions in examples," The `examples` have an incosistency of how the cl args are defined and parsed. Some rely on PL's main args as `finetune.py` does: https://github.com/huggingface/transformers/blob/master/examples/seq2seq/finetune.py#L410 

```
    parser = argparse.ArgumentParser()
    parser = pl.Trainer.add_argparse_args(parser)
```

others like `run_pl_glue.py` rely on `lightening_base.py`'s main args: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_pl_glue.py#L176

```
    parser = argparse.ArgumentParser()
    add_generic_args(parser, os.getcwd())
```
now that we pushed `--gpus` into  `lightening_base.py`'s main args the scripts that run PL's main args collide and we have:

```
fail.argparse.ArgumentError: argument --gpus: conflicting option string: --gpus
```

i.e. PL already supplies `--gpus` and many other args that some of the scripts in `examples` re-define. 

So either the example scripts need to stop using `pl.Trainer.add_argparse_args(parser)` and rely exclusively on `lightning_base.add_generic_args`, or we need a different clean approach. It appears that different scripts have different needs arg-wise. But they all use `lightning_base`.

The problem got exposed in: https://github.com/huggingface/transformers/pull/6027 and https://github.com/huggingface/transformers/pull/6307",0
113,https://github.com/huggingface/transformers/issues/6313,6313,[],closed,2020-08-07 00:20:07+00:00,2,Error trying to import SquadDataset,"## Environment info

- `transformers` version: 3.0.2
- Platform: Linux-4.15.0-108-generic-x86_64-with-glibc2.10
- Python version: 3.8.2
- PyTorch version (GPU?): 1.4.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help
@sgugger @julien-c 
 
## Information

I am trying to follow the run_squad_trainer example. However I am unable to import the SquadDataset from transformers. I tried updating to 3.0.2 but got the same error. 
https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_squad_trainer.py

```
from transformers import SquadDataset
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-6-13f8e9ce9352> in <module>
----> 1 from transformers import SquadDataset

ImportError: cannot import name 'SquadDataset' from 'transformers' (/home/brian/miniconda3/envs/ML38/lib/python3.8/site-packages/transformers/__init__.py)
```

## Expected behavior

Import runs without error. ",0
114,https://github.com/huggingface/transformers/issues/6316,6316,[],closed,2020-08-07 02:29:13+00:00,4,Dataloader number of workers in Trainer,"https://github.com/huggingface/transformers/blob/175cd45e13b2e33d1efec9e2ac217cba99f6ae58/src/transformers/trainer.py#L252


If you want to use the Trainer from trainer.py, you only have the option to use only 0 number of workers for your dataloader.

However, even if I change the source code to have 10 number or workers for the data loader, the model still uses the same thread.

",0
135,https://github.com/huggingface/transformers/issues/6353,6353,[],closed,2020-08-08 18:36:02+00:00,7,BartModel decodes sequence of incorrect length when decoder_input_ids is specified / Output shape mismatch due to when `use_cache` True/False,"From the [Bart docs](https://huggingface.co/transformers/model_doc/bart.html#bartmodel), the `decoder_input_ids` attribute should be a tensor of shape `(batch_size, target_sequence_length)`. If we call a `BartModel` without specifying `decoder_input_ids`, the decoded sequence length correctly matches that of `input_ids`. When it is specified, the output sequence is not of shape `target_sequence_length`.

## Environment

Name: torch
Version: 1.6.0+cu101

Name: transformers
Version: 3.0.2

Name: tokenizers
Version: 0.8.1rc1

The error can be reproduced in Colab or Kaggle. See [this notebook ](https://colab.research.google.com/gist/xhlulu/dd989fc7f96b777c01c083762375dfbe/bart-sequence-problems.ipynb)for example.

## Example

```python
import transformers as tfm

model = tfm.BartModel.from_pretrained('facebook/bart-base')
tokenizer = tfm.BartTokenizer.from_pretrained('facebook/bart-base')

input_seq = [
    ""What's the capital of Canada?"",
    ""What's the capital of USA?""    
]

output_seq = [
    ""It's Ottawa"",
    ""It's Washington""
]

input_tokens = tokenizer.batch_encode_plus(input_seq, return_tensors='pt', padding=True)
input_ids = input_tokens['input_ids']

output_tokens = tokenizer.batch_encode_plus(output_seq, return_tensors='pt', padding=True)
output_ids = output_tokens['input_ids']

print(input_ids.size(), output_ids.size())  # Returns torch.Size([2, 9]) torch.Size([2, 5])

# Okay
outputs = model.forward(input_ids)
outputs[0].size()  # Returns `torch.Size([2, 9, 768])`

# Incorrect
outputs = model.forward(input_ids, decoder_input_ids=output_ids)
outputs[0].size()  # Returns torch.Size([2, 1, 768])
```",0
136,https://github.com/huggingface/transformers/issues/6354,6354,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-08 19:37:19+00:00,3,GPU memory consumption increases while training,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Google Colab
- Python version: 3.6
- PyTorch version (GPU?):  1.6 
- Tensorflow version (GPU?):
- Using GPU in script?: 
- Using distributed or parallel set-up in script?: No

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 @LysandreJik @sgugger 

 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger 
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 tensorflow: @jplu 
documentation: @sgugger
 -->

## Information

Model I am using (Bert, XLNet ...): XLM Multi-lingual

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)
Please see below steps to reproduce

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [ ] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. Importing Python Libraries and preparing the environment
```python
!pip install git+https://github.com/huggingface/transformers
from transformers import ( 
    AutoTokenizer, 
    AutoConfig, 
    AutoModelForSequenceClassification
)
from torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'
```
2. Loading a pretrained model ""xlm-mlm-tlm-xnli15-1024""

```python

MODEL_NAME_OR_PATH = 'xlm-mlm-tlm-xnli15-1024'
CACHE_DIR='cache'
config = AutoConfig.from_pretrained(
    MODEL_NAME_OR_PATH,
    num_labels=7,
    cache_dir=CACHE_DIR,
)
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME_OR_PATH,
    cache_dir=CACHE_DIR,
)
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME_OR_PATH,
    from_tf=bool("".ckpt"" in MODEL_NAME_OR_PATH),
    config=config,
    cache_dir=CACHE_DIR
)
```
3. Check GPU usage

```
!nvidia-smi
```

4. Moving the model to CUDA

```
model.to(device)
```

then check GPU usage again

```
!nvidia-smi
```

5. Creating test inputs

```python
texts = [
         ""aloe vera , wassernabelkrautextrakt , ackerschachtelhalm extrakt , geranium extract , dandelion extract , natriummethyl two sulfolaurate dinatrium two sulfolaurate , sodium cocoyl isethionat , cocamidopropylbetain , cocamidopropylhydroxysultain , kokosglucoside , natrium chlorid , glyceryl oleat , natriumenzoat , guar hydroxypropyltrimonium chloride , tetrasodium glutamat diacetat , decyl glucoside , sodium levulinate , hydroxams盲ure , sodium pca , caprylyl glycol , zitronens盲ure , als koscher zertifizierte pflanzliches glycerin , eukalyptus枚l , pfefferminz枚l , zitronengrass枚l . zertifiziert als organisch . wir verwenden nur die besten nat眉rlichen zutaten . wenn m枚glich , verwenden wir onezerozero percent zertifizierte organische zutaten und niemals : petrochemikalien , sulfate , parabene , phthalate oder synthetische duftstoffe oder farben , tea , dea , glycol , silikon oder pegs . nur an menschen getested . in anderen worten : wir stellen nur absolut reine produkte her und garantieren mit onezerozero percent sicherheit , dass sie ihrem k枚rper keine chemikalien zuf眉hren ."",
         ""was es bewirkt das waschgel auf kokosnussbasis entfernt 眉bersch眉ssiges hautfett , w盲hrend das darin enthaltene aloe vera gel die haut erneuert . das gesichtspflege gel f眉r eine tiefenwirksame porenreinigung . ""
         ""stimmungsaufhellendes orangen枚l f眉r die massage ( kein 盲therisches 枚l f眉r duftlampen ) . ohne paraffin ohne mineral枚l , ohne parabene , ohne konservierungsmittel , selbstverst盲ndlich ohne tierversuche , vegan"",
         ""onezerozero percent natives kaltgepresstes biomandel枚l aus one . kaltpressung . sanfte und schonende mechanische verarbeitung in deutschland . "",
         ""aloe vera , wassernabelkrautextrakt , ackerschachtelhalm extrakt , geranium extract , dandelion extract , natriummethyl two sulfolaurate dinatrium two sulfolaurate , sodium cocoyl isethionat , cocamidopropylbetain , cocamidopropylhydroxysultain , kokosglucoside , natrium chlorid , glyceryl oleat , natriumenzoat , guar hydroxypropyltrimonium chloride , tetrasodium glutamat diacetat , decyl glucoside , sodium levulinate , hydroxams盲ure , sodium pca , caprylyl glycol , zitronens盲ure , als koscher zertifizierte pflanzliches glycerin , eukalyptus枚l , pfefferminz枚l , zitronengrass枚l . zertifiziert als organisch . wir verwenden nur die besten nat眉rlichen zutaten . wenn m枚glich , verwenden wir onezerozero percent zertifizierte organische zutaten und niemals : petrochemikalien , sulfate , parabene , phthalate oder synthetische duftstoffe oder farben , tea , dea , glycol , silikon oder pegs . nur an menschen getested . in anderen worten : wir stellen nur absolut reine produkte her und garantieren mit onezerozero percent sicherheit , dass sie ihrem k枚rper keine chemikalien zuf眉hren ."",
         ""was es bewirkt das waschgel auf kokosnussbasis entfernt 眉bersch眉ssiges hautfett , w盲hrend das darin enthaltene aloe vera gel die haut erneuert . das gesichtspflege gel f眉r eine tiefenwirksame porenreinigung . ""
         ""stimmungsaufhellendes orangen枚l f眉r die massage ( kein 盲therisches 枚l f眉r duftlampen ) . ohne paraffin ohne mineral枚l , ohne parabene , ohne konservierungsmittel , selbstverst盲ndlich ohne tierversuche , vegan"",
         ""onezerozero percent natives kaltgepresstes biomandel枚l aus one . kaltpressung . sanfte und schonende mechanische verarbeitung in deutschland . "",
         
]

encode = tokenizer(texts, padding='max_length', max_length=200, truncation=True, return_tensors='pt')
for k in encode:
  encode[k] = encode[k].to(device)
```

6. Re-run steps below to see the GPU usage increases every time we run

```python
model.train()
model(**encode)
!nvidia-smi
``` 

Got error as below:

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-242-decae3a1d2bf> in <module>()
      1 model.train()
----> 2 model(**encode)

8 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in linear(input, weight, bias)
   1674         ret = torch.addmm(bias, input, weight.t())
   1675     else:
-> 1676         output = input.matmul(weight.t())
   1677         if bias is not None:
   1678             output += bias

RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.17 GiB total capacity; 10.10 GiB already allocated; 13.81 MiB free; 10.74 GiB reserved in total by PyTorch)

```


However, if you modify the code in the step 6 as follow: 

```
model.train()
output = model(**encode)
print(output)
del output
!nvidia-smi
```
The GPU usage will be stable and the same as every run.


When I'm using batch_size >= 16 with Trainer class I have been facing this issue

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

The GPU usage should stay the same for every run. So that we can run a much bigger batch size.

Right now, I can only use per_device_batch_size <=12 with Trainer class.

Looking forward to learning from you and thank you so much!

<!-- A clear and concise description of what you would expect to happen. -->
",0
137,https://github.com/huggingface/transformers/issues/6360,6360,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-09 02:51:35+00:00,2,Bug in squad example with XLNet,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Linux-3.10.0-957.21.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
- Python version: 3.7.4
- PyTorch version (GPU?): 1.4.0 (False)
- Tensorflow version (GPU?): 2.0.0 (False)
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 examples/distillation: @VictorSanh

 -->

## Information

Model I am using (Bert, XLNet ...):
XLNet

The problem arises when using:
the official example scripts: (give details below)

The tasks I am working on is:
an official GLUE/SQUaD task: (give the name)


## To reproduce

Steps to reproduce the behavior:

1. run_squad.py with xlnet as the mode type
2. I think because the AutoModelForQuestionAnswering is mapping xlnet to XLNetForQuestionAnsweringSimple, it will have input error. Since XLNetForQuestionAnsweringSimple does not require cls_index, it will throw an error
3. https://github.com/huggingface/transformers/blob/d9149f00d1a4650bafa7e1cd73e10398193c852c/examples/question-answering/run_squad.py#L194

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior


<!-- A clear and concise description of what you would expect to happen. -->
",0
138,https://github.com/huggingface/transformers/issues/6362,6362,[],closed,2020-08-09 04:43:46+00:00,2,"[TFTrainer] Error ""iterating over `tf.Tensor` is not allowed""","## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2 (from pip)
- Platform: Linux-4.15.0-91-generic-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.6
- PyTorch version (GPU?): not installed (NA)
- Tensorflow version (GPU?): 2.3.0 (True) (Same error on TF2.2 and TF2.1)
- Using GPU in script?: Yes - GeForce GTX 1080 Ti
- Using distributed or parallel set-up in script?: No

### Who can help
 Trainer: @sgugger  tensorflow: @jplu 
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 tensorflow: @jplu 
documentation: @sgugger
 -->

## Information

Model I am using (Bert, XLNet ...): GPT2

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [X] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [X] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:
1.  Install Tensorflow 2.3.0, Transformers 3.0.2

1. Run the following code:

```python3
from transformers import TFGPT2LMHeadModel, TFTrainer, TFTrainingArguments
import tensorflow as tf

tfds_train_dataset = tf.data.Dataset.from_tensor_slices(
    tf.random.uniform([4000, 1024], minval=1, maxval=10, dtype=tf.int32))

model = TFGPT2LMHeadModel.from_pretrained(""gpt2"")

training_args = TFTrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

trainer = TFTrainer(
    model=model,
    args=training_args,
    train_dataset=tfds_train_dataset,
)

trainer.train()
```

2. Results in the following output + error:
```
2020-08-09 01:41:28.331697: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-09 01:41:30.461375: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-08-09 01:41:30.466239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-08-09 01:41:30.466271: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-09 01:41:30.468575: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-09 01:41:30.470629: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-09 01:41:30.471013: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-09 01:41:30.473522: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-09 01:41:30.474947: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-09 01:41:30.481193: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-09 01:41:30.482710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-09 01:41:30.483080: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-09 01:41:30.512602: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3210790000 Hz
2020-08-09 01:41:30.514335: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c678f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-09 01:41:30.514408: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-09 01:41:30.648534: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c92000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-09 01:41:30.648597: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-08-09 01:41:30.650365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-08-09 01:41:30.650446: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-09 01:41:30.650523: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-09 01:41:30.650586: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-09 01:41:30.650646: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-09 01:41:30.650708: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-09 01:41:30.650767: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-09 01:41:30.650829: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-09 01:41:30.653179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-09 01:41:30.653232: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-09 01:41:31.392168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-09 01:41:31.392212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-08-09 01:41:31.392225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-08-09 01:41:31.393566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7389 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-08-09 01:41:34.003855: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-08-09 01:41:34.145974: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
All model checkpoint weights were used when initializing TFGPT2LMHeadModel.

All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
Traceback (most recent call last):
  File ""gpt2-training_bug.py"", line 26, in <module>
    trainer.train()
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/transformers/trainer_tf.py"", line 412, in train
    for step, training_loss in enumerate(self._training_steps(train_ds, optimizer)):
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/transformers/trainer_tf.py"", line 459, in _training_steps
    for i, loss in enumerate(self._accumulate_next_gradients(ds)):
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/transformers/trainer_tf.py"", line 492, in _accumulate_next_gradients
    yield _accumulate_next()
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 823, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 697, in _initialize
    *args, **kwds))
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2855, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3213, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3075, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 600, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 973, in wrapper
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:

    /home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/transformers/trainer_tf.py:486 _accumulate_next  *
        per_replica_features, per_replica_labels = next(iterator)
    /home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:503 __iter__
        self._disallow_iteration()
    /home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:496 _disallow_iteration
        self._disallow_when_autograph_enabled(""iterating over `tf.Tensor`"")
    /home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:474 _disallow_when_autograph_enabled
        "" indicate you are trying to use an unsupported feature."".format(task))

    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.
```

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

Start Training

<!-- A clear and concise description of what you would expect to happen. -->
",0
139,https://github.com/huggingface/transformers/issues/6367,6367,[],closed,2020-08-09 16:35:17+00:00,0,[s2s] pass max_length to config through command line,"Problem:
In summarization, ideal beam search params vary between finetuning datasets. If you are finetuning pegasus-large on xsum, you want config.max_length=56, if you are finetuning pegasus-large on cnn-dailymail you want config.max_length=128. 

### Solutions
- the command line arg should be called `max_generate_length`

- This could also be addressed through adding `task_specific_params` for every dataset. Then you could pass `--task summarize_xsum` to finetune.py and things would work. Kinda lame though.",0
141,https://github.com/huggingface/transformers/issues/6369,6369,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-09 17:05:24+00:00,6,trainer/lightning_base: Arbitrary config updates through command line,"this issue [https://github.com/huggingface/transformers/issues/6367], and a recent one to add dropout to the command line, as well as the usage of task_specific_params during finetuning, are all one off solutions to address a larger problem. During finetuning/training, it is very difficult to arbitrarily set config attributes. For `examples/lightning_base.py`, you need to save a whole new config to json and put it in a directory, which is fairly annoying method for changing hyperparameters, so we add lots of them, like `--dropout --attention_dropout --encoder_layerdrop --decoder_layerdrop` through `argparse.add_argument`. 
It would be a better user experience if I could just pass any kwarg without editing the code.

This seems possible with the `fire` package. But I would prefer an `argparse` solution as there is another issue open to delete the `fire` dependency, I also asked a similar question on
[stackoverflow](https://stackoverflow.com/questions/63329044/python-argparse-allow-unregistered-arguments)",0
143,https://github.com/huggingface/transformers/issues/6373,6373,[],closed,2020-08-10 02:59:52+00:00,3,Pegasus finetuning diary,"Best score so far .2065 Rouge, much worse than paper. Generations appear to start lower case/be missing words at the beginning.



Clues:
- adding `<pad>` as prefix (like for generation) makes loss nan for at least 1000 steps (I killed it). 
- Without prefix, loss is nan for 5 steps, them improves.
- distillation with teacher produces huge hidden state MSE losses. This is probably unrelated and caused by the same large activations that break fp16.

Suspects:
- different causal mask than tf?
- tf doesn't shift labels or add a decoder prefix token. We shift labels but don't add a prefix token. there is a suraj issue where this appears to be suboptimal for t5 (which also has no bos token).
- bug in label smoothing


Best run:

![image](https://user-images.githubusercontent.com/6045025/89748679-5c7d0900-da92-11ea-9505-59ee413193d9.png)

![image](https://user-images.githubusercontent.com/6045025/89748681-5dae3600-da92-11ea-94bd-5043c586bd9c.png)
",0
145,https://github.com/huggingface/transformers/issues/6375,6375,[],closed,2020-08-10 06:28:09+00:00,1,CUDA Out of Memory,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     https://stackoverflow.com/questions/63335442/how-do-i-deal-with-cuda-out-of-memory-while-finetuning-bart
     -->

## Details
<!-- Description of your issue -->
I was trying to finetune BART on google collab using the xsum dataset and the finetuning script and I got this:
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.73 GiB total capacity; 13.67 GiB already allocated; 15.88 MiB free; 13.72 GiB reserved in total by PyTorch)
Does this mean I have to use a smaller model?

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:
https://stackoverflow.com/questions/63335442/how-do-i-deal-with-cuda-out-of-memory-while-finetuning-bart",0
147,https://github.com/huggingface/transformers/issues/6384,6384,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-10 12:36:51+00:00,4,"AttributeError: type object ""BartTokenizer"" has no attribute 'name'","## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Google Colab
- Python version: 3.7
- PyTorch version (GPU?): 
- Tensorflow version (GPU?):
- Using GPU in script?:  Yes
- Using distributed or parallel set-up in script?: don't know

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 tensorflow: @jplu 
documentation: @sgugger
 -->

## Information

Model I am using (Bert, XLNet ...):

The problem arises when using:
* [x] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [ ] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. Using the script provided on Hugging face library 
 : https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
",0
148,https://github.com/huggingface/transformers/issues/6392,6392,[],closed,2020-08-10 15:32:16+00:00,1,seq2seq examples require pytest,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Linux-5.4.0-42-generic-x86_64-with-glibc2.29
- Python version: 3.8.2
- PyTorch version (GPU?): 1.6.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: n/a
- Using distributed or parallel set-up in script?: n/a

### Who can help
 examples/seq2seq: @sshleifer
documentation: @sgugger

## To reproduce

Steps to reproduce the behavior:

1. Create a new virtual environment and set it up to run the examples tests. Do _not_ install `pytest` and `pytest-xdist`.
2. Run the tests with `unittest` as [described in the docs](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#tests)

## Expected behavior
The examples tests pass. Actual behavior:

```sh
======================================================================
ERROR: seq2seq.test_bash_script (unittest.loader._FailedTest)
----------------------------------------------------------------------
ImportError: Failed to import test module: seq2seq.test_bash_script
Traceback (most recent call last):
  File ""/usr/lib/python3.8/unittest/loader.py"", line 436, in _find_test_path
    module = self._get_module_from_name(name)
  File ""/usr/lib/python3.8/unittest/loader.py"", line 377, in _get_module_from_name
    __import__(name)
  File ""/home/dmlap/projects/transformers/examples/seq2seq/test_bash_script.py"", line 8, in <module>
    import pytest
ModuleNotFoundError: No module named 'pytest'


======================================================================
ERROR: seq2seq.test_seq2seq_examples (unittest.loader._FailedTest)
----------------------------------------------------------------------
ImportError: Failed to import test module: seq2seq.test_seq2seq_examples
Traceback (most recent call last):
  File ""/usr/lib/python3.8/unittest/loader.py"", line 436, in _find_test_path
    module = self._get_module_from_name(name)
  File ""/usr/lib/python3.8/unittest/loader.py"", line 377, in _get_module_from_name
    __import__(name)
  File ""/home/dmlap/projects/transformers/examples/seq2seq/test_seq2seq_examples.py"", line 10, in <module>
    import pytest
ModuleNotFoundError: No module named 'pytest'


----------------------------------------------------------------------
Ran 16 tests in 179.454s

FAILED (errors=2)
```

Perhaps the documentation should be updated to require `pytest`?",0
150,https://github.com/huggingface/transformers/issues/6395,6395,[],closed,2020-08-10 16:45:57+00:00,2,Bug in the question answering pipeline,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.6.0+cu101 (False)
- Tensorflow version (GPU?): 2.3.0 (False)
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no

## Information
The bug appears since transformers 3.0.1 but not before.

Model I am using distilbert-base-cased-distilled-squad:

The problem arises when using:
* [ ] my own modified scripts: 
```

from transformers import pipeline
model = ""distilbert-base-cased-distilled-squad""
qa_pipeline = pipeline(
    ""question-answering"",
    model=model,
    tokenizer=model,
)

instance = {
  ""question"": ""what is your product?"",
  ""context"": "" is an amazing new platform that help businesses of students from BarIlan University that are enthusiastic about conversational AI. The difference between our Sprybot platform and other chat bots is that constructing chat bot is a long and hard process and with Sprybot you can do it quickly and eaily. You can construct chatbot using our platform just by feeding textual description of you business that contain any details important for costumers. The time it takes to create a bot using our platform is the time takes you to describe your business. In order to create Sprybot we used natural language processing and state of the art deep learning artificial intelligence. At the moment you cant buy our product because its still under construction. Sprybot can answer questions about your business but it can not talk about anything else other than the information was fed to it.""
}
qa_pipeline(instance)

```


Notice: little changes in the context text make the bug to not show up
## To reproduce

Steps to reproduce the behavior:

1. [fully reproduced on google colab](https://colab.research.google.com/drive/1YqamXA6qq8xxWXhq6VqEA9clHsEVW7sh?usp=sharing)


```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-5-a5f26c48556d> in <module>()
      4 }
      5 
----> 6 qa_pipeline(instance)

1 frames
/usr/local/lib/python3.6/dist-packages/transformers/pipelines.py in __call__(self, *args, **kwargs)
   1314                         ),
   1315                     }
-> 1316                     for s, e, score in zip(starts, ends, scores)
   1317                 ]
   1318 

/usr/local/lib/python3.6/dist-packages/transformers/pipelines.py in <listcomp>(.0)
   1314                         ),
   1315                     }
-> 1316                     for s, e, score in zip(starts, ends, scores)
   1317                 ]
   1318 

KeyError: 0
```


## Expected behavior

get the qa pipline output with no errors
",0
151,https://github.com/huggingface/transformers/issues/6399,6399,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-10 20:12:24+00:00,7,DPR retriever module,"I see https://github.com/huggingface/transformers/pull/5279 that describes the DPR flow.

Just checking to see when the retriever module will be available.
Many thanks for making DPR available !",0
152,https://github.com/huggingface/transformers/issues/6400,6400,[],closed,2020-08-10 20:47:48+00:00,2,ZeroDivisionError with Reformer,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.0
- Platform: Linux-5.4.0-42-generic-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.6.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No

## Information

Model I am using (Bert, XLNet ...): **Reformer**

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [X] my own modified scripts: (give details below) (well, actually, not my own, but @patrickvonplaten 's) 

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [x] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. Execute @patrickvonplaten 's notebook available at https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb 
2. I've tried to run it on google colab and works fine. The problem appears when I try to run on my machine.
3. I've tried it with two different clean virtual environments (python 3.6 and 3.7), but they've both failed.
4. I haven't change the dataset, nor any model config/training args.
4. After calling trainer.train() I get the following error

```
---------------------------------------------------------------------------
ZeroDivisionError                         Traceback (most recent call last)
<ipython-input-13-02431faf649a> in <module>
      8 
      9 # train
---> 10 trainer.train()

/data/venv36/lib/python3.6/site-packages/transformers/trainer.py in train(self, model_path)
    394             t_total = self.args.max_steps
    395             num_train_epochs = (
--> 396                 self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1
    397             )
    398         else:

ZeroDivisionError: integer division or modulo by zero

```
## Expected behavior

The model should begin to train

Thanks!",0
153,https://github.com/huggingface/transformers/issues/6401,6401,"[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",closed,2020-08-10 21:24:24+00:00,7,"[TF Longformer] Add Multiple Choice, Seq Classification Model","# 馃殌 Feature request

`modeling_longformer.py` has the classes `LongformerForSequenceClassification`, `LongformerForMultipleChoice` and `LongformerForTokenClassification` which are not present  in `modeling_tf_longformer.py` at the moment.
Those classes should be equally added to `modeling_tf_longformer.py`.
 
## Motivation

The pretrained weights for TFLongformer are available so that these classes could be used for finetuning.

## Your contribution

This issue is a good first issue because it is not too complicated to add these models. One should take a look at `modeling_tf_roberta.py` to see how these models are implemented for `TFRoberta` and implement them analogous for `TFLongformer`. Please make sure that the docstring is correct and that test are added for each class (again Roberta can serve as an example here, check out `test_modeling_tf_roberta.py`).

I am happy to guide interested community contributors through the PR and help them get it merged.
",0
154,https://github.com/huggingface/transformers/issues/6406,6406,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-11 05:05:27+00:00,2,RuntimeError: Error while creating shape using tf-xlm-roberta-large,"I get the following runtime error after the 2nd fold.

this is my model:
maxlen = 50
`with strategy.scope():
        #bert_encoder = TFBertModel.from_pretrained(model_name)
        base_model = TFAutoModel.from_pretrained(model_name)
        input_word_ids = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = ""input_word_ids"")
        input_mask = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = ""input_mask"")
        input_type_ids = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = ""input_type_ids"")

        embedding = base_model([input_word_ids, input_mask, input_type_ids])[0]
        output = tf.keras.layers.Dense(3, activation = 'softmax')(embedding[:, 0, :])

        model = tf.keras.Model(inputs = [input_word_ids, input_mask, input_type_ids], outputs = output)
        model.compile(tf.keras.optimizers.Adam(lr = 1e-5), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])

`
And the traceback below:

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-27-a45ce90453f2> in <module>
     22 
     23     K.clear_session()
---> 24     model = build_model(maxlen, model_name)
     25     checkpoint = tf.keras.callbacks.ModelCheckpoint(
     26                 'XLMRoberta_fold-%i.h5'%fold, monitor = 'val_loss', verbose = 1, save_best_only = True,

<ipython-input-23-9faa2e5f1d9b> in build_model(maxlen, model_name)
      2     with strategy.scope():
      3         #bert_encoder = TFBertModel.from_pretrained(model_name)
----> 4         base_model = TFAutoModel.from_pretrained(model_name)
      5         input_word_ids = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = ""input_word_ids"")
      6         input_mask = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = ""input_mask"")

/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    421         for config_class, model_class in TF_MODEL_MAPPING.items():
    422             if isinstance(config, config_class):
--> 423                 return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
    424         raise ValueError(
    425             ""Unrecognized configuration class {} for this kind of TFAutoModel: {}.\n""

/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    482             return load_pytorch_checkpoint_in_tf2_model(model, resolved_archive_file, allow_missing_keys=True)
    483 
--> 484         model(model.dummy_inputs, training=False)  # build the network with dummy inputs
    485 
    486         assert os.path.isfile(resolved_archive_file), ""Error retrieving file {}"".format(resolved_archive_file)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    966           with base_layer_utils.autocast_context_manager(
    967               self._compute_dtype):
--> 968             outputs = self.call(cast_inputs, *args, **kwargs)
    969           self._handle_activity_regularization(inputs, outputs)
    970           self._set_mask_metadata(inputs, outputs, input_masks)

/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_roberta.py in call(self, inputs, **kwargs)
    229             heads.
    230         """"""
--> 231         outputs = self.roberta(inputs, **kwargs)
    232         return outputs
    233 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    966           with base_layer_utils.autocast_context_manager(
    967               self._compute_dtype):
--> 968             outputs = self.call(cast_inputs, *args, **kwargs)
    969           self._handle_activity_regularization(inputs, outputs)
    970           self._set_mask_metadata(inputs, outputs, input_masks)

/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_bert.py in call(self, inputs, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, training)
    604             # head_mask = tf.constant([0] * self.num_hidden_layers)
    605 
--> 606         embedding_output = self.embeddings([input_ids, position_ids, token_type_ids, inputs_embeds], training=training)
    607         encoder_outputs = self.encoder(
    608             [embedding_output, extended_attention_mask, head_mask, output_attentions, output_hidden_states],

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    962         # Eager execution on data tensors.
    963         with backend.name_scope(self._name_scope()):
--> 964           self._maybe_build(inputs)
    965           cast_inputs = self._maybe_cast_inputs(inputs)
    966           with base_layer_utils.autocast_context_manager(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)
   2414         # operations.
   2415         with tf_utils.maybe_init_scope(self):
-> 2416           self.build(input_shapes)  # pylint:disable=not-callable
   2417       # We must set also ensure that the layer is marked as built, and the build
   2418       # shape is stored since user defined build functions may not be calling

/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_bert.py in build(self, input_shape)
    144                 ""weight"",
    145                 shape=[self.vocab_size, self.hidden_size],
--> 146                 initializer=get_initializer(self.initializer_range),
    147             )
    148         super().build(input_shape)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)
    575         synchronization=synchronization,
    576         aggregation=aggregation,
--> 577         caching_device=caching_device)
    578     if regularizer is not None:
    579       # TODO(fchollet): in the future, this should be handled at the

/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)
    741         dtype=dtype,
    742         initializer=initializer,
--> 743         **kwargs_for_getter)
    744 
    745     # If we set an initializer and the variable processed it, tracking will not

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)
    139       synchronization=synchronization,
    140       aggregation=aggregation,
--> 141       shape=variable_shape if variable_shape else None)
    142 
    143 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    257   def __call__(cls, *args, **kwargs):
    258     if cls is VariableV1:
--> 259       return cls._variable_v1_call(*args, **kwargs)
    260     elif cls is Variable:
    261       return cls._variable_v2_call(*args, **kwargs)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)
    218         synchronization=synchronization,
    219         aggregation=aggregation,
--> 220         shape=shape)
    221 
    222   def _variable_v2_call(cls,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in getter(**kwargs)
     64 
     65   def getter(**kwargs):
---> 66     return captured_getter(captured_previous, **kwargs)
     67 
     68   return getter

/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py in creator_with_resource_vars(next_creator, **kwargs)
   1765         kwargs[""initial_value""] = kwargs[""initial_value""].wrapped_value
   1766 
-> 1767       return self._create_variable(next_creator, **kwargs)
   1768 
   1769     def distributed_getter(getter, *args, **kwargs):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py in _create_variable(self, next_creator, **kwargs)
    670                                            tpu_values.TPUMirroredVariable,
    671                                            tpu_values.TPUSyncOnReadVariable,
--> 672                                            **kwargs)
    673 
    674   def _reduce_to(self, reduce_op, value, destinations, experimental_hints):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/values.py in create_mirrored_variable(strategy, real_mirrored_creator, mirrored_cls, sync_on_read_cls, **kwargs)
    692   # here.
    693   with tape.stop_recording():
--> 694     value_list = real_mirrored_creator(**kwargs)
    695     var_cls = sync_on_read_cls if is_sync_on_read else mirrored_cls
    696     result = var_cls(strategy, value_list, aggregation)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py in _real_mirrored_creator(**kwargs)
    660 
    661           with context.device_policy(context.DEVICE_PLACEMENT_SILENT):
--> 662             v = next_creator(**kwargs)
    663 
    664           assert not isinstance(v, tpu_values.TPUMirroredVariable)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kwargs)
    196                         shape=None):
    197     """"""Call on Variable class. Useful to force the signature.""""""
--> 198     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
    199     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
    200       previous_getter = _make_getter(getter, previous_getter)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)
   2596         synchronization=synchronization,
   2597         aggregation=aggregation,
-> 2598         shape=shape)
   2599   else:
   2600     return variables.RefVariable(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    261       return cls._variable_v2_call(*args, **kwargs)
    262     else:
--> 263       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    264 
    265 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
   1432           aggregation=aggregation,
   1433           shape=shape,
-> 1434           distribute_strategy=distribute_strategy)
   1435 
   1436   def _init_from_args(self,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)
   1568                 name=""initial_value"", dtype=dtype)
   1569           if shape is not None:
-> 1570             if not initial_value.shape.is_compatible_with(shape):
   1571               raise ValueError(
1572                   ""The initial value's shape (%s) is not compatible with ""

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in shape(self)
   1063         # `_tensor_shape` is declared and defined in the definition of
   1064         # `EagerTensor`, in C.
-> 1065         self._tensor_shape = tensor_shape.TensorShape(self._shape_tuple())
   1066       except core._NotOkStatusException as e:
   1067         six.raise_from(core._status_to_exception(e.code, e.message), None)

RuntimeError: Error while creating shape",0
155,https://github.com/huggingface/transformers/issues/6407,6407,[],closed,2020-08-11 06:16:53+00:00,1,Slow Decoding Speed when using BertForLMModel,"I set the BertLMHeadModel as Decoder in my Seq2Seq model. It seems to work well in training. But when decoing, it decodes very slowly. I think there is no layer_past used in GPT2, XLNet in BertLMHeadModel and many attentions are computed repetitively? 
",0
156,https://github.com/huggingface/transformers/issues/6408,6408,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-11 07:17:15+00:00,3,"i have used t5_base for abstractive summarization but it is not giving good results,Could you please give me solution for this","# 馃枼 Benchmarking `transformers`

## Benchmark

Which part of `transformers` did you benchmark?

## Set-up

What did you run your benchmarks on? Please include details, such as: CPU, GPU? If using multiple GPUs, which parallelization did you use?

## Results

Put your results here!
",0
157,https://github.com/huggingface/transformers/issues/6409,6409,[],closed,2020-08-11 07:24:04+00:00,2,TF2 TPU slow?,"## Environment info
- `transformers` version: 3.0.2
- Platform: ubuntu 18.04
- Python version: 3.6
- Tensorflow version (GPU?): TF 2.3.0

### Who can help
@jplu 

## Information

Model I am using (Bert, XLNet ...): BERT

The problem arises when using:
* [x] my own modified scripts: (give details below)

The tasks I am working on is:
* [x] task: MLM

## To reproduce
I am using the TFTrainer from `transformers` for MLM pretraining. However, it seems that even for TPU training each batch is fed separately to the TPU, while it's usually more common to feed a bunch of batches to the TPU for efficiency (see https://github.com/tensorflow/models/blob/master/official/nlp/bert/model_training_utils.py#L226).

I am not sure that's the only problem, but MLM pretraining BERT is around 3x slower on TPU with the TFTrainer compared to the official implementation (https://github.com/google-research/bert).

For better TPU usage, we probably need sth like here:
https://github.com/tensorflow/models/blob/master/official/nlp/bert/model_training_utils.py#L345-L361",0
158,https://github.com/huggingface/transformers/issues/6410,6410,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-11 09:11:11+00:00,2,Cannot unzip the XNLI-MT 1.0 zip file.,"# 鉂?Questions & Help

Is there anyone who succeeding the unzip of XNLI-MT 1.0 zip file?

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->
",0
159,https://github.com/huggingface/transformers/issues/6414,6414,[],closed,2020-08-11 14:30:00+00:00,7,TypeError: forward() got an unexpected keyword argument 'labels',"## Environment info

- `transformers` version: 3.0.2
- Platform: Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid
- Python version: 3.7.7
- PyTorch version (GPU?): 1.6.0 (True)
- Tensorflow version (GPU?): 2.3.0 (False)
- Using GPU in script?: True
- Using distributed or parallel set-up in script?: Distributed

Hey there,

I've run into this issue and not sure how to fix it:

    TypeError: forward() got an unexpected keyword argument 'labels'

I'm running transformers v3.0.2 installed via pip

Please see my code below. There is nothing fancy going on, I'm just trying to train RobertaMLM for a few more epochs on a different dataset. 

```python
import os
import argparse
import datetime
from torch.utils.tensorboard import SummaryWriter
from transformers import RobertaModel, RobertaConfig, RobertaTokenizerFast, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments
from configs import model_directory, tensorboard_directory
from logger import get_logger

log = get_logger(__name__)


args = argparse.Namespace(
	seed=42,
	model_id=""Roberta2"",
	pretrained_model_name_or_path=""roberta-base"",
	vocab_file=""/data/nlp/roberta_vocabulary/roberta-base-vocab.json"",
	merges_file=""/data/nlp/roberta_vocabulary/roberta-base-merges.txt"",
	filename=""/data/nlp/trc2.txt"",
	block_size=2**7,
	epochs=25,
)

output_directory = os.path.join(model_directory, args.model_id)
os.makedirs(output_directory, exist_ok=True)
os.environ[""TOKENIZERS_PARALLELISM""] = ""false""


def build_model():
	tokenizer = RobertaTokenizerFast.from_pretrained(pretrained_model_name_or_path=args.pretrained_model_name_or_path, lowercase=True, add_prefix_space=True, max_len=512)

	config = RobertaConfig.from_pretrained(args.pretrained_model_name_or_path)
	config.output_hidden_states = False

	model = RobertaModel.from_pretrained(pretrained_model_name_or_path=args.pretrained_model_name_or_path, config=config, cache_dir=output_directory)

	dataset = LineByLineTextDataset(
		tokenizer=tokenizer,
		file_path=args.filename,
		block_size=args.block_size,
	)

	data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

	training_args = TrainingArguments(
		seed=args.seed,
		output_dir=output_directory,
		overwrite_output_dir=True,
		num_train_epochs=args.epochs,
		per_device_train_batch_size=128,
		save_steps=10_000,
		# save_total_limit=2,
		fp16=True,
		fp16_opt_level=""O1""
	)

	trainer = Trainer(
		model=model,
		args=training_args,
		data_collator=data_collator,
		train_dataset=dataset,
		prediction_loss_only=True,
	)

	trainer.train()
	trainer.save_model(output_directory)

```


tag: @sgugger ",0
