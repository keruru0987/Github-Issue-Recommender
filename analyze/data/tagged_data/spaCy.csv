,html_url,number,labels,state,created_at,comments,title,body,rel
80,https://github.com/explosion/spaCy/issues/2892,2892,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2018-11-01 05:19:56+00:00,2,Deploying to Azure Web Service fails with: en_core_web_sm,"## How to reproduce the problem
Follow this link: https://docs.microsoft.com/en-us/azure/app-service/containers/quickstart-python and using own python code as application.py containing this line: nlp_date = spacy.load('en_core_web_sm') 
Although it works locally in my computer (http://localhost:5000/), it fails when deployed to Azure with the following error:
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.

Similar error occurs when executing pip freeze > requirements.txt, since the line -> en-core-web-sm==2.0.0 appears in the list and app cannot be deployed:
remote: Collecting en-core-web-sm==1.6.0 (from -r requirements.txt (line 136))        
remote:   Could not find a version that satisfies the requirement en-core-web-sm==1.6.0 (from -r requirements.txt (line 136)) (from versions: )        
remote: No matching distribution found for en-core-web-sm==1.6.0 (from -r requirements.txt (line 136))
      
```bash
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
## Info about spaCy
* **spaCy version:** 2.0.16
* **Platform:** Windows-10-10.0.17763-SP0
* **Python version:** 3.7.1
* **Models:** en

[en_core_web_sm_IssueAzureAppServLog.txt](https://github.com/explosion/spaCy/files/2537281/en_core_web_sm_IssueAzureAppServLog.txt)
",2
300,https://github.com/explosion/spaCy/issues/3226,3226,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446637424, 'node_id': 'MDU6TGFiZWw0NDY2Mzc0MjQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/conda', 'name': 'conda', 'color': '676F6D', 'default': False, 'description': 'conda package manager'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}]",closed,2019-02-04 05:04:45+00:00,8,Unable to install spacy-en_core_web_sm with Conda,"<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

## How to reproduce the problem
<!-- Include the details of how the problem occurred. Which command did you run to install spaCy? Did you come across an error? What else did you try? -->

I did a conda install of Spacy, but I'm unable to install this package:
spacy-en_core_web_sm

The instructions on anaconda.org don't work. I'd like to use the package in Jupyter Notebooks which I'm running from Anaconda.

Command:

$ conda install -c danielfrg spacy-en_core_web_sm

```bash
Error:

Solving environment: failed

PackagesNotFoundError: The following packages are not available from current channels:

  - spacy-en_core_web_sm

Current channels:

  - https://conda.anaconda.org/danielfrg/osx-64
  - https://conda.anaconda.org/danielfrg/noarch
  - https://repo.anaconda.com/pkgs/main/osx-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/free/osx-64
  - https://repo.anaconda.com/pkgs/free/noarch
  - https://repo.anaconda.com/pkgs/r/osx-64
  - https://repo.anaconda.com/pkgs/r/noarch
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: macOS Mojave
* Python Version Used: 3 
* spaCy Version Used:
* Environment Information: Anaconda
",2
684,https://github.com/explosion/spaCy/issues/3777,3777,[],closed,2019-05-26 02:52:02+00:00,2,spaCy download command fails to install en_core_web_sm in conda environment,"<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

## How to reproduce the problem
<!-- Include the details of how the problem occurred. Which command did you run to install spaCy? Did you come across an error? What else did you try? -->
I created a conda environment with spaCy using the command:

>conda create -n fever spacy

This successfully created a conda environment with spaCy version 2.0.16.

I then used the standard spaCy download command:

>python -m spacy download en_core_web_sm

The model seems to download succesfully, but I get an an EnvironmentError message.


```bash
ERROR: Error [WinError 87] The parameter is incorrect while executing command python setup.py egg_info
ERROR: Could not install packages due to an EnvironmentError: [WinError 87] The parameter is incorrect
```



## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
## Info about spaCy

* **spaCy version:** 2.0.16
* **Platform:** Windows-10-10.0.18362-SP0
* **Python version:** 3.7.3
",2
709,https://github.com/explosion/spaCy/issues/3816,3816,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}, {'id': 1280480835, 'node_id': 'MDU6TGFiZWwxMjgwNDgwODM1', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/upgrade', 'name': 'upgrade', 'color': 'B35905', 'default': False, 'description': 'Issues related to upgrading spaCy'}]",closed,2019-06-03 15:13:44+00:00,3,en_core_web_sm Installation Issue,"Hi,

I am facing an issue with the en_core_web_Sm installation. I had spacy running before with 2.0.12 version with this library. However, to get access to more functionality I updated the spacy library and then faced issues with respect to reinstalling en_core_Web_sm language package. Now I am getting errors like 'No module named spacy.tokens' while doing python -m spacy download en_core_web_sm. I have faced this kind of error before too while installing or updating the library. Even downgrading is not helping. 

Please let me know what should be the steps
* Operating System: Windows 10
* Python Version Used: 3.6.7
* spaCy Version Used: 2.1.4
* Environment Information: virtual as well as local. Also trying to install using conda
",2
1183,https://github.com/explosion/spaCy/issues/4577,4577,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 1174487963, 'node_id': 'MDU6TGFiZWwxMTc0NDg3OTYz', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/jupyter', 'name': 'jupyter', 'color': '676F6D', 'default': False, 'description': 'Issues related to Jupyter notebook environments'}]",closed,2019-11-02 14:44:38+00:00,7,"OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","I have installed `spacy` and downloaded `en_core_web_sm` with:
```
pip3 install spacy
python3 -m spacy download en_core_web_sm
```

When running codes on Python3 default IDLE, it runs successfully:
```
import spacy
spacy.load(""en_core_web_sm"")
```

However, when I run above codes in **jupyter notebook**, it shows error:
```
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```

I tried several ways in `Jupyter notebook` like
```
!python3 -m spacy download en_core_web_sm
```
but it still shows the error. 

OS: MacOS

Could somebody help me fix this issue? Thanks in advance!

",2
1297,https://github.com/explosion/spaCy/issues/4756,4756,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2019-12-03 17:46:58+00:00,4,"OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","I have installed spacy and downloaded en_core_web_sm with:
```
pip3 install spacy
python3 -m spacy download en_core_web_sm
```
When I try to run the en_core_web_sm module in Python IDE with:
```
import spacy
nlp = spacy.load(""en_core_web_sm"")
```
it shows an error message:

```
 File ""/home/disha/Desktop/project/python/parser/venv/lib/python3.6/site-packages/spacy/__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""/home/disha/Desktop/project/python/parser/venv/lib/python3.6/site-packages/spacy/util.py"", line 169, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```
I have tried every possible solution which was suggested on GitHub and Stack Overflow but nothing worked

* Operating System: Ubuntu
* Python Version Used: 3.6
* spaCy Version Used: 2.2.3

Can somebody help me fix this issue? Thanks in advance!

",2
1399,https://github.com/explosion/spaCy/issues/4915,4915,[],closed,2020-01-15 15:40:42+00:00,2,"OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","I've gone through the troubleshooting steps in previous issues (like #4756 and #4577), but have been unable to resolve the error. 

I'm attempting to import and use version 2.2.3, which seems to be installed. Meaning, I installed it and previously everything worked fine. I haven't made any changes to my machine, just started a new script. and attempted to use:

```
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English
parser = English()
import en_core_web_sm
nlp = spacy.load(""en_core_web_sm"")
```


which yields


```bash
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 169, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```



## Your Environment
* **spaCy version:** 2.2.3
* **Platform:** Darwin-18.7.0-x86_64-i386-64bit
* **Python version:** 3.7.3
Mac OS

I've also tried the following:


```
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English
parser = English()
from spacy.lang.en import en_core_web_sm
```


Which yields: 


```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'en_core_web_sm' from 'spacy.lang.en' (/usr/local/lib/python3.7/site-packages/spacy/lang/en/__init__.py)
>>> nlp = spacy.load(""en_core_web_sm"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 169, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```


If I run each line separately, the errors I get are:
```
>>> from spacy.lang.en import en_core_web_sm
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'en_core_web_sm' from 'spacy.lang.en' (/usr/local/lib/python3.7/site-packages/spacy/lang/en/__init__.py)
>>> nlp = spacy.load(""en_core_web_sm"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 169, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```",2
1566,https://github.com/explosion/spaCy/issues/5260,5260,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2020-04-06 08:53:02+00:00,14,Cannot download models (like en_core_web_sm),"## Internet connection is good but cannot download
I am in Mongolia and cannot download any of the models from command line using:
```
python -m spacy download en_core_web_sm
```
and error follows:
```
Collecting en_core_web_sm==2.2.5
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0MB)
ERROR: Exception:
Traceback (most recent call last):
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py"", line 425, in _error_catcher
    yield
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py"", line 507, in read
    data = self._fp.read(amt) if not fp_closed else b""""
  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 457, in read
    n = self.readinto(b)
  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 501, in readinto
    n = self.fp.readinto(b)
  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socket.py"", line 589, in readinto
    return self._sock.recv_into(b)
  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py"", line 1071, in recv_into
    return self.read(nbytes, buffer)
  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py"", line 929, in read
    return self._sslobj.read(len, buffer)
socket.timeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/cli/base_command.py"", line 153, in _main
    status = self.run(options, args)
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/commands/install.py"", line 382, in run
    resolver.resolve(requirement_set)
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py"", line 201, in resolve
    self._resolve_one(requirement_set, req)
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py"", line 365, in _resolve_one
    abstract_dist = self._get_abstract_dist_for(req_to_install)
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py"", line 313, in _get_abstract_dist_for
    req, self.session, self.finder, self.require_hashes
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/operations/prepare.py"", line 194, in prepare_linked_requirement
    progress_bar=self.progress_bar
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/download.py"", line 465, in unpack_url
    progress_bar=progress_bar
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/download.py"", line 316, in unpack_http_url
    progress_bar)
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/download.py"", line 551, in _download_http_url
    _download_url(resp, link, content_file, hashes, progress_bar)
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/download.py"", line 255, in _download_url
    consume(downloaded_chunks)
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/utils/misc.py"", line 641, in consume
    deque(iterator, maxlen=0)
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/download.py"", line 223, in written_chunks
    for chunk in chunks:
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/utils/ui.py"", line 160, in iter
    for x in it:
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_internal/download.py"", line 212, in resp_read
    decode_content=False):
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py"", line 564, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py"", line 529, in read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/Users/bayartsogtyadamsuren/DDAM-Projects/isid/myenv/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py"", line 430, in _error_catcher
    raise ReadTimeoutError(self._pool, None, ""Read timed out."")
pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='github-production-release-asset-2e65be.s3.amazonaws.com', port=443): Read timed out.
```

and it is same using browser  https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-2.2.5

I checked my internet connection and other stuff working well. So I thought it is not because of my machine.

There are other closed issues similar to this one but none of them actually solved in terms of internet connection error.

How can I fix this?

## My Environment
```
* Operating System: MacOS
* Python Version Used: 3.7.4
* spaCy Version Used: 2.2.4
* Environment Information: pip virtualenv
```

**[update]** I also tried it in other environments but no difference:
Tried a new machine with:
``` 
* Operating System: Ubuntu 16.04.6 LTS
* Python Version Used: 3.6.9
* spaCy Version Used: 2.2.4
* Environment Information: pip virtualenv
```

- [x] I tried the whole new environment but still getting the same error
- [x] --time-out addition in command line also did not work
- [x] used different machine but did not work too",2
1605,https://github.com/explosion/spaCy/issues/5327,5327,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2020-04-19 17:22:02+00:00,2,"Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","## How to reproduce the behaviour

nlp = spacy.load(""en_core_web_sm"")

Works with Python 3.6 but not with Python 3.8 

## Your Environment
* Operating System: Darwin 
* Python Version Used: 3.6 / 3.8
* spaCy Version Used:  2.2.4
* Environment Information: None
",2
1759,https://github.com/explosion/spaCy/issues/5713,5713,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2020-07-05 22:07:57+00:00,4,Probelm in installing en_core_web_sm on Heroku,"I am having a problem in importing en_core_web_sm on Heroku, while the code works fine on the local machine it is not able to do so on the Heroku server.

```
import spacy
from spacy import displacy
import en_core_web_sm
from spacy import load

nlp = en_core_web_sm.load() # I am trying to use like this instead of spacy.load('en_core_web_sm')

```
I am installing the en_core_web_sm using URL, requirements.txt:

```
spacy==2.3.0
https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz


```
I have also tried putting link as 

`en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz
`
I have also tried using spacy.load('') but it also does not works.

I am using heroku free tier services.

I have referenced these links #1099 #308 but was unable to solve my problem.
local machine:
* **spaCy version:** 2.3.0
* **Platform:** Linux-4.15.0-106-generic-x86_64-with-Ubuntu-18.04-bionic
* **Python version:** 3.6.9


heroku 
* **spaCy version:** 2.3.0
* **Platform:** Linux-4.4.0-1073-aws-x86_64-with-debian-buster-sid
* **Python version:** 3.6.10


",2
1908,https://github.com/explosion/spaCy/issues/6237,6237,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2020-10-10 18:38:53+00:00,7,"OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","## How to reproduce the behaviour
run https://github.com/WolfgangFahl/ProceedingsTitleParser/blob/master/tests/test_Spacy.py in Jenkins Continuous integration environment on a Ubuntu 20.04 LTS machine


## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
* Python Version Used:
* spaCy Version Used:
* Environment Information:
```
 python3 -m spacy info --markdown

## Info about spaCy

* **spaCy version:** 2.3.2
* **Platform:** Linux-5.4.0-48-generic-x86_64-with-glibc2.29
* **Python version:** 3.8.5
``` bash
lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description: Ubuntu 20.04.1 LTS
Release: 20.04
Codename: focal
```
",2
2298,https://github.com/explosion/spaCy/issues/7379,7379,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2021-03-10 02:45:25+00:00,3,v3.0.4 can't get en_core_web_sm,"## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
``` sh
python -m spacy download en_core_web_sm
```
then I got a message
```
No compatible package v3.0.4 of spacy
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Linux
* Python Version Used: 3.7
* spaCy Version Used: 3.0.4
* Environment Information: root 
",2
2624,https://github.com/explosion/spaCy/issues/8820,8820,[],closed,2021-07-28 05:49:39+00:00,1,OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.,"
![image](https://user-images.githubusercontent.com/49754403/127270438-4caf10cb-b056-43aa-b2f2-d1361e14aab8.png)
",2
94,https://github.com/explosion/spaCy/issues/2910,2910,"[{'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}]",closed,2018-11-08 12:59:24+00:00,4,Virus alert for english model (en_core_web_sm-2.0.0),"## How to reproduce the behaviour

strings.json in 'en_core_web_sm-2.0.0/en_core_web_sm/en_core_web_sm-2.0.0/vocab'  triggers an alert with avast (JS:Downloader-FPP [Trj]).

See also: https://www.virustotal.com/fr/file/21c0157d2d05e3deafe86c936f45874ab612defbaeb59c0c91e0b6940958ffc7/analysis/

I guess some part of the json  (like certains urls e.g. http://www.al-jazirah.com.sa/cars/29112006/rood55.htm"" or words) triggers the antivirus.


## Your Environment

* **Platform:** Windows-10-10.0.17134-SP0
* **spaCy version:** 2.0.16
* **Python version:** 3.5.4
",1
162,https://github.com/explosion/spaCy/issues/3013,3013,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 1025171819, 'node_id': 'MDU6TGFiZWwxMDI1MTcxODE5', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20memory', 'name': 'perf / memory', 'color': 'B35905', 'default': False, 'description': 'Performance: memory use'}]",closed,2018-12-05 23:12:54+00:00,10,Memory leak using en_core_web_sm nlp,"## How to reproduce the behaviour
Using the en_core_web_sm model to tokenize text and we noticed our memory was climbing and led to OOM errors. 

Below is a script that performs the following steps:
1. Loads en_core_web_sm
2. Then runs a subprocess 10 times
3. First, creates a deep copy of the nlp model
4. Then, processes 1000 randomly created words
5. At the end of the subprocess, the deep copy is deleted and garbage is collected
6. Finally, at the end of the file, the nlp model is manually deleted and garbage is collected.

The results show that, despite the efforts to clean up the environment, the memory still rises.

## Environment

### Info about spaCy

* *spaCy version:* 2.0.16
* *Platform:* Darwin-18.0.0-x86_64-i386-64bit
    * Issue has also occurred on Alpine 3.6
* *Python version:* 3.6.5
* *Models:* en_core_web_sm, en

## Code:
```python
from copy import deepcopy
import gc
import os
import psutil
import random
import spacy
import string

num_runs = 10
num_words = 1000


def randomword(length):
   letters = string.ascii_lowercase
   return ''.join(random.choice(letters) for i in range(length))

def get_memory(process):
    return f""{round(process.memory_info()[0] / float(2 ** 20), 6)} MB""

if __name__ == '__main__':
    process = psutil.Process(os.getpid())

    print(""Loading model."")
    print(f""Pre-load memory: {get_memory(process)}"")
    nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])
    print(f""Post-load memory: {get_memory(process)}"")
    print("""")

    for i in range(num_runs):
        print(f""Run {i}."")
        print(f""\tPre-copy memory: {get_memory(process)}"")
        nlp_deep_copy = deepcopy(nlp)
        print(f""\tPost-copy memory: {get_memory(process)}"")

        for j in range(num_words):
            word = randomword(j)
            nlp_deep_copy(word)
        print(f""\tPost-execution memory: {get_memory(process)}"")
        del nlp_deep_copy
        gc.collect()
        print(f""\tPost-copy-delete memory: {get_memory(process)}"")

    print("""")
    print(f""Post-process memory: {get_memory(process)}"")
    del nlp
    gc.collect()
    print(f""Post-delete memory: {get_memory(process)}"")
```

## Results
> Loading model.
> Pre-load memory: 49.285156 MB
> Post-load memory: 157.777344 MB
>
> Run 0.
>  Pre-copy memory: 157.777344 MB
>  Post-copy memory: 246.890625 MB
>  Post-execution memory: 248.726562 MB
>  Post-copy-delete memory: 208.40625 MB
> Run 1.
>  Pre-copy memory: 208.40625 MB
>  Post-copy memory: 254.191406 MB
>  Post-execution memory: 254.535156 MB
>  Post-copy-delete memory: 211.253906 MB
> Run 2.
>  Pre-copy memory: 211.253906 MB
>  Post-copy memory: 257.320312 MB
>  Post-execution memory: 257.628906 MB
>  Post-copy-delete memory: 213.441406 MB
> Run 3.
>  Pre-copy memory: 213.441406 MB
>  Post-copy memory: 258.066406 MB
>  Post-execution memory: 258.070312 MB
>  Post-copy-delete memory: 213.417969 MB
> Run 4.
>  Pre-copy memory: 213.417969 MB
>  Post-copy memory: 258.003906 MB
>  Post-execution memory: 258.011719 MB
>  Post-copy-delete memory: 215.285156 MB
> Run 5.
>  Pre-copy memory: 215.285156 MB
>  Post-copy memory: 257.511719 MB
>  Post-execution memory: 257.523438 MB
>  Post-copy-delete memory: 213.511719 MB
> Run 6.
>  Pre-copy memory: 213.511719 MB
>  Post-copy memory: 257.988281 MB
>  Post-execution memory: 257.988281 MB
>  Post-copy-delete memory: 213.296875 MB
> Run 7.
>  Pre-copy memory: 213.296875 MB
>  Post-copy memory: 257.523438 MB
>  Post-execution memory: 257.523438 MB
>  Post-copy-delete memory: 213.511719 MB
> Run 8.
>  Pre-copy memory: 213.511719 MB
>  Post-copy memory: 258.121094 MB
>  Post-execution memory: 258.121094 MB
>  Post-copy-delete memory: 213.679688 MB
> Run 9.
>  Pre-copy memory: 213.679688 MB
>  Post-copy memory: 258.15625 MB
>  Post-execution memory: 258.15625 MB
>  Post-copy-delete memory: 213.679688 MB
> 
> Post-process memory: 213.679688 MB
> Post-delete memory: 137.910156 MB
",1
571,https://github.com/explosion/spaCy/issues/3632,3632,"[{'id': 542391290, 'node_id': 'MDU6TGFiZWw1NDIzOTEyOTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/meta', 'name': 'meta', 'color': 'f6f6f6', 'default': False, 'description': 'Meta topics, e.g. repo organisation and issue management'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2019-04-25 03:26:50+00:00,7,How is the en_core_web_sm model MIT licensed while it was trained on OntoNotes? ,"https://spacy.io/models/en#en_core_web_sm ([mirror](https://web.archive.org/web/20190425032735/https://spacy.io/models/en/)) indicates that the en_core_web_sm model model is MIT licensed and was trained on OntoNotes. OntoNotes comes from the Linguistic Data Consortium often (LDC) (https://catalog.ldc.upenn.edu/LDC2013T19 ([mirror](https://web.archive.org/web/20190425032749/https://catalog.ldc.upenn.edu/LDC2013T19)) assuming it is OntoNotes 5.0).  

From my understanding, the [LDC User Agreement for Non-Members](https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf) ([mirror](https://web.archive.org/web/20190425032800/https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf)) forbids commercial use. Assuming that Explosion AI is a for-profit member of LDC (~25kUSD/year), does the [LDC For-Profit Membership Agreement](https://catalog.ldc.upenn.edu/license/ldc-for-profit-membership.pdf) ([mirror](https://web.archive.org/web/20190425032811/https://catalog.ldc.upenn.edu/license/ldc-for-profit-membership.pdf)) allows to redistribute models trained on LDC corpora? It's not clear to me when reading the agreement. The most relevant section I could find in the agreement is:

>  Member may incorporate portions of the LDC Databases into its own work products, including for commercial purposes. Unless explicitly permitted here in, Member shall have no right to copy, redistribute, transmit, publish or otherwise use the LDC ??Databases for any other purpose.

I don't know whether redistributing a model trained on an LDC Database count as incorporating the database into one's product (allowed), or redistributing an LDC Database (not allowed). Do you interpret the agreement as redistributing a model trained on an LDC Database = incorporating the database into one's product,  hence it's allowed?

(PS: do not take this question as any form of support for LDC, which I consider to be a horrible organization grossly misusing [public funding](https://linguistics.stackexchange.com/q/14358/2680) ([mirror](https://web.archive.org/web/20190425032824/https://linguistics.stackexchange.com/questions/14358/why-doesnt-the-linguistic-data-consortium-release-its-data-sets-free-of-charge)) by placing speech and NLP datasets behind paywall and with non-commercial licenses).",1
661,https://github.com/explosion/spaCy/issues/3751,3751,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 881666230, 'node_id': 'MDU6TGFiZWw4ODE2NjYyMzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20vectors', 'name': 'feat / vectors', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Word vectors and similarity'}]",closed,2019-05-16 17:15:35+00:00,3,en_core_web_lg and en_core_web_sm appear to have different behaviors,"With *lg, an assign entity in *.oov doc is given an entity type (ORG), but its encoded vector is all zeros.  With *sm an assign entity in *.oov doc is given an entity type (ORG), but its encoded non-zeros.  If it is assigned, it should have a non-zero vector.

```
>>> nlp = spacy.load(""en_core_web_lg"")
>>> doc = nlp(""Algorithms requiring state saving may have additional implements such as do_pause(), do_resume_training(), and do_resume_prediction() asdkfj. "")
>>> for entity in doc.ents:
...    print(entity.text, entity.label_, entity.vector)
...
do_pause ORG [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
do_resume_training ORG [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
>>> nlp = spacy.load(""en_core_web_sm"")
>>> doc = nlp(""Algorithms requiring state saving may have additional implements such as do_pause(), do_resume_training(), and do_resume_prediction() asdkfj. "")
>>> for entity in doc.ents:
...    print(entity.text, entity.label_, entity.vector)
...
do_resume_training ORG [ 2.935705   -1.6344352  -0.7091226  -3.379816   -1.1515144  -1.4324439
  6.5350633   0.43284723  0.14685334  1.7885383  -1.9412032   2.8551323
-4.483956   -2.388044   -0.2656129  -1.7762061   7.849708    1.9202843
-0.21321201  5.149651   -1.0601426  -4.6507716   0.15843004  7.2607074
-4.5476007  -0.5686755   1.6029842  -3.6645074   3.6964858   4.497079
  3.858057    0.81090105  1.7358261  -1.306872    2.6336534   2.4586172
-3.5305743   0.27837503  6.2441683  -0.90567803  2.3897543   2.2444038
-2.8949065  -0.08926445 -0.91708314 -2.4435313  -1.906867    0.2591542
  1.6849548  -2.2797167  -3.5092854  -0.43277776 -3.7786288  -0.7380816
  1.7633438   1.0289862   1.7552676  -2.3559794  -2.361754   -2.4132857
-0.246999    0.75538003 -2.2591748  -0.65486443 -0.75442696  1.3932987
  4.221003   -1.1264579  -3.4884052   0.0267583  -4.1449175  -1.5139333
  3.1901374   3.2743845  -0.11721516 -0.72806126 -0.88657844  0.03259289
  1.1234093  -3.8731272   0.399422   -0.56952226 -4.035115    3.1042933
  2.1220927   1.9670552   1.0191011   1.1674907   0.69118625 -3.2309504
-0.71159846 -1.9799719  -2.4485335   0.8975849  -0.9927953   3.46949   ]
do_resume_prediction ORG [ 3.1021795  -3.021543    0.03051171  2.1726637  -1.1557665   1.4096172
  4.671034    2.1801455  -1.4936099   2.1656415  -1.5628407   2.3968487
-4.524896   -4.319704    0.7500775  -2.8530982   5.9419503   3.0862596
  2.4668055   2.4774098  -1.7857322  -3.559343    0.83501786  3.49148
-4.149811    1.1869466   1.3840894  -4.7484603   0.6460701   4.1284847
  3.539988    2.4491522  -0.23294425 -2.626965    5.514593    2.8935807
-2.2828197   1.492079    1.3388476  -0.588619    1.0403181   1.8740476
  1.0445874  -3.136752    1.7367148  -4.4921927  -3.3859704   1.4699485
  1.927714   -1.4104167  -5.0055737  -2.3230069  -3.725709   -1.0958152
  1.944841    2.2337859   1.0904824  -1.0589422  -0.24801636 -2.9401898
  0.08113813 -0.4420525   0.61556745 -0.0781751  -2.10739     1.8558246
  5.5077906  -5.6011496  -1.73262    -3.8097768  -1.0314898   0.6732393
  3.7871327   1.5742137  -1.2714251   2.785291   -1.9637618  -2.7397377
-1.1148953  -4.278485   -2.3162675   1.2481518  -4.9304085   3.0922399
  1.4344867   1.551682    2.380948   -0.7361007   0.49899593 -0.65800136
  1.9096855  -1.6374016  -0.51255196  0.04158095  1.333637    0.8427284 ]
```",1
724,https://github.com/explosion/spaCy/issues/3834,3834,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2019-06-11 05:24:09+00:00,5,Getting [Errno 24] Too many open files ../en_core_web_sm/meta.json,"Hi,
I am using **spaCy** in my project and It was working very well but after some time I got an error on server. 

 **[Errno 24] Too many open files:** '/home/ubuntu/webapp/venv/lib/python3.6/site-packages/en_core_web_sm/meta.json.

Can anybody suggest me a better solution ?
 ",1
833,https://github.com/explosion/spaCy/issues/3986,3986,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}]",closed,2019-07-18 09:01:21+00:00,2,is_oov and prob does not work for en_core_web_sm model,"## How to reproduce the behaviour
`lex = nlp.vocab[u""dog""]
print(lex.is_oov, lex.prob)`
producing
`True -20.0`

This bug had been reported #1204. 

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: masOS
* Python Version Used: 3.6
* spaCy Version Used: 2.1.4
* Environment Information:
",1
1057,https://github.com/explosion/spaCy/issues/4357,4357,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 710446668, 'node_id': 'MDU6TGFiZWw3MTA0NDY2Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/training', 'name': 'training', 'color': '087EA6', 'default': False, 'description': 'Training and updating models'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2019-10-02 02:44:10+00:00,3,Not Updating the default NER model - en_core_web_sm,"Hi,
The use case I am currently working, required to extract Person Name, Amount, SSN No. Out of that default model extracts Person and Money (I need to change it to as  Amount) and SSN would be my custom entity. So I am preparing the Tagged tsv for SSN and while training the Model I am providing en_core_web_sm model, my understanding is it will keep the default entities and add my custom entity SSN. But in actual, if I load this new model, its nigher extracting default entities not the custom.  
When I train a custom model, by passing None to the model parameter its extracting my custom entities. I am using following function to train the model:

`
def trainModel(self,trainingFile=None,model=None, new_model_name='new_model', output_dir=None, n_iter=10):
  
  try:
   with open (trainingFile, 'rb') as fp:
    TRAIN_DATA = pickle.load(fp)

   """"""
    Setting up the pipeline and entity recognizer, and training the new entity.
   """"""
   if model is not None:
    nlp = spacy.load(model)  # load existing spacy model
    logging.info(""Loaded model '%s'"" % model)
   else:
    nlp = spacy.blank('en')  # create blank Language class
    logging.info(""Created blank 'en' model"")
    reset_weights = False
   if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe('ner')
    nlp.add_pipe(ner)
    reset_weights = True        
   else:
    ner = nlp.get_pipe('ner')

   # Add Custom Labels
   for _, annotations in TRAIN_DATA:
    for ent in annotations.get('entities'):
     ner.add_label(ent[2])

   if model is None or reset_weights:
    optimizer = nlp.begin_training()
   else:
    optimizer = nlp.entity.create_optimizer()

   # Get names of other pipes to disable them during training to train only NER
   other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
   with nlp.disable_pipes(*other_pipes):  # only train NER
    for itn in range(n_iter):
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(texts, annotations, sgd=optimizer, drop=0.35,
                       losses=losses)
        logging.info('Losses: ' + str(losses))

   # Save model 
   if output_dir is not None:
    output_dir = Path(output_dir)
    if not output_dir.exists():
     output_dir.mkdir()
    nlp.meta['name'] = new_model_name  # rename model
    nlp.to_disk(output_dir)
    logging.info(""Saved model to ""  + str(output_dir))
    return nlp
   else:
    return nlp
  except Exception as e:
   logging.exception(""Unable to process "" + ""\n"" + ""Error = "" + str(e))
   return None
 # End trainModel
`
Please suggest.





## Environment
   * spaCy version: 2.1.8
   * Platform: Windows-10-10.0.16299-SP0
   * Python version: 3.7.1
",1
1162,https://github.com/explosion/spaCy/issues/4547,4547,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881666568, 'node_id': 'MDU6TGFiZWw4ODE2NjY1Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20pipeline', 'name': 'feat / pipeline', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Processing pipeline and components'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2019-10-28 19:53:30+00:00,3,Unable to update stop words list in en_core_web_sm,"## How to reproduce the behaviour
Attempting to append the default stop words list, but can't seem to add anything using either

```
more_stops = ['like', 'vet', 'veteran', 'Veterans']
STOP_WORDS.update(more_stops)
```

or 

`nlp.Defaults.stop_words |= {""like"", ""vet"", ""veteran"", ""Veterans""}`

Neither command results in an error message, but when I go on to filter out stop words they remain in the dataset.
",1
1177,https://github.com/explosion/spaCy/issues/4571,4571,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446637424, 'node_id': 'MDU6TGFiZWw0NDY2Mzc0MjQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/conda', 'name': 'conda', 'color': '676F6D', 'default': False, 'description': 'conda package manager'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2019-11-01 21:04:06+00:00,3,"AttributeError (no __reduce_cython__) while loading ""en_core_web_sm""","## How to reproduce the behaviour
```
import spacy
nlp = spacy.load(""en_core_web_sm"")
```
Error:
```
-----------------------------------------------------------------------
AttributeError                        Traceback (most recent call last)
<ipython-input-142-e49198ee6ee6> in <module>
      2 
      3 
----> 4 nlp = spacy.load(""en_core_web_sm"")
      5 
      6 from negspacy.negation import Negex

/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/__init__.py in load(name, **overrides)
     19 if sys.maxunicode == 65535:
     20     raise SystemError(Errors.E130)
---> 21 
     22 
     23 def load(name, **overrides):

/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/util.py in load_model(name, **overrides)
    112     RETURNS: Path or original argument.
    113     """"""
--> 114     if isinstance(path, basestring_):
    115         return Path(path)
    116     else:

/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/util.py in load_model_from_package(name, **overrides)
    133         if is_package(name):  # installed as package
    134             return load_model_from_package(name, **overrides)
--> 135         if Path(name).exists():  # path to model data directory
    136             return load_model_from_path(Path(name), **overrides)
    137     elif hasattr(name, ""exists""):  # Path or Path-like to model data

/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/en_core_web_sm/__init__.py in load(**overrides)
     10 
     11 def load(**overrides):
---> 12     return load_model_from_init_py(__file__, **overrides)

/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/util.py in load_model_from_init_py(init_file, **overrides)
    171         pipeline = nlp.Defaults.pipe_names
    172     elif pipeline in (False, None):
--> 173         pipeline = []
    174     for name in pipeline:
    175         if name not in disable:

/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/util.py in load_model_from_path(model_path, meta, **overrides)
    141 
    142 def load_model_from_link(name, **overrides):
--> 143     """"""Load a model from a shortcut link, or directory in spaCy data path.""""""
    144     path = get_data_path() / name / ""__init__.py""
    145     try:

/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/util.py in get_lang_class(lang)
     48 
     49     lang (unicode): Two-letter language code, e.g. 'en'.
---> 50     RETURNS (bool): Whether a Language class has been loaded.
     51     """"""
     52     global LANGUAGES

/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/__init__.py in import_module(name, package)
    125                 break
    126             level += 1
--> 127     return _bootstrap._gcd_import(name[level:], package, level)
    128 
    129 

/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap.py in _gcd_import(name, package, level)

/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap.py in _find_and_load(name, import_)

/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)

/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap.py in _load_unlocked(spec)

/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap_external.py in exec_module(self, module)

/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/lang/en/__init__.py in <module>
     13 from ..tokenizer_exceptions import BASE_EXCEPTIONS
     14 from ..norm_exceptions import BASE_NORMS
---> 15 from ...language import Language
     16 from ...attrs import LANG, NORM
     17 from ...util import update_exc, add_lookups

/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/language.py in <module>
     12 import srsly
     13 
---> 14 from .tokenizer import Tokenizer
     15 from .vocab import Vocab
     16 from .lemmatizer import Lemmatizer

/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/tokenizer.cpython-37m-darwin.so in init spacy.tokenizer()

AttributeError: type object 'spacy.tokenizer.array' has no attribute '__reduce_cython__'
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Mac 
* conda environment, freshly installed Spacy
* **spaCy version:** 2.1.8
* **Platform:** Darwin-18.7.0-x86_64-i386-64bit
* **Python version:** 3.7.2
",1
1416,https://github.com/explosion/spaCy/issues/4945,4945,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446637295, 'node_id': 'MDU6TGFiZWw0NDY2MzcyOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/osx', 'name': 'osx', 'color': '676F6D', 'default': False, 'description': 'Issues related to macOS / OSX'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2020-01-27 21:52:04+00:00,7,nlp=spacy.load('en_core_web_sm') KeyError: 'PUNCTSIDE_FIN',"Can someone please help?
I installed spacy 2.2.3 using conda install -c conda-forge/label/gcc7 spacy and the en model 2.2.5 using conda install -c conda-forge spacy-model-en_core_web_sm on anaconda3. Installation was successful and the kernel was restarted.

However, when I can import spacy -  nlp=spacy.load('en_core_web_sm'), get the following KeyError: 'PUNCTSIDE_FIN'.
 I read all the blogs on this topic but could not find any fix for my issue. 

operating system - macOS
python 3.7
anaconda3 (anaconda navigator 1..9.7)
spacy 2.2.3
spacy model 2.2.5",1
1610,https://github.com/explosion/spaCy/issues/5337,5337,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2020-04-22 00:14:41+00:00,2,python -m spacy download en_core_web_sm does nothing,"

## How to reproduce the problem
Running pip install spaCy goes smoothly without issues, but trying to download en_core_web_sm does nothing, it doesn't even show a error message, the command line just goes straight to the next command.

```
C:\Users\JGC\Desktop\Trabalhos\Python\autocompletebot>python -m spacy download en_core_web_sm

C:\Users\JGC\Desktop\Trabalhos\Python\autocompletebot>python3 -m spacy download en_core_web_sm

C:\Users\JGC\Desktop\Trabalhos\Python\autocompletebot>
```

I know it isn't installing it because when i try to run the command 'spacy.load(""en_core_web_sm"")' on python, even from the command line,  i get the following error message:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\JGC\AppData\Local\Programs\Python\Python37\lib\site-packages\spacy\__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""C:\Users\JGC\AppData\Local\Programs\Python\Python37\lib\site-packages\spacy\util.py"", line 169, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```

## Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Windows 10:
* Python 3.7.0b5
* spaCy 2.2.4:
",1
1618,https://github.com/explosion/spaCy/issues/5353,5353,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 881666463, 'node_id': 'MDU6TGFiZWw4ODE2NjY0NjM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20serialize', 'name': 'feat / serialize', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Serialization, saving and loading'}]",closed,2020-04-26 14:04:12+00:00,7,Potential bug: Pickling Error when reloading spacy en_core_web_sm ,"Hi, 

I have recently upgraded my spaCy from v2.1.6 to v2.2.4. When I tried to run sample codes for the first time using Spyder 3.7.3, the sample codes were executed successfully. When I re-ran the same sample codes, an exception is raised:

Reloaded modules: pkg_resources, pkg_resources.extern, pkg_resources._vendor, pkg_resources.extern.six, pkg_resources._vendor.six, pkg_resources.py31compat, pkg_resources.extern.appdirs, pkg_resources._vendor.packaging.__about__, pkg_resources.extern.packaging, pkg_resources.extern.packaging.version, pkg_resources.extern.packaging._structures, pkg_resources.extern.packaging.specifiers, pkg_resources.extern.packaging._compat, pkg_resources.extern.packaging.requirements, pkg_resources.extern.pyparsing, pkg_resources.extern.packaging.markers, pkg_resources.py2_warn, thinc, thinc.about, thinc._registry, catalogue, importlib_metadata, zipp, importlib_metadata._compat, thinc.neural, thinc.neural._classes, thinc.neural._classes.model, srsly, srsly._json_api, srsly.ujson, srsly.ujson.ujson, srsly.util, srsly._msgpack_api, srsly.msgpack, srsly.msgpack._version, srsly.msgpack.exceptions, srsly.msgpack._packer, srsly.msgpack._ext_type, srsly.msgpack.util, srsly.msgpack._unpacker, srsly.msgpack._msgpack_numpy, srsly._pickle_api, srsly.cloudpickle, srsly.cloudpickle.cloudpickle, thinc.neural.util, thinc.neural.train, thinc.neural.optimizers, thinc.neural.ops, thinc.linalg, blis, blis.cy, blis.py, thinc.neural._custom_kernels, thinc.neural._aligned_alloc, thinc.compat, thinc.neural.mem, thinc.check, thinc.extra, thinc.extra.wrapt, thinc.extra.wrapt.wrappers, thinc.extra.wrapt._wrappers, thinc.extra.wrapt.decorators, thinc.extra.wrapt.importer, thinc.exceptions, wasabi, wasabi.printer, wasabi.tables, wasabi.util, wasabi.traceback, thinc.v2v, thinc.neural._classes.affine, thinc.describe, thinc.neural._classes.relu, thinc.neural._classes.maxout, thinc.neural._classes.softmax, thinc.neural._classes.selu, thinc.neural._classes.mish, thinc.t2t, thinc.neural._classes.convolution, thinc.neural._classes.attention, thinc.neural._classes.rnn, thinc.api, thinc.neural._classes.function_layer, thinc.neural._classes.feed_forward, thinc.neural._lsuv, thinc.neural._classes.multiheaded_attention, thinc.t2v, thinc.neural.pooling, thinc.i2v, thinc.neural._classes.hash_embed, thinc.neural._classes.embed, thinc.neural._classes.static_vectors, thinc.extra.load_nlp, thinc.misc, thinc.neural._classes.batchnorm, thinc.neural._classes.layernorm, thinc.neural._classes.resnet, thinc.neural._classes.feature_extracter, thinc.linear, thinc.linear.linear, thinc.extra.search, thinc.neural._classes.difference, OpenSSL, OpenSSL.crypto, OpenSSL._util, OpenSSL.SSL, OpenSSL.version, thinc.extra.datasets, thinc.extra._vendorized, thinc.extra._vendorized.keras_data_utils, thinc.extra._vendorized.keras_generic_utils
Traceback (most recent call last):

  File ""<ipython-input-5-26155a2c6d0a>"", line 1, in <module>
    runfile('C:/Users/C_YEE/Dropbox/ESG Research/Energy Extractives/MarketLine/MarketLine.py', wdir='C:/Users/C_YEE/Dropbox/ESG Research/Energy Extractives/MarketLine')

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 786, in runfile
    execfile(filename, namespace)

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/C_YEE/Dropbox/ESG Research/Energy Extractives/MarketLine/MarketLine.py"", line 585, in <module>
    nlp = spacy.load('en_core_web_sm')

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\__init__.py"", line 30, in load
    return util.load_model(name, **overrides)

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\util.py"", line 164, in load_model
    return load_model_from_package(name, **overrides)

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\util.py"", line 185, in load_model_from_package
    return cls.load(**overrides)

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\en_core_web_sm\__init__.py"", line 12, in load
    return load_model_from_init_py(__file__, **overrides)

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\util.py"", line 228, in load_model_from_init_py
    return load_model_from_path(data_path, meta, **overrides)

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\util.py"", line 211, in load_model_from_path
    return nlp.from_disk(model_path, exclude=disable)

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\language.py"", line 947, in from_disk
    util.from_disk(path, deserializers, exclude)

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\util.py"", line 654, in from_disk
    reader(path / key)

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\language.py"", line 942, in <lambda>
    p, exclude=[""vocab""]

  File ""pipes.pyx"", line 661, in spacy.pipeline.pipes.Tagger.from_disk

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\util.py"", line 654, in from_disk
    reader(path / key)

  File ""pipes.pyx"", line 640, in spacy.pipeline.pipes.Tagger.from_disk.load_model

  File ""pipes.pyx"", line 548, in spacy.pipeline.pipes.Tagger.Model

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\_ml.py"", line 585, in build_tagger_model
    pretrained_vectors=pretrained_vectors,

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\_ml.py"", line 322, in Tok2Vec
    return _legacy_tok2vec.Tok2Vec(width, embed_size, **kwargs)

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\site-packages\spacy\ml\_legacy_tok2vec.py"", line 84, in Tok2Vec
    embed >> convolution ** conv_depth, pad=conv_depth

  File ""C:\Users\C_YEE\AppData\Roaming\Python\Python37\site-packages\thinc\check.py"", line 136, in checker
    return wrapped(*args, **kwargs)

  File ""C:\Users\C_YEE\AppData\Roaming\Python\Python37\site-packages\thinc\neural\_classes\model.py"", line 305, in __pow__
    return self._thread_local.operators[""**""](self, other)

  File ""C:\Users\C_YEE\AppData\Roaming\Python\Python37\site-packages\thinc\api.py"", line 148, in clone
    layers.append(copy.deepcopy(orig))

  File ""C:\Users\C_YEE\AppData\Local\Continuum\anaconda3\lib\copy.py"", line 169, in deepcopy
    rv = reductor(4)

  File ""C:\Users\C_YEE\AppData\Roaming\Python\Python37\site-packages\thinc\neural\_classes\model.py"", line 98, in __getstate__
    return srsly.pickle_dumps(self.__dict__)

  File ""C:\Users\C_YEE\AppData\Roaming\Python\Python37\site-packages\srsly\_pickle_api.py"", line 14, in pickle_dumps
    return cloudpickle.dumps(data, protocol=protocol)

  File ""C:\Users\C_YEE\AppData\Roaming\Python\Python37\site-packages\srsly\cloudpickle\cloudpickle.py"", line 1125, in dumps
    cp.dump(obj)

  File ""C:\Users\C_YEE\AppData\Roaming\Python\Python37\site-packages\srsly\cloudpickle\cloudpickle.py"", line 486, in dump
    raise pickle.PicklingError(msg)

PicklingError: Could not pickle object as excessively deep recursion required.

Please assist.",1
1645,https://github.com/explosion/spaCy/issues/5409,5409,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2020-05-06 10:53:01+00:00,1,en_core_web_sm has been built successfully in conda but facing issue with __init__.py file,"I have installed and built successfully en_web_core_sm but when i run & use code then getting issue. 
I used this code  for import. ** from spacy import en_core_web_sm** then got below error

**ImportError: cannot import name 'en_core_web_sm' from 'spacy' ( ~\Anaconda\lib\site-packages\spacy\__init__.py)**. 

Earlier when i installed couple of packages then after getting the same issue with __init__.py file. Could you please help me how to resolve this error. 


* Python Version Used:  3.7.6
* spaCy Version Used:    2.2.3
* Environment Information: Anaconda
",1
1649,https://github.com/explosion/spaCy/issues/5416,5416,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2020-05-07 22:45:41+00:00,2,en_core_web_sm is Down,"## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

Both of the following yield Error 403
```bash
python -m spacy download en_core_web_sm
wget https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Ubuntu 18.04 on three machines
* Python Version Used: 3.6
* spaCy Version Used: 2.2
* Environment Information: 
My project can be found here https://github.com/kevinlu1248/researchy-api, if that helps
",1
1839,https://github.com/explosion/spaCy/issues/5947,5947,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 2103359118, 'node_id': 'MDU6TGFiZWwyMTAzMzU5MTE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/resolved', 'name': 'resolved', 'color': 'f6f6f6', 'default': False, 'description': 'The issue was addressed / answered'}]",closed,2020-08-21 16:04:55+00:00,2,"Hi , Do we have any installation link for model en_core_web_sm for  s3 ?","<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

## How to reproduce the problem
<!-- Include the details of how the problem occurred. Which command did you run to install spaCy? Did you come across an error? What else did you try? -->

```bash
# copy-paste the error message here
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
* Python Version Used:
* spaCy Version Used:
* Environment Information:
",1
2033,https://github.com/explosion/spaCy/issues/6562,6562,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2020-12-14 03:20:16+00:00,1,`ConnectionRefusedError: [Errno 61] Connection refused` downloading en_core_web_sm,"Hello,

I ran into a `ConnectionRefusedError: [Errno 61] Connection refused` when downloading `en_core_web_sm`, any help with this would be appreciated, here is the full error message:
```
Traceback (most recent call last):
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/urllib3/connection.py"", line 157, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/urllib3/util/connection.py"", line 84, in create_connection
    raise err
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/urllib3/util/connection.py"", line 74, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 672, in urlopen
    chunked=chunked,
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 376, in _make_request
    self._validate_conn(conn)
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 994, in _validate_conn
    conn.connect()
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/urllib3/connection.py"", line 334, in connect
    conn = self._new_conn()
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/urllib3/connection.py"", line 169, in _new_conn
    self, ""Failed to establish a new connection: %s"" % e
urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x11a2e90f0>: Failed to establish a new connection: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send
    timeout=timeout
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 720, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/urllib3/util/retry.py"", line 436, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x11a2e90f0>: Failed to establish a new connection: [Errno 61] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/xujinghua/miniconda3/lib/python3.7/runpy.py"", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None, ""__main__"", mod_spec)
  File ""/Users/xujinghua/miniconda3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/spacy/__main__.py"", line 33, in <module>
    plac.call(commands[command], sys.argv[1:])
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/plac_core.py"", line 367, in call
    cmd, result = parser.consume(arglist)
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/plac_core.py"", line 232, in consume
    return cmd, self.func(*(args + varargs + extraopts), **kwargs)
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/spacy/cli/download.py"", line 44, in download
    shortcuts = get_json(about.__shortcuts__, ""available shortcuts"")
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/spacy/cli/download.py"", line 95, in get_json
    r = requests.get(url)
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/requests/api.py"", line 75, in get
    return request('get', url, params=params, **kwargs)
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/requests/api.py"", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/requests/sessions.py"", line 533, in request
    resp = self.send(prep, **send_kwargs)
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/requests/sessions.py"", line 646, in send
    r = adapter.send(request, **kwargs)
  File ""/Users/xujinghua/spc/lib/python3.7/site-packages/requests/adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x11a2e90f0>: Failed to establish a new connection: [Errno 61] Connection refused'))
```

Thanks,
JX",1
2055,https://github.com/explosion/spaCy/issues/6659,6659,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 514165920, 'node_id': 'MDU6TGFiZWw1MTQxNjU5MjA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20en', 'name': 'lang / en', 'color': '726DA8', 'default': False, 'description': 'English language data and models'}]",closed,2021-01-01 03:31:09+00:00,6,Where is en_core_web_sm of Python2 for spacy in Python3?,"I need to repeat an experiment. The experiment was conducted in python 2.7 and spacy 1.8.2. The snippet I need to repeat is:
```
import en_core_web_sm

nlp = en_core_web_sm.load()
len(list(nlp.vocab.strings))
```
The output from above is 776980.

Now I would like to repeat the experiment in python 3.7 and spacy 2.3. Where can I find the same size vocabulary?",1
2156,https://github.com/explosion/spaCy/issues/6950,6950,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 881666463, 'node_id': 'MDU6TGFiZWw4ODE2NjY0NjM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20serialize', 'name': 'feat / serialize', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Serialization, saving and loading'}]",closed,2021-02-05 17:38:03+00:00,2,Cannot pickle en_core_web_sm object in spacy==3.0,"## How to reproduce the behaviour

```python
import spacy
import pickle

en = spacy.load(""en_core_web_sm"")
# Error only happens *after* the `English` object is called
en(""hello world"")

with open(""temp.pkl"", ""wb"") as f:
    pickle.dump(en, f)

# AttributeError: Can't pickle local object 'Tok2Vec.predict.<locals>.<lambda>'
```

## Your Environment

- **spaCy version:** 3.0.1
- **Platform:** Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic
- **Python version:** 3.6.9
- **Pipelines:** en_core_web_sm (3.0.0)

Here is a [colab notebook](https://colab.research.google.com/drive/1flGS_X5D4uDsCUEa12U0IauE1N9_UNty) that reproduces the behavior.",1
2303,https://github.com/explosion/spaCy/issues/7384,7384,[],closed,2021-03-10 07:23:08+00:00,1,"Training new entities to pre-trained ""en_core_web_sm/lg"" NER model, model not identifying entities that comes with default model.","    nlp = spacy.load(""en_core_web_sm"")   
    ner = nlp.get_pipe(""ner"")
   
    for text, annotations in TRAIN_DATA:
        for ent in annotations.get(""entities""):
            # print(text[ent[0]:ent[1]], ent[2])
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    pipe_exceptions = [""ner"", ""trf_wordpiecer"", ""trf_tok2vec""]
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]

    with nlp.disable_pipes(*other_pipes):  # only train NER
        examples = []
        optimizer = nlp.resume_training()
        for text, annots in TRAIN_DATA:
            examples.append(Example.from_dict(nlp.make_doc(text), annots))
        for itn in range(n_iter):
            random.shuffle(examples)
            losses = {}
            for batch in minibatch(examples, size=compounding(4.0, 32.0, 1.001)):
                nlp.update(batch, sgd=optimizer, losses=losses)
            print('losses : ', losses)

I want  a model which recognizes pre-defined entities that comes with model and new entities that I trained.
Please suggest me

Thanks ",1
2363,https://github.com/explosion/spaCy/issues/7646,7646,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446637295, 'node_id': 'MDU6TGFiZWw0NDY2MzcyOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/osx', 'name': 'osx', 'color': '676F6D', 'default': False, 'description': 'Issues related to macOS / OSX'}]",closed,2021-04-03 04:51:04+00:00,5,[download en_core_web_sm] => ImportError dlopen  no suitable image found,"

## Spacy Language Installation Error
1. Create new python venv : `python3 -m venv ./venv`
2. Activate new python venv : `source ./venv/bin/activate`
3. Update pip, setuptools, wheel : `pip install -U pip setuptools wheel`
4. Install spacy [Latest] : `pip install -U spacy`
5. Install spacy language pack : `python -m spacy download en_core_web_sm`

## Error:
<img width=""1918"" alt=""error"" src=""https://user-images.githubusercontent.com/59952787/113468683-5266ab00-9465-11eb-88f7-0a9d3c7a2f69.png"">

## What I tried:
Following this [stackoverflow link](https://stackoverflow.com/questions/47858150/scipy-importerror-dlopen-no-suitable-image-found-in-python-3) I deleted whole venv folder, created a new one & then tried installing spacy with `python -m pip install spacy` Then tried installing language pack `python -m spacy download en_core_web_sm`

I also tried `python -m spacy.en.download all` that gave me same error.
<img width=""1920"" alt=""Screenshot 2021-04-03 at 10 18 42 AM"" src=""https://user-images.githubusercontent.com/59952787/113468788-fb150a80-9465-11eb-96ad-f203e6c6a33d.png"">


## Your Environment
Python Version : 3.7.9
OS : Mac Os Catilina v10.15.7
pip list in venv output:
```shell
Package             Version
------------------- ---------
appnope             0.1.2
argon2-cffi         20.1.0
async-generator     1.10
attrs               20.3.0
backcall            0.2.0
bleach              3.3.0
blis                0.7.4
catalogue           2.0.1
certifi             2020.12.5
cffi                1.14.5
chardet             4.0.0
click               7.1.2
cymem               2.0.5
decorator           5.0.3
defusedxml          0.7.1
entrypoints         0.3
idna                2.10
importlib-metadata  3.10.0
ipykernel           5.5.3
ipython             7.22.0
ipython-genutils    0.2.0
jedi                0.18.0
Jinja2              2.11.3
jsonschema          3.2.0
jupyter-client      6.1.12
jupyter-core        4.7.1
jupyterlab-pygments 0.1.2
MarkupSafe          1.1.1
mistune             0.8.4
murmurhash          1.0.5
nbclient            0.5.3
nbconvert           6.0.7
nbformat            5.1.3
nest-asyncio        1.5.1
notebook            6.3.0
numpy               1.20.2
packaging           20.9
pandocfilters       1.4.3
parso               0.8.2
pathy               0.4.0
pexpect             4.8.0
pickleshare         0.7.5
pip                 21.0.1
preshed             3.0.5
prometheus-client   0.10.0
prompt-toolkit      3.0.18
ptyprocess          0.7.0
pycparser           2.20
pydantic            1.7.3
Pygments            2.8.1
pyparsing           2.4.7
pyrsistent          0.17.3
python-dateutil     2.8.1
pyzmq               22.0.3
requests            2.25.1
Send2Trash          1.5.0
setuptools          54.2.0
six                 1.15.0
smart-open          3.0.0
spacy               3.0.5
spacy-legacy        3.0.2
srsly               2.4.0
terminado           0.9.4
testpath            0.4.4
thinc               8.0.2
tornado             6.1
tqdm                4.59.0
traitlets           5.0.5
typer               0.3.2
typing-extensions   3.7.4.3
urllib3             1.26.4
wasabi              0.8.2
wcwidth             0.2.5
webencodings        0.5.1
wheel               0.36.2
zipp                3.4.1
```",1
100,https://github.com/explosion/spaCy/issues/2917,2917,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2018-11-09 18:40:30+00:00,3,FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_lg/en_core_web_lg-2.0.0/vocab/strings.json',"## How to reproduce the behaviour
```
import spacy
nlp = spacy.load('en_core_web_lg')
```

```
Traceback (most recent call last):
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1664, in <module>
    main()
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1658, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1068, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/diego/Github/SummarizationConcepts/kmeans_word2vec_preprocessed.py"", line 21, in <module>
    nlp = spacy.load('en_core_web_lg')
  File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 21, in load
    return util.load_model(name, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 112, in load_model
    return load_model_from_link(name, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 129, in load_model_from_link
    return cls.load(**overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_lg/__init__.py"", line 12, in load
    return load_model_from_init_py(__file__, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 173, in load_model_from_init_py
    return load_model_from_path(data_path, meta, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 156, in load_model_from_path
    return nlp.from_disk(model_path)
  File ""/usr/local/lib/python3.7/site-packages/spacy/language.py"", line 647, in from_disk
    util.from_disk(path, deserializers, exclude)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 511, in from_disk
    reader(path / key)
  File ""/usr/local/lib/python3.7/site-packages/spacy/language.py"", line 635, in <lambda>
    self.vocab.from_disk(p) and _fix_pretrained_vectors_name(self))),
  File ""vocab.pyx"", line 376, in spacy.vocab.Vocab.from_disk
  File ""strings.pyx"", line 212, in spacy.strings.StringStore.from_disk
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/pathlib.py"", line 1165, in open
    opener=self._opener)
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/pathlib.py"", line 1019, in _opener
    return self._accessor.open(self, flags, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_lg/en_core_web_lg-2.0.0/vocab/strings.json'
```

Hi guys,

This was working 2 weeks ago and it doesn't anymore I don't know why. I tried re-installing spacy (v2.0.16) and the model with ```pip3.7 install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz```

Here is my folder:
```
{18-11-09 19:38}ibm:/usr/local/lib/python3.7/site-packages/spacy/data diego% tree en_core_web_lg 
en_core_web_lg
笏懌楳笏? __init__.py
笏懌楳笏? __pycache__
笏つ??? 笏披楳笏? __init__.cpython-37.pyc
笏懌楳笏? en_core_web_lg-2.0.0
笏つ??? 笏懌楳笏? accuracy.json
笏つ??? 笏懌楳笏? meta.json
笏つ??? 笏懌楳笏? ner
笏つ??? 笏つ??? 笏懌楳笏? cfg
笏つ??? 笏つ??? 笏懌楳笏? lower_model
笏つ??? 笏つ??? 笏懌楳笏? moves
笏つ??? 笏つ??? 笏懌楳笏? tok2vec_model
笏つ??? 笏つ??? 笏披楳笏? upper_model
笏つ??? 笏懌楳笏? parser
笏つ??? 笏つ??? 笏懌楳笏? cfg
笏つ??? 笏つ??? 笏懌楳笏? lower_model
笏つ??? 笏つ??? 笏懌楳笏? moves
笏つ??? 笏つ??? 笏懌楳笏? tok2vec_model
笏つ??? 笏つ??? 笏披楳笏? upper_model
笏つ??? 笏懌楳笏? tagger
笏つ??? 笏つ??? 笏懌楳笏? cfg
笏つ??? 笏つ??? 笏懌楳笏? model
笏つ??? 笏つ??? 笏披楳笏? tag_map
笏つ??? 笏懌楳笏? tokenizer
笏つ??? 笏披楳笏? vocab
笏つ???     笏懌楳笏? key2row
笏つ???     笏懌楳笏? lexemes.bin
笏つ???     笏披楳笏? vectors
笏披楳笏? meta.json

6 directories, 22 files
```

Do you have any idea why it doesn't work ?

Thanks 
## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Mac OSX 10.14.1
* Python Version Used: 3.7
* spaCy Version Used: 2.0.16 (also happened with 2.0.12)
* Environment Information:
",1
516,https://github.com/explosion/spaCy/issues/3552,3552,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 881666230, 'node_id': 'MDU6TGFiZWw4ODE2NjYyMzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20vectors', 'name': 'feat / vectors', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Word vectors and similarity'}, {'id': 1594093126, 'node_id': 'MDU6TGFiZWwxNTk0MDkzMTI2', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/scaling', 'name': 'scaling', 'color': 'B35905', 'default': False, 'description': 'Scaling, serving and parallelizing spaCy'}]",closed,2019-04-08 10:37:45+00:00,6,Multiprocessing spaCy: Can't find model 'en_model.vectors' in en_core_web_lg,"I am trying to process multiple files in parallel (a single NLP instantiation) and do sentence segmentation on them. Every process reads a file, and every line in that file is a JSON string. The JSON contains a `text` field, which I want to segment. 

This seems to work fine with small and medium models, but for the large spaCy model I get the error 

> OSError: [E050] Can't find model 'en_model.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.

Looking at the [English models](https://spacy.io/models/en), I'd assume that the small model doesn't have vectors - not the large model. On top of that, I'm not sure why vectors are required for this operation. So what is the problem?

I have wondered whether this is actually a memory issue, but running this with 3 threads and 16Gb of RAM, I don't think that should be an issue: even if the model is loaded three times, the memory should be able to hold that.

Finally, if there is a faster way to do *only* sentence segmentation that's better than the following, please do let me know. I'm also not sure whether having only one nlp instance is good practice in a multiprocessing context. Should it be copied?

```python
docs = list(self.nlp.pipe(lines))
sents = [sent for doc in docs for sent in doc.sents]
```

## How to reproduce the behaviour

1. Clone [this](https://github.com/BramVanroy/wiki-processing-debug) repo. 
2. Install spaCy and the small and large model
3. Run `python debug.py data/raw/ data/articles/ -s en_core_web_lg`

That should fail. Use the `en_core_web_sm` model, and it should run just fine.

## Info about spaCy

* **spaCy version:** 2.1.3
* **Platform:** Windows-10-10.0.17134-SP0
* **Python version:** 3.7.3",1
77,https://github.com/explosion/spaCy/issues/2888,2888,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2018-10-30 16:20:36+00:00,2,Connection error on trying to install Spacy English language models,"Hi,

I am trying to install spacy language models and the env details are as below. As suggested here, https://github.com/explosion/spaCy/issues/907, I tried to install it using the command:
`python -m spacy download en` and even tried using:
```
python -m spacy download en_core_web_md # download medium English model
python -m spacy link en_core_web_md en # link that model to the shortcut 'en'
```
## Environment

* Python Version Used:3.6.7
* spaCy Version Used: 2.0.16
* Environment Information: Windows-2012Server R2-6.3.9600-SP0

Following is the error I get:
![code](https://user-images.githubusercontent.com/38431040/47733245-316b5180-dc3e-11e8-8c89-7639e1802c5a.PNG)

",1
189,https://github.com/explosion/spaCy/issues/3054,3054,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}]",closed,2018-12-15 12:55:00+00:00,8,Long load time for en_core_web_md (more than 30 sec) leading to request timeout on Heroku,"## How to reproduce the behaviour
1. Run Django with gunicorn on Heroku (Gunicorn is not mandatory but that's my setup )
2. See that  en_core_web_md.load() takes long time leading to request timeouts.

## Your Environment
* Linux on Heroku (2GB)
* Python 3.5
* spaCy v2.0.18
",0
276,https://github.com/explosion/spaCy/issues/3188,3188,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}]",closed,2019-01-23 12:32:55+00:00,2,spacy.blank does not accept 'en_core_web_lg',"## How to reproduce the behaviour
After installing the 'en_core_web_lg' model, I'm unable to create a blank model from it.
I am able to do `spacy.load('en_core_web_lg')` but when I do `spacy.blank('en_core_web_lg')`
I get the following:

```
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
~/anaconda/envs/ri/lib/python3.6/site-packages/spacy/util.py in get_lang_class(lang)
     49         try:
---> 50             module = importlib.import_module('.lang.%s' % lang, 'spacy')
     51         except ImportError:

~/anaconda/envs/ri/lib/python3.6/importlib/__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

~/anaconda/envs/ri/lib/python3.6/importlib/_bootstrap.py in _gcd_import(name, package, level)

~/anaconda/envs/ri/lib/python3.6/importlib/_bootstrap.py in _find_and_load(name, import_)

~/anaconda/envs/ri/lib/python3.6/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)

ModuleNotFoundError: No module named 'spacy.lang.en_core_web_lg'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-21-2a56177fabf1> in <module>
----> 1 spacy.blank('en_core_web_lg')

~/anaconda/envs/ri/lib/python3.6/site-packages/spacy/__init__.py in blank(name, **kwargs)
     23 
     24 def blank(name, **kwargs):
---> 25     LangClass = util.get_lang_class(name)
     26     return LangClass(**kwargs)
     27 

~/anaconda/envs/ri/lib/python3.6/site-packages/spacy/util.py in get_lang_class(lang)
     50             module = importlib.import_module('.lang.%s' % lang, 'spacy')
     51         except ImportError:
---> 52             raise ImportError(Errors.E048.format(lang=lang))
     53         LANGUAGES[lang] = getattr(module, module.__all__[0])
     54     return LANGUAGES[lang]

ImportError: [E048] Can't import language en_core_web_lg from spacy.lang.
```
 
## Info about spaCy

* **spaCy version:** 2.0.16
* **Platform:** Darwin-18.2.0-x86_64-i386-64bit
* **Python version:** 3.6.6
* **Models:** en_core_web_lg, en
",0
416,https://github.com/explosion/spaCy/issues/3402,3402,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2019-03-12 22:14:05+00:00,5,Cannot load en_core_web_lg on Linux,"## How to reproduce the behaviour

```
import spacy
nlp = spacy.load(""en_core_web_lg"")
```

Returns
```
ValueError: cannot reshape array of size 179709932 into shape (684831,300)
```

## Your Environment

* **spaCy version:** 2.0.18
* **Platform:** Linux-4.14.88-72.76.amzn1.x86_64-x86_64-with-glibc2.3.4
* **Python version:** 3.6.7
* **Models:** en_core_web_lg

Additional packages that may be important:
```
numpy           1.16.2   
thinc           6.12.1
```

A nearly identical virtualenv on MacOS does not have this problem (Python 3.6.4 but otherwise the same).",0
0,https://github.com/explosion/spaCy/issues/2780,2780,[],closed,2018-09-20 12:32:17+00:00,3,Installing spaCy via `pip install spacy` on Windows 10 fails with UnicodeDecodeError,"## Problem
Installing spaCy via `pip install spacy` on Windows 10 fails with UnicodeDecodeError

```bash
Collecting spacy
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/24/de/ac14cd453c98656d6738a5669f96a4ac7f668493d5e6b78227ac933c5fd4/spacy-2.0.12.tar.gz
Requirement already satisfied: numpy>=1.7 in d:\python\python36\lib\site-packages (from spacy)
Requirement already satisfied: murmurhash<0.29,>=0.28 in d:\python\python36\lib\site-packages (from spacy)
Requirement already satisfied: cymem<1.32,>=1.30 in d:\python\python36\lib\site-packages (from spacy)
Requirement already satisfied: preshed<2.0.0,>=1.0.0 in d:\python\python36\lib\site-packages (from spacy)
Collecting thinc<6.11.0,>=6.10.3 (from spacy)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/94/b1/47a88072d0a38b3594c0a638a62f9ef7c742b8b8a87f7b105f7ed720b14b/thinc-6.10.3.tar.gz
Requirement already satisfied: plac<1.0.0,>=0.9.6 in d:\python\python36\lib\site-packages (from spacy)
Requirement already satisfied: ujson>=1.35 in d:\python\python36\lib\site-packages (from spacy)
Requirement already satisfied: dill<0.3,>=0.2 in d:\python\python36\lib\site-packages (from spacy)
Requirement already satisfied: regex==2017.4.5 in d:\python\python36\lib\site-packages (from spacy)
Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\python\python36\lib\site-packages (from spacy)
Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in d:\python\python36\lib\site-packages (from thinc<6.11.0,>=6.10.3->spacy)
Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in d:\python\python36\lib\site-packages (from thinc<6.11.0,>=6.10.3->spacy)
Requirement already satisfied: cytoolz<0.10,>=0.9.0 in d:\python\python36\lib\site-packages (from thinc<6.11.0,>=6.10.3->spacy)
Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in d:\python\python36\lib\site-packages (from thinc<6.11.0,>=6.10.3->spacy)
Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in d:\python\python36\lib\site-packages (from thinc<6.11.0,>=6.10.3->spacy)
Requirement already satisfied: six<2.0.0,>=1.10.0 in d:\python\python36\lib\site-packages (from thinc<6.11.0,>=6.10.3->spacy)
Requirement already satisfied: pyreadline>=1.7.1 in d:\python\python36\lib\site-packages (from dill<0.3,>=0.2->spacy)

Requirement already satisfied: idna<2.8,>=2.5 in d:\python\python36\lib\site-packages (from requests<3.0.0,>=2.13.0->spacy)
Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\python\python36\lib\site-packages (from requests<3.0.0,>=2.13.0->spacy)
Requirement already satisfied: urllib3<1.24,>=1.21.1 in d:\python\python36\lib\site-packages (from requests<3.0.0,>=2.13.0->spacy)
Requirement already satisfied: certifi>=2017.4.17 in d:\python\python36\lib\site-packages (from requests<3.0.0,>=2.13.0->spacy)
Requirement already satisfied: msgpack-python>=0.3.0 in d:\python\python36\lib\site-packages (from msgpack-numpy<1.0.0,>=0.4.1->thinc<6.11.0,>=6.10.3->spacy)
Requirement already satisfied: toolz>=0.8.0 in d:\python\python36\lib\site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy)
Building wheels for collected packages: spacy, thinc
  Running setup.py bdist_wheel for spacy ... error
  Failed building wheel for spacy
  Running setup.py clean for spacy
  Running setup.py bdist_wheel for thinc ... error
  Failed building wheel for thinc
  Running setup.py clean for thinc
Failed to build spacy thinc
Installing collected packages: thinc, spacy
  Found existing installation: thinc 6.10.2
    Uninstalling thinc-6.10.2:
      Successfully uninstalled thinc-6.10.2
  Running setup.py install for thinc ... error
  Rolling back uninstall of thinc
Exception:
Traceback (most recent call last):
  File ""D:\Python\Python36\lib\site-packages\pip-9.0.3-py3.6.egg\pip\compat\__init__.py"", line 73, in console_to_str
    return s.decode(sys.__stdout__.encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position 25: invalid continuation byte

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Python\Python36\lib\site-packages\pip-9.0.3-py3.6.egg\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""D:\Python\Python36\lib\site-packages\pip-9.0.3-py3.6.egg\pip\commands\install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""D:\Python\Python36\lib\site-packages\pip-9.0.3-py3.6.egg\pip\req\req_set.py"", line 784, in install
    **kwargs
  File ""D:\Python\Python36\lib\site-packages\pip-9.0.3-py3.6.egg\pip\req\req_install.py"", line 878, in install
    spinner=spinner,
  File ""D:\Python\Python36\lib\site-packages\pip-9.0.3-py3.6.egg\pip\utils\__init__.py"", line 676, in call_subprocess

    line = console_to_str(proc.stdout.readline())
  File ""D:\Python\Python36\lib\site-packages\pip-9.0.3-py3.6.egg\pip\compat\__init__.py"", line 75, in console_to_str
    return s.decode('utf_8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position 25: invalid continuation byte
You are using pip version 9.0.3, however version 18.0 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.
```

## Your Environment
* Operating System: Windows 10, 64 bit
* Python Version Used: 3.6.5
",0
1,https://github.com/explosion/spaCy/issues/2781,2781,"[{'id': 703368580, 'node_id': 'MDU6TGFiZWw3MDMzNjg1ODA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20all', 'name': 'lang / all', 'color': '726DA8', 'default': False, 'description': 'Global language data'}, {'id': 881718446, 'node_id': 'MDU6TGFiZWw4ODE3MTg0NDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20tokenizer', 'name': 'feat / tokenizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Tokenizer'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2018-09-20 14:20:14+00:00,4,Tokenizing Windows paths behaves differently than tokenizing UNIX paths,"I coincidentally stumbled upon this behaviour where paths in a text are treated differently depending on their 'style'. I'm unsure whether this is the desired behaviour. (Even though personally I'd like to see identical behaviour for the two.)

```
/u/slick/udfs/math.a --> /u / slick / udfs / math.a
d:\udfs\math.dll --> d:\udfs\math.dll
```

## How to reproduce the behaviour
```python
import spacy

nlp = spacy.load('en', disable=['parser', 'ner', 'tagger', 'textcat'])

unix = r'/u/slick/udfs/math.a'
win = r'd:\udfs\math.dll'

u_doc= nlp(unix)
w_doc = nlp(win )

unix = ' '.join(str(token) for token in u_doc)
win = ' '.join(str(token) for token in w_doc)

print(unix)
print(win)
```

## Your Environment
* Operating System: Windows 10 64 bit
* Python Version Used: 3.6.5
* spaCy Version Used: 2.0.12
",0
3,https://github.com/explosion/spaCy/issues/2783,2783,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 710446668, 'node_id': 'MDU6TGFiZWw3MTA0NDY2Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/training', 'name': 'training', 'color': '087EA6', 'default': False, 'description': 'Training and updating models'}]",closed,2018-09-20 15:34:12+00:00,4,Very high losses when adding training samples to NER model,"I have an en model trained for a custom set of entities. Sample size >3000 items across about 10-15 entities. Results are fair to good in use...When attempting to add new annotations to this model, I am getting some alarming results...losses exceed 300k and increase each iteration. Documents can be quite large...

I am using the ner_training code found in ""examples"" as is with the only change being a call to db to generate training data.  Running in a linux vm, ubuntu 18.04.  Using CPU, not GPU because I cannot get GPU working through vm and windows GPU won't compile...

Please help me understand if these very high losses are expected. They are 2+ orders of magnitude larger than the ones observed during model training from blank

if not model and output_dir:
        model=output_dir

    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank('en')  # create blank Language class
        print(""Created blank 'en' model"")

    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)
    else:
        ner = nlp.get_pipe('ner')

    TRAIN_DATA = load_ner_data()
    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = ner.create_optimizer()
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            losses = {}
            for text, annotations in TRAIN_DATA:
                nlp.update(
                    [text],  # batch of texts
                    [annotations],  # batch of annotations
                    drop=0.5,  # dropout - make it harder to memorise data
                    sgd=optimizer,  # callable to update weights
                    losses=losses)
            print(losses)
",0
5,https://github.com/explosion/spaCy/issues/2785,2785,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881666463, 'node_id': 'MDU6TGFiZWw4ODE2NjY0NjM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20serialize', 'name': 'feat / serialize', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Serialization, saving and loading'}]",closed,2018-09-20 22:32:36+00:00,2,Serialization of Vocab does not retain language,"I noticed a problem when serializing a `Vocab` object to disk:

```python
>> import spacy
>> nlp = spacy.load(""en"")
>> print(nlp.vocab.lang)`
'en'

>> nlp.vocab.to_disk(""vocab"")
>> vocab2 = spacy.vocab.Vocab().from_disk(""vocab"")
>> print(vocab2.lang)
''
```

This creates  problems when the vocabulary is used later on to create `Doc` objects, as the initialisation calls `_get_chunker(self.vocab.lang)`.

I'm using spacy 2.0.12. I also tried to use `vocab2 = spacy.load(""en"").from_disk(""vocab"")`, but this doesn't seem to properly load the vocabulary dumped on disk.",0
6,https://github.com/explosion/spaCy/issues/2787,2787,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 881666230, 'node_id': 'MDU6TGFiZWw4ODE2NjYyMzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20vectors', 'name': 'feat / vectors', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Word vectors and similarity'}]",closed,2018-09-21 10:43:22+00:00,6,Missing words?,"I'm LOVING spacy, but for my user case there appears to be key words missing from the vector model. For example when I run;
'reflexologist' in nlp.vocab
I get False.

As im new to both python and NLP, I was hoping there was an easy solution?
Thanks
",0
7,https://github.com/explosion/spaCy/issues/2789,2789,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-09-21 17:35:55+00:00,3,"Can't create a custom model for detecting ""Organizations"".","I can't create a custom model using Spacy. I have used the code given in the official documentations of <a href = ""https://spacy.io/usage/training""> Spacy </a> under the <a href = ""https://spacy.io/usage/training#example-train-ner"">Updating the Named Entity Recognizer</a> section.
I have used the same code, I have just changed the Training data with mine. While testing the model it is creating tokens but not the entities. Any help will be appriciated.

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows 10
* Python Version Used: python- 2.7 (Anaconda2)
* spaCy Version Used: 2.0.12
* Environment Information: Pycharm Community 

I have attached the code below.
[Model.txt](https://github.com/explosion/spaCy/files/2406380/Model.txt)
",0
8,https://github.com/explosion/spaCy/issues/2791,2791,[],closed,2018-09-24 00:10:43+00:00,2,Help with custom sentence segmentation in spaCy,"**Disclaimer**: This is not an Issue, I need some help with my usage. But stackoverflow [question](https://stackoverflow.com/questions/52458404/custom-sentence-segmentation-in-spacy) couldn't be of any help, so am posting it here. 

I want `spaCy` to use the sentence segmentation boundaries as I provide instead of its own processing. 

For example:
```python

get_sentences(""Bob meets Alice. @SentBoundary@ They play together."")
# => [""Bob meets Alice."", ""They play together.""]  # two sentences

get_sentences(""Bob meets Alice. They play together."")
# => [""Bob meets Alice. They play together.""]  # ONE sentence

get_sentences(""Bob meets Alice, @SentBoundary@ they play together."")
# => [""Bob meets Alice,"", ""they play together.""] # two sentences
```

This is what I have so far (borrowing things from documentation [here](https://spacy.io/usage/processing-pipelines#component-example1)):

```python
import spacy
nlp = spacy.load('en_core_web_sm')

def mark_sentence_boundaries(doc):
    for i, token in enumerate(doc):
        if token.text == '@SentBoundary@':
            doc[i+1].sent_start = True
    return doc

nlp.add_pipe(mark_sentence_boundaries, before='parser')

def get_sentences(text):
    doc = nlp(text)
    return (list(doc.sents))
```

But the results I get are as follows:

```python
# Ex1
get_sentences(""Bob meets Alice. @SentBoundary@ They play together."")
#=> [""Bob meets Alice."", ""@SentBoundary@"", ""They play together.""]

# Ex2
get_sentences(""Bob meets Alice. They play together."")
#=> [""Bob meets Alice."", ""They play together.""]

# Ex3
get_sentences(""Bob meets Alice, @SentBoundary@ they play together."")
#=> [""Bob meets Alice, @SentBoundary@"", ""they play together.""]
```

Following are main problems I am facing:

1. When sentence break is found, how to get rid of `@SentBoundary@` token.
2. How to disallow `spaCy` from splitting if `@SentBoundary@` is not present. I could perhaps remove 'parser' from the pipeline and that would work but I do need to keep that in pipeline.

Thank you for help!",0
10,https://github.com/explosion/spaCy/issues/2794,2794,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2018-09-25 03:48:13+00:00,9,Seeking explicit instructions on how to recompile local project with proposed patch,"Hi,

I'm having trouble teaching myself how to toy with/test patches to my local clone of spaCy in a virtual environment python repl. In particular, whenever I make changes to the code, I don't see that reflected in values returned in the repl. In addition to following the basic instructions here compiling from source (https://spacy.io/usage/#section-quickstart), I've also tried `pip install -I -e .`, `python setup.py clean`, and `python -m compileall -f *directory*`. I get a failure from Fabric that says `No module named fabric.api` even though it is installed in my virtual environment. None of these commands I found online have helped me see my changes.

It would be appreciated if the documents describing how to compile from source also said how to recompile from source with changes for use in a repl or in testing.

Thanks!",0
11,https://github.com/explosion/spaCy/issues/2796,2796,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446637231, 'node_id': 'MDU6TGFiZWw0NDY2MzcyMzE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/linux', 'name': 'linux', 'color': '676F6D', 'default': False, 'description': 'Issues related to Linux'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2018-09-25 09:41:59+00:00,3,Can not install spacy on Linux,"<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

Following below steps :

``` 
# git clone https://github.com/explosion/spaCy
# cd spaCy
# export PYTHONPATH=`pwd`
# pip install -r requirements.txt
# python setup.py build_ext --inplace
# python -m spacy download en
```

```linux
    from dill import objects
ImportError: cannot import name objects

```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Linux
* Python Version Used: 2.7
* spaCy Version Used:2.0.12
* Environment Information: 
",0
12,https://github.com/explosion/spaCy/issues/2798,2798,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}]",closed,2018-09-25 10:49:50+00:00,19,importing spacy results in RuntimeWarning,"I installed spacy with conda install -c conda-forge spacy. This all seemed to work fine, however, when I try to import spacy I get an error (see below). I even reinstalled numpy.

```
>>> import spacy
C:\Users\x\AppData\Local\Continuum\Anaconda3\lib\importlib\_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
C:\Users\x\AppData\Local\Continuum\Anaconda3\lib\importlib\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176
  return f(*args, **kwds)
C:\Users\x\AppData\Local\Continuum\Anaconda3\lib\importlib\_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
C:\Users\x\AppData\Local\Continuum\Anaconda3\lib\importlib\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176
  return f(*args, **kwds)
```

## Info about spaCy
* **spaCy version:** 2.0.12
* **Platform:** Windows-10-10.0.17134-SP0
* **Python version:** 3.6.6
* **Models:** en
",0
13,https://github.com/explosion/spaCy/issues/2800,2800,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 710446668, 'node_id': 'MDU6TGFiZWw3MTA0NDY2Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/training', 'name': 'training', 'color': '087EA6', 'default': False, 'description': 'Training and updating models'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-09-25 13:57:04+00:00,2,Too many labels result in a crash,"Hi, I'm currently trying to train a custom model with over 125 labels and I encounter the following error:
### Windows 10
```
Process finished with exit code -1073740791 (0xC0000409)
```
### Ubuntu 18.04
```
*** stack smashing detected ***: <unknown> terminated
Aborted (core dumped)
```

There seems to be a limit. Under 125 labels it works and over it, it crashes.

## How to reproduce the behaviour

```python
def __train_model(self, train_data, entity_types):
    nlp = spacy.blank(""en"")

    ner = nlp.create_pipe(""ner"")
    nlp.add_pipe(ner)

    for entity_type in list(entity_types):
        ner.add_label(entity_type)

    optimizer = nlp.begin_training()

    # Start training
    for i in range(20):
        losses = {}
        index = 0
        random.shuffle(train_data)

        for statement, entities in train_data:
            nlp.update([statement], [entities], sgd=optimizer, losses=losses, drop=0.5)

    return nlp
```

#### Unit Test
```python
    def test_train_with_max_supported_entity_types(self):
        train_data = TrainData()
        train_data.extend([(""One sentence"", {""entities"": []})])
        entity_types = {i for i in range(125)}

        model = self.train_model_processor.train(train_data, entity_types)

        assert_is_not_none(model)
```
So in the unit test whenever entity_types length is beyond 125, it crashes.

## Your Environment
* **spaCy version:** 2.0.12
* **Platform:** Windows-10-10.0.16299-SP0
* **Python version:** 3.7.0

* Environment Information:
16gb RAM, CPU: i7-3630QM

Any idea if there is a limit of labels ? If so, should it return an error message describing the error instead of crashing ?
",0
14,https://github.com/explosion/spaCy/issues/2804,2804,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-09-27 09:47:52+00:00,3,"Can't create a custom model for detecting ""Organizations""","Shouldn't my model produce the same entities, like it did for example mentioned in spaCy documentation? And can we do modifications to the existing ""en_core_web_sm"" model? I want the model to just recognize '_ORG_' with '_Inc_', '_Ltd_', '_LLC_' etc. The existing model is good, but it is also giving some junk values. For example there is a term 'AGREEMENT' in doc, the model is giving '_AGREEMENT_' as '_ORG_'. How can I avoid that? I'll be attaching a screenshot for your reference. 
Thanks for the help.
In the screenshot, we can see that ""_Employee Assistance Program_"" is not an organization.
![output](https://user-images.githubusercontent.com/28997234/46008855-88c85000-c0db-11e8-93af-b9a8063232a7.PNG)

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows 10
* Python Version Used: python- 2.7 (Anaconda2)
* spaCy Version Used: 2.0.12
* Environment Information: Pycharm Community 

I have attached the code below.
[Model.txt](https://github.com/explosion/spaCy/files/2423488/Model.txt)
",0
15,https://github.com/explosion/spaCy/issues/2809,2809,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}, {'id': 677479449, 'node_id': 'MDU6TGFiZWw2Nzc0Nzk0NDk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/%F0%9F%94%AE%20thinc', 'name': '醗 thinc', 'color': 'f6f6f6', 'default': False, 'description': ""spaCy's machine learning library Thinc""}]",closed,2018-09-27 15:47:06+00:00,4,RuntimeWarning when numpy is not imported,"I was trying the TextCategorizer example (https://github.com/explosion/spacy/blob/master/examples/training/train_textcat.py) with offline IMDB dataset. Hence I removed the import thinc.extra.datasets. That's when I started getting the following warning:

`RuntimeWarning: invalid value encountered in sqrt ret = sqrt(sqnorm)`

After analyzing I figured out that I need to import numpy before spaCy. It's kind of confusing when someone does not need numpy in there code.",0
16,https://github.com/explosion/spaCy/issues/2810,2810,"[{'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}, {'id': 881666463, 'node_id': 'MDU6TGFiZWw4ODE2NjY0NjM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20serialize', 'name': 'feat / serialize', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Serialization, saving and loading'}]",closed,2018-09-28 08:21:13+00:00,4,nlp.to_disk() throwing TypeError: __init__() got an unexpected keyword argument 'encoding',"## How to reproduce the behaviour
I was following the example here to train my own NER model https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py and I got the following error with the stack trace:

```
Traceback (most recent call last):
  File ""app.py"", line 121, in <module>
    nlp.to_disk(output_dir)
  File ""/home/ec2-user/ner_model/venv/lib64/python3.6/site-packages/spacy/language.py"", line 621, in to_disk
    util.to_disk(path, serializers, {p: False for p in disable})
  File ""/home/ec2-user/ner_model/venv/lib64/python3.6/site-packages/spacy/util.py"", line 503, in to_disk
    writer(path / key)
  File ""/home/ec2-user/ner_model/venv/lib64/python3.6/site-packages/spacy/language.py"", line 609, in <lambda>
    ('tokenizer', lambda p: self.tokenizer.to_disk(p, vocab=False)),
  File ""tokenizer.pyx"", line 354, in spacy.tokenizer.Tokenizer.to_disk
  File ""tokenizer.pyx"", line 355, in spacy.tokenizer.Tokenizer.to_disk
  File ""tokenizer.pyx"", line 384, in spacy.tokenizer.Tokenizer.to_bytes
  File ""/home/ec2-user/ner_model/venv/lib64/python3.6/site-packages/spacy/util.py"", line 486, in to_bytes
    return msgpack.dumps(serialized, use_bin_type=True, encoding='utf8')
  File ""/home/ec2-user/ner_model/venv/lib64/python3.6/site-packages/msgpack_numpy.py"", line 196, in packb
    return Packer(**kwargs).pack(o)
TypeError: __init__() got an unexpected keyword argument 'encoding'
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Amazon Linux 2, Mac OS X 10.13.2
* Python Version Used: 3.6
* spaCy Version Used: 2.0.12
* Environment Information: 
 My `pip list` returns

```
certifi (2018.8.24)
chardet (3.0.4)
cymem (1.31.2)
cytoolz (0.9.0.1)
dill (0.2.8.2)
idna (2.7)
msgpack (0.5.6)
msgpack-numpy (0.4.4.1)
murmurhash (0.28.0)
numpy (1.15.2)
pip (9.0.3)
plac (0.9.6)
preshed (1.0.1)
regex (2017.4.5)
requests (2.19.1)
setuptools (39.0.1)
six (1.11.0)
spacy (2.0.12)
thinc (6.10.3)
toolz (0.9.0)
tqdm (4.26.0)
ujson (1.35)
urllib3 (1.23)
wrapt (1.10.11)
```

Any ideas why my `to_disk()` throwing this error?",0
17,https://github.com/explosion/spaCy/issues/2811,2811,[],closed,2018-09-28 16:57:35+00:00,1, token.text causing AttributeError: 'list' object has no attribute 'text',"Hi, 

I'm working with a tool that uses SpaCy. The tool first takes a document and the performs sentence tokenization and generates a list for each sentence. After that, another method loops of the words in each sentence and then an AttributeError is triggered when calling `.text`:
  ```

Any help would truly be appreciated.
Thank you so much in advance.",0
18,https://github.com/explosion/spaCy/issues/2812,2812,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}]",closed,2018-09-29 08:18:56+00:00,2,"Installation problem: ""fatal error: 'float.h' file not found"" (from dependency ujson)","## How to reproduce the problem
try and install spaCy with different methods: 
- from IDE (IntelliJ, via ""tools>manage python packages"")
- using ""pip install spacy"" (or pip3) - in venv
- build from source
- try first build ujson separately

```bash
Complete output from command /Users/.../venv/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/ng/ykk4sfgn0w1fjfqvxwv_k8tw0000gp/T/pip-install-rqbvd5ge/ujson/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /private/var/folders/ng/ykk4sfgn0w1fjfqvxwv_k8tw0000gp/T/pip-record-rsikncpt/install-record.txt --single-version-externally-managed --compile --install-headers /Users/martinwunderlich/Dev/DeepTermExtraction/venv/include/site/python3.5/ujson:
    running install
    running build
    running build_ext
    building 'ujson' extension
    creating build
    creating build/temp.macosx-10.6-intel-3.5
    creating build/temp.macosx-10.6-intel-3.5/python
    creating build/temp.macosx-10.6-intel-3.5/lib
    /usr/bin/clang -fno-strict-aliasing -Wsign-compare -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -arch i386 -arch x86_64 -isysroot /Developer/SDKs/MacOSX10.6.sdk -g -I./python -I./lib -I/Users/.../venv/include -I/Library/Frameworks/Python.framework/Versions/3.5/include/python3.5m -c ./python/ujson.c -o build/temp.macosx-10.6-intel-3.5/./python/ujson.o -D_GNU_SOURCE
    /usr/bin/clang -fno-strict-aliasing -Wsign-compare -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -arch i386 -arch x86_64 -isysroot /Developer/SDKs/MacOSX10.6.sdk -g -I./python -I./lib -I/Users/.../venv/include -I/Library/Frameworks/Python.framework/Versions/3.5/include/python3.5m -c ./python/objToJSON.c -o build/temp.macosx-10.6-intel-3.5/./python/objToJSON.o -D_GNU_SOURCE
    /usr/bin/clang -fno-strict-aliasing -Wsign-compare -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -arch i386 -arch x86_64 -isysroot /Developer/SDKs/MacOSX10.6.sdk -g -I./python -I./lib -I/Users/.../venv/include -I/Library/Frameworks/Python.framework/Versions/3.5/include/python3.5m -c ./python/JSONtoObj.c -o build/temp.macosx-10.6-intel-3.5/./python/JSONtoObj.o -D_GNU_SOURCE
    /usr/bin/clang -fno-strict-aliasing -Wsign-compare -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -arch i386 -arch x86_64 -isysroot /Developer/SDKs/MacOSX10.6.sdk -g -I./python -I./lib -I/Users/.../venv/include -I/Library/Frameworks/Python.framework/Versions/3.5/include/python3.5m -c ./lib/ultrajsonenc.c -o build/temp.macosx-10.6-intel-3.5/./lib/ultrajsonenc.o -D_GNU_SOURCE
    In file included from ./lib/ultrajsonenc.c:46:
    In file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/9.0.0/include/float.h:36:
    /Developer/SDKs/MacOSX10.6.sdk/usr/include/float.h:8:15: fatal error: 'float.h' file not found
    #include_next <float.h>
                  ^~~~~~~~~
    1 error generated.
    error: command '/usr/bin/clang' failed with exit status 1
```

## Your Environment
* Operating System: Mac OS Sierra 10.12.6
* Python Version Used: 3.5.4
* spaCy Version Used: current stable (2.0)
* Environment Information: XCode 9.2

There seems to be a problem with missing header file float.h when trying to install spacy's dependency ujson. I have already tried to reinstall Xcode 9.2. with the command line tools, but this didn't help. I have also tried installing with sudo and installing ujson from source instead of with pip, but neither of these helped. 
Any other ideas? 
",0
19,https://github.com/explosion/spaCy/issues/2813,2813,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2018-09-29 12:33:29+00:00,3,Using a new spacy version in other projects,"I have used Spacy and developed it for a new language. I have built it and I assumed that is ready. however I can't use it in other projects. I want to use Rasa chatbot which is using spacy. How can I use my own spacy instead of the global one???


* Operating System: Linux
* Python Version Used: 3.6

",0
22,https://github.com/explosion/spaCy/issues/2817,2817,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}]",closed,2018-10-02 13:07:28+00:00,5,ValueError: [E088],"## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: 10 
* Python Version Used: Spyder, Python 3.6.4
* spaCy Version Used:(2.0.12)
   max_length=self.max_length))

ValueError: [E088] Text of length 5854574 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",0
23,https://github.com/explosion/spaCy/issues/2819,2819,"[{'id': 881666463, 'node_id': 'MDU6TGFiZWw4ODE2NjY0NjM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20serialize', 'name': 'feat / serialize', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Serialization, saving and loading'}]",closed,2018-10-03 01:37:33+00:00,2,Can't Pickle with Pyspark & spaCy version 2.0.12,"Hi,

I am facing this error  while using the textcat model with Apache Spark

`PicklingError: Can't pickle <cyfunction LinearModel.<lambda> at 0x10487c498>`",0
25,https://github.com/explosion/spaCy/issues/2821,2821,[],closed,2018-10-04 07:08:59+00:00,2,Cannot install SpaCy with Python3.6,"<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

## How to reproduce the problem
<!-- Include the details of how the problem occurred. Which command did you run to install spaCy? Did you come across an error? What else did you try? -->
Hi there. I am settling the environment for a chatbot named RASA.

I used `pip install spacy` to install spacy but met this error msg.
Before that, I've tried to 
`pip install --no-cache-dir spacy`  not work
` pip install swig`  not work

```bash
Failed building wheel for spacy
  Running setup.py clean for spacy
  Running setup.py bdist_wheel for murmurhash ... error
  Complete output from command /Users/admin/anaconda2/envs/ChatbotNeptune/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/murmurhash/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-wheel-g0d39ad9 --python-tag cp36:
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.7-x86_64-3.6
  creating build/lib.macosx-10.7-x86_64-3.6/murmurhash
  copying murmurhash/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/murmurhash
  copying murmurhash/about.py -> build/lib.macosx-10.7-x86_64-3.6/murmurhash
  creating build/lib.macosx-10.7-x86_64-3.6/murmurhash/tests
  copying murmurhash/tests/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/murmurhash/tests
  copying murmurhash/tests/test_import.py -> build/lib.macosx-10.7-x86_64-3.6/murmurhash/tests
  copying murmurhash/mrmr.pyx -> build/lib.macosx-10.7-x86_64-3.6/murmurhash
  copying murmurhash/__init__.pxd -> build/lib.macosx-10.7-x86_64-3.6/murmurhash
  copying murmurhash/mrmr.pxd -> build/lib.macosx-10.7-x86_64-3.6/murmurhash
  creating build/lib.macosx-10.7-x86_64-3.6/murmurhash/include
  creating build/lib.macosx-10.7-x86_64-3.6/murmurhash/include/murmurhash
  copying murmurhash/include/murmurhash/MurmurHash2.h -> build/lib.macosx-10.7-x86_64-3.6/murmurhash/include/murmurhash
  copying murmurhash/include/murmurhash/MurmurHash3.h -> build/lib.macosx-10.7-x86_64-3.6/murmurhash/include/murmurhash
  running build_ext
  building 'murmurhash.mrmr' extension
  creating build/temp.macosx-10.7-x86_64-3.6
  creating build/temp.macosx-10.7-x86_64-3.6/murmurhash
  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -I/private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/murmurhash/murmurhash/include -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -c murmurhash/mrmr.cpp -o build/temp.macosx-10.7-x86_64-3.6/murmurhash/mrmr.o -O3 -Wno-strict-prototypes -Wno-unused-function
  warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]
  1 warning generated.
  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -I/private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/murmurhash/murmurhash/include -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -c murmurhash/MurmurHash2.cpp -o build/temp.macosx-10.7-x86_64-3.6/murmurhash/MurmurHash2.o -O3 -Wno-strict-prototypes -Wno-unused-function
  warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]
  1 warning generated.
  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -I/private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/murmurhash/murmurhash/include -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -c murmurhash/MurmurHash3.cpp -o build/temp.macosx-10.7-x86_64-3.6/murmurhash/MurmurHash3.o -O3 -Wno-strict-prototypes -Wno-unused-function
  warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]
  1 warning generated.
  g++ -bundle -undefined dynamic_lookup -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -arch x86_64 build/temp.macosx-10.7-x86_64-3.6/murmurhash/mrmr.o build/temp.macosx-10.7-x86_64-3.6/murmurhash/MurmurHash2.o build/temp.macosx-10.7-x86_64-3.6/murmurhash/MurmurHash3.o -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -o build/lib.macosx-10.7-x86_64-3.6/murmurhash/mrmr.cpython-36m-darwin.so
  clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]
  ld: library not found for -lstdc++
  clang: error: linker command failed with exit code 1 (use -v to see invocation)
  error: command 'g++' failed with exit status 1

  ----------------------------------------
  Failed building wheel for murmurhash
  Running setup.py clean for murmurhash
  Running setup.py bdist_wheel for cymem ... error
  Complete output from command /Users/admin/anaconda2/envs/ChatbotNeptune/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/cymem/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-wheel-s5r8dgld --python-tag cp36:
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.7-x86_64-3.6
  creating build/lib.macosx-10.7-x86_64-3.6/cymem
  copying cymem/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/cymem
  copying cymem/about.py -> build/lib.macosx-10.7-x86_64-3.6/cymem
  package init file 'cymem/tests/__init__.py' not found (or not a regular file)
  creating build/lib.macosx-10.7-x86_64-3.6/cymem/tests
  copying cymem/tests/test_import.py -> build/lib.macosx-10.7-x86_64-3.6/cymem/tests
  copying cymem/cymem.pyx -> build/lib.macosx-10.7-x86_64-3.6/cymem
  copying cymem/__init__.pxd -> build/lib.macosx-10.7-x86_64-3.6/cymem
  copying cymem/cymem.pxd -> build/lib.macosx-10.7-x86_64-3.6/cymem
  running build_ext
  building 'cymem.cymem' extension
  creating build/temp.macosx-10.7-x86_64-3.6
  creating build/temp.macosx-10.7-x86_64-3.6/cymem
  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -c cymem/cymem.cpp -o build/temp.macosx-10.7-x86_64-3.6/cymem/cymem.o -O3 -Wno-strict-prototypes -Wno-unused-function
  warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]
  1 warning generated.
  g++ -bundle -undefined dynamic_lookup -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -arch x86_64 build/temp.macosx-10.7-x86_64-3.6/cymem/cymem.o -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -o build/lib.macosx-10.7-x86_64-3.6/cymem/cymem.cpython-36m-darwin.so
  clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]
  ld: library not found for -lstdc++
  clang: error: linker command failed with exit code 1 (use -v to see invocation)
  error: command 'g++' failed with exit status 1

  ----------------------------------------
  Failed building wheel for cymem
  Running setup.py clean for cymem
  Running setup.py bdist_wheel for preshed ... error
  Complete output from command /Users/admin/anaconda2/envs/ChatbotNeptune/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/preshed/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-wheel-8qe9et36 --python-tag cp36:
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.7-x86_64-3.6
  creating build/lib.macosx-10.7-x86_64-3.6/preshed
  copying preshed/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/preshed
  copying preshed/about.py -> build/lib.macosx-10.7-x86_64-3.6/preshed
  creating build/lib.macosx-10.7-x86_64-3.6/preshed/tests
  copying preshed/tests/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/preshed/tests
  copying preshed/tests/test_pop.py -> build/lib.macosx-10.7-x86_64-3.6/preshed/tests
  copying preshed/tests/test_counter.py -> build/lib.macosx-10.7-x86_64-3.6/preshed/tests
  copying preshed/tests/test_hashing.py -> build/lib.macosx-10.7-x86_64-3.6/preshed/tests
  copying preshed/counter.pyx -> build/lib.macosx-10.7-x86_64-3.6/preshed
  copying preshed/maps.pyx -> build/lib.macosx-10.7-x86_64-3.6/preshed
  copying preshed/maps.pxd -> build/lib.macosx-10.7-x86_64-3.6/preshed
  copying preshed/__init__.pxd -> build/lib.macosx-10.7-x86_64-3.6/preshed
  copying preshed/counter.pxd -> build/lib.macosx-10.7-x86_64-3.6/preshed
  running build_ext
  building 'preshed.maps' extension
  creating build/temp.macosx-10.7-x86_64-3.6
  creating build/temp.macosx-10.7-x86_64-3.6/preshed
  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -c preshed/maps.cpp -o build/temp.macosx-10.7-x86_64-3.6/preshed/maps.o -O3 -Wno-strict-prototypes -Wno-unused-function
  warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]
  preshed/maps.cpp:1643:35: warning: comparison of integers of different signs: 'long' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
        __pyx_t_1 = ((__pyx_v_power < __pyx_v_initial_size) != 0);
                      ~~~~~~~~~~~~~ ^ ~~~~~~~~~~~~~~~~~~~~
  preshed/maps.cpp:4298:34: warning: comparison of integers of different signs: 'int' and 'const __pyx_t_7preshed_4maps_key_t' (aka 'const unsigned long long') [-Wsign-compare]
      __pyx_t_1 = (((__pyx_v_i[0]) < __pyx_v_map_->length) != 0);
                     ~~~~~~~~~~~~  ^ ~~~~~~~~~~~~~~~~~~~~
  preshed/maps.cpp:4385:32: warning: comparison of integers of different signs: 'int' and 'const __pyx_t_7preshed_4maps_key_t' (aka 'const unsigned long long') [-Wsign-compare]
    __pyx_t_1 = (((__pyx_v_i[0]) == __pyx_v_map_->length) != 0);
                   ~~~~~~~~~~~~  ^  ~~~~~~~~~~~~~~~~~~~~
  preshed/maps.cpp:4462:32: warning: comparison of integers of different signs: 'int' and 'unsigned long long' [-Wsign-compare]
    __pyx_t_1 = (((__pyx_v_i[0]) == (__pyx_v_map_->length + 1)) != 0);
                   ~~~~~~~~~~~~  ^   ~~~~~~~~~~~~~~~~~~~~~~~~
  5 warnings generated.
  g++ -bundle -undefined dynamic_lookup -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -arch x86_64 build/temp.macosx-10.7-x86_64-3.6/preshed/maps.o -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -o build/lib.macosx-10.7-x86_64-3.6/preshed/maps.cpython-36m-darwin.so
  clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]
  ld: library not found for -lstdc++
  clang: error: linker command failed with exit code 1 (use -v to see invocation)
  error: command 'g++' failed with exit status 1

  ----------------------------------------
  Failed building wheel for preshed
  Running setup.py clean for preshed
  Running setup.py bdist_wheel for thinc ... error
  Complete output from command /Users/admin/anaconda2/envs/ChatbotNeptune/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/thinc/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-wheel-j1sqc_jg --python-tag cp36:
  Warning: The nvcc binary could not be located in your $PATH. For GPU capability, either add it to your path, or set $CUDA_HOME
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.7-x86_64-3.6
  creating build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/describe.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/misc.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/compat.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/check.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/t2v.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/about.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/api.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/loss.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/v2v.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/t2t.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/exceptions.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/i2v.py -> build/lib.macosx-10.7-x86_64-3.6/thinc
  creating build/lib.macosx-10.7-x86_64-3.6/thinc/tests
  copying thinc/tests/conftest.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests
  copying thinc/tests/strategies.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests
  copying thinc/tests/test_api_funcs.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests
  copying thinc/tests/util.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests
  copying thinc/tests/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests
  copying thinc/tests/test_util.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests
  creating build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_check_exceptions.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_model.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_hash_embed.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_beam_search.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_pooling.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_imports.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_rnn.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_loss.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_ops.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_mem.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_difference.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_affine.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  copying thinc/tests/unit/test_about.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/unit
  creating build/lib.macosx-10.7-x86_64-3.6/thinc/tests/integration
  copying thinc/tests/integration/test_batch_norm.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/integration
  copying thinc/tests/integration/test_affine_learns.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/integration
  copying thinc/tests/integration/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/integration
  copying thinc/tests/integration/test_mnist.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/integration
  copying thinc/tests/integration/test_roundtrip_bytes.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/integration
  copying thinc/tests/integration/test_feed_forward.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/integration
  copying thinc/tests/integration/test_shape_check.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/integration
  copying thinc/tests/integration/test_basic_tagger.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/integration
  creating build/lib.macosx-10.7-x86_64-3.6/thinc/tests/linear
  copying thinc/tests/linear/test_avgtron.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/linear
  copying thinc/tests/linear/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/linear
  copying thinc/tests/linear/test_sparse_array.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/linear
  copying thinc/tests/linear/test_linear.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/tests/linear
  creating build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  creating build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/vecs2vecs.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/util.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/pooling.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/vecs2vec.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/mem.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/vec2vec.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/train.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/_lsuv.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  creating build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/load_nlp.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/hpbff.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/datasets.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/wrappers.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  creating build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/attention.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/convolution.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/layernorm.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/maxout.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/embed.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/batchnorm.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/difference.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/feed_forward.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/affine.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/model.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/resnet.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/relu.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/static_vectors.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/lstm.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/hash_embed.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/elu.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/rnn.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/softmax.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  copying thinc/neural/_classes/selu.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural/_classes
  creating build/lib.macosx-10.7-x86_64-3.6/thinc/extra/_vendorized
  copying thinc/extra/_vendorized/keras_datasets.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra/_vendorized
  copying thinc/extra/_vendorized/keras_generic_utils.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra/_vendorized
  copying thinc/extra/_vendorized/keras_data_utils.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra/_vendorized
  copying thinc/extra/_vendorized/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra/_vendorized
  copying thinc/structs.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/typedefs.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/linalg.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/linalg.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/typedefs.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/__init__.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/structs.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/compile_time_constants.pxi -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/typedefs.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/linalg.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/structs.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc
  copying thinc/linear/sparse.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/features.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/avgtron.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/serialize.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/linear.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/serialize.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/__init__.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/avgtron.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/features.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/sparse.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/serialize.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/linear.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/avgtron.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/features.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/linear/sparse.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/linear
  copying thinc/neural/gpu_ops.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/_funcs.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/optimizers.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/ops.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/ops.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/__init__.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/_funcs.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/optimizers.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/ops.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/_funcs.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/neural/gpu_ops.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/neural
  copying thinc/extra/mb.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/search.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/cache.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/eg.pyx -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/__init__.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/eg.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/cache.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/search.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/mb.pxd -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/eg.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/cache.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/search.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  copying thinc/extra/mb.cpp -> build/lib.macosx-10.7-x86_64-3.6/thinc/extra
  running build_ext
  building 'thinc.linalg' extension
  creating build/temp.macosx-10.7-x86_64-3.6
  creating build/temp.macosx-10.7-x86_64-3.6/thinc
  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -I/private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/thinc/include -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -c thinc/linalg.cpp -o build/temp.macosx-10.7-x86_64-3.6/thinc/linalg.o -O3 -Wno-strict-prototypes -Wno-unused-function
  warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]
  1 warning generated.
  g++ -bundle -undefined dynamic_lookup -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -arch x86_64 build/temp.macosx-10.7-x86_64-3.6/thinc/linalg.o -L/Users/admin/anaconda2/envs/ChatbotNeptune/lib -o build/lib.macosx-10.7-x86_64-3.6/thinc/linalg.cpython-36m-darwin.so
  clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]
  ld: library not found for -lstdc++
  clang: error: linker command failed with exit code 1 (use -v to see invocation)
  error: command 'g++' failed with exit status 1

  ----------------------------------------
  Failed building wheel for thinc
  Running setup.py clean for thinc
Failed to build spacy murmurhash cymem preshed thinc
Installing collected packages: murmurhash, cymem, preshed, msgpack, msgpack-numpy, toolz, cytoolz, wrapt, plac, dill, thinc, ujson, regex, spacy
  Running setup.py install for murmurhash ... error
    Complete output from command /Users/admin/anaconda2/envs/ChatbotNeptune/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/murmurhash/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-record-8rpy_g5r/install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_py
    running build_ext
    building 'murmurhash.mrmr' extension
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include -arch x86_64 -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -I/private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/murmurhash/murmurhash/include -I/Users/admin/anaconda2/envs/ChatbotNeptune/include/python3.6m -c murmurhash/mrmr.cpp -o build/temp.macosx-10.7-x86_64-3.6/murmurhash/mrmr.o -O3 -Wno-strict-prototypes -Wno-unused-function
    clang: error: no such file or directory: 'murmurhash/mrmr.cpp'
    clang: error: no input files
    error: command 'gcc' failed with exit status 1

    ----------------------------------------
Command ""/Users/admin/anaconda2/envs/ChatbotNeptune/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/murmurhash/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-record-8rpy_g5r/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/nf/4nw7chhd5qsfhj5db_0nnqgm0000gn/T/pip-install-va1nd5e9/murmurhash/
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: 10.14 Mojave
* Python Version Used: 3.6.1
* spaCy Version Used: try to install 2.0 but not successful.
* Environment Information:
",0
26,https://github.com/explosion/spaCy/issues/2822,2822,"[{'id': 589944649, 'node_id': 'MDU6TGFiZWw1ODk5NDQ2NDk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20it', 'name': 'lang / it', 'color': '726DA8', 'default': False, 'description': 'Italian language data and models'}, {'id': 881718446, 'node_id': 'MDU6TGFiZWw4ODE3MTg0NDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20tokenizer', 'name': 'feat / tokenizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Tokenizer'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2018-10-04 20:39:00+00:00,4,"Italian ""po'"" word is tokenized incorrectly","## Your Environment
* **spaCy version:** 2.0.12
* **Platform:** Windows-10-10.0.16299-SP0
* **Python version:** 3.6.5
* **Models:** it

""Po'"" (with the apostrophe) is the truncated form of ""poco"" (that means ""a little"") and must be considered as a single word, while currently spacy considers ""po"" and ""'"" as two tokens.
Notice that ""Po"", without the apostrophe, is the name of the longhest river in Italy. But in this case it always has the capital P and is never followed by the apostrophe.",0
28,https://github.com/explosion/spaCy/issues/2824,2824,"[{'id': 514165920, 'node_id': 'MDU6TGFiZWw1MTQxNjU5MjA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20en', 'name': 'lang / en', 'color': '726DA8', 'default': False, 'description': 'English language data and models'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-10-05 18:51:31+00:00,6,Replicating benchmark NER results with Ontonotes 5,"# On replicating benchmark NER results with Ontonotes 5

I would like to replicate the results of the spaCy pre-trained NER model on the English portion of the Ontonotes 5 corpus (the spaCy documentation says I should get ~ 85% F1), but I could not find any details on how to replicate it exactly.

1. Which test set was used for the evaluation? I followed the instructions at http://cemantix.org/data/ontonotes.html, but there are two different test sets (""Test"" and ""CoNLL-2012 Test""). Also, were all subcorpora used, or was the pivot text (subdirectory `pt`, consisting of text from Old and New Testaments) removed? (since this does not have entity labels)

2. I am currently extracting entities one sentence at a time (e.g. doc = nlp(sentence_text); ents = doc.ents). Is this the right way to use spaCy's NER, or am I missing something? Doing it this way, I'm getting about 74% micro-F1 (using spaCy 2.0.12, with model en-core-web-lg 2.0.0).

Thanks!

## Which page or section is this issue related to?
https://spacy.io/usage/facts-figures#ner-accuracy-ontonotes5
",0
30,https://github.com/explosion/spaCy/issues/2826,2826,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 514165984, 'node_id': 'MDU6TGFiZWw1MTQxNjU5ODQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20de', 'name': 'lang / de', 'color': '726DA8', 'default': False, 'description': 'German language data and models'}, {'id': 710446668, 'node_id': 'MDU6TGFiZWw3MTA0NDY2Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/training', 'name': 'training', 'color': '087EA6', 'default': False, 'description': 'Training and updating models'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-10-07 14:14:16+00:00,2,"How to extract ""MONEY"" using de_core_news_sm","As spaCy's ""de_core_news_sm"" doesn't support MONEY entity I am wondering if somebody has tried doing the same and could share some advice.

Specifically, if using the POS-tagger and searching for patterns is enough or if it would make sense to train the de_core_news_sm model to recognize MONEY entities?

I am very new to ML in general so I am hoping that somebody could give me some advice - despite the very broad question.


## Your Environment
* **spaCy version:** 2.0.12
* **Platform:** Darwin-16.7.0-x86_64-i386-64bit
* **Python version:** 3.6.5
* **Models:** de
",0
31,https://github.com/explosion/spaCy/issues/2827,2827,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 312256136, 'node_id': 'MDU6TGFiZWwzMTIyNTYxMzY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/docs', 'name': 'docs', 'color': '087EA6', 'default': False, 'description': 'Documentation and website'}, {'id': 881665727, 'node_id': 'MDU6TGFiZWw4ODE2NjU3Mjc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20textcat', 'name': 'feat / textcat', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Text Classifier'}]",closed,2018-10-08 02:20:44+00:00,3,Does spaCy text classifier take into account new words in the text and creates embeddings for them?,"Are pre-trained embeddings that are part of the spacy 'en' model used here. Further, are embeddings learned for new words from the dataset?

## Which page or section is this issue related to?
https://spacy.io/usage/training#section-textcat

Thanks
",0
33,https://github.com/explosion/spaCy/issues/2829,2829,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446605073, 'node_id': 'MDU6TGFiZWw0NDY2MDUwNzM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/windows', 'name': 'windows', 'color': '676F6D', 'default': False, 'description': 'Issues related to Windows'}]",closed,2018-10-08 08:14:31+00:00,2,Not able to install spacy in azure web app,"**Operating System: Windows
Python Version Used:3.6/64-bit
spaCy Version : spacy-2.0.12**
**Installation command : python -m pip install -U spacy**
**Error :** 

```
D:\home\python364x64>python -m pip install -U spacy
Collecting spacy
Using cached https://files.pythonhosted.org/packages/24/de/ac14cd453c98656d6738a5669f96a4ac7f668493d5e6b78227ac933c5fd4/spacy-2.0.12.tar.gz
Requirement already satisfied, skipping upgrade: numpy>=1.7 in d:\home\python364x64\lib\site-packages (from spacy) (1.15.2)
Collecting murmurhash<0.29,>=0.28 (from spacy)
  Using cached https://files.pythonhosted.org/packages/5e/31/c8c1ecafa44db30579c8c457ac7a0f819e8b1dbc3e58308394fff5ff9ba7/murmurhash-0.28.0.tar.gz
Collecting cymem<1.32,>=1.30 (from spacy)
  Using cached https://files.pythonhosted.org/packages/f8/9e/273fbea507de99166c11cd0cb3fde1ac01b5bc724d9a407a2f927ede91a1/cymem-1.31.2.tar.gz
Collecting preshed<2.0.0,>=1.0.0 (from spacy)
  Using cached https://files.pythonhosted.org/packages/be/fc/09684555ce0ee7086675e6be698e4efeb6d9b315fd5aa96bed347572282b/preshed-1.0.1.tar.gz
Collecting thinc<6.11.0,>=6.10.3 (from spacy)
  Using cached https://files.pythonhosted.org/packages/94/b1/47a88072d0a38b3594c0a638a62f9ef7c742b8b8a87f7b105f7ed720b14b/thinc-6.10.3.tar.gz
Collecting plac<1.0.0,>=0.9.6 (from spacy)
  Using cached https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl
Collecting ujson>=1.35 (from spacy)
  Using cached https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz
Collecting dill<0.3,>=0.2 (from spacy)
  Using cached https://files.pythonhosted.org/packages/6f/78/8b96476f4ae426db71c6e86a8e6a81407f015b34547e442291cd397b18f3/dill-0.2.8.2.tar.gz
Collecting regex==2017.4.5 (from spacy)
  Using cached https://files.pythonhosted.org/packages/ad/0b/c1c5781a707e6ea01bcf57d8ad3c42125260fca67ef79206ecaef04a8754/regex-2017.04.05-cp36-none-win_amd64.whl
Collecting requests<3.0.0,>=2.13.0 (from spacy)
  Using cached https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl
Collecting msgpack<1.0.0,>=0.5.6 (from thinc<6.11.0,>=6.10.3->spacy)
  Using cached https://files.pythonhosted.org/packages/04/81/c6363198f24ec1c56e5c48ce685cb532e175125adade0cdb181c8c5fea6e/msgpack-0.5.6-cp36-cp36m-win_amd64.whl
Collecting msgpack-numpy<1.0.0,>=0.4.1 (from thinc<6.11.0,>=6.10.3->spacy)
  Using cached https://files.pythonhosted.org/packages/bb/a9/2a28ef55c9b2c197d8531bb9c05adce2ba454d37ca8d26c1d421c4945b0d/msgpack_numpy-0.4.4.1-py2.py3-none-any.whl
Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.11.0,>=6.10.3->spacy)
  Using cached https://files.pythonhosted.org/packages/36/f4/9728ba01ccb2f55df9a5af029b48ba0aaca1081bbd7823ea2ee223ba7a42/cytoolz-0.9.0.1.tar.gz
Collecting wrapt<1.11.0,>=1.10.0 (from thinc<6.11.0,>=6.10.3->spacy)
  Using cached https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz
Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.11.0,>=6.10.3->spacy)
  Using cached https://files.pythonhosted.org/packages/79/43/19c9fee28110cd47f73e6bc596394337fe9f3e5825b4de402bbf30b3beb5/tqdm-4.26.0-py2.py3-none-any.whl
Collecting six<2.0.0,>=1.10.0 (from thinc<6.11.0,>=6.10.3->spacy)
  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl
Collecting pyreadline>=1.7.1 (from dill<0.3,>=0.2->spacy)
  Using cached https://files.pythonhosted.org/packages/bc/7c/d724ef1ec3ab2125f38a1d53285745445ec4a8f19b9bb0761b4064316679/pyreadline-2.1.zip
Collecting idna<2.8,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)
  Using cached https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl
Collecting chardet<3.1.0,>=3.0.2 (from requests<3.0.0,>=2.13.0->spacy)
  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl
Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in d:\home\python364x64\lib\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.1.18)
Collecting urllib3<1.24,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)
  Using cached https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl
Collecting toolz>=0.8.0 (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy)
  Using cached https://files.pythonhosted.org/packages/14/d0/a73c15bbeda3d2e7b381a36afb0d9cd770a9f4adc5d1532691013ba881db/toolz-0.9.0.tar.gz
Installing collected packages: murmurhash, cymem, preshed, msgpack, msgpack-numpy, toolz, cytoolz, wrapt, plac, tqdm, six, pyreadline, dill, thinc, ujson, regex, idna, chardet, urllib3, requests, spacy
  Running setup.py install for murmurhash: started
    Running setup.py install for murmurhash: finished with status 'error'
    Complete output from command D:\home\python364x64\python.exe -u -c ""import setuptools, tokenize;__file__='D:\\local\\Temp\\pip-install-xfwjvrhv\\murmurhash\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record D:\local\Temp\pip-record-grwmpuqk\install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.6
    creating build\lib.win-amd64-3.6\murmurhash
    copying murmurhash\about.py -> build\lib.win-amd64-3.6\murmurhash
    copying murmurhash\__init__.py -> build\lib.win-amd64-3.6\murmurhash
    creating build\lib.win-amd64-3.6\murmurhash\tests
    copying murmurhash\tests\test_import.py -> build\lib.win-amd64-3.6\murmurhash\tests
    copying murmurhash\tests\__init__.py -> build\lib.win-amd64-3.6\murmurhash\tests
    copying murmurhash\mrmr.pyx -> build\lib.win-amd64-3.6\murmurhash
    copying murmurhash\mrmr.pxd -> build\lib.win-amd64-3.6\murmurhash
    copying murmurhash\__init__.pxd -> build\lib.win-amd64-3.6\murmurhash
    creating build\lib.win-amd64-3.6\murmurhash\include
    creating build\lib.win-amd64-3.6\murmurhash\include\murmurhash
    copying murmurhash\include\murmurhash\MurmurHash2.h -> build\lib.win-amd64-3.6\murmurhash\include\murmurhash
    copying murmurhash\include\murmurhash\MurmurHash3.h -> build\lib.win-amd64-3.6\murmurhash\include\murmurhash
    running build_ext
    building 'murmurhash.mrmr' extension
    creating build\temp.win-amd64-3.6
    creating build\temp.win-amd64-3.6\Release
    creating build\temp.win-amd64-3.6\Release\murmurhash
    D:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\Bin\amd64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT -ID:\home\python364x64\include -ID:\local\Temp\pip-install-xfwjvrhv\murmurhash\murmurhash\include -ID:\home\python364x64\include -ID:\home\python364x64\include ""-ID:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\Include"" ""-ID:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\Include"" ""-ID:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt"" ""-ID:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" /EHsc /Tpmurmurhash/mrmr.cpp /Fobuild\temp.win-amd64-3.6\Release\murmurhash/mrmr.obj /Ox /EHsc
    fatal error C1510: Cannot load language resource clui.dll.
    error: command 'D:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\Bin\\amd64\\cl.exe' failed with exit status 4
    
    ----------------------------------------
Command ""D:\home\python364x64\python.exe -u -c ""import setuptools, tokenize;__file__='D:\\local\\Temp\\pip-install-xfwjvrhv\\murmurhash\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record D:\local\Temp\pip-record-grwmpuqk\install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in D:\local\Temp\pip-install-xfwjvrhv\murmurhash\
```",0
34,https://github.com/explosion/spaCy/issues/2831,2831,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}]",closed,2018-10-09 00:09:18+00:00,3,Python crash while enumerating noun_chunks with as_doc method on span objects,"## How to reproduce the behaviour

```python
import spacy
nlp = spacy.load('en_core_web_lg')
doc = nlp(u'Apple is a great fruit. But it can be debatable if it is the best.')

for sentence in doc.sents:
    for ent in sentence.as_doc().ents:
        print(ent.text, ent.label_)

    # Causes python kernel to crash
    for chunk in sentence.as_doc().noun_chunks:
        print(chunk.text)
```
Output:
Apple ORG
Apple
a great fruit
**--> Python crashes.**


## Your Environment
* spaCy version      2.0.11
* Platform           Windows-10-10.0.16299-SP0
* Python version     3.6.4
* Models             en_core_web_lg

* Operating System: Win 10
* Python Version Used: Python 3.6.4 |Anaconda, Inc.
* spaCy Version Used: 
* Environment Information: Jupyter notebook
",0
35,https://github.com/explosion/spaCy/issues/2832,2832,"[{'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}, {'id': 1025171819, 'node_id': 'MDU6TGFiZWwxMDI1MTcxODE5', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20memory', 'name': 'perf / memory', 'color': 'B35905', 'default': False, 'description': 'Performance: memory use'}]",closed,2018-10-09 12:04:06+00:00,3,"When i load a new Spacy model, the memory used by the old assignment does not get released","Hello,

I am not able to switch between different Spacy models and release the memory used by old assigned models at the same time.  The logic of my application imply that I can switch between different models at runtime so I cannot avoid the load of a new model at this stage. 
A simple example:

```
import spacy
import threading

def printhello():
  threading.Timer(10.0, printhello).start()
  print ""Hello, World!""

nlp = spacy.load('/model1')
nlp = spacy.load('/model2')

doc = nlp(u'This is a sentence.')
printhello()
```

Notice that the dummy function printhello() is used here only to keep the program running so that I can look at the memory usage in real time. 

By looking at the memory consuption on this example, I have noticed that the allocated memory is the sum of the two models used singularly. Moreover, I tried to use the 'del' command after the first assignment and to call gc.collect() but that didn't help. I even tried to set ""nlp = None"" after the first assignment.
Do you know why the memory related to the first model doesn't get released once I re-assign the nlp variable to a new model? Is there some logic that holds in memory references of unassigned models?

Thanks.
 
## Info about my environment:
 python -m spacy info --markdown

* **Python version:** 2.7.15
* **Platform:** Darwin-17.7.0-x86_64-i386-64bit
* **spaCy version:** 2.0.11
* **Models:** en

",0
36,https://github.com/explosion/spaCy/issues/2833,2833,"[{'id': 881666463, 'node_id': 'MDU6TGFiZWw4ODE2NjY0NjM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20serialize', 'name': 'feat / serialize', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Serialization, saving and loading'}, {'id': 906542982, 'node_id': 'MDU6TGFiZWw5MDY1NDI5ODI=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20doc', 'name': 'feat / doc', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Doc, Span and Token objects'}]",closed,2018-10-09 17:52:51+00:00,4,Unable to pickle a Token,"I'm unable to serialize `Token`s. I think this used to work in an earlier version, since I get this exception from a friend's code that used to work fine. I suspect issue #2819 is a manifestation of the same problem, but with Spark wrapping the error message.

## How to reproduce the behavior

```
import spacy, pickle
nlp = spacy.load('en')
doc = nlp(""Hello, world"")

with open(""test_pckl"", ""wb"") as fh:
    pickle.dump(doc[0], fh)
```

## Your Environment

* **spaCy version:** 2.0.12
* **Platform:** Linux-4.10.0-28-generic-x86_64-with-debian-stretch-sid
* **Python version:** 3.6.5
* **Models:** en
* **pickleshare version:** 0.7.5 (also tested with 0.7.4)
",0
37,https://github.com/explosion/spaCy/issues/2834,2834,"[{'id': 514165920, 'node_id': 'MDU6TGFiZWw1MTQxNjU5MjA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20en', 'name': 'lang / en', 'color': '726DA8', 'default': False, 'description': 'English language data and models'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 881665874, 'node_id': 'MDU6TGFiZWw4ODE2NjU4NzQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20tagger', 'name': 'feat / tagger', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Part-of-speech tagger'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2018-10-09 19:01:44+00:00,4,'x' is tagged PUNCT,"
Displacy tags `x` in `What are some Spanish words that start with x?` as `PUNCT`.

https://explosion.ai/demos/displacy?text=What%20are%20some%20Spanish%20words%20that%20start%20with%20x%3F&model=en_core_web_sm&cpu=0&cph=0

<img width=""1101"" alt=""screen shot 2018-10-09 at 23 00 23"" src=""https://user-images.githubusercontent.com/11457984/46691987-2ad95380-cc17-11e8-962b-bfbf392b1ab9.png"">

",0
38,https://github.com/explosion/spaCy/issues/2835,2835,"[{'id': 881718446, 'node_id': 'MDU6TGFiZWw4ODE3MTg0NDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20tokenizer', 'name': 'feat / tokenizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Tokenizer'}, {'id': 1025171280, 'node_id': 'MDU6TGFiZWwxMDI1MTcxMjgw', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20speed', 'name': 'perf / speed', 'color': 'B35905', 'default': False, 'description': 'Performance: speed'}]",closed,2018-10-09 19:23:31+00:00,4,Spacy tokenizer hangs,"The following lines of code hangs, I tried debugging it but the code goes through tokenizer.pyx which seems to hang, as other sentences/docs seem to go through tests for lower, prefix, suffix and so forth in en\lex_attrs.py
```
import spacy
nlp = spacy.load('en')
nlp('oow.jspsearch.eventoracleopenworldsearch.technologyoraclesolarissearch.technologystoragesearch.technologylinuxsearch.technologyserverssearch.technologyvirtualizationsearch.technologyengineeredsystemspcodewwmkmppscem:')
```

The result of this is a hung process, note the process is still using CPU.",0
39,https://github.com/explosion/spaCy/issues/2837,2837,"[{'id': 312256136, 'node_id': 'MDU6TGFiZWwzMTIyNTYxMzY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/docs', 'name': 'docs', 'color': '087EA6', 'default': False, 'description': 'Documentation and website'}, {'id': 881666568, 'node_id': 'MDU6TGFiZWw4ODE2NjY1Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20pipeline', 'name': 'feat / pipeline', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Processing pipeline and components'}]",closed,2018-10-10 15:10:45+00:00,2,More in depth custom pipeline docs or examples,"Hi, I'd appreciate some more in depth docs on creating custom pipelines. More specifically custom pipelines that actually use some for of machine or deep learning technology. So that would require pipeline with some training and prediction capabilities (where the current examples I found only do predict). Perhaps drilling down on one of the existing thinc pipelines (for exampe the EntityRecognizer) would work? Or are there already examples anywhere on such pipelines that can get me started?
",0
40,https://github.com/explosion/spaCy/issues/2838,2838,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 906542982, 'node_id': 'MDU6TGFiZWw5MDY1NDI5ODI=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20doc', 'name': 'feat / doc', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Doc, Span and Token objects'}]",closed,2018-10-10 19:43:34+00:00,5,Split one token into several,"## Feature description
We now have a feature to merge tokens into one, implemented in the retokenizer. We're lacking the reverse feature: split one token into several. An API exist in _retokenize, but is not implemented yet.


",0
41,https://github.com/explosion/spaCy/issues/2839,2839,"[{'id': 312256136, 'node_id': 'MDU6TGFiZWwzMTIyNTYxMzY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/docs', 'name': 'docs', 'color': '087EA6', 'default': False, 'description': 'Documentation and website'}, {'id': 785817829, 'node_id': 'MDU6TGFiZWw3ODU4MTc4Mjk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/gpu', 'name': 'gpu', 'color': '676F6D', 'default': False, 'description': 'Using spaCy on GPU'}]",closed,2018-10-10 20:15:27+00:00,2,"Cuda9, Cuda10 installation doc. Outdated CUDA9 env variable?","The docs refer to setting the environment variable `CUDA9=1` for Cuda installation here: https://github.com/explosion/spaCy/blob/4cd9ec0f00788d08c053b885b5591fe134666a65/website/usage/_install/_instructions.jade#L83-L87

Related Cuda 9 installation issue from a year ago: https://github.com/explosion/spaCy/issues/1530#issuecomment-343213919

Checking on master at the moment (with the notoriously approximative Github search though) I don't see any reference of this flag in the code so I suppose this line can be dropped from the documentation.

I've also compiled it on my machine without the flag on Cuda10 without issue. 
",0
42,https://github.com/explosion/spaCy/issues/2840,2840,"[{'id': 312256136, 'node_id': 'MDU6TGFiZWwzMTIyNTYxMzY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/docs', 'name': 'docs', 'color': '087EA6', 'default': False, 'description': 'Documentation and website'}]",closed,2018-10-11 16:02:02+00:00,2,Juniper use outside of Spacy,"Hey all - we worked with @ines a while back to help get Juniper working with Binder so that Spacy's awesome landing page could run kernels interactively. It seems like this could be generally useful outside of the spacy documentation! I just took a pass at getting Juniper to work with a different site (running locally on Jekyll), and ran into the following error:

```
Failed to load https://hub.mybinder.org/user/binder-examples-jupyterlab-37zvnqwv/api/kernels?1539273084123: Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource.
```

I went to the Juniper repo (https://github.com/ines/juniper) but found that issues weren't enabled. So, I have a couple questions:

* First - is Juniper planning on being maintained as an open project for use outside of Spacy?
* Second - if so, any reason issues aren't allowed there? Should issues just be opened here?
* Third - if this is the right place for this issue, @ines any ideas on this ""Access-Control-Allow-Origin"" problem?",0
43,https://github.com/explosion/spaCy/issues/2841,2841,"[{'id': 312256136, 'node_id': 'MDU6TGFiZWwzMTIyNTYxMzY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/docs', 'name': 'docs', 'color': '087EA6', 'default': False, 'description': 'Documentation and website'}]",closed,2018-10-12 07:36:16+00:00,1,The link on doc is broken.,"<!-- Describe the problem or suggestion here. If you've found a mistake and you know the answer, feel free to submit a pull request straight away: https://github.com/explosion/spaCy/pulls -->

The link is broken.

## Which page or section is this issue related to?
<!-- Please include the URL and/or source. -->
https://spacy.io/usage/#source-windows
![image](https://user-images.githubusercontent.com/4702353/46854758-79176e00-ce34-11e8-9ac8-933333d41c07.png)
",0
61,https://github.com/explosion/spaCy/issues/2864,2864,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-10-19 00:16:31+00:00,2,No result for doc.ents,"## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

I am working on spacy en model on windows 10. I am working on jupyter notebook and I am unable to see any results when I run 'doc.ents'. When I am runnning the command 'python -m spacy info --markdown' I am seeing that even after getting a message about successfully linking en model to spacy I am not getting any list of models linked to my spacy.

for ent in doc.ents:
    print (""{} is of entity type {}"".format(ent.label_, ent.texxt) )
## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows 10
* Python Version Used: 3.6.5
* spaCy Version Used: 2.0.12
* Environment Information: 
",0
62,https://github.com/explosion/spaCy/issues/2868,2868,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}]",closed,2018-10-19 15:15:14+00:00,6,TypeError: __init__() got an unexpected keyword argument 'encoding',"## How to reproduce the behaviour
I can get the result if I'm using `displacy.render(doc, style='ent', jupyter=True)` but once I change the style to 'dep', it gives me an error message.

`import spacy
import en_core_web_sm
from spacy import displacy

nlp_sm = spacy.load('en_core_web_sm')
doc = nlp_sm(u'this is a sentence.')
displacy.render(doc, style='dep', jupyter=True)`


TypeError                                 Traceback (most recent call last)
<ipython-input-48-83f172436f08> in <module>()
----> 1 displacy.render(doc, style='dep', jupyter=True)

C:\ProgramData\Anaconda3\lib\site-packages\spacy\displacy\__init__.py in render(docs, style, page, minify, jupyter, options, manual)
     37     renderer, converter = factories[style]
     38     renderer = renderer(options=options)
---> 39     parsed = [converter(doc, options) for doc in docs] if not manual else docs
     40     _html['parsed'] = renderer.render(parsed, page=page, minify=minify).strip()
     41     html = _html['parsed']

C:\ProgramData\Anaconda3\lib\site-packages\spacy\displacy\__init__.py in <listcomp>(.0)
     37     renderer, converter = factories[style]
     38     renderer = renderer(options=options)
---> 39     parsed = [converter(doc, options) for doc in docs] if not manual else docs
     40     _html['parsed'] = renderer.render(parsed, page=page, minify=minify).strip()
     41     html = _html['parsed']

C:\ProgramData\Anaconda3\lib\site-packages\spacy\displacy\__init__.py in parse_deps(orig_doc, options)
     87     RETURNS (dict): Generated dependency parse keyed by words and arcs.
     88     """"""
---> 89     doc = Doc(orig_doc.vocab).from_bytes(orig_doc.to_bytes())
     90     if not doc.is_parsed:
     91         user_warning(Warnings.W005)

doc.pyx in spacy.tokens.doc.Doc.to_bytes()

C:\ProgramData\Anaconda3\lib\site-packages\spacy\util.py in to_bytes(getters, exclude)
    484         if key not in exclude:
    485             serialized[key] = getter()
--> 486     return msgpack.dumps(serialized, use_bin_type=True, encoding='utf8')
    487 
    488 

C:\ProgramData\Anaconda3\lib\site-packages\msgpack_numpy.py in packb(o, **kwargs)
    194     """"""
    195 
--> 196     return Packer(**kwargs).pack(o)
    197 
    198 def unpack(stream, **kwargs):

TypeError: __init__() got an unexpected keyword argument 'encoding'


## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows 7
* Python Version Used: 3.6
* spaCy Version Used: v2.0.12 (conda install)
* Environment Information: 
",0
63,https://github.com/explosion/spaCy/issues/2869,2869,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 906542982, 'node_id': 'MDU6TGFiZWw5MDY1NDI5ODI=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20doc', 'name': 'feat / doc', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Doc, Span and Token objects'}]",closed,2018-10-19 18:38:25+00:00,3,is_sent_start on first document token should be True instead of None,"Hey everyone,

From the documentation of **Token.is_sent_start**:
""... It also now returns None if the answer is unknown, and _fixes a quirk in the old logic that would always set the property to 0 for the first word of the document_.""

The value of is_sent_start for the first token in a document is now always **None** instead of 0. Is this the intended behavior? I would argue that **True** would be more reasonable and consistent. At least it caused some issues to my code when I concatenated several documents and the is_sent_start flag on each first document word was missing.

## How to reproduce the behaviour
```
import spacy
nlp = spacy.load('en')
text = ""This is the first sentence. This is the second sentence.""
doc = nlp(text)
for token in doc:
    print(token.i, token.is_sent_start, token.text)
```

## Info about spaCy
* **spaCy version:** 2.0.16
* **Platform:** Linux-4.13.0-38-generic-x86_64-with-debian-stretch-sid
* **Python version:** 3.7.0
* **Models:** en, de

Kind regards,
Andreas",0
64,https://github.com/explosion/spaCy/issues/2870,2870,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-10-22 00:08:12+00:00,6,newline \n captured in the entity parser?,"Hello there!

I am more and more excited by `spacy`, but I found some weird behavior in this example

```
doc = nlp(u'''This is some crazy test where I dont need an Apple
               Watch to make things bug''')    
      
for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)    
    
(u'Apple', 45, 50, u'ORG')
(u'\n           Watch', 50, 67, u'PERSON')

```
Why is the `newline` even added to the output here? In other examples, I had the newline itself parsed as a `GPE` entity such as
```

(u'\n', 1237, 1238, u'GPE')
(u'\n', 1293, 1294, u'GPE')
```

I am confused here. Is that expected? 
Thanks!",0
65,https://github.com/explosion/spaCy/issues/2871,2871,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}]",closed,2018-10-22 14:59:22+00:00,3,Mismatch between token rank and vocab vector find.,"I have found a discrepancy between a token's `rank` and the lookup using `tokenizer.vocab.vectors.find()`, with the word ""SUFFIX"" (all caps). Furthermore, the index returned by `.rank` is causing range error in a tensorflow model trained on spaCy word vectors.

## How to reproduce the behaviour
```
>>> import spacy
>>> nlp = spacy.load('en_core_web_lg')
>>> nlp('SUFFIX')[0].rank
684829
>>> nlp.tokenizer.vocab.vectors.find(key='SUFFIX')
-1
>>> nlp('suffix')[0].rank
31698
>>> nlp.tokenizer.vocab.vectors.find(key='suffix')
31698
>>> spacy.__version__
'2.0.12'
>>> en_core_web_lg.__version__
'2.0.0'
>>> 
```

The bug is in lines 3 and 4. The lowercase version matches, as expected. This was discovered when a tensorflow model threw the following error, indicating that this is an invalid rank:

```
StatusCode.INVALID_ARGUMENT, indices[0,6] = 684829 is not in [0, 684824)
```


## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: OSX 10.13.6
* Python Version Used: 3.6.6
* spaCy Version Used: 2.0.12
* spaCy Model Version: en_core_web_lg 2.0.0
",0
66,https://github.com/explosion/spaCy/issues/2872,2872,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881718446, 'node_id': 'MDU6TGFiZWw4ODE3MTg0NDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20tokenizer', 'name': 'feat / tokenizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Tokenizer'}]",closed,2018-10-22 16:09:20+00:00,4,Adding token match to custom tokenizer affects other token rules,"## How to reproduce the behaviour
It's possible I'm not understanding the tokenizer code and this is expected behavior, in which case I'll close, but this was a little different than how I was expecting things to work. Suppose I want to add a custom token match to always keep hyphenated words together as one token. I can do that by creating my own tokenizer like so:

```
import re
import spacy
from spacy.tokenizer import Tokenizer

custom_nlp = spacy.load('en')

hyphen_re = re.compile(r'[A-Za-z\d]+-[A-Za-z\d]+')

prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)
infix_re = spacy.util.compile_infix_regex(custom_nlp.Defaults.infixes)
suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)
custom_nlp.tokenizer = Tokenizer(custom_nlp.vocab, prefix_search=prefix_re.search, infix_finditer=infix_re.finditer,
                         suffix_search=suffix_re.search, token_match=hyphen_re.match)
```

The hyphenated words are tokenized correctly, but contractions with an apostrophe are no longer being split into separate tokens even though they shouldn't be caught by the token_match regex:

```
test_string = u""some-hyphenated words and some not: don't, won't, I'll""

print [t.text for t in custom_nlp(test_string)]
>>> [u'some-hyphenated',
 u'words',
 u'and',
 u'some',
 u'not',
 u':',
 u""don't"",
 u',',
 u""won't"",
 u',',
 u""I'll""]

# compared to the default behavior:
nlp = spacy.load('en')
print [t.text for t in nlp(test_string)]
>>> [u'some',
 u'-',
 u'hyphenated',
 u'words',
 u'and',
 u'some',
 u'not',
 u':',
 u'do',
 u""n't"",
 u',',
 u'wo',
 u""n't"",
 u',',
 u'I',
 u""'ll""]
```
If this is expected behavior, is there a way to _add_ custom token rules to the spacy defaults rather than _replace_ them, so that other tokenization rules are unaffected? Thanks!

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: MacOS High Sierra 10.13.6
* Python Version Used: 2.7
* spaCy Version Used: 2.0.11
* Environment Information: 
",0
67,https://github.com/explosion/spaCy/issues/2873,2873,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2018-10-23 23:01:52+00:00,2,Spacy model linking fails.,"Summary
--------------
This is a bug during install. It is related to issue [1138](https://github.com/explosion/spaCy/issues/1138). The issue does not seem to be resolved. The model is included in the requirements.txt, but linking fails. The linking only happens when trying to manually do a `python -m spacy download en`. Why is this not good? My CI/CD pipeline breaks.

Output during pip install:
---------------------------------
pip install -r requirements.txt 
Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#en_core_web_sm (from -r requirements.txt (line 88))
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)
    100% |笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 37.4MB 1.4MB/s 
Collecting absl-py==0.5.0 (from -r requirements.txt (line 1))
Collecting APScheduler==3.5.3 (from -r requirements.txt (line 2))
  Using cached https://files.pythonhosted.org/packages/97/3a/fa3213cc325091b7729616594611fff31d72c2d4d590418c3efdf7424ae2/APScheduler-3.5.3-py2.py3-none-any.whl
Collecting astor==0.7.1 (from -r requirements.txt (line 3))
  Using cached https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl
Collecting attrs==18.2.0 (from -r requirements.txt (line 4))
  Using cached https://files.pythonhosted.org/packages/3a/e1/5f9023cc983f1a628a8c2fd051ad19e76ff7b142a0faf329336f9a62a514/attrs-18.2.0-py2.py3-none-any.whl
Collecting Automat==0.7.0 (from -r requirements.txt (line 5))
  Using cached https://files.pythonhosted.org/packages/a3/86/14c16bb98a5a3542ed8fed5d74fb064a902de3bdd98d6584b34553353c45/Automat-0.7.0-py2.py3-none-any.whl
Collecting bleach==1.5.0 (from -r requirements.txt (line 6))
  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl
Collecting boto3==1.9.8 (from -r requirements.txt (line 7))
  Using cached https://files.pythonhosted.org/packages/38/aa/c100449bc8550c52b3f14e3b3831c2b5db6a4b7b752b7a11e76599508fc4/boto3-1.9.8-py2.py3-none-any.whl
Collecting botocore==1.12.16 (from -r requirements.txt (line 8))
  Downloading https://files.pythonhosted.org/packages/87/28/f24899d31a4c0da5e9c0c70a481503633f7b1e94c0671398546bc2241400/botocore-1.12.16-py2.py3-none-any.whl (4.7MB)
    100% |笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 4.7MB 7.0MB/s 
Collecting certifi==2018.8.24 (from -r requirements.txt (line 9))
  Using cached https://files.pythonhosted.org/packages/df/f7/04fee6ac349e915b82171f8e23cee63644d83663b34c539f7a09aed18f9e/certifi-2018.8.24-py2.py3-none-any.whl
Collecting chardet==3.0.4 (from -r requirements.txt (line 10))
  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl
Collecting click==6.7 (from -r requirements.txt (line 11))
  Using cached https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl
Collecting cloudpickle==0.5.6 (from -r requirements.txt (line 12))
  Using cached https://files.pythonhosted.org/packages/98/d6/a78a4589234cc6f47f29665c1225f30467db5fdaf4ca1fb52b0685bff108/cloudpickle-0.5.6-py2.py3-none-any.whl
Collecting coloredlogs==10.0 (from -r requirements.txt (line 13))
  Using cached https://files.pythonhosted.org/packages/08/0f/7877fc42fff0b9d70b6442df62d53b3868d3a6ad1b876bdb54335b30ff23/coloredlogs-10.0-py2.py3-none-any.whl
Collecting colorhash==1.0.2 (from -r requirements.txt (line 14))
  Using cached https://files.pythonhosted.org/packages/0e/e1/50dbc513aa74e99eca4c47f2a8206711f0bec436fdddd95eebaf7eaaa1aa/colorhash-1.0.2-py2.py3-none-any.whl
Collecting ConfigArgParse==0.13.0 (from -r requirements.txt (line 15))
Collecting constantly==15.1.0 (from -r requirements.txt (line 16))
  Using cached https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl
Collecting cycler==0.10.0 (from -r requirements.txt (line 17))
  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl
Collecting cymem<2.1.0,>=2.0.2 (from -r requirements.txt (line 18))
  Downloading https://files.pythonhosted.org/packages/6b/d5/c1583c90023608e71ee35b6943d2a5dc488d463b84ecd1c0fddbf23eed44/cymem-2.0.2-cp35-cp35m-manylinux1_x86_64.whl
Collecting cytoolz==0.9.0.1 (from -r requirements.txt (line 19))
Collecting decorator==4.3.0 (from -r requirements.txt (line 20))
  Using cached https://files.pythonhosted.org/packages/bc/bb/a24838832ba35baf52f32ab1a49b906b5f82fb7c76b2f6a7e35e140bac30/decorator-4.3.0-py2.py3-none-any.whl
Collecting dill==0.2.8.2 (from -r requirements.txt (line 21))
Collecting docopt==0.6.2 (from -r requirements.txt (line 22))
Collecting docutils==0.14 (from -r requirements.txt (line 23))
  Using cached https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl
Collecting fakeredis==0.10.3 (from -r requirements.txt (line 24))
  Using cached https://files.pythonhosted.org/packages/64/bd/2756ddf350c4bb308e3255f9dcd6610f8b01344947bf74d5d166dc66b0a2/fakeredis-0.10.3-py2.py3-none-any.whl
Collecting fbmessenger==5.3.0 (from -r requirements.txt (line 25))
  Using cached https://files.pythonhosted.org/packages/05/37/c99c7148b311ad69f82ef5bde07f605568dcf1a78688914cadac53ba23d0/fbmessenger-5.3.0-py2.py3-none-any.whl
Collecting Flask==1.0.2 (from -r requirements.txt (line 26))
  Using cached https://files.pythonhosted.org/packages/7f/e7/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b/Flask-1.0.2-py2.py3-none-any.whl
Collecting Flask-Cors==3.0.6 (from -r requirements.txt (line 27))
  Using cached https://files.pythonhosted.org/packages/d1/db/f3495569d5c3e2bdb9fb8a66c54503364abb6f35a9da2227cf5c9c50dc42/Flask_Cors-3.0.6-py2.py3-none-any.whl
Collecting future==0.16.0 (from -r requirements.txt (line 28))
Collecting gast==0.2.0 (from -r requirements.txt (line 29))
Collecting gevent==1.3.6 (from -r requirements.txt (line 30))
  Using cached https://files.pythonhosted.org/packages/a9/33/1fe2c8b3f9b6d0b667e42c8902beb6d944149e0330f564901d50fec76e41/gevent-1.3.6-cp35-cp35m-manylinux1_x86_64.whl
Collecting graphviz==0.8.4 (from -r requirements.txt (line 31))
  Using cached https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl
Collecting greenlet==0.4.15 (from -r requirements.txt (line 32))
  Using cached https://files.pythonhosted.org/packages/2e/65/27f35497cc0102a792390d056e793e064da95fc9eae45d75ae0ba49c0a0d/greenlet-0.4.15-cp35-cp35m-manylinux1_x86_64.whl
Collecting grpcio==1.15.0 (from -r requirements.txt (line 33))
  Using cached https://files.pythonhosted.org/packages/2d/2d/7d9cfd7b3fe8cb0281ea204f5f721749a4bfb048654f389887b82a295588/grpcio-1.15.0-cp35-cp35m-manylinux1_x86_64.whl
Collecting h5py==2.8.0 (from -r requirements.txt (line 34))
  Using cached https://files.pythonhosted.org/packages/d9/0a/f0dd6d533d6b5bd4c1ca186af2792186885a90b84df41f3e6867466761fc/h5py-2.8.0-cp35-cp35m-manylinux1_x86_64.whl
Collecting html5lib==0.9999999 (from -r requirements.txt (line 35))
Collecting humanfriendly==4.16.1 (from -r requirements.txt (line 36))
  Using cached https://files.pythonhosted.org/packages/fb/34/1e80e4a06020f7dfc7eee11387e400435236f992f80eaa6ed9b3d8252211/humanfriendly-4.16.1-py2.py3-none-any.whl
Collecting hyperlink==18.0.0 (from -r requirements.txt (line 37))
  Using cached https://files.pythonhosted.org/packages/a7/b6/84d0c863ff81e8e7de87cff3bd8fd8f1054c227ce09af1b679a8b17a9274/hyperlink-18.0.0-py2.py3-none-any.whl
Collecting idna==2.7 (from -r requirements.txt (line 38))
  Using cached https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl
Collecting incremental==17.5.0 (from -r requirements.txt (line 39))
  Using cached https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl
Collecting itsdangerous==0.24 (from -r requirements.txt (line 40))
Collecting Jinja2==2.10 (from -r requirements.txt (line 41))
  Using cached https://files.pythonhosted.org/packages/7f/ff/ae64bacdfc95f27a016a7bed8e8686763ba4d277a78ca76f32659220a731/Jinja2-2.10-py2.py3-none-any.whl
Collecting jmespath==0.9.3 (from -r requirements.txt (line 42))
  Using cached https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl
Collecting jsonpickle==0.9.6 (from -r requirements.txt (line 43))
Collecting jsonschema==2.6.0 (from -r requirements.txt (line 44))
  Using cached https://files.pythonhosted.org/packages/77/de/47e35a97b2b05c2fadbec67d44cfcdcd09b8086951b331d82de90d2912da/jsonschema-2.6.0-py2.py3-none-any.whl
Collecting Keras==2.2.2 (from -r requirements.txt (line 45))
  Using cached https://files.pythonhosted.org/packages/34/7d/b1dedde8af99bd82f20ed7e9697aac0597de3049b1f786aa2aac3b9bd4da/Keras-2.2.2-py2.py3-none-any.whl
Collecting Keras-Applications==1.0.4 (from -r requirements.txt (line 46))
  Using cached https://files.pythonhosted.org/packages/54/90/8f327deaa37a71caddb59b7b4aaa9d4b3e90c0e76f8c2d1572005278ddc5/Keras_Applications-1.0.4-py2.py3-none-any.whl
Collecting Keras-Preprocessing==1.0.2 (from -r requirements.txt (line 47))
  Using cached https://files.pythonhosted.org/packages/71/26/1e778ebd737032749824d5cba7dbd3b0cf9234b87ab5ec79f5f0403ca7e9/Keras_Preprocessing-1.0.2-py2.py3-none-any.whl
Collecting kiwisolver==1.0.1 (from -r requirements.txt (line 48))
  Using cached https://files.pythonhosted.org/packages/7e/31/d6fedd4fb2c94755cd101191e581af30e1650ccce7a35bddb7930fed6574/kiwisolver-1.0.1-cp35-cp35m-manylinux1_x86_64.whl
Collecting klein==17.10.0 (from -r requirements.txt (line 49))
  Using cached https://files.pythonhosted.org/packages/8a/6b/adc97a7bb3fb781fdd9e49177ad873c1479f87b9745271cbeda81cbb9cc8/klein-17.10.0-py2.py3-none-any.whl
Collecting Markdown==2.6.11 (from -r requirements.txt (line 50))
  Using cached https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl
Collecting MarkupSafe==1.0 (from -r requirements.txt (line 51))
Collecting matplotlib==2.2.3 (from -r requirements.txt (line 52))
  Using cached https://files.pythonhosted.org/packages/de/af/6258db9b26313dd7ad70dba30a60bec62bf030a44208d4cb62966206666f/matplotlib-2.2.3-cp35-cp35m-manylinux1_x86_64.whl
Collecting mattermostwrapper==2.1 (from -r requirements.txt (line 53))
  Using cached https://files.pythonhosted.org/packages/93/70/203660597d12788e958dd691aa11c3c29caa075eadb2ce94d2eb53099d1b/mattermostwrapper-2.1-py2.py3-none-any.whl
Collecting msgpack==0.5.6 (from -r requirements.txt (line 54))
  Using cached https://files.pythonhosted.org/packages/08/72/5a01d2a6a894e7f6966b0038445c748d7a16754cceb0e988699269d8152a/msgpack-0.5.6-cp35-cp35m-manylinux1_x86_64.whl
Collecting msgpack-numpy==0.4.3.2 (from -r requirements.txt (line 55))
  Using cached https://files.pythonhosted.org/packages/ad/45/464be6da85b5ca893cfcbd5de3b31a6710f636ccb8521b17bd4110a08d94/msgpack_numpy-0.4.3.2-py2.py3-none-any.whl
Collecting murmurhash==0.28.0 (from -r requirements.txt (line 56))
  Downloading https://files.pythonhosted.org/packages/50/b1/b5356c5ec4b2a66ab41bd2689830615c8b8a983c005daf52bcf7d8a9d647/murmurhash-0.28.0-cp35-cp35m-manylinux1_x86_64.whl
Collecting networkx==2.2 (from -r requirements.txt (line 57))
Collecting numpy==1.15.1 (from -r requirements.txt (line 58))
  Using cached https://files.pythonhosted.org/packages/0a/fa/afc1dc818589c9fd36a53f78999f2b5bd88bd5b167eb7d87fb56b565c185/numpy-1.15.1-cp35-cp35m-manylinux1_x86_64.whl
Collecting packaging==17.1 (from -r requirements.txt (line 59))
  Using cached https://files.pythonhosted.org/packages/ad/c2/b500ea05d5f9f361a562f089fc91f77ed3b4783e13a08a3daf82069b1224/packaging-17.1-py2.py3-none-any.whl
Collecting pathlib==1.0.1 (from -r requirements.txt (line 60))
Collecting plac==0.9.6 (from -r requirements.txt (line 61))
  Using cached https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl
Collecting preshed<2.1.0,>=2.0.1 (from -r requirements.txt (line 62))
  Downloading https://files.pythonhosted.org/packages/c5/e5/705d59b08fa3e5ba33d2371f6d52c1993539d5b3beb4130c40bb4e540706/preshed-2.0.1-cp35-cp35m-manylinux1_x86_64.whl (82kB)
    100% |笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 92kB 11.5MB/s 
Collecting protobuf==3.6.1 (from -r requirements.txt (line 63))
  Using cached https://files.pythonhosted.org/packages/bf/d4/db7296a1407cad69f043537ba1e05afab3646451a066ead7a314d8714388/protobuf-3.6.1-cp35-cp35m-manylinux1_x86_64.whl
Collecting PyHamcrest==1.9.0 (from -r requirements.txt (line 64))
  Using cached https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl
Collecting PyJWT==1.6.4 (from -r requirements.txt (line 65))
  Using cached https://files.pythonhosted.org/packages/93/d1/3378cc8184a6524dc92993090ee8b4c03847c567e298305d6cf86987e005/PyJWT-1.6.4-py2.py3-none-any.whl
Collecting pykwalify==1.6.0 (from -r requirements.txt (line 66))
  Using cached https://files.pythonhosted.org/packages/6e/59/55c32d59b462a9c3fae2ab25f179f91d61617c0215e9f8ba3f4966b2b8b1/pykwalify-1.6.0-py2.py3-none-any.whl
Collecting pyparsing==2.2.1 (from -r requirements.txt (line 67))
  Using cached https://files.pythonhosted.org/packages/42/47/e6d51aef3d0393f7d343592d63a73beee2a8d3d69c22b053e252c6cfacd5/pyparsing-2.2.1-py2.py3-none-any.whl
Collecting PySocks==1.6.8 (from -r requirements.txt (line 68))
Collecting python-crfsuite==0.9.6 (from -r requirements.txt (line 69))
  Using cached https://files.pythonhosted.org/packages/4d/2c/274b89d009bb019038feaf96abb263d7cbff069a0771841013cf1832156e/python_crfsuite-0.9.6-cp35-cp35m-manylinux1_x86_64.whl
Collecting python-dateutil==2.7.3 (from -r requirements.txt (line 70))
  Using cached https://files.pythonhosted.org/packages/cf/f5/af2b09c957ace60dcfac112b669c45c8c97e32f94aa8b56da4c6d1682825/python_dateutil-2.7.3-py2.py3-none-any.whl
Collecting python-telegram-bot==10.1.0 (from -r requirements.txt (line 71))
  Using cached https://files.pythonhosted.org/packages/f1/51/d1bd383522c12b313eddd7b97b8e7d6cd2a8e3b44b8ff3c88e4a7b045cc8/python_telegram_bot-10.1.0-py2.py3-none-any.whl
Collecting pytz==2018.5 (from -r requirements.txt (line 72))
  Using cached https://files.pythonhosted.org/packages/30/4e/27c34b62430286c6d59177a0842ed90dc789ce5d1ed740887653b898779a/pytz-2018.5-py2.py3-none-any.whl
Collecting PyYAML==3.13 (from -r requirements.txt (line 73))
Collecting rasa-core==0.10.3 (from -r requirements.txt (line 74))
  Using cached https://files.pythonhosted.org/packages/7a/8e/0066253d669183f6cee3b5a09a6d5d5c3589d7c1c238747447b60e76e3cb/rasa_core-0.10.3-py2.py3-none-any.whl
Collecting rasa-nlu==0.13.4 (from -r requirements.txt (line 75))
  Using cached https://files.pythonhosted.org/packages/98/10/04b19b2d29efc047a894b6ac3de7227839cab77afa2d68970a2dcb34d284/rasa_nlu-0.13.4-py2.py3-none-any.whl
Collecting redis==2.10.6 (from -r requirements.txt (line 76))
  Using cached https://files.pythonhosted.org/packages/3b/f6/7a76333cf0b9251ecf49efff635015171843d9b977e4ffcf59f9c4428052/redis-2.10.6-py2.py3-none-any.whl
Collecting regex==2018.01.10 (from -r requirements.txt (line 77))
  Using cached https://files.pythonhosted.org/packages/76/f4/7146c3812f96fcaaf2d06ff6862582302626a59011ccb6f2833bb38d80f7/regex-2018.01.10.tar.gz
Collecting requests==2.19.1 (from -r requirements.txt (line 78))
  Using cached https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl
Collecting ruamel.yaml==0.15.69 (from -r requirements.txt (line 79))
  Using cached https://files.pythonhosted.org/packages/f8/62/231740df82cfcbe65deae6f5f3ae41f533ed50159be5783a1581e373a767/ruamel.yaml-0.15.69-cp35-cp35m-manylinux1_x86_64.whl
Collecting s3transfer==0.1.13 (from -r requirements.txt (line 80))
  Using cached https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl
Collecting scikit-learn==0.19.2 (from -r requirements.txt (line 81))
  Using cached https://files.pythonhosted.org/packages/b6/e2/a1e254a4a4598588d4fe88b45ab88a226c289ecfd0f6c90474eb6a9ea6b3/scikit_learn-0.19.2-cp35-cp35m-manylinux1_x86_64.whl
Collecting scipy==1.1.0 (from -r requirements.txt (line 82))
  Using cached https://files.pythonhosted.org/packages/cd/32/5196b64476bd41d596a8aba43506e2403e019c90e1a3dfc21d51b83db5a6/scipy-1.1.0-cp35-cp35m-manylinux1_x86_64.whl
Collecting simplejson==3.16.0 (from -r requirements.txt (line 83))
Collecting six==1.11.0 (from -r requirements.txt (line 84))
  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl
Collecting sklearn-crfsuite==0.3.6 (from -r requirements.txt (line 85))
  Using cached https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl
Collecting slackclient==1.3.0 (from -r requirements.txt (line 86))
  Using cached https://files.pythonhosted.org/packages/0d/2f/1378e64a843a5a8a83d73caa59ac88c36c67e2b41ac0fab3422080ff13bd/slackclient-1.3.0-py2.py3-none-any.whl
Collecting spacy<3.0.0,>=2.0.0 (from -r requirements.txt (line 87))
  Downloading https://files.pythonhosted.org/packages/a6/71/9a127727e6130e10b84585d22f082b42f15cdb444d3604eb79c7bf249d39/spacy-2.0.16-cp35-cp35m-manylinux1_x86_64.whl (23.2MB)
    100% |笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 23.2MB 2.8MB/s 
Collecting tabulate==0.8.2 (from -r requirements.txt (line 89))
Collecting tensorboard==1.8.0 (from -r requirements.txt (line 90))
  Using cached https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl
Collecting tensorflow==1.8.0 (from -r requirements.txt (line 91))
  Using cached https://files.pythonhosted.org/packages/6d/dc/464f59597a5a8282585238e6e3a7bb3770c3c1f1dc8ee72bd5be257178ec/tensorflow-1.8.0-cp35-cp35m-manylinux1_x86_64.whl
Collecting termcolor==1.1.0 (from -r requirements.txt (line 92))
Collecting thinc<6.13.0,>=6.12.0 (from -r requirements.txt (line 93))
  Downloading https://files.pythonhosted.org/packages/e3/41/c072537c49900db582d0b4fab943d819b420db891d767c858a0aa36ee169/thinc-6.12.0-cp35-cp35m-manylinux1_x86_64.whl (1.9MB)
    100% |笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 1.9MB 14.5MB/s 
Collecting toolz==0.9.0 (from -r requirements.txt (line 94))
Collecting tqdm==4.26.0 (from -r requirements.txt (line 95))
  Using cached https://files.pythonhosted.org/packages/79/43/19c9fee28110cd47f73e6bc596394337fe9f3e5825b4de402bbf30b3beb5/tqdm-4.26.0-py2.py3-none-any.whl
Collecting twilio==6.17.0 (from -r requirements.txt (line 96))
  Using cached https://files.pythonhosted.org/packages/78/e6/b4043da46b968103f75c7d4ebe45a84ffaa5a1185b0f751f685e2e71629a/twilio-6.17.0-py2.py3-none-any.whl
Collecting Twisted==18.7.0 (from -r requirements.txt (line 97))
Collecting typing==3.6.6 (from -r requirements.txt (line 98))
  Using cached https://files.pythonhosted.org/packages/4a/bd/eee1157fc2d8514970b345d69cb9975dcd1e42cd7e61146ed841f6e68309/typing-3.6.6-py3-none-any.whl
Collecting tzlocal==1.5.1 (from -r requirements.txt (line 99))
Collecting ujson==1.35 (from -r requirements.txt (line 100))
Collecting urllib3==1.23 (from -r requirements.txt (line 101))
  Using cached https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl
Collecting websocket-client==0.53.0 (from -r requirements.txt (line 102))
  Using cached https://files.pythonhosted.org/packages/14/d4/6a8cd4e7f67da465108c7cc0a307a1c5da7e2cdf497330b682069b1d4758/websocket_client-0.53.0-py2.py3-none-any.whl
Collecting Werkzeug==0.14.1 (from -r requirements.txt (line 103))
  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl
Collecting wrapt==1.10.11 (from -r requirements.txt (line 104))
Collecting zope.interface==4.5.0 (from -r requirements.txt (line 105))
  Downloading https://files.pythonhosted.org/packages/8c/e2/467655b0abe3ebc856d2feb9a1682adb37569b797890b2f295194b07453b/zope.interface-4.5.0-cp35-cp35m-manylinux1_x86_64.whl (166kB)
    100% |笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 174kB 16.7MB/s 
Requirement already satisfied: setuptools>=0.7 in ./venv/lib/python3.5/site-packages (from APScheduler==3.5.3->-r requirements.txt (line 2)) (40.4.3)
Requirement already satisfied: wheel>=0.26; python_version >= ""3"" in ./venv/lib/python3.5/site-packages (from tensorboard==1.8.0->-r requirements.txt (line 90)) (0.32.2)
Building wheels for collected packages: regex, en-core-web-sm
  Running setup.py bdist_wheel for regex ... done
  Stored in directory: /home/aanchan/.cache/pip/wheels/74/17/3f/c77bba99efd74ba1a19862c9dd97f4b6d735e2826721dc00ff
  Running setup.py bdist_wheel for en-core-web-sm ... done
  Stored in directory: /home/aanchan/.cache/pip/wheels/54/7c/d8/f86364af8fbba7258e14adae115f18dd2c91552406edc3fdaa
Successfully built regex en-core-web-sm
Installing collected packages: six, absl-py, pytz, tzlocal, APScheduler, astor, attrs, Automat, html5lib, bleach, python-dateutil, docutils, urllib3, jmespath, botocore, s3transfer, boto3, certifi, chardet, click, cloudpickle, humanfriendly, coloredlogs, colorhash, ConfigArgParse, constantly, cycler, cymem, toolz, cytoolz, decorator, dill, docopt, redis, fakeredis, idna, requests, fbmessenger, itsdangerous, MarkupSafe, Jinja2, Werkzeug, Flask, Flask-Cors, future, gast, greenlet, gevent, graphviz, grpcio, numpy, h5py, hyperlink, incremental, jsonpickle, jsonschema, scipy, Keras-Applications, PyYAML, Keras-Preprocessing, Keras, kiwisolver, zope.interface, PyHamcrest, Twisted, klein, Markdown, pyparsing, matplotlib, mattermostwrapper, msgpack, msgpack-numpy, murmurhash, networkx, packaging, pathlib, plac, preshed, protobuf, PyJWT, pykwalify, PySocks, python-crfsuite, python-telegram-bot, typing, simplejson, tqdm, rasa-nlu, twilio, tensorboard, termcolor, tensorflow, websocket-client, slackclient, scikit-learn, ruamel.yaml, rasa-core, regex, tabulate, sklearn-crfsuite, ujson, wrapt, thinc, spacy, en-core-web-sm
Successfully installed APScheduler-3.5.3 Automat-0.7.0 ConfigArgParse-0.13.0 Flask-1.0.2 Flask-Cors-3.0.6 Jinja2-2.10 Keras-2.2.2 Keras-Applications-1.0.4 Keras-Preprocessing-1.0.2 Markdown-2.6.11 MarkupSafe-1.0 PyHamcrest-1.9.0 PyJWT-1.6.4 PySocks-1.6.8 PyYAML-3.13 Twisted-18.7.0 Werkzeug-0.14.1 absl-py-0.5.0 astor-0.7.1 attrs-18.2.0 bleach-1.5.0 boto3-1.9.8 botocore-1.12.16 certifi-2018.8.24 chardet-3.0.4 click-6.7 cloudpickle-0.5.6 coloredlogs-10.0 colorhash-1.0.2 constantly-15.1.0 cycler-0.10.0 cymem-2.0.2 cytoolz-0.9.0.1 decorator-4.3.0 dill-0.2.8.2 docopt-0.6.2 docutils-0.14 en-core-web-sm-2.0.0 fakeredis-0.10.3 fbmessenger-5.3.0 future-0.16.0 gast-0.2.0 gevent-1.3.6 graphviz-0.8.4 greenlet-0.4.15 grpcio-1.15.0 h5py-2.8.0 html5lib-0.9999999 humanfriendly-4.16.1 hyperlink-18.0.0 idna-2.7 incremental-17.5.0 itsdangerous-0.24 jmespath-0.9.3 jsonpickle-0.9.6 jsonschema-2.6.0 kiwisolver-1.0.1 klein-17.10.0 matplotlib-2.2.3 mattermostwrapper-2.1 msgpack-0.5.6 msgpack-numpy-0.4.3.2 murmurhash-0.28.0 networkx-2.2 numpy-1.15.1 packaging-17.1 pathlib-1.0.1 plac-0.9.6 preshed-2.0.1 protobuf-3.6.1 pykwalify-1.6.0 pyparsing-2.2.1 python-crfsuite-0.9.6 python-dateutil-2.7.3 python-telegram-bot-10.1.0 pytz-2018.5 rasa-core-0.10.3 rasa-nlu-0.13.4 redis-2.10.6 regex-2018.1.10 requests-2.19.1 ruamel.yaml-0.15.69 s3transfer-0.1.13 scikit-learn-0.19.2 scipy-1.1.0 simplejson-3.16.0 six-1.11.0 sklearn-crfsuite-0.3.6 slackclient-1.3.0 spacy-2.0.16 tabulate-0.8.2 tensorboard-1.8.0 tensorflow-1.8.0 termcolor-1.1.0 thinc-6.12.0 toolz-0.9.0 tqdm-4.26.0 twilio-6.17.0 typing-3.6.6 tzlocal-1.5.1 ujson-1.35 urllib3-1.23 websocket-client-0.53.0 wrapt-1.10.11 zope.interface-4.5.0

When trying to run script
----------------------------------
python train_online.py 
Traceback (most recent call last):
  File ""train_online.py"", line 67, in <module>
    run_twilio_bot()
  File ""train_online.py"", line 20, in run_twilio_bot
    interpreter = RasaNLUInterpreter('./models/current/nlu')
  File ""/home/aanchan/tmp/synap-rasa-chatbot/venv/lib/python3.5/site-packages/rasa_core/interpreter.py"", line 229, in __init__
    self._load_interpreter()
  File ""/home/aanchan/tmp/synap-rasa-chatbot/venv/lib/python3.5/site-packages/rasa_core/interpreter.py"", line 245, in _load_interpreter
    self.interpreter = Interpreter.load(self.model_directory)
  File ""/home/aanchan/tmp/synap-rasa-chatbot/venv/lib/python3.5/site-packages/rasa_nlu/model.py"", line 293, in load
    skip_validation)
  File ""/home/aanchan/tmp/synap-rasa-chatbot/venv/lib/python3.5/site-packages/rasa_nlu/model.py"", line 320, in create
    model_metadata, **context)
  File ""/home/aanchan/tmp/synap-rasa-chatbot/venv/lib/python3.5/site-packages/rasa_nlu/components.py"", line 419, in load_component
    cached_component, **context)
  File ""/home/aanchan/tmp/synap-rasa-chatbot/venv/lib/python3.5/site-packages/rasa_nlu/registry.py"", line 134, in load_component_by_name
    return component_clz.load(model_dir, metadata, cached_component, **kwargs)
  File ""/home/aanchan/tmp/synap-rasa-chatbot/venv/lib/python3.5/site-packages/rasa_nlu/utils/spacy_utils.py"", line 126, in load
    nlp = spacy.load(model_name, parser=False)
  File ""/home/aanchan/tmp/synap-rasa-chatbot/venv/lib/python3.5/site-packages/spacy/__init__.py"", line 21, in load
    return util.load_model(name, **overrides)
  File ""/home/aanchan/tmp/synap-rasa-chatbot/venv/lib/python3.5/site-packages/spacy/util.py"", line 119, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.

After trying to download the model manually:
--------------------------------------------------------------
python -m spacy download en
Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in ./venv/lib/python3.5/site-packages (2.0.0)

    Linking successful
    /home/aanchan/tmp/synap-rasa-chatbot/venv/lib/python3.5/site-packages/en_core_web_sm
    -->
    /home/aanchan/tmp/synap-rasa-chatbot/venv/lib/python3.5/site-packages/spacy/data/en

    You can now load the model via spacy.load('en')


## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type 
`python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Ubuntu 18.04
* Python Version Used: Python 3.5.6
* spaCy Version Used: 2.0.16
* Environment Information: virtual environment created using `virtualenv venv`
requirements.txt
-----------------------
absl-py==0.5.0
APScheduler==3.5.3
astor==0.7.1
attrs==18.2.0
Automat==0.7.0
bleach==1.5.0
boto3==1.9.8
botocore==1.12.16
certifi==2018.8.24
chardet==3.0.4
click==6.7
cloudpickle==0.5.6
coloredlogs==10.0
colorhash==1.0.2
ConfigArgParse==0.13.0
constantly==15.1.0
cycler==0.10.0
cymem<2.1.0,>=2.0.2
cytoolz==0.9.0.1
decorator==4.3.0
dill==0.2.8.2
docopt==0.6.2
docutils==0.14
fakeredis==0.10.3
fbmessenger==5.3.0
Flask==1.0.2
Flask-Cors==3.0.6
future==0.16.0
gast==0.2.0
gevent==1.3.6
graphviz==0.8.4
greenlet==0.4.15
grpcio==1.15.0
h5py==2.8.0
html5lib==0.9999999
humanfriendly==4.16.1
hyperlink==18.0.0
idna==2.7
incremental==17.5.0
itsdangerous==0.24
Jinja2==2.10
jmespath==0.9.3
jsonpickle==0.9.6
jsonschema==2.6.0
Keras==2.2.2
Keras-Applications==1.0.4
Keras-Preprocessing==1.0.2
kiwisolver==1.0.1
klein==17.10.0
Markdown==2.6.11
MarkupSafe==1.0
matplotlib==2.2.3
mattermostwrapper==2.1
msgpack==0.5.6
msgpack-numpy==0.4.3.2
murmurhash==0.28.0
networkx==2.2
numpy==1.15.1
packaging==17.1
pathlib==1.0.1
plac==0.9.6
preshed<2.1.0,>=2.0.1
protobuf==3.6.1
PyHamcrest==1.9.0
PyJWT==1.6.4
pykwalify==1.6.0
pyparsing==2.2.1
PySocks==1.6.8
python-crfsuite==0.9.6
python-dateutil==2.7.3
python-telegram-bot==10.1.0
pytz==2018.5
PyYAML==3.13
rasa-core==0.10.3
rasa-nlu==0.13.4
redis==2.10.6
regex==2018.01.10
requests==2.19.1
ruamel.yaml==0.15.69
s3transfer==0.1.13
scikit-learn==0.19.2
scipy==1.1.0
simplejson==3.16.0
six==1.11.0
sklearn-crfsuite==0.3.6
slackclient==1.3.0
spacy>=2.0.0,<3.0.0
https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#en_core_web_sm
tabulate==0.8.2
tensorboard==1.8.0
tensorflow==1.8.0
termcolor==1.1.0
thinc<6.13.0,>=6.12.0
toolz==0.9.0
tqdm==4.26.0
twilio==6.17.0
Twisted==18.7.0
typing==3.6.6
tzlocal==1.5.1
ujson==1.35
urllib3==1.23
websocket-client==0.53.0
Werkzeug==0.14.1
wrapt==1.10.11
zope.interface==4.5.0
",0
68,https://github.com/explosion/spaCy/issues/2874,2874,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881718446, 'node_id': 'MDU6TGFiZWw4ODE3MTg0NDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20tokenizer', 'name': 'feat / tokenizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Tokenizer'}]",closed,2018-10-24 07:29:54+00:00,2,Improper tokenization when using Tokenizer() instantiation,"In construction 1 in the documentation at https://spacy.io/api/tokenizer a tokenizer instance 
can be created like so:
```python
from spacy.tokenizer import Tokenizer
tokenizer = Tokenizer(nlp.vocab)
```

Even with the additional model load step, the Tokenizer instance doesn't tokenize properly.
## How to reproduce the behaviour
```python
import spacy
from spacy.tokenizer import Tokenizer

nlp = spacy.load('en_core_web_sm')
tok = Tokenizer(nlp.vocab)
print([t.text for t in tok(""Nowadays, it's a sentence."")])
print([t.text for t in nlp(""Nowadays, it's a sentence."")])
```
['Nowadays,', ""it's"", 'a', 'sentence.']
['Nowadays', ',', 'it', ""'s"", 'a', 'sentence', '.']



## Your Environment

* **spaCy version:** 2.0.16
* **Platform:** Darwin-18.0.0-x86_64-i386-64bit
* **Python version:** 3.7.0
* **Models:** en_core_web_md, es_core_news_md, en_core_web_lg, en_core_web_sm
",0
69,https://github.com/explosion/spaCy/issues/2875,2875,"[{'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 881666230, 'node_id': 'MDU6TGFiZWw4ODE2NjYyMzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20vectors', 'name': 'feat / vectors', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Word vectors and similarity'}, {'id': 2103359118, 'node_id': 'MDU6TGFiZWwyMTAzMzU5MTE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/resolved', 'name': 'resolved', 'color': 'f6f6f6', 'default': False, 'description': 'The issue was addressed / answered'}]",closed,2018-10-25 11:07:13+00:00,6,is_oov doesn't work correctly when using en_vectors_web_lg model,"## How to reproduce the behaviour
Here is an example:
```
In [3]: nlp = spacy.load('en_vectors_web_lg')                                                                                                                                                               

In [4]: doc = nlp(""The quick brown fox jumps over the lazy dog"")                                                                                                                                            

In [5]: [token.is_oov for token in doc]                                                                                                                                                                     
Out[5]: [True, True, True, True, True, True, True, True, True]

In [6]: nlp = spacy.load('en_core_web_lg')                                                                                                                                                                  

In [7]: doc = nlp(""The quick brown fox jumps over the lazy dog"")                                                                                                                                            

In [8]: [token.is_oov for token in doc]                                                                                                                                                                     
Out[8]: [False, False, False, False, False, False, False, False, False]
```

## Info about spaCy

* **spaCy version:** 2.0.12
* **Platform:** Darwin-17.7.0-x86_64-i386-64bit
* **Python version:** 3.6.4
* **Models:** en_core_web_lg, en_vectors_web_lg, en
",0
70,https://github.com/explosion/spaCy/issues/2877,2877,"[{'id': 785817829, 'node_id': 'MDU6TGFiZWw3ODU4MTc4Mjk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/gpu', 'name': 'gpu', 'color': '676F6D', 'default': False, 'description': 'Using spaCy on GPU'}, {'id': 1025171819, 'node_id': 'MDU6TGFiZWwxMDI1MTcxODE5', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20memory', 'name': 'perf / memory', 'color': 'B35905', 'default': False, 'description': 'Performance: memory use'}]",closed,2018-10-25 20:20:12+00:00,4,begin training with GPU takes several minutes and allocates 1.5GB of GPU memory,"## How to reproduce the behaviour
```
import spacy

print(spacy.prefer_gpu())
nlp = spacy.blank(""xx"")

if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe('ner')
    nlp.add_pipe(ner, last=True)
else:
    ner = nlp.get_pipe('ner')

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']

with nlp.disable_pipes(*other_pipes):  # only train NER
    optimizer = nlp.begin_training()
```


## Your Environment
Ubuntu 18
Python 3.5
Spacy 2.0.16

cupy==5.0.0
thinc==6.12.0
thinc-gpu-ops==0.0.3
",0
71,https://github.com/explosion/spaCy/issues/2878,2878,"[{'id': 514704814, 'node_id': 'MDU6TGFiZWw1MTQ3MDQ4MTQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/examples', 'name': 'examples', 'color': '087EA6', 'default': False, 'description': 'Code examples in /examples'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-10-25 23:32:17+00:00,3,Code example train_new_entity_type.py doesn't seem to work as expected.,"## How to reproduce the behaviour
On a fresh isntallation of spacey, the first things I did were start running the code examples. One example, https://github.com/explosion/spacy/blob/master/examples/training/train_ner.py, doesn't seem to work as expected. The newly trained entity, horse -> ANIMAL, doesn't get recognized. Here is the script output:

C:\python36\lib\importlib\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject
  return f(*args, **kwds)
C:\python36\lib\importlib\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 12 from C header, got 20 from PyObject
  return f(*args, **kwds)
C:\python36\lib\importlib\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject
  return f(*args, **kwds)
C:\python36\lib\importlib\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 12 from C header, got 20 from PyObject
  return f(*args, **kwds)
Created blank 'en' model
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))
Losses {'ner': 3.352960731417763}
Losses {'ner': 2.1387313530291516}
Losses {'ner': 1.4231954120514807}
Losses {'ner': 0.9599864604625694}
Losses {'ner': 1.5879636425572516}
Losses {'ner': 0.00012759523063688653}
Losses {'ner': 0.10524090359196434}
Losses {'ner': 0.05637522462945867}
Losses {'ner': 0.001495454859912165}
Losses {'ner': 8.717183293751417e-08}
Entities in 'Do you like horses?'

Is this a data set issue within the example?

## Your Environment
* **spaCy version:** 2.0.16
* **Platform:** Windows-10-10.0.17763-SP0
* **Python version:** 3.6.3
",0
72,https://github.com/explosion/spaCy/issues/2879,2879,"[{'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 906542982, 'node_id': 'MDU6TGFiZWw5MDY1NDI5ODI=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20doc', 'name': 'feat / doc', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Doc, Span and Token objects'}]",closed,2018-10-26 15:43:12+00:00,2,Token's property `is_currency` partially working,"Follow up on issue #2491

## How to reproduce the bug
I'm trying to use the token's property `is_currency` . As previously mention in the past issue,  the `is_currency` attribute was added after the current models were trained. But now, it seems they are partially added as shown in the table below (example : `竄?` but not `$` in the `en_core_web_sm` model).
```python
from spacy.lang.en import EnglishDefaults
from spacy.symbols import IS_CURRENCY
from spacy.lang.en import English
import pandas as pd

is_currency_original = EnglishDefaults.lex_attr_getters[IS_CURRENCY]
en_core_web_sm = spacy.load('en_core_web_sm')
en_core_web_lg = spacy.load('en_core_web_lg')
en_old = English()

data = []
text = ""$ 竄?""
for token in en_core_web_sm(text):
    data.append(['en_core_web_sm', token.text, token.tag_, token.is_currency, is_currency_original(token.text), unicodedata.category(token.text) == 'Sc'])
for token in en_core_web_lg(text):
    data.append(['en_core_web_lg', token.text, token.tag_, token.is_currency, is_currency_original(token.text), unicodedata.category(token.text) == 'Sc'])
for token in en_old(text):
    data.append(['English()', token.text, token.tag_, token.is_currency, is_currency_original(token.text), unicodedata.category(token.text) == 'Sc'])
pd.DataFrame(data, columns = [""model_name"", ""text"", ""tag_"", ""is_currency"", ""is_currency_original"", ""unicodedata""])
```

model_name | text | tag_ | is_currency | is_currency_original | unicodedata
-- | -- | -- | -- | -- | --
en_core_web_sm | $ | $ | False | True | True
en_core_web_sm | 竄? | . | True | True | True
en_core_web_lg | $ | $ | False | True | True
en_core_web_lg | 竄? | CD | False | True | True
English() | $ | ?? | True | True | True
English() | 竄? | ?? | True | True | True

So at the moment only the `English()` is working correctly. Is it the correct desired behavior ? What about the `tag_` attribute for the two core models, not referencing then as a currency ?
Do you have more information on the next model release to take into account this new attribute ?

## Your Environment (Info about spaCy)
* **spaCy version:** 2.0.12
* **Platform:** Linux-4.15.0-36-generic-x86_64-with-debian-buster-sid
* **Python version:** 3.6.6
* **Models:** en, en_core_web_lg, en_core_web_sm",0
73,https://github.com/explosion/spaCy/issues/2882,2882,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-10-29 08:36:06+00:00,3,is there a way with spaCy's NER to calculate metrics per entity type?,"Hello there,
I would like to ask if there is a way in the NER model in spaCy to extract the metrics (precision, recall, f1 score) per entity type?

Something that will look like this:

         precision    recall  f1-score   support
  B-LOC      0.810     0.784     0.797      1084
  I-LOC      0.690     0.637     0.662       325
 B-MISC      0.731     0.569     0.640       339
 I-MISC      0.699     0.589     0.639       557
  B-ORG      0.807     0.832     0.820      1400
  I-ORG      0.852     0.786     0.818      1104
  B-PER      0.850     0.884     0.867       735
  I-PER      0.893     0.943     0.917       634
avg / total 0.809 0.787 0.796 6178

Above example taken from: http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/

I have tried to post on stackoverflow as well but had no luck, apologies if this is not the right place to post this question or maybe a usage tag should be used if Im missing something in the documentation on how to do it.
https://stackoverflow.com/questions/52856057/is-there-a-way-with-spacys-ner-to-calculate-metrics-per-entity-type
Thank you!

## Your Environment
## Info about spaCy

* **spaCy version:** 2.0.12
* **Platform:** Linux-4.4.0-17134-Microsoft-x86_64-with-Ubuntu-18.04-bionic
* **Python version:** 3.6.6
* **Models:** en_core_web_lg
* Environment Information: I am using Ubuntu subsystem in the windows 10.
",0
74,https://github.com/explosion/spaCy/issues/2883,2883,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 677479449, 'node_id': 'MDU6TGFiZWw2Nzc0Nzk0NDk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/%F0%9F%94%AE%20thinc', 'name': '醗 thinc', 'color': 'f6f6f6', 'default': False, 'description': ""spaCy's machine learning library Thinc""}]",closed,2018-10-29 09:15:11+00:00,3,Setting up Spacy,"## How to reproduce the behaviour
Just do `import spacy`

## Environment

* Operating System: Ubuntu 16.04
* Python Version Used: 3.5.2
* spaCy Version Used: 2.0.16 
* Environment Information: Django 2.1.2

## Issues

`python -m spacy info --markdown` gives me

```
Traceback (most recent call last):
  File ""/usr/lib/python3.5/runpy.py"", line 174, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""/usr/lib/python3.5/runpy.py"", line 133, in _get_module_details
    return _get_module_details(pkg_main_name, error)
  File ""/usr/lib/python3.5/runpy.py"", line 109, in _get_module_details
    __import__(pkg_name)
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/spacy/__init__.py"", line 8, in <module>
    from thinc.neural.util import prefer_gpu, require_gpu
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/thinc/neural/__init__.py"", line 1, in <module>
    from ._classes.model import Model
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/thinc/neural/_classes/model.py"", line 12, in <module>
    from ..train import Trainer
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/thinc/neural/train.py"", line 3, in <module>
    from .optimizers import Adam, SGD, linear_decay
  File ""optimizers.pyx"", line 13, in init thinc.neural.optimizers
  File ""cymem.pxd"", line 4, in init thinc.neural.ops
AttributeError: module 'cymem.cymem' has no attribute 'PyMalloc'
```




When I do `python3  manage.py runserver` , I get

```
  File ""/home/cegprakash/workspace/python-engine/keyword_strategy.py"", line 4, in <module>
    import spacy
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/spacy/__init__.py"", line 10, in <module>
    from .cli.info import info as cli_info
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/spacy/cli/__init__.py"", line 1, in <module>
    from .download import download
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/spacy/cli/download.py"", line 11, in <module>
    from .link import link
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/spacy/cli/link.py"", line 8, in <module>
    from ..compat import symlink_to, path2str
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/spacy/compat.py"", line 32, in <module>
    from thinc.neural.optimizers import Optimizer
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/thinc/neural/__init__.py"", line 1, in <module>
    from ._classes.model import Model
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/thinc/neural/_classes/model.py"", line 12, in <module>
    from ..train import Trainer
  File ""/home/cegprakash/.local/share/virtualenvs/python-engine-USisEXbk/lib/python3.5/site-packages/thinc/neural/train.py"", line 3, in <module>
    from .optimizers import Adam, SGD, linear_decay
  File ""optimizers.pyx"", line 13, in init thinc.neural.optimizers
  File ""stringsource"", line 104, in init thinc.neural.ops
AttributeError: type object 'thinc.neural.ops.array' has no attribute '__reduce_cython__'

```

#2849 #2439",0
75,https://github.com/explosion/spaCy/issues/2884,2884,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 881682577, 'node_id': 'MDU6TGFiZWw4ODE2ODI1Nzc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20matcher', 'name': 'feat / matcher', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Token, phrase and dependency matcher'}]",closed,2018-10-29 14:12:53+00:00,5,Pattern matcher returns wrong match though quantified properly,"I have the following matcher:

``` 
self.__matcher.add('Subject', self.__add_subject, [
    {'LOWER': 'mit'},
    {'LOWER': 'dem'},
    {'LOWER': 'betreff'},
    {'IS_ALPHA': True, 'OP': '+'}
])
```
Given the following sentence:

""**Ich m??chte mit dem Betreff Kindergeburtstag und Noah weitermachen und frage entsprechend nach**""

It returns only:

**""mit dem Betreff Kindergeburtstag Noah""**

If I use the quantifiers ""?"" or ""*"" it is the same. As far as I understood the description, it should return:

""**mit dem Betreff Kindergeburtstag und Noah weitermachen und frage entsprechend nach**""

or am I wrong? If I am right, what's currently wrong? What I want to achieve is a phrase with 1-n lower tokens which are specified and then any amount of following tokens. 


* Python Version Used: 3.6.5
* spaCy Version Used: 2.0.16 (latest)
",0
76,https://github.com/explosion/spaCy/issues/2887,2887,[],closed,2018-10-30 03:32:44+00:00,2,Spacy errors out with PyFPE_jbuf  when packaging in a PEX file and running inside a docker container,"<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

## How to reproduce the problem
<!-- Include the details of how the problem occurred. Which command did you run to install spaCy? Did you come across an error? What else did you try? -->
I have a requirements.txt file that looks like this:
scikit-learn==0.19.2
lxml==4.2.5
numpy==1.15.1
scipy==1.1.0
xmltodict==0.11.0
tensorflow==1.8
chardet==3.0.4
beautifulsoup4==4.6.3
PyYAML==3.13
dpath==1.4.2
keras==2.2.4
spacy==2.0.16
dedupe==1.9.3
pandas==0.23.4
openpyxl==2.5.9
xlrd==1.1.0
PyInquirer==1.0.2
persist-queue==0.4.1
progress==1.4
tensorflow-hub==0.1.1
nltk==3.3
en-core-web-md==2.0.0

I am using Pantsbuild in order to gather the dependencies and then building a PEX file using this. When I execute this file in my local Ubuntu 16.04 VM, it works fine. However, when I copy this PEX file into a docker image '''python:3.5''' and execute the same command on the PEX file, I see the below error. Any clues what could be the cause?

```bash
    from spacy.lang.en import English
  File ""/root/.pex/install/spacy-2.0.16-cp35-cp35m-manylinux1_x86_64.whl.e389cc5823f8f17d7b31453f151cc429b3fbebcd/spacy-2.0.16-cp35-cp35m-manylinux1_x86_64.whl/spacy/__init__.py"", line 8, in <module>
    from thinc.neural.util import prefer_gpu, require_gpu
  File ""/root/.pex/install/thinc-6.12.0-cp35-cp35m-manylinux1_x86_64.whl.a14889ae18f3e114a3a3ba2822b67f0d241fb2e9/thinc-6.12.0-cp35-cp35m-manylinux1_x86_64.whl/thinc/neural/__init__.py"", line 1, in <module>
    from ._classes.model import Model
  File ""/root/.pex/install/thinc-6.12.0-cp35-cp35m-manylinux1_x86_64.whl.a14889ae18f3e114a3a3ba2822b67f0d241fb2e9/thinc-6.12.0-cp35-cp35m-manylinux1_x86_64.whl/thinc/neural/_classes/model.py"", line 12, in <module>
    from ..train import Trainer
  File ""/root/.pex/install/thinc-6.12.0-cp35-cp35m-manylinux1_x86_64.whl.a14889ae18f3e114a3a3ba2822b67f0d241fb2e9/thinc-6.12.0-cp35-cp35m-manylinux1_x86_64.whl/thinc/neural/train.py"", line 3, in <module>
    from .optimizers import Adam, SGD, linear_decay
  File ""optimizers.pyx"", line 13, in init thinc.neural.optimizers
  File ""ops.pyx"", line 18, in init thinc.neural.ops
  File ""/root/.pex/install/cytoolz-0.9.0.1-cp35-cp35m-linux_x86_64.whl.53465d9b9c496dc8ee1ce1d75a845f5fda455ae6/cytoolz-0.9.0.1-cp35-cp35m-linux_x86_64.whl/cytoolz/__init__.py"", line 3, in <module>
    from .functoolz import *
ImportError: /root/.pex/install/cytoolz-0.9.0.1-cp35-cp35m-linux_x86_64.whl.53465d9b9c496dc8ee1ce1d75a845f5fda455ae6/cytoolz-0.9.0.1-cp35-cp35m-linux_x86_64.whl/cytoolz/functoolz.cpython-35m-x86_64-linux-gnu.so: undefined symbol: PyFPE_jbuf
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Docker image python:3.5 (https://github.com/docker-library/python/blob/005dda958e7fdf517214d950c0ef8eb0201ab3a1/3.5/stretch/Dockerfile)
* Python Version Used: 3.5.6
* spaCy Version Used: 2.0.16
* Environment Information: PEX executable copied into a docker container
",0
78,https://github.com/explosion/spaCy/issues/2890,2890,[],closed,2018-10-31 18:41:20+00:00,3,Merged spans disappearing?,"Hi, maybe it's too late in the day and I'm not seeing what I'm doing wrong here, but I seem to have disappearing spans when merging:

## How to reproduce the behaviour
```
import en_core_web_sm

nlp = en_core_web_sm.load()

txt = ""Today it was my great honor to welcome the 2017 NCAA Football National Champion, Alabama Crimson Tide, to the White House.""
doc = nlp(txt)

print(""\nNCs before:"")
for nc in doc.noun_chunks: print(nc)
    
print(""\nEnts before:"")
for ent in doc.ents: print(ent)
        
span = list(doc.noun_chunks)[2]
span.merge()

print(""\nNCs after:"")
for nc in doc.noun_chunks: print(nc)

print(""\nEnts after:"")
for ent in doc.ents: print(ent)
```

This produces:

```
NCs before:
it
my great honor
the 2017 NCAA Football National Champion
Alabama Crimson Tide
the White House

Ents before:
Today
2017
NCAA Football National Champion
Tide
the White House

NCs after:
it
my great honor
Alabama Crimson Tide
the White House

Ents after:
Today
Tide
the White House
```

I.e. any noun chunk or entity included in the span seems to disappear. The result varies depending on which span is merged. Sometimes both noun chunks and entities disappear, and sometimes just the former.

## Your Environment
* spaCy version      2.0.13         
* Platform           Darwin-17.7.0-x86_64-i386-64bit
* Python version     3.6.6",0
79,https://github.com/explosion/spaCy/issues/2891,2891,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881666568, 'node_id': 'MDU6TGFiZWw4ODE2NjY1Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20pipeline', 'name': 'feat / pipeline', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Processing pipeline and components'}]",closed,2018-10-31 20:15:09+00:00,3,load custom pipelines,"Hello, i'm testing a custom component with the code from the documentation
https://spacy.io/usage/processing-pipelines#custom-components

The animal example

> nlp = spacy.load('en_core_web_sm')
> terms = (u'cat', u'dog', u'tree kangaroo', u'giant sea spider')
> entity_matcher = EntityMatcher(nlp, terms, 'ANIMAL')

The thing is...i want the same behavior of this example, but for an ARTIST list of 1.6M artists.

i've tested with 8.000 thousand names and it was fast to start... but with this huge number it is running in my machine for about 1 hour and it is still starting...

Is there a way to ""save"" the configuration to load all the names everytime?

thanks!

## Your Environment
* **spaCy version:** 2.0.16
* **Platform:** Darwin-17.7.0-x86_64-i386-64bit
* **Python version:** 3.7.0
* **Models:** en, pt
",0
81,https://github.com/explosion/spaCy/issues/2893,2893,"[{'id': 312256136, 'node_id': 'MDU6TGFiZWwzMTIyNTYxMzY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/docs', 'name': 'docs', 'color': '087EA6', 'default': False, 'description': 'Documentation and website'}]",closed,2018-11-01 13:33:41+00:00,2,Shared representation unclear at training,"I currently a bit confused about what the level of shared weights/representation is between the different components (tagger/ner/text classification). Consequently, _I'm not sure whether first training the NER and then training a text classification component is any different from training both at the same time._

 

My confusion stems from @honnibal's recent comment on an issue (https://github.com/explosion/spaCy/issues/2267):

> The update_shared argument is no longer current, as the pipeline now uses independent models for the parser, tagger and entity recognizer. Apologies for the confusion!

 

My understanding was that v2.0 brought a massive reduction in model size mainly due to sharing of weights. I would therefore think that training towards two tasks simultaneously (i.e. multitask training) could yield some improvements. But the above comment makes it a bit unclear. I think the documentation would do well with a clear diagram to explain what the connection is between the different components. Is it currently possible to update the shared weights when training two different components simultaneously?",0
82,https://github.com/explosion/spaCy/issues/2894,2894,"[{'id': 312256136, 'node_id': 'MDU6TGFiZWwzMTIyNTYxMzY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/docs', 'name': 'docs', 'color': '087EA6', 'default': False, 'description': 'Documentation and website'}]",closed,2018-11-01 23:24:06+00:00,3,cuda10 installation not working,"<!-- Describe the problem or suggestion here. If you've found a mistake and you know the answer, feel free to submit a pull request straight away: https://github.com/explosion/spaCy/pulls -->
docs state to use `spacy[cuda10]` for installation, but code expects `cuda100` https://github.com/explosion/spaCy/blob/e2ae25d6f55676ecc0f27c2dc1f6fe0b35e729d0/setup.py#L208
however `pip install -U spacy[cuda100]` then fails with
```
Collecting cupy-cuda100>=4.0; extra == ""cuda100"" (from spacy[cuda100])
  Could not find a version that satisfies the requirement cupy-cuda100>=4.0; extra == ""cuda100"" (from spacy[cuda100]) (from versions: )
```

## Which page or section is this issue related to?
<!-- Please include the URL and/or source. -->
https://spacy.io/usage/#gpu",0
83,https://github.com/explosion/spaCy/issues/2895,2895,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 710446668, 'node_id': 'MDU6TGFiZWw3MTA0NDY2Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/training', 'name': 'training', 'color': '087EA6', 'default': False, 'description': 'Training and updating models'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-11-02 08:28:41+00:00,4,Spacy : 2.1.0a1  : Customized NER model which is loaded from disk is not giving any detection of customized entity ?,"    spaCy version      2.1.0a1        
    Location           /home/prashant/.local/lib/python3.6/site-packages/spacy
    Platform           Linux-4.15.0-36-generic-x86_64-with-Ubuntu-18.04-bionic
    Python version     3.6.6          
    Models             en   

@honnibal @ines 

Hi ,
I already went through https://github.com/explosion/spaCy/issues/1337 but....doesn't work.
So after that I am writing this issue

one difference I noticed between en_core_web_sm and to_disk generated model is there is no tagger and parser folder.

When I am evaluating trained model in the same code it shows proper results for customized named entity  recognition. But when I am loading saved model from disk it shows nothing for the same test data that I used after just training to evaluate.


I tried with both technique **from disk()** and **spacy.load()**

Here  I am attaching my generated model in **zip** file.


nlp = spacy.blank('en').from_disk('/home/prashant/sales_agreemt/binary_models/en_CLOSING_DATE_EXTRACTION-0.0.0/en_CLOSING_DATE_EXTRACTION/en_CLOSING_DATE_EXTRACTION-0.0.0/')

[en_CLOSING_DATE_EXTRACTION-0.0.0.zip](https://github.com/explosion/spaCy/files/2541603/en_CLOSING_DATE_EXTRACTION-0.0.0.zip)

#nlp = spacy.load('/home/prashant/sales_agreemt/binary_models/en_CLOSING_DATE_EXTRACTION-0.0.0/en_CLOSING_DATE_EXTRACTION/en_CLOSING_DATE_EXTRACTION-0.0.0/l')

examples : 

with training and testing in same code

testing sample: _The closing of the sale will be on or before July 19 , 2018 , or within 7 days after objections made._

**Results :** closing_date :  July 19 , 2018 


with loading model from disk

testing sample: _The closing of the sale will be on or before July 19 , 2018 , or within 7 days after objections made._

**Results :** closing_date :  **NULL**   





both  Techniques **spacy.load** and **from disk()** failed

nlp = spacy.blank('en').from_disk('/home/prashant/sales_agreemt/binary_models/en_CLOSING_DATE_EXTRACTION-0.0.0/en_CLOSING_DATE_EXTRACTION/en_CLOSING_DATE_EXTRACTION-0.0.0/')
#nlp = spacy.load('/home/prashant/sales_agreemt/binary_models/en_CLOSING_DATE_EXTRACTION-0.0.0/en_CLOSING_DATE_EXTRACTION/en_CLOSING_DATE_EXTRACTION-0.0.0/ner/model')

",0
84,https://github.com/explosion/spaCy/issues/2896,2896,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881666230, 'node_id': 'MDU6TGFiZWw4ODE2NjYyMzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20vectors', 'name': 'feat / vectors', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Word vectors and similarity'}]",closed,2018-11-02 08:40:29+00:00,3,spacy.load keyword argument vectors=False doesn't work > 2.0,"## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
```
nlp = spacy.load(""en_core_web_lg"") 
```
takes the same amount of memory as
```
nlp = spacy.load(""en_core_web_lg"", vectors=False)
```
So, the vectors are still being loaded into memory. I want to deselect the vectors from being loaded, which consume a lot of space. I tried using `""en_core_web_sm""`, but the results are significantly different from the `""en_core_web_lg""` model. 
Following the issue on https://github.com/explosion/spaCy/issues/400 doesn't help. It seems to only work for <2.0

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Ubuntu
* Python Version Used: 2.7
* spaCy Version Used: 2.0.16
* Environment Information:
",0
85,https://github.com/explosion/spaCy/issues/2897,2897,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446605073, 'node_id': 'MDU6TGFiZWw0NDY2MDUwNzM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/windows', 'name': 'windows', 'color': '676F6D', 'default': False, 'description': 'Issues related to Windows'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2018-11-03 13:43:28+00:00,4,Microsoft Visual C++ 14.0,"I can't able to use the spaCy libray services, when I try to download spaCy in Pycharm tool it generates some horrible errors. The type and details all are given in the attached file.

[errorss.docx](https://github.com/explosion/spaCy/files/2545005/errorss.docx)

 
",0
86,https://github.com/explosion/spaCy/issues/2898,2898,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2018-11-03 14:23:51+00:00,2,ImportError: cannot import name 'ADVERBS',"Checked #2093 for a similar issue which was resolved by setting Locale however mine seems not caused by Locale. (I could be wrong and appreciate your help)

Spacy and en were working well before today when I had the below issue. It was generally caused by ""from ._adverbs import ADVERBS"" with the error ""ImportError: cannot import name 'ADVERBS'"". Below are the details.

>> import Spacy
>> spacy.load('en')
```bash
Traceback (most recent call last):
  File ""Anaconda3\lib\site-packages\spacy\util.py"", line 50, in get_lang_class
    module = importlib.import_module('.lang.%s' % lang, 'spacy')
  File 窶彌Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""\Anaconda3\lib\site-packages\spacy\lang\en\__init__.py"", line 10, in <module>
    from .lemmatizer import LEMMA_RULES, LEMMA_INDEX, LEMMA_EXC, LOOKUP
  File ""\Anaconda3\lib\site-packages\spacy\lang\en\lemmatizer\__init__.py"", line 7, in <module>
    from ._adverbs import ADVERBS
ImportError: cannot import name 'ADVERBS'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File \Anaconda3\lib\site-packages\spacy\__init__.py"", line 21, in load
    return util.load_model(name, **overrides)
  File ""\Anaconda3\lib\site-packages\spacy\util.py"", line 112, in load_model
    return load_model_from_link(name, **overrides)
  File ""\Anaconda3\lib\site-packages\spacy\util.py"", line 129, in load_model_from_link
    return cls.load(**overrides)
  File ""\Anaconda3\lib\site-packages\spacy\data\en\__init__.py"", line 12, in load
    return load_model_from_init_py(__file__, **overrides)
  File ""\Anaconda3\lib\site-packages\spacy\util.py"", line 173, in load_model_from_init_py
    return load_model_from_path(data_path, meta, **overrides)
  File ""\Anaconda3\lib\site-packages\spacy\util.py"", line 143, in load_model_from_path
    cls = get_lang_class(meta['lang'])
  File ""\Anaconda3\lib\site-packages\spacy\util.py"", line 52, in get_lang_class
    raise ImportError(Errors.E048.format(lang=lang))
ImportError: [E048] Can't import language en from spacy.lang.
```
I reinstalled spacy, re-downloaded the en_core_web_md and updated the linkage as below. But the issue remains.
python -m spacy download en
python -m spacy download en_core_web_md
python -m spacy link en_core_web_md en --force
    Linking successful
    \Anaconda3\lib\site-packages\en_core_web_md -->
    \Anaconda3\lib\site-packages\spacy\data\en
    You can now load the model via spacy.load('en')

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Win10
* Python Version Used: 3.6.5
* spaCy Version Used: 2.0.16
* Environment Information:  en_core_web_md
",0
87,https://github.com/explosion/spaCy/issues/2900,2900,"[{'id': 514165920, 'node_id': 'MDU6TGFiZWw1MTQxNjU5MjA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20en', 'name': 'lang / en', 'color': '726DA8', 'default': False, 'description': 'English language data and models'}, {'id': 981590115, 'node_id': 'MDU6TGFiZWw5ODE1OTAxMTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20lemmatizer', 'name': 'feat / lemmatizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Rule-based and lookup lemmatization'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2018-11-05 13:26:55+00:00,6,nlp(u'BUSINESS')[0].lemma_ == 'busines',"

I wonder if this is just some ad hoc machine learning weirdness, or the symptom of something more significant.

```import spacy
nlp = spacy.load('en_core_web_sm')
nlp(u'business')[0].lemma_
u'business'
nlp(u'BUSINESS')[0].lemma_
u'busines'

## Info about spaCy

* **Python version:** 2.7.10
* **Platform:** Darwin-17.6.0-x86_64-i386-64bit
* **spaCy version:** 2.0.11
* **Models:** en_core_web_sm
```",0
