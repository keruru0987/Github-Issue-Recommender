,html_url,number,labels,state,created_at,pull_request,comments,title,body,rel1,rel2,rel3,rel4,rel5,rel6,rel7,rel8,rel9,rel10
477,https://github.com/RaRe-Technologies/gensim/issues/2496,2496,[],closed,2019-05-17 08:39:45+00:00,,12,SparseTermSimilarityMatrix - TypeError: 'numpy.float32' object is not iterable,"I am using gensim 3.7.3 and python3.6.

I am following the exact example of SoftCosineSimilarity at https://radimrehurek.com/gensim/similarities/docsim.html
but with my own dataset and embeddings trained on Fasttext.
Dictionary and WordEmbeddingSimilarityIndex are executed properly but then I get an error when trying SparseTermSimilarityMatrix. I found a similar issue, that was solved in the pull below, but I still seem to get this error. However, I tried the exact same code with Word2Vec and the gensim imported common_texts and it worked. Why it doesnt work in my case, is it related to FastText?

https://github.com/RaRe-Technologies/gensim/pull/2356

My code:
```
from gensim.models import FastText
from gensim.corpora import Dictionary
from gensim.models import WordEmbeddingSimilarityIndex
from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix

model = FastText.load('fasttext_vector_100')
# this line works
model.wv.most_similar(positive=['test'], topn=2)
termsim_index = WordEmbeddingSimilarityIndex(model.wv)
# texts is similar to common_texts, list of lists of strings
dictionary = Dictionary(texts)
bow_corpus = [dictionary.doc2bow(document) for document in texts]
# it fails here
similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)
```


```
TypeError                                 Traceback (most recent call last)
<ipython-input-129-c33cf3beaa3e> in <module>()
----> 1 similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)  # construct similarity matrix

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\similarities\termsim.py in __init__(self, source, dictionary, tfidf, symmetric, positive_definite, nonzero_limit, dtype)
    232             most_similar = [
    233                 (dictionary.token2id[term], similarity)
--> 234                 for term, similarity in index.most_similar(t1, num_rows)
    235                 if term in dictionary.token2id]
    236 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\similarities\termsim.py in <listcomp>(.0)
    231             num_rows = nonzero_limit - num_nonzero
    232             most_similar = [
--> 233                 (dictionary.token2id[term], similarity)
    234                 for term, similarity in index.most_similar(t1, num_rows)
    235                 if term in dictionary.token2id]

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\keyedvectors.py in most_similar(self, t1, topn)
   1418         else:
   1419             most_similar = self.keyedvectors.most_similar(positive=[t1], topn=topn, **self.kwargs)
-> 1420             for t2, similarity in most_similar:
   1421                 if similarity > self.threshold:
   1422                     yield (t2, similarity**self.exponent)

TypeError: 'numpy.float32' object is not iterable

```",1,,,,,,,,,1
484,https://github.com/RaRe-Technologies/gensim/issues/2504,2504,[],closed,2019-05-28 17:44:13+00:00,,2,Getting Segmentation fault: 11 while running cosine similarity function on a bunch of documents.,"I'm using gensim to perform cosine similarity on a bunch of documents getting the Segmentation fault: 11. Could you please help me to resolve this issue?

**Error Trace:**
```
2019-05-28 15:11:22,779 : INFO : creating sparse index
2019-05-28 15:11:22,779 : INFO : creating sparse matrix from corpus
2019-05-28 15:11:22,780 : INFO : PROGRESS: at document #0/546
2019-05-28 15:11:22,790 : INFO : created <546x430 sparse matrix of type '<class 'numpy.float32'>'
        with 2191 stored elements in Compressed Sparse Row format>
2019-05-28 15:11:22,791 : INFO : creating sparse shard #0
2019-05-28 15:11:22,791 : INFO : saving index shard to /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,791 : INFO : saving SparseMatrixSimilarity object under /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0, separately None
2019-05-28 15:11:22,794 : INFO : saved /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,794 : INFO : loading SparseMatrixSimilarity object from /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,794 : INFO : loaded /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
**Segmentation fault: 11**
```
**Code**
```
    def cosine_similarity(self,documents, query_docs=None, task='pairwise_similarity', metric_threshold=0.85, num_best=20):
        self.log('computing cosine similarity started')
        # Compute cosine similarity between the query_docs and the documents.
        dictionary = Dictionary(documents)
        corpus = [dictionary.doc2bow(doc) for doc in documents]
        # index_tmpfile = get_tmpfile(""index"")
        index = Similarity(output_prefix=None,corpus=corpus, num_best=num_best, num_features=len(dictionary))
        similarities = []
        if task == 'pairwise_similarity':
            self.log('computing pairwise_similarity')
            for sim in index:
                similarities.append(sim)
        elif task == 'batch_query':
            self.log('computing similarity using batch query')

            query_docs = [self.tfidf[self.dictionary.doc2bow(doc)] for doc in query_docs]
            for sim in index[query_docs]:
                similarities.append(sim)
        # filter results based on metric threshold
        filtered_results = []
        for ind_sim in similarities:
            filtered_results.append([item[0] for item in ind_sim if item[1] >= metric_threshold])
        self.log('computing cosine similarity completed')
        return filtered_results
```",1,,,,,,,,,1
490,https://github.com/RaRe-Technologies/gensim/issues/2512,2512,[],closed,2019-05-30 16:40:10+00:00,,5,Sparse Matrix is taking a huge memory while computing cosine similarity.,"**Sparse Matrix is taking a huge memory while computing cosine similarity and it's dead slow.**

**Memory Consumed : 17.93 GB**

Kindly help to resolve this issue.

**Code**

```
def cossim(documents, query_docs=None, task='pairwise_similarity', metric_threshold=0.85, num_best=20):
    logging.info('computing cosine similarity started')
    logging.info(
        'semantic matcher configuration :: task : {} metric_threshold: {} num_best: {}'.format(
            task, metric_threshold, num_best))
    # Compute cosine similarity between the query_docs and the documents.
    dictionary = Dictionary(documents)
    # dictionary.filter_extremes()
    tfidf = TfidfModel(dictionary=dictionary)
    corpus = [tfidf[dictionary.doc2bow(doc)] for doc in documents]
    # corpus = [dictionary.doc2bow(doc) for doc in documents]

    index_tmpfile = get_tmpfile(""index"")
    index = Similarity(output_prefix=index_tmpfile, corpus=corpus, num_best=num_best, num_features=len(dictionary),chunksize=2048)
    similarities = []
    if task == 'pairwise_similarity':
        logging.info('computing pairwise_similarity')
        for sim in index:
            similarities.append(sim)
    elif task == 'batch_query':
        logging.info('computing similarity using batch query')
        tfidf_query_docs = [tfidf[dictionary.doc2bow(doc)] for doc in query_docs]
        for sim in index[tfidf_query_docs]:
            similarities.append(sim)
    # filter results based on metric threshold
    filtered_results = []
    logging.info('filtering results based on metric threshold {}'.format(metric_threshold))
    for ind_sim in similarities:
        filtered_results.append([item[0] for item in ind_sim if item[1] >= metric_threshold])
    logging.info('computing cosine similarity completed')
```

**Log Trace:**

```
2019-05-30 22:00:32,133 : INFO : no.of records in the data set :: 4398258
2019-05-30 22:00:32,606 : INFO : no.of records in the data set after dropping null values :: 4166943
2019-05-30 22:00:33,279 : INFO : no.of records in the data set after dropping duplicate values :: 101926
2019-05-30 22:00:43,623 : INFO : no.of records in the normalized data set :: 101926
2019-05-30 22:00:43,855 : INFO : computing cosine similarity started
2019-05-30 22:00:43,855 : INFO : semantic matcher configuration :: task : batch_query metric_threshold: 0.85 num_best: 20
2019-05-30 22:00:43,855 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2019-05-30 22:00:43,941 : INFO : adding document #10000 to Dictionary(2019 unique tokens: ['bracket', 'frame', 'front', 'instal', 'licens']...)
2019-05-30 22:00:44,027 : INFO : adding document #20000 to Dictionary(2963 unique tokens: ['bracket', 'frame', 'front', 'instal', 'licens']...)
2019-05-30 22:00:44,109 : INFO : adding document #30000 to Dictionary(3591 unique tokens: ['bracket', 'frame', 'front', 'instal', 'licens']...)
2019-05-30 22:00:44,194 : INFO : adding document #40000 to Dictionary(4196 unique tokens: ['bracket', 'frame', 'front', 'instal', 'licens']...)
2019-05-30 22:00:44,278 : INFO : adding document #50000 to Dictionary(4807 unique tokens: ['bracket', 'frame', 'front', 'instal', 'licens']...)
2019-05-30 22:00:44,361 : INFO : adding document #60000 to Dictionary(5258 unique tokens: ['bracket', 'frame', 'front', 'instal', 'licens']...)
2019-05-30 22:00:44,439 : INFO : adding document #70000 to Dictionary(5658 unique tokens: ['bracket', 'frame', 'front', 'instal', 'licens']...)
2019-05-30 22:00:44,519 : INFO : adding document #80000 to Dictionary(6350 unique tokens: ['bracket', 'frame', 'front', 'instal', 'licens']...)
2019-05-30 22:00:44,601 : INFO : adding document #90000 to Dictionary(6953 unique tokens: ['bracket', 'frame', 'front', 'instal', 'licens']...)
2019-05-30 22:00:44,649 : INFO : built Dictionary(7160 unique tokens: ['bracket', 'frame', 'front', 'instal', 'licens']...) from 95908 documents (total 283699 corpus positions)
2019-05-30 22:00:47,942 : INFO : starting similarity index under /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index
2019-05-30 22:00:49,742 : INFO : PROGRESS: fresh_shard size=10000
2019-05-30 22:00:51,522 : INFO : PROGRESS: fresh_shard size=20000
2019-05-30 22:00:53,401 : INFO : PROGRESS: fresh_shard size=30000
2019-05-30 22:00:53,931 : INFO : creating sparse index
2019-05-30 22:00:53,931 : INFO : creating sparse matrix from corpus
2019-05-30 22:00:53,932 : INFO : PROGRESS: at document #0/32768
2019-05-30 22:00:54,087 : INFO : PROGRESS: at document #10000/32768
2019-05-30 22:00:54,237 : INFO : PROGRESS: at document #20000/32768
2019-05-30 22:00:54,383 : INFO : PROGRESS: at document #30000/32768
2019-05-30 22:00:54,429 : INFO : created <32768x7160 sparse matrix of type '<class 'numpy.float32'>'
        with 100075 stored elements in Compressed Sparse Row format>
2019-05-30 22:00:54,429 : INFO : creating sparse shard #0
2019-05-30 22:00:54,430 : INFO : saving index shard to /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.0
2019-05-30 22:00:54,430 : INFO : saving SparseMatrixSimilarity object under /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.0, separately None
2019-05-30 22:00:54,435 : INFO : saved /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.0
2019-05-30 22:00:54,436 : INFO : loading SparseMatrixSimilarity object from /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.0
2019-05-30 22:00:54,440 : INFO : loaded /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.0
2019-05-30 22:00:54,485 : INFO : PROGRESS: fresh_shard size=0
2019-05-30 22:00:56,476 : INFO : PROGRESS: fresh_shard size=10000
2019-05-30 22:00:58,442 : INFO : PROGRESS: fresh_shard size=20000
2019-05-30 22:01:00,380 : INFO : PROGRESS: fresh_shard size=30000
2019-05-30 22:01:00,899 : INFO : creating sparse index
2019-05-30 22:01:00,899 : INFO : creating sparse matrix from corpus
2019-05-30 22:01:00,899 : INFO : PROGRESS: at document #0/32768
2019-05-30 22:01:01,047 : INFO : PROGRESS: at document #10000/32768
2019-05-30 22:01:01,186 : INFO : PROGRESS: at document #20000/32768
2019-05-30 22:01:01,325 : INFO : PROGRESS: at document #30000/32768
2019-05-30 22:01:01,368 : INFO : created <32768x7160 sparse matrix of type '<class 'numpy.float32'>'
        with 97181 stored elements in Compressed Sparse Row format>
2019-05-30 22:01:01,368 : INFO : creating sparse shard #1
2019-05-30 22:01:01,368 : INFO : saving index shard to /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.1
2019-05-30 22:01:01,368 : INFO : saving SparseMatrixSimilarity object under /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.1, separately None
2019-05-30 22:01:01,375 : INFO : saved /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.1
2019-05-30 22:01:01,375 : INFO : loading SparseMatrixSimilarity object from /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.1
2019-05-30 22:01:01,378 : INFO : loaded /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.1
2019-05-30 22:01:01,424 : INFO : PROGRESS: fresh_shard size=0
2019-05-30 22:01:03,198 : INFO : PROGRESS: fresh_shard size=10000
2019-05-30 22:01:05,090 : INFO : PROGRESS: fresh_shard size=20000
2019-05-30 22:01:06,891 : INFO : PROGRESS: fresh_shard size=30000
2019-05-30 22:01:06,960 : INFO : computing similarity using batch query
2019-05-30 22:01:10,174 : INFO : creating sparse index
2019-05-30 22:01:10,174 : INFO : creating sparse matrix from corpus
2019-05-30 22:01:10,175 : INFO : PROGRESS: at document #0/30372
2019-05-30 22:01:10,303 : INFO : PROGRESS: at document #10000/30372
2019-05-30 22:01:10,447 : INFO : PROGRESS: at document #20000/30372
2019-05-30 22:01:10,580 : INFO : PROGRESS: at document #30000/30372
2019-05-30 22:01:10,589 : INFO : created <30372x7160 sparse matrix of type '<class 'numpy.float32'>'
        with 82365 stored elements in Compressed Sparse Row format>
2019-05-30 22:01:10,589 : INFO : creating sparse shard #2
2019-05-30 22:01:10,589 : INFO : saving index shard to /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.2
2019-05-30 22:01:10,589 : INFO : saving SparseMatrixSimilarity object under /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.2, separately None
2019-05-30 22:01:10,594 : INFO : saved /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.2
2019-05-30 22:01:10,594 : INFO : loading SparseMatrixSimilarity object from /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.2
2019-05-30 22:01:10,597 : INFO : loaded /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/index.2

```
",1,,,,,,,,,1
98,https://github.com/RaRe-Technologies/gensim/issues/1785,1785,"[{'id': 175640, 'node_id': 'MDU6TGFiZWwxNzU2NDA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/feature', 'name': 'feature', 'color': '0b02e1', 'default': False, 'description': 'Issue described a new feature'}, {'id': 175642, 'node_id': 'MDU6TGFiZWwxNzU2NDI=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/wishlist', 'name': 'wishlist', 'color': 'd7e102', 'default': False, 'description': 'Feature request'}, {'id': 234670, 'node_id': 'MDU6TGFiZWwyMzQ2NzA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20medium', 'name': 'difficulty medium', 'color': '00d000', 'default': False, 'description': 'Medium issue: required good gensim understanding & python skills'}]",closed,2017-12-13 13:45:27+00:00,,1,Add smart information retrieval system for TFIDF,"https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System

The current [TFIDF](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/tfidfmodel.py) model uses natural TF and IDF for computing TFIDF. The idea is to try various transformation like logarithmic, augmented,boolean etc. before computing the vectors.

More about this - http://www.cs.odu.edu/~jbollen/IR04/readings/article1-29-03.pdf and https://nlp.stanford.edu/IR-book/pdf/06vect.pdf

Will send a PR tomorrow.
",,,,,,,,,,1
168,https://github.com/RaRe-Technologies/gensim/issues/1960,1960,"[{'id': 233081, 'node_id': 'MDU6TGFiZWwyMzMwODE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20easy', 'name': 'difficulty easy', 'color': '00ff00', 'default': False, 'description': 'Easy issue: required small fix'}]",closed,2018-03-07 18:00:04+00:00,,5,similarity_matrix: suspicous column indexing logic when tfidf model supplied,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Description
Expected gensim.models.keyedvectors WordEmbeddingsKeyedVectors property similarity_matrix to respect the reordering of terms dictated by a supplied tfidf model. This does not seem to be the case.  

#### Steps/Code/Corpus to Reproduce
This will be an unconventional bug report because I did not find this bug ""in the wild"".  In adapting the code for another purpose, I noticed some logic which might be problematic. I will 

- point out where I think the mistake lies in the original context
- show what corrected the problem in my context
- provide the test case that demonstrates correctness in my context 

##### Original Context: 

https://github.com/RaRe-Technologies/gensim/blob/f9669bb8a0b5b4b45fa8ff58d951a11d3178116d/gensim/models/keyedvectors.py#L516
```
        for row_number, w1_index in enumerate(word_indices):
            ...
            w1 = dictionary[w1_index]
            ...
            # Traverse upper triangle columns.
            if matrix_order <= nonzero_limit + 1:  # Traverse all columns.
                columns = (
                    (w2_index, self.similarity(w1, dictionary[w2_index]))
                    for w2_index in range(w1_index + 1, matrix_order)
                    if w1_index != w2_index and dictionary[w2_index] in self.vocab)
            ...
            for w2_index, similarity in columns:
                # Ensure that we don't exceed `nonzero_limit` by mirroring the upper triangle.
                if similarity > threshold and matrix_nonzero[w2_index] <= nonzero_limit:
                    element = similarity**exponent
                    matrix[w1_index, w2_index] = element
                    matrix_nonzero[w1_index] += 1
                    matrix[w2_index, w1_index] = element
                    matrix_nonzero[w2_index] += 1
```
A red flag for me was that while we consider the ""word_indices"" mapping for the row index, we are never considering it for the column index. Even in the case of rows, notice that we are filling ""matrix"" at the end without regard for the reordering that was supposed to have occurred. 

##### A Correction in Levenshtein Context: 

```
for row_number, w1_index in enumerate(word_indices):
...
    # Traverse upper triangle columns.
    # TODO: determine community preference for pylev.levenshtein or distance.levenshtein
    import pylev
    columns = []
    for col_number in range(row_number + 1, matrix_order):
        if row_number != col_number:
            w2_index = word_indices[col_number]
            w1 = dictionary[w1_index]
            w2 = dictionary[w2_index]
            similarity = pylev.levenshtein(w1, w2)/\
                         float(max(len(w1), len(w2)))
            columns.append((col_number, similarity))

    for col_number, similarity in columns:
        element = alpha * (1 - similarity) ** beta
        matrix[row_number, col_number] = element
        matrix[col_number, row_number] = element
```

##### Test Case
Consider the mini corpus:
```
mini_texts = [['abc'], ['lab', 'abc'], ['bad', 'abc']]
mini_dict = gensim.corpora.Dictionary(mini_texts)
mini_corpus = [mini_dict.doc2bow(text) for text in mini_texts]
mini_tfidf = gensim.models.TfidfModel(mini_corpus)
```
- The default ordering of terms is in order of appearance: 'abc', 'lab', 'bad'. 
- The Levenshtein similarity matrix returned without supplying tfidf will be: 
 ```
matrix([[1.        , 0.00740741, 0.        ],
        [0.00740741, 1.        , 0.00740741],
        [0.        , 0.00740741, 1.        ]], dtype=float32)


```

However, since 'abc' appears in every document, the tfidf coefficient will be zero. The ordering with respect to the tfidf weight will be: 'lab', 'bad', 'abc'. The default Levenshtein similarity scores are pretty symmetric, there are some reorderings for which the matrix will be the same. However, this example is contrived to so that the tfidf reordering will break the symmetry of the original Levenshtein similarity scores: 

```
matrix([[1.        , 0.00740741, 0.00740741],
        [0.00740741, 1.        , 0.        ],
        [0.00740741, 0.        , 1.        ]], dtype=float32)

```
We see that the revised logic correctly returns the reordered scores. The original logic returned the first matrix when the tfidf model was supplied to the function. A test case for the similarity_matrix function would need to be constructed so that a reordering of terms would lead to a definitive reordering of the relevant scores in the resulting matrix. 

#### Conclusion
While I have not taken the time to prove definitively that there is a bug in the original context, a unit test should be written to cover the case of a supplied tfidf reordering the terms in similarity_matrix. This is a high-risk piece of logic. If there is a bug, and a supplied tfidf isn't being incorporated properly, eventually SoftCosineSimilarity will return nonsense scores when fed documents transformed by the same tfidf model.  

#### Relevant Code
similarity_matrix: 
https://github.com/RaRe-Technologies/gensim/blob/f9669bb8a0b5b4b45fa8ff58d951a11d3178116d/gensim/models/keyedvectors.py#L440

existing unit tests: 
https://github.com/RaRe-Technologies/gensim/blob/f9669bb8a0b5b4b45fa8ff58d951a11d3178116d/gensim/test/test_keyedvectors.py#L30

See also related: #1961 
#### Versions
Darwin-15.6.0-x86_64-i386-64bit
('Python', '2.7.13 (default, Apr  4 2017, 08:46:44) \n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]')
('NumPy', '1.14.1')
('SciPy', '1.0.0')
('gensim', '3.4.0')
('FAST_VERSION', 0)


<!-- Thanks for contributing! -->

",,,,,,,,,,1
396,https://github.com/RaRe-Technologies/gensim/issues/2375,2375,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}]",closed,2019-02-06 15:17:25+00:00,,0,Off-by-one counting error in TFIDF,"Building a TFIDF model outputs this in the log:

```
INFO - 2019-02-06 16:09:54,670 - resulting dictionary: Dictionary(10000 unique tokens: [u'writings', u'foul', u'prefix', u'woods', u'hanging']...)
INFO - 2019-02-06 16:10:01,718 - collecting document frequencies
INFO - 2019-02-06 16:10:01,718 - PROGRESS: processing document #0
INFO - 2019-02-06 16:10:02,162 - calculating IDF weights for 1701 documents and 9999 features (2220559 matrix non-zeros)
```

Note the ""9999 features"", despite the dictionary having 10000 features. (any dictionary and any number of features -- the actual values don't matter, it's always off-by-one).

This seems to be a bug introduced in [this refactoring](https://github.com/RaRe-Technologies/gensim/commit/af01bc702f7ad48ed9ecacb256d9f6f7da6d3bc4#diff-896942304f9417b1b8842823b19d7431R128).

Not sure if this is an isolated problem or what other places are affected by similar ""optimizations"".",,,,,,,,,,1
442,https://github.com/RaRe-Technologies/gensim/issues/2444,2444,[],closed,2019-04-11 14:42:42+00:00,,7,Problem while passing wlocal function in tfidf model ,"#### Problem description
I tried to use math.sqrt function on term frequency when computing TF-IDF model as you declare in script documentation:

```
wlocals : function, optional
            Function for local weighting, default for `wlocal` is :func:`~gensim.utils.identity`
            (other options: :func:`math.sqrt`, :func:`math.log1p`, etc).
```
========================================
Gensim implementation:

```python
tf_array = self.wlocal(np.array(tf_array))
vector = [
   (termid, tf * self.idfs.get(termid))
   for termid, tf in zip(termid_array, tf_array) if abs(self.idfs.get(termid, 0.0)) > self.eps
]
```

Error:

```
  File ""/home/xmedved1/.local/lib/python3.6/site-packages/gensim/models/tfidfmodel.py"", line 432, in __getitem__
    tf_array = self.wlocal(np.array(tf_array))
TypeError: only size-1 arrays can be converted to Python scalars
```
======================================

Fix if I want to use  math.sqrt:

```python
tf_array = np.array(tf_array)
vector = [
    (termid, self.wlocal(tf) * self.idfs.get(termid))
    for termid, tf in zip(termid_array, tf_array) if abs(self.idfs.get(termid, 0.0)) > self.eps
]
```

==========================================
gensim version 3.7.2

==========================================
I don't know if this is a problem to fix, because in Gensim implementation I can do some normalization that includes all items in list (something like tf = n_i_j / sum(n_k_j) where i,k is token and i!=k and j is the document  鈥? this is not allowed in my fix). So I thing the problem is the documentation of wlocal parameter.

Best regards M
",,,,,,,,,,1
481,https://github.com/RaRe-Technologies/gensim/issues/2501,2501,[],closed,2019-05-24 21:54:12+00:00,,2,All vectors in lsi_model[corpus_tfidf] are not of num_topics length passed to LsiModel,"I am using a combination of TF-IDF and LSI as shown in this [tutorial](https://radimrehurek.com/gensim/tut2.html) to arrive at `corpus_lsi = lsi[corpus_tfidf]`. I am then in need of unpacking this TransformedCorpus into a num_documents x num_topics matrix, but am unable to do so because not all of the individual vectors in `corpus_lsi` end up being the length of `num_topics` that I defined while using `models.LsiModel`.

I have opted to use 1000 topics but, 4 out of 5 times, each document vector does not have 1000 elements. Instead, 1 or 2 will be 999 or 998 elements in length. I attempted to resolve this by setting `num_topics` to 990 but the issue persists just the same at a lower value. I am unable to provide the dataset I am using, but below is the code I have written:

This works just fine:
```
id2word = corpora.Dictionary(text_dict)
texts = text_dict

# Term Document Frequency (bag of words)
corpus = [id2word.doc2bow(text) for text in texts]

# TF-IDF
tfidf = TfidfModel(corpus, smartirs='ntc')
corpus_tfidf = tfidf[corpus]

# LSI: 1000 Topics
lsi_model = LsiModel(corpus=corpus_tfidf, id2word=id2word,
                     num_topics=1000, decay=0.5)

corpus_lsi = lsi_model[corpus_tfidf]
```

Converting `corpus_lsi` to a matrix runs without error, but does not work as expected:
```
static_corpus_lsi = np.array([[j[1] for j in i] for i in corpus_lsi])
```

Most of the time, the result is that `static_corpus_lsi` is not of shape num_documents x 1000 as I expect them to be. Some document vectors are of length 1000 and 1 or 2 are of slightly less length.

Versions Info:
```
Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic
('Python', '2.7.15rc1 (default, Nov 12 2018, 14:31:15) \n[GCC 7.3.0]')
('NumPy', '1.16.3')
('SciPy', '1.2.1')
('gensim', '3.6.0')
('FAST_VERSION', 1)
```
",,,,,,,,,,1
908,https://github.com/RaRe-Technologies/gensim/issues/3191,3191,[],open,2021-07-13 15:41:06+00:00,,0,Similarity Interface of Gensim giving low similarity score for exact same documents with TfIdf + LdaModel,"<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

I am trying to implement a document similarity API using the LDA Model of Gensim. To experiment with the performance, I tried implementing it by training the LDA Model with TfIdf vectors instead of the normal BoW corpus as described in the documentation. The problem which I am facing is that while using the Similarity API of Gensim for creating the index and finding out the similarity score, what I encountered is that if I try to match the same document with itself, sometimes, the Similarity values are not ~1. The values which I get are as low as ~0.06. This does not occur ALL the time, but for some documents only. I tested this again with 229 documents matching each document with itself, and I found that 45 of the documents give results less than 0.98, sometimes giving values like 0.65, 0.41 and similar. I would like some help on this, whether I am doing something wrong or is there a potential bug in the interface.

#### Steps/code/corpus to reproduce

##### Minimal Code used for testing:
NOTE: The corpus which I am using is confidential and limited to only the organization. So I would not be able to share the training corpus here, but for the reference, I will share the output of the LDA model which I am getting for 3 out of the 45 documents (these documents are getting the lowest score matching with themselves) which I used for testing. Hope that should be sufficient for debugging.
```python
docs = [ 'Document 1 as a string', 'Document 2 as a string', 'Document 3 as a string', 'and so on.....' ]
cleaned_docs = list(map(clean_function, docs))                 # Here, clean_function return tokens for each string. So, cleaned_docs is essentially a list of list of strings List[List[str]]
bow_corpus = [dictionary.doc2bow(i) for i in cleaned_docs]
tfidf_corpus = tfidf_model[bow_corpus]
lda_corpus = lda_model[tfidf_corpus]
index = Similarity(lda_corpus)
sims = index[lda_corpus]                   # Getting similarity for all combinations. Got a (229, 229) array for my case
final_sims = np.diag(sims)                 # Getting similarity with itself
print(final_sims)                          # Getting very low score with some docs
```
##### Output Vectors of LDAModel for 3 documents:
```python
[[(0, 0.17789464), (2, 0.03806097), (12, 0.2273234), (14, 0.08613937), (21, 0.13261063), (22, 0.17807047), (36, 0.058883864)],
[(1, 0.43381935), (2, 0.14317065), (3, 0.07986226), (36, 0.062136874)], 
[(0, 0.32848448), (2, 0.16667062), (14, 0.0485237), (15, 0.11480027), (18, 0.086506054), (35, 0.059970867)]]
```

```python
print(lda_model.lifecycle_events)
[{'msg': 'trained LdaModel(num_terms=100000, num_topics=40, decay=0.5, chunksize=2000) in 2080.85s', 'datetime': '2021-06-30T09:32:52.611017', 'gensim': '4.0.1', 'python': '3.6.12 (default, Jun 28 2021, 13:17:01) \n[GCC 5.4.0 20160609]', 'platform': 'Linux-4.4.0-1128-aws-x86_64-with-debian-stretch-sid', 'event': 'created'}, {'fname_or_handle': 'models/lda.model', 'separately': ""['expElogbeta', 'sstats']"", 'sep_limit': 10485760, 'ignore': ['state', 'dispatcher', 'id2word'], 'datetime': '2021-06-30T09:32:52.703852', 'gensim': '4.0.1', 'python': '3.6.12 (default, Jun 28 2021, 13:17:01) \n[GCC 5.4.0 20160609]', 'platform': 'Linux-4.4.0-1128-aws-x86_64-with-debian-stretch-sid', 'event': 'saving'}, {'fname_or_handle': 'models/lda.model', 'separately': ""['expElogbeta', 'sstats']"", 'sep_limit': 10485760, 'ignore': ['state', 'dispatcher', 'id2word'], 'datetime': '2021-06-30T09:32:58.758961', 'gensim': '4.0.1', 'python': '3.6.12 (default, Jun 28 2021, 13:17:01) \n[GCC 5.4.0 20160609]', 'platform': 'Linux-4.4.0-1128-aws-x86_64-with-debian-stretch-sid', 'event': 'saving'}, {'fname': 'models/lda.model', 'datetime': '2021-07-13T20:26:59.671845', 'gensim': '4.0.1', 'python': '3.8.10 (default, Jun  2 2021, 10:49:15) \n[GCC 9.4.0]', 'platform': 'Linux-5.8.0-59-generic-x86_64-with-glibc2.29', 'event': 'loaded'}]
```
```python
print(tfidf_model.lifecycle_events)
[{'msg': 'calculated IDF weights for 1174674 documents and 100000 features (100645197 matrix non-zeros)', 'datetime': '2021-06-30T08:58:10.744582', 'gensim': '4.0.1', 'python': '3.6.12 (default, Jun 28 2021, 13:17:01) \n[GCC 5.4.0 20160609]', 'platform': 'Linux-4.4.0-1128-aws-x86_64-with-debian-stretch-sid', 'event': 'initialize'}, {'fname_or_handle': 'models/tfidf.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-06-30T08:58:10.744733', 'gensim': '4.0.1', 'python': '3.6.12 (default, Jun 28 2021, 13:17:01) \n[GCC 5.4.0 20160609]', 'platform': 'Linux-4.4.0-1128-aws-x86_64-with-debian-stretch-sid', 'event': 'saving'}, {'fname': 'models/tfidf.model', 'datetime': '2021-07-13T20:27:03.663529', 'gensim': '4.0.1', 'python': '3.8.10 (default, Jun  2 2021, 10:49:15) \n[GCC 9.4.0]', 'platform': 'Linux-5.8.0-59-generic-x86_64-with-glibc2.29', 'event': 'loaded'}]
```

#### Versions

Please provide the output of:

```python
import platform; print(platform.platform())
Linux-5.8.0-59-generic-x86_64-with-glibc2.29

import sys; print(""Python"", sys.version)
Python 3.8.10 (default, Jun  2 2021, 10:49:15) 
[GCC 9.4.0]

import struct; print(""Bits"", 8 * struct.calcsize(""P""))
Bits 64

import numpy; print(""NumPy"", numpy.__version__)
NumPy 1.21.0

import scipy; print(""SciPy"", scipy.__version__)
SciPy 1.7.0

import gensim; print(""gensim"", gensim.__version__)
gensim 4.0.1

from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION)
FAST_VERSION 1
```",,,,,,,,,,1
943,https://github.com/RaRe-Technologies/gensim/issues/3244,3244,[],open,2021-10-05 23:00:33+00:00,,3,Self provided normalization function is not used.,"https://github.com/RaRe-Technologies/gensim/blob/5bec27767ad40712e8912d53a896cb2282c33880/gensim/models/tfidfmodel.py#L525

`self.normalize = matutils.unitvec` does not allow users to use self-defined normalization function. ",,,,,,,,,,1
944,https://github.com/RaRe-Technologies/gensim/issues/3245,3245,[],closed,2021-10-05 23:12:51+00:00,,1,Log level control,"https://github.com/RaRe-Technologies/gensim/blob/5bec27767ad40712e8912d53a896cb2282c33880/gensim/models/tfidfmodel.py#L447

I think it a good idea to control this logger output (give an option to disable) to avoid spamming the terminal.",,,,,,,,,,1
79,https://github.com/RaRe-Technologies/gensim/issues/1735,1735,[],closed,2017-11-22 05:26:22+00:00,,1,Doc2Vec most_similar not ranking docs proprerly,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

#### Description
I am on a task to rank a bunch of documents (1000~2000) against set of keyphrases (n grams) associated with given subject.
These are the steps being taken:

1. Extracted keyphrases for all the candidate documents.
2. Trained Gensim's Doc2Vec with list of keyphrases generated at step 1. Each doc has 20-30 keyphrases (2 - 4 grams each)
3. Inferred the target keyphrase vector (for the subject) using the model. Calculate most similar

Upon checking the resultant documents, I find that none of the key phrases from the subject were found in the top documents. Ironically, some of the documents, (which my another frequentist match approach algorithm) find most relevant are ranked negatively by doc2vec.

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Steps/Code/Corpus to Reproduce

Example:
```
documents = []
for item in resultTextData:
    keyphrases = item[""meta_keyphrases""].lower().split("","")
       
    id = []
    id.append(str(item[""text_id""]))
    documents.append(TaggedDocument(keyphrases,id))

model = Doc2Vec(size=50, window=8, min_count=5, workers=4, dm=0, negative=0,iter=100)
model.build_vocab(documents)
model.train(documents, total_examples=model.corpus_count,epochs=model.iter)

#inference
kp = model.infer_vector(targetDocument)
# sims = model.docvecs.most_similar([kp],topn=1150)  # gives you top 10 document tags and their cosine similarity
sims = model.docvecs.most_similar(positive=[model.infer_vector(targetDocument)])  # gives you top 10 document tags and their cosine similarity
print ""sims"", sims
```

",,,1,,,,,,1,
261,https://github.com/RaRe-Technologies/gensim/issues/2109,2109,[],closed,2018-06-28 12:58:23+00:00,,1,TypeError: '>' not supported between instances of 'function' and 'function',"def makeSummary(sentences, best_sentence, query, summary_length, lambta, IDF):
    summary = [best_sentence]
    print(""line 264  summary "",summary)
    sum_len = len(best_sentence.getPreProWords())

    MMRval = {}

    # keeping adding sentences until number of words exceeds summary length
    while (sum_len < summary_length):
        MMRval = {}

        for sent in sentences:
            MMRval[sent] = MMRScore(sent, query, summary, lambta, IDF)
            print(""type of MMR value   "",MMRval)

        **maxxer = max(MMRval, key=MMRval.get)**                                           #getting error here
        summary.append(maxxer)
        sentences.remove(maxxer)
        sum_len += len(maxxer.getPreProWords())

    return summary
=================================================
FULL CODE

import nltk
import os
import math
import string
import re
import sentence

from nltk.corpus import stopwords


# ---------------------------------------------------------------------------------
# Description	: Function to preprocess the files in the document cluster before
#				  passing them into the MMR summarizer system. Here the sentences
#				  of the document cluster are modelled as sentences after extracting
#				  from the files in the folder path. 
# Parameters	: file_name, name of the file in the document cluster
# Return 		: list of sentence object
# ---------------------------------------------------------------------------------
def processFile(file_name):
    # read file from provided folder path
    f = open(file_name, 'r')
    text_0 = f.read()

    # extract content in TEXT tag and remove tags
    text_1 = re.search(r""<TEXT>.*</TEXT>"", text_0, re.DOTALL)
    text_1 = re.sub(""<TEXT>\n"", """", text_1.group(0))
    text_1 = re.sub(""\n</TEXT>"", """", text_1)

    # replace all types of quotations by normal quotes
    text_1 = re.sub(""\n"", "" "", text_1)

    text_1 = re.sub(""\"""", ""\"""", text_1)
    text_1 = re.sub(""''"", ""\"""", text_1)
    text_1 = re.sub(""``"", ""\"""", text_1)

    text_1 = re.sub("" +"", "" "", text_1)
    # segment data into a list of sentences
    sentence_token = nltk.data.load('tokenizers/punkt/english.pickle')
    lines = sentence_token.tokenize(text_1.strip())

    # setting the stemmer
    sentences = []
    porter = nltk.PorterStemmer()

    # modelling each sentence in file as sentence object
    for line in lines:

        # original words of the sentence before stemming
        originalWords = line[:]
        line = line.strip().lower()

        # word tokenization
        sent = nltk.word_tokenize(line)

        # stemming words
        stemmedSent = [porter.stem(word) for word in sent]
        # stemmedSent = filter(lambda x: x != '.' and x != '`' and x != ',' and x != '?' and x != ""'""
        #                                and x != '!' and x != '''""''' and x != ""''"" and x != ""'s"", stemmedSent)

        # list of sentence objects
        if stemmedSent != []:
            sentences.append(sentence.sentence(file_name, stemmedSent, originalWords))


    return sentences


# ---------------------------------------------------------------------------------
# Description	: Function to find the term frequencies of the words in the
#				  sentences present in the provided document cluster
# Parameters	: sentences, sentences of the document cluster
# Return 		: dictonary of word, term frequency score
# ---------------------------------------------------------------------------------
def TFs(sentences):
    # initialize tfs dictonary
    tfs = {}

    # for every sentence in document cluster
    print(""line   79   sentences    "", sentences)
    for sent in sentences:
        # retrieve word frequencies from sentence object
        wordFreqs = sent.getWordFreq()
        # print(""line 82 frequency "",wordFreqs)

        # for every word
        for word in wordFreqs.keys():
            # if word already present in the dictonary
            if tfs.get(word, 0) != 0:
                tfs[word] = tfs[word] + wordFreqs[word]
            # else if word is being added for the first time
            else:
                tfs[word] = wordFreqs[word]
    return tfs


# ---------------------------------------------------------------------------------
# Description	: Function to find the inverse document frequencies of the words in
#				  the sentences present in the provided document cluster 
# Parameters	: sentences, sentences of the document cluster
# Return 		: dictonary of word, inverse document frequency score
# ---------------------------------------------------------------------------------
def IDFs(sentences):
    N = len(sentences)
    print(""line 101 total number of sentences are   "", N)
    print(""line 104 sentences  "", sentences)
    idf = 0
    idfs = {}
    words = {}
    w2 = []
    # every sentence in our cluster
    for sent in sentences:
        #print(""line 113 success"", (str(sent)))
        #print(""print  line 114  "", sent.getPreProWords())
        #print(""print  line 115  "", type(sent.getPreProWords()))
        #print(""print  line 116  "", len(list(sent.getPreProWords())))
        # every word in a sentence
        for word in sent.getPreProWords():
            print(""line 118 word is   "", word)
            # not to calculate a word's IDF value more than once
            if sent.getWordFreq().get(word, 0) != 0:
                words[word] = words.get(word, 0) + 1

    # for each word in words
    for word in words:
        n = words[word]
        # avoid zero division errors
        try:
            w2.append(n)
            idf = math.log10(float(N) / n)
        except ZeroDivisionError:
            idf = 0

        # reset variables
        idfs[word] = idf
        print(""words    "", word)
        print(""idf   "", idf)

    return idfs


# ---------------------------------------------------------------------------------
# Description	: Function to find TF-IDF score of the words in the document cluster
# Parameters	: sentences, sentences of the document cluster
# Return 		: dictonary of word, TF-IDF score
# ---------------------------------------------------------------------------------
def TF_IDF(sentences):
    # Method variables
    tfs = TFs(sentences)
    idfs = IDFs(sentences)
    retval = {}
    print(""tfs is    "" + str(tfs))
    print(""idfs is   "" + str(idfs))

    # for every word
    for word in tfs:
        # calculate every word's tf-idf
        print(""words  are    "" + word)
        print(""tfs of word is   "" + str(tfs[word]))
        # print(""idfs of word is   "" + str(idfs[word]))
        tf_idfs = tfs[word] * idfs[word]

        # add word and its tf-idf score to dictionary
        if retval.get(tf_idfs, None) == None:
            retval[tf_idfs] = [word]
        else:
            retval[tf_idfs].append(word)
    print(""TF IDF return value is   "", retval)

    return retval


# ---------------------------------------------------------------------------------
# Description	: Function to find the sentence similarity for a pair of sentences
#				  by calculating cosine similarity
# Parameters	: sentence1, first sentence
#				  sentence2, second sentence to which first sentence has to be compared
#				  IDF_w, dictinoary of IDF scores of words in the document cluster
# Return 		: cosine similarity score
# ---------------------------------------------------------------------------------
def sentenceSim(sentence1, sentence2, IDF_w):
    numerator = 0
    denominator = 0

    for word in sentence2.getPreProWords():
        numerator += sentence1.getWordFreq().get(word, 0) * sentence2.getWordFreq().get(word, 0) * IDF_w.get(word,
                                                                                                             0) ** 2

    for word in sentence1.getPreProWords():
        denominator += (sentence1.getWordFreq().get(word, 0) * IDF_w.get(word, 0)) ** 2

    # check for divide by zero cases and return back minimal similarity
    try:
        return numerator / math.sqrt(denominator)
    except ZeroDivisionError:
        return float(""-inf"")


# ---------------------------------------------------------------------------------
# Description	: Function to build a query of n words on the basis of TF-IDF value
# Parameters	: sentences, sentences of the document cluster
#				  IDF_w, IDF values of the words
#				  n, desired length of query (number of words in query)
# Return 		: query sentence consisting of best n words
# ---------------------------------------------------------------------------------
def buildQuery(sentences, TF_IDF_w, n):
    # sort in descending order of TF-IDF values
    scores = TF_IDF_w.keys()
    scores = sorted(scores,reverse=True)

    # scores.sort(reverse=True)

    i = 0
    j = 0
    queryWords = []

    # select top n words
    while (i < n):
        words = TF_IDF_w[scores[j]]
        for word in words:
            queryWords.append(word)
            i = i + 1
            if (i > n):
                break
        j = j + 1

    # return the top selected words as a sentence
    return sentence.sentence(""query"", queryWords, queryWords)


# ---------------------------------------------------------------------------------
# Description	: Function to find the best sentence in reference to the query
# Parameters	: sentences, sentences of the document cluster
#				  query, reference query
#				  IDF, IDF value of words of the document cluster
# Return 		: best sentence among the sentences in the document cluster
# ---------------------------------------------------------------------------------
def bestSentence(sentences, query, IDF):
    best_sentence = None
    maxVal = float(""-inf"")

    for sent in sentences:
        similarity = sentenceSim(sent, query, IDF)

        if similarity > maxVal:
            best_sentence = sent
            maxVal = similarity
    sentences.remove(best_sentence)

    return best_sentence


# ---------------------------------------------------------------------------------
# Description	: Function to create the summary set of a desired number of words 
# Parameters	: sentences, sentences of the document cluster
#				  best_sentnece, best sentence in the document cluster
#				  query, reference query for the document cluster
#				  summary_length, desired number of words for the summary
#				  labmta, lambda value of the MMR score calculation formula
#				  IDF, IDF value of words in the document cluster 
# Return 		: name 
# ---------------------------------------------------------------------------------
def makeSummary(sentences, best_sentence, query, summary_length, lambta, IDF):
    summary = [best_sentence]
    print(""line 264  summary "",summary)
    sum_len = len(best_sentence.getPreProWords())

    MMRval = {}

    # keeping adding sentences until number of words exceeds summary length
    while (sum_len < summary_length):
        MMRval = {}

        for sent in sentences:
            MMRval[sent] = MMRScore(sent, query, summary, lambta, IDF)
            print(""type of MMR value   "",MMRval)

       # maxxer = max(MMRval, key=MMRval.get)

        maxxer = max(MMRval, key=MMRval.get)
        summary.append(maxxer)
        sentences.remove(maxxer)
        sum_len += len(maxxer.getPreProWords())

    return summary







# ---------------------------------------------------------------------------------
# Description	: Function to calculate the MMR score given a sentence, the query
#				  and the current best set of sentences
# Parameters	: Si, particular sentence for which the MMR score has to be calculated
#				  query, query sentence for the particualr document cluster
#				  Sj, the best sentences that are already selected
#				  lambta, lambda value in the MMR formula
#				  IDF, IDF value for words in the cluster
# Return 		: name 
# ---------------------------------------------------------------------------------
def MMRScore(Si, query, Sj, lambta, IDF):
    Sim1 = sentenceSim(Si, query, IDF)
    l_expr = lambta * Sim1
    value = [float(""-inf"")]

    for sent in Sj:
        Sim2 = sentenceSim(Si, sent, IDF)
        value.append(Sim2)

    r_expr = (1 - lambta) * max(value)
    MMR_SCORE = l_expr - r_expr

    return MMRScore


# -------------------------------------------------------------
#	MAIN FUNCTION
# -------------------------------------------------------------
if __name__ == '__main__':

    # set the main Document folder path where the subfolders are present
    main_folder_path = os.getcwd() + ""/Documents""

    # read in all the subfolder names present in the main folder
    for folder in os.listdir(main_folder_path):

        print(""Running MMR Summarizer for files in folder: "", folder)
        # for each folder run the MMR summarizer and generate the final summary
        curr_folder = main_folder_path + ""/"" + folder

        # find all files in the sub folder selected
        files = os.listdir(curr_folder)
        print(""line 326  files are   "" + str(files))

        sentences = []

        for file in files:
            sentences = sentences + processFile(curr_folder + ""/"" + file)
        print(""line 332 sentences are   "" + str(sentences))
        print(""line 333    "", type(sentences))
        print(""lines 334 "", type(sentences))
        print(""line 335   "", type(sentences[1]))

        # calculate TF, IDF and TF-IDF scores
        #TF_w = TFs(sentences)
        #print(""line 338 TF_w is   "", TF_w)
        IDF_w = IDFs(sentences)
        print(""line 340 IDF-w is   "", IDF_w)
        TF_IDF_w = TF_IDF(sentences)

        # build query; set the number of words to include in our query
        query = buildQuery(sentences, TF_IDF_w, 10)

        # pick a sentence that best matches the query
        best1sentence = bestSentence(sentences, query, IDF_w)

        # build summary by adding more relevant sentences
        summary = makeSummary(sentences, best1sentence, query, 100, 0.5, IDF_w)

        final_summary = """"
        for sent in summary:
            final_summary = final_summary + sent.getOriginalWords() + ""\n""
        final_summary = final_summary[:-1]
        results_folder = os.getcwd() + ""/MMR_results""
        with open(os.path.join(results_folder, (str(folder) + "".MMR"")), ""w"") as fileOut:
            fileOut.write(final_summary)
",1,,,,,,,,1,
17,https://github.com/RaRe-Technologies/gensim/issues/1597,1597,[],closed,2017-09-23 16:45:02+00:00,,2,Error when most_important_docs in summarizer.py is None,"```python
In [0]: gensim.__version__
Out [0]: '2.3.0'
```
Description:
I was working on a set of Chinese sentences. And when I call the function `gensim.summarization.summarize()`.The Error below was occurred: 
``` python
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/gensim/summarization/summarizer.py"", line 215, in summarize
    extracted_sentences = _extract_important_sentences(sentences, corpus, most_important_docs, word_count)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/gensim/summarization/summarizer.py"", line 114, in _extract_important_sentences
    important_sentences = _get_important_sentences(sentences, corpus, important_docs)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/gensim/summarization/summarizer.py"", line 89, in _get_important_sentences
    return [sentences_by_corpus[tuple(important_doc)] for important_doc in important_docs]
TypeError: 'NoneType' object is not iterable
```
It seems that the important_doc is None, and NoneType cannot be iterated. 
Well, I didn't learn so much of TextRank Algorithm, and I am trying to go on to work. Maybe someone can tell what is happening? 

PS: Sorry that I could not afford the test case I was using, for it is full of Chinese name. (If someone ask me privately, maybe i could.) Anyway, there is a bug in it. For some reason the `most_important_docs` at line 212, `summarizer.py` is `None`. This situation should be handled properly. I suppose that summarize() should return None or raise some other Error for debugging when `most_important_docs` is `None`. Or even better, optimize the implementation of TextRank Algorithm, which is fully out of my ability for now... 
``` python
>>> s = '`a string full of different Chinese name, with the number of more than 1 thousand.`'
>>> import gensim.summarization as gsum
>>> gsum.summarize(s)
```
",,,,,,,,,1,
20,https://github.com/RaRe-Technologies/gensim/issues/1602,1602,[],closed,2017-09-28 18:13:06+00:00,,1,DTM tutorial steps extracted all the same topics for all timestamps,"Hi,

   I've tried following the DTM tutorial almost line by line to extract topics from a corpus.  Using scikit-learn, I can see that there are distinct topics in the corpus, and also that the topics change as a function of time.  However, dtm doesn't seem to produce anything that makes sense.

    Here is my 4-member list of documents:

      documents = [[u'worth', u'deli', u'month', u'enjoy', u'bacon', u'second', u'street', u'even', u'new',u'ever', u'told', u'met', u'daughter', u'brought', u'spoke', u'would', u'asset', u'type', u'tell', u'phone', u'hold', u'room', u'work', u'fresher', u'give', u'household', u'want', u'keep', u'bag', u'end', u'thing', u'travel', u'hot', u'lay', u'third', u'alka', u'greet', u'green', u'enter', u'order', u'wine', u'better', u'easier', u'bread', u'meat', u'went', u'forgot', u'laid', u'got', u'free', u'local', u'tilapia', u'shopper', u'top', u'took', u'hadnt', u'matter', u'mind', u'manner', u'seen', u'seem', u'brandon', u'blue', u'though', u'regular', u'stood', u'make', u'layout', u'aldi', u'stop', u'watermelon', u'twice', u'bad', u'respond', u'best', u'said', u'away', u'yogurt', u'never', u'fault', u'tone', u'speak', u'bathroom', u'beef', u'much', u'save', u'n', u'dept', u'near', u'neat', u'gave', u'sushi', u'cant', u'im', u'ie', u'hand', u'butter', u'kept', u'contact', u'left', u'previous', u'spread', u'gift', u'cooler', u'right', u'old', u'deal', u'ice', u'discount', u'super', u'dinner', u'wrap', u'way', u'head', u'offer', u'upset', u'drive', u'check', u'floor', u'tie', u'smell', u'kroger', u'longer', u'loyal', u'time', u'mile', u'milk', u'anyway', u'item', u'team', u'guy', u'pork', u'sign', u'cost', u'patient', u'clerk', u'along', u'wait', u'box', u'shift', u'love', u'extra', u'prefer', u'market', u'visit', u'live', u'creamer', u'checkout', u'today', u'cashier', u'car', u'product', u'may', u'date', u'man', u'talk', u'still', u'thank', u'main', u'non', u'within', u'name', u'receipt', u'happen', u'correct', u'cart', u'california', u'card', u'care', u'place', u'think', u'frequent', u'first', u'one', u'long', u'ring', u'open', u'tomorrow', u'size', u'checker', u'wide', u'raincheck', u'nicer', u'say', u'saw', u'take', u'sure', u'price', u'knew', u'paid', u'seafood', u'later', u'sale', u'senior', u'shop', u'show', u'bright', u'ground', u'slow', u'enough', u'get', u'seldom', u'husband', u'across', u'sacker', u'come', u'restroom', u'pop', u'mark', u'ive', u'everyday', u'pay', u'trip', u'week', u'assist', u'fruit', u'without', u'reward', u'money', u'young', u'rest', u'speed', u'except', u'real', u'around', u'read', u'world', u'tend', u'either', u'perfer', u'highest', u'wheat', u'ok', u'act', u'road', u'coupon', u'strip', u'area', u'start', u'low', u'heb', u'regard', u'toilet', u'certain', u'incorrect', u'personnel', u'event', u'poor', u'wife', u'missouri', u'follow', u'smile', u'fav', u'fat', u'ticket', u'list', u'small', u'neighborhood', u'andor', u'past', u'section', u'prior', u'amount', u'pick', u'put', u'eye', u'two', u'particular', u'town', u'none', u'del', u'scan', u'gallon', u'huge', u'okay', u'rude', u'short', u'egg', u'help', u'paper', u'might', u'good', u'return', u'food', u'found', u'hard', u'expect', u'beyond', u'health', u'print', u'friday', u'guess', u'pleasant', u'quick', u'reason', u'ask', u'major', u'dont', u'feel', u'number', u'done', u'gourmet', u'least', u'store', u'relationship', u'park', u'kind', u'wasnt', u'sell', u'self', u'jonmark', u'reach', u'plan', u'clear', u'part', u'clean', u'fine', u'find', u'express', u'cheaper', u'cold', u'x', u'see', u'close', u'grocer', u'sold', u'water', u'last', u'mega', u'whole', u'point', u'ran', u'pm', u'citizen', u'understand', u'demand', u'look', u'frozen', u'bill', u'smart', u'error', u'higher', u'walmart', u'lower', u'person', u'cut', u'also', u'eager', u'easter', u'complaint', u'big', u'bit', u'like', u'lost', u'often', u'back', u'understood', u'run', u'question', u'fast', u'doubt', u'line', u'us', u'nice', u'lane', u'e', u'came', u'fresh', u'hello', u'code', u'go', u'sent', u'download', u'cell', u'let', u'great', u'larger', u'survey', u'app', u'use', u'next', u'sore', u'didnt', u'high', u'friendlier', u'instead', u'stand', u'watch', u'light', u'counter', u'move', u'lb', u'front', u'day', u'stock', u'special', u'red', u'que', u'could', u'david', u'lot', u'shelf', u'bother', u'need', u'pharmacist', u'request', u'hi', u'fact', u'bring', u'keyera', u'chicken', u'staff', u'fuel', u'familiar', u'closer', u'neither', u'bought', u'job', u'instant', u'etc', u'comment', u'gone', u'walk', u'commend', u'ad', u'treat', u'vanilla', u'ill', u'almost', u'began', u'cream', u'upon', u'center', u'well', u'thought', u'rush', u'less', u'half', u'match', u'know', u'desk', u'helpful', u'soft', u'church', u'home', u'although', u'justin', u'buy', u'brand', u'bagger', u'made', u'whether', u'wish', u'smooth', u'cake', u'problem'], [u'four', u'deli', u'enjoy', u'second', u'street', u'even', u'new', u'ever', u'told', u'met', u'cart', u'daughter', u'credit', u'brought', u'total', u'spoke', u'would', u'call', u'asset', u'recommend', u'tell', u'warm', u'must', u'room', u'work', u'give', u'want', u'end', u'turn', u'far', u'hot', u'mess', u'earlier', u'wrong', u'lot', u'greet', u'green', u'fan', u'order', u'wine', u'better', u'mgmt', u'bread', u'meat', u'roast', u'went', u'side', u'saturday', u'got', u'free', u'rang', u'shopper', u'top', u'listen', u'took', u'esta', u'ran', u'mind', u'ginger', u'manner', u'seen', u'seem', u'blue', u'regular', u'dog', u'came', u'layout', u'explain', u'aldi', u'stop', u'watermelon', u'bag', u'bad', u'steak', u'steam', u'best', u'said', u'away', u'muy', u'bien', u'yogurt', u'never', u'theresa', u'speak', u'much', u'life', u'xxx', u'worker', u'near', u'neat', u'k', u'cant', u'im', u'id', u'suggest', u'make', u'rain', u'hand', u'butter', u'kept', u'game', u'left', u'save', u'apart', u'gift', u'right', u'old', u'deal', u'ice', u'corn', u'discount', u'super', u'afternoon', u'way', u'head', u'offer', u'true', u'fort', u'later', u'drive', u'mold', u'trip', u'floor', u'na', u'til', u'roll', u'felt', u'weekend', u'phone', u'loyal', u'time', u'mile', u'milk', u'brown', u'level', u'item', u'team', u'quick', u'guy', u'round', u'pork', u'sign', u'cost', u'appear', u'clerk', u'address', u'along', u'wait', u'love', u'extra', u'prefer', u'visit', u'live', u'thru', u'checkout', u'today', u'cashier', u'car', u'soup', u'freezer', u'alway', u'product', u'may', u'floral', u'man', u'basket', u'talk', u'cold', u'still', u'mail', u'main', u'half', u'son', u'wont', u'name', u'en', u'receipt', u'year', u'girl', u'es', u'correct', u'card', u'care', u'thing', u'place', u'think', u'frequent', u'first', u'one', u'long', u'ring', u'open', u'size', u'given', u'plastic', u'checker', u'wide', u'pre', u'raincheck', u'say', u'saw', u'take', u'sure', u'price', u'knew', u'paid', u'seafood', u'sale', u'senior', u'shop', u'shot', u'slow', u'behind', u'get', u'husband', u'concern', u'across', u'sacker', u'come', u'por', u'squash', u'case', u'ive', u'restroom', u'pay', u'check', u'week', u'assist', u'fruit', u'without', u'reward', u'comment', u'outdoor', u'money', u'rest', u'brianna', u'rose', u'except', u'around', u'either', u'satisfecha', u'tube', u'broken', u'found', u'throw', u'ok', u'area', u'hey', u'start', u'low', u'heb', u'enough', u'younger', u'longer', u'asst', u'gone', u'ad', u'certain', u'personnel', u'event', u'poor', u'missouri', u'month', u'tx', u'program', u'smile', u'fall', u'list', u'small', u'neighborhood', u'past', u'pass', u'stood', u'section', u'full', u'reason', u'prior', u'amount', u'pick', u'ask', u'select', u'eye', u'two', u'particular', u'none', u'hour', u'learn', u'clutter', u'prompt', u'scan', u'huge', u'rather', u'rude', u'soda', u'reflect', u'short', u'began', u'help', u'soon', u'paper', u'late', u'friendliest', u'might', u'good', u'return', u'food', u'bigger', u'fish', u'hard', u'expect', u'todo', u'friday', u'guess', u'pleasant', u'dont', u'feel', u'number', u'interact', u'least', u'storm', u'store', u'option', u'deshawn', u'kind', u'toward', u'wasnt', u'self', u'also', u'part', u'cereal', u'fine', u'find', u'access', u'express', u'cheaper', u'breast', u'set', u'see', u'bare', u'sea', u'close', u'sold', u'water', u'last', u'mega', u'whole', u'load', u'point', u'sweet', u'throughout', u'due', u'pm', u'gal', u'understand', u'look', u'frozen', u'bill', u'pack', u'pound', u'kroger', u'belong', u'tiljuanna', u'higher', u'walmart', u'older', u'spent', u'person', u'spend', u'cut', u'eager', u'app', u'complaint', u'big', u'redeem', u'bit', u'often', u'back', u'pet', u'though', u'per', u'run', u'bc', u'within', u'question', u'fast', u'line', u'us', u'nice', u'clean', u'lane', u'e', u'fresh', u'hello', u'go', u'young', u'lower', u'sent', u'booth', u'download', u'cell', u'chose', u'let', u'great', u'larger', u'survey', u'opinion', u'use', u'next', u'meet', u'didnt', u'high', u'bend', u'six', u'instead', u'stock', u'farm', u'watch', u'ronald', u'counter', u'houston', u'move', u'perfect', u'lo', u'coupon', u'weber', u'front', u'day', u'red', u'que', u'could', u'put', u'keep', u'sack', u'fair', u'system', u'shelf', u'need', u'gluten', u'pharmacist', u'face', u'fact', u'bring', u'chicken', u'staff', u'fuel', u'local', u'hope', u'familiar', u'rush', u'werent', u'twice', u'stuff', u'state', u'bought', u'job', u'thank', u'etc', u'co', u'walk', u'cent', u'vanilla', u'ripe', u'almost', u'cream', u'difficult', u'upon', u'center', u'well', u'thought', u'usual', u'less', u'bell', u'add', u'citizen', u'match', u'know', u'desk', u'like', u'dept', u'page', u'home', u'although', u'justin', u'three', u'scanner', u'buy', u'brand', u'bagger', u'eat', u'made', u'wish', u'problem', u'threw', u'special', u'sick', u'stay'], [u'four', u'hate', u'restock', u'worth', u'deli', u'frozen', u'enjoy', u'almond', u'bacon', u'second', u'street', u'ill', u'even', u'new', u'ever', u'told', u'men', u'met', u'brought', u'spoke', u'would', u'call', u'type', u'tell', u'hurt', u'phone', u'must', u'word', u'room', u'work', u'ms', u'mr', u'fresher', u'give', u'want', u'end', u'turn', u'hot', u'mess', u'earlier', u'wrong', u'greet', u'order', u'feedback', u'oven', u'better', u'easier', u'interrupt', u'one', u'bank', u'bread', u'meat', u'went', u'side', u'forgot', u'got', u'free', u'rang', u'shopper', u'top', u'took', u'esta', u'ran', u'raw', u'manner', u'seen', u'seem', u'though', u'regular', u'came', u'teresa', u'layout', u'explain', u'stop', u'bar', u'bad', u'best', u'said', u'away', u'gentleman', u'yogurt', u'never', u'speak', u'beef', u'three', u'beer', u'much', u'dept', u'near', u'neat', u'cant', u'im', u'id', u'suggest', u'make', u'split', u'hand', u'butter', u'kept', u'left', u'yet', u'previous', u'ham', u'save', u'gave', u'night', u'sacker', u'right', u'old', u'deal', u'bottom', u'ice', u'discount', u'dinner', u'wan', u'wal', u'gift', u'way', u'head', u'true', u'sale', u'face', u'check', u'floor', u'na', u'smell', u'roll', u'felt', u'diet', u'younger', u'faster', u'loyal', u'fact', u'time', u'mild', u'milk', u'row', u'item', u'quick', u'guy', u'round', u'pork', u'sign', u'run', u'clerk', u'slow', u'wait', u'box', u'bob', u'love', u'extra', u'prefer', u'super', u'visit', u'live', u'today', u'cashier', u'car', u'soup', u'kym', u'sunday', u'product', u'may', u'date', u'man', u'basket', u'talk', u'cold', u'still', u'group', u'thank', u'mail', u'gel', u'non', u'half', u'son', u'name', u'rock', u'pizza', u'el', u'receipt', u'es', u'correct', u'cart', u'card', u'care', u'thing', u'place', u'think', u'frequent', u'first', u'fast', u'ring', u'open', u'size', u'plastic', u'checker', u'wide', u'bag', u'say', u'saw', u'tshey', u'greek', u'note', u'take', u'green', u'sure', u'normal', u'price', u'paid', u'seafood', u'senior', u'shop', u'shot', u'bright', u'label', u'enough', u'get', u'across', u'come', u'por', u'turkey', u'mark', u'mart', u'case', u'ive', u'clutter', u'buyl', u'pay', u'trip', u'week', u'without', u'money', u'flavor', u'except', u'around', u'read', u'traffic', u'either', u'satisfecha', u'tube', u'wheat', u'broken', u'found', u'comparison', u'act', u'road', u'coupon', u'area', u'start', u'low', u'heb', u'expect', u'toilet', u'certain', u'file', u'fill', u'incorrect', u'personnel', u'event', u'trash', u'u', u'missouri', u'lack', u'dollar', u'month', u'tx', u'program', u'smile', u'woman', u'far', u'fat', u'list', u'small', u'neighborhood', u'andor', u'past', u'stood', u'section', u'public', u'movement', u'full', u'theresa', u'search', u'ahead', u'amount', u'pick', u'select', u'eye', u'two', u'taken', u'diamond', u'particular', u'none', u'hour', u'prompt', u'scan', u'accept', u'huge', u'rather', u'sandwich', u'okay', u'rude', u'short', u'egg', u'help', u'soon', u'paper', u'might', u'good', u'return', u'food', u'weight', u'fish', u'hard', u'finish', u'beyond', u'todo', u'shipment', u'friday', u'pleasant', u'reason', u'put', u'teach', u'fruit', u'dont', u'feel', u'number', u'smaller', u'done', u'miss', u'interact', u'least', u'station', u'jed', u'store', u'part', u'kind', u'cleaner', u'wasnt', u'self', u'reach', u'clean', u'fine', u'find', u'express', u'cheaper', u'silk', u'courteous', u'see', u'close', u'sold', u'water', u'last', u'mega', u'whole', u'point', u'sweet', u'throughout', u'belt', u'due', u'pm', u'look', u'pack', u'kroger', u'higher', u'walmart', u'lower', u'older', u'spent', u'person', u'spend', u'patel', u'cut', u'big', u'bit', u'often', u'back', u'pet', u'patient', u'within', u'question', u'long', u'line', u'checkout', u'us', u'nice', u'competitor', u'ago', u'lane', u'inout', u'fresh', u'hello', u'go', u'young', u'sent', u'cell', u'rotten', u'let', u'great', u'larger', u'survey', u'app', u'use', u'next', u'salad', u'didnt', u'kassidi', u'process', u'high', u'friendlier', u'instead', u'stand', u'counter', u'move', u'meatseafood', u'perfect', u'la', u'willing', u'holiday', u'front', u'day', u'stock', u'special', u'que', u'could', u'ask', u'keep', u'sack', u'floral', u'lot', u'shelf', u'bother', u'neel', u'need', u'mix', u'pharmacist', u'bring', u'chicken', u'longer', u'staff', u'jar', u'fuel', u'hope', u'familiar', u'rush', u'werent', u'stuff', u'grab', u'salmon', u'closer', u'state', u'bought', u'job', u'etc', u'comment', u'gone', u'walk', u'respect', u'ad', u'decent', u'treat', u'almost', u'cream', u'drink', u'center', u'well', u'thought', u'usual', u'less', u'bell', u'match', u'know', u'desk', u'like', u'soft', u'italian', u'home', u'avoid', u'although', u'buy', u'gluten', u'brand', u'hi', u'bagger', u'eat', u'also', u'made', u'wish', u'cake', u'problem', u'display', u'girl'], [u'worth', u'deli', u'school', u'enjoy', u'bacon', u'second', u'street', u'even', u'new', u'ever', u'told', u'never', u'credit', u'brought', u'total', u'spoke', u'would', u'call', u'type', u'tell', u'oscar', u'phone', u'hold', u'must', u'room', u'work', u'ms', u'give', u'want', u'end', u'thing', u'greet', u'order', u'better', u'easier', u'bread', u'meat', u'went', u'got', u'free', u'small', u'rang', u'shopper', u'listen', u'took', u'manner', u'seen', u'seem', u'though', u'regular', u'came', u'teresa', u'layout', u'earn', u'bag', u'bad', u'best', u'said', u'away', u'gentleman', u'yogurt', u'speak', u'bathroom', u'beef', u'three', u'much', u'life', u'worker', u'meyer', u'n', u'dept', u'near', u'neat', u'cant', u'im', u'make', u'rain', u'hand', u'butter', u'kept', u'contact', u'left', u'yet', u'save', u'gave', u'gift', u'night', u'right', u'old', u'deal', u'ice', u'corn', u'discount', u'super', u'dinner', u'nixon', u'son', u'wal', u'way', u'war', u'head', u'offer', u'true', u'sale', u'door', u'download', u'check', u'na', u'nd', u'til', u'felt', u'diet', u'longer', u'loyal', u'time', u'mile', u'milk', u'veg', u'brown', u'cool', u'item', u'quick', u'yall', u'pork', u'cost', u'appear', u'current', u'clerk', u'water', u'wait', u'love', u'extra', u'prefer', u'visit', u'live', u'thru', u'checkout', u'today', u'cashier', u'effort', u'car', u'product', u'may', u'floral', u'man', u'basket', u'still', u'thank', u'non', u'name', u'receipt', u'year', u'space', u'cart', u'card', u'place', u'think', u'first', u'one', u'long', u'ring', u'open', u'checker', u'wide', u'coconut', u'raincheck', u'sad', u'say', u'saw', u'take', u'sure', u'price', u'knew', u'seafood', u'later', u'drive', u'senior', u'shop', u'show', u'bright', u'corner', u'ground', u'slow', u'enough', u'black', u'get', u'behind', u'across', u'come', u'turkey', u'case', u'ive', u'etc', u'pay', u'trip', u'week', u'finish', u'assist', u'fruit', u'without', u'reward', u'comment', u'money', u'flavor', u'real', u'around', u'isiss', u'either', u'found', u'ok', u'stand', u'luck', u'road', u'mart', u'area', u'start', u'low', u'heb', u'regard', u'taylor', u'ad', u'cream', u'peak', u'wife', u'missouri', u'month', u'children', u'program', u'th', u'smile', u'woman', u'far', u'list', u'yasmin', u'neighborhood', u'tea', u'cash', u'past', u'pass', u'section', u'full', u'theresa', u'pick', u'put', u'eye', u'two', u'diamond', u'dion', u'particular', u'glad', u'none', u'learn', u'accept', u'huge', u'rather', u'okay', u'rude', u'short', u'help', u'paper', u'might', u'good', u'food', u'pregnant', u'fish', u'hard', u'expect', u'beyond', u'event', u'hill', u'friday', u'pleasant', u'reason', u'ask', u'dont', u'feel', u'number', u'least', u'store', u'park', u'part', u'kind', u'toward', u'wasnt', u'sell', u'self', u'lit', u'also', u'reach', u'clear', u'mylnn', u'clean', u'gold', u'fine', u'find', u'express', u'see', u'close', u'sold', u'last', u'mega', u'whole', u'bell', u'sweet', u'church', u'belt', u'due', u'pm', u'look', u'frozen', u'budget', u'pack', u'kroger', u'higher', u'walmart', u'lower', u'person', u'five', u'patel', u'easter', u'big', u'bit', u'often', u'back', u'run', u'question', u'fast', u'line', u'us', u'nice', u'helpful', u'lane', u'e', u'fresh', u'hello', u'go', u'young', u'access', u'let', u'great', u'larger', u'survey', u'app', u'use', u'next', u'sort', u'salad', u'meet', u'didnt', u'high', u'instead', u'toddler', u'stock', u'stop', u'counter', u'move', u'bunch', u'perfect', u'lb', u'coupon', u'front', u'day', u'circular', u'lentil', u'special', u'could', u'keep', u'date', u'accent', u'lot', u'shelf', u'need', u'pharmacist', u'upset', u'fact', u'chicken', u'staff', u'fuel', u'local', u'hope', u'familiar', u'werent', u'closer', u'bought', u'job', u'walk', u'scanner', u'main', u'ripe', u'almost', u'upon', u'center', u'well', u'thought', u'usual', u'less', u'obtain', u'citizen', u'know', u'like', u'onion', u'home', u'buy', u'brand', u'bagger', u'made', u'wish', u'cake', u'problem', u'threw', u'display', u'monday']]

   Then I do:

     class DTMcorpus(corpora.textcorpus.TextCorpus):

         def get_texts(self):
             return self.input

       def __len__(self):
            return len(self.input)

    corpus = DTMcorpus(documents)
    time_seq = [2,2]

 and 

    model = DtmModel(dtm_path, corpus, time_seq, num_topics=10,
                  id2word=corpus.dictionary, initialize_lda=True)

 everything seems to run ok, so finally I print out the first 3 topics from the first time slice:

    topics1 = model.show_topic(topicid=0, time=0, num_words=10)
    topics2 = model.show_topic(topicid=1,time=0,num_words=10)
    topics3 = model.show_topic(topicid=2,time=0,num_words=10)

which gives me

    ([(0.0013106159895150723, u'per'),
    (0.0013106159895150723, u'access'),
    (0.0013106159895150723, u'saturday'),
    (0.0013106159895150723, u'es'),
    (0.0013106159895150723, u'concern'),
    (0.0013106159895150723, u'throughout'),
    (0.0013106159895150723, u'appear'),
    (0.0013106159895150723, u'pet'),
    (0.0013106159895150723, u'tube'),
    (0.0013106159895150723, u'muy')],
    [(0.0013106159895150723, u'per'),
    (0.0013106159895150723, u'access'),
    (0.0013106159895150723, u'saturday'),
    (0.0013106159895150723, u'es'),
    (0.0013106159895150723, u'concern'),
    (0.0013106159895150723, u'throughout'),
    (0.0013106159895150723, u'appear'),
    (0.0013106159895150723, u'pet'),
    (0.0013106159895150723, u'tube'),
    (0.0013106159895150723, u'muy')],
    [(0.0013106159895150717, u'per'),
    (0.0013106159895150717, u'access'),
    (0.0013106159895150717, u'saturday'),
    (0.0013106159895150717, u'es'),
    (0.0013106159895150717, u'concern'),
    (0.0013106159895150717, u'throughout'),
    (0.0013106159895150717, u'appear'),
    (0.0013106159895150717, u'pet'),
    (0.0013106159895150717, u'tube'),
    (0.0013106159895150717, u'muy')])   

  I also get the exact same topics for any other time slice. 
   
   Could you please help.   Thank you!




",,,,,,,,,1,
55,https://github.com/RaRe-Technologies/gensim/issues/1676,1676,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 233081, 'node_id': 'MDU6TGFiZWwyMzMwODE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20easy', 'name': 'difficulty easy', 'color': '00ff00', 'default': False, 'description': 'Easy issue: required small fix'}, {'id': 721000065, 'node_id': 'MDU6TGFiZWw3MjEwMDAwNjU=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/good%20first%20issue', 'name': 'good first issue', 'color': '7057ff', 'default': True, 'description': 'Issue for new contributors (not required gensim understanding + very simple)'}]",closed,2017-10-31 18:31:14+00:00,,3,Bug in sklearn_api.hdp and in sklearn_api.ldamodel,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Description
The new `sklearn_api.hdp` (and also `ldamodel`) modules and/or their combination with `matutils.Sparse2Corpus` yield the wrong results when fitting models from sklearn vectorizers or other sklearn-styled sparse matrices, since they are stored in CSR format. This error might occur in other sklearn_api classes, too.

I believe that either the default value of Sparse2Corpus constructor's `documents_columns` parameter should be changed:

```python
class Sparse2Corpus(object):
    """"""
    Convert a matrix in scipy.sparse format into a streaming gensim corpus.
    This is the mirror function to `corpus2csc`.
    """"""

    # Change this to def __init__(self, sparse, documents_columns=False) ?
    def __init__(self, sparse, documents_columns=True):
        if documents_columns:
            self.sparse = sparse.tocsc()
        else:
            self.sparse = sparse.tocsr().T  # make sure shape[1]=number of docs (needed in len())
```

or the following call should include that `documents_columns=False`:

```python
# Same happens in lda model
class HdpTransformer(TransformerMixin, BaseEstimator):
    ...
    def fit(self, X, y=None):
        """"""
        Fit the model according to the given training data.
        Calls gensim.models.HdpModel
        """"""
        if sparse.issparse(X):
            corpus = matutils.Sparse2Corpus(X) # Change to matutils.Sparse2Corpus(X, False) ?
        ...
```


<!-- Example: Vocabulary size is not what I expected when training Word2Vec. -->

#### Steps/Code/Corpus to Reproduce

Example: The number of processed documents corresponds to the number of features.

```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from gensim.sklearn_api.hdp import HdpTransformer

# Small dataset configuration
ngs = fetch_20newsgroups()
samples = ngs.data[:100]

# Simple count vectorization
vectorizer = CountVectorizer()
# x is a sparse matrix
x = vectorizer.fit_transform(samples)
print(""%d documents, %d features"" % x.shape) # 100 documents, 6547 features

inv_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}

# Train a HDP
hdp_transformer = HdpTransformer(inv_vocab)
hdp_transformer.fit(x)

# Should be 100 but got 6547
print(""Processed documents %d"" % hdp_transformer.gensim_model.m_num_docs_processed) # 6547
```

#### Expected Results
We should expect that the hdp gensim model had processed the 100 documents in samples.
```python
print(""%d documents, %d features"" % x.shape)
# 100 documents, 6547 features
```

#### Actual Results
HDP processed 6547 documents
```python
print(""Processed documents: %d"" % hdp_transformer.gensim_model.m_num_docs_processed)
# Processed documents: 6547
```

#### Workaround, for the record
To make it work correctly, the sparse matrix `x` should be transposed.
```python
vectorizer = CountVectorizer()
x = vectorizer.fit_transform(samples)
inv_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}

hdp_transformer = HdpTransformer(inv_vocab)
hdp_transformer.fit(x.T)
```

#### Versions
```
Windows-8.1-6.3.9600
('Python', '2.7.13 |Anaconda custom (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)]')
('NumPy', '1.12.1')
('SciPy', '1.0.0')
('gensim', '3.0.1')
('FAST_VERSION', 0)
```

",,,,,,,,,1,
67,https://github.com/RaRe-Technologies/gensim/issues/1711,1711,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}]",open,2017-11-12 13:20:30+00:00,,2,segment_wiki doesn't handle links properly,"According to @menshikh-iv , segment_wiki will convert the external link `[http://www.example.org/ link text]` to ` ` (nothing).

This will lead to incorrect extraction: `[http://www.example.org/ This article] talks about such issues.` will become plain text `talks about this issues.`, which is not good / expected behaviour.

I see two possible resolutions (controlled by a CLI switch?):

1. keep only the anchor text: `This article talks about such issues.`
2. keep the anchor text, and add the URL after it: `This article http://www.example.org/ talks about such issues`
",,,,,,,,,1,
70,https://github.com/RaRe-Technologies/gensim/issues/1716,1716,[],closed,2017-11-14 14:09:46+00:00,,3,model.syn0norm[0] TypeError: 'NoneType' object is not subscriptable,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Description
Error while extracting normalized vectors from syn0norm
model.syn0norm[0] TypeError: 'NoneType' object is not subscriptable

#### Steps/Code/Corpus to Reproduce

```
import gensim

model = gensim.models.KeyedVectors.load_word2vec_format(config.GOOGLE_WORD2VEC_MODEL, binary=True)
print(model.syn0norm[0])
```


#### Expected Results
the first vectors in the syn0norm

#### Actual Results
model.syn0norm[0] TypeError: 'NoneType' object is not subscriptable


#### Versions
Darwin-17.0.0-x86_64-i386-64bit
Python 3.5.4 |Continuum Analytics, Inc.| (default, Aug 14 2017, 12:43:10) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]
NumPy 1.13.3
SciPy 1.0.0
gensim 2.3.0
FAST_VERSION 1


<!-- Thanks for contributing! -->

",,,,,,,,,1,
73,https://github.com/RaRe-Technologies/gensim/issues/1719,1719,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 234670, 'node_id': 'MDU6TGFiZWwyMzQ2NzA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20medium', 'name': 'difficulty medium', 'color': '00d000', 'default': False, 'description': 'Medium issue: required good gensim understanding & python skills'}]",open,2017-11-15 19:25:10+00:00,,6,Overlapping phrases,"#### Description
See below, but the most common phrase is not being detected as a bigram by `Phrases`.

#### Steps/Code/Corpus to Reproduce
```
s = ['i went to toys r us',
     'we are at babies r us',
     'where is babies r us',
     'do you mean toys r us']
s = [_.split() for _ in s]

phrases = Phrases(s, min_count=1, threshold=2)
list(phrases.export_phrases(s))
```
#### Actual Results
```
# [(b'toys r', 3.625),
#  (b'babies r', 3.625),
#  (b'babies r', 3.625),
#  (b'toys r', 3.625)]
```

#### Expected Results
Given that the phrase ""r us"" is the most common in the sentences, I would expect that this would be a common extracted phrase. There is mention in #794 that there is intent to avoid overlapping bigrams. Depending on the use case, overlapping bigrams may not be a problem if all of them pass the specified threshold. Maybe consider an overlapping_phrases flag? 

#### Versions
Windows-7-6.1.7601-SP1
Python 3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]
NumPy 1.13.3
SciPy 1.0.0
gensim 2.3.0
FAST_VERSION 0


",,,,,,,,,1,
124,https://github.com/RaRe-Technologies/gensim/issues/1862,1862,"[{'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}]",closed,2018-01-28 15:01:20+00:00,,6,EOFError while on make_wiki,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Description
Midway running `gensim.scripts.make_wiki` on latest WikiPedia article dump, EOFError was spit out, which stopped the processing.
<!-- Example: Vocabulary size is not what I expected when training Word2Vec. -->

```
2018-01-28 15:45:35,500 : INFO : resulting dictionary: Dictionary(2000000 unique tokens: [u'tripolitan', u'ftdna', u'padanagan', u'soestdijk', u'farmobil']...)
2018-01-28 15:45:35,573 : INFO : adding document #4180000 to Dictionary(2000000 unique tokens: [u'tripolitan', u'ftdna', u'padanagan', u'soestdijk', u'farmobil']...)
Process InputQueue-4:
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/multiprocessing/process.py"", line 267, in _bootstrap
    self.run()
  File ""/home/psukys/.local/lib/python2.7/site-packages/gensim/utils.py"", line 845, in run
    wrapped_chunk = [list(chunk)]
  File ""/home/psukys/.local/lib/python2.7/site-packages/gensim/corpora/wikicorpus.py"", line 361, in <genexpr>
    ((text, self.lemmatize, title, pageid, tokenization_params)
  File ""/home/psukys/.local/lib/python2.7/site-packages/gensim/corpora/wikicorpus.py"", line 221, in extract_pages
    for elem in elems:
  File ""/home/psukys/.local/lib/python2.7/site-packages/gensim/corpora/wikicorpus.py"", line 206, in <genexpr>
    elems = (elem for _, elem in iterparse(f, events=(""end"",)))
  File ""<string>"", line 100, in next
EOFError: compressed file ended before the logical end-of-stream was detected
```

#### Steps/Code/Corpus to Reproduce
Run on [latest](https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2) (21-Jan-2018 21:27         14705396388)
```
python -m gensim.script.make_wiki
```
",,,,,,,,,1,
136,https://github.com/RaRe-Technologies/gensim/issues/1888,1888,"[{'id': 175640, 'node_id': 'MDU6TGFiZWwxNzU2NDA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/feature', 'name': 'feature', 'color': '0b02e1', 'default': False, 'description': 'Issue described a new feature'}, {'id': 234670, 'node_id': 'MDU6TGFiZWwyMzQ2NzA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20medium', 'name': 'difficulty medium', 'color': '00d000', 'default': False, 'description': 'Medium issue: required good gensim understanding & python skills'}, {'id': 708430967, 'node_id': 'MDU6TGFiZWw3MDg0MzA5Njc=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/performance', 'name': 'performance', 'color': 'd93f0b', 'default': False, 'description': 'Issue related to performance (in HW meaning)'}]",open,2018-02-08 23:22:16+00:00,,3,Optimize sparse * random dense matrix multiply in LsiModel ,"Progress tracking bug.

- [x] Fork https://github.com/brianmingus/gensim
- [x] Find initial work https://gist.github.com/brianmingus/ff6c3e9816c2cb7de0efc17d9bed06e6
- [x] Get most up-to-date CSparse. This was extracted from SuiteSparse-5.1.0 and placed in gensim/gensim/CSparse
- [x] Find example gensim Cython compilation and refactor our setup.py code to comply
- [x] Refactor the python code into a gensim.csparse module
- [ ] Refactor the test code into a gensim-style test that ensures the new code produces identical results and is faster
  - [x] Figure out why pmultiply is returning an array of 0s 
  - [ ] Get the test to actually run
- [x] Current import structure is gensim.csparse.psparse is this ideal
- [x] Modify LsiModel to use pmultiply
  - [ ] Test that LsiModel works
- [x] Look into other uses of sparsetools, see what can be removed
- [ ] Fix import structure. csparse/gaxpy seems to clobber csparse/psparse/pmultiply",,,,,,,,,1,
196,https://github.com/RaRe-Technologies/gensim/issues/2005,2005,"[{'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}]",closed,2018-03-28 05:25:01+00:00,,2,LDA Python3 - TypeError: '>' not supported between instances of 'float' and 'NoneType',"I have trained an LDA model using gensim library and I am using it to extract topic vectors of a document and I am using the following code

    def clean_doc(data_string):    
        global en_stop
        tokenizer = RegexpTokenizer(r'\w+') #Create appropriate tokenizer
        p_stemmer = PorterStemmer() #Create object from Porter Stemmer
        #clean and tokenize document string
        raw = data_string.lower()
        tokens = tokenizer.tokenize(raw)
        # remove stop words from tokens
        stopped_tokens = [i for i in tokens if not i in en_stop]
        # stem tokens
        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
        return stemmed_tokens
    
    def infer_lda_vector(s, dictionary, model, dimensions):
        #s = s.decode('utf-8')
        vector = [0.0]*dimensions
        s = clean_doc(s)
        bow_vector = dictionary.doc2bow(s)   
        lda_vector = model[bow_vector]            
        for i in lda_vector:
            vector[i[0]] = i[1]
        return vector

I call it as follows:

    text = ""this a test""
    lda_vector = infer_lda_vector(text, dictionary, lda_model, 300)

This exact piece of code was working when I was using Python2.7 but when I updated my system to Python3.x, its throwing the following error:

    ---------------------------------------------------------------------------
    TypeError                                 Traceback (most recent call last)
    <ipython-input-36-723f03d03620> in <module>()
          1 text = ""this a a test""
    ----> 2 lda_vector = infer_lda_vector(text, dictionary, lda_model, 300)
          3 lda_vector
    
    <ipython-input-34-885205b68d9e> in infer_lda_vector(s, dictionary, model, dimensions)
         34     s = clean_doc(s)
         35     bow_vector = dictionary.doc2bow(s)
    ---> 36     lda_vector = model[bow_vector]
         37     for i in lda_vector:
         38         vector[i[0]] = i[1]
    
    C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\ldamodel.py in __getitem__(self, bow, eps)
       1158             `(topic_id, topic_probability)` 2-tuples.
       1159         """"""
    -> 1160         return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
       1161 
       1162     def save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs):
    
    C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\ldamodel.py in get_document_topics(self, bow, minimum_probability, minimum_phi_value, per_word_topics)
        979         if minimum_probability is None:
        980             minimum_probability = self.minimum_probability
    --> 981         minimum_probability = max(minimum_probability, 1e-8)  # never allow zero values in sparse output
        982 
        983         if minimum_phi_value is None:
    
    TypeError: '>' not supported between instances of 'float' and 'NoneType'

    
What am I doing wrong?
",,,,,,,,,1,
198,https://github.com/RaRe-Technologies/gensim/issues/2009,2009,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}]",closed,2018-03-30 14:38:51+00:00,,9,LDA Python3 - TypeError: '>' not supported between instances of 'float' and 'NoneType',"I had reported the same issues some time back and it was fixed by reinstallation but the issue has reappeared and reinstallation is not resolving it.

I have trained an LDA model using gensim library and I am using it to extract topic vectors of a document and I am using the following code

    def clean_doc(data_string):    
        global en_stop
        tokenizer = RegexpTokenizer(r'\w+') #Create appropriate tokenizer
        p_stemmer = PorterStemmer() #Create object from Porter Stemmer
        #clean and tokenize document string
        raw = data_string.lower()
        tokens = tokenizer.tokenize(raw)
        # remove stop words from tokens
        stopped_tokens = [i for i in tokens if not i in en_stop]
        # stem tokens
        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
        return stemmed_tokens
    
    def infer_lda_vector(s, dictionary, model, dimensions):
        #s = s.decode('utf-8')
        print (s)
        vector = [0.0]*dimensions
        s = clean_doc(s)
        print (s)
        bow_vector = dictionary.doc2bow(s)   
        lda_vector = model[bow_vector]            
        for i in lda_vector:
            vector[i[0]] = i[1]
        return vector

I call it as follows:

    text = ""this a test""
    lda_vector = infer_lda_vector(text, dictionary, lda_model, 300)

This exact piece of code was working when I was using Python2.7 but when I updated my system to Python3.x, its throwing the following error:

```
this is a test on office and firefox
['test', 'offic', 'firefox']
[(4, 1), (85, 1), (1458, 1)]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-12-ddb10c8f6b5d> in <module>()
      1 text = ""this is a test on office and firefox""
----> 2 lda_vector = infer_lda_vector(text, dictionary, lda_model, 300)
      3 lda_vector

<ipython-input-9-62f10637e316> in infer_lda_vector(s, dictionary, model, dimensions)
     36     bow_vector = dictionary.doc2bow(s)
     37     print (bow_vector)
---> 38     lda_vector = model[bow_vector]
     39     print (lda_vector)
     40     vector = [0.0]*dimensions

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldamodel.py in __getitem__(self, bow, eps)
   1158             `(topic_id, topic_probability)` 2-tuples.
   1159         """"""
-> 1160         return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
   1161 
   1162     def save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldamodel.py in get_document_topics(self, bow, minimum_probability, minimum_phi_value, per_word_topics)
    979         if minimum_probability is None:
    980             minimum_probability = self.minimum_probability
--> 981         minimum_probability = max(minimum_probability, 1e-8)  # never allow zero values in sparse output
    982 
    983         if minimum_phi_value is None:

TypeError: '>' not supported between instances of 'float' and 'NoneType'
```

What am I doing wrong?
",,,,,,,,,1,
224,https://github.com/RaRe-Technologies/gensim/issues/2046,2046,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 234670, 'node_id': 'MDU6TGFiZWwyMzQ2NzA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20medium', 'name': 'difficulty medium', 'color': '00d000', 'default': False, 'description': 'Medium issue: required good gensim understanding & python skills'}, {'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}, {'id': 1602334164, 'node_id': 'MDU6TGFiZWwxNjAyMzM0MTY0', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/impact%20LOW', 'name': 'impact LOW', 'color': '0052cc', 'default': False, 'description': 'Low impact on affected users'}]",open,2018-05-12 04:50:03+00:00,,2,Error while importing old wiki-dump (2010) with `WikiCorpus`,"#### Description
When I try to load the following Wikicorpus from 2010 ([Link](https://dumps.wikimedia.org/archive/enwiki/20100312/enwiki-20100312-pages-articles.xml.bz2)) I get an error (see bellow)


#### Code
```python
from gensim.corpora import WikiCorpus, MmCorpus
import gensim
import pattern
import pickle
import logging
import os.path
import sys


if __name__ == '__main__':
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info(""running %s"", ' '.join(sys.argv))

    wiki_corpus = WikiCorpus('enwiki-20100312-pages-articles.xml.bz2', lemmatize=True)
    print('corpus loaded')
```

#### Expected Results
Correct processing of the wikipedia corpus


#### Actual Results
```
C:\Users\fabiansvenkarst\Documents\BA\Wiki_py27\venv\Scripts\python.exe C:/Users/fabiansvenkarst/Documents/BA/Wiki_py27/Wiki_corpus_Verarbeitung.py
C:\Users\fabiansvenkarst\Documents\BA\Wiki_py27\venv\lib\site-packages\gensim\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
2018-05-12 04:38:51,516 : INFO : running C:/Users/fabiansvenkarst/Documents/BA/Wiki_py27/Wiki_corpus_Verarbeitung.py
Traceback (most recent call last):
  File ""C:/Users/fabiansvenkarst/Documents/BA/Wiki_py27/Wiki_corpus_Verarbeitung.py"", line 20, in <module>
    wiki_corpus = WikiCorpus('enwiki-20100312-pages-articles.xml.bz2', lemmatize=True)
  File ""C:\Users\fabiansvenkarst\Documents\BA\Wiki_py27\venv\lib\site-packages\gensim\corpora\wikicorpus.py"", line 552, in __init__
    self.dictionary = dictionary or Dictionary(self.get_texts())
  File ""C:\Users\fabiansvenkarst\Documents\BA\Wiki_py27\venv\lib\site-packages\gensim\corpora\dictionary.py"", line 79, in __init__
    self.add_documents(documents, prune_at=prune_at)
  File ""C:\Users\fabiansvenkarst\Documents\BA\Wiki_py27\venv\lib\site-packages\gensim\corpora\dictionary.py"", line 187, in add_documents
    for docno, document in enumerate(documents):
  File ""C:\Users\fabiansvenkarst\Documents\BA\Wiki_py27\venv\lib\site-packages\gensim\corpora\wikicorpus.py"", line 587, in get_texts
    for group in utils.chunkize(texts, chunksize=10 * self.processes, maxsize=1):
  File ""C:\Users\fabiansvenkarst\Documents\BA\Wiki_py27\venv\lib\site-packages\gensim\utils.py"", line 1219, in chunkize
    for chunk in chunkize_serial(corpus, chunksize, as_numpy=as_numpy):
  File ""C:\Users\fabiansvenkarst\Documents\BA\Wiki_py27\venv\lib\site-packages\gensim\utils.py"", line 1153, in chunkize_serial
    wrapped_chunk = [list(itertools.islice(it, int(chunksize)))]
  File ""C:\Users\fabiansvenkarst\Documents\BA\Wiki_py27\venv\lib\site-packages\gensim\corpora\wikicorpus.py"", line 579, in <genexpr>
    ((text, self.lemmatize, title, pageid, tokenization_params)
  File ""C:\Users\fabiansvenkarst\Documents\BA\Wiki_py27\venv\lib\site-packages\gensim\corpora\wikicorpus.py"", line 370, in extract_pages
    ns = elem.find(ns_path).text
AttributeError: 'NoneType' object has no attribute 'text'
```

#### Versions
('Python', '2.7.11 (v2.7.11:6d1b6a68f775, Dec  5 2015, 20:40:30) [MSC v.1500 64 bit (AMD64)]')
('NumPy', '1.14.3')
('SciPy', '1.1.0')
('gensim', '3.4.0')
('FAST_VERSION', 0)

",,,,,,,,,1,
634,https://github.com/RaRe-Technologies/gensim/issues/2742,2742,[],closed,2020-02-01 15:23:21+00:00,,1,Can't get attribute 'Word2VecKeyedVectors' during model.load() when using `hashfxn`,"#### Problem description

Created a model using very straightforward code with a hash function to improve reproducibility (used in an academic setting):

```
MODEL_DIR = '.../somepath'
hash =
model = Word2Vec(
        corpus,
        size=SIZE,
        window=WINDOW,
        min_count=1,
        workers=WORKERS,
        iter=ITER,
        seed=100,
        hashfxn=hash
    )

model.save(MODEL_DIR)
```

The code above runs without problem. However, when trying to load using `Word2Vec.load(MODEL_PATH)` it would throw a large trace stack. To narrow it down and save everyone some reading, I isolated it down to this line:

> AttributeError: Can't get attribute 'hash' on <module '__main__' from 'plot/demo/embedding.py'>

I've saved multiple models in my project, none of which require the use of a hashing function. The same code can load those other models without a problem. This is the only one model so it's quite clearly something to do with the hashing.

#### Steps/code/corpus to reproduce

When trying to load the same model:

```
model = Word2Vec.load(MODEL_PATH)
```

Returns the following Traceback:
```
Traceback (most recent call last):
  File ""/Users/samuel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 1330, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""/Users/samuel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
  File ""/Users/samuel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
  File ""/Users/samuel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/gensim/utils.py"", line 426, in load
    obj = unpickle(fname)
  File ""/Users/samuel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/gensim/utils.py"", line 1384, in unpickle
    return _pickle.load(f, encoding='latin1')
AttributeError: Can't get attribute 'hash' on <module '__main__' from 'plot/demo/embedding.py'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""plot/demo/embedding.py"", line 11, in <module>
    model = Word2Vec.load(MODEL_PATH)
  File ""/Users/samuel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 1341, in load
    return load_old_word2vec(*args, **kwargs)
  File ""/Users/samuel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/gensim/models/deprecated/word2vec.py"", line 172, in load_old_word2vec
    old_model = Word2Vec.load(*args, **kwargs)
  File ""/Users/samuel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/gensim/models/deprecated/word2vec.py"", line 1641, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""/Users/samuel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/gensim/models/deprecated/old_saveload.py"", line 87, in load
    obj = unpickle(fname)
  File ""/Users/samuel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/gensim/models/deprecated/old_saveload.py"", line 379, in unpickle
    return _pickle.loads(file_bytes, encoding='latin1')
AttributeError: Can't get attribute 'Word2VecKeyedVectors' on <module 'gensim.models.deprecated.keyedvectors' from '/Users/samuel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/gensim/models/deprecated/keyedvectors.py'>
```

Removing the hashing function will see the `Word2Vec.load()` function works again. I'm not using any keyed vectors helper function, just a straight up Word2Vec save and load. I hope it's easy to reproduce given how it only affects model created with a hash function. 

#### Versions

```
# Platform
Darwin-19.2.0-x86_64-i386-64bit
# Python
Python 3.6.7 | packaged by conda-forge | (default, Nov 20 2018, 18:20:05) 
[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.37)]
# numpy version
NumPy 1.15.4
# SciPy version
SciPy 1.1.0
# gensim version
gensim 3.8.1
# word2vec
FAST_VERSION 1
```",,,,,,1,,1,,
694,https://github.com/RaRe-Technologies/gensim/issues/2834,2834,[],closed,2020-05-10 15:58:31+00:00,,4,api.load of fasttext vectors returns Word2VecKeyedVectors ,"![image](https://user-images.githubusercontent.com/14852840/81504015-09d65a00-92b5-11ea-8b6c-21082e4479ff.png)

Firstly, thanks for the great library. 
When loading FastText using the API, the model returned is actually one with Word2VecKeyedVectors , not FastTextKeyedVectors, hence this causes issues for OOV loading
```
Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic
Python 3.6.9 (default, Apr 18 2020, 01:56:04) 
[GCC 8.4.0]
NumPy 1.18.4
SciPy 1.4.1
gensim 3.6.0
FAST_VERSION 1
```
",,,,,,1,,1,,
737,https://github.com/RaRe-Technologies/gensim/issues/2893,2893,[],closed,2020-07-23 20:41:33+00:00,,0," _pickle.UnpicklingError: invalid load key, '\xd0'.","<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

What are you trying to achieve? What is the expected result? What are you seeing instead?

#### Steps/code/corpus to reproduce

Include full tracebacks, logs and datasets if necessary. Please keep the examples minimal (""minimal reproducible example"").

#### Versions

Please provide the output of:

```python
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import gensim; print(""gensim"", gensim.__version__)
from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION)
```
",,,,,,1,,1,,
512,https://github.com/RaRe-Technologies/gensim/issues/2541,2541,"[{'id': 721000065, 'node_id': 'MDU6TGFiZWw3MjEwMDAwNjU=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/good%20first%20issue', 'name': 'good first issue', 'color': '7057ff', 'default': True, 'description': 'Issue for new contributors (not required gensim understanding + very simple)'}, {'id': 1072221028, 'node_id': 'MDU6TGFiZWwxMDcyMjIxMDI4', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/Hacktoberfest', 'name': 'Hacktoberfest', 'color': 'b396e0', 'default': False, 'description': 'Issues marked for hacktoberfest'}, {'id': 1583467927, 'node_id': 'MDU6TGFiZWwxNTgzNDY3OTI3', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/help%20wanted', 'name': 'help wanted', 'color': '1d76db', 'default': True, 'description': ''}]",closed,2019-06-28 12:30:21+00:00,,19,min_similarity & max_distance does not work in levsim,"#### Expecting

As doc says, `max_distance` is a more efficient way, more quickly.
https://github.com/RaRe-Technologies/gensim/blob/369cc638225a2080faec25c4e2f6448d31e0492b/gensim/similarities/levenshtein.py#L32-L37

#### Problem description

However, `max_distance` does not speed up the process

https://github.com/RaRe-Technologies/gensim/blob/369cc638225a2080faec25c4e2f6448d31e0492b/gensim/similarities/levenshtein.py#L48-L51",,,,1,,,,1,,
727,https://github.com/RaRe-Technologies/gensim/issues/2881,2881,[],closed,2020-07-13 10:35:24+00:00,,1,"KeyedVectors.load_word2vec_format('w2v.bin',binary=False) raise Value Error","<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

A value exception was thrown while loading the word vector file.

#### Steps/code/corpus to reproduce
```
from gensim.models import Word2Vec
from gensim.models import KeyedVectors


common_texts = [['human', 'interface', 'computer',' '],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]
model = Word2Vec(common_texts, size=10, window=5, min_count=1, workers=4)
model.wv.save_word2vec_format('w2v.bin',binary=False)
# load word embedding
KeyedVectors.load_word2vec_format('w2v.bin',binary=False)
```

闂锛氬姞杞借缁冭鏂欑殑璇嶅悜閲忔枃浠惰繃绋嬩腑鎶涘嚭鍊煎紓甯革細
When there is a space in the corpus, the loading word vector file will throw a ValueError.
鎶涘嚭寮傚父鐨勬枃浠朵綅缃?
**gensim/models/utils_any2vec.py**
```
parts = utils.to_unicode(line.rstrip(), encoding=encoding, errors=unicode_errors).split("" "")
if len(parts) != vector_size + 1:
    raise ValueError(""invalid vector on line %s (is this really the text format?)"" % line_no)
word, weights = parts[0], [datatype(x) for x in parts[1:]]
```

瑙ｅ喅鏂规锛氬幓闄ら鏂欎腑鐨勭┖鏍兼垨鑰呬娇鐢ㄥ姞杞芥ā鍨嬬殑鏂瑰紡
Solution: remove the space in the corpus and load model

#### Versions

Please provide the output of:

Linux-4.4.0-142-generic-x86_64-with-debian-stretch-sid
Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) 
[GCC 7.2.0]
NumPy 1.18.1
Sc```iPy 1.4.1
gensim 3.8.3
FAST_VERSION 1
",,1,,,,,,1,,
427,https://github.com/RaRe-Technologies/gensim/issues/2427,2427,[],closed,2019-03-23 18:23:32+00:00,,1,Error after loading KeyedVectors and using similarity queries,"<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

What are you trying to achieve? 
I am training a Word2Vec model, then splitting the word vectors into two groups (original vocabulary, selected words from the vocabulary) and storing them in two KeyedVectors files. After that I load the two KeyedVectors files and try to query for similarity based on words and vectors. 

What is the expected result?
Before storing and loading I can query both KeyedVectors with ""most_similar"" and ""similar_by_vector"". After restoring, I expect to be able to query the vectors pools as before. 

 What are you seeing instead?
I am able to query the original word vectors set, but not the one I've generated by restricting the original vocabulary (even if I could do it before).

#### Steps/code/corpus to reproduce
1. To train the model
```
import pickle
from gensim.models.word2vec import PathLineSentences
from gensim.models import KeyedVectors
from gensim.models import Word2Vec
import copy

sentences = PathLineSentences('./wiki_00_processed.txt')
model = Word2Vec(
        sentences,
        size=150,
        window=10,
        min_count=1,
        workers=10)
model.train(sentences, total_examples=model.corpus_count, epochs=10)
```

2. To split the vocabulary
``` 
def restrict_w2v(w2v_original, restricted_word_set):
    new_vectors = []
    new_vocab = {}
    new_index2entity = []
    new_vectors_norm = []
    w2v = copy.deepcopy(w2v_original) 
    
    for i in range(len(w2v.vocab)):
        word = w2v.index2entity[i]
        vec = w2v.vectors[i]
        vocab = w2v.vocab[word]
        vec_norm = w2v.vectors_norm[i]
        if word in restricted_word_set:
            vocab.index = len(new_index2entity)
            new_index2entity.append(word)
            new_vocab[word] = vocab
            new_vectors.append(vec)
            new_vectors_norm.append(vec_norm)

    w2v.vocab = new_vocab
    w2v.vectors = new_vectors
    w2v.index2entity = new_index2entity
    w2v.index2word = new_index2entity
    w2v.vectors_norm = new_vectors_norm
    return w2v

with open(""wiki_entities_00.txt"", ""rb"") as fp:  
    all_entities = pickle.load(fp)

word_vectors_all = model.wv
kb_word_vectors = restrict_w2v(word_vectors_all, all_entities)
```

3. Query before and after loading
```
test_vec = word_vectors_all['canada]
kb_word_vectors.similar_by_vector(test_vec) # works
kb_word_vectors.save(""kb_vocab_vecs_entities.kv"")
kb_vectors_restored = KeyedVectors.load(""kb_vocab_vecs_entities.kv"", mmap='r')
kb_vectors_restored.similar_by_vector(test_vec) # does not work
```

```
Error:
TypeError                                 Traceback (most recent call last)
<ipython-input-145-9a2fbad26fa8> in <module>
----> 1 kb_vectors_test.similar_by_vector(jazz_vec)

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in similar_by_vector(self, vector, topn, restrict_vocab)
    607 
    608         """"""
--> 609         return self.most_similar(positive=[vector], topn=topn, restrict_vocab=restrict_vocab)
    610 
    611     @deprecated(

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in most_similar(self, positive, negative, topn, restrict_vocab, indexer)
    520             negative = []
    521 
--> 522         self.init_sims()
    523 
    524         if isinstance(positive, string_types) and not negative:

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in init_sims(self, replace)
   1330         if getattr(self, 'vectors_norm', None) is None or replace:
   1331             logger.info(""precomputing L2-norms of word weight vectors"")
-> 1332             self.vectors_norm = _l2_norm(self.vectors, replace=replace)
   1333 
   1334     def relative_cosine_similarity(self, wa, wb, topn=10):

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in _l2_norm(m, replace)
   2344 
   2345     """"""
-> 2346     dist = sqrt((m ** 2).sum(-1))[..., newaxis]
   2347     if replace:
   2348         m /= dist

TypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'
```
Data Sets:
[wiki_00_processed.txt](https://github.com/RaRe-Technologies/gensim/files/2999667/wiki_00_processed.txt)
[wiki_entities_00.txt](https://github.com/RaRe-Technologies/gensim/files/2999668/wiki_entities_00.txt)


#### Versions

Please provide the output of:

```python 3.7.1
import platform; print(platform.platform())
Linux-4.18.0-16-generic-x86_64-with-debian-buster-sid

import sys; print(""Python"", sys.version)
Python 3.7.1 (default, Dec 14 2018, 19:28:38) 
[GCC 7.3.0]

import numpy; print(""NumPy"", numpy.__version__)
NumPy 1.16.2

import scipy; print(""SciPy"", scipy.__version__)
SciPy 1.1.0

import gensim; print(""gensim"", gensim.__version__)
gensim 3.7.1

from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION)
FAST_VERSION 1

```
",1,,,,,,,1,,
498,https://github.com/RaRe-Technologies/gensim/issues/2523,2523,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 233081, 'node_id': 'MDU6TGFiZWwyMzMwODE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20easy', 'name': 'difficulty easy', 'color': '00ff00', 'default': False, 'description': 'Easy issue: required small fix'}, {'id': 721000065, 'node_id': 'MDU6TGFiZWw3MjEwMDAwNjU=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/good%20first%20issue', 'name': 'good first issue', 'color': '7057ff', 'default': True, 'description': 'Issue for new contributors (not required gensim understanding + very simple)'}, {'id': 1072221028, 'node_id': 'MDU6TGFiZWwxMDcyMjIxMDI4', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/Hacktoberfest', 'name': 'Hacktoberfest', 'color': 'b396e0', 'default': False, 'description': 'Issues marked for hacktoberfest'}, {'id': 1602334164, 'node_id': 'MDU6TGFiZWwxNjAyMzM0MTY0', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/impact%20LOW', 'name': 'impact LOW', 'color': '0052cc', 'default': False, 'description': 'Low impact on affected users'}, {'id': 1602340302, 'node_id': 'MDU6TGFiZWwxNjAyMzQwMzAy', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/reach%20LOW', 'name': 'reach LOW', 'color': '5abc21', 'default': False, 'description': 'Affects only niche use-case users'}]",open,2019-06-10 12:37:46+00:00,,1,bug in gensim.summarization.mz_entropy.mz_keywords,"**Problem statement:**

It seems to be a bug if the text is too short and number of words is lower than blocksize. In my case the values were: `n_words (232.0) ` and `blocksize (1024)`.

**Log:**

```
gensim\summarization\mz_entropy.py:127: RuntimeWarning: invalid value encountered in double_scalars
  - __log_combinations(n_words, blocksize)
```

**Dirty solution:**

Override `blocksize` value from the default `1024` to something lower:

`mz_keywords(text, blocksize=128)`

",,,,,,,,1,,
42,https://github.com/RaRe-Technologies/gensim/issues/1651,1651,"[{'id': 233081, 'node_id': 'MDU6TGFiZWwyMzMwODE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20easy', 'name': 'difficulty easy', 'color': '00ff00', 'default': False, 'description': 'Easy issue: required small fix'}, {'id': 721000065, 'node_id': 'MDU6TGFiZWw3MjEwMDAwNjU=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/good%20first%20issue', 'name': 'good first issue', 'color': '7057ff', 'default': True, 'description': 'Issue for new contributors (not required gensim understanding + very simple)'}]",closed,2017-10-25 14:18:55+00:00,,12,Mutable vector returned by KeyedVectors.word_vector,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Description
TODO: change commented example
<!-- Example: Vocabulary size is not what I expected when training Word2Vec. -->

#### Steps/Code/Corpus to Reproduce
<!--
Example:
```
from gensim.models import word2vec

sentences = ['human', 'machine']
model = word2vec.Word2Vec(sentences)
print(model.syn0.shape) 
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->

Method `KeyedVectors.word_vector` returned ""mutable"" vector if we call `model['anywords']` (for `model[['anywords']]` works correctly because [vstack make a copy.](https://github.com/RaRe-Technologies/gensim/blob/269028975e0db48e37e01edfb54e66018db7b61b/gensim/models/keyedvectors.py#L600)

Simple example

```python
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format('path/to/model')
vector = model['word_from_model']
assert ~np.allclose(vector, np.zeros(vector.shape))  # check that our vector isn't zero
vector *= 0.

assert ~np.allclose(model['word_from_model'], np.zeros(vector.shape))  # failed, because now model['word_from_model'] is zero vector

```



#### Expected Results
assert passed

#### Actual Results
assert failed

#### What needs to fix
Add `arr.setflags(write=False)`  in [word_vec](https://github.com/RaRe-Technologies/gensim/blob/269028975e0db48e37e01edfb54e66018db7b61b/gensim/models/keyedvectors.py#L266)
",,,,,,,,1,,
102,https://github.com/RaRe-Technologies/gensim/issues/1790,1790,[],closed,2017-12-15 06:58:52+00:00,,6,accuracy had KeyError,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Description
TODO: change commented example
<!-- Example: Vocabulary size is not what I expected when training Word2Vec. -->

#### Steps/Code/Corpus to Reproduce
<!--
Example:
```
from gensim.models import word2vec

sentences = ['human', 'machine']
model = word2vec.Word2Vec(sentences)
print(model.syn0.shape) 
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->
while going to have an accuracy computation by calling
```
model.wv.accuracy('question-file-name')
```
I got error message as followings:

```
KeyErrorTraceback (most recent call last)
<ipython-input-123-8a545c3407ef> in <module>()
----> 1 model.wv.accuracy('./questions_words.txt',restrict_vocab=1000000)

/opt/anaconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.pyc in accuracy(self, questions, restrict_vocab, most_similar, case_insensitive)
    662 
    663         """"""
--> 664         ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
    665         ok_vocab = dict((w.upper(), v) for w, v in reversed(ok_vocab)) if case_insensitive else dict(ok_vocab)
    666 

KeyError: u'a'
```

When I looked into the code inside the file **keyedvectors.py** for the called function **accuracy** I found this:

```
ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
```

Then I double checked the size of both `self.vocab` and `self.index2word` , surprisingly I found both size are not matched. That told me that word 'a' is **NOT** in `self.vocab` though it **IS** in `self.index2word`.

My questions raised:

1. why and how happened for the different size of two (self.vocab and self.index2word)?
2. how about change the calculation code to:

```
ok_vocab = [(w, self.vocab[w]) for w in self.index2word if w in self.vocab][:restrict_vocab]
```

Any helps? Thanks.
",,,,,,,,1,,
128,https://github.com/RaRe-Technologies/gensim/issues/1874,1874,[],closed,2018-02-02 15:05:13+00:00,,1,Dov2Vec: AttributeError: Can't get attribute 'EuclideanKeyedVectors',"Hi,

I can't find anything related to my issue, so I'm posting it here. I trained a doc2vec model, then saved it using `model.save(""my.model"")`. It saved 4 files:

- my.model
- my.model.docvecs.doctag_syn0.npy
- my.model.syn1neg.npy
- my.model.wv.syn0.npy

I then try to load it, using `d2v = gensim.models.Doc2Vec.load('my.model')`, and it returns:
```
Traceback (most recent call last):
  File ""lstm.py"", line 41, in <module>
    d2v = gensim.models.Doc2Vec.load('my.model')
  File ""/home/fiorinin/deeplearning/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 1412, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""/home/fiorinin/deeplearning/lib/python3.4/site-packages/gensim/utils.py"", line 276, in load
    obj = unpickle(fname)
  File ""/home/fiorinin/deeplearning/lib/python3.4/site-packages/gensim/utils.py"", line 938, in unpickle
    return _pickle.load(f, encoding='latin1')
AttributeError: Can't get attribute 'EuclideanKeyedVectors' on <module 'gensim.models.keyedvectors' from '/home/fiorinin/deeplearning/lib/python3.4/site-packages/gensim/models/keyedvectors.py'>
```

Any idea? Is it normal that it uses word2vec anyway - I assume this is a parent class? 
I tried updating, just in case, but there's the same error. Also, note that the training was done on a different machine, accessing the same directory. Can it be an incompatibility maybe, and if so, how to solve it?

Thanks!",,,,,,,,1,,
142,https://github.com/RaRe-Technologies/gensim/issues/1901,1901,"[{'id': 175643, 'node_id': 'MDU6TGFiZWwxNzU2NDM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': 'Current issue related to documentation'}, {'id': 175986, 'node_id': 'MDU6TGFiZWwxNzU5ODY=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/testing', 'name': 'testing', 'color': '444444', 'default': False, 'description': 'Issue related with testing (code, documentation, etc)'}, {'id': 234670, 'node_id': 'MDU6TGFiZWwyMzQ2NzA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20medium', 'name': 'difficulty medium', 'color': '00d000', 'default': False, 'description': 'Medium issue: required good gensim understanding & python skills'}, {'id': 1584013467, 'node_id': 'MDU6TGFiZWwxNTg0MDEzNDY3', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/conda', 'name': 'conda', 'color': 'c9ef58', 'default': False, 'description': ''}]",open,2018-02-14 08:30:54+00:00,,1,Add anaconda-cloud badge & setup build for MacOSX,"Into
-----
We build gensim for [conda-forge/conda-cloud](https://github.com/conda-forge/gensim-feedstock), but nobody knows about it.

Todo
-----
- [x] (easy) Add conda-cloud badge to gensim `README.md` (#1905)
- [ ] (medium) Setup MacOSX build in gensim-feedstock, related issue https://github.com/conda-forge/gensim-feedstock/issues/2 
",,,,,,,1,,,
529,https://github.com/RaRe-Technologies/gensim/issues/2566,2566,[],open,2019-08-01 07:08:22+00:00,,1,Google Cloud Jupiter Notebook: ,"Hi, 

On a Google Cloud Jupiter Notebook, I am trying to import a Glove txt file using the following commands but I keep gettting the same error. 
```
from gensim.scripts.glove2word2vec import glove2word2vec 
from gensim.test.utils import get_tmpfile
tmp_file = get_tmpfile(""test_word2vec.txt"")
glove2word2vec(""glove.6B.300d.txt"", tmp_file)
externalModel=KeyedVectors.load_word2vec_format(tmp_file)
```
The error is:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-8-21d446f7b71f> in <module>
----> 1 externalModel=KeyedVectors.load_word2vec_format(tmp_file)

~/.local/lib/python3.5/site-packages/gensim/models/keyedvectors.py in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)
   1496         return _load_word2vec_format(
   1497             cls, fname, fvocab=fvocab, binary=binary, encoding=encoding, unicode_errors=unicode_errors,
-> 1498             limit=limit, datatype=datatype)
   1499 
   1500     def get_keras_embedding(self, train_embeddings=False):

~/.local/lib/python3.5/site-packages/gensim/models/utils_any2vec.py in _load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)
    392                 parts = utils.to_unicode(line.rstrip(), encoding=encoding, errors=unicode_errors).split("" "")
    393                 if len(parts) != vector_size + 1:
--> 394                     raise ValueError(""invalid vector on line %s (is this really the text format?)"" % line_no)
    395                 word, weights = parts[0], [datatype(x) for x in parts[1:]]
    396                 add_word(word, weights)

ValueError: invalid vector on line 59941 (is this really the text format?)
```
I have also tried to upgrade everything following the answers to similar issues with the following commands
```
pip3 install google-compute-engine
pip3 install --upgrade gensim smart_open
```
My current version is the following 

```
Name: gensim
Version: 3.8.0
Summary: Python framework for fast Vector Space Modelling
Home-page: http://radimrehurek.com/gensim
Author: Radim Rehurek
Author-email: me@radimrehurek.com
License: LGPLv2.1
Location: /home/jupyter/.local/lib/python3.5/site-packages
Requires: scipy, numpy, smart-open, six

```
Regards 
",,,,,,,1,,,
538,https://github.com/RaRe-Technologies/gensim/issues/2578,2578,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 1602278675, 'node_id': 'MDU6TGFiZWwxNjAyMjc4Njc1', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/reach%20HIGH', 'name': 'reach HIGH', 'color': '229e03', 'default': False, 'description': 'Affects most or all Gensim users'}, {'id': 1602334472, 'node_id': 'MDU6TGFiZWwxNjAyMzM0NDcy', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/impact%20MEDIUM', 'name': 'impact MEDIUM', 'color': '7af49f', 'default': False, 'description': 'Big annoyance for affected users'}]",closed,2019-08-14 04:38:37+00:00,,9,"""OverflowError: value too large to convert to int"" when training word2vec on a large corpus","Hi,
i am trying to train word2vec on on a large corpus (>500GB) and the newest gensim version (3.8.0) and an error occurs on every Thread:

`Exception in thread Thread-10:
Traceback (most recent call last):
  File ""/home/michael/miniconda3/lib/python3.7/threading.py"", line 917, in _bootstrap_inner
    self.run()
  File ""/home/michael/miniconda3/lib/python3.7/threading.py"", line 865, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/michael/miniconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py"", line 175, in _worker_loop_corpusfile
    total_examples=total_examples, total_words=total_words, **kwargs)
  File ""/home/michael/miniconda3/lib/python3.7/site-packages/gensim/models/word2vec.py"", line 794, in _do_train_epoch
    total_examples, total_words, work, neu1, self.compute_loss)
  File ""gensim/models/word2vec_corpusfile.pyx"", line 379, in gensim.models.word2vec_corpusfile.train_epoch_cbow
OverflowError: value too large to convert to int`

My command for training the Model is:
`Word2Vec(corpus_file=""encc_tokenized"", size = 1024, window = 8, workers = 16)`

Thank you for help",,,,,,,1,,,
551,https://github.com/RaRe-Technologies/gensim/issues/2594,2594,"[{'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}]",closed,2019-09-04 08:39:28+00:00,,1,ImportError: cannot import name 'WikiCorpus',"```
Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)] on win32
import gensim
Traceback (most recent call last):
  File ""C:\Users\koradg\AppData\Local\Programs\Python\Python36\lib\site-packages\IPython\core\interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-e70e92d32c6e>"", line 1, in <module>
    import gensim
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\koradg\AppData\Local\Programs\Python\Python36\lib\site-packages\gensim\__init__.py"", line 5, in <module>
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\koradg\AppData\Local\Programs\Python\Python36\lib\site-packages\gensim\corpora\__init__.py"", line 14, in <module>
    from .wikicorpus import WikiCorpus  # noqa:F401
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\koradg\AppData\Local\Programs\Python\Python36\lib\site-packages\gensim\corpora\wikicorpus.py"", line 29, in <module>
    from gensim.scripts import make_wiki
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\koradg\AppData\Local\Programs\Python\Python36\lib\site-packages\gensim\scripts\make_wiki.py"", line 45, in <module>
    from gensim.corpora import Dictionary, HashDictionary, MmCorpus,WikiCorpus
ImportError: cannot import name 'WikiCorpus'
```
``
how to resolve it?

_Originally posted by @gauravkoradiya in https://github.com/RaRe-Technologies/gensim/issues/2169#issuecomment-527799066_",,,,,,,1,,,
572,https://github.com/RaRe-Technologies/gensim/issues/2635,2635,[],closed,2019-10-17 03:27:58+00:00,,0,Wikicorpus interlinks storage should be list of tuples instead of dict. ,"<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

I am trying to get the list of interlinks occurring on a wikipedia page. The current parser uses a dict to store the links. However, the same link might be referred to using different text on the same page. 

E.g. for the `Anarchism` page in the test script the `Anarchism in Italy` link is referred using 2 different texts. 

```
Anarchism in Italy -> Italian anarchists
Anarchism in Italy -> Italy
Anarchism in Italy -> Italy
```

This motivates the reasoning that interlinks should be stored as a list of tuples instead of a dictionary. 

#### Steps/code/corpus to reproduce

Include full tracebacks, logs and datasets if necessary. Please keep the examples minimal (""minimal reproducible example"").

The issue is at the following line: 
https://github.com/RaRe-Technologies/gensim/blob/3e027c252eac3cf7e613f425ad8b070e8fe88065/gensim/corpora/wikicorpus.py#L174

And the subsequence assignment in later lines. 
The above code only keeps track of the last interlink mapping instead of all interlink mappings in the text. 

I will send a PR resolving this issue. 



#### Versions

Please provide the output of:

```python
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import gensim; print(""gensim"", gensim.__version__)
from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION)
```

Output of above: 

```
Darwin-18.7.0-x86_64-i386-64bit
Python 3.7.4 (default, Aug 13 2019, 15:17:50) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
NumPy 1.17.2
SciPy 1.3.1
gensim 3.8.1
FAST_VERSION -1
```
",,,,,,,1,,,
594,https://github.com/RaRe-Technologies/gensim/issues/2679,2679,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 234670, 'node_id': 'MDU6TGFiZWwyMzQ2NzA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20medium', 'name': 'difficulty medium', 'color': '00d000', 'default': False, 'description': 'Medium issue: required good gensim understanding & python skills'}, {'id': 1602257032, 'node_id': 'MDU6TGFiZWwxNjAyMjU3MDMy', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/impact%20HIGH', 'name': 'impact HIGH', 'color': 'b60205', 'default': False, 'description': 'Show-stopper for affected users'}, {'id': 1602279836, 'node_id': 'MDU6TGFiZWwxNjAyMjc5ODM2', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/reach%20MEDIUM', 'name': 'reach MEDIUM', 'color': 'ef7a1a', 'default': False, 'description': 'Affects a significant number of users'}]",open,2019-11-14 02:35:01+00:00,,3,"Some Doc2Vec vectors remain untrained, with either giant #s of docvecs or small contrived corpuses","Investigating the user report at <https://groups.google.com/d/msg/gensim/XbH5Sr6RBcI/w5-AIwpSAwAJ>, I ran with a (much-smaller) version of the synthetic-corpus there, & reproduced similarly inexplicable results, with doc-vectors that *should* have received at least *some* training-adjustment showing no change after an epoch of training. 

To demonstrate:

```python
import logging
logging.root.setLevel(level=logging.INFO)
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec
import numpy as np
import inspect

class DummyTaggedDocuments(object):

    def __init__(self, count=1001, shared_word=True, doc_word=True, digit_words=False):
        self.count = count
        self.shared_word = shared_word
        self.doc_word = doc_word
        self.digit_words = digit_words
        
    def __iter__(self):
        for i in range(self.count):
            words = []
            if self.shared_word:
                words += ['shared']
            if self.doc_word: 
                words += ['doc_'+str(i)]
            if self.digit_words:
                words += str(i)  
            tags = [i]
            if i == self.count - 1:
                logging.info(""yielding last DummyTaggedDocument %i"", i)
            yield TaggedDocument(words=words, tags=tags)

def test_d2v_docvecs_trained(doc_args={}, d2v_args={}):
    """"""Demo bug in Doc2Vec (& more?) as of gensim 3.8.1 with sample>0""""""
    docs = DummyTaggedDocuments(**doc_args)
    d2v_model = Doc2Vec(**d2v_args)
    
    d2v_model.build_vocab(docs, progress_per=100000)
    starting_vecs = d2v_model.docvecs.vectors_docs.copy()  
    
    d2v_model.train(docs, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs, report_delay=5)
    
    unchanged = np.all(starting_vecs==d2v_model.docvecs.vectors_docs, axis=1)
    unchanged_indexes = np.argwhere(unchanged)
    
    return (len(unchanged_indexes), list(unchanged_indexes))

test_d2v_docvecs_trained(d2v_args=dict(min_count=0, sample=0.01, vector_size=4, epochs=1, workers=1))
```

The return value of this test method should, for the given parameters, be a count of 0 unchanged vectors, and and empty-list of unchanged vector indexes. But in a typical run I'm getting instead:

```
(12,
 [array([0]),
  array([1]),
  array([4]),
  array([6]),
  array([9]),
  array([11]),
  array([13]),
  array([14]),
  array([15]),
  array([30]),
  array([33]),
  array([80])])
```

Though this is an artificial dataset, with peculiar 2-word documents, which will often only be 1-word documents (after frequent-word-downsampling of the term shared by all documents) 鈥?every document should be at least 1 word long, and thus get at least some training in any single epoch. The logging output regarding sampling accurately reports what the effects *should* be (at this sampling level):

```
INFO:gensim.models.doc2vec:collected 1002 word types and 1001 unique tags from a corpus of 1001 examples and 2002 words
INFO:gensim.models.word2vec:Loading a fresh vocabulary
INFO:gensim.models.word2vec:effective_min_count=0 retains 1002 unique words (100% of original 1002, drops 0)
INFO:gensim.models.word2vec:effective_min_count=0 leaves 2002 word corpus (100% of original 2002, drops 0)
INFO:gensim.models.word2vec:deleting the raw counts dictionary of 1002 items
INFO:gensim.models.word2vec:sample=0.01 downsamples 1 most-common words
INFO:gensim.models.word2vec:downsampling leaves estimated 1162 word corpus (58.1% of prior 2002)
``` 

I get similar evidence of unchanged vectors in `dm=0` (PV-DBOW) mode. However, running more epochs usually drives the number of unchanged vectors to 0. 

Turning off downsampling with `sample=0` ensures all vectors show some update, implying some error in the downsampling is involved. Essentially, that strongly implies something going wrong in  the code at or around:

https://github.com/RaRe-Technologies/gensim/blob/3d6596112f8f1fc0e839a32c5a00ef3d7365c264/gensim/models/doc2vec_inner.pyx#L344

But, a manual check of the precalculated `sample_int` values for all-but-the-most-frequent word suggests they're where they should be: a value that the random-int is never higher-than, and thus a value that should result in the corresponding words never being down-sampled. 

I may not have time to dig deeper anytime soon, so placing this recipe-to-reproduce & key observations so far here. 

Notably, `Word2Vec` & `FastText` may be using similar sampling logic 鈥?so even though there's not yet a sighting there, similar issues may exist. 

(Separately, I somewhat doubt this sample-related anomaly, whatever its cause, is necessarily related to actual original problem of the user in the thread referenced 鈥?which seemed only present in extremely large corpuses, likely with real many-word texts, over a normal number of repeated epochs, and perhaps only in the ""very tail-end"" doc-vectors.)


",,,,,,,1,,,
604,https://github.com/RaRe-Technologies/gensim/issues/2693,2693,[],open,2019-12-02 10:11:29+00:00,,1,Number of Sentences in corpusfile don't match trained sentences.,"#### Problem description

I'm training a fasttext model (CBOW) over a corpus, for instance `enwik8`. 
The number of sentences trained (or example_count as referred in log methods) on doesn't equal the number of sentences in the file (`wc -l` or `len(f.readlines())`, referred as `expected_count` or `total_examples` ). 
Why is this happening? Also, in the method [here](https://github.com/RaRe-Technologies/gensim/blob/e391f0c25599c751e127dde925e062c7132e4737/gensim/models/base_any2vec.py#L1301), this warning has been suppressed for corpus mode.


### Versions

```python
Linux-4.4.0-1096-aws-x86_64-with-debian-stretch-sid
Python 3.7.5 (default, Oct 25 2019, 15:51:11)
[GCC 7.3.0]
NumPy 1.17.2
SciPy 1.3.1
gensim 3.8.1
FAST_VERSION 1
```
",,,,,,,1,,,
605,https://github.com/RaRe-Technologies/gensim/issues/2694,2694,"[{'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}]",closed,2019-12-03 09:23:13+00:00,,4,Use linesentence to stream corpus from file,"I have used gensim to train word embedding about 6 months ago. At that time, the code used to stream data works just fine, but now I used it again, I meet so many error, one of them from fasttext model using too much ram. I think the document have not been updated 

```
corpus_file = datapath('sample.txt')
model = FT_gensim(size=100)

# build the vocabulary
model.build_vocab(corpus_file=corpus_file)

# train the model
model.train(
    corpus_file=corpus_file, epochs=model.epochs,
    total_examples=model.corpus_count, total_words=model.corpus_total_words
)

print(model)
```",,,,,,,1,,,
632,https://github.com/RaRe-Technologies/gensim/issues/2737,2737,[],closed,2020-01-28 18:22:59+00:00,,4,Word2Vec Wikipedia Corpus 2017 no vocabulary,"#### Problem description

When following the example code [here](https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html#sphx-glr-auto-examples-howtos-run-downloader-api-py) I receive a ""Word not in vocabulary"" error. Opening at the request of Radim: https://groups.google.com/forum/#!topic/gensim/ULW_OKrPtqE

#### Steps/code/corpus to reproduce
```python
from gensim.models.word2vec import Word2Vec
import gensim.downloader as gensim_download_api

wikipedia_corpus = gensim_download_api('wiki-english-2017001')
model_with_wikipedia = Word2Vec(wikipedia_corpus)
model_with_wikipedia.wv.most_similar('cat')
KeyError: ""word 'cat' not in vocabulary""
```

Logging output (truncated to non-repeat / progress):

```
INFO:gensim.models.word2vec:PROGRESS: at sentence #4920000, processed 14760000 words, keeping 3 word types
INFO:gensim.models.word2vec:collected 3 word types from a corpus of 14774682 raw words and 4924894 sentences
INFO:gensim.models.word2vec:Loading a fresh vocabulary
INFO:gensim.models.word2vec:effective_min_count=5 retains 3 unique words (100% of original 3, drops 0)
INFO:gensim.models.word2vec:effective_min_count=5 leaves 14774682 word corpus (100% of original 14774682, drops 0)
INFO:gensim.models.word2vec:deleting the raw counts dictionary of 3 items
INFO:gensim.models.word2vec:sample=0.001 downsamples 3 most-common words
INFO:gensim.models.word2vec:downsampling leaves estimated 853566 word corpus (5.8% of prior 14774682)
INFO:gensim.models.base_any2vec:estimated required memory for 3 words and 100 dimensions: 3900 bytes
INFO:gensim.models.word2vec:resetting layer weights
INFO:gensim.models.base_any2vec:training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
```

#### Versions

Linux-4.15.0-74-generic-x86_64-with-debian-buster-sid
Python 3.7.4 (default, Sep  5 2019, 19:15:53) 
[GCC 7.4.0]
NumPy 1.18.1
SciPy 1.4.1
gensim 3.8.1
FAST_VERSION 1",,,,,,,1,,,
636,https://github.com/RaRe-Technologies/gensim/issues/2744,2744,[],open,2020-02-04 13:00:16+00:00,,2,AttributeError: 'WikiCorpus' object has no attribute 'input',"<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

I am using WikiCorpus to read and process the wikipedia dump. However when I try to iterate  over  getstream() output, I get the error that there is no attribute input. Indeed, there is no input attribute but there is a fname attribute.
#### Steps/code/corpus to reproduce

```python
path = ""data/dewiki-latest-pages-articles.xml.bz2""
wiki = WikiCorpus(path)
for stream in wiki.getstream():
    print(stream)
    break
```

On the other hand, this fixes the problem, but I guess would be better if it gets fixed in the source code if the attribute missing is the problem.

```python
setattr(wiki, ""input"", wiki.fname)
```

#### Versions

```python
Linux-x86_64-with-debian-9.11
Python 3.7.0 (default, Oct  9 2018, 10:31:47) 
[GCC 7.3.0]
NumPy 1.17.3
SciPy 1.3.2
gensim 3.8.0
FAST_VERSION 1
```
",,,,,,,1,,,
644,https://github.com/RaRe-Technologies/gensim/issues/2754,2754,[],closed,2020-02-16 17:07:42+00:00,,6,LdaModel throws exception when the corpus is a sparse term by doc CSC matrix,"<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

I'm trying to fit an LDA modeling using a term by doc CSC matrix.

#### Steps/code/corpus to reproduce

![image](https://user-images.githubusercontent.com/1140359/74609111-c654c100-50b4-11ea-93d8-8ae785ab5e46.png)

```pytb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-29-da24849da160> in <module>
----> 1 model = LdaModel(X)

/usr/local/lib/python3.7/site-packages/gensim/models/ldamodel.py in __init__(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)
    431         if self.id2word is None:
    432             logger.warning(""no word id mapping provided; initializing from corpus, assuming identity"")
--> 433             self.id2word = utils.dict_from_corpus(corpus)
    434             self.num_terms = len(self.id2word)
    435         elif len(self.id2word) > 0:

/usr/local/lib/python3.7/site-packages/gensim/utils.py in dict_from_corpus(corpus)
    824 
    825     """"""
--> 826     num_terms = 1 + get_max_id(corpus)
    827     id2word = FakeDict(num_terms)
    828     return id2word

/usr/local/lib/python3.7/site-packages/gensim/utils.py in get_max_id(corpus)
    733     maxid = -1
    734     for document in corpus:
--> 735         if document:
    736             maxid = max(maxid, max(fieldid for fieldid, _ in document))
    737     return maxid

/usr/local/lib/python3.7/site-packages/scipy/sparse/base.py in __bool__(self)
    285             return self.nnz != 0
    286         else:
--> 287             raise ValueError(""The truth value of an array with more than one ""
    288                              ""element is ambiguous. Use a.any() or a.all()."")
    289     __nonzero__ = __bool__

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().
```

#### Versions

Darwin-19.3.0-x86_64-i386-64bit
Python 3.7.5 (default, Nov  1 2019, 02:16:23) 
[Clang 11.0.0 (clang-1100.0.33.8)]
NumPy 1.17.4
SciPy 1.3.3
gensim 3.8.1
FAST_VERSION 0
",,,,,,,1,,,
655,https://github.com/RaRe-Technologies/gensim/issues/2768,2768,[],closed,2020-03-13 12:16:17+00:00,,1,AttributeError: 'MmCorpus' object has no attribute 'get_texts',"
#### When loading wiki corpus cannot use get_texts method.
I have serialized a wiki corpus after processed with the following method:

`MmCorpus.serialize('deWikiProcessed.gensim', wiki)
`
However when I load it and try to use get_texts attribute I get an error. 

```python
wiki = MmCorpus('deWikiProcessed.gensim')
wiki.metadata = True
for content, (page_id, title) in wiki.get_texts():
         pass
```

I need to load the wiki in order to train a new doc2vec model. I use the .get_text() method to get the articles again.
Is there any way I can retrieve the method get_texts, or I just need to reprocess the wiki corpus again and not use MmCorpus format? I was hoping there was a way to convert MmCorpus to WikiCorpus object in order to get the .get_texts() method

#### Versions

```python
Linux-5.0.7-kd-cluster-x86_64-with-debian-9.12
Python 3.7.0 (default, Oct  9 2018, 10:31:47) 
[GCC 7.3.0]
NumPy 1.17.3
SciPy 1.4.1
gensim 3.8.0
FAST_VERSION 0
```
",,,,,,,1,,,
723,https://github.com/RaRe-Technologies/gensim/issues/2876,2876,"[{'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}]",closed,2020-07-07 21:38:18+00:00,,14,corpus.mallet returned non-zero exit status 1.,"#### Problem description

So many people have had this issue, and I have tried all the fixes suggested, to no avail. The path is correct, and I have changed it multiple times to remove spaces etc. I get the same error. Bearing in mind I have no idea how to provide all the information necessary, please respond with precise instructions as to how to debug this issue.

**Error is**
CalledProcessError: Command 'mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\DraGoN\AppData\Local\Temp\b76d8f_corpus.txt --output C:\Users\DraGoN\AppData\Local\Temp\b76d8f_corpus.mallet' returned non-zero exit status 1.

I have looked and the temp files do exist in the temp directory. I have even tried editing the .bat file to hard code the mallet_home directory, and the java installation directory. Nothing works. I get the same error.

#### Steps/code/corpus to reproduce
```python
import os
from gensim.models.wrappers import LdaMallet

os.environ.update({'MALLET_HOME':r'C:/Users/DraGoN/Documents/python/mallet-2.0.8'})
mallet_path = 'mallet-2.0.8/bin/mallet' # update this path

#Alternative LDA model, download here and put in directory - https://www.machinelearningplus.com/wp-content/uploads/2018/03/mallet-2.0.8.zip
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)`
```
#### Versions

Please provide the output of:

```python
Windows-10-10.0.18362-SP0
Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]
NumPy 1.19.0
SciPy 1.4.1
gensim 3.8.3
FAST_VERSION 1
```
",,,,,,,1,,,
310,https://github.com/RaRe-Technologies/gensim/issues/2187,2187,[],closed,2018-09-15 20:08:27+00:00,,4,Load Pre-trained word2vec vectors,"Hi,

I have heard a lot about gensim, but now when I am trying to use it, for probably the simplest task, i.e loading pre-trained embeddings, I am stuck for hours.

Consider:

```
from gensim.models import KeyedVectors
# Load vectors directly from the file
model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
# Access vectors for specific words with a keyed lookup:
vector = model['simple']
```

```
python word2vec.pyTraceback (most recent call last):  File ""word2vec.py"", line 3, in <module>
    model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
  File ""/home/andy/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py"", line 1436, in load_word2vec_format    limit=limit, datatype=datatype)
  File ""/home/andy/anaconda3/lib/python3.6/site-packages/gensim/models/utils_any2vec.py"", line 178, in _load_word2vec_format
    result.vectors = zeros((vocab_size, vector_size), dtype=datatype)
MemoryError
```

Could gensim be used to load word2vec pre-trained embeddings released by google? How could I do so?

Cheers!",,,,,,1,,,,
553,https://github.com/RaRe-Technologies/gensim/issues/2596,2596,[],closed,2019-09-04 22:17:49+00:00,,1,Pre-Trained Doc2Vec Models,"I am looking for a french pre-trained model for Doc2Vec. I found a lot of information about the non usability of Word2Vec models in Doc2Vec. I have a corpus of 10,000 little documents, but I think this number is too small to train a performing model? Does anyone have a suggestion for me? Thank you!",,,,,,1,,,,
558,https://github.com/RaRe-Technologies/gensim/issues/2602,2602,[],closed,2019-09-14 12:13:06+00:00,,6,Import error when loading model,"<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

I'm trying to load a model I created from an other script with SaveLoad class. Everything worked fined when I last checked it, but since a few days I got an import error. I don't remember having changed anything specific to the model loading, so I'm not sure what's happening. Here is the error:
```python
WARNING:this function is deprecated, use smart_open.open instead
Traceback (most recent call last):
  File ""bin/advisor.py"", line 32, in <module>
    adv.start()
  File ""./semantic/advisor.py"", line 256, in start
    sentences, relevances = self.compare_sentences()
  File ""./semantic/advisor.py"", line 198, in compare_sentences
    model = sv.load('models/{id}.mdl'.format(id=self.corpus_id))
  File ""/usr/local/lib/python3.5/dist-packages/gensim/utils.py"", line 426, in load
    obj = unpickle(fname)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/utils.py"", line 1384, in unpickle
    return _pickle.load(f, encoding='latin1')
ImportError: No module named 'numpy.random._pickle'
```

#### Steps/code/corpus to reproduce

Here is the code used to loading the model:
```python
sv = SaveLoad()
model = sv.load('models/{id}.mdl'.format(id=id)).
```
It's a Word2Vec model, nothing really 'exotic'. I tried loading the model in the python console with
```python
with open('./my_model.mdl', 'rb') as f:
     res = pickle.load(f)
```
And I get me the same error:
```python
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
ImportError: No module named 'numpy.random._pickle'
```
So I think that the problem happens at the model creation more than at the model loading ?
However here is the code for model creation
```python
model = Word2Vec(window=CONTEXT_WINDOW_SIZE, workers=4, sg=0, seed=21, min_count=1, size=300,
                             batch_words=1000, hs=1, negative=0)
[training]
 model.save(my_path + '/../models/{id}.mdl'.format(id=corpus_id))
```
I could provide the pickle file but I'm not sure it's a good idea to provide it on the issue for security reasons, I guess it could seem fishy
#### Versions


```python
>>> import platform; print(platform.platform())
Linux-4.4.0-142-generic-x86_64-with-Ubuntu-16.04-xenial
>>> import sys; print(""Python"", sys.version)
Python 3.5.2 (default, Nov 12 2018, 13:43:14) 
[GCC 5.4.0 20160609]
>>> import numpy; print(""NumPy"", numpy.__version__)
NumPy 1.16.3
>>> import scipy; print(""SciPy"", scipy.__version__)
SciPy 0.19.1
>>> import gensim; print(""gensim"", gensim.__version__)
gensim 3.7.1
>>> from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION)
FAST_VERSION 1
```
Don't hesitate to point at anything I could provide to help
Thanks a lot ;-)",,,,,,1,,,,
576,https://github.com/RaRe-Technologies/gensim/issues/2642,2642,"[{'id': 175986, 'node_id': 'MDU6TGFiZWwxNzU5ODY=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/testing', 'name': 'testing', 'color': '444444', 'default': False, 'description': 'Issue related with testing (code, documentation, etc)'}, {'id': 708430967, 'node_id': 'MDU6TGFiZWw3MDg0MzA5Njc=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/performance', 'name': 'performance', 'color': 'd93f0b', 'default': False, 'description': 'Issue related to performance (in HW meaning)'}, {'id': 1072221028, 'node_id': 'MDU6TGFiZWwxMDcyMjIxMDI4', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/Hacktoberfest', 'name': 'Hacktoberfest', 'color': 'b396e0', 'default': False, 'description': 'Issues marked for hacktoberfest'}, {'id': 1583467927, 'node_id': 'MDU6TGFiZWwxNTgzNDY3OTI3', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/help%20wanted', 'name': 'help wanted', 'color': '1d76db', 'default': True, 'description': ''}, {'id': 1602257032, 'node_id': 'MDU6TGFiZWwxNjAyMjU3MDMy', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/impact%20HIGH', 'name': 'impact HIGH', 'color': 'b60205', 'default': False, 'description': 'Show-stopper for affected users'}, {'id': 1602279836, 'node_id': 'MDU6TGFiZWwxNjAyMjc5ODM2', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/reach%20MEDIUM', 'name': 'reach MEDIUM', 'color': 'ef7a1a', 'default': False, 'description': 'Affects a significant number of users'}]",closed,2019-10-21 20:51:55+00:00,,17,Speed up word2vec / fasttext model loading,"Loading a large word2vec model with `load_word_format(binary=True)` is slow. Users complain that loading the ""standard"" models published by Facebook is too slow, plus it also affects the speed of our own tests and tutorial autogeneration.

Some numbers:

```python
time gensim.models.keyedvectors.KeyedVectors.load_word2vec_format('./word2vec-google-news-300.gz', binary=True)
2019-10-21 22:24:08,326 : INFO : loading projection weights from ./word2vec-google-news-300.gz
2019-10-21 22:26:54,620 : INFO : loaded (3000000, 300) matrix from ./word2vec-google-news-300.gz
CPU times: user 2min 42s, sys: 3.64 s, total: 2min 46s
Wall time: 2min 46s
```

The I/O part itself = only loading the bytes from the file without any interpretation, takes about 30 seconds:

```python
time full = io.BytesIO(smart_open.open('./word2vec-google-news-300.gz', 'rb').read())
CPU times: user 20.9 s, sys: 8.13 s, total: 29.1 s
Wall time: 31.9 s
```

鈥hich means our parsing code is taking up the majority of the `load_word2vec_format` time. Ideally, we shouldn't need much more than the 30 seconds (= the raw I/O speed) for the full `load_word_format(binary=True)`. Nearly 3 minutes is too much.

**Task**: Optimize `load_word2vec_format`, especially the `binary=True` branch. The code seems to live here:
https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/utils_any2vec.py#L369",,,,,,1,,,,
584,https://github.com/RaRe-Technologies/gensim/issues/2663,2663,"[{'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}]",open,2019-10-31 17:01:29+00:00,,2,DOCSIM Problem adding new articles to a loaded model,"#### Problem description
Using gensim I want to save a model and load it afterwards and still be able to use the model as it always was on memory

#### Steps/code/corpus to reproduce
As far as I can gather if I add a document to the model I need to save otherwise It will not be loaded properly

In the example I create the model and save them when the program is run for the first time and everything runs fine. If I let the program do the index.add_articles on the first run even if I don't save the model afterwards the program will not run a second time.

The program will run if I save after the add_article.

```python
import os
from gensim.similarities import Similarity
from gensim import corpora

def main():
    stoplist = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', ""you're""}

    articles_1_10 =  [""This is the first sentence"", ""And here is another one"", ""how many sentences can I write before this starts to get boring"",
                      ""This sentences are the sentences to construct the dictionary"",""I think I want ten of them"",""how many of them are in here already?"",
                      ""Only six so far with this one is the seven"",""I think we are almost there"",""Why didn't I get the first paragraphs of a book"",""I think this is the last one""]

    articles_11_15 =  [""This second set are the articles that will be added when the object is created"", ""So they will be saved when the object is saved"",
                       ""And they will be compared agains each other in the first print"", ""So there is not much to them this ones are not causing the issue"", ""This is the last sentence""]

    articles_16_20 = [""This third set are the articles that will be added after the loading"", ""So they will be saved when the object is saved"",
                       ""And they will be compared agains each other only in the second print"", ""This ones are the ones causing the issue"", ""This is the last sentence of them all""]

    articles_21_22 = [""this is getting stupid"", ""i dont know what is going on""]

    dictionary_path = ""temp_d1.dict""
    model_path = ""temp_test1""

    if os.path.isfile(dictionary_path):
        dictionary = corpora.Dictionary.load(dictionary_path)
    else:
        texts = [[word for word in document.lower().split() if word not in stoplist] for document in articles_1_10]
        dictionary = corpora.Dictionary(texts)
        dictionary.save(dictionary_path)

    if os.path.isfile(model_path):
        index = Similarity.load(model_path)
    else:
        texts = [dictionary.doc2bow(document.lower().split()) for document in articles_11_15]
        index = Similarity(model_path, texts, num_features=len(dictionary))  # create index
        index.save(model_path)

    print([x for x in index])
    texts2 = [dictionary.doc2bow(document.lower().split()) for document in articles_16_20]
    index.add_documents(texts2)
    print([x for x in index])
    #index.save(model_path)
    #texts3 = [dictionary.doc2bow(document.lower().split()) for document in articles_16_20]
    #index.add_documents(texts3)

if __name__ == '__main__':
    main()

```

#### Versions

Linux-5.3.4-arch1-1-ARCH-x86_64-with-arch
Python 3.6.6 (default, Oct 29 2018, 15:19:57) 
[GCC 8.2.1 20180831]
NumPy 1.17.3
SciPy 1.3.1
gensim 3.8.1
FAST_VERSION 1
",,,,,,1,,,,
586,https://github.com/RaRe-Technologies/gensim/issues/2666,2666,[],open,2019-11-01 15:21:03+00:00,,2,Memory Error While Loading a FastText Model Previously Trained via Gensim 3.4.0.,"<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

I'm trying to load a fast text model previously trained by myself, via gensim 3.4.0.
 
The Error: 

`MemoryError: Unable to allocate array with shape (1757544, 300) and data type float32`
#### Steps/code/corpus to reproduce
By upgrading my gensim version to the newest version I can no longer load the pretrained model. No other steps are needed.

With Gensim 3.4.0 the following code needs a maximum memory of `1068.94921875 MiB` while with Gensim 3.8.0 it gets out of memory error. I have a total of 8GB RAM.

```
from gensim.models import FastText

from memory_profiler import memory_usage

def myfunc():
    fasttext_loc = 'wordvectors/FastText/ft.txt'
    model = FastText.load(fasttext_loc)

mem = max(memory_usage(myfunc))

print(""Maximum memory used: {0} MiB"".format(str(mem)))
```

This is the output of `ls -l` in my model's directory(my ft.txt file is around 16MB, the folder is around 800MB in total): 

> -rw-rw-r-- 1 farhood farhood  16584918 Apr  5  2019 ft.txt
> -rw-rw-r-- 1 farhood farhood  63660128 Apr  5  2019 ft.txt.trainables.syn1neg.npy
> -rw-rw-r-- 1 farhood farhood  63660128 Apr  5  2019 ft.txt.trainables.syn1.npy
> -rw-rw-r-- 1 farhood farhood 290947328 Apr  5  2019 ft.txt.trainables.vectors_ngrams_lockf.npy
> -rw-rw-r-- 1 farhood farhood  63660128 Apr  5  2019 ft.txt.trainables.vectors_vocab_lockf.npy
> -rw-rw-r-- 1 farhood farhood 290947328 Apr  5  2019 ft.txt.wv.vectors_ngrams.npy
> -rw-rw-r-- 1 farhood farhood  63660128 Apr  5  2019 ft.txt.wv.vectors.npy
> -rw-rw-r-- 1 farhood farhood  63660128 Apr  5  2019 ft.txt.wv.vectors_vocab.npy

#### Versions
Works on 3.4.0, doesn't work in the newest version 3.8.0.


Please provide the output of:

```python
Python 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import platform; print(platform.platform())
Linux-5.0.0-32-generic-x86_64-with-debian-buster-sid
>>> import sys; print(""Python"", sys.version)
Python 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0]
>>> import numpy; print(""NumPy"", numpy.__version__)
NumPy 1.17.3
>>> import scipy; print(""SciPy"", scipy.__version__)
SciPy 1.3.1
>>> import gensim; print(""gensim"", gensim.__version__)
gensim 3.4.0
>>> from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION)
FAST_VERSION 1
```
",,,,,,1,,,,
717,https://github.com/RaRe-Technologies/gensim/issues/2862,2862,[],open,2020-06-20 02:16:10+00:00,,1,load_facebook_model() perturbs model with lower quality,"How can I get this to load the original model without reducing the quality as shown in the log? I plan to continue training and it automatically downsamples. When I call `model.build_vocab_from_freq()`, it also reduces from the intended vocab.
```
    from  gensim.models.fasttext import *
    model_path = datapath(model_file)
    model = load_facebook_model(model_path)

INFO:gensim.models._fasttext_bin:loading 2000000 words for fastText model from /home/me/data/external_models/cc.en.300.bin
INFO:gensim.models.word2vec:resetting layer weights
INFO:gensim.models.word2vec:Updating model with new vocabulary
INFO:gensim.models.word2vec:New added 2000000 unique words (50% of original 4000000) and increased the count of 2000000 pre-existing words (50% of original 4000000)
INFO:gensim.models.word2vec:deleting the raw counts dictionary of 2000000 items
INFO:gensim.models.word2vec:sample=1e-05 downsamples 6996 most-common words
INFO:gensim.models.word2vec:downsampling leaves estimated 390315457935 word corpus (70.7% of prior 552001338161)
INFO:gensim.models.fasttext:loaded (4000000, 300) weight matrix for fastText model from /home/me/data/external_models/cc.en.300.bin
```
The data I'm using is
`crawl-300d-2M-subword.zip: 2 million word vectors trained with subword information on Common Crawl (600B tokens).`

",,,,,,1,,,,
768,https://github.com/RaRe-Technologies/gensim/issues/2950,2950,[],open,2020-09-16 18:10:33+00:00,,3,Overflow error after unicode errors when loading a 'large' model built with gensim,"<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

What are you trying to achieve? 
I am loading a `fasttext` model built with `gensim`, using `gensim.models.fasttext.load_facebook_model` so I can use the model.

What is the expected result?
The model loads correctly. 

 What are you seeing instead?
Overflow error, preceded by unicode parsing errors.

#### Steps/code/corpus to reproduce

I get an overflow error when I try to load a `fasttext` model which I built with `gensim`. I have tried with versions 3.8.3 and then rebuild and load with the head of the code 4.0.0-dev as of yesterday. It's not reproducible because I cannot share the corpus.

Here is the stack trace:


      In [21]: ft = load_facebook_model('data/interim/ft_model.bin')
      2020-09-16 15:59:59,526 : MainThread : INFO : loading 582693 words for fastText model from data/interim/ft_model.
      bin
      2020-09-16 15:59:59,626 : MainThread : ERROR : failed to decode invalid unicode bytes b'\x8a\x08'; replacing in
      lid characters, using '\\x8a\x08'
      2020-09-16 15:59:59,684 : MainThread : ERROR : failed to decode invalid unicode bytes b'\xb0\x03'; replacing in
      lid characters, using '\\xb0\x03'
      2020-09-16 15:59:59,775 : MainThread : ERROR : failed to decode invalid unicode bytes b'\xb5\x01'; replacing in
      lid characters, using '\\xb5\x01'
      2020-09-16 15:59:59,801 : MainThread : ERROR : failed to decode invalid unicode bytes b'\x99\xe9\xa2\x9d'; repl
      ing invalid characters, using '\\x99棰?
      ---------------------------------------------------------------------------
      OverflowError                             Traceback (most recent call last)
      <ipython-input-21-3b4a7ad71a41> in <module>
      ----> 1 ft = load_facebook_model('data/interim/ft_model.bin')

      /m/virtualenvs/<snip>/lib/python3.6/site-packages/gensim/models/fasttext.py in load_f
      ebook_model(path, encoding)
         1140
         1141     """"""
      -> 1142     return _load_fasttext_format(path, encoding=encoding, full_model=True)
         1143
         1144

      /m/virtualenvs/<snip>/lib/python3.6/site-packages/gensim/models/fasttext.py in _load_
      sttext_format(model_file, encoding, full_model)
         1220     """"""
         1221     with gensim.utils.open(model_file, 'rb') as fin:
      -> 1222         m = gensim.models._fasttext_bin.load(fin, encoding=encoding, full_model=full_model)
         1223
         1224     model = FastText(

      /m/virtualenvs/<snip>/python3.6/site-packages/gensim/models/_fasttext_bin.py in l
      d(fin, encoding, full_model)
          342     model.update(raw_vocab=raw_vocab, vocab_size=vocab_size, nwords=nwords, ntokens=ntokens)
          343
      --> 344     vectors_ngrams = _load_matrix(fin, new_format=new_format)
          345
          346     if not full_model:

      /m/virtualenvs/<snip>/lib/python3.6/site-packages/gensim/models/_fasttext_bin.py in _
      ad_matrix(fin, new_format)
          276         matrix = _fromfile(fin, _FLOAT_DTYPE, count)
          277     else:
      --> 278         matrix = np.fromfile(fin, _FLOAT_DTYPE, count)
          279
          280     assert matrix.shape == (count,), 'expected (%r,),  got %r' % (count, matrix.shape)

      OverflowError: Python int too large to convert to C ssize_t

* There are no errors or warnings in the model building using the same . 
* A quick check showed there are no unicode errors in the input file, but very well possible that there are Chinese characters. 
* The `count` variable is calculated as `count = num_vectors * dim`. Both of these are astronomical at 10^23, `dim` should be 100, so there must be some unpacking problem here already. The unpacking of model params pre vocab look ok. 
* The input dataset is somewhat large at 26 GB, one epoch is sufficient. 
* The build and load works with a truncated file which is 4.8 GB. So change in size as well as corpus -- could be that the problematic input is not included. 
* The same input file works when running with the python `fasttext` module, so I have a workaround. 

The count of the erroneous words are also off the scale:

      In [41]: raw_vocab['\\x8a\x08']
      Out[41]: 7088947288457871360

      In [42]: raw_vocab['\\xb0\x03']
      Out[42]: 3774297962713186304

      In [43]: raw_vocab['\\xb5\x01']
      Out[43]: 7092324988178399232


I saw that there were many changes from `int` to `long long` both in 3.8.3 and also in 4.0.0-dev so my hypothesis was that it would be resolved when updating but I got the same error. 

I don't know if this is sufficient information to go in in order to pin it down, please let me know if I can help with more information. 

#### Versions

Please provide the output of:

```python
>>> import platform; print(platform.platform())
Linux-2.6.32-754.3.5.el6.x86_64-x86_64-with-centos-6.10-Final
>>> import sys; print(""Python"", sys.version)
Python 3.6.10 (default, Jul  8 2020, 16:15:16) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-23)]
>>> import struct; print(""Bits"", 8 * struct.calcsize(""P""))
Bits 64
>>> import numpy; print(""NumPy"", numpy.__version__)
NumPy 1.19.2
>>> import scipy; print(""SciPy"", scipy.__version__)
SciPy 1.5.2
>>> import gensim; print(""gensim"", gensim.__version__)
gensim 3.8.3
>>> from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION)
FAST_VERSION 1
```
",,,,,,1,,,,
778,https://github.com/RaRe-Technologies/gensim/issues/2969,2969,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 1602279836, 'node_id': 'MDU6TGFiZWwxNjAyMjc5ODM2', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/reach%20MEDIUM', 'name': 'reach MEDIUM', 'color': 'ef7a1a', 'default': False, 'description': 'Affects a significant number of users'}, {'id': 1602334472, 'node_id': 'MDU6TGFiZWwxNjAyMzM0NDcy', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/impact%20MEDIUM', 'name': 'impact MEDIUM', 'color': '7af49f', 'default': False, 'description': 'Big annoyance for affected users'}]",open,2020-09-30 21:04:09+00:00,,16,Load full native fastText Facebook model is partial,"#### Problem description

Hidden vectors are bad. I'm using the gensim.models.fasttext.load_facebook_model function to load the .bin file, but the syn1 fails loading. Also trainables.syn1neg is full of zeros.

'FastTextTrainables' object has no attribute 'syn1'

#### Steps/code/corpus to reproduce

Simply using `ft = gensim.models.fasttext.load_facebook_model(fname)`  on Facebook's model.
Then `ft.syn1` or `ft.trainables.syn1neg` which returns the zero array.

#### Versions

Please provide the output of:
Windows-2012ServerR2-6.3.9600-SP0
Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]
Bits 64
NumPy 1.18.3
SciPy 1.4.1
gensim 3.8.3
FAST_VERSION 0

",,,,,,1,,,,
323,https://github.com/RaRe-Technologies/gensim/issues/2218,2218,[],closed,2018-10-08 22:03:31+00:00,,4,Doc2Vec not optimal CPU utilization on high number of cores with all-in-memory corpus,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Description

I know that there are a few related issues currently open or closed, but couldn't find specific information for the case where all corpus is loaded to memory, sorry if I missed something.

When I'm trying to run Doc2Vec with an all-in-memory corpus on a 64 CPU machine with `workers=40`, I see partial CPU usage.

Main process uses ~%400 CPU, and there are 40 other processes using CPU ~%10 each. This in total corresponds to a usage of 8 cores in full.

This limitation is breaking for me because I want to train document embeddings on a corpus with ~25M documents, training of just 1 epoch would take 1 day with current speed.

<!-- Example: Vocabulary size is not what I expected when training Word2Vec. -->

#### Steps/Code/Corpus to Reproduce
```
import gensim
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

model = Doc2Vec(vector_size=300, window=15, max_vocab_size=None, min_count=5, hs=0, negative=5, ns_exponent=0.75, sample=10e-5)

# tagged_docs is a list of TaggedDocument instances loaded in memory, not an iterator or generator
model.build_vocab(tagged_docs, progress_per=10000)
model.workers = 40
model.train(tagged_docs, total_examples=len(tagged_docs), epochs=1)
```

#### Expected Results
<!-- Example: Expected shape of (100,2).-->
I expect near ideal CPU utilization.
#### Actual Results
It seems to be using only 8 of the cores in full.

```
2018-10-08 17:42:26,205 : INFO : training on a 30200106 raw words (16766153 effective words) took 62.0s, 270407 effective words/s
```

#### Versions
Linux-3.10.0-862.11.6.el7.x86_64-x86_64-with-redhat-7.5-Maipo
Python 3.6.6 | packaged by conda-forge | (default, Jul 26 2018, 09:53:17) 
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]
NumPy 1.15.2
SciPy 1.1.0
gensim 3.6.0
FAST_VERSION 1

<!-- Thanks for contributing! -->

",,,1,,1,,,,,
212,https://github.com/RaRe-Technologies/gensim/issues/2029,2029,"[{'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}]",closed,2018-04-12 19:36:02+00:00,,2,word2vec2tensor.py : Number of tensors do not match the number of lines in metadata,"#### Description
When I used word2vec2tensor.py on my model, the resulting tsv files (_tensor and _metadata) have different dimensions: when I load them into TensorBoard, it tells me that ""Number of tensors (10000) do not match the number of lines in metadata (25681)"".

Is there a max limit in the number of tensors produced by the code?

#### Expected Results
tensor.tsv and metadata.tsv with the same number 

#### Actual Results
tensor.tsv has 10000 tensors, metadata  has 25681.


#### Versions
""numpy"",""1.11.0""
""sklearn"", ""0.17""
""scipy"", ""0.15.1""
'gensim','3.1.0'


",,,,,1,,,,,
234,https://github.com/RaRe-Technologies/gensim/issues/2060,2060,[],closed,2018-05-25 04:25:40+00:00,,1,Why I got negative number of model.score(sentences)?,"I use Word2Vec model to get the word2vec,and want to classify whether it's a correct sentence or not.
And when i use model.score to get the log probability of this sentence,I got a negative score.Why?
Thx for answering this question:)",,,,,1,,,,,
271,https://github.com/RaRe-Technologies/gensim/issues/2120,2120,[],closed,2018-07-06 12:28:50+00:00,,1,Why I got more docvec vectors than the number of my actual number of docs?,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Description
I am using Doc2Vec to embed my docs into vectors, the training was successful but the results are strange to me. I have 46k docs, while the size of docvecs is 200k. I don't know what's wrong with my code.
<!-- Example: Vocabulary size is not what I expected when training Word2Vec. -->

#### Steps/Code/Corpus to Reproduce
```
class LabeledLineSentence(object):
    def __init__(self, doc_list, labels_list):
        self.labels_list = labels_list
        self.doc_list = doc_list
    def __iter__(self):
        for idx, doc in enumerate(self.doc_list):
              yield gensim.models.doc2vec.TaggedDocument(doc, [self.labels_list[idx]])


df = pd.read_csv(""data/DATA_HADM.csv"", escapechar='\\')
texts = df['text'].values  # this a ~46K array
label = list(df['id'])

size = 600 
it = LabeledLineSentence(texts, label)
    
model_d2v = Doc2Vec(min_count=10, window=5, vector_size=size, sample=1e-3, negative=5, workers=8)
model_d2v.build_vocab(it)
model_d2v.train(it, epochs=10, total_examples=model_d2v.corpus_count)
```
<!--
Example:
```
from gensim.models import word2vec

sentences = ['human', 'machine']
model = word2vec.Word2Vec(sentences)
print(model.syn0.shape) 
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->

#### Expected Results
I expect the `model_d2v.docvecs` is a 46k-by-600 matrix.
<!-- Example: Expected shape of (100,2).-->

#### Actual Results
The actual resulting matrix is a 200k-by-600 matrix, I don't understand.
<!-- Example: Actual shape of (100,5). 

Please paste or specifically describe the actual output or traceback. -->

#### Versions
Linux-4.4.0-1062-aws-x86_64-with-debian-stretch-sid
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) 
[GCC 7.2.0]
NumPy 1.14.3
SciPy 1.1.0
gensim 3.4.0
FAST_VERSION 1

<!--
Please run the following snippet and paste the output below.
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import gensim; print(""gensim"", gensim.__version__)
from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION)
-->


<!-- Thanks for contributing! -->

",,,,,1,,,,,
316,https://github.com/RaRe-Technologies/gensim/issues/2199,2199,[],closed,2018-09-24 16:30:51+00:00,,1,How to pick the number of topics with Umass measure?,"I am trying to use Umass measure to pick the best number of topics, but I do not know what Umass exactly means? About the coherence score, is it the bigger, the better, or just the opposite? Below is the output of my test with Umass measure. How many topics should I pick?
![image](https://user-images.githubusercontent.com/43496800/45965139-3e52bf00-c05a-11e8-9b23-4e4d9684475a.png)
",,,,,1,,,,,
413,https://github.com/RaRe-Technologies/gensim/issues/2402,2402,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}]",closed,2019-03-06 09:20:25+00:00,,20,AssertionError: unexpected number of vectors when loading Korean FB model,"Hi,

I downloaded pretrained word vector file (.bin) from facebook 
(https://fasttext.cc/docs/en/crawl-vectors.html)
However, when I tried to use this model it happens to make error.

    from gensim.models import FastText
    fasttext_model = FastText.load_fasttext_format('cc.ko.300.bin', encoding='utf8')

    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte

But weird thing is that it operates well when I use old version bin file (https://fasttext.cc/docs/en/pretrained-vectors.html)

So I tried to find the solution and found that this problem had happend and solved.

The issue was made through facebook fasttext issue
(https://github.com/facebookresearch/fastText/issues/715)
And they fixed it by
(https://github.com/facebookresearch/fastText/commit/e13484bcb261cda51d33c4940ab5e207aba3ee79)

So, I think gensim load_fasttext_format function had this UnicodeDecodeError because of above problem.

Can you help me to find and solve this problem?

------------------------------------------


I tried changing
word = word_bytes.decode(encoding, errors='replace')
word = word_bytes.decode(encoding, errors='ignore')
in gensim\models\_fasttext_bin.py , line 177
But both made same error
    File ""C:\Users\User\PycharmProjects\ksenticnet\venv\lib\site-packages\gensim\models\keyedvectors.py"", line 2207, in init_post_load
    assert vectors.shape[0] == vocab_words + self.bucket, 'unexpected number of vectors'
AssertionError: unexpected number of vectors

#### Versions
Windows-10-10.0.17134-SP0
Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)]
NumPy 1.16.2
SciPy 1.2.1
gensim 3.7.1
FAST_VERSION 0

",,,,,1,,,,,
496,https://github.com/RaRe-Technologies/gensim/issues/2521,2521,[],closed,2019-06-07 12:26:08+00:00,,1,Increasing Perplexity with number of topics,"In order to evaluate the best number of topics for my dataset, I split the set into testset and trainingset (25%, 75%, 18k documents alltogether). But somehow my perplexity keeps increasing on the testset. I don't know exactly why.

You can see the perplexity here: https://imgur.com/FE9KF2Z

Corpus can be found here: http://uploaded.net/file/nkru8zi7

I split the corpus the following way (using sklearn)
corpus_train, corpus_test = train_test_split(corpus, test_size=0.25)

Then I apply LDA on the train_set. Then get the perplexity using

perplexity = 2 ** (-1.0 * lda_model.log_perplexity(corpus_test))

on the testset.

I do a new split for every number of topic (So I don't have one split and use it for every number of topics, but do a new split everytime before applying lda)

Any suggestions?",,,,,1,,,,,
155,https://github.com/RaRe-Technologies/gensim/issues/1940,1940,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 234670, 'node_id': 'MDU6TGFiZWwyMzQ2NzA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20medium', 'name': 'difficulty medium', 'color': '00d000', 'default': False, 'description': 'Medium issue: required good gensim understanding & python skills'}]",closed,2018-02-28 05:24:26+00:00,,12,"FastText native VS original, different outputs","## Intro

As the person mentioned in [mailing list](https://groups.google.com/forum/#!topic/gensim/zPp8vr4n8pI), he receives different result with a pre-trained model with gensim code & original facebook code.

## How to reproduce
1. Install Facebook FastText
    ```bash
    wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip
    unzip v0.1.0.zip
    cd fastText-0.1.0
    make
    ```
2. Download pre-trained vectors from https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md (I pick english, only as example), you need `bin+text` link
3. Unpack archive with vectors
4. Try to retreive vectors with gensim
    ```python
    from gensim.models import FastText

    m = FastText.load_fasttext_format(""wiki.en.vec"")
    print(m[""hello""])  # existent word
    print(m[""someundefinedword""])  # non-existent word
5. Repeat it with original FastText implementation
    ```bash
    ./fasttext  print-word-vectors ../wiki.en.bin
    hello
    someundefinedword
    ```
6. Compare vectors from (5) and (6)

### Expected Results
Vectors for ""hello"" and ""someundefinedword"" exactly same (from gensim & Facebook)

### Actual result
Exactly same vectors for ""hello"", but different for ""someundefinedword""

CC: @manneshiva
",,,,1,,,,,,
185,https://github.com/RaRe-Technologies/gensim/issues/1989,1989,"[{'id': 175643, 'node_id': 'MDU6TGFiZWwxNzU2NDM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': 'Current issue related to documentation'}, {'id': 234670, 'node_id': 'MDU6TGFiZWwyMzQ2NzA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20medium', 'name': 'difficulty medium', 'color': '00d000', 'default': False, 'description': 'Medium issue: required good gensim understanding & python skills'}]",open,2018-03-20 08:32:18+00:00,,3,Benchmark Gensim vs Amazon SageMaker ,"#### Description
We want to compare gensim and [SageMaker](https://aws.amazon.com/ru/sagemaker/):

- LSI (after #1896) vs [SageMaker PCA](https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html)
- LDA vs [SageMaker NTM](https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html) vs [SageMaker LDA](https://docs.aws.amazon.com/sagemaker/latest/dg/lda.html)

#### Metrics
- hardware performance:
  - time
  - memory
- algorithm performance (corresponding to a particular algorithm. for example - perplexity for LDA, etc). Also possible solve some supervised task with end2end evaluation with simple linear classifier on top).

The expected result is a benchmark table + a blog post describing the motivation, results and analysis.",,,,1,,,,,,
509,https://github.com/RaRe-Technologies/gensim/issues/2537,2537,[],closed,2019-06-25 18:37:18+00:00,,10,FastText inconsistent normalization in adjust_vectors method vs word_vec,"<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

There is inconsistency in normalization in ```FastTextKeyedVectors#adjust_vectors``` vs ```WordEmbeddingsKeyedVectors#word_vec```

#### Steps/code/corpus to reproduce

adjust_vectors:
```
            word_vec /= len(ngram_hashes) + 1
```

word_vec method:
```
            return word_vec / len(ngram_hashes)
```

#### Versions

```
Linux-4.18.0-24-generic-x86_64-with-Ubuntu-18.04-bionic
Python 3.7.1 (default, Oct 22 2018, 11:21:55) 
[GCC 8.2.0]
NumPy 1.16.3
SciPy 1.2.1
gensim 3.7.3
FAST_VERSION 1
```
",,,,1,,,,,,
685,https://github.com/RaRe-Technologies/gensim/issues/2817,2817,[],open,2020-05-01 02:11:07+00:00,,0,Investigate segfault with tensorflow with Py3.6 on Travis CI,"@gojomo yes, the crashes are reliable. I narrowed it down to having tensorflow installed:

* tensorflow installed => Travis py3.6 crashes during `test_doc2vec`
* tensorflow not installed => Travis all tests pass, including 3.6

So my ""fix"" was https://github.com/RaRe-Technologies/gensim/pull/2814/commits/8cd68b2a5ea537f2953bdd592b1a1973b148431d but it's really a crude hack. Still a mystery to me what's happening. There may be a deeper issue lurking there, which affects other Pythons too but doesn't cause a Travis crash.

_Originally posted by @piskvorky in https://github.com/RaRe-Technologies/gensim/pull/2814#issuecomment-622036840_",,,,1,,,,,,
700,https://github.com/RaRe-Technologies/gensim/issues/2842,2842,[],closed,2020-05-16 21:13:49+00:00,,3,KeyedVectors importing keras instead of tensorflow.keras?,"Hello there,

I got a super quick question regarding the function `get_keras_embeddings` of KeyedVectors.py. Is there a reason behind requiring keras package to be installed (specifically referring to [these lines](https://github.com/RaRe-Technologies/gensim/blob/2360459e0014f5db8fc587005933a6399efab435/gensim/models/keyedvectors.py#L1415-L1419)) instead of going through tensorflow 2.0 API (i.e., `import tensorflow.keras`)? 
",,,,1,,,,,,
733,https://github.com/RaRe-Technologies/gensim/issues/2887,2887,[],closed,2020-07-19 04:08:30+00:00,,5,Measure performance of gensim 4.0.0 vs previous versions,"Not every 1-line decision; just ones that are in inner loops of hot-spot code.

Definitely a big TODO: compare performance before/after.

_Originally posted by @piskvorky in https://github.com/_render_node/MDExOlB1bGxSZXF1ZXN0MzQ5Mjk1NTk1/timeline/more_items_",,,,1,,,,,,
774,https://github.com/RaRe-Technologies/gensim/issues/2961,2961,"[{'id': 175643, 'node_id': 'MDU6TGFiZWwxNzU2NDM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': 'Current issue related to documentation'}]",closed,2020-09-28 12:54:03+00:00,,4,Documentation of strip_punctuation vs strip_punctuation2 in gensim.parsing.preprocessing,"Thanks for all the hard work on this fantastic library. I found a small quirk today, not really a bug, just a bit of a rough edge:

In `gensim.parsing` [preprocessing.py ](https://github.com/RaRe-Technologies/gensim/blob/e210f73c42c5df5a511ca27166cbc7d10970eab2/gensim/parsing/preprocessing.py#L121) `strip_punctuation2` is defined: `strip_punctuation2 = strip_punctuation`.

In the [documentation](https://radimrehurek.com/gensim/parsing/preprocessing.html) the description of [`strip_punctuation2`](https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.strip_punctuation2) is a duplication of [`strip_punctuation`](https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.strip_punctuation) rather than a statement of equality.

I noticed this while reading the documentation and, assuming I was missing an obvious distinction, attempting to hand diff the the docs for the two functions. When I gave up and flipped to the source it became obvious how the two functions are related.
",,,,1,,,,,,
787,https://github.com/RaRe-Technologies/gensim/issues/2988,2988,[],closed,2020-10-22 20:07:33+00:00,,1,`word2vec.doesnt_match` numpy vstack deprecation warning ,"#### Problem description

I followed [this instruction](https://radimrehurek.com/gensim/scripts/glove2word2vec.html) to load GloVe model. When I run:  `model.doesnt_match(""breakfast cereal dinner lunch"".split())` from the [tutorial](https://rare-technologies.com/word2vec-tutorial/), it produces FutureWarning on the `vstack` function. It seems that [I am not the first person to encounter this error as well](https://stackoverflow.com/questions/56593904/word2vec-doesnt-match-function-throws-numpy-warning). It might also be similar to [Issue 2432](https://github.com/RaRe-Technologies/gensim/issues/2432). The error reads:  

> C:\Path_to_gensim\keyedvectors.py:877:  FutureWarning: arrays to stack must be passed as a ""sequence"" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.  
> vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)   

#### Steps/code/corpus to reproduce

```python  
from gensim.test.utils import datapath, get_tmpfile  
from gensim.models import KeyedVectors  
from gensim.scripts.glove2word2vec import glove2word2vec  

glove_file = datapath('test_glove.txt')  
tmp_file = get_tmpfile(""test_word2vec.txt"")  
_ = glove2word2vec(glove_file, tmp_file)  
model = KeyedVectors.load_word2vec_format(tmp_file)  
model.doesnt_match(""breakfast cereal dinner lunch"".split())

```

#### Versions

```python
Windows-10-10.0.17763-SP0   
python 3.8.2   (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AMD64)]   
Bits 64   
NumPy 1.19.0   
SciPy 1.5.2   
gensim 3.8.3   
FAST_VERSION 0
```",,,,1,,,,,,
4,https://github.com/RaRe-Technologies/gensim/issues/1578,1578,[],closed,2017-09-08 16:53:14+00:00,,1,Doc2Vec Segmentation Fault Windows and Linux,"I've tried this basic code on both Linux and Windows.  I'm trying to do some online training and it seems like after a couple passes it throws a seg fault.

Code to recreate problem.
````
from gensim.models.doc2vec import Doc2Vec, LabeledSentence, TaggedDocument


sentences = [('food', 'I like to eat broccoli and bananas.'),
             ('food', 'I ate a banana and spinach smoothie for breakfast.'),
             ('animals', 'Chinchillas and kittens are cute.'),
             ('animals', 'My sister adopted a kitten yesterday.'),
             ('animals', 'Look at this cute hamster munching on a piece of broccoli.')]

convSentences = []
for s in sentences:
    convSentences.append(LabeledSentence(tags=[s[0]], words = s[1].split()))

model = Doc2Vec(size=300, window=8, min_count=1, workers=1)

print(""Pass 1:"")
model.build_vocab([convSentences[0]])
model.train([convSentences[0]], total_examples=model.corpus_count)

print(""Pass 2:"")
model.build_vocab([convSentences[1]], update=True)
model.train([convSentences[1]], total_examples=model.corpus_count)

print(""Pass 3:"")
model.build_vocab([convSentences[2]], update=True)
model.train([convSentences[2]], total_examples=model.corpus_count)

print(""Pass 4:"")
model.build_vocab([convSentences[3]], update=True)
model.train([convSentences[3]], total_examples=model.corpus_count)

print(""Pass 5:"")
model.build_vocab([convSentences[4]], update=True)
model.train([convSentences[4]], total_examples=model.corpus_count)
````

Here's the output running in Windows Idle.  Python 3.5.2

```
Warning (from warnings module):
  File ""C:\Python35\lib\site-packages\gensim\utils.py"", line 855
    warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
UserWarning: detected Windows; aliasing chunkize to chunkize_serial
Pass 1:
Pass 2:
Pass 3:
```

Passes 1-3 go quick, then a long pause and Linux throws a segmentation fault, Windows throws an unspecified error.",,,1,,,,,,,
64,https://github.com/RaRe-Technologies/gensim/issues/1701,1701,[],closed,2017-11-08 07:47:32+00:00,,4,Doc2Vec training hangs,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Description

Hi, I tried training a model, with 
```
from gensim.models import Doc2Vec

model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)
model.build_vocab(tagged_docs)
model.train(tagged_docs, total_examples=model.corpus_count, epochs=10)
```
but I'm getting the following exception (after which Python hangs):

<!--
Example:
```
from gensim.models import word2vec

sentences = ['human', 'machine']
model = word2vec.Word2Vec(sentences)
print(model.syn0.shape) 
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->

#### Actual Results
<!-- Example: Actual shape of (100,5). 

Please paste or specifically describe the actual output or traceback. -->
```
01:03:25 PM INFO gensim.models.doc2vec:collecting all words and their counts
01:03:25 PM INFO gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
01:03:26 PM INFO gensim.models.doc2vec:PROGRESS: at example #10000, processed 3433858 words (6104765/s), 76029 word types, 10000 tags
01:03:26 PM INFO gensim.models.doc2vec:PROGRESS: at example #20000, processed 6635183 words (5692429/s), 103582 word types, 20000 tags
01:03:27 PM INFO gensim.models.doc2vec:PROGRESS: at example #30000, processed 10021476 words (5520268/s), 126018 word types, 30000 tags
01:03:28 PM INFO gensim.models.doc2vec:PROGRESS: at example #40000, processed 13649301 words (5489036/s), 145348 word types, 40000 tags
01:03:28 PM INFO gensim.models.doc2vec:collected 157200 word types and 47050 unique tags from a corpus of 47050 examples and 16232493 words
01:03:28 PM INFO gensim.models.word2vec:Loading a fresh vocabulary
01:03:29 PM INFO gensim.models.word2vec:min_count=1 retains 157200 unique words (100% of original 157200, drops 0)
01:03:29 PM INFO gensim.models.word2vec:min_count=1 leaves 16232493 word corpus (100% of original 16232493, drops 0)
01:03:29 PM INFO gensim.models.word2vec:deleting the raw counts dictionary of 157200 items
01:03:29 PM INFO gensim.models.word2vec:sample=0.0001 downsamples 686 most-common words
01:03:29 PM INFO gensim.models.word2vec:downsampling leaves estimated 13004737 word corpus (80.1% of prior 16232493)
01:03:29 PM INFO gensim.models.word2vec:estimated required memory for 157200 words and 100 dimensions: 223180000 bytes
01:03:30 PM INFO gensim.models.word2vec:resetting layer weights
01:03:31 PM INFO gensim.models.word2vec:training model with 7 workers on 157200 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10
Exception in thread Thread-8:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 810, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 763, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/tarun/.local/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 884, in job_producer
    for sent_idx, sentence in enumerate(sentences):
  File ""/home/tarun/.local/lib/python2.7/site-packages/gensim/utils.py"", line 692, in __iter__
    for document in self.corpus:
TypeError: 'NoneType' object is not iterable
```
#### Versions
<!--
Please run the following snippet and paste the output below.
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import gensim; print(""gensim"", gensim.__version__)
from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION)
-->
('Python', '2.7.6 (default, Oct 26 2016, 20:30:19) \n[GCC 4.8.4]')
('NumPy', '1.13.1')
('SciPy', '0.19.1')
('gensim', '2.3.0')
('cython', '0.27.3')


<!-- Thanks for contributing! -->
Thanks for the help in advance.
",,,1,,,,,,,
235,https://github.com/RaRe-Technologies/gensim/issues/2061,2061,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}]",closed,2018-05-25 15:00:57+00:00,,2,Doc2Vec.infer_vector learning rate decays extremely fast (non-linearly),"I am working with a corpus of very short documents and noticed that the inferred vectors for the same document were very different.

```python
from scipy.spatial.distance import pdist, squareform
testdoc = ""This is a small sample document.""
vectors = [d2vmod.infer_vector(testdoc) for _ in range(5)]
squareform(pdist(vectors, ""cosine""))
```

```
array([[0.        , 0.05987812, 0.06183155, 0.06931093, 0.05466599],
       [0.05987812, 0.        , 0.03724874, 0.05006329, 0.04789369],
       [0.06183155, 0.03724874, 0.        , 0.04771786, 0.05983109],
       [0.06931093, 0.05006329, 0.04771786, 0.        , 0.0367826 ],
       [0.05466599, 0.04789369, 0.05983109, 0.0367826 , 0.        ]])
```

More training steps makes things worse in this case:
```python
vectors = [d2vmod.infer_vector(testdoc, 10000) for _ in range(5)]
squareform(pdist(vectors, ""cosine""))
```
```
array([[0.        , 0.27392197, 0.308742  , 0.51374501, 0.45744246],
       [0.27392197, 0.        , 0.14912033, 0.32902151, 0.1822687 ],
       [0.308742  , 0.14912033, 0.        , 0.2895444 , 0.27019636],
       [0.51374501, 0.32902151, 0.2895444 , 0.        , 0.38096254],
       [0.45744246, 0.1822687 , 0.27019636, 0.38096254, 0.        ]])
```
Note: This is more extreme than what I'm seeing with more domain-specific sample documents, where start to get more consistent after about 5000 steps.

I believe this is happening because the learning rate decays extremely rapidly:
 https://github.com/RaRe-Technologies/gensim/blob/8b810918d59781116794a6679999afdc76b857ef/gensim/models/doc2vec.py#L565

```python
alpha = 0.025
min_alpha = 0.001
steps = 100
for i in range(steps):
    print(alpha)
    alpha = ((alpha - min_alpha) / (steps - i)) + min_alpha
```
```
0.025
0.00124
0.0010024242424242424
0.0010000247371675943
...
```
Notice that `alpha` is very close to `min_alpha` after the first step and this is exaggerated even more when the number of steps is larger. 

When I change Doc2Vec to have a linear decay in learning rate
```python
alpha_delta = (alpha-min_alpha)/(steps-1)
for i in range(steps):
    # ...
    alpha -= alpha_delta
```

I get much better results. With 20 steps, we get pairwise cosine distances of
```
array([[0.        , 0.01617053, 0.02467067, 0.01828433, 0.01834735],
       [0.01617053, 0.        , 0.01879757, 0.00910884, 0.01358116],
       [0.02467067, 0.01879757, 0.        , 0.01521225, 0.01392789],
       [0.01828433, 0.00910884, 0.01521225, 0.        , 0.01121792],
       [0.01834735, 0.01358116, 0.01392789, 0.01121792, 0.        ]])
```
, with 100 we get
```
array([[0.        , 0.00282428, 0.00373375, 0.00331408, 0.00362875],
       [0.00282428, 0.        , 0.0036147 , 0.0028999 , 0.00210812],
       [0.00373375, 0.0036147 , 0.        , 0.0032986 , 0.00361321],
       [0.00331408, 0.0028999 , 0.0032986 , 0.        , 0.00318849],
       [0.00362875, 0.00210812, 0.00361321, 0.00318849, 0.        ]])
```
, and with 1000 steps:
```
array([[0.        , 0.00055459, 0.000633  , 0.00074271, 0.00036596],
       [0.00055459, 0.        , 0.00067211, 0.00075522, 0.00058975],
       [0.000633  , 0.00067211, 0.        , 0.00109709, 0.00049239],
       [0.00074271, 0.00075522, 0.00109709, 0.        , 0.00072527],
       [0.00036596, 0.00058975, 0.00049239, 0.00072527, 0.        ]])
```",,,1,,,,,,,
242,https://github.com/RaRe-Technologies/gensim/issues/2077,2077,[],closed,2018-06-04 08:46:09+00:00,,1,Doc2vec most_similar method returns similarity score higher than 1,"#### Description
I have trained doc2vec model by following this tutorial for 500.000 documents. 
https://github.com/abtpst/Doc2Vec/blob/master/trainDoc2Vec.py

However, when I try to find most_similar documents for a given document, the results have similarity higher than 1. As you can see in the code and the outputs, docvecs.similarity returns 0.246 and docvecs.most_similar return 9.996 for similarity between same documents. You can see the code and the output below:

Code:
```
from gensim.models import doc2vec

def myhash(obj):
    return hash(obj) % (2 ** 32)    

model = doc2vec.Doc2Vec(hashfxn=myhash)
model = doc2vec.Doc2Vec.load(""d2v.model"")

tag1 = ""012020171590""
tag2 = ""0109201716181""

print(tag1 in model.docvecs.doctags)
print(tag2 in model.docvecs.doctags)

print(model.docvecs.similarity(tag1, tag2))
print(model.docvecs.most_similar(tag1))
```

#### Results
True
True
0.24682570854972158
[('0109201716181', 9.996172904968262), ('0120201611372', 9.853036880493164), ('010120166996', 9.613503456115723), ('012020173027', 8.97104263305664), ('01202017423', 8.886014938354492), ('01002009541', 8.783470153808594), ('00002004106', 8.682585716247559), ('0109201616963', 8.671405792236328), ('011020171931', 8.659266471862793), ('011020175199', 8.573907852172852)]

#### Versions
Linux-4.9.0-6-amd64-x86_64-with-debian-9.4
Python 3.5.3 (default, Jan 19 2017, 14:11:04)
NumPy 1.14.3
SciPy 1.1.0
gensim 3.4.0
FAST_VERSION 1

",,,1,,,,,,,
246,https://github.com/RaRe-Technologies/gensim/issues/2083,2083,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 234670, 'node_id': 'MDU6TGFiZWwyMzQ2NzA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20medium', 'name': 'difficulty medium', 'color': '00d000', 'default': False, 'description': 'Medium issue: required good gensim understanding & python skills'}]",open,2018-06-05 11:20:27+00:00,,3,Doc2vec fails to train when using build_vocab_from_freq(),"#### Description
I have a Doc2Vec model trained using the `build_vocab_from_file()` function. This is so I can include a `<PAD>` token manually at index 0. This token does not appears in the original dataset, but is needed further down my program.

#### Steps/Code/Corpus to Reproduce
Here is a simple example of of what I am trying to achieve:
```python
import collections, sys

import gensim
from gensim import models
from gensim.models.doc2vec import TaggedDocument

lines = [u'It is a truth universally acknowledged',
        u'This was invitation enough.', 
        u'An invitation to dinner was soon afterwards dispatched']
words = [line.split() for line in lines]
doc_labels = [u'text0', u'tex1', u'text2']
word_freq = collections.Counter([w for line in words for w in line])
word_freq['<PAD>'] = sys.maxint # this ensure that the pad token has index 0 in gensim's vocabulary

class DocIterator(object):
    def __init__(self, docs, labels):
        self.docs = docs
        self.labels = labels
    def __iter__(self):
        for idx, doc in enumerate(self.docs):
            yield TaggedDocument(words=doc, tags=[self.labels[idx]])
            
doc_it = DocIterator(words, doc_labels)
model = gensim.models.Doc2Vec(vector_size=100, min_count=0)
model.build_vocab_from_freq(word_freq)
model.train(doc_it, total_examples=len(lines), epochs=10)
```

#### Expected Results
Expected size of `model.docvecs.count` is 3 (not 0).

#### Actual Results
Actual size of `model.docvecs.count` is 0

`print(model.docvecs.count)` -> 0

#### Versions
Linux-3.19.0-82-generic-x86_64-with-Ubuntu-15.04-vivid
('Python', '2.7.9 (default, Apr  2 2015, 15:33:21) \n[GCC 4.9.2]')
('NumPy', '1.14.3')
('SciPy', '1.1.0')
('gensim', '3.4.0')
('FAST_VERSION', 1)

Now my questions are:
- What is the correct way of using `build_vocab_from_freq()` to get a valid model? 
- Failling this, what is the best way to force gensim to include an unseen token at a specific index value in the vocabulary?",,,1,,,,,,,
248,https://github.com/RaRe-Technologies/gensim/issues/2085,2085,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 175643, 'node_id': 'MDU6TGFiZWwxNzU2NDM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': 'Current issue related to documentation'}, {'id': 233081, 'node_id': 'MDU6TGFiZWwyMzMwODE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20easy', 'name': 'difficulty easy', 'color': '00ff00', 'default': False, 'description': 'Easy issue: required small fix'}]",open,2018-06-07 13:13:53+00:00,,10,Doc2Vec to wikipedia articles notebook error -  object has no attribute,"For the Doc2Vec to wikipedia articles notebook (https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb) I get this error:

```python
pre = Doc2Vec(min_count=0)
pre.scan_vocab(documents)
```

```
executed in 11ms, finished 09:09:33 2018-06-07
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-10-0b281772fe9f> in <module>()
      1 pre = Doc2Vec(min_count=0)
----> 2 pre.scan_vocab(documents)
AttributeError: 'Doc2Vec' object has no attribute 'scan_vocab'
```

I also get a similar error for the next cell of the notebook:

```
AttributeError: 'Doc2Vec' object has no attribute 'scale_vocab'
```

Your Doc2Vec notebook on the Lee dataset (https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb) works fine for me though.

=======================
I am using Ubuntu 16.04.4 , Python 3.6, and the latest version of Gensim.",,,1,,,,,,,
293,https://github.com/RaRe-Technologies/gensim/issues/2167,2167,[],closed,2018-08-26 11:06:05+00:00,,1,Doc2Vec understanding why are there many docvectors made?,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Description
TODO: change commented example
<!-- Example: Vocabulary size is not what I expected when training Word2Vec. -->
I expected the `Doc2Vec` model to make the number of paragraph vectors equal to the number of tags used to train the model.

In the below code snipped it looks like the count of the `docvecs` is equal to the maximum tag. I'm not too sure why.
#### Steps/Code/Corpus to Reproduce

```python
""""""
Doc2Vec learns representations for words and labels simultaneously.
If you wish to only learn representations for words,
you can use the flag train_lbls=False in your Doc2Vec class. Similarly,
if you only wish to learn representations for labels and leave the word
representations fixed, the model also has the flag train_words=False.
""""""
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

EPOCHS = 500
LEARNING_RATE = 0.0001
DIMENSIONS = 100

sentences = [
    TaggedDocument([""hi"", ""there""], [1]),
    TaggedDocument([""byte""], [2]),

]
model = Doc2Vec(vector_size=DIMENSIONS, window=2, min_count=1, alpha=LEARNING_RATE, workers=4)

model.build_vocab(sentences, update=False, trim_rule=None)
print(""paragraph embeddings: %s"" % len(model.docvecs))  # 3.
print(""docvecs count %s"" % model.docvecs.count)  # 3.
print(model.docvecs.doctags)  # {}
print(""Offset %s"" % model.docvecs.offset2doctag)  # []
print(len(model.docvecs.vectors_docs))  # 3.
```

outputs
```
paragraph embeddings: 3
docvecs count 3
{}
Offset []
3
```
Which is along the lines of what I expected because only two tags are used. I don't really get why there are 3 elements in `model.docvecs`?

When I change the max tag to 200 for example

```python
sentences = [
    TaggedDocument([""hi"", ""there""], [1]),
    TaggedDocument([""byte""], [200]),

]
model = Doc2Vec(vector_size=DIMENSIONS, window=2, min_count=1, alpha=LEARNING_RATE, workers=4)
```
The output changes to 
```
paragraph embeddings: 201
docvecs count 201
{}
Offset []
201
```

Why does this occur? Does this mean 200 vectors are made even though there are only two tags in the corpus?


#### Expected Results
<!-- Example: Expected shape of (100,2).-->
*N/A*
#### Actual Results
<!-- Example: Actual shape of (100,5). 

Please paste or specifically describe the actual output or traceback. -->
*N/A*

#### Versions
<!--
Please run the following snippet and paste the output below.
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import gensim; print(""gensim"", gensim.__version__)
from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION)
-->
Darwin-15.6.0-x86_64-i386-64bit
Python 3.6.4 (default, Jan 6 2018, 11:49:38)
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]
NumPy 1.15.1
SciPy 1.1.0
gensim 3.5.0
FAST_VERSION 0

<!-- Thanks for contributing! -->

",,,1,,,,,,,
301,https://github.com/RaRe-Technologies/gensim/issues/2176,2176,[],closed,2018-09-07 16:50:37+00:00,,2,Gensim FastText() on custom corpus generates only .bin file ??,"I am using the Gensim implementation of FastText to train a custom corpus of text (and I don't want to use the pre-trained ones) . But the below command only saves the model as .bin file and there is no equivalent .vec file. Is this by design ? or am I missing something ?

>  model = FastText(sentences, size=embedding_size, window=10, min_count=1, iter=10, word_ngrams=1, sg=0, max_vocab_size=5000)

>  model.save(""somename"")
> 

please advice ",,1,,,,,,,,
364,https://github.com/RaRe-Technologies/gensim/issues/2305,2305,"[{'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}, {'id': 1162250977, 'node_id': 'MDU6TGFiZWwxMTYyMjUwOTc3', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/fasttext', 'name': 'fasttext', 'color': 'ad52ea', 'default': False, 'description': 'Issues related to the FastText model'}]",closed,2018-12-21 15:09:33+00:00,,5,Save gensim model in binary format .bin with ngrams,"I'm able to save the model in binary format without ngrams (.vec like object) so is not possible to query oov words, is there a way to save the model in binary format with ngrams and open it?
(if I try to load a bin model generated with facebook fastText gives me following expection: Supervised fastText models are not supported, even if the model got unsupervised training.",,1,,,,,,,,
588,https://github.com/RaRe-Technologies/gensim/issues/2668,2668,"[{'id': 175643, 'node_id': 'MDU6TGFiZWwxNzU2NDM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': 'Current issue related to documentation'}, {'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}]",closed,2019-11-04 10:34:22+00:00,,5,No module named 'gensim.models.fasttext_bin',"[(https://radimrehurek.com/gensim/models/_fasttext_bin.html)](url)
I tried this to load bin file of facebook pretrained fasttext wordembedding, but it warns No module named 'gensim.models.fasttext_bin'.
I have installed gensim 3.8,1",,1,,,,,,,,
125,https://github.com/RaRe-Technologies/gensim/issues/1869,1869,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 234670, 'node_id': 'MDU6TGFiZWwyMzQ2NzA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20medium', 'name': 'difficulty medium', 'color': '00d000', 'default': False, 'description': 'Medium issue: required good gensim understanding & python skills'}]",closed,2018-02-01 10:03:05+00:00,,6,MmCorpus file-like object support bug,"**Into**
We have some ""weird"" behavior if a user passes a `file-like` object to `MmCorpus`, based on [this mailing list thread](https://groups.google.com/forum/#!topic/gensim/420pRBwexKQ)

**Demonstration**

```python
from gensim.corpora import MmCorpus
import bz2

f = bz2.BZ2File(""testcorpus.mm.bz2"")
print(f.closed)  # 0
corpus = MmCorpus(f)
print(f.closed)  # 1 ???
```

**What happens**
File-like object was closed when we call `MmReader`, problem located here

https://github.com/RaRe-Technologies/gensim/blob/5342153eb4f4b02bb45bfa3951eef8250ac9f6b6/gensim/matutils.py#L1274


`with` automatically close `file-like` when we out of scope, **this is OK if we open this file**, but we **shouldn't close file-like passed from user**.

Related PR #1867 


**UPD:** another problem here - call `IndexCopus.__init__`, that didn't support `file-like` object at all.",,1,,,,,,,,
315,https://github.com/RaRe-Technologies/gensim/issues/2198,2198,"[{'id': 708355863, 'node_id': 'MDU6TGFiZWw3MDgzNTU4NjM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/need%20info', 'name': 'need info', 'color': 'fbca04', 'default': False, 'description': 'Not enough information for reproduce an issue, need more info from author'}]",closed,2018-09-23 22:13:13+00:00,,7,"Version 3.6.0 doesn't install files gensim/models/doc2vec_corpusfile.so, gensim/models/fasttext_corpusfile.so, gensim/models/word2vec_corpusfile.so","It writes these names into the plist files for the ```--record``` option (```setup.py install --record {plist}```), but doesn't actually install them:
```
===> Checking for items in pkg-plist which are not in STAGEDIR
Error: Missing: %%PYTHON_SITELIBDIR%%/gensim/models/doc2vec_corpusfile.so
Error: Missing: %%PYTHON_SITELIBDIR%%/gensim/models/fasttext_corpusfile.so
Error: Missing: %%PYTHON_SITELIBDIR%%/gensim/models/word2vec_corpusfile.so
```
",,1,,,,,,,,
341,https://github.com/RaRe-Technologies/gensim/issues/2258,2258,"[{'id': 175641, 'node_id': 'MDU6TGFiZWwxNzU2NDE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/bug', 'name': 'bug', 'color': 'e10c02', 'default': True, 'description': 'Issue described a bug'}, {'id': 233081, 'node_id': 'MDU6TGFiZWwyMzMwODE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20easy', 'name': 'difficulty easy', 'color': '00ff00', 'default': False, 'description': 'Easy issue: required small fix'}, {'id': 1162250977, 'node_id': 'MDU6TGFiZWwxMTYyMjUwOTc3', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/fasttext', 'name': 'fasttext', 'color': 'ad52ea', 'default': False, 'description': 'Issues related to the FastText model'}]",closed,2018-11-05 16:10:29+00:00,,5,Integer overflow during `FastText` training with `corpus_file`,"<!--
If your issue is a usage or a general question, please submit it here instead:
- Mailing List: https://groups.google.com/forum/#!forum/gensim
For more information, see Recipes&FAQ: https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ
-->

<!-- Instructions For Filing a Bug: https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md -->

#### Description

<!-- Example: Vocabulary size is not what I expected when training Word2Vec. -->
`model = FastText(corpus_file=""sentences_norm.txt.gz"", workers=14, iter=5, size=200, sg=1, hs=1)`

with the following sizes 

```
2018-11-05 16:57:52,809 : INFO : collected 6532860 word types from a corpus of 4728738902 raw words and 238627116 sentences
2018-11-05 16:57:52,809 : INFO : Loading a fresh vocabulary
2018-11-05 16:58:00,788 : INFO : effective_min_count=5 retains 1887156 unique words (28% of original 6532860, drops 4645704)
2018-11-05 16:58:00,788 : INFO : effective_min_count=5 leaves 4721157112 word corpus (99% of original 4728738902, drops 7581790)
2018-11-05 16:58:07,437 : INFO : deleting the raw counts dictionary of 6532860 items
2018-11-05 16:58:07,615 : INFO : sample=0.001 downsamples 26 most-common words
2018-11-05 16:58:07,615 : INFO : downsampling leaves estimated 3749158657 word corpus (79.4% of prior 4721157112)
2018-11-05 16:58:11,281 : INFO : constructing a huffman tree from 1887156 words
2018-11-05 16:59:36,077 : INFO : built huffman tree with maximum node depth 30
2018-11-05 17:00:17,300 : INFO : estimated required memory for 1887156 words, 1929637 buckets and 200 dimensions: 7871448352 bytes
2018-11-05 17:00:17,398 : INFO : resetting layer weights
2018-11-05 17:01:43,333 : INFO : Total number of ngrams is 1929637
2018-11-05 17:02:11,990 : INFO : training model with 14 workers on 1887156 vocabulary and 200 features, using sg=1 hs=1 sample=0.001 negative=5 window=5
```

yields 

```
Exception in thread Thread-2120:
Traceback (most recent call last):
  File ""/home/joelkuiper/anaconda3/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/home/joelkuiper/anaconda3/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/joelkuiper/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 175, in _worker_loop_corpusfile
    total_examples=total_examples, total_words=total_words, **kwargs)
  File ""/home/joelkuiper/anaconda3/lib/python3.6/site-packages/gensim/models/fasttext.py"", line 561, in _do_train_epoch
    total_examples, total_words, work, neu1)
  File ""gensim/models/fasttext_corpusfile.pyx"", line 126, in gensim.models.fasttext_corpusfile.train_epoch_sg
OverflowError: value too large to convert to int
```` 

on all workers. Note that the sg and hs parameters seem to have no relation to this, also happens without them. 

### Steps to reproduce
`model = FastText(corpus_file=""sentences_norm.txt.gz"", workers=14, iter=5,size=200)`

#### Expected Results
Should train the model

#### Actual Results
Exception thrown, no further output. 

```
Traceback (most recent call last):
  File ""/home/joelkuiper/anaconda3/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/home/joelkuiper/anaconda3/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/joelkuiper/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 175, in _worker_loop_corpusfile
    total_examples=total_examples, total_words=total_words, **kwargs)
  File ""/home/joelkuiper/anaconda3/lib/python3.6/site-packages/gensim/models/fasttext.py"", line 561, in _do_train_epoch
    total_examples, total_words, work, neu1)
  File ""gensim/models/fasttext_corpusfile.pyx"", line 126, in gensim.models.fasttext_corpusfile.train_epoch_sg
OverflowError: value too large to convert to int
```

#### Versions
<!--
Please run the following snippet and paste the output below.
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import gensim; print(""gensim"", gensim.__version__)
from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION)
-->

Python 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) 
[GCC 7.3.0]
NumPy 1.15.3
SciPy 1.1.0
gensim 3.6.0

On Ubuntu 16.04

**edit** seems to work fine when passing in a `LineSentence` object
<!-- Thanks for contributing! -->

",,1,,,,,,,,
264,https://github.com/RaRe-Technologies/gensim/issues/2112,2112,"[{'id': 175643, 'node_id': 'MDU6TGFiZWwxNzU2NDM=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': 'Current issue related to documentation'}, {'id': 233081, 'node_id': 'MDU6TGFiZWwyMzMwODE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20easy', 'name': 'difficulty easy', 'color': '00ff00', 'default': False, 'description': 'Easy issue: required small fix'}, {'id': 1072221028, 'node_id': 'MDU6TGFiZWwxMDcyMjIxMDI4', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/Hacktoberfest', 'name': 'Hacktoberfest', 'color': 'b396e0', 'default': False, 'description': 'Issues marked for hacktoberfest'}]",closed,2018-06-28 17:46:47+00:00,,6,`WmdSimilarity` corpus type mismatch in documentation ,"
#### Description
It seems that the  WmdSimilarity doesn't need the bow_corpus .
The corpus can be in token level, so the following code is not needed:
```python
bow_corpus = [dictionary.doc2bow(document) for document in common_texts]
```

#### Versions
```
Darwin-15.6.0-x86_64-i386-64bit
Python 3.6.3 |Intel Corporation| (default, May  3 2018, 23:25:54) 
[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)]
NumPy 1.14.3
SciPy 1.1.0
gensim 3.4.0
FAST_VERSION 1
```
",1,,,,,,,,,
300,https://github.com/RaRe-Technologies/gensim/issues/2175,2175,"[{'id': 175640, 'node_id': 'MDU6TGFiZWwxNzU2NDA=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/feature', 'name': 'feature', 'color': '0b02e1', 'default': False, 'description': 'Issue described a new feature'}, {'id': 233081, 'node_id': 'MDU6TGFiZWwyMzMwODE=', 'url': 'https://api.github.com/repos/RaRe-Technologies/gensim/labels/difficulty%20easy', 'name': 'difficulty easy', 'color': '00ff00', 'default': False, 'description': 'Easy issue: required small fix'}]",closed,2018-09-07 15:28:01+00:00,,14,Feature suggestion: relative cosine similarity for word2vec ,"Hi all,
Based on this paper, do you think it worths the effort to implement relative cosine similarity measure? 
https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf

Note that I'm not suggesting this as a potential contributor but as a grateful user.
Thank you,
Viktor
",1,,,,,,,,,
387,https://github.com/RaRe-Technologies/gensim/issues/2351,2351,[],closed,2019-01-25 10:45:36+00:00,,2,cosine similarity is not giving 1 for two same input documents,"https://groups.google.com/forum/#!topic/gensim/YiMDDtLCI7o

Any similarity to a document by itself should be 1. It seems to me it is something wrong with cosine similarity in gensim, because I do not get 1.

def cossim(texts):
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    tfidf = TfidfModel(corpus, dictionary, normalize=True)
    index = MatrixSimilarity(tfidf[corpus], num_features=len(dictionary))
    simmatrix = index[corpus]
    return simmatrix

Input text in the form of 2d list of words is [here](https://groups.google.com/group/gensim/attach/6bb39f0def72f/input_2d_text_list.txt?part=0.1&authuser=0). The cosine similarity scores for these five documents I'v got from above code are:

[[0.4484578  0.03050137 0.01659208 0.03342406 0.11152142]
 [0.01370624 0.31444198 0.1637049  0.14261982 0.05780634]
 [0.00734405 0.16124928 0.38912225 0.14186516 0.03908984]
 [0.01621059 0.15392897 0.1554464  0.34419927 0.06053988]
 [0.07290775 0.08409916 0.05773569 0.08160509 0.3643248 ]]


Numbers in diagonal should be all 1, but they are not.",1,,,,,,,,,
410,https://github.com/RaRe-Technologies/gensim/issues/2398,2398,[],closed,2019-02-28 09:27:46+00:00,,3,Similarity class does not use constant memory,"I am using the Similarity class to query a corpus (2.5M docs, features reduced to 300 using LSI). It gave me 77 shards, each ~57MB which looks ok, however when I query the index it loads everything (full 4GB) into memory, which is exactly what I wanted to avoid. The documentation states the Similarity class runs with constant memory, but that is not what happens. I used default values for chunk and shard sizes, which are 256 and 32768. The documentation states

> shardsize should be chosen so that a shardsize x chunksize matrix of floats fits comfortably into main memory.

That would be 32 MB. Why is it using 4 GB instead?

Tested on:
Windows-2012ServerR2-6.3.9600-SP0
Python 3.6.4, AMD64
NumPy 1.16.2
SciPy 1.2.1
gensim 3.7.1 (also didn't work before updating with neither 3.6.0 nor 3.4.0)

",1,,,,,,,,,
411,https://github.com/RaRe-Technologies/gensim/issues/2400,2400,[],closed,2019-03-04 10:39:12+00:00,,1,Similarity object gone after computer restart,"<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description
I'm trying to load a saved similarity object from disk.
Whenever I'm trying to load the object after I restarted the computer, the loaded object is failing to work.
I believe the problem is that maybe the tmp/ folder is cleaned when the computer restarts.

What are you trying to achieve? What is the expected result? What are you seeing instead?
I'm trying to use the Similarity index that was saved on the disk.

#### Steps/code/corpus to reproduce
I get the following traceback when calling:
index = similarities.Similarity.load(""path to .index file"") 
index.similarity_by_id(0) // Arbitrary check that the index is working as needed.

FileNotFoundError                         Traceback (most recent call last)
<ipython-input-302-400050246fc0> in <module>
----> 1 index.similarity_by_id(0)

~/.local/lib/python3.6/site-packages/gensim/similarities/docsim.py in similarity_by_id(self, docpos)
    622 
    623         """"""
--> 624         query = self.vector_by_id(docpos)
    625         norm, self.norm = self.norm, False
    626         result = self[query]

~/.local/lib/python3.6/site-packages/gensim/similarities/docsim.py in vector_by_id(self, docpos)
    593         if not self.shards or docpos < 0 or docpos >= pos:
    594             raise ValueError(""invalid document position: %s (must be 0 <= x < %s)"" % (docpos, len(self)))
--> 595         result = shard.get_document_id(docpos - pos + len(shard))
    596         return result
    597 

~/.local/lib/python3.6/site-packages/gensim/similarities/docsim.py in get_document_id(self, pos)
    186         """"""
    187         assert 0 <= pos < len(self), ""requested position out of range""
--> 188         return self.get_index().index[pos]
    189 
    190     def __getitem__(self, query):

~/.local/lib/python3.6/site-packages/gensim/similarities/docsim.py in get_index(self)
    162         if not hasattr(self, 'index'):
    163             logger.debug(""mmaping index from %s"", self.fullname())
--> 164             self.index = self.cls.load(self.fullname(), mmap='r')
    165         return self.index
    166 

~/.local/lib/python3.6/site-packages/gensim/utils.py in load(cls, fname, mmap)
    424         compress, subname = SaveLoad._adapt_by_suffix(fname)
    425 
--> 426         obj = unpickle(fname)
    427         obj._load_specials(fname, mmap, compress, subname)
    428         logger.info(""loaded %s"", fname)

~/.local/lib/python3.6/site-packages/gensim/utils.py in unpickle(fname)
   1379 
   1380     """"""
-> 1381     with smart_open(fname, 'rb') as f:
   1382         # Because of loading from S3 load can't be used (missing readline in smart_open)
   1383         if sys.version_info > (3, 0):

~/.local/lib/python3.6/site-packages/smart_open/smart_open_lib.py in smart_open(uri, mode, **kw)
    179         raise TypeError('mode should be a string')
    180 
--> 181     fobj = _shortcut_open(uri, mode, **kw)
    182     if fobj is not None:
    183         return fobj

~/.local/lib/python3.6/site-packages/smart_open/smart_open_lib.py in _shortcut_open(uri, mode, **kw)
    299     #
    300     if six.PY3:
--> 301         return open(parsed_uri.uri_path, mode, buffering=buffering, **open_kwargs)
    302     elif not open_kwargs:
    303         return open(parsed_uri.uri_path, mode, buffering=buffering)

FileNotFoundError: [Errno 2] No such file or directory: '.shrd.0'


Include full tracebacks, logs and datasets if necessary. Please keep the examples minimal (""minimal reproducible example"").

#### Versions

Please provide the output of:

```python
import platform; print(platform.platform()) - Linux-4.15.0-45-generic-x86_64-with-Ubuntu-18.04-bionic
import sys; print(""Python"", sys.version) - Python 3.6.7 (default, Oct 22 2018, 11:32:17)  [GCC 8.2.0]
import numpy; print(""NumPy"", numpy.__version__) - NumPy 1.16.1
import scipy; print(""SciPy"", scipy.__version__) - SciPy 1.2.1
import gensim; print(""gensim"", gensim.__version__) - gensim 3.7.1
from gensim.models import word2vec;print(""FAST_VERSION"", word2vec.FAST_VERSION) - FAST_VERSION 1
```
",1,,,,,,,,,
