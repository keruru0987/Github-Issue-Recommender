,html_url,number,labels,state,created_at,pull_request,comments,title,body,1,2,3,4,5,6,7,8,9,10
1062,https://github.com/allenai/allennlp/issues/2856,2856,[],closed,2019-05-17 12:31:22+00:00,,1,Multi Labels prediction using Allennlp,"Hi,
We have a use case where we need to predict the label as well as the classes within the label. It is like predicting two columns (multi label target prediction). I am not sure whether I can use the existing library example for my requirement.
Please suggest me whether it's possible using allennlp.
Thanks in Advance",,,,,,,1,,,1
47,https://github.com/allenai/allennlp/issues/654,654,[],closed,2017-12-30 02:49:40+00:00,,3,unlabeled test set cannot be included in training vocabulary ,"In the ""Kaggle competition"" use case, the test dataset has no labels. This means that if you try to specify `test_data_path` (e.g. in order to get the embeddings for that vocabulary), the trainer will crash with

```
allennlp.common.checks.ConfigurationError: 'You cannot construct a Dataset with non-homogeneous Instances.'
```

I was able to workaround this by having the `DatasetReader` generate fake labels, but that's not a good solution.",,,,,,,,,,1
87,https://github.com/allenai/allennlp/issues/817,817,[],closed,2018-02-10 01:08:37+00:00,,1,The NER JS code doesn't check for tag type consistency when creating spans from BIOUL labels,"This means that the visual demo output does not match the output of running a model locally in the case that the tag sequence is ill defined. We should implement constrained decoding to fix this, because it should also improve the model.

Here is the relevant code:

https://github.com/allenai/allennlp/blob/master/demo/src/components/NamedEntityComponent.js#L172",,,,,,,,,,1
195,https://github.com/allenai/allennlp/issues/1109,1109,[],closed,2018-04-19 23:07:18+00:00,,8,Semantic role labeling prediction doesn't work,"The prediction example on http://allennlp.org/models throws an exception on cbe58971a15aaf6a80d3cc53b1e7c4a9ba4e089c.

```
$ echo '{""sentence"": ""Did Uriah honestly think he could beat the game in under three hours?""}' > examples.jsonl
$ python -m allennlp.run predict https://s3-us-west-2.amazonaws.com/allennlp/models/srl-model-2017.09.05.tar.gz examples.jsonl

/Users/michael/miniconda3/envs/allennlp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-04-19 16:06:02,107 - INFO - allennlp.models.archival - extracting archive file /Users/michael/.allennlp/datasets/aHR0cHM6Ly9zMy11cy13ZXN0LTIuYW1hem9uYXdzLmNvbS9hbGxlbm5scC9tb2RlbHMvc3JsLW1vZGVsLTIwMTcuMDkuMDUudGFyLmd6.e53f556a3939b010393f1ed059dd0c61-7 to temp dir /var/folders/v5/tlk1sh3d0l94fcmz10vtzmyh0000gn/T/tmpgrwwn00z
2018-04-19 16:06:03,180 - INFO - allennlp.data.vocabulary - Loading token dictionary from /var/folders/v5/tlk1sh3d0l94fcmz10vtzmyh0000gn/T/tmpgrwwn00z/vocabulary.
2018-04-19 16:06:03,265 - INFO - allennlp.common.params - model.type = srl
2018-04-19 16:06:03,266 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2018-04-19 16:06:03,266 - INFO - allennlp.common.params - model.text_field_embedder.tokens.type = embedding
2018-04-19 16:06:03,266 - INFO - allennlp.common.params - model.text_field_embedder.tokens.num_embeddings = None
2018-04-19 16:06:03,266 - INFO - allennlp.common.params - model.text_field_embedder.tokens.vocab_namespace = tokens
2018-04-19 16:06:03,267 - INFO - allennlp.common.params - model.text_field_embedder.tokens.embedding_dim = 100
2018-04-19 16:06:03,267 - INFO - allennlp.common.params - model.text_field_embedder.tokens.pretrained_file = None
2018-04-19 16:06:03,267 - INFO - allennlp.common.params - model.text_field_embedder.tokens.projection_dim = None
2018-04-19 16:06:03,267 - INFO - allennlp.common.params - model.text_field_embedder.tokens.trainable = True
2018-04-19 16:06:03,268 - INFO - allennlp.common.params - model.text_field_embedder.tokens.padding_index = None
2018-04-19 16:06:03,268 - INFO - allennlp.common.params - model.text_field_embedder.tokens.max_norm = None
2018-04-19 16:06:03,268 - INFO - allennlp.common.params - model.text_field_embedder.tokens.norm_type = 2.0
2018-04-19 16:06:03,268 - INFO - allennlp.common.params - model.text_field_embedder.tokens.scale_grad_by_freq = False
2018-04-19 16:06:03,268 - INFO - allennlp.common.params - model.text_field_embedder.tokens.sparse = False
Traceback (most recent call last):
  File ""/Users/michael/hack/allenai/allennlp/allennlp/common/params.py"", line 94, in pop
    value = self.params.pop(key)
KeyError: 'encoder'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/michael/miniconda3/envs/allennlp/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/michael/miniconda3/envs/allennlp/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/michael/hack/allenai/allennlp/allennlp/run.py"", line 18, in <module>
    main(prog=""python -m allennlp.run"")
  File ""/Users/michael/hack/allenai/allennlp/allennlp/commands/__init__.py"", line 65, in main
    args.func(args)
  File ""/Users/michael/hack/allenai/allennlp/allennlp/commands/predict.py"", line 159, in _predict
    predictor = _get_predictor(args)
  File ""/Users/michael/hack/allenai/allennlp/allennlp/commands/predict.py"", line 105, in _get_predictor
    overrides=args.overrides)
  File ""/Users/michael/hack/allenai/allennlp/allennlp/models/archival.py"", line 149, in load_archive
    cuda_device=cuda_device)
  File ""/Users/michael/hack/allenai/allennlp/allennlp/models/model.py"", line 274, in load
    return cls.by_name(model_type)._load(config, serialization_dir, weights_file, cuda_device)
  File ""/Users/michael/hack/allenai/allennlp/allennlp/models/model.py"", line 221, in _load
    model = Model.from_params(vocab, model_params)
  File ""/Users/michael/hack/allenai/allennlp/allennlp/models/model.py"", line 195, in from_params
    model = cls.by_name(choice).from_params(vocab, params)
  File ""/Users/michael/hack/allenai/allennlp/allennlp/models/semantic_role_labeler.py"", line 209, in from_params
    encoder = Seq2SeqEncoder.from_params(params.pop(""encoder""))
  File ""/Users/michael/hack/allenai/allennlp/allennlp/common/params.py"", line 96, in pop
    raise ConfigurationError(""key \""{}\"" is required at location \""{}\"""".format(key, self.history))
allennlp.common.checks.ConfigurationError: 'key ""encoder"" is required at location ""model.""'
```",,,,,,,,,,1
231,https://github.com/allenai/allennlp/issues/1194,1194,[],closed,2018-05-10 23:02:53+00:00,,2,"After pytorch 0.4, make `LabelField.as_tensor a 0-tensor?","it feels like that would make sense? although it would break a ton of things. quite likely there are other places where this would make sense too.

(the genesis of this thought was working on the tutorial and having to explain why every use of `labels` requires a `.squeeze(-1)`)",,,,,,,,,,1
427,https://github.com/allenai/allennlp/issues/1659,1659,[],closed,2018-08-24 01:52:20+00:00,,2,Should we make CrfTagger work with all sequence labeling datasets?,"Currently, `CrfTagger` only works with BIO-tagging datasets, since a `SpanF1Metric` is created by default.

It'd be simple to adapt it to work with datasets like CCG tagging / POS tagging, but that might involve adding some more arguments to the model (e.g., an argument for `constrain_crf_decoding` and `calculate_span_f1`).

Do we want to add these arguments, considering that the `CrfTagger` is used in the tutorial (and thus it might make sense to have it be as simple as possible).",,,,,,,,,,1
436,https://github.com/allenai/allennlp/issues/1681,1681,[],closed,2018-08-28 21:01:41+00:00,,2,Order of predicted labels from textual-entailment,"What is order of the labels outputed by the textual entailment model? I assume it is Entailment, Contradiction and Neutral, but wanted to double check that assumption is correct. 

eg
```
'premise_tokens': ['The', 'primary', 'colors', 'are', 'green', ',', 'yellow', 
',', 'and', 'red',.', '@@NULL@@'],
 'hypothesis_tokens': ['Red', 'is', 'a', 'primary', 'color', '.', '@@NULL@@'],
 'label_probs': [0.5689316391944885, 0.17109070718288422, 0.2599776089191437],
```",,,,,,,,,,1
465,https://github.com/allenai/allennlp/issues/1752,1752,[],closed,2018-09-11 22:57:25+00:00,,2,Unseen span labels in constituency parser model,"I'm evaluating the constituency parser model on the PTB fragment distributed with NLTK (which I compiled into one file with all the merged trees).

However, I believe I ran into an issue marked in a ""TODO"" in the code [here](https://github.com/allenai/allennlp/blob/4920249af097d1557be6ece3e5ce80d1f6ea0333/allennlp/data/dataset_readers/penn_tree_bank.py#L184):

```
TODO(Mark): If we encounter a gold nested labelling at test time
which we haven't encountered, we won't be able to run the model
at all.
```

Specifically, the issue is when there is a span label in the evaluation data that wasn't in the training data. When this occurs, the model throws an error because OOV labels are dis-allowed. In my case the span label was ""SQ-FRAG"", indicating nested ""SQ"" and ""FRAG"" spans. I'm guessing this has to do with using the NLTK version of PTB instead of the LDC version (which isn't freely available).

I fixed the error by changing the span label namespace to something without the suffix ""label"" (ie ""test""), which I understand enables OOV tokens to be legal. The model evaluates and returns the following results:

```
2018-09-11 22:47:41,597 - INFO - allennlp.commands.evaluate - Finished evaluating.
2018-09-11 22:47:41,597 - INFO - allennlp.commands.evaluate - Metrics:
2018-09-11 22:47:41,597 - INFO - allennlp.commands.evaluate - tag_accuracy: 0.01969830953647653
2018-09-11 22:47:41,597 - INFO - allennlp.commands.evaluate - evalb_recall: 0.9104670632597776
2018-09-11 22:47:41,597 - INFO - allennlp.commands.evaluate - evalb_precision: 0.9092911330451097
2018-09-11 22:47:41,597 - INFO - allennlp.commands.evaluate - evalb_f1_measure: 0.9098787182085938
```

I'm not exactly sure what it should output, but I suspect that ""tag_accuracy"" shouldn't be that low, so I suspect my attempted ""fix"" broke something else. Or perhaps the NLTK dataset is sufficiently different that this won't work at all. I would appreciate if someone knew if there was an easy fix to this issue or not.",,,,,,,,,,1
586,https://github.com/allenai/allennlp/issues/1998,1998,[],closed,2018-11-01 12:21:11+00:00,,1,LabelField Indexing Error,"I've written a rough class to read and index a text classification dataset so that it can be used with pretrained ELMo embeddings:

```
import pandas as pd
from allennlp.data import Instance
from allennlp.data.fields import TextField, LabelField
from allennlp.data.tokenizers import WordTokenizer
from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer
from allennlp.data.vocabulary import Vocabulary
from allennlp.data.dataset import Batch
from allennlp.data.dataset_readers import DatasetReader

class TabularReader(DatasetReader):
    def __init__(self, text_name, label_name, sep, batch_size):
        super().__init__(lazy=False)
        self.sep = sep
        self.text_name = text_name
        self.label_name = label_name
        self.batch_size = batch_size

        self.vocab = Vocabulary()
        self.tokeniser = WordTokenizer()
        self.token_indexers = {""character_ids"": ELMoTokenCharactersIndexer()}

    def text_to_instance(self, tokens, label):
        sentence_field = TextField(tokens, self.token_indexers)
        label_field = LabelField(label=label, label_namespace=""labels"")
        fields = {""sentence"": sentence_field, ""labels"": label_field}
        return Instance(fields)

    def _read(self, file_path):
        df = pd.read_csv(file_path, self.sep)
        text_col, label_col = df[self.text_name], df[self.label_name]

        instances = []
        for idx, sentence in enumerate(text_col):
            instances.append(self.text_to_instance(
                self.tokeniser.tokenize(sentence), label_col[idx]))

            if (idx + 1) % self.batch_size == 0:
                batch = Batch(instances)
                instances = []
                batch.index_instances(self.vocab)
                yield batch.as_tensor_dict()
```
However it throws a confusing OOV `KeyError` when it tries to index the `LabelField`:

```
Traceback (most recent call last):
  File ""test_elmo.py"", line 56, in <module>
    train_dataset = reader.read(""data/train.tsv"")
  File ""/Users/gabrielgordon-hall/anaconda/lib/python3.6/site-packages/allennlp/data/dataset_readers/dataset_reader.py"", line 73, in read
    instances = [instance for instance in Tqdm.tqdm(instances)]
  File ""/Users/gabrielgordon-hall/anaconda/lib/python3.6/site-packages/allennlp/data/dataset_readers/dataset_reader.py"", line 73, in <listcomp>
    instances = [instance for instance in Tqdm.tqdm(instances)]
  File ""/Users/gabrielgordon-hall/anaconda/lib/python3.6/site-packages/tqdm/_tqdm.py"", line 937, in __iter__
    for obj in iterable:
  File ""test_elmo.py"", line 47, in _read
    batch.index_instances(self.vocab)
  File ""/Users/gabrielgordon-hall/anaconda/lib/python3.6/site-packages/allennlp/data/dataset.py"", line 155, in index_instances
    instance.index_fields(vocab)
  File ""/Users/gabrielgordon-hall/anaconda/lib/python3.6/site-packages/allennlp/data/instance.py"", line 60, in index_fields
    field.index(vocab)
  File ""/Users/gabrielgordon-hall/anaconda/lib/python3.6/site-packages/allennlp/data/fields/label_field.py"", line 83, in index
    self._label_id = vocab.get_token_index(self.label, self._label_namespace)  # type: ignore
  File ""/Users/gabrielgordon-hall/anaconda/lib/python3.6/site-packages/allennlp/data/vocabulary.py"", line 591, in get_token_index
    return self._token_to_index[namespace][self._oov_token]
KeyError: '@@UNKNOWN@@'
```
Why does the `self._token_to_index` throw this error? Surely it should just add the token to the `__NamespaceDependentDefaultDict`? Or am I barking up the wrong tree?

",,,,,,,,,,1
608,https://github.com/allenai/allennlp/issues/2038,2038,[],closed,2018-11-10 21:23:36+00:00,,2,Vocab labels empty,"**Describe the bug**
A clear and concise description of what the bug is.
I use this reader
```python
class MRDatasetReader(DatasetReader):
    def __init__(self,
             tokenizer: Tokenizer = None,
             token_indexers: Dict[str, TokenIndexer] = None,
             lazy: bool = False) -> None:
        super().__init__(lazy)

        self._tokenizer = tokenizer or WordTokenizer()
        self._token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}
    
    def _read(self, file_path):
        with open(cached_path(file_path), ""r"") as data_file:
            df = pd.read_csv(file_path, dtype={""PhraseId"": np.float, ""Sentiment"": np.float, ""SentenceId"": np.float})
            #print(""count = "", len(train.where(train['Sentiment'] == 0).dropna()))
            for i, item in df.iterrows():
                phrase_id = item[""PhraseId""]
                sentence_id = item[""SentenceId""]
                phrase = item[""Phrase""]
                sentiment = item[""Sentiment""]
                yield self.text_to_instance(phrase_id, sentence_id, phrase, sentiment)
            
    def text_to_instance(self, phrase_id, sentence_id, phrase, sentiment) -> Instance:
        tokenized_phrase = self._tokenizer.tokenize(phrase)
        
        phrase_field = TextField(tokenized_phrase, self._token_indexers)
        #phrase_id_field = MetadataField(phrase_id)
        #sentence_id_field = MetadataField(sentence_id)
        fields = {
            ""phrase"": phrase_field
        }
        
        #print(f""sentiment = {sentiment} | sentiment-1 = {sentiment-1}"")
        fields[""labels""] = LabelField(sentiment, skip_indexing=True)
            
        return Instance(fields)
```
sentiment is an int.

now when I load my vocabulary `vocab = Vocabulary.from_instances(val_dataset)`

and print this: `vocab._token_to_index[""labels""]`

it returns {}

this is a problem ...

now if i replace the LabelField like this: `fields[""labels""] = LabelField(str(int(sentiment)))`

it returns then this: `{'2': 0, '3': 1, '1': 2, '4': 3, '0': 4}` (for this print: `vocab._token_to_index[""labels""]`)



**To Reproduce**
Steps to reproduce the behavior
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen. Ok

skip_indexing work fine .. i need to get something like {0:0, 1:1, 2:2, etc..}. i dont know but definitely not empty like {}. i need labels to pass to forward with full size, not just 0 when I call `print(""vocab size output ="", vocab.get_vocab_size(""labels""))`. when I do it without skip_indexing

**System (please complete the following information):**
 - OS: [e.g. OSX, Linux] kaggle
 - Python version: [if it's not 3.6.1 or later, that's probably your problem] its the kaggle version
 - AllenNLP version: [e.g. v0.7.0, or ""I installed from master""] 0.7.1
 - PyTorch version: (if you installed it yourself) 0.4.1

**Additional context**
Add any other context about the problem here.
Yes

i load vocab like this: `vocab = Vocabulary.from_instances(train_dataset + val_dataset)`

before i add token_indexers to datasetreader

```
token_indexers = { 
    ""tokens"": SingleIdTokenIndexer(lowercase_tokens=True),
    ""elmo"": ELMoTokenCharactersIndexer(namespace=""elmo""),
    ""token_characters"": TokenCharactersIndexer(character_tokenizer=CharacterTokenizer(byte_encoding=""utf-8"", start_tokens=[259], end_tokens=[260]))
}
```",,,,,,,,,,1
674,https://github.com/allenai/allennlp/issues/2147,2147,[],closed,2018-12-06 06:33:14+00:00,,4,Unknown label during NER 'predict',"**System (please complete the following information):**
 - OS: Linux
 - Python version: 3.6.7
 - AllenNLP version: v0.6.2

I am using the allennlp  'evaluate' command on an NER model trained on a subset on the Ontonotes 5.0 dataset. While making predictions on the test set, I encounter an unknown label, 'I-TIME' due to which the model throws and error:
`  File ""/home/radhikaparik/.conda/envs/allennlp/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/radhikaparik/.conda/envs/allennlp/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/run.py"", line 18, in <module>
    main(prog=""allennlp"")
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/commands/__init__.py"", line 70, in main
    args.func(args)
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/commands/evaluate.py"", line 151, in evaluate_from_args
    metrics = evaluate(model, instances, iterator, args.cuda_device)
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/commands/evaluate.py"", line 103, in evaluate
    for batch in generator_tqdm:
  File ""/home/radhikaparik/.conda/envs/allennlp/lib/python3.6/site-packages/tqdm/_tqdm.py"", line 979, in __iter__
    for obj in iterable:
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/data/iterators/data_iterator.py"", line 148, in __call__
    batch.index_instances(self.vocab)
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/data/dataset.py"", line 156, in index_instances
    instance.index_fields(vocab)
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/data/instance.py"", line 60, in index_fields
    field.index(vocab)
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/data/fields/sequence_label_field.py"", line 88, in index
    for label in self.labels]
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/data/fields/sequence_label_field.py"", line 88, in <listcomp>
    for label in self.labels]
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/data/vocabulary.py"", line 571, in get_token_index
    return self._token_to_index[namespace][self._oov_token]
KeyError: '@@UNKNOWN@@'`

 I tried discarding the example by removing it from the batch of instances, but that throws the following error:

`Traceback (most recent call last):
  File ""/home/radhikaparik/.conda/envs/allennlp/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/radhikaparik/.conda/envs/allennlp/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/run.py"", line 18, in <module>
                field_tensors[field].append(tensors)
    main(prog=""allennlp"")
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/commands/__init__.py"", line 70, in main
    args.func(args)
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/commands/evaluate.py"", line 151, in evaluate_from_args
    metrics = evaluate(model, instances, iterator, args.cuda_device)
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/commands/evaluate.py"", line 103, in evaluate
    for batch in generator_tqdm:
  File ""/home/radhikaparik/.conda/envs/allennlp/lib/python3.6/site-packages/tqdm/_tqdm.py"", line 979, in __iter__
    for obj in iterable:
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/data/iterators/data_iterator.py"", line 150, in __call__
    padding_lengths = batch.get_padding_lengths()
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/data/dataset.py"", line 58, in get_padding_lengths
    for instance in self.instances]
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/data/dataset.py"", line 58, in <listcomp>
    for instance in self.instances]
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/data/instance.py"", line 69, in get_padding_lengths
    lengths[field_name] = field.get_padding_lengths()
  File ""/home/radhikaparik/taskonomy-nlp/allennlp/data/fields/text_field.py"", line 76, in get_padding_lengths
    raise ConfigurationError(""You must call .index(vocabulary) on a ""
allennlp.common.checks.ConfigurationError: 'You must call .index(vocabulary) on a field before determining padding lengths.'
[INFO/MainProcess] process shutting down`

Is there a standard way to deal with unseen labels during 'evaluate'? 
",,,,,,,,,,1
680,https://github.com/allenai/allennlp/issues/2155,2155,[],closed,2018-12-08 09:27:13+00:00,,3,Label Counter to automatically generate label weights,"**Is your feature request related to a problem? Please describe.**
When training a model with unbalanced labels, I would like to use weights in loss function, which is not very friendly in AllenNLP.

**Describe the solution you'd like**
By adding label counts in vocabulary, thus users could use this sttributes to build label weights automatically in any model.

",,,,,,,,,,1
748,https://github.com/allenai/allennlp/issues/2285,2285,[],closed,2019-01-05 12:57:50+00:00,,0,[Bug] The type of `MultiLabelField.as_tensor`'s returned value is not always `LongTensor`,"**Describe the bug**
There is a test case producing errors:
```python
import numpy
import pytest

from allennlp.common.testing import AllenNlpTestCase
from allennlp.data.fields import MultiLabelField


class TestMultiLabelField(AllenNlpTestCase):
    def test_as_tensor_returns_integer_tensor(self):
        f = MultiLabelField([2, 3], skip_indexing=True, label_namespace=""test1"", num_labels=5)
        tensor = f.as_tensor(f.get_padding_lengths()).detach().cpu().tolist()
        assert tensor == [0, 0, 1, 1, 0]
        assert set([type(item) for item in tensor]) == set([int])
```

Run this test case, and you will get an error as follow:
```txt
...
Expected :{<class 'int'>}
Actual   :{<class 'float'>}
...
AssertionError
```

**System (please complete the following information):**
- OS: Linux
- Python version: 3.6.7
- AllenNLP version: 0.8.1-unreleased
- PyTorch version: 1.0.0

**Additional context**
PR is coming soon.
",,,,,,,,,,1
811,https://github.com/allenai/allennlp/issues/2415,2415,"[{'id': 887719346, 'node_id': 'MDU6TGFiZWw4ODc3MTkzNDY=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Contributions%20welcome', 'name': 'Contributions welcome', 'color': '02b8d1', 'default': False, 'description': ''}]",closed,2019-01-22 03:10:05+00:00,,1,MultiLabelField's `empty_field` method always fails,"**Describe the bug**
MultiLabelField's `empty` method always fails, preventing it from being used with padding.

**To Reproduce**
```python
from allennlp.data.fields import MultiLabelField
f = MultiLabelField([])
f.empty_field()
```
Prints error:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/julian/.dotfiles/local/miniconda3/lib/python3.6/site-packages/allennlp/data/fields/multilabel_field.py"", line 120, in empty_field
    return MultiLabelField([], self._label_namespace, skip_indexing=True)
  File ""/Users/julian/.dotfiles/local/miniconda3/lib/python3.6/site-packages/allennlp/data/fields/multilabel_field.py"", line 68, in __init__
    raise ConfigurationError(""In order to skip indexing, num_labels can't be None."")
allennlp.common.checks.ConfigurationError: ""In order to skip indexing, num_labels can't be None.""
```
Also,
```python
f = MultiLabelField([], skip_indexing = True, num_labels = 4)
f.empty_field()
```
throws the same error.

**Expected behavior**
Basically I think the implementation of `empty_field(self)` should be something like:
```python
return MultiLabelField([], self._label_namespace, skip_indexing = self._num_labels is not None, self._num_labels)
```
This way it would bypass the indexing step when possible (i.e., `num_labels` is already known, for example if we're producing an empty field from an already-indexed one), but always do it when necessary (i.e., when `num_labels` is not already known).

**System (please complete the following information):**
 - OS: OSX
 - Python version: 3.6.5
 - AllenNLP version: v0.8.0

**Additional context**
This makes it impossible to use MultiLabelField inside ListField. Surprising that nobody has come across this before but I guess MultiLabelField is not often used...",,,,,,,,,,1
937,https://github.com/allenai/allennlp/issues/2651,2651,[],closed,2019-03-27 13:44:01+00:00,,0,TextClassificationJsonReader doesn't allow skip_label_indexing=True,"**Describe the bug**
The DataReader subclass TextClassificationJsonReader gives the following `ConfigurationError`
when reading data that has numeric labels (starting from 0):

```
ConfigurationError: 'In order to skip indexing, your labels must be integers. Found label = 0'
```

**To Reproduce**
1. Create a json lines file named 'example.jsonl' that contains two rows:
```
with open(""example.jsonl"", ""w"") as f:
    f.write(""{\""label\"": 0, \""text\"": \""This text has label 0\""}\n"")
    f.write(""{\""label\"": 1, \""text\"": \""This text has label 1\""}"")
```
2. Import and instantiate TextClassificationJsonReader class:
```
from allennlp.data.dataset_readers import TextClassificationJsonReader
tcjr = TextClassificationJsonReader(skip_label_indexing=True)
```
3. Attempt to read data from file.
```
reader = tcjr.read(file_path='example.jsonl')
```
**Expected behavior**
Provide a list of `Instance`s. 

**System**
 - OS: Linux
 - Python version: 3.6.7
 - AllenNLP version: 0.8.2",,,,,,,,,,1
1017,https://github.com/allenai/allennlp/issues/2788,2788,[],closed,2019-05-01 16:54:51+00:00,,3,Embed padded LabelFields,"**System:**
 - OS: OSX
 - Python version: 3.6.8
 - AllenNLP version: v0.8.3
 - PyTorch version: v1.0.0

**Question**
I know this isn't the most common use-case, but I was wondering if this was even possible without manually indexing the label vocabulary. The issue right now is that potential padding tokens are indexed as -1 for ```LabelField```s. Using a generic ```Embedding``` module makes embedding this very hacky, because -1 throws an ```IndexError```. 

The ideal scenario would be to allow unknown/pad labels to be indexed as a positive integer, and combine that with perhaps ```padding_index``` during embedding, so they'd be embedded as a vector of zeroes.

Is there an obvious way to do this that I'm just missing? Cheers.",,,,,,,,,,1
1078,https://github.com/allenai/allennlp/issues/2883,2883,"[{'id': 887719346, 'node_id': 'MDU6TGFiZWw4ODc3MTkzNDY=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Contributions%20welcome', 'name': 'Contributions welcome', 'color': '02b8d1', 'default': False, 'description': ''}]",open,2019-05-24 04:29:51+00:00,,3,AUC for Multi-label Multi-class classification task,"I am looking forward to have  AUC for multi-label multi-class classification task but the AllenNLP  library currently does not support it 

Has anyone tried it earlier ?",,,,,,,,,,1
1175,https://github.com/allenai/allennlp/issues/3050,3050,[],closed,2019-07-11 08:44:48+00:00,,6,How to index labels when using BERT?,"I am using BERT as the token encoder, and then do sequence tagging. The original labels are strings.

I am puzzled how to do the label indexing in this case. By instruction, we need to pass empty vocabulary to BERT indexer; however, for the labels I must build an vocabulary first, and use SingleIdTokenIndexer. I am not sure how to correctly achieve this. We can assume we only have 'tokens' and 'labels' in the instance where 'labels' is a SequenceLabelField. Any quick example is very much appreciated!",,,,,,,,,,1
1229,https://github.com/allenai/allennlp/issues/3137,3137,[],closed,2019-08-12 08:48:51+00:00,,2,BUG: Can't create an empty SequenceLabelField with a ListField,"**Describe the bug**
Can not create a SequenceLabelField with an empty ListField provided.

The init function will check whether the length of provided ListField and labels are same. This will cause an error in `empty_field`, which pass a 0-length label and a 1-length empty ListField.


```
allennlp.common.checks.ConfigurationError: ""Label length and sequence length don't match: 0 and 1""
```
**To Reproduce**
```
empty_text_field = TextField([], {'tokens': SingleIdTokenIndexer()}).empty_field()
empty_span_field = SpanField(-1, -1, empty_text_field).empty_field()
empty_list_field = ListField([empty_span_field]).empty_field()
empty_seq_label_field = SequenceLabelField([0], sequence_field=empty_list_field).empty_field()
```

**Expected behavior**
return an empty SequenceLabelField

",,,,,,,,,,1
1236,https://github.com/allenai/allennlp/issues/3156,3156,"[{'id': 887719346, 'node_id': 'MDU6TGFiZWw4ODc3MTkzNDY=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Contributions%20welcome', 'name': 'Contributions welcome', 'color': '02b8d1', 'default': False, 'description': ''}]",open,2019-08-14 14:49:06+00:00,,2,"compute span-f1 with  label_encoding :""BIOUL""","**System (please complete the following information):**
 - OS: Mac os
 - Python version: 3.6
 - AllenNLP version: latest
 - PyTorch version: 1.2

**Question**

* I want to compute span-f1 with 'BIOUL' label encoding, and I find that the function 'bioul_tags_to_spans' does not allow ill-formed spans. But  the prediction output of the model
may be ill-formed, and will raise 'InvalidTagSequence' Error, so that the span-f1 can not compute.

",,,,,,,,,,1
1334,https://github.com/allenai/allennlp/issues/3335,3335,[],closed,2019-10-09 12:22:34+00:00,,2,FBetaMeasure works incorrectly with both average and labels ,"**Describe the bug**
Suppose I have classification problem with 2 classes of interest and so-called negative class (like O in BIO scheme). I would like to compute f1-micro/macro for all classes of interest except negative class, so assuming that its vocabulary index in target namespace is 0, I want something like this:
In model code:
```
self.metric = FBetaMeasure(
                average='micro',
                labels=list(range(1, self.n_classes))
            )
```
In the `fbeta_measure.py`:
```
def __call__(...):
        ...
        if self._average == 'micro':
            tp_sum = tp_sum.sum()
            pred_sum = pred_sum.sum()
            true_sum = true_sum.sum()
       ...
        if self._labels is not None:
            # Retain only selected labels and order them
            precision = precision[self._labels]
            recall = recall[self._labels]
            fscore = fscore[self._labels]
```
When using sum or mean, we get tensor of size 0 which cannot be further indexed by `self._labels`.
Seems like simply changing order of the snippets above fixes the problem.

I can make a PR that fixes this.


**System (please complete the following information):**
AllenNLP==0.9.0
PyTorch==1.2.0

",,,,,,,,,,1
1355,https://github.com/allenai/allennlp/issues/3377,3377,[],closed,2019-10-18 15:23:52+00:00,,2,TypeError: forward() got an unexpected keyword argument 'labels',"Training a BERT model using PyTorch transformers (following the tutorial [here](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)).

Following statement in the tutorial 
`loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)`  
leads to 
`TypeError: forward() got an unexpected keyword argument 'labels'`

Here is the full error,

```
TypeError                                 Traceback (most recent call last)
<ipython-input-53-56aa2f57dcaf> in <module>
     26         optimizer.zero_grad()
     27         # Forward pass
---> 28         loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
     29         train_loss_set.append(loss.item())
     30         # Backward pass

~/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--> 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

TypeError: forward() got an unexpected keyword argument 'labels'

```
I cant seem to figure out what kind of argument the forward() function expects.

There is a similar problem [here](https://github.com/allenai/allennlp/issues/2528), but I still do not get what the solution is.

**System information:**
 - OS: Ubuntu 16.04 LTS
 - Python version: 3.6.x
 - Torch version: 1.3.0 
 - Torch Vision version: 0.4.1
- PyTorch transformers version: 1.2.0
",,,,,,,,,,1
1413,https://github.com/allenai/allennlp/issues/3470,3470,[],closed,2019-11-19 21:40:26+00:00,,2,BERT option for SrlReader causes SemanticRoleLabeler to crash during training,"When trying to train `allennlp.models.semantic_role_labeler.SemanticRoleLabeler` using instances that were created with `allennlp.data.dataset_readers.semantic_role_labeling.SrlReader(bert_model_name=""<model>"")`, the training crashes with the following traceback:

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-4-f35af24bb309> in <module>
      8 iterator.index_with(vocab)
      9 trainer = Trainer(model, torch.optim.Adam(model.parameters()), iterator, instances)
---> 10 trainer.train()

c:\users\ben\miniconda3\envs\srlviz\lib\site-packages\allennlp\training\trainer.py in train(self)
    476         for epoch in range(epoch_counter, self._num_epochs):
    477             epoch_start_time = time.time()
--> 478             train_metrics = self._train_epoch(epoch)
    479 
    480             # get peak of memory usage

c:\users\ben\miniconda3\envs\srlviz\lib\site-packages\allennlp\training\trainer.py in _train_epoch(self, epoch)
    318             self.optimizer.zero_grad()
    319 
--> 320             loss = self.batch_loss(batch_group, for_training=True)
    321 
    322             if torch.isnan(loss):

c:\users\ben\miniconda3\envs\srlviz\lib\site-packages\allennlp\training\trainer.py in batch_loss(self, batch_group, for_training)
    259             batch = batch_group[0]
    260             batch = nn_util.move_to_device(batch, self._cuda_devices[0])
--> 261             output_dict = self.model(**batch)
    262 
    263         try:

c:\users\ben\miniconda3\envs\srlviz\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--> 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

c:\users\ben\miniconda3\envs\srlviz\lib\site-packages\allennlp\models\semantic_role_labeler.py in forward(self, tokens, verb_indicator, tags, metadata)
    136 
    137         """"""
--> 138         embedded_text_input = self.embedding_dropout(self.text_field_embedder(tokens))
    139         mask = get_text_field_mask(tokens)
    140         embedded_verb_indicator = self.binary_feature_embedding(verb_indicator.long())

c:\users\ben\miniconda3\envs\srlviz\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--> 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

c:\users\ben\miniconda3\envs\srlviz\lib\site-packages\allennlp\modules\text_field_embedders\basic_text_field_embedder.py in forward(self, text_field_input, num_wrapping_dims, **kwargs)
    129                 # is bijective and just use the key directly.
    130                 tensors = [text_field_input[key]]
--> 131                 token_vectors = embedder(*tensors, **forward_params_values)
    132             embedded_representations.append(token_vectors)
    133         return torch.cat(embedded_representations, dim=-1)

c:\users\ben\miniconda3\envs\srlviz\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--> 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

c:\users\ben\miniconda3\envs\srlviz\lib\site-packages\allennlp\modules\token_embedders\embedding.py in forward(self, inputs)
    142                              norm_type=self.norm_type,
    143                              scale_grad_by_freq=self.scale_grad_by_freq,
--> 144                              sparse=self.sparse)
    145 
    146         # Now (if necessary) add back in the extra dimensions.

c:\users\ben\miniconda3\envs\srlviz\lib\site-packages\torch\nn\functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1482         # remove once script supports set_grad_enabled
   1483         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-> 1484     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1485 
   1486 

RuntimeError: index out of range: Tried to access index 101 out of table with 1 rows. at C:\w\1\s\windows\pytorch\aten\src\TH/generic/THTensorEvenMoreMath.cpp:418
```

This happens when `<model>` is either `bert_base_uncased`, `bert_base_cased`, or `multi_cased_L-12_H-768_A-12` (which I downloaded from [here](https://github.com/google-research/bert/blob/master/multilingual.md)). If I don't use the `bert_model_name` option, everything works as expected.

Steps to reproduce the behavior:
```python
reader = SrlReader(bert_model_name=""bert-base-uncased"")
instances = reader.read(""conll-2012/data/test/"")
vocab = Vocabulary.from_instances(instances)
word_embeddings = BasicTextFieldEmbedder({'tokens': Embedding(num_embeddings=vocab.get_vocab_size(), embedding_dim=100)})
encoder = PytorchSeq2SeqWrapper(StackedAlternatingLstm(input_size=200, hidden_size=300, num_layers=8, recurrent_dropout_probability=0.1, use_highway=True))
model = SemanticRoleLabeler(vocab, word_embeddings, encoder, 100)
iterator = BucketIterator(sorting_keys=[(""tokens"", ""num_tokens"")])
iterator.index_with(vocab)
trainer = Trainer(model, torch.optim.Adam(model.parameters()), iterator, instances)
trainer.train()
```

Specs:
 - OS: Windows 10 x64
 - Python version: 3.7.3
 - AllenNLP version: v0.9.0
 - PyTorch version: 1.3.1
",,,,,,,,,,1
1488,https://github.com/allenai/allennlp/issues/3608,3608,"[{'id': 887719346, 'node_id': 'MDU6TGFiZWw4ODc3MTkzNDY=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Contributions%20welcome', 'name': 'Contributions welcome', 'color': '02b8d1', 'default': False, 'description': ''}]",closed,2020-01-13 21:46:52+00:00,,3,test_auc_gold_labels_behaviour in auc_test.py fails intermittently,"**Describe the bug**
The `test_auc_gold_labels_behaviour` test in  `allennlp/tests/training/metrics/auc_test.py::AucTest` fails intermittently when the generated labels contains all `3s`.

Error message:

```python
==================================================================================================== FAILURES ====================================================================================================
_____________________________________________________________________________________ AucTest.test_auc_gold_labels_behaviour _____________________________________________________________________________________

self = <allennlp.tests.training.metrics.auc_test.AucTest testMethod=test_auc_gold_labels_behaviour>

   
    def test_auc_gold_labels_behaviour(self):
        auc = Auc(positive_label=4)
        predictions = torch.randn(8).float()
        labels = torch.randint(3, 5, (8,)).long()
>       auc(predictions, labels)

allennlp/tests/training/metrics/auc_test.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <allennlp.training.metrics.auc.Auc object at 0x7f548d86f438>, predictions = tensor([ 0.4905, -2.1212,  1.6025, -1.8595, -1.5871,  1.8005,  0.0391, -0.0910])
gold_labels = tensor([3, 3, 3, 3, 3, 3, 3, 3]), mask = None

    def __call__(
        self,
        predictions: torch.Tensor,
        gold_labels: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
    ):
        """"""
        Parameters
        ----------
        predictions : ``torch.Tensor``, required.
            A one-dimensional tensor of prediction scores of shape (batch_size).
        gold_labels : ``torch.Tensor``, required.
            A one-dimensional label tensor of shape (batch_size), with {1, 0}
            entries for positive and negative class. If it's not binary,
            `positive_label` should be passed in the initialization.
        mask: ``torch.Tensor``, optional (default = None).
            A one-dimensional label tensor of shape (batch_size).
        """"""
    
        predictions, gold_labels, mask = self.unwrap_to_tensors(predictions, gold_labels, mask)
    
        # Sanity checks.
        if gold_labels.dim() != 1:
            raise ConfigurationError(
                ""gold_labels must be one-dimensional, ""
                ""but found tensor of shape: {}"".format(gold_labels.size())
            )
        if predictions.dim() != 1:
            raise ConfigurationError(
                ""predictions must be one-dimensional, ""
                ""but found tensor of shape: {}"".format(predictions.size())
            )
    
        unique_gold_labels = torch.unique(gold_labels)
        if unique_gold_labels.numel() > 2:
            raise ConfigurationError(
                ""AUC can be used for binary tasks only. gold_labels has {} unique labels, ""
                ""expected at maximum 2."".format(unique_gold_labels.numel())
            )
    
        gold_labels_is_binary = set(unique_gold_labels.tolist()) <= {0, 1}
        if not gold_labels_is_binary and self._positive_label not in unique_gold_labels:
            raise ConfigurationError(
                ""gold_labels should be binary with 0 and 1 or initialized positive_label ""
>               ""{} should be present in gold_labels"".format(self._positive_label)
            )
E           allennlp.common.checks.ConfigurationError: 'gold_labels should be binary with 0 and 1 or initialized positive_label 4 should be present in gold_labels'

allennlp/training/metrics/auc.py:68: ConfigurationError
```

**To Reproduce**
This can be reproduced by setting the following seed in the test:
```python
   def test_auc_gold_labels_behaviour(self):        
        torch.manual_seed(1178076155441788456)
        auc = Auc(positive_label=4)
        predictions = torch.randn(8).float()
        labels = torch.randint(3, 5, (8,)).long()
        auc(predictions, labels)
```

**Expected behavior**
The test should always pass

**System (please complete the following information):**
```
OS: ubuntu 18.04
torch==1.3.1
numpy==1.18.0
pytest==5.3.2
python 3.6.9

allennlp: installed from source; commit 95ef61

```

**Additional context**
The test fails at least 2 times out of 30 times I tried, each time using a different seed
",,,,,,,,,,1
1938,https://github.com/allenai/allennlp/issues/4636,4636,[],closed,2020-09-14 16:19:22+00:00,,2,What types of sequence labeling models are possible to train?,"I'm having a bit out trouble finding what types of sequence labeling models, in particular, for NER, are possible to train using AllenNLP?

In other words, what are all the architectures that one can use? For example, BI-LSTM-CRF.",,,,,,,,,,1
104,https://github.com/allenai/allennlp/issues/877,877,[],closed,2018-02-18 00:25:09+00:00,,3,"MacOS: ""AssertionError: Torch not compiled with CUDA enabled""","I do NOT have a GPU on my MacOS. I think by setting `""cuda_device"": 0,` in my config file everything should be cool (i.e. using CPU, instead of trying to use my non-existent GPUs). However, getting this error: 

```

huntsman-ve501-0123:Desktop daniel$ python3.6 -m allennlp.run train ~/Desktop/bidaf.json -s ~/
/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-02-17 19:21:36,531 - INFO - allennlp.common.params - random_seed = 13370
2018-02-17 19:21:36,532 - INFO - allennlp.common.params - numpy_seed = 1337
2018-02-17 19:21:36,532 - INFO - allennlp.common.params - pytorch_seed = 133
2018-02-17 19:21:36,533 - INFO - allennlp.common.checks - Pytorch version: 0.3.0.post4
2018-02-17 19:21:36,535 - INFO - allennlp.common.params - dataset_reader.type = squad
2018-02-17 19:21:36,535 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = word
2018-02-17 19:21:36,535 - INFO - allennlp.common.params - dataset_reader.tokenizer.word_splitter.type = spacy
2018-02-17 19:21:36,535 - INFO - allennlp.common.params - dataset_reader.tokenizer.word_splitter.language = en_core_web_sm
2018-02-17 19:21:36,535 - INFO - allennlp.common.params - dataset_reader.tokenizer.word_splitter.pos_tags = False
2018-02-17 19:21:36,535 - INFO - allennlp.common.params - dataset_reader.tokenizer.word_splitter.parse = False
2018-02-17 19:21:36,535 - INFO - allennlp.common.params - dataset_reader.tokenizer.word_splitter.ner = False
2018-02-17 19:21:37,069 - INFO - allennlp.common.params - dataset_reader.tokenizer.word_filter.type = pass_through
2018-02-17 19:21:37,069 - INFO - allennlp.common.params - dataset_reader.tokenizer.word_stemmer.type = pass_through
2018-02-17 19:21:37,069 - INFO - allennlp.common.params - dataset_reader.tokenizer.start_tokens = None
2018-02-17 19:21:37,069 - INFO - allennlp.common.params - dataset_reader.tokenizer.end_tokens = None
2018-02-17 19:21:37,070 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2018-02-17 19:21:37,070 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2018-02-17 19:21:37,070 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2018-02-17 19:21:37,070 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.type = characters
2018-02-17 19:21:37,070 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.namespace = token_characters
2018-02-17 19:21:37,071 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.character_tokenizer.byte_encoding = utf-8
2018-02-17 19:21:37,071 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.character_tokenizer.lowercase_characters = False
2018-02-17 19:21:37,071 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.character_tokenizer.start_tokens = [259]
2018-02-17 19:21:37,071 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.character_tokenizer.end_tokens = [260]
2018-02-17 19:21:37,071 - INFO - allennlp.common.params - train_data_path = /Users/daniel/Desktop/train_all_RemediaOnly_squadFormat.json
2018-02-17 19:21:37,071 - INFO - allennlp.commands.train - Reading training data from /Users/daniel/Desktop/train_all_RemediaOnly_squadFormat.json
2018-02-17 19:21:37,072 - INFO - allennlp.data.dataset_readers.reading_comprehension.squad - Reading file at /Users/daniel/Desktop/train_all_RemediaOnly_squadFormat.json
2018-02-17 19:21:37,073 - INFO - allennlp.data.dataset_readers.reading_comprehension.squad - Reading the dataset
100%|##########| 55/55 [00:00<00:00, 181.70it/s]
2018-02-17 19:21:37,379 - INFO - allennlp.common.params - validation_data_path = /Users/daniel/Desktop/train_all_RemediaOnly_squadFormat.json
2018-02-17 19:21:37,379 - INFO - allennlp.commands.train - Reading validation data from /Users/daniel/Desktop/train_all_RemediaOnly_squadFormat.json
2018-02-17 19:21:37,379 - INFO - allennlp.data.dataset_readers.reading_comprehension.squad - Reading file at /Users/daniel/Desktop/train_all_RemediaOnly_squadFormat.json
2018-02-17 19:21:37,380 - INFO - allennlp.data.dataset_readers.reading_comprehension.squad - Reading the dataset
100%|##########| 55/55 [00:00<00:00, 222.10it/s]
2018-02-17 19:21:37,629 - INFO - allennlp.common.params - test_data_path = None
2018-02-17 19:21:37,630 - INFO - allennlp.commands.train - Creating a vocabulary using validation, train data.
2018-02-17 19:21:37,631 - INFO - allennlp.common.params - vocabulary.directory_path = None
2018-02-17 19:21:37,631 - INFO - allennlp.common.params - vocabulary.min_count = 1
2018-02-17 19:21:37,631 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None
2018-02-17 19:21:37,631 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')
2018-02-17 19:21:37,631 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False
2018-02-17 19:21:37,631 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
100%|##########| 544/544 [00:01<00:00, 354.94it/s]
2018-02-17 19:21:39,172 - WARNING - root - vocabulary serialization directory /Users/daniel/vocabulary is not empty
2018-02-17 19:21:39,182 - INFO - allennlp.common.params - model.type = bidaf
2018-02-17 19:21:39,183 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2018-02-17 19:21:39,183 - INFO - allennlp.common.params - model.text_field_embedder.tokens.type = embedding
2018-02-17 19:21:39,183 - INFO - allennlp.common.params - model.text_field_embedder.tokens.num_embeddings = None
2018-02-17 19:21:39,183 - INFO - allennlp.common.params - model.text_field_embedder.tokens.vocab_namespace = tokens
2018-02-17 19:21:39,183 - INFO - allennlp.common.params - model.text_field_embedder.tokens.embedding_dim = 100
2018-02-17 19:21:39,184 - INFO - allennlp.common.params - model.text_field_embedder.tokens.pretrained_file = https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz
2018-02-17 19:21:39,184 - INFO - allennlp.common.params - model.text_field_embedder.tokens.projection_dim = None
2018-02-17 19:21:39,184 - INFO - allennlp.common.params - model.text_field_embedder.tokens.trainable = False
2018-02-17 19:21:39,184 - INFO - allennlp.common.params - model.text_field_embedder.tokens.padding_index = None
2018-02-17 19:21:39,184 - INFO - allennlp.common.params - model.text_field_embedder.tokens.max_norm = None
2018-02-17 19:21:39,184 - INFO - allennlp.common.params - model.text_field_embedder.tokens.norm_type = 2.0
2018-02-17 19:21:39,184 - INFO - allennlp.common.params - model.text_field_embedder.tokens.scale_grad_by_freq = False
2018-02-17 19:21:39,184 - INFO - allennlp.common.params - model.text_field_embedder.tokens.sparse = False
2018-02-17 19:21:39,185 - INFO - allennlp.modules.token_embedders.embedding - Reading embeddings from file
2018-02-17 19:21:46,334 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer
2018-02-17 19:21:46,355 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.type = character_encoding
2018-02-17 19:21:46,355 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.embedding.num_embeddings = 262
2018-02-17 19:21:46,355 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.embedding.vocab_namespace = token_characters
2018-02-17 19:21:46,355 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.embedding.embedding_dim = 16
2018-02-17 19:21:46,355 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.embedding.pretrained_file = None
2018-02-17 19:21:46,355 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.embedding.projection_dim = None
2018-02-17 19:21:46,355 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.embedding.trainable = True
2018-02-17 19:21:46,355 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.embedding.padding_index = None
2018-02-17 19:21:46,356 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.embedding.max_norm = None
2018-02-17 19:21:46,356 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.embedding.norm_type = 2.0
2018-02-17 19:21:46,356 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.embedding.scale_grad_by_freq = False
2018-02-17 19:21:46,356 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.embedding.sparse = False
2018-02-17 19:21:46,356 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.encoder.type = cnn
2018-02-17 19:21:46,356 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.encoder.embedding_dim = 16
2018-02-17 19:21:46,357 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.encoder.output_dim = None
2018-02-17 19:21:46,357 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.encoder.num_filters = 100
2018-02-17 19:21:46,357 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.encoder.conv_layer_activation = relu
2018-02-17 19:21:46,357 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.encoder.ngram_filter_sizes = [5]
2018-02-17 19:21:46,359 - INFO - allennlp.common.params - model.text_field_embedder.token_characters.dropout = 0.2
2018-02-17 19:21:46,359 - INFO - allennlp.common.params - model.num_highway_layers = 2
2018-02-17 19:21:46,360 - INFO - allennlp.common.params - model.phrase_layer.type = lstm
2018-02-17 19:21:46,360 - INFO - allennlp.common.params - model.phrase_layer.batch_first = True
2018-02-17 19:21:46,360 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
2018-02-17 19:21:46,360 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: 
2018-02-17 19:21:46,360 - INFO - allennlp.common.params - model.phrase_layer.bidirectional = True
2018-02-17 19:21:46,361 - INFO - allennlp.common.params - model.phrase_layer.input_size = 200
2018-02-17 19:21:46,361 - INFO - allennlp.common.params - model.phrase_layer.hidden_size = 100
2018-02-17 19:21:46,361 - INFO - allennlp.common.params - model.phrase_layer.num_layers = 1
2018-02-17 19:21:46,361 - INFO - allennlp.common.params - model.phrase_layer.dropout = 0.2
2018-02-17 19:21:46,361 - INFO - allennlp.common.params - model.phrase_layer.batch_first = True
2018-02-17 19:21:46,364 - INFO - allennlp.common.params - model.similarity_function.type = linear
2018-02-17 19:21:46,364 - INFO - allennlp.common.params - model.similarity_function.tensor_1_dim = 200
2018-02-17 19:21:46,364 - INFO - allennlp.common.params - model.similarity_function.tensor_2_dim = 200
2018-02-17 19:21:46,364 - INFO - allennlp.common.params - model.similarity_function.combination = x,y,x*y
2018-02-17 19:21:46,365 - INFO - allennlp.common.params - model.similarity_function.activation = linear
2018-02-17 19:21:46,365 - INFO - allennlp.common.params - model.modeling_layer.type = lstm
2018-02-17 19:21:46,365 - INFO - allennlp.common.params - model.modeling_layer.batch_first = True
2018-02-17 19:21:46,365 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
2018-02-17 19:21:46,365 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: 
2018-02-17 19:21:46,366 - INFO - allennlp.common.params - model.modeling_layer.bidirectional = True
2018-02-17 19:21:46,366 - INFO - allennlp.common.params - model.modeling_layer.input_size = 800
2018-02-17 19:21:46,366 - INFO - allennlp.common.params - model.modeling_layer.hidden_size = 100
2018-02-17 19:21:46,366 - INFO - allennlp.common.params - model.modeling_layer.num_layers = 2
2018-02-17 19:21:46,366 - INFO - allennlp.common.params - model.modeling_layer.dropout = 0.2
2018-02-17 19:21:46,366 - INFO - allennlp.common.params - model.modeling_layer.batch_first = True
2018-02-17 19:21:46,376 - INFO - allennlp.common.params - model.span_end_encoder.type = lstm
2018-02-17 19:21:46,377 - INFO - allennlp.common.params - model.span_end_encoder.batch_first = True
2018-02-17 19:21:46,377 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
2018-02-17 19:21:46,377 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: 
2018-02-17 19:21:46,377 - INFO - allennlp.common.params - model.span_end_encoder.bidirectional = True
2018-02-17 19:21:46,377 - INFO - allennlp.common.params - model.span_end_encoder.input_size = 1400
2018-02-17 19:21:46,377 - INFO - allennlp.common.params - model.span_end_encoder.hidden_size = 100
2018-02-17 19:21:46,378 - INFO - allennlp.common.params - model.span_end_encoder.num_layers = 1
2018-02-17 19:21:46,378 - INFO - allennlp.common.params - model.span_end_encoder.dropout = 0.2
2018-02-17 19:21:46,378 - INFO - allennlp.common.params - model.span_end_encoder.batch_first = True
2018-02-17 19:21:46,392 - INFO - allennlp.common.params - model.dropout = 0.2
2018-02-17 19:21:46,392 - INFO - allennlp.common.params - model.initializer = []
2018-02-17 19:21:46,392 - INFO - allennlp.common.params - model.regularizer = []
2018-02-17 19:21:46,392 - INFO - allennlp.common.params - model.mask_lstms = True
2018-02-17 19:21:46,395 - INFO - allennlp.nn.initializers - Initializing parameters
2018-02-17 19:21:46,396 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2018-02-17 19:21:46,396 - INFO - allennlp.nn.initializers -    _highway_layer._module._layers.0.bias
2018-02-17 19:21:46,396 - INFO - allennlp.nn.initializers -    _highway_layer._module._layers.0.weight
2018-02-17 19:21:46,396 - INFO - allennlp.nn.initializers -    _highway_layer._module._layers.1.bias
2018-02-17 19:21:46,397 - INFO - allennlp.nn.initializers -    _highway_layer._module._layers.1.weight
2018-02-17 19:21:46,397 - INFO - allennlp.nn.initializers -    _matrix_attention._similarity_function._bias
2018-02-17 19:21:46,397 - INFO - allennlp.nn.initializers -    _matrix_attention._similarity_function._weight_vector
2018-02-17 19:21:46,397 - INFO - allennlp.nn.initializers -    _modeling_layer._module.bias_hh_l0
2018-02-17 19:21:46,398 - INFO - allennlp.nn.initializers -    _modeling_layer._module.bias_hh_l0_reverse
2018-02-17 19:21:46,398 - INFO - allennlp.nn.initializers -    _modeling_layer._module.bias_hh_l1
2018-02-17 19:21:46,398 - INFO - allennlp.nn.initializers -    _modeling_layer._module.bias_hh_l1_reverse
2018-02-17 19:21:46,399 - INFO - allennlp.nn.initializers -    _modeling_layer._module.bias_ih_l0
2018-02-17 19:21:46,399 - INFO - allennlp.nn.initializers -    _modeling_layer._module.bias_ih_l0_reverse
2018-02-17 19:21:46,399 - INFO - allennlp.nn.initializers -    _modeling_layer._module.bias_ih_l1
2018-02-17 19:21:46,399 - INFO - allennlp.nn.initializers -    _modeling_layer._module.bias_ih_l1_reverse
2018-02-17 19:21:46,399 - INFO - allennlp.nn.initializers -    _modeling_layer._module.weight_hh_l0
2018-02-17 19:21:46,399 - INFO - allennlp.nn.initializers -    _modeling_layer._module.weight_hh_l0_reverse
2018-02-17 19:21:46,400 - INFO - allennlp.nn.initializers -    _modeling_layer._module.weight_hh_l1
2018-02-17 19:21:46,400 - INFO - allennlp.nn.initializers -    _modeling_layer._module.weight_hh_l1_reverse
2018-02-17 19:21:46,400 - INFO - allennlp.nn.initializers -    _modeling_layer._module.weight_ih_l0
2018-02-17 19:21:46,400 - INFO - allennlp.nn.initializers -    _modeling_layer._module.weight_ih_l0_reverse
2018-02-17 19:21:46,400 - INFO - allennlp.nn.initializers -    _modeling_layer._module.weight_ih_l1
2018-02-17 19:21:46,401 - INFO - allennlp.nn.initializers -    _modeling_layer._module.weight_ih_l1_reverse
2018-02-17 19:21:46,401 - INFO - allennlp.nn.initializers -    _phrase_layer._module.bias_hh_l0
2018-02-17 19:21:46,401 - INFO - allennlp.nn.initializers -    _phrase_layer._module.bias_hh_l0_reverse
2018-02-17 19:21:46,401 - INFO - allennlp.nn.initializers -    _phrase_layer._module.bias_ih_l0
2018-02-17 19:21:46,401 - INFO - allennlp.nn.initializers -    _phrase_layer._module.bias_ih_l0_reverse
2018-02-17 19:21:46,402 - INFO - allennlp.nn.initializers -    _phrase_layer._module.weight_hh_l0
2018-02-17 19:21:46,402 - INFO - allennlp.nn.initializers -    _phrase_layer._module.weight_hh_l0_reverse
2018-02-17 19:21:46,402 - INFO - allennlp.nn.initializers -    _phrase_layer._module.weight_ih_l0
2018-02-17 19:21:46,402 - INFO - allennlp.nn.initializers -    _phrase_layer._module.weight_ih_l0_reverse
2018-02-17 19:21:46,402 - INFO - allennlp.nn.initializers -    _span_end_encoder._module.bias_hh_l0
2018-02-17 19:21:46,402 - INFO - allennlp.nn.initializers -    _span_end_encoder._module.bias_hh_l0_reverse
2018-02-17 19:21:46,402 - INFO - allennlp.nn.initializers -    _span_end_encoder._module.bias_ih_l0
2018-02-17 19:21:46,402 - INFO - allennlp.nn.initializers -    _span_end_encoder._module.bias_ih_l0_reverse
2018-02-17 19:21:46,403 - INFO - allennlp.nn.initializers -    _span_end_encoder._module.weight_hh_l0
2018-02-17 19:21:46,403 - INFO - allennlp.nn.initializers -    _span_end_encoder._module.weight_hh_l0_reverse
2018-02-17 19:21:46,403 - INFO - allennlp.nn.initializers -    _span_end_encoder._module.weight_ih_l0
2018-02-17 19:21:46,403 - INFO - allennlp.nn.initializers -    _span_end_encoder._module.weight_ih_l0_reverse
2018-02-17 19:21:46,403 - INFO - allennlp.nn.initializers -    _span_end_predictor._module.bias
2018-02-17 19:21:46,403 - INFO - allennlp.nn.initializers -    _span_end_predictor._module.weight
2018-02-17 19:21:46,403 - INFO - allennlp.nn.initializers -    _span_start_predictor._module.bias
2018-02-17 19:21:46,403 - INFO - allennlp.nn.initializers -    _span_start_predictor._module.weight
2018-02-17 19:21:46,403 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_token_characters._embedding._module.weight
2018-02-17 19:21:46,404 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias
2018-02-17 19:21:46,404 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight
2018-02-17 19:21:46,404 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.weight
2018-02-17 19:21:46,404 - INFO - allennlp.common.params - iterator.type = bucket
2018-02-17 19:21:46,404 - INFO - allennlp.common.params - iterator.sorting_keys = [['passage', 'num_tokens'], ['question', 'num_tokens']]
2018-02-17 19:21:46,404 - INFO - allennlp.common.params - iterator.padding_noise = 0.1
2018-02-17 19:21:46,404 - INFO - allennlp.common.params - iterator.biggest_batch_first = False
2018-02-17 19:21:46,404 - INFO - allennlp.common.params - iterator.batch_size = 40
2018-02-17 19:21:46,404 - INFO - allennlp.data.dataset - Indexing dataset
100%|##########| 272/272 [00:00<00:00, 292.86it/s]
2018-02-17 19:21:47,334 - INFO - allennlp.data.dataset - Indexing dataset
100%|##########| 272/272 [00:00<00:00, 312.04it/s]
2018-02-17 19:21:48,219 - INFO - allennlp.common.params - trainer.patience = 10
2018-02-17 19:21:48,219 - INFO - allennlp.common.params - trainer.validation_metric = +em
2018-02-17 19:21:48,219 - INFO - allennlp.common.params - trainer.num_epochs = 10
2018-02-17 19:21:48,219 - INFO - allennlp.common.params - trainer.cuda_device = 0
2018-02-17 19:21:48,220 - INFO - allennlp.common.params - trainer.grad_norm = 5.0
2018-02-17 19:21:48,220 - INFO - allennlp.common.params - trainer.grad_clipping = None
Traceback (most recent call last):
  File ""/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/site-packages/allennlp/run.py"", line 13, in <module>
    main(prog=""python -m allennlp.run"")
  File ""/usr/local/lib/python3.6/site-packages/allennlp/commands/__init__.py"", line 77, in main
    args.func(args)
  File ""/usr/local/lib/python3.6/site-packages/allennlp/commands/train.py"", line 73, in train_model_from_args
    train_model_from_file(args.param_path, args.serialization_dir)
  File ""/usr/local/lib/python3.6/site-packages/allennlp/commands/train.py"", line 89, in train_model_from_file
    return train_model(params, serialization_dir)
  File ""/usr/local/lib/python3.6/site-packages/allennlp/commands/train.py"", line 174, in train_model
    trainer_params)
  File ""/usr/local/lib/python3.6/site-packages/allennlp/training/trainer.py"", line 517, in from_params
    model = model.cuda(cuda_device)
  File ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 216, in cuda
    return self._apply(lambda t: t.cuda(device))
  File ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 146, in _apply
    module._apply(fn)
  File ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 146, in _apply
    module._apply(fn)
  File ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 152, in _apply
    param.data = fn(param.data)
  File ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 216, in <lambda>
    return self._apply(lambda t: t.cuda(device))
  File ""/usr/local/lib/python3.6/site-packages/torch/_utils.py"", line 61, in _cuda
    with torch.cuda.device(device):
  File ""/usr/local/lib/python3.6/site-packages/torch/cuda/__init__.py"", line 186, in __enter__
    _lazy_init()
  File ""/usr/local/lib/python3.6/site-packages/torch/cuda/__init__.py"", line 120, in _lazy_init
    _check_driver()
  File ""/usr/local/lib/python3.6/site-packages/torch/cuda/__init__.py"", line 55, in _check_driver
    raise AssertionError(""Torch not compiled with CUDA enabled"")
AssertionError: Torch not compiled with CUDA enabled
```

In the log, note the mention of ""trainer.cuda_device = 0"".

",,1,0,0,1,0,0,0,0,0
1675,https://github.com/allenai/allennlp/issues/4071,4071,[],closed,2020-04-15 19:11:36+00:00,,7,Error when loading the coreference SpanBERT model,"**Describe the bug**
Loading the model for coreference resolution seems to not work: the model is not loaded/recognized.

Log:
`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/predictors/predictor.py"", line 256, in from_path
    return Predictor.from_archive(load_archive(archive_path, cuda_device=cuda_device), predictor_name,
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/models/archival.py"", line 230, in load_archive
    cuda_device=cuda_device)
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/models/model.py"", line 327, in load
    return cls.by_name(model_type)._load(config, serialization_dir, weights_file, cuda_device)
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/models/model.py"", line 265, in _load
    model = Model.from_params(vocab=vocab, params=model_params)
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 365, in from_params
    return subclass.from_params(params=params, **extras)
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 386, in from_params
    kwargs = create_kwargs(cls, params, **extras)
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 133, in create_kwargs
    kwargs[name] = construct_arg(cls, name, annotation, param.default, params, **extras)
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 229, in construct_arg
    return annotation.from_params(params=subparams, **subextras)
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 365, in from_params
    return subclass.from_params(params=params, **extras)
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"", line 160, in from_params
    for name, subparams in token_embedder_params.items()
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"", line 160, in <dictcomp>
    for name, subparams in token_embedder_params.items()
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 359, in from_params
    default_to_first_choice=default_to_first_choice)
  File ""/home/CE/skr/anaconda3/envs/env_allennlp2/lib/python3.7/site-packages/allennlp/common/params.py"", line 363, in pop_choice
    raise ConfigurationError(message)
allennlp.common.checks.ConfigurationError: 'pretrained_transformer_mismatched not in acceptable choices for model.text_field_embedder.token_embedders.tokens.type: [\'embedding\', \'character_encoding\', \'elmo_token_embedder\', \'elmo_token_embedder_multilang\', \'openai_transformer_embedder\', \'bert-pretrained\', \'language_model_token_embedder\', \'bidirectional_lm_token_embedder\', \'bag_of_word_counts\', \'pass_through\', \'pretrained_transformer\']. You should either use the --include-package flag to make sure the correct module is loaded, or use a fully qualified class name in your config file like {""model"": ""my_module.models.MyModel""} to have it imported automatically.'
`

**To Reproduce**
The code:
`from allennlp.predictors.predictor import Predictor`
`predictor = Predictor.from_path(""https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz"")`
`predictor.predict(document=""The woman reading a newspaper sat on the bench with her dog."")`
The error is raised after defining the predictor variable. 


**Expected behavior**
Load the model with no errors.

**System (please complete the following information):**
 - OS:  Linux
 - Python version: 3.7.0
 - AllenNLP version: 0.9.0 (installed with pip into a clean conda env)


**Additional context**
I was following the demo [here](https://demo.allennlp.org/coreference-resolution). I also looked at this [example](https://www.kaggle.com/sattree/1-coref-visualization-jupyter-allenlp-stanford) - the part of loading the coreference model, but the same error was raised. Using the `predictor_name` argument found [here](https://github.com/allenai/allennlp-demo/blob/55b05b36bcd1dd234011760e1597e2410b4a6f09/models.json) didn't solve the issue either.
",,,,1,1,,,,1,
588,https://github.com/allenai/allennlp/issues/2006,2006,[],closed,2018-11-02 08:48:50+00:00,,1,Allennp Use unspecified GPU,"**Describe the bug**
I run my code on a serve which has 4 GPUs.
I specify the model to run on  GPU: 1.(Trainer(..., cuda_device=1)
but the thread will also occupy some memory(hundreds of Mbs) of  GPU: 0 .
If the memory of  GPU: 0 was almost full , an out of memory will be occured.
![image](https://user-images.githubusercontent.com/36198692/47901493-11949180-debb-11e8-8eca-ce3a234bbbb6.png)

**Expected behavior**
The thread will only occupy the memory of GPU I specified.

**System:**
 - OS: Linux
 - Python version: 3.6.5
 - AllenNLP version: v0.6.1
 - PyTorch version: 0.4.1
 - IDE: Jupyter Notebook 
",,,1,,,,,,1,
24,https://github.com/allenai/allennlp/issues/581,581,[],closed,2017-12-06 15:14:30+00:00,,11,error on importing Token,"As I try to use jupyter notebook in a Python3.6 virtual environment  these lines issued an error:
*********************************************************************************
from allennlp.data import Token
from allennlp.data.fields import TextField, LabelField
from allennlp.data.token_indexers import SingleIdTokenIndexer
*********************************************************************************
I got this error:

ImportError                               Traceback (most recent call last)
<ipython-input-7-755d3200beb4> in <module>()
----> 1 from allennlp.data import Token
      2 from allennlp.data.fields import TextField, LabelField
      3 from allennlp.data.token_indexers import SingleIdTokenIndexer

ImportError: cannot import name 'Token'
I am running a Linux distro Fedora 25, on an AMD Phenon 6 processor, with no GPU.",,,,,,,,,1,
307,https://github.com/allenai/allennlp/issues/1378,1378,[],closed,2018-06-14 13:43:28+00:00,,12,eliminate `requirements_test.txt` + add tests to setup.py,"yesterday's bad release was due (in large part) to the distinction between `requirements.txt` and `requirements_test`.txt.

briefly, I added a new feature that required a ""new"" dependency, tried to pip install it, saw that it was already there, and so assumed it was already transitively included in our requirements. but it turns out it was actually one of the *test* requirements (which I have installed, but which don't get pip installed). TeamCity didn't catch this either, because it also installs test requirements!

I failed to catch it manually at the ""test pypi test install"" step, so that's bad on me, but our release process shouldn't rely on that sort of manual verification (and anyway it could have been a much more subtle bug).

This is admittedly an edge case, but (I hit it and so) I think it speaks to having *ONE INSTALL PATH*. If our CI system isn't testing the same ""environment"" that's going into our pip package, that's very bad.

(Before we do this we should eliminate the Jupyter dependency from requirements_test.txt)

along similar lines, I think we should distribute our tests as part of the pip package, so that we can TEST TEST our pip install",,,,,,,,,1,
683,https://github.com/allenai/allennlp/issues/2161,2161,[],closed,2018-12-10 05:54:12+00:00,,3,tqdm erroneous behaviour in jupyter notebook,"**Describe the bug**
Running the tutorial (https://allennlp.org/tutorials) on a jupyter notebook results in bad progress bars (image attached)
![image](https://user-images.githubusercontent.com/10602903/49713185-9bdbbe00-fbfc-11e8-83c1-c39972d8fdd7.png)

**To Reproduce**
Copy paste the tutorial (https://allennlp.org/tutorials) to a jupyter notebook cell and run. 
**Expected behavior**
The progress bar should not behave like this.

**System (please complete the following information):**
 - Ubuntu 16.04
 - Python version: 3.6
 - AllenNLP version: v0.7.2
 - PyTorch version: 0.4.1

**Additional context**
The error is likely due to tqdm import https://github.com/allenai/allennlp/blob/master/allennlp/common/tqdm.py#L6

Need to check if it is being run on a jupyter notebook or not, if it is then it should be `from tqdm import tqdm_notebook as _tqdm`.
",,,,,,,,,1,
726,https://github.com/allenai/allennlp/issues/2241,2241,[],closed,2018-12-26 20:29:08+00:00,,4,ConfigurationError: 'Cannot register simple_tagger as Model; name already in use for SimpleTagger',"
 - Ubuntu 16.04
 - Python version: 3.6.
 - allennlp 0.7.2
 - PyTorch version: 0.4.1
- Jupyter 4.4.0

I am getting the error:
ConfigurationError: 'Cannot register simple_tagger as Model; name already in use for SimpleTagger'


I am trying to reproduce code from:
https://github.com/allenai/allennlp/blob/master/allennlp/models/simple_tagger.py

",,,,,,,,,1,
972,https://github.com/allenai/allennlp/issues/2706,2706,[],closed,2019-04-10 03:59:07+00:00,,5,allennlp install failed on google cloud,"`pip install allennlp` fails with:
```
Installing collected packages: jmespath, botocore, s3transfer, boto3, tensorboar
dX, overrides, jsonpickle, aws-xray-sdk, pbr, mock, pyaml, pycryptodome, ecdsa,p
ython-jose, jsondiff, xmltodict, responses, moto, unidecode, parsimonious, pytor
ch-pretrained-bert, flaky, word2number, rsa, colorama, awscli, ftfy, conllu, edi
tdistance, jsonnet, allennlp, thinc
Could not install packages due to an EnvironmentError: [Errno 13] Permission den
ied: '/opt/anaconda3/lib/python3.7/site-packages/jmespath-0.9.4.dist-info'
Consider using the `--user` option or check the permissions.
```

Tried conda install option via `conda install -c allennlp allennlp` and that fails with:
```
WARNING: The conda.compat module is deprecated and will be removed in a future r
elease.
Collecting package metadata: done
Solving environment: /
The environment is inconsistent, please check the package plan carefully
The following packages are causing the inconsistency:

  - defaults/linux-64::spyder-kernels==0.3.0=py37_0
  - defaults/linux-64::anaconda==2018.12=py37_0
  - defaults/linux-64::nb_conda_kernels==2.2.0=py37_1
  - defaults/linux-64::dask==1.0.0=py37_0
  - defaults/linux-64::spyder==3.3.2=py37_0
  - defaults/linux-64::qtconsole==4.4.3=py37_0
  - defaults/linux-64::_ipyw_jlab_nb_ext_conf==0.1.0=py37_0
  - defaults/linux-64::jupyter_console==6.0.0=py37_0
  - defaults/linux-64::jupyterlab==0.35.3=py37_0
  - defaults/linux-64::nb_conda==2.2.1=py37_0
  - defaults/linux-64::jupyterlab_server==0.2.0=py37_0
  - defaults/linux-64::distributed==1.25.1=py37_0
  - defaults/linux-64::notebook==5.7.4=py37_0
  - defaults/linux-64::ipython==7.2.0=py37h39e3cac_0
  - defaults/linux-64::nbpresent==3.0.2=py37_1
  - defaults/linux-64::jupyter==1.0.0=py37_7
  - defaults/linux-64::blaze==0.11.3=py37_0
  - defaults/linux-64::odo==0.5.1=py37_0
failed

PackagesNotFoundError: The following packages are not available from current channels:

  - allennlp -> awscli[version='>=1.11.91']
  - allennlp -> conllu==0.11
  - allennlp -> editdistance
  - allennlp -> flask==0.12.4
  - allennlp -> jsonnet==0.10.0
  - allennlp -> moto==1.3.4
  - allennlp -> overrides
  - allennlp -> parsimonious==0.8.0
  - allennlp -> tensorboardx==1.2
  - allennlp -> pyhocon==0.3.35

Current channels:

  - https://conda.anaconda.org/allennlp/linux-64
  - https://conda.anaconda.org/allennlp/noarch
  - https://repo.anaconda.com/pkgs/main/linux-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/free/linux-64
  - https://repo.anaconda.com/pkgs/free/noarch
  - https://repo.anaconda.com/pkgs/r/linux-64
  - https://repo.anaconda.com/pkgs/r/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.
```",,,,,,,,,1,
1031,https://github.com/allenai/allennlp/issues/2810,2810,[],closed,2019-05-07 14:23:09+00:00,,5,Kernel keeps dying when using pretrained model in jupyter notebook,"I tried to run the code given in the readme, which worked well in the IDE, but kept causing the kernel dies when run in Jupyter Notebook.

<img width=""869"" alt=""Screen Shot 2019-05-08 at 12 21 23 AM"" src=""https://user-images.githubusercontent.com/34934236/57307071-3fd38200-7127-11e9-87f9-15489772e9e3.png"">


A screenshot of the error:
<img width=""698"" alt=""Screen Shot 2019-05-08 at 12 16 08 AM"" src=""https://user-images.githubusercontent.com/34934236/57306673-968c8c00-7126-11e9-8d32-0b2a0764027e.png"">


",,,,,,,,,1,
1718,https://github.com/allenai/allennlp/issues/4192,4192,[],closed,2020-05-03 14:34:25+00:00,,5,Prediction Error when doing Textual Entailment using roberta,"**Describe the bug**

I tried to run textual entailment using the demo code given on allennlp:
`from allennlp.predictors.predictor import Predictor
import allennlp_models.nli
predictor = Predictor.from_path(""https://storage.googleapis.com/allennlp-public-models/snli-roberta-large-2020.02.27.tar.gz"", predictor_name=""textual-entailment"")
predictor.predict(hypothesis=""Two women are sitting on a blanket near some rocks talking about politics."",premise=""Two women are wandering along the shore drinking iced tea."")`

**First, I tried to run it on my local jupyter notebook and got this error:**
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-52-f3796018f3e0> in <module>
      1 predictor.predict(
      2   hypothesis=""Two women are sitting on a blanket near some rocks talking about politics."",
----> 3   premise=""Two women are wandering along the shore drinking iced tea.""
      4 )

TypeError: predict() got an unexpected keyword argument 'hypothesis'
```

**Then I tried to run the same code on the google-colab and got this error:**

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-3-f3796018f3e0> in <module>()
      1 predictor.predict(
      2   hypothesis=""Two women are sitting on a blanket near some rocks talking about politics."",
----> 3   premise=""Two women are wandering along the shore drinking iced tea.""
      4 )

7 frames
/usr/local/lib/python3.6/dist-packages/allennlp/predictors/decomposable_attention.py in predict(self, premise, hypothesis)
     37             [entailment, contradiction, neutral].
     38         """"""
---> 39         return self.predict_json({""premise"": premise, ""hypothesis"": hypothesis})
     40 
     41     @overrides

/usr/local/lib/python3.6/dist-packages/allennlp/predictors/predictor.py in predict_json(self, inputs)
     64     def predict_json(self, inputs: JsonDict) -> JsonDict:
     65         instance = self._json_to_instance(inputs)
---> 66         return self.predict_instance(instance)
     67 
     68     def json_to_labeled_instances(self, inputs: JsonDict) -> List[Instance]:

/usr/local/lib/python3.6/dist-packages/allennlp/predictors/predictor.py in predict_instance(self, instance)
    187 
    188     def predict_instance(self, instance: Instance) -> JsonDict:
--> 189         outputs = self._model.forward_on_instance(instance)
    190         return sanitize(outputs)
    191 

/usr/local/lib/python3.6/dist-packages/allennlp/models/model.py in forward_on_instance(self, instance)
    139         `torch.Tensors` into numpy arrays and remove the batch dimension.
    140         """"""
--> 141         return self.forward_on_instances([instance])[0]
    142 
    143     def forward_on_instances(self, instances: List[Instance]) -> List[Dict[str, numpy.ndarray]]:

/usr/local/lib/python3.6/dist-packages/allennlp/models/model.py in forward_on_instances(self, instances)
    165             dataset.index_instances(self.vocab)
    166             model_input = util.move_to_device(dataset.as_tensor_dict(), cuda_device)
--> 167             outputs = self.make_output_human_readable(self(**model_input))
    168 
    169             instance_separated_output: List[Dict[str, numpy.ndarray]] = [

/usr/local/lib/python3.6/dist-packages/allennlp/models/basic_classifier.py in make_output_human_readable(self, output_dict)
    171                 [
    172                     self.vocab.get_token_from_index(token_id.item(), namespace=self._namespace)
--> 173                     for token_id in instance_tokens
    174                 ]
    175             )

/usr/local/lib/python3.6/dist-packages/allennlp/models/basic_classifier.py in <listcomp>(.0)
    171                 [
    172                     self.vocab.get_token_from_index(token_id.item(), namespace=self._namespace)
--> 173                     for token_id in instance_tokens
    174                 ]
    175             )

/usr/local/lib/python3.6/dist-packages/allennlp/data/vocabulary.py in get_token_from_index(self, index, namespace)
    663 
    664     def get_token_from_index(self, index: int, namespace: str = ""tokens"") -> str:
--> 665         return self._index_to_token[namespace][index]
    666 
    667     def get_vocab_size(self, namespace: str = ""tokens"") -> int:

KeyError: 1596
```

**To Reproduce**
Steps to reproduce the behavior
1. `pip install allennlp==1.0.0rc3 allennlp-models==1.0.0rc3`
2. `from allennlp.predictors.predictor import Predictor`
3. `import allennlp_models.nli`
4. `predictor = Predictor.from_path(""https://storage.googleapis.com/allennlp-public-models/snli-roberta-large-2020.02.27.tar.gz"", predictor_name=""textual-entailment"")` 
5. `predictor.predict(
  hypothesis=""Two women are sitting on a blanket near some rocks talking about politics."",
  premise=""Two women are wandering along the shore drinking iced tea.""
)`
**Expected behavior**
Should return values describing entailment, contradiction and neutrality.

**System (please complete the following information):**
 - OS: [Microsoft, GoogleColab(Unix)]
 - Python version: [3.6.10]
 - AllenNLP version: [ v1.0.0.rc3]
 - PyTorch version: (1.3.1)
",,,,,,,,,1,
1805,https://github.com/allenai/allennlp/issues/4373,4373,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2020-06-18 04:36:36+00:00,,4,Predict is not working for pre-trained fine grained NER model,"## Description

Hi. I tried to load the pre-trained model of fine-grained NER model but Predictor.from_path is not working.

Here's my code 

`!pip3 install allennlp`
`!pip3 install allennlp-models`
`from allennlp.predictors import Predictor`
`al = Predictor.from_path(""https://storage.googleapis.com/allennlp-public-models/fine-grained-ner-model-elmo-2018.12.21.tar.gz"")`

<details>
<summary><b>Python traceback:</b></summary>
<p>

<!-- Paste the traceback from any exception (if there was one) in between the next two lines below -->
```
ConfigurationError                        Traceback (most recent call last)
<ipython-input-16-7116a695b09f> in <module>()
      2 get_ipython().system('pip install --pre allennlp-models')
      3 from allennlp.predictors import Predictor
----> 4 al = Predictor.from_path(""https://storage.googleapis.com/allennlp-public-models/fine-grained-ner-model-elmo-2018.12.21.tar.gz"")
      5 al.predict(sentence=document)

9 frames
/usr/local/lib/python3.6/dist-packages/allennlp/predictors/predictor.py in from_path(cls, archive_path, predictor_name, cuda_device, dataset_reader_to_load, frozen, import_plugins)
    273             plugins.import_plugins()
    274         return Predictor.from_archive(
--> 275             load_archive(archive_path, cuda_device=cuda_device),
    276             predictor_name,
    277             dataset_reader_to_load=dataset_reader_to_load,

/usr/local/lib/python3.6/dist-packages/allennlp/models/archival.py in load_archive(archive_file, cuda_device, opt_level, overrides, weights_file)
    195         serialization_dir=serialization_dir,
    196         cuda_device=cuda_device,
--> 197         opt_level=opt_level,
    198     )
    199 

/usr/local/lib/python3.6/dist-packages/allennlp/models/model.py in load(cls, config, serialization_dir, weights_file, cuda_device, opt_level)
    396             # get_model_class method, that recurses whenever it finds a from_archive model type.
    397             model_class = Model
--> 398         return model_class._load(config, serialization_dir, weights_file, cuda_device, opt_level)
    399 
    400     def extend_embedder_vocab(self, embedding_sources_mapping: Dict[str, str] = None) -> None:

/usr/local/lib/python3.6/dist-packages/allennlp/models/model.py in _load(cls, config, serialization_dir, weights_file, cuda_device, opt_level)
    293         # want the code to look for it, so we remove it from the parameters here.
    294         remove_pretrained_embedding_params(model_params)
--> 295         model = Model.from_params(vocab=vocab, params=model_params)
    296 
    297         # Force model to cpu or gpu, as appropriate, to make sure that the embeddings are

/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py in from_params(cls, params, constructor_to_call, constructor_to_inspect, **extras)
    578                     constructor_to_call=constructor_to_call,
    579                     constructor_to_inspect=constructor_to_inspect,
--> 580                     **extras,
    581                 )
    582             else:

/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py in from_params(cls, params, constructor_to_call, constructor_to_inspect, **extras)
    607             else:
    608                 # This class has a constructor, so create kwargs for it.
--> 609                 kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
    610 
    611             return constructor_to_call(**kwargs)  # type: ignore

/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py in create_kwargs(constructor, cls, params, **extras)
    179 
    180         constructed_arg = pop_and_construct_arg(
--> 181             cls.__name__, param_name, annotation, param.default, params, **extras
    182         )
    183 

/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py in pop_and_construct_arg(class_name, argument_name, annotation, default, params, **extras)
    285         return None
    286 
--> 287     return construct_arg(class_name, name, popped_params, annotation, default, **extras)
    288 
    289 

/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py in construct_arg(class_name, argument_name, popped_params, annotation, default, **extras)
    319             elif isinstance(popped_params, dict):
    320                 popped_params = Params(popped_params)
--> 321             return annotation.from_params(params=popped_params, **subextras)
    322         elif not optional:
    323             # Not optional and not supplied, that's an error!

/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py in from_params(cls, params, constructor_to_call, constructor_to_inspect, **extras)
    531         if not isinstance(params, Params):
    532             raise ConfigurationError(
--> 533                 ""from_params was passed a `params` object that was not a `Params`. This probably ""
    534                 ""indicates malformed parameters in a configuration file, where something that ""
    535                 ""should have been a dictionary was actually a list, or something else. ""

ConfigurationError: from_params was passed a `params` object that was not a `Params`. This probably indicates malformed parameters in a configuration file, where something that should have been a dictionary was actually a list, or something else. This happened when constructing an object of type <class 'allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator'>.
```

</p>
</details>

## Environment

<!-- Provide the name of operating system below (e.g. OS X, Linux) -->
OS: Linux

<!-- Provide the Python version you were using (e.g. 3.7.1) -->
Python version: 3.7.6

<details>
<summary><b>Output of <code>pip freeze</code>:</b></summary>
<p>

<!-- Paste the output of `pip freeze` in between the next two lines below -->
```
alabaster==0.7.12
anaconda-client==1.7.2
anaconda-navigator==1.9.12
anaconda-project==0.8.3
argh==0.26.2
asn1crypto==1.3.0
astroid==2.3.3
astropy==4.0
atomicwrites==1.3.0
attrs==19.3.0
autopep8==1.4.4
Babel==2.8.0
backcall==0.1.0
backports.functools-lru-cache==1.6.1
backports.shutil-get-terminal-size==1.0.0
backports.tempfile==1.0
backports.weakref==1.0.post1
bcrypt==3.1.7
beautifulsoup4==4.8.2
bitarray==1.2.1
bkcharts==0.2
bleach==3.1.0
blis==0.4.1
bokeh==1.4.0
boto==2.49.0
Bottleneck==1.3.2
catalogue==1.0.0
certifi==2019.11.28
cffi==1.14.0
chardet==3.0.4
Click==7.0
cloudpickle==1.3.0
clyent==1.2.2
colorama==0.4.3
comtypes==1.1.7
conda==4.8.3
conda-build==3.18.11
conda-package-handling==1.6.0
conda-verify==3.4.2
contextlib2==0.6.0.post1
cryptography==2.8
cycler==0.10.0
cymem==2.0.3
Cython==0.29.15
cytoolz==0.10.1
dask==2.11.0
decorator==4.4.1
defusedxml==0.6.0
diff-match-patch==20181111
distributed==2.11.0
docutils==0.16
en-core-web-lg==2.3.0
en-core-web-md==2.3.0
en-core-web-sm==2.3.0
entrypoints==0.3
et-xmlfile==1.0.1
fastcache==1.1.0
filelock==3.0.12
flake8==3.7.9
Flask==1.1.1
fsspec==0.6.2
future==0.18.2
gevent==1.4.0
glob2==0.7
greenlet==0.4.15
h5py==2.10.0
HeapDict==1.0.1
html5lib==1.0.1
hypothesis==5.5.4
idna==2.8
imageio==2.6.1
imagesize==1.2.0
importlib-metadata==1.6.1
intervaltree==3.0.2
ipykernel==5.1.4
ipython==7.12.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
isort==4.3.21
itsdangerous==1.1.0
jdcal==1.4.1
jedi==0.14.1
Jinja2==2.11.1
joblib==0.14.1
json5==0.9.1
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==5.3.4
jupyter-console==6.1.0
jupyter-core==4.6.1
jupyterlab==1.2.6
jupyterlab-server==1.0.6
keyring==21.1.0
kiwisolver==1.1.0
lazy-object-proxy==1.4.3
libarchive-c==2.8
llvmlite==0.31.0
locket==0.2.0
lxml==4.5.0
MarkupSafe==1.1.1
matplotlib==3.1.3
mccabe==0.6.1
menuinst==1.4.16
mistune==0.8.4
mkl-fft==1.0.15
mkl-random==1.1.0
mkl-service==2.3.0
mock==4.0.1
more-itertools==8.2.0
mpmath==1.1.0
msgpack==0.6.1
multipledispatch==0.6.0
murmurhash==1.0.0
navigator-updater==0.2.1
nbconvert==5.6.1
nbformat==5.0.4
networkx==2.4
nltk==3.4.5
nose==1.3.7
notebook==6.0.3
numba==0.48.0
numexpr==2.7.1
numpy==1.18.1
numpydoc==0.9.2
olefile==0.46
openpyxl==3.0.3
packaging==20.1
pandas==1.0.1
pandocfilters==1.4.2
paramiko==2.7.1
parso==0.5.2
partd==1.1.0
path==13.1.0
pathlib2==2.3.5
pathtools==0.1.2
patsy==0.5.1
pep8==1.7.1
pexpect==4.8.0
pickleshare==0.7.5
Pillow==7.0.0
pkginfo==1.5.0.1
plac==0.9.6
pluggy==0.13.1
ply==3.11
preshed==3.0.2
prometheus-client==0.7.1
prompt-toolkit==3.0.3
psutil==5.6.7
py==1.8.1
pycodestyle==2.5.0
pycosat==0.6.3
pycparser==2.19
pycrypto==2.6.1
pycurl==7.43.0.5
pydocstyle==4.0.1
pyflakes==2.1.1
Pygments==2.5.2
pylint==2.4.4
PyNaCl==1.3.0
pyodbc===4.0.0-unsupported
pyOpenSSL==19.1.0
pyparsing==2.4.6
pyreadline==2.1
pyrsistent==0.15.7
PySocks==1.7.1
pytest==5.3.5
pytest-arraydiff==0.3
pytest-astropy==0.8.0
pytest-astropy-header==0.1.2
pytest-doctestplus==0.5.0
pytest-openfiles==0.4.0
pytest-remotedata==0.3.2
python-dateutil==2.8.1
python-jsonrpc-server==0.3.4
python-language-server==0.31.7
pytz==2019.3
PyWavelets==1.1.1
pywin32==227
pywin32-ctypes==0.2.0
pywinpty==0.5.7
PyYAML==5.3
pyzmq==18.1.1
QDarkStyle==2.8
QtAwesome==0.6.1
qtconsole==4.6.0
QtPy==1.9.0
requests==2.22.0
rope==0.16.0
Rtree==0.9.3
ruamel-yaml==0.15.87
scikit-image==0.16.2
scikit-learn==0.22.1
scipy==1.4.1
seaborn==0.10.0
Send2Trash==1.5.0
sentencepiece==0.1.91
simplegeneric==0.8.1
singledispatch==3.4.0.3
six==1.14.0
snowballstemmer==2.0.0
sortedcollections==1.1.2
sortedcontainers==2.1.0
soupsieve==1.9.5
spacy==2.3.0
Sphinx==2.4.0
sphinxcontrib-applehelp==1.0.1
sphinxcontrib-devhelp==1.0.1
sphinxcontrib-htmlhelp==1.0.2
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==1.0.2
sphinxcontrib-serializinghtml==1.1.3
sphinxcontrib-websupport==1.2.0
spyder==4.0.1
spyder-kernels==1.8.1
SQLAlchemy==1.3.13
srsly==1.0.2
statsmodels==0.11.0
sympy==1.5.1
tables==3.6.1
tblib==1.6.0
terminado==0.8.3
testpath==0.4.4
thinc==7.4.1
toolz==0.10.0
torch==1.5.0
torchtext==0.6.0
torchvision==0.6.0
tornado==6.0.3
tqdm==4.42.1
traitlets==4.3.3
ujson==1.35
unicodecsv==0.14.1
urllib3==1.25.8
wasabi==0.6.0
watchdog==0.10.2
wcwidth==0.1.8
webencodings==0.5.1
Werkzeug==1.0.0
widgetsnbextension==3.5.1
win-inet-pton==1.1.0
win-unicode-console==0.5
wincertstore==0.2
wrapt==1.11.2
xlrd==1.2.0
XlsxWriter==1.2.7
xlwings==0.17.1
xlwt==1.3.0
xmltodict==0.12.0
yapf==0.28.0
zict==1.0.0
zipp==2.2.0
```

</p>
</details>


",,,,,,,,,1,
1823,https://github.com/allenai/allennlp/issues/4418,4418,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}, {'id': 887719346, 'node_id': 'MDU6TGFiZWw4ODc3MTkzNDY=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Contributions%20welcome', 'name': 'Contributions welcome', 'color': '02b8d1', 'default': False, 'description': ''}]",closed,2020-06-30 06:07:48+00:00,,4,AugmentedLSTM not working,"<!--
Please fill this template entirely and do not erase any of it.
We reserve the right to close without a response bug reports which are incomplete.

If you can't fill in the checklist then it's likely that this is a question, not a bug,
in which case it probably belongs on our discource forum instead:

https://discourse.allennlp.org/
-->

## Checklist

<!-- To check an item on the list replace [ ] with [x]. -->

- [x] I have verified that the issue exists against the `master` branch of AllenNLP.
- [x] I have read the relevant section in the [contribution guide](https://github.com/allenai/allennlp/blob/master/CONTRIBUTING.md#bug-fixes-and-new-features) on reporting bugs.
- [x] I have checked the [issues list](https://github.com/allenai/allennlp/issues) for similar or identical bug reports.
- [x] I have checked the [pull requests list](https://github.com/allenai/allennlp/pulls) for existing proposed fixes.
- [x] I have checked the [CHANGELOG](https://github.com/allenai/allennlp/blob/master/CHANGELOG.md) and the [commit log](https://github.com/allenai/allennlp/commits/master) to find out if the bug was already fixed in the master branch.
- [x] I have included in the ""Description"" section below a traceback from any exceptions related to this bug.
- [x] I have included in the ""Related issues or possible duplicates"" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
- [x] I have included in the ""Environment"" section below the name of the operating system and Python version that I was using when I discovered this bug.
- [x] I have included in the ""Environment"" section below the output of `pip freeze`.
- [x] I have included in the ""Steps to reproduce"" section below a minimally reproducible example.


## Description
The AugmentedLSTM class from the pytorch_seq2vec_wrapper module doesn't work, and gives the error as below. This is probably due to the hidden_size argument being assigned to a different variable in the class, namely lstm_dim, in the allennlp/modules/augmented_lstm.py file. 
<!-- Please provide a clear and concise description of what the bug is here. -->

<details>
<summary><b>Python traceback:</b></summary>
<p>

<!-- Paste the traceback from any exception (if there was one) in between the next two lines below -->
```
Traceback (most recent call last):
  File ""/home/harshn/anaconda3/bin/allennlp"", line 8, in <module>
    sys.exit(run())
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/__main__.py"", line 19, in run
    main(prog=""allennlp"")
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/commands/__init__.py"", line 92, in main
    args.func(args)
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/commands/train.py"", line 112, in train_model_from_args
    dry_run=args.dry_run,
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/commands/train.py"", line 171, in train_model_from_file
    dry_run=dry_run,
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/commands/train.py"", line 230, in train_model
    dry_run=dry_run,
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/commands/train.py"", line 418, in _train_worker
    params=params, serialization_dir=serialization_dir, local_rank=process_rank,
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 580, in from_params
    **extras,
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 611, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/commands/train.py"", line 627, in from_partial_objects
    model_ = model.construct(vocab=vocabulary_)
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/common/lazy.py"", line 46, in construct
    return self._constructor(**kwargs)
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 446, in constructor
    return value_cls.from_params(params=deepcopy(popped_params), **constructor_extras)
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 580, in from_params
    **extras,
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 611, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File ""/home/harshn/thesis/allennlp_tests/models/base_classifier.py"", line 75, in __init__
    self._classifier_input_dim = self._seq2vec_encoder.get_output_dim()
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/allennlp/modules/seq2vec_encoders/pytorch_seq2vec_wrapper.py"", line 60, in get_output_dim
    return self._module.hidden_size * (2 if is_bidirectional else 1)
  File ""/home/harshn/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 594, in __getattr__
    type(self).__name__, name))
AttributeError: 'AugmentedLstm' object has no attribute 'hidden_size'
```

</p>
</details>


## Related issues or possible duplicates

- None


## Environment

<!-- Provide the name of operating system below (e.g. OS X, Linux) -->
OS:
Ubuntu 18.04.4 on WSL
<!-- Provide the Python version you were using (e.g. 3.7.1) -->
Python version:
3.7.6
<details>
<summary><b>Output of <code>pip freeze</code>:</b></summary>
<p>

<!-- Paste the output of `pip freeze` in between the next two lines below -->
```
alabaster==0.7.12
allennlp==1.0.0rc5
allennlp-models==1.0.0rc5
anaconda-client==1.7.2
anaconda-navigator==1.9.12
anaconda-project==0.8.3
argh==0.26.2
asn1crypto==1.3.0
astroid==2.3.3
astropy==4.0
atomicwrites==1.3.0
attrs==19.3.0
autopep8==1.4.4
Babel==2.8.0
backcall==0.1.0
backports.functools-lru-cache==1.6.1
backports.shutil-get-terminal-size==1.0.0
backports.tempfile==1.0
backports.weakref==1.0.post1
beautifulsoup4==4.8.2
bitarray==1.2.1
bkcharts==0.2
bleach==3.1.0
blis==0.2.4
bokeh==1.4.0
boto==2.49.0
boto3==1.13.11
botocore==1.16.11
Bottleneck==1.3.2
certifi==2019.11.28
cffi==1.14.0
chardet==3.0.4
Click==7.0
cloudpickle==1.3.0
clyent==1.2.2
colorama==0.4.3
conda==4.8.2
conda-build==3.18.11
conda-package-handling==1.6.0
conda-verify==3.4.2
conllu==2.3.2
contextlib2==0.6.0.post1
cryptography==2.8
cycler==0.10.0
cymem==2.0.3
Cython==0.29.15
cytoolz==0.10.1
dask==2.11.0
decorator==4.4.1
defusedxml==0.6.0
diff-match-patch==20181111
distributed==2.11.0
docutils==0.15.2
dyNET==2.1
editdistance==0.5.3
en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz
entrypoints==0.3
et-xmlfile==1.0.1
fastcache==1.1.0
filelock==3.0.12
flake8==3.7.9
flaky==3.6.1
Flask==1.1.1
Flask-Cors==3.0.8
fsspec==0.6.2
ftfy==5.7
future==0.18.2
gensim==3.8.3
gevent==1.4.0
glob2==0.7
gmpy2==2.0.8
greenlet==0.4.15
h5py==2.10.0
HeapDict==1.0.1
html5lib==1.0.1
hypothesis==5.5.4
idna==2.8
imageio==2.6.1
imagesize==1.2.0
importlib-metadata==1.5.0
intervaltree==3.0.2
ipykernel==5.1.4
ipython==7.12.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
isort==4.3.21
itsdangerous==1.1.0
jdcal==1.4.1
jedi==0.14.1
jeepney==0.4.2
Jinja2==2.11.1
jmespath==0.10.0
joblib==0.14.1
json5==0.9.1
jsonnet==0.16.0
jsonpickle==1.4.1
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==5.3.4
jupyter-console==6.1.0
jupyter-core==4.6.1
jupyterlab==1.2.6
jupyterlab-server==1.0.6
kenlm @ https://github.com/kpu/kenlm/archive/master.zip
keyring==21.1.0
kiwisolver==1.1.0
lazy-object-proxy==1.4.3
libarchive-c==2.8
lief==0.9.0
llvmlite==0.31.0
locket==0.2.0
lxml==4.5.0
MarkupSafe==1.1.1
matplotlib==3.1.3
mccabe==0.6.1
mistune==0.8.4
mkl-fft==1.0.15
mkl-random==1.1.0
mkl-service==2.3.0
mock==4.0.1
more-itertools==8.2.0
mpmath==1.1.0
msgpack==0.6.1
multipledispatch==0.6.0
murmurhash==1.0.2
navigator-updater==0.2.1
nbconvert==5.6.1
nbformat==5.0.4
networkx==2.4
nltk==3.4.5
nose==1.3.7
notebook==6.0.3
numba==0.48.0
numexpr==2.7.1
numpy==1.18.4
numpydoc==0.9.2
oauthlib==3.1.0
olefile==0.46
openpyxl==3.0.3
overrides==3.0.0
packaging==20.1
pandas==1.0.1
pandocfilters==1.4.2
parsimonious==0.8.1
parso==0.5.2
partd==1.1.0
path==13.1.0
pathlib2==2.3.5
pathtools==0.1.2
patsy==0.5.1
pbr==2.1.0
pep8==1.7.1
pexpect==4.8.0
pickleshare==0.7.5
Pillow==7.0.0
pkginfo==1.5.0.1
plac==0.9.6
pluggy==0.13.1
ply==3.11
preshed==2.0.1
prometheus-client==0.7.1
prompt-toolkit==3.0.3
protobuf==3.12.1
psutil==5.6.7
ptyprocess==0.6.0
py==1.8.1
py-rouge==1.1
pycodestyle==2.5.0
pycosat==0.6.3
pycparser==2.19
pycrypto==2.6.1
pycurl==7.43.0.5
pydocstyle==4.0.1
pyenchant==2.0.0
pyflakes==2.1.1
Pygments==2.5.2
pylint==2.4.4
pyodbc===4.0.0-unsupported
pyOpenSSL==19.1.0
pyparsing==2.4.6
pyrsistent==0.15.7
PySocks==1.7.1
pytest==5.3.5
pytest-arraydiff==0.3
pytest-astropy==0.8.0
pytest-astropy-header==0.1.2
pytest-doctestplus==0.5.0
pytest-openfiles==0.4.0
pytest-remotedata==0.3.2
python-dateutil==2.8.1
python-jsonrpc-server==0.3.4
python-language-server==0.31.7
pytorch-pretrained-bert==0.6.2
pytorch-transformers==1.1.0
pytz==2019.3
PyWavelets==1.1.1
pyxdg==0.26
PyYAML==5.3
pyzmq==18.1.1
QDarkStyle==2.8
QtAwesome==0.6.1
qtconsole==4.6.0
QtPy==1.9.0
regex==2020.5.14
requests==2.22.0
requests-oauthlib==1.3.0
responses==0.10.14
rope==0.16.0
Rtree==0.9.3
ruamel-yaml==0.15.87
s3transfer==0.3.3
sacremoses==0.0.43
scikit-image==0.16.2
scikit-learn==0.22.1
scipy==1.4.1
seaborn==0.10.0
SecretStorage==3.1.2
semantic-version==2.8.5
Send2Trash==1.5.0
sentencepiece==0.1.90
simplegeneric==0.8.1
singledispatch==3.4.0.3
six==1.14.0
smart-open==2.0.0
snowballstemmer==2.0.0
sortedcollections==1.1.2
sortedcontainers==2.1.0
soupsieve==1.9.5
spacy==2.1.9
Sphinx==2.4.0
sphinxcontrib-applehelp==1.0.1
sphinxcontrib-devhelp==1.0.1
sphinxcontrib-htmlhelp==1.0.2
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==1.0.2
sphinxcontrib-serializinghtml==1.1.3
sphinxcontrib-websupport==1.2.0
spyder==4.0.1
spyder-kernels==1.8.1
SQLAlchemy==1.3.13
sqlparse==0.3.1
srsly==1.0.2
statsmodels==0.11.0
sympy==1.5.1
tables==3.6.1
tblib==1.6.0
tensorboardX==2.0
terminado==0.8.3
testpath==0.4.4
thinc==7.0.8
tokenizers==0.7.0
toolz==0.10.0
torch==1.5.0
torchtext==0.6.0
tornado==6.0.3
tqdm==4.42.1
traitlets==4.3.3
transformers==2.9.1
tweepy==3.8.0
ujson==1.35
unicodecsv==0.14.1
Unidecode==1.1.1
urllib3==1.25.8
wasabi==0.6.0
watchdog==0.10.2
wcwidth==0.1.8
webencodings==0.5.1
Werkzeug==1.0.0
widgetsnbextension==3.5.1
word2number==1.1
wrapt==1.11.2
wurlitzer==2.0.0
wxconv @ https://www.github.com/irshadbhat/indic-wx-converter/archive/master.zip
xlrd==1.2.0
XlsxWriter==1.2.7
xlwt==1.3.0
xmltodict==0.12.0
yapf==0.28.0
zict==1.0.0
zipp==2.2.0
```

</p>
</details>


## Steps to reproduce
Run the following code, with a temp_training.txt and temp_validation.txt file, with random examples of the format:
The fox jumped over the fence 1
The dog is cute 0

Note that the code is quite similar to, and inspired from, the tutorial present on the allennlp.org website.
<details>
<summary><b>Example source:</b></summary>
<p>

<!-- Add a fully runnable example in between the next two lines below that will reproduce the bug -->
```
from typing import Iterator, List, Dict

import torch
import torch.optim as optim
import numpy as np

from allennlp.data import Instance
from allennlp.data.fields import TextField, LabelField

from allennlp.data.dataset_readers import DatasetReader

from allennlp.common.file_utils import cached_path

from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer
from allennlp.data.tokenizers import Token

from allennlp.data.vocabulary import Vocabulary

from allennlp.models import Model

from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder
from allennlp.modules.token_embedders import Embedding
from allennlp.modules.seq2vec_encoders import Seq2VecEncoder
from allennlp.modules.seq2vec_encoders.pytorch_seq2vec_wrapper import AugmentedLstmSeq2VecEncoder
from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits
from allennlp.nn import InitializerApplicator, util

from allennlp.training.metrics import CategoricalAccuracy

# from allennlp.data.iterators import BucketIterator
from allennlp.data.dataloader import DataLoader

from allennlp.training.trainer import Trainer

from allennlp.predictors import SentenceTaggerPredictor

torch.manual_seed(1)

class PosDatasetReader(DatasetReader):
    """"""
    DatasetReader for PoS tagging data, one sentence per line, like

        The###DET dog###NN ate###V the###DET apple###NN
    """"""

    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:
        super().__init__(lazy=False)
        self.token_indexers = token_indexers or {""tokens"": SingleIdTokenIndexer()}

    def text_to_instance(self, tokens: List[Token], tag: int = None) -> Instance:
        sentence_field = TextField(tokens, self.token_indexers)
        fields = {""sentence"": sentence_field}

        if tag:
            label_field = LabelField(label=tag)
            fields[""label""] = label_field

        return Instance(fields)

    def _read(self, file_path: str) -> Iterator[Instance]:
        with open(file_path) as f:
            for line in f:
                pairs = line.strip().split()
                sentence = pairs[:-1]
                tag = pairs[-1]
                yield self.text_to_instance([Token(word) for word in sentence], tag)

class LstmTagger(Model):

    def __init__(self,
                 word_embeddings: TextFieldEmbedder,
                 encoder: Seq2VecEncoder,
                 vocab: Vocabulary) -> None:

        super().__init__(vocab)
        self.word_embeddings = word_embeddings
        self.encoder = encoder

        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),
                                          out_features=vocab.get_vocab_size('labels'))
        self._classification_layer = torch.nn.Linear(self._classifier_input_dim, self._num_labels)
        self._accuracy = CategoricalAccuracy()
        self._loss = torch.nn.CrossEntropyLoss()
        initializer(self)
        self.accuracy = CategoricalAccuracy()

    def forward(self,
                sentence: Dict[str, torch.Tensor],
                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:

        mask = get_text_field_mask(sentence)

        embeddings = self.word_embeddings(sentence)

        encoder_out = self.encoder(embeddings, mask)

        tag_logits = self._classification_layer(encoder_out)
        
        probs = torch.nn.functional.softmax(logits, dim=-1)
        output_dict = {""logits"": logits, ""probs"": probs}
        output_dict[""token_ids""] = util.get_token_ids_from_text_field_tensors(sentence)
        if label is not None:
            loss = self._loss(logits, label.long().view(-1))
            output_dict[""loss""] = loss
            self._accuracy(logits, label)

        return output_dict

    def get_metrics(self, reset: bool = False) -> Dict[str, float]:
        return {""accuracy"": self.accuracy.get_metric(reset)}

reader = PosDatasetReader()

train_dataset = reader.read(cached_path(
    'temp_training.txt'))
validation_dataset = reader.read(cached_path(
    'temp_validation.txt'))

vocab = Vocabulary.from_instances(train_dataset + validation_dataset)

EMBEDDING_DIM = 6
HIDDEN_DIM = 6

token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),
                            embedding_dim=EMBEDDING_DIM)
word_embeddings = BasicTextFieldEmbedder({""tokens"": token_embedding})

lstm = AugmentedLstmSeq2VecEncoder(EMBEDDING_DIM, HIDDEN_DIM)

model = LstmTagger(word_embeddings, lstm, vocab)

if torch.cuda.is_available():
    cuda_device = 0

    model = model.cuda(cuda_device)
else:

    cuda_device = -1

optimizer = optim.SGD(model.parameters(), lr=0.1)

# iterator = BucketIterator(batch_size=2, sorting_keys=[(""sentence"", ""num_tokens"")])
data_loader = DataLoader(train_dataset, batch_size=2)
val_data_loader = DataLoader(validation_dataset, batch_size=2)

iterator.index_with(vocab)

# trainer = Trainer(model=model,
#                   optimizer=optimizer,
#                   iterator=iterator,
#                   train_dataset=train_dataset,
#                   validation_dataset=validation_dataset,
#                   patience=10,
#                   num_epochs=1000,
#                   cuda_device=cuda_device)

trainer = Trainer(model=model,
                  optimizer=optimizer,
                  data_loader=data_loader,
                  validation_dataloader=val_data_loader,
                  patience=10,
                  num_epochs=1000,
                  cuda_device=cuda_device)

trainer.train()

predictor = SentenceTaggerPredictor(model, dataset_reader=reader)

tag_logits = predictor.predict(""The dog ate the apple"")['tag_logits']

tag_ids = np.argmax(tag_logits, axis=-1)

print([model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])

# Here's how to save the model.
with open(""/tmp/model.th"", 'wb') as f:
    torch.save(model.state_dict(), f)

vocab.save_to_files(""/tmp/vocabulary"")

# And here's how to reload the model.
vocab2 = Vocabulary.from_files(""/tmp/vocabulary"")

model2 = LstmTagger(word_embeddings, lstm, vocab2)

with open(""/tmp/model.th"", 'rb') as f:
    model2.load_state_dict(torch.load(f))

if cuda_device > -1:
    model2.cuda(cuda_device)

predictor2 = SentenceTaggerPredictor(model2, dataset_reader=reader)
tag_logits2 = predictor2.predict(""The dog ate the apple"")['tag_logits']
np.testing.assert_array_almost_equal(tag_logits2, tag_logits)
```

</p>
</details>
",,,,,,,,,1,
1856,https://github.com/allenai/allennlp/issues/4474,4474,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2020-07-14 15:33:59+00:00,,7,[Models] Wrong usage of *cls_pooler* in the SST Roberta model,"<!--
Please fill this template entirely and do not erase any of it.
We reserve the right to close without a response bug reports which are incomplete.

If you can't fill in the checklist then it's likely that this is a question, not a bug,
in which case it probably belongs on our discource forum instead:

https://discourse.allennlp.org/
-->

## Checklist

<!-- To check an item on the list replace [ ] with [x]. -->

- [x] I have verified that the issue exists against the `master` branch of AllenNLP.
- [x] I have read the relevant section in the [contribution guide](https://github.com/allenai/allennlp/blob/master/CONTRIBUTING.md#bug-fixes-and-new-features) on reporting bugs.
- [x] I have checked the [issues list](https://github.com/allenai/allennlp/issues) for similar or identical bug reports.
- [x] I have checked the [pull requests list](https://github.com/allenai/allennlp/pulls) for existing proposed fixes.
- [x] I have checked the [CHANGELOG](https://github.com/allenai/allennlp/blob/master/CHANGELOG.md) and the [commit log](https://github.com/allenai/allennlp/commits/master) to find out if the bug was already fixed in the master branch.
- [ ] I have included in the ""Description"" section below a traceback from any exceptions related to this bug.
- [ ] I have included in the ""Related issues or possible duplicates"" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
- [x] I have included in the ""Environment"" section below the name of the operating system and Python version that I was using when I discovered this bug.
- [x] I have included in the ""Environment"" section below the output of `pip freeze`.
- [x] I have included in the ""Steps to reproduce"" section below a minimally reproducible example.


## Description

<!-- Please provide a clear and concise description of what the bug is here. -->

I think the usage of the *cls_pooler* as `seq2vec_encoder` is not appropriate [in this model](https://github.com/allenai/allennlp-models/blob/888596c8d41fcde755e91ca00474b88009175700/training_config/classification/stanford_sentiment_treebank_roberta.jsonnet#L44). If i am not mistaken the `PretrainedTransformerMismatchedIndexer/Embedder` get rid of the special tokens via the `offsets`, so the *cls_pooler* just takes the embedding of the first ""real text"" token.

<details>
<summary><b>Python traceback:</b></summary>
<p>

<!-- Paste the traceback from any exception (if there was one) in between the next two lines below -->
```
```

</p>
</details>


## Related issues or possible duplicates

- None


## Environment

<!-- Provide the name of operating system below (e.g. OS X, Linux) -->
OS: Ubuntu 20.04

<!-- Provide the Python version you were using (e.g. 3.7.1) -->
Python version: 3.7.7

<details>
<summary><b>Output of <code>pip freeze</code>:</b></summary>
<p>

<!-- Paste the output of `pip freeze` in between the next two lines below -->
```
absl-py==0.9.0
aiohttp==3.6.2
alembic==1.4.2
allennlp==1.0.0
appdirs==1.4.4
astroid==2.4.2
async-timeout==3.0.1
attrs==19.3.0
azure-core==1.7.0
azure-storage-blob==12.3.2
backcall==0.2.0
beautifulsoup4==4.9.1
-e git+git@github.com:recognai/biome-text.git@7a22136a713f634587702e096f778ea44aa94123#egg=biome_text
black==19.10b0
bleach==3.1.5
blis==0.4.1
bokeh==2.0.2
boto3==1.14.7
botocore==1.17.7
cachetools==4.1.1
cachey==0.2.1
captum==0.2.0
catalogue==1.0.0
certifi==2020.6.20
cffi==1.14.0
chardet==3.0.4
click==7.1.2
cloudpickle==1.4.1
colorama==0.4.3
coverage==5.1
cryptography==2.9.2
cycler==0.10.0
cymem==2.0.3
dask==2.17.2
dask-elk==0.4.0
databricks-cli==0.11.0
decorator==4.4.2
defusedxml==0.6.0
distributed==2.19.0
docker==4.2.2
docutils==0.15.2
elasticsearch==7.8.0
en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz
entrypoints==0.3
fastapi==0.55.1
filelock==3.0.12
Flask==1.1.2
Flask-Cors==3.0.8
flatdict==4.0.1
fsspec==0.7.4
future==0.18.2
gevent==1.4.0
gitdb==4.0.5
GitPython==3.1.3
google==2.0.3
google-auth==1.18.0
google-auth-oauthlib==0.4.1
gorilla==0.3.0
greenlet==0.4.16
grpcio==1.30.0
gunicorn==20.0.4
h11==0.9.0
h5py==2.10.0
HeapDict==1.0.1
httptools==0.1.1
idna==2.9
importlib-metadata==1.6.1
importlib-resources==2.0.1
ipykernel==5.3.0
ipython==7.15.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
isodate==0.6.0
isort==4.3.21
itsdangerous==1.1.0
jedi==0.17.1
Jinja2==2.11.2
jmespath==0.10.0
joblib==0.15.1
json5==0.9.5
jsonnet==0.16.0
jsonpickle==1.4.1
jsonschema==3.2.0
jupyter-client==6.1.3
jupyter-core==4.6.3
jupyterlab==2.1.5
jupyterlab-server==1.1.5
kiwisolver==1.2.0
lazy-object-proxy==1.4.3
locket==0.2.0
lxml==4.5.1
Mako==1.1.3
Markdown==3.2.2
MarkupSafe==1.1.1
matplotlib==3.2.2
mccabe==0.6.1
memory-profiler==0.57.0
mistune==0.8.4
mkl-fft==1.1.0
mkl-random==1.1.1
mkl-service==2.3.0
mlflow==1.9.1
more-itertools==8.4.0
msgpack==0.6.2
msrest==0.6.17
multidict==4.7.6
murmurhash==1.0.2
nbconvert==5.6.1
nbdime==2.0.0
nbformat==5.0.7
nltk==3.5
notebook==6.0.3
numpy==1.18.1
oauthlib==3.1.0
olefile==0.46
overrides==3.0.0
packaging==20.4
pandas==1.0.5
pandocfilters==1.4.2
parso==0.7.0
partd==1.1.0
pathspec==0.8.0
pdoc3==0.8.1
pexpect==4.8.0
pickleshare==0.7.5
Pillow==7.1.2
plac==1.1.3
pluggy==0.13.1
preshed==3.0.2
prometheus-client==0.8.0
prometheus-flask-exporter==0.14.1
prompt-toolkit==3.0.5
protobuf==3.12.2
psutil==5.7.0
ptyprocess==0.6.0
py==1.8.2
py-spy==0.3.3
pyarrow==0.17.1
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycparser==2.20
pydantic==1.5.1
Pygments==2.6.1
pygraphviz==1.3
pylint==2.5.3
pyparsing==2.4.7
pyrsistent==0.16.0
pytest==5.4.3
pytest-cov==2.10.0
pytest-notebook==0.6.0
pytest-pylint==0.14.1
python-dateutil==2.8.1
python-editor==1.0.4
pytz==2020.1
PyYAML==5.3.1
pyzmq==19.0.1
querystring-parser==1.2.4
ray==0.8.6
redis==3.4.1
regex==2020.6.8
requests==2.24.0
requests-oauthlib==1.3.0
rsa==4.6
s3fs==0.4.2
s3transfer==0.3.3
sacremoses==0.0.43
scikit-learn==0.23.1
scipy==1.5.0
Send2Trash==1.5.0
sentencepiece==0.1.91
six==1.15.0
smmap==3.0.4
sortedcontainers==2.2.2
soupsieve==2.0.1
spacy==2.2.4
SQLAlchemy==1.3.13
sqlparse==0.3.1
srsly==1.0.2
starlette==0.13.2
tabulate==0.8.7
tblib==1.6.0
tensorboard==2.2.2
tensorboard-plugin-wit==1.7.0
tensorboardX==2.0
terminado==0.8.3
testpath==0.4.4
thinc==7.4.0
threadpoolctl==2.1.0
tokenizers==0.7.0
toml==0.10.1
toolz==0.10.0
torch==1.5.1
torchvision==0.6.0a0+35d732a
tornado==6.0.4
tqdm==4.46.1
traitlets==4.3.3
transformers==2.11.0
typed-ast==1.4.1
typing-extensions==3.7.4.2
ujson==2.0.3
urllib3==1.25.9
uvicorn==0.11.5
uvloop==0.14.0
wasabi==0.7.0
wcwidth==0.2.4
webencodings==0.5.1
websocket-client==0.57.0
websockets==8.1
Werkzeug==1.0.1
widgetsnbextension==3.5.1
wrapt==1.12.1
xlrd==1.2.0
yarl==1.4.2
zict==2.0.0
zipp==3.1.0
```

</p>
</details>


## Steps to reproduce


<details>
<summary><b>Example source:</b></summary>
<p>

<!-- Add a fully runnable example in between the next two lines below that will reproduce the bug -->
```
from allennlp.data.tokenizers import SpacyTokenizer
from allennlp.data.token_indexers import PretrainedTransformerMismatchedIndexer
from allennlp.data.fields import TextField
from allennlp.data.vocabulary import Vocabulary
from allennlp.data.instance import Instance
from allennlp.data import Batch

from allennlp.modules.token_embedders import PretrainedTransformerMismatchedEmbedder
from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder


input_str = ""Check this annoying string!""

tokenizer = SpacyTokenizer()
token_indexer = {
    ""tokens"": PretrainedTransformerMismatchedIndexer(
        model_name=""distilroberta-base""
    )
}

tf = TextField(tokenizer.tokenize(input_str), token_indexer)
instance = Instance({""text"": tf})
vocab = Vocabulary.from_instances([instance])
batch = Batch([instance])
batch.index_instances(vocab)
padding_length = batch.get_padding_lengths()

embedder = PretrainedTransformerMismatchedEmbedder(
    model_name=""distilroberta-base""
)
tf_embedder = BasicTextFieldEmbedder({""tokens"": embedder})

tensor_dict = batch.as_tensor_dict(padding_length)
embeddings = tf_embedder(tensor_dict[""text""])

print(tf)
print(tensor_dict)
print(embeddings)
```

</p>
</details>
",,,,,,,,,1,
1892,https://github.com/allenai/allennlp/issues/4549,4549,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2020-08-11 14:23:55+00:00,,2,RuntimeError: expected device cpu and dtype Float but got device cpu and dtype Bool,"<!--
Please fill this template entirely and do not erase any of it.
We reserve the right to close without a response bug reports which are incomplete.

If you can't fill in the checklist then it's likely that this is a question, not a bug,
in which case it probably belongs on our discource forum instead:

https://discourse.allennlp.org/
-->

## Checklist

<!-- To check an item on the list replace [ ] with [x]. -->

- [ ] I have verified that the issue exists against the `master` branch of AllenNLP.
- [ ] I have read the relevant section in the [contribution guide](https://github.com/allenai/allennlp/blob/master/CONTRIBUTING.md#bug-fixes-and-new-features) on reporting bugs.
- [ ] I have checked the [issues list](https://github.com/allenai/allennlp/issues) for similar or identical bug reports.
- [ ] I have checked the [pull requests list](https://github.com/allenai/allennlp/pulls) for existing proposed fixes.
- [ ] I have checked the [CHANGELOG](https://github.com/allenai/allennlp/blob/master/CHANGELOG.md) and the [commit log](https://github.com/allenai/allennlp/commits/master) to find out if the bug was already fixed in the master branch.
- [ ] I have included in the ""Description"" section below a traceback from any exceptions related to this bug.
- [ ] I have included in the ""Related issues or possible duplicates"" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
- [ ] I have included in the ""Environment"" section below the name of the operating system and Python version that I was using when I discovered this bug.
- [ ] I have included in the ""Environment"" section below the output of `pip freeze`.
- [ ] I have included in the ""Steps to reproduce"" section below a minimally reproducible example.


## Description

<!-- Please provide a clear and concise description of what the bug is here. -->

<details>
<summary><b>Python traceback:</b></summary>
<p>

<!-- Paste the traceback from any exception (if there was one) in between the next two lines below -->
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-3-415939853a93> in <module>
  7 emissions = torch.randn(seq_length, batch_size, num_tags)
  8 tags = torch.tensor([[0.0, 1.0], [1.0, 1.0], [0.0, 1.0]], dtype=torch.long)  # 
(seq_length, batch_size)
----> 9 model(emissions, tags)

~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py in 
__call__(self, *input, **kwargs)
545                     result = (result,)
546                 input = result
--> 547         if torch._C._get_tracing_state():
548             result = self._slow_forward(*input, **kwargs)
549         else:

<ipython-input-2-2d984bd97cf1> in forward(self, inputs, tags, mask)
329             mask = mask.to(torch.bool)
330 
--> 331         log_denominator = self._input_likelihood(inputs, mask)
332         log_numerator = self._joint_likelihood(inputs, tags, mask)
333 

<ipython-input-2-2d984bd97cf1> in _input_likelihood(self, logits, mask)
249             # In valid positions (mask == True) we want to take the logsumexp over the 
current_tag dimension
250             # of `inner`. Otherwise (mask == False) we want to retain the previous 
alpha.
--> 251             alpha = util.logsumexp(inner, 1) * mask[i].view(batch_size, 1) + alpha * 
(
252                 ~mask[i]
253             ).view(batch_size, 1)

RuntimeError: expected device cpu and dtype Float but got device cpu and dtype Bool
```

</p>
</details>


## Related issues or possible duplicates

- None


## Environment

<!-- Provide the name of operating system below (e.g. OS X, Linux) -->
OS: Linux Ubuntu 18.04.4 LTS 

<!-- Provide the Python version you were using (e.g. 3.7.1) -->
Python version: Python 3.6.9 :: Anaconda, Inc.

<details>
<summary><b>Output of <code>pip freeze</code>:</b></summary>
<p>

<!-- Paste the output of `pip freeze` in between the next two lines below -->
```
absl-py==0.7.1
alembic==1.0.11
argh==0.26.2
asn1crypto==0.24.0
astor==0.7.1
atomicwrites==1.3.0
attrs==19.1.0
backcall==0.1.0
bcrypt==3.1.6
bert-serving-client==1.9.8
bert-serving-server==1.9.8
bert-tensorflow==1.0.1
bleach==3.1.0
blis==0.2.4
bokeh==1.2.0
boto==2.49.0
boto3==1.13.1
botocore==1.16.1
bz2file==0.98
cchardet==2.1.4
certifi==2020.4.5.1
cffi==1.12.3
chainer==6.1.0
chardet==3.0.4
click==7.1.2
cliff==2.15.0
cmd2==0.9.15
colorama==0.4.1
colorlog==4.0.2
cryptography==2.7
cycler==0.10.0
cymem==2.0.2
cysignals==1.10.2
Cython==0.29.12
cytoolz==0.9.0.1
dataclasses==0.7
decorator==4.4.0
defusedxml==0.6.0
dill==0.2.9
DocumentFeatureSelection==1.5
docutils==0.15.2
eli5==0.10.1
entrypoints==0.3
fastprogress==0.1.21
fasttext==0.9.1
filelock==3.0.12
future==0.17.1
gast==0.2.2
geniatagger-python==0.1
gensim==3.4.0
google-pasta==0.1.7
googleapis-common-protos==1.6.0
GPUtil==1.4.0
graphviz==0.11
grpcio==1.16.1
h5py==2.9.0
hyperopt==0.1.2
hypopt==1.0.9
idna==2.9
imageio==2.5.0
imbalanced-learn==0.4.3
importlib-metadata==0.19
ipykernel==5.1.1
ipynb==0.5.1
ipython==7.5.0
ipython-genutils==0.2.0
ipywidgets==7.4.2
jedi==0.13.3
jieba==0.39
Jinja2==2.10.1
jmespath==0.9.5
joblib==0.13.2
jsonnet==0.16.0
jsonpickle==1.4.1
jsonschema==3.0.1
jupyter==1.0.0
jupyter-client==5.2.4
jupyter-console==6.0.0
jupyter-core==4.4.0
jupyter-tensorboard==0.1.10
Keras==2.2.4
Keras-Applications==1.0.8
keras-bert==0.79.0
keras-contrib==2.0.8
keras-embed-sim==0.7.0
keras-layer-normalization==0.13.0
keras-multi-head==0.22.0
keras-pos-embd==0.11.0
keras-position-wise-feed-forward==0.6.0
Keras-Preprocessing==1.0.9
keras-self-attention==0.41.0
keras-transformer==0.31.0
kiwisolver==1.1.0
ktrain==0.4.2
langdetect==1.0.7
lightgbm==2.2.3
lime==0.1.1.34
llvmlite==0.29.0
lxml==4.3.4
Mako==1.1.0
Markdown==3.1.1
MarkupSafe==1.1.1
matplotlib==3.1.1
mistune==0.8.4
mkl-fft==1.0.14
mkl-random==1.0.1
mkl-service==2.3.0
mock==3.0.5
more-itertools==7.2.0
msgpack==0.6.1
msgpack-numpy==0.4.3.2
murmurhash==1.0.2
nbconvert==5.5.0
nbformat==4.4.0
networkx==2.3
nltk==3.4.1
nose==1.3.7
notebook==5.7.8
numba==0.45.1
numpy==1.17.1
olefile==0.46
opencv-python==4.1.1.26
opt-einsum==3.1.0
optuna==0.14.0
overrides==3.0.0
packaging==19.0
pandas==0.24.2
pandocfilters==1.4.2
paramiko==2.5.0
parso==0.4.0
pbr==5.1.3
pexpect==4.7.0
pickleshare==0.7.5
pierogi==0.2.0
Pillow==6.2.0
plac==0.9.6
plotly==4.0.0
pluggy==0.12.0
preshed==2.0.1
prettytable==0.7.2
prometheus-client==0.7.0
promise==2.2.1
prompt-toolkit==2.0.9
protobuf==3.11.3
psutil==5.6.3
ptyprocess==0.6.0
py==1.8.0
pybind11==2.3.0
pycparser==2.19
PyDispatcher==2.0.5
pydot==1.4.1
pydotplus==2.0.2
pyfasttext==0.4.6
Pygments==2.4.0
pymongo==3.8.0
PyNaCl==1.3.0
pyOpenSSL==19.0.0
pypandoc==1.4
pyparsing==2.4.0
pyperclip==1.7.0
pyrsistent==0.14.11
PySocks==1.7.0
pytest==5.1.0
python-crfsuite==0.9.7
python-dateutil==2.8.1
python-editor==1.0.4
pytils==0.3
pytorch-crf==0.7.2
pytorch-lightning==0.6.0
pytorch-pretrained-bert==0.6.2
pytorch-transformers==1.2.0
pytz==2019.1
PyWavelets==1.0.3
PyYAML==5.1
pyzmq==18.0.1
qtconsole==4.5.1
regex==2020.4.4
requests==2.23.0
retrying==1.3.3
rope==0.16.0
s3transfer==0.3.3
sacremoses==0.0.43
scikit-image==0.15.0
scikit-learn==0.21.3
scipy==1.3.1
seaborn==0.9.0
selenium==3.141.0
Send2Trash==1.5.0
sentencepiece==0.1.86
seqeval==0.0.12
six==1.14.0
sklearn-crfsuite==0.3.6
smart-open==1.8.2
spacy==2.1.6
SQLAlchemy==1.3.6
sqlitedict==1.6.0
srsly==0.0.7
stevedore==1.30.1
tabulate==0.8.5
tb-nightly==1.15.0a20190806
tensorboard==1.14.0
tensorboardX==2.0+022f060
tensorflow==1.13.1
tensorflow-datasets==1.2.0
tensorflow-estimator==1.14.0
tensorflow-hub==0.4.0
tensorflow-metadata==0.15.0
termcolor==1.1.0
terminado==0.8.2
test-tube==0.7.5
testfixtures==6.8.2
testpath==0.4.2
tf-estimator-nightly==1.14.0.dev2019080601
thinc==7.0.8
tokenization==1.0.7
tokenizers==0.7.0
tool==0.8.0
tools==0.1.9
toolz==0.9.0
torch==1.6.0
torchvision==0.7.0
tornado==6.0.2
tqdm==4.46.0
traitlets==4.3.2
transformers==2.11.0
typing==3.6.6
typing-extensions==3.6.6
ujson==1.35
umap-learn==0.3.10
urllib3==1.25.9
wasabi==0.2.2
wcwidth==0.1.7
webencodings==0.5.1
websockets==8.1
Werkzeug==0.15.2
wget==3.2
widgetsnbextension==3.4.2
word2vec==0.10.2
wordcloud==1.5.0
wrapt==1.10.11
xlrd==1.2.0
yellowbrick==0.9.1
zipp==0.5.2
```

</p>
</details>


## Steps to reproduce


<details>
<summary><b>Example source:</b></summary>
<p>

<!-- Add a fully runnable example in between the next two lines below that will reproduce the bug -->
```
num_tags = 2
model = ConditionalRandomField(num_tags)
seq_length = 3  # maximum sequence length in a batch
batch_size = 2  # number of samples in the batch
emissions = torch.randn(seq_length, batch_size, num_tags, dtype=torch.float32)
tags = torch.tensor([[0.0, 1.0], [1.0, 1.0], [0.0, 1.0]])  # (seq_length, batch_size)
model(emissions, tags)
```

</p>
</details>
",,,,,,,,,1,
1927,https://github.com/allenai/allennlp/issues/4610,4610,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2020-08-27 12:57:25+00:00,,3,Demo results for Sentiment Analysis model does not yields same results as locally downloaded model,"<!--
Please fill this template entirely and do not erase any of it.
We reserve the right to close without a response bug reports which are incomplete.

If you can't fill in the checklist then it's likely that this is a question, not a bug,
in which case it probably belongs on our discource forum instead:

https://discourse.allennlp.org/
-->

## Checklist

<!-- To check an item on the list replace [ ] with [x]. -->

- [x] I have verified that the issue exists against the `master` branch of AllenNLP.
- [x] I have read the relevant section in the [contribution guide](https://github.com/allenai/allennlp/blob/master/CONTRIBUTING.md#bug-fixes-and-new-features) on reporting bugs.
- [x] I have checked the [issues list](https://github.com/allenai/allennlp/issues) for similar or identical bug reports.
- [x] I have checked the [pull requests list](https://github.com/allenai/allennlp/pulls) for existing proposed fixes.
- [x] I have checked the [CHANGELOG](https://github.com/allenai/allennlp/blob/master/CHANGELOG.md) and the [commit log](https://github.com/allenai/allennlp/commits/master) to find out if the bug was already fixed in the master branch.
- [x] I have included in the ""Description"" section below a traceback from any exceptions related to this bug.
- [x] I have included in the ""Related issues or possible duplicates"" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
- [x] I have included in the ""Environment"" section below the name of the operating system and Python version that I was using when I discovered this bug.
- [x] I have included in the ""Environment"" section below the output of `pip freeze`.
- [x] I have included in the ""Steps to reproduce"" section below a minimally reproducible example.


## Description
After using the demo for sentiment Analysis (GloVe-LSTM) on the URL :https://demo.allennlp.org/sentiment-analysis/MjI2NDEzOQ== I tried to replicate the results on the local system by following the instructions on the same URL. After downloading model on the local machine, i tried to calculate the sentiment of some generic text and its very strange that the results are different on the demo than those on my local machine.
<!-- .  -->

<details>
<summary><b>Python traceback:</b></summary>
<p>

<!-- Paste the traceback from any exception (if there was one) in between the next two lines below -->
```
```

</p>
</details>


## Related issues or possible duplicates

- None


## Environment

<!-- Provide the name of operating system below (e.g. OS X, Linux) -->
OS: Linux

<!-- Provide the Python version you were using (e.g. 3.7.1) -->
Python version: 3.6.9

<details>
<summary><b>Output of <code>pip freeze</code>:</b></summary>
<p>

<!-- Paste the output of `pip freeze` in between the next two lines below -->
```
allennlp==1.0.0
allennlp-models==1.0.0
argon2-cffi==20.1.0
attrs==20.1.0
backcall==0.2.0
bleach==3.1.5
blis==0.4.1
boto3==1.14.48
botocore==1.17.48
catalogue==1.0.0
certifi==2020.6.20
cffi==1.14.2
chardet==3.0.4
click==7.1.2
conllu==3.0
cymem==2.0.3
dataclasses==0.7
decorator==4.4.2
defusedxml==0.6.0
docutils==0.15.2
en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz
entrypoints==0.3
filelock==3.0.12
future==0.18.2
h5py==2.10.0
idna==2.10
importlib-metadata==1.7.0
iniconfig==1.0.1
ipykernel==5.3.4
ipython==7.16.1
ipython-genutils==0.2.0
ipywidgets==7.5.1
jedi==0.17.2
Jinja2==2.11.2
jmespath==0.10.0
joblib==0.16.0
jsonnet==0.16.0
jsonpickle==1.4.1
jsonschema==3.2.0
jupyter-client==6.1.7
jupyter-core==4.6.3
MarkupSafe==1.1.1
mistune==0.8.4
more-itertools==8.4.0
murmurhash==1.0.2
nbconvert==5.6.1
nbformat==5.0.7
nltk==3.5
notebook==6.1.3
numpy==1.19.1
overrides==3.0.0
packaging==20.4
pandas==1.1.1
pandocfilters==1.4.2
parso==0.7.1
pexpect==4.8.0
pickleshare==0.7.5
plac==1.1.3
pluggy==0.13.1
preshed==3.0.2
prometheus-client==0.8.0
prompt-toolkit==3.0.6
protobuf==3.13.0
ptyprocess==0.6.0
py==1.9.0
py-rouge==1.1
pycparser==2.20
Pygments==2.6.1
pyparsing==2.4.7
pyrsistent==0.16.0
pytest==6.0.1
python-dateutil==2.8.1
pytz==2020.1
pyzmq==19.0.2
regex==2020.7.14
requests==2.24.0
s3transfer==0.3.3
sacremoses==0.0.43
scikit-learn==0.23.2
scipy==1.5.2
Send2Trash==1.5.0
sentencepiece==0.1.91
six==1.15.0
spacy==2.2.4
srsly==1.0.2
tensorboardX==2.1
terminado==0.8.3
testpath==0.4.4
thinc==7.4.0
threadpoolctl==2.1.0
tokenizers==0.7.0
toml==0.10.1
torch==1.5.1
tornado==6.0.4
tqdm==4.48.2
traitlets==4.3.3
transformers==2.11.0
urllib3==1.25.10
wasabi==0.8.0
wcwidth==0.2.5
webencodings==0.5.1
widgetsnbextension==3.5.1
word2number==1.1
zipp==3.1.0

```

</p>
</details>


## Steps to reproduce


<details>
<summary><b>Example source: </b></summary>
<p>

<!-- Add a fully runnable example in between the next two lines below that will reproduce the bug -->
```
from allennlp.predictors.predictor import Predictor
import allennlp_models.classification
predictor = Predictor.from_path(""https://storage.googleapis.com/allennlp-public-models/basic_stanford_sentiment_treebank-2020.06.09.tar.gz"")
predictor.predict(
  sentence=""The movie was overall good but not very excellent.""
)

output:
{'logits': [0.28388914465904236, -0.3324323296546936],
 'probs': [0.6493814587593079, 0.35061854124069214],
 'token_ids': [24, 20, 106, 965, 45, 22, 28, 72, 473, 7],
 'label': '1',
 'tokens': ['The',
  'movie',
  'was',
  'overall',
  'good',
  'but',
  'not',
  'very',
  'excellent',
  '.']}
```
Note: If we input the same text into the Demo, we get the following output:
The model is quite sure the sentence is Negative. (98.2%)
</p>
</details>
",,,,,,,,,1,
1962,https://github.com/allenai/allennlp/issues/4687,4687,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2020-09-30 17:49:56+00:00,,2,allennlp-models Seq2Seq dataset reader does not work with pretrained transformer tokenizers,"<!--
Please fill this template entirely and do not erase any of it.
We reserve the right to close without a response bug reports which are incomplete.
-->

## Checklist

<!-- To check an item on the list replace [ ] with [x]. -->

- [x] I have verified that the issue exists against the `master` branch of AllenNLP.
- [x] I have read the relevant section in the [contribution guide](https://github.com/allenai/allennlp/blob/master/CONTRIBUTING.md#bug-fixes-and-new-features) on reporting bugs.
- [x] I have checked the [issues list](https://github.com/allenai/allennlp/issues) for similar or identical bug reports.
- [x] I have checked the [pull requests list](https://github.com/allenai/allennlp/pulls) for existing proposed fixes.
- [x] I have checked the [CHANGELOG](https://github.com/allenai/allennlp/blob/master/CHANGELOG.md) and the [commit log](https://github.com/allenai/allennlp/commits/master) to find out if the bug was already fixed in the master branch.
- [x] I have included in the ""Description"" section below a traceback from any exceptions related to this bug.
- [x] I have included in the ""Related issues or possible duplicates"" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
- [x] I have included in the ""Environment"" section below the name of the operating system and Python version that I was using when I discovered this bug.
- [x] I have included in the ""Environment"" section below the output of `pip freeze`.
- [x] I have included in the ""Steps to reproduce"" section below a minimally reproducible example.


## Description

<!-- Please provide a clear and concise description of what the bug is here. -->

The Seq2Seq dataset reader from allennlp-models does not work with pretrained transformer tokenizers.

<details>
<summary><b>Python traceback:</b></summary>
<p>

<!-- Paste the traceback from any exception (if there was one) in between the next two lines below -->
```
Traceback (most recent call last):
  File ""/home/void/.miniconda3/envs/sqgen/bin/allennlp"", line 8, in <module>
    sys.exit(run())
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/__main__.py"", line 34, in run
    main(prog=""allennlp"")
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/commands/__init__.py"", line 92, in main
    args.func(args)
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/commands/train.py"", line 109, in train_model_from_args
    train_model_from_file(
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/commands/train.py"", line 169, in train_model_from_file
    return train_model(
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/commands/train.py"", line 232, in train_model
    model = _train_worker(
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/commands/train.py"", line 430, in _train_worker
    train_loop = TrainModel.from_params(
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py"", line 591, in from_params
    return retyped_subclass.from_params(
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py"", line 622, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py"", line 192, in create_kwargs
    constructed_arg = pop_and_construct_arg(
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py"", line 302, in pop_and_construct_arg
    return construct_arg(class_name, name, popped_params, annotation, default, **extras)
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py"", line 336, in construct_arg
    return annotation.from_params(params=popped_params, **subextras)
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py"", line 591, in from_params
    return retyped_subclass.from_params(
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py"", line 624, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File ""/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp_models/generation/dataset_readers/seq2seq.py"", line 85, in __init__
    self._start_token, self._end_token = self._source_tokenizer.tokenize(
ValueError: too many values to unpack (expected 2)
```

</p>
</details>


## Related issues or possible duplicates

- None


## Environment

<!-- Provide the name of operating system below (e.g. OS X, Linux) -->
OS: GNU/LINUX 5.8.9_1

<!-- Provide the Python version you were using (e.g. 3.7.1) -->
Python version: 3.8.5

<details>
<summary><b>Output of <code>pip freeze</code>:</b></summary>
<p>

<!-- Paste the output of `pip freeze` in between the next two lines below -->
```
alabaster==0.7.12
alembic==1.4.3
allennlp==1.1.0
allennlp-models==1.1.0
apache-airflow==1.10.12
apispec==1.3.3
argcomplete==1.12.0
argon2-cffi==20.1.0
astor==0.8.1
async-generator==1.10
attrs==19.3.0
Babel==2.8.0
backcall==0.2.0
bandit==1.6.2
bleach==3.2.1
blis==0.4.1
boto3==1.15.4
botocore==1.18.4
cached-property==1.5.2
catalogue==1.0.0
cattrs==1.0.0
certifi==2020.4.5.2
cffi==1.14.3
chardet==3.0.4
click==7.1.2
colorama==0.4.3
colorlog==4.0.2
configparser==3.5.3
conllu==4.1
coverage==5.1
croniter==0.3.34
cymem==2.0.3
darglint==1.4.1
datasets==1.0.1
decorator==4.4.2
defusedxml==0.6.0
dictdiffer==0.8.1
dill==0.3.2
dnspython==2.0.0
doc8==0.8.1
docutils==0.16
dparse==0.5.1
email-validator==1.1.1
entrypoints==0.3
eradicate==1.0
filelock==3.0.12
flake8==3.8.3
flake8-bandit==2.1.2
flake8-broken-line==0.2.0
flake8-bugbear==19.8.0
flake8-commas==2.0.0
flake8-comprehensions==3.2.3
flake8-debugger==3.2.1
flake8-docstrings==1.5.0
flake8-eradicate==0.3.0
flake8-isort==3.0.1
flake8-plugin-utils==1.3.1
flake8-polyfill==1.0.2
flake8-pytest-style==1.3.0
flake8-quotes==2.1.2
flake8-rst-docstrings==0.0.12
flake8-string-format==0.2.3
Flask==1.1.2
Flask-Admin==1.5.4
Flask-AppBuilder==2.3.0
Flask-Babel==1.0.0
Flask-Caching==1.3.3
Flask-JWT-Extended==3.24.1
Flask-Login==0.4.1
Flask-OpenID==1.2.5
Flask-SQLAlchemy==2.4.4
flask-swagger==0.2.14
Flask-WTF==0.14.3
ftfy==5.8
funcsigs==1.0.2
future==0.18.2
gitdb==4.0.5
GitPython==3.1.3
graphviz==0.14.1
gunicorn==20.0.4
h5py==2.10.0
idna==2.9
imagesize==1.2.0
importlib-metadata==1.6.1
ipykernel==5.3.4
ipython==7.16.1
ipython-genutils==0.2.0
ipywidgets==7.5.1
iso8601==0.1.13
isort==4.3.21
itsdangerous==1.1.0
jedi==0.17.2
Jinja2==2.11.2
jmespath==0.10.0
joblib==0.16.0
json-merge-patch==0.2
jsonnet==0.16.0
jsonpickle==1.4.1
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==6.1.7
jupyter-console==6.2.0
jupyter-core==4.6.3
jupyterlab-pygments==0.1.1
lazy-object-proxy==1.5.1
lockfile==0.12.2
m2r==0.2.1
Mako==1.1.3
Markdown==2.6.11
MarkupSafe==1.1.1
marshmallow==3.6.1
marshmallow-enum==1.5.1
marshmallow-polyfield==5.9
marshmallow-sqlalchemy==0.23.1
mccabe==0.6.1
mistune==0.8.4
more-itertools==8.4.0
murmurhash==1.0.2
mypy==0.782
mypy-extensions==0.4.3
natsort==7.0.1
nbclient==0.5.0
nbconvert==6.0.3
nbformat==5.0.7
nest-asyncio==1.4.0
nitpick==0.22.2
nltk==3.5
notebook==6.1.4
numpy==1.19.2
overrides==3.1.0
packaging==20.4
pandas==0.25.3
pandocfilters==1.4.2
parso==0.7.1
pbr==5.4.5
pendulum==1.4.4
pep8-naming==0.9.1
pexpect==4.8.0
pickleshare==0.7.5
plac==1.1.3
pluggy==0.13.1
preshed==3.0.2
prison==0.1.3
prometheus-client==0.8.0
prompt-toolkit==3.0.3
protobuf==3.13.0
psutil==5.7.2
ptyprocess==0.6.0
py==1.8.1
py-rouge==1.1
py4j==0.10.9
pyarrow==1.0.1
pycodestyle==2.6.0
pycparser==2.20
pydocstyle==5.0.2
pyflakes==2.2.0
Pygments==2.6.1
PyJWT==1.7.1
pyparsing==2.4.7
pyrsistent==0.17.3
pyspark==3.0.1
pytest==5.4.3
pytest-cov==2.10.1
pytest-randomly==3.4.1
python-daemon==2.2.4
python-dateutil==2.8.1
python-editor==1.0.4
python-nvd3==0.15.0
python-openid==2.2.5
python-slugify==4.0.0
pytz==2020.1
pytzdata==2020.1
PyYAML==5.3.1
pyzmq==19.0.2
qtconsole==4.7.7
QtPy==1.9.0
regex==2020.9.27
requests==2.23.0
restructuredtext-lint==1.3.1
ruamel.yaml==0.16.10
ruamel.yaml.clib==0.2.0
s3transfer==0.3.3
sacremoses==0.0.43
safety==1.9.0
scikit-learn==0.23.2
scipy==1.5.2
Send2Trash==1.5.0
sentencepiece==0.1.91
setproctitle==1.1.10
six==1.15.0
smmap==3.0.4
snowballstemmer==2.0.0
sortedcontainers==2.2.2
spacy==2.3.2
Sphinx==2.4.4
sphinx-autodoc-typehints==1.10.3
sphinxcontrib-applehelp==1.0.2
sphinxcontrib-devhelp==1.0.2
sphinxcontrib-htmlhelp==1.0.3
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==1.0.3
sphinxcontrib-serializinghtml==1.1.4
SQLAlchemy==1.3.19
SQLAlchemy-JSONField==0.9.0
SQLAlchemy-Utils==0.36.8
srsly==1.0.2
stevedore==2.0.0
tabulate==0.8.7
tenacity==4.12.0
tensorboardX==2.1
terminado==0.8.3
testfixtures==6.14.1
testpath==0.4.4
text-unidecode==1.3
thinc==7.4.1
threadpoolctl==2.1.0
thrift==0.13.0
tokenizers==0.8.1rc1
toml==0.10.0
tomlkit==0.7.0
torch==1.6.0
tornado==6.0.4
tqdm==4.49.0
traitlets==4.3.3
transformers==3.0.2
typed-ast==1.4.1
typing-extensions==3.7.4.2
tzlocal==1.5.1
unicodecsv==0.14.1
urllib3==1.25.9
wasabi==0.8.0
wcwidth==0.2.4
webencodings==0.5.1
wemake-python-styleguide==0.14.1
Werkzeug==0.16.1
widgetsnbextension==3.5.1
word2number==1.1
WTForms==2.3.3
xxhash==2.0.0
zipp==3.1.0
zope.deprecation==4.4.0

```

</p>
</details>


## Steps to reproduce
Use any Seq2Seq dataset with the bart model:

<details>
<summary><b>Example source:</b></summary>
<p>

<!-- Add a fully runnable example in between the next two lines below that will reproduce the bug -->
```
local model_name = ""sshleifer/distilbart-xsum-12-6"";
local data_base_url = ""data/pubmedqa/processed/"";

{
  ""train_data_path"": data_base_url + ""train.tsv"",
  ""validation_data_path"": data_base_url + ""test.tsv"",
  ""dataset_reader"": {
    ""type"": ""seq2seq"",
    ""source_tokenizer"": {
      ""type"": ""pretrained_transformer"",
      ""model_name"": model_name
    },
    ""source_token_indexers"": {
      ""tokens"": {
        ""type"": ""pretrained_transformer"",
        ""model_name"": model_name,
        ""namespace"": ""tokens""
      }
    },
    ""source_add_start_token"": false,
    ""source_add_end_token"": false,
    ""target_add_start_token"": false,
    ""target_add_end_token"": false,
    ""source_max_tokens"": 512,
    ""target_max_tokens"": 40,
  },
  ""model"": {
    ""type"": ""bart"",
    ""model_name"": model_name
  },
  ""data_loader"": {
    ""batch_size"": 2,
    ""shuffle"": true
  },
  ""trainer"": {
    ""num_epochs"": 1,
    ""optimizer"": {
      ""type"": ""huggingface_adamw"",
      ""lr"": 3e-5,
      ""betas"": [0.9, 0.999],
      ""eps"": 1e-8,
      ""correct_bias"": true
    },
    ""learning_rate_scheduler"": {
      ""type"": ""polynomial_decay"",
    },
    ""grad_norm"": 1.0,
  }
}
```

</p>
</details>
",,,,,,,,,1,
1966,https://github.com/allenai/allennlp/issues/4691,4691,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}, {'id': 2763124639, 'node_id': 'MDU6TGFiZWwyNzYzMTI0NjM5', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/stale', 'name': 'stale', 'color': 'ededed', 'default': False, 'description': None}]",closed,2020-10-01 15:26:18+00:00,,14,AttributeError: 'Tensor' object has no attribute 'keys' when using ComposedSeq2Seq,"<!--
Please fill this template entirely and do not erase any of it.
We reserve the right to close without a response bug reports which are incomplete.
-->

## Checklist

<!-- To check an item on the list replace [ ] with [x]. -->

- [x] I have verified that the issue exists against the `master` branch of AllenNLP.
- [x] I have read the relevant section in the [contribution guide](https://github.com/allenai/allennlp/blob/master/CONTRIBUTING.md#bug-fixes-and-new-features) on reporting bugs.
- [x] I have checked the [issues list](https://github.com/allenai/allennlp/issues) for similar or identical bug reports.
- [x] I have checked the [pull requests list](https://github.com/allenai/allennlp/pulls) for existing proposed fixes.
- [x] I have checked the [CHANGELOG](https://github.com/allenai/allennlp/blob/master/CHANGELOG.md) and the [commit log](https://github.com/allenai/allennlp/commits/master) to find out if the bug was already fixed in the master branch.
- [x] I have included in the ""Description"" section below a traceback from any exceptions related to this bug.
- [x] I have included in the ""Related issues or possible duplicates"" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
- [x] I have included in the ""Environment"" section below the name of the operating system and Python version that I was using when I discovered this bug.
- [x] I have included in the ""Environment"" section below the output of `pip freeze`.
- [x] I have included in the ""Steps to reproduce"" section below a minimally reproducible example.


## Description

<!-- Please provide a clear and concise description of what the bug is here. -->

<details>
<summary><b>Python traceback:</b></summary>
<p>

<!-- Paste the traceback from any exception (if there was one) in between the next two lines below -->
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/CE/skr/anaconda3/lib/python3.6/site-packages/allennlp/training/trainer.py"", line 867, in train
    train_metrics = self._train_epoch(epoch)
  File ""/home/CE/skr/anaconda3/lib/python3.6/site-packages/allennlp/training/trainer.py"", line 589, in _train_epoch
    batch_outputs = self.batch_outputs(batch, for_training=True)
  File ""/home/CE/skr/anaconda3/lib/python3.6/site-packages/allennlp/training/trainer.py"", line 479, in batch_outputs
    output_dict = self._pytorch_model(**batch)
  File ""/home/CE/skr/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/CE/skr/anaconda3/lib/python3.6/site-packages/allennlp_models/generation/models/composed_seq2seq.py"", line 121, in forward
    return self._decoder(state, target_tokens)
  File ""/home/CE/skr/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/CE/skr/anaconda3/lib/python3.6/site-packages/allennlp_models/generation/modules/seq_decoders/auto_regressive.py"", line 416, in forward
    output_dict = self._forward_loss(state_forward_loss, target_tokens)
  File ""/home/CE/skr/anaconda3/lib/python3.6/site-packages/allennlp_models/generation/modules/seq_decoders/auto_regressive.py"", line 161, in _forward_loss
    target_embedding = self.target_embedder(targets)
  File ""/home/CE/skr/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/CE/skr/anaconda3/lib/python3.6/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"", line 56, in forward
    if self._token_embedders.keys() != text_field_input.keys():
AttributeError: 'Tensor' object has no attribute 'keys'
```

</p>
</details>


## Related issues or possible duplicates

- None


## Environment

<!-- Provide the name of operating system below (e.g. OS X, Linux) -->
OS: Linux

<!-- Provide the Python version you were using (e.g. 3.7.1) -->
Python version: 3.6.5

<details>
<summary><b>Output of <code>pip freeze</code>:</b></summary>
<p>

<!-- Paste the output of `pip freeze` in between the next two lines below -->
```
alabaster==0.7.10
allennlp==1.1.0
allennlp-models==1.1.0
anaconda-client==1.6.14
anaconda-navigator==1.8.7
anaconda-project==0.8.2
asn1crypto==0.24.0
astroid==1.6.3
astropy==3.0.2
attrs==18.1.0
Babel==2.5.3
backcall==0.1.0
backports.shutil-get-terminal-size==1.0.0
beautifulsoup4==4.6.0
bitarray==0.8.1
bkcharts==0.2
blaze==0.11.3
bleach==2.1.3
blis==0.4.1
bokeh==0.12.16
boto==2.48.0
boto3==1.15.6
botocore==1.18.6
Bottleneck==1.2.1
catalogue==1.0.0
certifi==2018.4.16
cffi==1.11.5
chardet==3.0.4
click==6.7
cloudpickle==0.5.3
clyent==1.2.2
colorama==0.3.9
conda==4.5.4
conda-build==3.10.5
conda-verify==2.0.0
conllu==4.1
contextlib2==0.5.5
cryptography==2.2.2
cycler==0.10.0
cymem==2.0.3
Cython==0.28.2
cytoolz==0.9.0.1
dask==0.17.5
dataclasses==0.7
datashape==0.5.4
decorator==4.3.0
distributed==1.21.8
docutils==0.14
en-core-web-sm==2.3.1
entrypoints==0.2.3
et-xmlfile==1.0.1
fastcache==1.0.2
filelock==3.0.4
Flask==1.0.2
Flask-Cors==3.0.4
ftfy==5.8
future==0.18.2
gevent==1.3.0
glob2==0.6
gmpy2==2.0.8
greenlet==0.4.13
h5py==2.7.1
heapdict==1.0.0
html5lib==1.0.1
idna==2.6
imageio==2.3.0
imagesize==1.0.0
importlib-metadata==2.0.0
ipykernel==4.8.2
ipython==6.4.0
ipython-genutils==0.2.0
ipywidgets==7.2.1
isort==4.3.4
itsdangerous==0.24
jdcal==1.4
jedi==0.12.0
Jinja2==2.10
jmespath==0.10.0
joblib==0.16.0
jsonnet==0.16.0
jsonpickle==1.4.1
jsonschema==2.6.0
jupyter==1.0.0
jupyter-client==5.2.3
jupyter-console==5.2.0
jupyter-core==4.4.0
jupyterlab==0.32.1
jupyterlab-launcher==0.10.5
kiwisolver==1.0.1
lazy-object-proxy==1.3.1
llvmlite==0.23.1
locket==0.2.0
lxml==4.2.1
MarkupSafe==1.0
matplotlib==2.2.2
mccabe==0.6.1
mistune==0.8.3
mkl-fft==1.0.0
mkl-random==1.0.1
more-itertools==4.1.0
mpmath==1.0.0
msgpack-python==0.5.6
multipledispatch==0.5.0
murmurhash==1.0.2
navigator-updater==0.2.1
nbconvert==5.3.1
nbformat==4.4.0
networkx==2.1
nltk==3.3
nose==1.3.7
notebook==5.5.0
numba==0.38.0
numexpr==2.6.5
numpy==1.14.3
numpydoc==0.8.0
odo==0.5.1
olefile==0.45.1
openpyxl==2.5.3
overrides==3.1.0
packaging==17.1
pandas==0.23.0
pandocfilters==1.4.2
parso==0.2.0
partd==0.3.8
path.py==11.0.1
pathlib2==2.3.2
patsy==0.5.0
pep8==1.7.1
pexpect==4.5.0
pickleshare==0.7.4
Pillow==5.1.0
pkginfo==1.4.2
plac==1.1.3
pluggy==0.6.0
ply==3.11
preshed==3.0.2
prompt-toolkit==1.0.15
protobuf==3.13.0
psutil==5.4.5
ptyprocess==0.5.2
py==1.5.3
py-rouge==1.1
pycodestyle==2.4.0
pycosat==0.6.3
pycparser==2.18
pycrypto==2.6.1
pycurl==7.43.0.1
pyflakes==1.6.0
Pygments==2.2.0
pylint==1.8.4
pyodbc==4.0.23
pyOpenSSL==18.0.0
pyparsing==2.2.0
PySocks==1.6.8
pytest==3.5.1
pytest-arraydiff==0.2
pytest-astropy==0.3.0
pytest-doctestplus==0.1.3
pytest-openfiles==0.3.0
pytest-remotedata==0.2.1
python-dateutil==2.7.3
pytz==2018.4
PyWavelets==0.5.2
PyYAML==3.12
pyzmq==17.0.0
QtAwesome==0.4.4
qtconsole==4.3.1
QtPy==1.4.1
regex==2020.9.27
requests==2.18.4
rope==0.10.7
ruamel-yaml==0.15.35
s3transfer==0.3.3
sacremoses==0.0.43
scikit-image==0.13.1
scikit-learn==0.19.1
scipy==1.1.0
seaborn==0.8.1
Send2Trash==1.5.0
sentencepiece==0.1.91
simplegeneric==0.8.1
singledispatch==3.4.0.3
six==1.11.0
snowballstemmer==1.2.1
sortedcollections==0.6.1
sortedcontainers==1.5.10
spacy==2.3.2
Sphinx==1.7.4
sphinxcontrib-websupport==1.0.1
spyder==3.2.8
SQLAlchemy==1.2.7
srsly==1.0.2
statsmodels==0.9.0
sympy==1.1.1
tables==3.4.3
tblib==1.3.2
tensorboardX==2.1
terminado==0.8.1
testpath==0.3.1
thinc==7.4.1
tokenizers==0.8.1rc1
toolz==0.9.0
torch==1.6.0
torchtext==0.7.0
tornado==5.0.2
tqdm==4.49.0
traitlets==4.3.2
transformers==3.0.2
typing==3.6.4
unicodecsv==0.14.1
Unidecode==1.1.1
urllib3==1.22
wasabi==0.8.0
wcwidth==0.1.7
webencodings==0.5.1
Werkzeug==0.14.1
widgetsnbextension==3.2.1
word2number==1.1
wrapt==1.10.11
xlrd==1.1.0
XlsxWriter==1.0.4
xlwt==1.3.0
zict==0.1.3
zipp==3.2.0

```

</p>
</details>


## Steps to reproduce


<details>
<summary><b>Example source:</b></summary>
<p>

<!-- Add a fully runnable example in between the next two lines below that will reproduce the bug -->
```
reader = Seq2SeqDatasetReader(source_tokenizer=WhitespaceTokenizer(),
		target_tokenizer=WhitespaceTokenizer(), 
		source_token_indexers={'tokens': SingleIdTokenIndexer()}, 
		target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')})

train_dataset = reader.read(train_path)
validation_dataset = reader.read(val_path)
test_dataset = reader.read(test_path)
vocab = Vocabulary.from_instances(train_dataset + validation_dataset, min_count={'tokens': 1, 'target_tokens': 1})

train_dataset.index_with(vocab)
validation_dataset.index_with(vocab)

SRC_EMBEDDING_DIM = 128
TG_EMBEDDING_DIM = 128
HIDDEN_DIM = 512
enc_layers = 3
dec_layers = 3
enc_heads = 2
dec_heads = 2
ff_dim = 512
proj_dim = 128
enc_dropout = 0.2
dec_dropout = 0.2
max_decoding_steps = 40
beam = 3
CUDA_DEVICE = 0

src_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),
                             embedding_dim=SRC_EMBEDDING_DIM)
source_embedder = BasicTextFieldEmbedder({""tokens"": src_embedding})

trg_embedding = Embedding(num_embeddings=vocab.get_vocab_size('target_tokens'),
                             embedding_dim=TG_EMBEDDING_DIM)
target_embedder = BasicTextFieldEmbedder({""target_tokens"": trg_embedding})

encoder = StackedSelfAttentionEncoder(input_dim=SRC_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,
                                      projection_dim=proj_dim, feedforward_hidden_dim=ff_dim, num_layers=enc_layers,
                                      num_attention_heads=enc_heads, dropout_prob=enc_dropout, use_positional_encoding=True)

decoder_net = StackedSelfAttentionDecoderNet(decoding_dim=TG_EMBEDDING_DIM, target_embedding_dim=TG_EMBEDDING_DIM,
                                             feedforward_hidden_dim=128, num_layers=dec_layers,                                             num_attention_heads=dec_heads, use_positional_encoding=True, dropout_prob=dec_dropout)

decoder = AutoRegressiveSeqDecoder(vocab=vocab, decoder_net=decoder_net, max_decoding_steps=max_decoding_steps,
                                target_embedder=target_embedder, target_namespace='target_tokens', beam_size=beam)

model = ComposedSeq2Seq(vocab=vocab, source_text_embedder=source_embedder,
                        encoder=encoder, decoder=decoder)

model = model.cuda(CUDA_DEVICE)

optimizer = optim.Adam(model.parameters(), lr=0.0005)

train_data_loader = PyTorchDataLoader(train_dataset,batch_sampler=BucketBatchSampler(train_dataset,batch_size=16))
dev_data_loader = PyTorchDataLoader(validation_dataset,batch_sampler=BucketBatchSampler(validation_dataset,batch_size=16))

trainer = GradientDescentTrainer(model=model, optimizer=optimizer,data_loader=train_data_loader,
                                 validation_data_loader=dev_data_loader,num_epochs=3)

trainer.train()
```

</p>
</details>
I'm using the ComposedSeq2Seq class. The error is raised also when I replace the Transformer encoder with an LSTM, and the same with the decoder. I suspect the error is connected to the data loader or iterator.",,,,,,,,,1,
1969,https://github.com/allenai/allennlp/issues/4703,4703,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2020-10-05 02:36:10+00:00,,3,"Simple/Smooth/Integrated Gradient examples are returning `'grad_input_1': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]`","## Description

Following the example in this Usage section: 
 https://demo.allennlp.org/sentiment-analysis/MjMxMzE4Mw==

Here is my notebook:
https://github.com/data-science-on-aws/workshop/blob/925387e/07_train/wip/99_AllenNLP_RoBERTa_Prediction.ipynb

## Related issues or possible duplicates

- None


## Environment

<!-- Provide the name of operating system below (e.g. OS X, Linux) -->
OS:  Linux

<!-- Provide the Python version you were using (e.g. 3.7.1) -->
Python version: 3.6.10

<details>
<summary><b>Output of <code>pip freeze</code>:</b></summary>
<p>

<!-- Paste the output of `pip freeze` in between the next two lines below -->
```
absl-py==0.10.0
alabaster==0.7.12
allennlp==1.0.0
allennlp-models==1.0.0
anaconda-client==1.7.2
anaconda-project==0.8.3
argh==0.26.2
asn1crypto==1.3.0
astor==0.8.1
astroid==2.4.2
astropy==4.0.1.post1
astunparse==1.6.3
atomicwrites==1.3.0
attrs==19.3.0
Automat==20.2.0
autopep8==1.4.4
autovizwidget==0.15.0
awscli==1.18.137
awswrangler==1.9.3
Babel==2.8.0
backcall==0.1.0
backports.shutil-get-terminal-size==1.0.0
bcrypt==3.2.0
beautifulsoup4==4.8.2
bitarray==1.2.1
bkcharts==0.2
bleach==3.1.4
blis==0.4.1
bokeh==2.0.1
boto==2.49.0
boto3==1.14.60
botocore==1.17.60
Bottleneck==1.3.2
cached-property==1.5.1
cachetools==4.1.1
catalogue==1.0.0
certifi==2020.6.20
cffi==1.14.0
chardet==3.0.4
click==7.1.1
cloudpickle==1.3.0
clyent==1.2.2
colorama==0.4.3
conllu==3.0
contextlib2==0.6.0.post1
cryptography==2.8
cycler==0.10.0
cymem==2.0.3
Cython==0.29.15
cytoolz==0.10.1
dask==2.14.0
dataclasses==0.7
decorator==4.4.2
defusedxml==0.6.0
diff-match-patch==20181111
distributed==2.14.0
distro==1.5.0
docker==4.3.1
docker-compose==1.26.2
dockerpty==0.4.1
docopt==0.6.2
docutils==0.15.2
en-core-web-sm==2.2.5
entrypoints==0.3
enum-compat==0.0.3
enum34==1.1.10
environment-kernels==1.1.1
et-xmlfile==1.0.1
fastcache==1.1.0
filelock==3.0.12
flake8==3.7.9
Flask==1.1.1
fsspec==0.7.1
future==0.18.2
gast==0.2.2
gevent==1.4.0
glob2==0.7
gmpy2==2.0.8
google-auth==1.21.1
google-auth-oauthlib==0.4.1
google-pasta==0.2.0
greenlet==0.4.15
grpcio==1.32.0
h5py==2.10.0
hdijupyterutils==0.15.0
HeapDict==1.0.1
html5lib==1.0.1
hypothesis==5.8.3
idna==2.10
imageio==2.8.0
imagesize==1.2.0
importlib-metadata==1.7.0
intervaltree==3.0.2
ipykernel==5.1.4
ipyparallel==6.2.4
ipython==7.13.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
isort==4.3.21
itsdangerous==1.1.0
jdcal==1.4.1
jedi==0.15.2
jeepney==0.4.3
Jinja2==2.11.1
jmespath==0.9.4
joblib==0.14.1
json5==0.9.4
jsonnet==0.16.0
jsonpickle==1.4.1
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==6.1.2
jupyter-console==6.1.0
jupyter-core==4.6.3
jupyterlab==1.2.6
jupyterlab-server==1.1.0
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.2
keyring==21.1.1
kiwisolver==1.1.0
lazy-object-proxy==1.4.3
libarchive-c==2.8
lief==0.9.0
llvmlite==0.31.0
locket==0.2.0
lxml==4.5.0
Markdown==3.2.2
MarkupSafe==1.1.1
matplotlib==3.1.3
mccabe==0.6.1
mistune==0.8.4
mkl-fft==1.0.15
mkl-random==1.1.0
mkl-service==2.3.0
mock==4.0.1
more-itertools==8.2.0
mpmath==1.1.0
msgpack==1.0.0
multipledispatch==0.6.0
murmurhash==1.0.2
nb-conda==2.2.1
nb-conda-kernels==2.2.3
nbconvert==5.6.1
nbformat==5.0.4
networkx==2.4
nltk==3.4.5
nose==1.3.7
notebook==6.0.3
numba==0.48.0
numexpr==2.7.1
numpy==1.18.5
numpydoc==0.9.2
nvidia-ml-py==10.418.84
oauthlib==3.1.0
olefile==0.46
opencv-python==4.2.0.32
openpyxl==3.0.3
opt-einsum==3.3.0
overrides==3.0.0
packaging==20.3
pandas==1.0.5
pandocfilters==1.4.2
paramiko==2.7.1
parso==0.5.2
partd==1.1.0
path==13.1.0
pathlib2==2.3.5
pathtools==0.1.2
patsy==0.5.1
pep8==1.7.1
pexpect==4.8.0
pickleshare==0.7.5
Pillow==7.1.2
pkginfo==1.5.0.1
plac==1.1.3
plotly==4.9.0
pluggy==0.13.1
ply==3.11
preshed==3.0.2
prometheus-client==0.7.1
prompt-toolkit==3.0.4
protobuf==3.13.0
protobuf3-to-dict==0.1.5
psutil==5.7.0
psycopg2==2.7.5
psycopg2-binary==2.8.6
ptyprocess==0.6.0
py==1.8.1
py-rouge==1.1
py4j==0.10.7
pyarrow==1.0.1
pyasn1==0.4.8
pyasn1-modules==0.2.8
PyAthena==1.10.7
pycodestyle==2.5.0
pycosat==0.6.3
pycparser==2.20
pycrypto==2.6.1
pycurl==7.43.0.5
pydocstyle==4.0.1
pyflakes==2.1.1
pygal==2.4.0
Pygments==2.6.1
pykerberos==1.2.1
pylint==2.5.3
PyMySQL==0.10.1
PyNaCl==1.4.0
pyodbc===4.0.0-unsupported
pyOpenSSL==19.1.0
pyparsing==2.4.6
pyrsistent==0.16.0
PySocks==1.7.1
pyspark==2.3.4
pytest==5.4.1
pytest-arraydiff==0.3
pytest-astropy==0.8.0
pytest-astropy-header==0.1.2
pytest-doctestplus==0.5.0
pytest-openfiles==0.4.0
pytest-remotedata==0.3.2
python-dateutil==2.8.1
python-dotenv==0.14.0
python-jsonrpc-server==0.3.4
python-language-server==0.31.10
pytz==2019.3
PyWavelets==1.1.1
pyxdg==0.26
PyYAML==5.3.1
pyzmq==18.1.1
QDarkStyle==2.8
QtAwesome==0.7.0
qtconsole==4.7.2
QtPy==1.9.0
regex==2020.7.14
requests==2.24.0
requests-kerberos==0.12.0
requests-oauthlib==1.3.0
retrying==1.3.3
rope==0.16.0
rsa==4.5
Rtree==0.9.4
ruamel-yaml==0.15.87
s3fs==0.4.0
s3transfer==0.3.3
sacremoses==0.0.43
sagemaker==2.9.2
sagemaker-experiments==0.1.24
sagemaker-pyspark==1.4.0
scikit-image==0.16.2
scikit-learn==0.23.1
scipy==1.4.1
seaborn==0.10.0
SecretStorage==3.1.2
Send2Trash==1.5.0
sentencepiece==0.1.91
simplegeneric==0.8.1
singledispatch==3.4.0.3
six==1.15.0
smdebug==0.9.3
smdebug-rulesconfig==0.1.5
snowballstemmer==2.0.0
sortedcollections==1.1.2
sortedcontainers==2.1.0
soupsieve==2.0
spacy==2.2.4
sparkmagic==0.15.0
Sphinx==3.0.4
sphinxcontrib-applehelp==1.0.2
sphinxcontrib-devhelp==1.0.2
sphinxcontrib-htmlhelp==1.0.3
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==1.0.3
sphinxcontrib-serializinghtml==1.1.4
sphinxcontrib-websupport==1.2.1
spyder==4.1.2
spyder-kernels==1.9.0
SQLAlchemy==1.3.13
sqlalchemy-redshift==0.8.1
srsly==1.0.2
statsmodels==0.11.0
stepfunctions==2.0.0rc1
sympy==1.5.1
tables==3.6.1
tblib==1.6.0
tenacity==6.2.0
tensorboard==2.1.1
tensorboard-plugin-wit==1.7.0
tensorboardX==2.1
tensorflow==2.1.0
tensorflow-estimator==2.1.0
termcolor==1.1.0
terminado==0.8.3
testpath==0.4.4
texttable==1.6.2
thinc==7.4.0
threadpoolctl==2.1.0
tokenizers==0.7.0
toml==0.10.1
toolz==0.10.0
torch==1.5.1
torch-model-archiver==0.1.1
torchserve==0.1.1
tornado==6.0.4
tqdm==4.44.1
traitlets==4.3.3
transformers==2.11.0
typed-ast==1.4.1
typing-extensions==3.7.4.1
ujson==1.35
unicodecsv==0.14.1
urllib3==1.25.10
wasabi==0.8.0
watchdog==0.10.2
wcwidth==0.1.9
webencodings==0.5.1
websocket-client==0.57.0
Werkzeug==1.0.1
widgetsnbextension==3.5.1
word2number==1.1
wrapt==1.12.1
wurlitzer==2.0.0
xlrd==1.2.0
XlsxWriter==1.2.8
xlwt==1.3.0
yapf==0.28.0
zict==2.0.0
zipp==3.1.0
```

</p>
</details>


## Steps to reproduce
```
!pip install -q allennlp==1.0.0
!pip install -q allennlp-models==1.0.0
```
```
from allennlp.predictors.predictor import Predictor

predictor = Predictor.from_path(""https://storage.googleapis.com/allennlp-public-models/sst-roberta-large-2020.06.08.tar.gz"")

sentence = ""a very well-made, funny and entertaining picture.""

inputs = {""sentence"": sentence}

from allennlp.interpret.saliency_interpreters import SimpleGradient

simple_gradient_interpreter = SimpleGradient(predictor)

simple_gradient_interpretation = simple_gradient_interpreter.saliency_interpret_from_json(inputs)

print(simple_gradient_interpretation)
```
OUTPUT
```
{'instance_1': {'grad_input_1': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
```

</p>
</details>
",,,,,,,,,1,
1972,https://github.com/allenai/allennlp/issues/4715,4715,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}, {'id': 887719346, 'node_id': 'MDU6TGFiZWw4ODc3MTkzNDY=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Contributions%20welcome', 'name': 'Contributions welcome', 'color': '02b8d1', 'default': False, 'description': ''}]",closed,2020-10-07 17:23:47+00:00,,2,Rouge metric incorrectly computed with distributed training,"<!--
Please fill this template entirely and do not erase any of it.
We reserve the right to close without a response bug reports which are incomplete.

If you have a question rather than a bug, please ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/allennlp) rather than posting an issue here.
-->

## Checklist

<!-- To check an item on the list replace [ ] with [x]. -->

- [x] I have verified that the issue exists against the `master` branch of AllenNLP.
- [x] I have read the relevant section in the [contribution guide](https://github.com/allenai/allennlp/blob/master/CONTRIBUTING.md#bug-fixes-and-new-features) on reporting bugs.
- [x] I have checked the [issues list](https://github.com/allenai/allennlp/issues) for similar or identical bug reports.
- [x] I have checked the [pull requests list](https://github.com/allenai/allennlp/pulls) for existing proposed fixes.
- [x] I have checked the [CHANGELOG](https://github.com/allenai/allennlp/blob/master/CHANGELOG.md) and the [commit log](https://github.com/allenai/allennlp/commits/master) to find out if the bug was already fixed in the master branch.
- [x] I have included in the ""Description"" section below a traceback from any exceptions related to this bug.
- [x] I have included in the ""Related issues or possible duplicates"" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
- [x] I have included in the ""Environment"" section below the name of the operating system and Python version that I was using when I discovered this bug.
- [x] I have included in the ""Environment"" section below the output of `pip freeze`.
- [x] I have included in the ""Steps to reproduce"" section below a minimally reproducible example.


## Description

<!-- Please provide a clear and concise description of what the bug is here. -->
Training the same model with and without distributed training produces drastically different metrics and incorrect metrics with distributed.

In fact distributed training on 8 GPUs gives me scores > 1 which is obviously incorrect.

<details>
<summary><b>Multi GPU metrics:</b></summary>
<p>

<!-- Paste the traceback from any exception (if there was one) in between the next two lines below -->
```
  ""validation_ROUGE-1_R"": 3.7029976844787598,
  ""validation_ROUGE-2_R"": 1.9398850351572037,
  ""validation_ROUGE-1_P"": 4.3116472363471985,
  ""validation_ROUGE-2_P"": 2.3042131861050925,
  ""validation_ROUGE-1_F1"": 3.819778541723887,
  ""validation_ROUGE-2_F1"": 2.039711813131968,
  ""validation_ROUGE-L"": 3.579911470413208,
```

</p>
</details>

<details>
<summary><b>Single GPU metrics:</b></summary>
<p>

<!-- Paste the traceback from any exception (if there was one) in between the next two lines below -->
```
  ""validation_ROUGE-1_R"": 0.49840065422148055,
  ""validation_ROUGE-2_R"": 0.28016004520356264,
  ""validation_ROUGE-1_P"": 0.5445739355913551,
  ""validation_ROUGE-2_P"": 0.3068941746277846,
  ""validation_ROUGE-1_F1"": 0.5003892512410818,
  ""validation_ROUGE-2_F1"": 0.28298027227105127,
  ""validation_ROUGE-L"": 0.4661126545409945,
```

</p>
</details>


## Related issues or possible duplicates

- #4050 is a possibly related issue


## Environment

<!-- Provide the name of operating system below (e.g. OS X, Linux) -->
OS: GNU/Linux

<!-- Provide the Python version you were using (e.g. 3.7.1) -->
Python version: 3.8.5

<details>
<summary><b>Output of <code>pip freeze</code>:</b></summary>
<p>

<!-- Paste the output of `pip freeze` in between the next two lines below -->
```
alabaster==0.7.12
alembic==1.4.3
-e git+https://github.com/allenai/allennlp.git@edcb6d3466d2c4263f1e6a5731c6ace5358f47e8#egg=allennlp
-e git+https://github.com/allenai/allennlp-models.git@a330876f95cfec99f0ab724fbc0237f7d1f3288c#egg=allennlp_models
apache-airflow==1.10.12
apispec==1.3.3
argcomplete==1.12.0
argon2-cffi==20.1.0
astor==0.8.1
async-generator==1.10
attrs==19.3.0
Babel==2.8.0
backcall==0.2.0
bandit==1.6.2
bert-serving-client==1.10.0
bleach==3.2.1
blis==0.4.1
boto3==1.15.4
botocore==1.18.4
cached-property==1.5.2
catalogue==1.0.0
cattrs==1.0.0
certifi==2020.4.5.2
cffi==1.14.3
chardet==3.0.4
click==7.1.2
colorama==0.4.3
colorlog==4.0.2
configparser==3.5.3
conllu==4.2
coverage==5.1
croniter==0.3.34
cymem==2.0.3
darglint==1.4.1
datasets==1.0.1
decorator==4.4.2
defusedxml==0.6.0
dictdiffer==0.8.1
dill==0.3.2
dnspython==2.0.0
doc8==0.8.1
docutils==0.16
dparse==0.5.1
email-validator==1.1.1
entrypoints==0.3
eradicate==1.0
filelock==3.0.12
flake8==3.8.3
flake8-bandit==2.1.2
flake8-broken-line==0.2.0
flake8-bugbear==19.8.0
flake8-commas==2.0.0
flake8-comprehensions==3.2.3
flake8-debugger==3.2.1
flake8-docstrings==1.5.0
flake8-eradicate==0.3.0
flake8-isort==3.0.1
flake8-plugin-utils==1.3.1
flake8-polyfill==1.0.2
flake8-pytest-style==1.3.0
flake8-quotes==2.1.2
flake8-rst-docstrings==0.0.12
flake8-string-format==0.2.3
Flask==1.1.2
Flask-Admin==1.5.4
Flask-AppBuilder==2.3.0
Flask-Babel==1.0.0
Flask-Caching==1.3.3
Flask-JWT-Extended==3.24.1
Flask-Login==0.4.1
Flask-OpenID==1.2.5
Flask-SQLAlchemy==2.4.4
flask-swagger==0.2.14
Flask-WTF==0.14.3
ftfy==5.8
funcsigs==1.0.2
future==0.18.2
gitdb==4.0.5
GitPython==3.1.3
graphviz==0.14.1
gunicorn==20.0.4
h5py==2.10.0
idna==2.9
imagesize==1.2.0
importlib-metadata==1.6.1
ipykernel==5.3.4
ipython==7.16.1
ipython-genutils==0.2.0
ipywidgets==7.5.1
iso8601==0.1.13
isort==4.3.21
itsdangerous==1.1.0
jedi==0.17.2
Jinja2==2.11.2
jmespath==0.10.0
joblib==0.16.0
json-merge-patch==0.2
jsonnet==0.16.0
jsonpickle==1.4.1
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==6.1.7
jupyter-console==6.2.0
jupyter-core==4.6.3
jupyterlab-pygments==0.1.1
lazy-object-proxy==1.5.1
lockfile==0.12.2
m2r==0.2.1
Mako==1.1.3
Markdown==2.6.11
MarkupSafe==1.1.1
marshmallow==3.6.1
marshmallow-enum==1.5.1
marshmallow-polyfield==5.9
marshmallow-sqlalchemy==0.23.1
mccabe==0.6.1
mistune==0.8.4
more-itertools==8.4.0
murmurhash==1.0.2
mypy==0.782
mypy-extensions==0.4.3
natsort==7.0.1
nbclient==0.5.0
nbconvert==6.0.3
nbformat==5.0.7
nest-asyncio==1.4.0
nitpick==0.22.2
nltk==3.5
notebook==6.1.4
numpy==1.19.2
overrides==3.1.0
packaging==20.4
pandas==0.25.3
pandocfilters==1.4.2
parso==0.7.1
pbr==5.4.5
pendulum==1.4.4
pep8-naming==0.9.1
pexpect==4.8.0
pickleshare==0.7.5
plac==1.1.3
pluggy==0.13.1
preshed==3.0.2
prison==0.1.3
prometheus-client==0.8.0
prompt-toolkit==3.0.3
protobuf==3.13.0
psutil==5.7.2
ptyprocess==0.6.0
py==1.8.1
py-rouge==1.1
py4j==0.10.9
pyarrow==1.0.1
pycodestyle==2.6.0
pycparser==2.20
pydocstyle==5.0.2
pyflakes==2.2.0
Pygments==2.6.1
PyJWT==1.7.1
pyparsing==2.4.7
pyrsistent==0.17.3
pyspark==3.0.1
pytest==5.4.3
pytest-cov==2.10.1
pytest-randomly==3.4.1
python-daemon==2.2.4
python-dateutil==2.8.1
python-editor==1.0.4
python-nvd3==0.15.0
python-openid==2.2.5
python-slugify==4.0.0
pytz==2020.1
pytzdata==2020.1
PyYAML==5.3.1
pyzmq==19.0.2
qtconsole==4.7.7
QtPy==1.9.0
regex==2020.9.27
requests==2.23.0
restructuredtext-lint==1.3.1
ruamel.yaml==0.16.10
ruamel.yaml.clib==0.2.0
s3transfer==0.3.3
sacremoses==0.0.43
safety==1.9.0
scikit-learn==0.23.2
scipy==1.5.2
Send2Trash==1.5.0
sentencepiece==0.1.91
setproctitle==1.1.10
six==1.15.0
smmap==3.0.4
snowballstemmer==2.0.0
sortedcontainers==2.2.2
spacy==2.3.2
Sphinx==2.4.4
sphinx-autodoc-typehints==1.10.3
sphinxcontrib-applehelp==1.0.2
sphinxcontrib-devhelp==1.0.2
sphinxcontrib-htmlhelp==1.0.3
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==1.0.3
sphinxcontrib-serializinghtml==1.1.4
SQLAlchemy==1.3.19
SQLAlchemy-JSONField==0.9.0
SQLAlchemy-Utils==0.36.8
srsly==1.0.2
stevedore==2.0.0
tabulate==0.8.7
tenacity==4.12.0
tensorboardX==2.1
terminado==0.8.3
testfixtures==6.14.1
testpath==0.4.4
text-unidecode==1.3
thinc==7.4.1
threadpoolctl==2.1.0
thrift==0.13.0
tokenizers==0.8.1rc2
toml==0.10.0
tomlkit==0.7.0
torch==1.6.0
tornado==6.0.4
tqdm==4.49.0
traitlets==4.3.3
transformers==3.3.1
typed-ast==1.4.1
typing-extensions==3.7.4.2
tzlocal==1.5.1
unicodecsv==0.14.1
urllib3==1.25.9
wasabi==0.8.0
wcwidth==0.2.4
webencodings==0.5.1
wemake-python-styleguide==0.14.1
Werkzeug==0.16.1
widgetsnbextension==3.5.1
word2number==1.1
WTForms==2.3.3
xxhash==2.0.0
zipp==3.1.0
zope.deprecation==4.4.0
```

</p>
</details>


## Steps to reproduce
Train BART twice, once with distributed training on multiple GPUs and the other on a single GPU without distributed training and compare the metrics.json file.",,,,,,,,,1,
2009,https://github.com/allenai/allennlp/issues/4810,4810,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2020-11-22 15:15:12+00:00,,1,GradientDescentTrainer breaks when constructed with validation_data_loader==None and learning_rate_scheduler!=None,"<!--
Please fill this template entirely and do not erase any of it.
We reserve the right to close without a response bug reports which are incomplete.

If you have a question rather than a bug, please ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/allennlp) rather than posting an issue here.
-->

## Checklist

<!-- To check an item on the list replace [ ] with [x]. -->

- [x] I have verified that the issue exists against the `master` branch of AllenNLP.
- [x] I have read the relevant section in the [contribution guide](https://github.com/allenai/allennlp/blob/master/CONTRIBUTING.md#bug-fixes-and-new-features) on reporting bugs.
- [x] I have checked the [issues list](https://github.com/allenai/allennlp/issues) for similar or identical bug reports.
- [x] I have checked the [pull requests list](https://github.com/allenai/allennlp/pulls) for existing proposed fixes.
- [x] I have checked the [CHANGELOG](https://github.com/allenai/allennlp/blob/master/CHANGELOG.md) and the [commit log](https://github.com/allenai/allennlp/commits/master) to find out if the bug was already fixed in the master branch.
- [x] I have included in the ""Description"" section below a traceback from any exceptions related to this bug.
- [x] I have included in the ""Related issues or possible duplicates"" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
- [x] I have included in the ""Environment"" section below the name of the operating system and Python version that I was using when I discovered this bug.
- [x] I have included in the ""Environment"" section below the output of `pip freeze`.
- [x] I have included in the ""Steps to reproduce"" section below a minimally reproducible example.


## Description

When you construct GradientDescentTrainer with validation_data_loader==None and learning_rate_scheduler!=None, the code
breaks when an update step is performed for learning_rate_scheduler.  This is a typical case for training a Transformer model. 

This issue appeared since version 1.2.0 and is present now in the master branch.

<details>
<summary><b>Python traceback:</b></summary>
<p>

```
UnboundLocalErrorTraceback (most recent call last)
<ipython-input-9-513339770b41> in <module>
     46 
     47 try:
---> 48     metrics = trainer.train()
     49 except KeyboardInterrupt:
     50     pass

/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/training/trainer.py in train(self)
    964         """"""
    965         try:
--> 966             return self._try_train()
    967         finally:
    968             # make sure pending events are flushed to disk and files are closed properly

/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/allennlp/training/trainer.py in _try_train(self)
   1080             # if it doesn't, the validation metric passed here is ignored.
   1081             if self._learning_rate_scheduler:
-> 1082                 self._learning_rate_scheduler.step(this_epoch_val_metric)
   1083             if self._momentum_scheduler:
   1084                 self._momentum_scheduler.step(this_epoch_val_metric)

UnboundLocalError: local variable 'this_epoch_val_metric' referenced before assignment
```

The problem happens because the ```this_epoch_val_metric``` on the line 987 is not initialized.


<!-- Paste the traceback from any exception (if there was one) in between the next two lines below -->

</p>
</details>


## Related issues or possible duplicates

- None


## Environment

<!-- Provide the name of operating system below (e.g. OS X, Linux) -->
OS: Ubuntu 18.04.3 LTS

<!-- Provide the Python version you were using (e.g. 3.7.1) -->
Python version: 3.7.4

<details>
<summary><b>Output of <code>pip freeze</code>:</b></summary>
<p>

<!-- Paste the output of `pip freeze` in between the next two lines below -->
```
absl-py==0.8.1
actleto==0.1.0
alabaster==0.7.12
allennlp==1.2.2
allennlp-models==1.1.0
annoy==1.16.2
astor==0.8.0
astroid==2.3.3
astropy==3.2.3
atomicwrites==1.3.0
attrs==19.3.0
Babel==2.7.0
backcall==0.1.0
bleach==3.1.0
blis==0.2.4
bokeh==1.4.0
boto==2.49.0
boto3==1.16.6
botocore==1.19.6
bpemb==0.3.2
category-encoders==2.1.0
certifi==2019.9.11
chardet==3.0.4
Click==7.0
cloudpickle==1.2.2
colorama==0.4.1
confuse==1.0.0
conllu==4.1
cycler==0.10.0
cymem==2.0.3
Cython==0.29.14
dask==2.8.0
decorator==4.4.1
deeppavlov==0.6.1
defusedxml==0.6.0
Deprecated==1.2.10
distributed==2.8.0
docopt==0.6.2
docutils==0.15.2
editdistance==0.5.3
eli5==0.10.1
en-core-web-sm==2.1.0
entrypoints==0.3
fastcluster==1.1.25
fasttext==0.9.1
fb-re2==1.0.7
filelock==3.0.12
fitter==1.1.11
flair==0.6.1
flaky==3.6.1
Flask==1.1.1
Flask-Cors==3.0.8
forestci==0.3
fsspec==0.6.0
ftfy==5.6
future==0.18.2
gast==0.3.2
gdown==3.12.2
gensim==3.8.1
gevent==1.4.0
gitdb2==2.0.6
GitPython==3.0.5
google-pasta==0.1.8
graphviz==0.13.2
greenlet==0.4.15
grpcio==1.25.0
gym==0.15.4
h5py==2.10.0
hdbscan==0.8.23
HeapDict==1.0.1
htmlmin==0.1.12
hydra-core==0.11.3
hyperopt==0.2.5
idna==2.8
imageio==2.6.1
imagesize==1.1.0
imbalanced-learn==0.5.0
imgaug==0.3.0
importlib-metadata==0.23
ipykernel==5.1.3
ipython==7.9.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
isanlp==0.0.6
isort==4.3.21
itsdangerous==1.1.0
Janome==0.4.1
jedi==0.15.1
Jinja2==2.10.3
jmespath==0.9.4
joblib==0.14.0
json5==0.8.5
jsonnet==0.14.0
jsonpickle==0.9.6
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==5.3.4
jupyter-console==6.0.0
jupyter-contrib-core==0.3.3
jupyter-contrib-nbextensions==0.5.1
jupyter-core==4.6.1
jupyter-highlight-selected-word==0.2.0
jupyter-latex-envs==1.4.6
jupyter-nbextensions-configurator==0.4.1
jupyterlab==1.2.3
jupyterlab-server==1.0.6
Keras==2.3.1
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
keras-vis==0.4.1
kiwisolver==1.1.0
konoha==4.6.2
langdetect==1.0.8
lazy-object-proxy==1.4.3
libact==0.1.3b0
lightgbm==2.3.1
lime==0.1.1.36
line-profiler==2.1.1
llvmlite==0.30.0
locket==0.2.0
lxml==4.4.1
Markdown==3.1.1
MarkupSafe==1.1.1
matplotlib==3.1.1
mccabe==0.6.1
missingno==0.4.2
mistune==0.8.4
mlxtend==0.17.0
more-itertools==7.2.0
mpld3==0.3
msgpack==0.6.2
munch==2.5.0
murmurhash==1.0.2
nbconvert==5.6.1
nbformat==4.4.0
networkx==2.4
nltk==3.4.5
nmslib==2.0.5
nose==1.3.7
notebook==6.0.2
numba==0.46.0
numexpr==2.7.0
numpy==1.17.4
numpydoc==0.9.1
omegaconf==1.4.1
opencv-python==4.1.1.26
opencv-python-headless==4.1.1.26
overrides==3.1.0
packaging==19.2
pandas==0.25.3
pandas-profiling==2.3.0
pandocfilters==1.4.2
parsimonious==0.8.1
parso==0.5.1
partd==1.0.0
patool==1.12
patsy==0.5.1
pexpect==4.7.0
phik==0.9.8
pickleshare==0.7.5
Pillow==6.2.1
plac==0.9.6
plotly==4.3.0
pluggy==0.13.0
pprofile==2.0.2
preshed==2.0.1
progressbar==2.5
prometheus-client==0.7.1
prompt-toolkit==2.0.10
protobuf==3.10.0
psutil==5.6.5
ptyprocess==0.6.0
py==1.8.0
py-cpuinfo==5.0.0
py-rouge==1.1
pybind11==2.4.dev4
pydot==1.4.1
pyglet==1.3.2
Pygments==2.4.2
pylint==2.4.4
pymystem3==0.2.0
pyparsing==2.4.5
pyrsistent==0.15.5
pytest==5.2.4
pytest-pylint==0.14.1
python-crfsuite==0.9.7
python-dateutil==2.8.0
pytorch-pretrained-bert==0.6.2
pytorch-transformers==1.1.0
pytz==2019.3
PyWavelets==1.1.1
PyYAML==5.1.2
pyzmq==18.1.1
qtconsole==4.6.0
regex==2019.11.1
requests==2.22.0
responses==0.10.6
retrying==1.3.3
s3transfer==0.3.3
sacred==0.8.0
sacremoses==0.0.35
scikit-image==0.16.2
scikit-learn==0.21.3
scipy==1.3.2
seaborn==0.9.0
segtok==1.5.10
Send2Trash==1.5.0
sentence-transformers==0.3.8
sentencepiece==0.1.91
seqeval==1.2.2
Shapely==1.6.4.post2
sharedmem==0.3.7
six==1.13.0
sklearn==0.0
sklearn-crfsuite==0.3.6
skorch==0.6.0
smart-open==1.9.0
smmap2==2.0.5
snowballstemmer==2.0.0
sortedcontainers==2.1.0
spacy==2.1.9
Sphinx==2.2.1
sphinxcontrib-applehelp==1.0.1
sphinxcontrib-devhelp==1.0.1
sphinxcontrib-htmlhelp==1.0.2
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==1.0.2
sphinxcontrib-serializinghtml==1.1.3
sqlitedict==1.7.0
sqlparse==0.3.0
srsly==0.2.0
statsmodels==0.10.1
tables==3.6.1
tabulate==0.8.6
tblib==1.5.0
tensorboard==1.14.0
tensorboardX==1.9
tensorflow-estimator==1.14.0
tensorflow-gpu==1.14.0
termcolor==1.1.0
terminado==0.8.3
testpath==0.4.4
thinc==7.0.8
tokenizers==0.9.3
toolz==0.10.0
torch==1.6.0
torchvision==0.4.2
tornado==6.0.3
tqdm==4.38.0
traitlets==4.3.3
transformers==3.5.1
typed-ast==1.4.0
ujson==1.35
Unidecode==1.1.1
urllib3==1.25.7
wasabi==0.4.0
wcwidth==0.1.7
webencodings==0.5.1
Werkzeug==0.16.0
widgetsnbextension==3.5.1
word2number==1.1
wrapt==1.11.2
xgboost==0.90
zict==1.0.0
zipp==0.6.0
```

</p>
</details>


## Steps to reproduce


<details>
<summary><b>Example source:</b></summary>
<p>

<!-- Add a fully runnable example in between the next two lines below that will reproduce the bug -->
Can be replicated in Colab: https://colab.research.google.com/drive/1w3lhUG1zvvx8XHFcEP_921KGdZc_Ie8Y?usp=sharing

</p>
</details>
",,,,,,,,,1,
2100,https://github.com/allenai/allennlp/issues/5024,5024,"[{'id': 887719346, 'node_id': 'MDU6TGFiZWw4ODc3MTkzNDY=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Contributions%20welcome', 'name': 'Contributions welcome', 'color': '02b8d1', 'default': False, 'description': ''}, {'id': 1875662321, 'node_id': 'MDU6TGFiZWwxODc1NjYyMzIx', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Feature%20request', 'name': 'Feature request', 'color': '5558ba', 'default': False, 'description': ''}]",open,2021-02-26 16:51:47+00:00,,4,A google colab for the guide,"**Is your feature request related to a problem? Please describe.**
(Continuing the discussion from here https://github.com/allenai/allennlp/issues/5017) 
- While going through the guide, I felt that it would be a great addition if a google colab notebook can be created. 
- As one begins to go through a new ML library / framework, it is very helpful to run the code side by side as going through the guide. 
- A lot of people are in general more acquainted with Jupyter notebooks, and thus a simple Jupyter notebook can really onboard the new users very fast.
- We can also add content in the `markdown` cells, so one can learn while going through it!

**Describe the solution you'd like**
Creation of a Colab for the guide.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
",,,,,,,,,1,
2132,https://github.com/allenai/allennlp/issues/5081,5081,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2021-03-31 09:54:26+00:00,,1,FeedForward not pickable,"## Checklist

<!-- To check an item on the list replace [ ] with [x]. -->

- [x] I have verified that the issue exists against the `master` branch of AllenNLP.
- [x] I have read the relevant section in the [contribution guide](https://github.com/allenai/allennlp/blob/master/CONTRIBUTING.md#bug-fixes-and-new-features) on reporting bugs.
- [x] I have checked the [issues list](https://github.com/allenai/allennlp/issues) for similar or identical bug reports.
- [x] I have checked the [pull requests list](https://github.com/allenai/allennlp/pulls) for existing proposed fixes.
- [x] I have checked the [CHANGELOG](https://github.com/allenai/allennlp/blob/master/CHANGELOG.md) and the [commit log](https://github.com/allenai/allennlp/commits/master) to find out if the bug was already fixed in the master branch.
- [x] I have included in the ""Description"" section below a traceback from any exceptions related to this bug.
- [x] I have included in the ""Related issues or possible duplicates"" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
- [x] I have included in the ""Environment"" section below the name of the operating system and Python version that I was using when I discovered this bug.
- [x] I have included in the ""Environment"" section below the output of `pip freeze`.
- [x] I have included in the ""Steps to reproduce"" section below a minimally reproducible example.


## Description

Hi, I'm unable to use multiprocessing code that is using a Predictor having a FeedForward component. The worker is not able to copy the object with error: `AttributeError: Can't pickle local object '<lambda>.<locals>.<lambda>'`. I've also reported a runnable script that allows you to replicate the error.

## Related issues or possible duplicates

- I believe this could be a problem to what the `Lazy` class previously had. It doesn't look the `FeedForward` class has any lambda function in it though.


## Environment

<!-- Provide the name of operating system below (e.g. OS X, Linux) -->
OS: MacOS

<!-- Provide the Python version you were using (e.g. 3.7.1) -->
Python version: 3.8

<details>
<summary><b>Output of <code>pip freeze</code>:</b></summary>
<p>

<!-- Paste the output of `pip freeze` in between the next two lines below -->
```
absl-py==0.12.0
addict==2.4.0
ai2thor==2.1.0
aiohttp==3.7.4.post0
allennlp @ git+https://github.com/allenai/allennlp@4baf19ab7b3aeae9f1c47700d4ac71616af0316e
appnope==0.1.2
argon2-cffi==20.1.0
async-generator==1.10
async-timeout==3.0.1
attrs==20.3.0
aws-requests-auth==0.4.3
backcall==0.2.0
bleach==3.3.0
blis==0.7.4
boto3==1.17.31
botocore==1.20.31
cachetools==4.2.1
catalogue==2.0.1
certifi==2020.12.5
cffi==1.14.5
chardet==4.0.0
click==7.1.2
configparser==5.0.2
cycler==0.10.0
cymem==2.0.5
decorator==4.4.2
defusedxml==0.7.1
docker-pycreds==0.4.0
entrypoints==0.3
filelock==3.0.12
Flask==1.1.2
fsspec==0.8.7
future==0.18.2
futures==3.1.1
gitdb==4.0.7
GitPython==3.1.14
google-auth==1.28.0
google-auth-oauthlib==0.4.3
grpcio==1.36.1
h5py==3.2.1
idna==2.10
iniconfig==1.1.1
ipykernel==5.5.0
ipython==7.21.0
ipython-genutils==0.2.0
ipywidgets==7.6.3
itsdangerous==1.1.0
jedi==0.18.0
Jinja2==2.11.3
jmespath==0.10.0
joblib==1.0.1
jsonnet==0.17.0
jsonpickle==2.0.0
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==6.1.12
jupyter-console==6.3.0
jupyter-core==4.7.1
jupyterlab-pygments==0.1.2
jupyterlab-widgets==1.0.0
kiwisolver==1.3.1
lmdb==1.1.1
Markdown==3.3.4
MarkupSafe==1.1.1
matplotlib==3.3.4
mistune==0.8.4
mkl-fft==1.3.0
mkl-random==1.1.1
mkl-service==2.3.0
more-itertools==8.7.0
msgpack==1.0.2
multidict==5.1.0
murmurhash==1.0.5
nbclient==0.5.3
nbconvert==6.0.7
nbformat==5.1.2
nest-asyncio==1.5.1
networkx==2.5
nltk==3.5
notebook==6.3.0
numpy @ file:///opt/concourse/worker/volumes/live/5572694e-967a-4c0c-52cf-b53d43e72de9/volume/numpy_and_numpy_base_1603491881791/work
oauthlib==3.1.0
olefile==0.46
open3d==0.12.0
opencv-python==4.5.1.48
overrides==3.1.0
packaging==20.9
pandas==1.2.3
pandocfilters==1.4.3
parso==0.8.1
pathtools==0.1.2
pathy==0.4.0
pexpect==4.8.0
pickleshare==0.7.5
Pillow @ file:///opt/concourse/worker/volumes/live/b6dec6d8-7e9d-4d46-615c-691883c22eae/volume/pillow_1615057391629/work
pluggy==0.13.1
plyfile==0.7.3
preshed==3.0.5
progressbar2==3.53.1
prometheus-client==0.9.0
promise==2.3
prompt-toolkit==3.0.17
protobuf==3.15.6
psutil==5.8.0
ptyprocess==0.7.0
py==1.10.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycparser==2.20
pydantic==1.7.3
Pygments==2.8.1
pyparsing==2.4.7
pyrsistent==0.17.3
pytest==6.2.2
python-dateutil==2.8.1
python-utils==2.5.6
pytorch-lightning==1.2.4
pytz==2021.1
PyYAML==5.3.1
pyzmq==22.0.3
qtconsole==5.0.3
QtPy==1.9.0
regex==2021.3.17
requests==2.25.1
requests-oauthlib==1.3.0
rsa==4.7.2
s3transfer==0.3.5
sacremoses==0.0.43
scikit-learn==0.24.1
scipy==1.6.1
Send2Trash==1.5.0
sentencepiece==0.1.95
sentry-sdk==1.0.0
shortuuid==1.0.1
six @ file:///opt/concourse/worker/volumes/live/5b31cb27-1e37-4ca5-6e9f-86246eb206d2/volume/six_1605205320872/work
sklearn==0.0
smart-open==3.0.0
smmap==4.0.0
spacy==3.0.5
spacy-legacy==3.0.1
srsly==2.4.0
subprocess32==3.5.4
tensorboard==2.4.1
tensorboard-plugin-wit==1.8.0
tensorboardX==2.1
terminado==0.9.3
testpath==0.4.4
thinc==8.0.2
threadpoolctl==2.1.0
tokenizers==0.10.1
toml==0.10.2
torch==1.7.1
torchaudio==0.8.0a0+a751e1d
torchvision==0.8.2
tornado==6.1
tqdm==4.59.0
traitlets==5.0.5
transformers==4.3.3
typer==0.3.2
typing-extensions @ file:///home/ktietz/src/ci_mi/typing_extensions_1612808209620/work
urllib3==1.26.4
wandb==0.10.24
wasabi==0.8.2
wcwidth==0.2.5
webencodings==0.5.1
Werkzeug==1.0.1
widgetsnbextension==3.5.1
yarl==1.6.3

```

</p>
</details>


## Steps to reproduce


<!-- Add a fully runnable example in between the next two lines below that will reproduce the bug -->
```
import torch.multiprocessing as mp
from allennlp.common import Params
from allennlp.modules import FeedForward


def run(model):
    pass


def main():
    model = FeedForward.from_params(Params({
        ""input_dim"": 768,
        ""hidden_dims"": [13],
        ""num_layers"": 1,
        ""activations"": [""linear""],
        ""dropout"": [0.0]
    }))

    model.share_memory()

    threads = []

    for n in range(4):
        thread = mp.Process(target=run, args=[model])
        thread.start()
        threads.append(thread)

    for t in threads:
        t.join()


if __name__ == ""__main__"":
    main()

```

</p>
</details>
",,,,,,,,,1,
2137,https://github.com/allenai/allennlp/issues/5094,5094,"[{'id': 2763124639, 'node_id': 'MDU6TGFiZWwyNzYzMTI0NjM5', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/stale', 'name': 'stale', 'color': 'ededed', 'default': False, 'description': None}]",closed,2021-04-04 12:31:39+00:00,,10,Cannot import allennlp into jupyter,"Hi,
I am trying to learn AllenNLP, I have read a few tutorials (would love additional recs) and am ready to start coding. 
For some reason I am not able to import allennlp - before any of the libraries (which won't import either, naturally).  
I have pip installed it via the terminal (on a mac), and also tried using anaconda, with no success.  
Any ideas? ",,,,,,,,,1,
155,https://github.com/allenai/allennlp/issues/1007,1007,[],closed,2018-03-21 05:28:02+00:00,,2,Is transfer learning possible using pretrain Machine Comprehension model?,"I want to add some more data, similar format as SQuAD to already trained model ""https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz"". Same way pre-trained VGG Model are frequently used to Classify Objects in Photographs using transfer learning? Is this doable? Will it help me get a better answer to the type of passage and answer, I might have in the specific field? Or is it same because at end of the day its just passage and answer?",,1,0,0,1,0,0,0,0,
143,https://github.com/allenai/allennlp/issues/977,977,[],closed,2018-03-13 07:11:19+00:00,,3,[Feature] seq2seq dataset reader should support maximum sequence length,"I think in practical, the source/target sequence to seq2seq model should be truncated to the maximum sequence length to avoid the OOM problem, but the current seq2seq dataset reader seems not to support this configuration.",,,,,,,,1,,
281,https://github.com/allenai/allennlp/issues/1318,1318,[],closed,2018-05-31 20:17:37+00:00,,3,Importing AllenNLP gives limited and confusing logging,"I noticed this when I moved serving the demo to its own main method.  Here's the logging output I got when I ran it:

```
python -m allennlp.service.server_flask                                                                                                                                                                                                (allennlp) 
/Users/michael/miniconda3/envs/allennlp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
100%|鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻坾 46175392/46175392 [00:44<00:00, 1038030.23B/s]
/Users/michael/miniconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  ""num_layers={}"".format(dropout, num_layers))
100%|鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻坾 54204041/54204041 [00:46<00:00, 1171213.91B/s]
Did not use initialization regex that was passed: encoder\\\\._module.layer_0\\\\.input_linearity\\\\.weight
Did not use initialization regex that was passed: encoder\\\\._module.layer_.*bias
Did not use initialization regex that was passed: encoder\\\\._module\\\\.layer_0\\\\.state_linearity\\\\.weight
100%|鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻坾 697657697/697657697 [11:37<00:00, 1000410.58B/s]
WARNING:allennlp.nn.initializers:Did not use initialization regex that was passed: .*token_embedder_tokens\\._projection.*weight
100%|鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅| 59248617/59248617 [01:10<00:00, 841949.24B/s]
100%|鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻坾 711852086/711852086 [11:19<00:00, 1047839.38B/s]
100%|鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻坾 710802195/710802195 [11:18<00:00, 1048347.16B/s]
```

I was pretty confused, and I thought that my process eventually froze.  But really I just couldn't see it telling me my server was available on port 8000 because that was being logged at the INFO level.

I think users who import `allennlp` may run into similar issues.",,,,,,,,1,,
442,https://github.com/allenai/allennlp/issues/1701,1701,[],closed,2018-08-31 03:09:55+00:00,,1,Custom HighwayLSTM kernel might need long type lengths,"When running the highway lstm cuda kernel tests, I get:

```
_______ TestCustomHighwayLSTM.test_validation_forward_pass_is_deterministic_in_model_with_dropout _______

self = <alternating_highway_lstm_test.TestCustomHighwayLSTM testMethod=test_validation_forward_pass_is_deterministic_in_model_with_dropout>

    def test_validation_forward_pass_is_deterministic_in_model_with_dropout(self):

        _, model, _, model_input, lengths = self.get_models_and_inputs(5, 3, 11, 2, 5, dropout_prob=0.5)
        model.eval()
        model_input = pack_padded_sequence(model_input, lengths, batch_first=True)
>       output, _ = model(model_input)

allennlp/tests/custom_extensions/alternating_highway_lstm_test.py:28:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../../../miniconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/module.py:477: in __call__
    result = self.forward(*input, **kwargs)
allennlp/modules/alternating_highway_lstm.py:252: in forward
    memory_accumulator, dropout_weights, lengths_variable, gates)
allennlp/modules/alternating_highway_lstm.py:50: in forward
    is_training)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<built-in method highway_lstm_forward_cuda of CompiledLib object at 0x7f025309b778>, 3, 11, 5, 2, 5, ...)
kwargs = {}

    @wraps(function)
    def safe_call(*args, **kwargs):
        args = tuple(ffi.cast(_torch_to_cffi.get(arg.type(), 'void') + '*', arg._cdata)
                     if isinstance(arg, torch.Tensor) or torch.is_storage(arg)
                     else arg
                     for arg in args)
        args = (function,) + args
>       result = torch._C._safe_call(*args, **kwargs)
E       TypeError: initializer for ctype 'struct THIntTensor *' must be a pointer to same type, not cdata 'struct THLongTensor *'

../../../miniconda3/envs/allennlp/lib/python3.6/site-packages/torch/utils/ffi/__init__.py:202: TypeError
```

It looks like changing the `lengths` from type THIntTensor to THLongTensor causes it to run, but then it doesn't match the baseline. Perhaps a hint for fixing it, though? Would look further into it but I don't really know CUDA / C++...

cc: @DeNeutoy ",,,,,,,,1,,
598,https://github.com/allenai/allennlp/issues/2020,2020,[],closed,2018-11-07 00:27:21+00:00,,7,Length mismatch after openai text_field_embedder,"**Describe the bug**
Hi, I open this new issue to provide details.
I am implementing [OpenaiGPT for classification task](https://github.com/tingkai-zhang/Generative-Pre-trained-Transformer) using allennlp.

The output shape of `text field embedder` doesn't match with the input.

After text of shape [batch_size, num_timestamp, num_embedding] is forward after [text_filed_embedder](). The number of timestamps shrinks, which differs from the original GPT pytorch implementation.

I check the source code, seems that it returns [batch_size, seq_length, num_embedding].

And the problem comes that when i [add a classifier head](https://github.com/tingkai-zhang/Generative-Pre-trained-Transformer/blob/33e837355973a47db0d2dcf3bf0391aab03c338b/my_library/modules/Heads/ClfHead.py#L18) on top of transformer embedder.
It requires the `h` which is the  output of `text field embedder` has the same number of timestamps as the input


**To Reproduce**

[the project repo](https://github.com/tingkai-zhang/Generative-Pre-trained-Transformer)

The lines of code uses [text field embedder](https://github.com/tingkai-zhang/Generative-Pre-trained-Transformer/blob/33e837355973a47db0d2dcf3bf0391aab03c338b/my_library/models/GPT.py#L55)

[Classifier head](https://github.com/tingkai-zhang/Generative-Pre-trained-Transformer/blob/33e837355973a47db0d2dcf3bf0391aab03c338b/my_library/modules/Heads/ClfHead.py#L18)


**System (please complete the following information):**
 - OS:  Linux
 - Python version:  3.6
 - AllenNLP version:  v0.7.1

**Additional context**

",,,,,,,,1,,
642,https://github.com/allenai/allennlp/issues/2089,2089,[],closed,2018-11-22 13:43:20+00:00,,3,EncoderBase sort_and_run_forward crashes with all-zero length batch,"**Describe the bug**
Seq2Vec encoding does not work when the current batch of sequences contains only sequences of length 0.

**To Reproduce**
Try to `forward` any batch tensor `(bt, seq, x)` of samples through `PytorchSeq2VecWrapper` with a padding mask of all zeros in shape `(bt, seq)`

**Expected behavior**
I expect the computation to succeed (pretty much just returning the inputs)

It does work when only _some_ of the input sequences have zero length. Only iff _all_ sequences are empty, it crashes.

**System (please complete the following information):**
 - OS: Linux -- Ubuntu 18.04
 - Python version: 3.6.6 / Anaconda
 - AllenNLP version: 0.7.1
 - PyTorch version: Not installed myself

**Additional context**
My research involves classification of dependency tree nodes, which may have any number of children (including 0 children) attached to them. Depending on the batch size, an individual batch may end up encoding only nodes without any children, thus all lengths are zero.
I currently ""hotwire"" your `Seq2Vec` encoding whenever I detect this, but the interface would be much cleaner if your library could just handle this edge case anyway.

Keep up the good work!",,,,,,,,1,,
749,https://github.com/allenai/allennlp/issues/2287,2287,[],closed,2019-01-06 06:59:38+00:00,,1,[Suggestion] Warn default value of `min_padding_length` when using `TokenCharactersIndexer`,"**Is your feature request related to a problem? Please describe.**
This issue is related to #1954, #2210 and #2044.

**Describe the solution you'd like**
I think it is a better idea to provide _no_ default value of `min_padding_length` in this method, but due to backward compatibility, its default value of `0` is provided.
It may be more friendly to warn users when they use default_value unconsciously.
Does it make sense?

**Additional context**
I am willing to create PR about this.
",,,,,,,,1,,
860,https://github.com/allenai/allennlp/issues/2512,2512,[],closed,2019-02-13 20:55:21+00:00,,1,Maximum Context Length in QuAC,"**Question**
 
https://github.com/allenai/allennlp/blob/9f87aa56177ef3dc1b0c58ebb49d15584c500e29/allennlp/data/dataset_readers/reading_comprehension/util.py#L325

Does this mean in QuACDataReader, the maximum context length is 3?",,,,,,,,1,,
864,https://github.com/allenai/allennlp/issues/2518,2518,[],closed,2019-02-16 14:21:41+00:00,,5,Unexpected encoded sequences' lengths (lack of encoding specification for argparse),"**Describe the bug**
Running the ""allennlp elmo"" command might produce sequences of length different than the original intended one. The issue lies without not being able to specify an encoding type with which to read the input file, which will have the argparse.FileType('r') reader interpret some unicode characters as 0 length white spaces.

**To Reproduce**
Consider a file with the following tokenized (and space separated) input sentences (where the third sentence follows the expected behaviour):

""SUBJECT_ENTITY ( Croatian : zlo膷ina ) is a film released in OBJECT_ENTITY in 1981 , directed by Fabijan 艩ovagovi膰 .""
""SUBJECT_ENTITY ( Somali : Yuusuf , Arabic : 賲丨賲丿 賷賵爻賮 毓亘丿賷鈥庘€?) was the OBJECT_ENTITY .""
""SUBJECT_ENTITY was launched at 15:36 UTC on OBJECT_ENTITY .""

The sentences have a sequence length (separated on ' ') of, respectively:
20
16
9

Running the command ""allennlp elmo file_name file_name_out"" will however produce encoded sentences of lengths, respectively:
21
18
9

The problem can be traced to the lack of proper encoding in line 100 of commands/elmo.py:
subparser.add_argument('input_file', type=argparse.FileType('r'), help='The path to the input file.')

In this case, simply replacing , type=argparse.FileType('r') with type=argparse.FileType('r', encoding='utf-8') fixes the issue.

**Expected behavior**
Expected encoddings with sequence lengths of, respectively:
20
16
9


**System (please complete the following information):**
 - OS: tested on both Windows 10 Pro and Debian GNU/Linux 9 (stretch)
 - Python version: 3.6.5 Anaconda on both systems
 - AllenNLP version: 0.8.1 on both systems
 - PyTorch version: Windows (1.0.0 CPU) / Linux (1.0.1 GPU)
",,,,,,,,1,,
942,https://github.com/allenai/allennlp/issues/2659,2659,[],closed,2019-03-28 16:00:28+00:00,,1,How to pass source length to elmo ,"**System (please complete the following information):**
 - OS: [e.g. OSX, Linux]  Linux 
 - Python version: [e.g. 3.6.1] 3.6.8
 - AllenNLP version: [e.g. v0.8.2, or ""I installed from master""] 0.8.2
 - PyTorch version: (if you installed it yourself) 1.0.1.post2

**Question**

I am following [this](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md#using-elmo-as-a-pytorch-module-to-train-a-new-model) to use elmo embeddings in my pytorch code (not based on allennlp). 

I have a dataset class and try to calculate the elmo embedding in the `__getitem__` method. An excerpt from the code is given below. I am working on a dialogue dataset where I have multiple round of questions. I am using `batch_to_ids` to first convert my token list to character ids and then passing it through `Elmo` module.

* How can I pass max source length to Elmo class so that it can pad (or reduce the sequence size) automatically? The example does not say that I have to pad my sequences beforehand.  

* Is it a good practice to have elmo embeddings in the dataset class and pass it through the iterator or should I use it in the model when I have the embedding layer? The former has the advantage that I can calculate the elmo embedding right before I convert it to tensor using a vocabulary. 

```
from allennlp.modules.elmo import Elmo, batch_to_ids

config ={
options_file: ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json""
weight_file: ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5""
}
class RandomDataset(Dataset):
    def __init__(self):
        ...
        ...

        self.elmo = Elmo(config[""options_file""], config[""weight_file""],
                 2, dropout=config[""elmo_dropout""])


    def __getitem__(self, index):
        ...
        ...

        ques_embeddings = []
        for i in range(len(dialog)):
            ques_char_ids = batch_to_ids(dialog[i][""question""])
            ques_embeddings.append(self.elmo(ques_char_ids)['elmo_representations'][0])
            
            # Here I convert the tokens to tensor
            dialog[i][""question""] = self.vocabulary.to_indices(dialog[i][""question""])
            dialog[i][""question""] = dialog[i][""question""][: self.config[""max_sequence_length""] - 1]
        
        ques_embeddings = torch.stack(ques_embeddings,0)
```
 
I am getting the following error which is reasonable as the questions can have different lengths.
How can I pass this `config[""max_sequence_length""]` to elmo embedder? 
```
RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 27 and 21 in dimension 2 at /pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1307
```


",,,,,,,,1,,
948,https://github.com/allenai/allennlp/issues/2668,2668,[],closed,2019-03-29 18:56:53+00:00,,1,Mask and Tokens lengths do not match,"**System:**
* OS: OSX
* Python version: 3.6.5
* AllenNLP version: 0.8.1
* PyTorch version: 0.4.1

**Question**

I have a dataset reader with BertIndexer and its tokenizer.

```python
token_indexer = PretrainedBertIndexer(""bert-base-uncased"")
def tokenizer(s):
    return [Token(t) for t in token_indexer.wordpiece_tokenizer(s)]
reader = MyReader(token_indexers={""tokens"": token_indexer}, tokenizer=tokenizer)
```

On converting an instance field to tensors using `as_tensor_dict()` the `mask` does not include values for `start_tokens` and `end_tokens`.   See https://github.com/allenai/allennlp/blob/0abefe2566c843af063dd0e4608002321cc4aa28/allennlp/data/token_indexers/wordpiece_indexer.py#L177

`field[""tokens""].shape - field[""mask""].shape` is 2.

Is this supposed to work like this? This seems to be counter intuitive when the mask shape needs to match with data shape at the time of passing to neural network.",,,,,,,,1,,
977,https://github.com/allenai/allennlp/issues/2715,2715,[],closed,2019-04-14 11:31:15+00:00,,2,Shared padding length between instances of TokenCharactersIndexer with same vocabulary namespace,"**Describe the bug**
In my model I'd like to split each word into characters in two ways. For example:
* the -> [""t"", ""h"", ""e""]  (to create word embedding by processing character sequence with CNN)
* the -> [START, ""t"", ""h"", ""e"", END] (to train char-level language model on words)

For that purpose I create two separate `TokenCharactersIndexer`s in my `DatasetReader` (note that they share the same vocab namespace). I expect that padded tensors obtained from them should have different sizes (second way requires two more indices for every word), but they are the same - basically additional zeros are appended to the first tensor. Is that behavior intended? And, if so, how to handle my use case properly?

Thank you!

**To Reproduce**
Run the code
```
from allennlp.data import DatasetReader, Instance, Token, Vocabulary
from allennlp.data.dataset import Batch
from allennlp.data.fields import TextField
from allennlp.data.token_indexers import TokenCharactersIndexer
from allennlp.data.tokenizers import CharacterTokenizer


class SimpleReader(DatasetReader):
    def __init__(self):
        super().__init__()
        self.indexers = {
            ""char_indexer"": TokenCharactersIndexer(),
            ""char_indexer_with_start_end"": TokenCharactersIndexer(
                character_tokenizer=CharacterTokenizer(
                    start_tokens=[""START""], end_tokens=[""END""]
                )
            ),
        }

    def text_to_instance(self, text) -> Instance:
        return Instance({""source"": TextField([Token(text)], self.indexers)})


if __name__ == ""__main__"":
    text = ""the""
    instance = SimpleReader().text_to_instance(text)
    vocab = Vocabulary.from_instances([instance])
    batch = Batch([instance])
    batch.index_instances(vocab)
    print(batch.as_tensor_dict()[""source""])

```

**Expected behavior**
I expected to see

`{'char_indexer': tensor([[[2, 3, 4]]]), 'char_indexer_with_start_end': tensor([[[5, 2, 3, 4, 6]]])}`

But got

`{'char_indexer': tensor([[[2, 3, 4, 0, 0]]]), 'char_indexer_with_start_end': tensor([[[5, 2, 3, 4, 6]]])}`

**System (please complete the following information):**
 - AllenNLP version: 0.8.3
",,,,,,,,1,,
1168,https://github.com/allenai/allennlp/issues/3039,3039,[],closed,2019-07-07 08:55:14+00:00,,2,"BidirectionalEndpointSpanExtractor.forward ""exclusive_span_ends > sequence_lengths.unsqueeze(-1)"" seems bug.","**Describe the bug**
BidirectionalEndpointSpanExtractor.forward 

```
 if (exclusive_span_starts < 0).any() or (exclusive_span_ends > sequence_lengths.unsqueeze(-1)).any():
            raise ValueError(f""Adjusted span indices must lie inside the length of the sequence tensor, ""
                             f""but found: exclusive_span_starts: {exclusive_span_starts}, ""
                             f""exclusive_span_ends: {exclusive_span_ends} for a sequence tensor with lengths ""
                             f""{sequence_lengths}."")
```

```(exclusive_span_ends > sequence_lengths.unsqueeze(-1)``` should be fixed as ```(exclusive_span_ends >= sequence_lengths.unsqueeze(-1)```.  Because, when ```exclusive_span_ends == sequence_lengths``` will be out of boundary.

",,,,,,,,1,,
1202,https://github.com/allenai/allennlp/issues/3099,3099,[],closed,2019-07-26 15:30:05+00:00,,4,Limit dataset size,"Hi, first I'd like to say many thanks for this great project, it surely helps language researchers and engineers through the world

I explored the project code and could not find way to read a subset of the data through `DatasetReader`. I think it could be useful for the purpose of development phase where we usually don't want to load the entire dataset. Worth noting there's a parameter in the iterator class `DataIterator.instances_per_epoch` which limits the number of samples per epoch, however as I understand it's called after loading the entire dataset to memory

I searched a bit and found there's a [solution](http://docs.python.org/2/library/itertools.html#itertools.takewhile) to limit python generators via its `itertools` module. Thought about wrapping the call to the generic method `DatasetReader._read()` in `DatasetReader.read()`, let me please know what you think.

In any case it's pretty easy to bypass this issue by manually creating a folder which contains subset of the data, but I think this way is more convenient. 
",,,,,,,,1,,
1484,https://github.com/allenai/allennlp/issues/3599,3599,"[{'id': 887719346, 'node_id': 'MDU6TGFiZWw4ODc3MTkzNDY=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Contributions%20welcome', 'name': 'Contributions welcome', 'color': '02b8d1', 'default': False, 'description': ''}]",closed,2020-01-10 07:07:38+00:00,,2,`LinearAttention` fails to give correct results when matrix length=1,"**Describe the bug**
In `linear_attention._forward_internal`, this line:
`return self._activation(combined_tensors.squeeze(1) + self._bias)`.
It seems `squeeze(1)` here is incorrect.
Because if the `matrix` has the shape `(batch_size, length, encoding_dim)`, the `combined_tensors` will have the shape of `(batch_size, length)`.
If the length>1, the `squeeze` operation is redundant, while the length is 1, the dim of length will be squeezed, which causes the error.
So I don't figure out why there is a strange `squeeze` operation.

**To Reproduce**
```
from allennlp.modules.attention import LinearAttention
attention=LinearAttention(tensor_1_dim=2,tensor_2_dim=2)
vector=torch.Tensor(2,2)
matrix=torch.Tensor(2,1,2)
# shape: (2,1)
mask=torch.tensor([[1],[1]])
# Expected shape (2, 1), but got (2,)
weights=attention(vector,matrix,mask)
```
It's OK to use AdditiveAttention

**System (please complete the following information):**
I installed from master

",,,,,,,,1,,
1652,https://github.com/allenai/allennlp/issues/4035,4035,[],closed,2020-04-07 00:49:24+00:00,,4,Cases where the length of the IterableDataset is not overriden properly,"```_LazyInstances``` (allennlp's subclass of ```IterableDataset```) returns ```1``` when the ```__len__``` method is not overridden.  
This, for example, causes a problem with ```SlantedTriangular``` which expects the length of the dataset in batches as constructor parameter if used programmatically.

See https://github.com/allenai/allennlp/pull/4028#issuecomment-610098163 and https://github.com/allenai/allennlp/pull/4028#issuecomment-610109566  for more context.
",,,,,,,,1,,
1780,https://github.com/allenai/allennlp/issues/4324,4324,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2020-06-05 08:33:10+00:00,,1,Upper limit on dependencies versions,"Similar issues:

- #2650

I was wondering why do you quite often limit the upper version of your dependencies. See below a list of all examples from the current `setup.py`

- `""torch>=1.5.0,<1.6.0""`
- `""spacy>=2.1.0,<2.3""`
- `""transformers>=2.9,<2.12""`
- `""filelock>=3.0,<3.1""`

Are you just afraid that the future versions of these packages are going to break `allennlp`? Or you already somehow know they will be incompatible?

In my case, I am encountering a lot of issues because of this strategy. I want to use an older version `allennlp==0.9.0` however there you assert `'spacy>=2.1.0,<2.2'`. My other dependencies, however, require more recent `spacy>2.2`.

Thanks for your response!",,,,,,,,1,,
2146,https://github.com/allenai/allennlp/issues/5113,5113,"[{'id': 887719346, 'node_id': 'MDU6TGFiZWw4ODc3MTkzNDY=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Contributions%20welcome', 'name': 'Contributions welcome', 'color': '02b8d1', 'default': False, 'description': ''}, {'id': 1875662321, 'node_id': 'MDU6TGFiZWwxODc1NjYyMzIx', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Feature%20request', 'name': 'Feature request', 'color': '5558ba', 'default': False, 'description': ''}]",closed,2021-04-12 12:24:41+00:00,,1,Length penalty for nn.beam_search.BeamSearch,"I see that in `nn.beam_search.BeamSearch` there is no way to specify the length penalty. How is implemented now? Is it using length penalty or not?

I imagine that the desiderata should be to use a simple penalty but I cannot understand from the code if that is the case.
",,,,,,,,1,,
2198,https://github.com/allenai/allennlp/issues/5254,5254,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2021-06-11 07:20:24+00:00,,5,pretrained_transformer_indexer sets token_ids and mask to different lengths,"In `pretrained_transformer_indexer` the method `tokens_to_indices` adds `token_ids`, `mask` and `type_ids` to the output dict.

https://github.com/allenai/allennlp/blob/a6cfb1221520fca7a5cc55bef001c6a79a6a3e2f/allennlp/data/token_indexers/pretrained_transformer_indexer.py#L94

This is then passed to `_postprocess_output` which potentially resizes token_ids and type_ids (i.e. # Strips original special tokens), but it returns `segment_concat_mask` instead of `mask`. `mask` is now the same length as `token_ids`.

Also note the `_postprocess_output` only executes if _max_length is None. So special tokens only get stripped in this case? And the index sets `self._num_added_start_tokens` and `self._num_added_start_tokens` to 1 regardless of whether the tokenizer has include_special_tokens=true or false.

The issue I'm running into, is I'm using a CrfTagger with pretrained_transformer, setting the encoder to pass_through. The pass_through encoder accepts token_ids and mask, which are now misaligned so an exception is thrown.

It seems that the only combination which works is to not set `max_length` (in which case `segment_concat_mask` doesn't get added).

I'm using the json below to construct the tokenizer, token_indexer and model. 

```
{
    ""tokenizer"": {
        ""type"": ""pretrained_transformer"",
        ""model_name"": ""/path/to/custom/model""
    },
    ""token_indexers"": {
        ""tokens"": {
        ""type"": ""pretrained_transformer"",
        ""model_name"": ""/path/to/custom/model"",
        ""max_length"": 128
        }
    }
}

{
    ""text_field_embedder"":{
        ""token_embedders"":{
            ""tokens"": {
                ""type"":""pretrained_transformer"",
                ""model_name"":""path/to/custom/model"",
                ""max_length"":128
            }
        }
    },
    ""encoder"":{
        ""type"":""pass_through"",
        ""input_dim"":128
    },
    ""label_encoding"":""BIOUL"",
    ""constrain_crf_decoding"":true,
    ""include_start_end_transitions"":true,
    ""dropout"":0.5,
    ""verbose_metrics"":false,
    ""calculate_span_f1"":true
}
```





",,,,,,,,1,,
2204,https://github.com/allenai/allennlp/issues/5263,5263,"[{'id': 2395308352, 'node_id': 'MDU6TGFiZWwyMzk1MzA4MzUy', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/question', 'name': 'question', 'color': '1d76db', 'default': True, 'description': ''}]",closed,2021-06-15 17:41:47+00:00,,5,Passage limit for Reading comprehension by using Transformer QA pretrained model ,"Please ask questions on [Stack Overflow](https://stackoverflow.com/questions/tagged/allennlp) rather than on GitHub.  We monitor and triage questions on Stack Overflow with the AllenNLP label and questions there are more easily searchable for others.

What is the max passage limit or hardware limit to use transformer-qa model for reading comprehension:
**Predictor.from_path('https://storage.googleapis.com/allennlp-public-models/transformer-qa-2020-10-03.tar.gz').predict(passage=passage, question=question)**

I'm getting ""DefaultCPUAllocator: not enough memory: you tried to allocate 23437770752 bytes. Buy new RAM!"" error
",,,,,,,,1,,
2213,https://github.com/allenai/allennlp/issues/5304,5304,"[{'id': 2395308352, 'node_id': 'MDU6TGFiZWwyMzk1MzA4MzUy', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/question', 'name': 'question', 'color': '1d76db', 'default': True, 'description': ''}]",closed,2021-07-08 08:46:25+00:00,,3,how to manage memory for modeling seq2seq model using varaible length training dataset? (model:allenai/led-base-16384),"I asked the same question on stackoverflow (https://stackoverflow.com/questions/68298295/allennlp-how-to-manage-memory-for-modeling-seq2seq-model-using-varaible-length)

--------------------------

## Environment info
- `transformers` version: 4.6.1
- Platform: Linux-5.4.0-74-generic-x86_64-with-glibc2.10
- Python version: 3.8.8
- PyTorch version (GPU?): 1.8.0 (True)
- GPU: GeForce RTX 3090; 24G;

Models: allenai/led-base-16384

## Issue
I re-write the original paper code (https://github.com/allenai/qasper-led-baseline) for `qasper` task (QA for long article) for not using `allennlp` lib in original code due to dependency problem, but I encounter OOM issue which didn't happen to `allennlp` code. So I am curious `allennlp trainer` make some magic.

## To reproduce
OOM happens when using the code below. But the fine tune with `allennlp` trainer under the same maximum length of sequence doesn't encounter OOM issue. I don't know why OOM does not happen to `allennlp` and want to solve OOM without `allennlp`. 
```
model = AutoModelForSeq2SeqLM.from_pretrained(""allenai/led-base-16384"", gradient_checkpointing=True, use_cache=False)
max_len = 15000
dummy = torch.ones(size=(1, max_len), device='cuda').long()

dummy2 = torch.zeros(size=(1, max_len), device='cuda').long()
dummy2[:100] = 1

dummy3 = torch.zeros(size=(1, 1024), device='cuda').long()
outputs = model(input_ids=dummy, attention_mask=dummy, global_attention_mask=dummy2, decoder_input_ids=dummy3)
```
",,,,,,,,1,,
1680,https://github.com/allenai/allennlp/issues/4082,4082,[],closed,2020-04-16 21:39:15+00:00,,13,Coref_Resolver (predictor) on AllenNLP not working locally on my MAC,"**Describe the bug**
I have an allennlp virt env and have allennlp installed successfully.  I am able to use the dep parse predictor perfectly for analysis, but when I use the coref predictor via the same exact steps as the dep parse predictor, it doesn't not work.

**To Reproduce**
Steps to reproduce the behavior
1. Go to 'allennlp.org'
2. Click on 'usage tab and use the demo code for coreference resolution'
3. Locally you should have a virt env with allennlp installed successfully
4. conda activate allennlp_virt_env with Python 3.6.10

**Expected behavior**
This is an example of using the dependency parser from allennlp
`(allennlp) rebeccaflores@Rebeccas-MBP envs % python
Python 3.6.10 | packaged by conda-forge | (default, Apr  6 2020, 14:40:13) 
[GCC Clang 9.0.1 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import allennlp
>>> from allennlp.predictors.predictor import Predictor
/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.
  FutureWarning)
>>> predictor = Predictor.from_path(""https://s3-us-west-2.amazonaws.com/allennlp/models/biaffine-dependency-parser-ptb-2018.08.23.tar.gz"")
Did not use initialization regex that was passed: .*bias_ih.*
Did not use initialization regex that was passed: .*bias_hh.*
Did not use initialization regex that was passed: .*weight_hh.*
Did not use initialization regex that was passed: .*weight_ih.*
>>> predictor.predict(
...   sentence=""If I bring 10 dollars tomorrow, can you buy me lunch?""
... )
Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.
{'arc_loss': 0.00019108332344330847, 'tag_loss': 0.0031930117402225733, 'loss': 0.003384095150977373, 'words': ['If', 'I', 'bring', '10', 'dollars', 'tomorrow', ',', 'can', 'you', 'buy', 'me', 'lunch', '?'], 'pos': ['IN', 'PRP', 'VBP', 'CD', 'NNS', 'NN', ',', 'MD', 'PRP', 'VB', 'PRP', 'NN', '.'], 'predicted_dependencies': ['mark', 'nsubj', 'advcl', 'num', 'dobj', 'tmod', 'punct', 'aux', 'nsubj', 'root', 'iobj', 'dobj', 'punct'], 'predicted_heads': [3, 3, 10, 5, 3, 3, 10, 10, 10, 0, 10, 10, 10], 'hierplane_tree': {'text': 'If I bring 10 dollars tomorrow , can you buy me lunch ?', 'root': {'word': 'buy', 'nodeType': 'root', 'attributes': ['VB'], 'link': 'root', 'spans': [{'start': 41, 'end': 45}], 'children': [{'word': 'bring', 'nodeType': 'advcl', 'attributes': ['VBP'], 'link': 'advcl', 'spans': [{'start': 5, 'end': 11}], 'children': [{'word': 'If', 'nodeType': 'mark', 'attributes': ['IN'], 'link': 'mark', 'spans': [{'start': 0, 'end': 3}]}, {'word': 'I', 'nodeType': 'nsubj', 'attributes': ['PRP'], 'link': 'nsubj', 'spans': [{'start': 3, 'end': 5}]}, {'word': 'dollars', 'nodeType': 'dobj', 'attributes': ['NNS'], 'link': 'dobj', 'spans': [{'start': 14, 'end': 22}], 'children': [{'word': '10', 'nodeType': 'num', 'attributes': ['CD'], 'link': 'num', 'spans': [{'start': 11, 'end': 14}]}]}, {'word': 'tomorrow', 'nodeType': 'tmod', 'attributes': ['NN'], 'link': 'tmod', 'spans': [{'start': 22, 'end': 31}]}]}, {'word': ',', 'nodeType': 'punct', 'attributes': [','], 'link': 'punct', 'spans': [{'start': 31, 'end': 33}]}, {'word': 'can', 'nodeType': 'aux', 'attributes': ['MD'], 'link': 'aux', 'spans': [{'start': 33, 'end': 37}]}, {'word': 'you', 'nodeType': 'nsubj', 'attributes': ['PRP'], 'link': 'nsubj', 'spans': [{'start': 37, 'end': 41}]}, {'word': 'me', 'nodeType': 'iobj', 'attributes': ['PRP'], 'link': 'iobj', 'spans': [{'start': 45, 'end': 48}]}, {'word': 'lunch', 'nodeType': 'dobj', 'attributes': ['NN'], 'link': 'dobj', 'spans': [{'start': 48, 'end': 54}]}, {'word': '?', 'nodeType': 'punct', 'attributes': ['.'], 'link': 'punct', 'spans': [{'start': 54, 'end': 56}]}]}, 'nodeTypeToStyle': {'root': ['color5', 'strong'], 'dep': ['color5', 'strong'], 'nsubj': ['color1'], 'nsubjpass': ['color1'], 'csubj': ['color1'], 'csubjpass': ['color1'], 'pobj': ['color2'], 'dobj': ['color2'], 'iobj': ['color2'], 'mark': ['color2'], 'pcomp': ['color2'], 'xcomp': ['color2'], 'ccomp': ['color2'], 'acomp': ['color2'], 'aux': ['color3'], 'cop': ['color3'], 'det': ['color3'], 'conj': ['color3'], 'cc': ['color3'], 'prep': ['color3'], 'number': ['color3'], 'possesive': ['color3'], 'poss': ['color3'], 'discourse': ['color3'], 'expletive': ['color3'], 'prt': ['color3'], 'advcl': ['color3'], 'mod': ['color4'], 'amod': ['color4'], 'tmod': ['color4'], 'quantmod': ['color4'], 'npadvmod': ['color4'], 'infmod': ['color4'], 'advmod': ['color4'], 'appos': ['color4'], 'nn': ['color4'], 'neg': ['color0'], 'punct': ['color0']}, 'linkToPosition': {'nsubj': 'left', 'nsubjpass': 'left', 'csubj': 'left', 'csubjpass': 'left', 'pobj': 'right', 'dobj': 'right', 'iobj': 'right', 'pcomp': 'right', 'xcomp': 'right', 'ccomp': 'right', 'acomp': 'right'}}}
>>> data = predictor.predict(""Hermione rode on the train with her friends Ron and Harry to learn at Hogwartz"")
>>> print(type(data))
<class 'dict'>
>>> print(data.keys())
dict_keys(['arc_loss', 'tag_loss', 'loss', 'words', 'pos', 'predicted_dependencies', 'predicted_heads', 'hierplane_tree'])
>>> pos_tags = data['pos']
>>> doc = data['words']
>>> dep_parses = data['predicted_dependencies']
>>> print(pos_tags)
['NNP', 'VBD', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NNS', 'NNP', 'CC', 'NNP', 'TO', 'VB', 'IN', 'NNP']
>>> print(doc)
['Hermione', 'rode', 'on', 'the', 'train', 'with', 'her', 'friends', 'Ron', 'and', 'Harry', 'to', 'learn', 'at', 'Hogwartz']
>>> print(dep_parses)
['nsubj', 'root', 'prep', 'det', 'pobj', 'prep', 'poss', 'pobj', 'nsubj', 'cc', 'conj', 'aux', 'xcomp', 'prep', 'pobj']`

**System (please complete the following information):**
 - OS: MAC OS Catalina
 - Python version: 3.6.10
 - AllenNLP version: [v0.9.0, or ""I installed from terminal using the following commands:""]
`conda install allennlp -c pytorch -c allennlp -c conda-forge`
 - PyTorch version: (if you installed it yourself) I used following command to install pytorch `conda install pytorch torchvision -c pytorch`

**Additional context**
This is my stack trace for the error:
`>>> coref_resolver = Predictor.from_path(""https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/predictors/predictor.py"", line 105, in from_path
    return Predictor.from_archive(load_archive(archive_path), predictor_name)
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/models/archival.py"", line 153, in load_archive
    cuda_device=cuda_device)
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/models/model.py"", line 321, in load
    return cls.by_name(model_type)._load(config, serialization_dir, weights_file, cuda_device)
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/models/model.py"", line 268, in _load
    model = Model.from_params(vocab=vocab, params=model_params)
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/from_params.py"", line 274, in from_params
    return subclass.from_params(params=params, **extras)
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/from_params.py"", line 285, in from_params
    kwargs = create_kwargs(cls, params, **extras)
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/from_params.py"", line 147, in create_kwargs
    kwargs[name] = annotation.from_params(params=subparams, **subextras)
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/from_params.py"", line 274, in from_params
    return subclass.from_params(params=params, **extras)
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"", line 117, in from_params
    for name, subparams in token_embedder_params.items()
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"", line 117, in <dictcomp>
    for name, subparams in token_embedder_params.items()
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/from_params.py"", line 260, in from_params
    default_to_first_choice=default_to_first_choice)
  File ""/Users/rebeccaflores/miniconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/params.py"", line 259, in pop_choice
    raise ConfigurationError(message)
allennlp.common.checks.ConfigurationError: ""pretrained_transformer_mismatched not in acceptable choices for model.text_field_embedder.token_embedders.tokens.type: ['embedding', 'character_encoding', 'elmo_token_embedder', 'openai_transformer_embedder']""
>>> 
`
",,1,,,,,1,,,
46,https://github.com/allenai/allennlp/issues/653,653,[],closed,2017-12-29 20:30:09+00:00,,3,JSONL requirement in `Predict` is too restrictive,"for context, I am playing around with using AllenNLP for 

https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge

I have trained a model, and now I need to create a CSV of predictions on the test set. The input file is a CSV {""comment_id"", ""comment_text""}. Right now in order to run `predict` on this, I have to convert it to a JSONL file. That's not hard, but it seems like an unnecessary step.

The relevant code in `predict.py` is

```
    batch_json_data = []
    for line in input_file:
        if not line.isspace():
            # Collect batch size amount of data.
            json_data = json.loads(line)
            batch_json_data.append(json_data)
            if len(batch_json_data) == batch_size:
                _run_predictor(batch_json_data)
                batch_json_data = []
```

Of course we don't want to break backward compatibility. One immediate idea is to give the base `Predictor` class a method

```
def line_to_json(line: str) -> JsonDict:
    return json.loads(line)
```

and then call that (rather than `json.loads`) from `predict.py`. Then in my example I could override it like

```
def line_to_json(line: str) -> JsonDict:
    id, text = parse_csv_line(line)
    return {""id"": id, ""text"": text}
```

and be able to use my `test.csv` rather than have to make a JSONL file.

and then probably we'd do the same for the output:

```
def json_output_to_line(line: str) -> JsonDict:
    return json.dumps(line)
```

and I'd override that too
",,,,,,,1,,,
114,https://github.com/allenai/allennlp/issues/901,901,[],closed,2018-02-20 17:39:13+00:00,,5,KeyError during model prediction,"I tried running the following instructions for the [coreference resolution models](http://allennlp.org/models):
```
echo '{""document"": ""The woman reading a newspaper sat on the bench with her dog.""}' > examples.jsonl
python -m allennlp.run predict \
    https://s3-us-west-2.amazonaws.com/allennlp/models/coref-model-2017.11.09.tar.gz \
    examples.jsonl
```
and I get the following error:
```
2018-02-20 17:31:46,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.weight
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/stage/allennlp/allennlp/run.py"", line 18, in <module>
    main(prog=""python -m allennlp.run"")
  File ""/stage/allennlp/allennlp/commands/__init__.py"", line 67, in main
    args.func(args)
  File ""/stage/allennlp/allennlp/commands/predict.py"", line 172, in predict_inner
    predictor = _get_predictor(args, predictors)
  File ""/stage/allennlp/allennlp/commands/predict.py"", line 114, in _get_predictor
    overrides=args.overrides)
  File ""/stage/allennlp/allennlp/models/archival.py"", line 149, in load_archive
    cuda_device=cuda_device)
  File ""/stage/allennlp/allennlp/models/model.py"", line 235, in load
    model.load_state_dict(model_state)
  File ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 522, in load_state_dict
    .format(name))
KeyError: 'unexpected key ""_mention_feedforward._module._linear_layers.0.weight"" in state_dict'
```

I tried this both using the docker installation (`docker run -p 8000:8000 -it --rm allennlp/allennlp`)
and the [installation from source](https://github.com/allenai/allennlp/blob/v0.3.0/tutorials/getting_started/installation.md#installing-from-source). 

I also tried with the latest PyTorch (0.3.1) and the recommended (0.2.0).
",,,,,,,1,,,
119,https://github.com/allenai/allennlp/issues/917,917,[],closed,2018-02-23 20:40:30+00:00,,3,Option to predict tagger model without SpacyWordSplitter?,"I am training a SimpleTagger instance and when trying to use the trained model to predict unseen data, I keep running into the problem that the output has more words than the gold. 
I looked into the code and instances of the class `sentence_tagger` always use the SpacyTokenizer (see here: https://github.com/allenai/allennlp/blob/master/allennlp/service/predictors/sentence_tagger.py#L22). 

Since I am formatting the training data so that I have a `word###tag` format, I am assuming that the splitter is not applied here. Assuming that the test tgt is formatted in a spacy-compliant way seems like a pretty big assumption to me. 
Since you already have the JustSpaceTokenizer here (https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/word_splitter.py#L128), I am wondering if there a method exists to tell the predictor to use that one (without monkeypatching the class on my end). 
If not, I suggest adding a tokenizer option to the config file. ",,,,,,,1,,,
1045,https://github.com/allenai/allennlp/issues/2833,2833,[],closed,2019-05-13 13:18:42+00:00,,1,KeyError: 'predictions' When use SimpleSeq2SeqPredictor to predict string,"Please first search our GitHub repository for similar questions.  If you don't find a similar example you can use the following template:

**System (please complete the following information):**
 - OS: Ubunti 18.04
 - Python version: 3.6.7
 - AllenNLP version: v0.8.3
 - PyTorch version: 1.1.0

**Question**
When I Try to predict string using SimpleSeq2SeqPredictor, It always show that
```bash
Traceback (most recent call last):
  File ""predict.py"", line 96, in <module>
    p = predictor.predict(i)
  File ""venv/lib/python3.6/site-packages/allennlp/predictors/seq2seq.py"", line 17, in predict
    return self.predict_json({""source"" : source})
  File ""/venv/lib/python3.6/site-packages/allennlp/predictors/predictor.py"", line 56, in predict_json
    return self.predict_instance(instance)
  File ""/venv/lib/python3.6/site-packages/allennlp/predictors/predictor.py"", line 93, in predict_instance
    outputs = self._model.forward_on_instance(instance)
  File ""/venv/lib/python3.6/site-packages/allennlp/models/model.py"", line 124, in forward_on_instance
    return self.forward_on_instances([instance])[0]
  File ""/venv/lib/python3.6/site-packages/allennlp/models/model.py"", line 153, in forward_on_instances
    outputs = self.decode(self(**model_input))
  File ""/venv/lib/python3.6/site-packages/allennlp/models/encoder_decoders/simple_seq2seq.py"", line 247, in decode
    predicted_indices = output_dict[""predictions""]
KeyError: 'predictions'
```

I try to do a translate system, but I am newbie, most of code come from
https://github.com/mhagiwara/realworldnlp/blob/master/examples/mt/mt.py
http://www.realworldnlpbook.com/blog/building-seq2seq-machine-translation-models-using-allennlp.html

this is my training code
```py
EN_EMBEDDING_DIM = 256
ZH_EMBEDDING_DIM = 256
HIDDEN_DIM = 256
CUDA_DEVICE = 0
prefix = 'small'

reader = Seq2SeqDatasetReader(
    source_tokenizer=WordTokenizer(),
    target_tokenizer=CharacterTokenizer(),
    source_token_indexers={'tokens': SingleIdTokenIndexer()},
    target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')},
    lazy = True)
train_dataset = reader.read(f'./{prefix}-data/train.tsv')
validation_dataset = reader.read(f'./{prefix}-data/val.tsv')

vocab = Vocabulary.from_instances(train_dataset,
                                    min_count={'tokens': 3, 'target_tokens': 3})

en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),
                            embedding_dim=EN_EMBEDDING_DIM)
# encoder = PytorchSeq2SeqWrapper(
#     torch.nn.LSTM(EN_EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))
encoder = StackedSelfAttentionEncoder(input_dim=EN_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, projection_dim=128, feedforward_hidden_dim=128, num_layers=1, num_attention_heads=8)

source_embedder = BasicTextFieldEmbedder({""tokens"": en_embedding})

# attention = LinearAttention(HIDDEN_DIM, HIDDEN_DIM, activation=Activation.by_name('tanh')())
# attention = BilinearAttention(HIDDEN_DIM, HIDDEN_DIM)
attention = DotProductAttention()

max_decoding_steps = 20   # TODO: make this variable
model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,
                        target_embedding_dim=ZH_EMBEDDING_DIM,
                        target_namespace='target_tokens',
                        attention=attention,
                        beam_size=8,
                        use_bleu=True)
optimizer = optim.Adam(model.parameters())
iterator = BucketIterator(batch_size=32, sorting_keys=[(""source_tokens"", ""num_tokens"")])

iterator.index_with(vocab)
if torch.cuda.is_available():
    cuda_device = 0
    model = model.cuda(cuda_device)
else:
    cuda_device = -1
trainer = Trainer(model=model,
                    optimizer=optimizer,
                    iterator=iterator,
                    train_dataset=train_dataset,
                    validation_dataset=validation_dataset,
                    num_epochs=50,
                    serialization_dir=f'ck/{prefix}/',
                    cuda_device=cuda_device)

# for i in range(50):
    # print('Epoch: {}'.format(i))
trainer.train()

predictor = SimpleSeq2SeqPredictor(model, reader)

for instance in itertools.islice(validation_dataset, 10):
    print('SOURCE:', instance.fields['source_tokens'].tokens)
    print('GOLD:', instance.fields['target_tokens'].tokens)
    print('PRED:', predictor.predict_instance(instance)['predicted_tokens'])

# Here's how to save the model.
with open(f""ck/{prefix}/manually_save_model.th"", 'wb') as f:
    torch.save(model.state_dict(), f)
vocab.save_to_files(f""ck/{prefix}/vocabulary"")
```

and this is my predict code
```py
EN_EMBEDDING_DIM = 256
ZH_EMBEDDING_DIM = 256
HIDDEN_DIM = 256
CUDA_DEVICE = 0
prefix = 'big'

reader = Seq2SeqDatasetReader(
    source_tokenizer=WordTokenizer(),
    target_tokenizer=CharacterTokenizer(),
    source_token_indexers={'tokens': SingleIdTokenIndexer()},
    target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')},
    lazy = True)
# train_dataset = reader.read(f'./{prefix}-data/train.tsv')
# validation_dataset = reader.read(f'./{prefix}-data/val.tsv')

# vocab = Vocabulary.from_instances(train_dataset,
#                                     min_count={'tokens': 3, 'target_tokens': 3})
vocab = Vocabulary.from_files(""ck/small/vocabulary"")

en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),
                            embedding_dim=EN_EMBEDDING_DIM)
# encoder = PytorchSeq2SeqWrapper(
#     torch.nn.LSTM(EN_EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))
encoder = StackedSelfAttentionEncoder(input_dim=EN_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, projection_dim=128, feedforward_hidden_dim=128, num_layers=1, num_attention_heads=8)

source_embedder = BasicTextFieldEmbedder({""tokens"": en_embedding})

# attention = LinearAttention(HIDDEN_DIM, HIDDEN_DIM, activation=Activation.by_name('tanh')())
# attention = BilinearAttention(HIDDEN_DIM, HIDDEN_DIM)
attention = DotProductAttention()

max_decoding_steps = 20   # TODO: make this variable
model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,
                        target_embedding_dim=ZH_EMBEDDING_DIM,
                        target_namespace='target_tokens',
                        attention=attention,
                        beam_size=8,
                        use_bleu=True)

# And here's how to reload the model.
with open(""./ck/small/best.th"", 'rb') as f:
    model.load_state_dict(torch.load(f))

predictor = Seq2SeqPredictor(model, dataset_reader=reader)
# print(predictor.predict(""The dog ate the apple""))


test = [
    'Surely ,he has no power over those who believe and put their trust in their Lord ;',
    'And assuredly We have destroyed the generations before you when they did wrong ,while their apostles came unto them with the evidences ,and they were not such as to believe . In this wise We requite the sinning people .',
    'And warn your tribe ( O Muhammad SAW ) of near kindred .',
    'And to the Noble Messengers whom We have mentioned to you before ,and to the Noble Messengers We have not mentioned to you ; and Allah really did speak to Moosa .',
    'It is He who gave you hearing ,sight ,and hearts ,but only few of you give thanks .',
    'spreading in them much corruption ?',
    'That will envelop the people . This will be a painful punishment .',
    'When you received it with your tongues and spoke with your mouths what you had no knowledge of ,and you deemed it an easy matter while with Allah it was grievous .',
    'of which you are disregardful .',
    'Whoever disbelieves ,then the calamity of his disbelief is only on him ; and those who do good deeds ,are preparing for themselves .'
]




for i in test:
    p = predictor.predict(i) # <------------------- ERROR !!!!!!!
    print(p) 
```

Am I do something wrong ?",,,,,,,1,,,
1085,https://github.com/allenai/allennlp/issues/2893,2893,[],closed,2019-05-27 03:44:44+00:00,,4,Error while trying to use biaffine_dependency_parser predictor,"**Describe the bug**
I am trying to use the pre-trained model `biaffine_dependency_parser` predictor, however, I am facing an exception:


```sh
172.17.0.1 - - [2019-05-27 03:30:33] ""POST /predict HTTP/1.1"" 500 477 0.004113
[2019-05-27 03:31:20,680] ERROR in app: Exception on /predict [POST]
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python3.6/site-packages/flask_cors/extension.py"", line 161, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise
    raise value
  File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python3.6/site-packages/allennlp/service/server_simple.py"", line 109, in predict
    prediction = predictor.predict_json(data)
  File ""/usr/local/lib/python3.6/site-packages/allennlp/predictors/predictor.py"", line 55, in predict_json
    instance = self._json_to_instance(inputs)
  File ""/usr/local/lib/python3.6/site-packages/allennlp/predictors/biaffine_dependency_parser.py"", line 108, in _json_to_instance
    if self._dataset_reader.use_language_specific_pos: # type: ignore
AttributeError: 'Conll2003DatasetReader' object has no attribute 'use_language_specific_pos'
```

**To Reproduce**
Steps to reproduce the behavior

- Get the latest image of the `allennlp`: `docker pull allennlp/allennlp:v0.8.3`
- Run the installed docker image: 
`docker run --rm -p 8000:8000 --entrypoint="""" -v $HOME/.allennlp:/root/.allennlp allennlp/allennlp:v0.8.3 python -m allennlp.service.server_simple --archive-path https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.12.18.tar.gz --predictor biaffine-dependency-parser --field-name sentence`  

- Execute this post request: 
`curl 'http://localhost:8000/predict' -H 'Content-Type: application/json' --data-binary '{""sentence"":""Say Hello World""}'`

**Expected behavior**
`{""arc_loss"":0.00048828125,""hierplane_tree"":{""linkToPosition"":{""acomp"":""right"",""ccomp"":""right"",""csubj"":""left"",""csubjpass"":""left"",""dobj"":""right"",""iobj"":""right"",""nsubj"":""left"",""nsubjpass"":""left"",""pcomp"":""right"",""pobj"":""right"",""xcomp"":""right""},""nodeTypeToStyle"":{""acomp"":[""color2""],""advcl"":[""color3""],""advmod"":[""color4""],""amod"":[""color4""],""appos"":[""color4""],""aux"":[""color3""],""cc"":[""color3""],""ccomp"":[""color2""],""conj"":[""color3""],""cop"":[""color3""],""csubj"":[""color1""],""csubjpass"":[""color1""],""dep"":[""color5"",""strong""],""det"":[""color3""],""discourse"":[""color3""],""dobj"":[""color2""],""expletive"":[""color3""],""infmod"":[""color4""],""iobj"":[""color2""],""mark"":[""color2""],""mod"":[""color4""],""neg"":[""color0""],""nn"":[""color4""],""npadvmod"":[""color4""],""nsubj"":[""color1""],""nsubjpass"":[""color1""],""number"":[""color3""],""pcomp"":[""color2""],""pobj"":[""color2""],""poss"":[""color3""],""possesive"":[""color3""],""prep"":[""color3""],""prt"":[""color3""],""punct"":[""color0""],""quantmod"":[""color4""],""root"":[""color5"",""strong""],""tmod"":[""color4""],""xcomp"":[""color2""]},""root"":{""attributes"":[""VB""],""children"":[{""attributes"":[""NN""],""children"":[{""attributes"":[""UH""],""link"":""dep"",""nodeType"":""dep"",""spans"":[{""end"":10,""start"":4}],""word"":""Hello""}],""link"":""dobj"",""nodeType"":""dobj"",""spans"":[{""end"":16,""start"":10}],""word"":""World""}],""link"":""root"",""nodeType"":""root"",""spans"":[{""end"":4,""start"":0}],""word"":""Say""},""text"":""Say Hello World""},""loss"":0.30580803751945496,""pos"":[""VB"",""UH"",""NN""],""predicted_dependencies"":[""root"",""dep"",""dobj""],""predicted_heads"":[0,3,1],""slug"":""ODMwODcw"",""tag_loss"":0.30531975626945496,""words"":[""Say"",""Hello"",""World""]}
`

**System (please complete the following information):**
 - OS: macOS Mojave 
 - Python version: 3.6.1
 - AllenNLP version:  v0.8.3
 - PyTorch version: Default which is installed in the image
",,,,,,,1,,,
1137,https://github.com/allenai/allennlp/issues/2985,2985,[],closed,2019-06-21 08:10:16+00:00,,4,How to load and predict from a model after training properly,"**System (please complete the following information):**
 - OS: training: Linux, prediction OSX
 - Python version: 3.6.5
 - AllenNLP version: 0.8.4
 - PyTorch version: 1.1.0

**Question**
I have trained a very small(toy) model for mapping snippet of code to their intent (check out [CoNaLA](https://conala-corpus.github.io/) ) I am planning to iterate on it and create custom models and all. So, I did not use the jsonnet based training for this. I coded everything in Python and then at the end of the training I saved the model. So the relevant code at the time of training looks like this - 

```python
class Py2CommentWordSplitter(WordSplitter):
    @overrides
    def split_words(self, sentence: str):
        tokens = [Token(m) for m in sentence.split()]
        return tokens


# In[35]:


class Py2CommentsDataSetReader(Seq2SeqDatasetReader):

    def __init__(self,
                 source_tokenizer: Tokenizer = None,
                 target_tokenizer: Tokenizer = None,
                 source_token_indexers: Dict[str, TokenIndexer] = None,
                 target_token_indexers: Dict[str, TokenIndexer] = None,
                 source_add_start_token: bool = True,
                 delimiter: str = 't',
                 lazy: bool = False):
        source_tokenizer = WordTokenizer(word_splitter=Py2CommentWordSplitter())
        target_tokenizer = WordTokenizer()
        super().__init__(source_tokenizer=source_tokenizer,
                         target_tokenizer=target_tokenizer,
                         source_token_indexers={'tokens': SingleIdTokenIndexer()},
                         target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')},
                         lazy=lazy)

    @overrides
    def _read(self, file_path):
        with open(cached_path(file_path), ""r"", encoding='utf-8') as data_file:
            print(""Reading instances from lines in file at: %s"", file_path)
            for line_num, line in enumerate(data_file):
                line = line.strip(""\n"")
                if not line:
                    continue 
                line_parts = line.split('\t')
                if len(line_parts) != 2:
                    # print(""Invalid line format: %s (line number %d)"" % (line, line_num + 1))
                    continue
                source_sequence, target_sequence = line_parts
                if not source_sequence:
                    continue 
                yield self.text_to_instance(source_sequence, target_sequence)

reader = Py2CommentsDataSetReader()


# In[43]:


train_dataset = reader.read('train.tsv')


# In[44]:


validation_dataset = reader.read('valid.tsv')


# In[45]:


vocab = Vocabulary.from_instances(train_dataset + validation_dataset,
                                      min_count={'tokens': 3, 'target_tokens': 3})


# In[46]:


vocab.get_vocab_size('tokens')


# In[47]:


INTENT_EMBEDDING_DIM = 32
SNIPPET_EMBEDDING_DIM = 32
HIDDEN_DIM = 64
CUDA_DEVICE = 0


# In[48]:


en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),
                             embedding_dim=SNIPPET_EMBEDDING_DIM)


# In[49]:


encoder = StackedSelfAttentionEncoder(input_dim=INTENT_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, projection_dim=64, feedforward_hidden_dim=64, num_layers=2, num_attention_heads=8,dropout_prob=0.3)


# In[50]:


source_embedder = BasicTextFieldEmbedder({""tokens"": en_embedding})


# In[51]:


attention = BilinearAttention(HIDDEN_DIM, HIDDEN_DIM)


# In[52]:


max_decoding_steps = 20


# In[53]:


model = SimpleSeq2Seq(vocab=vocab,
                       source_embedder=source_embedder,
                       encoder=encoder, 
                       max_decoding_steps=max_decoding_steps,
                       target_embedding_dim=SNIPPET_EMBEDDING_DIM,
                       target_namespace='target_tokens',
                       attention=attention,
                       beam_size=5).cuda()


# In[54]:


optimizer = optim.Adam(model.parameters())


# In[55]:


iterator = BucketIterator(batch_size=8, sorting_keys=[(""source_tokens"", ""num_tokens"")], padding_noise=0.1)


# In[56]:


iterator.index_with(vocab)


# In[57]:


trainer = Trainer(model=model,
                      optimizer=optimizer,
                      iterator=iterator,
                      train_dataset=train_dataset,
                      validation_dataset=validation_dataset,
                      num_epochs=30,
                      cuda_device=CUDA_DEVICE,
                       should_log_parameter_statistics=False)


# In[58]:


trainer.train()


with open(""model/model.th"", 'wb') as f:
    torch.save(model.state_dict(), f)

vocab.save_to_files(""model/vocabulary"")
```
I trained the model on an AWS instance (running ubuntu)
After that, I downloaded the files and I am now trying to load them locally in my laptop 
The code I am using to do this is the following - 
```python
def get_simple_predictor(model_dir, model_config):
    if not Path(model_dir).exists() or not Path(model_dir).is_dir():
        raise NotImplementedError(""The model seems not to exist"")
    if not (Path(model_dir)/""model.th"").is_file():
        raise FileNotFoundError(""The model file is not found"")
    if not (Path(model_dir)/""vocabulary"").is_dir():
        raise NotADirectoryError(""The vocabulary is not a directory"")
    
    reader = Py2CommentsDataSetReader()
    vocab = Vocabulary.from_files(model_dir + ""/vocabulary"")

    source_embedding_dim = model_config['source_embedding_dim']
    target_embedding_dim = model_config['target_embedding_dim']
    hidden_dim = model_config['hidden_dim']

    source_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),
                             embedding_dim=source_embedding_dim)
    encoder = StackedSelfAttentionEncoder(input_dim=source_embedding_dim, 
                                          hidden_dim=hidden_dim, 
                                          projection_dim=64,
                                          feedforward_hidden_dim=64, 
                                          num_layers=2,
                                          num_attention_heads=8,
                                          dropout_prob=0.3)

    source_embedder = BasicTextFieldEmbedder({""tokens"": source_embedding})
    attention = BilinearAttention(hidden_dim, hidden_dim)
    max_decoding_steps = model_config['max_decoding_steps']

    model = SimpleSeq2Seq(vocab=vocab,
                          source_embedder=source_embedder,
                          encoder=encoder, 
                          max_decoding_steps=max_decoding_steps,
                          target_embedding_dim=target_embedding_dim,
                          target_namespace='target_tokens',
                          attention=attention,
                          beam_size=5)
    
    with open(Path(model_dir)/""model.th"", ""rb"") as f:
        model.load_state_dict(torch.load(f, map_location=model_config['map_location']))
    
    predictor = SimpleSeq2SeqPredictor(model, dataset_reader=reader)
    return predictor
```
The hyper-parameters match exactly the values used at training time. When I call this function it does not throw an error and it returns the `predictor` object. However, when I am using that to do something like - `predictor.predict(""mystring . replace ( ' ' , '! !' ) . split ( '!' )"")` it throws an error. Here is the stacktrace - 

```
KeyError                                  Traceback (most recent call last)
<ipython-input-3-9e10c1070b7a> in <module>
----> 1 predictor.predict(""mystring . replace ( ' ' , '! !' ) . split ( '!' )"")

~/.pyenv/versions/3.6.5/envs/autosoft-webapp/lib/python3.6/site-packages/allennlp/predictors/seq2seq.py in predict(self, source)
     15
     16     def predict(self, source: str) -> JsonDict:
---> 17         return self.predict_json({""source"" : source})
     18
     19     @overrides

~/.pyenv/versions/3.6.5/envs/autosoft-webapp/lib/python3.6/site-packages/allennlp/predictors/predictor.py in predict_json(self, inputs)
     56     def predict_json(self, inputs: JsonDict) -> JsonDict:
     57         instance = self._json_to_instance(inputs)
---> 58         return self.predict_instance(instance)
     59
     60     @contextmanager

~/.pyenv/versions/3.6.5/envs/autosoft-webapp/lib/python3.6/site-packages/allennlp/predictors/predictor.py in predict_instance(self, instance)
     93
     94     def predict_instance(self, instance: Instance) -> JsonDict:
---> 95         outputs = self._model.forward_on_instance(instance)
     96         return sanitize(outputs)
     97

~/.pyenv/versions/3.6.5/envs/autosoft-webapp/lib/python3.6/site-packages/allennlp/models/model.py in forward_on_instance(self, instance)
    122         ``torch.Tensors`` into numpy arrays and remove the batch dimension.
    123         """"""
--> 124         return self.forward_on_instances([instance])[0]
    125
    126     def forward_on_instances(self,

~/.pyenv/versions/3.6.5/envs/autosoft-webapp/lib/python3.6/site-packages/allennlp/models/model.py in forward_on_instances(self, instances)
    151             dataset.index_instances(self.vocab)
    152             model_input = util.move_to_device(dataset.as_tensor_dict(), cuda_device)
--> 153             outputs = self.decode(self(**model_input))
    154
    155             instance_separated_output: List[Dict[str, numpy.ndarray]] = [{} for _ in dataset.instances]

~/.pyenv/versions/3.6.5/envs/autosoft-webapp/lib/python3.6/site-packages/allennlp/models/encoder_decoders/simple_seq2seq.py in decode(self, output_dict)
    245         corresponding tokens, and adds a field called ``predicted_tokens`` to the ``output_dict``.
    246         """"""
--> 247         predicted_indices = output_dict[""predictions""]
    248         if not isinstance(predicted_indices, numpy.ndarray):
    249             predicted_indices = predicted_indices.detach().cpu().numpy()

KeyError: 'predictions'
```

I am not sure how to fix this problem. Any help will be great. Thanks a lot!",,,,,,,1,,,
1213,https://github.com/allenai/allennlp/issues/3114,3114,[],closed,2019-08-05 07:35:39+00:00,,2,predictor.predict_json uses the dataset_reader config entry instead of using the validation_dataset_reader entry,"predictor.predict_json uses the dataset_reader config entry instead of using the validation_dataset_reader entry (when such exists). 

This causes params from training (such as is_training=True if such param is defined) to be passed to the predictor. 

Expected behavior - have the predictor use the validation_dataset_reader when such exists. 

Perhaps this issue needs to be merged, but i would still like to emphasis that i'm surprised that the default is not to use the validation_dataset_reader in predict. 

",,,,,,,1,,,
1265,https://github.com/allenai/allennlp/issues/3218,3218,[],closed,2019-09-05 20:26:36+00:00,,5,Predict during training,"**Is your feature request related to a problem? Please describe.**
I would like to log model predictions for every epoch, even though I am only keeping checkpoints of the latest 3 epochs. This enables post-analysis on the predictions without the disk overhead of logging model weights every epoch. One use case is to generate training curves for new metrics on old (expensive) training runs.

**Describe the solution you'd like**
Either a Predict callback or a ""predict"" field added to the Validate callback

I looked into subclassing or modifying the Validate callback, but since I need the `output_dict`, I would also need to alter `CallbackTrainer.batch_loss()` to return the `output_dict` instead of just returning loss. For now, I have written a hack that duplicates CallbackTrainer.batch_loss() functionality inside the callback. ",,,,,,,1,,,
1426,https://github.com/allenai/allennlp/issues/3489,3489,[],closed,2019-11-27 17:05:43+00:00,,3,Using dataset reader at prediction time,"**System:**
 - OS: [Linux, Windows 7]
 - Python version: [3.7]
 - AllenNLP version: [0.9.0]
 - PyTorch version: 1.3.0

**Question**
If I use the dataset reader to read in the data for the predictor, what part of the data is taken into consideration for the prediction?

For example, I'm using the conll2003 predictor. If I provide POS-tag in the data to predict, do they make a difference in the outcome of the prediction? Alternatively / Additionaly, can I include a POS tagger into the NER model? What would I have to change in the config file.",,,,,,,1,,,
1608,https://github.com/allenai/allennlp/issues/3937,3937,[],closed,2020-03-11 09:51:48+00:00,,3,loss and metric calculation for joint predicate and role prediction on SRL,"Hi, I am trying to modify the SrlBert model to jointly predict the predicates and the corresponding semantic roles as opposed to using the gold predicate information. Specifically, I am planning to first decide which tokens are the predicates and store their embeddings. Then, I will pass the predicate embeddings and regular token embeddings through a bilinear layer to assign the role for each token w.r.t a predicate. However, I have problems on BERT's wordpiece tokenizer and the loss calculation. I will try to solve the mapping problem between the wordpieces and words for the predicates myself first although I will greatly appreciate any tip or guides. My main problem is the metric and loss calculation for joint predicate prediction and role tagging problem, especially in the case of multiple predicates in a sentence.  Is there a built-in module for that? If not, I am open for suggestions.
Btw, I am using version 0.9.0 of AllenNLP.
Thank you.

Edit: My training instance fields include of a list of SequenceLabelField. Please see the relevant code (excluding metadata fields etc) snippet from my dataset reader:

```
        tag_fields = []
        for (_, tags) in srl_frames:
            if self.bert_tokenizer is not None:
                new_tags = _convert_tags_to_wordpiece_tags(tags, offsets)
                tag_fields.append(SequenceLabelField(new_tags, text_field))
            else:
                tag_fields.append(SequenceLabelField(tags, text_field))

        fields[""tags""] = ListField(tag_fields)
```
",,,,,,,1,,,
1786,https://github.com/allenai/allennlp/issues/4332,4332,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2020-06-05 22:57:21+00:00,,4,Predict doesn't work for SNLI on 1.0.0rc5,"```
$ echo '{""hypothesis"": ""Two women are sitting on a blanket near some rocks talking about politics."", ""premise"": ""Two women are wandering along the shore drinking iced tea.""}' | allennlp predict --predictor textual-entailment https://storage.googleapis.com/allennlp-public-models/snli-roberta-large-2020.04.30.tar.gz -
2020-06-05 15:56:14,840 - INFO - transformers.file_utils - PyTorch version 1.5.0 available.
2020-06-05 15:56:15,495 - INFO - allennlp.models.archival - loading archive file https://storage.googleapis.com/allennlp-public-models/snli-roberta-large-2020.04.30.tar.gz from cache at /home/michaels/.allennlp/cache/589d6edb6a58b240ecd4c9fdbe356edf24cc1200ff1fb0c65835bfdc8b05ba1c.90841b7d888cf623f8ffdafbdd06a233a1fa2eb2f2ed42376f3cd690ecd49462
2020-06-05 15:56:15,514 - INFO - allennlp.models.archival - extracting archive file /home/michaels/.allennlp/cache/589d6edb6a58b240ecd4c9fdbe356edf24cc1200ff1fb0c65835bfdc8b05ba1c.90841b7d888cf623f8ffdafbdd06a233a1fa2eb2f2ed42376f3cd690ecd49462 to temp dir /tmp/tmps6xeghmu
2020-06-05 15:56:24,813 - INFO - allennlp.common.params - type = from_instances
2020-06-05 15:56:24,814 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmps6xeghmu/vocabulary.
2020-06-05 15:56:24,814 - INFO - allennlp.common.params - model.type = basic_classifier
2020-06-05 15:56:24,814 - INFO - allennlp.common.params - model.regularizer = None
2020-06-05 15:56:24,814 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2020-06-05 15:56:24,815 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer
2020-06-05 15:56:24,815 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = roberta-large
2020-06-05 15:56:24,815 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512
2020-06-05 15:56:25,129 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/michaels/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748
2020-06-05 15:56:25,129 - INFO - transformers.configuration_utils - Model config RobertaConfig {
  ""architectures"": [
    ""RobertaForMaskedLM""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""bos_token_id"": 0,
  ""eos_token_id"": 2,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 1024,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 4096,
  ""layer_norm_eps"": 1e-05,
  ""max_position_embeddings"": 514,
  ""model_type"": ""roberta"",
  ""num_attention_heads"": 16,
  ""num_hidden_layers"": 24,
  ""pad_token_id"": 1,
  ""type_vocab_size"": 1,
  ""vocab_size"": 50265
}

2020-06-05 15:56:25,177 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/roberta-large-pytorch_model.bin from cache at /home/michaels/.cache/torch/transformers/2339ac1858323405dffff5156947669fed6f63a0c34cfab35bda4f78791893d2.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536
2020-06-05 15:56:34,879 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/michaels/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748
2020-06-05 15:56:34,880 - INFO - transformers.configuration_utils - Model config RobertaConfig {
  ""architectures"": [
    ""RobertaForMaskedLM""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""bos_token_id"": 0,
  ""eos_token_id"": 2,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 1024,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 4096,
  ""layer_norm_eps"": 1e-05,
  ""max_position_embeddings"": 514,
  ""model_type"": ""roberta"",
  ""num_attention_heads"": 16,
  ""num_hidden_layers"": 24,
  ""pad_token_id"": 1,
  ""type_vocab_size"": 1,
  ""vocab_size"": 50265
}

2020-06-05 15:56:35,521 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/michaels/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
2020-06-05 15:56:35,521 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/michaels/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
2020-06-05 15:56:35,909 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/michaels/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748
2020-06-05 15:56:35,910 - INFO - transformers.configuration_utils - Model config RobertaConfig {
  ""architectures"": [
    ""RobertaForMaskedLM""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""bos_token_id"": 0,
  ""eos_token_id"": 2,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 1024,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 4096,
  ""layer_norm_eps"": 1e-05,
  ""max_position_embeddings"": 514,
  ""model_type"": ""roberta"",
  ""num_attention_heads"": 16,
  ""num_hidden_layers"": 24,
  ""pad_token_id"": 1,
  ""type_vocab_size"": 1,
  ""vocab_size"": 50265
}

2020-06-05 15:56:36,542 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/michaels/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
2020-06-05 15:56:36,542 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/michaels/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
2020-06-05 15:56:36,629 - INFO - allennlp.common.params - model.seq2vec_encoder.type = cls_pooler
2020-06-05 15:56:36,629 - INFO - allennlp.common.params - model.seq2vec_encoder.embedding_dim = 1024
2020-06-05 15:56:36,629 - INFO - allennlp.common.params - model.seq2vec_encoder.cls_is_last_token = False
2020-06-05 15:56:36,629 - INFO - allennlp.common.params - model.seq2seq_encoder = None
2020-06-05 15:56:36,630 - INFO - allennlp.common.params - model.feedforward.input_dim = 1024
2020-06-05 15:56:36,630 - INFO - allennlp.common.params - model.feedforward.num_layers = 1
2020-06-05 15:56:36,630 - INFO - allennlp.common.params - model.feedforward.hidden_dims = 1024
2020-06-05 15:56:36,630 - INFO - allennlp.common.params - model.feedforward.activations = tanh
2020-06-05 15:56:36,630 - INFO - allennlp.common.params - type = tanh
2020-06-05 15:56:36,630 - INFO - allennlp.common.params - model.feedforward.dropout = 0.0
2020-06-05 15:56:36,636 - INFO - allennlp.common.params - model.dropout = 0.1
2020-06-05 15:56:36,636 - INFO - allennlp.common.params - model.num_labels = None
2020-06-05 15:56:36,637 - INFO - allennlp.common.params - model.label_namespace = labels
2020-06-05 15:56:36,637 - INFO - allennlp.common.params - model.namespace = tags
2020-06-05 15:56:36,637 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f8c862de050>
2020-06-05 15:56:36,637 - INFO - allennlp.nn.initializers - Initializing parameters
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _classification_layer.bias
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _classification_layer.weight
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _feedforward._linear_layers.0.bias
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _feedforward._linear_layers.0.weight
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight
2020-06-05 15:56:36,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight
2020-06-05 15:56:36,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.output.dense.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.output.dense.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.key.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.key.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.query.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.query.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.value.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.value.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.intermediate.dense.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.intermediate.dense.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.output.LayerNorm.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.output.LayerNorm.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.output.dense.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.output.dense.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.output.dense.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.output.dense.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.key.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.key.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.query.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.query.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.value.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.value.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.intermediate.dense.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.intermediate.dense.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.output.LayerNorm.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.output.LayerNorm.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.output.dense.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.output.dense.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.output.dense.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.output.dense.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.key.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.key.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.query.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.query.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.value.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.value.weight
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.intermediate.dense.bias
2020-06-05 15:56:36,640 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.intermediate.dense.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.output.LayerNorm.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.output.LayerNorm.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.output.dense.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.output.dense.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.output.dense.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.output.dense.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.key.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.key.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.query.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.query.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.value.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.value.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.intermediate.dense.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.intermediate.dense.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.output.LayerNorm.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.output.LayerNorm.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.output.dense.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.output.dense.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.output.dense.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.output.dense.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.key.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.key.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.query.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.query.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.value.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.value.weight
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.intermediate.dense.bias
2020-06-05 15:56:36,641 - INFO - allennlp.nn.initializers -    _text_field_em",,,,,,,1,,,
1922,https://github.com/allenai/allennlp/issues/4599,4599,"[{'id': 887719346, 'node_id': 'MDU6TGFiZWw4ODc3MTkzNDY=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Contributions%20welcome', 'name': 'Contributions welcome', 'color': '02b8d1', 'default': False, 'description': ''}, {'id': 1875662321, 'node_id': 'MDU6TGFiZWwxODc1NjYyMzIx', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Feature%20request', 'name': 'Feature request', 'color': '5558ba', 'default': False, 'description': ''}]",closed,2020-08-25 21:29:14+00:00,,1,"When loading archived fine-tuned models for prediction, prevent non-fine-tuned pretrained transformer models from being downloaded ","**Is your feature request related to a problem? Please describe.**
When loading an archived model with a pretrained transformer embedder for prediction, where the model has been fine-tuned on a dataset, a huggingface pretrained transformer model is always downloaded to ~/.cache/torch. Then the archived model is loaded and replaces the downloaded model. When doing prediction with an existing archived model, the huggingface download is not necessary.

**Describe the solution you'd like**
For prediction, prevent pretrained transformer models from being downloaded.

**Describe alternatives you've considered**
There doesn't seem to be a way to pass an argument to from_path() to prevent the model from being downloaded.

**Additional context**

The downloading happens here:
model.py: ```model = Model.from_params(vocab=vocab, params=model_params)```
pretrained_transformer_embedder.py: 
```        
self.transformer_model = cached_transformers.get(
            model_name, True, override_weights_file, override_weights_strip_prefix
        )
```
cached_transformers.py: ```transformer = AutoModel.from_pretrained(model_name)```

There is logic in model.py to remove pretrained embedding parameters: ```remove_pretrained_embedding_params(model_params)```
However, this seems to only target pretrained embeddings like Glove via the `pretrained_file` config file parameter.
",,,,,,,1,,,
2106,https://github.com/allenai/allennlp/issues/5032,5032,[],closed,2021-03-02 16:17:29+00:00,,2,Textual Entailment PREDICTION code for Python missing a comma on the official demo page,"As this is not a bug or just a mistake, I found a syntax error while researching the Textual Entailment feature on the website (https://demo.allennlp.org/textual-entailment/elmo-snli).

The snippet for the predictor is missing a comma and will yield a syntax error:

```
from allennlp.predictors.predictor import Predictor
import allennlp_models.tagging

predictor = Predictor.from_path(""https://storage.googleapis.com/allennlp-public-models/decomposable-attention-elmo-2020.04.09.tar.gz"")
predictor.predict(
    premise=""Two women are wandering along the shore drinking iced tea."" 
    hypothesis=""Two women are sitting on a blanket near some rocks talking about politics.""
)
```",,,,,,,1,,,
159,https://github.com/allenai/allennlp/issues/1023,1023,[],closed,2018-03-23 21:06:28+00:00,,2,Apply Elmo to non-english task,"Hi, since the source code is not available (#922),  is it appropriate to apply Elmo to non-english task?

Besides, according to #939, ```python -m allennlp.elmo <args>``` is suggested to be used to dump the vectors for a predefined corpus.  But the command doesn't work actually. 

```No module named allennlp.elmo```",,,,,,1,,,,
74,https://github.com/allenai/allennlp/issues/770,770,[],closed,2018-02-05 17:36:21+00:00,,3,Give a more helpful error message when model loading fails during training,"https://github.com/allenai/allennlp/issues/756#issuecomment-362942024 brings up an error that I've seen somewhat frequently.  When you're still iterating on your model, it's really easy to run into a situation where allennlp tries to continue training from a previous epoch of a different model configuration.  The error message is particularly unhelpful in these situations.  It's even worse that the error message only appears _after_ you've loaded your data and computed the vocabulary, because these might take several minutes.

At the beginning of the `train` command (before loading any data), we should check if the `serialization` directory exists.  If it does, there are a few (not mutually-exclusive) options:
1. Check if the saved model parameters there match the current model parameters.  If they do, log a big message that we'll be continuing training from where it left off, noting that if you've changed model code things might crash later with cryptic messages.  If they don't, crash with an informative error message.
2. Add an `--overwrite-results` flag, or something, that always removes any existing serialization directory before training.
3. Catch a `RuntimeError` when we call `_restore_checkpoint` in `Trainer`, and change it into a more informative error message, suggesting using `--overwrite-results`, or something.

Actually, I'd probably recommend doing all three of those.",,,,1,1,,,,,
131,https://github.com/allenai/allennlp/issues/950,950,[],closed,2018-03-05 06:37:52+00:00,,2,Loading weights for part of the saved model?,"Now I can use the fine tune command to process different datasets, however, if the model architecture is changed, the train process need to restart from 0?",,,,1,1,,,,,
269,https://github.com/allenai/allennlp/issues/1289,1289,[],closed,2018-05-26 00:42:59+00:00,,7,Saving and loading an in-memory `Model` & `config.json` is very hard,"Here is a script I just wrote to load a model with the custom LSTM kernel and re-pack it into a new model so we can put it in the demo and distribute it. Turned out that we don't have any functionality for creating an archive from an in memory `Model` and `Config` and doing so was pretty messy. Hacking stuff like this is pretty key for e.g transfer learning and research in general - ""what happens if I take part of this model and put it in this other one"".


```
from allennlp.models.archival import load_archive, archive_model
from allennlp.common.params import Params
from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder

import tarfile
import torch
import numpy as np
import os
import shutil
import json

root_dir = '/net/efs/aristo/allennlp/srl_lm/emnlp/srl/5b-elmo/'
fname = '/net/efs/aristo/allennlp/srl_lm/emnlp/srl/5b-elmo/model.tar.gz'

archive = load_archive(fname)


model = archive.model
params = archive.config
new_params = params.duplicate()

encoder_params = params.get(""model"").get(""encoder"").as_dict()
new_encoder_params = encoder_params
new_encoder_params[""type""] = ""alternating_lstm""
new_encoder_params[""use_input_projection_bias""] = False
new_params[""model""][""encoder""] = new_encoder_params

old_encoder = model.encoder._module
new_encoder = Seq2SeqEncoder.from_params(Params(new_encoder_params.copy()))


weight_index = 0
bias_index = 0

num_layers = 8
for layer_index in range(num_layers):
    layer = getattr(new_encoder._module, 'layer_%d' % layer_index)
    input_weight = layer.input_linearity.weight
    state_weight = layer.state_linearity.weight
    bias = layer.state_linearity.bias

    input_weight.data.t().copy_(old_encoder.weight.data[weight_index: weight_index + input_weight.nelement()].view_as(input_weight.t()))
    weight_index += input_weight.nelement()

    state_weight.data.t().copy_(old_encoder.weight.data[weight_index: weight_index + state_weight.nelement()].view_as(state_weight.t()))

    weight_index += state_weight.nelement()

    bias.data.copy_(old_encoder.bias.data[bias_index:bias_index + bias.nelement()])
    bias_index += bias.nelement()

model.encoder = new_encoder

serialisation_dir = ""./new_model/""
os.makedirs(""./new_model/"")

shutil.copytree(os.path.join(root_dir, ""vocabulary""), os.path.join(serialisation_dir, ""vocabulary""))

with open(os.path.join(root_dir, ""files_to_archive.json""), ""r"") as f:
    fta = json.loads(f.read())

print(fta)
with open(os.path.join(serialisation_dir, ""config.json""), ""w"") as f:
    f.write(json.dumps(new_params.as_dict(quiet=True)))

model_state = model.state_dict()
torch.save(model_state, os.path.join(serialisation_dir, ""best.th""))

archive_model(serialisation_dir, files_to_archive=fta)
```
",,,,1,1,,,,,
334,https://github.com/allenai/allennlp/issues/1441,1441,[],closed,2018-06-29 12:23:09+00:00,,3,Loading a pretrained model,"How do I load a pretrained model programmatically that was created using the command line. If I use the -all option, it creates the hdf5 file. How can I use this file to generate embeddings for a new sentence? Looks like the Elmo class requires an option file and a weights file. How can I get those from the hdf5 file? 

Thank you.",,,,1,1,,,,,
499,https://github.com/allenai/allennlp/issues/1826,1826,[],closed,2018-09-26 20:28:40+00:00,,4,Vocabulary loading includes hidden files,"Not exactly a bug, but when loading a vocabulary from files, it automatically loads every file in the `vocabulary` directory. This might include hidden files (such as those starting with a period in the filename) which, say, MacOS has decided to insert. It would probably be better to avoid this.
",,,,1,1,,,,,
675,https://github.com/allenai/allennlp/issues/2148,2148,[],closed,2018-12-06 09:21:45+00:00,,2,Event2Mind loading state_dict error,"Sorry for misused terms - very new here.

When loading the event2mind model from path, archive or using the command line I encountered the following error:

`RuntimeError: Error(s) in loading state_dict for Event2Mind:
	Missing key(s) in state_dict: ""_states.xintent.embedder.weight"", ""_states.xintent.decoder_cell.weight_ih"", ""_states.xintent.decoder_cell.weight_hh"", ""_states.xintent.decoder_cell.bias_ih"", ""_states.xintent.decoder_cell.bias_hh"", ""_states.xintent.output_projection_layer.weight"", ""_states.xintent.output_projection_layer.bias"", ""_states.xreact.embedder.weight"", ""_states.xreact.decoder_cell.weight_ih"", ""_states.xreact.decoder_cell.weight_hh"", ""_states.xreact.decoder_cell.bias_ih"", ""_states.xreact.decoder_cell.bias_hh"", ""_states.xreact.output_projection_layer.weight"", ""_states.xreact.output_projection_layer.bias"", ""_states.oreact.embedder.weight"", ""_states.oreact.decoder_cell.weight_ih"", ""_states.oreact.decoder_cell.weight_hh"", ""_states.oreact.decoder_cell.bias_ih"", ""_states.oreact.decoder_cell.bias_hh"", ""_states.oreact.output_projection_layer.weight"", ""_states.oreact.output_projection_layer.bias"". 
	Unexpected key(s) in state_dict: ""xintent_embedder.weight"", ""xintent_decoder_cell.weight_ih"", ""xintent_decoder_cell.weight_hh"", ""xintent_decoder_cell.bias_ih"", ""xintent_decoder_cell.bias_hh"", ""xintent_output_project_layer.weight"", ""xintent_output_project_layer.bias"", ""xreact_embedder.weight"", ""xreact_decoder_cell.weight_ih"", ""xreact_decoder_cell.weight_hh"", ""xreact_decoder_cell.bias_ih"", ""xreact_decoder_cell.bias_hh"", ""xreact_output_project_layer.weight"", ""xreact_output_project_layer.bias"", ""oreact_embedder.weight"", ""oreact_decoder_cell.weight_ih"", ""oreact_decoder_cell.weight_hh"", ""oreact_decoder_cell.bias_ih"", ""oreact_decoder_cell.bias_hh"", ""oreact_output_project_layer.weight"", ""oreact_output_project_layer.bias"". `

It looked to me like the state_dict for pytorch was missing the _states prefix for some of the keys, so I added these lines to Model.py, immediately after the definition of model_state:

`        missing_keys = [""_states.xintent.embedder.weight"", ""_states.xintent.decoder_cell.weight_ih"", ""_states.xintent.decoder_cell.weight_hh"", ""_states.xintent.decoder_cell.bias_ih"", ""_states.xintent.decoder_cell.bias_hh"", ""_states.xintent.output_projection_layer.weight"", ""_states.xintent.output_projection_layer.bias"", ""_states.xreact.embedder.weight"", ""_states.xreact.decoder_cell.weight_ih"", ""_states.xreact.decoder_cell.weight_hh"", ""_states.xreact.decoder_cell.bias_ih"", ""_states.xreact.decoder_cell.bias_hh"", ""_states.xreact.output_projection_layer.weight"", ""_states.xreact.output_projection_layer.bias"", ""_states.oreact.embedder.weight"", ""_states.oreact.decoder_cell.weight_ih"", ""_states.oreact.decoder_cell.weight_hh"", ""_states.oreact.decoder_cell.bias_ih"", ""_states.oreact.decoder_cell.bias_hh"", ""_states.oreact.output_projection_layer.weight"", ""_states.oreact.output_projection_layer.bias""]
        unexpected_keys = [""xintent_embedder.weight"", ""xintent_decoder_cell.weight_ih"", ""xintent_decoder_cell.weight_hh"", ""xintent_decoder_cell.bias_ih"", ""xintent_decoder_cell.bias_hh"", ""xintent_output_project_layer.weight"", ""xintent_output_project_layer.bias"", ""xreact_embedder.weight"", ""xreact_decoder_cell.weight_ih"", ""xreact_decoder_cell.weight_hh"", ""xreact_decoder_cell.bias_ih"", ""xreact_decoder_cell.bias_hh"", ""xreact_output_project_layer.weight"", ""xreact_output_project_layer.bias"", ""oreact_embedder.weight"", ""oreact_decoder_cell.weight_ih"", ""oreact_decoder_cell.weight_hh"", ""oreact_decoder_cell.bias_ih"", ""oreact_decoder_cell.bias_hh"", ""oreact_output_project_layer.weight"", ""oreact_output_project_layer.bias""]`

`for missing_key, unexpected_key in zip(missing_keys, unexpected_keys):
        model_state[missing_key] = model_state[unexpected_key]
        del model_state[unexpected_key]`

This did the trick, but I'm guessing something less hacky would be ideal?

**System:**
 - OS: Linux
 - Python version: 3.7.0
 - AllenNLP version: 0.7.2",,,,1,1,,,,,
759,https://github.com/allenai/allennlp/issues/2311,2311,[],closed,2019-01-08 20:48:06+00:00,,14,Add support for loading and saving models that were created from code,"**Describe the bug**
If a model has an `__init__(self, *args, **kwargs)` method that introduce some additional behavior (note the `*args` or `**kwargs` arguments), `Model.from_params` raises an error `ConfigurationError: 'key ""kwargs"" is required at location """"'`

**To Reproduce**
Run the code below:
```
from allennlp.common import Params
from allennlp.models import Model
from allennlp.data import Vocabulary

@Model.register('base_model')
class BaseModel(Model):
    def __init__(self, hidden_size: int, vocab: Vocabulary) -> None:
        super().__init__(vocab)

        self.hidden_size = hidden_size
        
@Model.register('second_model')
class SecondModel(BaseModel):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        
        self.new_layer = 128

second_model_params = Params({'type': 'second_model', 'hidden_size': 128})

second_model = Model.from_params(second_model_params, vocab=Vocabulary())
```

**Expected behavior**
An object of the class SecondModel is created

**System (please complete the following information):**
 - OS: Ubuntu 18.04
 - Python version: 3.6.7
 - AllenNLP version: 0.8.2-unreleased, installed from master
 - PyTorch version: 1.0.0

**Additional context**
In addition, it prevents model inheritance with additional parameters:
```def __init__(self, new_param, *args, **kwargs)```

This is similar to #1480 and removing the `__init__` method allows creating of the model, but in my case I need the `__init__` as it adds some additional functionality. 
",,,,1,1,,,,,
801,https://github.com/allenai/allennlp/issues/2398,2398,[],closed,2019-01-19 12:05:05+00:00,,11,embedding is different on same input after loading from fine-tuned model,"Hi,

I have raised this issue before but unfortunately that didn't help me.
I have a model which uses elmo and after training my model (fine-tuning elmo alongside), I load my trained model.

```python
    model.eval()
    state = T.load(""model.pth"")
    model.load_state_dict(state['state_dict'])
```

Then I try to check the embedding of the fine-tuned elmo from inside the trained model. I get the embedding on a sample input repeatedly and print the embedding sum after every 10 steps. The sum is varying a lot which is consequently changing my model predictions. I have tried to follow the notes on statefulness but that didn't help either.

```python
    for step in range(100):
        # embedding = model.elmo(query_[:3])['elmo_representations'][0]
        # gs = model(query_, good_)
        user_query = 'uses of iphone 4s'.split(' ')
        embedding = model.elmo(batch_to_ids([user_query]).to(T.device(args.device)))['elmo_representations'][0]
        if step % 10 == 0:
            print(embedding.sum())
```

`tensor(-63.9617, device='cuda:3', grad_fn=)        

tensor(-32.3701, device='cuda:3', grad_fn=)                                                                                                                                          
tensor(-59.4102, device='cuda:3', grad_fn=)                                                                                                                                          
tensor(-56.8035, device='cuda:3', grad_fn=)                                                                                                                                          
tensor(-31.6015, device='cuda:3', grad_fn=)                                                                                                                                          
tensor(-24.1763, device='cuda:3', grad_fn=)                                                                                                                                          
tensor(-25.7312, device='cuda:3', grad_fn=)                                                                                                                                          
tensor(-25.7935, device='cuda:3', grad_fn=)                                                                                                                                          
tensor(-25.0103, device='cuda:3', grad_fn=)                                                                                                                                          
tensor(-24.9410, device='cuda:3', grad_fn=)`




As you can see from the o/p, there is a inconsistency with the embedding returned. Can you please help?",,,,1,1,,,,,
857,https://github.com/allenai/allennlp/issues/2506,2506,[],closed,2019-02-13 01:47:44+00:00,,4,how to turn off INFO messages sent to console (loading SRL model),"Please first search our GitHub repository for similar questions.  If you don't find a similar example you can use the following template:

**System (please complete the following information):**
 - OS: [e.g. Windows]
 - Python version: [3.6.7]
 - AllenNLP version: ""I installed from master""]
 - PyTorch version: 1.0.1

**Question**
* I am calling ""Predictor.from_path()"" on the SRL model in my python app (as per docs example 
 at https://allennlp.org/models).  When I run, I see over 100 timestamped INFO messages being sent to the console.  I want to control the output of my app (for clean debugging and for release to users).  How can I turn off these messages (and make room for my own console output)?  Thanks in advance. *

",,,,1,1,,,,,
888,https://github.com/allenai/allennlp/issues/2561,2561,[],closed,2019-03-01 21:43:17+00:00,,1,Best practice in loading pre-trained BERT classification layers ?,"I see on the [HuggingFaces](https://github.com/huggingface/pytorch-pretrained-BERT) repository that there is an option to download the Bert encoder along with the different pre-trained classification layers like the pre-trained MaskedLM classification layer. Is there an easy or recommended way to load these pre-trained classification layers in AllenNLP?

Thanks!
Anthony",,,,1,1,,,,,
1162,https://github.com/allenai/allennlp/issues/3026,3026,[],closed,2019-07-01 04:48:52+00:00,,5,Making gradients flow after loading archive(trained model) in evaluation phase,"Hi, I'm currently working on BERT Language model. I trained my model based on BERT and saved my model as an archive. Then in the evaluation(prediction) phase, I loaded my trained model with ""allennlp.models.archival.load_archive"" and predicted the test set. But what I want here is that the gradient flows from the evaluation dataset and weights are finetuned accordingly. (such as from Mikolov's ""Recurrent Neural Network Based Language Model""). This question might be silly, but I wonder if  there is a way that I can make gradient flow to the archive model(trained model)? (as in the prediction phase, not as in training phase)",,,,1,1,,,,,
1189,https://github.com/allenai/allennlp/issues/3079,3079,[],closed,2019-07-18 10:56:25+00:00,,12,Native pytorch Multiprocessing for data loading,"

### Summary
We've struggled for a while to make our multiprocessing code performant (and in it's current state, i'm not actually sure it works). Instead of doing this ourselves (or even trying to fix what we currently have), we should use the optimized `torch.util.data.DatasetLoader` when pytorch release v1.1.X.

I dug into this a bit and I think I came up with a solution that will work with the _next_ pytorch release. The reason for this is the introduction of a `torch.utils.data.IterableDataset`, which allows datasets to be viewed as streams (compared to being indexable, which currently they are required to be).
https://pytorch.org/docs/master/data.html#torch.utils.data.IterableDataset

### Current Blockers

1. The pytorch `Dataset` class assumes all datasets have a `__len__` method, which may not be true of allennlp datasets, if they are lazy, or infinite streams.

2. The way pytorch exposes creating iterables is via `__getitem__` and correspondingly which indices a batch is composed of. This is not suitable for allennlp, where we must be able to control the full generator to do things like bucketing by length.

### Implementation idea

I think that this can be implemented in a backward compatible way via:

1. A `torch.utils.data.IterableDataset` subclass which performs the functions of our iterators, e.g loading `max_instances_in_memory` at a time, sorting by length etc but still yielding single instances.

2. Modifying `MultiProcessDataIterator` to create a `DataLoader` inside its `__call__` method (as it has the same `Iterable[TensorDict]` api as the `DataLoader`), where internally, the `Iterable[Instances]` it is passed is expected to be the `IterableDataset` subclass from above.

Basically the current `Iterator` functionality would be moved into subclassed `IterableDatasets` and we would iterate over these datasets using only the `DataLoader`, where the only job of the DataLoader would be to 1. do multiprocessing and 2. convert instances to tensor dicts.


### Controlling multiple workers

Rather than having wrapper classes which control sharding in dataset readers like the `MultiprocessDatasetIterator/Reader`, dataset_readers should control their own sharding if they expect to be used for multiprocess reading using `torch.utils.data.get_worker_info()`. This returns `None` if not in a data reading process, but returns worker information if it is. This could be used to e.g read different data shards from a directory.

### Possible difficult things

1. Variable sized batches - some of the allennlp data functionality allows you to return variable sized batches, such as batches with a fixed number of tokens in. I'm not sure how to do this currently with the torch dataloader. I think it involves implementing a `BatchSampler` which returns different lengths of indices, but it's a little complicated because we want to view our datasets as streams, and not indexable objects, so i'm not quite sure how that would work.

2. Caching instances - I'm not sure how this works with multiprocessing - but part of the reason we need it in the first place is because the multiprocessing doesn't work properly anyway.

Below is an example of a very simple wrapper of an allennlp dataset reader to allow use with the `DataLoader` from pytorch. Currently it is difficult to bucket this reader, because of the way we are forced to access only one element at a time.

```python

from torch.utils.data import Dataset, DataLoader


from allennlp.data.dataset_readers import LanguageModelingReader, DatasetReader
from allennlp.data.fields.text_field import TextField
from allennlp.data.dataset import Batch
from allennlp.data.instance import Instance
from allennlp.data.vocabulary import Vocabulary

class AllennlpDataset(Dataset):

    def __init__(self,
                 reader: DatasetReader,
                 dataset_path: str):

        self.reader = reader
        self.dataset_path = dataset_path

        self.iterable = self.reader.read(dataset_path)
        self.iterator = (x for x in self.iterable)

        self._length = None

    def __len__(self):
        """"""
        This is gross but will go away in the next pytorch release,
        as they are introducing an `IterableDataset` class which doesn't
        need to have a length:
        https://pytorch.org/docs/master/data.html#torch.utils.data.IterableDataset
        """"""
        if self._length is None:
            self._length = 0
            for i in self.iterator:
                self._length += 1
            self.iterator = (x for x in self.iterable)
        return self._length

    def __getitem__(self, idx) -> Instance:
        get_next = next(self.iterator, None)
        if get_next is None:
            self.iterator = (x for x in self.iterable)
            get_next = next(self.iterator)
        return get_next


vocab = Vocabulary.from_instances((dataset[i] for i in range(len(dataset))))

# Function to tell the torch DataLoader how to batch up our custom data, i.e Instances
def allennlp_collocate(batch):
    batch = Batch(batch)
    batch.index_instances(vocab)
    return batch.as_tensor_dict(batch.get_padding_lengths())

dataset = AllennlpDataset(LanguageModelingReader(lazy=True), ""./baby_data.txt"")
loader = DataLoader(dataset, batch_size=2, num_workers =2, collate_fn=allennlp_collocate)

for batch in loader:
    print(batch)
```


Let me know what you think or if you can foresee any other big problems!
@matt-gardner, @joelgrus and @brendan-ai2, would be good to get your opinion on this.",,,,1,1,,,,,
1330,https://github.com/allenai/allennlp/issues/3329,3329,[],closed,2019-10-06 21:26:26+00:00,,1,FileNotFoundError on loading Elmo model,"**Describe the bug**
can't seem to load elmo_token_embedder with `2x4096_512_2048cnn_2xhighway_options.json` not found. I tried manually going in the location and add that file but it's not picking that up either

**System (please complete the following information):**
 - OS: macos 10.14.4
 - Python version: 3.7
 - AllenNLP version: 0.7.0
 - PyTorch version: 0.4.1

**traceback**

```
10/06/2019 17:17:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.elmo.type = elmo_token_embedder
[2019-10-06 17:17:54 -0400] [62395] [ERROR] Exception in worker process
Traceback (most recent call last):
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/gunicorn/arbiter.py"", line 583, in spawn_worker
    worker.init_process()
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/gunicorn/workers/base.py"", line 129, in init_process
    self.load_wsgi()
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/gunicorn/workers/base.py"", line 138, in load_wsgi
    self.wsgi = self.app.wsgi()
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/gunicorn/app/base.py"", line 67, in wsgi
    self.callable = self.load()
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py"", line 52, in load
    return self.load_wsgiapp()
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py"", line 41, in load_wsgiapp
    return util.import_app(self.app_uri)
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/gunicorn/util.py"", line 362, in import_app
    app = eval(obj, vars(mod))
  File ""<string>"", line 1, in <module>
  File ""/Users/anushkmittal/downloads/hmtl/demo/server.py"", line 51, in build_app
    ALL_RESOURCE = AllResource(model_name=model_name, mode=mode)
  File ""/Users/anushkmittal/downloads/hmtl/demo/server.py"", line 15, in __init__
    self.jmd = HMTLPredictor(model_name=model_name)
  File ""/Users/anushkmittal/downloads/hmtl/demo/hmtlPredictor.py"", line 129, in __init__
    model, vocab, token_indexers = load_model(model_name=model_name)
  File ""/Users/anushkmittal/downloads/hmtl/demo/hmtlPredictor.py"", line 115, in load_model
    model = Model.from_params(vocab=vocab, params=model_params, regularizer=None)
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 245, in from_params
    return subclass.from_params(params=params, **extras)
  File ""../hmtl/models/hmtl.py"", line 200, in from_params
    return cls(vocab=vocab, params=params, regularizer=regularizer)
  File ""../hmtl/models/hmtl.py"", line 49, in __init__
    text_field_embedder = BasicTextFieldEmbedder.from_params(vocab=vocab, params=text_field_embedder_params)
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"", line 117, in from_params
    for name, subparams in token_embedder_params.items()
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"", line 117, in <dictcomp>
    for name, subparams in token_embedder_params.items()
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/allennlp/common/from_params.py"", line 245, in from_params
    return subclass.from_params(params=params, **extras)
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/allennlp/modules/token_embedders/elmo_token_embedder.py"", line 98, in from_params
    params.add_file_to_archive('options_file')
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/allennlp/common/params.py"", line 157, in add_file_to_archive
    self.files_to_archive[f""{self.history}{name}""] = cached_path(self.get(name))
  File ""/Users/anushkmittal/miniconda3/envs/hmtl_two/lib/python3.7/site-packages/allennlp/common/file_utils.py"", line 92, in cached_path
    raise FileNotFoundError(""file {} not found"".format(url_or_filename))
FileNotFoundError: file ../data/elmo/2x4096_512_2048cnn_2xhighway_options.json not found
```",,,,1,1,,,,,
1432,https://github.com/allenai/allennlp/issues/3496,3496,[],closed,2019-12-01 07:16:36+00:00,,0,FileNotFoundError when loading SNLI dataset,"I encountered FileNotFoundError when loading SNLI test set while it should exist (I was able to download the file using the url)
```
single_id_indexer = SingleIdTokenIndexer(lowercase_tokens=True) 
tokenizer = WordTokenizer(end_tokens=[""@@NULL@@""])
test_reader = SnliReader(token_indexers={'tokens': single_id_indexer}, tokenizer=tokenizer)
test_dataset = test_reader.read('https://s3-us-west-2.amazonaws.com/allennlp/datasets/snli/snli_1.0_test.jsonl')
```

```
0it [00:00, ?it/s]FileNotFoundError(""file :'https://s3-us-west-2.amazonaws.com/allennlp/datasets/snli/snli_1.0_test.jsonl' not found"",)
> /home/user/miniconda3/envs/triggers/lib/python3.6/site-packages/allennlp/common/file_utils.py(106)cached_path()
    105         # File, but it doesn't exist.
--> 106         raise FileNotFoundError(""file {} not found"".format(url_or_filename))
    107     else:
```

btw loading the dev set runs without error
```
single_id_indexer = SingleIdTokenIndexer(lowercase_tokens=True) 
tokenizer = WordTokenizer(end_tokens=[""@@NULL@@""])
dev_reader = SnliReader(token_indexers={'tokens': single_id_indexer}, tokenizer=tokenizer)
dev_dataset = dev_reader.read('https://s3-us-west-2.amazonaws.com/allennlp/datasets/snli/snli_1.0_dev.jsonl')
```",,,,1,1,,,,,
1498,https://github.com/allenai/allennlp/issues/3641,3641,[],closed,2020-01-17 14:03:51+00:00,,2,Error when loading dependency-predictor: biaffine-dependency-parser-ptb-2018.08.23.tar.gz,"```
Traceback (most recent call last):
  File ""/home/sxx/Desktop/allenNLPtest/test.py"", line 2, in <module>
    predictor = Predictor.from_path(""/home/sxx/Desktop/biaffine-dependency-parser-ud-2018.08.23.tar.gz"")
  File ""/home/sxx/Desktop/allennlp-master/allennlp/predictors/predictor.py"", line 267, in from_path
    load_archive(archive_path, cuda_device=cuda_device),
  File ""/home/sxx/Desktop/allennlp-master/allennlp/models/archival.py"", line 243, in load_archive
    cuda_device=cuda_device,
  File ""/home/sxx/Desktop/allennlp-master/allennlp/models/model.py"", line 330, in load
    return model_class._load(config, serialization_dir, weights_file, cuda_device)
  File ""/home/sxx/Desktop/allennlp-master/allennlp/models/model.py"", line 267, in _load
    model = Model.from_params(vocab=vocab, params=model_params)
  File ""/home/sxx/Desktop/allennlp-master/allennlp/common/from_params.py"", line 435, in from_params
    **extras,
  File ""/home/sxx/Desktop/allennlp-master/allennlp/common/from_params.py"", line 463, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File ""/home/sxx/Desktop/allennlp-master/allennlp/common/from_params.py"", line 167, in create_kwargs
    cls.__name__, param_name, annotation, param.default, params, **extras
  File ""/home/sxx/Desktop/allennlp-master/allennlp/common/from_params.py"", line 265, in construct_arg
    return annotation.from_params(params=subparams, **subextras)
  File ""/home/sxx/Desktop/allennlp-master/allennlp/common/from_params.py"", line 435, in from_params
    **extras,
  File ""/home/sxx/Desktop/allennlp-master/allennlp/common/from_params.py"", line 463, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File ""/home/sxx/Desktop/allennlp-master/allennlp/common/from_params.py"", line 170, in create_kwargs
    params.assert_empty(cls.__name__)
  File ""/home/sxx/Desktop/allennlp-master/allennlp/common/params.py"", line 462, in assert_empty
    ""Extra parameters passed to {}: {}"".format(class_name, self.params)
allennlp.common.checks.ConfigurationError: ""Extra parameters passed to BasicTextFieldEmbedder: {'tokens': {'embedding_dim': 100, 'sparse': True, 'trainable': True, 'type': 'embedding'}}""

Process finished with exit code 1
```
",,,,1,1,,,,,
1586,https://github.com/allenai/allennlp/issues/3884,3884,[],closed,2020-03-02 15:19:57+00:00,,3,Support fast embeddings loading,"**Is your feature request related to a problem? Please describe.**
I麓m frustrated when I have to load a model, and it takes a long time to load a per-trained model file to predict.

**Describe the solution you'd like**

Some solutions such as: 
https://github.com/ThoughtRiver/lmdb-embeddings

Convert the embedding to a LMDB database, and promise to : ""a traditional approach glove-840B takes around 2 minutes to load and 4GB in memory. Managed with LMDB, glove-840B can be accessed immediately and takes only a couple MB in memory, for a negligible impact on runtime (around 1% slower).""

Would allennlp accept a PR with a dependency with lmdb-embeddings?",,,,1,1,,,,,
1670,https://github.com/allenai/allennlp/issues/4058,4058,[],closed,2020-04-12 04:53:14+00:00,,1,Sentiment Analysis Roberta large loading error,"**Describe the bug**
ConfigurationError: 'pretrained_transformer_mismatched not in acceptable choices for model.text_field_embedder.token_embedders.tokens.type: [\'embedding\', \'character_encoding\', \'elmo_token_embedder\', \'elmo_token_embedder_multilang\', \'openai_transformer_embedder\', \'bert-pretrained\', \'language_model_token_embedder\', \'bidirectional_lm_token_embedder\', \'bag_of_word_counts\', \'pass_through\', \'pretrained_transformer\']. You should either use the --include-package flag to make sure the correct module is loaded, or use a fully qualified class name in your config file like {""model"": ""my_module.models.MyModel""} to have it imported automatically.'

**To Reproduce**
Steps to reproduce the behavior
```python
from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path(""https://storage.googleapis.com/allennlp-public-models/sst-roberta-large-2020.02.17.tar.gz"")
predictor.predict(
  sentence=""a very well-made, funny and entertaining picture.""
)
```
**Expected behavior**
Should work.
",,,,1,1,,,,,
1683,https://github.com/allenai/allennlp/issues/4087,4087,[],closed,2020-04-16 23:25:21+00:00,,1,Data loading for multi-GPU has unexpected behavior,"It says this in the prerelease notes:
> TODO: As of #3529 dataset readers used in the distributed setting need to shard instances to separate processes internally. We should fix this before releasing a final version.

We should follow up on whether this is still true and whether it still matters.",,,1,1,1,,,,,
234,https://github.com/allenai/allennlp/issues/1206,1206,[],closed,2018-05-13 01:14:13+00:00,,4,TestTrainer.test_trainer_respects_keep_serialized_model_every_num_seconds fails when running without GPU on GPU machine,"Rather odd situation: when running the tests on a GPU machine with `CUDA_VISIBLE_DEVICES=""""`, [`TestTrainer.test_trainer_respects_keep_serialized_model_every_num_seconds`](https://github.com/allenai/allennlp/blob/master/tests/training/trainer_test.py#L160) consistently fails since each epoch takes a rather sluggish ~1.4 seconds, causing every epoch checkpoint to be saved (thus failing the test).

The easy fix would be to just increase the sleep to 2.5 seconds and `keep_serialized_model_every_num_seconds` to 5, but not sure if that's a fix worth making.

Interestingly, it works fine as is on a machine without a GPU (my macbook). Not too sure what to make of this...",,1,1,,1,,,,,
1298,https://github.com/allenai/allennlp/issues/3265,3265,[],closed,2019-09-19 11:28:04+00:00,,10,Slow training in machines with multiple GPUs,"**System (please complete the following information):**
 - OS: Linux Ubuntu 18.04.3 LTS
 - Python version: 3.6.8
 - AllenNLP version: v0.8.5
 - PyTorch version: 1.2.0

**Question**
I have used used AllenNLP NER training in multiple machines with multiple GPUs (DGX1, IBM AC922 and a custom DIGITS DevBox) and I have verified the following behavior in all of them:

1. There is no difference of training time when you use one or more GPUs for the **same** training. If you use 2 GPUs instead of 1, the amount of batches reduces by half, but each batch takes twice the time to process, so it doesn't make any difference. 

2. When you have multiple trainings, running each one in a separate GPU (using CUDA_VISIBLE_DEVICES), the first training starts OK, but the following ones seems to perform very slowly, even though they are running on separate GPUs, hidden from each other using CUDA_VISIBLE_DEVICES.  The same happens with inference as well.

Is this normal behavior? Anything I could do to improve this? I'm having a hard time taking advantage of robust machines with multiple GPUs. This has happened since AllenNLP 0.7.2 running PyTorch 0.4.1 as well, in all machines I used. 

One note is that for AC922 and DGX1, the GPUs are NVLinked, but in the DIGITS DevBox there isn't this connection. And the behavior is the same anyway. 

Thanks!",,1,1,,1,,,,,
25,https://github.com/allenai/allennlp/issues/583,583,[],closed,2017-12-06 21:31:57+00:00,,1,simple_seq2seq GPU error,"At the line below, _encoder_outputs_mask_ Tensor is explicitly cast to be a **CPU** Tensor, which results in error if the model runs on GPU:
https://github.com/allenai/allennlp/blob/7f5d9f7587e1a799e161c37e3383686323e34956/allennlp/models/encoder_decoders/simple_seq2seq.py#L217-L219

Possible fix might look like this:
```python
# Ensuring mask is also a FloatTensor. Or else the multiplication within attention will
# complain.
mask_type = torch.cuda.FloatTensor if encoder_outputs_mask.is_cuda else torch.FloatTensor
encoder_outputs_mask = encoder_outputs_mask.type(mask_type)
```

If appropriate, I can make a PR. ",,,1,,1,,,,,
58,https://github.com/allenai/allennlp/issues/697,697,[],closed,2018-01-18 12:07:31+00:00,,12,GPU doesn't get recognised with docker image,"I can successfully run all tests on CPU but for some reason I can't get the GPU to work on a DGX-1. pytorch's official image works but allennlp one doesn't:

```
$ nvidia-docker run --rm -ti --ipc=host pytorch/pytorch:latest
root@5a84fdfadea1:/workspace# python -c ""import torch;print(torch.cuda.is_available())""
True

$ nvidia-docker run -p 8000:8000 -it --rm allennlp/allennlp
root@5fc4428edb93:/stage# python -c ""import torch;print(torch.cuda.is_available())""
False
```

Has anyone an idea about what might be going on?",,,1,,1,,,,,
69,https://github.com/allenai/allennlp/issues/758,758,[],closed,2018-02-02 17:51:43+00:00,,9,Multiple GPU training,See #749 for a discussion of remaining issues to fully support multiple GPU training.,,,1,,0,,,,,
100,https://github.com/allenai/allennlp/issues/860,860,[],closed,2018-02-15 20:43:42+00:00,,9,GPU Memory Exhausted while using Elmo,"Hello, I'm training the wikitables_semantic_parser with Elmo representations and am running out of gpu memory. I also don't have cudnn installed(since i don't have root access) and am running with the cudnn backend disabled. Here's the full stack trace:

```
2018-02-15 12:29:10,275 - INFO - allennlp.training.trainer - Epoch 0/19
2018-02-15 12:29:10,275 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 13889.476
2018-02-15 12:29:10,552 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 0
2018-02-15 12:29:10,552 - INFO - allennlp.training.trainer - GPU 1 memory usage MB: 673
2018-02-15 12:29:10,553 - INFO - allennlp.training.trainer - GPU 2 memory usage MB: 0
  0%|          | 0/6886 [00:00<?, ?it/s]2018-02-15 12:29:10,554 - INFO - allennlp.training.trainer - Training
parse_acc: 0.00, loss: 13.04 ||:  14%|#4        | 975/6886 [08:46<53:09,  1.85it/s]THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/THC/generic/THCStorage.cu line=58 error=2 : out of memory
Traceback (most recent call last):
  File ""anaconda3/envs/allennlp/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""anaconda3/envs/allennlp/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""allennlp/allennlp/run.py"", line 18, in <module>
    main(prog=""python -m allennlp.run"")
  File ""allennlp/allennlp/commands/__init__.py"", line 81, in main
    args.func(args)
  File ""allennlp/allennlp/commands/train.py"", line 107, in train_model_from_args
    train_model_from_file(args.param_path, args.serialization_dir, args.overrides, args.file_friendly_logging)
  File ""allennlp/allennlp/commands/train.py"", line 126, in train_model_from_file
    return train_model(params, serialization_dir, file_friendly_logging)
  File ""allennlp/allennlp/commands/train.py"", line 220, in train_model
    metrics = trainer.train()
  File ""allennlp/allennlp/training/trainer.py"", line 642, in train
    train_metrics = self._train_epoch(epoch)
  File ""allennlp/allennlp/training/trainer.py"", line 432, in _train_epoch
    loss = self._batch_loss(batch, for_training=True)
  File ""allennlp/allennlp/training/trainer.py"", line 369, in _batch_loss
    output_dict = self._model(**batch)
  File ""anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File ""allennlp/allennlp/models/encoder_decoders/wikitables_semantic_parser.py"", line 183, in forward
    table_mask = util.get_text_field_mask(table_text, num_wrapping_dims=1).float()
  File ""anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File ""allennlp/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"", line 52, in forward
    token_vectors = embedder(tensor)
  File ""anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File ""allennlp/allennlp/modules/time_distributed.py"", line 35, in forward
    reshaped_outputs = self._module(*reshaped_inputs)
  File ""anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File ""allennlp/allennlp/modules/token_embedders/elmo_token_embedder.py"", line 62, in forward
    elmo_output = self._elmo(inputs)
  File ""anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File ""allennlp/allennlp/modules/elmo.py"", line 103, in forward
    bilm_output = self._elmo_lstm(reshaped_inputs)
  File ""anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File ""allennlp/allennlp/modules/elmo.py"", line 441, in forward
    token_embedding = self._token_embedder(inputs)
  File ""anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File ""allennlp/allennlp/modules/elmo.py"", line 262, in forward
    convolved, _ = torch.max(convolved, dim=-1)
RuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/THC/generic/THCStorage.cu:58
```",,,1,,0,,,,,
121,https://github.com/allenai/allennlp/issues/919,919,[],closed,2018-02-24 19:45:04+00:00,,7,Using GPU for textual entailment prediction fails,"I tried to use GPU when doing an entailment prediction, i.e. by running:
```
decomposableAttentionPredictor = DEFAULT_MODELS['textual-entailment'].predictor()
decomposableAttentionPredictor.predict_json({'hypothesis': '...', 'premise': '...'}, 0)
```

and received the following error: 

```
Traceback (most recent call last):
  File ""brain.py"", line 30, in <module>
    decomposableAttentionPredictor.predict_json({'hypothesis': 'init', 'premise': 'init'}, 0)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/allennlp/service/predictors/predictor.py"", line 36, in predict_json
    outputs = self._model.forward_on_instance(instance, cuda_device)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/allennlp/models/model.py"", line 120, in forward_on_instance
    return self.forward_on_instances([instance], cuda_device)[0]
  File ""/home/me/anaconda3/lib/python3.6/site-packages/allennlp/models/model.py"", line 138, in forward_on_instances
    outputs = self.decode(self(**model_input))
  File ""/home/me/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/allennlp/models/decomposable_attention.py"", line 123, in forward
    embedded_premise = self._text_field_embedder(premise)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"", line 52, in forward
    token_vectors = embedder(tensor)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/allennlp/modules/token_embedders/elmo_token_embedder.py"", line 62, in forward
    elmo_output = self._elmo(inputs)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/allennlp/modules/elmo.py"", line 103, in forward
    bilm_output = self._elmo_lstm(reshaped_inputs)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/allennlp/modules/elmo.py"", line 441, in forward
    token_embedding = self._token_embedder(inputs)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/allennlp/modules/elmo.py"", line 243, in forward
    self._char_embedding_weights
  File ""/home/me/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py"", line 923, in embedding
    scale_grad_by_freq, sparse
  File ""/home/me/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py"", line 59, in forward
    output = torch.index_select(weight, 0, indices.view(-1))
TypeError: torch.index_select received an invalid combination of arguments - got (torch.FloatTensor, int, torch.cuda.LongTensor), but expected (torch.FloatTensor source, int dim, torch.LongTensor index)
```

I imagine it's just a matter of adding a .cpu() but I wasn't sure if there's anything else that needs to be changed -- or am I doing something wrong?
",,,1,,,,,,,
152,https://github.com/allenai/allennlp/issues/998,998,[],closed,2018-03-19 15:09:16+00:00,,5,train --recover doesn't work with GPU + Adam optimizer,"crashes when it tries to resume training

```
2018-03-19 13:41:54,514 - INFO - allennlp.data.dataset_readers.reading_comprehension.triviaqa - sample this iteration True
Traceback (most recent call last):
  File ""/home/joelg/miniconda3/envs/allennlp2/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/joelg/miniconda3/envs/allennlp2/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/joelg/allennlp/allennlp/run.py"", line 18, in <module>
    main(prog=""python -m allennlp.run"")
  File ""/home/joelg/allennlp/allennlp/commands/__init__.py"", line 73, in main
    args.func(args)
  File ""/home/joelg/allennlp/allennlp/commands/train.py"", line 100, in train_model_from_args
    args.recover)
  File ""/home/joelg/allennlp/allennlp/commands/train.py"", line 130, in train_model_from_file
    return train_model(params, serialization_dir, file_friendly_logging, recover)
  File ""/home/joelg/allennlp/allennlp/commands/train.py"", line 303, in train_model
    metrics = trainer.train()
  File ""/home/joelg/allennlp/allennlp/training/trainer.py"", line 649, in train
    train_metrics = self._train_epoch(epoch)
  File ""/home/joelg/allennlp/allennlp/training/trainer.py"", line 458, in _train_epoch
    self._optimizer.step()
  File ""/home/joelg/miniconda3/envs/allennlp2/lib/python3.6/site-packages/torch/optim/adam.py"", line 69, in step
    exp_avg.mul_(beta1).add_(1 - beta1, grad)
TypeError: add_ received an invalid combination of arguments - got (float, torch.cuda.FloatTensor), but expected one of:
 * (float value)
 * (torch.FloatTensor other)
 * (torch.SparseFloatTensor other)
 * (float value, torch.FloatTensor other)
      didn't match because some of the arguments have invalid types: (float, torch.cuda.FloatTensor)
 * (float value, torch.SparseFloatTensor other)
      didn't match because some of the arguments have invalid types: (float, torch.cuda.FloatTensor)
```",,,1,,,,,,,
154,https://github.com/allenai/allennlp/issues/1002,1002,[],closed,2018-03-20 15:36:38+00:00,,7,gpu memory leakage into another one,"hey,

I'm trying to use the full gpus capacity, so I'm running several processes in parallel (several gpus on the same machine).
After running the second process, I noticed it uses some memory from the first allocated gpu (and causing it to run out of memory after a while).

This is how I run the process:

> python -m allennlp.run predict \
https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz \
--cuda-device 1 --batch-size 64 sentences_01 --predictor=constituency-parser > sentences_01.pred

(with cuda-device 0 for the first one)

is this the correct way of running it?",,,1,,,,,,,
194,https://github.com/allenai/allennlp/issues/1107,1107,[],closed,2018-04-19 21:39:55+00:00,,9,Model has a GPU argument at load and predict,"Presently the [model class](https://github.com/allenai/allennlp/blob/master/allennlp/models/model.py) takes a `cuda_device` in both the [load method](https://github.com/allenai/allennlp/blob/cbe58971a15aaf6a80d3cc53b1e7c4a9ba4e089c/allennlp/models/model.py#L199) but so does the [forward method](https://github.com/allenai/allennlp/blob/cbe58971a15aaf6a80d3cc53b1e7c4a9ba4e089c/allennlp/models/model.py#L122).  Ideally calls to forward would, at least by default, use the same `cuda_device` that the model was loaded on.

Would it even work to load a model on one device but to make predictions on another?",,,1,,,,,,,
228,https://github.com/allenai/allennlp/issues/1185,1185,[],closed,2018-05-08 18:40:34+00:00,,1,`allennlp train` should fail faster if trainer.cuda_device >= 0 but no GPU is available,"right now it reads in all the datasets and so on before it starts training and crashes. sometimes reading datasets takes a long time! it would be better if the command aborted immediately and printed a 

""hey, your experiment specified a GPU but none is available; if you want to run on CPU use the following override"" or something",,,1,,,,,,,
300,https://github.com/allenai/allennlp/issues/1360,1360,[],closed,2018-06-12 09:53:34+00:00,,5,Help with debugging potential memory leak in bidaf code on GPU,"Hi folks,

There seems to be a memory leak in the bidaf code when using GPU for training. The memory footprint keeps rising in a pattern that I have not yet been able to decipher. I have checked that this is not a data issue as the number of tokens in all passages have been limited by a max value(~800).  
I have not been able to reproduce this while training on the CPU. Unfortunately, I am not quite sure how to pinpoint the reason for this increase in memory usage. Any suggestions about how to go about it or any tools I should use will be extremely helpful.

Cheers,
Aman",,,1,,,,,,,
332,https://github.com/allenai/allennlp/issues/1439,1439,[],closed,2018-06-28 22:51:16+00:00,,4,[Is this a bug?]: BiDAF appears to access wrong metadata instances when running on multiple GPUs,"This loop https://github.com/allenai/allennlp/blob/43fc89ee59944dca0de01ca5ab2843c9926323d6/allennlp/models/reading_comprehension/bidaf.py#L274 iterates over the range obtained from a tensor dimension corresponding to batch size https://github.com/allenai/allennlp/blob/43fc89ee59944dca0de01ca5ab2843c9926323d6/allennlp/models/reading_comprehension/bidaf.py#L176 accessing elements of the `metadata` list with indices in this range. This is reasonable in a CPU or single GPU scenario where `forward()` operates on the entire batch generated by the trainer algorithm. With multiple GPUs, all tensors passed into `forward()` are sliced and scattered across GPU instances with `forward()` transparently receiving only a subset of data (e.g. with configured batch size of 20 on 8 GPUs, we see actual `batch_size` in `forward()` to be either 2 or 3). Metadata is copied into each call to `forward()` in its entirety since it does not contain tensors. Therefore the majority of the metadata access happens with a wrong index: the tensors operated on are not conformal to the array of metadata instances.

I wonder if the results of BiDAF training on multiple GPUs were previously found deficient or inferior to training them on a single GPU with otherwise identical configuration. This apparent disconnect in the code may explain such an outcome. I have only run across this issue by comparing BiDAF code with the multi-paragraph version in https://github.com/allenai/allennlp/pull/804 which actually generates a runtime exception because of it. Of course, being an absolute newbie in this codebase I may be completely wrong!",,,1,,,,,,,
400,https://github.com/allenai/allennlp/issues/1585,1585,[],closed,2018-08-07 17:15:45+00:00,,6,MultiGPU Trainer test fails,"**Describe the bug**
Trainer multigpu test fails

**To Reproduce**
Steps to reproduce the behavior
run the tests on a multigpu system.

**Expected behavior**
test passes

**System (please complete the following information):**
 - OS: Linux
 - Python version: 3.6
 - AllenNLP version: v0.5.1

**Additional context**

Here's the full traceback:

```
_____________________________ TestTrainer.test_trainer_can_run_multiple_gpu _____________________________

self = <allennlp.tests.training.trainer_test.TestTrainer testMethod=test_trainer_can_run_multiple_gpu>

    @pytest.mark.skipif(torch.cuda.device_count() < 2,
                        reason=""Need multiple GPUs."")
    def test_trainer_can_run_multiple_gpu(self):
        multigpu_iterator = BasicIterator(batch_size=4)
        multigpu_iterator.index_with(self.vocab)
        trainer = Trainer(self.model, self.optimizer,
                          multigpu_iterator, self.instances, num_epochs=2,
                          cuda_device=[0, 1])
>       trainer.train()

tests/training/trainer_test.py:88:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
training/trainer.py:679: in train
    train_metrics = self._train_epoch(epoch)
training/trainer.py:463: in _train_epoch
    loss = self._batch_loss(batch, for_training=True)
training/trainer.py:396: in _batch_loss
    output_dict = self._data_parallel(batch)
training/trainer.py:387: in _data_parallel
    losses = gather([output['loss'] for output in outputs], used_device_ids[0], 0)
../torch/nn/parallel/scatter_gather.py:68: in gather
    return gather_map(outputs)
../torch/nn/parallel/scatter_gather.py:55: in gather_map
    return Gather.apply(target_device, dim, *outputs)
../torch/nn/parallel/_functions.py:54: in forward
    ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

i = tensor(0.7947, device='cuda:0')

>   ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))
E   RuntimeError: dimension specified as 0 but tensor has no dimensions

../torch/nn/parallel/_functions.py:54: RuntimeError
----------------------------------------- Captured stderr call ------------------------------------------
2018-08-07 10:12:19,024 - INFO - allennlp.common.checks - Pytorch version: 0.4.0
0it [00:00, ?it/s]2018-08-07 10:12:19,025 - INFO - allennlp.data.dataset_readers.sequence_tagging - Reading instances from lines in file at: /home/nfliu/miniconda3/envs/test/lib/python3.6/site-packages/allennlp/tests/fixtures/data/sequence_tagging.tsv
4it [00:00, 4651.29it/s]
2018-08-07 10:12:19,026 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
100%|鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅| 4/4 [00:00<00:00, 70492.50it/s]
2018-08-07 10:12:19,028 - INFO - allennlp.training.trainer - WARNING: Multiple GPU support is experimental not recommended for use. In some cases it may lead to incorrect results or undefined behavior.
2018-08-07 10:12:19,029 - INFO - allennlp.training.trainer - Beginning training.
2018-08-07 10:12:19,030 - INFO - allennlp.training.trainer - Epoch 0/1
2018-08-07 10:12:19,030 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 2981.964
2018-08-07 10:12:19,115 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1961
2018-08-07 10:12:19,117 - INFO - allennlp.training.trainer - GPU 1 memory usage MB: 10
  0%|          | 0/1 [00:00<?, ?it/s]2018-08-07 10:12:19,119 - INFO - allennlp.training.trainer - Training
------------------------------------------- Captured log call -------------------------------------------
checks.py                   29 INFO     Pytorch version: 0.4.0
sequence_tagging.py         57 INFO     Reading instances from lines in file at: /home/nfliu/miniconda3/envs/test/lib/python3.6/site-packages/allennlp/tests/fixtures/data/sequence_tagging.tsv
vocabulary.py              341 INFO     Fitting token dictionary from dataset.
trainer.py                 280 INFO     WARNING: Multiple GPU support is experimental not recommended for use. In some cases it may lead to incorrect results or undefined behavior.
trainer.py                 671 INFO     Beginning training.
trainer.py                 426 INFO     Epoch 0/1
trainer.py                 427 INFO     Peak CPU memory usage MB: 2981.964
trainer.py                 429 INFO     GPU 0 memory usage MB: 1961
trainer.py                 429 INFO     GPU 1 memory usage MB: 10
trainer.py                 452 INFO     Training
```",,,1,,,,,,,
513,https://github.com/allenai/allennlp/issues/1865,1865,[],closed,2018-10-04 18:54:13+00:00,,1,ELMO Embedder using GPU #0 even when initialized with different cuda id. ,"**Describe the bug**
When running an elmoembedder initialized with a cuda id, it still seems to use gpu 0 as well as whichever gpu it should be.

**To Reproduce**
Steps to reproduce the behavior
1. On a machine with multiple GPUs, initialize multiple `ElmoEmbedders`, one per GPU (`ElmoEmbedder(options_file, weight_file, cuda_id)`).
2. Run all those processes at once (so you should have one elmo process per GPU). 
3. Monitor `nvidia-smi`.
4. You will see two processes per ELMO process, one on each GPU and then one process per ELMO instance on GPU 0. 
![image](https://user-images.githubusercontent.com/13563214/46496027-cd36a880-c7cb-11e8-802d-192a48722613.png)


**Expected behavior**
I would not expect processes to use GPUs other than the one their assigned. Or if they do, some documentation would be great (maybe I missed it somewhere?). 

**System (please complete the following information):**
 - OS: Description:	Ubuntu 16.04.5 LTS
 - Python version: `Python 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) 
[GCC 7.2.0] on linux`
 - AllenNLP version: `0.6.1`
 - PyTorch version: `0.4.1`

**Additional context**
I'm running this on an Amazon p3.16xlarge server.",,,1,,,,,,,
521,https://github.com/allenai/allennlp/issues/1882,1882,[],closed,2018-10-09 07:50:57+00:00,,17,Enabling multi-gpu training,"**Is your feature request related to a problem? Please describe.**
We have been working on the allennlp bi-DAF model for machine comprehension and it has come to our noticed that the multi-gpu code is a work in progress. 

**Describe alternatives you've considered**
We have found a work around from the issue #1439 but the problem with that is everytime the library is updated, we will have to make those changes in our code and rebuilld the whole thing.

**Describe the solution you'd like**
Since we already have our code using allennlp library, we would like an attempt to fix this problem hoping that the we will not have to touch the multi gpu code again when there is a new release. We saw from your latest issues that this is being worked on and would like to know if we could expedite this feature by contributing to it. Do let us know if there are any specific issues we could pick up with regards to this. @matt-gardner 

Thanks,

",,,1,,,,,,,
543,https://github.com/allenai/allennlp/issues/1920,1920,[],closed,2018-10-18 18:23:45+00:00,,5,Feature request: call NVIDIA Management Library directly to get GPU statistics,"I stumbled across this on the [Fast.ai forum via Twitter](https://forums.fast.ai/t/show-gpu-utilization-metrics-inside-training-loop-without-subprocess-call/26594): apparently the `nvidia-smi` command just calls the NVIDIA Management Library to get GPU statistics, which means we can avoid [this subprocess call](https://github.com/allenai/allennlp/blob/606a61abf04e3108949022ae1bcea975b2adb560/allennlp/common/util.py#L325) by just directly calling a wrapper around the c library from Python. The discussion on the forum gives a simple example using the `nvidia-ml-py3` wrapper.

I'm happy to submit a PR to replace the subprocess call to `nvidia-smi` with this approach if you'd like.",,,1,,,,,,,
602,https://github.com/allenai/allennlp/issues/2026,2026,[],closed,2018-11-08 15:32:05+00:00,,7,pytorch ELMo on gpu,"hello, i want to try ELMo but it's too slow on cpu and i can't make it work on gpu.

The following code work (as device is cpu) but it's slow.
and if i change the device for ""cuda"" i got an error : 
""Expected object of type torch.cuda.LongTensor but found type torch.LongTensor""
despite `character_ids` being on gpu.

What is the probleme in my code?
How use ELMo on gpu?

```
from allennlp.modules.elmo import Elmo, batch_to_ids

options_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json""
weight_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5""

elmo = Elmo(options_file, weight_file, 2, dropout=0)


dummy_sentence = ['dummy' for i in range(100)]
train_sentences = [dummy_sentence for i in range(100)]

character_ids = batch_to_ids(train_sentences)

import torch
device = torch.device('cpu')
# device = torch.device('cuda:0')

character_ids.to(device)
elmo.to(device)
    
embeddings = elmo(character_ids)

print(embeddings)
```",,,1,,,,,,,
620,https://github.com/allenai/allennlp/issues/2057,2057,[],closed,2018-11-15 10:41:10+00:00,,10,ProductionRuleField not compatible with multiple GPUs,"
**System (please complete the following information):**
 - OS: Linux
 - Python version: 3.7
 - AllenNLP version: 0.7.1
 - PyTorch version: 0.4.1

**Question**
I am training mml parser on the wikitables dataset. I found it would be out of cuda memory if I use a larger batch size than 8 on a single Tesla P100. Even the batch size 8 causes the out-of-memory error sometimes. How can I train on multiple gpus with allennlp? I don't think the `cuda_device` field supports list input in the config file for now.
",,,1,,,,,,,
621,https://github.com/allenai/allennlp/issues/2058,2058,[],closed,2018-11-15 13:35:05+00:00,,3,Unspecified GPU used in multi-gpu setup,"Actual behaviour:
I try to run my training on GPU-1 in a multi-GPU setup containing 3 GPUs: numbered 0,1,2. Though I have specified GPU-1, it occupies some memory(around 500MB) in GPU-0. I didn't see this happening when I used GPU-0(maybe somewhere the model was using .cuda(0) by default). Though this was mentioned in issues #2006 and #1002, I think the additional context below may help fixing it.

Expected behaviour:
 The training should run on only the specified GPU(i.e., GPU-1).

Steps to reproduce the behaviour:
Run any model on a multi-GPU setup with device other than 0.

Additional context:
As regards the 500MB, I tried initializing a variable in vanilla Pytorch by `some_variable = torch.Tensor([1,2,3]).cuda(1)`...it occupies 500MB on device-1 which is same as the specified device(GPU-1). So looks like some variable/model in allennlp was being initialized with .cuda(0). I partially debugged it further and found that until the execution of this line in `trainer.py`, the model was occupying device-1(as specified in config.json) and that the memory in GPU-0  is occupied in the execution in this part of code: https://github.com/allenai/allennlp/blob/07b574912af3f6049f95851d0886c19c41e5d6ad/allennlp/training/trainer.py#L428
I am not able to debug further. FYI, also made the changes in my code for multi-gpu training from #1445. 

**System (please complete the following information):**
 - OS: Linux
 - Python version: 3.6.5 
 - AllenNLP version: '0.7.2-unreleased'
 - PyTorch version: Not applicable
Thanks!",,,1,,,,,,,
656,https://github.com/allenai/allennlp/issues/2110,2110,[],closed,2018-11-28 23:26:28+00:00,,4,"Configuration error // Training model using GPU  ""cuda_device"": 0, QUAC Training dialog_qa.jsonnet configuration file ","**System (please complete the following information):**
 - OS: Linux
 - Python version: 3.6.6
 - AllenNLP version: v0.7.1
 - PyTorch version: 0.41

**Question**
--------------

I'm getting the following error to train a QuAC model 

**Configuration Error**
--------------------------

""Experiment specified a GPU but none is available, if you want to run on CPU use the override trainer.coda_device = -1 in the json config file""  
![no gpu](https://user-images.githubusercontent.com/30128513/49188951-d166e880-f36c-11e8-99fc-02f6f3e05645.PNG)

**Comman line**
--------------------------
allennlp train /home/x/anaconda3/allennlp/training_config/dialog_qa.jsonnet -s /home/x/anaconda3/allennlp/save_directory/

I have a GPU with the following features, and wanted to train QUAC model using dialog_qa.jsonnet configuration file.

GPU Compute Type gn5 | ecs.gn5-c4g1.xlarge | 4 vCPU | 30 GiB | 1 * NVIDIA P100 | 1 * 440 GiB | Intel Xeon E5-2682v4 | 2.5 GHz

I'am i missing some other configurations?

Regards,
Ali",,,1,,,,,,,
663,https://github.com/allenai/allennlp/issues/2127,2127,[],closed,2018-12-02 04:47:37+00:00,,7,Add Multi-GPU for evaluation," Hi, I found that if I use multi-gpu for training (e.g., 8 GPUs), the evaluation on test after training will still use one GPU, which leads to OOM every time (since the batch_size is for multiple GPUs). 

Of course, the workaround is simply to manually call evaluate and override batch_size, but it is a little inconvenient.

**Describe the solution you'd like**
1. be able to set a separate batch_size for evaluating in config, or
2. add multiple-GPU supports for evaluation.",,,1,,,,,,,
686,https://github.com/allenai/allennlp/issues/2167,2167,"[{'id': 723800354, 'node_id': 'MDU6TGFiZWw3MjM4MDAzNTQ=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'e99695', 'default': False, 'description': 'A great place to start for first time contributors'}, {'id': 887719346, 'node_id': 'MDU6TGFiZWw4ODc3MTkzNDY=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Contributions%20welcome', 'name': 'Contributions welcome', 'color': '02b8d1', 'default': False, 'description': ''}]",closed,2018-12-11 00:39:49+00:00,,5,"Change ""unable to check gpu_memory_mb()"" error to a warning, remove uninformative stack trace","**Describe the bug**


```
2018-12-10 23:59:23,547 - ERROR - allennlp.common.util - unable to check gpu_memory_mb(), continuing
Traceback (most recent call last):
  File ""/home/v-qizhou/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/util.py"", line 343, in gpu_memory_mb
    encoding='utf-8')
  File ""/home/v-qizhou/anaconda3/envs/allennlp/lib/python3.6/subprocess.py"", line 336, in check_output
    **kwargs).stdout
  File ""/home/v-qizhou/anaconda3/envs/allennlp/lib/python3.6/subprocess.py"", line 403, in run
    with Popen(*popenargs, **kwargs) as process:
  File ""/home/v-qizhou/anaconda3/envs/allennlp/lib/python3.6/subprocess.py"", line 709, in __init__
    restore_signals, start_new_session)
  File ""/home/v-qizhou/anaconda3/envs/allennlp/lib/python3.6/subprocess.py"", line 1275, in _execute_child
    restore_signals, start_new_session, preexec_fn)
OSError: [Errno 12] Cannot allocate memory
2018-12-10 23:59:23,547 - INFO - allennlp.training.trainer - Training
```


**To Reproduce**
No idea how to reproduce.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System (please complete the following information):**
 - OS: Linux
 - Python version: 3.6.7
 - AllenNLP version: 0.7.2
 - PyTorch version: (if you installed it yourself)

**Additional context**
Under linux `screen`
",,,1,,,,,,,
698,https://github.com/allenai/allennlp/issues/2182,2182,[],closed,2018-12-14 02:14:55+00:00,,4,Task SNLI has down due to the  GPU Tensor,"**Describe the bug**
Task SNLI has down due to the  GPU Tensor

**To Reproduce**
Steps to reproduce the behavior
1. Download snli data
2.Run 'allennlp train training_config/decomposable_attention.jsonnet -s /netdisk/one/allennlp/snli_default'
3. error : 
![image](https://user-images.githubusercontent.com/11539889/49979161-bf01c700-ff88-11e8-9550-602c11542d43.png)

**Expected behavior**
A clear and concise description of what you expected to happen.

**System (please complete the following information):**
 - OS: CentOS 7
 - Python version: 3.6.5
 - AllenNLP version: I installed from master yesterday
 - PyTorch version: default from requirements.txt

",,,1,,,,,,,
703,https://github.com/allenai/allennlp/issues/2189,2189,[],closed,2018-12-15 20:36:01+00:00,,2,Model training should be in GPU but model parameters are in CPU,"**Describe the bug**
I'm trying to train a model using `allennlp train` in GPU in v0.7.2 but it fails when it's about to start training with the following stack trace:

```
Traceback (most recent call last):
  File ""env/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""env/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""env/lib/python3.6/site-packages/allennlp/run.py"", line 18, in <module>
    main(prog=""allennlp"")
  File ""env/lib/python3.6/site-packages/allennlp/commands/__init__.py"", line 72, in main
    args.func(args)
  File ""env/lib/python3.6/site-packages/allennlp/commands/train.py"", line 111, in train_model_from_args
    args.force)
  File ""env/lib/python3.6/site-packages/allennlp/commands/train.py"", line 142, in train_model_from_file
    return train_model(params, serialization_dir, file_friendly_logging, recover, force)
  File ""env/lib/python3.6/site-packages/allennlp/commands/train.py"", line 351, in train_model
    metrics = trainer.train()
  File ""env/lib/python3.6/site-packages/allennlp/training/trainer.py"", line 755, in train
    train_metrics = self._train_epoch(epoch)
  File ""env/lib/python3.6/site-packages/allennlp/training/trainer.py"", line 525, in _train_epoch
    self.optimizer.step()
  File ""env/lib/python3.6/site-packages/torch/optim/adagrad.py"", line 92, in step
    state['sum'].addcmul_(1, grad, grad)
RuntimeError: Expected object of backend CPU but got backend CUDA for argument #4 'tensor1'
```

It works if I set the device to -1 (CPU) or if I use allennlp v0.7.1.

Debugging a bit, I found that this is the change that breaks my code:

https://github.com/allenai/allennlp/pull/1944/files/3bed856fafa07a2a47d512a6102dacc8b12cf856#r241959869

merged <2mo ago. That change seems a bug to me. Or am I missing something?

**To Reproduce**
`allennlp train` some model in GPU.

**Expected behavior**
It should train without crashing.

**System (please complete the following information):**
 - OS: Linux
 - Python version: 3.6.7 - miniconda
 - AllenNLP version: 0.7.2
 - PyTorch version: both 0.4.1 and 1.0.0",,,1,,,,,,,
220,https://github.com/allenai/allennlp/issues/1172,1172,[],closed,2018-05-02 19:20:47+00:00,,2,How do I read in Machine Comprehension predict output?,"I'm a bit confused as to the format that returned by running the predict command and how to programmatically access it. 

For example, when I run the following example code from site: 

```
echo '{""passage"": ""A reusable launch system (RLS, or reusable launch vehicle, RLV) is a launch system which is capable of launching a payload into space more than once. "", ""question"": ""What is RLS""}' > examples.jsonl

python -m allennlp.run predict \
    https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz \
    examples.jsonl > output.txt
```

I get back a file formatted like:
```
(allen_nlp) MacBook-Pro-5:AllenNLP dhairya$ cat output.txt 
input:  {'passage': 'A reusable launch system (RLS, or reusable launch vehicle, RLV) is a launch system which is capable of launching a payload into space more than once. ', 'question': 'What is RLS'}
prediction:  {""passage_question_attention"": [[0.6167629361152649, 0.2354782521724701, 0.1477588564157486], [0.6327872276306152, 0.25898393988609314, 0.10822883248329163], [0.5114424228668213, 0.2422594428062439, 0.24629813432693481], [0.5788295269012451, 0.29724520444869995, 0.12392525374889374], [0.35226598381996155, 
```

From the documentation, it seem you're providing a HOCON formatted file. But I have issues reading in using pyhocon. I tried reading this like so: 
```
import json
from pyhocon import ConfigFactory

cf = ConfigFactory.parse_file(""output.txt"")
```
But that crashes with the following error: 
ParseSyntaxException: Expected ""}"" (at char 9), (line:1, col:10)

What exactly is the format of the output returned by the predict command? Also I'd appreciate any tips on how to read it in programmatically. 

Thanks!
 ",,1,,,,,,,,
317,https://github.com/allenai/allennlp/issues/1399,1399,[],closed,2018-06-20 14:40:26+00:00,,1,Issues with Implementing Machine Comprehension,"I want to create a python script that takes a ""passage"" and a ""question"" and returns a string that contains the answer (or at least the location of the answer). I have been running into several errors and I have come to the point where I think I have run out of help material to look into and research. I am using the pre-trained model and I have formatted the passage and question as Dict[str, tensor.LongTensor], which are the required parameters for the forward method in allennlp.models.reading_comprehension.


I have tried using predictors, but predictor.predict_json(inputs) returns a dictionary containing the attentions and I can't figure out how to extract the ""answer"" from that.

I have also attached a text file containing my code. 
Any help would be great :)

[allennlp_machine_comprehension.txt](https://github.com/allenai/allennlp/files/2119783/allennlp_machine_comprehension.txt)
",,1,,,,,,,,
355,https://github.com/allenai/allennlp/issues/1482,1482,[],closed,2018-07-12 08:45:22+00:00,,1,Machine comprehension (BiDAF),"I am using BiDAF model for retrieving answers from the passage. For the same question, I have multiple relevant passages which can answer the question. Can I feed all the passages together to the model and obtain results once? By passing ""n"" passages to the model for n answers for a question, it is taking 1.5 sec*n on average.  I have used pretrained elmo word representations in the comprehension model. 

Is there any other way I can reduce total run time to obtain the answers from n passages.

Thank you, 

Regards,",,1,,,,,,,,
362,https://github.com/allenai/allennlp/issues/1510,1510,[],closed,2018-07-19 06:59:36+00:00,,1,Re: Machine Comprehension,"Hi,
Is it possible to get a response in terms of ""yes"" and ""no"" when we query an evidence text.
For e.g. if there is a topic on robotics and the user query, Are robots human?, Then the response should be ""No"" and may be some explaination to follow rather than a just a selected text from the evidence text.
is this possible?
",,1,,,,,,,,
370,https://github.com/allenai/allennlp/issues/1531,1531,[],closed,2018-07-25 15:41:38+00:00,,4,AllenNLP fails to load tarballs created on a Mac,"Here's a bug reported by @nikett.

**Describe the bug**
When you create a model tarball on a Mac, the system includes some binary dotfiles that cause predictions to crash when using the model.

**To Reproduce**
1. Create a tarball with `tar -czvf my_model.tar.gz config.json vocabulary/* weights.th`.
2. Make a prediction using the tarball you just created.
3. You will get an error similar to `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa3 in position 37: invalid start byte`.

**System (please complete the following information):**
 - OSX
 - Python 3.6

**Additional context**
You can work around the error on a mac by creating your tarball as follows `COPYFILE_DISABLE=1; export COPYFILE_DISABLE; tar -czvf my_model.tar.gz config.json vocabulary/* weights.th`
",,1,,,,,,,,
464,https://github.com/allenai/allennlp/issues/1749,1749,[],closed,2018-09-11 12:32:10+00:00,,4,AllenNLP Machine Comprehension Model is generating different outputs at different runs,"I have created a server using the archived model. It is generating different outputs at different runs. But it should not be the case
 Model : => ""https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz""
Objective is to build a question answering machine given a paragraph( comprehend the paragraph)


**To Reproduce (Python code) (Below are the package requirements)**
      
       from allennlp.models import load_archive
       from allennlp.data import DatasetReader
       from allennlp.data.dataset import Batch

    archive = load_archive(""bidaf-model-2017.09.15-charpad.tar.gz"")
    #archive.model
    #type(archive.model)

    config = archive.config.duplicate()
    dataset_reader_params = config[""dataset_reader""]
    dataset_reader = DatasetReader.from_params(dataset_reader_params)

    model = archive.model
    question=""How is GloVe different from word2vec?""
    passage=""Both models learn geometrical encodings (vectors) of words from their co-occurrence information (how frequently they appear together in large text corpora). They differ in that word2vec is a predictive model, whereas GloVe is a count-based model. See this paper for more on the distinctions between these two approaches: http://clic.cimec.unitn.it/marco.... Predictive models learn their vectors in order to improve their predictive ability of Loss(target word | context words; Vectors), i.e. the loss of predicting the target words from the context words given the vector representations. In word2vec, this is cast as a feed-forward neural network and optimized as such using SGD, etc. Count-based models learn their vectors by essentially doing dimensionality reduction on the co-occurrence counts matrix. They first construct a large matrix of (words x context) co-occurrence information, i.e. for each word (the rows), you count how frequently we see this word in some context (the columns) in a large corpus.  The number of contexts is of course large, since it is essentially combinatorial in size. So then they factorize this matrix to yield a lower-dimensional (word x features) matrix, where each row now yields a vector representation for each word. In general, this is done by minimizing a reconstruction loss which tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data. In the specific case of GloVe, the counts matrix is preprocessed by normalizing the counts and log-smoothing them. This turns out to be A Good Thing in terms of the quality of the learned representations. However, as pointed out, when we control for all the training hyper-parameters, the embeddings generated using the two methods tend to perform very similarly in downstream NLP tasks. The additional benefits of GloVe over word2vec is that it is easier to parallelize the implementation which means it is easier to train over more data, which, with these models, is always A Good Thing.""
     for i in range(5):
          instance = dataset_reader.text_to_instance( question,passage ,char_spans=[(6, 10)])
          instances = [instance]
          dataset = Batch(instances)
          dataset.index_instances(model.vocab)

          cuda_device = model._get_prediction_device()
          model_input = dataset.as_tensor_dict(cuda_device=cuda_device)

          outputs = model(**model_input)
          response = outputs['best_span_str']
          print(response)


In above case model is taking a question and a passage but it is generating different output at different run . Ideally it should return unique output at each case. I am really puzzled 

System:
 - OS: OS 11.2
_**cat requirements.txt**_ 
alabaster==0.7.11
allennlp==0.6.0
atomicwrites==1.1.5
attrs==18.1.0
awscli==1.15.81
Babel==2.6.0
boto3==1.7.62
botocore==1.10.80
certifi==2018.8.13
cffi==1.11.2
chardet==3.0.4
click==6.7
colorama==0.3.9
conllu==0.11
cookies==2.2.1
cymem==1.31.2
cytoolz==0.9.0.1
dill==0.2.8.2
docutils==0.14
editdistance==0.4
en-core-web-sm==2.0.0
flaky==3.4.0
Flask==0.12.1
Flask-Cors==3.0.3
gevent==1.3.5
greenlet==0.4.14
h5py==2.8.0
idna==2.7
imagesize==1.0.0
itsdangerous==0.24
Jinja2==2.10
jmespath==0.9.3
jsonnet==0.10.0
MarkupSafe==1.0
more-itertools==4.3.0
msgpack==0.5.6
msgpack-numpy==0.4.3.1
murmurhash==0.28.0
nltk==3.3
numpy==1.15.0
numpydoc==0.8.0
overrides==1.9
packaging==17.1
parsimonious==0.8.0
plac==0.9.6
pluggy==0.7.1
preshed==1.0.1
protobuf==3.6.1
py==1.5.4
pyasn1==0.4.4
pycparser==2.18
Pygments==2.2.0
pyparsing==2.2.0
pytest==3.7.2
python-dateutil==2.7.3
pytz==2017.3
PyYAML==3.13
regex==2017.4.5
requests==2.19.1
responses==0.9.0
rsa==3.4.2
s3transfer==0.1.13
scikit-learn==0.19.2
scipy==1.1.0
six==1.11.0
snowballstemmer==1.2.1
spacy==2.0.12
Sphinx==1.7.7
sphinxcontrib-websupport==1.1.0
tensorboardX==1.2
thinc==6.10.3
toolz==0.9.0
torch==0.4.1
tqdm==4.24.0
typing==3.6.4
ujson==1.35
Unidecode==1.0.22
urllib3==1.23
Werkzeug==0.14.1
wrapt==1.10.11



",,1,,,,,,,,
567,https://github.com/allenai/allennlp/issues/1960,1960,[],closed,2018-10-25 01:08:18+00:00,,4,QUESTIONS About using machine comprehension in an own project ,"HI guys,
I have made a basic pdf search engine using elastic search, and made an application which connects using Node Js. I am trying to add  Allen NLP 's machine comprehension in elastic search inside the node. Js based application, can you please tell me how to do that? , I am a litle new in this ",,1,,,,,,,,
589,https://github.com/allenai/allennlp/issues/2007,2007,[],closed,2018-11-02 13:16:45+00:00,,1,is it possible to build a 'Machine Comprehension' model in danish or any other language ?,"i was exploring this 'Machine Comprehension' capability of allennlp. In english it works pretty fine. How can we make it work for other languages like Danish or French ? 
",,1,,,,,,,,
647,https://github.com/allenai/allennlp/issues/2095,2095,[],closed,2018-11-24 17:46:21+00:00,,3,Multiple prediction spans from machine comprehension (BiDAF)?,"Hi,

Now the predictor of BiDAF machine comprehension returns one passage span with highest score. Can I use it to return a list of span with decreasing scores, starting from the highest one?

Thanks a lot,
Qiao",,1,,,,,,,,
708,https://github.com/allenai/allennlp/issues/2198,2198,[],closed,2018-12-17 16:40:38+00:00,,1,Multiple documents for Machine comprehension,"Please first search our GitHub repository for similar questions.  If you don't find a similar example you can use the following template:

**System (please complete the following information):**
 - OS: [e.g. OSX, Linux] Linux
 - Python version: [e.g. 3.6.1] 3.6.2
 - AllenNLP version: [e.g. v0.7.2, or ""I installed from master""] I installed from master
 - PyTorch version: (if you installed it yourself) Using google cloud ML VM

**Question**
I have multiple documents . Right now I am combining all documents text as single passage and sending to AllenNLP along with question. I believe this slows down the process. Is there any alternative in the library?

Referring to : [https://allennlp.org/models](https://allennlp.org/models)

code:
```
from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path(""https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz"")
predictor.predict(
  passage=""The Matrix is a 1999 science fiction action film written and directed by The Wachowskis, starring Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, Hugo Weaving, and Joe Pantoliano."",
  question=""Who stars in The Matrix?""
)
```

",,1,,,,,,,,
807,https://github.com/allenai/allennlp/issues/2408,2408,[],closed,2019-01-21 13:02:57+00:00,,2,Machine Comprehension,How can I use the model obtained after training on a SQUAD data set onto an unsupervised training machine comprehension task? I am expecting a unsupervised fine-tuning task(Transfer Learning). Is this possible or do we always require a supervised task for fine-tuning?,,1,,,,,,,,
883,https://github.com/allenai/allennlp/issues/2547,2547,[],closed,2019-02-27 02:14:45+00:00,,1,Does the Allennlp have a Macro F1 value calculation function?,"Hi~

It seems that the default is calculating the Micro F1?

Besides, could we use the dataset tagged with the BIOES scheme?  Need we do extra processing or setting?

And I try to train serveral times on the windows but failed, does it means that Allennlp currently not supports windows? (ubuntu is fine.)

Hope to get your relpy, Thanks a lot. 

",,1,,,,,,,,
901,https://github.com/allenai/allennlp/issues/2577,2577,[],closed,2019-03-08 03:33:58+00:00,,2,Allen nlp is using all the cpu  threads on machine.,"**System (please complete the following information):**
 - OS: Linux
 - Python version: Python 3.6.6 |Anaconda, Inc.
 - AllenNLP version: 0.8.2
 - PyTorch version: 1.0.1.post2 via anaconda

**Question**
I am training the tutorial model from the website with and without jsonnet.

When I am using it without jsonnet, I am able to set `set_num_threads` and restrict the number of threads and CPU being used on the machine.

If I run with `allennlp` command, it starts using all the available threads on the machine.

```
allennlp train config/sst_initial.json -s set -f --include-package models
```
How can I restrict the number of CPU used by `allennlp` while using jsonnet?

Moreover, In general how can I run any time of setup code using jsonnet?",,1,,,,,,,,
1070,https://github.com/allenai/allennlp/issues/2869,2869,[],closed,2019-05-20 12:44:03+00:00,,2,Implementing custom Machine Comprehension Model,Can we train a Allennlp MC model with our own data. ,,1,,,,,,,,
1080,https://github.com/allenai/allennlp/issues/2885,2885,[],closed,2019-05-24 09:53:55+00:00,,3,use naqanet insted of bidaf for reading comprehension in local machine.,"**System (please complete the following information):**
 - OS:  Windows 10 pro
 - Python version:  3.6.8 
 - AllenNLP version: allennlp 0.8.4-unreleased
 - PyTorch version:  1.1.0

**Question**
I'm using allennlp in my local machine and run it successfully. 
but I have to run the reading comprehension part using NaQanet instead of Bidaf. 
from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path(""https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz"")
predictor.predict(
  passage=""The Matrix is a 1999 science fiction action film written and directed by The Wachowskis, starring Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, Hugo Weaving, and Joe Pantoliano."",
  question=""Who stars in The Matrix?""
)
replaced to 
from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path(""https://s3-us-west-2.amazonaws.com/allennlp/models/naqanet-2019.03.01.tar.gz"")
predictor.forward(
  passage=""The Matrix is a 1999 science fiction action film written and directed by The Wachowskis, starring Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, Hugo Weaving, and Joe Pantoliano."",
  question=""Who stars in The Matrix?""
)
it gives me an error 
  raise EOFError(""Compressed file ended before the ""
EOFError: Compressed file ended before the end-of-stream marker was reached

is anyone can please help me. 

thank you.",,1,,,,,,,,
1178,https://github.com/allenai/allennlp/issues/3057,3057,[],closed,2019-07-11 21:20:29+00:00,,1,Error when tried bert for Machine comprehension model,"  File ""/home/ssathishbarani/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/from_params.py"", line 365, in from_params
    return subclass.from_params(params=params, **extras)
  File ""/home/ssathishbarani/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"", line 160, in from_params
    for name, subparams in token_embedder_params.items()
  File ""/home/ssathishbarani/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"", line 160, in <dictcomp>
    for name, subparams in token_embedder_params.items()
  File ""/home/ssathishbarani/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/from_params.py"", line 365, in from_params
    return subclass.from_params(params=params, **extras)
  File ""/home/ssathishbarani/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/modules/token_embedders/embedding.py"", line 301, in from_params
    vocab_namespace)
  File ""/home/ssathishbarani/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/modules/token_embedders/embedding.py"", line 372, in _read_pretrained_embeddings_file
    vocab, namespace)
  File ""/home/ssathishbarani/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/modules/token_embedders/embedding.py"", line 416, in _read_embeddings_from_text_file
    raise ConfigurationError(""No embeddings of correct dimension found; you probably ""
allennlp.common.checks.ConfigurationError: ""No embeddings of correct dimension found; you probably misspecified your embedding_dim parameter, or didn't pre-populate your Vocabulary",,1,,,,,,,,
1283,https://github.com/allenai/allennlp/issues/3245,3245,[],closed,2019-09-13 11:33:19+00:00,,2,Machine comprehension gets wrong answers with conversations (2 People) as input,"I have been using the machine comprehension module with conversations (2 people) as input. The input context is something like ""I have collided with an iceberg. Are you still under command? Yes."" etc... . 

If I now try to predict the answer to the question already asked in the context (""Are you still under command?"") I of course want it to get ""Yes"" as an answer. But this does not work at all, it just gives some other span as best answer. Is there something I can do about it or is there a certain data set that I could train the network on which includes those problems in the training data?

I'd be really grateful if somebody could help me!",,1,,,,,,,,
1666,https://github.com/allenai/allennlp/issues/4054,4054,[],closed,2020-04-10 23:49:15+00:00,,3,Do you have to use Conda to install allennlp on Mac?,"**Describe the bug**
I tried `pip install allennlp`, but I get a long laundry list of errors.  I saw the other threads where `conda install allennlp -c pytorch -c allennlp -c conda-forge` was recommended

**To Reproduce**
Steps to reproduce the behavior
1. pip install allennlp

**Expected behavior**
allennlp successfully installed

**System (please complete the following information):**
 - OS: Mac Catalina 10.14
 - Python version: 3.8.2
 - AllenNLP version: v0.9.0
 - PyTorch version:  I used command `pip install torch torchvision` recommended from their website
- gcc version: 
Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/4.2.1
Apple clang version 11.0.3 (clang-1103.0.32.29)
Target: x86_64-apple-darwin19.4.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

**Additional context**
My MacBookPro is brand new, so it might be dependencies that I don't have yet installed.  In the past I've experienced nightmares with Anaconda, so I honestly was trying to get away with just managing the python versions I need with pyenv/homebrew.  Is there a list of package requirements available on the internet for being able to use allennlp with a pyenv instead of a conda env, so I can avoid having to download Anaconda?  If not that's okay just want to make sure I'm not crazy.

**stack trace**

rebeccaflores@Rebeccas-MBP code % pip3 install allennlp
Collecting allennlp
  Using cached allennlp-0.9.0-py3-none-any.whl (7.6 MB)
Collecting tensorboardX>=1.2
  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)
     |鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅| 195 kB 3.0 MB/s 
Collecting responses>=0.7
  Downloading responses-0.10.12-py2.py3-none-any.whl (15 kB)
Collecting pytorch-transformers==1.1.0
  Downloading pytorch_transformers-1.1.0-py3-none-any.whl (158 kB)
     |鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅| 158 kB 4.8 MB/s 
Collecting word2number>=1.1
  Downloading word2number-1.1.zip (9.7 kB)
Collecting tqdm>=4.19
  Using cached tqdm-4.45.0-py2.py3-none-any.whl (60 kB)
Collecting jsonpickle
  Using cached jsonpickle-1.3-py2.py3-none-any.whl (32 kB)
Collecting ftfy
  Downloading ftfy-5.7.tar.gz (58 kB)
     |鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅| 58 kB 11.3 MB/s 
Collecting pytorch-pretrained-bert>=0.6.0
  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)
     |鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅| 123 kB 8.7 MB/s 
Collecting matplotlib>=2.2.3
  Using cached matplotlib-3.2.1-cp38-cp38-macosx_10_9_x86_64.whl (12.4 MB)
Collecting spacy<2.2,>=2.1.0
  Using cached spacy-2.1.9.tar.gz (30.7 MB)
  Installing build dependencies ... error
  ERROR: Command errored out with exit status 1:
   command: /Users/rebeccaflores/.pyenv/versions/3.8.2/bin/python3.8 /Users/rebeccaflores/.pyenv/versions/3.8.2/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-build-env-c9xh98j9/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'wheel>0.32.0,<0.33.0' Cython 'cymem>=2.0.2,<2.1.0' 'preshed>=2.0.1,<2.1.0' 'murmurhash>=0.28.0,<1.1.0' 'thinc>=7.0.8,<7.1.0'
       cwd: None
  Complete output (667 lines):
  Collecting setuptools
    Using cached setuptools-46.1.3-py3-none-any.whl (582 kB)
  Collecting wheel<0.33.0,>0.32.0
    Using cached wheel-0.32.3-py2.py3-none-any.whl (21 kB)
  Collecting Cython
    Using cached Cython-0.29.16-cp38-cp38-macosx_10_9_x86_64.whl (2.0 MB)
  Collecting cymem<2.1.0,>=2.0.2
    Using cached cymem-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl (31 kB)
  Collecting preshed<2.1.0,>=2.0.1
    Using cached preshed-2.0.1.tar.gz (113 kB)
  Collecting murmurhash<1.1.0,>=0.28.0
    Using cached murmurhash-1.0.2-cp38-cp38-macosx_10_9_x86_64.whl (19 kB)
  Collecting thinc<7.1.0,>=7.0.8
    Using cached thinc-7.0.8.tar.gz (1.9 MB)
  Collecting blis<0.3.0,>=0.2.1
    Using cached blis-0.2.4.tar.gz (1.5 MB)
  Collecting wasabi<1.1.0,>=0.0.9
    Using cached wasabi-0.6.0-py3-none-any.whl (20 kB)
  Collecting srsly<1.1.0,>=0.0.6
    Using cached srsly-1.0.2-cp38-cp38-macosx_10_9_x86_64.whl (183 kB)
  Collecting numpy>=1.7.0
    Using cached numpy-1.18.2-cp38-cp38-macosx_10_9_x86_64.whl (15.2 MB)
  Collecting plac<1.0.0,>=0.9.6
    Using cached plac-0.9.6-py2.py3-none-any.whl (20 kB)
  Collecting tqdm<5.0.0,>=4.10.0
    Using cached tqdm-4.45.0-py2.py3-none-any.whl (60 kB)
  Installing collected packages: setuptools, wheel, Cython, cymem, preshed, murmurhash, numpy, blis, wasabi, srsly, plac, tqdm, thinc
      Running setup.py install for preshed: started
      Running setup.py install for preshed: finished with status 'done'
      Running setup.py install for blis: started
      Running setup.py install for blis: still running...
      Running setup.py install for blis: still running...
      Running setup.py install for blis: finished with status 'error'
      ERROR: Command errored out with exit status 1:
       command: /Users/rebeccaflores/.pyenv/versions/3.8.2/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/setup.py'""'""'; __file__='""'""'/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-record-iyghhacy/install-record.txt --single-version-externally-managed --prefix /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-build-env-c9xh98j9/overlay --compile --install-headers /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-build-env-c9xh98j9/overlay/include/python3.8/blis
           cwd: /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/
      Complete output (628 lines):
      BLIS_COMPILER? None
      running install
      running build
      running build_py
      creating build
      creating build/lib.macosx-10.15-x86_64-3.8
      creating build/lib.macosx-10.15-x86_64-3.8/blis
      copying blis/benchmark.py -> build/lib.macosx-10.15-x86_64-3.8/blis
      copying blis/__init__.py -> build/lib.macosx-10.15-x86_64-3.8/blis
      copying blis/about.py -> build/lib.macosx-10.15-x86_64-3.8/blis
      creating build/lib.macosx-10.15-x86_64-3.8/blis/tests
      copying blis/tests/__init__.py -> build/lib.macosx-10.15-x86_64-3.8/blis/tests
      copying blis/tests/test_dotv.py -> build/lib.macosx-10.15-x86_64-3.8/blis/tests
      copying blis/tests/test_gemm.py -> build/lib.macosx-10.15-x86_64-3.8/blis/tests
      copying blis/tests/common.py -> build/lib.macosx-10.15-x86_64-3.8/blis/tests
      copying blis/cy.pyx -> build/lib.macosx-10.15-x86_64-3.8/blis
      copying blis/py.pyx -> build/lib.macosx-10.15-x86_64-3.8/blis
      copying blis/__init__.pxd -> build/lib.macosx-10.15-x86_64-3.8/blis
      copying blis/cy.pxd -> build/lib.macosx-10.15-x86_64-3.8/blis
      running build_ext
      unix
      py_compiler gcc
      {'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'HOSTTYPE': 'x86_64', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'LANG': 'C.UTF-8', 'OLDPWD': '/home/matt/repos/flame-blis', 'VIRTUAL_ENV': '/home/matt/repos/cython-blis/env3.6', 'USER': 'matt', 'PWD': '/home/matt/repos/cython-blis', 'HOME': '/home/matt', 'NAME': 'LAPTOP-OMKOB3VM', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SHELL': '/bin/bash', 'TERM': 'xterm-256color', 'SHLVL': '1', 'LOGNAME': 'matt', 'PATH': '/home/matt/repos/cython-blis/env3.6/bin:/tmp/google-cloud-sdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu/Scripts:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu:/mnt/c/Python37/Scripts:/mnt/c/Python37:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/WiFi/bin:/mnt/c/Program Files/Common Files/Intel/WirelessCommon:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/LLVM/bin:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files/nodejs:/mnt/c/Users/matt/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/matt/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/matt/AppData/Roaming/npm:/snap/bin:/mnt/c/Program Files/Oracle/VirtualBox', 'PS1': '(env3.6) \\[\\e]0;\\u@\\h: \\w\\a\\]${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ ', 'VAGRANT_HOME': '/home/matt/.vagrant.d/', 'LESSOPEN': '| /usr/bin/lesspipe %s', '_': '/home/matt/repos/cython-blis/env3.6/bin/python'}
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/config/bulldozer/bli_cntx_init_bulldozer.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_cntx_init_bulldozer.o -O2 -funroll-all-loops -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang: warning: optimization flag '-funroll-all-loops' is not supported [-Wignored-optimization-argument]
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/config/generic/bli_cntx_init_generic.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_cntx_init_generic.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/config/haswell/bli_cntx_init_haswell.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_cntx_init_haswell.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/config/penryn/bli_cntx_init_penryn.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_cntx_init_penryn.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/config/piledriver/bli_cntx_init_piledriver.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_cntx_init_piledriver.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/config/sandybridge/bli_cntx_init_sandybridge.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_cntx_init_sandybridge.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/config/steamroller/bli_cntx_init_steamroller.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_cntx_init_steamroller.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_amaxv_zen_int.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/kernels/zen/1/bli_axpyv_zen_int.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_axpyv_zen_int.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/kernels/zen/1/bli_axpyv_zen_int10.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_axpyv_zen_int10.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/kernels/zen/1/bli_dotv_zen_int.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_dotv_zen_int.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/kernels/zen/1/bli_dotv_zen_int10.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_dotv_zen_int10.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/kernels/zen/1/bli_dotxv_zen_int.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_dotxv_zen_int.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/kernels/zen/1/bli_scalv_zen_int.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_scalv_zen_int.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/kernels/zen/1/bli_scalv_zen_int10.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_scalv_zen_int10.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/kernels/zen/1f/bli_axpyf_zen_int_8.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_axpyf_zen_int_8.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/frame/thread/bli_thrinfo.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_thrinfo.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/frame/util/bli_util_check.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_util_check.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/frame/util/bli_util_fpa.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_util_fpa.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/frame/util/bli_util_oapi.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_util_oapi.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/frame/util/bli_util_oapi_ba.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_util_oapi_ba.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/frame/util/bli_util_oapi_ex.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_util_oapi_ex.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/frame/util/bli_util_tapi.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_util_tapi.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/frame/util/bli_util_tapi_ba.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_util_tapi_ba.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/frame/util/bli_util_tapi_ex.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_util_tapi_ex.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      clang -c /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/frame/util/bli_util_unb_var1.c -o /var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/tmp823jgl2z/bli_util_unb_var1.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.5.0-6"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/darwin-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64
      Compiler gcc
      building 'blis.cy' extension
      creating build/temp.macosx-10.15-x86_64-3.8
      creating build/temp.macosx-10.15-x86_64-3.8/blis
      clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/include -I/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/blis/_src/include/darwin-x86_64 -I/Users/rebeccaflores/.pyenv/versions/3.8.2/include/python3.8 -c blis/cy.c -o build/temp.macosx-10.15-x86_64-3.8/blis/cy.o -std=c99
      error: $MACOSX_DEPLOYMENT_TARGET mismatch: now ""10.7"" but ""10.15"" during configure
      ----------------------------------------
  ERROR: Command errored out with exit status 1: /Users/rebeccaflores/.pyenv/versions/3.8.2/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/setup.py'""'""'; __file__='""'""'/private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-install-catc9hnz/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-record-iyghhacy/install-record.txt --single-version-externally-managed --prefix /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-build-env-c9xh98j9/overlay --compile --install-headers /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-build-env-c9xh98j9/overlay/include/python3.8/blis Check the logs for full command output.
  ----------------------------------------
ERROR: Command errored out with exit status 1: /Users/rebeccaflores/.pyenv/versions/3.8.2/bin/python3.8 /Users/rebeccaflores/.pyenv/versions/3.8.2/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /private/var/folders/73/d31pnw3124q46b1knl9bx1v00000gn/T/pip-build-env-c9xh98j9/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'wheel>0.32.0,<0.33.0' Cython 'cymem>=2.0.2,<2.1.0' 'preshed>=2.0.1,<2.1.0' 'murmurhash>=0.28.0,<1.1.0' 'thinc>=7.0.8,<7.1.0' Check the logs for full command output.
",,1,,,,,,,,
2199,https://github.com/allenai/allennlp/issues/5258,5258,"[{'id': 1875662321, 'node_id': 'MDU6TGFiZWwxODc1NjYyMzIx', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/Feature%20request', 'name': 'Feature request', 'color': '5558ba', 'default': False, 'description': ''}]",closed,2021-06-13 14:11:13+00:00,,14,Add a conda install for Mac,"There is a [conda-forge](https://anaconda.org/conda-forge/allennlp) install for Linux. Could you please also add one for Mac?

https://conda-forge.org/#add_recipe

",,1,,,,,,,,
14,https://github.com/allenai/allennlp/issues/544,544,[],closed,2017-11-27 21:40:27+00:00,,5,install spacy model via pip / requirements.txt instead of `spacy download`,"isn't that simpler?

https://spacy.io/usage/models#models-download",1,,,,,,,,,
33,https://github.com/allenai/allennlp/issues/608,608,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2017-12-13 14:08:30+00:00,,4,Data pipeline tutorial incorrectly describes vocab namespace issues,"Issue:
The keys of the ""token_indexers"" dict passed to TextField aren't used to create the vocabulary namespace (I'm not entirely sure that this is the intended behavior but the tutorial on http://allennlp.org/tutorials/data-pipeline suggests so). Instead the TokenIndexers internal namespace is used, which leads to the behavior as shown in the example code. The specified namespace is ""some_namespace"" but the index is stored under the namespace ""tokens"", which is the default namespace for SingleIdTokenIndexer.

Example code:
```
from allennlp.data import Token
from allennlp.data.fields import TextField
from allennlp.data.token_indexers import SingleIdTokenIndexer
from allennlp.data import Instance
from allennlp.data import Dataset

from allennlp.data import Vocabulary

text_field = TextField(list(map(Token, [""Here"", ""are"", ""some"", ""longer"", ""words"", "".""])),
                       token_indexers={""some_namespace"": SingleIdTokenIndexer()})
dataset = Dataset([Instance({""sentence"": text_field})])

vocab = Vocabulary.from_dataset(dataset)
dataset.index_instances(vocab)

print('Namespaces:', vocab._index_to_token)
print('Namespace content:', vocab.get_index_to_token_vocabulary(""some_namespace""))
```

Output:
```
Namespaces: defaultdict(None, {'tokens': {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'Here', 3: 'are', 4: 'some', 5: 'longer', 6: 'words', 7: '.'}})
Namespace content: {0: '@@PADDING@@', 1: '@@UNKNOWN@@'}
```",1,,,,,,,,,
34,https://github.com/allenai/allennlp/issues/610,610,"[{'id': 605609792, 'node_id': 'MDU6TGFiZWw2MDU2MDk3OTI=', 'url': 'https://api.github.com/repos/allenai/allennlp/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}]",closed,2017-12-14 00:16:28+00:00,,6,Data pipeline tutorial issues,"While working through the data pipeline tutorial (http://allennlp.org/tutorials/data-pipeline), I noticed a couple of things:

1. when installing allennlp v0.2.3 via pip (python 3.6.1), the tutorial fails, because at this point Dataset has no as_tensor_dict(..) method.

2. in the second example, in
```
print(vocab.get_index_to_token_vocabulary(""tokens""), ""\n"")
.
print(vocab.get_index_to_token_vocabulary(""chars""), ""\n"")
```
""vocab"" should be replaced with ""word_and_char_vocab""

3. there is a missing comma between ""good"" and ""."" in line
```
review2 = TextField(list(map(Token, [""This"", ""movie"", ""was"", ""quite"", ""slow"", ""but"", ""good"" "".""])), token_indexers={""tokens"": SingleIdTokenIndexer()})
```",1,,,,,,,,,
97,https://github.com/allenai/allennlp/issues/856,856,[],closed,2018-02-15 19:46:31+00:00,,3,create an easy way for people to test that their pip install is working correctly and that they've got all the right dependencies and downloads,"possibly something as simple as adding a

```
python -m allennlp.run test-install
```

command or something",1,,,,,,,,,
129,https://github.com/allenai/allennlp/issues/946,946,[],closed,2018-03-02 19:23:01+00:00,,0,pip should install an `allennlp` script,"lots of python packages do that, so that people can then

```bash
allennlp train ...
```

instead of 

```
python -m allennlp.run train ...
```

should be relatively easy:

http://python-packaging.readthedocs.io/en/latest/command-line-scripts.html

",1,,,,,,,,,
226,https://github.com/allenai/allennlp/issues/1178,1178,[],closed,2018-05-06 14:05:43+00:00,,2,Unable to install allennlp via pip in Windows,"When I tried installing allennlp in my machine with following configuration,
```
Windows 10 x64
Python 3.6.5
```

I am getting the following error,

```
Collecting allennlp
  Using cached https://files.pythonhosted.org/packages/c5/60/fa613bdea022bd6c26176f5786efcc0b5a6d8acf97131e324179a99fcbaf/allennlp-0.4.2-py3-none-any.whl
Collecting psycopg2 (from allennlp)
  Using cached https://files.pythonhosted.org/packages/00/95/4c5d19affca312e1c06d4f88241ebc564bf5269addd191ec4962f0c93553/psycopg2-2.7.4-cp36-cp36m-win32.whl
Collecting flask==0.12.1 (from allennlp)
  Using cached https://files.pythonhosted.org/packages/f4/43/fb2d5fb1d10e1d0402dd57836cf9a78b7f69c8b5f76a04b6e6113d0d7c5a/Flask-0.12.1-py2.py3-none-any.whl
Collecting scipy (from allennlp)
  Using cached https://files.pythonhosted.org/packages/30/2a/8bd20295c774e3f19b5f8b71d75ef7e802673852ca3ae2e1d231d0f1c7a2/scipy-1.1.0-cp36-none-win32.whl
Requirement already satisfied: typing in c:\users\user\appdata\local\programs\python\python36-32\lib\site-packages (from allennlp) (3.5.3.0)
Collecting pyhocon==0.3.35 (from allennlp)
  Downloading https://files.pythonhosted.org/packages/95/b9/72883593ce531e95ac8190e251ae6f5377eada69504248bf1aebfce4c5b4/pyhocon-0.3.35.tar.gz (94kB)
    100% |鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅| 102kB 238kB/s
Requirement already satisfied: numpy in c:\users\user\appdata\local\programs\python\python36-32\lib\site-packages (from allennlp) (1.12.1)
Collecting torch==0.3.1 (from allennlp)
  Could not find a version that satisfies the requirement torch==0.3.1 (from allennlp) (from versions: 0.1.2, 0.1.2.post1)
No matching distribution found for torch==0.3.1 (from allennlp)
```

How can I fix this ?",1,,,,,,,,,
245,https://github.com/allenai/allennlp/issues/1228,1228,[],closed,2018-05-16 22:53:36+00:00,,2,running allennlp test-install after installing with pip leads to many errors,"After installing AllenNLP with: `pip install git+git://github.com/allenai/allennlp.git` and running `allennlp test-install`, a bunch of errors come up. Raising an issue just to enumerate them all. A full gist of the test traceback is here: https://gist.github.com/nelson-liu/78d08ad0090ad8f95c34675942f5bb4d

- The biggest issue is that we lose the directory structure in the repo. For example, `allennlp` with pip would get installed in `/home/nfliu/miniconda3/envs/test/lib/python3.6/site-packages/allennlp`. this contains _only_ the module (so it's [this folder](https://github.com/allenai/allennlp/tree/master/allennlp)).  The `tutorials`, `scripts`, `READMEs`, etc. aren't actually installed anywhere, so all of the paths in the tests break (e.g., script tests that shell out to run `perl ./scripts/some_eval_script.pl`).

- On a separate related note it looks like the precompiled custom extensions in the module aren't copied over either, maybe because they're not `.py` files and they get ignored automatically? This makes the `test_custom_extensions` / `alternating_lstm` tests break, even though they should work. This also means that custom extensions in general aren't working in the `pip install`ed version of allennlp.

## Things that are actually broken in the pip-installed version

- the custom highway lstm (since the extensions aren't included). fixed by #1229 
- `evalb_bracketing_scorer`, since `scripts/EVALB` does not exist. fixed by #1233 
- any tests that rely on files in `./tutorials` and `./scripts`. wip solution in #1232 

The other test errors are just errors with the tests themselves, not with the allennlp package. Which is unfortunate, but it doesn't affect _core_ library functionality.",1,,,,,,,,,
283,https://github.com/allenai/allennlp/issues/1324,1324,[],closed,2018-05-31 22:16:11+00:00,,1,Create a Dockerfile that includes a basic pip installation of AllenNLP,"To make using AllenNLP easy for downstream users, we should create a simple Dockerfile that has a pip installation of AllenNLP.  This would make it easy for users to use AllenNLP as a library or a command in a Docker environment, and would provide a base image from which people could run AllenNLP jobs on Beaker.

```
FROM python:3.6.3-jessie

...

RUN pip install allennlp

ENTRYPOINT [""allennlp""]
```",1,,,,,,,,,
288,https://github.com/allenai/allennlp/issues/1332,1332,[],closed,2018-06-03 12:30:35+00:00,,5,Unable to install allennlp via pip,"When I tried to install allennlp via pip3, I ran into this issue:
```
sanic-plugins-framework 0.5.2.dev20180201 has requirement sanic>=0.7.0, but you'll have sanic 0.6.0 which is incompatible.
sanic-cors 0.9.4 has requirement sanic>=0.7.0, but you'll have sanic 0.6.0 which is incompatible.
```
I suspect that it's because in the setup.py, the required installs are:
```
sanic==0.6.0
sanic-cors
```
And the latest version of sanic-cors is incompatible with sanic==0.6.0.

Does anybody know how to deal with this problem? Thanks  alot!",1,,,,,,,,,
346,https://github.com/allenai/allennlp/issues/1468,1468,[],closed,2018-07-08 04:18:47+00:00,,1,pyparsing error when training allennlp coref with ELMo,"Hi, 
I am trying to train a coreference model with ELMo, but I got a **pyparsing.ParseSyntaxException**.  I use pytorch-0.4.0 and allennlp-0.5.0. Thank you!

pyparsing.ParseException: Expected {Forward: ... | {{""="" | "":"" | ""+=""} - [Suppress:({{{""#"" | ""//""} - SkipTo:({Suppress:(W:(
)) | StringEnd})} | Suppress:(W:(
))})]... - ConcatenatedValueParser:([{Suppress:({[Suppress:(W:(
,))] {""#"" | ""//""} - SkipTo:({Suppress:(W:(
)) | StringEnd})}) | {Suppress:(""include"") {Re:('"".*?""[ \t]*') | {{""url"" | ""file""} - Suppress:(""("") - Re:('"".*?""[ \t]*') - Suppress:("")"")}}} | Re:('[ \t]*\\$\\{[^\\}]+\\}[ \t]*') | Forward: ... | Forward: ... | Re:('[+-]?(\\d*\\.\\d+|\\d+(\\.\\d+)?)([eE]\\d+)?(?=$|[ \t]*([\\$\\}\\],#\n\r]|//))') | ""true"" | ""false"" | ""null"" | Re:('"""""".*?""""""') | Re:('"".*?""[ \t]*') | Re:('(?:\\\\|[^\\[\\{\\s\\]\\}#,=\\$])+[ \t]*') | Suppress:({""\"" - Suppress:(W:(
))})}]...)}} (at char 2932), (line:97, col:24)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/net/zf2/jz4fu/Github/allennlp/allennlp/run.py"", line 18, in <module>
    main(prog=""python -m allennlp.run"")
  File ""/net/zf2/jz4fu/Github/allennlp/allennlp/commands/__init__.py"", line 65, in main
    args.func(args)
  File ""/net/zf2/jz4fu/Github/allennlp/allennlp/commands/train.py"", line 99, in train_model_from_args
    args.recover)
  File ""/net/zf2/jz4fu/Github/allennlp/allennlp/commands/train.py"", line 128, in train_model_from_file
    params = Params.from_file(parameter_filename, overrides)
  File ""/net/zf2/jz4fu/Github/allennlp/allennlp/common/params.py"", line 285, in from_file
    file_dict = pyhocon.ConfigFactory.parse_file(params_file)
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyhocon/config_parser.py"", line 51, in parse_file
    return ConfigFactory.parse_string(content, os.path.dirname(filename), resolve)
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyhocon/config_parser.py"", line 90, in parse_string
    return ConfigParser().parse(content, basedir, resolve)
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyhocon/config_parser.py"", line 272, in parse
    config = config_expr.parseString(content, parseAll=True)[0]
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1632, in parseString
    raise exc
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1622, in parseString
    loc, tokens = self._parse( instring, 0 )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3395, in parseImpl
    loc, exprtokens = e._parse( instring, loc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3530, in parseImpl
    ret = e._parse( instring, loc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3717, in parseImpl
    return self.expr._parse( instring, loc, doActions, callPreParse=False )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3386, in parseImpl
    loc, exprtokens = e._parse( instring, loc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3717, in parseImpl
    return self.expr._parse( instring, loc, doActions, callPreParse=False )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3919, in parseImpl
    return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3858, in parseImpl
    loc, tmptokens = self_expr_parse( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3530, in parseImpl
    ret = e._parse( instring, loc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3717, in parseImpl
    return self.expr._parse( instring, loc, doActions, callPreParse=False )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3717, in parseImpl
    return self.expr._parse( instring, loc, doActions, callPreParse=False )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3386, in parseImpl
    loc, exprtokens = e._parse( instring, loc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3530, in parseImpl
    ret = e._parse( instring, loc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3386, in parseImpl
    loc, exprtokens = e._parse( instring, loc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3717, in parseImpl
    return self.expr._parse( instring, loc, doActions, callPreParse=False )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3919, in parseImpl
    return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3848, in parseImpl
    loc, tokens = self_expr_parse( instring, loc, doActions, callPreParse=False )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3530, in parseImpl
    ret = e._parse( instring, loc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3717, in parseImpl
    return self.expr._parse( instring, loc, doActions, callPreParse=False )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3386, in parseImpl
    loc, exprtokens = e._parse( instring, loc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3717, in parseImpl
    return self.expr._parse( instring, loc, doActions, callPreParse=False )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3919, in parseImpl
    return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3858, in parseImpl
    loc, tmptokens = self_expr_parse( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3530, in parseImpl
    ret = e._parse( instring, loc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3717, in parseImpl
    return self.expr._parse( instring, loc, doActions, callPreParse=False )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3717, in parseImpl
    return self.expr._parse( instring, loc, doActions, callPreParse=False )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 1379, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""/zf2/jz4fu/anaconda3/lib/python3.6/site-packages/pyparsing.py"", line 3391, in parseImpl
    raise ParseSyntaxException._from_exception(pe)
pyparsing.ParseSyntaxException: Expected {Forward: ... | {{""="" | "":"" | ""+=""} - [Suppress:({{{""#"" | ""//""} - SkipTo:({Suppress:(W:(
)) | StringEnd})} | Suppress:(W:(
))})]... - ConcatenatedValueParser:([{Suppress:({[Suppress:(W:(
,))] {""#"" | ""//""} - SkipTo:({Suppress:(W:(
)) | StringEnd})}) | {Suppress:(""include"") {Re:('"".*?""[ \t]*') | {{""url"" | ""file""} - Suppress:(""("") - Re:('"".*?""[ \t]*') - Suppress:("")"")}}} | Re:('[ \t]*\\$\\{[^\\}]+\\}[ \t]*') | Forward: ... | Forward: ... | Re:('[+-]?(\\d*\\.\\d+|\\d+(\\.\\d+)?)([eE]\\d+)?(?=$|[ \t]*([\\$\\}\\],#\n\r]|//))') | ""true"" | ""false"" | ""null"" | Re:('"""""".*?""""""') | Re:('"".*?""[ \t]*') | Re:('(?:\\\\|[^\\[\\{\\s\\]\\}#,=\\$])+[ \t]*') | Suppress:({""\"" - Suppress:(W:(
))})}]...)}} (at char 2932), (line:97, col:24)",1,,,,,,,,,
429,https://github.com/allenai/allennlp/issues/1662,1662,[],closed,2018-08-24 17:43:03+00:00,,9,When running pip-installed AllenNLP in AllenNLP source directory uses source files.,"**Describe the bug**

If i run the pip-installed AllenNLP in the allennlp source directory (e.g., if i want to train with the pip-version on one of the training configs in the source directory).


**To Reproduce**
Steps to reproduce the behavior
```
cd ~
conda create -n allennlp_pip python=3.6
source activate allennlp_pip
pip install allennlp

cd /allennlp/source/directory
allennlp train --help
```

**Expected behavior**

No crash, since it uses the pip version.

**Actual behavior**

I get an import error because it tries to import the files in the source directory, which have different requirements than the current pip installed version:

```
$ allennlp train --help
Traceback (most recent call last):
  File ""/Users/nfliu/miniconda3/envs/allennlp_pip/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/nfliu/miniconda3/envs/allennlp_pip/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/nfliu/Documents/Github/allennlp/allennlp/run.py"", line 15, in <module>
    from allennlp.commands import main  # pylint: disable=wrong-import-position
  File ""/Users/nfliu/Documents/Github/allennlp/allennlp/commands/__init__.py"", line 6, in <module>
    from allennlp.commands.configure import Configure
  File ""/Users/nfliu/Documents/Github/allennlp/allennlp/commands/configure.py"", line 22, in <module>
    from allennlp.common.configuration import configure, Config, render_config
  File ""/Users/nfliu/Documents/Github/allennlp/allennlp/common/__init__.py"", line 1, in <module>
    from allennlp.common.params import Params
  File ""/Users/nfliu/Documents/Github/allennlp/allennlp/common/params.py"", line 30, in <module>
    from allennlp.common.file_utils import cached_path
  File ""/Users/nfliu/Documents/Github/allennlp/allennlp/common/file_utils.py"", line 16, in <module>
    import boto3
ModuleNotFoundError: No module named 'boto3'
```

**System (please complete the following information):**
 - OS: OS X
 - Python version: 3.6.6
 - AllenNLP version: 0.6
 - PyTorch version: 0.4.1

cc: @schmmd / @joelgrus , any clue of what's going on here? Maybe something odd with PYTHONPATH?",1,,,,,,,,,
478,https://github.com/allenai/allennlp/issues/1788,1788,[],closed,2018-09-19 02:56:40+00:00,,1,Is it possible to pip install allennlp on python 3.5.1 ?,"The latest allennlp requires python version to be 3.6. 
Is it possible to pip install allennlp on python 3.5.1 ? Because updating python is not an option right now for me.
",1,,,,,,,,,
572,https://github.com/allenai/allennlp/issues/1969,1969,[],closed,2018-10-25 19:35:19+00:00,,11,Pip3 install failing on jsonnet,"**Describe the bug**

I am getting the following stacktrace error when attempting to install with the command `pip install allennlp`.  It is failing on jsonnet package but I don't know why.

>   Failed building wheel for jsonnet
  Running setup.py clean for jsonnet
Failed to build jsonnet
Installing collected packages: jsonnet, allennlp, jsondiff, aws-xray-sdk, atomicwrites, attrs, asn1crypto, alabaster
  Running setup.py install for jsonnet ... error
    Complete output from command /usr/local/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/private/var/folders/sp/4bfpfrfd4m3bx3nl84rvwd5w0000gn/T/pip-install-s0sa1az1/jsonnet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /private/var/folders/sp/4bfpfrfd4m3bx3nl84rvwd5w0000gn/T/pip-record-akb978cv/install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_ext
    make: `core/desugarer.o' is up to date.
    make: `core/formatter.o' is up to date.
    make: `core/libjsonnet.o' is up to date.
    make: `core/lexer.o' is up to date.
    make: `core/parser.o' is up to date.
    make: `core/pass.o' is up to date.
    make: `core/static_analysis.o' is up to date.
    make: `core/string_utils.o' is up to date.
    make: `core/vm.o' is up to date.
    make: `third_party/md5/md5.o' is up to date.
    building '_jsonnet' extension
    creating build
    creating build/temp.macosx-10.6-intel-3.6
    creating build/temp.macosx-10.6-intel-3.6/python
    /usr/bin/clang -fno-strict-aliasing -Wsign-compare -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -arch i386 -arch x86_64 -g -Iinclude -Ithird_party/md5 -I/Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c python/_jsonnet.c -o build/temp.macosx-10.6-intel-3.6/python/_jsonnet.o
    python/_jsonnet.c:147:19: warning: comparison of integers of different signs: 'int' and 'const size_t' (aka 'const unsigned long') [-Wsign-compare]
        for (i = 0; i < ctx->argc; ++i) {
                    ~ ^ ~~~~~~~~~
    1 warning generated.
    python/_jsonnet.c:147:19: warning: comparison of integers of different signs: 'int' and 'const size_t' (aka 'const unsigned long') [-Wsign-compare]
        for (i = 0; i < ctx->argc; ++i) {
                    ~ ^ ~~~~~~~~~
    1 warning generated.
    creating build/lib.macosx-10.6-intel-3.6
    /usr/bin/clang++ -bundle -undefined dynamic_lookup -arch i386 -arch x86_64 -g build/temp.macosx-10.6-intel-3.6/python/_jsonnet.o core/desugarer.o core/formatter.o core/libjsonnet.o core/lexer.o core/parser.o core/pass.o core/static_analysis.o core/string_utils.o core/vm.o third_party/md5/md5.o -o build/lib.macosx-10.6-intel-3.6/_jsonnet.cpython-36m-darwin.so
    clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]
    clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]
    ld: library not found for -lstdc++
    clang: error: linker command failed with exit code 1 (use -v to see invocation)
    error: command '/usr/bin/clang++' failed with exit status 1
    
    ----------------------------------------
Command ""/usr/local/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/private/var/folders/sp/4bfpfrfd4m3bx3nl84rvwd5w0000gn/T/pip-install-s0sa1az1/jsonnet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /private/var/folders/sp/4bfpfrfd4m3bx3nl84rvwd5w0000gn/T/pip-record-akb978cv/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/sp/4bfpfrfd4m3bx3nl84rvwd5w0000gn/T/pip-install-s0sa1az1/jsonnet/


**System (please complete the following information):**
 - OS: OSX
 - Python version: 3.6.3
 - AllenNLP version: Latest from pip
 - PyTorch version: 0.4.0

**Additional context**
If there are any workarounds, I am very willing to try them.  I am blocked by something until I can resolve this issue.  Thanks so much.
",1,,,,,,,,,
590,https://github.com/allenai/allennlp/issues/2009,2009,[],closed,2018-11-04 04:16:28+00:00,,8,pip install allennlp in conda error fatal error: ffi.h: No such file or directory,"**Describe the bug**
I have a clean install of anaconda and I tried to install allennlp by
`pip install allennlp`
I got this output:
Collecting allennlp
  Using cached https://files.pythonhosted.org/packages/db/98/3b620e6472a240a4451c6daece6ce9d806fde0b0250606f910557a5353f6/allennlp-0.7.1-py3-none-any.whl
Collecting awscli>=1.11.91 (from allennlp)
  Using cached https://files.pythonhosted.org/packages/70/8b/16738ab3f1e0292b1d3b71f192227ab07575a216416b0562e74e6aad9c0a/awscli-1.16.47-py2.py3-none-any.whl
Collecting moto==1.3.4 (from allennlp)
  Using cached https://files.pythonhosted.org/packages/ee/8f/7b36e81ff067d0e7bf90f7210b351c0cfe6657f79fa4dcb0cb4787462e05/moto-1.3.4-py2.py3-none-any.whl
Requirement already satisfied: conllu==0.11 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (0.11)
Requirement already satisfied: pytest in ./anaconda3/lib/python3.7/site-packages (from allennlp) (3.8.0)
Requirement already satisfied: ftfy in ./anaconda3/lib/python3.7/site-packages (from allennlp) (5.5.0)
Requirement already satisfied: unidecode in ./anaconda3/lib/python3.7/site-packages (from allennlp) (1.0.22)
Requirement already satisfied: overrides in ./anaconda3/lib/python3.7/site-packages (from allennlp) (1.9)
Requirement already satisfied: tensorboardX==1.2 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (1.2)
Collecting boto3 (from allennlp)
  Using cached https://files.pythonhosted.org/packages/7a/7e/fe8faa29e771a09c528ee70e1fd9b317006021c48311ecccc78c22ebe739/boto3-1.9.37-py2.py3-none-any.whl
Requirement already satisfied: scikit-learn in ./anaconda3/lib/python3.7/site-packages (from allennlp) (0.19.2)
Requirement already satisfied: pytz==2017.3 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (2017.3)
Requirement already satisfied: nltk in ./anaconda3/lib/python3.7/site-packages (from allennlp) (3.3)
Requirement already satisfied: numpy in ./anaconda3/lib/python3.7/site-packages (from allennlp) (1.15.1)
Requirement already satisfied: sqlparse==0.2.4 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (0.2.4)
Requirement already satisfied: spacy<2.1,>=2.0 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (2.0.16)
Collecting cffi==1.11.2 (from allennlp)
  Using cached https://files.pythonhosted.org/packages/c9/70/89b68b6600d479034276fed316e14b9107d50a62f5627da37fafe083fde3/cffi-1.11.2.tar.gz
Requirement already satisfied: responses>=0.7 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (0.10.2)
Requirement already satisfied: tqdm>=4.19 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (4.26.0)
Requirement already satisfied: requests>=2.18 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (2.19.1)
Requirement already satisfied: scipy in ./anaconda3/lib/python3.7/site-packages (from allennlp) (1.1.0)
Requirement already satisfied: gevent==1.3.6 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (1.3.6)
Requirement already satisfied: h5py in ./anaconda3/lib/python3.7/site-packages (from allennlp) (2.8.0)
Requirement already satisfied: parsimonious==0.8.0 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (0.8.0)
Collecting flask-cors==3.0.3 (from allennlp)
  Using cached https://files.pythonhosted.org/packages/83/a7/c7243ffd096a491013956c9ee71e2ed0b7d14979fafe89986ca2d30fc6f7/Flask_Cors-3.0.3-py2.py3-none-any.whl
Requirement already satisfied: numpydoc==0.8.0 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (0.8.0)
Requirement already satisfied: jsonnet==0.10.0; sys_platform != ""win32"" in ./anaconda3/lib/python3.7/site-packages (from allennlp) (0.10.0)
Requirement already satisfied: flask==0.12.4 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (0.12.4)
Requirement already satisfied: flaky in ./anaconda3/lib/python3.7/site-packages (from allennlp) (3.4.0)
Requirement already satisfied: matplotlib==2.2.3 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (2.2.3)
Requirement already satisfied: torch<0.5.0,>=0.4.1 in ./anaconda3/lib/python3.7/site-packages (from allennlp) (0.4.1.post2)
Requirement already satisfied: editdistance in ./anaconda3/lib/python3.7/site-packages (from allennlp) (0.5.2)
Requirement already satisfied: PyYAML<=3.13,>=3.10 in ./anaconda3/lib/python3.7/site-packages (from awscli>=1.11.91->allennlp) (3.13)
Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in ./anaconda3/lib/python3.7/site-packages (from awscli>=1.11.91->allennlp) (0.3.9)
Requirement already satisfied: docutils>=0.10 in ./anaconda3/lib/python3.7/site-packages (from awscli>=1.11.91->allennlp) (0.14)
Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)
  Using cached https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl
Collecting s3transfer<0.2.0,>=0.1.12 (from awscli>=1.11.91->allennlp)
  Using cached https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl
Collecting botocore==1.12.37 (from awscli>=1.11.91->allennlp)
  Using cached https://files.pythonhosted.org/packages/d0/35/1461771f778b67984a75a9853a2b3ed25e65a4345e669a81b50c67c930ab/botocore-1.12.37-py2.py3-none-any.whl
Requirement already satisfied: werkzeug in ./anaconda3/lib/python3.7/site-packages (from moto==1.3.4->allennlp) (0.14.1)
Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./anaconda3/lib/python3.7/site-packages (from moto==1.3.4->allennlp) (2.7.3)
Collecting pyaml (from moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/17/c1/5892f756109e54ed53c753129b0da4acf6b6add8dff5a85b18667553b16d/pyaml-17.12.1-py2.py3-none-any.whl
Requirement already satisfied: cryptography>=2.0.0 in ./anaconda3/lib/python3.7/site-packages (from moto==1.3.4->allennlp) (2.3.1)
Requirement already satisfied: boto>=2.36.0 in ./anaconda3/lib/python3.7/site-packages (from moto==1.3.4->allennlp) (2.49.0)
Collecting docker>=2.5.1 (from moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/c2/76/b8091dc6d9db038af62ae88f228da656a84632cf5d7a84dcf54c613d3fd0/docker-3.5.1-py2.py3-none-any.whl
Collecting mock (from moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl
Collecting cookies (from moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/6a/60/557f84aa2db629e5124aa05408b975b1b5d0e1cec16cde0bfa06aae097d3/cookies-2.2.1-py2.py3-none-any.whl
Collecting aws-xray-sdk<0.96,>=0.93 (from moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/a4/a5/da7887285564f9e0ae5cd25a453cca36e2cd43d8ccc9effde260b4d80904/aws_xray_sdk-0.95-py2.py3-none-any.whl
Requirement already satisfied: Jinja2>=2.7.3 in ./anaconda3/lib/python3.7/site-packages (from moto==1.3.4->allennlp) (2.10)
Collecting xmltodict (from moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/42/a9/7e99652c6bc619d19d58cdd8c47560730eb5825d43a7e25db2e1d776ceb7/xmltodict-0.11.0-py2.py3-none-any.whl
Collecting jsondiff==1.1.1 (from moto==1.3.4->allennlp)
Requirement already satisfied: six>1.9 in ./anaconda3/lib/python3.7/site-packages (from moto==1.3.4->allennlp) (1.11.0)
Collecting python-jose<3.0.0 (from moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/bf/5c/5fa238c0c5b0656994b52721dd8b1d7bf52ebd8786518dde794f44de86b6/python_jose-2.0.2-py2.py3-none-any.whl
Requirement already satisfied: py>=1.5.0 in ./anaconda3/lib/python3.7/site-packages (from pytest->allennlp) (1.6.0)
Requirement already satisfied: setuptools in ./anaconda3/lib/python3.7/site-packages (from pytest->allennlp) (40.2.0)
Requirement already satisfied: attrs>=17.4.0 in ./anaconda3/lib/python3.7/site-packages (from pytest->allennlp) (18.2.0)
Requirement already satisfied: more-itertools>=4.0.0 in ./anaconda3/lib/python3.7/site-packages (from pytest->allennlp) (4.3.0)
Requirement already satisfied: atomicwrites>=1.0 in ./anaconda3/lib/python3.7/site-packages (from pytest->allennlp) (1.2.1)
Requirement already satisfied: pluggy>=0.7 in ./anaconda3/lib/python3.7/site-packages (from pytest->allennlp) (0.7.1)
Requirement already satisfied: wcwidth in ./anaconda3/lib/python3.7/site-packages (from ftfy->allennlp) (0.1.7)
Requirement already satisfied: protobuf>=0.3.2 in ./anaconda3/lib/python3.7/site-packages (from tensorboardX==1.2->allennlp) (3.6.1)
Collecting jmespath<1.0.0,>=0.7.1 (from boto3->allennlp)
  Using cached https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl
Requirement already satisfied: ujson>=1.35 in ./anaconda3/lib/python3.7/site-packages (from spacy<2.1,>=2.0->allennlp) (1.35)
Requirement already satisfied: thinc<6.13.0,>=6.12.0 in ./anaconda3/lib/python3.7/site-packages (from spacy<2.1,>=2.0->allennlp) (6.12.0)
Requirement already satisfied: plac<1.0.0,>=0.9.6 in ./anaconda3/lib/python3.7/site-packages (from spacy<2.1,>=2.0->allennlp) (0.9.6)
Requirement already satisfied: regex==2018.01.10 in ./anaconda3/lib/python3.7/site-packages (from spacy<2.1,>=2.0->allennlp) (2018.1.10)
Requirement already satisfied: preshed<2.1.0,>=2.0.1 in ./anaconda3/lib/python3.7/site-packages (from spacy<2.1,>=2.0->allennlp) (2.0.1)
Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./anaconda3/lib/python3.7/site-packages (from spacy<2.1,>=2.0->allennlp) (1.0.1)
Requirement already satisfied: dill<0.3,>=0.2 in ./anaconda3/lib/python3.7/site-packages (from spacy<2.1,>=2.0->allennlp) (0.2.8.2)
Requirement already satisfied: msgpack-numpy<0.4.4 in ./anaconda3/lib/python3.7/site-packages (from spacy<2.1,>=2.0->allennlp) (0.4.3.2)
Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./anaconda3/lib/python3.7/site-packages (from spacy<2.1,>=2.0->allennlp) (2.0.2)
Requirement already satisfied: pycparser in ./anaconda3/lib/python3.7/site-packages (from cffi==1.11.2->allennlp) (2.18)
Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/lib/python3.7/site-packages (from requests>=2.18->allennlp) (3.0.4)
Requirement already satisfied: idna<2.8,>=2.5 in ./anaconda3/lib/python3.7/site-packages (from requests>=2.18->allennlp) (2.7)
Requirement already satisfied: urllib3<1.24,>=1.21.1 in ./anaconda3/lib/python3.7/site-packages (from requests>=2.18->allennlp) (1.23)
Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.7/site-packages (from requests>=2.18->allennlp) (2018.8.24)
Requirement already satisfied: greenlet>=0.4.14 in ./anaconda3/lib/python3.7/site-packages (from gevent==1.3.6->allennlp) (0.4.15)
Requirement already satisfied: sphinx>=1.2.3 in ./anaconda3/lib/python3.7/site-packages (from numpydoc==0.8.0->allennlp) (1.7.9)
Requirement already satisfied: click>=2.0 in ./anaconda3/lib/python3.7/site-packages (from flask==0.12.4->allennlp) (6.7)
Requirement already satisfied: itsdangerous>=0.21 in ./anaconda3/lib/python3.7/site-packages (from flask==0.12.4->allennlp) (0.24)
Requirement already satisfied: cycler>=0.10 in ./anaconda3/lib/python3.7/site-packages (from matplotlib==2.2.3->allennlp) (0.10.0)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in ./anaconda3/lib/python3.7/site-packages (from matplotlib==2.2.3->allennlp) (2.2.0)
Requirement already satisfied: kiwisolver>=1.0.1 in ./anaconda3/lib/python3.7/site-packages (from matplotlib==2.2.3->allennlp) (1.0.1)
Requirement already satisfied: pyasn1>=0.1.3 in ./anaconda3/lib/python3.7/site-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.4)
Requirement already satisfied: asn1crypto>=0.21.0 in ./anaconda3/lib/python3.7/site-packages (from cryptography>=2.0.0->moto==1.3.4->allennlp) (0.24.0)
Collecting docker-pycreds>=0.3.0 (from docker>=2.5.1->moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/ea/bf/7e70aeebc40407fbdb96fa9f79fc8e4722ea889a99378303e3bcc73f4ab5/docker_pycreds-0.3.0-py2.py3-none-any.whl
Collecting websocket-client>=0.32.0 (from docker>=2.5.1->moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/26/2d/f749a5c82f6192d77ed061a38e02001afcba55fe8477336d26a950ab17ce/websocket_client-0.54.0-py2.py3-none-any.whl
Collecting pbr>=0.11 (from mock->moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/76/0c/304d968fe010ba7c2ecc1d57e28741ddd3a305439dcf1cdb3b6f896a3c00/pbr-5.1.0-py2.py3-none-any.whl
Requirement already satisfied: wrapt in ./anaconda3/lib/python3.7/site-packages (from aws-xray-sdk<0.96,>=0.93->moto==1.3.4->allennlp) (1.10.11)
Collecting jsonpickle (from aws-xray-sdk<0.96,>=0.93->moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/ca/ce/97404d5aeb58e6155c216825c81b50f6eca8a5345c582317ae48391878f8/jsonpickle-1.0-py2.py3-none-any.whl
Requirement already satisfied: MarkupSafe>=0.23 in ./anaconda3/lib/python3.7/site-packages (from Jinja2>=2.7.3->moto==1.3.4->allennlp) (1.0)
Collecting ecdsa<1.0 (from python-jose<3.0.0->moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/63/f4/73669d51825516ce8c43b816c0a6b64cd6eb71d08b99820c00792cb42222/ecdsa-0.13-py2.py3-none-any.whl
Collecting pycryptodome<4.0.0,>=3.3.1 (from python-jose<3.0.0->moto==1.3.4->allennlp)
  Using cached https://files.pythonhosted.org/packages/6e/87/89e3bc9ba9c3670ea3e8ed0ae1b97bc0452cca04f548624af8a90ac92ac2/pycryptodome-3.7.0-cp37-cp37m-manylinux1_x86_64.whl
Collecting future<1.0 (from python-jose<3.0.0->moto==1.3.4->allennlp)
Requirement already satisfied: cytoolz<0.10,>=0.9.0 in ./anaconda3/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.0->spacy<2.1,>=2.0->allennlp) (0.9.0.1)
Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in ./anaconda3/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.0->spacy<2.1,>=2.0->allennlp) (0.5.6)
Requirement already satisfied: Pygments>=2.0 in ./anaconda3/lib/python3.7/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.2.0)
Requirement already satisfied: snowballstemmer>=1.1 in ./anaconda3/lib/python3.7/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.2.1)
Requirement already satisfied: babel!=2.0,>=1.3 in ./anaconda3/lib/python3.7/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.6.0)
Requirement already satisfied: alabaster<0.8,>=0.7 in ./anaconda3/lib/python3.7/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (0.7.11)
Requirement already satisfied: imagesize in ./anaconda3/lib/python3.7/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)
Requirement already satisfied: packaging in ./anaconda3/lib/python3.7/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (17.1)
Requirement already satisfied: sphinxcontrib-websupport in ./anaconda3/lib/python3.7/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)
Requirement already satisfied: toolz>=0.8.0 in ./anaconda3/lib/python3.7/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.0->spacy<2.1,>=2.0->allennlp) (0.9.0)
Building wheels for collected packages: cffi
  Running setup.py bdist_wheel for cffi ... error
  Complete output from command /mnt/cephfs2/asr/users/ming.tu/anaconda3/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-11bng7n1/cffi/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /tmp/pip-wheel-91pno_hs --python-tag cp37:
  Package libffi was not found in the pkg-config search path.
  Perhaps you should add the directory containing `libffi.pc'
  to the PKG_CONFIG_PATH environment variable
  No package 'libffi' found
  Package libffi was not found in the pkg-config search path.
  Perhaps you should add the directory containing `libffi.pc'
  to the PKG_CONFIG_PATH environment variable
  No package 'libffi' found
  Package libffi was not found in the pkg-config search path.
  Perhaps you should add the directory containing `libffi.pc'
  to the PKG_CONFIG_PATH environment variable
  No package 'libffi' found
  Package libffi was not found in the pkg-config search path.
  Perhaps you should add the directory containing `libffi.pc'
  to the PKG_CONFIG_PATH environment variable
  No package 'libffi' found
  Package libffi was not found in the pkg-config search path.
  Perhaps you should add the directory containing `libffi.pc'
  to the PKG_CONFIG_PATH environment variable
  No package 'libffi' found
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.7
  creating build/lib.linux-x86_64-3.7/cffi
  copying cffi/vengine_cpy.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/model.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/commontypes.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/error.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/backend_ctypes.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/cparser.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/ffiplatform.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/vengine_gen.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/api.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/lock.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/__init__.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/setuptools_ext.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/cffi_opcode.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/verifier.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/recompiler.py -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/_cffi_include.h -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/parse_c_type.h -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/_embedding.h -> build/lib.linux-x86_64-3.7/cffi
  copying cffi/_cffi_errors.h -> build/lib.linux-x86_64-3.7/cffi
  running build_ext
  building '_cffi_backend' extension
  creating build/temp.linux-x86_64-3.7
  creating build/temp.linux-x86_64-3.7/c
  gcc -pthread -B /mnt/cephfs2/asr/users/ming.tu/anaconda3/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DUSE__THREAD -DHAVE_SYNC_SYNCHRONIZE -I/usr/include/ffi -I/usr/include/libffi -I/mnt/cephfs2/asr/users/ming.tu/anaconda3/include/python3.7m -c c/_cffi_backend.c -o build/temp.linux-x86_64-3.7/c/_cffi_backend.o
  c/_cffi_backend.c:15:17: fatal error: ffi.h: No such file or directory
   #include <ffi.h>
                   ^
  compilation terminated.
  error: command 'gcc' failed with exit status 1

  ----------------------------------------
  Failed building wheel for cffi
  Running setup.py clean for cffi
Failed to build cffi
twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.
Installing collected packages: rsa, jmespath, botocore, s3transfer, awscli, pyaml, docker-pycreds, websocket-client, docker, pbr, mock, cookies, jsonpickle, aws-xray-sdk, xmltodict, boto3, jsondiff, ecdsa, pycryptodome, future, python-jose, moto, cffi, flask-cors, allennlp
  Found existing installation: cffi 1.11.5
    Uninstalling cffi-1.11.5:
      Successfully uninstalled cffi-1.11.5
  Running setup.py install for cffi ... error
    Complete output from command /mnt/cephfs2/asr/users/ming.tu/anaconda3/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-11bng7n1/cffi/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-p7w_9vxs/install-record.txt --single-version-externally-managed --compile:
    Package libffi was not found in the pkg-config search path.
    Perhaps you should add the directory containing `libffi.pc'
    to the PKG_CONFIG_PATH environment variable
    No package 'libffi' found
    Package libffi was not found in the pkg-config search path.
    Perhaps you should add the directory containing `libffi.pc'
    to the PKG_CONFIG_PATH environment variable
    No package 'libffi' found
    Package libffi was not found in the pkg-config search path.
    Perhaps you should add the directory containing `libffi.pc'
    to the PKG_CONFIG_PATH environment variable
    No package 'libffi' found
    Package libffi was not found in the pkg-config search path.
    Perhaps you should add the directory containing `libffi.pc'
    to the PKG_CONFIG_PATH environment variable
    No package 'libffi' found
    Package libffi was not found in the pkg-config search path.
    Perhaps you should add the directory containing `libffi.pc'
    to the PKG_CONFIG_PATH environment variable
    No package 'libffi' found
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.7
    creating build/lib.linux-x86_64-3.7/cffi
    copying cffi/vengine_cpy.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/model.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/commontypes.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/error.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/backend_ctypes.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/cparser.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/ffiplatform.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/vengine_gen.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/api.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/lock.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/__init__.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/setuptools_ext.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/cffi_opcode.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/verifier.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/recompiler.py -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/_cffi_include.h -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/parse_c_type.h -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/_embedding.h -> build/lib.linux-x86_64-3.7/cffi
    copying cffi/_cffi_errors.h -> build/lib.linux-x86_64-3.7/cffi
    running build_ext
    building '_cffi_backend' extension
    creating build/temp.linux-x86_64-3.7
    creating build/temp.linux-x86_64-3.7/c
    gcc -pthread -B /mnt/cephfs2/asr/users/ming.tu/anaconda3/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DUSE__THREAD -DHAVE_SYNC_SYNCHRONIZE -I/usr/include/ffi -I/usr/include/libffi -I/mnt/cephfs2/asr/users/ming.tu/anaconda3/include/python3.7m -c c/_cffi_backend.c -o build/temp.linux-x86_64-3.7/c/_cffi_backend.o
    c/_cffi_backend.c:15:17: fatal error: ffi.h: No such file or directory
     #include <ffi.h>
                     ^
    compilation terminated.
    error: command 'gcc' failed with exit status 1

    ----------------------------------------
  Rolling back uninstall of cffi
Command ""/mnt/cephfs2/asr/users/ming.tu/anaconda3/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-11bng7n1/cffi/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-p7w_9vxs/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-install-11bng7n1/cffi/
You are using pip version 10.0.1, however version 18.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.

**To Reproduce**

**Expected behavior**

**System (please complete the following information):**
 - OS: CentOS Linux
 - Python version: Python3.7
 - AllenNLP version: default
 - PyTorch version: 0.4.1

**Additional context**
It's a cluster and I don't have permission to run ""sudo apt-get xxx""
",1,,,,,,,,,
596,https://github.com/allenai/allennlp/issues/2017,2017,[],closed,2018-11-05 23:39:16+00:00,,2,EVALB errors when using pip install,"@nelson-liu recently made [a PR to build EVALB if it doesn't exist](https://github.com/allenai/allennlp/pull/1964) so he could train the SRL model via Docker.  He succeeded in doing this from the main Dockerfile, but it still doesn't work when installing AllenNLP via pip.

EVALB is included at `/usr/local/lib/python3.6/site-packages/allennlp/tools/EVALB/` and it seems to be compiled, but the process errors out.

```
2018-11-05T23:12:22.831824463Z	rm -f evalb
2018-11-05T23:12:22.833699671Z	gcc -Wall -g -o evalb evalb.c
2018-11-05T23:12:22.972013565Z	evalb.c: In function 鈥榤ain鈥?
2018-11-05T23:12:22.972099997Z	evalb.c:379:22: warning: pointer targets in passing argument 1 of 鈥榝gets鈥?differ in signedness [-Wpointer-sign]
2018-11-05T23:12:22.97212103Z	     for(Line=1;fgets(buff,5000,fd1)!=NULL;Line++){
2018-11-05T23:12:22.972124759Z	                      ^
2018-11-05T23:12:22.972199025Z	In file included from evalb.c:21:0:
2018-11-05T23:12:22.97220439Z	/usr/include/stdio.h:622:14: note: expected 鈥榗har * __restrict__鈥?but argument is of type 鈥榰nsigned char *鈥?
2018-11-05T23:12:22.97221632Z	 extern char *fgets (char *__restrict __s, int __n, FILE *__restrict __stream)
2018-11-05T23:12:22.972219117Z	              ^
2018-11-05T23:12:22.972221492Z	evalb.c:386:9: warning: pointer targets in passing argument 1 of 鈥榮trcpy鈥?differ in signedness [-Wpointer-sign]
2018-11-05T23:12:22.972224103Z	  strcpy(buff1,buff);
2018-11-05T23:12:22.972226475Z	         ^
2018-11-05T23:12:22.972323105Z	In file included from evalb.c:24:0:
2018-11-05T23:12:22.972336209Z	/usr/include/string.h:129:14: note: expected 鈥榗har * __restrict__鈥?but argument is of type 鈥榰nsigned char *鈥?
2018-11-05T23:12:22.972340073Z	 extern char *strcpy (char *__restrict __dest, const char *__restrict __src)
2018-11-05T23:12:22.972342889Z	              ^
2018-11-05T23:12:22.972345497Z	evalb.c:386:15: warning: pointer targets in passing argument 2 of 鈥榮trcpy鈥?differ in signedness [-Wpointer-sign]
2018-11-05T23:12:22.972348423Z	  strcpy(buff1,buff);
2018-11-05T23:12:22.972350922Z	               ^
2018-11-05T23:12:22.972353329Z	In file included from evalb.c:24:0:
2018-11-05T23:12:22.97235596Z	/usr/include/string.h:129:14: note: expected 鈥榗onst char * __restrict__鈥?but argument is of type 鈥榰nsigned char *鈥?
2018-11-05T23:12:22.972358683Z	 extern char *strcpy (char *__restrict __dest, const char *__restrict __src)
2018-11-05T23:12:22.97236129Z	              ^
2018-11-05T23:12:22.97236375Z	evalb.c:389:11: warning: pointer targets in passing argument 1 of 鈥榝gets鈥?differ in signedness [-Wpointer-sign]
2018-11-05T23:12:22.972366852Z	  if(fgets(buff,5000,fd2)==NULL){
2018-11-05T23:12:22.972369267Z	           ^
2018-11-05T23:12:22.972371925Z	In file included from evalb.c:21:0:
2018-11-05T23:12:22.972374333Z	/usr/include/stdio.h:622:14: note: expected 鈥榗har * __restrict__鈥?but argument is of type 鈥榰nsigned char *鈥?
2018-11-05T23:12:22.972377617Z	 extern char *fgets (char *__restrict __s, int __n, FILE *__restrict __stream)
2018-11-05T23:12:22.972380067Z	              ^
2018-11-05T23:12:22.972423646Z	evalb.c:404:14: warning: pointer targets in passing argument 1 of 鈥榝gets鈥?differ in signedness [-Wpointer-sign]
2018-11-05T23:12:22.972428396Z	     if(fgets(buff,5000,fd2)!=NULL){
2018-11-05T23:12:22.972430816Z	              ^
2018-11-05T23:12:22.972433169Z	In file included from evalb.c:21:0:
2018-11-05T23:12:22.972435539Z	/usr/include/stdio.h:622:14: note: expected 鈥榗har * __restrict__鈥?but argument is of type 鈥榰nsigned char *鈥?
2018-11-05T23:12:22.97243808Z	 extern char *fgets (char *__restrict __s, int __n, FILE *__restrict __stream)
2018-11-05T23:12:22.972440699Z	              ^
2018-11-05T23:12:22.974330514Z	evalb.c: In function 鈥榬ead_line鈥?
2018-11-05T23:12:22.97434194Z	evalb.c:659:11: warning: variable 鈥榥鈥?set but not used [-Wunused-but-set-variable]
2018-11-05T23:12:22.97435082Z	     int   n;              /* temporary remembering the position */
2018-11-05T23:12:22.974354915Z	           ^
2018-11-05T23:12:22.97471461Z	evalb.c: In function 鈥榗alc_result鈥?
2018-11-05T23:12:22.974724145Z	evalb.c:879:23: warning: pointer targets in passing argument 2 of 鈥榮trncpy鈥?differ in signedness [-Wpointer-sign]
2018-11-05T23:12:22.974727252Z	        strncpy(my_buf,buf1+bracket1[i].buf_start,l);
2018-11-05T23:12:22.974730054Z	                       ^
2018-11-05T23:12:22.974772004Z	In file included from evalb.c:24:0:
2018-11-05T23:12:22.974779845Z	/usr/include/string.h:132:14: note: expected 鈥榗onst char * __restrict__鈥?but argument is of type 鈥榰nsigned char *鈥?
2018-11-05T23:12:22.974782984Z	 extern char *strncpy (char *__restrict __dest,
2018-11-05T23:12:22.974785658Z	              ^
2018-11-05T23:12:22.974866489Z	evalb.c:893:17: warning: pointer targets in passing argument 2 of 鈥榮trncpy鈥?differ in signedness [-Wpointer-sign]
2018-11-05T23:12:22.974873456Z	  strncpy(my_buf,buf1+bracket1[i].buf_start,l);
2018-11-05T23:12:22.974876275Z	                 ^
2018-11-05T23:12:22.974878899Z	In file included from evalb.c:24:0:
2018-11-05T23:12:22.974881364Z	/usr/include/string.h:132:14: note: expected 鈥榗onst char * __restrict__鈥?but argument is of type 鈥榰nsigned char *鈥?
2018-11-05T23:12:22.974884345Z	 extern char *strncpy (char *__restrict __dest,
2018-11-05T23:12:22.974886886Z	              ^
2018-11-05T23:12:22.974992069Z	evalb.c:905:17: warning: pointer targets in passing argument 2 of 鈥榮trncpy鈥?differ in signedness [-Wpointer-sign]
2018-11-05T23:12:22.974999875Z	  strncpy(my_buf,buf+bracket2[j].buf_start,l);
2018-11-05T23:12:22.975002451Z	                 ^
2018-11-05T23:12:22.975004865Z	In file included from evalb.c:24:0:
2018-11-05T23:12:22.975007242Z	/usr/include/string.h:132:14: note: expected 鈥榗onst char * __restrict__鈥?but argument is of type 鈥榰nsigned char *鈥?
2018-11-05T23:12:22.975009908Z	 extern char *strncpy (char *__restrict __dest,
2018-11-05T23:12:22.975012444Z	              ^
2018-11-05T23:12:22.975167579Z	evalb.c:933:22: warning: pointer targets in passing argument 2 of 鈥榮trncpy鈥?differ in signedness [-Wpointer-sign]
2018-11-05T23:12:22.975175169Z	       strncpy(my_buf,buf1+bracket1[i].buf_start,l);
2018-11-05T23:12:22.975177902Z	                      ^
2018-11-05T23:12:22.975180615Z	In file included from evalb.c:24:0:
2018-11-05T23:12:22.975183218Z	/usr/include/string.h:132:14: note: expected 鈥榗onst char * __restrict__鈥?but argument is of type 鈥榰nsigned char *鈥?
2018-11-05T23:12:22.975186157Z	 extern char *strncpy (char *__restrict __dest,
2018-11-05T23:12:22.975188828Z	              ^
2018-11-05T23:12:23.122217168Z	Traceback (most recent call last):
2018-11-05T23:12:23.12223291Z	  File ""/usr/local/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
2018-11-05T23:12:23.122245764Z	    ""__main__"", mod_spec)
2018-11-05T23:12:23.122248658Z	  File ""/usr/local/lib/python3.6/runpy.py"", line 85, in _run_code
2018-11-05T23:12:23.122251412Z	    exec(code, run_globals)
2018-11-05T23:12:23.12225381Z	  File ""/usr/local/lib/python3.6/site-packages/allennlp/run.py"", line 18, in <module>
2018-11-05T23:12:23.122256982Z	    main(prog=""allennlp"")
2018-11-05T23:12:23.122259481Z	  File ""/usr/local/lib/python3.6/site-packages/allennlp/commands/__init__.py"", line 72, in main
2018-11-05T23:12:23.122262242Z	    args.func(args)
2018-11-05T23:12:23.122264615Z	  File ""/usr/local/lib/python3.6/site-packages/allennlp/commands/train.py"", line 111, in train_model_from_args
2018-11-05T23:12:23.122267368Z	    args.force)
2018-11-05T23:12:23.122269744Z	  File ""/usr/local/lib/python3.6/site-packages/allennlp/commands/train.py"", line 142, in train_model_from_file
2018-11-05T23:12:23.122278644Z	    return train_model(params, serialization_dir, file_friendly_logging, recover, force)
2018-11-05T23:12:23.12228115Z	  File ""/usr/local/lib/python3.6/site-packages/allennlp/commands/train.py"", line 351, in train_model
2018-11-05T23:12:23.122283984Z	    metrics = trainer.train()
2018-11-05T23:12:23.12228644Z	  File ""/usr/local/lib/python3.6/site-packages/allennlp/training/trainer.py"", line 757, in train
2018-11-05T23:12:23.122289024Z	    val_loss, num_batches = self._validation_loss()
2018-11-05T23:12:23.122291564Z	  File ""/usr/local/lib/python3.6/site-packages/allennlp/training/trainer.py"", line 710, in _validation_loss
2018-11-05T23:12:23.122294204Z	    loss = self.batch_loss(batch, for_training=False)
2018-11-05T23:12:23.122296722Z	  File ""/usr/local/lib/python3.6/site-packages/allennlp/training/trainer.py"", line 430, in batch_loss
2018-11-05T23:12:23.122299384Z	    output_dict = self.model(**batch)
2018-11-05T23:12:23.122301854Z	  File ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
2018-11-05T23:12:23.122304751Z	    result = self.forward(*input, **kwargs)
2018-11-05T23:12:23.12230725Z	  File ""/usr/local/lib/python3.6/site-packages/allennlp/models/constituency_parser.py"", line 238, in forward
2018-11-05T23:12:23.122309954Z	    self._evalb_score(predicted_trees, batch_gold_trees)
2018-11-05T23:12:23.122312324Z	  File ""/usr/local/lib/python3.6/site-packages/allennlp/training/metrics/evalb_bracketing_scorer.py"", line 80, in __call__
2018-11-05T23:12:23.122314927Z	    self._evalb_program_path, compile_command))
2018-11-05T23:12:23.122317998Z	allennlp.common.checks.ConfigurationError: ""You must compile the EVALB scorer before using it. Run 'make' in the 'scripts/EVALB/evalb' directory or run: python -c 'from allennlp.training.metrics import EvalbBracketingScorer; EvalbBracketingScorer.compile_evalb()'""
```

I'm putting together a repro.",1,,,,,,,,,
692,https://github.com/allenai/allennlp/issues/2175,2175,[],closed,2018-12-12 23:44:08+00:00,,14,investigate pip-tools,"https://pypi.org/project/pip-tools/

has several features, most relevant is *automatically generating requirements.txt from setup.py*

not sure how to incorporate it into our workflow, but worth thinking about",1,,,,,,,,,
771,https://github.com/allenai/allennlp/issues/2337,2337,[],closed,2019-01-11 05:24:45+00:00,,3,Checks for files break when used with globs.,"This PR adds checks for filepaths in `train.py`: https://github.com/allenai/allennlp/pull/2308

Unfortunately, dataset readers can interpret the path string in different way. The MultiprocessDatasetReader, in particular, expects a glob.

We should revert PR #2308 and add a test that actually trains with the MultiprocessDatasetReader.

Trace:
```
Traceback (most recent call last):
  File ""/home/vidurj/miniconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/vidurj/miniconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/vidurj/miniconda3/lib/python3.6/site-packages/allennlp/run.py"", line 18, in <module>
    main(prog=""allennlp"")
  File ""/home/vidurj/miniconda3/lib/python3.6/site-packages/allennlp/commands/__init__.py"", line 101, in main
    args.func(args)
  File ""/home/vidurj/miniconda3/lib/python3.6/site-packages/allennlp/commands/train.py"", line 111, in train_model_from_args
    args.force)
  File ""/home/vidurj/miniconda3/lib/python3.6/site-packages/allennlp/commands/train.py"", line 144, in train_model_from_file
    return train_model(params, serialization_dir, file_friendly_logging, recover, force)
  File ""/home/vidurj/miniconda3/lib/python3.6/site-packages/allennlp/commands/train.py"", line 298, in train_model
    all_datasets = datasets_from_params(params)
  File ""/home/vidurj/miniconda3/lib/python3.6/site-packages/allennlp/commands/train.py"", line 154, in datasets_from_params
    check_for_data_path(data_path, data_name)
  File ""/home/vidurj/miniconda3/lib/python3.6/site-packages/allennlp/common/checks.py"", line 75, in check_for_data_path
    data_path = cached_path(data_path) 
  File ""/home/vidurj/miniconda3/lib/python3.6/site-packages/allennlp/common/file_utils.py"", line 104, in cached_path
    raise FileNotFoundError(""file {} not found"".format(url_or_filename))
FileNotFoundError: file billion/training-monolingual.tokenized.shuffled/* not found
```",1,,,,,,,,,
839,https://github.com/allenai/allennlp/issues/2473,2473,[],closed,2019-02-01 14:57:21+00:00,,32,Installing allennlp through pip using conda virtual environment fails,"**Describe the bug**

I am trying to install allennlp through pip under conda virtual environment, however it fails and leave error message like this:

```
Building wheels for collected packages: overrides, jsonnet, nltk, parsimonious, numpydoc, msgpack, regex, ujson, dill, jsondiff, PyYAML, wrapt, cytoolz, future, toolz
  Running setup.py bdist_wheel for overrides ... done
  Stored in directory: /home/ichn/.cache/pip/wheels/f7/27/b8/b4f46c59426a11e7f2d4e472b870ec14c21b4beab2e1afa725
  Running setup.py bdist_wheel for jsonnet ... error
  Complete output from command /home/ichn/anaconda3/envs/torch/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-grk1qblh/jsonnet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /tmp/pip-wheel-t66ososi --python-tag cp37:
  running bdist_wheel
  running build
  running build_ext
  g++ -c -g -O3 -Wall -Wextra -Woverloaded-virtual -pedantic -std=c++0x -fPIC -Iinclude -Ithird_party/md5 core/desugarer.cpp -o core/desugarer.o
  core/desugarer.cpp: In member function 鈥榲oid Desugarer::desugar(AST*&, unsigned int)鈥?
  core/desugarer.cpp:612:51: warning: this statement may fall through [-Wimplicit-fallthrough=]
                   case BOP_MANIFEST_UNEQUAL: invert = true;
                                              ~~~~~~~^~~~~~
  core/desugarer.cpp:613:17: note: here
                   case BOP_MANIFEST_EQUAL: {
                   ^~~~
  g++ -c -g -O3 -Wall -Wextra -Woverloaded-virtual -pedantic -std=c++0x -fPIC -Iinclude -Ithird_party/md5 core/formatter.cpp -o core/formatter.o
  g++ -c -g -O3 -Wall -Wextra -Woverloaded-virtual -pedantic -std=c++0x -fPIC -Iinclude -Ithird_party/md5 core/libjsonnet.cpp -o core/libjsonnet.o
  g++ -c -g -O3 -Wall -Wextra -Woverloaded-virtual -pedantic -std=c++0x -fPIC -Iinclude -Ithird_party/md5 core/lexer.cpp -o core/lexer.o
  g++ -c -g -O3 -Wall -Wextra -Woverloaded-virtual -pedantic -std=c++0x -fPIC -Iinclude -Ithird_party/md5 core/parser.cpp -o core/parser.o
  g++ -c -g -O3 -Wall -Wextra -Woverloaded-virtual -pedantic -std=c++0x -fPIC -Iinclude -Ithird_party/md5 core/pass.cpp -o core/pass.o
  g++ -c -g -O3 -Wall -Wextra -Woverloaded-virtual -pedantic -std=c++0x -fPIC -Iinclude -Ithird_party/md5 core/static_analysis.cpp -o core/static_analysis.o
  g++ -c -g -O3 -Wall -Wextra -Woverloaded-virtual -pedantic -std=c++0x -fPIC -Iinclude -Ithird_party/md5 core/string_utils.cpp -o core/string_utils.o
  g++ -c -g -O3 -Wall -Wextra -Woverloaded-virtual -pedantic -std=c++0x -fPIC -Iinclude -Ithird_party/md5 core/vm.cpp -o core/vm.o
  g++ -c -g -O3 -Wall -Wextra -Woverloaded-virtual -pedantic -std=c++0x -fPIC -Iinclude -Ithird_party/md5 third_party/md5/md5.cpp -o third_party/md5/md5.o
  building '_jsonnet' extension
  creating build
  creating build/temp.linux-x86_64-3.7
  creating build/temp.linux-x86_64-3.7/python
  gcc -pthread -B /home/ichn/anaconda3/envs/torch/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Iinclude -Ithird_party/md5 -I/home/ichn/anaconda3/envs/torch/include/python3.7m -c python/_jsonnet.c -o build/temp.linux-x86_64-3.7/python/_jsonnet.o
  python/_jsonnet.c: In function 鈥榗python_native_callback鈥?
  python/_jsonnet.c:147:19: warning: comparison of integer expressions of different signedness: 鈥榠nt鈥?and 鈥榮ize_t鈥?{aka 鈥榗onst long unsigned int鈥檥 [-Wsign-compare]
       for (i = 0; i < ctx->argc; ++i) {
                     ^
  creating build/lib.linux-x86_64-3.7
  g++ -pthread -shared -B /home/ichn/anaconda3/envs/torch/compiler_compat -L/home/ichn/anaconda3/envs/torch/lib -Wl,-rpath=/home/ichn/anaconda3/envs/torch/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/python/_jsonnet.o core/desugarer.o core/formatter.o core/libjsonnet.o core/lexer.o core/parser.o core/pass.o core/static_analysis.o core/string_utils.o core/vm.o third_party/md5/md5.o -o build/lib.linux-x86_64-3.7/_jsonnet.cpython-37m-x86_64-linux-gnu.so
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/python/_jsonnet.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/python/_jsonnet.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/python/_jsonnet.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/python/_jsonnet.o: unable to initialize decompress status for section .debug_info
  build/temp.linux-x86_64-3.7/python/_jsonnet.o: file not recognized: file format not recognized
  collect2: error: ld returned 1 exit status
  error: command 'g++' failed with exit status 1
  
  ----------------------------------------
  Failed building wheel for jsonnet
  Running setup.py clean for jsonnet
  Running setup.py bdist_wheel for nltk ... done
  Stored in directory: /home/ichn/.cache/pip/wheels/f1/98/72/c2ba4734bc46df30b9c3bd3eb037c52ab8ae0110f8fa15200a
  Running setup.py bdist_wheel for parsimonious ... done
  Stored in directory: /home/ichn/.cache/pip/wheels/f1/a4/4b/7cac60fa74b7c16017cd9c67ab65736d3d9318064ae65e0ee0
  Running setup.py bdist_wheel for numpydoc ... done
  Stored in directory: /home/ichn/.cache/pip/wheels/11/76/d4/16c19c2378616c3389916bc6d7b1134b72bfe6f7abd9f80243
  Running setup.py bdist_wheel for msgpack ... done
  Stored in directory: /home/ichn/.cache/pip/wheels/3f/78/5a/92a8797deabe61189baf597a855e9529f6b20a391d9924d968
  Running setup.py bdist_wheel for regex ... error
  Complete output from command /home/ichn/anaconda3/envs/torch/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-grk1qblh/regex/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /tmp/pip-wheel-6v75m_hn --python-tag cp37:
  /home/ichn/anaconda3/envs/torch/lib/python3.7/site-packages/setuptools/dist.py:470: UserWarning: Normalizing '2018.01.10' to '2018.1.10'
    normalized_version,
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.7
  copying regex_3/regex.py -> build/lib.linux-x86_64-3.7
  copying regex_3/_regex_core.py -> build/lib.linux-x86_64-3.7
  copying regex_3/test_regex.py -> build/lib.linux-x86_64-3.7
  running build_ext
  building '_regex' extension
  creating build/temp.linux-x86_64-3.7
  creating build/temp.linux-x86_64-3.7/regex_3
  gcc -pthread -B /home/ichn/anaconda3/envs/torch/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/ichn/anaconda3/envs/torch/include/python3.7m -c regex_3/_regex.c -o build/temp.linux-x86_64-3.7/regex_3/_regex.o
  gcc -pthread -B /home/ichn/anaconda3/envs/torch/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/ichn/anaconda3/envs/torch/include/python3.7m -c regex_3/_regex_unicode.c -o build/temp.linux-x86_64-3.7/regex_3/_regex_unicode.o
  gcc -pthread -shared -B /home/ichn/anaconda3/envs/torch/compiler_compat -L/home/ichn/anaconda3/envs/torch/lib -Wl,-rpath=/home/ichn/anaconda3/envs/torch/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/regex_3/_regex.o build/temp.linux-x86_64-3.7/regex_3/_regex_unicode.o -o build/lib.linux-x86_64-3.7/_regex.cpython-37m-x86_64-linux-gnu.so
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/regex_3/_regex.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/regex_3/_regex.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/regex_3/_regex.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/regex_3/_regex.o: unable to initialize decompress status for section .debug_info
  build/temp.linux-x86_64-3.7/regex_3/_regex.o: file not recognized: file format not recognized
  collect2: error: ld returned 1 exit status
  error: command 'gcc' failed with exit status 1
  
  ----------------------------------------
  Failed building wheel for regex
  Running setup.py clean for regex
  Running setup.py bdist_wheel for ujson ... error
  Complete output from command /home/ichn/anaconda3/envs/torch/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-grk1qblh/ujson/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /tmp/pip-wheel-97buh6vk --python-tag cp37:
  Warning: 'classifiers' should be a list, got type 'filter'
  running bdist_wheel
  running build
  running build_ext
  building 'ujson' extension
  creating build
  creating build/temp.linux-x86_64-3.7
  creating build/temp.linux-x86_64-3.7/python
  creating build/temp.linux-x86_64-3.7/lib
  gcc -pthread -B /home/ichn/anaconda3/envs/torch/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I./python -I./lib -I/home/ichn/anaconda3/envs/torch/include/python3.7m -c ./python/ujson.c -o build/temp.linux-x86_64-3.7/./python/ujson.o -D_GNU_SOURCE
  gcc -pthread -B /home/ichn/anaconda3/envs/torch/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I./python -I./lib -I/home/ichn/anaconda3/envs/torch/include/python3.7m -c ./python/objToJSON.c -o build/temp.linux-x86_64-3.7/./python/objToJSON.o -D_GNU_SOURCE
  ./python/objToJSON.c: In function 鈥楶yUnicodeToUTF8鈥?
  ./python/objToJSON.c:154:18: warning: initialization discards 鈥榗onst鈥?qualifier from pointer target type [-Wdiscarded-qualifiers]
       char *data = PyUnicode_AsUTF8AndSize(obj, &len);
                    ^~~~~~~~~~~~~~~~~~~~~~~
  gcc -pthread -B /home/ichn/anaconda3/envs/torch/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I./python -I./lib -I/home/ichn/anaconda3/envs/torch/include/python3.7m -c ./python/JSONtoObj.c -o build/temp.linux-x86_64-3.7/./python/JSONtoObj.o -D_GNU_SOURCE
  gcc -pthread -B /home/ichn/anaconda3/envs/torch/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I./python -I./lib -I/home/ichn/anaconda3/envs/torch/include/python3.7m -c ./lib/ultrajsonenc.c -o build/temp.linux-x86_64-3.7/./lib/ultrajsonenc.o -D_GNU_SOURCE
  ./lib/ultrajsonenc.c:156:23: warning: 鈥榞_hexChars鈥?is static but used in inline function 鈥楤uffer_AppendShortHexUnchecked鈥?which is not static
     *(outputOffset++) = g_hexChars[(value & 0x000f) >> 0];
                         ^~~~~~~~~~
  ./lib/ultrajsonenc.c:155:23: warning: 鈥榞_hexChars鈥?is static but used in inline function 鈥楤uffer_AppendShortHexUnchecked鈥?which is not static
     *(outputOffset++) = g_hexChars[(value & 0x00f0) >> 4];
                         ^~~~~~~~~~
  ./lib/ultrajsonenc.c:154:23: warning: 鈥榞_hexChars鈥?is static but used in inline function 鈥楤uffer_AppendShortHexUnchecked鈥?which is not static
     *(outputOffset++) = g_hexChars[(value & 0x0f00) >> 8];
                         ^~~~~~~~~~
  ./lib/ultrajsonenc.c:153:23: warning: 鈥榞_hexChars鈥?is static but used in inline function 鈥楤uffer_AppendShortHexUnchecked鈥?which is not static
     *(outputOffset++) = g_hexChars[(value & 0xf000) >> 12];
                         ^~~~~~~~~~
  gcc -pthread -B /home/ichn/anaconda3/envs/torch/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I./python -I./lib -I/home/ichn/anaconda3/envs/torch/include/python3.7m -c ./lib/ultrajsondec.c -o build/temp.linux-x86_64-3.7/./lib/ultrajsondec.o -D_GNU_SOURCE
  creating build/lib.linux-x86_64-3.7
  gcc -pthread -shared -B /home/ichn/anaconda3/envs/torch/compiler_compat -L/home/ichn/anaconda3/envs/torch/lib -Wl,-rpath=/home/ichn/anaconda3/envs/torch/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/./python/ujson.o build/temp.linux-x86_64-3.7/./python/objToJSON.o build/temp.linux-x86_64-3.7/./python/JSONtoObj.o build/temp.linux-x86_64-3.7/./lib/ultrajsonenc.o build/temp.linux-x86_64-3.7/./lib/ultrajsondec.o -o build/lib.linux-x86_64-3.7/ujson.cpython-37m-x86_64-linux-gnu.so
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/./python/ujson.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/./python/ujson.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/./python/ujson.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/./python/ujson.o: unable to initialize decompress status for section .debug_info
  build/temp.linux-x86_64-3.7/./python/ujson.o: file not recognized: file format not recognized
  collect2: error: ld returned 1 exit status
  error: command 'gcc' failed with exit status 1
  
  ----------------------------------------
  Failed building wheel for ujson
  Running setup.py clean for ujson
  Running setup.py bdist_wheel for dill ... done
  Stored in directory: /home/ichn/.cache/pip/wheels/f6/d1/a7/c90dbb9c5613295c70d96d60e78c3e2b283143fddbbd57e14d
  Running setup.py bdist_wheel for jsondiff ... done
  Stored in directory: /home/ichn/.cache/pip/wheels/8f/c9/36/f9e8aea16af567ce91abbe6b8b6b650877b9e17ce8aa97fb42
  Running setup.py bdist_wheel for PyYAML ... done
  Stored in directory: /home/ichn/.cache/pip/wheels/11/c5/f8/4e054145468ca00fd2ab4a6c20bf7e09ec57b879572c865ee6
  Running setup.py bdist_wheel for wrapt ... done
  Stored in directory: /home/ichn/.cache/pip/wheels/10/a6/59/eab55ff1e60d10ca0404baf6e7b8baf52908091133608bf289
  Running setup.py bdist_wheel for cytoolz ... error
  Complete output from command /home/ichn/anaconda3/envs/torch/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-grk1qblh/cytoolz/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /tmp/pip-wheel-ip_zgny_ --python-tag cp37:
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.7
  creating build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/_signatures.py -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/__init__.py -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/utils_test.py -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/_version.py -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/compatibility.py -> build/lib.linux-x86_64-3.7/cytoolz
  creating build/lib.linux-x86_64-3.7/cytoolz/curried
  copying cytoolz/curried/operator.py -> build/lib.linux-x86_64-3.7/cytoolz/curried
  copying cytoolz/curried/__init__.py -> build/lib.linux-x86_64-3.7/cytoolz/curried
  copying cytoolz/curried/exceptions.py -> build/lib.linux-x86_64-3.7/cytoolz/curried
  copying cytoolz/dicttoolz.pyx -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/itertoolz.pyx -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/utils.pyx -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/recipes.pyx -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/functoolz.pyx -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/dicttoolz.pxd -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/__init__.pxd -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/recipes.pxd -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/utils.pxd -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/functoolz.pxd -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/itertoolz.pxd -> build/lib.linux-x86_64-3.7/cytoolz
  copying cytoolz/cpython.pxd -> build/lib.linux-x86_64-3.7/cytoolz
  creating build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_none_safe.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_recipes.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_curried.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_tlz.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_itertoolz.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_functoolz.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/dev_skip_test.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_embedded_sigs.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_utils.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_docstrings.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_inspect_args.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_doctests.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_curried_toolzlike.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_serialization.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_compatibility.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_signatures.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_dev_skip_test.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  copying cytoolz/tests/test_dicttoolz.py -> build/lib.linux-x86_64-3.7/cytoolz/tests
  running build_ext
  building 'cytoolz.dicttoolz' extension
  creating build/temp.linux-x86_64-3.7
  creating build/temp.linux-x86_64-3.7/cytoolz
  gcc -pthread -B /home/ichn/anaconda3/envs/torch/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/ichn/anaconda3/envs/torch/include/python3.7m -c cytoolz/dicttoolz.c -o build/temp.linux-x86_64-3.7/cytoolz/dicttoolz.o
  gcc -pthread -shared -B /home/ichn/anaconda3/envs/torch/compiler_compat -L/home/ichn/anaconda3/envs/torch/lib -Wl,-rpath=/home/ichn/anaconda3/envs/torch/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/cytoolz/dicttoolz.o -o build/lib.linux-x86_64-3.7/cytoolz/dicttoolz.cpython-37m-x86_64-linux-gnu.so
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/cytoolz/dicttoolz.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/cytoolz/dicttoolz.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/cytoolz/dicttoolz.o: unable to initialize decompress status for section .debug_info
  /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/cytoolz/dicttoolz.o: unable to initialize decompress status for section .debug_info
  build/temp.linux-x86_64-3.7/cytoolz/dicttoolz.o: file not recognized: file format not recognized
  collect2: error: ld returned 1 exit status
  error: command 'gcc' failed with exit status 1
  
  ----------------------------------------
  Failed building wheel for cytoolz
  Running setup.py clean for cytoolz
  Running setup.py bdist_wheel for future ... done
  Stored in directory: /home/ichn/.cache/pip/wheels/3f/66/fe/9c4fd5c707a9f26993ba157f0752d84d5c7e26aedbefb84f76
  Running setup.py bdist_wheel for toolz ... done
  Stored in directory: /home/ichn/.cache/pip/wheels/73/ad/e1/f8fe78eeb9e2b31ea8396419d92adc107c553ff7eb47ad12d9
Successfully built overrides nltk parsimonious numpydoc msgpack dill jsondiff PyYAML wrapt future toolz
Failed to build jsonnet regex ujson cytoolz
Installing collected packages: overrides, jsonnet, wcwidth, ftfy, singledispatch, nltk, regex, murmurhash, ujson, cymem, dill, idna, chardet, urllib3, requests, msgpack, msgpack-numpy, tqdm, wrapt, preshed, toolz, cytoolz, plac, thinc, spacy, sqlparse, itsdangerous, click, werkzeug, MarkupSafe, Jinja2, flask, flask-cors, editdistance, flaky, cycler, pytz, kiwisolver, python-dateutil, pyparsing, matplotlib, greenlet, gevent, atomicwrites, py, pluggy, attrs, more-itertools, pytest, responses, jsonpickle, aws-xray-sdk, cookies, xmltodict, pbr, mock, jsondiff, jmespath, docutils, botocore, s3transfer, boto3, PyYAML, pyaml, boto, websocket-client, docker-pycreds, docker, asn1crypto, cryptography, future, ecdsa, pycryptodome, python-jose, moto, parsimonious, Pygments, alabaster, sphinxcontrib-websupport, imagesize, snowballstemmer, babel, packaging, sphinx, numpydoc, protobuf, tensorboardX, h5py, conllu, scipy, scikit-learn, unidecode, pytorch-pretrained-bert, colorama, pyasn1, rsa, awscli, allennlp
  Running setup.py install for jsonnet ... error
    Complete output from command /home/ichn/anaconda3/envs/torch/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-grk1qblh/jsonnet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-3ccckdjt/install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_ext
    make: 'core/desugarer.o' is up to date.
    make: 'core/formatter.o' is up to date.
    make: 'core/libjsonnet.o' is up to date.
    make: 'core/lexer.o' is up to date.
    make: 'core/parser.o' is up to date.
    make: 'core/pass.o' is up to date.
    make: 'core/static_analysis.o' is up to date.
    make: 'core/string_utils.o' is up to date.
    make: 'core/vm.o' is up to date.
    make: 'third_party/md5/md5.o' is up to date.
    building '_jsonnet' extension
    creating build
    creating build/temp.linux-x86_64-3.7
    creating build/temp.linux-x86_64-3.7/python
    gcc -pthread -B /home/ichn/anaconda3/envs/torch/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Iinclude -Ithird_party/md5 -I/home/ichn/anaconda3/envs/torch/include/python3.7m -c python/_jsonnet.c -o build/temp.linux-x86_64-3.7/python/_jsonnet.o
    python/_jsonnet.c: In function 鈥榗python_native_callback鈥?
    python/_jsonnet.c:147:19: warning: comparison of integer expressions of different signedness: 鈥榠nt鈥?and 鈥榮ize_t鈥?{aka 鈥榗onst long unsigned int鈥檥 [-Wsign-compare]
         for (i = 0; i < ctx->argc; ++i) {
                       ^
    creating build/lib.linux-x86_64-3.7
    g++ -pthread -shared -B /home/ichn/anaconda3/envs/torch/compiler_compat -L/home/ichn/anaconda3/envs/torch/lib -Wl,-rpath=/home/ichn/anaconda3/envs/torch/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/python/_jsonnet.o core/desugarer.o core/formatter.o core/libjsonnet.o core/lexer.o core/parser.o core/pass.o core/static_analysis.o core/string_utils.o core/vm.o third_party/md5/md5.o -o build/lib.linux-x86_64-3.7/_jsonnet.cpython-37m-x86_64-linux-gnu.so
    /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/python/_jsonnet.o: unable to initialize decompress status for section .debug_info
    /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/python/_jsonnet.o: unable to initialize decompress status for section .debug_info
    /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/python/_jsonnet.o: unable to initialize decompress status for section .debug_info
    /home/ichn/anaconda3/envs/torch/compiler_compat/ld: build/temp.linux-x86_64-3.7/python/_jsonnet.o: unable to initialize decompress status for section .debug_info
    build/temp.linux-x86_64-3.7/python/_jsonnet.o: file not recognized: file format not recognized
    collect2: error: ld returned 1 exit status
    error: command 'g++' failed with exit status 1
    
    ----------------------------------------
Command ""/home/ichn/anaconda3/envs/torch/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-grk1qblh/jsonnet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-3ccckdjt/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-install-grk1qblh/jsonnet/
```

**To Reproduce**

I am using conda 5.3.1 under archlinux with pytorch==1.0.0 preinstalled as part of the environment, and gcc of version

```
(torch) 鉃? ~ g++ --version
g++ (GCC) 8.2.1 20181127
Copyright (C) 2018 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

also tried install gcc through conda and then rerun

`pip install allennlp`

but fails as well.

**Expected behavior**

allennlp successfully installed


**System (please complete the following information):**
 - Linux
 - Python version: 3.7.2
 - AllenNLP version: I installed from master
 - PyTorch version: 1.0.0

",1,,,,,,,,,
861,https://github.com/allenai/allennlp/issues/2514,2514,[],closed,2019-02-14 06:06:38+00:00,,2,BrokenPipeError while training bidirectional language models,"**Describe the bug**
We often observe a `BrokePipeError` while training transformer bidirectional models using the standard AllenNLP settings for the bidirectional LM transformer. This occurs at the end of an epoch. Here is a copy of the log. The program continues running.
```
[INFO/Process-20:1] process shutting down
[INFO/Process-20:1] process exiting with exitcode 0
[INFO/Process-20] worker 0 finished (1/1)
[INFO/Process-20] process shutting down
[INFO/Process-20] process exiting with exitcode 0
[INFO/Process-21] created temp directory /tmp/pymp-uhoy6pc2
loss: 6.6689 ||: 100%|##########| 1/1 [00:55<00:00, 55.63s/it]
[INFO/Process-21] process shutting down
[INFO/Process-21] process exiting with exitcode 0
loss: 6.6664 ||: : 13it [01:06, 39.20s/it]
loss: 6.6438 ||: : 25it [01:16, 27.71s/it]
loss: 6.6302 ||: : 37it [01:27, 19.66s/it]
loss: 6.6168 ||: : 49it [01:38, 14.03s/it]
loss: 6.6033 ||: : 61it [01:48, 10.09s/it]
loss: 6.5903 ||: : 73it [01:59, 7.32s/it]
loss: 6.5774 ||: : 85it [02:09, 5.39s/it]
[INFO/MainProcess] worker 0 finished (1 / 1)
[INFO/MainProcess] sending shutdown message to manager
[INFO/SyncManager-19] process shutting down
[INFO/SyncManager-19] Failure to send message: ('#RETURN', None)
[INFO/SyncManager-19] ... request was (None, 'shutdown', (), {})
[INFO/SyncManager-19] ... exception was BrokenPipeError(32, 'Broken pipe')
[INFO/SyncManager-19] process exiting with exitcode 0
loss: 6.5672 ||: : 96it [02:19, 1.45s/it]

2019-01-17 10:21:41,718 - INFO - allennlp.training.trainer - Validating
0%| | 0/1 [00:00<?, ?it/s]
```

**System (please complete the following information):**
 - OS: CentOS Linux 7
 - Python version: 3.6.6
 - AllenNLP version: Installed from master on 14th January, 2019
 - PyTorch version: 1.0.0
",1,,,,,,,,,
1115,https://github.com/allenai/allennlp/issues/2937,2937,[],closed,2019-06-10 17:40:47+00:00,,4,Allennlp in production - Exporting the pipeline to torch script,"@joelgrus  You replied on a thread I was following in hacker news about exploring production features of pytorch 1.0.

> AllenNLP dev here. We're going to do a ""PyTorch 1.0"" release of AllenNLP next week, and then after that we're planning to investigate how to incorporate the new ""production"" aspects.
https://news.ycombinator.com/item?id=18683758

Could we change most of the modules in the pipeline to be written in subset of python that torch jit script supports? 

Or do you have anything on your mind on how to do this ?",1,,,,,,,,,
1308,https://github.com/allenai/allennlp/issues/3291,3291,[],closed,2019-09-27 07:21:14+00:00,,3,`allennlp: command not found` After installing via pip.,"**Describe the bug**
After installing allennlp via pip, I try to use it and I get `allennlp: command not found` .

**To Reproduce**
Python3.7
```
 $ pip install allennlp
```
Steps to reproduce the behavior
```
$ allennlp -V
allennlp: command not found
```",1,,,,,,,,,
1354,https://github.com/allenai/allennlp/issues/3373,3373,[],closed,2019-10-17 22:26:25+00:00,,12,commit #3308 breaks pip install --editable,"**Describe the bug**
After PR #3308 `pip install --editable` does not work, while it works before that commit. The following error is returned:
```
ERROR: Command errored out with exit status 1:
   command: /venvs/dev/bin/python3.7 /venvs/dev/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /tmp/tmpvccnto17
       cwd: /code/allennlp
  Complete output (4 lines):
  Traceback (most recent call last):
    File ""/venvs/dev/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py"", line 15, in <module>
      from glob import glob
  ModuleNotFoundError: No module named 'glob'
  ----------------------------------------
ERROR: Command errored out with exit status 1: /venvs/dev/bin/python3.7 /venvs/dev/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py get_requires_for_build_whee
l /tmp/tmpvccnto17 Check the logs for full command output.
```
**To Reproduce**
Steps to reproduce the behavior
```
# PR #3308 
git checkout 3dda5ac9
# fails:
pip install --editable .
# prior commit  
git checkout d7b38a89  
# works:
pip install --editable .

```
**Expected behavior**
installation should work

**System (please complete the following information):**
 - OS: Ubuntu 16.04
 - Python version: 3.7
 - AllenNLP version: I installed from master
 - PyTorch version: 1.3

**Additional context**
Running in a virtualenv in a docker container, built from `nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04`
",1,,,,,,,,,
1428,https://github.com/allenai/allennlp/issues/3492,3492,[],closed,2019-11-28 17:13:23+00:00,,2,Install via pip misses files from data.tokenizers,"**Describe the bug**
I installed the 0.9.0 and 0.8.5 via pip install and I could not find the tokenizers
I managed it by installing from git manual.

**To Reproduce**
```pip install allennlp```

**Expected behavior**
At least spacy tokenizer in `allennlp/data/tokenizers`, shouln't it be there?

**System (please complete the following information):**
 - OS: Linux, Debian
 - Python version: 3.7.5rc
 - AllenNLP version: [e.g. v0.9.0, or ""I installed from master""]
 - PyTorch version: 1.0.2

**Additional context**
Add any other context about the problem here.
",1,,,,,,,,,
1448,https://github.com/allenai/allennlp/issues/3520,3520,[],closed,2019-12-13 21:43:30+00:00,,4,pip allennlp,"Hello, 

The pip version is causing conflicts, any pip updated version soon?  it causes conflicts with spacy v2.2.  I have been installing allennlp from source. 
",1,,,,,,,,,
1490,https://github.com/allenai/allennlp/issues/3610,3610,[],closed,2020-01-14 03:53:10+00:00,,2,"Remove reference to MODELS.md in README (and replace it with the right pointer, whatever that is)","https://github.com/allenai/allennlp/blob/master/MODELS.md returns a 404 and https://allenai.github.io/allennlp-docs/MODELS.md is also broken (see below)

Is there a working link somewhere?

![image](https://user-images.githubusercontent.com/1892071/72312852-44e5bb00-363e-11ea-8ced-ed4220c69e88.png)
",1,,,,,,,,,
1540,https://github.com/allenai/allennlp/issues/3757,3757,[],closed,2020-02-10 19:06:44+00:00,,13,Update models in the demo,"This isn't really an allennlp issue, but I'm putting it here so that we can track it in our 1.0 milestone.  We should update our demo to include the new models that we're building.  Specifically, at least:

- [ ] [TransformerQA](https://github.com/allenai/allennlp-reading-comprehension/pull/19)
- [x] Update coref models
- [x] New DROP NMN model
- [x] NLI models

We could also consider updating some of our ""annotate a sentence"" models, though those seem like lower-priority items to me.",1,,,,,,,,,
