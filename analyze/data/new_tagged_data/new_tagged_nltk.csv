,html_url,number,labels,state,created_at,pull_request,comments,title,body,rel1,rel2,rel3,rel4,rel5,rel6,rel7,rel8,rel9,rel10
806,https://github.com/nltk/nltk/issues/2682,2682,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2021-03-30 12:09:44+00:00,,1,Segfault with for fresh ubuntu 20.04 install using conda,"The python interpreter segfaults when running in a miniconda environment on a fresh install of ubuntu 20.04.2. This seems to happen intermittently, both while running ""pip"" during the conda setup of an environment and during the execution of code like below. 

The segfault always occurs when running the following code, which reads texts from files and tokenizes the result. The segfault location changes from run to run. Also the exact same code can run on another computer with the same conda environment on a ubuntu 18.04.

The core dumps always points to some function in the unicodeobject.c file in python but the exact function changes from crash to crash. At least one crash has a clear dereferenced pointer 0x0 where the ""unicode object"" should be.

My guess is that something causes the python interpreter to throw away the pointed to unicode object while it is still being worked on causing a segfault. But any bug in the interpreter or NLTK should have been noticed by more users, and I cannot find anyone with similar issues. This issue was also reported to the python development team which replied that the usage of the c-API is used in an incorrect way https://bugs.python.org/issue43668

Things tried that didn't fix the issue:
1. Reformatting and reinstalling ubuntu
2. Switched to ubuntu 18.04 (on this computer, another computer with 18.04 can run the code just fine)
3. Replacing hardware, to ensure that RAM, or SSD disk isn't broken
4. Changing to python versions 3.8.6, 3.8.8, 3.9.2
5. Cloning the conda environment from a working computer to the broken one

Attached is one stacktrace of the fault handler along with it's corresponding core dump stack trace from gdb.

```
(eo) axel@minimind:~/test$ python tokenizer_mini.py 
2021-03-30 11:10:15.588399: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-03-30 11:10:15.588426: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Fatal Python error: Segmentation fault

Current thread 0x00007faa73bbe740 (most recent call first):
  File ""tokenizer_mini.py"", line 36 in preprocess_string
  File ""tokenizer_mini.py"", line 51 in <module>
Segmentation fault (core dumped)
```

```
#0  raise (sig=<optimized out>) at ../sysdeps/unix/sysv/linux/raise.c:50
#1  <signal handler called>
#2  find_maxchar_surrogates (num_surrogates=<synthetic pointer>, maxchar=<synthetic pointer>, 
    end=0x4 <error: Cannot access memory at address 0x4>, begin=0x0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/unicodeobject.c:1703
#3  _PyUnicode_Ready (unicode=0x7f7e4e04d7f0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/unicodeobject.c:1742
#4  0x000055cd65f6df6a in PyUnicode_RichCompare (left=0x7f7e4cf43fb0, right=<optimized out>, op=2)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/unicodeobject.c:11205
#5  0x000055cd6601712a in do_richcompare (op=2, w=0x7f7e4e04d7f0, v=0x7f7e4cf43fb0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/object.c:726
#6  PyObject_RichCompare (op=2, w=0x7f7e4e04d7f0, v=0x7f7e4cf43fb0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/object.c:774
#7  PyObject_RichCompareBool (op=2, w=0x7f7e4e04d7f0, v=0x7f7e4cf43fb0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/object.c:796
#8  list_contains (a=0x7f7e4e04b4c0, el=0x7f7e4cf43fb0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/listobject.c:455
#9  0x000055cd660be41b in PySequence_Contains (ob=0x7f7e4cf43fb0, seq=0x7f7e4e04b4c0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/abstract.c:2083
#10 cmp_outcome (w=0x7f7e4e04b4c0, v=0x7f7e4cf43fb0, op=<optimized out>, tstate=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:5082
#11 _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:2977
#12 0x000055cd6609f706 in PyEval_EvalFrameEx (throwflag=0, f=0x7f7e4f4d3c40)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:738
#13 function_code_fastcall (globals=<optimized out>, nargs=<optimized out>, args=<optimized out>, co=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/call.c:284
#14 _PyFunction_Vectorcall (func=<optimized out>, stack=<optimized out>, nargsf=<optimized out>, kwnames=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/call.c:411
#15 0x000055cd660be54f in _PyObject_Vectorcall (kwnames=0x0, nargsf=<optimized out>, args=0x7f7f391985b8, callable=0x7f7f39084160)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Include/cpython/abstract.h:115
#16 call_function (kwnames=0x0, oparg=<optimized out>, pp_stack=<synthetic pointer>, tstate=0x55cd66c2e880)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:4963
#17 _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:3500
#18 0x000055cd6609e503 in PyEval_EvalFrameEx (throwflag=0, f=0x7f7f39198440)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:4298
#19 _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, 
    argcount=<optimized out>, kwnames=<optimized out>, kwargs=<optimized out>, kwcount=<optimized out>, kwstep=<optimized out>, 
    defs=<optimized out>, defcount=<optimized out>, kwdefs=<optimized out>, closure=<optimized out>, name=<optimized out>, 
    qualname=<optimized out>) at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:4298
#20 0x000055cd6609f559 in PyEval_EvalCodeEx (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, 
    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:4327
#21 0x000055cd661429ab in PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:718
#22 0x000055cd66142a43 in run_eval_code_obj (co=0x7f7f3910f240, globals=0x7f7f391fad80, locals=0x7f7f391fad80)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/pythonrun.c:1165
#23 0x000055cd6615c6b3 in run_mod (mod=<optimized out>, filename=<optimized out>, globals=0x7f7f391fad80, locals=0x7f7f391fad80, 
    flags=<optimized out>, arena=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/pythonrun.c:1187
--Type <RET> for more, q to quit, c to continue without paging--
#24 0x000055cd661615b2 in pyrun_file (fp=0x55cd66c2cdf0, filename=0x7f7f391bbee0, start=<optimized out>, globals=0x7f7f391fad80, 
    locals=0x7f7f391fad80, closeit=1, flags=0x7ffe3ee6f8e8)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/pythonrun.c:1084
#25 0x000055cd66161792 in pyrun_simple_file (flags=0x7ffe3ee6f8e8, closeit=1, filename=0x7f7f391bbee0, fp=0x55cd66c2cdf0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/pythonrun.c:439
#26 PyRun_SimpleFileExFlags (fp=0x55cd66c2cdf0, filename=<optimized out>, closeit=1, flags=0x7ffe3ee6f8e8)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/pythonrun.c:472
#27 0x000055cd66161d0d in pymain_run_file (cf=0x7ffe3ee6f8e8, config=0x55cd66c2da70)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Modules/main.c:391
#28 pymain_run_python (exitcode=0x7ffe3ee6f8e0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Modules/main.c:616
#29 Py_RunMain () at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Modules/main.c:695
#30 0x000055cd66161ec9 in Py_BytesMain (argc=<optimized out>, argv=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Modules/main.c:1127
#31 0x00007f7f3a3620b3 in __libc_start_main (main=0x55cd65fe3490 <main>, argc=2, argv=0x7ffe3ee6fae8, init=<optimized out>, 
    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7ffe3ee6fad8) at ../csu/libc-start.c:308
#32 0x000055cd660d7369 in _start () at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ast.c:937
```

The conda environment used is below, using Miniconda3-py38_4.9.2-Linux-x86_64.sh (note that the segfault does sometimes occur during the setup of a conda environment so it's probably not related to the env)
```
name: eo
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.8.8
  - pip=20.3.1
  - pip:
    - transformers==4.3.2
    - tensorflow_gpu==2.4.0
    - scikit-learn==0.23.2
    - nltk==3.5
    - matplotlib==3.2.1
    - seaborn==0.11.0
    - tensorflow-addons==0.11.2
    - tf-models-official==2.4.0
    - gspread==3.6.0
    - oauth2client==4.1.3
    - ipykernel==5.4.2
    - autopep8==1.5.4
    - torch==1.7.1
```


The code below consistently reproduces the problem, the files read are simple text files containing unicode text:

```python
from nltk.tokenize import wordpunct_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords
import pickle
from pathlib import Path
import faulthandler
faulthandler.enable()


def load_data(root_path, feature, index):
    feature_root = root_path / feature
    dir1 = str(index // 10_000)
    base_path = feature_root / dir1 / str(index)
    full_path = base_path.with_suffix('.txt')
    data = None
    with open(full_path, 'r', encoding='utf-8') as f:
        data = f.read()
    return data


def preprocess_string(text, stemmer, stop_words):
    word_tokens = wordpunct_tokenize(text.lower())
    alpha_tokens = []
    for w in word_tokens:
        try:
            if (w.isalpha() and w not in stop_words):
                alpha_tokens.append(w)
        except:
            print(""Something went wrong when handling the word: "", w)

    clean_tokens = []
    for w in alpha_tokens:
        try:
            word = stemmer.stem(w)
            clean_tokens.append(word)
        except:
            print(""Something went wrong when stemming the word: "", w)
            clean_tokens.append(w)
    return clean_tokens


stop_words = stopwords.words('english')
stemmer = SnowballStemmer(language='english')
tokenizer = Tokenizer()

root_path = '/srv/patent/EbbaOtto/E'
for idx in range(0, 57454):
    print(f'Processed {idx}/57454', end='\r')
    desc = str(load_data(Path(root_path), 'clean_description', idx))
    desc = preprocess_string(desc, stemmer, stop_words)
    tokenizer.fit_on_texts([desc])

```",,,,,,,,1,1,
798,https://github.com/nltk/nltk/issues/2666,2666,[],open,2021-02-08 14:02:44+00:00,,1,Encountered an old issue #1387 with the latest version,"I've got the same error after doing similar stuff. (See #1387.)

```
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
G:\Anaconda3\lib\site-packages\IPython\core\formatters.py in __call__(self, obj)
    343             method = get_real_method(obj, self.print_method)
    344             if method is not None:
--> 345                 return method()
    346             return None
    347         else:

G:\Anaconda3\lib\site-packages\nltk\tree.py in _repr_png_(self)
    817                 raise LookupError
    818 
--> 819             with open(out_path, ""rb"") as sr:
    820                 res = sr.read()
    821             os.remove(in_path)

FileNotFoundError: [Errno 2] No such file or directory: 'G:\\Users\\01\\AppData\\Local\\Temp\\tmpkyszdc9g.png'
```

(Python)

```
Error: /syntaxerror in (binary token, type=155)
Operand stack:
   --nostringval--   瀵邦喛钂?Execution stack:
   %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval-
-   2   %stopped_push   --nostringval--   --nostringval--   --nostringval--   fa
lse   1   %stopped_push   1926   1   3   %oparray_pop   1925   1   3   %oparray_
pop   --nostringval--   1909   1   3   %oparray_pop   1803   1   3   %oparray_po
p   --nostringval--   %errorexec_pop   .runexec2   --nostringval--   --nostringv
al--   --nostringval--   2   %stopped_push
Dictionary stack:
   --dict:1173/1684(ro)(G)--   --dict:0/20(G)--   --dict:82/200(L)--   --dict:23
/50(L)--
Current allocation mode is local
Last OS error: No such file or directory
GPL Ghostscript 9.05: Unrecoverable error, exit code 1
```

(commmand line)

Seems the problem is that ghostscript failed - strange.

It is reproducible as https://github.com/nltk/nltk/issues/1387#issuecomment-216426399:

(in ipython notebook)

```
import nltk

nltk.tree.Tree.fromstring(""(test (this tree))"")
```

This old issue is closed and referenced by some PRs, so I thought it was fixed - or this is a problem of Jupyter Notebook?",,,,,,,1,,1,
575,https://github.com/nltk/nltk/issues/2259,2259,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",open,2019-03-30 05:35:37+00:00,,0,Stanford POS tags are missing'_' in nltk.tag.StanfordPOSTagger,"I am using the stanford-postagger-2018-10-16/stanford-postagger-3.9.2.jar. 
'_' is tagged as 'CD' by Stanford POS Tagger when running using Java jar program, and the same is getting missed when receiving in Python Nltk StanfordPOSTagger.

**Input Sentence:**  
""_ computer is made to implement simulation"" 

**Stanford Jar result:** 
__SYM computer_NN is_VBZ made_VBN to_TO implement_VB simulation_NN 

**NLTK TAG StanfordPOSTagger result:** 

[('', 'CD'), ('computer', 'NN'), ('is', 'VBZ'), ('made', 'VBN'), ('to', 'TO'), ('implement', 'VB'), ('simulation', 'NN')]


**Code Snippet:** 
`from nltk.tag import StanfordPOSTagger`
`TAGGER_MODEL = 'stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger'`
`TAGGER_JAR = 'stanford-postagger-2018-10-16/stanford-postagger-3.9.2.jar' `
`stanford_tagger = StanfordPOSTagger(TAGGER_MODEL,TAGGER_JAR)`
`t ='_ computer is made to implement simulation'`
`ttk = nltk.tokenize.word_tokenize(t)`
`sfttk = stanford_tagger.tag(ttk)`
`print(sfttk)`

This is similar to #1632



",,,,,1,,,,1,
559,https://github.com/nltk/nltk/issues/2235,2235,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2019-02-15 14:21:56+00:00,,0,Uniform interface for similarity measures in Wordnet,"Hello,

played around a little with NLTK and noticed that similarity measures in Wordnet behave differently when comparing similarity of different parts-of-speech - say, a noun and a verb. Some quietly return None, some throw an exception.

For example - having

noun = nltk.corpus.wordnet.synset('table.n.01')
verb = nltk.corpus.wordnet.synset('go.v.01')

and

ic = nltk.corpus.wordnet_ic.ic(""ic_brown.dat"")

the different methods to calculate similarity will behave as follows:

noun.jcn_similarity(verb, ic) # throws a WordNetError exception
noun.lch_similarity(verb) # throws a WordNetError exception
noun.lin_similarity(verb, ic) # throws a WordNetError exception
noun.path_similarity(verb) # returns None
noun.res_similarity(verb, ic) # throws a WordNetError exception
noun.wup_similarity(verb) # returns None

So, 2 out of 6 similarity measures return None, and 4 of 6 throw an exception in this case. What is the preferred behaviour for this case and why should not it be uniform for all the similarity measures?",,,,,,,,,1,
591,https://github.com/nltk/nltk/issues/2278,2278,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",closed,2019-04-24 01:29:19+00:00,,1,Wordnet similarity quirky when only one synsets needs root,"Sometimes when only one synset needs root, the similarity between the synsets are not commutative:

```python
from nltk.corpus import wordnet as nltk_wn

ncat = nltk_wn.synset('cat.n.01')
nbuy = nltk_wn.synset('buy.v.01')

print(nltk_wn.path_similarity(nbuy, ncat), nltk_wn.wup_similarity(nbuy, ncat))
print(nltk_wn.path_similarity(ncat, nbuy), nltk_wn.wup_similarity(ncat, nbuy))
```

[out]:

```
0.058823529411764705 0.1111111111111111
None None
```

Details on https://stackoverflow.com/q/20075335/610569 ",,,,,,,,,1,
680,https://github.com/nltk/nltk/issues/2451,2451,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-10-31 16:15:42+00:00,,0,Mesure Arabic Text similarity ,"I need to find the similarity between many documents that contain Arabic language plain texts.  
Example: input: text1=""丕賴賱丕 賵爻賴賱丕"", text2=""丕賴賱丕 賵爻賴賱丕"" 
                Output: Similarity = 1.0",,,,,,,,,1,
685,https://github.com/nltk/nltk/issues/2459,2459,"[{'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}]",closed,2019-11-10 16:17:21+00:00,,3,nltk.metrics.distance.jaro_similarity returns lower values than it should,"Jaro similarity is supposed to give the same results if the strings are reversed:

```
from nltk.metrics import distance as dist
a='rureiter'
b='enreiter'
print(""regular={}, reversed={}"".format(dist.jaro_similarity(a, b), dist.jaro_similarity(a[::-1], b[::-1])))
```
The code above prints `regular=0.722222222222, reversed=0.833333333333`.

In fact, manual pen-and-paper examination shows that the correct similarity metric, in both cases 
is 0.833333333333:

Separate the strings into prefix and suffix:
```
a = 'ru' + 'reiter'
b = 'en' + 'reiter'
```
The suffixes of `a` and `b` are equal. Because the suffixes are equal, and the prefixes have nothing in common between them, then the expected values are `matches == 6`  and `transpositions == 0`. With these values:

```
a = 'ru' + 'reiter'
b = 'en' + 'reiter'
matches = 6
transpositions = 0
print(
            1
            / 3
            * (
                matches / len(a)
                + matches / len(b)
                + (matches - transpositions // 2) / matches
            )
        )
```
The above code, unlike nltk, gives the correct answer of 0.8333333333333333 .

The issue lies in the fact that the current implementation does not try to minimize the number of transpositions in its algorithm, contrary to its documentation:

> The Jaro distance between is the min no. of single-character transpositions
>     required to change one word into another

The implementation simply finds the first occurrence of each character of the first string (`a`) in the second string (`b`). This order of matching does not guarantee that the match will be optimal.  In this example, the match is:
The first character of `a`  is ""r"", and is matched against the third character of `b`. From that point, the suffix ""reiter"" can't be matched in full. Worse, the next match of `a` is character ""e"" which is matched against the first character of `b`. This matching makes a transposition:

```
r u r e i t e r
 \  _/    
  \/    
  /\   
 /  | 
e n r e i t e r
```
Later, things get even worse. The last ""e"" of `a` gets matched against the middle ""e"" of `b`:
```
r u r e i t e r
 \  _/     /
  \/    __/
  /\   /
 /  | /
e n r e i t e r
```
This matching causes more transpositions, for no reason.

A correct Jaro algorithm should find the minimum value of transposition possible.

With `matches=6`, and `transpositions=4` the result is 0.722222222222 .

Note that `transpositions=4` because the list of matched indices of the second string is sorted, while the first is not. Before sorting:
```
flagged_1 = [0, 3, 4, 5, 6] 
flagged_2 = [2, 0, 4, 5, 3] 
```
After sorting:
```
flagged_1 = [0, 3, 4, 5, 6] 
flagged_2 = [0, 2, 3, 4, 5]  # 4 different entries
```
",,,,,,,,,1,
701,https://github.com/nltk/nltk/issues/2491,2491,[],open,2020-01-24 09:29:35+00:00,,3,Reading a CFG from string with escaped single and double quotes in terminals results in failure,"As mentioned in the topic title, reading a grammar string which contains productions with escaped single and/or double quotes in the terminals on the RHS results in different ValueErrors.

Example A:
The following code:
```
grammar = CFG.fromstring(""""""
            S -> A
            A -> 'manager\'s, discount'
            """""")
```
raises:
```
   raise ValueError('Expected a nonterminal, found: ' + string[pos:])
ValueError: Expected a nonterminal, found: , discount'
```

Example B:
The following code:
```
grammar = CFG.fromstring(""""""
            S -> A
            A -> ""\""manager\'s discount\""""
            """""")
```
raises:
```
    raise ValueError('Unterminated string')
ValueError: Unterminated string
```

The reason for this is as follows:
The regex pattern `_TERMINAL_RE = re.compile(r'( ""[^""]+"" | \'[^\']+\' ) \s*', re.VERBOSE)` used by the helper function `def _read_production` in https://www.nltk.org/_modules/nltk/grammar.html does not account for escaped single or double quote characters.

Proposed solution:
For the case when a terminal is enclosed with single quotes, modifying the regex to ` r'( ""[^""]+"" | \'([^\']|[\\\'])+\' ) \s*'` should enable it to account for escaped single quotes. Something similar can be done for the case with double quotes in the regex.",,,,,,,,,1,
741,https://github.com/nltk/nltk/issues/2562,2562,"[{'id': 81645781, 'node_id': 'MDU6TGFiZWw4MTY0NTc4MQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/language-model', 'name': 'language-model', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",closed,2020-06-20 19:56:18+00:00,,1,nltk.everygrams() does not work with non-reusable iterables and iterables without lengths.,"`nltk.util.everygrams()` ([code](https://github.com/nltk/nltk/blob/develop/nltk/util.py#L577-L600), [docs](https://nltk.readthedocs.io/en/latest/api/nltk.html?highlight=everygrams#nltk.util.everygrams)) claims to support both sequences and iterables. However, for iterables that are not reusable or iterables without a length function, everygrams either raises an exception or behaves incorrectly. In particular:
* When calling `everygrams(obj)` without its optional `max_len` argument, it attempts to take the length of its first argument, which may not support `len(obj)` resulting in a `TypeError`.
* When calling `everygrams(obj, max_len=2)` for example, if obj is an iterable that is not reusable it will be exhausted after only the ngrams of length `1` have been found and `everygrams()` ends up returning only the ngrams of length `1`.

I am happy to make a PR for this.

## Non-Working Examples

For example, starting with the example from the documentation
```python
>>> from nltk import everygrams
>>> sent = 'a b c'.split()
>>> list(everygrams(sent))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
>>> list(everygrams(sent, max_len=2))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c')]
```

We should expect that `everygrams(iter(sent))` should have the same output as `everygrams(sent)`. Instead, it raises a `TypeError`.
```python
>>> list(everygrams(sent))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
>>> list(everygrams(iter(sent)))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../python3.8/site-packages/nltk/util.py"", line 603, in everygrams
    max_len = len(sequence)
TypeError: object of type 'list_iterator' has no len()
```
Likewise, we should expect that `everygrams(iter(sent), max_len=2)` should have the same output as `everygrams(sent, max_len=2)`. This is not the case.
```python
>>> list(everygrams(sent, max_len=2))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c')]
>>> list(everygrams(iter(sent), max_len=2))
[('a',), ('b',), ('c',)]
```


## Possible Solutions

There are several possible solutions:
1. Do not accept iterables.
    * Since nltk is commonly used to process large amounts of text data, iterables are the most natural and practical approach to avoid loading data into memory all at once.
2. If the argument is an iterable, cache it, computing its length in the process, in case it cannot be reused.
    * This would require loading the data into memory all at once, which should be avoided.
3. Use a `history` sliding window of size `max_len` to produce the ngrams. This strategy is similar to `history` in the implementation of ngram.
    * This would be my suggested approach since it provides the most functionality. However, it would have the drawback of changing the order of the output ngrams. For example, 
      ```python
      >>> list(old_everygrams(iter(sent)))
      [('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
      >>> list(new_everygrams(iter(sent)))
      [('a',), ('a', 'b'), ('a', 'b', 'c'), ('b',), ('b', 'c'), ('c',)]
      ```

I am happy to make a PR for this.",,,,,,,,,1,
747,https://github.com/nltk/nltk/issues/2576,2576,[],closed,2020-07-30 02:39:07+00:00,,1,Connection timeout,"Got error when run `nltk.download('all')` from windows:
```
Error downloading 'framenet_v17' from
[nltk_data]    |     <https://raw.githubusercontent.com/nltk/nltk_data
[nltk_data]    |     /gh-pages/packages/corpora/framenet_v17.zip>:
[nltk_data]    |     <urlopen error [WinError 10060] A connection
[nltk_data]    |     attempt failed because the connected party did
[nltk_data]    |     not properly respond after a period of time, or
[nltk_data]    |     established connection failed because connected
[nltk_data]    |     host has failed to respond>`
```
Also got the similar error from Azure databricks as
```
<urlopen error [Errno 110] Connection timed out>
```",,,,,,,,,1,
755,https://github.com/nltk/nltk/issues/2586,2586,[],closed,2020-08-20 20:04:05+00:00,,5,"PunktTokenizer: Inconsistency in two snippets, different languages","I am trying to work with the following example snippet [I found on StackOverflow](https://stackoverflow.com/questions/29746635/nltk-sentence-tokenizer-custom-sentence-starters)

**Example 1: Works**
```py
from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktLanguageVars

class BulletPointLangVars(PunktLanguageVars):
    sent_end_chars = ('.', '?', '!', '鈥?)

tokenizer = PunktSentenceTokenizer(lang_vars = BulletPointLangVars())
sentences = tokenizer.tokenize(u""鈥?I am a sentence 鈥?I am another sentence"")
for sentence in sentences:
    print(sentence)
```

The above works, and provides the expected output. 
Edit: The above fails if I remove the space preceding 鈥? after some more debugging.

Now, I'm trying to work with the same with minor modifications using a unicode full-stop corresponding to a different language. 


**Example 2: Fails**
```py

from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktLanguageVars

class BulletPointLangVars(PunktLanguageVars):
    sent_end_chars = ('.', '?', '!', '\u0964')

tokenizer = PunktSentenceTokenizer(lang_vars = BulletPointLangVars())
sentences = tokenizer.tokenize(u""唳夃Κ唳班唳粪唳熰唳班Κ唳む 唳多唳班 唳忇Ξ 唳唳權唳曕唳囙唳?唳ㄠ唳囙Α唰?唳膏唳Μ唳距Π 唳嗋 唳嗋 唳熰 唳︵唳侧唳侧唳?唳灌唳班 唳溹唳ㄠ唳む 唳夃Ζ唳唳Θ唰囙Π 唳夃Ζ唰嵿Μ唰嬥Η唳?唳曕Π唰囙唰囙Θ啷?唳呧Θ唳侧唳囙Θ唰囙Π 唳唳о唳Ξ唰?唳忇 唳呧Θ唰佮Ψ唰嵿唳距Θ唰?唳曕唳ㄠ唳︵唳班唰?唳唳ㄠΜ 唳膏Ξ唰嵿Κ唳?唳夃Θ唰嵿Θ唰熰Θ唳Θ唰嵿Δ唰嵿Π唰€ 唳多唳班 唳班Ξ唰囙Χ 唳唳栢Π唳苦唳距Σ 鈥樴Θ唳苦Χ唳距唰嵿鈥? 唳夃Κ唳膏唳ム唳?唳涏唳侧唳ㄠイ 唳忇 唳夃Κ唳侧唰嵿Ψ唰嵿Ο唰?唳夃Κ唳班唳粪唳熰唳班Κ唳む 唳灌唳班唳溹唳ㄠ唳む唳?唳侧唳椸 唳忇Μ唳?唰ㄠЕ唰┼Е-唳忇Π 唳溹Θ唰嵿Ο 唳唳班Δ唳苦Ψ唰嵿唳距Θ唰囙Π 唳侧唰嵿Ψ唰嵿Ο 唳?唳Π唳苦唳侧唳Θ唳距Π 唳ㄠΕ唳?唳唳班唳距Χ 唳曕Π唰囙唰囙Θ啷?  唳呧Θ唰佮Ψ唰嵿唳距Θ唰?唳唰嵿Δ唳唳?唳班唳栢Δ唰?唳椸唰熰 唳多唳班 唳ㄠ唳囙Α唰?唳Σ唰囙唰囙Θ, 唳溹Σ唳唰熰 唳Π唳苦Μ唳班唳むΘ 唳ム唳曕 唳膏唳唳膏唳ム唳?唳膏Ξ唳膏唳 啾?唳唳ㄠΜ唳溹唳む 唳嗋 唳 唳膏Ξ唳膏唳?唳膏Ξ唳膏唳唳?唳唳栢唳唳栢 唳灌唰嵿唰? 唳膏唳椸唳侧唳曕 唳膏Ξ唳距Η唳距Θ唰囙Π 唳溹Θ唰嵿Ο 唳嗋唳嗋唳熰 唳膏 唳呧Θ唰嵿Ο唳距Θ唰嵿Ο 唳夃唰嵿 唳多唳曕唳粪 唳唳班Δ唳苦Ψ唰嵿唳距Θ唰囙Π 唳椸Μ唰囙Ψ唳｀唳?唳夃Κ唳?唳椸唳班唳む唳?唳︵唳撪唳?唳夃唳苦啷?唳︵唳多唳?唳膏Ξ唳膏唳唳椸唳侧唳?唳膏唳ム唳む唳多唳?唳膏Ξ唳距Η唳距Θ唰囙Π 唳唳班Δ唰€唰?唳唳班Δ唳苦Ψ唰嵿唳距Θ唳椸唳侧 唳唳?唳忇Ξ唳?唳曕唳涏 唳曕唳?唳曕Π唳, 唳唳む 唳膏Ξ唳距唰囙Π 唳夃Κ唳?唳む唳?唳囙Δ唳苦Μ唳距唳?唳唳班Ν唳距Μ 唳Π唰? 唳む唳灌Σ唰囙 唳膏唳椸唳侧 唳唳多唳 唳膏Π唰嵿Μ唳多唳班唳粪唳?唳唳班Δ唳苦Ψ唰嵿唳距Θ 唳灌唰?唳夃唳む 唳唳班Μ唰囙イ    唳膏唳唳溹唳?唳唳唳ㄠ唳?唳膏Ξ唳膏唳唳?唳膏Ξ唳距Η唳距Θ 唳栢唳溹Δ唰?唳唳膏Π唳曕唳班 唳曕唳粪唳む唳班唰?  唳多唳曕唳粪唳曕唳粪唳む唳班 唳椸Μ唰囙Ψ唳｀ 唳?唳夃Θ唰嵿Θ唰熰Θ唰囙Π 唳溹Θ唰嵿Ο 唳栢唳侧 唳Θ唰?唳唳ㄠ唰熰唳椸唳?唳む唳ㄠ 唳嗋唰嵿Μ唳距Θ 唳溹唳ㄠ唰熰唳涏唳ㄠイ 唳嗋唳嗋唳熰唳?唳涏唳む唳班唳距Δ唰嵿Π唰€唳︵唳? 唳椸唳班唳唳?唳唳班Δ 唳?唳曕唳粪唳︵唳?唳ㄠ唳ㄠ 唳膏Ξ唳膏唳距Π 唳膏Ξ唳距Η唳距Θ 唳涏唰溹唳?唳曕 唳曕Π唰?唳唳粪唳熰唳曕Π 唳?唳唳班唳熰唳?唳膏Ξ唰冟Ζ唰嵿Η 唳多Ω唰嵿Ο 唳夃唳?唳︵Θ 唳唳︵唳о 唳曕Π唳?唳唰? 唳多唳班 唳ㄠ唳囙Α唰? 唳膏唳?唳唳粪唳椸唳侧 唳ㄠ唰熰唳?唳曕唳?唳曕Π唳距Π 唳Π唳距Ξ唳班唳?唳︵唳ㄠイ 唳夃唰嵿唳多唳曕唳粪 唳唳班Δ唳苦Ψ唰嵿唳距Θ唳椸唳侧唳曕 唳忇唳曕Ν唳距Μ唰?唳ㄠ, 唳多唳侧唳?唳膏唳膏唳ム唳椸唳侧唳?唳膏唰嵿唰?唳溹唳?唳唳佮Η唰?唳呧Δ唳距Η唰佮Θ唳苦 唳唳班Ο唰佮唰嵿Δ唳?唳夃Ζ唰嵿Ν唳距Μ唳?唳曕Π唳む 唳灌Μ唰囙イ 唳忇Π 唳Σ唰?唳︵唳班唳?唳?唳Σ唳距Λ唳?唳唳む唳む唳?唳ㄠ唳ㄠ 唳唳班唳侧唳?唳唳膏唳むΜ唳距唳ㄠ 唳膏唳唳о 唳灌Μ唰囙イ   唳ㄠΔ唰佮Θ 唳多唳曕唳粪唳ㄠ唳む唳?唳唳班Ω唳權唳椸 唳夃Κ唳班唳粪唳熰唳班Κ唳む 唳Σ唰囙唰囙Θ, 唳唳班Δ唳曕 唳嗋Θ唰嵿Δ唳班唳溹唳む唳?唳多唳曕唳粪 唳唳班Δ唳苦Ψ唰嵿唳距Θ唰囙Π 唳椸Θ唰嵿Δ唳唳 唳Π唳苦Γ唳?唳曕Π唳距Π 唳溹Θ唰嵿Ο 唳忇 唳ㄠ唳む 唳膏唳距唳?唳灌Μ唰囙イ 唳夃唰嵿 唳多唳曕唳粪 唳唳班Δ唳苦Ψ唰嵿唳距Θ唰囙Π 唳唳ㄠ唳ㄠ唳ㄠ唳ㄠ 唳膏Π唳曕唳? 唳唳多唳Μ唳苦Ζ唰嵿Ο唳距Σ唰? 唳多唳曕唳粪唳唳?唳?唳唳膏Π唳曕唳班 唳唳班Δ唳苦Ψ唰嵿唳距Θ唳椸唳侧唳曕 唳忇唳唳椸 唳曕唳?唳曕Π唳む 唳灌Μ唰囙イ   唳嗋唳嗋唳熰 唳︵唳侧唳侧 唳多唳侧唳唳︵唳唳?唳椸唳距Π 唳曕唳ㄠ唳︵唳?唳灌唰?唳夃唳涏 唳Σ唰?唳夃Κ唳班唳粪唳熰唳班Κ唳む 唳膏Θ唰嵿Δ唰嬥Ψ 唳唳班唳距Χ 唳曕Π唰囙唰囙Θ啷?唳忇 唳唳班Ω唳權唳椸 唳む唳ㄠ 唳唳ㄠΜ 唳膏Ξ唰嵿Κ唳?唳夃Θ唰嵿Θ唰熰Θ 唳Θ唰嵿Δ唰嵿Π唳曕唳?鈥樴唳ㄠ唳ㄠΔ 唳唳班Δ 唳呧Ν唳苦Ο唳距Θ鈥?唳曕Π唰嵿Ξ唳膏唳氞唳む 唳︵唳侧唳侧 唳嗋唳嗋唳熰 唳呧Θ唰佮唳熰唰囙Π 唳唳唳曕 唳唳侧Θ 唳曕Π唳距 唳忇 唳唳班Δ唳苦Ψ唰嵿唳距Θ唰囙Π 唳唳班Χ唳傕Ω唳?唳曕Π唰囙唰囙Θ啷?   唳多唳班 唳唳栢Π唳苦唳距Σ, 唳忇 唳呧Θ唰佮Ψ唰嵿唳距Θ唰囙Π 唳夃Ζ唰嵿Μ唰嬥Η唳?唳曕Π唳距Π 唳溹Θ唰嵿Ο 唳夃Κ唳班唳粪唳熰唳班Κ唳む唳?唳唳班Δ唳?唳曕唳む唰嵿唳む 唳溹唳ㄠ唰熰唳涏唳ㄠイ 唳む唳ㄠ 唳Σ唰囙唰囙Θ, 唳嗋Ξ唳距Ζ唰囙Π 唳︵唳多唳?唳涏唳む唳班唳距Δ唰嵿Π唰€唳︵唳?唳溹Θ唰嵿Ο 唳忇唳熰 唳嗋Η唰佮Θ唳苦 唳?唳夃Θ唰嵿Θ唳?唳多唳曕唳粪 唳唳Μ唳膏唳ム 唳椸唰?唳む唳侧Δ唰囙Ж唰︵Ж唰?唳?唳ㄠΔ唰佮Θ 唳溹唳む唰?唳多唳曕唳粪唳ㄠ唳む 唳膏唳距唳?唳灌Μ唰囙イ 唳嗋唳嗋唳熰 唳︵唳侧唳侧唳? 唳椸唳班Μ唳 唰Е 唳唳班唳?唳夃Σ唰嵿Σ唰囙 唳曕Π唰?唳む唳ㄠ 唳Σ唰囙唰囙Θ 唳膏唳班 唳︵唳?唳唳?唳唳?唰оН 唳唳距Ξ唳距Π唰€唳?唳唳班唳︵唳о 唳侧唳距 唳曕Π唳涏, 唳む唳?唳嗋唳嗋唳熰 唳︵唳侧唳侧 唳ㄠ唳ㄠ唳唳 唳曕唳班唳椸Π唳?唳膏唳距唳む 唳︵唰熰唳涏, 唳 唳膏Ξ唰熰唳Ο唰嬥唰€ 唳?唳唳侧唳Μ唳距Θ啷?唳唳椸Δ 唰?唳唳班 唳忇 唳多唳曕唳粪 唳唳班Δ唳苦Ψ唰嵿唳距Θ唰囙Π 唳多唳曕唳粪 唳多唳曕唳粪唳曕 唳?唳涏唳む唳班唳距Δ唰嵿Π唰€唳班 唰Е唰?唳?唳唳多 唳唳熰唳ㄠ唳熰唳?唳嗋Μ唰囙Ζ唳?唳曕Π唰囙唰囙Θ 唳忇Μ唳?唳む唳佮Ζ唰囙Π 唰оЕ唳灌唳溹唳班唳?唳唳多 唳椸Μ唰囙Ψ唳｀ 唳Δ唰嵿Π 唳唳唳ㄠ唳?唳嗋Θ唰嵿Δ唳班唳溹唳む唳?唳Δ唰嵿Π 唳Δ唰嵿Π唳苦唳距 唳涏唳唳ㄠ 唳灌唰囙唰囙イ 唰ㄠЕ唰оК 唳膏唳侧 唳膏Π唳曕唳?唳忇 唳唳班Δ唳苦Ψ唰嵿唳距Θ唳曕 唳唳栢唳ㄠ 唳椸Μ唰囙Ψ唳｀唳?唳溹Θ唰嵿Ο 唰оЕ唰?唳曕唳熰 唳熰唳曕 唳︵唰熰唳涏唳? 唰ㄠЕ唰оН 唳膏唳侧 唳む 唳唰溹 唳灌唰囙唰?唰Е唰?唳曕唳熰 唳熰唳曕啷?   唳︵唳侧唳侧 唳嗋唳嗋唳熰唳?唳∴唳班唳曕唳熰Π 唳呧Η唰嵿Ο唳距Κ唳?唳 唳班唳唰嬥Κ唳距Σ 唳班唳?唳溹唳ㄠ唰熰唳涏唳? 唰ㄠЕ唰┼Е 唳膏唳侧唳?唳 唳侧唰嵿Ψ 唳唳む唳班 唳忇 唳多唳曕唳粪 唳唳班Δ唳苦Ψ唰嵿唳? 唳ㄠ唰熰唳涏, 唳む唳?唳Σ唰?唳涏唳む唳班唳距Δ唰嵿Π唰€, 唳唳班唳曕唳むΘ唰€, 唳多唳曕唳粪 唳多唳曕唳粪唳曕 唳?唳曕Π唰嵿Ξ唰€唳Π唰嵿唳︵唳?唳溹唳Θ唰?唳囙Δ唳苦Μ唳距唳?唳唳班Ν唳距Μ 唳唳 唳忇Μ唳?唳嗋唳距Ξ唰€ 唳︵唳ㄠ 唳︵唳多唳?唳唳班唳む唳む 唳む 唳膏唳距唳?唳灌Μ唰囙イ   唳呧Θ唰佮Ψ唰嵿唳距Θ唰囙Π 唳︵唳唳む唰?唳Π唰嵿Μ唰?唳呧Η唰嵿Ο唳距Κ唳?唳︵唳唳權唳栢唳? 唳呧Η唰嵿Ο唳距Κ唳?唳忇Ξ 唳唳侧唳曕唳粪唳｀唳?唳唰嵿Δ唳唳?唳班唳栢唳ㄠイ 鈥樴唳?唳嗋唳熰 唳︵唳侧唳侧唳?唰Е 唳唳班唳?唳夃唳曕Π唰嵿Ψ唳む唳?唳膏唳唳む唳氞唳班Γ唳?唳?唳Μ唳苦Ψ唰嵿Ο唰?唳Π唳苦唳侧唳Θ唳锯€?唳多唳班唳粪 唳忇 唳嗋Σ唰嬥唳ㄠ唰?唳忇 唳唳班Δ唳苦Ψ唰嵿唳距Θ唰囙Π 唳唳班唳曕唳むΘ 唳∴唳班唳曕唳熰Π 唳呧Η唰嵿Ο唳距Κ唳?唳 唳忇Ω 唳班唳溹, 唳呧Η唰嵿Ο唳距Κ唳?唳嗋Π 唳忇Ω 唳多唳班唳灌, 唳呧Η唰嵿Ο唳距Κ唳?唳膏唳班唳ㄠ唳︵唳?唳唳班Ω唳距Ζ 唳?唳呧Η唰嵿Ο唳距Κ唳?唳嗋Π 唳曕 唳多唳唳距唳曕Π 唳呧唳?唳ㄠ唳ㄠイ"")
for sentence in sentences:
    print(sentence)

```

`\u0964` corresponds to the Devanagiri full-stop (I have tried putting the normal one as well). I am not getting similar results as example 1. What could be going wrong here? ",,,,,,,,,1,
777,https://github.com/nltk/nltk/issues/2627,2627,[],closed,2020-11-23 21:10:26+00:00,,1,Does feature-based CFG support generation like CFG does?,"with `nltk.CFG`, it is possible to generate sentences recognized by a CFG grammar:
http://www.nltk.org/howto/generate.html

But is it possible to do the similar generation for feature-based CFG (https://www.nltk.org/book/ch09.html#ref-load_parser1)?

And if not, is there a relatively-easy workaround to enable the generation?
What comes in my mind is to use CFG to generate sentences and then use more detailed F-CFG to keep only recognized sentences.

I don't have in mind more complex language to generate than the grammars offered here:
https://www.nltk.org/book/ch09.html#ref-load_parser1
Just with a larger lexicon.
 

 ",,,,,,,,,1,
262,https://github.com/nltk/nltk/issues/1741,1741,"[{'id': 718738467, 'node_id': 'MDU6TGFiZWw3MTg3Mzg0Njc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/windows%20related', 'name': 'windows related', 'color': 'fce4ba', 'default': False, 'description': None}]",closed,2017-05-31 14:01:05+00:00,,3,Anyway to install latest version on Windows XP SP3?,"Tried installing via the Windows Installer `nltk-3.2.4.win32.exe` at PyPI, and get this error when I launch the downloaded executable... ""<PATH-TO-FILE>...not a valid win32 application"".

I believe this has to do with the build system or compiler used, So is there any other way?

Windows XP SP3
Python 2.7.13",,,,,,,1,1,,
94,https://github.com/nltk/nltk/issues/1414,1414,[],closed,2016-06-16 16:01:40+00:00,,1,pip download-cache unavailable,"Users that use `pip 8.0.0` and above experience the following issue (especially while running `tox` tests):

```
no such option: --download-cache
ERROR: InvocationError: '/path/nltk/.tox/py34/bin/pip install --download-cache=/path/nltk/.tox/_download scipy scikit-learn'
```

Since `pip 6.0` the `--download-cache` flag used to be deprecated, but could still be used. On the contrary, newer versions of `pip` are not backward compatible with regard to this flag.

**6.0 (2014-12-22)**:

> - **DEPRECATION** `pip install --download-cache` and `pip wheel --download-cache` command line flags have been deprecated and the functionality removed. Since pip now automatically configures and uses it鈥檚 internal HTTP cache which supplants the `--download-cache` the existing options have been made non functional but will still be accepted until their removal in pip v8.0. For more information please see https://pip.pypa.io/en/latest/reference/pip_install.html#caching

**8.0.0 (2016-01-19):**

> - **BACKWARD INCOMPATIBLE** Remove the --download-cache which had been deprecated and no-op'd in 6.0.

Since two years passed since the `--download-cache` flag has been deprecated, I suggest to completely remove it from `tox.ini`.
",,,,,,,,1,,
131,https://github.com/nltk/nltk/issues/1484,1484,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2016-10-20 14:20:36+00:00,,1,ipipan corpus not found and NKJP,"Running `tox -e py35` I get the following error:

```
ERROR: Failure: LookupError (
**********************************************************************
  Resource 'corpora/ipipan' not found.  Please use the NLTK
  Downloader to obtain the resource:  >>> nltk.download()
```

But the `ipipan` corpus is not available on nltk_data:

```
>>> nltk.download('ipipan')
[nltk_data] Error loading ipipan: Package 'ipipan' not found in index
```

I know there are several open issues and references to this corpus and `nkjp`:
- #913 (Issue: ipipan module looks dead)
- #844 (Pull Request: NKJP Corpus Reader)
- nltk/nltk_data#17 (Pull Request: NKJP corpus on nltk_data)
- [National Corpus of Polish](https://groups.google.com/forum/#!topic/nltk-dev/VMMCXejQJMQ) (on NLTK-dev)

@stevenbird 
What should we do with ipipan and NKJP modules? They are currently breaking `python3.5` tests and nltk_data does not have corpora for them.

Also note: I do not know why tests with previous python versions do not automatically break.
",,,,,,,,1,,
555,https://github.com/nltk/nltk/issues/2228,2228,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2019-02-08 18:03:51+00:00,,4,make it installable with pip or at least link download page,"To reproduce:

- start with an environment without `nltk`
- run `sudo pip install nltk`
- create file with simple `nltk` script (see below)
- run this script

I tested with 

```import nltk

print(nltk.word_tokenize(""test string     aaaaa\n\n\nfinal.""))
``` 

based on https://stackoverflow.com/a/37559340/4130619


I got error where relevant part is 
```
LookupError: 
**********************************************************************
  Resource punkt not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('punkt')
  
  Attempted to load tokenizers/punkt/PY3/english.pickle
```

It is surprising as I would expect pip to install package and all necessary resources, not only parts of it.

If it is really necessary to make installation more complicated - please link the real download page or help page in the error message.

full error message: https://pastebin.com/raw/VsjrxvZ5",,,,,,,,1,,
702,https://github.com/nltk/nltk/issues/2492,2492,"[{'id': 718738467, 'node_id': 'MDU6TGFiZWw3MTg3Mzg0Njc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/windows%20related', 'name': 'windows related', 'color': 'fce4ba', 'default': False, 'description': None}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2020-01-30 09:55:42+00:00,,4,Install NTLK did not work on clean Anaconda python 3.7,"To reproduce
On a Windows 10 machine uninstall old versions of Anaconda and Pycharm. Delete by hand everything python or anaconda I could find (including the registry).
Install Anaconda
pip install ntlk.
You get an error ""could not find version to satisfy the requirement"". It doesn't say version for what.
What fixes the problem is
pip install ntlk=3.3
Which means I can't use the newest version of NTLK.
This used to work before. I noticed there is a new version now.",,,,,,,,1,,
714,https://github.com/nltk/nltk/issues/2518,2518,[],closed,2020-03-23 21:15:19+00:00,,1,Difficulty installing NLTK,"Hello,

I keep getting this error when I attempt to intall NLTK:

![NLTK](https://user-images.githubusercontent.com/62572582/77363845-d3a34200-6d29-11ea-9afb-0625aa67b652.png)

What do I do to resolve this problem?

Thanks very much,
Sophia",,,,,,,,1,,
721,https://github.com/nltk/nltk/issues/2525,2525,[],closed,2020-04-04 15:18:52+00:00,,4,install NLTK ,"Last login: Sat Apr  4 17:02:21 on ttys000
hanadys-mbp:~ hanadyahmed$ import nltk
-bash: import: command not found
hanadys-mbp:~ hanadyahmed$ python
Python 2.7.10 (default, Jul 14 2015, 19:46:27) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import nltk
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named nltk
>>> 
",,,,,,,,1,,
840,https://github.com/nltk/nltk/issues/2766,2766,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}]",closed,2021-07-29 17:37:25+00:00,,3,NTLK invalid package metdata breaks install via poetry,"It appears poetry is enforcing PEP440 on package metadata.  The NLTK uses package metadata that does not conform and so poetry fails to install (with poetry 1.2.0 and later, it seems - 1.1.4 doesn't seem affected).

```
[[package]]
name = ""nltk""
version = ""3.6.2""
description = ""Natural Language Toolkit""
category = ""main""
optional = false
python-versions = "">=3.5.*""
```

Error message (in docker)
```
STEP 14: RUN poetry install
Creating virtualenv science-lens-BYYjfetP-py3.8 in /home/1001/.cache/pypoetry/virtualenvs
Installing dependencies from lock file

  InvalidVersion

  Invalid PEP 440 version: '3.5.'

  at /usr/local/lib/python3.8/site-packages/poetry/core/version/pep440/parser.py:67 in parse
       63鈹?    @classmethod
       64鈹?    def parse(cls, value: str, version_class: Optional[Type[""PEP440Version""]] = None):
       65鈹?        match = cls._regex.search(value) if value else None
       66鈹?        if not match:
    鈫? 67鈹?            raise InvalidVersion(f""Invalid PEP 440 version: '{value}'"")
       68鈹?
       69鈹?        if version_class is None:
       70鈹?            from poetry.core.version.pep440.version import PEP440Version
       71鈹?

The following error occurred when trying to handle this error:


  ValueError

  Could not parse version constraint: >=3.5.*

  at /usr/local/lib/python3.8/site-packages/poetry/core/semver/helpers.py:139 in parse_single_constraint
      135鈹?
      136鈹?        try:
      137鈹?            version = Version.parse(version)
      138鈹?        except ValueError:
    鈫?139鈹?            raise ValueError(
      140鈹?                ""Could not parse version constraint: {}"".format(constraint)
      141鈹?            )
      142鈹?
      143鈹?        if op == ""<"":
subprocess exited with status 1
subprocess exited with status 1
error building at STEP ""RUN poetry install"": exit status 1
level=error msg=""exit status 1""
```",,,,,,,,1,,
852,https://github.com/nltk/nltk/issues/2796,2796,"[{'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2021-09-04 02:51:20+00:00,,2,NLTK installation failed,"![image](https://user-images.githubusercontent.com/88143204/132080065-cbec8e76-252a-4654-b7fa-fb31b0bc29e8.png)
",,,,,,,,1,,
87,https://github.com/nltk/nltk/issues/1398,1398,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-05-14 14:58:49+00:00,,0,Version number for nltk corpus data,"Hi,

I'm packaging nltk-data for Arch Linux, but where is the latest version number? I could not find this anywhere.

Here's the package that I'm in the process of updating: [nltk-data](https://www.archlinux.org/packages/community/any/nltk-data/).
Here's the old sourceforge page that is currently down: [nltk.sourceforge.net](http://nltk.sourceforge.net/).

An addition to the FAQ, a release log or change log would be great.
",,,,,,,1,,,
142,https://github.com/nltk/nltk/issues/1506,1506,[],closed,2016-11-14 19:49:12+00:00,,2,Is NLTK FrameNet support frozen at FN version 1.5?,"In #719 there was a question regarding support for newer versions of FrameNet beyond 1.5, yet there didn't appear to be any resolution although the issue is closed.  Is there a way to load newer versions of FrameNet (I have 1.7 burning a hole on my hard drive)?

Thanks!",,,,,,,1,,,
218,https://github.com/nltk/nltk/issues/1656,1656,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",open,2017-03-18 12:35:05+00:00,,3,New version of scipy causes fisher test to break,"Our [CI Server](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/testReport/(root)/metrics_doctest/metrics_doctest/) reports this failing test:

```
File ""/scratch/jenkins/workspace/nltk/TOXENV/py35-jenkins/jdk/jdk8latestOnlineInstall/nltk/test/metrics.doctest"", line 242, in metrics.doctest
Failed example:
    bam.fisher(20, (42, 20), N) > bam.fisher(20, (41, 27), N)
Expected:
    False
Got:
    True
```

@pierpaolo points out that this test failure coincided with a new release of scipy. The failure goes away when we modify tox.ini to specify the previous version:

```
pip install -I 'scipy < 0.19'
; pip install -I scipy
```
",,,,,,,1,,,
565,https://github.com/nltk/nltk/issues/2245,2245,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-03-03 03:55:36+00:00,,2,the BLEU socre calculated by the version 3.4 is different from the version 3.2,"run this code:
from nltk.translate.bleu_score import sentence_bleu
reference = [['the', 'cat',""is"",""sitting"",""on"",""the"",""mat""]]
test = [""on"",'the',""mat"",""is"",""a"",""cat""]
score = sentence_bleu(  reference, test)
print(score)

version 3.2 print:
0.4548019047027907
/home/jren/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: 
Corpus/Sentence contains 0 counts of 4-gram overlaps.
BLEU scores might be undesirable; use SmoothingFunction().
  warnings.warn(_msg)

version 3.4 print:
D:\ProgramData\Anaconda3\lib\site-packages\nltk\translate\bleu_score.py:523: UserWarning:
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
5.5546715329196825e-78",,,,,,,1,,,
569,https://github.com/nltk/nltk/issues/2250,2250,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2019-03-10 09:47:05+00:00,,0,PunktTokenizer does not use the correct version of the pickled model on Python 3.x,"Hi, I'm trying to package my program with NLTK and nltk_data using PyInstaller. So to minimize the size of the data file, I removed the zip file and all models for Python 2.x in `nltk_data/tokenizers/punkt` (only the `PY3` folder is left).

But it seems that the `PunktTokenizer` always uses the Python 2.x version of the pickled model regardless of Python version I'm using. And the error message says that it can't find `tokenizers/punkt/english.pickle` instead of `tokenizers/punkt/PY3/english.pickle`.

Removing the `PY3` folder is okay, so it seems that the Python 3 version of the pickled model is never used.

OS: Windows 10 64-bit
Python version: 3.7.2 64-bit
NLTK version: 3.4",,,,,,,1,,,
592,https://github.com/nltk/nltk/issues/2279,2279,[],closed,2019-04-25 19:21:32+00:00,,2,ImportError: Module use of python35.dll conflicts with this version of Python.,"Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\__init__.py"", line 137, in <module>
    from nltk.stem import *
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\stem\__init__.py"", line 29, in <module>
    from nltk.stem.snowball import SnowballStemmer
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\stem\snowball.py"", line 32, in <module>
    from nltk.corpus import stopwords
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\corpus\__init__.py"", line 66, in <module>
    from nltk.corpus.reader import *
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\corpus\reader\__init__.py"", line 105, in <module>
    from nltk.corpus.reader.panlex_lite import *
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\corpus\reader\panlex_lite.py"", line 15, in <module>
    import sqlite3
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\sqlite3\__init__.py"", line 23, in <module>
    from sqlite3.dbapi2 import *
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\sqlite3\dbapi2.py"", line 27, in <module>
    from _sqlite3 import *
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
ImportError: Module use of python35.dll conflicts with this version of Python.",,,,,,,1,,,
599,https://github.com/nltk/nltk/issues/2295,2295,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2019-05-09 04:10:52+00:00,,1,Treebank detokenizer fails to undo quote conversion,"Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)] on win32
NLTK version: 3.4.1

Quotes `""` are converted to ` `` ` or `''` by the treebank tokenizer. The detokenizer is supposed to revert its regexes but fails to do so for these quotes. The space is lost also.

```python
>>> from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer
>>> tmp = TreebankWordTokenizer().tokenize('How ""are"" you?')
>>> tmp
['How', '``', 'are', ""''"", 'you', '?']
>>> TreebankWordDetokenizer().tokenize(tmp)
'How``are""you?'
```",,,,,,,1,,,
700,https://github.com/nltk/nltk/issues/2490,2490,[],closed,2020-01-23 01:28:11+00:00,,2,Release new version,"I'm looking forward to upgrade my Python to 3.8, and I was expecting to see the bug fixes mentioned in #2359 .

However those are currently only available in master.

Any plan to upgrade the package available in PyPI?",,,,,,,1,,,
787,https://github.com/nltk/nltk/issues/2644,2644,[],open,2020-12-26 15:00:57+00:00,,1,How to generate bigram language model with Katz Backoff smoothing(nltk version 3.5.0),"Python: 3.6.8
nltk: 3.5.0

I am new to nltk and also a NLP newbie. Recently I am trying to generate a bigram language model from a corpus with **Katz Backoff smoothing**, with which I can calculate the text's probability in this corpus.
I noticed that there is some possible methods in NLTK 3.0.0 documentation (http://www.nltk.org/_modules/nltk/model/ngram.html#NgramModel), which is abandoned in version 3.5.0.

Since I want to generate the bigram language model with Katz Backoff smoothing with nltk鈥檚 latest version, can anyone give me some help or suggestions on how to do this?",,,,,,,1,,,
864,https://github.com/nltk/nltk/issues/2827,2827,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 910625822, 'node_id': 'MDU6TGFiZWw5MTA2MjU4MjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/CI', 'name': 'CI', 'color': '76dbed', 'default': False, 'description': ''}, {'id': 1346305519, 'node_id': 'MDU6TGFiZWwxMzQ2MzA1NTE5', 'url': 'https://api.github.com/repos/nltk/nltk/labels/third-party', 'name': 'third-party', 'color': '42e5b7', 'default': False, 'description': ''}]",closed,2021-09-27 21:43:59+00:00,,0,Update third party tools to newer versions,"From #2820 (@tomaarsen)

The Stanford and MaltParser tools have had newer versions released than the ones used in `third-party.sh`. This may be as simple as modifying the download link to point to the new versions.
",,,,,,,1,,,
874,https://github.com/nltk/nltk/issues/2857,2857,[],open,2021-10-15 16:24:46+00:00,,6,regex package version  2021.10.8 not working,"While importing nltk it gives error on macbook M1 , python version 3.9.7

```
dlopen(/venv/lib/python3.9/site-packages/regex/_regex.cpython-39-darwin.so, 2): no suitable image found.  Did find:
	t/venv/lib/python3.9/site-packages/regex/_regex.cpython-39-darwin.so: code signature in (/venv/lib/python3.9/site-packages/regex/_regex.cpython-39-darwin.so) not valid for use in process using Library Validation: Trying to load an unsigned lib
```
	
	
After degrading regex package version to 2021.8.3 it works fine",,,,,,,1,,,
500,https://github.com/nltk/nltk/issues/2127,2127,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2018-09-21 01:01:29+00:00,,2,Loading NLTK corpus like Wordnet from blobs,"nltk.data.load(""URL"") works fine if there are specific items to be loaded like a pickle file or stopwords, Wordnet is a compiled library and there needs to be some way to load this from a blob storage.

In AWS Lambda, the set of zipped files can contain the wordnet compiled binaries and can be loaded.
But if wordnet needs to be imported on runtime from NLTK from the cloud, the best way to do it is from a blob, (without using nltk.download() and dealing with system paths)

This issue is raised to support corpora loading from blobs.",,,,1,,1,,,,
0,https://github.com/nltk/nltk/issues/1246,1246,[],closed,2016-01-04 19:27:09+00:00,,5,"Add ""ll"" to nltk.corpus.stopwords.word(""english"")","I think that ""ll"" should be added to this corpus, as ""s"" and ""t"" are already there, and when sentences with contractions such as ""they'll"" or ""you'll"" are tokenized, ""ll"" will be added as a token, and if we filter out stopwords, ""ll"" should be removed as well.
",,,1,,,1,,,,1
304,https://github.com/nltk/nltk/issues/1800,1800,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",open,2017-08-01 20:23:28+00:00,,5,One Letter Typo in English Stoplist,"The last element returned by 
```
from nltk.corpus import stopwords
stopwords.words('english')
```
is 'wouldn' when I believe it should be wouldn't",,,1,,,1,,,,
756,https://github.com/nltk/nltk/issues/2588,2588,[],open,2020-08-29 02:12:44+00:00,,0,Possible additions to english stopwords,"This is a proposal to improve current list of english stop words. 

**Code used to access stop words:**

```
from nltk.corpus import stopwords
print(stopwords.words(""english""))
```
**Output:**
_['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', ""you're"", ""you've"", ""you'll"", ""you'd"", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', ""she's"", 'her', 'hers', 'herself', 'it', ""it's"", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', ""that'll"", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', ""don't"", 'should', ""should've"", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', ""aren't"", 'couldn', ""couldn't"", 'didn', ""didn't"", 'doesn', ""doesn't"", 'hadn', ""hadn't"", 'hasn', ""hasn't"", 'haven', ""haven't"", 'isn', ""isn't"", 'ma', 'mightn', ""mightn't"", 'mustn', ""mustn't"", 'needn', ""needn't"", 'shan', ""shan't"", 'shouldn', ""shouldn't"", 'wasn', ""wasn't"", 'weren', ""weren't"", 'won', ""won't"", 'wouldn', ""wouldn't""]_

**Proposal:**
While inspecting stop words, I have noticed that these variations of stop words could potentially be added to the corpus: _""cannot"", ""could"", ""done"", ""let"", ""may"" ""mayn"",  ""might"",  ""must"", ""need"", ""ought"", ""oughtn"", ""shall"",  ""would""_. ",,,1,,,1,,,,1
41,https://github.com/nltk/nltk/issues/1322,1322,[],closed,2016-03-08 09:49:36+00:00,,2,german stopwords,"the german stopword file contains wrong inflected forms of ""uns""

instead of

```
'unse', 'unsem', 'unsen', 'unser', 'unses'
```

in lines 190-194 of the stopword file `stopwords/german`, it should be 

```
'unsere', 'unserem', 'unseren', 'unser', 'unseres'
```
",,,,,,1,,,,1
63,https://github.com/nltk/nltk/issues/1367,1367,[],closed,2016-04-18 22:52:57+00:00,,5,russian stopwords,"A list of russian stopwords need to be expanded by following words:

badwords = [
    u'褟', u'邪', u'写邪', u'薪芯', u'褌械斜械', u'屑薪械', u'褌褘', u'懈', u'褍', u'薪邪', u'褖邪', u'邪谐邪', 
    u'褌邪泻', u'褌邪屑', u'泻邪泻懈械', u'泻芯褌芯褉褘泄', u'泻邪泻邪褟', u'褌褍写邪', u'写邪胁邪泄', u'泻芯褉芯褔械', u'泻邪卸械褌褋褟', u'胁芯芯斜褖械',
    u'薪褍', u'薪械', u'褔械褌', u'薪械邪', u'褋胁芯懈', u'薪邪褕械', u'褏芯褌褟', u'褌邪泻芯械', u'薪邪锌褉懈屑械褉', u'泻邪褉芯褔', u'泻邪泻-褌芯',
    u'薪邪屑', u'褏屑', u'胁褋械屑', u'薪械褌', u'写邪', u'芯薪芯', u'褋胁芯械屑', u'锌褉芯', u'胁褘', u'屑', u'褌写',
    u'胁褋褟', u'泻褌芯-褌芯', u'褔褌芯-褌芯', u'胁邪屑', u'褝褌芯', u'褝褌邪', u'褝褌懈', u'褝褌芯褌', u'锌褉褟屑', u'谢懈斜芯', u'泻邪泻', u'屑褘',
    u'锌褉芯褋褌芯', u'斜谢懈薪', u'芯褔械薪褜', u'褋邪屑褘械', u'褌胁芯械屑', u'胁邪褕邪', u'泻褋褌邪褌懈', u'胁褉芯写械', u'褌懈锌邪', u'锌芯泻邪', u'芯泻'

]
",,,,,,1,,,,1
114,https://github.com/nltk/nltk/issues/1446,1446,[],closed,2016-08-05 07:48:56+00:00,,1,Contributing stopwords for Slovene,"I've tried to find the proper folder/file to contribute my list of Slovenian stopwords to, but after an extensive search I wasn't able to find anything.
Where is the proper folder to submit the list of stopwords to and what is the desired format?
Thanks!
",,,,,,1,,,,1
143,https://github.com/nltk/nltk/issues/1507,1507,[],closed,2016-11-14 22:44:25+00:00,,3,nltk.data.path not working,"```
import nltk
nltk.data.path.append('nltk_data')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
```

I am trying to change path but i continues to search at home location of nltk data... any idea why?",,,,,,1,,,,
376,https://github.com/nltk/nltk/issues/1928,1928,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",closed,2018-01-03 14:13:53+00:00,,11,Unclosed file in stopwords corpora,"/Users/kiddo/anaconda/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/Users/kiddo/nltk_data/corpora/stopwords/english'>
  return concat([self.open(f).read() for f in fileids])

That's a warning that I found on debugging mode. I thought that maybe you would like to fix that before the next release.",,,,,,1,,,,1
511,https://github.com/nltk/nltk/issues/2147,2147,[],closed,2018-10-15 16:14:20+00:00,,2,LookupError: Resource \x1b[93mstopwords\x1b[0m not found.,"```python
download('stopwords', download_dir=tempfile.gettempdir())  # Download stopwords list.
stop_words = stopwords.words('english')
```

Looking at the logs this seems to search the dictionary in default folders but not the specified one `[Mon Oct 15 16:06:09.191137 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   >>> nltk.download('stopwords')`, but I have specified the `download_dir` folder.

whole logging follows:

```
[Mon Oct 15 16:06:09.191052 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     except LookupError: raise e
[Mon Oct 15 16:06:09.191077 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240] LookupError: 
[Mon Oct 15 16:06:09.191086 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240] **********************************************************************
[Mon Oct 15 16:06:09.191095 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   Resource \x1b[93mstopwords\x1b[0m not found.
[Mon Oct 15 16:06:09.191103 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   Please use the NLTK Downloader to obtain the resource:
[Mon Oct 15 16:06:09.191111 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240] 
[Mon Oct 15 16:06:09.191119 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   \x1b[31m>>> import nltk
[Mon Oct 15 16:06:09.191137 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   >>> nltk.download('stopwords')
[Mon Oct 15 16:06:09.191145 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   \x1b[0m
[Mon Oct 15 16:06:09.191152 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   Searched in:
[Mon Oct 15 16:06:09.191159 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/var/www/nltk_data'
[Mon Oct 15 16:06:09.191167 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/share/nltk_data'
[Mon Oct 15 16:06:09.191174 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/local/share/nltk_data'
[Mon Oct 15 16:06:09.191181 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/lib/nltk_data'
[Mon Oct 15 16:06:09.191189 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/local/lib/nltk_data'
[Mon Oct 15 16:06:09.191196 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/nltk_data'
[Mon Oct 15 16:06:09.191203 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/share/nltk_data'
[Mon Oct 15 16:06:09.191211 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/lib/nltk_data'
[Mon Oct 15 16:06:09.191218 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240] **********************************************************************
[Mon Oct 15 16:06:09.191225 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]
```

I have previously downloaded 

```python
def load_resources():
    # nltk dataset
    start = time()
    logger.info(""Downloading stopwords and tokenizer..."")
    download('punkt', download_dir=tempfile.gettempdir())  # Download data for tokenizer.
    download('stopwords', download_dir=tempfile.gettempdir())  # Download stopwords list.
    logger.info('Downloading took %.2f seconds to run.' % (time() - start))
```
so the files should be there since I can see the logging:




",,,,,,1,,,,1
726,https://github.com/nltk/nltk/issues/2532,2532,[],closed,2020-04-14 06:48:13+00:00,,1,Lookup Error: Resource [93mstopwords[0m not found.   Please use the NLTK Downloader to obtain the resource,"I am using ntlk for the project and I have used mod_wsgi to configure with apache server. It worked on the localhost but while I tried with apache server to run the code. It shows LookUpError. It is something related to the path. I have tried `import nltk` and `nltk.download('stopwords')` in the shell and it shows downloading at path ""/home/ec2-user/nltk-data"". I also tried adding the path using `nltk.data.path.append('/home/ec2-user/nltk-data')`. I have also given permission to access the directory in wsgi configuration file. I had also used `import sys` and `sys.path.append('/home/ec2-user/nltk_data')`. But somehow the app is not searching nltk at the downloaded folder. I think appending the path would work but it didn't and still with this issue. I have also attached the output while accessing to 8000 port. Please do find the attached file and also suggest me for the possible solutions. Thank you.
![nltk](https://user-images.githubusercontent.com/43409127/79194327-18635a00-7e4c-11ea-99f5-d16cf88099ce.png)
 ",,,,,,1,,,,1
853,https://github.com/nltk/nltk/issues/2800,2800,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2021-09-08 20:21:22+00:00,,3,Swahili stopwords are missing ,"When I try to access the swahili stopwords using the below feature, I'm getting a traceback that the swahili stopwords are missing in the documentation, I'm currently working on building a swahili language model with data gathered from Wikipedia swahili articles and popular swahili blogs,  I was thinking of contributing the stopwords to the library to allow others users to easily load them. 

Best regards
Kalebu",,,,,,1,,,,1
64,https://github.com/nltk/nltk/issues/1369,1369,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}]",closed,2016-04-21 03:23:18+00:00,,0,FrameNet corpus reader: load annotated exemplar sentences and full-text documents,"Proposed interface:
## General
- `fn.sents(exemplars=True, full_text=True)`
- `fn.annotations([luNameRegex], exemplars=True, full_text=True)`
- `sent.annotationSet[0]` for POS tagging, `sent.annotationSet[>0]` for frame annotations
## Lexicographic exemplars (one annotationSet per sentence; not organized into documents)
- `fn.exemplars([luNameRegex])`
- `lu.exemplars`
- `lu.subCorpus`
## Full-text annotations
- `fn.docs([docNameRegex])`
- `fn.ft_sents([docNameRegex])`
- `doc.sentence`
- `fn.documents()`, which is an index of the documents, is renamed to `fn.docs_metadata()`
",,,,,1,,,,,
77,https://github.com/nltk/nltk/issues/1384,1384,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2016-04-29 20:41:46+00:00,,6,"Japanese Corpus readers do not return properly formatted (word, tag) tuples","There are still problems with how POS Tagging works for these corpora. This afternoon, I loaded up JEITA, and called jeita.tagged_words(). The problem is that the second half of each tuple in JEITA doesn't contain a tag that is easy to test against a POS-tagger. The second half of each tuple contains both orthographic information (each word in the corpus has a spelling for each syllabary in Japanese) and the tag information, so a word tagged as a noun won't have the same tag as another word tagged as a noun. This leads to quite a few problems when testing a tagger against the corpus. 

Open up one of the .chasen files and you'll see what I mean. Here's line 6 of a0010.chasen in the jeita.zip file. 

鍑恒倠  銉囥儷  鍑恒倠  鍕曡-鑷珛   涓€娈? 鍩烘湰褰?
There's four ( or maybe five) elements here. The first three are ways of writing the word /deru/, and the last is the tag (verb, transitive, group 1, plain form.) 

So I wrote this loop: 

for sent in tagged_sents:
    for(word, tag) in sent:
        print(word)
        print(tag)

And here's some sample output: 

鍑恒倠
銉囥儷  鍑恒倠  鍕曡-鑷珛   涓€娈? 鍩烘湰褰?
As you can see, the tag includes two forms of orthography, which throws things off.

(Also, as a side note, it would be really great if we could have a ""simple"" pos tag version of these files, which didn't include some of the additional categories like ""plain form"" or which group (ichidan/godan) the verb belonged too, since I don't think a lot of parsers care too much about which is which, but doing this would probably take help from a Japanese fluent individual.) 

I can check again with KNBC, the other Japanese corpus included in NLTK, but it does even funkier things with tags last I checked.
",,,,,1,,,,,
88,https://github.com/nltk/nltk/issues/1399,1399,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-05-15 22:49:34+00:00,,0,API for extracting syntactic-semantic features,"@annefried's [SitEnt Syntactic-Semantic Features](https://github.com/annefried/syntSemFeatures) is a tool for English that marks things like countability on NPs and morphological tense (perfect, progressive, etc.) on verbs. It relies on PTB-style POS tags and Stanford dependencies, and bundles some wordlists (e.g. from CELEX). Seems like it would be useful to have some or all of this functionality in NLTK.
",,,,,1,,,,,
219,https://github.com/nltk/nltk/issues/1658,1658,[],closed,2017-03-19 12:23:41+00:00,,4,Unable to find stanford-postagger.jar on CI server,"The CI server reports that NLTK cannot find the Stanford POS tagger:
https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/TOXENV=py27-jenkins,jdk=jdk8latestOnlineInstall/testReport/nltk.tag/stanford/StanfordPOSTagger/

Cf. http://stackoverflow.com/questions/34726200/nltk-was-unable-to-find-stanford-postagger-jar-set-the-classpath-environment-va",,,,,1,,,,,
340,https://github.com/nltk/nltk/issues/1860,1860,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 789518993, 'node_id': 'MDU6TGFiZWw3ODk1MTg5OTM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/need-help', 'name': 'need-help', 'color': 'e0609e', 'default': False, 'description': None}]",closed,2017-10-16 11:20:35+00:00,,8,[Question] Add custom Regex to improve date recognition for tokenizer and POS tagger,"I am trying to recognizing a simple kind of date (""XX/XX/XXXX"") in my Python3 code.
For that, I would like to create a regex, and to add a tag for this one: ""DATE"" (for example). 

This is the code I wrote:
```
# Recognize month/day/year
patterns = [(r'\d{2}/\d{2}/\d{4}', 'DATE')]
# Build a tagger
reg_tagger = nltk.RegexpTagger(patterns)
default_tagger = nltk.data.load(""taggers/maxent_treebank_pos_tagger/english.pickle"")
# Build a tagger that add reg_tagger in an existing tagger (MaxEnt)
tagger = nltk.UnigramTagger(model=reg_tagger, backoff=default_tagger)
# Tag the words in each sentence
tags = [tagger.tag(_s) for _s in sentences]
```

Unfortunately, I got this error:
```
>>> python3.6 scripts.py examples/090003ea802cef84.txt
Traceback (most recent call last):
  File ""scripts.py"", line 202, in <module>
    tags = get_tags_from_sentences(sentences)
  File ""scripts.py"", line 50, in get_tags_from_sentences
    tags = [tagger.tag(_s) for _s in sentences]
  File ""scripts.py"", line 50, in <listcomp>
    tags = [tagger.tag(_s) for _s in sentences]
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/sequential.py"", line 63, in tag
    tags.append(self.tag_one(tokens, i, tags))
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/sequential.py"", line 83, in tag_one
    tag = tagger.choose_tag(tokens, index, history)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/sequential.py"", line 142, in choose_tag
    return self._context_to_tag.get(context)
AttributeError: 'RegexpTagger' object has no attribute 'get'
```

So, my question is: is it possible to add a custom regex (for my case, to recognize dates) in a default tagger, please?

Thanks a lot",,,,,1,,,,,
346,https://github.com/nltk/nltk/issues/1876,1876,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",closed,2017-10-31 22:42:08+00:00,,6,POS tagging after Tokenization using CoreNLP classes,"Hello,

I want to use the `CoreNLPTagger` to tokenize and POS-tag a big corpus.
However, there is no option to specify additional properties to the `raw_tag_sents` method in the `CoreNLPTagger` (in contrary to the `tokenize` method in `CoreNLPTokenizer`, which lets you specify additional properties). Therefore I'm not able to tell the tokenizer to e.g. not normalize the brackets and other stuff.

For example, I want to use the following tokenization options:
```python
additional_properties = {
            'tokenize.options': 'ptb3Escaping=false, unicodeQuotes=true, splitHyphenated=true, normalizeParentheses=false, normalizeOtherBrackets=false',
            'annotators': 'tokenize, ssplit, pos'
        }
```

Using the tokenizer before the tagger does also not work, as this will revert any of the additional options you set in the tokenizer.

I may be doing something wrong here, but I hope you can help me.

Thanks

",,,,,1,,,,,
445,https://github.com/nltk/nltk/issues/2038,2038,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}]",open,2018-06-08 19:46:05+00:00,,1,How to add your own tags for POS tagging over default taggers using nltk?,"import nltk.tag, nltk.data
tagger_path = '/home/amit/nltk_data/taggers/maxent_treebank_pos_tagger/english.pickle'
default_tagger = nltk.data.load(tagger_path)
tagger = nltk.tag.UnigramTagger(model=model, backoff=default_tagger)
tagged=tagger.tag(text)
#model is a dict which has the required tags ,""tagged"" gives tags according to default_tagger but I want to put tags to text from model dict .Please , explain me what is wrong here ?",,,,,1,,,,,
504,https://github.com/nltk/nltk/issues/2133,2133,"[{'id': 718733436, 'node_id': 'MDU6TGFiZWw3MTg3MzM0MzY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/Off-topic', 'name': 'Off-topic', 'color': 'f7b4e1', 'default': False, 'description': None}]",closed,2018-09-27 10:02:30+00:00,,1,How to distinguish adverbs of manner from other adverbe categories,"Is theire a way to have more detailed pos tagging that specifies which type of adverbes, adjectives it is (manner, frequency, time, etc.) ?",,,,,1,,,,,
621,https://github.com/nltk/nltk/issues/2331,2331,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",closed,2019-07-04 22:55:39+00:00,,3,PickleCorpusView raising UnicodeDecodeError when reading file,"I have been trying to save a modified corpus produced with `LazyMap` using`PickleCorpusView.write` as shown in the [PickleCorpusView documentation](https://www.nltk.org/_modules/nltk/corpus/reader/util.html#PickleCorpusView).

The writing seems to be working properly. However, when trying to read the pickled file, it raises a `UnicodeDecodeError`.

The following code does not even use a `LazyMap` but raises the same error

```python
Python 3.7.3 (default, Jun 24 2019, 04:54:02) 
[GCC 9.1.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import nltk
>>> nltk.corpus.reader.PickleCorpusView.write('In the real use case, this would be the result of LazyMap producing a list of list of tuples (word and PoS tag).', 'test.pickle')
>>> nltk.corpus.reader.PickleCorpusView('test.pickle')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/collections.py"", line 231, in __repr__
    for elt in self:
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/corpus/reader/util.py"", line 306, in iterate_from
    tokens = self.read_block(self._stream)
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/corpus/reader/util.py"", line 525, in read_block
    result.append(pickle.load(stream))
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/data.py"", line 1177, in read
    chars = self._read(size)
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/data.py"", line 1469, in _read
    chars, bytes_decoded = self._incr_decode(bytes)
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/data.py"", line 1491, in _incr_decode
    return self.decode(bytes, 'strict')
  File ""/home/user/.virtualenv/venv/lib/python3.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
```

Am I missing something from the documentation?

I am using Python 3.7.3 and NLTK 3.4.1 running on Arch Linux (kernel 5.1.15). The same error also occurred in Ubuntu.",,,,,1,,,,,
694,https://github.com/nltk/nltk/issues/2479,2479,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-12-15 19:54:50+00:00,,1,Server not working,"andeshs-MacBook-Pro:stanford-corenlp-full-2018-02-27 sandesh$ [main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 8
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.8 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[main] ERROR CoreNLP - Could not pre-load annotators in server; encountered exception:
edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:60)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:43)
	at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.<init>(NumberSequenceClassifier.java:86)
	at edu.stanford.nlp.ie.NERClassifierCombiner.<init>(NERClassifierCombiner.java:135)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.<init>(NERCombinerAnnotator.java:131)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:68)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$44(StanfordCoreNLP.java:546)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$69(StanfordCoreNLP.java:625)
	at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
	at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:495)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:201)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:194)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:181)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1497)
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:237)
	at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:382)
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:36)
	... 16 more
Caused by: java.lang.reflect.InvocationTargetException
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:233)
	... 18 more
Caused by: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException
	at de.jollyday.util.CalendarUtil.<init>(CalendarUtil.java:42)
	at de.jollyday.HolidayManager.<init>(HolidayManager.java:66)
	at de.jollyday.impl.DefaultHolidayManager.<init>(DefaultHolidayManager.java:46)
	at edu.stanford.nlp.time.JollyDayHolidays$MyXMLManager.<init>(JollyDayHolidays.java:148)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.ReflectAccess.newInstance(ReflectAccess.java:166)
	at java.base/jdk.internal.reflect.ReflectionFactory.newInstance(ReflectionFactory.java:404)
	at java.base/java.lang.Class.newInstance(Class.java:591)
	at de.jollyday.caching.HolidayManagerValueHandler.instantiateManagerImpl(HolidayManagerValueHandler.java:60)
	at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:41)
	at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:13)
	at de.jollyday.util.Cache.get(Cache.java:51)
	at de.jollyday.HolidayManager.createManager(HolidayManager.java:168)
	at de.jollyday.HolidayManager.getInstance(HolidayManager.java:148)
	at edu.stanford.nlp.time.JollyDayHolidays.init(JollyDayHolidays.java:57)
	at edu.stanford.nlp.time.Options.<init>(Options.java:119)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:44)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.<init>(TimeExpressionExtractorImpl.java:39)
	... 24 more
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:602)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
	... 45 more
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
",,,,,1,,,,,
696,https://github.com/nltk/nltk/issues/2483,2483,[],closed,2019-12-23 18:34:13+00:00,,1,POS tagger crashes when text starts with a white space.,"Dear NLTK team,

the POS parsing crashes when the text starts with one (or several) white space(s). The problem occurs in the normalize() call.

/usr/local/lib/python3.6/dist-packages/nltk/tag/perceptron.py in normalize(self, word)
    238         elif word.isdigit() and len(word) == 4:
    239             return '!YEAR'
--> 240         elif word[0].isdigit():
    241             return '!DIGITS'
    242         else:

IndexError: string index out of range

I guess that testing the length() of the word could solve the issue.
lstriping the input text is not a correct one, as for many reasons, the text has to be preserved untouched by ntlk.

Best regards

Jerome",,,,,1,,,,,
846,https://github.com/nltk/nltk/issues/2781,2781,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 718773983, 'node_id': 'MDU6TGFiZWw3MTg3NzM5ODM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/classifier', 'name': 'classifier', 'color': '60d2ff', 'default': False, 'description': None}, {'id': 1346305519, 'node_id': 'MDU6TGFiZWwxMzQ2MzA1NTE5', 'url': 'https://api.github.com/repos/nltk/nltk/labels/third-party', 'name': 'third-party', 'color': '42e5b7', 'default': False, 'description': ''}]",open,2021-08-11 08:14:35+00:00,,2,Building own classifier based POS tagger using SklearnClassifier and ClassifierBasedPOSTagger,"I'm trying to build my own classifier based POS tagger using `SklearnClassifier` and `ClassifierBasedPOSTagger`. The code that I've tried is given below.

```
from nltk.corpus import treebank
nltk.download('treebank')

data = treebank.tagged_sents()
train_data = data[:3500]
test_data = data[3500:]
```

```
from nltk.classify import SklearnClassifier
from sklearn.naive_bayes import BernoulliNB
from nltk.tag.sequential import ClassifierBasedPOSTagger

bnb = SklearnClassifier(BernoulliNB())
bnb_tagger = ClassifierBasedPOSTagger(train=train_data,
                                      classifier_builder=bnb.train)

# evaluate tagger on test data and sample sentence
print(bnb_tagger.evaluate(test_data))

# see results on our previously defined sentence
print(bnb_tagger.tag(nltk.word_tokenize(sentence)))
```

This code is yielding the following error:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
C:\Users\ABDULL~1.IMR\AppData\Local\Temp/ipykernel_6580/266992580.py in <module>
      4 
      5 bnb = SklearnClassifier(BernoulliNB())
----> 6 bnb_tagger = ClassifierBasedPOSTagger(train=train_data,
      7                                       classifier_builder=bnb.train)
      8 

~\Miniconda3\envs\nlp_course\lib\site-packages\nltk\tag\sequential.py in __init__(self, feature_detector, train, classifier_builder, classifier, backoff, cutoff_prob, verbose)
    637 
    638         if train:
--> 639             self._train(train, classifier_builder, verbose)
    640 
    641     def choose_tag(self, tokens, index, history):

~\Miniconda3\envs\nlp_course\lib\site-packages\nltk\tag\sequential.py in _train(self, tagged_corpus, classifier_builder, verbose)
    673         if verbose:
    674             print(""Training classifier ({} instances)"".format(len(classifier_corpus)))
--> 675         self._classifier = classifier_builder(classifier_corpus)
    676 
    677     def __repr__(self):

~\Miniconda3\envs\nlp_course\lib\site-packages\nltk\classify\scikitlearn.py in train(self, labeled_featuresets)
    110 
    111         X, y = list(zip(*labeled_featuresets))
--> 112         X = self._vectorizer.fit_transform(X)
    113         y = self._encoder.fit_transform(y)
    114         self._clf.fit(X, y)

~\Miniconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\_dict_vectorizer.py in fit_transform(self, X, y)
    288             Feature vectors; always 2-d.
    289         """"""
--> 290         return self._transform(X, fitting=True)
    291 
    292     def inverse_transform(self, X, dict_type=dict):

~\Miniconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\_dict_vectorizer.py in _transform(self, X, fitting)
    233                     if feature_name in vocab:
    234                         indices.append(vocab[feature_name])
--> 235                         values.append(self.dtype(v))
    236 
    237             indptr.append(len(indices))

TypeError: float() argument must be a string or a number, not 'NoneType'
```
How to do it right?",,,,,1,,,,,
858,https://github.com/nltk/nltk/issues/2812,2812,"[{'id': 3375726484, 'node_id': 'LA_kwDOAASTVs7JNX-U', 'url': 'https://api.github.com/repos/nltk/nltk/labels/deprecation', 'name': 'deprecation', 'color': '201B0C', 'default': False, 'description': ''}, {'id': 3375727332, 'node_id': 'LA_kwDOAASTVs7JNYLk', 'url': 'https://api.github.com/repos/nltk/nltk/labels/discussion', 'name': 'discussion', 'color': '71909E', 'default': False, 'description': ''}]",open,2021-09-21 16:16:07+00:00,,3,Finishing up Stanford Deprecation,"Hello!

As some of you might be aware, several Stanford related classes have been deprecated back in 2017. They are the following:
* [`nltk.tag.StanfordTagger`](https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L31)
* [`nltk.tag.StanfordPOSTagger`](https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L139)
* [`nltk.tag.StanfordNERTagger`](https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L176)
* [`nltk.parse.GenericStanfordParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/stanford.py#L28)
* [`nltk.parse.StanfordParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/stanford.py#L274)
* [`nltk.parse.StanfordDependencyParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/stanford.py#L341)
* [`nltk.parse.StanfordNeuralDependencyParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/stanford.py#L407)
* [`nltk.tokenize.StanfordTokenizer`](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/stanford.py#L22)
* [`nltk.tokenize.StanfordSegmenter`](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/stanford_segmenter.py#L32)

These have been replaced by the following newer classes:<sup>[1](#f1)</sup>
* [`nltk.parse.GenericCoreNLPParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L176)
* [`nltk.parse.CoreNLPParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L394)
* [`nltk.parse.CoreNLPDependencyParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L546)

Note that each of these new classes rely on a `CoreNLPServer` running. One of the ways to get this to run is directly from the source using Java, as mentioned in https://github.com/nltk/nltk/pull/1735#issuecomment-306091826 by the author of most of these changes, @alvations. He used:
```
wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip && cd stanford-corenlp-full-2016-10-31

java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
-preload tokenize,ssplit,pos,lemma,parse,depparse \
-status_port 9000 -port 9000 -timeout 15000
```
Note that newer versions of the stanford-corenlp package are available nowadays.
Alternatively, the [`CoreNLPServer`](https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L38) class can also be used to run the server in Python, though I haven't gotten that to work on Windows.

---

### What now?

All of these Stanford classes contain DeprecationWarnings placed back in 2017, such as this one:
https://github.com/nltk/nltk/blob/d21646dbd547cdd02d0c60f8e23d1d28a9fd1266/nltk/tokenize/stanford_segmenter.py#L71-L82

Clearly, we need to make some changes here. We're on v3.6.3 now.

With this issue I invite some discussion on the following options (among others):
1. Remove the deprecated classes in their entirety.
2. Remove the bodies of the methods, and point to a documentation reference of porting these methods to the newer CoreNLP equivalents.
3. Keep them, but don't maintain them if we have issues in the future. 

Personally I'm leaning towards either 1 or 2.

However, before simply removing potentially often used code, I went over each of the deprecated classes to see if there are indeed new equivalents, and for adding to the documentation somewhere.

---

### Stanford updating reference

The following table contains the deprecated classes with their main methods, and the equivalent newer classes and methods. Each line on the left column is equivalent to a line on the right column.

<table>
<tr>
<th> Old </th>
<th> New </th>
</tr>
<tr>
<td colspan=""2"" align=""center""><b>POS Tagger</b></td>
</tr>
<tr>
<td>

```python
>>> from nltk.tag.stanford import StanfordPOSTagger
>>> tagger = StanfordPOSTagger()
>>> tagger.tag(...)
>>> tagger.tag_sents(...)
>>> tagger.parse_output(...)
>>> ...
```

</td>
<td>

```python
>>> from nltk.parse import CoreNLPParser
>>> parser = CoreNLPParser(tagtype=""pos"")
>>> parser.tag(...)
>>> parser.tag_sents(...)
>>> *deprecated*
>>> parser.raw_tag_sents(...)
```

</td>
</tr>
<tr>
<td colspan=""2"" align=""center""><b>NER Tagger</b></td>
</tr>
<tr>
<td>

```python
>>> from nltk.tag.stanford import StanfordNERTagger
>>> tagger = StanfordNERTagger()
>>> tagger.tag(...)
>>> tagger.tag_sents(...)
>>> tagger.parse_output(...)
>>> ...
```

</td>
<td>

```python
>>> from nltk.parse import CoreNLPParser
>>> parser = CoreNLPParser(tagtype=""ner"")
>>> parser.tag(...)
>>> parser.tag_sents(...)
>>> *deprecated*
>>> parser.raw_tag_sents(...)
```

</td>
</tr>
<tr>
<td colspan=""2"" align=""center""><b>StanfordParser</b></td>
</tr>
<tr>
<td>

```python
>>> from nltk.parse.stanford import StanfordParser
>>> parser = StanfordParser()
>>> parser.parse_sents(...)
>>> parser.raw_parse(...)
>>> parser.raw_parse_sents(...)
>>> parser.tagged_parse(...)
>>> parser.tagged_parse_sents(...)
>>> ...
```

</td>
<td>

```python
>>> from nltk.parse import CoreNLPParser
>>> parser = CoreNLPParser()
>>> parser.parse_sents(...)
>>> parser.raw_parse(...)
>>> parser.raw_parse_sents(...)
>>> *deprecated*
>>> *deprecated*
>>> parser.parse_text()
```

</td>
</tr>
<tr>
<td colspan=""2"" align=""center""><b>StanfordTokenizer</b></td>
</tr>
<tr>
<td>

```python
>>> from nltk.tokenize.stanford import StanfordTokenizer
>>> tokenizer = StanfordTokenizer()
>>> tokenizer.tokenize(...)
>>> tokenizer.tokenize_sents(...)
```

</td>
<td>

```python
>>> from nltk.parse import CoreNLPParser
>>> parser = CoreNLPParser()
>>> parser.tokenize(...)
>>> parser.tokenize_sents(...)
```

</td>
</tr>
<tr>
<td colspan=""2"" align=""center""><b>StanfordSegmenter</b></td>
</tr>
<tr>
<td>

```python
>>> from nltk.tokenize import StanfordSegmenter
>>> segmenter = StanfordSegmenter()
>>> segmenter.tokenize(...)
>>> segmenter.tokenize_sents(...)
>>> segmenter.segment_file(...)
>>> segmenter.segment(...)
>>> segmenter.segment_sents(...)
```

</td>
<td>

```python
>>> from nltk.parse import CoreNLPParser
>>> parser = CoreNLPParser()
>>> parser.tokenize(...)
>>> parser.tokenize_sents(...)
>>> *deprecated*
>>> *deprecated*
>>> *deprecated*
```

</td>
</tr>
</table>

### Notes

* `StanfordDependencyParser` used to have the same methods as `StanfordParser`. Nowadays, you should use `CoreNLPDependencyParser` instead, which has the same methods as `CoreNLPParser`.

---

My goal with this PR is to reach a consensus on how to move forwards, and then create a PR with those agreed upon changes, so feel free to share your opinion.

- Tom Aarsen

---

### Footnotes
<a name=""f1"">1</a>: `StanfordNeuralDependencyParser` was never fully implemented, and as a result does not exist in the newer `CoreNLP...` format.",,,,,1,,,,,
6,https://github.com/nltk/nltk/issues/1254,1254,[],closed,2016-01-18 06:09:47+00:00,,4,Loading jars from custom path.,"Hello Team,
 I want to load the stanfor-parser.jar file from my custom defined path. I dont want to set ENV variable for jar locations. Is this possible ? If yes then how?

Thanks,
Rahul 
",,,,1,,,,,,
18,https://github.com/nltk/nltk/issues/1279,1279,[],closed,2016-01-29 18:56:00+00:00,,8,OSError when downloading/unzipping NLTK data (Python 3.5.1),"I'm getting an OSError when I try to download data via the NLTK command line interface. This occurs when unzipping `corpora/panlex_lite.zip`

Running NLTK 3.1 on Python 3.5.1 (Python installed via Homebrew, NLTK installed via pip) on Mac OS X 10.11.3

Tried running `python3 -m nltk.downloader all` as suggested on http://www.nltk.org/data.html

```
sandip ~> python3 -m nltk.downloader all
[nltk_data] Downloading collection 'all'
[nltk_data]    | 
[nltk_data]    | Downloading package abc to /Users/sandip/nltk_data...
[nltk_data]    |   Package abc is already up-to-date!
[nltk_data]    | Downloading package alpino to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package alpino is already up-to-date!
[nltk_data]    | Downloading package biocreative_ppi to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package biocreative_ppi is already up-to-date!
[nltk_data]    | Downloading package brown to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package brown is already up-to-date!
[nltk_data]    | Downloading package brown_tei to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package brown_tei is already up-to-date!
[nltk_data]    | Downloading package cess_cat to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package cess_cat is already up-to-date!
[nltk_data]    | Downloading package cess_esp to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package cess_esp is already up-to-date!
[nltk_data]    | Downloading package chat80 to
[nltk_data]    |     /Users/sandip/nltk_data...
...
[nltk_data]    | Downloading package panlex_lite to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Unzipping corpora/panlex_lite.zip.
Traceback (most recent call last):
  File ""/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 2267, in <module>
    halt_on_error=options.halt_on_error)
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 664, in download
    for msg in self.incr_download(info_or_id, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 543, in incr_download
    for msg in self.incr_download(info.children, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 529, in incr_download
    for msg in self._download_list(info_or_id, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 572, in _download_list
    for msg in self.incr_download(item, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 549, in incr_download
    for msg in self._download_package(info, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 638, in _download_package
    for msg in _unzip_iter(filepath, zipdir, verbose=False):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 2039, in _unzip_iter
    outfile.write(contents)
OSError: [Errno 22] Invalid argument
```
",,,,1,,,,,,
55,https://github.com/nltk/nltk/issues/1340,1340,[],closed,2016-03-23 17:00:09+00:00,,0,Problem with lazy corpus loading in CHILDES?,"I would expect the following to display the beginning of the list fairly quickly:

``` py
>>> from nltk.corpus.reader.childes import CHILDESCorpusReader
>>> childes = CHILDESCorpusReader('/Users/nschneid/nltk_data/corpora/childes/data-xml/Eng-USA', '.*.xml')
>>> childes.tagged_words()
```

But it hangs, suggesting that it's trying to load the entire corpus. Is there a more efficient way to implement this?
",,,,1,,,,,,
89,https://github.com/nltk/nltk/issues/1400,1400,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-05-18 09:56:39+00:00,,1,Perceptron data loading issue with UNC paths,"Hi,

We're storing our data under an UNC path (i.e. our NLTK_DATA is on the lines of \\mypath.com\nltkdata), and this causes the following error on data loading for tag/perceptron.py. All other data loading seems to be fine (e.g. chunk/__init__.py has no problems).

We've been tweaking perceptron.py a little to debug, so the line numbers are a little off. We're currently using this alteration which is working:

`AP_MODEL_LOC = 'taggers/averaged_perceptron_tagger/'+PICKLE`

Traceback as follows:

```
Traceback (most recent call last):
 File "".\test_parser.py"", line 20, in <module>
   parsed = parser.parse(data['text'])
 File ""C:\Users\mike\Project\parser.py"", line 50, in parse
   tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]
 File ""C:\Users\mike\Project\lib\site-packages\nltk\tag\__init__.py"", line 110, in pos_tag
   tagger = PerceptronTagger()
 File ""C:\Users\mike\Project\lib\site-packages\nltk\tag\perceptron.py"", line 143, in __init__
   self.load(AP_MODEL_LOC)
 File ""C:\Users\mike\Project\lib\site-packages\nltk\tag\perceptron.py"", line 211, in load
   self.model.weights, self.tagdict, self.classes = load(loc)
 File ""C:\Users\mike\Project\lib\site-packages\nltk\data.py"", line 800, in load
   opened_resource = _open(resource_url)
 File ""C:\Users\mike\Project\lib\site-packages\nltk\data.py"", line 921, in _open
   return find(path_, ['']).open()
 File ""C:\Users\mike\Project\lib\site-packages\nltk\data.py"", line 640, in find
   raise LookupError(resource_not_found)
LookupError:
**********************************************************************
 Resource u'/C:/Users/mike/Project/mypath.com/nltkdata/taggers/averaged
 _perceptron_tagger/averaged_perceptron_tagger.pickle' not found.
 Please use the NLTK Downloader to obtain the resource:  >>>
 nltk.download()
 Searched in:
   - u''
**********************************************************************
```

Cheers,
Mike
",,,,1,,,,,,
297,https://github.com/nltk/nltk/issues/1791,1791,[],closed,2017-07-27 04:45:49+00:00,,5,NlTK downloading corpus using nltk.download() giving 403 error,"I've tried downloading punkt corpus using nltk.download() as well as manually from http://www.nltk.org/nltk_data/ both are giving me a 403 forbidden, Varnish cache server error.",,,,1,,,,,,
300,https://github.com/nltk/nltk/issues/1794,1794,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 81645781, 'node_id': 'MDU6TGFiZWw4MTY0NTc4MQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/language-model', 'name': 'language-model', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2017-07-27 16:44:18+00:00,,1,Adding appropriate message for nltk.download when loading pre-trained models,"Following up on #1787, it would be great if the users download specific corpora / models instead of abusing nltk.download('all').

@alexisdimi gave a good suggestion to warn the user with the appropriate package to download when an error is raised at model/data loading.

For the various model, the message would have to be added when the error is raised while loading the model. 

To start off, these should be the more popular models that users might load

 - `punkt` / `sent_tokenize`:  https://github.com/nltk/nltk/blob/develop/nltk/tokenize/__init__.py#L84
 - `pos_tag`: https://github.com/nltk/nltk/blob/develop/nltk/tag/__init__.py#L84
 - `ne_chunk`: https://github.com/nltk/nltk/blob/develop/nltk/chunk/__init__.py

Additionally, a quick search on https://github.com/nltk/nltk/search?utf8=%E2%9C%93&q=%22from+nltk.data+import+load%22&type= reveal several more.",,,,1,,,,,,
490,https://github.com/nltk/nltk/issues/2112,2112,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",open,2018-09-10 07:50:26+00:00,,6,CoreNLPParser tag() should allow properties overloading,"With the current `CoreNLPParser.tag()`, the ""retokenization"" by Stanford CoreNLP is unexpected:

```python
>>> from nltk.parse.corenlp import CoreNLPParser
>>> ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
>>> sent = ['my', 'phone', 'number', 'is', '1111', '1111', '1111']
>>> ner_tagger.tag(sent)
[('my', 'O'),
 ('phone', 'O'),
 ('number', 'O'),
 ('is', 'O'),
 ('1111\xa01111\xa01111', 'NUMBER')]
```

The expected behavior should be:

```python
>>> from nltk.parse.corenlp import CoreNLPParser
>>> ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
>>> sent = ['my', 'phone', 'number', 'is', '1111', '1111', '1111']
>>> ner_tagger.tag(sent)
[('my', 'O'), ('phone', 'O'), ('number', 'O'), ('is', 'O'), ('1111', 'DATE'), ('1111', 'DATE'), ('1111', 'DATE')]
```

Proposed solution is to allow `properties` arguments overloading for `.tag()` and `.tag_sents()`, i.e. at https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L348 and by default use `properties = {'tokenize.whitespace':'true'}` because we are concatenating the tokens by spaces in `tag_sents()`.

```python

    def tag_sents(self, sentences, properties=None):
        """"""
        Tag multiple sentences.

        Takes multiple sentences as a list where each sentence is a list of
        tokens.

        :param sentences: Input sentences to tag
        :type sentences: list(list(str))
        :rtype: list(list(tuple(str, str))
        """"""
        # Converting list(list(str)) -> list(str)
        sentences = (' '.join(words) for words in sentences)
        if properties == None:
            properties = {'tokenize.whitespace':'true'}
        return [sentences[0] for sentences in self.raw_tag_sents(sentences, properties)]

    def tag(self, sentence, properties=None):
        """"""
        Tag a list of tokens.

        :rtype: list(tuple(str, str))

        >>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
        >>> tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
        >>> parser.tag(tokens)
        [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
        ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]

        >>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
        >>> tokens = ""What is the airspeed of an unladen swallow ?"".split()
        >>> parser.tag(tokens)
        [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),
        ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),
        ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
        """"""
        return self.tag_sents([sentence], properties)[0]

    def raw_tag_sents(self, sentences, properties=None):
        """"""
        Tag multiple sentences.

        Takes multiple sentences as a list where each sentence is a string.

        :param sentences: Input sentences to tag
        :type sentences: list(str)
        :rtype: list(list(list(tuple(str, str)))
        """"""
        default_properties = {'ssplit.isOneSentence': 'true',
                              'annotators': 'tokenize,ssplit,' }

        default_properties.update(properties or {})

        # Supports only 'pos' or 'ner' tags.
        assert self.tagtype in ['pos', 'ner']
        default_properties['annotators'] += self.tagtype
        for sentence in sentences:
            tagged_data = self.api_call(sentence, properties=default_properties)
            yield [[(token['word'], token[self.tagtype]) for token in tagged_sentence['tokens']]
                    for tagged_sentence in tagged_data['sentences']]
```

That should enforce the list of string tokens input by the users. 

Details on https://stackoverflow.com/questions/52250268/why-do-corenlp-ner-tagger-and-ner-tagger-join-the-separated-numbers-together

If we allow the `.tag()` to overload the properties before the `raw_tag_sents`, that'll also allow users to easily handle cases like #1876 ",,,,1,,,,,,
505,https://github.com/nltk/nltk/issues/2134,2134,[],closed,2018-10-01 13:47:36+00:00,,2,WordNetError when loading lemmas with numeric values >= 10,"Since at least NLTK 3.3 running the below code  results in the error shown below.

```python
>>> from nltk.corpus import wordnet as wn
>>> wn.lemma('jump.v.11.jump')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/fabian/venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py"", line 1272, in lemma
    raise WordNetError('no lemma %r in %r' % (lemma_name, synset_name))
nltk.corpus.reader.wordnet.WordNetError: no lemma '.jump' in 'jump.v.1'
```

I have reproduced it inside a clean venv with the latest nltk commit (https://github.com/nltk/nltk/commit/b991f244558154d09041d5a62b0c0e55faaab802 at the time of writing) on ubuntu 16.04 with python 3.6.5 and verified this bug does not happen with NLTK version 3.2.2

Based on experimentation I believe this bug affects all lemmas where the number XX (as in  jump.v.XX.jump) is 10 or above.",,,,1,,,,,,
544,https://github.com/nltk/nltk/issues/2212,2212,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2019-01-09 02:01:25+00:00,,4,Error loading home: Package 'home' not found in index and nltk.data.path becomes empty,"I want to use `nltk.pos_tag`, and I have downloaded `punkt` and `averaged_perceptron_tagger`. My codes work fine on MacOS. Then I copy my `nltk_data` folder to another linux and configure the nltk.data.path. When I run the same codes again, the `nltk.word_tokenize` can work fine but `nltk.post_tag` triggers an error 
```
LookupError:
**********************************************************************
  Resource \u001b[93mhome\u001b[0m not found.
  Please use the NLTK Downloader to obtain the resource:
  \u001b[31m>>> import nltk
  >>> nltk.download('home')
  \u001b[0m
  Attempted to load \u001b[93m/home/admin/work/nltk_data.zip/nltk_data/taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m
  Searched in:
    - u''
**********************************************************************
```
However, when I try to run `nltk.download('home')`, I have another error:
```
Error loading home: Package 'home' not found in index
```
And I feel very strange the search path becomes empty.
Anyone can give me some suggestions?",,,,1,,,,,,
608,https://github.com/nltk/nltk/issues/2308,2308,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2019-05-24 00:07:14+00:00,,3,"Error Downloading Punkt ""W/python.stderr: [nltk_data] Error loading punkt: <urlopen error [Errno 7] No address     [nltk_data]     associated with hostname>""","I am using the module chaquopy to use python on android and am trying to install nltk dependencies. Upon calling the nltk.download(""punkt"") I get the following error:

```
W/python.stderr: [nltk_data] Error loading punkt: <urlopen error [Errno 7] No address
    [nltk_data]     associated with hostname>
```

I was unable to find an error matching here or on stackoverflow.",,,,1,,,,,,
758,https://github.com/nltk/nltk/issues/2591,2591,[],closed,2020-08-31 08:14:52+00:00,,0,Error in downloading ,"![WhatsApp Image 2020-08-31 at 12 46 26 PM](https://user-images.githubusercontent.com/65659281/91697813-5c52c380-eb8f-11ea-8e2b-1986ab534cfa.jpeg)
Trying to download stopwards from nltk , I used
import nltk
nltk.download()
I'm getting this error how to resolve this?",,,,1,,,,,,
661,https://github.com/nltk/nltk/issues/2421,2421,[],open,2019-09-26 05:44:49+00:00,,1,Cannot get WordNet synsets for English without lemmatization,"Querying wordnet with `wordnet.synsets()` will lemmatize the query word, but only for English. While this is useful for many applications, sometimes I do not want such lemmatization. For instance, I have dictionary forms for multiple languages (from, e.g., Swadesh lists) and I want to detect differences in polysemy between languages, but the lemmatization inflates the apparent polysemy for English. There appears to be no way (in the public API) to do an English query without lemmatization.

For example:

```python
>>> wn.synsets('eyeglasses')
[Synset('spectacles.n.01'), Synset('monocle.n.01')]
>>> wn.synsets('eyeglasses')[0].lemma_names()
['spectacles', 'specs', 'eyeglasses', 'glasses']
>>> wn.synsets('eyeglasses')[1].lemma_names()
['monocle', 'eyeglass']
```

The second synset (`monocle.n.01`) was found because 'eyeglass' appears in its lemmas, but not 'eyeglasses', which is only in the first synset. Sometimes specifying the POS can help, as with 'scissors' and 'scissor.v.01', but not always (as with 'eyeglasses' above, both are 'n'). I end up needing to write a wrapper like this:

```python
def synsets(lemma, pos=None, lang='eng', check_exceptions=True):
    results = wn.synsets(lemma,
                         pos=pos,
                         lang=lang,
                         check_exceptions=check_exceptions)
    if lang == 'eng':
        results = [ss for ss in results if lemma in ss.lemma_names()]
    return results
```

Am I missing something or is this currently the best way around the issue?",,1,1,,,,,,,
209,https://github.com/nltk/nltk/issues/1641,1641,[],closed,2017-02-28 18:43:03+00:00,,0,Non-English lemmas containing capital letters cannot be looked up using wordnet.lemmas() or wordnet.synsets(),"This is an existing bug that I stumbled across while using the German WordNet from the EOMW via my custom-WordNet loading code in my unmerged PR at https://github.com/nltk/nltk/pull/1621. (It's dramatically more serious for German, since all nouns in German are capitalised and so a huge fraction of the language doesn't work, but it also affects the existing OMW WordNets with support built into NLTK.)

Consider the synset representing London, England. While the synset name is in lowercase, its lemmas are capitalised in both the English WordNet...

```
>>> london_synset = wn.synset('london.n.01')
>>> london_synset.definition()
'the capital and largest city of England; located on the Thames in southeastern England; financial and industrial and cultural center'
>>> london_synset.lemmas()
[Lemma('london.n.01.London'), Lemma('london.n.01.Greater_London'), Lemma('london.n.01.British_capital'), Lemma('london.n.01.capital_of_the_United_Kingdom')]
```

... and also in the French WordNet:

```
>>> london_synset.lemmas(lang='fra')
[Lemma('london.n.01.Grand_Londres'), Lemma('london.n.01.Hellgate:_London'), Lemma('london.n.01.London'), Lemma('london.n.01.Londres')]
```

But when using the English WordNet, I can look up the synset (or an individual `Lemma`) by lemma by passing in 'London' in whatever capitalisation I like:

```
>>> wn.synsets('London')
[Synset('london.n.01'), Synset('london.n.02')]
>>> wn.synsets('london')
[Synset('london.n.01'), Synset('london.n.02')]
>>> wn.synsets('lOnDoN')
[Synset('london.n.01'), Synset('london.n.02')]
>>> wn.lemmas('london')
[Lemma('london.n.01.London'), Lemma('london.n.02.London')]
>>> wn.lemmas('London')
[Lemma('london.n.01.London'), Lemma('london.n.02.London')]
>>> wn.lemmas('LoNdoN')
[Lemma('london.n.01.London'), Lemma('london.n.02.London')]
```

In non-English, on the other hand, it is impossible to look up this synset by lemma, because the first line of `wn.synsets()` coerces the `lemma` passed in to lowercase, and that lemma is then used as a key to look up the synset in a lemma-to-synset dictionary in which `Londres` is *capitalised*.

```
>>> wn.synsets('Londres', lang='fra')
[]
>>> wn.synsets('londres', lang='fra')
[]
>>> wn.lemmas('londres', lang='fra')
[]
>>> wn.lemmas('Londres', lang='fra')
[]
```

(Contrast this with lemmas that are lowercased in the French WordNet's tab file; they can be looked up regardless of how the `lemma` passed to `synsets()` is capitalised:

```
>>> wn.synsets('calin', lang='fra')
[Synset('cuddlesome.s.01')]
>>> wn.synsets('cAlIn', lang='fra')
[Synset('cuddlesome.s.01')]
```

)

To match the English behaviour, the behaviour of `synsets()` for non-English WordNets should be adjusted so that the lookup is properly case-insensitive. This was probably the intent of coercing the given `lemma` to lowercase before doing the lookup, but fails if the `Lemma` to be looked up is spelt with a capital letter in the actual WordNet data.",,,1,,,,,,,
305,https://github.com/nltk/nltk/issues/1801,1801,[],closed,2017-08-02 23:31:10+00:00,,1,'EnglishStemmer' has no attribute 'stematize',"So I'm trying to use the `SnowballStemmer` and I'm getting this error. From the Python 3.6.2 REPL:

```python
>>>from nltk.stem import SnowballStemmer
>>>stemmer = SnowballStemmer('english')

Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/virtualenvpath/python3.6/site-packages/nltk/stem/snowball.py"", line 93, in __init__
    self.stem = self.stemmer.stematize
AttributeError: 'EnglishStemmer' object has no attribute 'stematize'
```

I've tried with spanish, german and danish at it throws the same error with the corresponding stemmer class.

This is taken right from the NLTK Docs page. Is this a bug or I am missing something? ",,,1,,,,,,,
474,https://github.com/nltk/nltk/issues/2089,2089,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-08-21 06:02:51+00:00,,0,`wordnet.ic()` does not support non-English corpus,"I was trying to creates an information content lookup dictionary from a Japanese corpus.(KNBC)
```python
import nltk
nltk.download('wordnet')
nltk.download('knbc')
nltk.download('omw')

from nltk.corpus import wordnet as wn

# I had to HACK the 'lang' param into nltk source code, to support Japanese 
knbc_ic = wn.ic(knbc, False, 0.0, lang='jpn')
```
the bug rooted here (should be `self.synsets(ww, lang=lang)` ):
 
https://github.com/nltk/nltk/blob/378fee689b493edc197282444efe8300a936db79/nltk/corpus/reader/wordnet.py#L1907",,,1,,,,,,,
713,https://github.com/nltk/nltk/issues/2517,2517,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2020-03-21 21:04:41+00:00,,0,"Currently, NLTK pos_tag only supports English and Russian (i.e. lang='eng' or lang='rus')","Can anyone know that how to address this issue? btw, I have update the nltk package, thanks in advance.",,,1,,,,,,,
9,https://github.com/nltk/nltk/issues/1258,1258,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-01-20 10:33:09+00:00,,1,Potential wordnet lemmatization issue,"I am not sure if its an issue with wordnet corpus or with nltk or my way of using it, but I've found one case where I am not getting expected result:
~/nltk_data/corpora/wordnet/noun.exc contain this line:
antae anta

where (according to wikipedia) anta is singular and antae plural.
yet this code:

from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()
print(wnl.lemmatize('antae'))

prints antae, not anta.
same goes for antalkalies -> antalkali
but antefixa -> antefix works fine.

Is it a bug (and where?) or I am wrong expecting it to be lemmatized this way?
",,1,,,,,,,,
173,https://github.com/nltk/nltk/issues/1575,1575,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 25396640, 'node_id': 'MDU6TGFiZWwyNTM5NjY0MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/book', 'name': 'book', 'color': 'e102d8', 'default': False, 'description': None}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-01-03 12:42:21+00:00,,8,stem_word and lower case stem output in v3.2.2,"Hi,

Just wanted to confirm the following things affected by the recent upgrade to v3.2.2:
- There is no `stem_word` function in `PorterStemmer()`. I had to replace it with `stem()`
- Unlike before, `stem` returns lower case of a word e.g. `stem` for `Stemming` (http://text-processing.com/demo/stem/ is the same as before).

Cheers,
Ehsan",,1,,,,,,,,
183,https://github.com/nltk/nltk/issues/1600,1600,[],closed,2017-01-20 18:40:40+00:00,,5,"3.2.2 stemming ""oed"" crashes","```
from nltk.stem.porter import *
stemmer = PorterStemmer()
stemmer.stem(""oed"")
```

--> crash

The problem is a vowel suffixed by ""ed"" or ""ing"". It works in 3.2.1

python version: 3.5.2",,1,,,,,,,,
214,https://github.com/nltk/nltk/issues/1648,1648,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-03-10 05:18:04+00:00,,4,Porting several stemming algorithms to NLTK,"`whoosh` has a couple of great things that we can port to NLTK, other than the porter stemmer, there are:

 - [Paice-Husk stemming ](https://bitbucket.org/mchaput/whoosh/src/e344fb64067e45d47ec62dc65a75a50be51264a7/src/whoosh/lang/paicehusk.py?at=default&fileviewer=file-view-default) 
 - [Lovin stemmer](https://bitbucket.org/mchaput/whoosh/src/e344fb64067e45d47ec62dc65a75a50be51264a7/src/whoosh/lang/lovins.py?at=default&fileviewer=file-view-default)

This will be an easy port since the original implementation is in Python. Simply port the code, write some tests and document the functions appropriately, put the module in `nltk/nltk/stem/` and do a pull-request =)

Any takers?",,1,,,,,,,,
255,https://github.com/nltk/nltk/issues/1724,1724,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2017-05-18 01:03:57+00:00,,3,Better WordNet Reader,"This is a proposal to improve the WordNet interface in terms of code quality, functionalities and speed. Please feel free to add to this issues about current wordnet reader in `nltk` and how and what should be improved. 

-----

Suggestions / Issues
====

- Are there still issues with the [`_morphy()` function](https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L68)?  c.f. http://stackoverflow.com/questions/33594721/why-nltk-lemmatization-has-wrong-output-even-if-verb-exc-has-added-right-value

- What is the best params settings for `lowest_common_subsumers` in `wup_similarity`? See https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L899 

- Should the list of exceptions in the Princeton WordNet be extended? c.f. http://stackoverflow.com/questions/22999273/python-nltk-lemmatization-of-the-word-further-with-wordnet 

- [Add in the option to manually add a new root node; this will be useful for verb similarity as there exist multiple verb taxonomies](https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1946)

- Allow direct offset + pos query, i.e. `wn.synset('dog.n.01') == wn.synset('2084071-n')`.

- Pre-compute similarities for all Synsets. 
  - We know that the Information Content (IC) based similarities scores should have been frozen by now since any new WordNet updates/extensions won't affect it so it's possible to precompute these and save them. That'll speed up the similarity matching quite drastically. 
  - As for the path related similarities, they would change with WordNet versions but its nice to still have them pre-computed respective to the WordNet versions from 3.0 onwards.



Code 
====

- Would `float('inf')` be a better approach than `_INF = 1e300` at https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L68 ? 

- Should encoding be a ""set-able"" parameter for the WordNetCorpusReader? https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1052  

- Should imports be at the top of the script unless it's an `@abstractmethod` accessible from outside of the `CorpusReader` object? e.g.   
  - At `langs()` https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1145
  - At `closure()`  https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L537

- Proper indentation should be preferred for `try-except`/`if-else` and one-liner shortcuts, e.g. https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1165

- `get_version()` should be called once at initialization and not be repeated called at `get_root()` https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L423
",,1,,,,,,,,
287,https://github.com/nltk/nltk/issues/1778,1778,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}]",open,2017-07-13 23:14:41+00:00,,5,"PorterStemmer seems to be stemming ""this"" -> ""thi""","I'm not sure whether it's the expected output but NLTK PorterStemmer is giving different output as compared to https://pypi.python.org/pypi/stemming/1.0

From NLTK:

```python
>>> from nltk.stem import PorterStemmer
>>> porter = PorterStemmer()
>>> porter.stem('this')
u'thi'
```

From `stemming`

```python
>>> from stemming.porter2 import stem
>>> stem('this') 
'this'
```",,1,,,,,,,,
736,https://github.com/nltk/nltk/issues/2550,2550,[],closed,2020-06-02 03:21:10+00:00,,2,"WordNetLemmatizer replaces ""does"" with ""doe""","Hello,
I'm using `WordNetLemmatizer` to make the word lemmatization. My corpus contains a big number of occurrences of word ""does"". However, `WordNetLemmatizer` replaces all of them with ""doe"". I'm using it as follows:
```
from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatized_comments = [[lemmatizer.lemmatize(w) for w in c] for c in tqdm(prepared_comments)]
```
where `prepared_comments` is a list of tokenized word sequences. Am I using the `WordNetLemmatizer` wrong, or is it a bug of `WordNetLemmatizer`?",,1,,,,,,,,
822,https://github.com/nltk/nltk/issues/2718,2718,[],closed,2021-05-28 06:52:23+00:00,,2,Error when using nltk.stem.arlstem2 module,"I was trying to use the stemming function in arlstem2 object and always found an error that there is no object called adject but, i tried to fix adject and replaced it with adjective in the source code it worked perfectly.

I just wanted to mention that it works for me :)

![SharedScreenshot](https://user-images.githubusercontent.com/64975785/119942357-02522380-bf92-11eb-98ee-0fbfe7d8d5a1.jpg)
",,1,,,,,,,,
2,https://github.com/nltk/nltk/issues/1250,1250,[],closed,2016-01-13 10:30:10+00:00,,4,Tokenization error,"There is an error of processing dots in the tokenizer:
The following code

```
    val ptbt = new PTBTokenizer(
      new StringReader(""Tokenization is performed.Parameters can be specified.""),
      new CoreLabelTokenFactory(), """")
    while (ptbt.hasNext()) {
      val label = ptbt.next()
      val w = label.originalText()
      println(w)
    }
```

gives ""performed.Parameters"" as an word. If I put a space after 'performed' it will work, but still, natural text can be messy and omitting spaces after punctuation is very common.
",1,,,,,,,,,
129,https://github.com/nltk/nltk/issues/1482,1482,[],closed,2016-10-17 10:05:18+00:00,,1,Text tilling,"Can I use text tilling for searching boundaries of sentences among continuous text without punctuation? What is then supposed to be a paragraph?
",1,,,,,,,,,
152,https://github.com/nltk/nltk/issues/1523,1523,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 159235791, 'node_id': 'MDU6TGFiZWwxNTkyMzU3OTE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/dependency%20parsing', 'name': 'dependency parsing', 'color': 'eb6420', 'default': False, 'description': None}]",closed,2016-11-24 09:28:36+00:00,,2,whitespace in corenlp_options in nltk.parse.stanford causes OSError,"I needed to keep punctuation, so I added corenlp_options='-ouputFormatOptions includePunctuationDependencies' when creating an instance of a dependency parser.

Because corenlp_options is appended to cmd, the Popen fails.  Changing `cmd.append(self.corenlp_options)` to `cmd.extend(self.corenlp_options.split())` would fix the issue.

Edit:
The Neural Dependency Parser has a default corelnp_options that contains whitespace, and it works fine.  note that because it uses += to extend, that there is no whitespace inserted between user-defined options and default options.  

I don't know why NDP runs fine with whitespace in corenlp_options and StanfordDependencyParser does not.",1,,,,,,,,,
552,https://github.com/nltk/nltk/issues/2222,2222,[],open,2019-01-31 00:54:48+00:00,,0,TweetTokenizer and punctuation inside URLs,"The twitter tokenizer exhibits (what I think is) undesirable behavior when tokenizing URLs. For example:
```
tok.tokenize('http://t.co/LYsklSmIVS 鈥渉ttp://t.co/LYsklSmIVS鈥?鈥渉ttp://t.co/LYsklSmIVS鈥漻xx')
```
yields
```
['http://t.co/LYsklSmIVS',  '鈥?,  'http://t.co/LYsklSmIVS',  '鈥?,  '鈥?,  'http://t.co/LYsklSmIVS鈥漻xx']
```
where
```
['http://t.co/LYsklSmIVS',  '鈥?,  'http://t.co/LYsklSmIVS',  '鈥?,  '鈥?,  'http://t.co/LYsklSmIVS',  '鈥?,  'xxx']```
is what I would prefer.

The issue is that the regular expression used to tokenize URLs looks for the longest substring that could be an URL, and technically most punctuation marks can occur inside an URL. The regex does make an exception for a single punctuation mark at the end of an URL before a word break, but that doesn't help if there's a space missing before the next token.

The current URL matcher is, in my opinion, too greedy for working with casual online texts.  While it's legally possible for an URL to have a character like `鈥漙 in the middle of it, it's much more likely that `鈥漙 ought to be split off as a separate token.

In a way, parsing URLs in tweets should be trivial (because they all take the form `http://t.co/...`) and there's no real need for any fancy URL-matching regex.  But, on the other hand, twitter might change their URL format at any time and people might be using this tagger for parsing texts from other sites, so we don't want to make too many assumptions.

Any thoughts about the best way to handle this?  ",1,,,,,,,,,
604,https://github.com/nltk/nltk/issues/2303,2303,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2019-05-21 13:51:29+00:00,,1,TreebankWordTokenizer [BUG],"Looks like there's a broken regexp on Treebank's `PUNCTUATION` patterns:

```
([^\.])(\.)([\]\)}>""\']*)\s*$
```
should `""` be escaped?
",1,,,,,,,,,
666,https://github.com/nltk/nltk/issues/2431,2431,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-10-18 10:22:29+00:00,,0,punctuation of plain text,I have plain text in lower case German language without any stop or comma or any punctuation. How to get back the punctuation and stop of sentences? to get the text into sentence structure. I tried but unsuccessful. Please guide. ,1,,,,,,,,,
678,https://github.com/nltk/nltk/issues/2449,2449,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-10-31 16:04:57+00:00,,0,Arabic Text Normalization,"I have plain text in the Arabic language with stops, commas, punctuations, and diacritics. How to get the clean text by removing the stops, commas, punctuations, and diacritics from the input texts.
Example:  input = "".廿賳 丕賱賯乇丕亍 賷賯乇丐賵賳 丕賱賯乇丌賳 賯乇丕亍丞 噩賲賷賱丞"" 
                 output = ""丕賳 丕賱賯乇丕亍 賷賯乇亍賵賳 丕賱賯乇丕賳 賯乇丕亍丞 噩賲賷賱丞""",1,,,,,,,,,
686,https://github.com/nltk/nltk/issues/2460,2460,[],closed,2019-11-14 14:23:29+00:00,,6,Danish sentence tokenizer fails to split on newline,"When sentences are terminated with a newline rather than .!? the tokenizer fails to split. If it's wrong in Danish, it is probably wrong in a bunch of other languages. 

nltk version 3.4.5
Python version 3.6.9
Ubuntu Linux 18.04

```python
import nltk 
sent_tokenizer = nltk.data.load('tokenizers/punkt/danish.pickle')
text = """"""Den normale kropstemperatur er 37,0潞C, n氓r den m氓les i endetarmen; temperaturen er lavere, n氓r den m氓les i f.eks. 酶ret eller munden
Feber defineres som kernetemperatur over 38掳C m氓lt i endetarmen
De allerfleste sygdomme med feber er harml酶se og helbreder sig selv. Det g忙lder f.eks. fork酶lelser og andre virussygdomme
I nogle tilf忙lde kan feberen skyldes alvorlige infektioner, som f.eks. hjernehindebet忙ndelse (meningitis), blodforgiftning (sepsis) og nyreb忙kkenbet忙ndelse (pyelonefritis)
""""""
print(sent_tokenizer.tokenize(text))
```

```
['Den normale kropstemperatur er 37,0潞C, n氓r den m氓les i endetarmen; temperaturen er lavere, n氓r den m氓les i f.eks. 酶ret eller munden\nFeber defineres som kernetemperatur over 38掳C m氓lt i endetarmen\nDe allerfleste sygdomme med feber er harml酶se og helbreder sig selv.', 'Det g忙lder f.eks. fork酶lelser og andre virussygdomme\nI nogle tilf忙lde kan feberen skyldes alvorlige infektioner, som f.eks. hjernehindebet忙ndelse (meningitis), blodforgiftning (sepsis) og nyreb忙kkenbet忙ndelse (pyelonefritis)']
```",1,,,,,,,,,
689,https://github.com/nltk/nltk/issues/2471,2471,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-12-03 12:48:14+00:00,,0,same word but differnnet tokenizer,"hi锛寃hen I processed my data锛?I find the same word may get different tokenizer.
for example锛宼he word锛?<can't>, may get  锛?can>  <'t> or <ca> <n't>.
Could you tell me the reason, thank you very much.",1,,,,,,,,,
722,https://github.com/nltk/nltk/issues/2526,2526,[],closed,2020-04-05 16:39:22+00:00,,1,AttributeError: 'StringTokenizer' object has no attribute '_string',"I was trying to tokenize a doc using nltk.tokenize.api.StringTokenizer. (I've managed to tokenize it with word_tokenizer, so no need for solutions)

Is this normal behaviour?
```python
text1
Out[5]: ""event information speakers alison shafer customer success manager, millward brown digital sarah friedman customer success manager, millward brown digital for retail brands, the holiday season is already kicking into gear. from various traffic patterns, to keyword searches and conversion rates, having a holistic understanding of your strengths, weaknesses, and how your competitors fare is crucial to optimizing and improving your digital strategy. with compete pro, you can do just that this holiday season. during this webinar we鈥檒l use retail as an example to walk through the search market share feature and its benefits in monitoring keywords that spark consumer interest to you and your competitors. we'll also help you understand industry conversion rates, and how compete pro can help you monitor referral and incoming/outgoing traffic sources driving engagement on your site. in doing so, you can accurately benchmark and analyze your effectiveness this holiday season.""
tokenizer = nltk.tokenize.api.StringTokenizer()
tokenizer.tokenize(text1)

Traceback (most recent call last):
  File ""C:\Users\Cristina\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3296, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-7-838af32942bb>"", line 1, in <module>
    tokenizer.tokenize(text1)
  File ""C:\Users\Cristina\Anaconda3\lib\site-packages\nltk\tokenize\api.py"", line 74, in tokenize
    return s.split(self._string)
AttributeError: 'StringTokenizer' object has no attribute '_string'
```",1,,,,,,,,,
731,https://github.com/nltk/nltk/issues/2543,2543,[],open,2020-05-17 16:19:48+00:00,,5,NLTK Sentence tokenizer does not tokenize properly if there exists 'e.g.' or 'i.e.' in sentence.,"Like i have sentence:
'The first approach, single-molecule simulation, taken by the StochSim simulator, tracks individual molecules and their state (e.g., what other molecules they are bound to) so that only the complexes formed at any given time are enumerated (and not all possible complexes) [11].'

The sentence tokenizer splits this sentence and gives me following two sentences;
The first approach , single-molecule simulation , taken by the StochSim simulator , tracks individual molecules and their state ( e.g.

and

, what other molecules they are bound to ) so that only the complexes formed at any given time are enumerated ( and not all possible complexes ) [ 11 ] .

How can this be resolved?

",1,,,,,,,,,
