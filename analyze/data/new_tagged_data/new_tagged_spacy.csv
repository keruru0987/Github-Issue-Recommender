,html_url,number,labels,state,created_at,pull_request,comments,title,body,rel1,rel2,rel3,rel4,rel5,rel6,rel7,rel8,rel9,rel10
3,https://github.com/explosion/spaCy/issues/2783,2783,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 710446668, 'node_id': 'MDU6TGFiZWw3MTA0NDY2Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/training', 'name': 'training', 'color': '087EA6', 'default': False, 'description': 'Training and updating models'}]",closed,2018-09-20 15:34:12+00:00,,4,Very high losses when adding training samples to NER model,"I have an en model trained for a custom set of entities. Sample size >3000 items across about 10-15 entities. Results are fair to good in use...When attempting to add new annotations to this model, I am getting some alarming results...losses exceed 300k and increase each iteration. Documents can be quite large...

I am using the ner_training code found in ""examples"" as is with the only change being a call to db to generate training data.  Running in a linux vm, ubuntu 18.04.  Using CPU, not GPU because I cannot get GPU working through vm and windows GPU won't compile...

Please help me understand if these very high losses are expected. They are 2+ orders of magnitude larger than the ones observed during model training from blank

if not model and output_dir:
        model=output_dir

    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank('en')  # create blank Language class
        print(""Created blank 'en' model"")

    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)
    else:
        ner = nlp.get_pipe('ner')

    TRAIN_DATA = load_ner_data()
    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = ner.create_optimizer()
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            losses = {}
            for text, annotations in TRAIN_DATA:
                nlp.update(
                    [text],  # batch of texts
                    [annotations],  # batch of annotations
                    drop=0.5,  # dropout - make it harder to memorise data
                    sgd=optimizer,  # callable to update weights
                    losses=losses)
            print(losses)
",,,,,,,,,,1
28,https://github.com/explosion/spaCy/issues/2824,2824,"[{'id': 514165920, 'node_id': 'MDU6TGFiZWw1MTQxNjU5MjA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20en', 'name': 'lang / en', 'color': '726DA8', 'default': False, 'description': 'English language data and models'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-10-05 18:51:31+00:00,,6,Replicating benchmark NER results with Ontonotes 5,"# On replicating benchmark NER results with Ontonotes 5

I would like to replicate the results of the spaCy pre-trained NER model on the English portion of the Ontonotes 5 corpus (the spaCy documentation says I should get ~ 85% F1), but I could not find any details on how to replicate it exactly.

1. Which test set was used for the evaluation? I followed the instructions at http://cemantix.org/data/ontonotes.html, but there are two different test sets (""Test"" and ""CoNLL-2012 Test""). Also, were all subcorpora used, or was the pivot text (subdirectory `pt`, consisting of text from Old and New Testaments) removed? (since this does not have entity labels)

2. I am currently extracting entities one sentence at a time (e.g. doc = nlp(sentence_text); ents = doc.ents). Is this the right way to use spaCy's NER, or am I missing something? Doing it this way, I'm getting about 74% micro-F1 (using spaCy 2.0.12, with model en-core-web-lg 2.0.0).

Thanks!

## Which page or section is this issue related to?
https://spacy.io/usage/facts-figures#ner-accuracy-ontonotes5
",,,,,,,,,,1
56,https://github.com/explosion/spaCy/issues/2858,2858,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 710446668, 'node_id': 'MDU6TGFiZWw3MTA0NDY2Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/training', 'name': 'training', 'color': '087EA6', 'default': False, 'description': 'Training and updating models'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-10-17 03:22:11+00:00,,2,NER can't get any entity after training iterations,"Hi
Really great tool~ But i do get a problem when trying to extract information from some short 'Software label' phrases. I am using [spaCy example code](https://github.com/explosion/spacy/blob/master/examples/training/train_new_entity_type.py) with 7 examples (below). Started with a `blank` 'en' model.

With 1 iteration, I get following entities. Not only they are wrong (acceptable), but they are duplicated also.
```bash
for entity in nlp('Office SharePoint CAL 2007 Portuguese MVL Device CAL').ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)
Office SharePoint 0 17 LANGUAGE
Office SharePoint 0 17 LANGUAGE
```

Then I tried examples with 10 iterations. I suppose the NER model should be over fitting enough. But this time same code/string returns 0 entities.

Could you give some suggestions on this?

Also i created `word2vec` vectors with `Gensim` and imported into my spaCy blank models. But it seems it doesn't help. Could you give some info about how spaCy uses imported vectors? Only on similarity or tagging/NER also?

 I saw [your video](https://www.youtube.com/watch?v=sqDHBH9IjRU) and my understanding is you are using your own embedding (not word2vec or any others) and CNN to create NER model. How spaCy label unlabeled Span (like Office SharePoint) based on the model?

Thanks!!! 

#### Training data
```python
TRAIN_DATA = [
    ('Office SharePoint CAL 2007 Dutch MVL User CAL', {
        'entities': [(0, 16, 'PRODUCT'), (27, 31, 'LANGUAGE')]
    }),
    ('Office SharePoint CAL 2007 French MVL Device CAL', {
        'entities': [(0, 16, 'PRODUCT'), (27, 32, 'LANGUAGE')]
    }),
    ('Office SharePoint CAL 2007 Finnish MVL Device CAL', {
        'entities': [(0, 16, 'PRODUCT'), (27, 33, 'LANGUAGE')]
    }),
    ('Office SharePoint CAL 2007 Greek MVL Device CAL', {
        'entities': [(0, 16, 'PRODUCT'), (27, 31, 'LANGUAGE')]
    }),
    ('Office SharePoint CAL 2007 Italian MVL Device CAL', {
        'entities': [(0, 16, 'PRODUCT'), (27, 33, 'LANGUAGE')]
    }),
    ('Office SharePoint CAL 2007 Norwegian MVL Device CAL', {
        'entities': [(0, 16, 'PRODUCT'), (27, 35, 'LANGUAGE')]
    }),
    ('Office SharePoint CAL 2007 Portuguese MVL Device CAL', {
        'entities': [(0, 16, 'PRODUCT'), (27, 36, 'LANGUAGE')]
    }),   
]
```
## Your Environment
## Info about spaCy

* **spaCy version:** 2.0.12
* **Platform:** Linux-3.10.0-862.3.2.el7.x86_64-x86_64-with-centos-7.5.1804-Core
* **Python version:** 3.6.5
* **Models:** en
",,,,,,,,,,1
73,https://github.com/explosion/spaCy/issues/2882,2882,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-10-29 08:36:06+00:00,,3,is there a way with spaCy's NER to calculate metrics per entity type?,"Hello there,
I would like to ask if there is a way in the NER model in spaCy to extract the metrics (precision, recall, f1 score) per entity type?

Something that will look like this:

         precision    recall  f1-score   support
  B-LOC      0.810     0.784     0.797      1084
  I-LOC      0.690     0.637     0.662       325
 B-MISC      0.731     0.569     0.640       339
 I-MISC      0.699     0.589     0.639       557
  B-ORG      0.807     0.832     0.820      1400
  I-ORG      0.852     0.786     0.818      1104
  B-PER      0.850     0.884     0.867       735
  I-PER      0.893     0.943     0.917       634
avg / total 0.809 0.787 0.796 6178

Above example taken from: http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/

I have tried to post on stackoverflow as well but had no luck, apologies if this is not the right place to post this question or maybe a usage tag should be used if Im missing something in the documentation on how to do it.
https://stackoverflow.com/questions/52856057/is-there-a-way-with-spacys-ner-to-calculate-metrics-per-entity-type
Thank you!

## Your Environment
## Info about spaCy

* **spaCy version:** 2.0.12
* **Platform:** Linux-4.4.0-17134-Microsoft-x86_64-with-Ubuntu-18.04-bionic
* **Python version:** 3.6.6
* **Models:** en_core_web_lg
* Environment Information: I am using Ubuntu subsystem in the windows 10.
",,,,,,,,,,1
83,https://github.com/explosion/spaCy/issues/2895,2895,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 710446668, 'node_id': 'MDU6TGFiZWw3MTA0NDY2Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/training', 'name': 'training', 'color': '087EA6', 'default': False, 'description': 'Training and updating models'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}]",closed,2018-11-02 08:28:41+00:00,,4,Spacy : 2.1.0a1  : Customized NER model which is loaded from disk is not giving any detection of customized entity ?,"    spaCy version      2.1.0a1        
    Location           /home/prashant/.local/lib/python3.6/site-packages/spacy
    Platform           Linux-4.15.0-36-generic-x86_64-with-Ubuntu-18.04-bionic
    Python version     3.6.6          
    Models             en   

@honnibal @ines 

Hi ,
I already went through https://github.com/explosion/spaCy/issues/1337 but....doesn't work.
So after that I am writing this issue

one difference I noticed between en_core_web_sm and to_disk generated model is there is no tagger and parser folder.

When I am evaluating trained model in the same code it shows proper results for customized named entity  recognition. But when I am loading saved model from disk it shows nothing for the same test data that I used after just training to evaluate.


I tried with both technique **from disk()** and **spacy.load()**

Here  I am attaching my generated model in **zip** file.


nlp = spacy.blank('en').from_disk('/home/prashant/sales_agreemt/binary_models/en_CLOSING_DATE_EXTRACTION-0.0.0/en_CLOSING_DATE_EXTRACTION/en_CLOSING_DATE_EXTRACTION-0.0.0/')

[en_CLOSING_DATE_EXTRACTION-0.0.0.zip](https://github.com/explosion/spaCy/files/2541603/en_CLOSING_DATE_EXTRACTION-0.0.0.zip)

#nlp = spacy.load('/home/prashant/sales_agreemt/binary_models/en_CLOSING_DATE_EXTRACTION-0.0.0/en_CLOSING_DATE_EXTRACTION/en_CLOSING_DATE_EXTRACTION-0.0.0/l')

examples : 

with training and testing in same code

testing sample: _The closing of the sale will be on or before July 19 , 2018 , or within 7 days after objections made._

**Results :** closing_date :  July 19 , 2018 


with loading model from disk

testing sample: _The closing of the sale will be on or before July 19 , 2018 , or within 7 days after objections made._

**Results :** closing_date :  **NULL**   





both  Techniques **spacy.load** and **from disk()** failed

nlp = spacy.blank('en').from_disk('/home/prashant/sales_agreemt/binary_models/en_CLOSING_DATE_EXTRACTION-0.0.0/en_CLOSING_DATE_EXTRACTION/en_CLOSING_DATE_EXTRACTION-0.0.0/')
#nlp = spacy.load('/home/prashant/sales_agreemt/binary_models/en_CLOSING_DATE_EXTRACTION-0.0.0/en_CLOSING_DATE_EXTRACTION/en_CLOSING_DATE_EXTRACTION-0.0.0/ner/model')

",,,,,,,,,,1
164,https://github.com/explosion/spaCy/issues/3015,3015,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}, {'id': 710446668, 'node_id': 'MDU6TGFiZWw3MTA0NDY2Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/training', 'name': 'training', 'color': '087EA6', 'default': False, 'description': 'Training and updating models'}]",closed,2018-12-06 07:26:08+00:00,,2,NER Training  on spark,"## Feature description
Need to train custom entities for NER model using Spark as my Data is huge , Is there any way to use spark to create a NER model 


",,,,,,,,,,1
206,https://github.com/explosion/spaCy/issues/3081,3081,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 496348994, 'node_id': 'MDU6TGFiZWw0OTYzNDg5OTQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/%F0%9F%8C%99%20nightly', 'name': '馃寵 nightly', 'color': 'ffffff', 'default': False, 'description': 'Discussion and contributions related to nightly builds'}, {'id': 710446668, 'node_id': 'MDU6TGFiZWw3MTA0NDY2Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/training', 'name': 'training', 'color': '087EA6', 'default': False, 'description': 'Training and updating models'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2018-12-21 10:01:29+00:00,,9,NER Model retrain failed,"Hello.
 
I've tried to retrain existing model NER several ways and get very poor results - 20-40% for NER P/R/F after training.
Initially I tried to train it with very few examples and then to check that retraining is working I started to check with source training data.
I downloaded Wikiner training data and used english source with ~2 million words. I used this set for checking results with spacy evalute.
I loaded 5k sentences from Wikiner and train xx_core_web_sm model with recently updated code from 'Updating the Named Entity Recognizer' running 100 epoch.
After that I checked model on Wikiner en and get next results: NER P 24.43 NER R 38.11 NER F 29.77
I think this is ideal environment for retraining model check, because I used huge amount of different data that model was trained before (https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting). So why results so pure comparing to initial model giving 
NER P 83.47, NER R 82.58, NER F 83.02. Is it possible to use model retraining at all?


Training log output:
Losses {'ner': 15063.066990082474}
Losses {'ner': 11629.889364530538}
Losses {'ner': 10494.161782337735}
Losses {'ner': 9623.809613426754}
Losses {'ner': 9142.404902400052}
Losses {'ner': 8639.16547262442}
Losses {'ner': 8459.444947359047}
Losses {'ner': 7941.9721914940055}
Losses {'ner': 7669.527320367472}
Losses {'ner': 7608.771949650602}
Losses {'ner': 7327.176064978128}
Losses {'ner': 7075.115230791447}
Losses {'ner': 7004.6924016013345}
Losses {'ner': 6670.628368807335}
Losses {'ner': 6719.906040001562}
Losses {'ner': 6541.140641508225}
Losses {'ner': 6373.065197750666}
Losses {'ner': 6187.52255028603}
Losses {'ner': 6188.456346469873}
Losses {'ner': 5945.065942535327}
Losses {'ner': 5928.391734070782}
Losses {'ner': 5781.87251618183}
Losses {'ner': 5726.948722697338}
Losses {'ner': 5698.842095540828}
Losses {'ner': 5594.911816915247}
Losses {'ner': 5476.623139583039}
Losses {'ner': 5399.218761012343}
Losses {'ner': 5393.962858256385}
Losses {'ner': 5180.310507435423}
Losses {'ner': 5184.917663437949}
Losses {'ner': 5255.646767346439}
Losses {'ner': 5049.3438865619}
Losses {'ner': 5006.051801752485}
Losses {'ner': 5126.498390270155}
Losses {'ner': 4931.881132303227}
Losses {'ner': 4903.77620897046}
Losses {'ner': 4815.970680039226}
Losses {'ner': 4869.446105438782}
Losses {'ner': 4752.001489196585}
Losses {'ner': 4749.1452747045105}
Losses {'ner': 4684.490625249131}
Losses {'ner': 4686.65270677296}
Losses {'ner': 4652.0041196014845}
Losses {'ner': 4532.425295642297}
Losses {'ner': 4597.851694887338}
Losses {'ner': 4603.022341920004}
Losses {'ner': 4585.898221105738}
Losses {'ner': 4362.555321542932}
Losses {'ner': 4359.977057576009}
Losses {'ner': 4431.069526548233}
Losses {'ner': 4537.661763865623}
Losses {'ner': 4404.21058083518}
Losses {'ner': 4317.334126414206}
Losses {'ner': 4324.015868989382}
Losses {'ner': 4280.837153941132}
Losses {'ner': 4085.5653676661364}
Losses {'ner': 4206.998216835518}
Losses {'ner': 4150.090601174477}
Losses {'ner': 4223.763730703552}
Losses {'ner': 4037.0037649564206}
Losses {'ner': 4224.653343702988}
Losses {'ner': 4132.8999594737315}
Losses {'ner': 4145.879738595007}
Losses {'ner': 4018.5912377032187}
Losses {'ner': 4106.8283101004345}
Losses {'ner': 4036.8007862988065}
Losses {'ner': 3991.2323565833617}
Losses {'ner': 3950.298744434533}
Losses {'ner': 3975.29642784109}
Losses {'ner': 3863.9817495027673}
Losses {'ner': 3861.8948883296307}
Losses {'ner': 3948.0756895615227}
Losses {'ner': 3897.68691941695}
Losses {'ner': 3962.5085762721856}
Losses {'ner': 3941.8293609340326}
Losses {'ner': 3751.5198740399314}
Losses {'ner': 3844.6950125026724}
Losses {'ner': 3807.371357590735}
Losses {'ner': 3829.3558887053737}
Losses {'ner': 3887.934958377564}
Losses {'ner': 3787.0113654127767}
Losses {'ner': 3754.949522888718}
Losses {'ner': 3785.3073014897486}
Losses {'ner': 3744.398673595233}
Losses {'ner': 3710.48395772588}
Losses {'ner': 3735.278587472417}
Losses {'ner': 3714.209315601868}
Losses {'ner': 3800.8156407004385}
Losses {'ner': 3647.9371526723157}
Losses {'ner': 3708.170002996947}
Losses {'ner': 3580.7662130972803}
Losses {'ner': 3685.329725901364}
Losses {'ner': 3624.1697674026136}
Losses {'ner': 3605.2621245154864}
Losses {'ner': 3610.3301118786544}
Losses {'ner': 3412.9983680328114}
Losses {'ner': 3694.3831085043644}
Losses {'ner': 3587.3276896920747}
Losses {'ner': 3589.6796999717358}
Losses {'ner': 3562.5060677709325}

",,,,,,,,,,1
262,https://github.com/explosion/spaCy/issues/3166,3166,[],closed,2019-01-16 10:34:28+00:00,,2,Loss is always Zero while creating a NER model,"**I'm creating a custom ner model using the CLI command**
`python -m spacy train en ./docs/model ./docs/data/train.json ./docs/data/sample_test_data.json -n 5 -P -T`.

**The problem is:**
The loss is always zero even though the train and test dataset are different. And obviously, the NER model is not performing as expected, even period (.) is considered as an entity.

### Output:

**dropout_from = 0.2 by default
dropout_to = 0.2 by default
dropout_decay = 0.0 by default
batch_from = 1 by default
batch_to = 16 by default
batch_compound = 1.001 by default
max_doc_len = 5000 by default
beam_width = 1 by default
beam_density = 0.0 by default
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))
learn_rate = 0.001 by default
optimizer_B1 = 0.9 by default
optimizer_B2 = 0.999 by default
optimizer_eps = 1e-08 by default
L2_penalty = 1e-06 by default
grad_norm_clip = 1.0 by default
parser_hidden_depth = 1 by default
parser_maxout_pieces = 2 by default
token_vector_width = 128 by default
hidden_width = 200 by default
embed_size = 7000 by default
history_feats = 0 by default
history_width = 0 by default
Itn.  Dep Loss  NER Loss  UAS     NER P.  NER R.  NER F.  Tag %   Token %  CPU WPS  GPU WPS
0     0.000     0.000     0.000   5.882   40.000  10.256  100.000 100.000  349.7    0.0
1     0.000     0.000     0.000   13.333  40.000  20.000  100.000 100.000  781.1    0.0
2     0.000     0.000     0.000   0.000   0.000   0.000   100.000 100.000  851.0    0.0
3     0.000     0.000     0.000   5.000   40.000  8.889   100.000 100.000  705.4    0.0
4     0.000     0.000     0.000   0.000   0.000   0.000   100.000 100.000  717.5    0.0
Saving model...**

The training data set file size is 3.5 Mb.

Not able to find where I'm making a mistake.

## Environment
* Operating System: Windows 8.1
* Python Version Used: 3.6.8
* spaCy Version Used: 2.0.18

",,,,,,,,,,1
266,https://github.com/explosion/spaCy/issues/3172,3172,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2019-01-19 06:06:00+00:00,,3,spaCy Custom NER model returning nothing even after training many times ,"
![screenshot from 2019-01-19 11-36-44](https://user-images.githubusercontent.com/40916262/51422998-b2ebd380-1bde-11e9-9a0a-7d8a86069676.png)
spaCy Custom NER model doesn't return the required result.It always returns nothing even after training for more than a 1000 times. The model is getting created,the model is getting trained perfectly and after calling the custom model in jupyter notebook, it returns nothing.What can be the problem here or I need to train with more and more data ?
As it can be seen in above image that it is returning nothing and in the below image it is getting trainied perfectly.
![screenshot from 2019-01-19 11-30-53](https://user-images.githubusercontent.com/40916262/51422963-5b4d6800-1bde-11e9-95d2-5164f5c9ab89.png)


",,,,,,,,,,1
299,https://github.com/explosion/spaCy/issues/3224,3224,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}]",closed,2019-02-03 23:12:18+00:00,,2,Evaluation of NER model is producing incorrect results,"**How to reproduce the behaviour**

using training script https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py

I executed this command on the train_ner.py script with my training data (approximately 1000 sentences)

`python train_ner.py -m en_core_web_lg -o outputModel`

**Your Environment**

Operating System:
Windows 10

**Python Version Used:**
Anaconda environment with python version 3.6.8

**spaCy Version Used:**
2.0.18

**Evaluation Script**

```
import spacy
from spacy.gold import GoldParse
from spacy.scorer import Scorer

def evaluate(ner_model, examples):
    scorer = Scorer()
    for input_, annot in examples:
        doc_gold_text = ner_model.make_doc(input_)
        gold = GoldParse(doc_gold_text, entities=annot['entities'])
        pred_value = ner_model(input_)
        scorer.score(pred_value, gold)
    return scorer.scores
```

When evaluating the model on the validation data, 

```
evalFold1 = [(""""""Text 鈥?..

spacyModel = spacy.load('en_core_web_lg')
nermodel = spacy.load('myModel')

results = evaluate(nermodel , evalFold1)

```
results produces the following output 

```
{'uas': 0.0,
 'las': 0.0,
 'ents_p': 0.0,
 'ents_r': 0.0,
 'ents_f': 0.0,
 'tags_acc': 0.0,
 'token_acc': 100.0}
```

It also does the same thing when I load the spacy model (en_core_web_lg) and use it to evaluate that same data.

EDIT: this model is the first in 5 part of a 5 fold cross validation set. 

Data is split up like

training folds 2, 3, 4, 5 evaluate on 1 <- only this one gives bad result
training folds 1, 3, 4, 5 evaluate on 2
etc.

So the first model evaluated on the first validation set is giving the incorrect result.  The other 4 are giving normal results (actual values for the P, R and F are like 93%)

",,,,,,,,,,1
1701,https://github.com/explosion/spaCy/issues/5568,5568,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881666230, 'node_id': 'MDU6TGFiZWw4ODE2NjYyMzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20vectors', 'name': 'feat / vectors', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Word vectors and similarity'}, {'id': 2103359118, 'node_id': 'MDU6TGFiZWwyMTAzMzU5MTE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/resolved', 'name': 'resolved', 'color': 'f6f6f6', 'default': False, 'description': 'The issue was addressed / answered'}]",closed,2020-06-10 13:23:59+00:00,,2,UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors,"## How to reproduce the behaviour
<!-- import spacy
from spacy.vectors import Vectors
import os
import pandas as pd
import numpy as np
nlp = spacy.load('en_core_web_lg')
vectors = Vectors(shape=(10000, 300))
nlp.vocab.vectors = vectors
print(nlp.vocab.vectors.shape)
import re
re_c = re.compile(r'\w+')


# switch for debug
flag_print = True

# switch to clear existing data
flag_clear = True

#threshold value for determining section
threshold = 0.5


# to get extract sections from the resume -- add or remove from  'similar_to' accordingly
similar_to = {
    'edu' : ['education', 'study', 'academics', 'institute', 'school', 'college'],
    'exp' : ['job', 'internship', 'training', 'research', 'career', 'profession', 'role','project', 'responsibility', 'description', 'work experience', 'workshop', 'conference'],
    'skill' : ['skill', 'languages', 'technology', 'framework', 'tools', 'database'],
    'extra' : ['introduction', 'intro', 'achievement', 'hobby', 'links', 'additional', 
               'personal', 'award', 'objective', 'miscellaneous', 'interest']
}

list_of_sections = similar_to.keys()

# to bring similar_words to their normal forms
for section in list_of_sections:
    new_list = []
    
    for word in similar_to[section]:
        docx = nlp(word)
        new_list.append(docx[0].lemma_)
        
    if flag_print:
        print(section, new_list)
        
    similar_to[section] = new_list
    
    
# function to remove unnecessary symbols and stopwords 
def modify(word):
    try:
        symbols = '''~'`!@#$%^&*)(_+-=}{][|\:;"",./<>?'''
        mod_word = ''
        
        for char in word:
            if (char not in symbols):
                mod_word += char.lower()

        docx = nlp(mod_word)

        if (len(mod_word) == 0 or docx[0].is_stop):
            return None
        else:
            return docx[0].lemma_
    except:
        return None # to handle the odd case of characters like 'x02', etc.
    
if flag_print:
    test_words = ['Hello!!', '.,<>', 'India', 'of', '..freedoM..', 'e-mail']
    
    for word in test_words:
        print(word, '--returned-->', modify(word))
        
# utility function to skip line when no alphabet present
def is_empty(line):
    for c in line:
        if (c.isalpha()):
            return False
    return True
      
if flag_print:
    test_words = ['.', '<.>', 'Speak', 'out', '""Eric""', 'freemail...']
    
    for word in test_words:
        print(word, '--returned-->', is_empty(word)) 
        
        
dict_of_data_series = {}
flag_print = False

for file_name in os.listdir(os.getcwd()+'\\CVs'):
    if flag_print:
        print('\n')
        print('*'*25) 
        print(file_name) 
        print('*'*25) 
        
    main_file_handler = open('CVs/'+file_name, 'r', encoding='latin-1')  
    previous_section  = 'extra'
    
    curr_data_series = pd.Series([""""]*len(list_of_sections), index=list_of_sections)
                   
    for line in main_file_handler:
        # skip line if empty
        if (len(line.strip()) == 0 or is_empty(line)):
            continue
                
        # processing next line
        list_of_words_in_line = re_c.findall(line)
        list_of_imp_words_in_line  = []
        
        for i in range(len(list_of_words_in_line)):
            modified_word = modify(list_of_words_in_line[i])
            
            if (modified_word):
                list_of_imp_words_in_line.append(modified_word)

        curr_line = ' '.join(list_of_imp_words_in_line)
        doc = nlp(curr_line)
        section_value = {}
            
        # initializing section values to zero
        for section in list_of_sections:
            section_value[section] = 0.0
        section_value[None] = 0.0
            
        # updating section values    
        for token in doc:
            # if(doc and doc.vector_norm):
                for section in list_of_sections:
                    for word in similar_to[section]:
                        word_token = doc.vocab[word]
                        section_value[section] = max(section_value[section], float(word_token.similarity(token)))

        # determining the next section based on section values and threshold
        most_likely_section = None
        for section in list_of_sections:
            #print '>>', section, section_value[section]
            if (section_value[most_likely_section] < section_value[section] and section_value[section] > threshold):
                most_likely_section = section
            
        # updating the section
        if (previous_section != most_likely_section and most_likely_section is not None):
            previous_section = most_likely_section
                

        # writing data to the pandas series
        try:
            docx = nlp(line)
        except:
            continue  # to handle the odd case of characters like 'x02', etc.
        mod_line = ''
        for token in docx:
            if (not token.is_stop):
                mod_line += token.lemma_ + ' '
        
        curr_data_series[previous_section] += mod_line
            
    dict_of_data_series[file_name] = curr_data_series
    if flag_print:
        print(curr_data_series)
    main_file_handler.close()
    
data_frame = pd.DataFrame(dict_of_data_series)
data_frame.to_csv('prc_data.csv', sep='\t')
#data_frame.head()   . -->

## Your Environmenta
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
* Python Version Used:
* spaCy Version Used:
* Environment Information:
",,1,,,,,,,1,
225,https://github.com/explosion/spaCy/issues/3115,3115,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}]",closed,2019-01-03 14:22:16+00:00,,5,Pandas Dataframes into NLP pipelines?,"Hi everyone,

First things first: Thanks for that great framework! 

I have a problem with CSV files when I want to give them to an entire nlp pipeline. It appears to be as if it is not possible to inject a pandas dataframe into the pipeline without decomposing the dataframe structure.
It tried the following:

`df = df.apply(nlp)`

which results in:
> (""Argument 'string' has incorrect type (expected str, got Series)"", 'occurred at index DocID')

Then I tried to put each column individually into the pipeline. 
```
for column in df:
     df[column] = df[column].apply(nlp)
```
This worked, but it is not helpful, since each pipeline component needs the information of each cell.  What can I do? Is there a workaround for my problem?

Thanks in advance!",,,,,,,,,1,
257,https://github.com/explosion/spaCy/issues/3160,3160,[],closed,2019-01-15 03:15:22+00:00,,2,spaCy & Spark 2.4: Pandas scalar UDF registration results in TypeError: __init__() got an unexpected keyword argument 'encoding',"## How to reproduce the behaviour

With Spark 2.4.0 and using a Pandas scalar UDF, registering a UDF fails to serialize spaCy.

```
  File ""/home/ubuntu/anaconda3/envs/subjectline-sentiment/lib/python3.6/pickle.py"", line 751, in save_tuple
    save(element)
  File ""/home/ubuntu/anaconda3/envs/subjectline-sentiment/lib/python3.6/pickle.py"", line 496, in save
    rv = reduce(self.proto)
  File ""vectors.pyx"", line 108, in spacy.vectors.Vectors.__reduce__
  File ""vectors.pyx"", line 409, in spacy.vectors.Vectors.to_bytes
  File ""/home/ubuntu/anaconda3/envs/subjectline-sentiment/lib/python3.6/site-packages/spacy/util.py"", line 486, in to_bytes
    return msgpack.dumps(serialized, use_bin_type=True, encoding='utf8')
  File ""/home/ubuntu/anaconda3/envs/subjectline-sentiment/lib/python3.6/site-packages/msgpack_numpy.py"", line 196, in packb
    return Packer(**kwargs).pack(o)
TypeError: __init__() got an unexpected keyword argument 'encoding'
```

To reproduce:
```
lang=""en""
global nlp
nlp = spacy.load(lang)

def spacy_tokenize(text):
    doc = nlp(text)
    return [token.text for token in doc]

@pandas_udf(ArrayType(StringType()), PandasUDFType.SCALAR) 
def pandas_tokenize(x):
    return x.apply(spacy_tokenize)

tokenize_pandas = spark.udf.register(""pandas_tokenize"", pandas_tokenize)
```

## Your Environment
## Info about spaCy
* **spaCy version:** 2.0.13
* **Platform:** Linux-4.4.0-1070-aws-x86_64-with-debian-stretch-sid
* **Python version:** 3.6.8
* **Models:** en_core_web_lg, en
* **PySpark:** 2.4.0
",,,,,,,,,1,
289,https://github.com/explosion/spaCy/issues/3208,3208,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}]",closed,2019-01-29 10:01:24+00:00,,3,Spacy import breaks pandas output in jupyter,"## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

```python
# In[1]:
import pandas as pd
import numpy as np

# In[2]:
df = pd.DataFrame(np.ones((2, 2)))

# In[3]:
df  # shows the dataframe

# In[4]:
import spacy

# In[5]:
df  # leads to an exception
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* **spaCy version:** 2.0.16
* **Platform:** Linux-4.15.0-43-generic-x86_64-with-debian-buster-sid
* **Python version:** 3.7.0
* **Models:** en, en_core_web_lg

## Notes
This only affects pandas 0.24.0 (0.23.4 works fine). Functions like `.head()` are also affected.
Exception details:

```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~/anaconda3/envs/fat_ml/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj)
    343             method = get_real_method(obj, self.print_method)
    344             if method is not None:
--> 345                 return method()
    346             return None
    347         else:

~/anaconda3/envs/fat_ml/lib/python3.7/site-packages/pandas/core/frame.py in _repr_html_(self)
    647         # display HTML, so this check can be removed when support for
    648         # IPython 2.x is no longer needed.
--> 649         if console.in_qtconsole():
    650             # 'HTML output is disabled in QtConsole'
    651             return None

~/anaconda3/envs/fat_ml/lib/python3.7/site-packages/pandas/io/formats/console.py in in_qtconsole()
    121             ip.config.get('KernelApp', {}).get('parent_appname', """") or
    122             ip.config.get('IPKernelApp', {}).get('parent_appname', """"))
--> 123         if 'qtconsole' in front_end.lower():
    124             return True
    125     except NameError:

AttributeError: 'LazyConfigValue' object has no attribute 'lower'
```",,,,,,,,,1,
520,https://github.com/explosion/spaCy/issues/3556,3556,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 1025171280, 'node_id': 'MDU6TGFiZWwxMDI1MTcxMjgw', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20speed', 'name': 'perf / speed', 'color': 'B35905', 'default': False, 'description': 'Performance: speed'}]",closed,2019-04-08 20:25:32+00:00,,8,"incredibly slow multiprocessing with joblib, pandas and loaded Spacy LG model","## Info about spaCy

* **spaCy version:** 2.1.3
* **Platform:** Windows-10
* **Python version:** 3.7.1
* **CPU** XEON E5-2687W v3 (10 cores)

Hello there, I am struggling to get `spacy` to work with `joblib` when an external model is loaded even with a tiny dataframe. 

Here is a very simple (and fully working example).

### no multiprocessing: the whole parsing takes less than 20 seconds

```
import pandas as pd
import numpy as np
import spacy
import re
from joblib import Parallel, delayed
import time

mydf = pd.DataFrame({'mytext' : ['the cat eats the chocolate mouse',
                                     'the cat eats the chocolate mouse',
                                     'the cat eats the chocolate mouse']})
    
mydf = pd.concat([mydf] * 1000, ignore_index = True)
    
    
def myfunc(dataframe):

    #loading model
    
    print('loading model')
    nlp = spacy.load('C:\\\\Users\john\\Documents\\en_core_web_lg-2.1.0\\en_core_web_lg\\en_core_web_lg-2.1.0')

    mytokens = []
      
    print('start processing')
    for parsed_doc in nlp.pipe(iter(dataframe.mytext), batch_size=2000):
          if parsed_doc.is_parsed:
              mytokens.append([re.sub(' ' , '-',tok.text) for tok in parsed_doc.noun_chunks]) 
          else:
              mytokens.append(None)
    
    dataframe['noun_chunks'] = mytokens
        
    print('collapse noun chunks')
    dataframe['noun_chunks'] = dataframe.noun_chunks.apply(lambda x: ' '.join(map(str, x)))

    print(dataframe.head(2))
    return dataframe


t0 = time.time()   
myfunc(mydf)
print (time.time() - t0)
```
which gives
```
loading model
start processing
collapse noun chunks
                             mytext                  noun_chunks
0  the cat eats the chocolate mouse  the-cat the-chocolate-mouse
1  the cat eats the chocolate mouse  the-cat the-chocolate-mouse
14.932860136032104
```

###  multiprocessing with `joblib`: takes forever and never ends


```
import pandas as pd
import numpy as np
import spacy
import re
from joblib import Parallel, delayed
import time

def myfunc(dataframe):

    #loading model
    
    print('loading model')
    nlp = spacy.load('C:\\\\Users\john\\Documents\\en_core_web_lg-2.1.0\\en_core_web_lg\\en_core_web_lg-2.1.0')

    mytokens = []
      
    print('start processing')
    for parsed_doc in nlp.pipe(iter(dataframe.mytext), batch_size=2000):
          if parsed_doc.is_parsed:
              mytokens.append([re.sub(' ' , '-',tok.text) for tok in parsed_doc.noun_chunks]) 
          else:
              mytokens.append(None)
    
    dataframe['noun_chunks'] = mytokens
        
    print('collapse noun chunks')
    dataframe['noun_chunks'] = dataframe.noun_chunks.apply(lambda x: ' '.join(map(str, x)))

    print(dataframe.head(2))
    return dataframe

def chunker(seq, size):
    return (seq[pos:pos + size] for pos in range(0, len(seq), size))

if __name__ == '__main__':
         
    mydf = pd.DataFrame({'mytext' : ['the cat eats the chocolate mouse',
                                     'the cat eats the chocolate mouse',
                                     'the cat eats the chocolate mouse']})
    
    mydf = pd.concat([mydf] * 1000, ignore_index = True)
    
     
    t0 = time.time()   
    mylist = Parallel(n_jobs=8,  verbose=200, backend = 'multiprocessing', prefer=""processes"")(delayed(myfunc)(chunk) for chunk in chunker(mydf, 100))
    mydata = pd.concat(mylist)
    print (time.time() - t0)
     
```

Do you know what is the issue here? How can I run this in parallel efficiently? My real dataset has millions of observations.

Thanks!!",,,,,,,,,1,
1788,https://github.com/explosion/spaCy/issues/5790,5790,[],closed,2020-07-20 21:15:10+00:00,,0,Select rows of a Pandas DataFrame based on Spacy Rule Matcher,"I need to slice a pandas DataFrame based on spacy rule-based matcher results. The following is what I tried. This produces an empty DataFrame. However, It should extract the following two rows from df.
Civil Defence Minister Peeni Henare heartbroken over Northland flooding
Far North flooding: New photos reveal damage to roads 



  ```
import pandas as pd
import numpy as np
import spacy
from spacy.matcher import Matcher
    
df = pd.DataFrame([['Eight people believed injured in serious SH1 crash involving truck and three cars at Hunterville',
     'Fire and emergency responding to incident at Mataura, Southland ouvea premix site',
     'Civil Defence Minister Peeni Henare heartbroken over Northland flooding',
     'Far North flooding: New photos reveal damage to roads']]).T
    df.columns = ['col1']
    
nlp = spacy.load(""en_core_web_sm"")
    
flood_pattern = [{'LOWER': 'flooding'}]
    
matcher = Matcher(nlp.vocab, validate=True)
matcher.add(""FLOOD_DIS"", None, flood_pattern)
titles = (_ for _ in df['col1'])
g = (d for d in nlp.pipe(titles) if matcher(d))
x = list(g)
    
df2 = df[df['col1'].isin(x)]
df2
```


",,,,,,,,,1,
1912,https://github.com/explosion/spaCy/issues/6250,6250,[],closed,2020-10-14 08:24:15+00:00,,1,Import of bert-base-multilingual-cased limited to less than 512 Token,"Hi there,

I am currently working on a binary text classifcation (mainly german language) problem and I have build a spaCy pipeline for that. In order to evaluate which model performs best for my task I have compared several pretrained models from the spacy library. So far, the multi-language Modell (xx_ent_wiki_sm) has achieved by far the best results. Therefore, I would now like to test the ""big"" bert-base-multilingual-cased modell.

To do that, I downloaded the bert-base-multilingual-cased tf-checkpoint and converted it to a pytorch model with the convert_bert_original_tf_checkpoint_to_pytorch.py script (https://github.com/huggingface/transformers/blob/master/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py). After that I was able to ""load this pyTorch model from the dedicated path"" (https://github.com/explosion/spacy-transformers) to my pipeline.

The problem arrises, when I start the training with the same training data, as I used with the models (limited to 512 tokens) I get the error message:
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
1812 # remove once script supports set_grad_enabled
1813 _no_grad_embeddingrenorm(weight, input, max_norm, norm_type)
-> 1814 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
1815
1816

IndexError: index out of range in self

After researching this problem and adjusting my length of my data input my only track at the moment is, that if I reduce the my input size below 256 words, it is working fine. I tried to adjust my config file in different manners (see optimizers adjustements in code below) to get the training done - without success. 

Does someone have an idea or a clue, how I could fix that problem? Any idea would be highly appreciated!

Thanks a lot in advance!

```
`with nlp.disable_pipes(*other_pipes):  # only train textcat
        optimizer = nlp.begin_training()
        optimizer.token_vector_width = 512
        optimizer.embed_size = 150000  
        optimizer.hidden_width = 512
        print(""Training the model..."")
        print(""{:^5}\t{:^5}\t{:^5}\t{:^5}"".format(""LOSS"", ""P"", ""R"", ""F""))
        batch_sizes = compounding(4.0, 32.0, 1.001)

        results = pd.DataFrame(index=np.arange(n_iter), columns=[""textcat_p"", ""textcat_r"", ""textcat_f"", ""textcat_l""], )
        
        for i in range(n_iter):
            losses = {}
            # batch up the examples using spaCy's minibatch
            random.shuffle(train_data)
            batches = minibatch(train_data, size=batch_sizes)
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)`
```


## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
* Python Version Used: 3.6
* spaCy Version Used: 2.3.2
* spaCy-transformers Version Used: 0.6.2
",,,,,,,,,1,
2078,https://github.com/explosion/spaCy/issues/6703,6703,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}]",closed,2021-01-11 09:32:02+00:00,,4,Inconsistent Iteration Results,"<!-- Describe your issue here. Please keep in mind that the GitHub issue tracker is mostly intended for reports related to the spaCy code base and source, and for bugs and enhancements. If you're looking for help with your code, consider posting a question here:

- GitHub Discussions: https://github.com/explosion/spaCy/discussions
- Stack Overflow: http://stackoverflow.com/questions/tagged/spacy
-->

## Your Environment

<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->

Greetings,

I have been studying dative alternation in corpora and struggling with the inconsistent results. Here is my code;

```
dative_results_directory = ""C:/Users/fatih/OneDrive/Belgeler/Dative/turkish""
access_rights = 0o755
nlp = spacy.load(""en_core_web_lg"")
nlp.max_length = 150000000
dative_sents = []
nsubj = []
root = []
dative = []
direct_obj = []
pre_obj = []
sents = []
nsubj_pre = []
root_pre = []
dative_pre = []
direct_obj_pre = []
sent_pre = []
dative_sentences_pre = []
dative_sentences = []
print(""Please choose a text file from directory"")
text = open(askopenfilename(), encoding = ""utf-8"").read()

def text_preproc(x):
  x = x.lower()
  x = x.encode('ascii', 'ignore').decode()
  x = re.sub(r'https*\S+', ' ', x)
  x = re.sub(r'@\S+', ' ', x)
  x = re.sub(r'#\S+', ' ', x)
  x = re.sub(r'\'\w+', '', x)
  x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)
  x = re.sub(r'\w*\d+\w*', '', x)
  x = re.sub(r'\s{2,}', ' ', x)
  return x

print(""Text files cleaned succesfully, now parsing text..."")
 
doc = nlp(text_preproc(text))

print(""dative sentences are being extracted..."")

for token in doc:
    if token.dep_ == ""dative"" and token.head.pos_ == ""VERB"":
        dative_sents.append(token.sent)
        
for t in dative_sents:
            for d in t:
                if d.dep_ == ""dative"" and d.pos_ != ""ADP"" and d.head.pos_ == ""VERB"":
                    for n in d.head.children:
                        if n.dep_ == ""nsubj"":
                            for x in d.head.children:
                                if x.dep_ == ""dobj"":
                                    
                                        dative.append(d.text)
                                        root.append(d.head.text)
                                        nsubj.append(n.text)
                                        direct_obj.append(x.text)
                                        dative_sentences.append(d.sent)
                                                                            
for a in dative_sents:
            for b in a:
                if b.dep_ == ""dative"" and b.pos_ == ""ADP"" and b.head.pos_ == ""VERB"":
                    for m in b.head.children:
                        if m.dep_ == ""nsubj"":
                            for k in b.head.children:
                                if k.dep_ == ""dobj"":
                                    for l in b.children:
                                        if l.dep_ == ""pobj"":
                                    
                                         dative_pre.append(b.text)
                                         root_pre.append(b.head.text)
                                         nsubj_pre.append(m.text)
                                         direct_obj_pre.append(k.text)
                                         pre_obj.append(l.text)
                                         dative_sentences_pre.append(b.sent)
                                         
            
dative_without_pre = pd.DataFrame({""Nsubj"" : nsubj, ""Verb"" : root, ""Dative"" : dative, ""Direct_Obj"" : direct_obj, ""Dative_Sentences"" : dative_sentences})
dative_with_pre = pd.DataFrame({""Nsubj"" : nsubj_pre, ""Verb"" : root_pre, ""Direct_Obj"" : direct_obj_pre, ""Dative"" : dative_pre, ""Prepositional Object"": pre_obj, ""Dative_Sentences"" : dative_sentences_pre})
``` 

Problem is that I see dative_sents has 376 span(dative sentences). However, when combined dative_sentences (this is the dative alternation without preposition) and dative_sentences_pre (this is the datives with prepositional phrase), overall sentence (span number) does not match the original number of span in list dative_sents. 

Why is the difference? 

```
dative_sentences list size 126
dative_sentences_pre list size 121 
dative_sents list size 376

```

Also itireation over noun_chunks provide different results;

```
dative_sents = []
for nc in doc.noun_chunks:
    if nc.root.dep_ == ""dative"" and nc.root.head.pos_ == ""VERB"":
        dative_sents.append(nc.sent)
```
`dative_sents list size 167`

## Info about spaCy

* **spaCy version:** 2.3.5
* **Platform:** Windows-10-10.0.19041-SP0
* **Python version:** 3.8.5
*  Environment Information: Anaconda - Spyder
",,,,,,,,,1,
2586,https://github.com/explosion/spaCy/issues/8649,8649,[],closed,2021-07-08 13:28:19+00:00,,2,spaCy train - 'Invalid NaN value when encoding double' when creating meta.json,"## How to reproduce the behaviour
I am trying to run spacy train for textcat, in a way that I hoped was nothing too complicated. Here's the command I run:
`python -m spacy train config_train.cfg --paths.train ./train.spacy --paths.dev ./test.spacy --output ./output`
and the config file:
[config_train.cfg.txt](https://github.com/explosion/spaCy/files/6784787/config_train.cfg.txt)

and the training data comes from a dataframe with two fields: ""text"" (a news article) and ""binary_human_label"" (""positive"" or ""negative""), which I turn into docs and into a DocBin as follows:

`def make_docs(df):
    docs = []
    data = df[['text','binary_human_label']].to_records(index=False)
    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):
        if label == ""positive"":
            doc.cats[""positive""] = 0.9
            doc.cats[""negative""] = 0.1
        else:
            doc.cats[""positive""] = 0.1
            doc.cats[""negative""] = 0.9
        docs.append(doc)
    doc_bin = DocBin(docs=docs)
    return doc_bin`

 which I then send to_disk.

My expectation is that I can `spacy train` with these files, and indeed I can! When I exclude `--output` in the command. Something about saving `meta.json` is breaking, stacktrace below:

`Traceback (most recent call last):
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/training/loop.py"", line 114, in train
    raise e
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/training/loop.py"", line 104, in train
    save_checkpoint(is_best_checkpoint)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/training/loop.py"", line 67, in save_checkpoint
    before_to_disk(nlp).to_disk(output_path / DIR_MODEL_LAST)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/language.py"", line 1801, in to_disk
    util.to_disk(path, serializers, exclude)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/util.py"", line 1159, in to_disk
    writer(path / key)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/language.py"", line 1792, in <lambda>
    serializers[""meta.json""] = lambda p: srsly.write_json(p, self.meta)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/srsly/_json_api.py"", line 75, in write_json
    json_data = json_dumps(data, indent=indent)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/srsly/_json_api.py"", line 26, in json_dumps
    result = ujson.dumps(data, indent=indent, escape_forward_slashes=False)
OverflowError: Invalid Nan value when encoding double

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/__main__.py"", line 4, in <module>
    setup_cli()
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/cli/_util.py"", line 68, in setup_cli
    command(prog_name=COMMAND)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/typer/main.py"", line 497, in wrapper
    return callback(**use_params)  # type: ignore
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/cli/train.py"", line 59, in train_cli
    train(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/training/loop.py"", line 118, in train
    save_checkpoint(False)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/training/loop.py"", line 67, in save_checkpoint
    before_to_disk(nlp).to_disk(output_path / DIR_MODEL_LAST)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/language.py"", line 1801, in to_disk
    util.to_disk(path, serializers, exclude)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/util.py"", line 1159, in to_disk
    writer(path / key)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/spacy/language.py"", line 1792, in <lambda>
    serializers[""meta.json""] = lambda p: srsly.write_json(p, self.meta)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/srsly/_json_api.py"", line 75, in write_json
    json_data = json_dumps(data, indent=indent)
  File ""/Users/daniel/.local/share/virtualenvs/internet-access-disruption-cv49WcSZ/lib/python3.9/site-packages/srsly/_json_api.py"", line 26, in json_dumps
    result = ujson.dumps(data, indent=indent, escape_forward_slashes=False)
OverflowError: Invalid Nan value when encoding double`

Running `python -m spacy debug data config_train.cfg` gets me:

=============================== Training stats ===============================
Language: en
Training pipeline: tok2vec, textcat
200 training docs
52 evaluation docs
鈿?1 training examples also in evaluation data
鈿?Low number of examples to train a new pipeline (200)

============================== Vocab & Vectors ==============================
鈩?21129 total word(s) in the data (2579 unique)
鈩?684830 vectors (684830 unique keys, 300 dimensions)
鈿?876 words in training data without vectors (0.04%)

============================ Text Classification ============================
鈩?Text Classification: 0 new label(s), 2 existing label(s)
鈩?The train data contains instances without mutually-exclusive classes.
Use '--textcat-multilabel' when training.

================================== Summary ==================================
鉁?2 checks passed
鈿?3 warnings

(I know this is a terribly small number of documents, I'm trying to bootstrap up a model.)


## Info about spaCy

- **spaCy version:** 3.0.1
- **Platform:** macOS-11.4-x86_64-i386-64bit
- **Python version:** 3.9.5
- **Pipelines:** en_core_web_lg (3.0.0), en_core_web_trf (3.0.0)

Thank you very much for your help.",,,,,,,,,1,
2617,https://github.com/explosion/spaCy/issues/8798,8798,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 1594093126, 'node_id': 'MDU6TGFiZWwxNTk0MDkzMTI2', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/scaling', 'name': 'scaling', 'color': 'B35905', 'default': False, 'description': 'Scaling, serving and parallelizing spaCy'}]",open,2021-07-23 15:52:53+00:00,,3,"`nlp.pipe(..., n_process>1)` won't return if wrapped by `tqdm()` and `zip()`","This is a pretty specific set of circumstances that I discovered today, but on the off chance that it is useful to someone, here it is.

If you include the output of `nlp.pipe(...,n_process>1)` in a `zip()` within `tqdm()` it will hang interminably. See below

## How to reproduce the behaviour
```
#!/usr/bin/env python

import pandas as pd
import spacy
from tqdm import tqdm

data = [
    {""text"": ""I just wanna tell you how I'm feeling"", ""id"": 0},
    {""text"": ""Gotta make you understand"", ""id"": 1},
    {""text"": ""Never gonna give you up"", ""id"": 2},
    {""text"": ""Never gonna let you down"", ""id"": 3},
    {""text"": ""Never gonna run around and desert you"", ""id"": 4},
    {""text"": ""Never gonna make you cry"", ""id"": 5},
    {""text"": ""Never gonna say goodbye"", ""id"": 6},
    {""text"": ""Never gonna tell a lie and hurt you"", ""id"": 7},
]


df = pd.DataFrame(data)

nlp = spacy.load(""en_core_web_md"")

# Works with a single process

for id, doc in tqdm(
    zip(
       df[""id""],
        nlp.pipe(
            df[""text""],
            n_process=1,
        ),
    )
):

    print(id, doc.text)


# Works with no zip and multiple processes

for doc in tqdm(
        nlp.pipe(
            df[""text""],
            n_process=2,
        ),
):

    print(doc.text)

# Hangs with multiple processes and zip

for id, doc in tqdm(
    zip(
       df[""id""],
        nlp.pipe(
            df[""text""],
            n_process=2,
        ),
    )
):

    print(id, doc.text)
```
Output:

```
$python script.py
0it [00:00, ?it/s]0 I just wanna tell you how I'm feeling
1 Gotta make you understand
2 Never gonna give you up
3 Never gonna let you down
4 Never gonna run around and desert you
5 Never gonna make you cry
6 Never gonna say goodbye
7 Never gonna tell a lie and hurt you
8it [00:00, 977.21it/s]
0it [00:00, ?it/s]I just wanna tell you how I'm feeling
Gotta make you understand
Never gonna give you up
Never gonna let you down
Never gonna run around and desert you
Never gonna make you cry
Never gonna say goodbye
Never gonna tell a lie and hurt you
8it [00:00, 345.82it/s]
0it [00:00, ?it/s]0 I just wanna tell you how I'm feeling
1 Gotta make you understand
2 Never gonna give you up
3 Never gonna let you down
4 Never gonna run around and desert you
5 Never gonna make you cry
6 Never gonna say goodbye
7 Never gonna tell a lie and hurt you
8it [00:00, 342.59it/s]

```

## Your Environment
## Info about spaCy

- **spaCy version:** 3.0.1
- **Platform:** Linux-5.8.0-50-generic-x86_64-with-glibc2.29
- **Python version:** 3.8.5
- **Pipelines:** en_ner_bc5cdr_md (0.4.0), en_core_web_md (3.0.0), en_ner_craft_md (0.4.0), en_ner_bionlp13cg_md (0.4.0), en_ner_jnlpba_md (0.4.0)

",,,,,,,,,1,
1732,https://github.com/explosion/spaCy/issues/5642,5642,"[{'id': 881665973, 'node_id': 'MDU6TGFiZWw4ODE2NjU5NzM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20parser', 'name': 'feat / parser', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Dependency Parser'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2020-06-24 19:23:16+00:00,,4,Differences in dependency trees built by models of different versions,"Hello!

I trained a tagger/parser for portuguese using spaCy 2.1.8 and got a consistent model (let's call it model A), but, when I upgraded to spaCy 2.2, I discovered it didn't work the same way in the newer version. I trained a new model (model B) for this version using the exact same corpus and got metrics (tokenizer, tags, uas and las) relatively close to the ones presented by model A, as show in the table below.

| metric | model A<br>(spaCy 2.1.8) | model B<br>(spaCy 2.2) |
|:-:|:-:|:-:|
| tokenizer | 98.55 | 98.35 |
| tags | 94.71 | 96.98 |
| uas | 85.76 | 85.48 |
| las | 82.52 | 82.12 |

However, when the models are used to annotate the same sentence, they give different results. For example, in the sentence `[...] ou seja ser谩 necess谩rios dois planetas para suprir a demanda [...]`, model A builds the following dependency tree:

![Batelada](https://user-images.githubusercontent.com/44068014/85617946-6e1b6600-b636-11ea-8307-21c2c7dab4c9.png)

In the meantime, model B builds a slightly different tree, as shown below. 

![Quarentena](https://user-images.githubusercontent.com/44068014/85617959-72e01a00-b636-11ea-9267-142100b6f533.png)

I know there is a difference in how models apply lemmas from spaCy 2.1 to 2.2, but is there any other changes that could affect the model behaviour? How can two models trained with the exact same corpus and with similar metrics can diverge while constructing dependency trees?",,,1,,,,,1,,
19,https://github.com/explosion/spaCy/issues/2813,2813,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2018-09-29 12:33:29+00:00,,3,Using a new spacy version in other projects,"I have used Spacy and developed it for a new language. I have built it and I assumed that is ready. however I can't use it in other projects. I want to use Rasa chatbot which is using spacy. How can I use my own spacy instead of the global one???


* Operating System: Linux
* Python Version Used: 3.6

",,,,,,,,1,,
23,https://github.com/explosion/spaCy/issues/2819,2819,"[{'id': 881666463, 'node_id': 'MDU6TGFiZWw4ODE2NjY0NjM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20serialize', 'name': 'feat / serialize', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Serialization, saving and loading'}]",closed,2018-10-03 01:37:33+00:00,,2,Can't Pickle with Pyspark & spaCy version 2.0.12,"Hi,

I am facing this error  while using the textcat model with Apache Spark

`PicklingError: Can't pickle <cyfunction LinearModel.<lambda> at 0x10487c498>`",,,,,,,,1,,
99,https://github.com/explosion/spaCy/issues/2916,2916,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 1025171280, 'node_id': 'MDU6TGFiZWwxMDI1MTcxMjgw', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20speed', 'name': 'perf / speed', 'color': 'B35905', 'default': False, 'description': 'Performance: speed'}]",closed,2018-11-09 18:09:14+00:00,,2,Which version of Spacy should I use in order to take advantage of multiprocessing on CPUs?,"Hi, which version of Spacy should I use in order to take advantage of multiprocessing?

This is my code:
```
start = time.time()
for doc in nlp.pipe(array_texts, batch_size=args.batchSize, n_threads=args.nThreads):
    result.append(doc.cats)
print('Time Elapsed {} ms'.format((time.time() - start)*1000))

```
array_texts is a numpy array with mutiple texts.

* Operating System: CentOS Linux release 7.4.1708 (Core)
* Python Version Used: 3.6.5
* spaCy Version Used: 2.0.12 (installed using: pip install spacy)
* Environment Information: Anaconda 3
* Hardware: Intel(R) Xeon(R) Platinum 8153 CPU

_Originally posted by @felipheggaliza in https://github.com/explosion/spaCy/issue_comments#issuecomment-437445351_",,,,,,,,1,,
166,https://github.com/explosion/spaCy/issues/3018,3018,"[{'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}, {'id': 642658286, 'node_id': 'MDU6TGFiZWw2NDI2NTgyODY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20ja', 'name': 'lang / ja', 'color': '726DA8', 'default': False, 'description': 'Japanese language data and models'}]",closed,2018-12-06 15:28:18+00:00,,3,New mecab-python3 version breaks Japanese,"The python package used for Mecab, [mecab-python3](https://github.com/SamuraiT/mecab-python3), had a release for the first time since 2014 recently. Unfortunately one of the parts spaCy relies on is broken, see SamuraiT/mecab-python3#19. 

Since this is an optional dependency I don't think we can peg an old version for it, but maybe we should put a note somewhere if it's not fixed quickly. I will see what I can do about fixing it upstream.

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

    import spacy
    ja = spacy.blank('ja')
    print(ja(""a b c d""))
    # output: a b c db c dc dd

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->

* **spaCy version:** 2.0.18
* **Platform:** Linux-4.18.9-arch1-1-ARCH-x86_64-with-arch
* **Python version:** 3.7.1",,,,,,,,1,,
234,https://github.com/explosion/spaCy/issues/3128,3128,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881752122, 'node_id': 'MDU6TGFiZWw4ODE3NTIxMjI=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20cli', 'name': 'feat / cli', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Command-line interface'}]",closed,2019-01-08 14:07:54+00:00,,4,Conversion to IOB2 failing,"Hi,

following the closure of a previous request (#2970), for the conversion of the format IOB2, I was attempting a conversion with Spacy Version: 2.0.18 and Python 3.
The error message is ValueError: too many values to unpack (expected 2), below a detail.

The test is done with the same IOB2 file content of #2970  
Alex|I-PER
is|O
going|O
to|O
Los|B-LOC
Angeles|I-LOC

Any help is appreciated.

MyMac-iMac:Spacy itsme$ python3 -m spacy convert my_test.iob2 . -c iob
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/spacy/__main__.py"", line 31, in <module>
    plac.call(commands[command], sys.argv[1:])
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/plac_core.py"", line 328, in call
    cmd, result = parser.consume(arglist)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/plac_core.py"", line 207, in consume
    return cmd, self.func(*(args + varargs + extraopts), **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/spacy/cli/convert.py"", line 47, in convert
    n_sents=n_sents, use_morphology=morphology)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/spacy/cli/converters/iob2json.py"", line 16, in iob2json
    sentences = read_iob(file_)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/spacy/cli/converters/iob2json.py"", line 35, in read_iob
    words, iob = zip(*tokens)
ValueError: too many values to unpack (expected 2)",,,,,,,,1,,
314,https://github.com/explosion/spaCy/issues/3248,3248,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 496348994, 'node_id': 'MDU6TGFiZWw0OTYzNDg5OTQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/%F0%9F%8C%99%20nightly', 'name': '馃寵 nightly', 'color': 'ffffff', 'default': False, 'description': 'Discussion and contributions related to nightly builds'}, {'id': 881682577, 'node_id': 'MDU6TGFiZWw4ODE2ODI1Nzc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20matcher', 'name': 'feat / matcher', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Token, phrase and dependency matcher'}]",closed,2019-02-08 07:18:51+00:00,,4,pickling of PhraseMatcher not resolved yet in version '2.1.0a6' (nightly build),"#1971 Issue show that pickling is solved for PhraseMatcher. But, there still seems to be a bug in version **2.1.0a6**

Minimal reproducible example
```python
import spacy
from spacy.matcher import PhraseMatcher
import pickle

nlp = spacy.load(""en"")
matcher = PhraseMatcher(nlp.vocab, attr='LOWER')
matcher.add('president', None, nlp(u""Barack Obama""), nlp(u""Trump""))
matcher.add('health', None, nlp(u""health care reform""), nlp(u""hospital""))
print(len(matcher)) # shows 4
with open(""./phrase_matcher.pkl"", 'wb') as f:
    pickle.dump(obj=matcher,file=f)

with open(""./phrase_matcher.pkl"", 'rb') as f:
    loaded_matcher = pickle.load(f)
print(len(loaded_matcher)) # shows 0
```

## One more Bug:

According to documentation: `len(matcher)` should show 2 (only number of rule keys). But here it shows 4 (number of rules)",,,,,,,,1,,
358,https://github.com/explosion/spaCy/issues/3315,3315,"[{'id': 514704814, 'node_id': 'MDU6TGFiZWw1MTQ3MDQ4MTQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/examples', 'name': 'examples', 'color': '087EA6', 'default': False, 'description': 'Code examples in /examples'}, {'id': 881665727, 'node_id': 'MDU6TGFiZWw4ODE2NjU3Mjc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20textcat', 'name': 'feat / textcat', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Text Classifier'}]",closed,2019-02-22 18:14:03+00:00,,5,TextClassifier example gives very different results between versions 2.0.18 and 2.1,"## How to reproduce the behaviour
Install spacy 2.0.18, download the english language models, and run the example text classifier program https://github.com/explosion/spaCy/blob/master/examples/training/train_textcat.py with -m en_core_web_md -o output_md2_0 .  The only change made to the example program was to print the spacy version.

You get reasonable results:

python3 train_textcat.py -m en_core_web_md -o output_md2_0
spacy 2.0.18  <- I added this output to the program to display which version is being used
Loaded model 'en_core_web_md'
Loading IMDB data...
Using 2000 examples (1600 training, 400 evaluation)
Training the model...
LOSS 	  P  	  R  	  F  
86.797	0.850	0.754	0.799
33.462	0.851	0.846	0.848
13.090	0.851	0.846	0.848
6.030	0.843	0.856	0.850
3.122	0.839	0.882	0.860
2.093	0.832	0.887	0.859
1.040	0.831	0.882	0.856
0.980	0.831	0.856	0.843
0.599	0.833	0.846	0.840
0.779	0.833	0.872	0.852
0.371	0.836	0.862	0.848
0.289	0.824	0.862	0.842
0.227	0.823	0.856	0.839
0.223	0.817	0.846	0.831
0.190	0.831	0.856	0.843
0.407	0.839	0.856	0.848
0.169	0.842	0.846	0.844
0.392	0.838	0.851	0.845
0.187	0.838	0.851	0.845
0.447	0.843	0.856	0.850
This movie sucked {'POSITIVE': 0.2305149883031845}
Saved model to output_md2_0
Loading from output_md2_0
This movie sucked {'POSITIVE': 0.13291716575622559}

It does seem a bit odd that the classification is better after reloading the model from disk, but either result is reasonable.

Now setup a 2.1.0a9.dev0 (I also saw this with 2.1.0a7) environment and run the same command line again

python3 train_textcat.py -m en_core_web_md -o output_md2_1
spacy 2.1.0a9.dev0 <- I added this output to the program to display which version is being used
Loaded model 'en_core_web_md'
Loading IMDB data...
Using 2000 examples (1600 training, 400 evaluation)
Training the model...
LOSS 	  P  	  R  	  F  
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
176.750	0.537	1.000	0.699
This movie sucked {'POSITIVE': 1.0}
Saved model to output_md2_1
Loading from output_md2_1
This movie sucked {'POSITIVE': 1.0}

As you can see the model doesn't train well and the results of predictions are totally incorrect.  I saw the same results when using the small model and no model. 


## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Linux-4.9.125-linuxkit-x86_64-with-Ubuntu-18.04-bionic
* Python Version Used: 3.6.7
* spaCy Version Used: 2.0.18 and 2.1.0a9.dev0 (results also seen with 2.1.0a7)
* Environment Information:
",,,,,,,,1,,
490,https://github.com/explosion/spaCy/issues/3506,3506,"[{'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}, {'id': 1025171280, 'node_id': 'MDU6TGFiZWwxMDI1MTcxMjgw', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20speed', 'name': 'perf / speed', 'color': 'B35905', 'default': False, 'description': 'Performance: speed'}]",closed,2019-03-29 16:19:33+00:00,,5,Spacy nlp (v2.1.3) using all threads with Mac OS and conda version of numpy,"I am aware that in spacy 2.0 n_threads was not kept into account while running spacy nlp and all the threads available were used for the computations. (https://github.com/explosion/spaCy/issues/2075)
According to the cited issue, this should have been fixed with spacy 2.1.

I noticed that I have the same problem with spacy 2.1.3 and the conda version of numpy installed (on Mac OS X). If switching to the pip numpy only 1 thread is used as expected.

Thanks for taking this into account!

## My Environment
* Operating System: Mac OS X
* Python Version Used: 3.6.8
* spaCy Version Used: 2.1.3
* Environment Information: conda


",,,,,,,,1,,
615,https://github.com/explosion/spaCy/issues/3697,3697,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 111380488, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/help%20wanted', 'name': 'help wanted', 'color': 'f6f6f6', 'default': True, 'description': 'Contributions welcome!'}, {'id': 881666230, 'node_id': 'MDU6TGFiZWw4ODE2NjYyMzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20vectors', 'name': 'feat / vectors', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Word vectors and similarity'}]",closed,2019-05-07 20:18:17+00:00,,5,Make most_similar work like Gensim's version,"## Feature description
<!-- Please describe the feature: Which area of the library is it related to? What specific solution would you like? -->

Currently `nlp.vocab.vectors.most_similar()` returns only the single most similar vector for each vector passed to it. Gensim's `most_similar()` [returns the n-most similar vectors to a given vector](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar). This seems like a more useful implementation.

## Could the feature be a [custom component](https://spacy.io/usage/processing-pipelines#custom-components) or [spaCy plugin](https://spacy.io/universe)?
If so, we will tag it as [`project idea`](https://github.com/explosion/spaCy/labels/project%20idea) so other users can take it on.
This would not be a custom component or plugin.
",,,,,,,,1,,
731,https://github.com/explosion/spaCy/issues/3845,3845,"[{'id': 542391290, 'node_id': 'MDU6TGFiZWw1NDIzOTEyOTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/meta', 'name': 'meta', 'color': 'f6f6f6', 'default': False, 'description': 'Meta topics, e.g. repo organisation and issue management'}]",closed,2019-06-13 19:00:16+00:00,,2,What is spacy's versioning philosophy?,"What is spacy's versioning philosophy? The models have notes in the README on how to interpret versioning but there's no equivalent for the library. While I understand that semantic versioning is the expectation here, I'm not clear on what the definition of a ""backwards compatible bugfix"" (patch update) is with respect to an NLP library. For example, in the 2.1.4 notes there is a bugfix that modifies the stopword list (issue #3530). For consumers that expect a particular parsing behavior (e.g. a model trained on spacy output) such a change could lead to an unexpected result. So is that really backwards-compatible from the perspective of a user?

Also, minor version updates in spacy sometimes require new models, which also doesn't seem backwards compatible.  

-- Eric",,,,,,,,1,,
737,https://github.com/explosion/spaCy/issues/3851,3851,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446637424, 'node_id': 'MDU6TGFiZWw0NDY2Mzc0MjQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/conda', 'name': 'conda', 'color': '676F6D', 'default': False, 'description': 'conda package manager'}]",closed,2019-06-14 17:05:29+00:00,,4,Problem upgrading to version 2.1.4 via conda-forge,"## Problem
I have been trying to upgrade to version 2.1.4 using conda-forge, however, without success, i only seem to be able to reach version 2.0.12. Would you be able to assist please?

And keep up the great work!

## How to reproduce the problem
Using command: conda install -c conda-forge spacy 


## Environment info
spaCy version: 2.0.12
Platform: Windows-10-10.0.17763-SP0
Python version 3.7.1

",,,,,,,,1,,
958,https://github.com/explosion/spaCy/issues/4183,4183,"[{'id': 111380489, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/invalid', 'name': 'invalid', 'color': 'f6f6f6', 'default': True, 'description': 'Invalid issues: typos, mistakes etc.'}]",closed,2019-08-23 10:45:02+00:00,,2,"File ""/tmp/pip-build-gyfVq7/numpy/setup.py"", line 31, in <module>   raise RuntimeError(""Python version >= 3.5 required."")","<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

## How to reproduce the problem
<!-- Include the details of how the problem occurred. Which command did you run to install spaCy? Did you come across an error? What else did you try? -->

```bash
# copy-paste the error message here
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
* Python Version Used:
* spaCy Version Used:
* Environment Information:
",,,,,,,,1,,
959,https://github.com/explosion/spaCy/issues/4184,4184,"[{'id': 111380489, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/invalid', 'name': 'invalid', 'color': 'f6f6f6', 'default': True, 'description': 'Invalid issues: typos, mistakes etc.'}]",closed,2019-08-23 10:45:03+00:00,,2,"File ""/tmp/pip-build-gyfVq7/numpy/setup.py"", line 31, in <module>   raise RuntimeError(""Python version >= 3.5 required."")","<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

## How to reproduce the problem
<!-- Include the details of how the problem occurred. Which command did you run to install spaCy? Did you come across an error? What else did you try? -->

```bash
# copy-paste the error message here
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
* Python Version Used:
* spaCy Version Used:
* Environment Information:
",,,,,,,,1,,
960,https://github.com/explosion/spaCy/issues/4185,4185,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}]",closed,2019-08-23 10:45:05+00:00,,7,RuntimeError Error: Numpy 1.15.0 requires Python version >= 3.5.,"<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

## How to reproduce the problem
<!-- Include the details of how the problem occurred. Which command did you run to install spaCy? Did you come across an error? What else did you try? -->

```bash
# copy-paste the error message here
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
* Python Version Used:
* spaCy Version Used:
* Environment Information:
",,,,,,,,1,,
332,https://github.com/explosion/spaCy/issues/3277,3277,"[{'id': 634628353, 'node_id': 'MDU6TGFiZWw2MzQ2MjgzNTM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20es', 'name': 'lang / es', 'color': '726DA8', 'default': False, 'description': 'Spanish language data and models'}, {'id': 881718446, 'node_id': 'MDU6TGFiZWw4ODE3MTg0NDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20tokenizer', 'name': 'feat / tokenizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Tokenizer'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2019-02-14 16:06:29+00:00,,2,Incorrect tokenization of dash punctuation in Spanish,"In Spanish text, the conventions for using dashes and em-dashes as punctuation seems to be considerably different than in English. Spacy often does not tokenize the dash or em-dash as a separate token, instead keeping it attached to the closest word.

For example, the Spanish sentence:
鈥擸o me llamo... 鈥搈urmur贸 el ni帽o鈥?Emilio S谩nchez P茅rez.
English Translation:
""My name is..."", murmured the boy, ""Emilio Sanchez Perez.""

Here, the Spanish dash is used like a comma. The em-dash at the beginning of the sentence is used like a double quote. I believe that the fact that there is no space between the dash and word is throwing off the tokenizer.

The Spanish sentence above is tokenized as:
鈥擸o
me
llamo
...
鈥搈urmur贸
el
ni帽o鈥?
Emilio
S谩nchez
P茅rez
.

I would expect the tokenization to be
鈥?
Yo
me
llamo
...
鈥?
murmur贸
el
ni帽o
鈥?
Emilio
S谩nchez
P茅rez
.

## Your Environment
* **spaCy version:** 2.0.12
* **Platform:** Darwin-18.0.0-x86_64-i386-64bit
* **Python version:** 3.7.0
* **Models:** de, es, en
",,,,,,,1,,,
830,https://github.com/explosion/spaCy/issues/3983,3983,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 703368580, 'node_id': 'MDU6TGFiZWw3MDMzNjg1ODA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20all', 'name': 'lang / all', 'color': '726DA8', 'default': False, 'description': 'Global language data'}]",closed,2019-07-17 21:56:43+00:00,,3,Installing spacy with fewer languages to save disk space,"I'm building a docker container that contains a the Python library `spacy`. I'm now trying to reduce the size of this container, and `spacy` appears to be the main contributor to the disk image size.
 
Without any models installed, and without any other code/dependencies etc, `spacy` consumes around 500MB of disk when installed. It looks like the lang folder within this package is the main cause of this disk usage.

I'd like to be able to install spacy with a smaller set of languages, or to be able to remove them in a supported manner post-install, but I've not seen any docs that explain how this might be done.
 
My repro steps are:

```bash
mkdir foo1                  # create a folder 
cd foo1                     # change directory
python3 -m venv .venv       # create virtual environment
source .venv/bin/activate   # activate virtual environment
pip install --upgrade pip   # upgrade pip
pip install spacy           # install spacy
``` 

After doing this, I then navigate into the following folder...
 
`foo1/.venv/lib/python3.7/site-packages`
 
... and can see that the spacy folder is very large:
 
```bash
$ du -sh spacy
425M	spacy
```
 
Specifically, it's the language folder that's large:

```bash
$ du -sh spacy/lang
401M spacy/lang
``` 

There are 52 languages in that folder, and for many situations I only want one or two languages. For the specific app I'm working on now, I'm only interested in English.
 
When I look at the sizes, English is the 14th largest (only showing the top 14 in this list)...

```bash
$ du -sH spacy/lang/* | sort -n -r 

142024 spacy/lang/tr
86608 spacy/lang/pt
78368 spacy/lang/nb
76592 spacy/lang/da
74840 spacy/lang/sv
60672 spacy/lang/ca
50880 spacy/lang/es
48296 spacy/lang/fr
41688 spacy/lang/de
36960 spacy/lang/nl
34008 spacy/lang/it
32632 spacy/lang/ro
24160 spacy/lang/lt
8712 spacy/lang/en  <--- THE ONE I WANT
```

Is there a spacy-specifc way of installing `spacy` without all of these languages?
 
I can hack around post-install, but is there a safer way to install fewer languages?

My environment, and details about the versions installed in the above...

```bash
$ pip freeze
blis==0.2.4
certifi==2019.6.16
chardet==3.0.4
cymem==2.0.2
idna==2.8
murmurhash==1.0.2
numpy==1.16.4
plac==0.9.6
preshed==2.0.1
requests==2.22.0
spacy==2.1.6
srsly==0.0.7
thinc==7.0.8
tqdm==4.32.2
urllib3==1.25.3
wasabi==0.2.2
```

``` bash
$ python --version
Python 3.7.4
```

Installed on MacOS
",,,,,,,1,,,
1114,https://github.com/explosion/spaCy/issues/4451,4451,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881718446, 'node_id': 'MDU6TGFiZWw4ODE3MTg0NDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20tokenizer', 'name': 'feat / tokenizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Tokenizer'}]",closed,2019-10-15 15:25:03+00:00,,4,Tokenizer: add_special_case not working when special token not separated by whitespace,"Hi! I'm trying to add special cases to the tokenizer, but they're only applied if the special tokens are separated by whitespace.

## Code example:
```
import spacy
from spacy.symbols import ORTH, LEMMA, POS, DEP

nlp = spacy.load(""en_core_web_sm"")

word = ""<TITLE>""
rule = [{ORTH: word}]
nlp.tokenizer.add_special_case(word, rule)

word = ""</TITLE>""
rule = [{ORTH: word}]
nlp.tokenizer.add_special_case(word, rule)

result = [t.text for t in nlp(""<TITLE>Hello World</TITLE>"")]
```
Expected result: `['<TITLE>', 'Hello', 'World', '</TITLE>']`
Actual result: `['<', 'TITLE', '>', 'Hello', 'World</TITLE', '>']`

This, however, works:

```
result = [t.text for t in nlp(""<TITLE> Hello World </TITLE>"")]
>> ['<TITLE>', 'Hello', 'World', '<TITLE>']
```

This bug seems to have come up in the past already (#1061) and crept back in? At least the linked issue is what pointed me in the right direction.

## Environment
* **spaCy version:** 2.2.1
* **Platform:** Windows-10-10.0.17763-SP0
* **Python version:** 3.7.4
* **Models:** en",,,,,,,1,,,
1220,https://github.com/explosion/spaCy/issues/4634,4634,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881718446, 'node_id': 'MDU6TGFiZWw4ODE3MTg0NDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20tokenizer', 'name': 'feat / tokenizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Tokenizer'}]",closed,2019-11-12 21:02:12+00:00,,2,missing space treatment around punctuation & digits,"## Feature description
I am trying to clean / normalize a text with potential missing spaces around punctuation, like:
`word,digit` or `digit/digit`. 
I could not find any documentation how to tweak it with the current default tokenizer. 

```
import spacy
nlp = spacy.load(""en_core_web_sm"")

[(x, x.pos_) for x in nlp('fibrous type,5 mm in diameter,partially resected')]

# [(fibrous, 'ADJ'),
#  (type,5, 'PROPN'),
#  (mm, 'PROPN'),
#  (in, 'ADP'),
#  (diameter, 'NOUN'),
#  (,, 'PUNCT'),
#  (partially, 'ADV'),
#  (resected, 'VERB')]
```
This also relates to the problem of inconsistent tokenization of `/` in fractions:

```
[(x, x.pos_) for x in nlp('0/1 lymph nodes, 1/2 full')]
# [(0/1, 'PUNCT'),
#  (lymph, 'PROPN'),
#  (nodes, 'PROPN'),
#  (,, 'PUNCT'),
#  (1/2, 'NUM'),
#  (full, 'ADJ')]
```",,,,,,,1,,,
1888,https://github.com/explosion/spaCy/issues/6126,6126,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 514165920, 'node_id': 'MDU6TGFiZWw1MTQxNjU5MjA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20en', 'name': 'lang / en', 'color': '726DA8', 'default': False, 'description': 'English language data and models'}, {'id': 881718446, 'node_id': 'MDU6TGFiZWw4ODE3MTg0NDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20tokenizer', 'name': 'feat / tokenizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Tokenizer'}]",closed,2020-09-23 12:41:39+00:00,,4,Feature Request-ish: Consider adding the semi-colon as a default INFIX for the English tokenizer,"<!--- Please provide a summary in the title and describe your issue here.
Is this a bug or feature request? If a bug, include all the steps that led to the issue.

If you're looking for help with your code, consider posting a question on Stack Overflow instead:
http://stackoverflow.com/questions/tagged/spacy -->

I'd argue that a semi-colon in the middle of some English string is always a good place to split.  Just as you have the comma as both SUFFIX and INFIX by default, I'd add the semi-colon.

I noticed this behavior when doing tokenization of list items separated by semi-colons.  With no space after the semi-colon, the whole mess got tokenized as a single token.

Thanks.
",,,,,,,1,,,
24,https://github.com/explosion/spaCy/issues/2820,2820,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2018-10-03 15:51:10+00:00,,5,Installation of models error,"**Hi, I've been having a problem installing language models after the installation of spacy. Ive installed spacy on a virtual environment (in a virtual machine - to avoid corporate security blocks). However when I run: ""python -m spacy download en"" I receive the following error:**

(.env) C:\Users\DTUser\python\.env>python -m spacy download en
_Traceback (most recent call last):
  File ""C:\Users\DTUser\AppData\Local\Programs\Python\Python37-32\lib\runpy.py"",
 line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""C:\Users\DTUser\AppData\Local\Programs\Python\Python37-32\lib\runpy.py"",
 line 142, in _get_module_details
    return _get_module_details(pkg_main_name, error)
  File ""C:\Users\DTUser\AppData\Local\Programs\Python\Python37-32\lib\runpy.py"",
 line 109, in _get_module_details
    __import__(pkg_name)
  File ""C:\Users\DTUser\python\.env\lib\site-packages\spacy\__init__.py"", line 4
, in <module>
    from .cli.info import info as cli_info
  File ""C:\Users\DTUser\python\.env\lib\site-packages\spacy\cli\__init__.py"", li
ne 1, in <module>
    from .download import download
  File ""C:\Users\DTUser\python\.env\lib\site-packages\spacy\cli\download.py"", li
ne 11, in <module>
    from .link import link
  File ""C:\Users\DTUser\python\.env\lib\site-packages\spacy\cli\link.py"", line 8
, in <module>
    from ..compat import symlink_to, path2str
  File ""C:\Users\DTUser\python\.env\lib\site-packages\spacy\compat.py"", line 9,
in <module>
    from thinc.neural.util import copy_array
  File ""C:\Users\DTUser\python\.env\lib\site-packages\thinc\neural\__init__.py"",
 line 1, in <module>
    from ._classes.model import Model
  File ""C:\Users\DTUser\python\.env\lib\site-packages\thinc\neural\_classes\mode
l.py"", line 12, in <module>
    from ..train import Trainer
  File ""C:\Users\DTUser\python\.env\lib\site-packages\thinc\neural\train.py"", li
ne 3, in <module>
    from .optimizers import Adam, SGD, linear_decay
  File ""optimizers.pyx"", line 13, in init thinc.neural.optimizers
  File ""ops.pyx"", line 1, in init thinc.neural.ops
ImportError: murmurhash.mrmr does not export expected C function hash128_x86_

And installing the model direcly from github doesnt help:


__Could not install packages due to an EnvironmentError: HTTPSConnectionPool(host=
'github-production-release-asset-2e65be.s3.amazonaws.com', port=443): Max retrie
s exceeded with url: /84940268/69ded28e-c3ef-11e7-94dc-d5b03d9597d8?X-Amz-Algori
thm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20181003%2Fus-east-
1%2Fs3%2Faws4_request&X-Amz-Date=20181003T152038Z&X-Amz-Expires=300&X-Amz-Signat
ure=c681200f1e9ed510ea0f0e788f8a58e02f1698b027709076ad416e020825786e&X-Amz-Signe
dHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3
Den_core_web_sm-2.0.0.tar.gz&response-content-type=application%2Foctet-stream (C
aused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED]
certificate verify failed: unable to get local issuer certificate (_ssl.c:1045)'
)))__

I have tried updating pip or downloading different models with no avail. Im quite new to python so any help would be greatly appreciated!

## How to reproduce the problem
create and activate virtual environment then run:
""python -m spacy download en""
and then
""pip install https://github.com/explosion/spac
y-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz
Collecting https://github.com/explosion/spacy-models/releases/download/en_core_w
eb_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz)""

## Your Environment
I also got an error when using the  `python -m spacy info --markdown` command. 
* Operating System: Virtual machine (Windows 7) within windows 10 local computer
* Python Version Used: 3.7
* spaCy Version Used: 2.1.12
",,,,,,,1,,,
684,https://github.com/explosion/spaCy/issues/3777,3777,[],closed,2019-05-26 02:52:02+00:00,,2,spaCy download command fails to install en_core_web_sm in conda environment,"<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

## How to reproduce the problem
<!-- Include the details of how the problem occurred. Which command did you run to install spaCy? Did you come across an error? What else did you try? -->
I created a conda environment with spaCy using the command:

>conda create -n fever spacy

This successfully created a conda environment with spaCy version 2.0.16.

I then used the standard spaCy download command:

>python -m spacy download en_core_web_sm

The model seems to download succesfully, but I get an an EnvironmentError message.


```bash
ERROR: Error [WinError 87] The parameter is incorrect while executing command python setup.py egg_info
ERROR: Could not install packages due to an EnvironmentError: [WinError 87] The parameter is incorrect
```



## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
## Info about spaCy

* **spaCy version:** 2.0.16
* **Platform:** Windows-10-10.0.18362-SP0
* **Python version:** 3.7.3
",,,,,,,1,,,
1845,https://github.com/explosion/spaCy/issues/5967,5967,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 2103359118, 'node_id': 'MDU6TGFiZWwyMTAzMzU5MTE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/resolved', 'name': 'resolved', 'color': 'f6f6f6', 'default': False, 'description': 'The issue was addressed / answered'}]",closed,2020-08-25 00:43:09+00:00,,6,spaCy-models: Please Consider Distributing via PyPi,"## Feature Summary

Release `spaCy` models via PyPi

## Feature Description

We use `spaCy` in an enterprise setting. For security, the hosts that build production docker images cannot connect to the external internet. This introduces complexity when trying to install packages like `spacy-models`, where the recommended installation method is to either install from a Github release (requiring a connection to github.com) or to vendor the package (avoids networking issues, but bloats individual repos).

Publishing the models through PyPi would be beneficial in that `spacy-models` would no longer be installed differently than other packages & would also allow us to benefit from the security that PyPi provides (e.g. ability to mirror the package index on our internal network, assurance that package versions are immutable, etc.).

Perhaps you could start with adding the small models to PyPi, as they would not run into default package size restrictions. PyPi allows package authors to [file a request](https://github.com/pypa/pypi-support/issues/new?labels=limit+request&template=limit-request.md&title=Limit+Request%3A+PROJECT_NAME+-+000+MB) increasing the maximum allowable size of the package: the increased limits would easily support the medium models. There is also [precedent](https://github.com/pypa/pypi-support/issues/50) for setting size limits that would allow for distributing the large models as well.",,,,,,,1,,,
2400,https://github.com/explosion/spaCy/issues/7761,7761,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 2103359118, 'node_id': 'MDU6TGFiZWwyMTAzMzU5MTE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/resolved', 'name': 'resolved', 'color': 'f6f6f6', 'default': False, 'description': 'The issue was addressed / answered'}]",closed,2021-04-13 06:21:58+00:00,,9,Getting error while downloading language model,"
- Operating System: Win 10 64 Bit
- Python Version Used:  3.7
- spaCy Version Used: 3.0
- Environment Information: Base

I am getting below error when i try to download any language model from spacy
 WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)'))': /explosion/spacy-models/releases/download/nl_core_news_sm-3.0.0/nl_core_news_sm-3.0.0-py3-none-any.whl

ERROR: Could not install packages due to an EnvironmentError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /explosion/spacy-models/releases/download/nl_core_news_sm-3.0.0/nl_core_news_sm-3.0.0-py3-none-any.whl (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)')))
",,,,,,,1,,,
2449,https://github.com/explosion/spaCy/issues/7962,7962,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2021-04-30 09:55:57+00:00,,3,Installing numpy and spaCy on an Apple M1,"### Update June 2021: numpy 1.21 released

With the release of numpy 1.21, you should be able to install spaCy with a
simple `pip install` command as long as Xcode and python are installed:

```bash
pip install spacy
```

Alternatively, you can continue to use the experimental conda-forge packages as
described in option 3.

----------

Notes:

This guide is cross-posted and updated slightly from the [original post](https://adrianeboyd.github.io/installing-numpy-spacy-apple-m1/) for better visibility for spaCy users. Note that this situation should improve after the release of numpy 1.21.

If you run into problems, please search for related questions in the [Installation section](https://github.com/explosion/spaCy/discussions/categories/help-installation) of the new discussion board, and start a new discussion thread if needed.

----------

## Installing numpy 1.20 or earlier and spaCy on an Apple M1

Installing numpy or packages that compile against numpy on an Apple M1 is not straight-forward because the most recent version of numpy (1.20) requires a version of wheel in `pyproject.toml` ([PEP 518](https://www.python.org/dev/peps/pep-0518/)) that doesn't work on Big Sur. Since numpy and spaCy are configured to use build isolation by default ([PEP 517](https://www.python.org/dev/peps/pep-0517/)), a simple `pip install spacy` does not work: the numpy install fails due to the version of wheel and then the spaCy install fails due to the failed numpy installation. (For reference, you need `wheel>=0.36.1` for Big Sur. The good news is that it looks like this will be fixed in numpy 1.21.)

This guide describes three options for installing numpy and spaCy that have been tested as of March 2021:

- Option 1: Install with pip with build isolation

  - Install spaCy and its dependencies with pip with the fewest steps
  - For users who want to install spaCy once

- Option 2: Install with pip without build isolation

  - Primarily for developers who plan to recompile spaCy frequently

- Option 3: Install binary packages from conda-forge

  - No compiling required using the experimental [miniforge OS X ARM64 conda installer](https://github.com/conda-forge/miniforge)
  - For conda users who want to install spaCy once without compiling

### Install Xcode and Python

If necessary, install Xcode and Python.

Install Xcode:

```bash
xcode-select --install
```

Install Python 3.9 from:

- Python 3.9.4 (or newer) from source or the universal2 installer:

  [https://www.python.org/downloads/release/python-394/](https://www.python.org/downloads/release/python-394/)

- Python 3.9 from conda-forge:

  [https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh](https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh)

- Python 3.9 from homebrew:

  [https://formulae.brew.sh/formula/python@3.9](https://formulae.brew.sh/formula/python@3.9)

- **NOT RECOMMENDED:** The system Python 3.8.2 in `/usr/bin/python3` does not seem to work correctly: I could compile and install numpy 1.20.1, but numpy segfaults in the middle of its core test suite. Python's official support for Big Sur starts at version 3.9.1.

### Option 1: Install spaCy (and numpy) with pip with build isolation

The following steps describe how to install spaCy and numpy with pip with build isolation enabled. After you've gotten through the initial setup, spaCy can be installed or updated with a single `pip` command in a new virtual environment.

1. Install a BLAS library. As an example, install OpenBLAS through homebrew:

   ```bash
   brew install openblas
   ```

1. Create and activate a new virtual environment:

   - venv

     ```bash
     python3.9 -m venv myvenv
     source myvenv/bin/activate
     python -m pip install -U pip setuptools wheel
     ```

   - conda

     ```bash
     conda create -n myvenv python=3.9
     conda activate myvenv
     ```

1. Create a file called `constraints.txt` that specifies an older version of numpy without the `wheel` problem for use in isolated builds:

   ```text
   numpy==1.19.2
   ```

1. In the `pip install` command, set `PIP_CONSTRAINT` to the path to `constraints.txt` and `OPENBLAS` to the path for openblas (or whichever [numpy settings](https://numpy.org/devdocs/user/building.html) you choose):

   ```bash
   PIP_CONSTRAINT=constraints.txt OPENBLAS=""$(brew --prefix openblas)"" pip install spacy
   ```

   It will take 7-8 minutes to compile and install all the required packages.

   Note that setting `PIP_CONSTRAINT` **IS NOT** equivalent to setting `pip install -c constraints.txt`. The `-c constraints.txt` setting is only applied to the packages specified at the top level with `pip install package`, not within the isolated build environments. In contrast, the environment variable `PIP_CONSTRAINT` is visible at the top level and within the isolated build environments, so the numpy constraints are set correctly in the isolated builds for spaCy and its dependencies.

### Option 2: Install numpy and spaCy without build isolation

This option is best for developers who are recompiling spaCy frequently. If you disable build isolation, you need to install the requirements once initially by hand, but then you can use your local numpy installation for all future builds and recompile spaCy quickly.

To do this, you have to go step by step through the dependencies that compile against numpy to install their requirements and disable build isolation.

If you're using python from conda-forge, you might want to go ahead and use their numpy+openblas, too, and then you wouldn't need to install openblas separately with homebrew.

1.  Install numpy with either conda or pip:

    - conda (BLAS libraries are included automatically):

      ```bash
      conda install numpy
      ```

    - pip (install OpenBLAS with homebrew as in option 1)

       ```bash
       pip install cython
       OPENBLAS=""$(brew --prefix openblas)"" pip install numpy --no-build-isolation
       ```

      With build isolation disabled, you can install the most recent version of numpy.

    - for developers: install a nightly dev numpy wheel from scipy (OpenBLAS is included)

      Download a wheel ending in `cp39-macosx_11_0_arm64.whl` from https://anaconda.org/scipy-wheels-nightly/numpy/files
 and install with pip, e.g.:

       ```bash
       pip install numpy-1.21.0.dev0+1386.ga9409e139-cp39-cp39-macosx_11_0_arm64.whl
       ```
      
1.  Install the dependencies for `spacy` step by step, installing the requirements and disabling build isolation in turn for each dependency that compiles against numpy:

    ```bash
    pip install -r https://raw.githubusercontent.com/explosion/cython-blis/master/requirements.txt
    pip install blis --no-build-isolation
    pip install -r https://raw.githubusercontent.com/explosion/thinc/master/requirements.txt
    pip install thinc --no-build-isolation
    pip install -r https://raw.githubusercontent.com/explosion/spaCy/master/requirements.txt
    pip install spacy --no-build-isolation
    ```

    Note that the requirements from the `master` branch in the repo may be slightly ahead of the latest `blis`/`thinc`/`spacy` releases on PyPI. If you replace `master` with the exact version above (e.g., `v3.0.5`, so `https://raw.githubusercontent.com/explosion/spaCy/v3.0.5/requirements.txt`), you can access the exact requirements for a particular release.

1.  If you have a local copy of the spaCy repo and you're planning on recompiling frequently, you can create an editable install by replacing the last step in the previous section with:

    ```bash
    cd spaCy
    pip install --no-build-isolation --editable .
    ```

    Or using `setup.py` and parallel build jobs for spaCy to speed things up:

    ```bash
    cd spaCy
    python setup.py build_ext --inplace -j 4
    python setup.py develop
    ```

### Option 3: Binary packages from conda-forge

If you'd rather not compile anything, there are experimental OS X ARM 64 packages on [`conda-forge`](https://anaconda.org/conda-forge/spacy/) for all recent releases of spaCy, including spaCy v2.3. You can use the [miniforge installer](https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh) and then install spaCy with:

```bash
conda install spacy
```

or for spaCy v2:

```bash
conda install ""spacy~=2.3.5""
```

Be aware that the conda-forge packages are cross-compiled and not tested during the conda-forge build process. However I can report that I've run all the tests locally with no issues.

### Run the test suite

To test your local install, install the development dependencies and run the test suite. To simplify the instructions here, these are development dependencies for spaCy v3.0.5. Newer versions may be slightly different (check [`requirements.txt`](https://github.com/explosion/spaCy/blob/master/requirements.txt) in the spaCy repo):

```bash
pip install ""pytest>=5.2.0"" ""pytest-timeout>=1.3.0,<2.0.0"" ""mock>=2.0.0,<3.0.0"" ""flake8>=3.5.0,<3.6.0"" hypothesis
pytest --pyargs spacy
```",,,,,,,1,,,
267,https://github.com/explosion/spaCy/issues/3175,3175,"[{'id': 981590115, 'node_id': 'MDU6TGFiZWw5ODE1OTAxMTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20lemmatizer', 'name': 'feat / lemmatizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Rule-based and lookup lemmatization'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2019-01-21 00:13:53+00:00,,6,Lemmatizer fails on popular English proper nouns,"English lemmatizer seems to fail converting popular English proper nouns to a singular form, for example: ""Americans"", ""Russians"", ""Democrats"", ""Republicans"". It also fails on common nouns, when they are used as proper nouns, such as ""60 Minutes show"". In all these cases POS is properly identified as a proper noun.

The issue might be related to this one: https://github.com/explosion/spaCy/issues/781.

## Your Environment
* Operating System: MacOS Mojave v10.14.2
* Python Version Used: 2.7.15
* spaCy Version Used: 2.0.18",,,,,,1,,,,
321,https://github.com/explosion/spaCy/issues/3258,3258,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 703368580, 'node_id': 'MDU6TGFiZWw3MDMzNjg1ODA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20all', 'name': 'lang / all', 'color': '726DA8', 'default': False, 'description': 'Global language data'}]",closed,2019-02-11 21:29:51+00:00,,8,Package size issue: consider a different format for lemmatizer dictionaries,"## Feature description

The spacy package is almost 300 MB, which is a lot. This comes mostly from the `spacy/lang/*/lemmatizer` files which include huge lookup tables in uncompressed Python source files.

Storing this as package_data, in gzip'd form, could save about 90%.

<!-- Please describe the feature: Which area of the library is it related to? What specific solution would you like? -->

## Could the feature be a [custom component](https://spacy.io/usage/processing-pipelines#custom-components) or [spaCy plugin](https://spacy.io/universe)?

No. This would mean a change to this repository's structure, and the need for the maintainers to run a little script to create the gzip'd files from the source data before release. But going from 300 MB to 30 MB I think is worth it.",,,,,,1,,,,
343,https://github.com/explosion/spaCy/issues/3297,3297,"[{'id': 981590115, 'node_id': 'MDU6TGFiZWw5ODE1OTAxMTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20lemmatizer', 'name': 'feat / lemmatizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Rule-based and lookup lemmatization'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2019-02-19 22:50:13+00:00,,2,Lemmatizer doesn't always work for capitalized text (text in all caps and first letter capitalized cases),"**How to reproduce the behaviour**

The lemmatizer doesn't always work properly when text is provided in uppercase or is capitalized. In the example below, IDENTIFIED and Identify are not lemmatized, GROUPING and MASKS are not lemmatized, however INSTRUCTIONS is lemmatized. 

```
nlp = spacy.load('en_core_web_sm')
doc = nlp(u""IDENTIFIED Identified identified MISTAKES WITH GROUPING grouping MASKS INSTRUCTIONS Masks"")
[token.lemma_ for token in doc]
>>> ['identified', 'identified', 'identify', 'mistakes', 'with', 'grouping', 'group', 'masks', 'instruction', 'mask']  
```


**Your Environment**

* Operating System: macOS High Sierra 10.13.6
* Python Version Used: 3.6.5 
* spaCy Version Used: 2.0.18

",,,,,,1,,,,
445,https://github.com/explosion/spaCy/issues/3444,3444,"[{'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2019-03-20 14:59:27+00:00,,3,Incorrect lemma from lemmatizer,"**Right:**
`[w.lemma_ for w in nlp('funnier')]` -> `['funny']` 

**Wrong:**
`[w.lemma_ for w in nlp('faster')]` ->`['faster']`

I think for word _faster_ lemma should be _fast_",,,,,,1,,,,
618,https://github.com/explosion/spaCy/issues/3700,3700,"[{'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}, {'id': 981590115, 'node_id': 'MDU6TGFiZWw5ODE1OTAxMTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20lemmatizer', 'name': 'feat / lemmatizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Rule-based and lookup lemmatization'}]",closed,2019-05-07 23:02:27+00:00,,4,incorrect lemma from lemmatizer ,"## How to reproduce the behavior
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

I am not sure it's a bug, but just wanted to let you know that the lemma of the following words: 'car', 'carriers', 'scar',  (SIM) card, 
is always car. Is it correct from the lemmatization point of views? 

## Your Environment 
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: windows 
* Python Version Used: 3.7 
* spaCy Version Used: 2.1.1 
* Environment Information: conda 
",,,,,,1,,,,
636,https://github.com/explosion/spaCy/issues/3724,3724,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}]",closed,2019-05-11 11:35:38+00:00,,2,Lemmatizer and Pipeline conflicting results,"## How to reproduce the behavior
If I run:
```py
from spacy.lemmatizer import Lemmatizer
lemmatizer = Lemmatizer()

print(lemmatizer(""faces"", ""NN""))
print(lemmatizer(""faces"", ""NNS""))
```
I get ""faces""

If I run: 
```py
import spacy
nlp = spacy.load('en', disable=[""parser"", ""ner""])
print(nlp(""faces"")[0].lemma_)
```
I get ""face""


## Info about spaCy
* **spaCy version:** 2.1.3                                         
* **Platform:** Linux-3.10.0-957.10.1.el7.x86_64-x86_64-with-centos-7.6.1810-Cor e
* **Python version:** 3.7.3 
* **Models:** en
--



",,,,,,1,,,,
651,https://github.com/explosion/spaCy/issues/3740,3740,[],closed,2019-05-14 09:39:45+00:00,,7,Question regarding lemmatizer on verb ending with -ing / noun,"<!-- Describe your issue here. Please keep in mind that the GitHub issue tracker is mostly intended for reports related to the spaCy code base and source, and for bugs and feature requests. If you're looking for help with your code, consider posting a question on Stack Overflow instead: http://stackoverflow.com/questions/tagged/spacy -->
Hi! I'd like to change the behaviour of the lemmatizer regarding verb ending with -ing.
In the current version, sometimes, when a noun like ""plastering"", ""quilting"" or ""painting"" is being processed, the result of the lemmatization is the ""basic form"" of the verb.
For example:
""plastering"" -> ""plaster""
""quilting"" -> ""quilt""
""plastic tube for building"" -> ""plastic tube for build""

I'd like to keep the whole noun after the lemmatization and even if I have to ""hardcode"" it. Is this something easy to set up ? How can I do so ?

Thank you for your help

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows
* Python Version Used: 3.6.6
* spaCy Version Used: 2.1.3



",,,,,,1,,,,
924,https://github.com/explosion/spaCy/issues/4126,4126,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 703433806, 'node_id': 'MDU6TGFiZWw3MDM0MzM4MDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20nb', 'name': 'lang / nb', 'color': '726DA8', 'default': False, 'description': 'Norwegian (Bokm氓l) language data and models'}, {'id': 981590115, 'node_id': 'MDU6TGFiZWw5ODE1OTAxMTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20lemmatizer', 'name': 'feat / lemmatizer', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Rule-based and lookup lemmatization'}]",closed,2019-08-15 15:07:28+00:00,,2,Empty files in Norwegian lemmatizer,"Not sure if this is an issue or not, but there are several files in the Norwegian (`nb`) lemmatizer directory that are basically empty. Here's an example - this is the whole file:

```
# coding: utf8
from __future__ import unicode_literals


NOUNS = set(
    """"""
"""""".split()
)
```

The `NOUNS` object is imported and used elsewhere so maybe it's a placeholder for something but this just seems pointless. 

If this actually serves no purpose, I guess the files should be deleted. On the other hand, if it's there for a reason, it's probably best to add a comment explaining what that is. ",,,,,,1,,,,
946,https://github.com/explosion/spaCy/issues/4164,4164,"[{'id': 542391290, 'node_id': 'MDU6TGFiZWw1NDIzOTEyOTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/meta', 'name': 'meta', 'color': 'f6f6f6', 'default': False, 'description': 'Meta topics, e.g. repo organisation and issue management'}, {'id': 739883093, 'node_id': 'MDU6TGFiZWw3Mzk4ODMwOTM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20hr', 'name': 'lang / hr', 'color': '726da8', 'default': False, 'description': 'Croatian language data and models'}]",closed,2019-08-21 18:44:14+00:00,,7,Creating a Croatian lemmatizer - licence issue,"Hello all,

I am looking to extend the croatian language model in spaCy with a look-up lemmatizer.
I found a great source of lemmas (over 100 000 lemmas in more than a million forms) here : http://meta-share.ffzg.hr/repository/browse/croatian-morphological-lexicon-v50/2d429672703d11e28a985ef2e4e6c59e27b37c59b92d42a5be839f7daff7ecfb/

I have already adaptet the lexicon into a lemmatizer and tested it out locally. It seems to work well.
The issue I`m having with submitting this lemmatizer to the spaCy codebase is the lexicons licence:

The lexicon is licensed under CC BY NC SA licence and spaCy is released under an MIT licence.

TL;DR; Is it okay to adapt the Croatian morphological lexicon into a lemmatizer given the different licenses that the two projects use?",,,,,,1,,,,
1084,https://github.com/explosion/spaCy/issues/4406,4406,"[{'id': 312256136, 'node_id': 'MDU6TGFiZWwzMTIyNTYxMzY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/docs', 'name': 'docs', 'color': '087EA6', 'default': False, 'description': 'Documentation and website'}]",closed,2019-10-09 04:37:29+00:00,,3,Misspelling on Lemmatizer Example,"<!-- Describe the problem or suggestion here. If you've found a mistake and you know the answer, feel free to submit a pull request straight away: https://github.com/explosion/spaCy/pulls -->

## Which page or section is this issue related to?
<!-- Please include the URL and/or source. -->
from spacy.lemmatizer import Lemmatizer
from spacy.lookups import Lookups
_**lookups = Loookups()**_
lookups.add_table(""lemma_rules"", {""noun"": [[""s"", """"]]})
lemmatizer = Lemmatizer(lookups)
lemmas = lemmatizer(""ducks"", ""NOUN"")
assert lemmas == [""duck""] 
![Screenshot from 2019-10-09 07-35-15](https://user-images.githubusercontent.com/13104952/66451976-6ba4bc00-ea67-11e9-9909-e625073bfab7.png)

Suggestion:
 Remove extra **_o_** in the _**lookups = Loookups()**_",,,,,,1,,,,
1691,https://github.com/explosion/spaCy/issues/5539,5539,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446605073, 'node_id': 'MDU6TGFiZWw0NDY2MDUwNzM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/windows', 'name': 'windows', 'color': '676F6D', 'default': False, 'description': 'Issues related to Windows'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2020-06-03 07:21:22+00:00,,5,Windows Docker - Spacy language model installation in python returns ImportError: DLL load failed: The specified module could not be found,"I am building a Windows-based Docker image to run a Flask Application. For that, I need SpaCy language model to be installed. But I am encountering the following issue again and again and could not find any robust solution till yet.

**Runtime:** Windows Container (Docker)

**Error Trace:**  

    Step 6/9 : RUN python -m spacy download en_core_web_sm
     ---> Running in 6f8f33207c8f
            Traceback (most recent call last):
          File ""C:\Python\lib\runpy.py"", line 183, in _run_module_as_main
            mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
          File ""C:\Python\lib\runpy.py"", line 142, in _get_module_details
            return _get_module_details(pkg_main_name, error)
          File ""C:\Python\lib\runpy.py"", line 109, in _get_module_details
            __import__(pkg_name)
          File ""C:\Python\lib\site-packages\spacy\__init__.py"", line 12, in <module>
            from . import pipeline
          File ""C:\Python\lib\site-packages\spacy\pipeline\__init__.py"", line 4, in <module>
            from .pipes import Tagger, DependencyParser, EntityRecognizer, EntityLinker
          File ""pipes.pyx"", line 1, in init spacy.pipeline.pipes
        ImportError: DLL load failed: The specified module could not be found.


**Dockerfile:**

    FROM winamd64/python:3.7-windowsservercore
    COPY requirements.txt .
    COPY models/* ./models/
    RUN pip install --no-cache-dir -r requirements.txt
    RUN python -m nltk.downloader stopwords
    RUN python -m spacy download en_core_web_sm
    COPY . .
    EXPOSE 5000
    CMD python waitress_server.py

**requirements.txt:**  

    Flask==1.1.1
    future==0.17.1
    httplib2==0.13.1
    nltk==3.4.5
    numpy==1.18.2
    pandas
    pandocfilters==1.4.2
    pickleshare==0.7.5
    regex==2019.8.19
    requests>=2.13.0
    requests-oauthlib
    requests-toolbelt
    scikit-learn==0.22.1
    scipy==1.3.1
    simplejson==3.16.0
    urllib3==1.24.3
    xlrd==1.2.0
    zipp==0.6.0
    lightgbm
    sner
    flask-bcrypt
    waitress==1.4.4
    spacy

**waitress_server.py:**

    print(""Hello World"")

Other files are some trained NLP models.

**Note:** 

 - I have tried with multiple versions of windows images but no result
 - I have tried with multiple NumPy and spacy versions
 - Found an Issue on GitHub [https://github.com/explosion/spaCy/issues/4733#issuecomment-561123652][1] But it doesn't seem to solve the issue.

Thanks in Advance!


  [1]: https://github.com/explosion/spaCy/issues/4733#issuecomment-561123652",,1,,,1,,,,,
2414,https://github.com/explosion/spaCy/issues/7820,7820,[],closed,2021-04-19 01:11:39+00:00,,2,ImportError: [E048] Can't import language en from spacy.lang: cannot import name 'PRON_LEMMA' from 'spacy.symbols',"## How to reproduce the behaviour

```
from collections import Counter   #this package helps us get the most frequent words out of a list
import re    #we need this module to define regex
import os    
import os.path
import string
import nltk
from nltk.tokenize import MWETokenizer  #import tokenizer
from nltk import word_tokenize, pos_tag  #we need thse packages to identify the tense (future vs past) of a textual document
tokenizer = MWETokenizer()
nltk.download('stopwords')  #download the list of stopwords, if you have not already done so
from nltk.corpus import stopwords  #import the list of stopwords
from nltk.stem.snowball import SnowballStemmer  #import stemmer module
stemmer = SnowballStemmer('english')
import pandas as pd

pip install spacy==2.3.5

pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz



import spacy    #this package helps us identify location (cities, states, rivers etc.)

import en_core_web_sm
nlp = en_core_web_sm.load()
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows 
* Python Version Used: 3.8 
* spaCy Version Used: 2.3.5
* Environment Information: N/A 
",,1,,,1,,,,,
86,https://github.com/explosion/spaCy/issues/2898,2898,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2018-11-03 14:23:51+00:00,,2,ImportError: cannot import name 'ADVERBS',"Checked #2093 for a similar issue which was resolved by setting Locale however mine seems not caused by Locale. (I could be wrong and appreciate your help)

Spacy and en were working well before today when I had the below issue. It was generally caused by ""from ._adverbs import ADVERBS"" with the error ""ImportError: cannot import name 'ADVERBS'"". Below are the details.

>> import Spacy
>> spacy.load('en')
```bash
Traceback (most recent call last):
  File ""Anaconda3\lib\site-packages\spacy\util.py"", line 50, in get_lang_class
    module = importlib.import_module('.lang.%s' % lang, 'spacy')
  File 鈥淺Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""\Anaconda3\lib\site-packages\spacy\lang\en\__init__.py"", line 10, in <module>
    from .lemmatizer import LEMMA_RULES, LEMMA_INDEX, LEMMA_EXC, LOOKUP
  File ""\Anaconda3\lib\site-packages\spacy\lang\en\lemmatizer\__init__.py"", line 7, in <module>
    from ._adverbs import ADVERBS
ImportError: cannot import name 'ADVERBS'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File \Anaconda3\lib\site-packages\spacy\__init__.py"", line 21, in load
    return util.load_model(name, **overrides)
  File ""\Anaconda3\lib\site-packages\spacy\util.py"", line 112, in load_model
    return load_model_from_link(name, **overrides)
  File ""\Anaconda3\lib\site-packages\spacy\util.py"", line 129, in load_model_from_link
    return cls.load(**overrides)
  File ""\Anaconda3\lib\site-packages\spacy\data\en\__init__.py"", line 12, in load
    return load_model_from_init_py(__file__, **overrides)
  File ""\Anaconda3\lib\site-packages\spacy\util.py"", line 173, in load_model_from_init_py
    return load_model_from_path(data_path, meta, **overrides)
  File ""\Anaconda3\lib\site-packages\spacy\util.py"", line 143, in load_model_from_path
    cls = get_lang_class(meta['lang'])
  File ""\Anaconda3\lib\site-packages\spacy\util.py"", line 52, in get_lang_class
    raise ImportError(Errors.E048.format(lang=lang))
ImportError: [E048] Can't import language en from spacy.lang.
```
I reinstalled spacy, re-downloaded the en_core_web_md and updated the linkage as below. But the issue remains.
python -m spacy download en
python -m spacy download en_core_web_md
python -m spacy link en_core_web_md en --force
    Linking successful
    \Anaconda3\lib\site-packages\en_core_web_md -->
    \Anaconda3\lib\site-packages\spacy\data\en
    You can now load the model via spacy.load('en')

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Win10
* Python Version Used: 3.6.5
* spaCy Version Used: 2.0.16
* Environment Information:  en_core_web_md
",,,,,1,,,,,
1136,https://github.com/explosion/spaCy/issues/4491,4491,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446637424, 'node_id': 'MDU6TGFiZWw0NDY2Mzc0MjQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/conda', 'name': 'conda', 'color': '676F6D', 'default': False, 'description': 'conda package manager'}]",closed,2019-10-21 14:44:03+00:00,,9,ImportError: cannot import name 'get_string_id' from 'spacy.strings',"Installing and loading spaCy on a new machine. I've done this a few times, but have never run into errors before. I had a handful error messages to get through just to install and import spacy (things like #2514) and in the end I manually deleted all spaCy files and directories to do a clean installation. 

Not sure what to do here.

### My code:
```
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English
parser = English()
from spacy import en_core_web_sm
nlp = spacy.load(""en_core_web_sm"")
```

### Full Traceback:
```---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-28-8db0160a0952> in <module>
      2 #!python -m spacy download en
      3 from spacy.lang.en.stop_words import STOP_WORDS
----> 4 from spacy.lang.en import English
      5 parser = English()
      6 from spacy import en_core_web_sm

~/anaconda3/lib/python3.7/site-packages/spacy/lang/en/__init__.py in <module>
     12 from ..tokenizer_exceptions import BASE_EXCEPTIONS
     13 from ..norm_exceptions import BASE_NORMS
---> 14 from ...language import Language
     15 from ...attrs import LANG, NORM
     16 from ...util import update_exc, add_lookups

~/anaconda3/lib/python3.7/site-packages/spacy/language.py in <module>
     15 from .vocab import Vocab
     16 from .lemmatizer import Lemmatizer
---> 17 from .lookups import Lookups
     18 from .pipeline import DependencyParser, Tagger
     19 from .pipeline import Tensorizer, EntityRecognizer, EntityLinker

~/anaconda3/lib/python3.7/site-packages/spacy/lookups.py in <module>
      8 from .errors import Errors
      9 from .util import SimpleFrozenDict, ensure_path
---> 10 from .strings import get_string_id
     11 
     12 

ImportError: cannot import name 'get_string_id' from 'spacy.strings' (/Users/lmcquillan/anaconda3/lib/python3.7/site-packages/spacy/strings.cpython-37m-darwin.so)
```

## My Environment
* **Operating System: Mac
* **spaCy version:** 2.2.1
* **Platform:** Darwin-18.7.0-x86_64-i386-64bit
* **Python version:** 3.7.3
* **Models:** en
",,,,,1,,,,,
1282,https://github.com/explosion/spaCy/issues/4733,4733,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446605073, 'node_id': 'MDU6TGFiZWw0NDY2MDUwNzM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/windows', 'name': 'windows', 'color': '676F6D', 'default': False, 'description': 'Issues related to Windows'}]",closed,2019-11-29 11:09:22+00:00,,8,ImportError: DLL load failed: The specified module could not be found.,"<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

<!-- Include the details of how the problem occurred. Which command did you run to install spaCy? Did you come across an error? What else did you try? -->
The error occurs after installation and running the following command for checking the validation:
`python -m spacy validate`

```bash
(venv) C:\Users\user_profile\Desktop\new_pro>python -m spacy validate
Traceback (most recent call last):
  File ""C:\Users\user_profile\AppData\Local\Programs\Python\Python36\lib\runpy.py"", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""C:\Users\user_profile\AppData\Local\Programs\Python\Python36\lib\runpy.py"", line 142, in _get_module_details
    return _get_module_details(pkg_main_name, error)
  File ""C:\Users\user_profile\AppData\Local\Programs\Python\Python36\lib\runpy.py"", line 109, in _get_module_details
    __import__(pkg_name)
  File ""C:\Users\user_profile\Desktop\new_pro\venv\lib\site-packages\spacy\__init__.py"", line 12, in <module>
    from . import pipeline
  File ""C:\Users\user_profile\Desktop\new_pro\venv\lib\site-packages\spacy\pipeline\__init__.py"", line 4, in <module>
    from .pipes import Tagger, DependencyParser, EntityRecognizer, EntityLinker
  File ""pipes.pyx"", line 1, in init spacy.pipeline.pipes
ImportError: DLL load failed: The specified module could not be found.

```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows 10 Pro N
* Python Version Used: 3.6.6
* spaCy Version Used: 2.2.3
* Environment Information: The result of running `python -m spacy info --markdown` is the same error message included above.
",,,,,1,,,,,
1460,https://github.com/explosion/spaCy/issues/5035,5035,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2020-02-19 12:18:59+00:00,,4,Packaging spacy with pyinstaller: ImportError: cannot import name _custom_kernels,"## Overview
I'm packaging a python script which imports spacy with pyinstaller https://github.com/pyinstaller/pyinstaller into an executable. I get the error in the title when running the executable.

## Environment
* Operating System: Linux-4.15.0-88-generic-x86_64-with-Ubuntu-18.04-bionic
* Python Version Used: 3.6.9
* spaCy Version Used: 2.2.3
* Spacy location: /home/user/spacy-pyinstaller/venv/lib/python3.6/site-packages/spacy
* pyinstaller Version Used: 4.0.dev0+a1f92c6a08


## Reproduction steps
All necessary files for reproduction are available at https://github.com/michaelhochleitner/spacy-pyinstaller .

I'm packaging the following python script. 
```
import spacy
```
I use the following command to package the script.
```
pyinstaller nlp.spec 
```

Here is my .spec file:
```
# -*- mode: python ; coding: utf-8 -*-

block_cipher = None


a = Analysis(['nlp.py'],
             pathex=['/home/user/spacy-pyinstaller'],
             binaries=[],
             datas=[],
             hiddenimports=['srsly.msgpack.util','cymem','cymem.cymem','preshed.maps','thinc.linalg','murmurhash'],
             hookspath=[],
             runtime_hooks=[],
             excludes=[],
             win_no_prefer_redirects=False,
             win_private_assemblies=False,
             cipher=block_cipher,
             noarchive=False)
pyz = PYZ(a.pure, a.zipped_data,
             cipher=block_cipher)
exe = EXE(pyz,
          a.scripts,
          [],
          exclude_binaries=True,
          name='nlp',
          debug=False,
          bootloader_ignore_signals=False,
          strip=False,
          upx=True,
          console=True )
coll = COLLECT(exe,
               a.binaries,
               a.zipfiles,
               a.datas,
               strip=False,
               upx=True,
               upx_exclude=[],
               name='nlp')

```
When running
```
./dist/nlp/nlp
```
I get the following output
```
Traceback (most recent call last):
  File ""nlp.py"", line 1, in <module>
    import spacy
  File ""/home/mh/spacy-pyinstaller/venv/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py"", line 489, in exec_module
    exec(bytecode, module.__dict__)
  File ""spacy/__init__.py"", line 10, in <module>
  File ""/home/mh/spacy-pyinstaller/venv/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py"", line 489, in exec_module
    exec(bytecode, module.__dict__)
  File ""thinc/neural/__init__.py"", line 4, in <module>
  File ""/home/mh/spacy-pyinstaller/venv/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py"", line 489, in exec_module
    exec(bytecode, module.__dict__)
  File ""thinc/neural/_classes/model.py"", line 11, in <module>
  File ""/home/mh/spacy-pyinstaller/venv/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py"", line 489, in exec_module
    exec(bytecode, module.__dict__)
  File ""thinc/neural/train.py"", line 7, in <module>
  File ""optimizers.pyx"", line 14, in init thinc.neural.optimizers
  File ""ops.pyx"", line 24, in init thinc.neural.ops
ImportError: cannot import name _custom_kernels
[16858] Failed to execute script nlp
```
I don't know if this issue should be in the pyinstaller project or the spacy project. So I'm opening issues in both.


## Related issues
https://github.com/pyinstaller/pyinstaller/issues/4696
https://github.com/explosion/spaCy/issues/2536
https://github.com/explosion/spaCy/issues/3831
https://stackoverflow.com/questions/59645155/spacy-2-2-3-filenotfounderror-errno-2-no-such-file-or-directory-thinc-neur
",,,,,1,,,,,
1544,https://github.com/explosion/spaCy/issues/5217,5217,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2020-03-27 14:58:25+00:00,,7,ImportError: DLL load failed: The specified module could not be found.,"## How to reproduce the behaviour
When running my python script in a Docker container on Windows, I get the following error when importing spacy :

```
Traceback (most recent call last):
  File ""application.py"", line 5, in <module>
    import spacy
  File ""C:\Python\lib\site-packages\spacy\__init__.py"", line 12, in <module>
    from . import pipeline
  File ""C:\Python\lib\site-packages\spacy\pipeline\__init__.py"", line 4, in <module>
    from .pipes import Tagger, DependencyParser, EntityRecognizer, EntityLinker
  File ""pipes.pyx"", line 1, in init spacy.pipeline.pipes
ImportError: DLL load failed: The specified module could not be found.
```

Exactly the same issue has been described before, see https://github.com/explosion/spaCy/issues/4733, albeit not running in Docker

When I execute the same script on the host computer, it runs fine (same python and spacy versions as in the container)

## Environment

* Operating System: Windows Server 2016
* Python Version Used: 3.7.5
* spaCy Version Used: 2.2.4
* Environment Information:
",,,,,1,,,,,
1846,https://github.com/explosion/spaCy/issues/5976,5976,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2020-08-26 10:19:37+00:00,,2,ImportError: preshed.maps does not export expected C function map_clear,"Just upgrade Spacy to version 2.3.2 to evoid the bug in 2.2.4 where token.morph.__repr__() returns a list instead of a str. Now when I `import spacy` I get the following exception:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/conda/anaconda3/envs/femdwell-debug/lib/python3.6/site-packages/spacy/__init__.py"", line 12, in <module>
    from . import pipeline
  File ""/home/conda/anaconda3/envs/femdwell-debug/lib/python3.6/site-packages/spacy/pipeline/__init__.py"", line 4, in <module>
    from .pipes import Tagger, DependencyParser, EntityRecognizer, EntityLinker
  File ""pipes.pyx"", line 25, in init spacy.pipeline.pipes
  File ""/home/conda/anaconda3/envs/femdwell-debug/lib/python3.6/site-packages/spacy/pipeline/functions.py"", line 5, in <module>
    from ..matcher import Matcher
  File ""/home/conda/anaconda3/envs/femdwell-debug/lib/python3.6/site-packages/spacy/matcher/__init__.py"", line 5, in <module>
    from .phrasematcher import PhraseMatcher
  File ""phrasematcher.pyx"", line 1, in init spacy.matcher.phrasematcher
ImportError: preshed.maps does not export expected C function map_clear
```
This only appears on one of my computers, but unfortunately it is the one where I need to develop.

The computer is a 64bit intel and is running Ubuntu 18.04.5 LTS . Spacy was installed into a conda (version 4.7.10) environment using ""pip install -U spacy"".",,,,,1,,,,,
1857,https://github.com/explosion/spaCy/issues/6022,6022,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 642658286, 'node_id': 'MDU6TGFiZWw2NDI2NTgyODY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20ja', 'name': 'lang / ja', 'color': '726DA8', 'default': False, 'description': 'Japanese language data and models'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2020-09-03 08:14:02+00:00,,5,ImportError: DLL load failed: The specified module could not be found. (Error while loading Japanese Model),"## How to reproduce the behaviour
On Anaconda Prompt, I created an environment : my_env
Here, I installed spacy (```pip install -U spacy```) and downloaded English and Japanese Model (```python -m spacy download ja_core_news_sm```). Loading english model gave no error.

But when I do : 
```
import spacy
nlp = spacy.load(""ja_core_news_sm"")
```

It gave the following error :

```
Traceback (most recent call last):
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\spacy\lang\ja\__init__.py"", line 31, in try_sudachi_import
    from sudachipy import dictionary, tokenizer
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\sudachipy\__init__.py"", line 15, in <module>
    from . import utf8inputtextbuilder
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\sudachipy\utf8inputtextbuilder.py"", line 16, in <module>
    from .dictionarylib.categorytype import CategoryType
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\anno\lib\site-packages\sudachipy\dictionarylib\__init__.py"", line 19, in <module>
    from . import doublearraylexicon
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\sudachipy\dictionarylib\doublearraylexicon.py"", line 17, in <module>
    from dartsclone import DoubleArray
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\dartsclone\__init__.py"", line 1, in <module>
    from dartsclone._dartsclone import DoubleArray
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\spacy\__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\spacy\util.py"", line 170, in load_model
    return load_model_from_package(name, **overrides)
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\spacy\util.py"", line 191, in load_model_from_package
    return cls.load(**overrides)
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\ja_core_news_sm\__init__.py"", line 12, in load
    return load_model_from_init_py(__file__, **overrides)
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\spacy\util.py"", line 239, in load_model_from_init_py
    return load_model_from_path(data_path, meta, **overrides)
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\spacy\util.py"", line 203, in load_model_from_path
    nlp = cls(meta=meta, **overrides)
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\spacy\language.py"", line 186, in __init__
    make_doc = factory(self, **meta.get(""tokenizer"", {}))
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\spacy\lang\ja\__init__.py"", line 273, in create_tokenizer
    return JapaneseTokenizer(cls, nlp, config)
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\spacy\lang\ja\__init__.py"", line 139, in __init__
    self.tokenizer = try_sudachi_import(self.split_mode)
  File ""C:\Users\nitin.trivedi\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\spacy\lang\ja\__init__.py"", line 44, in try_sudachi_import
    ""Japanese support requires SudachiPy and SudachiDict-core ""
ImportError: Japanese support requires SudachiPy and SudachiDict-core (https://github.com/WorksApplications/SudachiPy). Install with `pip install sudachipy sudachidict_core` or install spaCy with `pip install spacy[ja]`.
``` 

I checked for dartsclone and verified that it is already present.


## Environment
* Operating System: Windows 10
* Python Version Used: 3.6.10
* spaCy Version Used: 2.3.2
* Environment Information: Anaconda
",,,,,1,,,,,
1943,https://github.com/explosion/spaCy/issues/6329,6329,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2020-11-02 18:25:16+00:00,,3,ImportError: cannot import name set_gpu_allocator,"
![image](https://user-images.githubusercontent.com/46970976/97904467-8602a500-1d0e-11eb-8506-fcc46f0435bf.png)
Thinc version  8.0.0rc1. 
I'm trying to use the new feature from spacy-nightly so I download the latest version of thinc. However, I could not import certain function as the the demo code. 

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Azure R studio 
* Python Version Used: 3.7
* spaCy Version Used: 2.3 and nightly 

",,,,,1,,,,,
2074,https://github.com/explosion/spaCy/issues/6697,6697,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446605073, 'node_id': 'MDU6TGFiZWw0NDY2MDUwNzM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/windows', 'name': 'windows', 'color': '676F6D', 'default': False, 'description': 'Issues related to Windows'}]",closed,2021-01-08 07:22:23+00:00,,9,ImportError: DLL load failed while importing nn_parser: The specified module could not be found.,"when I try to download models or Validate or use info --markdown I get this error.
```
Traceback (most recent call last):
  File ""C:\Users\MSI\AppData\Local\Programs\Python\Python39\lib\runpy.py"", line 188, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""C:\Users\MSI\AppData\Local\Programs\Python\Python39\lib\runpy.py"", line 147, in _get_module_details
    return _get_module_details(pkg_main_name, error)
  File ""C:\Users\MSI\AppData\Local\Programs\Python\Python39\lib\runpy.py"", line 111, in _get_module_details
    __import__(pkg_name)
  File ""C:\Users\MSI\Desktop\py\nlps\lib\site-packages\spacy\__init__.py"", line 12, in <module>
    from . import pipeline
  File ""C:\Users\MSI\Desktop\py\nlps\lib\site-packages\spacy\pipeline\__init__.py"", line 4, in <module>
    from .pipes import Tagger, DependencyParser, EntityRecognizer, EntityLinker
  File ""pipes.pyx"", line 1, in init spacy.pipeline.pipes
ImportError: DLL load failed while importing nn_parser: The specified module could not be found.`
```

I've tried it with python 3.8.6 64-bit, 3.9.0 64-bit and with virtual env. I get the same error.

Here is my pip list for vitual env. 
```
blis       0.7.4
catalogue  1.0.0
certifi    2020.12.5
chardet    4.0.0
cymem      2.0.5
idna       2.10
murmurhash 1.0.5
numpy      1.19.5
pip        20.3.3
plac       1.1.3
preshed    3.0.5
requests   2.25.1
setuptools 49.2.1
spacy      2.3.5
srsly      1.0.5
thinc      7.4.5
tqdm       4.55.1
urllib3    1.26.2
wasabi     0.8.0
```

The message I get when I install spacy 
```
(nlps) C:\Users\MSI\Desktop\py>pip install spacy
Collecting spacy
  Using cached spacy-2.3.5-cp39-cp39-win_amd64.whl (9.4 MB)
Requirement already satisfied: setuptools in c:\users\msi\desktop\py\nlps\lib\site-packages (from spacy) (49.2.1)
Collecting blis<0.8.0,>=0.4.0
  Using cached blis-0.7.4-cp39-cp39-win_amd64.whl (6.5 MB)
Collecting catalogue<1.1.0,>=0.0.7
  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)
Collecting cymem<2.1.0,>=2.0.2
  Using cached cymem-2.0.5-cp39-cp39-win_amd64.whl (36 kB)
Collecting murmurhash<1.1.0,>=0.28.0
  Using cached murmurhash-1.0.5-cp39-cp39-win_amd64.whl (21 kB)
Collecting numpy>=1.15.0
  Using cached numpy-1.19.5-cp39-cp39-win_amd64.whl (13.3 MB)
Collecting plac<1.2.0,>=0.9.6
  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)
Collecting preshed<3.1.0,>=3.0.2
  Using cached preshed-3.0.5-cp39-cp39-win_amd64.whl (112 kB)
Collecting requests<3.0.0,>=2.13.0
  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)
Collecting certifi>=2017.4.17
  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)
Collecting chardet<5,>=3.0.2
  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)
Collecting idna<3,>=2.5
  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)
Collecting srsly<1.1.0,>=1.0.2
  Using cached srsly-1.0.5-cp39-cp39-win_amd64.whl (177 kB)
Collecting thinc<7.5.0,>=7.4.1
  Using cached thinc-7.4.5-cp39-cp39-win_amd64.whl (904 kB)
Collecting tqdm<5.0.0,>=4.38.0
  Downloading tqdm-4.55.1-py2.py3-none-any.whl (68 kB)
     |鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅鈻堚枅| 68 kB 138 kB/s
Collecting urllib3<1.27,>=1.21.1
  Using cached urllib3-1.26.2-py2.py3-none-any.whl (136 kB)
Collecting wasabi<1.1.0,>=0.4.0
  Using cached wasabi-0.8.0-py3-none-any.whl (23 kB)
Installing collected packages: numpy, murmurhash, cymem, wasabi, urllib3, tqdm, srsly, preshed, plac, idna, chardet, certifi, catalogue, blis, thinc, requests, spacy
Successfully installed blis-0.7.4 catalogue-1.0.0 certifi-2020.12.5 chardet-4.0.0 cymem-2.0.5 idna-2.10 murmurhash-1.0.5 numpy-1.19.5 plac-1.1.3 preshed-3.0.5 requests-2.25.1 spacy-2.3.5 srsly-1.0.5 thinc-7.4.5 tqdm-4.55.1 urllib3-1.26.2 wasabi-0.8.0
```",,,,,1,,,,,
2363,https://github.com/explosion/spaCy/issues/7646,7646,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446637295, 'node_id': 'MDU6TGFiZWw0NDY2MzcyOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/osx', 'name': 'osx', 'color': '676F6D', 'default': False, 'description': 'Issues related to macOS / OSX'}]",closed,2021-04-03 04:51:04+00:00,,5,[download en_core_web_sm] => ImportError dlopen  no suitable image found,"

## Spacy Language Installation Error
1. Create new python venv : `python3 -m venv ./venv`
2. Activate new python venv : `source ./venv/bin/activate`
3. Update pip, setuptools, wheel : `pip install -U pip setuptools wheel`
4. Install spacy [Latest] : `pip install -U spacy`
5. Install spacy language pack : `python -m spacy download en_core_web_sm`

## Error:
<img width=""1918"" alt=""error"" src=""https://user-images.githubusercontent.com/59952787/113468683-5266ab00-9465-11eb-88f7-0a9d3c7a2f69.png"">

## What I tried:
Following this [stackoverflow link](https://stackoverflow.com/questions/47858150/scipy-importerror-dlopen-no-suitable-image-found-in-python-3) I deleted whole venv folder, created a new one & then tried installing spacy with `python -m pip install spacy` Then tried installing language pack `python -m spacy download en_core_web_sm`

I also tried `python -m spacy.en.download all` that gave me same error.
<img width=""1920"" alt=""Screenshot 2021-04-03 at 10 18 42 AM"" src=""https://user-images.githubusercontent.com/59952787/113468788-fb150a80-9465-11eb-96ad-f203e6c6a33d.png"">


## Your Environment
Python Version : 3.7.9
OS : Mac Os Catilina v10.15.7
pip list in venv output:
```shell
Package             Version
------------------- ---------
appnope             0.1.2
argon2-cffi         20.1.0
async-generator     1.10
attrs               20.3.0
backcall            0.2.0
bleach              3.3.0
blis                0.7.4
catalogue           2.0.1
certifi             2020.12.5
cffi                1.14.5
chardet             4.0.0
click               7.1.2
cymem               2.0.5
decorator           5.0.3
defusedxml          0.7.1
entrypoints         0.3
idna                2.10
importlib-metadata  3.10.0
ipykernel           5.5.3
ipython             7.22.0
ipython-genutils    0.2.0
jedi                0.18.0
Jinja2              2.11.3
jsonschema          3.2.0
jupyter-client      6.1.12
jupyter-core        4.7.1
jupyterlab-pygments 0.1.2
MarkupSafe          1.1.1
mistune             0.8.4
murmurhash          1.0.5
nbclient            0.5.3
nbconvert           6.0.7
nbformat            5.1.3
nest-asyncio        1.5.1
notebook            6.3.0
numpy               1.20.2
packaging           20.9
pandocfilters       1.4.3
parso               0.8.2
pathy               0.4.0
pexpect             4.8.0
pickleshare         0.7.5
pip                 21.0.1
preshed             3.0.5
prometheus-client   0.10.0
prompt-toolkit      3.0.18
ptyprocess          0.7.0
pycparser           2.20
pydantic            1.7.3
Pygments            2.8.1
pyparsing           2.4.7
pyrsistent          0.17.3
python-dateutil     2.8.1
pyzmq               22.0.3
requests            2.25.1
Send2Trash          1.5.0
setuptools          54.2.0
six                 1.15.0
smart-open          3.0.0
spacy               3.0.5
spacy-legacy        3.0.2
srsly               2.4.0
terminado           0.9.4
testpath            0.4.4
thinc               8.0.2
tornado             6.1
tqdm                4.59.0
traitlets           5.0.5
typer               0.3.2
typing-extensions   3.7.4.3
urllib3             1.26.4
wasabi              0.8.2
wcwidth             0.2.5
webencodings        0.5.1
wheel               0.36.2
zipp                3.4.1
```",,,,,1,,,,,
2526,https://github.com/explosion/spaCy/issues/8309,8309,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 2103359118, 'node_id': 'MDU6TGFiZWwyMTAzMzU5MTE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/resolved', 'name': 'resolved', 'color': 'f6f6f6', 'default': False, 'description': 'The issue was addressed / answered'}]",closed,2021-06-08 14:59:38+00:00,,9,Import spacy gives ImportError for click.exceptions (typer),"## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
`import spacy`

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: RHEL 7.6
* Python Version Used: 3.6
* spaCy Version Used: 3.0.6
* Environment Information: VirtualBox VM

```
Stack trace is as follows:
Traceback (most recent call last):
  File ""/home/brick/eclipse-workspace/NeuralCorefWork/Trial1.py"", line 3, in <module>
    import spacy
  File ""/home/brick/.local/lib/python3.6/site-packages/spacy/__init__.py"", line 14, in <module>
    from .cli.info import info  # noqa: F401
  File ""/home/brick/.local/lib/python3.6/site-packages/spacy/cli/__init__.py"", line 3, in <module>
    from ._util import app, setup_cli  # noqa: F401
  File ""/home/brick/.local/lib/python3.6/site-packages/spacy/cli/_util.py"", line 8, in <module>
    import typer
  File ""/home/brick/.local/lib/python3.6/site-packages/typer/__init__.py"", line 7, in <module>
    from click.exceptions import Exit as Exit
ImportError: cannot import name 'Exit'
```",,,,,1,,,,,
790,https://github.com/explosion/spaCy/issues/3923,3923,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2019-07-08 14:02:41+00:00,,3,Trying to load a model in the same script that downloaded it raises OSError,"Hi,

When downloading a model using `spacy.cli.download` and then directly using `spacy.load` (in the same interpreter), it raises an OSError because `spacy.utils.is_package` returns False: https://github.com/explosion/spaCy/blob/a7fd42d937d405088803da3fb3b301cc81cea719/spacy/util.py#L133-L134 
We currently need to start a new interpreter for the list of packages to be updated with the newly installed model.
Is this the expected behaviour?
Is there a way to download a model at run time and using it directly? (I'd like my package to download the model lazily only when needed and without having to execute the script twice).

Thanks,

## How to reproduce the behaviour
```bash
$ pip uninstall -y en_core_web_sm; python -c ""import spacy; spacy.cli.download('en_core_web_sm'); spacy.load('en_core_web_sm')""

OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Ubuntu 18.04
* Python Version Used: 3.7.3
* spaCy Version Used: 2.1.3
* Environment Information:
",,,,1,,,,,,
1131,https://github.com/explosion/spaCy/issues/4480,4480,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 1174487963, 'node_id': 'MDU6TGFiZWwxMTc0NDg3OTYz', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/jupyter', 'name': 'jupyter', 'color': '676F6D', 'default': False, 'description': 'Issues related to Jupyter notebook environments'}]",closed,2019-10-19 20:47:18+00:00,,2,"Jupyter notebook on Anaconda OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","## spaCy validation on Jupyter
!python -m spacy validate

鉁?Loaded compatibility table

====================== Installed models (spaCy v2.1.8) ======================
鈩?spaCy installation:
/Users/shreyansjain_2/anaconda3/envs/ENV3/lib/python3.7/site-packages/spacy

TYPE      NAME             MODEL            VERSION                            
package   en-core-web-sm   en_core_web_sm   2.1.0   鉁?
package   en-core-web-lg   en_core_web_lg   2.1.0   鉁?
link      en_core_web_lg   en_core_web_lg   2.1.0   鉁?
link      en_default       en_core_web_sm   2.1.0   鉁?
link      en               en_core_web_sm   2.1.0   鉁?


-----------
##  Spacy works well on terminal :
(ENV3) Shreyanss-MBP:~ shreyansjain_2$ python -m spacy validate
鉁?Loaded compatibility table

====================== Installed models (spaCy v2.1.8) ======================
鈩?spaCy installation:
/Users/shreyansjain_2/anaconda3/envs/ENV3/lib/python3.7/site-packages/spacy

TYPE      NAME             MODEL            VERSION                            
package   en-core-web-sm   en_core_web_sm   2.1.0   鉁?
package   en-core-web-lg   en_core_web_lg   2.1.0   鉁?
link      en_core_web_lg   en_core_web_lg   2.1.0   鉁?
link      en               en_core_web_sm   2.1.0   鉁?

(ENV3) Shreyanss-MBP:~ shreyansjain_2$ python
Python 3.7.4 (default, Aug 13 2019, 15:17:50) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import spacy
>>> nlp = spacy.load('en_core_web_sm')
>>> doc = nlp(""She ate the  pizza"")
>>> for token in doc:
...    print(token.text, token.pos_)
... 
She PRON
ate VERB
the DET
  SPACE
pizza NOUN
>>> 

---------

##  I am not able to fix this on jupyter.
Already did following on Jupyter notbook :
!python -m spacy download en
!python -m spacy link en_core_web_sm en_default

-------------



##  Here is the error on jupyter when I run any of following commands : 
import spacy
nlp = spacy.load('en')

---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-21-5a0e94ab804c> in <module>
      1 #Spacy Statistical model
      2 import spacy
----> 3 nlp = spacy.load('en')

~/anaconda3/lib/python3.7/site-packages/spacy/__init__.py in load(name, **overrides)
     25     if depr_path not in (True, False, None):
     26         deprecation_warning(Warnings.W001.format(path=depr_path))
---> 27     return util.load_model(name, **overrides)
     28 
     29 

~/anaconda3/lib/python3.7/site-packages/spacy/util.py in load_model(name, **overrides)
    169     elif hasattr(name, ""exists""):  # Path or Path-like to model data
    170         return load_model_from_path(name, **overrides)
--> 171     raise IOError(Errors.E050.format(name=name))
    172 
    173 

OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.

or 

nlp = spacy.load('en_core_web_sm')

---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-15-12102ff9e1a9> in <module>
----> 1 nlp = spacy.load('en_core_web_sm')

~/anaconda3/lib/python3.7/site-packages/spacy/__init__.py in load(name, **overrides)
     25     if depr_path not in (True, False, None):
     26         deprecation_warning(Warnings.W001.format(path=depr_path))
---> 27     return util.load_model(name, **overrides)
     28 
     29 

~/anaconda3/lib/python3.7/site-packages/spacy/util.py in load_model(name, **overrides)
    169     elif hasattr(name, ""exists""):  # Path or Path-like to model data
    170         return load_model_from_path(name, **overrides)
--> 171     raise IOError(Errors.E050.format(name=name))
    172 
    173 

OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.



## Please suggest a solution.

<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

## How to reproduce the problem

Here is the error on jupyter when I run any of following commands : 
import spacy
nlp = spacy.load('en')

or

nlp = spacy.load('en_core_web_sm')

OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.


## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: macos : 10.15 (19A602)
* Python Version Used: 3.7.4 (Anaconda installation)
* spaCy Version Used: 2.1.0
* Environment Information: macbook pro
",,,,1,,,,,,
1183,https://github.com/explosion/spaCy/issues/4577,4577,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 1174487963, 'node_id': 'MDU6TGFiZWwxMTc0NDg3OTYz', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/jupyter', 'name': 'jupyter', 'color': '676F6D', 'default': False, 'description': 'Issues related to Jupyter notebook environments'}]",closed,2019-11-02 14:44:38+00:00,,7,"OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","I have installed `spacy` and downloaded `en_core_web_sm` with:
```
pip3 install spacy
python3 -m spacy download en_core_web_sm
```

When running codes on Python3 default IDLE, it runs successfully:
```
import spacy
spacy.load(""en_core_web_sm"")
```

However, when I run above codes in **jupyter notebook**, it shows error:
```
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```

I tried several ways in `Jupyter notebook` like
```
!python3 -m spacy download en_core_web_sm
```
but it still shows the error. 

OS: MacOS

Could somebody help me fix this issue? Thanks in advance!

",,,,1,,,,,,
1223,https://github.com/explosion/spaCy/issues/4638,4638,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2019-11-13 10:41:49+00:00,,5,OSError: [E050] Can't find model 'de_core_news_sm'... only on debug mode,"After installing spacy` pip3 install -U spacy` and downloading `python -m spacy download de_core_news_sm`, I can run the following code:

```
import spacy

nlp = spacy.load(""de_core_news_sm"")

text = ""Das ist den Tisch""

my_doc = nlp(text)

token_list = []
for token in my_doc:
    token_list.append(token.text)
print(token_list)
```
Then I tried to debug the code (on Pycharm) and got the following error:
OSError: [E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.

I have also tried the solution from #4577 but did not work!
```
import de_core_news_sm
nlp = de_core_news_sm.load()
```
Does anybody know the reason and how to fix it? Thank you in advanced!

* Operating System: MacOS 
* Python Version Used: 3.6.8
* spaCy Version Used: 2.1.6
* IDE: Pycharm
",,,,1,,,,,,
1297,https://github.com/explosion/spaCy/issues/4756,4756,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2019-12-03 17:46:58+00:00,,4,"OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","I have installed spacy and downloaded en_core_web_sm with:
```
pip3 install spacy
python3 -m spacy download en_core_web_sm
```
When I try to run the en_core_web_sm module in Python IDE with:
```
import spacy
nlp = spacy.load(""en_core_web_sm"")
```
it shows an error message:

```
 File ""/home/disha/Desktop/project/python/parser/venv/lib/python3.6/site-packages/spacy/__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""/home/disha/Desktop/project/python/parser/venv/lib/python3.6/site-packages/spacy/util.py"", line 169, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```
I have tried every possible solution which was suggested on GitHub and Stack Overflow but nothing worked

* Operating System: Ubuntu
* Python Version Used: 3.6
* spaCy Version Used: 2.2.3

Can somebody help me fix this issue? Thanks in advance!

",,,,1,,,,,,
1399,https://github.com/explosion/spaCy/issues/4915,4915,[],closed,2020-01-15 15:40:42+00:00,,2,"OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","I've gone through the troubleshooting steps in previous issues (like #4756 and #4577), but have been unable to resolve the error. 

I'm attempting to import and use version 2.2.3, which seems to be installed. Meaning, I installed it and previously everything worked fine. I haven't made any changes to my machine, just started a new script. and attempted to use:

```
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English
parser = English()
import en_core_web_sm
nlp = spacy.load(""en_core_web_sm"")
```


which yields


```bash
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 169, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```



## Your Environment
* **spaCy version:** 2.2.3
* **Platform:** Darwin-18.7.0-x86_64-i386-64bit
* **Python version:** 3.7.3
Mac OS

I've also tried the following:


```
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English
parser = English()
from spacy.lang.en import en_core_web_sm
```


Which yields: 


```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'en_core_web_sm' from 'spacy.lang.en' (/usr/local/lib/python3.7/site-packages/spacy/lang/en/__init__.py)
>>> nlp = spacy.load(""en_core_web_sm"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 169, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```


If I run each line separately, the errors I get are:
```
>>> from spacy.lang.en import en_core_web_sm
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'en_core_web_sm' from 'spacy.lang.en' (/usr/local/lib/python3.7/site-packages/spacy/lang/en/__init__.py)
>>> nlp = spacy.load(""en_core_web_sm"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 169, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```",,,,1,,,,,,
1452,https://github.com/explosion/spaCy/issues/5015,5015,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 1174487963, 'node_id': 'MDU6TGFiZWwxMTc0NDg3OTYz', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/jupyter', 'name': 'jupyter', 'color': '676F6D', 'default': False, 'description': 'Issues related to Jupyter notebook environments'}]",closed,2020-02-13 21:26:37+00:00,,5,"OSError: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
From 'jupyter lab'
import spacy
print(spacy.__version__)
nlp = spacy.load(""en_core_web_lg"")

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->

## Info about spaCy

* **spaCy version:** 2.2.3
* **Platform:** Linux-4.4.0-164-generic-x86_64-with-debian-stretch-sid
* **Python version:** 3.6.7
* Operating System:
Ubuntu 16.04LTS
* Python Version Used:
Anaconda 3.6
* spaCy Version Used:
2.2.3
* Environment Information:
Jupyter lab

I can load 'en_core_wb_lg' from ipython, but not from jupyter lab:

![Screen Shot 2020-02-13 at 1 23 16 PM](https://user-images.githubusercontent.com/3105499/74479680-6e278000-4e64-11ea-9a1b-746f5271706d.png)
![Screen Shot 2020-02-13 at 1 23 05 PM](https://user-images.githubusercontent.com/3105499/74479682-6f58ad00-4e64-11ea-8d96-dae4b0bbd946.png)

Here's iPython:
```
(lda2vec) ubuntu@ip-10-0-1-71:~/Lda2vec-Tensorflow/tests/twenty_newsgroups/data$ ipython
Python 3.6.7 | packaged by conda-forge | (default, Nov  6 2019, 16:19:42) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.12.0 -- An enhanced Interactive Python. Type '?' for help.


In [1]: import spacy                                                            

In [2]: print(spacy.__version__)                                                
2.2.3

In [3]: nlp = spacy.load(""en_core_web_lg"")                                      
nlp

In [4]: nlp                                                                     
Out[4]: <spacy.lang.en.English at 0x7f4802ee36d8>
```",,,,1,,,,,,
1492,https://github.com/explosion/spaCy/issues/5098,5098,"[{'id': 1414776544, 'node_id': 'MDU6TGFiZWwxNDE0Nzc2NTQ0', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20nel', 'name': 'feat / nel', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity linking'}]",closed,2020-03-04 12:02:18+00:00,,8,spaCy wiki_entity_linking OSError: Invalid data stream,"I am encountering an unexpected error `OSerror: invalid data stream` when trying to use [wiki_entity_linker](https://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking) to create a knowledge base for training a Named Entity Linking model.  I followed all the steps in the README. I find this really annoying since it takes about an hour and a half to run before I encounter this error. I have read similar issues, but they seem to be for training different models, and I am not sure the solutions presented are relevant to the specific task I am trying to do. 

## How to reproduce the behavior
```
user@server$ python3 ./bin/wiki_entity_linking/wikidata_pretrain_kb.py -la en latest-all.json.bz2  enwiki-latest-pages-articles-multistream.xml.bz2  knowledge-base/en/ en_core_web_lg
2020-03-04 11:56:40,012 - INFO - __main__ - Creating KB with Wikipedia and WikiData
2020-03-04 11:56:40,013 - INFO - __main__ - STEP 1: Loading NLP model en_core_web_lg
2020-03-04 11:56:54,400 - INFO - __main__ - STEP 2: Writing prior probabilities to knowledge-base/en/prior_prob.csv
2020-03-04 12:06:29,408 - INFO - bin.wiki_entity_linking.wikipedia_processor - processed 25000000 lines of Wikipedia XML dump
2020-03-04 12:15:25,684 - INFO - bin.wiki_entity_linking.wikipedia_processor - processed 50000000 lines of Wikipedia XML dump
 . . . . . . . . . . 
 . . . . . . . . . .
2020-03-04 13:33:52,502 - INFO - bin.wiki_entity_linking.wikipedia_processor - processed 350000000 lines of Wikipedia XML dump
Traceback (most recent call last):
  File ""../.local/lib/python3.7/site-packages/bin/wiki_entity_linking/wikidata_pretrain_kb.py"", line 179, in <module>
    plac.call(main)
  File ""/home/user/.local/lib/python3.7/site-packages/plac_core.py"", line 367, in call
    cmd, result = parser.consume(arglist)
  File ""/home/user/.local/lib/python3.7/site-packages/plac_core.py"", line 232, in consume
    return cmd, self.func(*(args + varargs + extraopts), **kwargs)
  File ""../.local/lib/python3.7/site-packages/bin/wiki_entity_linking/wikidata_pretrain_kb.py"", line 99, in main
    wp.read_prior_probs(wp_xml, prior_prob_path, limit=limit_prior)
  File ""/home/user/.local/lib/python3.7/site-packages/bin/wiki_entity_linking/wikipedia_processor.py"", line 100, in read_prior_probs
    line = file.readline()
  File ""/usr/lib/python3.7/bz2.py"", line 215, in readline
    return self._buffer.readline(size)
  File ""/usr/lib/python3.7/_compression.py"", line 68, in readinto
    data = self.read(len(byte_view))
  File ""/usr/lib/python3.7/_compression.py"", line 103, in read
    data = self._decompressor.decompress(rawblock, size)
OSError: Invalid data stream
```

The README also provides instructions for testing the code using a limited number of lines. I was able to do this and did not seem to encounter any of the above errors. 

```
user@server$ python3 ./bin/wiki_entity_linking/wikidata_pretrain_kb.py -lt 20000 -lp 2000 -lw 3000 -f 1  latest-all.json.bz2  enwiki-latest-pages-articles-multistream.xml.bz2  knowledge-base/en/ en_core_web_lg
2020-03-04 13:51:39,148 - INFO - __main__ - Creating KB with Wikipedia and WikiData
2020-03-04 13:51:39,148 - INFO - __main__ - STEP 1: Loading NLP model en_core_web_lg
2020-03-04 13:51:52,418 - INFO - __main__ - STEP 2: Writing prior probabilities to knowledge-base/en/prior_prob.csv
2020-03-04 13:51:52,418 - WARNING - __main__ - Warning: reading only 2000 lines of Wikipedia dump
2020-03-04 13:51:52,488 - INFO - bin.wiki_entity_linking.wikipedia_processor - processed 2000 lines of Wikipedia XML dump
2020-03-04 13:51:52,489 - INFO - bin.wiki_entity_linking.wikipedia_processor - Finished. processed 2000 lines of Wikipedia XML dump
2020-03-04 13:51:52,493 - INFO - __main__ - STEP 3: Calculating and writing entity frequencies to knowledge-base/en/entity_freq.csv
2020-03-04 13:51:52,495 - INFO - __main__ - STEP 4: Parsing and writing Wikidata entity definitions to knowledge-base/en/entity_defs.csv
2020-03-04 13:51:52,496 - WARNING - __main__ - Warning: reading only 3000 lines of Wikidata dump
2020-03-04 13:52:00,875 - INFO - bin.wiki_entity_linking.wikidata_processor - Finished. Processed 3000 lines of WikiData JSON dump
2020-03-04 13:52:00,878 - INFO - __main__ - STEP 4b: Writing Wikidata entity aliases to knowledge-base/en/entity_alias.csv
2020-03-04 13:52:00,882 - INFO - __main__ - STEP 4c: Writing Wikidata entity descriptions to knowledge-base/en/entity_descriptions.csv
2020-03-04 13:52:00,885 - INFO - __main__ - STEP 5: Parsing and writing Wikipedia gold entities to knowledge-base/en/gold_entities.jsonl
2020-03-04 13:52:00,886 - WARNING - __main__ - Warning: reading only 20000 lines of Wikipedia dump
2020-03-04 13:55:26,294 - INFO - bin.wiki_entity_linking.wikipedia_processor - Processed 10000 articles
2020-03-04 13:58:28,323 - INFO - bin.wiki_entity_linking.wikipedia_processor - Processed 20000 articles
2020-03-04 13:58:28,323 - INFO - bin.wiki_entity_linking.wikipedia_processor - Finished. Processed 20000 articles
2020-03-04 13:58:28,326 - INFO - __main__ - STEP 6: Creating the KB at knowledge-base/en/kb
2020-03-04 13:58:28,334 - INFO - bin.wiki_entity_linking.kb_creator - Loaded pretrained vectors of size 300
2020-03-04 13:58:28,334 - INFO - bin.wiki_entity_linking.kb_creator - Filtering entities with fewer than 1 mentions
2020-03-04 13:58:28,336 - INFO - bin.wiki_entity_linking.kb_creator - Kept 2 entities from the set of 2760
2020-03-04 13:58:28,336 - INFO - bin.wiki_entity_linking.kb_creator - Training entity encoder
2020-03-04 13:58:28,479 - INFO - bin.wiki_entity_linking.train_descriptions - loss: 0.9442245960235596
2020-03-04 13:58:28,515 - INFO - bin.wiki_entity_linking.train_descriptions - loss: 0.842606782913208
2020-03-04 13:58:28,550 - INFO - bin.wiki_entity_linking.train_descriptions - loss: 0.7469222545623779
2020-03-04 13:58:28,590 - INFO - bin.wiki_entity_linking.train_descriptions - loss: 0.6605334281921387
2020-03-04 13:58:28,626 - INFO - bin.wiki_entity_linking.train_descriptions - loss: 0.5846852660179138
2020-03-04 13:58:28,626 - INFO - bin.wiki_entity_linking.train_descriptions - Trained entity descriptions on 10 (non-unique) descriptions across 5 epochs
2020-03-04 13:58:28,626 - INFO - bin.wiki_entity_linking.train_descriptions - Final loss: 0.5846852660179138
2020-03-04 13:58:28,626 - INFO - bin.wiki_entity_linking.kb_creator - Getting entity embeddings
2020-03-04 13:58:28,646 - INFO - bin.wiki_entity_linking.train_descriptions - Encoded: 2 entities
2020-03-04 13:58:28,647 - INFO - bin.wiki_entity_linking.kb_creator - Adding 2 entities
2020-03-04 13:58:28,647 - INFO - bin.wiki_entity_linking.kb_creator - Adding aliases from Wikipedia and Wikidata
2020-03-04 13:58:28,647 - INFO - bin.wiki_entity_linking.kb_creator - Adding WP aliases
2020-03-04 13:58:28,649 - INFO - __main__ - kb entities: 2
2020-03-04 13:58:28,650 - INFO - __main__ - kb aliases: 0
2020-03-04 13:58:33,069 - INFO - __main__ - Done!
```

## Info about spaCy
* **spaCy version:** 2.2.3
* **Platform:** Linux-5.3.0-40-generic-x86_64-with-Ubuntu-19.10-eoan
* **Python version:** 3.7.5
* **Models:** en

",,,,1,,,,,,
1533,https://github.com/explosion/spaCy/issues/5183,5183,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 642658666, 'node_id': 'MDU6TGFiZWw2NDI2NTg2NjY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20nl', 'name': 'lang / nl', 'color': '726DA8', 'default': False, 'description': 'Dutch language data and models'}, {'id': 1174487963, 'node_id': 'MDU6TGFiZWwxMTc0NDg3OTYz', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/jupyter', 'name': 'jupyter', 'color': '676F6D', 'default': False, 'description': 'Issues related to Jupyter notebook environments'}]",closed,2020-03-23 13:00:08+00:00,,3,OSError: [E050] Can't find model 'nl_core_news_sm' ,"Everytime I install the Dutch model in Google Colab by running
    
     !pip install -U spacy`
     !python -m spacy download nl_core_news_sm`

I get the message 

     鉁?Download and installation successful
     You can now load the model via spacy.load('nl_core_news_sm')`

which is good! ... however, when I then run

    import spacy
    nlp = spacy.load('nl_core_news_sm')

I get

`OSError: [E050] Can't find model 'nl_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.`

This is annoying, because I need to restart my runtime, run again from top to bottom and then it works (for some reason). For the English model however, there are no installation problems. Is this a bug or does this have to do with Google Colab?",,,,1,,,,,,
1817,https://github.com/explosion/spaCy/issues/5897,5897,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2020-08-08 09:45:08+00:00,,4,"OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","```
import kindred

trainCorpus = kindred.bionlpst.load('2016-BB3-event-train')
devCorpus = kindred.bionlpst.load('2016-BB3-event-dev')
predictionCorpus = devCorpus.clone()
predictionCorpus.removeRelations()
classifier = kindred.RelationClassifier()
classifier.train(trainCorpus)
classifier.predict(predictionCorpus)
f1score = kindred.evaluate(devCorpus, predictionCorpus, metric='f1score')
```
When I try to run the code above, I get the following error:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\SC\Anaconda3\lib\site-packages\kindred\RelationClassifier.py"", line 76, in train
    parser = kindred.Parser(model=self.model)
  File ""C:\Users\SC\Anaconda3\lib\site-packages\kindred\Parser.py"", line 33, in __init__
    Parser._models[model] = spacy.load(model, disable=['ner'])
  File ""C:\Users\SC\Anaconda3\lib\site-packages\spacy\__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""C:\Users\SC\Anaconda3\lib\site-packages\spacy\util.py"", line 175, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.

How can I solve this problem?
## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows10
* Python Version Used: python3.7.4
* spaCy Version Used: 2.3.2
* kindred Version Used: 2.7.1
* Environment Information:
",,,,1,,,,,,
1908,https://github.com/explosion/spaCy/issues/6237,6237,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2020-10-10 18:38:53+00:00,,7,"OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","## How to reproduce the behaviour
run https://github.com/WolfgangFahl/ProceedingsTitleParser/blob/master/tests/test_Spacy.py in Jenkins Continuous integration environment on a Ubuntu 20.04 LTS machine


## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
* Python Version Used:
* spaCy Version Used:
* Environment Information:
```
 python3 -m spacy info --markdown

## Info about spaCy

* **spaCy version:** 2.3.2
* **Platform:** Linux-5.4.0-48-generic-x86_64-with-glibc2.29
* **Python version:** 3.8.5
``` bash
lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04.1 LTS
Release:	20.04
Codename:	focal
```
",,,,1,,,,,,
2030,https://github.com/explosion/spaCy/issues/6541,6541,"[{'id': 496348994, 'node_id': 'MDU6TGFiZWw0OTYzNDg5OTQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/%F0%9F%8C%99%20nightly', 'name': '馃寵 nightly', 'color': 'ffffff', 'default': False, 'description': 'Discussion and contributions related to nightly builds'}, {'id': 881666568, 'node_id': 'MDU6TGFiZWw4ODE2NjY1Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20pipeline', 'name': 'feat / pipeline', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Processing pipeline and components'}, {'id': 2373637018, 'node_id': 'MDU6TGFiZWwyMzczNjM3MDE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20config', 'name': 'feat / config', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Training config'}]",closed,2020-12-10 12:03:35+00:00,,6,OSError: [E050] in nlp.initialize(),"I am trying to train an entity linker but I am getting the error
```
OSError: [E050] Can't find model 'corpus/en_vectors'. It doesn't seem to be a Python package or a valid path to a data directory.
```
in `initialize()`.

I've looked at the docs [here](https://nightly.spacy.io/api/entitylinker), but am struggling to see why the error is occurring. 

## How to reproduce the behaviour

```
from spacy.kb import KnowledgeBase
from spacy.training import Example
import spacy

nlp = spacy.load('en_core_web_lg')
# here I usually load my local vocab path, but the same error occurs without this
# nlp.vocab.from_disk(self.vocab_path)
nlp.vocab.vectors.name = ""spacy_pretrained_vectors""

def create_kb(vocab):
    entity_vector_length = 300
    kb = KnowledgeBase(vocab=vocab, entity_vector_length=entity_vector_length)
    # here I usually load my local knowledge base, but the same error occurs if you dont add anything 
    # kb.from_disk(self.kb_path)
    return kb

entity_linker = nlp.add_pipe(""entity_linker"")
entity_linker.set_kb(create_kb)

train_data = []
text_1 = ""Russ Cochran his reprints include EC Comics.""
dict_1 = {(0, 12): {""Q7381115"": 1.0, ""Q2146908"": 0.0}}
train_data.append((text_1, {""links"": dict_1}))
text_2 = ""Russ Cochran has been publishing comic art.""
dict_2 = {(0, 12): {""Q7381115"": 1.0, ""Q2146908"": 0.0}}
train_data.append((text_2, {""links"": dict_2}))
text_3 = ""Russ Cochran captured his first major title with his son as caddie.""
dict_3 = {(0, 12): {""Q7381115"": 0.0, ""Q2146908"": 1.0}}
train_data.append((text_3, {""links"": dict_3}))
text_4 = ""Russ Cochran was a member of University of Kentucky's golf team.""
dict_4 = {(0, 12): {""Q7381115"": 0.0, ""Q2146908"": 1.0}}
train_data.append((text_4, {""links"": dict_4}))

examples = []
for text, annotation in train_data:
    doc = nlp.make_doc(text)
    example = Example.from_dict(doc, annotation)
    examples.append(example)

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""entity_linker""]
with nlp.select_pipes(disable=other_pipes):
    optimizer = nlp.initialize()
    for itn in range(n_iter):
        random.shuffle(examples)
        losses = {}
        batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            nlp.update(
                batch, drop=0.2, losses=losses, sgd=optimizer,
            )
```

I'm getting this same issue when I try to load my own knowledge base and vocab. For this I thought maybe I needed to change the config file to point to the correct vectors location (which is ""local_kb_path/vocab/vectors""), so I tried:
```
config = {""initialize"": {""vectors"": 'local_kb_path/vocab/vectors'}}
entity_linker = nlp.add_pipe(""entity_linker"", config=config)
```
but this gives 'extra fields not permitted'.

Many thanks!

## Your Environment

- **spaCy version:** 3.0.0rc2
- **Platform:** Darwin-18.6.0-x86_64-i386-64bit
- **Python version:** 3.7.9
- **Pipelines:** en_core_web_md (3.0.0a0), en_core_web_lg (3.0.0a0)
",,,,1,,,,,,
2553,https://github.com/explosion/spaCy/issues/8417,8417,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 1174487963, 'node_id': 'MDU6TGFiZWwxMTc0NDg3OTYz', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/jupyter', 'name': 'jupyter', 'color': '676F6D', 'default': False, 'description': 'Issues related to Jupyter notebook environments'}, {'id': 2103359118, 'node_id': 'MDU6TGFiZWwyMTAzMzU5MTE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/resolved', 'name': 'resolved', 'color': 'f6f6f6', 'default': False, 'description': 'The issue was addressed / answered'}, {'id': 2700846039, 'node_id': 'MDU6TGFiZWwyNzAwODQ2MDM5', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/v2', 'name': 'v2', 'color': '1a1e23', 'default': False, 'description': 'spaCy v2.x'}]",closed,2021-06-16 18:07:56+00:00,,5,Google colab cannot find installed spacy model : OSError: [E050] ,"## Description 
running in Google Colab the command : 

`! python -m spacy download en_core_web_lg`

followed by  :  

```python
import spacy
nlp = spacy.load('en_core_web_lg')
```
returns the following error : 

```
OSError: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```
## How to reproduce the behaviour
Run this google colab : 
https://gist.github.com/Hadrien-Cornier/5b87e232277c8063756f13b70417c64d
## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
`NAME=""Ubuntu""
VERSION=""18.04.5 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.5 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic`
* Python Version Used: Python 3.7.10
* spaCy Version Used: 2.2.4
* Environment Information: Google colab
",,,,1,,,,,,
2559,https://github.com/explosion/spaCy/issues/8468,8468,[],closed,2021-06-22 06:54:06+00:00,,1,OSError following the prodigy / project recipe in docs,"The error I am getting when trying the spacy, prodigy-nighly integration following the documentation for the new projects workflow is `OSError: [E053] Could not read meta.json from data/processed/data.jsonl`

It seems to be because the data is passed in the place of the model so a switch of params fixes it.
",,,,1,,,,,,
2624,https://github.com/explosion/spaCy/issues/8820,8820,[],closed,2021-07-28 05:49:39+00:00,,1,OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.,"
![image](https://user-images.githubusercontent.com/49754403/127270438-4caf10cb-b056-43aa-b2f2-d1361e14aab8.png)
",,,,1,,,,,,
2636,https://github.com/explosion/spaCy/issues/8877,8877,[],closed,2021-08-03 17:41:23+00:00,,4,OSError: [E050] Can't find model 'es_core_news_sm',"Hi!

I was trying to load the Spanish model in Google Collab and got this error.

**OSError: [E050] Can't find model 'es_core_news_sm'**

Here is the code I used:

#!spacy download es_core_news_sm
import spacy

nlp = spacy.load(""es_core_news_sm"")
doc = nlp(""Sevilla es una maravilla."")
for token in doc:
    print(token.text)

Does anyone know how I couldfix it, please? 

Cheers,
Katerina

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
* Python Version Used:
* spaCy Version Used:
* Environment Information:
",,,,1,,,,,,
207,https://github.com/explosion/spaCy/issues/3082,3082,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 703433806, 'node_id': 'MDU6TGFiZWw3MDM0MzM4MDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20nb', 'name': 'lang / nb', 'color': '726DA8', 'default': False, 'description': 'Norwegian (Bokm氓l) language data and models'}]",closed,2018-12-21 11:07:25+00:00,,36,Core norwegian (nb) model,"All I want for Christmas is a core `nb` model for spacy. And we're getting close!

Last week the Language Technology Group at the University of Oslo released [NER annotations on top of the Norwegian Dependency Treebank](https://github.com/ltgoslo/norne). The [`nb` tag map](https://github.com/explosion/spaCy/blob/2dc6c52ccc5a0e5093c8b5e4f8cf4083b0de64dd/spacy/lang/nb/tag_map.py#L3-L10) is made for it, thanks to @katarkor. So there's now both UD/POS and NER data.

TODO:

- [x] Word vectors
  - Large pre-trained word embeddings are available in the [NLPL word embeddings repository](http://vectors.nlpl.eu/repository/)
  - There's [a paper that compares them](https://www.duo.uio.no/handle/10852/61756). 
  - Can they be used directly, or should we build new ones using a spacy-tokenized version of those corpora? 
- [x] Figure out tagger/parser issues
  - ~[Model appears to diverge](https://github.com/explosion/spaCy/pull/1882#issuecomment-449344534)? Both @ohenrik and I have run into this.~ Not a problem after upgrading to 2.1.x
  - ~[How come sentence segmentation is not working?](https://gist.github.com/jarib/cd51c0b15dd8421c9f607e860f796b08)~ Fixed by passing `-n 10` to `spacy convert` and removing `--gold-preproc` from `spacy train`.
- [ ] Write more tests for Norwegian

",,,1,,,,,,,
313,https://github.com/explosion/spaCy/issues/3247,3247,"[{'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2019-02-07 21:37:05+00:00,,3,"Incorrect dependency tree for ""there is...""","The dependency tree is wrong for sentences that look like `there is a man..` or `there was a man...`
. The branch to 'man' is labeled as `attr` but I believe it should be `nsubj` as shown in the image from http://corenlp.run/ below.

## How to reproduce the behaviour
```
>>> nlp = spacy.load('en')
>>> parsed = nlp('there is a man outside')
>>> displacy.serve(parsed)
```

outputs:
![screen shot 2019-02-07 at 9 30 55 pm](https://user-images.githubusercontent.com/3913371/52444431-44849c00-2b20-11e9-8d2b-93127efa7029.png)

but I believe the arrow to 'man' should be 'nsubj' as in:
![screen shot 2019-02-07 at 9 36 30 pm](https://user-images.githubusercontent.com/3913371/52444499-77c72b00-2b20-11e9-8b1d-32e749b61535.png)



## Your Environment
* **spaCy version:** 2.0.18
* **Platform:** Darwin-18.2.0-x86_64-i386-64bit
* **Python version:** 3.7.0
* **Models:** en_core_web_md, en_core_web_lg, en",,,1,,,,,,,
451,https://github.com/explosion/spaCy/issues/3450,3450,"[{'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2019-03-20 22:07:58+00:00,,2,Potential incorrect dependency tree,"I might be wrong (please correct me) but I believe this is a mistake in the dependency tree so it might be suited for the master thread (https://github.com/explosion/spaCy/issues/3052):

Given the sentence `""The assault resulted in robert fracturing his pelvis""`, I believe the subtree containing `""in robert""` should be rooted on `""fracturing""` not on `""resulted""` as shown below:

![Screen Shot 2019-03-20 at 10 00 18 PM](https://user-images.githubusercontent.com/3913371/54722240-f9689a80-4b5b-11e9-8cdd-9cb5fc2bbef7.png)


Links to interact: https://explosion.ai/demos/displacy?text=the%20assault%20resulted%20in%20robert%20fracturing%20his%20pelvis&model=en_core_web_lg&cpu=1&cph=1


To be more precise, I expected something like the tree given for
`""the assault resulted in robert needing stitches""`
https://explosion.ai/demos/displacy?text=the%20assault%20resulted%20in%20robert%20needing%20stitches&model=en_core_web_lg&cpu=1&cph=1",,,1,,,,,,,
904,https://github.com/explosion/spaCy/issues/4096,4096,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 881752122, 'node_id': 'MDU6TGFiZWw4ODE3NTIxMjI=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20cli', 'name': 'feat / cli', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Command-line interface'}]",closed,2019-08-08 07:54:04+00:00,,3,Add check for cycles in dependency trees to debug-data CLI,"## Feature description

The debug-data CLI should also check for cycles in dependency trees.

Cycles should be identified elsewhere, too, to prevent the kind of error from #4083, but a check in debug-data would also be helpful.",,,1,,,,,,,
1360,https://github.com/explosion/spaCy/issues/4854,4854,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 111380488, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/help%20wanted', 'name': 'help wanted', 'color': 'f6f6f6', 'default': True, 'description': 'Contributions welcome!'}, {'id': 881680006, 'node_id': 'MDU6TGFiZWw4ODE2ODAwMDY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20visualizers', 'name': 'feat / visualizers', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Built-in displaCy and other visualizers'}]",open,2019-12-30 19:14:29+00:00,,4,displaCy dependency tree labels backwards (and upside down) in RTL languages in certain browsers,"## How to reproduce the behaviour

Reproducing this is difficult without a depencency parse available for an RTL language, but I've included a link to HTML with the errant RTL. The link is [here](https://gist.github.com/erip/82869875762d67893937a93320fb055f). 

## Your Environment

## Info about spaCy

* **spaCy version:** 2.2.3
* **Platform:** Darwin-19.2.0-x86_64-i386-64bit
* **Python version:** 3.6.7


",,,1,,,,,,,
1676,https://github.com/explosion/spaCy/issues/5505,5505,"[{'id': 514165920, 'node_id': 'MDU6TGFiZWw1MTQxNjU5MjA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20en', 'name': 'lang / en', 'color': '726DA8', 'default': False, 'description': 'English language data and models'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 881665973, 'node_id': 'MDU6TGFiZWw4ODE2NjU5NzM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20parser', 'name': 'feat / parser', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Dependency Parser'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}, {'id': 2103359118, 'node_id': 'MDU6TGFiZWwyMTAzMzU5MTE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/resolved', 'name': 'resolved', 'color': 'f6f6f6', 'default': False, 'description': 'The issue was addressed / answered'}]",closed,2020-05-26 03:23:35+00:00,,3,"Unable to restore the original sentence with the dependency tree, and inconsistency between output and the demo in https://explosion.ai/demos","## How to reproduce the behaviour

```
import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_sm')
sent = 'A street sign is seen in front of a bench and some flowers'
doc = nlp(sent)
for token in doc:
    print(token.text, token.dep_, token.head.text, token.head.pos_, [child for child in token.children])
displacy.serve(doc, style='dep')
```

The output in the console:
```
A det sign NOUN []
street compound sign NOUN []
sign nsubjpass seen VERB [A, street, flowers]
is auxpass seen VERB []
seen ROOT seen VERB [sign, is, in]
in prep seen VERB [front]
front pobj in ADP [of]
of prep front NOUN [bench]
a det bench NOUN []
bench pobj of ADP [a, and]
and cc bench NOUN []
some det flowers NOUN []
flowers acl sign NOUN [some]
```

The visualization of displacy:
![image](https://user-images.githubusercontent.com/10328486/82857229-0395de00-9f43-11ea-9186-2318a2529f03.png)

I am unable to restore the order of words in the original sentence with this dependency tree. The word `flowers` is in the right of the root `seen`  , but it is a child of the word `sign` which is in the left of the root `seen`.

The output of the spacy demo (https://explosion.ai/demos/displacy?text=A%20street%20sign%20is%20seen%20in%20front%20of%20a%20bench%20and%20some%20flowers&model=en_core_web_sm&cpu=1&cph=0) seems correct, but inconsistent with the output I got.

## Your Environment
## Info about spaCy

* **spaCy version:** 2.2.4
* **Platform:** Linux-4.4.0-142-generic-x86_64-with-debian-jessie-sid
* **Python version:** 3.7.4

* Operating System: Ubuntu 14.04.6 LTS
* Python Version Used: python 3.7
* spaCy Version Used: 2.2.4
* Environment Information: en-core-web-sm 2.2.5 is installed
",,,1,,,,,,,
1911,https://github.com/explosion/spaCy/issues/6244,6244,[],closed,2020-10-12 17:32:53+00:00,,1,Convert CONLL file to a list of Doc objects,"<!-- Describe your issue here. Please keep in mind that the GitHub issue tracker is mostly intended for reports related to the spaCy code base and source, and for bugs and feature requests. If you're looking for help with your code, consider posting a question on Stack Overflow instead: http://stackoverflow.com/questions/tagged/spacy -->

Is there a way to convert CONLL file into list of Doc objects without having to parse the sentence using the nlp object. I have a list of annotations that I have to pass to the automatic component that uses Doc objects as input. I have found a way to create the doc:

doc = Doc(nlp.vocab, words=[...])

And that I can use the from_array function to recreate the other linguistic features. This array can be recreated by using index value from StringStore object, I have successfully created Doc object with LEMMA and TAG information but cannot recreate HEAD data. My question is how to pass HEAD data to Doc object using from_array method. The confusing thing about the HEAD is that for sentence that has this structure:

```
Ona 2
je 2
oti拧la 2
u 4
拧kolu 2
. 2
```

The output of this code snippet:

```
from spacy.attrs import TAG, HEAD, DEP
doc.to_array([TAG, HEAD, DEP])
```

is:

```
array([[10468770234730083819,                    2,                  429],
       [ 5333907774816518795,                    1,                  405],
       [11670076340363994323,                    0,  8206900633647566924],
       [ 6471273018469892813,                    1,  8110129090154140942],
       [ 7055653905424136462, 18446744073709551614,                  435],
       [ 7173976090571422945, 18446744073709551613,                  445]],
      dtype=uint64)
```

I cannot correlate the center column of the `from_array` output to dependency tree structure given above.
I also posted this question on [stackoverflow](https://stackoverflow.com/questions/64317488/convert-conll-file-to-a-list-of-doc-objects) if this is not right place to ask this question.
Thanks in advance for the help,

Daniel


## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
## Info about spaCy

* **spaCy version:** 2.3.2
* **Platform:** Darwin-19.6.0-x86_64-i386-64bit
* **Python version:** 3.7.4
* **Models:** hr

",,,1,,,,,,,
1944,https://github.com/explosion/spaCy/issues/6334,6334,"[{'id': 703410409, 'node_id': 'MDU6TGFiZWw3MDM0MTA0MDk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20pl', 'name': 'lang / pl', 'color': '726DA8', 'default': False, 'description': 'Polish language data and models'}, {'id': 881665973, 'node_id': 'MDU6TGFiZWw4ODE2NjU5NzM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20parser', 'name': 'feat / parser', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Dependency Parser'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}]",closed,2020-11-03 17:01:32+00:00,,7,Surprisingly low scores for parsing polish,"Hello,

I've been trying to compare various models for polish, and noticed that parsing performance is rather lacking, and definitely below the scores posted on your website. It might very well be some mistake on my part, but I've tried evaluating the parser both by using official CONLLU evaluation script, and by using built-in evaluator for spaCy (after converting data to json). Scores for the current version of PDB treebank are:

**UAS**: 76.46%
**LAS**: 68.68%

by using the spaCy evaluator I obtain the following:
**UAS**: 79.55%
**LAS**: 60.47%

Curiously, the results for the LFG treebank are significantly better, but still below the scores you report:
**UAS**: 89.29%
**LAS**: 71.66

It is not really clear to me which version of the Dependency treebank you've used. Back when I was cooperating with you on training the models, I've reccomended using the newest version of PDB, but the website mentions SZ which is its ""ancestor"". If this is correct, was there any reason for choosing the older version?

I'm using spaCy 2.3.2, on Python 3.6.9, with Linux Mint 19.3.",,,1,,,,,,,
2066,https://github.com/explosion/spaCy/issues/6678,6678,"[{'id': 111380487, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/enhancement', 'name': 'enhancement', 'color': '20834E', 'default': True, 'description': 'Feature requests and improvements'}, {'id': 111380488, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/help%20wanted', 'name': 'help wanted', 'color': 'f6f6f6', 'default': True, 'description': 'Contributions welcome!'}, {'id': 496348994, 'node_id': 'MDU6TGFiZWw0OTYzNDg5OTQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/%F0%9F%8C%99%20nightly', 'name': '馃寵 nightly', 'color': 'ffffff', 'default': False, 'description': 'Discussion and contributions related to nightly builds'}, {'id': 881682577, 'node_id': 'MDU6TGFiZWw4ODE2ODI1Nzc=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20matcher', 'name': 'feat / matcher', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Token, phrase and dependency matcher'}, {'id': 1025171280, 'node_id': 'MDU6TGFiZWwxMDI1MTcxMjgw', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20speed', 'name': 'perf / speed', 'color': 'B35905', 'default': False, 'description': 'Performance: speed'}]",closed,2021-01-06 03:59:44+00:00,,9,DependencyMatcher has exponential time complexity,"## TL;DR
Unfortunately the current implementation of DependencyMatcher can take very very long time to match large documents ^^

## How slow ?

This is best illustrated with an example. Consider this dependency tree: 

```
 [[
    {""RIGHT_ID"": ""is"", ""RIGHT_ATTRS"": {""LEMMA"": ""be""}},
    {
        ""LEFT_ID"": ""is"",
        ""REL_OP"": "">"",
        ""RIGHT_ID"": ""subj"",
        ""RIGHT_ATTRS"": {""DEP"": ""nsubj""},
    },
    {
        ""LEFT_ID"": ""is"",
        ""REL_OP"": "">"",
        ""RIGHT_ID"": ""adj"",
        ""RIGHT_ATTRS"": {""POS"": ""ADJ"",},
    },
]])
```
Now let's try to match from 1 to 100 repetitions of the sentence ""The dress is beautiful. "" and observe how match-time increases as we increase the size of the document. To compare them let's also benchmark the time taken to parse the document, the time taken by a simple matcher and the time taken by a simple matcher with an on_match callback parsing the dependency tree.

Here is the full benchmark script:

```
import spacy
from spacy.matcher import DependencyMatcher, Matcher
from time import time


nlp = spacy.load(""en_core_web_sm"")
text = ""The dress is beautiful. ""

dependency_matcher = DependencyMatcher(nlp.vocab)
dependency_matcher.add(""test"", [[
    {""RIGHT_ID"": ""is"", ""RIGHT_ATTRS"": {""LEMMA"": ""be""}},
    {
        ""LEFT_ID"": ""is"",
        ""REL_OP"": "">"",
        ""RIGHT_ID"": ""subj"",
        ""RIGHT_ATTRS"": {""DEP"": ""nsubj""},
    },
    {
        ""LEFT_ID"": ""is"",
        ""REL_OP"": "">"",
        ""RIGHT_ID"": ""adj"",
        ""RIGHT_ATTRS"": {""POS"": ""ADJ"",},
    },
]])

matcher = Matcher(nlp.vocab)
matcher.add(""test"", [[
    {""DEP"": ""nsubj""},
    {""LEMMA"": ""be""},
    {""POS"": ""ADJ""},
]])

def callback(matcher, doc, i, matches):
    _, start, _ = matches[i]
    # We are looking for a single token
    match = doc[start]
    subjs = []
    adjs = []
    for child in match.children:
        if child.dep_ == ""nsubj"":
            subjs.append(child)
        elif child.pos_ == ""ADJ"":
            adjs.append(child)

matcher_with_callback = Matcher(nlp.vocab)
matcher_with_callback.add(""test"", [[
    {""LEMMA"": ""be""},
]], on_match=callback)


def test(n):
    input_text = text*n

    # Benchmark pipeline
    start = time()
    doc = nlp(input_text)
    end = time()

    parse_time = end - start

    # Benchmark a simple matcher
    start = time()
    nb_matches_matcher = len(matcher(doc))
    end = time()

    matcher_time = end - start

    # Benchmark a matcher with on_match callback
    start = time()
    nb_matches_matcher_with_callack  = len(matcher_with_callback(doc))
    end = time()

    matcher_with_callback_time = end - start

    # Benchmark dependency matcher
    start = time()
    nb_matches_dependency_matcher = len(dependency_matcher(doc))
    end = time()

    dependency_matcher_time = end - start

    print(
        f""{n}, {parse_time}, {nb_matches_matcher}, {matcher_time}, ""
        f""{nb_matches_matcher_with_callack}, {matcher_with_callback_time}, ""
        f""{nb_matches_dependency_matcher}, {dependency_matcher_time}""
    )

for n in range(1,100):
    test(n)
```

Let's also enable cProfile to get some additional insights:

```
$ python -m cProfile -s cumtime bechmark.py
```

Raw output:
- timing: http://ix.io/2L0K
- cProfile: http://ix.io/2L0O

Here is a plot of the result:

![image](https://user-images.githubusercontent.com/10875013/103724416-2786cb00-4fcc-11eb-8514-d78735cfdd9b.png)

And here is the same plot in log scale:

![image](https://user-images.githubusercontent.com/10875013/103724493-4c7b3e00-4fcc-11eb-9ea8-baad07157b3c.png)

The dependency matcher (in yellow) takes almost 14 seconds to match 100 repetitions of the string ""The dress is beautiful."", that's really a lot and makes it unfeasible to use DependencyMatcher to process large amount of data or use it for real-time applications.
What's worse is that processing time grows exponentially, which makes DependencyMatcher only usable for small documents.

## Why so slow ?
Here are the top entries of the cProfile report:

```
   126702727 function calls (101821149 primitive calls) in 331.038 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    907/1    0.004    0.000  331.045  331.045 {built-in method builtins.exec}
        1    0.000    0.000  331.045  331.045 test.py:1(<module>)
       99    0.025    0.000  330.145    3.335 test.py:52(test)
       99    0.021    0.000  327.809    3.311 dependencymatcher.pyx:242(__call__)
24835899/99  106.136    0.000  327.747    3.311 dependencymatcher.pyx:300(recurse)
 49338927    7.338    0.000  221.613    0.000 _asarray.py:14(asarray)
 49343983  214.279    0.000  214.279    0.000 {built-in method numpy.array}
       99    0.388    0.004    2.268    0.023 language.py:952(__call__)
      396    0.003    0.000    1.369    0.003 model.py:308(predict)
```
The benchmarks script took 331 seconds to run. About 327 seconds (99% of the time) was spent inside DependencyMatcher.recurse, 221 seconds of which were used just to build numpy arrays.

The reason why DependencyMatcher.recurse is so slow lies in this lines:

https://github.com/explosion/spaCy/blob/87562e470d0a38d1919c60941afbda5765f97ef7/spacy/matcher/dependencymatcher.pyx#L262-L264

In practice we are calling recurse with all possible combinations of matched nodes, no matter their location within the document.
For example, consider two repetitions of the above string:

""The dress is beautiful. The dress is beautiful.""

Spacy will try correctly match the first ""is"" with the subject and adjective from the first sentence, however it will also try to match the first ""is"" with the subject and adjective from the second sentence, same for ""is"" in the second sentence and the tokens of the first sentence. This also apply to any other combinations. Multiply this for 100 repetitions of the sentence and you got 24835899 calls to recurse (which explains the 14 seconds runtime) :P 

## How can we fix this ?

A way to largely improve performance would be to abandon the recurse method and instead use an iterative method that:
- first, groups matches that belong to the same tree
- then, matches tokens from each group separately

I believe this should execute in linear time with respect to the document size.

In addition I would remove the conversion to numpy arrays if not necessary, as it turns out to be quite expensive.

Let me know what you think :) 

PS: thank you very much for developing and maintaining this awesome tool ^^

## Info about spaCy

- **spaCy version:** 3.0.0rc2
- **Platform:** Linux-5.10.3-arch1-1-x86_64-with-glibc2.2.5
- **Python version:** 3.8.7
- **Pipelines:** en_core_web_sm (3.0.0a0), en_core_web_trf (3.0.0a0)

",,,1,,,,,,,
2194,https://github.com/explosion/spaCy/issues/7042,7042,[],closed,2021-02-12 06:46:36+00:00,,0,Add custom token to the nlp,"Hi,

I want to ask a custom token into parser, so the dependency parser can return different result from the current one. More specifically, here is the current dependency tree:
<img width=""1671"" alt=""Screen Shot 2021-02-12 at 07 45 07"" src=""https://user-images.githubusercontent.com/4360657/107738072-416fa800-6d06-11eb-9425-66257b5a7c17.png"">

But Rs is actually the currency of India, so Rs should be quantmod for 10. Can you please tell me how I could guide spaCy to do it without training a new model? Thank you
 

",,,1,,,,,,,
2208,https://github.com/explosion/spaCy/issues/7097,7097,"[{'id': 881665973, 'node_id': 'MDU6TGFiZWw4ODE2NjU5NzM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20parser', 'name': 'feat / parser', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Dependency Parser'}, {'id': 1025171697, 'node_id': 'MDU6TGFiZWwxMDI1MTcxNjk3', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20accuracy', 'name': 'perf / accuracy', 'color': 'B35905', 'default': False, 'description': 'Performance: accuracy'}, {'id': 2103359118, 'node_id': 'MDU6TGFiZWwyMTAzMzU5MTE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/resolved', 'name': 'resolved', 'color': 'f6f6f6', 'default': False, 'description': 'The issue was addressed / answered'}]",closed,2021-02-16 19:38:39+00:00,,4,Spacy confuses verb and proper noun,"Hi,

I need to parse the sentence: ""Northstar Capital Invests in Alpaca Audiology"". It is supposed that ""invests"" is the root verb of this sentence. Instead, because Invest is capitalized, spacy produces a dependency tree like this:

<img width=""899"" alt=""Screen Shot 2021-02-16 at 20 35 04"" src=""https://user-images.githubusercontent.com/4360657/108112772-da9c1700-7096-11eb-887e-58c5bdbbe6ad.png"">

Even when I try to lowercase the word ""invests"", it still generates this tree, even though invest is not a noun:

<img width=""831"" alt=""Screen Shot 2021-02-16 at 21 38 35"" src=""https://user-images.githubusercontent.com/4360657/108119056-88132880-709f-11eb-93bc-65f2bfdb2eed.png"">


Can you give me some hint on how to guide spacy to handle this case? Thanks a lot",,,1,,,,,,,
2213,https://github.com/explosion/spaCy/issues/7113,7113,"[{'id': 514165920, 'node_id': 'MDU6TGFiZWw1MTQxNjU5MjA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20en', 'name': 'lang / en', 'color': '726DA8', 'default': False, 'description': 'English language data and models'}, {'id': 906542982, 'node_id': 'MDU6TGFiZWw5MDY1NDI5ODI=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20doc', 'name': 'feat / doc', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Doc, Span and Token objects'}, {'id': 2103359118, 'node_id': 'MDU6TGFiZWwyMTAzMzU5MTE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/resolved', 'name': 'resolved', 'color': 'f6f6f6', 'default': False, 'description': 'The issue was addressed / answered'}]",closed,2021-02-18 10:13:52+00:00,,3,Cases not taken into account in method spacy.lang.en.syntax_iterators.noun_chunks?,"## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
```
import spacy
from spacy import displacy

nlp = spacy.load(""en_core_web_lg"")

text = 'We applied value added taxes (""VAT"").'
doc = nlp(text)

for chunk in doc.noun_chunks:
    print(chunk.text)

file_name = ""./dependencies.html""
options = {""compact"": True, ""collapse_punct"": False, ""jupyter"": False}
html = displacy.render(doc, style=""dep"", options=options)
with open(file_name, ""w"") as file:
    file.write(html)
```
gives the following output:
```
We
value added taxes
""VAT
```
and the following .html-file:
![image](https://user-images.githubusercontent.com/42806356/108340508-f14a8700-71d8-11eb-8cf6-42100ec59ced.png)

## Problem description

I assume the way we want to merge noun chunks is either to leave out the quotation marks or include them both.
If we look at the dependency tree of the word VAT, it seems like this would encourage such a behavior. 
This leads me to think that there are some cases not taken into account in the method spacy.lang.en.syntax_iterators.noun_chunks?

## Environment

* **spaCy version:** 2.3.2
* **Platform:** Linux-5.4.0-62-generic-x86_64-with-debian-buster-sid
* **Python version:** 3.6.12

",,,1,,,,,,,
249,https://github.com/explosion/spaCy/issues/3146,3146,"[{'id': 111380486, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/duplicate', 'name': 'duplicate', 'color': 'f6f6f6', 'default': True, 'description': 'Issues that have been reported before'}, {'id': 514165920, 'node_id': 'MDU6TGFiZWw1MTQxNjU5MjA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20en', 'name': 'lang / en', 'color': '726DA8', 'default': False, 'description': 'English language data and models'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2019-01-11 15:02:16+00:00,,4,Stop words are not working for en_core_web_lg,"## How to reproduce the behaviour
Compare:

```python
nlp = spacy.load('en_core_web_lg')
doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')
[(w, w.is_stop) for w in doc]
```

which yields:

```
[(Apple, False),
 (is, False),
 (looking, False),
 (at, False),
 (buying, False),
 (U.K., False),
 (startup, False),
 (for, False),
 ($, False),
 (1, False),
 (billion, False)]
```

and

```python
nlp = spacy.load('en_core_web_sm')
doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')
[(w, w.is_stop) for w in doc]
```

yielding:

```
[(Apple, False),
 (is, True),
 (looking, False),
 (at, True),
 (buying, False),
 (U.K., False),
 (startup, False),
 (for, True),
 ($, False),
 (1, False),
 (billion, False)]
```

The former doesn't detect stop words at all. As per #1574 and #1625 it seems to be a known issue. Unfortunately, these two issues were closed (and locked) even thou the problem is still there.

Did I miss something or is it still an issue and the workaround is something like https://github.com/explosion/spaCy/issues/1625#issuecomment-346185033?

## Info about spaCy

* **spaCy version:** 2.0.18
* **Platform:** Darwin-18.2.0-x86_64-i386-64bit
* **Python version:** 3.6.8
* **Models:** en_core_web_lg, en_core_web_sm",1,1,,,,,,,,
123,https://github.com/explosion/spaCy/issues/2950,2950,[],closed,2018-11-20 12:51:14+00:00,,1,"spacy ""is_stop"" and lemma_ issue...","spacy stopwords checker seems not working properly; also  with  lemmatizer/tagger ther's problem if I want filter PRON pos tag, it always returns -PRON- pos:

```python
nlp = spacy.load('en_core_web_lg', disable=())
doc = nlp('How did Quebec nationalists see their province as a nation in the 1960s?')
print([i.lemma_ if i.pos_ != 'PRON' else i.text for i in doc if not i.is_stop])
# ['how', 'do', 'quebec', 'nationalist', 'see', '-PRON-', 'province', 'as', 'a', 'nation', 'in', 'the', '1960', '?']
```
why???

edit:  
ok, work around for "" is_stop"" issue  at  #922",,1,,,,,,,,
380,https://github.com/explosion/spaCy/issues/3346,3346,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 642658286, 'node_id': 'MDU6TGFiZWw2NDI2NTgyODY=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20ja', 'name': 'lang / ja', 'color': '726DA8', 'default': False, 'description': 'Japanese language data and models'}]",closed,2019-02-28 07:11:56+00:00,,2,Japanese stop words are not loaded when initializing the language,"Hi, I'm trying to get the stop words of all languages supported by spaCy programmatically, and it seems to me there's an inconsistency with the Japanese language.

```
>>> import importlib
>>> spacy_lang = importlib.import_module('spacy.lang.en') # Use string literals here to loop over all languages supported by spaCy
>>> stop_words = spacy_lang.STOP_WORDS # English and other languages OK
>>> spacy_lang = importlib.import_module('spacy.lang.ja')
>>> stop_words = spacy_lang.STOP_WORDS # Fail
Traceback (most recent call last):
  File ""<pyshell#18>"", line 1, in <module>
    stop_words = spacy_lang.STOP_WORDS
AttributeError: module 'spacy.lang.ja' has no attribute 'STOP_WORDS'
```
This one works for the Japanese language:
```
>>> import importlib
>>> spacy_stop_words = importlib.import_module('spacy.lang.ja.stop_words') # Iterate over all languages
>>> stop_words = spacy_stop_words.STOP_WORDS
```
And I've found another way to do this (not working for the Japanese language currently):
```
>>> import spacy
>>> nlp = spacy.blank('en')
>>> stop_words = nlp.Defaults.stop_words
```

I'm not sure which one would be the best way to fetch all lists of stop words, I suppose that the last one would have a little overhead while loading the blank model? And is there a nicer way to retrieve the lists directly?

* Operating System: Windows 10 64-bit
* Python Version Used: 3.7.2 64-bit
* spaCy Version Used: 2.1.0a10
",,1,,,,,,,,
539,https://github.com/explosion/spaCy/issues/3592,3592,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}]",closed,2019-04-16 07:35:50+00:00,,6,Unable load spacy with pyintsaller on other system,"I had created an exe file which works fine on my  system but when i try to run on other system i was unable to load the model.

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:Windows 10
* Python Version Used:3.6
* spaCy Version Used:2.0.18
* Environment Information:64 bit

When i run exe in other system in which python is not installed its throws me below error

File ""spacy\util.py"", line 109, in load_model
OSError: [E049] Can't find spaCy data directory: 'None'. Check your installation and permissions, or use spacy.util.set_data_path to customise the location if necessary.

I had set spacy.util.set_data_path to to data location 
In my .spec file i had included below imports:
```
datas=[(""D:/anaconda3/Lib/site-packages/ftfy/char_classes.dat"", ""ftfy""),('model1.json', '.' ),('model1.h5', '.' ),('wordcloud\*.py','wordcloud'),('wordcloud\*.pyd','wordcloud'),('wordcloud\__pycache__\*.pyc','wordcloud'),('wordcloud\stopwords','wordcloud'),('wordcloud\DroidSansMono.ttf','wordcloud'),('D:/anaconda3/Lib/site-packages/spacy/data','spacy'),('D:/anaconda3/Lib/site-packages/spacy/data/*.py','spacy'),('D:/anaconda3/Lib/site-packages/spacy/data/en_core_web_sm/*.py','spacy'),('D:/anaconda3/Lib/site-packages/spacy/data/en_core_web_sm/*.json','spacy'),('D:/anaconda3/Lib/site-packages/spacy/data/en_core_web_sm/*.py','spacy'),('D:/anaconda3/Lib/site-packages/spacy/data/en_core_web_sm/__pycache__/*.pyc','spacy'),('D:/anaconda3/Lib/site-packages/spacy/data/en_core_web_sm/en_core_web_sm-2.0.0/*.json','spacy'),('D:/anaconda3/Lib/site-packages/spacy/data/en_core_web_sm/en_core_web_sm-2.0.0','spacy')],
             hiddenimports=['cython','sklearn.pipeline' ,'sklearn', 'sklearn.ensemble', 'sklearn.neighbors.typedefs', 'sklearn.neighbors.quad_tree', 'sklearn.tree._utils','h5py.defs', 'h5py.utils', 'h5py.h5ac', 'h5py._proxy','PIL','cymem.cymem', 'thinc.linalg', 'murmurhash.mrmr', 'cytoolz.utils', 'cytoolz._signatures', 'spacy.strings', 'spacy.morphology', 'spacy.lexeme', 'spacy.tokens', 'spacy.gold', 'spacy.tokens.underscore', 'spacy.parts_of_speech', 'dill', 'spacy.tokens.printers', 'spacy.tokens._retokenize', 'spacy.syntax', 'spacy.syntax.stateclass', 'spacy.syntax.transition_system', 'spacy.syntax.nonproj', 'spacy.syntax.nn_parser', 'spacy.syntax.arc_eager', 'thinc.extra.search', 'spacy.syntax._beam_utils', 'spacy.syntax.ner', 'thinc.neural._classes.difference', 'spacy.vocab', 'spacy.lemmatizer', 'spacy._ml', 'ftfy', 'spacy.lang', 'spacy.lang.en','spacy.util','pylab'],

```
Any help would be highly appreciated ?",,1,,,,,,,,
682,https://github.com/explosion/spaCy/issues/3775,3775,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881663930, 'node_id': 'MDU6TGFiZWw4ODE2NjM5MzA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ner', 'name': 'feat / ner', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Named Entity Recognizer'}, {'id': 881666568, 'node_id': 'MDU6TGFiZWw4ODE2NjY1Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20pipeline', 'name': 'feat / pipeline', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Processing pipeline and components'}]",closed,2019-05-23 23:25:38+00:00,,5,EntityRuler causes NER entities to go missing,"## How to reproduce the behaviour

I've got a simple NLP setup with a single pattern for the EntityRuler. It's to find a series of capitalized words, followed by a stopword.

```python
nlp = spacy.load(""en_core_web_sm"")
ruler = EntityRuler(nlp)

capitalized_word = ""([A-Z][a-z]+)""
corporate_stopwords = ""([Ii]nc|[Cc]orp|[Cc]o)""

patterns = [
    {""label"": ""COMPANY"", ""pattern"": [{""TEXT"": {""REGEX"": capitalized_word}, 'OP': '+'}, {'TEXT': {""REGEX"": corporate_stopwords}}]}
]
ruler.add_patterns(patterns)

nlp.add_pipe(ruler, before='ner')
```

If I run this piece of text against it, it looks like my pattern is working:
```python
doc = nlp(""Acme Inc. have announced a new product in conjunction with FooBar Baz corp. This rounds out the market segment created by Stark Co., a company started by Tony Stark."")
print([(ent.text, ent.label_) for ent in doc.ents])

[('Acme Inc.', 'COMPANY'), ('FooBar Baz corp', 'COMPANY'), ('Stark Co.', 'COMPANY')]
```

Except for 1 thing:
If I remove the EntityRuler, the same doc has a `PERSON` entity in it that's missing when using the EntityRuler:

```python
[('Acme Inc.', 'ORG'), ('FooBar Baz', 'ORG'), ('Stark Co.', 'ORG'), ('Tony Stark', 'PERSON')]
```

## Environment
* **spaCy version:** 2.1.4
* **Platform:** Darwin-17.7.0-x86_64-i386-64bit
* **Python version:** 3.7.3
",,1,,,,,,,,
1162,https://github.com/explosion/spaCy/issues/4547,4547,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881666568, 'node_id': 'MDU6TGFiZWw4ODE2NjY1Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20pipeline', 'name': 'feat / pipeline', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Processing pipeline and components'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}]",closed,2019-10-28 19:53:30+00:00,,3,Unable to update stop words list in en_core_web_sm,"## How to reproduce the behaviour
Attempting to append the default stop words list, but can't seem to add anything using either

```
more_stops = ['like', 'vet', 'veteran', 'Veterans']
STOP_WORDS.update(more_stops)
```

or 

`nlp.Defaults.stop_words |= {""like"", ""vet"", ""veteran"", ""Veterans""}`

Neither command results in an error message, but when I go on to filter out stop words they remain in the dataset.
",,1,,,,,,,,
1464,https://github.com/explosion/spaCy/issues/5045,5045,"[{'id': 514165920, 'node_id': 'MDU6TGFiZWw1MTQxNjU5MjA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20en', 'name': 'lang / en', 'color': '726DA8', 'default': False, 'description': 'English language data and models'}]",closed,2020-02-21 11:29:20+00:00,,3,Spacy considers spelled-out numbers as stop words,"Why Spacy considers the spell-out numbers (one, five, thirty, etc.) as stop words?
![Screenshot (96)](https://user-images.githubusercontent.com/42888835/75031738-f513d280-54cc-11ea-9178-3674db8cfedc.png)




* Operating System: Windows 8
* Python Version Used: 3.7.3
* spaCy Version Used: 2.2.3
* Environment Information: Anaconda``
",,1,,,,,,,,
1785,https://github.com/explosion/spaCy/issues/5785,5785,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 2103359118, 'node_id': 'MDU6TGFiZWwyMTAzMzU5MTE4', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/resolved', 'name': 'resolved', 'color': 'f6f6f6', 'default': False, 'description': 'The issue was addressed / answered'}]",closed,2020-07-20 09:49:08+00:00,,2,"Tokenizer removing unexpected words. WHy is the word ""first"" removed?","When I tokenize the sentence ""First west Yorkshire"" I get ['west', 'yorkshire']. WHy is the word ""first"" removed?

## How to reproduce the behaviour

```
def get_spacy_english_stop_words():
    nlp = en_core_web_sm.load(),            # Create our list of stopwords
    stop_words = spacy.lang.en.stop_words.STOP_WORDS
    return stop_words

# Creating our tokenizer function
def spacy_tokenizer(
    sentence: str,
    stop_words=get_spacy_english_stop_words(),
    punctuations = string.punctuation, # Create our list of punctuation marks
    parser=English(),                  # Load English tokenizer, tagger, parser, NER and word vectors
    )->list:
    
    # Creating our token object, which is used to create documents with linguistic annotations.
    mytokens = parser(sentence)

    # Lemmatizing each token and converting each token into lowercase
    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != ""-PRON-"" else word.lower_ for word in mytokens ]

    # Removing stop words
    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]

    # return preprocessed list of tokens
    return mytokens

print(spacy_tokenizer(""First west yorkshire""))
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Linux Ubuntu
* Python Version Used: 3.6
* spaCy Version Used: 2.3.2
* Environment Information:-
",,1,,,,,,,,
2021,https://github.com/explosion/spaCy/issues/6500,6500,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 881666568, 'node_id': 'MDU6TGFiZWw4ODE2NjY1Njg=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20pipeline', 'name': 'feat / pipeline', 'color': 'ddfc74', 'default': False, 'description': 'Feature: Processing pipeline and components'}, {'id': 925719279, 'node_id': 'MDU6TGFiZWw5MjU3MTkyNzk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/more-info-needed', 'name': 'more-info-needed', 'color': 'f6f6f6', 'default': False, 'description': 'This issue needs more information'}, {'id': 1025171819, 'node_id': 'MDU6TGFiZWwxMDI1MTcxODE5', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20memory', 'name': 'perf / memory', 'color': 'B35905', 'default': False, 'description': 'Performance: memory use'}]",closed,2020-12-03 20:18:58+00:00,,5,"spaCy freezes my PC (memory: 100%, disk usage: 100%) regardless my corpus being small","I have a very simple task: given a list of input documents, I have to split each document in sentences, each sentences in words, and save a list of lists of words. Example:
`in = [""Hello. I am Mark"", ""What is your name?""]`
Have to become:
`out = [[""hello""], [""i"",""am"",""mark""], [""what"", ""is"", ""your"", ""name"", ""?""]]`
Yes, ""I am Mark"" is a different element because it is a different sentence.
Then i will pass the vector `out` to GenSim in order to train a word2vec model on it.

Here is my spaCy code.
Initialization:
```
nlp = spacy.load(""en_core_web_sm"", disable=[""parser"", ""ner""])
nlp.add_pipe(""sentencizer"")
```
Processing:
```
def build_sent_corpus_spacy(incorpus):
    
    print(""Using Spacy"")
    
    corpus = []
    for docn,doc in enumerate(nlp.pipe(incorpus)):
        print(f""Processing {docn}/{len(incorpus)} ({docn/len(incorpus):.2%})"", end=""\r"")
        for sent in doc.sents:
            sentl = list()
            for word in sent:
                if config[""lemmatization""]:
                    word = word.lemma_
                # !! We need to lower here because stopwords are lowercase !!
                word = str(word).lower()
                if word not in string.punctuation and \
                word not in enstopwords and \
                count_alpha(word)>=1:
                    word = word.replace(""."","""")
                    sentl.append(word)
            corpus.append(sentl)
    return corpus
```
Here are some stats about my corpus (obtained with NLTK tokenization for the reasons afterwards):
```
N. documents: 2,344
N. sentences: 406,037
N. words: 11,385,332
Max sentences per doc: 4,915
Max word per sentence: 3,463
Max word per doc: 168,282
```

Well, when I call the `build_sent_corpus_spacy` function on my corpus my PC goes completely stuck. Using the task manager I can see that memory used is 100% (I have 16 GB of RAM) and  disk usage is 100%. The PC goes completely stuck and spaCy never ends its processing. I have to manually kill the python process.

And, please note, I am even using the ""small"" 20 MB linguistic model (*_sm), not the transformers one!

On a small sidenote, [this blog article](https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5) compared different tokenizers and found spaCy to be the slowest.

How is spaCy fast? I don't think it scales well with document size.

Correct me if I am wrong or if there is something wrong in my initialization/processing.

## Info about spaCy

- **spaCy version:** 3.0.0rc2
- **Platform:** Windows-10-10.0.18362-SP0
- **Python version:** 3.8.5
- **Pipelines:** en_core_web_sm (3.0.0a0), en_core_web_trf (3.0.0a0)",,1,,,,,,,,
2188,https://github.com/explosion/spaCy/issues/7033,7033,"[{'id': 1502120672, 'node_id': 'MDU6TGFiZWwxNTAyMTIwNjcy', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/feat%20/%20ux', 'name': 'feat / ux', 'color': 'ddfc74', 'default': False, 'description': 'Feature: User experience, error messages etc.'}]",closed,2021-02-11 11:10:09+00:00,,3,Can't seem to be able to filter warnings in Spacy 3,"## How to reproduce the behaviour

I'm trying to remove stopwords from a sentence, while disabling pipeline steps that (I think!) are unnecessary for the task (and I assume slow down execution!).
Code may read as follows:

```python
import spacy
spacy.cli.download(""en_core_web_sm"")

# following https://spacy.io/usage/v2-3 on warnings
# I tried also env variables as in https://stackoverflow.com/questions/58294046/ignore-spacy-deprecation-warning
import warnings
warnings.filterwarnings(""ignore"", message=r""\[W108\]"", category=UserWarning)

nlp = spacy.load('en_core_web_sm')
sent = ""I want to remove all stopwords from this very sentence.""

sent = nlp(sent, disable=['ner', 'parser', 'tagger']) # disabling tagger as I don't think it's needed for stopwords removal?
sent_clean = "" "".join([t.text for t in sent if (not t.is_stop)])
```

expected behavior would be to see no `W108` warnings, but I still see them:

```
[W108] The rule-based lemmatizer did not find POS annotation for the token 'I'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
[W108] The rule-based lemmatizer did not find POS annotation for the token 'want'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
[W108] The rule-based lemmatizer did not find POS annotation for the token 'to'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
[W108] The rule-based lemmatizer did not find POS annotation for the token 'remove'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
[W108] The rule-based lemmatizer did not find POS annotation for the token 'all'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
[W108] The rule-based lemmatizer did not find POS annotation for the token 'stopwords'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
[W108] The rule-based lemmatizer did not find POS annotation for the token 'from'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
[W108] The rule-based lemmatizer did not find POS annotation for the token 'this'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
[W108] The rule-based lemmatizer did not find POS annotation for the token 'very'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
[W108] The rule-based lemmatizer did not find POS annotation for the token 'sentence'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
[W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
```

Am I doing anything silly? Thanks!

## Your Environment

- **spaCy version:** 3.0.1
- **Platform:** Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic
- **Python version:** 3.6.9
- **Pipelines:** en_core_web_sm (3.0.0)
",,1,,,,,,,,
2507,https://github.com/explosion/spaCy/issues/8230,8230,"[{'id': 514165920, 'node_id': 'MDU6TGFiZWw1MTQxNjU5MjA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/lang%20/%20en', 'name': 'lang / en', 'color': '726DA8', 'default': False, 'description': 'English language data and models'}]",closed,2021-05-28 22:23:19+00:00,,3,token.is_stop not the same as token.lemma_.lower() in nlp.Defaults.stop_words,"## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

import spacy

nlp = spacy.load(""en_core_web_sm"")

example = 'Both go and goes should be removed as stopwords.'

result_1 = [token for token in d if not token.is_stop]

result_2 = [token.lemma_ for token in d if not token.lemma_.lower() in nlp.Defaults.stop_words]

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->

- **spaCy version:** 3.0.6
- **Platform:** macOS-10.16-x86_64-i386-64bit
- **Python version:** 3.8.5
- **Pipelines:** en_core_web_sm (3.0.0)",,1,,,,,,,,
80,https://github.com/explosion/spaCy/issues/2892,2892,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2018-11-01 05:19:56+00:00,,2,Deploying to Azure Web Service fails with: en_core_web_sm,"## How to reproduce the problem
Follow this link: https://docs.microsoft.com/en-us/azure/app-service/containers/quickstart-python and using own python code as application.py containing this line: nlp_date = spacy.load('en_core_web_sm') 
Although it works locally in my computer (http://localhost:5000/), it fails when deployed to Azure with the following error:
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.

Similar error occurs when executing pip freeze > requirements.txt, since the line -> en-core-web-sm==2.0.0 appears in the list and app cannot be deployed:
remote: Collecting en-core-web-sm==1.6.0 (from -r requirements.txt (line 136))        
remote:   Could not find a version that satisfies the requirement en-core-web-sm==1.6.0 (from -r requirements.txt (line 136)) (from versions: )        
remote: No matching distribution found for en-core-web-sm==1.6.0 (from -r requirements.txt (line 136))
      
```bash
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
## Info about spaCy
* **spaCy version:** 2.0.16
* **Platform:** Windows-10-10.0.17763-SP0
* **Python version:** 3.7.1
* **Models:** en

[en_core_web_sm_IssueAzureAppServLog.txt](https://github.com/explosion/spaCy/files/2537281/en_core_web_sm_IssueAzureAppServLog.txt)
",1,,,,,,,,,
94,https://github.com/explosion/spaCy/issues/2910,2910,"[{'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}]",closed,2018-11-08 12:59:24+00:00,,4,Virus alert for english model (en_core_web_sm-2.0.0),"## How to reproduce the behaviour

strings.json in 'en_core_web_sm-2.0.0/en_core_web_sm/en_core_web_sm-2.0.0/vocab'  triggers an alert with avast (JS:Downloader-FPP [Trj]).

See also: https://www.virustotal.com/fr/file/21c0157d2d05e3deafe86c936f45874ab612defbaeb59c0c91e0b6940958ffc7/analysis/

I guess some part of the json  (like certains urls e.g. http://www.al-jazirah.com.sa/cars/29112006/rood55.htm"" or words) triggers the antivirus.


## Your Environment

* **Platform:** Windows-10-10.0.17134-SP0
* **spaCy version:** 2.0.16
* **Python version:** 3.5.4
",1,,,,,,,,,
100,https://github.com/explosion/spaCy/issues/2917,2917,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2018-11-09 18:40:30+00:00,,3,FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_lg/en_core_web_lg-2.0.0/vocab/strings.json',"## How to reproduce the behaviour
```
import spacy
nlp = spacy.load('en_core_web_lg')
```

```
Traceback (most recent call last):
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1664, in <module>
    main()
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1658, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1068, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/diego/Github/SummarizationConcepts/kmeans_word2vec_preprocessed.py"", line 21, in <module>
    nlp = spacy.load('en_core_web_lg')
  File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 21, in load
    return util.load_model(name, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 112, in load_model
    return load_model_from_link(name, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 129, in load_model_from_link
    return cls.load(**overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_lg/__init__.py"", line 12, in load
    return load_model_from_init_py(__file__, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 173, in load_model_from_init_py
    return load_model_from_path(data_path, meta, **overrides)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 156, in load_model_from_path
    return nlp.from_disk(model_path)
  File ""/usr/local/lib/python3.7/site-packages/spacy/language.py"", line 647, in from_disk
    util.from_disk(path, deserializers, exclude)
  File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 511, in from_disk
    reader(path / key)
  File ""/usr/local/lib/python3.7/site-packages/spacy/language.py"", line 635, in <lambda>
    self.vocab.from_disk(p) and _fix_pretrained_vectors_name(self))),
  File ""vocab.pyx"", line 376, in spacy.vocab.Vocab.from_disk
  File ""strings.pyx"", line 212, in spacy.strings.StringStore.from_disk
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/pathlib.py"", line 1165, in open
    opener=self._opener)
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/pathlib.py"", line 1019, in _opener
    return self._accessor.open(self, flags, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_lg/en_core_web_lg-2.0.0/vocab/strings.json'
```

Hi guys,

This was working 2 weeks ago and it doesn't anymore I don't know why. I tried re-installing spacy (v2.0.16) and the model with ```pip3.7 install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz```

Here is my folder:
```
{18-11-09 19:38}ibm:/usr/local/lib/python3.7/site-packages/spacy/data diego% tree en_core_web_lg 
en_core_web_lg
鈹溾攢鈹€ __init__.py
鈹溾攢鈹€ __pycache__
鈹偮犅?鈹斺攢鈹€ __init__.cpython-37.pyc
鈹溾攢鈹€ en_core_web_lg-2.0.0
鈹偮犅?鈹溾攢鈹€ accuracy.json
鈹偮犅?鈹溾攢鈹€ meta.json
鈹偮犅?鈹溾攢鈹€ ner
鈹偮犅?鈹偮犅?鈹溾攢鈹€ cfg
鈹偮犅?鈹偮犅?鈹溾攢鈹€ lower_model
鈹偮犅?鈹偮犅?鈹溾攢鈹€ moves
鈹偮犅?鈹偮犅?鈹溾攢鈹€ tok2vec_model
鈹偮犅?鈹偮犅?鈹斺攢鈹€ upper_model
鈹偮犅?鈹溾攢鈹€ parser
鈹偮犅?鈹偮犅?鈹溾攢鈹€ cfg
鈹偮犅?鈹偮犅?鈹溾攢鈹€ lower_model
鈹偮犅?鈹偮犅?鈹溾攢鈹€ moves
鈹偮犅?鈹偮犅?鈹溾攢鈹€ tok2vec_model
鈹偮犅?鈹偮犅?鈹斺攢鈹€ upper_model
鈹偮犅?鈹溾攢鈹€ tagger
鈹偮犅?鈹偮犅?鈹溾攢鈹€ cfg
鈹偮犅?鈹偮犅?鈹溾攢鈹€ model
鈹偮犅?鈹偮犅?鈹斺攢鈹€ tag_map
鈹偮犅?鈹溾攢鈹€ tokenizer
鈹偮犅?鈹斺攢鈹€ vocab
鈹偮犅?    鈹溾攢鈹€ key2row
鈹偮犅?    鈹溾攢鈹€ lexemes.bin
鈹偮犅?    鈹斺攢鈹€ vectors
鈹斺攢鈹€ meta.json

6 directories, 22 files
```

Do you have any idea why it doesn't work ?

Thanks 
## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Mac OSX 10.14.1
* Python Version Used: 3.7
* spaCy Version Used: 2.0.16 (also happened with 2.0.12)
* Environment Information:
",1,,,,,,,,,
162,https://github.com/explosion/spaCy/issues/3013,3013,"[{'id': 111380485, 'node_id': 'MDU6TGFiZWwxMTEzODA0ODU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/bug', 'name': 'bug', 'color': 'DD2A27', 'default': True, 'description': 'Bugs and behaviour differing from documentation'}, {'id': 1025171819, 'node_id': 'MDU6TGFiZWwxMDI1MTcxODE5', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/perf%20/%20memory', 'name': 'perf / memory', 'color': 'B35905', 'default': False, 'description': 'Performance: memory use'}]",closed,2018-12-05 23:12:54+00:00,,10,Memory leak using en_core_web_sm nlp,"## How to reproduce the behaviour
Using the en_core_web_sm model to tokenize text and we noticed our memory was climbing and led to OOM errors. 

Below is a script that performs the following steps:
1. Loads en_core_web_sm
2. Then runs a subprocess 10 times
3. First, creates a deep copy of the nlp model
4. Then, processes 1000 randomly created words
5. At the end of the subprocess, the deep copy is deleted and garbage is collected
6. Finally, at the end of the file, the nlp model is manually deleted and garbage is collected.

The results show that, despite the efforts to clean up the environment, the memory still rises.

## Environment

### Info about spaCy

* *spaCy version:* 2.0.16
* *Platform:* Darwin-18.0.0-x86_64-i386-64bit
    * Issue has also occurred on Alpine 3.6
* *Python version:* 3.6.5
* *Models:* en_core_web_sm, en

## Code:
```python
from copy import deepcopy
import gc
import os
import psutil
import random
import spacy
import string

num_runs = 10
num_words = 1000


def randomword(length):
   letters = string.ascii_lowercase
   return ''.join(random.choice(letters) for i in range(length))

def get_memory(process):
    return f""{round(process.memory_info()[0] / float(2 ** 20), 6)} MB""

if __name__ == '__main__':
    process = psutil.Process(os.getpid())

    print(""Loading model."")
    print(f""Pre-load memory: {get_memory(process)}"")
    nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])
    print(f""Post-load memory: {get_memory(process)}"")
    print("""")

    for i in range(num_runs):
        print(f""Run {i}."")
        print(f""\tPre-copy memory: {get_memory(process)}"")
        nlp_deep_copy = deepcopy(nlp)
        print(f""\tPost-copy memory: {get_memory(process)}"")

        for j in range(num_words):
            word = randomword(j)
            nlp_deep_copy(word)
        print(f""\tPost-execution memory: {get_memory(process)}"")
        del nlp_deep_copy
        gc.collect()
        print(f""\tPost-copy-delete memory: {get_memory(process)}"")

    print("""")
    print(f""Post-process memory: {get_memory(process)}"")
    del nlp
    gc.collect()
    print(f""Post-delete memory: {get_memory(process)}"")
```

## Results
> Loading model.
> Pre-load memory: 49.285156 MB
> Post-load memory: 157.777344 MB
>
> Run 0.
> 	Pre-copy memory: 157.777344 MB
> 	Post-copy memory: 246.890625 MB
> 	Post-execution memory: 248.726562 MB
> 	Post-copy-delete memory: 208.40625 MB
> Run 1.
> 	Pre-copy memory: 208.40625 MB
> 	Post-copy memory: 254.191406 MB
> 	Post-execution memory: 254.535156 MB
> 	Post-copy-delete memory: 211.253906 MB
> Run 2.
> 	Pre-copy memory: 211.253906 MB
> 	Post-copy memory: 257.320312 MB
> 	Post-execution memory: 257.628906 MB
> 	Post-copy-delete memory: 213.441406 MB
> Run 3.
> 	Pre-copy memory: 213.441406 MB
> 	Post-copy memory: 258.066406 MB
> 	Post-execution memory: 258.070312 MB
> 	Post-copy-delete memory: 213.417969 MB
> Run 4.
> 	Pre-copy memory: 213.417969 MB
> 	Post-copy memory: 258.003906 MB
> 	Post-execution memory: 258.011719 MB
> 	Post-copy-delete memory: 215.285156 MB
> Run 5.
> 	Pre-copy memory: 215.285156 MB
> 	Post-copy memory: 257.511719 MB
> 	Post-execution memory: 257.523438 MB
> 	Post-copy-delete memory: 213.511719 MB
> Run 6.
> 	Pre-copy memory: 213.511719 MB
> 	Post-copy memory: 257.988281 MB
> 	Post-execution memory: 257.988281 MB
> 	Post-copy-delete memory: 213.296875 MB
> Run 7.
> 	Pre-copy memory: 213.296875 MB
> 	Post-copy memory: 257.523438 MB
> 	Post-execution memory: 257.523438 MB
> 	Post-copy-delete memory: 213.511719 MB
> Run 8.
> 	Pre-copy memory: 213.511719 MB
> 	Post-copy memory: 258.121094 MB
> 	Post-execution memory: 258.121094 MB
> 	Post-copy-delete memory: 213.679688 MB
> Run 9.
> 	Pre-copy memory: 213.679688 MB
> 	Post-copy memory: 258.15625 MB
> 	Post-execution memory: 258.15625 MB
> 	Post-copy-delete memory: 213.679688 MB
> 
> Post-process memory: 213.679688 MB
> Post-delete memory: 137.910156 MB
",1,,,,,,,,,
189,https://github.com/explosion/spaCy/issues/3054,3054,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}]",closed,2018-12-15 12:55:00+00:00,,8,Long load time for en_core_web_md (more than 30 sec) leading to request timeout on Heroku,"## How to reproduce the behaviour
1. Run Django with gunicorn on Heroku (Gunicorn is not mandatory but that's my setup )
2. See that  en_core_web_md.load() takes long time leading to request timeouts.

## Your Environment
* Linux on Heroku (2GB)
* Python 3.5
* spaCy v2.0.18
",1,,,,,,,,,
276,https://github.com/explosion/spaCy/issues/3188,3188,"[{'id': 111380490, 'node_id': 'MDU6TGFiZWwxMTEzODA0OTA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/usage', 'name': 'usage', 'color': '087EA6', 'default': False, 'description': 'General spaCy usage'}]",closed,2019-01-23 12:32:55+00:00,,2,spacy.blank does not accept 'en_core_web_lg',"## How to reproduce the behaviour
After installing the 'en_core_web_lg' model, I'm unable to create a blank model from it.
I am able to do `spacy.load('en_core_web_lg')` but when I do `spacy.blank('en_core_web_lg')`
I get the following:

```
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
~/anaconda/envs/ri/lib/python3.6/site-packages/spacy/util.py in get_lang_class(lang)
     49         try:
---> 50             module = importlib.import_module('.lang.%s' % lang, 'spacy')
     51         except ImportError:

~/anaconda/envs/ri/lib/python3.6/importlib/__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

~/anaconda/envs/ri/lib/python3.6/importlib/_bootstrap.py in _gcd_import(name, package, level)

~/anaconda/envs/ri/lib/python3.6/importlib/_bootstrap.py in _find_and_load(name, import_)

~/anaconda/envs/ri/lib/python3.6/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)

ModuleNotFoundError: No module named 'spacy.lang.en_core_web_lg'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-21-2a56177fabf1> in <module>
----> 1 spacy.blank('en_core_web_lg')

~/anaconda/envs/ri/lib/python3.6/site-packages/spacy/__init__.py in blank(name, **kwargs)
     23 
     24 def blank(name, **kwargs):
---> 25     LangClass = util.get_lang_class(name)
     26     return LangClass(**kwargs)
     27 

~/anaconda/envs/ri/lib/python3.6/site-packages/spacy/util.py in get_lang_class(lang)
     50             module = importlib.import_module('.lang.%s' % lang, 'spacy')
     51         except ImportError:
---> 52             raise ImportError(Errors.E048.format(lang=lang))
     53         LANGUAGES[lang] = getattr(module, module.__all__[0])
     54     return LANGUAGES[lang]

ImportError: [E048] Can't import language en_core_web_lg from spacy.lang.
```
 
## Info about spaCy

* **spaCy version:** 2.0.16
* **Platform:** Darwin-18.2.0-x86_64-i386-64bit
* **Python version:** 3.6.6
* **Models:** en_core_web_lg, en
",1,,,,,,,,,
300,https://github.com/explosion/spaCy/issues/3226,3226,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446637424, 'node_id': 'MDU6TGFiZWw0NDY2Mzc0MjQ=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/conda', 'name': 'conda', 'color': '676F6D', 'default': False, 'description': 'conda package manager'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}, {'id': 621625469, 'node_id': 'MDU6TGFiZWw2MjE2MjU0Njk=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/third-party', 'name': 'third-party', 'color': 'f6f6f6', 'default': False, 'description': 'Third-party packages and services'}]",closed,2019-02-04 05:04:45+00:00,,8,Unable to install spacy-en_core_web_sm with Conda,"<!-- Before submitting an issue, make sure to check the docs and closed issues to see if any of the solutions work for you. Installation problems can often be related to Python environment issues and problems with compilation. -->

## How to reproduce the problem
<!-- Include the details of how the problem occurred. Which command did you run to install spaCy? Did you come across an error? What else did you try? -->

I did a conda install of Spacy, but I'm unable to install this package:
spacy-en_core_web_sm

The instructions on anaconda.org don't work. I'd like to use the package in Jupyter Notebooks which I'm running from Anaconda.

Command:

$ conda install -c danielfrg spacy-en_core_web_sm

```bash
Error:

Solving environment: failed

PackagesNotFoundError: The following packages are not available from current channels:

  - spacy-en_core_web_sm

Current channels:

  - https://conda.anaconda.org/danielfrg/osx-64
  - https://conda.anaconda.org/danielfrg/noarch
  - https://repo.anaconda.com/pkgs/main/osx-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/free/osx-64
  - https://repo.anaconda.com/pkgs/free/noarch
  - https://repo.anaconda.com/pkgs/r/osx-64
  - https://repo.anaconda.com/pkgs/r/noarch
```

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: macOS Mojave
* Python Version Used: 3 
* spaCy Version Used:
* Environment Information: Anaconda
",1,,,,,,,,,
308,https://github.com/explosion/spaCy/issues/3238,3238,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 446605073, 'node_id': 'MDU6TGFiZWw0NDY2MDUwNzM=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/windows', 'name': 'windows', 'color': '676F6D', 'default': False, 'description': 'Issues related to Windows'}, {'id': 781096900, 'node_id': 'MDU6TGFiZWw3ODEwOTY5MDA=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/compat', 'name': 'compat', 'color': '676F6D', 'default': False, 'description': 'Cross-platform and cross-Python compatibility'}]",closed,2019-02-07 06:19:59+00:00,,4,"Issue in loading spacy.load('en_core_web_lg'). Tried many options, but no use.Please help.","## KIndly please help, i am getting error while loading spacy.load('en_core_web_lg')
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
Traceback (most recent call last):
  File ""C:\Users\joshua\Desktop\NLP\python programs\nlp1.py"", line 4, in <module>
    spacy.load('en_core_web_lg')
  File ""C:\Python37\lib\site-packages\spacy\__init__.py"", line 21, in load
    return util.load_model(name, **overrides)
  File ""C:\Python37\lib\site-packages\spacy\util.py"", line 114, in load_model
    return load_model_from_package(name, **overrides)
  File ""C:\Python37\lib\site-packages\spacy\util.py"", line 135, in load_model_from_package
    return cls.load(**overrides)
  File ""C:\Python37\lib\site-packages\en_core_web_lg\__init__.py"", line 12, in load
    return load_model_from_init_py(__file__, **overrides)
  File ""C:\Python37\lib\site-packages\spacy\util.py"", line 173, in load_model_from_init_py
    return load_model_from_path(data_path, meta, **overrides)
  File ""C:\Python37\lib\site-packages\spacy\util.py"", line 156, in load_model_from_path
    return nlp.from_disk(model_path)
  File ""C:\Python37\lib\site-packages\spacy\language.py"", line 647, in from_disk
    util.from_disk(path, deserializers, exclude)
  File ""C:\Python37\lib\site-packages\spacy\util.py"", line 511, in from_disk
    reader(path / key)
  File ""C:\Python37\lib\site-packages\spacy\language.py"", line 635, in <lambda>
    self.vocab.from_disk(p) and _fix_pretrained_vectors_name(self))),
  File ""vocab.pyx"", line 380, in spacy.vocab.Vocab.from_disk
  File ""vectors.pyx"", line 391, in spacy.vectors.Vectors.from_disk
  File ""C:\Python37\lib\site-packages\spacy\util.py"", line 511, in from_disk
    reader(path / key)
  File ""vectors.pyx"", line 384, in spacy.vectors.Vectors.from_disk.load_vectors
  File ""C:\Python37\lib\site-packages\numpy\lib\npyio.py"", line 440, in load
    pickle_kwargs=pickle_kwargs)
  File ""C:\Python37\lib\site-packages\numpy\lib\format.py"", line 704, in read_array
    array = numpy.fromfile(fp, dtype=dtype, count=count)
MemoryError

## Your Environment
<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:Windows
* Python Version Used:Python 3.7.2
* spaCy Version Used: 2.0.18
* Environment Information:
",1,,,,,,,,,
416,https://github.com/explosion/spaCy/issues/3402,3402,"[{'id': 446604395, 'node_id': 'MDU6TGFiZWw0NDY2MDQzOTU=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/install', 'name': 'install', 'color': 'B35905', 'default': False, 'description': 'Installation issues'}, {'id': 560441651, 'node_id': 'MDU6TGFiZWw1NjA0NDE2NTE=', 'url': 'https://api.github.com/repos/explosion/spaCy/labels/models', 'name': 'models', 'color': '726DA8', 'default': False, 'description': 'Issues related to the statistical models'}]",closed,2019-03-12 22:14:05+00:00,,5,Cannot load en_core_web_lg on Linux,"## How to reproduce the behaviour

```
import spacy
nlp = spacy.load(""en_core_web_lg"")
```

Returns
```
ValueError: cannot reshape array of size 179709932 into shape (684831,300)
```

## Your Environment

* **spaCy version:** 2.0.18
* **Platform:** Linux-4.14.88-72.76.amzn1.x86_64-x86_64-with-glibc2.3.4
* **Python version:** 3.6.7
* **Models:** en_core_web_lg

Additional packages that may be important:
```
numpy           1.16.2   
thinc           6.12.1
```

A nearly identical virtualenv on MacOS does not have this problem (Python 3.6.4 but otherwise the same).",1,,,,,,,,,
