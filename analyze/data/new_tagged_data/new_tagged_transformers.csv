,html_url,number,labels,state,created_at,pull_request,comments,title,body,rel1,rel2,rel3,rel4,rel5,rel6,rel7,rel8,rel9,rel10
118,https://github.com/huggingface/transformers/issues/6320,6320,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-07 08:10:46+00:00,,1,Multi-gpu LM finetuning,"Hello, 

how can I run LM finetuning with more than one gpu (specifically I want to train gpt2-medium on Google Cloud with four nvidia T4, 64GB).

What are the arguments to pass to `run_language_modeling.py` script?

Thanks",,,,,,,,,,1
577,https://github.com/huggingface/transformers/issues/7307,7307,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}, {'id': 2107554019, 'node_id': 'MDU6TGFiZWwyMTA3NTU0MDE5', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Distributed%20Training%20/%20Models', 'name': 'Distributed Training / Models', 'color': 'fef2c0', 'default': False, 'description': ''}, {'id': 2209491906, 'node_id': 'MDU6TGFiZWwyMjA5NDkxOTA2', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/gpt2', 'name': 'gpt2', 'color': '45cca5', 'default': False, 'description': ''}]",closed,2020-09-22 02:17:48+00:00,,5,Cuda OOM training gpt2-xl with Trainer in multi-GPUs,"# 鉂?Questions & Help

I am currently trying to finetune the gpt2-xl. I have 2 tesla T4 GPUs. However, I get the CUDA OOM error... when I look at the use of the gpus I see that the first one is full, but the second one still has enough space. Here is my code:

```
from transformers import TextDataset,DataCollatorForLanguageModeling, AutoTokenizer

import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

from transformers import GPT2LMHeadModel, Trainer, TrainingArguments
model = GPT2LMHeadModel.from_pretrained(""gpt2-xl"").to(device)

from transformers import TextDataset,DataCollatorForLanguageModeling, AutoTokenizer,  TrainingArguments, Trainer
tokenizer = AutoTokenizer.from_pretrained(""gpt2-xl"")
train_dataset = TextDataset(
          tokenizer=tokenizer,
          file_path='dataset_training.txt',
          block_size=128)

data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False,
    )

training_args = TrainingArguments(
    output_dir='./results',         # output directory
    num_train_epochs=2,              # total # of training epochs
    per_device_train_batch_size=1,  # batch size per device during training
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs', 
    
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    prediction_loss_only=True,
)

trainer.train()
```
I get ""CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 14.73 GiB total capacity; 13.61 GiB already allocated; 31.88 MiB free; 13.98 GiB reserved in total by PyTorch)""

When I run nvidia-smi I see:

```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+================
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   75C    P0    34W /  70W |  15047MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |
| N/A   56C    P0    29W /  70W |   9479MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|===============================================
|    0      1756      C   /opt/conda/bin/python                      15037MiB |
|    1      1756      C   /opt/conda/bin/python                       9469MiB |
+-----------------------------------------------------------------------------+
```
**My question is:** Am I making a mistake? or how can a large model be trained with multiple GPUs?
",,,,,,,,,,1
854,https://github.com/huggingface/transformers/issues/7833,7833,[],closed,2020-10-15 19:18:30+00:00,,9,[s2s trainer] tests fail on multi-gpu machine,"#### Command
```bash
RUN_SLOW=1 USE_CUDA=1 pytest examples/seq2seq/test_finetune_trainer.py
```


#### Traceback
```python
=========================================================== test session starts ===========================================================
platform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1
rootdir: /home/shleifer/transformers_fork, inifile: pytest.ini
plugins: forked-1.1.3, hydra-core-1.0.0, xdist-1.31.0, requests-mock-1.8.0
collected 2 items

examples/seq2seq/test_finetune_trainer.py /home/shleifer/transformers_fork/src/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)
  FutureWarning,
F/home/shleifer/transformers_fork/src/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)
  FutureWarning,
F

================================================================ FAILURES =================================================================
__________________________________________________________ test_finetune_trainer __________________________________________________________

    def test_finetune_trainer():
>       output_dir = run_trainer(1, ""12"", MBART_TINY, 1)

examples/seq2seq/test_finetune_trainer.py:19:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
examples/seq2seq/test_finetune_trainer.py:105: in run_trainer
    main()
examples/seq2seq/finetune_trainer.py:294: in main
    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None
src/transformers/trainer.py:583: in train
    train_dataloader = self.get_train_dataloader()
src/transformers/trainer.py:386: in get_train_dataloader
    train_sampler = self._get_train_sampler()
examples/seq2seq/seq2seq_trainer.py:108: in _get_train_sampler
    self.args.per_device_train_batch_size, distributed=self.args.n_gpu > 1
examples/seq2seq/utils.py:156: in make_sortish_sampler
    return DistributedSortishSampler(self, batch_size, shuffle=shuffle, **kwargs)
examples/seq2seq/utils.py:368: in __init__
    num_replicas = dist.get_world_size()
../miniconda3/envs/nb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:582: in get_world_size
    return _get_group_size(group)
../miniconda3/envs/nb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:196: in _get_group_size
    _check_default_pg()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _check_default_pg():
        """"""
        Helper that checks if the default ProcessGroup has been initialized, with
        assertion

        """"""
        assert _default_pg is not None, \
>           ""Default process group is not initialized""
E       AssertionError: Default process group is not initialized

../miniconda3/envs/nb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:187: AssertionError
_______________________________________________________ test_finetune_trainer_slow ________________________________________________________

    @slow
    def test_finetune_trainer_slow():
        # TODO(SS): This will fail on devices with more than 1 GPU.
        # There is a missing call to __init__process_group somewhere
>       output_dir = run_trainer(eval_steps=2, max_len=""128"", model_name=MARIAN_MODEL, num_train_epochs=3)

examples/seq2seq/test_finetune_trainer.py:30:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
examples/seq2seq/test_finetune_trainer.py:105: in run_trainer
    main()
examples/seq2seq/finetune_trainer.py:294: in main
    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None
src/transformers/trainer.py:583: in train
    train_dataloader = self.get_train_dataloader()
src/transformers/trainer.py:386: in get_train_dataloader
    train_sampler = self._get_train_sampler()
examples/seq2seq/seq2seq_trainer.py:108: in _get_train_sampler
    self.args.per_device_train_batch_size, distributed=self.args.n_gpu > 1
examples/seq2seq/utils.py:156: in make_sortish_sampler
    return DistributedSortishSampler(self, batch_size, shuffle=shuffle, **kwargs)
examples/seq2seq/utils.py:368: in __init__
    num_replicas = dist.get_world_size()
../miniconda3/envs/nb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:582: in get_world_size
    return _get_group_size(group)
../miniconda3/envs/nb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:196: in _get_group_size
    _check_default_pg()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _check_default_pg():
        """"""
        Helper that checks if the default ProcessGroup has been initialized, with
        assertion

        """"""
        assert _default_pg is not None, \
>           ""Default process group is not initialized""
E       AssertionError: Default process group is not initialized

../miniconda3/envs/nb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:187: AssertionError
========================================================= short test summary info =========================================================
FAILED examples/seq2seq/test_finetune_trainer.py::test_finetune_trainer - AssertionError: Default process group is not initialized
FAILED examples/seq2seq/test_finetune_trainer.py::test_finetune_trainer_slow - AssertionError: Default process group is not initialized
=========================================================== 2 failed in 11.51s ============================================================
```

",,,,,,,,,,1
982,https://github.com/huggingface/transformers/issues/8134,8134,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-10-28 20:45:29+00:00,,2,Error with multi-gpu training ,"I'm trying to build a QuestionAnswering model using transformers
It works with single gpu training but fails with multiple gpus. 
Is there any bug in the below code?


```
class QAModel(pl.LightningModule):
    def __init__(self):
        super(QAModel, self).__init__()
        
        self.model_type = parameters[""BaseModel_type""]
        self.config = AutoConfig.from_pretrained(model_name)
        self.base_model = AutoModelForQuestionAnswering.from_pretrained(model_name, config = self.config)
        self.tokenizer = tokenizer
        
        
    def forward(self,
                input_ids=None,
                attention_mask=None,
                token_type_ids=None,
                start_positions=None, 
                end_positions=None):

        outputs = self.base_model(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            start_positions=start_positions, 
            end_positions=end_positions,
        )

        return outputs

    
    def prepare_data(self):
        self.train_dataset, _, _ = load_data(parameters[""TRAIN_FILE""], is_training=True)
                                                                                
        self.val_dataset, self.val_examples, self.val_features = load_data(parameters[""DEV_FILE""], is_training=False)
                                                                                
        self.test_dataset, self.test_examples, self.test_features = load_data(parameters[""TEST_FILE""], is_training=False)


    def train_dataloader(self):
        return DataLoader(dataset=self.train_dataset, batch_size=parameters[""batch_size""], shuffle=True, num_workers=parameters[""num_threads""])

    def val_dataloader(self):
        return DataLoader(dataset=self.val_dataset, batch_size=parameters[""batch_size""], num_workers=parameters[""num_threads""])

    def test_dataloader(self):
        return DataLoader(dataset=self.test_dataset, batch_size=parameters[""batch_size""], num_workers=parameters[""num_threads""])

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=parameters[""learning_rate""])
    
    def training_step(self, batch, batch_idx):
        inputs = {
                ""input_ids"": batch[0],
                ""attention_mask"": batch[1],
                ""token_type_ids"": batch[2],
                ""start_positions"": batch[3],
                ""end_positions"": batch[4],
            }
        outputs = self.forward(**inputs)        
        loss = outputs[0]
        return {""loss"": loss}    


    def validation_step(self, batch, batch_idx):
        inputs = {
                ""input_ids"": batch[0],
                ""attention_mask"": batch[1],
                ""token_type_ids"": batch[2],
            }

    
        feature_indices = batch[3]

        outputs = self.forward(**inputs)        
                       

model = QAModel()
trainer = pl.Trainer(gpus=-1, distributed_backend='dp', max_epochs=parameters[""epochs""])

trainer.fit(model)
```

I get this error on running it with multiple gpus:
```
RuntimeError: grad can be implicitly created only for scalar outputs
```",,,,,,,,,,1
1127,https://github.com/huggingface/transformers/issues/8424,8424,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-11-09 19:21:07+00:00,,6,Electra multi-gpu pretraining.,"Hi, I am pretraining the Electra model with my own data, for now, I am pretraining using one GPU in my machine. Can we use multi GPUs to pretrain Electra? Thanks for your reply
",,,,,,,,,,1
1447,https://github.com/huggingface/transformers/issues/9064,9064,[],closed,2020-12-11 15:11:53+00:00,,1,Embedding documents on multi-GPU single-ode Docker using pretrained models of huggingface transformers and pytorch DistributedDataParallel,"Hi, 
This is a question.
I am trying to embed some documents including a couple of sentences using huggingface transformers models. I have multi-gpu single-node and I want to do embedding parallel and distributed in all 8 gpus. I tried to use pytorch DistributedDataParallel, but I think all sentences are sending to all GPUs and for all sentences, it is returning one tensor. this is a sample code:

import torch
import torch.nn as nn
import torch.nn.functional as F
import time

import argparse
import os

from transformers import AlbertTokenizer, AlbertModel
import numpy
from tqdm import tqdm
from torch.utils.data import DataLoader,TensorDataset

def parse_args():
    parse = argparse.ArgumentParser()
    parse.add_argument(
            '--local_rank',
            dest = 'local_rank',
            type = int,
            default = 0,
            )
    parse.add_argument(""--gpu"", type=str, default='None',
                        help=""choose gpu device."")
    return parse.parse_args()



def train():
    args = parse_args()

    if not args.gpu == 'None':
        device = torch.device(""cuda"")
        os.environ[""CUDA_VISIBLE_DEVICES""]=args.gpu
    else:
        device = torch.device(""cpu"")

    torch.cuda.set_device(args.local_rank)

    torch.distributed.init_process_group(
                backend='nccl',
                init_method='env://',
                )

    tokenizer = AlbertTokenizer.from_pretrained('albert-xxlarge-v2')


    sentences=['I love tea',    
               'He hates tea',
               'We love tea', 
               'python coder',
               'geeksforgeeks',
               'coder in geeksforgeeks']


    sentence_tokens = []
    for sent in (sentences):

        token_id = tokenizer.encode(sent, max_length=128, add_special_tokens=True, pad_to_max_length=True)

        sentence_tokens.append(token_id)
    original_sentences = torch.tensor(sentence_tokens)
    train_dataset = TensorDataset(original_sentences)

    #setup training sampler
    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,num_replicas=len(sentences))

    #setup training data loader with the train sampler setup
    train_dataloader = DataLoader(train_dataset, batch_size=16,sampler=train_sampler, shuffle=False)


    model = AlbertModel.from_pretrained('albert-xxlarge-v2', return_dict=True)

    model = model.to(device)
    model = nn.parallel.DistributedDataParallel(model,
            device_ids = [args.local_rank, ],
            output_device = args.local_rank,\
            find_unused_parameters=True
            )
    for batch in (train_dataloader):

        batch_input_tensors = batch[0].to('cuda')
        outputs = model(batch_input_tensors)
        last_hidden_states = outputs.last_hidden_state

        average= torch.mean(last_hidden_states,dim=1)

if __name__ == ""__main__"":
    train()

all of sentences are sending to all 8 GPUs and output as last_hidden_states is only one tensor. I got the average of tensor elements because I thought at the end they should be same but they aren't. how can do it distributed and sentences distribute to GPUs and embed over there? and finally for each sentence or for my final case each Doc I have one tensor as feature vector? thanks
",,,,,,,,,,1
1615,https://github.com/huggingface/transformers/issues/9371,9371,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}, {'id': 2604155188, 'node_id': 'MDU6TGFiZWwyNjA0MTU1MTg4', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Benchmarks', 'name': 'Benchmarks', 'color': '2DF372', 'default': False, 'description': 'Issues related to Memory regressions in tests and scripts'}, {'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNjkwMzA3MTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Performance', 'name': 'Performance', 'color': '207F32', 'default': False, 'description': ''}]",closed,2020-12-31 17:47:12+00:00,,26,Excessive GPU-GPU communication with GPT2 making multi-GPU training slow?,"Summary: on a multi-GPU system, training GPT2 seems to scale poorly unless a very fast GPU-GPU interconnect like NVLink is available. In particular, without NVLink using two GPUs is *slower* than using just one GPU.

## Environment info

- `transformers` version: 4.1.1
- Platform: Linux-5.8.0-rc7-custom-x86_64-with-glibc2.29
- Python version: 3.8.5
- PyTorch version (GPU?): 1.8.0.dev20201214+cu110 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No?
- Hardware: 2 x NVIDIA RTX 3090 w/NVLink

### Who can help

Maybe @LysandreJik or @patrickvonplaten ?

## Information

Model I am using (Bert, XLNet ...): GPT2

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [X] my own modified scripts: (give details below)

The script is a pretty basic example of training a medium-size GPT2 from scratch. The script is here: https://panda.moyix.net/~moyix/train_csrc.py

The dataset and tokenized vocab:
* Dataset: https://panda.moyix.net/~moyix/plainsrc_all.txt.gz (718M, gzipped)
* Vocab: https://panda.moyix.net/~moyix/csrc_vocab.tar.gz

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [X] my own task or dataset: (give details below)

Training a GPT2 language model on C source code.

## To reproduce

Run with only one GPU: `CUDA_VISIBLE_DEVICES=0 python train_csrc.py`

Run with two GPUs, NVLink disabled: `NCCL_P2P_DISABLE=1 python train_csrc.py`

Run with two GPUs and NVLink enabled: `python train_csrc.py`

Here is some benchmarking I did with my dataset on transformers 3.3.1 and 4.1.1 (note the difference in ETA is just because 3.3.1 only seems to report the ETA for the current epoch):

Version|NVLINK|GPUs|ETA|Perf
--------|--------|-----|-----|-----
4.1.1 |  Yes  |  2GPU |  419:52:28 |  1.94it/s
4.1.1 |  No   |  2GPU | 1025:06:27 |  1.26s/it
4.1.1 |  N/A  |  1GPU |  599:14:57 |  2.72it/s
3.3.1 |  Yes  |  2GPU |   83:46:51 |  1.94it/s
3.3.1 |  No   |  2GPU |  204:54:22 |  1.26s/it
3.3.1 |  N/A  |  1GPU |  119:02:34 |  2.73it/s

You can see that using two GPUs is actually slower than using a single GPU, unless NVLink is available (599 hours for 1 GPU vs 1025 hours for two GPUs). So presumably there is a large amount of GPU-GPU communication going on?

## Expected behavior

Scaling should be roughly linear with the number of GPUs. Unfortunately I am not very familiar with the implementation details of GPT2 in Huggingface, but others report roughly linear scaling with Transformer models like BERT so it should work here as well: https://towardsdatascience.com/training-bert-at-a-university-eedcf940c754

Although I have  a system with NVLink at home, this issue is still affecting me because I would like to be able to run this on the university HPC cluster, where most nodes do not have NVLink.",,,,,,,,,,1
1764,https://github.com/huggingface/transformers/issues/9642,9642,[],closed,2021-01-17 14:25:43+00:00,,10,Multi-GPU inference with Tensorflow backend,"Is this already supported maybe? I know that multi-GPU TRAINING is supported with TF* models pretty well. But not inference. What is the recommended way when one wants to do inference for a large batch of text (tens of millions rows)? Currently only one of GPUs gets loaded. Tensorflow have a [guide ](https://www.tensorflow.org/tutorials/distribute/save_and_load) on how to use model saved in the native tf format to do distributed inference under a scope:

```python
another_strategy = tf.distribute.MirroredStrategy()
with another_strategy.scope():
  loaded = tf.saved_model.load(saved_model_path)
  inference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]

  dist_predict_dataset = another_strategy.experimental_distribute_dataset(
      predict_dataset)

  # Calling the function in a distributed manner
  for batch in dist_predict_dataset:
    another_strategy.run(inference_func,args=(batch,))
```
However, it seems that transformers do not support saving in this native format? At least TFDistilBertForSequenceClassification, when loaded back, has damaged input signatures (no attention_mask, wrong sequence length, fake None inputs) and can't process anything. And this very tracker is crowded with similar questions which are left unanswered. Can anyone shed some light on best approach to distributed inference please? Also adding a bullet on this to the documentation would be extremely helpful for many folks.",,,,,,,,,,1
2057,https://github.com/huggingface/transformers/issues/10188,10188,[],closed,2021-02-15 12:27:58+00:00,,4,Failing Multi-GPU trainer test,"This test is currently [failing in a multi-GPU setup](https://github.com/huggingface/transformers/runs/1902689616?check_suite_focus=true):

```
FAILED tests/test_trainer_distributed.py::TestTrainerDistributed::test_trainer
```

The error is the following:

```
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8
```

cc @sgugger @stas00 ",,,,,,,,,,1
2075,https://github.com/huggingface/transformers/issues/10223,10223,[],closed,2021-02-16 23:09:12+00:00,,4,Slow Multi-GPU DDP training with run_clm.py and GPT2,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.3.0
- Platform: Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10
- Python version: 3.8.6
- PyTorch version (GPU?): 1.7.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: yes (using official `run_clm.py`)
- Using distributed or parallel set-up in script?: using DDP with `run_clm.py`

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @jplu

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @n1t0, @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

HF projects:

- nlp datasets: [different repo](https://github.com/huggingface/nlp)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->
- gpt2: @patrickvonplaten, @LysandreJik
- trainer, maintained examples: @sgugger

## Information

Model I am using (Bert, XLNet ...): gpt2-medium

The problem arises when using:
* [x] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [x] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. Generate some dummy text data (3000 examples) and save a csv:
```python
import pandas as pd
text = "" "".join(100*[""Here is some very long text.""])
text = 3000*[text]
pd.Series(text).to_frame(""text"").to_csv(""data_temp.csv"",index=False)
```
2. Run official [`run_clm.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py) from examples using 3 gpus and DDP:
```shell
CUDA_VISIBLE_DEVICES=""1,2,3"" python -m torch.distributed.launch --nproc_per_node 3 run_clm.py \
--model_name_or_path gpt2-medium \
--do_train \
--output_dir /proj/semafor/kirill/tmp \
--per_device_train_batch_size 4 \
--block_size 128 \
--train_file data_temp.csv \
--fp16
```
3. Using 3 GeForce RTX 2080Ti with 11Gbs, tqdm says it should approximately take 1 hour: `48/4101 [00:38<53:51,  1.25it/s`. The memory in each GPU is maxed out: ` 10782MiB / 11019MiB `

4. Now, if I just run the same script on a single GPU:
```shell
CUDA_VISIBLE_DEVICES=""3"" python run_clm.py \
--model_name_or_path gpt2-medium \
--do_train \
--output_dir /proj/semafor/kirill/tmp \
--per_device_train_batch_size 4 \
--block_size 128 \
--train_file data_temp.csv \
--fp16
```

It's actually a little faster: `260/12303 [00:57<44:02,  4.56it/s`  and the GPU memory is not maxed out: `9448MiB / 11019MiB`

I can actually double the `--per_device_train_batch_size` from `4 -> 8` and get it down to under 30 mins per epoch: `138/6153 [00:36<26:30,  3.78it/s`

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

So I expected that:
-  DDP training on 3 GPUs to be faster than a single GPU (It's actually a little slower).
-  If I can load a batch of 8 on a device in a single GPU mode then it should work in multi-GPU mode as well (it doesn't, I get OOM error).",,,,,,,,,,1
2234,https://github.com/huggingface/transformers/issues/10477,10477,[],closed,2021-03-02 10:22:31+00:00,,8,Facing NCCL error on Multi-GPU training(on single machine) using run_glue.py script,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.3.2
- Platform: Linux-4.19.0-14-cloud-amd64-x86_64-with-debian-buster-sid
- Python version: 3.7.9
- PyTorch version (GPU?): 1.7.0 (True)
- Tensorflow version (GPU?): 2.4.1 (True)
- Using GPU in script?: 4xTesla T4 (GCP)
- Using distributed or parallel set-up in script?: torch.distributed.launch

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @jplu

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

HF projects:

- nlp datasets: [different repo](https://github.com/huggingface/nlp)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

## Information

Model I am using (Bert, XLNet ...): DistilRoberta

The problem arises when using:
* [*] the official example scripts: (give details below)
* [  ] my own modified scripts: (give details below)


The tasks I am working on is:
* [  ] an official GLUE/SQUaD task: (give the name)
* [*] my own task or dataset: (give details below)

Regression task with a single output, using BertForSequenceClassification

## To reproduce

Steps to reproduce the behavior:

1.python -m torch.distributed.launch --nproc_per_node 4 /home/run_glue.py   --train_file /home/data/train.csv   --validation_file /home/data/dev.csv   --test_file /home/data/test.csv   --model_name_or_path distilroberta-base   --output_dir /home/model   --num_train_epochs 5   --per_device_train_batch_size 1   --per_device_eval_batch_size 16   --do_train   --do_eval   --fp16   --gradient_accumulation_steps 2   --do_predict   --logging_steps 100   --evaluation_strategy steps   --save_steps 100   --overwrite_output_dir

File ""/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py"", line 442, in init_process_group
732793de051f:1895:1925 [1] NCCL INFO transport/shm.cc:101 -> 2
732793de051f:1895:1925 [1] NCCL INFO transport.cc:30 -> 2
732793de051f:1895:1925 [1] NCCL INFO transport.cc:49 -> 2
732793de051f:1895:1925 [1] NCCL INFO init.cc:766 -> 2
732793de051f:1895:1925 [1] NCCL INFO init.cc:840 -> 2
732793de051f:1895:1925 [1] NCCL INFO group.cc:73 -> 2 [Async thread]
    barrier()
  File ""/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py"", line 1947, in barrier
Traceback (most recent call last):
  File ""/home/run_text_classification.py"", line 480, in <module>
    work = _default_pg.barrier()
RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1603729138878/work/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8
    main()
  File ""/home/run_text_classification.py"", line 163, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File ""/opt/conda/lib/python3.7/site-packages/transformers/hf_argparser.py"", line 180, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File ""<string>"", line 60, in __init__
  File ""/opt/conda/lib/python3.7/site-packages/transformers/training_args.py"", line 478, in __post_init__
    if is_torch_available() and self.device.type != ""cuda"" and self.fp16:
  File ""/opt/conda/lib/python3.7/site-packages/transformers/file_utils.py"", line 1346, in wrapper
    return func(*args, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/transformers/training_args.py"", line 583, in device
    return self._setup_devices

732793de051f:1897:1927 [3] include/shm.h:28 NCCL WARN Call to posix_fallocate failed : No space left on device
  File ""/opt/conda/lib/python3.7/site-packages/transformers/file_utils.py"", line 1336, in __get__
732793de051f:1897:1927 [3] NCCL INFO include/shm.h:41 -> 2

732793de051f:1897:1927 [3] include/shm.h:48 NCCL WARN Error while creating shared memory segment nccl-shm-recv-b3d54cebe4167a34-0-2-3 (size 9637888)
<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior
Expected model training to proceed smoothly using 4xGPU. When I run the said script with nproc_per_node=1(or even 2), it runs smoothly but setting it as 4 gives strange errors. 

After updating to 1.9.0 I face a different error:

RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:832, unhandled system error, NCCL version 2.7.8 ncclSystemError: System call (socket, malloc, munmap, etc) failed.
<!-- A clear and concise description of what you would expect to happen. -->
",,,,,,,,,,1
2320,https://github.com/huggingface/transformers/issues/10634,10634,[],closed,2021-03-10 19:25:01+00:00,,1,Issues with Multi-GPU,"- `transformers` version: 4.3.3
- Platform: Linux-4.15.0-132-generic-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.8.0 (True)
- Tensorflow version (GPU?): 2.3.0 (True)
- Using GPU in script?: Yes, multi GeForce RTX 2080 Ti GPUs
- Using distributed or parallel set-up in script?: DataParallel
- NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2

I have tried to run the IMDb review sequence classification from https://huggingface.co/transformers/custom_datasets.html on two GPUs, using `DataParallel`:


```
import os
os.environ[""CUDA_VISIBLE_DEVICES""]=""6,7""
import time
import torch
import torch.nn as nn
from pathlib import Path
from sklearn.model_selection import train_test_split
from transformers import DistilBertTokenizerFast
from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments


def read_imdb_split(split_dir):
    split_dir = Path(split_dir)
    texts = []
    labels = []
    for label_dir in [""pos"", ""neg""]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is ""neg"" else 1)

    return texts, labels

class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
n_gpus = torch.cuda.device_count()


train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

train_dataset = IMDbDataset(train_encodings, train_labels)
val_dataset = IMDbDataset(val_encodings, val_labels)
test_dataset = IMDbDataset(test_encodings, test_labels)

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

model = DistilBertForSequenceClassification.from_pretrained(""distilbert-base-uncased"")
if n_gpus > 1:
    model = nn.DataParallel(model)
model.to(device)

trainer = Trainer(
    model=model,                         # the instantiated 馃 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()
```

For **torch==1.8.0**, no matter I use single GPU or multi GPU, I encounter the same CUDA error (shown below). For **torch==1.7.1**, I am able to run the code on single GPU with no issue. However, with multi-GPU, the `Input, output and indices must be on the current device` error occurs (also shown below). 


With **torch==1.8.0**:
```
2021-03-10 19:16:07.624155: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|                                                                                                                                                       | 0/1875 [00:00<?, ?it/s]Traceback (most recent call last):
  File ""test.py"", line 80, in <module>
    trainer.train()
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/trainer.py"", line 940, in train
    tr_loss += self.training_step(model, inputs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/trainer.py"", line 1304, in training_step
    loss = self.compute_loss(model, inputs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/trainer.py"", line 1334, in compute_loss
    outputs = model(**inputs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 86, in parallel_apply
    output.reraise()
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/_utils.py"", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 61, in _worker
    output = module(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 86, in parallel_apply
    output.reraise()
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/_utils.py"", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 61, in _worker
    output = module(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py"", line 623, in forward
    return_dict=return_dict,
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py"", line 487, in forward
    return_dict=return_dict,
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py"", line 309, in forward
    x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_attentions=output_attentions
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py"", line 256, in forward
    output_attentions=output_attentions,
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py"", line 177, in forward
    q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/linear.py"", line 94, in forward
    return F.linear(input, self.weight, self.bias)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/functional.py"", line 1753, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)`
```

With **torch==1.7.1**:
```
2021-03-10 19:22:14.938302: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|                                                                                                                                                       | 0/1875 [00:00<?, ?it/s]Traceback (most recent call last):
  File ""test.py"", line 80, in <module>
    trainer.train()
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/trainer.py"", line 940, in train
    tr_loss += self.training_step(model, inputs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/trainer.py"", line 1304, in training_step
    loss = self.compute_loss(model, inputs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/trainer.py"", line 1334, in compute_loss
    outputs = model(**inputs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 161, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 171, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 86, in parallel_apply
    output.reraise()
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/_utils.py"", line 428, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 61, in _worker
    output = module(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 161, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 171, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 86, in parallel_apply
    output.reraise()
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/_utils.py"", line 428, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 61, in _worker
    output = module(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py"", line 623, in forward
    return_dict=return_dict,
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py"", line 480, in forward
    inputs_embeds = self.embeddings(input_ids)  # (bs, seq_length, dim)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py"", line 107, in forward
    word_embeddings = self.word_embeddings(input_ids)  # (bs, max_seq_length, dim)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/sparse.py"", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/functional.py"", line 1852, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Input, output and indices must be on the current device
```

With **torch==1.5.0**:
```
2021-03-10 19:23:51.586005: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|                                                                                                                                                       | 0/1875 [00:00<?, ?it/s]Traceback (most recent call last):
  File ""test.py"", line 80, in <module>
    trainer.train()
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/trainer.py"", line 940, in train
    tr_loss += self.training_step(model, inputs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/trainer.py"", line 1304, in training_step
    loss = self.compute_loss(model, inputs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/trainer.py"", line 1334, in compute_loss
    outputs = model(**inputs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 155, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 165, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 85, in parallel_apply
    output.reraise()
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/_utils.py"", line 395, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 60, in _worker
    output = module(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 155, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 165, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 85, in parallel_apply
    output.reraise()
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/_utils.py"", line 395, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 60, in _worker
    output = module(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py"", line 623, in forward
    return_dict=return_dict,
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py"", line 480, in forward
    inputs_embeds = self.embeddings(input_ids)  # (bs, seq_length, dim)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py"", line 107, in forward
    word_embeddings = self.word_embeddings(input_ids)  # (bs, max_seq_length, dim)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/modules/sparse.py"", line 114, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File ""/mnt/sdb/env1/lib/python3.6/site-packages/torch/nn/functional.py"", line 1724, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generic/THCTensorIndex.cu:403
```

",,,,,,,,,,1
2388,https://github.com/huggingface/transformers/issues/10768,10768,[],closed,2021-03-17 09:28:37+00:00,,5,Bug in multi-gpu training setting max_iters ,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.3.3
- Platform:
- Python version: 3.8
- PyTorch version (GPU?): 1.8
- Tensorflow version (GPU?): - 
- Using GPU in script?: yes 
- Using distributed or parallel set-up in script?: yes 

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @jplu

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger,  @patrickvonplaten, @patil-suraj

HF projects:

- nlp datasets: [different repo](https://github.com/huggingface/nlp)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

 trainer: @sgugger

## Information

I am training T5 model using the command in the repo on 4 GPUs in a distributed way, the issue arises that if one set max_iters then the number of iterations with 4 GPUs is not divided by 4 anymore, only one get speed up if max_iters is not set, and this looks like this is a bug. 


## To reproduce

Steps to reproduce the behavior:
Please run 
python -m torch.distributed.launch \
    --nproc_per_node 8 \
    examples/seq2seq/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name xsum \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate \
    --max_train_samples 500 \
    --max_val_samples 500
   --max_iters 100 

compare the results with the case you run on 1 GPU, both would have the same number of iterations to get completed once running which is not correct 


## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
",,,,,,,,,,1
2544,https://github.com/huggingface/transformers/issues/11045,11045,[],closed,2021-04-03 00:52:24+00:00,,6,Multi-GPU seq2seq example evaluation significantly slower than legacy example evaluation,"
### Who can help
@patil-suraj @sgugger 

Models:
T5

## Information

I've been doing multi-GPU evaluation for some weeks using a Transformers pull from Feb 12th, just using the example scripts for training/evaluating custom datasets (specifically `run_distributed_eval.py` , though that seq2seq example is now legacy: https://github.com/huggingface/transformers/tree/master/examples/legacy/seq2seq ) 

Today I grabbed a fresh pull and migrated the data over to the JSON lines format for the new seq2seq example `run_summarization.py` : https://github.com/huggingface/transformers/blob/master/examples/seq2seq/run_summarization.py

run_summarization.py appears to use all visible GPUs to do the evaluation (great!), but it also appears significantly slower than the old run_distributed_eval.py .

When examining GPU utilization using `nvtop`, it appears that it allocates GPU memory from all devices (much more from device 0), but only uses device 0 for processing:

![image](https://user-images.githubusercontent.com/3813268/113463500-dde72a00-93da-11eb-9f60-f0b52b182cee.png)

## Script

In case it's my issue and I'm not invoking it correctly (I know the legacy one required being invoked with `torch.distributed.launch` for multi-GPU evaluation), the runscript I'm using is:
```
#/bin/bash
python run_summarization.py \
    --model_name_or_path mymodel-debug1000 \
    --do_predict \
    --train_file mydata/train.json \
    --validation_file mydata/val.json \
    --test_file mydata/val.json \
    --max_source_length 256 \
    --max_target_length 512 \
    --num_beams 8 \
    --source_prefix """" \
    --output_dir tst-debug \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate

```",,,,,,,,,,1
2854,https://github.com/huggingface/transformers/issues/11618,11618,[],closed,2021-05-06 19:58:24+00:00,,1,[fairscale] rng states saving fails in an extended multi-gpu test,"@sgugger, following up on the RNG states PR
```
RUN_SLOW=1 pytest tests/extended/test_trainer_ext.py::TestTrainerExt::test_run_seq2seq_fully_sharded_ddp_fp16 -sv
```
fails on multi-gpu scheduled CI:

```
File ""/__w/transformers/transformers/examples/pytorch/translation/run_translation.py"", line 589, in <module>
  main()
File ""/__w/transformers/transformers/examples/pytorch/translation/run_translation.py"", line 522, in main
  train_result = trainer.train(resume_from_checkpoint=checkpoint)
File ""/__w/transformers/transformers/src/transformers/trainer.py"", line 1316, in train
  self._maybe_log_save_evaluate(tr_loss, model, trial, epoch)
File ""/__w/transformers/transformers/src/transformers/trainer.py"", line 1397, in _maybe_log_save_evaluate
  self._save_checkpoint(model, trial, metrics=metrics)
File ""/__w/transformers/transformers/src/transformers/trainer.py"", line 1534, in _save_checkpoint
stderr: Saving model checkpoint to /tmp/tmpb6tfhvqb/checkpoint-1
  torch.save(rng_states, os.path.join(output_dir, f""rng_state_{local_rank}.pth""))
File ""/opt/conda/lib/python3.8/site-packages/torch/serialization.py"", line 369, in save
  with _open_file_like(f, 'wb') as opened_file:
File ""/opt/conda/lib/python3.8/site-packages/torch/serialization.py"", line 230, in _open_file_like
  return _open_file(name_or_buffer, mode)
File ""/opt/conda/lib/python3.8/site-packages/torch/serialization.py"", line 211, in __init__
  super(_open_file, self).__init__(open(name, mode))
stderr: FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpb6tfhvqb/checkpoint-1/rng_state_1.pth'
```

https://github.com/huggingface/transformers/runs/2514006341?check_suite_focus=true

Oddly enough it works fine for me if I try it locally.

",,,,,,,,,,1
3874,https://github.com/huggingface/transformers/issues/13516,13516,[],closed,2021-09-10 15:23:16+00:00,,5,Loading SentenceTransformers (DistilBertModel) model using from_pretrained(...) HF function into a DPRQuestionEncoder model,"## Environment info
```
- `transformers` version: 4.10.0
- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.11
- PyTorch version (GPU?): 1.9.0+cu102 (True)
- Tensorflow version (GPU?): 2.6.0 (True)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No
```

### Who can help
research_projects/rag: @patrickvonplaten, @lhoestq

## To reproduce

Steps to reproduce the behavior:

_./semanticsearch_model_ is a DistilBertModel created with SentenceTransformers it is structured as follow: 
![image](https://user-images.githubusercontent.com/10784227/132877566-0abb56d2-8657-43a1-a84e-cfdf98fece87.png)

```
from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer

model_name = './semanticsearch_model'
model = DPRQuestionEncoder.from_pretrained(model_name)
```

# Error
```
You are using a model of type distilbert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.

NotImplementedErrorTraceback (most recent call last)
<ipython-input-52-1f1b990b906b> in <module>
----> 1 model = DPRQuestionEncoder.from_pretrained(model_name)
      2 # https://github.com/huggingface/transformers/blob/41cd52a768a222a13da0c6aaae877a92fc6c783c/src/transformers/models/dpr/modeling_dpr.py#L520

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1358                 raise
   1359         elif from_pt:
-> 1360             model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_state_dict_into_model(
   1361                 model,
   1362                 state_dict,

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in _load_state_dict_into_model(cls, model, state_dict, pretrained_model_name_or_path, ignore_mismatched_sizes, _fast_init)
   1462             )
   1463             for module in unintialized_modules:
-> 1464                 model._init_weights(module)
   1465 
   1466         # copy state_dict so _load_from_state_dict can modify it

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in _init_weights(self, module)
    577         Initialize the weights. This method should be overridden by derived class.
    578         """"""
--> 579         raise NotImplementedError(f""Make sure `_init_weigths` is implemented for {self.__class__}"")
    580 
    581     def tie_weights(self):

NotImplementedError: Make sure `_init_weigths` is implemented for <class 'transformers.models.dpr.modeling_dpr.DPRQuestionEncoder'>
```

## Another thing
It seems that _from_pretrained_ function is not able to load all the weights (i.e. it ignores the 1_Pooling, 2_Dense folders which contains the last two layers beyond the Transformer model).

## Expected behavior

Load correctly the model.
",,1,,,1,,,,1,
87,https://github.com/huggingface/transformers/issues/6259,6259,[],closed,2020-08-05 03:44:18+00:00,,2,Bart encoder with add_final_layer_norm,"https://github.com/huggingface/transformers/blob/91cb95461e438dc57555c4f57f8ce95a56328036/src/transformers/modeling_bart.py#L305

I feel like it should be 
self.layer_norm = LayerNorm(config.d_model) if config.add_final_layer_norm else None

as in the BartDecoder.
https://github.com/huggingface/transformers/blob/91cb95461e438dc57555c4f57f8ce95a56328036/src/transformers/modeling_bart.py#L486
",,,,,,,,,1,
104,https://github.com/huggingface/transformers/issues/6292,6292,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-06 14:45:02+00:00,,1,inconclusive truncation strategies in encode_plus?,"# 鉂?Questions & Help

## Details

Not sure if this is a bug or missing feature or I just misunderstood something: specifically, I'm wondering whether a common truncation strategy is missing from [`encode_plus`](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__). More specifically, if you have a input sequence pair consisting of two texts `a` and `b` and invoke `encode_plus` as follows:

``` 
encode_plus(text=a, text_pair=b, max_length=100, truncation=whatever)
```

It seems that there is no truncation strategy that simply cuts of tokens from the end of the (internally concatenated) input sequence consisting of a and b. Instead the three options allow either to truncate from only a, from only b, or from the longest first (which could be either a or b, depending on the input). How can one remove token by token from the right until max_length is reached, e.g., also in the case that len(a)=200 and len(b)=2. In this example, no option seems to be suitable, e.g., ""longest first"" would remove only from a, ""only first"" likewise only from a (both of these remove from the first input a, but should in my case remove from end of the total sequence, e.g., first b then a if necessary), and ""only second"" only from b (which would be removed entirely, but since only second is defined, a will not be truncated so the total length is 200 and thus still longner than max length)

SO link: https://stackoverflow.com/questions/63280435/huggingface-transformers-truncation-strategy-in-encode-plus",,,,,,,,,1,
198,https://github.com/huggingface/transformers/issues/6487,6487,[],closed,2020-08-14 16:01:47+00:00,,6,about encoder and decoder input when using seq2seq model,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->
Hello, I'm trying to using seq2seq model (such as bart and EncoderDecoderModel(bert2bert))
And I'm little bit confused about input_ids, decoder_input_ids, tgt in model inputs.

As I know in seq2seq model, decoder_input should have special token(\<s> or something) before the sentence and target should have special token(\</s> or somethin) after the sentence. for example, `decoder_input = <s> A B C D E` , `target = A B C D E</s>`

so my question is 
1. Should I put the these special tokens in decoder_inputs_ids and tgt_ids when using seq2seq model in this library?
or can i just pass the decoder_input_ids and tgt_ids without any special token ids?

2. Also, should I put `add_special_tokens=True` for encoder input_ids and put \</s> or \<eos> token after target ids?
for example, `input = a b c d e, decoder_input = <s>A B C D E, target = A B C D E</s>`
<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->",,,,,,,,,1,
264,https://github.com/huggingface/transformers/issues/6632,6632,[],closed,2020-08-21 06:01:20+00:00,,3,"Error on `PreTrainedTokenizerBase.batch_encode_plus` with `return_overflowing_tokens=True, truncation=True`","## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2 (master branch)
- Platform: macOS-10.14.6-x86_64-i386-64bit
- Python version: 3.8.1
- PyTorch version (GPU?): 1.6.0 (False)
- Tensorflow version (GPU?): 2.3.0 (False)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 examples/bert-loses-patience: @JetRunner
 tensorflow: @jplu
 examples/token-classification: @stefan-it
 documentation: @sgugger
 -->

 tokenizers: @mfuntowicz

## Information

Model I am using (Bert, XLNet ...): bert-base-uncased

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [x] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [x] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. Run the below code

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")
tokenizer.batch_encode_plus(
    [""foo"", ""bar "" * 1000], return_overflowing_tokens=True, truncation=True, padding=True
)
```

raises the following error:

```
Traceback (most recent call last):
  File ""foo.py"", line 4, in <module>
    tokenizer.batch_encode_plus(
  File ""/Users/user/work/transformers/src/transformers/tokenization_utils_base.py"", line 2121, in batch_encode_plus
    return self._batch_encode_plus(
  File ""/Users/user/work/transformers/src/transformers/tokenization_utils.py"", line 534, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
  File ""/Users/user/work/transformers/src/transformers/tokenization_utils.py"", line 606, in _batch_prepare_for_model
    batch_outputs = self.pad(
  File ""/Users/user/work/transformers/src/transformers/tokenization_utils_base.py"", line 2305, in pad
    assert all(
AssertionError: Some items in the output dictionnary have a different batch size than others.
```

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->

No error",,,,,,,,,1,
274,https://github.com/huggingface/transformers/issues/6652,6652,[],closed,2020-08-21 19:41:07+00:00,,5,"['encoder.version', 'decoder.version'] are unexpected when loading a pretrained BART model","Using an example from the bart doc:
https://huggingface.co/transformers/model_doc/bart.html#bartforconditionalgeneration

```
from transformers import BartTokenizer, BartForConditionalGeneration
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')
TXT = ""My friends are <mask> but they eat too many carbs.""

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')
input_ids = tokenizer([TXT], return_tensors='pt')['input_ids']
logits = model(input_ids)[0]

masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
probs = logits[0, masked_index].softmax(dim=0)
values, predictions = probs.topk(5)

print(tokenizer.decode(predictions).split())
```
gives:

```
Some weights of the model checkpoint at facebook/bart-large were not used 
when initializing BartForConditionalGeneration: 
['encoder.version', 'decoder.version']

- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
test:9: UserWarning: This overload of nonzero is deprecated:
        nonzero()
Consider using one of the following signatures instead:
        nonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1597302504919/work/torch/csrc/utils/python_arg_parser.cpp:864.)
  masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
['good', 'great', 'all', 'really', 'very']
```

well, there is one more issue of using a weird deprecated `nonzero()` invocation, which has to do with some strange undocumented requirement to pass the `as_tuple` arg, since pytorch 1.5 .https://github.com/pytorch/pytorch/issues/43425

we have `authorized_missing_keys`:
 `authorized_missing_keys = [r""final_logits_bias"", r""encoder\.version"", r""decoder\.version""]`
https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bart.py#L942
which correctly updates `missing_keys`  - should there be also an `authorized_unexpected_keys` which would clean up `unexpected_keys`?

(note: I re-edited this issue once I understood it better to save reader's time, the history is there if someone needs it)

And found another variety of it: for `['model.encoder.version', 'model.decoder.version']`
```
tests/test_modeling_bart.py::BartModelIntegrationTests::test_mnli_inference Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']
- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
PASSED
```",,,,,,,,,1,
349,https://github.com/huggingface/transformers/issues/6823,6823,[],closed,2020-08-30 01:47:07+00:00,,1,How to use encode_plus to force padding to specific length,"I am using the following code in the __getitem__() method of my dataset:

`    

    class MyDataset(Dataset):
        def __init__(self, myargs):

            #other code here
            self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased)
            self.max_len =50

`

__getitem__ method:

`    

        def __getitem__(self, idx):
           #other code here
           text_encoded = self.tokenizer.encode_plus(
              text,
              add_special_tokens=True,
              padding=True,
              truncation=True,
              max_length=self.max_len,
              return_token_type_ids=False,
              pad_to_max_length=True,
              return_attention_mask=True,
              return_tensors='pt',)

        input_ids=text_encoded['input_ids'].flatten()
        attention_mask=text_encoded['attention_mask'].flatten()

    >>input_ids: torch.Size([9]), attention_mask: torch.Size([9])
    >>input_ids: torch.Size([21]), attention_mask: torch.Size([21])

`

Even though I have set padding and truncation to True and set a max_length the returned lengths of the input_ids and attention_mask values in the retured text_encoded dict are variable depending on the input text. Is this normal behaviour? if so how can I ensure that ecery rturned sample is padded out to and truncated at a specific length?",,,,,,,,,1,
384,https://github.com/huggingface/transformers/issues/6899,6899,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-09-02 04:41:52+00:00,,2,Can the GPT2 of Transformers receive output hidden_states from external Encoder?,"# 鉂?Questions & Help
<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->
I want to use the GPT2 receive the output hidden_states from bert to calculate self_attention ,how can I do?
Thanks.
<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",,,,,,,,,1,
391,https://github.com/huggingface/transformers/issues/6912,6912,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-09-02 19:34:48+00:00,,3,batch_encode_plus does not lead to the same predictions as encode_plus,"I use batch_encode_plus to speed up the predictions but it leads to different results compared to ""encode_plus""
```
tokenizer = AutoTokenizer.from_pretrained(""deepset/roberta-base-squad2"")
inputs = tokenizer.encode_plus(QA_input['question'], QA_input['context'], padding = True, add_special_tokens=True, 

```
and
```
tokenizer = AutoTokenizer.from_pretrained(""deepset/roberta-base-squad2"")
questions = [(q['question'],q['context']) for q in q_dict]
inputs = tokenizer.batch_encode_plus(questions, padding=True, add_special_tokens=True, return_tensors=""pt"")

```

Most of the time in the predictions are the same but sometimes they are different:


```
def predict_batch_using_model(model, model_name, q_dict):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    questions = [(q['question'],q['context']) for q in q_dict]
    inputs = tokenizer.batch_encode_plus(questions, padding=True, add_special_tokens=True, return_tensors=""pt"")
    logger.debug('inputs batch_encode_plus: %s\n',inputs)
    
    if torch.cuda.is_available():
        inputs.to('cuda')
    
    answer_start_scores, answer_end_scores = model(**inputs)
    
    # a list of (answer, probs_start)
    answer_probs_start_batch = []
    
    for i in range(len(q_dict)):
        input_ids = inputs[""input_ids""].tolist()[i]

        text_tokens = tokenizer.convert_ids_to_tokens(input_ids)

        answer_start = torch.argmax(
            answer_start_scores[i]
        )  # Get the most likely beginning of answer with the argmax of the score
        answer_end = torch.argmax(answer_end_scores[i]) + 1  # Get the most likely end of answer with the argmax of the score
        logger.debug('answer_start, answer_end: %d %d %d\n',i, answer_start, answer_end)
        answer = tokenizer.convert_tokens_to_string(text_tokens[answer_start:answer_end])

        total_scores = answer_start_scores[i].add_(answer_end_scores[i])  # in place addition
        total_scores = total_scores.cpu().data.numpy()

        probs = _compute_softmax(total_scores)

        answer_probs_start_batch.append( ( answer, probs[answer_start]))
    
    return answer_probs_start_batch

def predict_using_model(model, model_name, QA_input):
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    inputs = tokenizer.encode_plus(QA_input['question'], QA_input['context'], padding = True, add_special_tokens=True, return_tensors=""pt"")
    logger.debug('inputs encode_plus: %s\n',inputs)
    
    if torch.cuda.is_available():
        inputs.to('cuda')

    input_ids = inputs[""input_ids""].tolist()[0]

    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)
    answer_start_scores, answer_end_scores = model(**inputs)

    answer_start = torch.argmax(
        answer_start_scores
    )  # Get the most likely beginning of answer with the argmax of the score
    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score
    logger.debug('answer_start, answer_end: %d %d %d\n',0, answer_start, answer_end)

    answer = tokenizer.convert_tokens_to_string(text_tokens[answer_start:answer_end])

    total_scores = answer_start_scores.add_(answer_end_scores)  # in place addition
    total_scores = total_scores.cpu().data.numpy()

    probs = _compute_softmax(total_scores)

    return answer, probs[answer_start]

```
the input dictionary
`
{""q"": ""what is color of thomas train"", ""gt_answer"": ""blue"", ""results"": [{""system"": ""Google KG"", ""response"": [], ""latency"": 350}, {""system"": ""Google CSE"", ""response"": [{""source"": ""www.strasburgrailroad.com"", ""title"": ""15 Fun Facts About Thomas the Tank Engine - Strasburg Rail Road"", ""snippet"": ""Aug 15, 2017 ... Thomas' iconic blue color is also the official color of the North Western Railway. \nBefore Thomas was blue he was originally teal green with\u00a0...""}, {""source"": ""www.youtube.com"", ""title"": ""Learn Colors with My First Railways | Playing Around with Thomas ..."", ""snippet"": ""Oct 21, 2017 ... About Thomas & Friends: Based on a series of children's books, \""Thomas & \nFriends\"" features Thomas the Tank Engine adventures with other\u00a0...""}, {""source"": ""en.wikipedia.org"", ""title"": ""Thomas the Tank Engine - Wikipedia"", ""snippet"": ""Thomas the Tank Engine is an anthropomorphised fictional steam locomotive in \nThe Railway ... In The Adventure Begins which is a retelling of Thomas's early \ndays on Sodor, he is a bluish-green colour when he first arrives on Sodor, his \ntanks\u00a0...""}, {""source"": ""play.thomasandfriends.com"", ""title"": ""Meet the Thomas & Friends Engines | Thomas & Friends"", ""snippet"": ""Discover all the engines from Sodor! Thomas & Friends fans can learn about all \ntheir favorite characters from the Thomas & Friends books, TV series and\u00a0...""}, {""source"": ""www.theguardian.com"", ""title"": ""Thomas the Tank Engine had to shut the hell up to save children ..."", ""snippet"": ""Jul 22, 2014 ... Thomas the Tank Engine had to shut the hell up to save children everywhere. \nThis article is more than 6 years old. Tracy Van Slyke. Classism\u00a0...""}, {""source"": ""www.amazon.com"", ""title"": ""RoomMates RMK1035SCS Thomas & Friends Peel ... - Amazon.com"", ""snippet"": ""RoomMates RMK1035SCS Thomas & Friends Peel and Stick Wall Decals ,Multi \ncolor. +. RoomMates RMK1831SCS Thomas The Tank Engine Peel and Stick\u00a0...""}, {""source"": ""ttte.fandom.com"", ""title"": ""Nia | Thomas the Tank Engine Wikia | Fandom"", ""snippet"": ""Nia is a Kenyan tank engine who befriended and accompanied Thomas on his \njourney ... Noticing how heavy his train was getting, she offered to help, but \nThomas ... of the Steam Team to have a snowplough that is not the same colour \nas her.""}, {""source"": ""www.amazon.com"", ""title"": ""Thomas The Tank Engine Color Block Cotton Hand ... - Amazon.com"", ""snippet"": ""Buy Thomas The Tank Engine Color Block Cotton Hand Towel: Home & Kitchen - \nAmazon.com \u2713 FREE DELIVERY possible on eligible purchases.""}, {""source"": ""ttte.fandom.com"", ""title"": ""Thomas the Tank Engine Colors"", ""snippet"": ""Thomas the Tank Engine: Colors is a book. Characters Thomas, Edward, Henry, \nJames, Percy, Bill...""}, {""source"": ""www.pinterest.com"", ""title"": ""Train cake, Thomas train cake, Thomas the train"", ""snippet"": ""Fondant Train Topper with Mini Train Cupcake Toppers. Each Topper is made to \norder and can be customized to suit your color scheme. Lot comes\u00a0...""}], ""latency"": 663}, {""system"": ""Bing entity"", ""response"": [], ""latency"": 698}, {""system"": ""Bing web"", ""response"": [{""source"": ""www.youtube.com"", ""title"": ""What Color Was Thomas the Tank Engine? | The Earl's Quiz ..."", ""snippet"": ""Based on a series of children's books, \""Thomas & Friends\"" features Thomas the Tank Engine adventures with other locomotives on the island of Sodor. Thomas often gets into trouble, but never gives ...""}, {""source"": ""british-learning.com"", ""title"": ""Thomas The Train Color Pages To Print \u2013 Learning How to Read"", ""snippet"": ""Thomas and friends coloring pages 55 thomas and friends pictures to print and color. 55 thomas and friends printable coloring pages for kids. 30 free printable thomas the train coloring pages. For boys and girls kids and adults teenagers and toddlers preschoolers and older kids at school.""}, {""source"": ""www.hometalk.com"", ""title"": ""Does anybody know what color blue is used for Thomas the ..."", ""snippet"": ""Here is a step by step YouTube guide to painting Thomas The Tank Engine and midway through, the blue used is referred to as a medium blue. Lighter than Navy, darker than Sky, maybe like a colonial blue? https://www.youtube.com/watch?v=MU8L6tIHk08""}], ""latency"": 879}], ""dt"": ""2020-08-14T15:06:39.638346+00:00""}`


  we observe difference in the prediction of the tenth context. Any reason for that?
",,,,,,,,,1,
402,https://github.com/huggingface/transformers/issues/6936,6936,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-09-04 03:10:18+00:00,,2,Load BERT+GPT2 in EncoderDecoder,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
I am working on modelling an EncoderDecoderModel using weights of BERT and GPT2, after going through lots of repo and issues I found that currently its not possible, but I found a model card that has used this model of BERT+GPT2 on dataset cnn-dailymail [here](https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16). I would like to know in which version of transformers is that possible and one thing more that there was an attribute passed in `TrainingArguments` module, that was `predict_from_generate`, I can't find that in `transformers`: 3.1.0, 3.0.2, 2.11.0, please clear me in which version does these parameters constitute.

@patrickvonplaten Please answer my query
<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->",,,,,,,,,1,
459,https://github.com/huggingface/transformers/issues/7043,7043,[],closed,2020-09-10 13:36:10+00:00,,2,Batch_encode_plus with is_pretokenized=True outputs incomplete input_ids,"## Environment info

- `transformers` version: 2.11.0
- Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.5.1+cu101 (True)
- Tensorflow version (GPU?): 2.3.0 (True)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No

### Who can help

 tokenizers: @mfuntowicz

## Information

Model I am using (Bert, XLNet ...): Distilbert, Roberta

I am trying to use batch_encode_plus with pretokenized inputs but the input_encodings are different than if the same text is run individually through encode_plus or if the same text is batch encoded without pretokenization.

## To reproduce

Code and outputs:

```python
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased', use_fast=True)
input_text = ['Roman Atwood is a content creator.', 'The Boston Celtics play their home games at TD Garden.']
sample_batch = [x.split(' ') for x in input_text]
sample_batch
```
Output: [['Roman', 'Atwood', 'is', 'a', 'content', 'creator.'],
 ['The', 'Boston',  'Celtics', 'play', 'their', 'home', 'games',  'at', 'TD', 'Garden.']]

```python
encoded_dict_batch = tokenizer.batch_encode_plus(sample_batch, is_pretokenized=True, padding=True, return_tensors='pt', truncation=True, max_length=125)
print(encoded_dict_batch)
```

The output is this which was far fewer non-zero tokens than I expected:

{'input_ids': tensor([[ 101, 2264,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,  102],
        [ 101, 1109,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1]])}

The attention mask is also a problem with all the padded indices being 1s.

Run only the first sentence through encoding (not in batch). All other parameters remain the same.

```python
encoded_dict_single = tokenizer.encode_plus(sample_batch[0], is_pretokenized=True, padding=True, return_tensors='pt', truncation=True, max_length=125)
print(encoded_dict_single)
```
This produces a much more sane output:

{'input_ids': tensor([[ 101, 2264, 1335, 2615, 1110,  170, 3438, 9264,  119,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}

The input ids here from index 2 onwards are replaced by 0 in the previous output even though it's well below max_length.

The words are truncated as well:

```python
print(encoded_dict_single.words())
print(encoded_dict_batch.words())
```
[None, 0, 1, 1, 2, 3, 4, 5, 6, None]
[None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]

Strangely enough, when calling batch_encode_plus with a pretokenized batch that consists of only 1 item in the list, it works fine.

```python
batch_of_one = [sample_batch[0]]
encoded_dict_batch_of_one = tokenizer.batch_encode_plus(batch_of_one, is_pretokenized=True, padding=True, return_tensors='pt', truncation=True, max_length=125)
print(encoded_dict_batch_of_one)
```
Output:
{'input_ids': tensor([[ 101, 2264, 1335, 2615, 1110,  170, 3438, 9264,  119,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}

I've tried with roberta as well and had the same results. Also, the same sentences in a batch but without pretokenization produces the correct outputs. It seems to be only the combination of pretokenized and batched sentences that are a problem.

## Expected behavior

The input_ids should not be replaced by 0s when tokenizing batched and pre-tokenized inputs.
",,,,,,,,,1,
469,https://github.com/huggingface/transformers/issues/7063,7063,[],closed,2020-09-11 02:20:43+00:00,,2,EncoderDecoderModel  generate function,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->
I follow the EncoderDecoder tutorials(https://huggingface.co/transformers/model_doc/encoderdecoder.html), but it seems that when i use generate function, 
generated = model.generate(input_ids, decoder_start_token_id=tokenizer.pad_token_id)
i have to use pad_token_id.If i ommit that parmas, there is a error message, I can't figure out why.

    device=next(self.parameters()).device,
TypeError: full() received an invalid combination of arguments - got (tuple, NoneType, device=torch.device, dtype=torch.dtype), but expected one of:
 * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
 * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)


tutorials code:
>>> from transformers import EncoderDecoderModel, BertTokenizer
>>> import torch

>>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints

>>> # forward
>>> input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
>>> outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)

>>> # training
>>> outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids, return_dict=True)
>>> loss, logits = outputs.loss, outputs.logits

>>> # save and load from pretrained
>>> model.save_pretrained(""bert2bert"")
>>> model = EncoderDecoderModel.from_pretrained(""bert2bert"")

>>> # generation
>>> generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.pad_token_id)


<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",,,,,,,,,1,
74,https://github.com/huggingface/transformers/issues/6215,6215,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-03 14:47:34+00:00,,2,Add `do_lower_case` handling to GPT2TokenizerFast and descendant tokenizers,"I'm currently pretraining a model from scratch based on RoBERTa architecture and am using a custom `ByteLevelBPETokenizer` tokenizer trained on my data with `lowercase` set to `True`. 

For ease of use, I load it with
```
from transformers import RobertaTokenizerFast

tokenizer = RobertaTokenizerFast(
                        'custom-vocab.json',
                        'custom-merges.txt',
                    )
```

The issue, is that unlike `BertTokenizerFast`, `RobertaTokenizerFast` (and it's parent class `GPT2TokenizerFast`) doesn't handle the `do_lower_case` parameter. 

I prefer using a BPE rather than WordPiece on this task so at the moment I make sure to lowercase text before tokenization but it can be error prone in the future.

Looking at the code, it seems `GPT2TokenizerFast` and descendants are the main ones lacking the parameter.

Could it be possible to add this?
",,,,,,1,,1,,
52,https://github.com/huggingface/transformers/issues/6173,6173,[],closed,2020-07-31 11:59:52+00:00,,2,"My finetuned gpt2 model is taking wayy too long to generate samples, like 5-8 minutes","I fine tuned the gpt2 model using transformers, i trained it on a lyrics dataset, and after successful training, when i do model.generate(args), it takes like a hell lot of time to genrate results
What Should i do?
",,,,,,,,1,,
60,https://github.com/huggingface/transformers/issues/6188,6188,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-01 15:34:30+00:00,,4,taeminlee/kogpt2 not working,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- None: hosted inference API

### Who can help
@LysandreJik 
@julien-c 
@TevenLeScao


## Information

Model I am using (Bert, XLNet ...):

The problem arises when using:
* [O] the official example scripts: Hosted Inference API testing page
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [O] my own task or dataset: Text generation, or anything

## To reproduce

Steps to reproduce the behavior:

1. https://huggingface.co/taeminlee/kogpt2?text=鞝?鞚措鞚€+頇嶊父霃?
2. Or type any text
3. Model returns just the text

## Expected behavior

Generated text should be returned, but isn't.
",,,,,,,,1,,
63,https://github.com/huggingface/transformers/issues/6192,6192,[],closed,2020-08-01 19:01:41+00:00,,20,GPT2 crashing at loss.backward(),"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Ubuntu
- Python version: 3.6
- PyTorch version (GPU?): 1.5.0
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: Yes

@LysandreJik 

## Information

Trying to finetune GPT2 model but the GPU is crashing after `loss.backward()`. I thought it might be just my code but I ran some different code involving finetuning GPT2 and that as well crashed in the same manner. 

Getting this warning as well. 
```
WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
``` 

A week or 2 back, everything was working fine but now the same code is crashing on `loss.backward()`. 

",,,,,,,,1,,
73,https://github.com/huggingface/transformers/issues/6211,6211,[],closed,2020-08-03 12:19:09+00:00,,2,Error when fine tuning GPT2 on GPU ,"An error occur when training with `run_language_modeling.py` file (Ubuntu 18, pytorch with cuda compiled), while the error does not occur on Macbook without the GPU

```
  File ""train.py"", line 283, in <module>
    main()
  File ""train.py"", line 247, in main
    trainer.train(model_path=model_path)
  File ""/home/antonio/anaconda3/lib/python3.7/site-packages/transformers/trainer.py"", line 518, in train
    for step, inputs in enumerate(epoch_iterator):
  File ""/home/antonio/anaconda3/lib/python3.7/site-packages/tqdm/_tqdm.py"", line 1017, in __iter__
    for obj in iterable:
  File ""/home/antonio/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 346, in __next__
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File ""/home/antonio/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 47, in fetch
    return self.collate_fn(data)
  File ""/home/antonio/anaconda3/lib/python3.7/site-packages/transformers/data/data_collator.py"", line 90, in __call__
    labels[labels == self.tokenizer.pad_token_id] = -100
TypeError: eq() received an invalid combination of arguments - got (NoneType), but expected one of:
 * (Tensor other)
      didn't match because some of the arguments have invalid types: (NoneType)
 * (Number other)
      didn't match because some of the arguments have invalid types: (NoneType)
```

Command and arguments used to run:
```bash
python train.py \
    --output_dir ckpts/prova \
    --model_type=gpt2 \
    --model_name_or_path=gpt2 \
    --do_train \
    --train_data_file data.train \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --save_steps 200
```

Any idea on how to solve this?",,,,,,,,1,,
90,https://github.com/huggingface/transformers/issues/6264,6264,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-05 09:19:18+00:00,,4,TF LMHead very slow: TFGPT2LMHeadModel is 7 times slower than Torch GPT2LMHeadModel ,"The benchmark shows the Tensorflow TFGPT2LMHeadModel is 7 times (140ms) slower than Torch GPT2LMHeadModel implementation (20ms). Based on the profile tool from latest https://www.tensorflow.org/tfx/serving/tensorboard, the bottleneck is here. https://github.com/huggingface/transformers/blob/d9149f00d1a4650bafa7e1cd73e10398193c852c/src/transformers/modeling_tf_utils.py#L796

```
from transformers import *

args = PyTorchBenchmarkArguments(
    models=[""gpt2_pt""],
    batch_sizes=[1], sequence_lengths=[8])

config_base = GPT2Config(n_layer=2, n_head=2)
config_base.architectures = [""GPT2LMHeadModel""]
benchmark = PyTorchBenchmark(args, configs=[config_base])
print(benchmark.run())

args = TensorFlowBenchmarkArguments(
    models=[""gpt2_tf""],
    batch_sizes=[1], sequence_lengths=[8])

config_base = GPT2Config(n_layer=2, n_head=2)
config_base.architectures = [""GPT2LMHeadModel""]
benchmark = TensorFlowBenchmark(args, configs=[config_base])
print(benchmark.run())
```

Without LMHeadModel, the benchmark is 6ms vs 8ms.",,,,,,,,1,,
103,https://github.com/huggingface/transformers/issues/6291,6291,[],closed,2020-08-06 14:14:12+00:00,,2,Why is the lm_head layer in GPT2LMHeadModel not a parameter?,"I loaded the model by 
```
from transformers import GPT2LMHeadModel
gpt2 = GPT2LMHeadModel.from_pretrained('distilgpt2')
```
doing `[n for n,p in gpt2.named_parameters()]` gives me:
```
['gpt2.transformer.wte.weight', 'gpt2.transformer.wpe.weight', 'gpt2.transformer.h.0.ln_1.weight', 'gpt2.transformer.h.0.ln_1.bias', 'gpt2.transformer.h.0.attn.c_attn.weight', 'gpt2.transformer.h.0.attn.c_attn.bias', 'gpt2.transformer.h.0.attn.c_proj.weight', 'gpt2.transformer.h.0.attn.c_proj.bias', 'gpt2.transformer.h.0.ln_2.weight', 'gpt2.transformer.h.0.ln_2.bias', 'gpt2.transformer.h.0.mlp.c_fc.weight', 'gpt2.transformer.h.0.mlp.c_fc.bias', 'gpt2.transformer.h.0.mlp.c_proj.weight', 'gpt2.transformer.h.0.mlp.c_proj.bias', 'gpt2.transformer.h.1.ln_1.weight', 'gpt2.transformer.h.1.ln_1.bias', 'gpt2.transformer.h.1.attn.c_attn.weight', 'gpt2.transformer.h.1.attn.c_attn.bias', 'gpt2.transformer.h.1.attn.c_proj.weight', 'gpt2.transformer.h.1.attn.c_proj.bias', 'gpt2.transformer.h.1.ln_2.weight', 'gpt2.transformer.h.1.ln_2.bias', 'gpt2.transformer.h.1.mlp.c_fc.weight', 'gpt2.transformer.h.1.mlp.c_fc.bias', 'gpt2.transformer.h.1.mlp.c_proj.weight', 'gpt2.transformer.h.1.mlp.c_proj.bias', 'gpt2.transformer.h.2.ln_1.weight', 'gpt2.transformer.h.2.ln_1.bias', 'gpt2.transformer.h.2.attn.c_attn.weight', 'gpt2.transformer.h.2.attn.c_attn.bias', 'gpt2.transformer.h.2.attn.c_proj.weight', 'gpt2.transformer.h.2.attn.c_proj.bias', 'gpt2.transformer.h.2.ln_2.weight', 'gpt2.transformer.h.2.ln_2.bias', 'gpt2.transformer.h.2.mlp.c_fc.weight', 'gpt2.transformer.h.2.mlp.c_fc.bias', 'gpt2.transformer.h.2.mlp.c_proj.weight', 'gpt2.transformer.h.2.mlp.c_proj.bias', 'gpt2.transformer.h.3.ln_1.weight', 'gpt2.transformer.h.3.ln_1.bias', 'gpt2.transformer.h.3.attn.c_attn.weight', 'gpt2.transformer.h.3.attn.c_attn.bias', 'gpt2.transformer.h.3.attn.c_proj.weight', 'gpt2.transformer.h.3.attn.c_proj.bias', 'gpt2.transformer.h.3.ln_2.weight', 'gpt2.transformer.h.3.ln_2.bias', 'gpt2.transformer.h.3.mlp.c_fc.weight', 'gpt2.transformer.h.3.mlp.c_fc.bias', 'gpt2.transformer.h.3.mlp.c_proj.weight', 'gpt2.transformer.h.3.mlp.c_proj.bias', 'gpt2.transformer.h.4.ln_1.weight', 'gpt2.transformer.h.4.ln_1.bias', 'gpt2.transformer.h.4.attn.c_attn.weight', 'gpt2.transformer.h.4.attn.c_attn.bias', 'gpt2.transformer.h.4.attn.c_proj.weight', 'gpt2.transformer.h.4.attn.c_proj.bias', 'gpt2.transformer.h.4.ln_2.weight', 'gpt2.transformer.h.4.ln_2.bias', 'gpt2.transformer.h.4.mlp.c_fc.weight', 'gpt2.transformer.h.4.mlp.c_fc.bias', 'gpt2.transformer.h.4.mlp.c_proj.weight', 'gpt2.transformer.h.4.mlp.c_proj.bias', 'gpt2.transformer.h.5.ln_1.weight', 'gpt2.transformer.h.5.ln_1.bias', 'gpt2.transformer.h.5.attn.c_attn.weight', 'gpt2.transformer.h.5.attn.c_attn.bias', 'gpt2.transformer.h.5.attn.c_proj.weight', 'gpt2.transformer.h.5.attn.c_proj.bias', 'gpt2.transformer.h.5.ln_2.weight', 'gpt2.transformer.h.5.ln_2.bias', 'gpt2.transformer.h.5.mlp.c_fc.weight', 'gpt2.transformer.h.5.mlp.c_fc.bias', 'gpt2.transformer.h.5.mlp.c_proj.weight', 'gpt2.transformer.h.5.mlp.c_proj.bias', 'gpt2.transformer.ln_f.weight', 'gpt2.transformer.ln_f.bias']
```
while running `print (gpt2)` gives me:
```
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
```

My question is why is the lm_head layer not included as the model's parameters? It bothers me at the moment because I am trying to only finetune the LM layer and realise I cant because doing something like `torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=lr, eps=1e-08)` will results in an error as the parameter list is empty
",,,,,,,,1,,
2506,https://github.com/huggingface/transformers/issues/10996,10996,[],closed,2021-03-31 14:25:29+00:00,,2,"GPT Neo, Print Most Probable Next Word: String Indices Must Be Integers","
This code is supposed to generate the next most probable word. However, the following problem arises.

```
!pip install git+https://github.com/huggingface/transformers.git
import torch
from transformers import GPTNeoForCausalLM, AutoTokenizer
model = GPTNeoForCausalLM.from_pretrained(""EleutherAI/gpt-neo-1.3B"")
tokenizer = AutoTokenizer.from_pretrained(""EleutherAI/gpt-neo-1.3B"")

prompt = """"""In the""""""
prompt = prompt.strip()
text = tokenizer.encode(prompt)
myinput, past = torch.tensor([text]), None
logits, past = model(myinput, past_key_values = past)
logits = logits[0,-1]
probabilities = torch.nn.functional.softmax(logits)
best_logits, best_indices = logits.topk(10)
best_words = [tokenizer.decode([idx.item()]) for idx in best_indices]
text.append(best_indices[0].item())
best_probabilities = probabilities[best_indices].tolist()
words = []
for i in range(10):
    m = (best_words[i])
    print(m)
```

`TypeError: string indices must be integers`",,,,,,,,1,,
2651,https://github.com/huggingface/transformers/issues/11255,11255,[],closed,2021-04-14 18:10:21+00:00,,5,"Big Bird generate() ""local variable 'next_tokens' referenced before assignment""","I am facing this problem when doing text summarization. I am using google/bigbird-roberta-base and I get the following error when calling model.generate(input, max_length = 4096, num_beams=4, early_stopping=True, length_penalty = 0.8):

```
Input length of input_ids is 4096, but ``max_length`` is set to 4096.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.
---------------------------------------------------------------------------
UnboundLocalError                         Traceback (most recent call last)
<ipython-input-13-90a633800ba7> in <module>()
----> 1 get_ipython().run_cell_magic('time', '', ' \ni = 0\nsize = 1\nout = []\nend = False\nprint_iters = 100\nsave_iters = 5\n \nwhile True:\n  if (i+size) >= n:\n    last = n\n    end = True\n  else:\n    last = i + size   \n \n  result = make_gen(   model_sum, tokens[i:last, :].detach().clone()  )\n \n  for j in range(result.shape[0]):\n    out.append(result[j])\n \n  if last % (print_iters*size) == 0:\n    print(last)\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    if last % (print_iters*size*save_iters) == 0:\n      with open(path_output + name + "".pkl"", \'wb\') as f:\n        pickle.dump(out, f)\n      print(""Saved to disk"")\n \n  if end:\n    break\n  i = last')

6 frames
<decorator-gen-53> in time(self, line, cell, local_ns)

<timed exec> in <module>()

/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py in beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)
   1808 
   1809         sequence_outputs = beam_scorer.finalize(
-> 1810             input_ids, beam_scores, next_tokens, next_indices, pad_token_id=pad_token_id, eos_token_id=eos_token_id
   1811         )
   1812 

UnboundLocalError: local variable 'next_tokens' referenced before assignment
```

",,,,,,,,1,,
2770,https://github.com/huggingface/transformers/issues/11458,11458,[],closed,2021-04-26 20:23:36+00:00,,2,"""Is next sentence"" pre-training task availability for Language Modeling scripts","# 馃殌 Feature request

BERT was trained on 2 pretraining tasks. The scripts [here ](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling) in the repo for language modeling only provide scripts for masked language modeling.

Is there any specific reason for this?

I was hoping if those scripts could be extended to include ""next sentence"" pretraining task to remain faithful to the pretraining methodology used by BERT if I choose to further pretrain on some corpus.

## Motivation

BERT uses 2 pre-training tasks and the scripts provide only of them.

## Your contribution

I am not sure if I can extend the script myself. I will gladly look into them if I know the there aren't any good reasons for them not being provided by HF in the first place.

Thanks a lot!",,,,,,,,1,,
4115,https://github.com/huggingface/transformers/issues/14001,14001,[],closed,2021-10-14 03:37:09+00:00,,3,How should I try to feed new memory states to the next segments,"Here is a simple exemple for my training loop based on Trainer class
```python
transfo_xl_config = transformers.TransfoXLConfig(**hparams)
model = transformers.TransfoXLForSequenceClassification(transfo_xl_config)

training_args = transformers.TrainingArguments(""test_trainer"", no_cuda=True)
trainer = transformers.Trainer(
    model=model,
    args=training_args,
    train_dataset=MyDataset('train'),
    eval_dataset=MyDataset('test')
)
trainer.train()
```
The question is how should I feed new memory states to the next segments in the next batch when using TransfoXLModel",,,,,,,,1,,
464,https://github.com/huggingface/transformers/issues/7051,7051,[],closed,2020-09-10 17:47:30+00:00,,2,How to pass tokenized hypotheses to TFRobertaForSequenceClassification model directly for faster inference?,"Hi, 
I have been running the zero shot classification pipeline for my use case by passing each text and it鈥檚 corresponding list of hypotheses labels, however it takes around 3 hours on a 32B GPU to classify ~22000 sentences(each sentence can have a varying number of labels ranging from 70 to 140 labels).

Is there a way to reduce the computation time by passing the embeddings to the sequence classification model directly rather than the raw list of labels?",,,,,,1,1,,,
246,https://github.com/huggingface/transformers/issues/6592,6592,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-19 10:59:10+00:00,,3,Unable to save and load RoBERTa model using tensorflow,"I have trained my model with Roberta-base and tested, it works.
But when i try to save the model I get following error:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in list_dependencies(self, obj)
    126                   obj._object_identifier,  # pylint: disable=protected-access
    127                   name,
--> 128                   extra_dependencies.keys()))
    129         yield base.TrackableReference(name, extra_dependencies[name])
    130       else:

ValueError: Error when exporting object <tensorflow.python.keras.layers.core.Activation object at 0x7fef7d287128> of with identifier=_tf_keras_layer. The object has an attribute named regularization_losses, which is reserved. List of all reserved attributes: dict_keys(['regularization_losses', 'variables', 'trainable_variables', 'keras_api'])


Unable to solve it.

Is it some obvious mistake am doing?


# Code:


path = '/path/model_1'
tf.keras.models.save_model(model_roberta_base,path, overwrite=True,include_optimizer=False,save_format='tf')
new_model = tf.keras.models.load_model(path)
result =predct_func(new_model,""Technology driven by Medical imaging and Artificial Intellignence"")
plot_result(result)",1,,,,1,,1,,,
2531,https://github.com/huggingface/transformers/issues/11031,11031,[],closed,2021-04-02 02:58:21+00:00,,1,Roberta and XLNet sentence pair training example,I want to use RoBERTa and XLNet for sentence pair input task (like sentence similarity task pair input). Can you explain with code example?,,,,1,,,1,,,
0,https://github.com/huggingface/transformers/issues/6074,6074,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-27 22:02:32+00:00,,2,Cannot use the RobertaForMultipleChoice model for processing multiple choice questions with 4 options,"Hello,

When I try to use the `RobertaForMultipleChoice` pretrained model with the code below, it generates an error:

```python
from transformers import RobertaTokenizer, RobertaForMultipleChoice
import torch
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForMultipleChoice.from_pretrained('roberta-base')
prompt = ""In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.""
choice0 = ""It is eaten with a fork and a knife.""
choice1 = ""It is eaten while held in the hand.""
choice2 = ""It is eathen with a napkin.""
choice3 = ""It is not eatable.""
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1
encoding = tokenizer([[prompt, prompt,prompt,prompt], [choice0, choice1, choice2, choice3]], return_tensors='pt', return_token_type_ids=True,padding=True)
```

The error message is:

```python

  File ""/Users/hyunjindominiquecho/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py"", line 534, in _batch_encode_plus
    ids, pair_ids = ids_or_pair_ids

ValueError: too many values to unpack (expected 2)
```

What am I doing wrong here? Thank you.

",,,,,,,1,,,
1,https://github.com/huggingface/transformers/issues/6078,6078,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-27 23:27:04+00:00,,1,model.roberta.from_pretrained() fails to change the parameters,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:

Hi all, my problem is ""how to use a pretrained 3-way sequence classification model to fine-tune on a 2-way classification task"". I am using ""https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py""; 
in the code I change

```
    model_args.model_name_or_path = 'roberta-large'
    model = AutoModelForSequenceClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool("".ckpt"" in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
    )
    model.roberta.from_pretrained(path_to_my_3way_pretrained_model)
```
This is what I tried. Pls note that I can not used the ""AutoModelForSequenceClassification.from_pretrained()"" to load ""path_to_my_3way_pretrained_model"" directly because there will be class mismatch (i.e., 3-way model does not apply to 2-way task); but I think no matter it is 3-way or 2-way sequence classification, what their architectures share is the roberta part; so I used the model.roberta to load the parameters from my pretrained model. 

However, I found the roberta parameters do not change, I checked as follows
```
    for name, param in model.named_parameters():
        if param.requires_grad and name == 'roberta.encoder.layer.16.attention.self.value.weight':
            print('new:', name, param.data)
```

Any clue why it doesn't work? Or any solution how to use a fine-tune a pretrained 3-way model on a 2-way task? Thanks",,,,,,,1,,,
16,https://github.com/huggingface/transformers/issues/6109,6109,[],closed,2020-07-28 20:29:57+00:00,,0,StopIteration error in RobertaForMultipleChoice,"Hello,
I am trying to execute the line below for my `RobertaForMultipleChoice` model:
```python
# retrieve the resulting mc_loss
mc_loss = model(input_ids = input_ids, attention_mask = attention_mask, labels =  mc_labels)[0]
```
but this generates the following error:

```python
Traceback (most recent call last):
  File ""STAT946_final_project_code_v4.py"", line 623, in <module>
    success_rate_list_diag_normal = main_function_diag_normal('/home/ec2-user/test.txt', 'test_ans_num.txt', num_iter, log_interval)
  File ""STAT946_final_project_code_v4.py"", line 414, in main_function_diag_normal
    best_model_RobertaForMultipleChoice_diag_normal = train_loop(model_RobertaForMultipleChoice, tokenizer, optimizer_1, scheduler_1, log_interval, svi_diag_normal, guide_diag_normal, best_model_RobertaForMultipleChoice_diag_normal)
  File ""STAT946_final_project_code_v4.py"", line 341, in train_loop
    optimizer, scheduler, log_interval, svi, guide, epoch)
  File ""STAT946_final_project_code_v4.py"", line 236, in train_mc_head
    mc_loss = model(input_ids = input_ids, attention_mask = attention_mask, labels =  mc_labels)[0]
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/pyro/nn/module.py"", line 413, in __call__
    return super().__call__(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_roberta.py"", line 441, in forward
    output_hidden_states=output_hidden_states,
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/pyro/nn/module.py"", line 413, in __call__
    return super().__call__(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py"", line 732, in forward
    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py"", line 228, in get_extended_attention_mask
    extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility
  File ""/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py"", line 159, in dtype
    first_tuple = next(gen)
StopIteration
```

How can I get around this type of error? Thank you,",,,,,,,1,,,
29,https://github.com/huggingface/transformers/issues/6127,6127,[],closed,2020-07-29 11:45:28+00:00,,1,Initializing XLMRobertaTokenizer using pretrained tokenizer expects serialized vocab,"Hi,
I am training an XLMRoberta model from scratch on Hindi. I am using a sentencepiece tokenizer trained exclusively on monolingual data following the steps mentioned in the [tokenizers repository](https://github.com/huggingface/tokenizers/tree/704cf3fdd2f607ead58a561b892b510b49c301db/bindings/python#using-the-provided-tokenizers). This results in the creation of `vocab.json` and `merges.txt`.
However when I try to initialize the tokenizer using `XLMRobertaTokenizer.from_pretrained` I get an error saying
```assumed 'models/sentencepiece' was a path, a model identifier, or url to a directory containing vocabulary files named ['sentencepiece.bpe.model'] but couldn't find such vocabulary files at this path or url. ``` 
I am assuming this is a serialized file based on [huggingface.co model](https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model) but don't know how to serialize my vocab.json file. I have already tried using `pickle` and `numpy`
Versions used:
transformers: 2.9.1
tokenizers: 0.7.0",,,,,,,1,,,
49,https://github.com/huggingface/transformers/issues/6164,6164,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-30 22:32:05+00:00,,2,RoBERTa ``tokenizer.decode`` does not produce the same sentence.,"## Environment info
- `transformers` version: 3.0.2
- Platform: Linux-4.15.0-74-generic-x86_64-with-glibc2.27
- Python version: 3.8.0
- PyTorch version (GPU?): 1.5.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: No.
- Using distributed or parallel set-up in script?: No.
     

### Who can help
@mfuntowicz
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 tensorflow: @jplu 
documentation: @sgugger
 -->

## Information

Model I am using (Bert, XLNet ...): RoBERTa

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [x] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [x] my own task or dataset: (give details below)

## To reproduce

This code example should reproduce the issue:

```python3
from transformers import RobertaTokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
s = """"""Meanwhile, Tucci's 'straight guy', the emphatic doctor Seger, is not developed into a more interesting character, like the fallible 'straight guys' Cuddy and Wilson.""""""
outputs = tokenizer(s)
input_ids = outputs['input_ids']
ss = tokenizer.decode(input_ids, skip_special_tokens=True)
print('s='+s)
print('ss='+ss)
```


<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior
I expect ``s`` and ``ss`` should be exactly the same. However, they are not. The outputs are:

```bash
s=Meanwhile, Tucci's 'straight guy', the emphatic doctor Seger, is not developed into a more interesting character, like the fallible 'straight guys' Cuddy and Wilson.

ss=Meanwhile, Tucci's'straight guy', the emphatic doctor Seger, is not developed into a more interesting character, like the fallible'straight guys' Cuddy and Wilson.
```
There are two spaces missing before ``'straight guy'``. 
I am not sure if this behavior is expected or it is a bug.
The thing is I want to use the sentence produced by the ``decode`` function and  I find the output is not exactly the same as the original sentence.

Thanks for the help!
<!-- A clear and concise description of what you would expect to happen. -->
",,,,,,,1,,,
54,https://github.com/huggingface/transformers/issues/6177,6177,[],closed,2020-07-31 20:49:29+00:00,,4,RoBERTa for QuestionAnswering ,"I am trying to replicate the example in this link 
https://github.com/huggingface/transformers/pull/1502/files, but I get the following error : 
``
ValueError                                Traceback (most recent call last)
<ipython-input-23-823cc70a5d4f> in <module>
----> 1 token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]
      2 start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))
      3 all_tokens = tokenizer.convert_ids_to_tokens(input_ids)
      4 print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))

<ipython-input-23-823cc70a5d4f> in <listcomp>(.0)
----> 1 token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]
      2 start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))
      3 all_tokens = tokenizer.convert_ids_to_tokens(input_ids)
      4 print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))

ValueError: 102 is not in list
``
I see the same error discussed in https://github.com/huggingface/transformers/issues/2261

Any ideas how I could resolve this issue ?
Thanks in advance.
",,,,,,,1,,,
64,https://github.com/huggingface/transformers/issues/6193,6193,[],closed,2020-08-01 23:07:00+00:00,,8,Some weights not initialized in pre-trained RobertaForMaskedLM,"The bug is similar to #2202.

I am trying to evaluate MLM perplexity (without training/finetuning) using Roberta with `run_language_modeling.py` (from the [official example](https://github.com/huggingface/transformers/tree/master/examples/language-modeling)). However, some weights seems to be reinitialized instead of getting loading from the pretrained Roberta checkpoint.

## To Reproduce (~~with master branch~~):

```
import logging
logging.basicConfig(level=logging.INFO)
from transformers import RobertaForMaskedLM
_ = RobertaForMaskedLM.from_pretrained('roberta-base')
```

It gives the following warning message:
```
WARNING:transformers.modeling_utils:Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

The perplexities I get on direct evaluation on Wikitext-2/103 datasets are also much higher than the official Roberta implementation from fairseq. I suspect this could be the reason.",,,,,,,1,,,
98,https://github.com/huggingface/transformers/issues/6279,6279,[],closed,2020-08-06 03:17:56+00:00,,1,TFRobertaMarkedLM model output issue,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->
I fine tuned a TFRobertaMarkedLM thanks to this notebook:
https://www.kaggle.com/riblidezso/finetune-xlm-roberta-on-jigsaw-test-data-with-mlm

My model config is like this:
{
  ""architectures"": [
    ""RobertaForMaskedLM""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""bos_token_id"": 0,
  ""eos_token_id"": 2,
  ""gradient_checkpointing"": false,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""layer_norm_eps"": 1e-05,
  ""max_position_embeddings"": 514,
  ""model_type"": ""roberta"",
  ""num_attention_heads"": 12,
  ""num_hidden_layers"": 12,
  ""pad_token_id"": 1,
  ""type_vocab_size"": 1,
  ""vocab_size"": 50265
}

I found my model output shape is **[n, sent_max_len, vocb_size]**, whereas I'd like to get **[n, sent_max_len, emb_size]**. 

Is it because I should not use model.predict(test_sent)[0]? Thanks!

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",,,,,,,1,,,
154,https://github.com/huggingface/transformers/issues/6406,6406,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-11 05:05:27+00:00,,2,RuntimeError: Error while creating shape using tf-xlm-roberta-large,"I get the following runtime error after the 2nd fold.

this is my model:
maxlen = 50
`with strategy.scope():
        #bert_encoder = TFBertModel.from_pretrained(model_name)
        base_model = TFAutoModel.from_pretrained(model_name)
        input_word_ids = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = ""input_word_ids"")
        input_mask = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = ""input_mask"")
        input_type_ids = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = ""input_type_ids"")

        embedding = base_model([input_word_ids, input_mask, input_type_ids])[0]
        output = tf.keras.layers.Dense(3, activation = 'softmax')(embedding[:, 0, :])

        model = tf.keras.Model(inputs = [input_word_ids, input_mask, input_type_ids], outputs = output)
        model.compile(tf.keras.optimizers.Adam(lr = 1e-5), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])

`
And the traceback below:

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-27-a45ce90453f2> in <module>
     22 
     23     K.clear_session()
---> 24     model = build_model(maxlen, model_name)
     25     checkpoint = tf.keras.callbacks.ModelCheckpoint(
     26                 'XLMRoberta_fold-%i.h5'%fold, monitor = 'val_loss', verbose = 1, save_best_only = True,

<ipython-input-23-9faa2e5f1d9b> in build_model(maxlen, model_name)
      2     with strategy.scope():
      3         #bert_encoder = TFBertModel.from_pretrained(model_name)
----> 4         base_model = TFAutoModel.from_pretrained(model_name)
      5         input_word_ids = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = ""input_word_ids"")
      6         input_mask = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = ""input_mask"")

/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    421         for config_class, model_class in TF_MODEL_MAPPING.items():
    422             if isinstance(config, config_class):
--> 423                 return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
    424         raise ValueError(
    425             ""Unrecognized configuration class {} for this kind of TFAutoModel: {}.\n""

/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    482             return load_pytorch_checkpoint_in_tf2_model(model, resolved_archive_file, allow_missing_keys=True)
    483 
--> 484         model(model.dummy_inputs, training=False)  # build the network with dummy inputs
    485 
    486         assert os.path.isfile(resolved_archive_file), ""Error retrieving file {}"".format(resolved_archive_file)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    966           with base_layer_utils.autocast_context_manager(
    967               self._compute_dtype):
--> 968             outputs = self.call(cast_inputs, *args, **kwargs)
    969           self._handle_activity_regularization(inputs, outputs)
    970           self._set_mask_metadata(inputs, outputs, input_masks)

/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_roberta.py in call(self, inputs, **kwargs)
    229             heads.
    230         """"""
--> 231         outputs = self.roberta(inputs, **kwargs)
    232         return outputs
    233 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    966           with base_layer_utils.autocast_context_manager(
    967               self._compute_dtype):
--> 968             outputs = self.call(cast_inputs, *args, **kwargs)
    969           self._handle_activity_regularization(inputs, outputs)
    970           self._set_mask_metadata(inputs, outputs, input_masks)

/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_bert.py in call(self, inputs, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, training)
    604             # head_mask = tf.constant([0] * self.num_hidden_layers)
    605 
--> 606         embedding_output = self.embeddings([input_ids, position_ids, token_type_ids, inputs_embeds], training=training)
    607         encoder_outputs = self.encoder(
    608             [embedding_output, extended_attention_mask, head_mask, output_attentions, output_hidden_states],

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    962         # Eager execution on data tensors.
    963         with backend.name_scope(self._name_scope()):
--> 964           self._maybe_build(inputs)
    965           cast_inputs = self._maybe_cast_inputs(inputs)
    966           with base_layer_utils.autocast_context_manager(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)
   2414         # operations.
   2415         with tf_utils.maybe_init_scope(self):
-> 2416           self.build(input_shapes)  # pylint:disable=not-callable
   2417       # We must set also ensure that the layer is marked as built, and the build
   2418       # shape is stored since user defined build functions may not be calling

/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_bert.py in build(self, input_shape)
    144                 ""weight"",
    145                 shape=[self.vocab_size, self.hidden_size],
--> 146                 initializer=get_initializer(self.initializer_range),
    147             )
    148         super().build(input_shape)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)
    575         synchronization=synchronization,
    576         aggregation=aggregation,
--> 577         caching_device=caching_device)
    578     if regularizer is not None:
    579       # TODO(fchollet): in the future, this should be handled at the

/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)
    741         dtype=dtype,
    742         initializer=initializer,
--> 743         **kwargs_for_getter)
    744 
    745     # If we set an initializer and the variable processed it, tracking will not

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)
    139       synchronization=synchronization,
    140       aggregation=aggregation,
--> 141       shape=variable_shape if variable_shape else None)
    142 
    143 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    257   def __call__(cls, *args, **kwargs):
    258     if cls is VariableV1:
--> 259       return cls._variable_v1_call(*args, **kwargs)
    260     elif cls is Variable:
    261       return cls._variable_v2_call(*args, **kwargs)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)
    218         synchronization=synchronization,
    219         aggregation=aggregation,
--> 220         shape=shape)
    221 
    222   def _variable_v2_call(cls,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in getter(**kwargs)
     64 
     65   def getter(**kwargs):
---> 66     return captured_getter(captured_previous, **kwargs)
     67 
     68   return getter

/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py in creator_with_resource_vars(next_creator, **kwargs)
   1765         kwargs[""initial_value""] = kwargs[""initial_value""].wrapped_value
   1766 
-> 1767       return self._create_variable(next_creator, **kwargs)
   1768 
   1769     def distributed_getter(getter, *args, **kwargs):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py in _create_variable(self, next_creator, **kwargs)
    670                                            tpu_values.TPUMirroredVariable,
    671                                            tpu_values.TPUSyncOnReadVariable,
--> 672                                            **kwargs)
    673 
    674   def _reduce_to(self, reduce_op, value, destinations, experimental_hints):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/values.py in create_mirrored_variable(strategy, real_mirrored_creator, mirrored_cls, sync_on_read_cls, **kwargs)
    692   # here.
    693   with tape.stop_recording():
--> 694     value_list = real_mirrored_creator(**kwargs)
    695     var_cls = sync_on_read_cls if is_sync_on_read else mirrored_cls
    696     result = var_cls(strategy, value_list, aggregation)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py in _real_mirrored_creator(**kwargs)
    660 
    661           with context.device_policy(context.DEVICE_PLACEMENT_SILENT):
--> 662             v = next_creator(**kwargs)
    663 
    664           assert not isinstance(v, tpu_values.TPUMirroredVariable)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kwargs)
    196                         shape=None):
    197     """"""Call on Variable class. Useful to force the signature.""""""
--> 198     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
    199     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
    200       previous_getter = _make_getter(getter, previous_getter)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)
   2596         synchronization=synchronization,
   2597         aggregation=aggregation,
-> 2598         shape=shape)
   2599   else:
   2600     return variables.RefVariable(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    261       return cls._variable_v2_call(*args, **kwargs)
    262     else:
--> 263       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    264 
    265 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
   1432           aggregation=aggregation,
   1433           shape=shape,
-> 1434           distribute_strategy=distribute_strategy)
   1435 
   1436   def _init_from_args(self,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)
   1568                 name=""initial_value"", dtype=dtype)
   1569           if shape is not None:
-> 1570             if not initial_value.shape.is_compatible_with(shape):
   1571               raise ValueError(
1572                   ""The initial value's shape (%s) is not compatible with ""

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in shape(self)
   1063         # `_tensor_shape` is declared and defined in the definition of
   1064         # `EagerTensor`, in C.
-> 1065         self._tensor_shape = tensor_shape.TensorShape(self._shape_tuple())
   1066       except core._NotOkStatusException as e:
   1067         six.raise_from(core._status_to_exception(e.code, e.message), None)

RuntimeError: Error while creating shape",,,,,,,1,,,
196,https://github.com/huggingface/transformers/issues/6484,6484,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-14 12:35:10+00:00,,4,Assertion error when training a new RoBERTa from scratch,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Linux-3.10.0-862.14.4.el7.x86_64-x86_64-with-centos-7.5.1804-Core
- Python version: 3.6.10
- PyTorch version (GPU?): 1.6.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: <No.

### Who can help

Maybe @LysandreJik  ? :-) 


## Information

Model I am using RoBERTa:

The problem arises when using:
* [x] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [x] my own task or dataset: (give details below)

The dataset is a simple line-by-line dataset.

## To reproduce

Steps to reproduce the behavior:

1. Train a tokenizer according to [this](https://huggingface.co/blog/how-to-train#2-train-a-tokenizer)
2. Split line-by-line dataset into train and eval
3. Run below:

```sh
python run_language_modeling.py \
    --output_dir $MODEL_DIR/myBERT-small-v1 \
    --model_type roberta \
    --mlm \
    --config_name $MODEL_DIR/myBERT-small \
    --tokenizer_name $MODEL_DIR/myBERT-small \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 8 \
    --learning_rate 1e-4 \
    --num_train_epochs 5 \
    --save_total_limit 2 \
    --save_steps 2000 \
    --per_gpu_train_batch_size 16 \
    --evaluate_during_training \
    --line_by_line \
    --train_data_file $HOME/myBERT/train.txt \
    --eval_data_file $HOME/myBERT/eval.txt \
    --seed 42
```

```log
08/13/2020 14:23:20 - INFO - transformers.configuration_utils -   loading configuration file /home/erippeth/myBERT/model/myBERT-small/config.json
08/13/2020 14:23:20 - INFO - transformers.configuration_utils -   Model config RobertaConfig {
  ""architectures"": [
    ""RobertaForMaskedLM""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""bos_token_id"": 0,
  ""eos_token_id"": 2,
  ""gradient_checkpointing"": false,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""layer_norm_eps"": 1e-05,
  ""max_position_embeddings"": 514,
  ""model_type"": ""roberta"",
  ""num_attention_heads"": 12,
  ""num_hidden_layers"": 12,
  ""pad_token_id"": 1,
  ""type_vocab_size"": 1,
  ""vocab_size"": 52000
}

08/13/2020 14:23:20 - INFO - transformers.tokenization_utils_base -   Model name '/home/erippeth/myBERT/model/myBERT-small' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '/home/erippeth/myBERT/model/myBERT-small' is a path, a model identifier, or url to a directory containing tokenizer files.
08/13/2020 14:23:20 - INFO - transformers.tokenization_utils_base -   Didn't find file /home/erippeth/myBERT/model/myBERT-small/added_tokens.json. We won't load it.
08/13/2020 14:23:20 - INFO - transformers.tokenization_utils_base -   Didn't find file /home/erippeth/myBERT/model/myBERT-small/special_tokens_map.json. We won't load it.
08/13/2020 14:23:20 - INFO - transformers.tokenization_utils_base -   Didn't find file /home/erippeth/myBERT/model/myBERT-small/tokenizer_config.json. We won't load it.
08/13/2020 14:23:20 - INFO - transformers.tokenization_utils_base -   Didn't find file /home/erippeth/myBERT/model/myBERT-small/tokenizer.json. We won't load it.
08/13/2020 14:23:20 - INFO - transformers.tokenization_utils_base -   loading file /home/erippeth/myBERT/model/myBERT-small/vocab.json
08/13/2020 14:23:20 - INFO - transformers.tokenization_utils_base -   loading file /home/erippeth/myBERT/model/myBERT-small/merges.txt
08/13/2020 14:23:20 - INFO - transformers.tokenization_utils_base -   loading file None
08/13/2020 14:23:20 - INFO - transformers.tokenization_utils_base -   loading file None
08/13/2020 14:23:20 - INFO - transformers.tokenization_utils_base -   loading file None
08/13/2020 14:23:20 - INFO - transformers.tokenization_utils_base -   loading file None
08/13/2020 14:23:21 - INFO - __main__ -   Training new model from scratch
/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/transformers/modeling_auto.py:709: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.
  FutureWarning,
08/13/2020 14:23:27 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at /home/erippeth/myBERT/train.txt
08/13/2020 17:40:20 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at /home/erippeth/myBERT/eval.txt
08/13/2020 18:56:31 - WARNING - transformers.trainer -   You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.
08/13/2020 18:56:31 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
08/13/2020 18:56:31 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.
08/13/2020 18:56:31 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.
08/13/2020 18:56:31 - INFO - transformers.trainer -   ***** Running training *****
08/13/2020 18:56:31 - INFO - transformers.trainer -     Num examples = 16661098
08/13/2020 18:56:31 - INFO - transformers.trainer -     Num Epochs = 5
08/13/2020 18:56:31 - INFO - transformers.trainer -     Instantaneous batch size per device = 8
08/13/2020 18:56:31 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 16
08/13/2020 18:56:31 - INFO - transformers.trainer -     Gradient Accumulation steps = 1
08/13/2020 18:56:31 - INFO - transformers.trainer -     Total optimization steps = 5206595
^MEpoch:   0%|          | 0/5 [00:00<?, ?it/s]
^MIteration:   0%|          | 0/1041319 [00:00<?, ?it/s]ESC[A
^MIteration:   0%|          | 1/1041319 [00:01<508:20:24,  1.76s/it]ESC[A
^MIteration:   0%|          | 2/1041319 [00:02<395:24:33,  1.37s/it]ESC[A
^MIteration:   0%|          | 3/1041319 [00:02<306:50:22,  1.06s/it]ESC[A/opt/conda/conda-bld/pytorch_1595629416375/work/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [229,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/opt/conda/conda-bld/pytorch_1595629416375/work/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [229,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
...
/opt/conda/conda-bld/pytorch_1595629416375/work/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [275,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
Iteration:   0%|          | 3/1041319 [00:03<332:03:04,  1.15s/it]
Epoch:   0%|          | 0/5 [00:03<?, ?it/s]
Traceback (most recent call last):
  File ""run_language_modeling.py"", line 281, in <module>
    main()
  File ""run_language_modeling.py"", line 245, in main
    trainer.train(model_path=model_path)
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/transformers/trainer.py"", line 499, in train
    tr_loss += self._training_step(model, inputs, optimizer)
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/transformers/trainer.py"", line 622, in _training_step
    outputs = model(**inputs)
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/transformers/modeling_roberta.py"", line 239, in forward
    output_hidden_states=output_hidden_states,
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/transformers/modeling_bert.py"", line 762, in forward
    output_hidden_states=output_hidden_states,
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/transformers/modeling_bert.py"", line 439, in forward
    output_attentions,
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/transformers/modeling_bert.py"", line 371, in forward
    hidden_states, attention_mask, head_mask, output_attentions=output_attentions,
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/transformers/modeling_bert.py"", line 315, in forward
    hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions,
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/erippeth/miniconda3/envs/myBERT/lib/python3.6/site-packages/transformers/modeling_bert.py"", line 258, in forward
    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
RuntimeError: CUDA error: device-side assert triggered
```

## Expected behavior

Model should train without failure, but instead it fails with an assertion error. I believe this is related to an embedding dimension issue, but the config's `vocab_size` matches the length of the newly-trained tokenizer and this is the embedding dimension set in the training script.",,,,,,,1,,,
346,https://github.com/huggingface/transformers/issues/6819,6819,[],closed,2020-08-29 18:44:26+00:00,,3,Unable to establish Lock on cached tokenizer output from RobertaTokenizer,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Linux-3.10.0-1127.13.1.el7.x86_64-x86_64-with-redhat-7.8-Maipo
- Python version: 3.7.5
- PyTorch version (GPU?): 1.6.0 (True)
- Tensorflow version (GPU?): 2.3.0 (True)
- Using GPU in script?: yes
- Using distributed or parallel set-up in script?: no

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people. -->
 
 tokenizers: @mfuntowicz 

## Information

Model I am using (Bert, XLNet ...): Roberta (`roberta-large-mnli`)

The problem arises when using:
* [X] the official example scripts: `run_glue.py`
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [X] an official GLUE/SQUaD task: MNLI
* [ ] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. download GLUE data using official download script and place it in the root of `transformers`
2. `python run_glue.py --model_name_or_path roberta-large-mnli --data_dir ../glue_data --output_dir tmp --task_name MNLI --do_eval

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

```bash
Traceback (most recent call last):
  File ""./run_glue.py"", line 247, in <module>
    main()
  File ""./run_glue.py"", line 143, in main
    if training_args.do_eval
  File ""/home/USER/anaconda3/envs/nlu/lib/python3.7/site-packages/transformers/data/datasets/glue.py"", line 106, in __init__
    with FileLock(lock_path):
  File ""/home/USER/anaconda3/envs/nlu/lib/python3.7/site-packages/filelock.py"", line 323, in __enter__
    self.acquire()
  File ""/home/USER/anaconda3/envs/nlu/lib/python3.7/site-packages/filelock.py"", line 271, in acquire
    self._acquire()
  File ""/home/USER/anaconda3/envs/nlu/lib/python3.7/site-packages/filelock.py"", line 384, in _acquire
    fd = os.open(self._lock_file, open_mode)
FileNotFoundError: [Errno 2] No such file or directory: '../glue_data/cached_dev_RobertaTokenizer_128_mnli.lock'
```

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->

The script should be able to create the above-mentioned cached file if one doesn't exist, and acquire lock and load it if it does.
",,,,,,,1,,,
362,https://github.com/huggingface/transformers/issues/6848,6848,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-31 15:21:19+00:00,,1,unexpected behavior on RoBERTa tokenizer when using additional special tokens ,"## Environment info
- `transformers` version: 3.0.2
- Platform: Windows-10-10.0.18362-SP0
- Python version: 3.8.3
- PyTorch version (GPU?): 1.6.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: <no
- Using distributed or parallel set-up in script?: no

### Who can help
 tokenizers: @mfuntowicz 

## Information


When trying to tokenize using RoBERTa tokenized and a special token and using add_prefix_space=True, the token following the special token does not get a space. 

## To reproduce

Steps to reproduce the behavior:

1. run the following code

```
tokenizer = RobertaTokenizer.from_pretrained(""roberta-base"")
tokenizer.add_special_tokens('[d_s]')
print(tokenizer('[d_s] test', add_prefix_space=True))
print((tokenizer('test', add_prefix_space=True))
```

## output
{'input_ids': [0, 1296, 2], 'attention_mask': [1, 1, 1]}
{'input_ids': [0, 50271, 21959, 2], 'attention_mask': [1, 1, 1, 1]}


## Expected behavior
{'input_ids': [0, 1296, 2], 'attention_mask': [1, 1, 1]}
{'input_ids': [0, 50271, 1296, 2], 'attention_mask': [1, 1, 1, 1]}

The second token should not change because of the first one.
",,,,,,,1,,,
417,https://github.com/huggingface/transformers/issues/6966,6966,[],closed,2020-09-05 20:23:09+00:00,,3,SPM Tokenizer confusion with fairseq Roberta,"Hi,

I have pre-trained a custom Roberta Model from scratch with a unigram sentencepiece model (also trained from scratch). I have converted the model from fairseq to huggingface with this [script](https://github.com/huggingface/transformers/blob/master/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py) successfully.

I have tried loading the model in huggingface, which was successful, but the issue lies with the tokenizer, I tried using the Roberta tokenizer but it screamed at me because it was looking for merges and vocab. I then loaded my spm model with AlbertTokenizer, but when I try to test it out with a simple fill_mask, the answer tokens are incorrect. How do I correctly use my SPM model with Roberta? I also have dict.txt from fairseq.

Any help would be appreciated!",,,,,,,1,,,
3,https://github.com/huggingface/transformers/issues/6082,6082,[],closed,2020-07-28 02:21:15+00:00,,4,馃悰 Inconsistencies between BartTokenizer and BartTokenizerFast,"# 馃悰 Bug

## Description

It's possible to use the argument `add_prefix_space` with `BartTokenizer` :

```
from transformers import BartTokenizer

t = BartTokenizer.from_pretrained(""facebook/bart-large"")
x = t(""This is an example."", add_prefix_space=True)
```

But when doing the same with `BartTokenizerFast` :
```
from transformers import BartTokenizerFast

t = BartTokenizerFast.from_pretrained(""facebook/bart-large"")
x = t(""This is an example."", add_prefix_space=True)
```

It throws the following error :

```
ValueError: Keyword arguments {'add_prefix_space': True} not recognized.
```

## To reproduce

[Colab notebook](https://colab.research.google.com/drive/1f0W2llsJfVIkXsYk0XK1C3oiy2xSqYOx?usp=sharing)

## Work-around

It seems working if the argument is specified in the constructor :
```
from transformers import BartTokenizerFast

t = BartTokenizerFast.from_pretrained(""facebook/bart-large"", add_prefix_space=True)
x = t(""This is an example."")
```

@sshleifer ",,,,,,1,,,,
24,https://github.com/huggingface/transformers/issues/6119,6119,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-07-29 08:18:43+00:00,,3,馃悰 Empty TypeError on BartTokenizerFast.decode(tensor),"## Environment info
`transformers` `3.0.2`

### Who can help
 Summarization: @sshleifer 

## To reproduce

```python
import torch
from transformers import BartTokenizerFast

t = BartTokenizerFast.from_pretrained('facebook/bart-large')

x = torch.tensor([0, 34, 45, 23, 54, 65, 765, 2])
t.decode(x)
```

will throw an empty `TypeError` :

```
File ""/home/me/.venv/summarization/lib/python3.6/site-packages/tokenizers/implementations/base_tokenizer.py"", line 267, in decode
    return self._tokenizer.decode(ids, skip_special_tokens=skip_special_tokens)
TypeError
```

To reproduce : [Colab Notebook](https://colab.research.google.com/drive/1bnP8TvmRrHrMD-7H2MOQQSE8QrhCb7SC?usp=sharing)

## Expected behavior

No error thrown, like with regular `Tokenizer` :

```python
import torch
from transformers import BartTokenizer

t = BartTokenizer.from_pretrained('facebook/bart-large')

x = torch.tensor([0, 34, 45, 23, 54, 65, 765, 2])
t.decode(x)
```

> `<s> has not at who one short</s>`",,,,,,1,,,,
59,https://github.com/huggingface/transformers/issues/6186,6186,[],closed,2020-08-01 10:24:56+00:00,,1,Remove inconsistency between BertTokenizer and BertTokenizerFast ,"# 馃殌 Feature request
`BertTokenizerFast` has the option to specify `strip_accents=False`. The `BertTokenizer` does not have this option. This inconsistency should be removed by adding the `strip_accents` parameter to `BertTokenizer`.

## Motivation
Without adding this, the `BertTokenizer` can not be used for language models which are lowercase but have accents.

In case of a language model with lowercase and with accents you are forced to load the tokenizer by this:

```python
tokenizer = AutoTokenizer.from_pretrained(""<model_name_or_path>"", use_fast=True, strip_accents=False)
```

This will NOT work: `tokenizer = AutoTokenizer.from_pretrained(""<model_name_or_path>"")`

And even this would not work: `tokenizer = AutoTokenizer.from_pretrained(""<model_name_or_path>"", strip_accents=False)`

## Your contribution
With some hints I am willing to contribute.",,,,,,1,,,,
65,https://github.com/huggingface/transformers/issues/6194,6194,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-02 04:10:00+00:00,,4,longformertokenizerFast gives error,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version:
- Platform:
- Python version:
- PyTorch version (GPU?):
- Tensorflow version (GPU?):
- Using GPU in script?:
- Using distributed or parallel set-up in script?:

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 tensorflow: @jplu 
documentation: @sgugger
 -->

## Information

Model I am using (Bert, XLNet ...):

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [ X ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [ X ] my own task or dataset: (give details below)

## To reproduce

using LongformerTokenizerFast gives error. but using LongformerTokenizer works without any issues keeping everything same

```

---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-39-263240bbee7e> in <module>
----> 1 main()

<ipython-input-37-2c27a8a4db79> in main()
     99     )
    100 
--> 101     train_dataset = CustomDataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
    102     eval_dataset = CustomDataset(data_args, tokenizer=tokenizer, mode=""test"") if training_args.do_eval else None
    103 

<ipython-input-36-85278feb74ec> in __init__(self, args, tokenizer, limit_length, mode)
    184                     max_length=args.max_seq_length,
    185                     label_list=label_list,
--> 186                     output_mode=self.output_mode,
    187                 )
    188                 start = time.time()

/opt/conda/lib/python3.7/site-packages/transformers/data/processors/glue.py in glue_convert_examples_to_features(examples, tokenizer, max_length, task, label_list, output_mode)
     63         return _tf_glue_convert_examples_to_features(examples, tokenizer, max_length=max_length, task=task)
     64     return _glue_convert_examples_to_features(
---> 65         examples, tokenizer, max_length=max_length, task=task, label_list=label_list, output_mode=output_mode
     66     )
     67 

/opt/conda/lib/python3.7/site-packages/transformers/data/processors/glue.py in _glue_convert_examples_to_features(examples, tokenizer, max_length, task, label_list, output_mode)
    133         max_length=max_length,
    134         padding=""max_length"",
--> 135         truncation=True,
    136     )
    137 

/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in __call__(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_pretokenized, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   1918                 return_length=return_length,
   1919                 verbose=verbose,
-> 1920                 **kwargs,
   1921             )
   1922         else:

/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_pretokenized, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   2103             return_length=return_length,
   2104             verbose=verbose,
-> 2105             **kwargs,
   2106         )
   2107 

/opt/conda/lib/python3.7/site-packages/transformers/tokenization_gpt2.py in _batch_encode_plus(self, *args, **kwargs)
    385         )
    386 
--> 387         return super()._batch_encode_plus(*args, **kwargs)
    388 
    389     def _encode_plus(self, *args, **kwargs) -> BatchEncoding:

/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py in _batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_pretokenized, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
    378         else:
    379             encodings = self._tokenizer.encode_batch(
--> 380                 batch_text_or_text_pairs, add_special_tokens=add_special_tokens, is_pretokenized=is_pretokenized
    381             )
    382 

/opt/conda/lib/python3.7/site-packages/tokenizers/implementations/base_tokenizer.py in encode_batch(self, inputs, is_pretokenized, add_special_tokens)
    247             raise ValueError(""encode_batch: `inputs` can't be `None`"")
    248 
--> 249         return self._tokenizer.encode_batch(inputs, is_pretokenized, add_special_tokens)
    250 
    251     def decode(self, ids: List[int], skip_special_tokens: Optional[bool] = True) -> str:

Exception: Truncation error: Specified max length is too low to respect the various constraints
```
",,,,,,1,,,,
99,https://github.com/huggingface/transformers/issues/6282,6282,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-06 08:02:57+00:00,,1,Using tensorrt model.engine Inference speed is relatively fast. Why is onnxruntime based on tensorrt as slow as CPU inference,"# 鉂?Questions & Help
https://github.com/huggingface/transformers/blob/master/notebooks/04-onnx-export.ipynb
Based on different providers to infer the  .onnx model, why is CUDA the fastest and tensorrt based model particularly slow
Using tensorrt model.engine Inference speed is relatively fast. Why is onnxruntime based on tensorrt as slow as CPU inference
",,,,,,1,,,,
142,https://github.com/huggingface/transformers/issues/6370,6370,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-09 17:47:39+00:00,,2,FastTokenizer not returning batch_size for offset_mapping for short texts,"## Environment info
- `transformers` version: 3.0.2
- Platform: Linux-5.4.0-42-generic-x86_64-with-glibc2.10
- Python version: 3.8.2
- PyTorch version (GPU?): 1.6.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

# Who can help
 tokenizers: @mfuntowicz

## Information
When working with `padding` and `truncation` on short texts (smaller than `max_len`),
the FastTokenizer will return the batch_size dimension if `return_tensors=None`. 
However, when `return_tensors=""pt""` or `return_tensors=""np""` are enabled (I haven't tested it on Tensorflow), they **won't return the batch dimension**.

## To reproduce

Loading fast tokenizer:
```python3
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"", use_fast=True)
```

Behavior on ""short"" texts without `return_tensors`:
```python3

out = tokenizer(""test text"",
                padding='max_length',
                truncation=True,
                return_overflowing_tokens=True,
                return_offsets_mapping=True,
                )

# Convert to tensor outside the tokenizer
print(torch.tensor(out[""offset_mapping""]).shape)
>>> torch.Size([1, 512, 2])
```

Behavior with `return_tensors`:
```python3

out = tokenizer(""test text"",
                padding='max_length',
                truncation=True,
                return_overflowing_tokens=True,
                return_offsets_mapping=True,
                return_tensors=""pt""  # Similarly with ""np""
                )

print(out[""offset_mapping""].shape)
>>> torch.Size([512, 2])
```

## Expected behavior
```python3

out = tokenizer(""test text"",
                padding='max_length',
                truncation=True,
                return_overflowing_tokens=True,
                return_offsets_mapping=True,
                return_tensors=""pt""
                )

print(out[""offset_mapping""].shape)
>>> torch.Size([1, 512, 2])
```
",,,,,,1,,,,
181,https://github.com/huggingface/transformers/issues/6459,6459,[],closed,2020-08-13 10:46:03+00:00,,1,Autotokenizer not returning instance of LongformerTokenizerFast,"## Environment info
     
- `transformers` version: 3.0.2
- Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.6.0+cu101 (False)
- Tensorflow version (GPU?): 2.3.0 (False)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: Google Colab

## Information

Model I am using :  Longformer 
Path: **'allenai/longformer-base-4096'** and **'allenai/longformer-large-4096'**

The problem arises when trying to load 'Fast' version for Longformer using Autotokenizer, the returned tokenizer instance is an object of LongformerTokenizer and not LongformerTokenizerFast. 

![Annotation 2020-08-13 160156](https://user-images.githubusercontent.com/20542313/90124665-68ace300-dd7e-11ea-8cc7-fdd80f070bec.png)

I require the offset mappings for a sub task of extracting word embeddings.

## To reproduce
Just as in the screenshot i am adding the code below to instantiate the tokenizer object:
```
longformer_tokenizer = AutoTokenizer.from_pretrained(
    pretrained_model_name_or_path = 'allenai/longformer-base-4096', use_fast=True)
print(longformer_tokenizer.is_fast)
print(longformer_tokenizer)
```
And since its not an instance of  transformers.LongformerTokenizerFast, I cannot `return_offsets_mapping=True`
As in the below code throws `NotImplementedError`

```
longformer_encoded_dict = longformer_tokenizer.encode_plus(text=sequence_3,
                                                    add_special_tokens = True,
                                                    max_length = 75,
                                                    truncation = True,
                                                    pad_to_max_length = False,
                                                    return_token_type_ids = False,
                                                    return_attention_mask = True,
                                                    return_overflowing_tokens = False,
                                                    return_special_tokens_mask = False,
                                                    return_offsets_mapping=True)
```
**Error**
`
NotImplementedError: return_offsets_mapping is not available when using Python tokenizers.To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast.`

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->


@mfuntowicz
@patrickvonplaten
",,,,,,1,,,,
220,https://github.com/huggingface/transformers/issues/6545,6545,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-17 20:14:57+00:00,,6,QA pipeline fails when using fast tokenizer,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Linux-5.3.0-1032-gcp-x86_64-with-debian-buster-sid
- Python version: 3.7.7
- PyTorch version (GPU?): 1.6.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 examples/bert-loses-patience: @JetRunner
 tensorflow: @jplu 
 documentation: @sgugger
 -->
@LysandreJik @mfuntowicz 

## Information

Model I am using (Bert, XLNet ...): ""sshleifer/tiny-distilbert-base-cased-distilled-squad""

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [x] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [ ] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. Modify unit test https://github.com/huggingface/transformers/blob/master/tests/test_pipelines.py#L644-L647 to use fast tokenizer
`nlp = pipeline(task=""question-answering"", model=model_name, tokenizer=(model_name, {""use_fast"": True}))`
2. Run modified unit test

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->
The test fails with error `ValueError: TextInputSequence must be str`

Complete failure result:
```
Testing started at 16:06 ...
Launching pytest with arguments test_pipelines.py::QAPipelineTests::test_torch_question_answering in /home/lttazz99/transformers_perf_tests/transformers/tests

============================= test session starts ==============================
platform linux -- Python 3.7.7, pytest-6.0.1, py-1.9.0, pluggy-0.13.1 -- /home/lttazz99/miniconda3/envs/qna/bin/python
cachedir: .pytest_cache
rootdir: /home/lttazz99/transformers_perf_tests/transformers
collected 1 item                                                               

test_pipelines.py::QAPipelineTests::test_torch_question_answering FAILED [100%]HuggingFace None
was None
founded None
in None
Paris. None

tests/test_pipelines.py:642 (QAPipelineTests.test_torch_question_answering)
self = <tests.test_pipelines.QAPipelineTests testMethod=test_torch_question_answering>

    @require_torch
    def test_torch_question_answering(self):
        for model_name in QA_FINETUNED_MODELS:
            nlp = pipeline(task=""question-answering"", model=model_name, tokenizer=(model_name, {""use_fast"": True}))
>           self._test_qa_pipeline(nlp)

test_pipelines.py:647: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_pipelines.py:626: in _test_qa_pipeline
    mono_result = nlp(valid_inputs[0])
../src/transformers/pipelines.py:1675: in __call__
    tqdm_enabled=False,
../src/transformers/data/processors/squad.py:369: in squad_convert_examples_to_features
    is_training=is_training))
../src/transformers/data/processors/squad.py:165: in squad_convert_example_to_features
    return_token_type_ids=True,
../src/transformers/tokenization_utils_base.py:2043: in encode_plus
    **kwargs,
../src/transformers/tokenization_utils_fast.py:458: in _encode_plus
    **kwargs,
../src/transformers/tokenization_utils_fast.py:369: in _batch_encode_plus
    is_pretokenized=is_pretokenized,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Tokenizer(vocabulary_size=28996, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PA...sk_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False, wordpieces_prefix=##)
sequence = [2777, 1108, 20164, 10932, 2271, 7954, ...]
pair = ['Hu', '##gging', '##F', '##ace', 'was', 'founded', ...]
is_pretokenized = False, add_special_tokens = True

    def encode(
        self,
        sequence: InputSequence,
        pair: Optional[InputSequence] = None,
        is_pretokenized: bool = False,
        add_special_tokens: bool = True,
    ) -> Encoding:
        """""" Encode the given sequence and pair. This method can process raw text sequences as well
        as already pre-tokenized sequences.
    
        Args:
            sequence: InputSequence:
                The sequence we want to encode. This sequence can be either raw text or
                pre-tokenized, according to the `is_pretokenized` argument:
    
                - If `is_pretokenized=False`: `InputSequence` is expected to be `str`
                - If `is_pretokenized=True`: `InputSequence` is expected to be
                    `Union[List[str], Tuple[str]]`
    
            is_pretokenized: bool:
                Whether the input is already pre-tokenized.
    
            add_special_tokens: bool:
                Whether to add the special tokens while encoding.
    
        Returns:
            An Encoding
        """"""
        if sequence is None:
            raise ValueError(""encode: `sequence` can't be `None`"")
    
>       return self._tokenizer.encode(sequence, pair, is_pretokenized, add_special_tokens)
E       ValueError: TextInputSequence must be str

../../../miniconda3/envs/qna/lib/python3.7/site-packages/tokenizers/implementations/base_tokenizer.py:212: ValueError

Assertion failed

Assertion failed


=================================== FAILURES ===================================
________________ QAPipelineTests.test_torch_question_answering _________________

self = <tests.test_pipelines.QAPipelineTests testMethod=test_torch_question_answering>

    @require_torch
    def test_torch_question_answering(self):
        for model_name in QA_FINETUNED_MODELS:
            nlp = pipeline(task=""question-answering"", model=model_name, tokenizer=(model_name, {""use_fast"": True}))
>           self._test_qa_pipeline(nlp)

test_pipelines.py:647: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_pipelines.py:626: in _test_qa_pipeline
    mono_result = nlp(valid_inputs[0])
../src/transformers/pipelines.py:1675: in __call__
    tqdm_enabled=False,
../src/transformers/data/processors/squad.py:369: in squad_convert_examples_to_features
    is_training=is_training))
../src/transformers/data/processors/squad.py:165: in squad_convert_example_to_features
    return_token_type_ids=True,
../src/transformers/tokenization_utils_base.py:2043: in encode_plus
    **kwargs,
../src/transformers/tokenization_utils_fast.py:458: in _encode_plus
    **kwargs,
../src/transformers/tokenization_utils_fast.py:369: in _batch_encode_plus
    is_pretokenized=is_pretokenized,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Tokenizer(vocabulary_size=28996, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PA...sk_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False, wordpieces_prefix=##)
sequence = [2777, 1108, 20164, 10932, 2271, 7954, ...]
pair = ['Hu', '##gging', '##F', '##ace', 'was', 'founded', ...]
is_pretokenized = False, add_special_tokens = True

    def encode(
        self,
        sequence: InputSequence,
        pair: Optional[InputSequence] = None,
        is_pretokenized: bool = False,
        add_special_tokens: bool = True,
    ) -> Encoding:
        """""" Encode the given sequence and pair. This method can process raw text sequences as well
        as already pre-tokenized sequences.
    
        Args:
            sequence: InputSequence:
                The sequence we want to encode. This sequence can be either raw text or
                pre-tokenized, according to the `is_pretokenized` argument:
    
                - If `is_pretokenized=False`: `InputSequence` is expected to be `str`
                - If `is_pretokenized=True`: `InputSequence` is expected to be
                    `Union[List[str], Tuple[str]]`
    
            is_pretokenized: bool:
                Whether the input is already pre-tokenized.
    
            add_special_tokens: bool:
                Whether to add the special tokens while encoding.
    
        Returns:
            An Encoding
        """"""
        if sequence is None:
            raise ValueError(""encode: `sequence` can't be `None`"")
    
>       return self._tokenizer.encode(sequence, pair, is_pretokenized, add_special_tokens)
E       ValueError: TextInputSequence must be str

../../../miniconda3/envs/qna/lib/python3.7/site-packages/tokenizers/implementations/base_tokenizer.py:212: ValueError
----------------------------- Captured stdout call -----------------------------
HuggingFace None
was None
founded None
in None
Paris. None
=============================== warnings summary ===============================
tests/test_pipelines.py::QAPipelineTests::test_torch_question_answering
  /home/lttazz99/transformers_perf_tests/transformers/src/transformers/tokenization_utils_base.py:1319: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
    FutureWarning,

-- Docs: https://docs.pytest.org/en/stable/warnings.html
=========================== short test summary info ============================
FAILED test_pipelines.py::QAPipelineTests::test_torch_question_answering - Va...
========================= 1 failed, 1 warning in 2.64s =========================

Process finished with exit code 1

Assertion failed

Assertion failed
```


## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
Test should pass.
---
NOTES:
This errors appears to be coming because [here](https://github.com/huggingface/transformers/blob/98ee802023a4db76879c761e7ce3677eb4555871/src/transformers/tokenization_utils_fast.py#L366) the inputs are the query tokens' ids and the context token text.
This in turn is passed to the [fast tokenizer](https://github.com/huggingface/tokenizers/blob/5d8728a26b654784572612ae119edc451205c2ff/bindings/python/py_src/tokenizers/implementations/base_tokenizer.py#L211) which expects `str` for both the sequence and the pair but instead gets a sequence of ints for the query and the text for the context.
The pipeline fails with when any Bert-like fast tokenizer is used irrespective of the model.
The unit test is the easiest way to reproduce this error.",,,,,,1,,,,
722,https://github.com/huggingface/transformers/issues/7604,7604,[],closed,2020-10-06 07:37:22+00:00,,1,way to make inference Zero Shot pipeline faster?,"Hi 
Can you guys give me tips how to make Zero Shot pipeline inference faster?

My current approach right now is reducing the model size/parameter 
(trying to train ""base model"" instead of ""large model)


Is there another approach?

CCing @joeddav",,,,,,1,,,,
796,https://github.com/huggingface/transformers/issues/7735,7735,[],closed,2020-10-12 14:25:46+00:00,,4,Tokenizer Fast bug: ValueError: TextInputSequence must be str,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version:
- Platform: In a Colab enviroment aswell as on my local windows version
- Python version: 3.7.4
- PyTorch version (GPU?): Yes and No
- Tensorflow version (GPU?): I didn't try with tensorflow, but I suspect that it has nothing to do with it
- Using GPU in script?: I used the automodeling on a GPU session in Colab
- Using distributed or parallel set-up in script?: Nope

### Who can help
@mfuntowicz 

## Information
Model I am using: Initially Electra but I tested it out with BERT, DistilBERT and RoBERTa

It's using your scripts, but again, it believe it wouldn't work if I did it myself. The model is trained on SQuAD.

#### Error traceback
```
""""""
Traceback (most recent call last):
  File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 119, in worker
    result = (True, func(*args, **kwds))
  File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 44, in mapstar
    return list(map(*args))
  File ""/usr/local/lib/python3.6/dist-packages/transformers/data/processors/squad.py"", line 165, in squad_convert_example_to_features
    return_token_type_ids=True,
  File ""/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py"", line 2050, in encode_plus
    **kwargs,
  File ""/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_fast.py"", line 473, in _encode_plus
    **kwargs,
  File ""/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_fast.py"", line 376, in _batch_encode_plus
    is_pretokenized=is_split_into_words,
  File ""/usr/local/lib/python3.6/dist-packages/tokenizers/implementations/base_tokenizer.py"", line 212, in encode
    return self._tokenizer.encode(sequence, pair, is_pretokenized, add_special_tokens)
ValueError: TextInputSequence must be str
""""""
```

## To reproduce
Steps to reproduce the behavior:

1. Download model and tokenizer (fast)
2. Test it out with the transformers pipeline for a question answering task

I've also made a small notebook to test it out for yourself.  [here](https://colab.research.google.com/drive/11_qK3w7OWBTYC_GdspAjFna2XJkBgODU?usp=sharing)

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior
Instead of giving an error, I would expect the tokenizer to work...
<!-- A clear and concise description of what you would expect to happen. -->",,,,,,1,,,,
1275,https://github.com/huggingface/transformers/issues/8720,8720,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}, {'id': 1897896961, 'node_id': 'MDU6TGFiZWwxODk3ODk2OTYx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Migration', 'name': 'Migration', 'color': 'e99695', 'default': False, 'description': ''}]",closed,2020-11-22 22:41:01+00:00,,2,Broken links in example for torch.load() after. converting tensorflow checkpoint to pytorch save model,"The links for  run_bert_extract_features.py, run_bert_classifier.py, and run_bert_squad.py are all broken [here](https://huggingface.co/transformers/v2.4.0/converting_tensorflow_models.html). Could someone point me to a notebook where I can find examples for loading from a PyTorch save file pytorch_model.bin?
",,,1,,1,,,,,
1352,https://github.com/huggingface/transformers/issues/8874,8874,[],closed,2020-12-01 15:22:40+00:00,,3,Results are different when fine-tuning continues after loading model from checkpoint ,"## Environment info
     
- `transformers` version: 4.0.0
- Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.7.0+cu101 (True)
- Tensorflow version (GPU?): 2.3.0 (True)
- Using GPU in script?: yes (device: cuda:0, n_gpu: 1)
- Using distributed or parallel set-up in script?: False

### Who can help

@sgugger
@stefan-it
## Information

Model I am using (Bert, XLNet ...): bert-base-cased

The problem arises when using:
* [x] the official example scripts: run_ner_old.py


The tasks I am working on is:
* [x] my own task or dataset:  token classification for a rhetoric device

## To reproduce

Steps to reproduce the behavior:

1. Run run_ner_old script and save model after one epoch (282 steps):

```
python3  ./run_ner_old.py \
--data_dir ./data/ \
--labels ./data/labels.txt \
--model_name_or_path bert-base-cased \
--output_dir ./output/ \
--max_seq_length  128 \
--num_train_epochs 2 \
--per_device_train_batch_size 16 \
--save_steps 282 \
--seed 1 \
--do_train \
--do_eval 
```
2. Run ner_old_script from checkpoint-282:
```
python3  ./run_ner_old.py \
--data_dir ./data/ \
--labels ./data/labels.txt \
--model_name_or_path ./output/checkpoint-282 \
--tokenizer bert-base-cased \
--output_dir ./output2/ \
--max_seq_length  128 \
--num_train_epochs 2 \
--per_device_train_batch_size 16 \
--save_steps 282 \
--seed 1 \
--do_train \
--do_eval 
```
3. Compare evaluation results

**First experiment:** 
Run the script `run_ner_old.py` as showed above to fine-tune BERT.
I saved the model after the first epoch (282 steps).

**Second experiment:**
Run the script `run_ner_old.py` as showed above to fine-tune BERT, starting from checkpoint-282 from the first experiment:
```
[INFO|trainer.py:662] 2020-12-01 14:35:09,848 >> ***** Running training *****
[INFO|trainer.py:663] 2020-12-01 14:35:09,848 >>   Num examples = 4501
[INFO|trainer.py:664] 2020-12-01 14:35:09,848 >>   Num Epochs = 2
[INFO|trainer.py:665] 2020-12-01 14:35:09,849 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:666] 2020-12-01 14:35:09,849 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:667] 2020-12-01 14:35:09,849 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:668] 2020-12-01 14:35:09,849 >>   Total optimization steps = 564
[INFO|trainer.py:681] 2020-12-01 14:35:09,851 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:682] 2020-12-01 14:35:09,851 >>   Continuing training from epoch 1
[INFO|trainer.py:683] 2020-12-01 14:35:09,851 >>   Continuing training from global step 282
[INFO|trainer.py:684] 2020-12-01 14:35:09,851 >>   Will skip the first 0 batches in the first epoch
```
This seems right as the training continues from step 282 and it trains one complete epoch (""skip the first 0 batches""). 

 But when I **compare the results**, they are slightly different:
1. experiment: eval_f1 = 0.9226747985188413
2. experiment: eval_f1 = 0.9211328976034858

Also the loss after 500 steps is already different:
1. experiment:
`{'loss': 0.09096851348876953, 'learning_rate': 5.673758865248227e-06, 'epoch': 1.773049645390071}
`
2. experiment:
`
{'loss': 0.010856814384460449, 'learning_rate': 5.673758865248227e-06, 'epoch': 1.773049645390071}
`

## Expected behavior

I would have expected that both trained models should produce the same results since the second experiment does exactly the same but in two steps. (The model is saved and loaded between the two epochs).


The *checkpoint-282* directory consists of the following files:
```
config.json
optimizer.pt
pytorch_model.bin
scheduler.pt
trainer_state.json
training_args.bin
vocab.txt
```
It does not seem that there is any random initialization since I added the seed and the results do not change when running again.

Did I forget to save or load anything? 

Cheers",,,1,,1,,,,,
2159,https://github.com/huggingface/transformers/issues/10364,10364,[],closed,2021-02-24 02:54:51+00:00,,5,Loading mBART Large 50 MMT (many-to-many) is slow,"## Environment info

I'm installing the library directly from `master` and running it in a kaggle notebook.

<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.4.0.dev0
- Platform: Linux-5.4.89+-x86_64-with-debian-buster-sid
- Python version: 3.7.9
- PyTorch version (GPU?): 1.7.0 (False)
- Tensorflow version (GPU?): 2.4.1 (False)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- bart: @patrickvonplaten, @patil-suraj

Library:

- text generation: @patrickvonplaten


HF projects:

- nlp datasets: [different repo](https://github.com/huggingface/nlp)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

## Information

Model I am using (Bert, XLNet ...): mBART-Large 50 MMT (many-to-many)

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [x] my own modified scripts: (give details below)

After caching the weights of the model, load it with `from_pretrained` is significantly slower compared with `torch.load`.

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [x] my own task or dataset: (give details below)

Machine Translation

## To reproduce

Here's the [kaggle notebook](https://www.kaggle.com/xhlulu/reproducing-speed-issues-with-mbart-large-50) reproducing the issue. Here's a [colab notebook](https://colab.research.google.com/drive/1fKuLG_U6uw4x8LqcIQFEFjQYjnc1nBzQ?usp=sharing) showing essentially the same thing.

Steps to reproduce the behavior:

1. Load model with `model = MBartForConditionalGeneration.load_pretrained(""facebook/mbart-large-50-many-to-many-mmt"")`
2. Save model with `model.save_pretrained('./my-model')`
3. Save model with `torch.save(model, 'model.pt')`
4. Reload and time with `MBartForConditionalGeneration.load_pretrained('./my-model')`
5. Load with `torch.load('model.pt')`

The step above can be reproduced inside a kaggle notebook:
```python
model = MBartForConditionalGeneration.load_pretrained(""facebook/mbart-large-50-many-to-many-mmt"")
model.save_pretrained('./my-model/')
torch.save(model, 'model.pt')
%time model = MBartForConditionalGeneration.from_pretrained(""./my-model/"")
%time torch_model = torch.load('model.pt')
```

We will notice that loading with `from_pretrained` (step 4) is significantly slower than `torch.load` (step 5); the former takes over 1 minute and the latter just a few seconds (or around 20s if it hasn't been previously loaded in memory; see [notebook](https://www.kaggle.com/xhlulu/use-saved-torch-model)).

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->

The model should take less than 1 minute to load if it has already been cached (see step 1)
",,,1,,1,,,,,
3208,https://github.com/huggingface/transformers/issues/12245,12245,[],closed,2021-06-18 06:50:51+00:00,,6,"TFBertForMaskedLM won't reload from saved checkpoint, shape mismatch issue","## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.5.1-4.7
- Platform: Debian GNU/Linux 10 (buster)
- Python version: 3.9.2
- PyTorch version (GPU?): N/A
- Tensorflow version (GPU?): 2.5.0
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help
@Rocketknight1, @LysandreJik, @sgugger

## Information

Model I am using: TFBertForMaskedLM

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [X] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [ X] my own task or dataset: (give details below)

I believe this issue also affects the official TFTrainer implementation
as the checkpoint restore snippet was adapted from it.

## To reproduce

Steps to reproduce the behavior:

1. Generate Masked Batch
2. initialize TF Model and assign CheckpointManager
3. Save model checkpoint
4. initialize new TF Model and assign CheckpointManager
5. restore from checkpoint


```
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForMaskedLM, AutoConfig, TFAutoModelForCausalLM
import tensorflow as tf

random_sentences = [""You'll see the rainbow bridge after it rains cats and dogs."",
""They looked up at the sky and saw a million stars."",
""The bullet pierced the window shattering it before missing Danny's head by mere millimeters."",
""He was willing to find the depths of the rabbit hole in order to be with her.""]

tok = AutoTokenizer.from_pretrained('bert-base-uncased')
input_ids = tok.batch_encode_plus(random_sentences,return_tensors='np',padding=True)['input_ids']

#Create masked tokens as labels
labels = np.ones_like(input_ids)*-100
mask = (np.random.uniform(size=input_ids.shape)<=0.2) & (input_ids != 0)
labels[mask]=tok.mask_token_id

batch= {'input_ids':tf.convert_to_tensor(input_ids),
        'labels':tf.convert_to_tensor(labels)}

""""""## Run model and save checkpoint""""""

model = TFAutoModelForMaskedLM.from_pretrained('bert-base-uncased')
checkpoint = tf.train.Checkpoint(model=model)
model.ckpt_manager = tf.train.CheckpointManager(checkpoint, './', max_to_keep=1)
out = model(**batch)
print(out.loss.numpy())
model.ckpt_manager.save()

""""""## Re-Initialize from config alone an load existing checkpoint""""""

cfg = AutoConfig.from_pretrained('bert-base-uncased')
model2 = TFAutoModelForMaskedLM.from_config(cfg)
checkpoint2 = tf.train.Checkpoint(model=model2)
model2.ckpt_manager = tf.train.CheckpointManager(checkpoint2, './', max_to_keep=1)
latest_ckpt = tf.train.latest_checkpoint('./')
status = checkpoint2.restore(latest_ckpt)
status.assert_existing_objects_matched()

out = model2(**batch)
print(out.loss.numpy())
```

## Expected behavior

Expect to fully restore from checkpoint

## Current Behavior, error output

```---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-12-5ec2de12ee44> in <module>()
----> 1 out = model2(**batch)
      2 out.loss

19 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in set_shape(self, shape)
   1238       raise ValueError(
   1239           ""Tensor's shape %s is not compatible with supplied shape %s"" %
-> 1240           (self.shape, shape))
   1241 
   1242   # Methods not supported / implemented for Eager Tensors.

ValueError: Tensor's shape (512, 768) is not compatible with supplied shape [2, 768]
```

## Link to colab
https://colab.research.google.com/drive/12pwo4WSueOT523hh1INw5J_SLpkK0IgB?usp=sharing

",,,1,,1,,,,,
5,https://github.com/huggingface/transformers/issues/6084,6084,[],closed,2020-07-28 06:17:54+00:00,,2,ValueError raises when load Flaubert from pre-train with Transformers >=3.0.0,"# 馃悰 Bug

## Information

Model I am using (Bert, XLNet ...): **Flaubert**

Language I am using the model on (English, Chinese ...): **French**

The problem arises when using: my own modified scripts

## To reproduce

**Just load Flaubert model with from_pretrained:**
```
MODEL = ""flaubert/flaubert_large_cased""
transformer_layer = TFAutoModel.from_pretrained(MODEL, from_pt=True)
```

**Then a ValueError is raised:**

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-22-4235ae313fbd> in <module>()
      1     MODEL = ""flaubert/flaubert_large_cased""
----> 2     transformer_layer = TFAutoModel.from_pretrained(MODEL, from_pt=True)

/usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    421         for config_class, model_class in TF_MODEL_MAPPING.items():
    422             if isinstance(config, config_class):
--> 423                 return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
    424         raise ValueError(
    425             ""Unrecognized configuration class {} for this kind of TFAutoModel: {}.\n""

/usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    479         if from_pt:
    480             # Load from a PyTorch checkpoint
--> 481             return load_pytorch_checkpoint_in_tf2_model(model, resolved_archive_file, allow_missing_keys=True)
    482 
    483         model(model.dummy_inputs, training=False)  # build the network with dummy inputs

/usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_pytorch_utils.py in load_pytorch_checkpoint_in_tf2_model(tf_model, pytorch_checkpoint_path, tf_inputs, allow_missing_keys)
     91 
     92     return load_pytorch_weights_in_tf2_model(
---> 93         tf_model, pt_state_dict, tf_inputs=tf_inputs, allow_missing_keys=allow_missing_keys
     94     )
     95 

/usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_pytorch_utils.py in load_pytorch_weights_in_tf2_model(tf_model, pt_state_dict, tf_inputs, allow_missing_keys)
    123 
    124     if tf_inputs is not None:
--> 125         tf_model(tf_inputs, training=False)  # Make sure model is built
    126 
    127     # Adapt state dict - TODO remove this and update the AWS weights files instead

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    966           with base_layer_utils.autocast_context_manager(
    967               self._compute_dtype):
--> 968             outputs = self.call(cast_inputs, *args, **kwargs)
    969           self._handle_activity_regularization(inputs, outputs)
    970           self._set_mask_metadata(inputs, outputs, input_masks)

/usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_xlm.py in call(self, inputs, **kwargs)
    635             heads.
    636         """"""
--> 637         outputs = self.transformer(inputs, **kwargs)
    638         return outputs
    639 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    966           with base_layer_utils.autocast_context_manager(
    967               self._compute_dtype):
--> 968             outputs = self.call(cast_inputs, *args, **kwargs)
    969           self._handle_activity_regularization(inputs, outputs)
    970           self._set_mask_metadata(inputs, outputs, input_masks)

/usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_flaubert.py in call(self, inputs, attention_mask, langs, token_type_ids, position_ids, lengths, cache, head_mask, inputs_embeds, training, output_attentions, output_hidden_states)
    267                 tensor_normalized = self.layer_norm1[i](tensor)
    268                 attn_outputs = self.attentions[i](
--> 269                     [tensor_normalized, attn_mask, None, cache, head_mask[i]], training=training
    270                 )
    271                 attn = attn_outputs[0]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    966           with base_layer_utils.autocast_context_manager(
    967               self._compute_dtype):
--> 968             outputs = self.call(cast_inputs, *args, **kwargs)
    969           self._handle_activity_regularization(inputs, outputs)
    970           self._set_mask_metadata(inputs, outputs, input_masks)

/usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_xlm.py in call(self, inputs, training)
    139         Self-attention (if kv is None) or attention over source sentence (provided by kv).
    140         """"""
--> 141         input, mask, kv, cache, head_mask, output_attentions = inputs
    142         # Input is (bs, qlen, dim)
    143         # Mask is (bs, klen) (non-causal) or (bs, klen, klen)

ValueError: not enough values to unpack (expected 6, got 5)
```
## Expected behavior
Load the model successfully. 

- It can be achieved with Transformers version <= 2.11.0

- I also try CamemBERT, and Bert-like models in other languages like tr, ru, es, pt, it. None of them have this problem.

## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: **>=3.0.0 ( 3.0.0, 3.0.1, 3.0.2 all have this problem.)**

(I try this on google colab with GPU and TPU. The environment does not matter here. But I still provide the one with GPU.)
- Platform: Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.5.1+cu101 (True)
- Tensorflow version (GPU?): 2.2.0 (True)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No
",1,,,,1,,,,,
76,https://github.com/huggingface/transformers/issues/6222,6222,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-03 19:22:49+00:00,,4,memory benchmarking: should the cudnn kernels loading be included,"Bringing this discussion from slack to a dedicated issue so it's easier to track and discuss:

Here is the gist of the discussion so far:

I said:

Currently `benchmark._measure_memory()` doesn't take into account cudnn kernel loading, so even if you measure the most trivial function on pytorch you end up with ~0.6 (Titan X) to 1GB (Tesla T4) reported.

Usually, I deal with this issue by first doing: `torch.ones((1, 1)).cuda()` and then doing any measuring.

For example here is a rough prototype on a new test I'm working to do regressions on mem/speed for basic functionality:
https://colab.research.google.com/drive/1n6J3tc8FT4ER1vBCTAtU4__U5Px2mxwI?usp=sharing
All it does is measuring memory consumption and speed for init, fwd and fwd+bwd - hope it's easy to read.

As you can see I had to first measure a baseline with the cheat I mentioned above, and then subtract it from all the subsequent memory measurements.

----

@patrickvonplaten suggested that it is that way so that:

a) The number includes all the required memory to run a model.
b) better comparison with tensorflow

---

fwd+init is fwd+init+cudnn load. In order to measure fwd+init, you need to load cudnn kernels first. You can do multiple concurrent inits and cudnn overheard will be happening only once.

I think the API should provide for flexible approach, by allowing:

1. show me the bottom line for any operation - i.e. how much memory was consumed when `func` is run
2. show me the delta, subtracting the cudnn overhead which happens once

Obviously, the caller can ask the benchmarking function to do all those measurements, including the manual measurement of cudnn overhead, and then do the accounting herself. But perhaps it could have an optional argument that would perform something like torch.ones((1, 1)).cuda() and substract that.

Well, perhaps the simplest approach is to just have a way to query how much memory cudnn loading takes and then leave the rest of it as it is, so the caller will do all the accounting. And perhaps if such accounting is repetitive than it can be abstracted into another layer.

It might be easier to show in code, so a new shortcut is added to measure the loading of cudnn:
```
mem, _ = benchmark._measure_memory_cudnn_load()
mem_load = mem.bytes - mem_load
mem, _ = benchmark._measure_memory(func)
mem_diff = mem.bytes - mem_load
```
Now this can be wrapped into:
```
mem, _ = benchmark._measure_memory_delta(func)
mem_diff = mem.bytes - mem_load
```
does this make sense?

---

So this is where we are at, thoughts?",1,,,,1,,,,,
78,https://github.com/huggingface/transformers/issues/6226,6226,[],closed,2020-08-03 20:40:43+00:00,,12,Can't load config for [community model],"Although I can use a fine-tuned GPT2 model from code, the model page complains about the config file (which is already uploaded). 
at https://huggingface.co/akhooli/gpt2-small-arabic-poetry (for a prompt), I get: 
```
Can't load config for 'akhooli/gpt2-small-arabic-poetry'. Make sure that: - 'akhooli/gpt2-small-arabic-poetry' is a correct model identifier listed on 'https://huggingface.co/models' - or 'akhooli/gpt2-small-arabic-poetry' is the correct path to a directory containing a config.json file 
```",1,,,,1,,,,,
140,https://github.com/huggingface/transformers/issues/6368,6368,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-09 17:05:20+00:00,,5,Can't load a saved tokenizer with AutoTokenizer.from_pretrained without saving Config as well,"### Environment info
- `transformers` version: master (https://github.com/huggingface/transformers/commit/6e8a38568eb874f31eb49c42285c3a634fca12e7)

### Who can help
 tokenizers: @mfuntowicz

### Information
When saving a tokenizer with .save_pretrained, it can be loaded with the class it was saved with but not with AutoTokenizer:
```
from transformers import BertTokenizer, AutoTokenizer
BertTokenizer.from_pretrained(""bert-base-cased"").save_pretrained(""."")

BertTokenizer.from_pretrained(""."") # works

AutoTokenizer.from_pretrained(""."") # throws exception
```
The error is:
```
Traceback (most recent call last):
  File ""/home/transformers/src/transformers/configuration_utils.py"", line 333, in get_config_dict
    local_files_only=local_files_only,
  File ""/home/transformers/src/transformers/file_utils.py"", line 684, in cached_path
    raise EnvironmentError(""file {} not found"".format(url_or_filename))
OSError: file ./config.json not found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/transformers/src/transformers/tokenization_auto.py"", line 205, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File ""/home/transformers/src/transformers/configuration_auto.py"", line 203, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File ""/home/transformers/src/transformers/configuration_utils.py"", line 346, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for '.'. Make sure that:

- '.' is a correct model identifier listed on 'https://huggingface.co/models'

- or '.' is the correct path to a directory containing a config.json file
```

If a configuration is saved as well, then loading with AutoTokenizer does work:
```
from transformers import BertTokenizer, BertConfig, AutoTokenizer
BertConfig.from_pretrained(""bert-base-cased"").save_pretrained(""."")
BertTokenizer.from_pretrained(""bert-base-cased"").save_pretrained(""."")

AutoTokenizer.from_pretrained(""."") # works
```

### Expected behavior
I'd expect that loading a tokenizer with AutoTokenizer would require the same files as a dedicated tokenizer class (e.g. BertTokenizer) requires.
",1,,,,1,,,,,
149,https://github.com/huggingface/transformers/issues/6394,6394,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-10 16:17:12+00:00,,4,Error while loading albert for token classification,"## Environment info
- `transformers` version: 3.0.2
- Platform: Windows-10-10.0.18362-SP0
- Python version: 3.6.10
- PyTorch version (GPU?): not installed (NA)
- Tensorflow version (GPU?): 2.2.0 (False)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help
@LysandreJik
@jplu 
 
## Information

Model I am using albert-base-v2 or albert-base-v1:

The tasks I am working on is:
token classification using albert-base-v2 or v1

## To reproduce
```

>>> from transformers import AlbertTokenizer, TFAlbertForTokenClassification
>>> import tensorflow as tf

>>> tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', cache_dir = 'cache')
>>> model = TFAlbertForTokenClassification.from_pretrained('albert-base-v2', cache_dir = 'cache')
```

When I run above script I get error:

```

Traceback (most recent call last):
  File ""C:\Users\703235761\AppData\Local\Continuum\anaconda3\envs\slot\lib\site-packages\transformers\modeling_tf_utils.py"", line 581, in from_pretrained
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""model_utils.py"", line 7, in <module>
    model = TFAlbertForTokenClassification.from_pretrained('albert-base-v1', cache_dir = 'cache')
  File ""C:\Users\703235761\AppData\Local\Continuum\anaconda3\envs\slot\lib\site-packages\transformers\modeling_tf_utils.py"", line 588, in from_pretrained
    raise EnvironmentError(msg)
OSError: Can't load weights for 'albert-base-v1'. Make sure that:

- 'albert-base-v1' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'albert-base-v1' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.
```

## Expected behavior

I think this model was supposed to work with TFAlbertModel as well.
Thanks in advance! :-) 
",1,,,,1,,,,,
212,https://github.com/huggingface/transformers/issues/6517,6517,[],closed,2020-08-16 12:14:23+00:00,,3,Can't load t5-11b from pre-trained,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform:
- Python version: 3.8.2
- PyTorch version 1.6


### Who can help

 T5: @patrickvonplaten

## Information

The model I am using: T5


## To reproduce

Steps to reproduce the behavior:

```
import transformers
transformers.T5ForConditionalGeneration.from_pretrained(""t5-11b"")
```

```


OSError: Can't load weights for 't5-11b'. Make sure that:

- 't5-11b' is a correct model identifier listed on 'https://huggingface.co/models'

- or 't5-11b' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.

```


<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior
the model should be loaded.
",1,,,,1,,,,,
213,https://github.com/huggingface/transformers/issues/6520,6520,[],closed,2020-08-16 15:27:50+00:00,,10,Can't load pegasus models.,"Hi, 

I've tried uploading Peagsus from PreTrainedModel and PreTrainedTokenizer but run into KeyError, I have transformers 3.0.2 - any idea why that might be happening?",1,,,,1,,,,,
217,https://github.com/huggingface/transformers/issues/6539,6539,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-17 15:49:17+00:00,,2,Widget can't load model,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.0.2
- Platform: Linux-5.4.0-42-generic-x86_64-with-glibc2.27
- Python version: 3.8.2
- PyTorch version (GPU?): 1.6.0 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.
 
 albert, bert, GPT2, XLM: @LysandreJik 
 tokenizers: @mfuntowicz
 Trainer: @sgugger
 Speed and Memory Benchmarks: @patrickvonplaten
 Model Cards: @julien-c
 Translation: @sshleifer
 Summarization: @sshleifer
 TextGeneration: @TevenLeScao 
 examples/distillation: @VictorSanh
 nlp datasets: [different repo](https://github.com/huggingface/nlp)
 rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)
 Text Generation: @TevenLeScao
 blenderbot: @mariamabarham
 Bart: @sshleifer
 Marian: @sshleifer
 T5: @patrickvonplaten
 Longformer/Reformer: @patrickvonplaten
 TransfoXL/XLNet: @TevenLeScao 
 examples/seq2seq: @sshleifer
 examples/bert-loses-patience: @JetRunner
 tensorflow: @jplu 
 documentation: @sgugger
 -->
@julien-c @mfuntowicz  (not sure I聽tagged correctly because related to inference widget, not listed category)

## Information

Model I am using (Bert, XLNet ...): huggingtweets/borisdayma (GPT-2)

The problem arises when using:
* [X] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [X] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:

1. Try to do inference from [huggingtweets/borisdayma model page](https://huggingface.co/huggingtweets/borisdayma)

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

Error displayed in interface:

> 鈿狅笍 Can't load weights for 'huggingtweets/borisdayma'. Make sure that: - 'huggingtweets/borisdayma' is a correct model identifier listed on 'https://huggingface.co/models' - or 'huggingtweets/borisdayma' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.

![image](https://user-images.githubusercontent.com/715491/90415619-f9cdd380-e076-11ea-879b-7dde7e1601cf.png)

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->

This works locally:

```python
generator = pipeline('text-generation', model='huggingtweets/borisdayma')
generator(""<|endoftext|>I really like"")
```",1,,,,1,,,,,
271,https://github.com/huggingface/transformers/issues/6646,6646,[],closed,2020-08-21 16:03:02+00:00,,2,Error when loading my trained model,"Hello, 

I tried to train the question-answering model using `bert-base-uncased` on SQUADv1.1. The training process seems to be successfully completed. However, when I load the trained model, it said that `File ""h5py/h5f.pyx"", line 88, in h5py.h5f.open
OSError: Unable to open file (file signature not found)`

Here is my configuration for the training process: 
```
model_name_or_path: bert-base-uncased
do_train: True
do_eval: True
overwrite_output_dir: True
num_train_epochs: 10
per_device_train_batch_size: 12
per_device_eval_batch_size: 12
warmup_steps: 100
weight_decay: 0.01
learning_rate: 3e-5
evaluate_during_training: True
save_steps: 5000
```

And here is what I stored in my model directory: 
```
checkpoint-10000  checkpoint-35000  checkpoint-55000  pytorch_model.bin
checkpoint-15000  checkpoint-40000  checkpoint-60000  special_tokens_map.json
checkpoint-20000  checkpoint-45000  checkpoint-65000  tokenizer_config.json
checkpoint-25000  checkpoint-5000   checkpoint-70000  training_args.bin
checkpoint-30000  checkpoint-50000  config.json vocab.txt
```

I tried to load my model by 
```
self._model = BertForQuestionAnswering.from_pretrained(`./model/trained_squad/`, from_tf=True)
```

It would be appreciated if anyone can give me a clue about what happens here. Is there anything wrong with my training process? 

Best,
Yanchao

",1,,,,1,,,,,
277,https://github.com/huggingface/transformers/issues/6657,6657,[],closed,2020-08-22 00:29:08+00:00,,3,"Error while loading pretrained model with ""return_dict=True""","# 鉂?Questions & Help

torch: 1.6.0+cu101
Transformers: 3.0.2

**Error with ""return_dict=True""** 

```
from transformers import BertTokenizer, BertForPreTraining
import torch
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForPreTraining.from_pretrained('bert-base-uncased', return_dict=True)
```

```
TypeError                                 Traceback (most recent call last)
<ipython-input-3-5eca8cb45c88> in <module>()
      2 import torch
      3 tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
----> 4 model = BertForPreTraining.from_pretrained('bert-base-uncased', return_dict=True)

/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    670 
    671         # Instantiate model.
--> 672         model = cls(config, *model_args, **model_kwargs)
    673 
    674         if state_dict is None and not from_tf:

TypeError: __init__() got an unexpected keyword argument 'return_dict'
```

",1,,,,1,,,,,
553,https://github.com/huggingface/transformers/issues/7258,7258,[],closed,2020-09-19 22:57:05+00:00,,4,"[save/load model] authorized keys, no save keys, etc.","We already have:

```
   # modeling_bart:
    authorized_missing_keys = [r""final_logits_bias"", r""encoder\.version"", r""decoder\.version""]
```
Once https://github.com/huggingface/transformers/pull/7224 is merged we will have:

```
class SomeModel(PretrainedFSMTModel):
    base_model_prefix = ""model""
    authorized_missing_keys = [
        ""model.encoder.embed_positions.weight"",
        ""model.decoder.embed_positions.weight"",
    ]
    keys_to_never_save= [
        ""model.encoder.embed_positions.weight"",
        ""model.decoder.embed_positions.weight"",
    ]
```

I'd like to discuss several things:

1. let's pick consistent intuitive names for the group of these class variables - e.g. both ending with `_keys` or starting with `keys_` and use more descriptive mnemonics?

I suggest:

* `keys_to_ignore_on_load`
* `keys_to_ignore_on_save`

---------

2. why is the current implementation for `authorized_missing_keys` uses a regex search?

```
   # modeling_bart:
    authorized_missing_keys = [r""final_logits_bias"", r""encoder\.version"", r""decoder\.version""]
    [...]
   # modeling_utils:
            if cls.authorized_missing_keys is not None:
                for pat in cls.authorized_missing_keys:
                    missing_keys = [k for k in missing_keys if re.search(pat, k) is None]
```
when a simple direct comparison `k in list` would work just fine? i.e.: (untested)
```
if cls.authorized_missing_keys
    missing_keys = [k for k in missing_keys if k is not in cls.authorized_missing_keys]
```

it'd make the listing of the keys easier to write/read.
```
    authorized_missing_keys = [""final_logits_bias"", ""encoder.version"", ""decoder.version""]
```

-------------------

3. I think we may have an issue with `model.` prefix present in some saved model `state_dict`s and lacking in others - or is it a save/load issue where it has the prefix on the way out, but not back in? Note, that `authorized_missing_keys` doesn't have the `model.` prefix. Perhaps that's why the regex was used - to catch w/ and w/o the prefix?

So perhaps the normalization could happen in the core libs (load?) and the models listing special ""needs"" keys should all either have the prefix or not. 

----

Thank you for your input.
",1,,,,1,,,,,
173,https://github.com/huggingface/transformers/issues/6444,6444,"[{'id': 2039044877, 'node_id': 'MDU6TGFiZWwyMDM5MDQ0ODc3', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/marian', 'name': 'marian', 'color': '30cc95', 'default': False, 'description': ''}]",closed,2020-08-12 15:40:22+00:00,,3,Can't download  'Helsinki-NLP/opus-mt-hye-eng' model,"## Environment info
     
- `transformers` version: 3.0.2
- Platform: Linux-5.3.0-51-generic-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.3.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: NO
- Using distributed or parallel set-up in script?: not sure



## Information

Model I am using: MarianMTModel, AutoModelWithLMHead

The problem arises when using the official example scripts (https://huggingface.co/Helsinki-NLP/opus-mt-hye-eng):

```python 
from transformers import AutoTokenizer, AutoModelWithLMHead
tokenizer = AutoTokenizer.from_pretrained(""Helsinki-NLP/opus-mt-hye-eng"")
model = AutoModelWithLMHead.from_pretrained(""Helsinki-NLP/opus-mt-hye-eng"")
```

Gives error 

```
/home/sonja/.local/lib/python3.6/site-packages/transformers/modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.
  FutureWarning,
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
~/.local/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    654                 if resolved_archive_file is None:
--> 655                     raise EnvironmentError
    656             except EnvironmentError:

OSError: 

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
<ipython-input-2-04055899a280> in <module>
      1 from transformers import AutoTokenizer, AutoModelWithLMHead
      2 tokenizer = AutoTokenizer.from_pretrained(""Helsinki-NLP/opus-mt-hye-eng"")
----> 3 model = AutoModelWithLMHead.from_pretrained(""Helsinki-NLP/opus-mt-hye-eng"")

~/.local/lib/python3.6/site-packages/transformers/modeling_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    804         for config_class, model_class in MODEL_WITH_LM_HEAD_MAPPING.items():
    805             if isinstance(config, config_class):
--> 806                 return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
    807         raise ValueError(
    808             ""Unrecognized configuration class {} for this kind of AutoModel: {}.\n""

~/.local/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    660                     f""- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}.\n\n""
    661                 )
--> 662                 raise EnvironmentError(msg)
    663 
    664             if resolved_archive_file == archive_file:

OSError: Can't load weights for 'Helsinki-NLP/opus-mt-hye-eng'. Make sure that:

- 'Helsinki-NLP/opus-mt-hye-eng' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'Helsinki-NLP/opus-mt-hye-eng' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.
```

Tried to download the model manually from link I got while debugging (https://cdn.huggingface.co/Helsinki-NLP/opus-mt-hye-eng/pytorch_model.bin) but it doesn't return anything relatable. Although for 'hye-rus' model (https://cdn.huggingface.co/Helsinki-NLP/opus-mt-hye-rus/pytorch_model.bin) I can easily download the file. Works fine for ""eng-hye"" and ""rus-hye"" too.

Hj盲lp, @sshleifer (sorry if mistagged)",,,,,1,,,,,
721,https://github.com/huggingface/transformers/issues/7602,7602,[],closed,2020-10-06 05:50:57+00:00,,4,RAG : Can we fine-tune RAG with update frequency method similar to Fairseq framework?,RAG fine-tune script needs 8 GPUs to train. Is there any chance that the training can be done with less number of GPUs using the update frequency? ,,,,1,,,,,,
860,https://github.com/huggingface/transformers/issues/7848,7848,[],closed,2020-10-16 08:43:48+00:00,,9,RuntimeError with DistributedDataParallel ,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.3.1
- Platform: Ubuntu 18.04
- Python version: 3.7.7.final.0
- PyTorch version (GPU?): 1.6.0 gpu
- Tensorflow version (GPU?): X
- Using GPU in script?: yes
- Using distributed or parallel set-up in script?: distributed set-up
- Conda : 4.8.3 
- cuda =11.0

### Who can help
 @LysandreJik  @sgugger

## Information

Model I am using (Bert, XLNet ...): bert mulitilingual cased

The problem arises when using:
* [x] my own modified scripts : Triplet Loss applied to sentence similarity, to practice writing efficient distributed learning code.


The tasks I am working on is:
* [x] my own task or dataset: Triplet Loss applied to sentence similarity, to practice writing efficient distributed learning code.


## To reproduce

I have not been able to generte a minimal version yet.

Stack trace

```
Traceback (most recent call last):
[...]
  File ""my_program.py"", line 246, in epoch
    loss.backward()
  File ""conda/lib/python3.7/site-packages/torch/tensor.py"", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""conda/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 127, in backward
    allow_unreachable=True)  # allow_unreachable flag

RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [1, 200]] is at version 9; expected version 7 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
```

My model : 

```python

def __init__(self, trainfile, devfile, outdir, rank=0):
         # default config
          self.max_sent_len = 200
          self.batchsize = 10
          self.lr = 2e-5
          self.accumulated_batches = 6
          self.max_epochs = 10
          self.seed = 42
          self.embs = ""bert-base-multilingual-cased""
          self.tokenizer, self.model = self.transformers(self.embs)
          self.tripletloss = lossless_triplet_loss.LosslessTripletLoss()
  
          print(""use %s gpus"" % torch.cuda.device_count())
          self.model = self.model.to(rank)
          self.model = torch.nn.parallel.DistributedDataParallel(self.model, device_ids=[rank], output_device=rank, find_unused_para  meters=True)
          self.rank = rank
  
          half = False
          self.model = self.model.to(self.rank) 
  
          self.optimizer = optim.Adam(filter(lambda param: param.requires_grad, self.model.parameters()), lr=self.lr)
  
          self.traindataset = data.TripleDataset2(trainfile, self.tokenizer, self.max_sent_len)
          self.devdataset = data.TripleDataset2(devfile, self.tokenizer, self.max_sent_len)
          self.traindataloader = DataLoader(self.traindataset,
                                            collate_fn=self.collate_fn,
                                            batch_size = self.batchsize,
                                            num_workers = 0)
          self.devdataloader = DataLoader(self.devdataset,
                                          collate_fn=self.collate_fn,
                                          batch_size = self.batchsize,
                                         num_workers = 0)

 def train(self, epoch):
          self.model.train()
  
          iterations = int(math.ceil(len(self.traindataset)/self.batchsize))
          iterator = tqdm.tqdm(enumerate(self.traindataloader),
                               ncols=100,
                               ascii=True,
                               desc=""epoch %d"" % (epoch),
                               mininterval=1.0,
                               total = iterations)
          lastloss = None
          for batch_idx, anchor_pos_neg in iterator:
              (anchor, amask), (pos, pmask), (neg, nmask) = anchor_pos_neg
  
              anchor = anchor.to(self.rank)
              pos = pos.to(self.rank)
              neg = neg.to(self.rank)
  
              amask = amask.to(self.rank)
              pmask = pmask.to(self.rank)
              nmask = nmask.to(self.rank)
  
              anchor_vec = self.tokens2vec(anchor, attention_mask=amask)
              pos_vec = self.tokens2vec(pos, attention_mask=pmask)
              neg_vec = self.tokens2vec(neg, attention_mask=nmask)
              loss, distp, distn = self.calc_triplet_loss(anchor_vec, pos_vec, neg_vec)
  
              loss.backward()
              if (batch_idx+1) % self.accumulated_batches == 0:
                  self.optimizer.step()
                  self.optimizer.zero_grad()

      def tokens2vec(self, tokens_tensor, attention_mask=None):
          hidden_states = self.model(tokens_tensor, attention_mask=attention_mask)[0]
          token_vecs = torch.squeeze(hidden_states, dim=0)
  
          a = torch.sum(torch.mul(token_vecs, torch.unsqueeze(attention_mask, 2)), dim=1)
          m = torch.unsqueeze(torch.sum(attention_mask, dim=1), 1)
          sentence_embedding = torch.sigmoid(torch.div(a, m))
          return sentence_embedding
  
  
      def transformers(self, name):
          if name.startswith(""bert""):
              from transformers import BertTokenizer, BertModel
              tokenizer = BertTokenizer.from_pretrained(name, do_lower_case=False)
              model = BertModel.from_pretrained(name)
          else:
              tokenizer = None
              model = None
              print(""ERROR: invalid embeddings name <%s>"" % name, file=sys.stderr)
          return tokenizer, model

```

The error happens with batch_idx = 0.

200 is my maximum sequence length. I only use Longtensor as input to BertModel (for anchor, pos and neg). 
What I find strange is that I am sending samples in batches of size 10, so I am wondering if my problem is really with the embedding layer, or rather caused by my custom collate_fn  : 

```python
  def collate_fn(self, batch):
          anchor = []
          pos = []
          neg = []
          anchor_mask = []
          pos_mask = []
          neg_mask = []
          for a,p,n in batch:
              anchor.append(a[0])
              pos.append(p[0])
              neg.append(n[0])
              anchor_mask.append(a[1])
              pos_mask.append(p[1])
              neg_mask.append(n[1])
          return (torch.stack(anchor, dim=0),torch.stack(anchor_mask, dim=0)), \
               (torch.stack(pos, dim=0),torch.stack(pos_mask, dim=0)), \
               (torch.stack(neg, dim=0),torch.stack(neg_mask, dim=0))
```

My model is working well with only one GPU, or when using DataParallel. My issue arises only with DistributedDataParallel.


## Expected behavior
A running training.",,,,1,,,,,,
1625,https://github.com/huggingface/transformers/issues/9391,9391,"[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOTkwOTE4Mjcw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Good%20First%20Issue', 'name': 'Good First Issue', 'color': 'bbf794', 'default': False, 'description': ''}]",closed,2021-01-03 05:51:03+00:00,,7,Similar usage of `past_key_values` in CausalLM and Seq2SeqLM,"# 馃殌 Feature request

It seems GPT-2 and BartDecoder has a different style of `past_key_values`.

In GPT-2, `past_key_values` is explained as below:
(the explanation is from https://huggingface.co/transformers/model_doc/gpt2.html#gpt2model)

```
(parameters)
past_key_values (List[torch.FloatTensor] of length config.n_layers) 鈥?Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see past_key_values output below). Can be used to speed up sequential decoding. The input_ids which have their past given to this model should not be passed as input_ids as they have already been computed.


(returns)
past_key_values (List[torch.FloatTensor], optional, returned when use_cache=True is passed or when config.use_cache=True) 鈥?List of torch.FloatTensor of length config.n_layers, with each tensor of shape (2, batch_size, num_heads, sequence_length, embed_size_per_head)).

Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see past_key_values input) to speed up sequential decoding.
```

In BartDecoder and its inner BartDecoderLayer, `past_key_values` is explained and treated as below:
(the explanation is from https://huggingface.co/transformers/model_doc/bart.html#bartmodel)

``` 
past_key_values (Tuple[Tuple[torch.Tensor]] of length config.n_layers with each tuple having 2 tuples each of which has 2 tensors of shape (batch_size, num_heads, sequence_length - 1, embed_size_per_head)) 鈥?

Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.
```

`v4.1.1 modeling_bart`
https://github.com/huggingface/transformers/blob/v4.1.1/src/transformers/models/bart/modeling_bart.py

``` python
# in BartDecoder

        for idx, decoder_layer in enumerate(self.layers):
            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
            if output_hidden_states:
                all_hidden_states += (hidden_states,)
            dropout_probability = random.uniform(0, 1)
            if self.training and (dropout_probability < self.layerdrop):
                continue

            past_key_value = past_key_values[idx] if past_key_values is not None else None

            hidden_states, layer_self_attn, present_key_value, layer_cross_attn = decoder_layer(
                hidden_states,
                attention_mask=combined_attention_mask,
                encoder_hidden_states=encoder_hidden_states,
                encoder_attention_mask=encoder_attention_mask,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
            )
```

``` python
# in BartDecoderLayer

        # Self Attention
        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2
        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None
        # add present self-attn cache to positions 1,2 of present_key_value tuple
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            past_key_value=self_attn_past_key_value,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
        )
        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states
        if not self.normalize_before:
            hidden_states = self.self_attn_layer_norm(hidden_states)

        # Cross-Attention Block
        cross_attn_present_key_value = None
        cross_attn_weights = None
        if encoder_hidden_states is not None:
            residual = hidden_states
            if self.normalize_before:
                hidden_states = self.encoder_attn_layer_norm(hidden_states)

            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple
            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None
            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
                hidden_states=hidden_states,
                key_value_states=encoder_hidden_states,
                attention_mask=encoder_attention_mask,
                past_key_value=cross_attn_past_key_value,
                output_attentions=output_attentions,
            )
            hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)
            hidden_states = residual + hidden_states
            if not self.normalize_before:
                hidden_states = self.encoder_attn_layer_norm(hidden_states)

            # add cross-attn to positions 3,4 of present_key_value tuple
            present_key_value = present_key_value + cross_attn_present_key_value
```

## Motivation

It seems that one of the aims of the refactoring of Bart by @patrickvonplaten https://github.com/huggingface/transformers/pull/8900 is ""Allow to use BartEncoder and BartDecoder separately from the BartModel"".

I appreciate this very much and would love to treat `BartDecoder` as well as `gpt2`, but I feel that the difference in the handling of `past_key_values` is a barrier.

In `gpt2`, `past_key_value` in `past_key_values` is `torch.tensor` with each tensor of shape (2, batch_size, num_heads, sequence_length, embed_size_per_head)).
However, in `Bart`, `past_key_value` in `past_key_values` is `Tuple[torch.Tensor]` and `self_atten` part is not a tensor but a ""2 tensors of shape (batch_size, num_heads, sequence_length - 1, embed_size_per_head))"".

If we want to handle `self_attn_past_key_value` in `Bart` like that in `gpt2`, is it the right way to concatename the 2 tensors in `past_key_value`?
Or, is there the other correct way to treat it?

Thank you in advance.",,,,1,,,,,,
2275,https://github.com/huggingface/transformers/issues/10543,10543,[],closed,2021-03-05 12:04:45+00:00,,2,Similar issue like #1091 in Blenderbot,"Tokenizer and model are not in sync. I am using ""facebook/blenderbot-400M-distill"" 

Tokenizer has 8009 base tokens where as model has 8008. 

Could you please help me with this?

`from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration

mname = ""facebook/blenderbot-400M-distill""

model = BlenderbotForConditionalGeneration.from_pretrained(mname)
tokenizer = BlenderbotTokenizer.from_pretrained(mname, local_files_only=True)

print(len(tokenizer))
print(model.config.to_dict()['vocab_size'])`

Here is the output that I get.
8009
8008


## Environment info

- `transformers` version: 4.3.2
- Platform: Windows-10-10.0.18362-SP0
- Python version: 3.8.6
- PyTorch version (GPU?): 1.7.1+cpu (False)
- Tensorflow version (GPU?): 2.3.1 (False)
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj


",,,,1,,,,,,
2582,https://github.com/huggingface/transformers/issues/11114,11114,"[{'id': 1843244711, 'node_id': 'MDU6TGFiZWwxODQzMjQ0NzEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/New%20model', 'name': 'New model', 'color': 'fbca04', 'default': False, 'description': ''}]",closed,2021-04-07 09:13:04+00:00,,3,"Add CodeTrans, a model for source code generation, documentation generation and similar subtasks.","# 馃専 New model addition

 Paper: https://arxiv.org/abs/2104.02443

<!-- Important information -->

## Open source status

* [ ] the model implementation is available: https://github.com/agemagician/CodeTrans
* [ ] the model weights are available:
https://github.com/agemagician/CodeTrans
Currently tensorflow.
* [ ] who are the authors: @agemagician 
",,,,1,,,,,,
3257,https://github.com/huggingface/transformers/issues/12366,12366,[],closed,2021-06-26 06:27:47+00:00,,5,Tokens Jumbling,"I have trained a ""bert-base-uncased"" model (with lesser layers) on my dataset with my tokenizer for sentence similarity task. The final sentence embedding is formed using mean pooling strategy. Now during inference if my tokens for sentence1 is [t1,t2,t3,t4,t5] and for senetence2, I randomly shuffle these tokens example [t3,t1,t2,t5,t4], the score is really high, but sentence2 does't make any sense.
I tested the pretrained bert-base-uncased model and found out same problem, as shown below
Enter text1:a man is riding a horse
Enter text2:a riding man is a horse
Tokens1: [101, 1037, 2158, 2003, 5559, 1037, 3586, 102]
Tokens2: [101, 1037, 5559, 2158, 2003, 1037, 3586, 102]
Similarity: 0.703365683555603
Enter text1:A boy can throw a stone up to a maximum height
Enter text2:A stone up to a boy can maximum throw a height
Tokens1: [101, 1037, 2879, 2064, 5466, 1037, 2962, 2039, 2000, 1037, 4555, 4578, 102]
Tokens2: [101, 1037, 2962, 2039, 2000, 1037, 2879, 2064, 4555, 5466, 1037, 4578, 102]
Similarity: 0.9277969598770142
![aa](https://user-images.githubusercontent.com/47495143/123504257-b3250e80-d675-11eb-90b6-99bf8743e6ad.jpeg)
",,,,1,,,,,,
30,https://github.com/huggingface/transformers/issues/6132,6132,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}, {'id': 1834056635, 'node_id': 'MDU6TGFiZWwxODM0MDU2NjM1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Core:%20Tokenization', 'name': 'Core: Tokenization', 'color': 'FF4446', 'default': False, 'description': 'Internals of the library; Tokenization.'}, {'id': 2009457320, 'node_id': 'MDU6TGFiZWwyMDA5NDU3MzIw', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/translation', 'name': 'translation', 'color': 'b2d2f4', 'default': False, 'description': 'machine translation utilities and models'}]",closed,2020-07-29 14:39:44+00:00,,1,MBartTokenizerTrimmed to support truncated embeddings,"Motivation:
The embeddings table for MBART is huge, but only ~40K of the entries are used/finetuned for most wmt tasks. See https://github.com/pytorch/fairseq/issues/2120 

- needs vocab.json (fairseq Dictionary)
- needs to call `encode_as_pieces` with restricted vocabulary.

I will take this.",,,,1,,,,,,
492,https://github.com/huggingface/transformers/issues/7114,7114,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-09-14 07:56:43+00:00,,2,How to return the word embeddings and how to understand the hidden_states in return?,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->
I am new here, transformers is amazing. What I want is to get the embeddings of words with context , 鈥渓ogits鈥?is obviously a bit thin, I think the hidden_states may represent the word embedding, but I can not understand the output of the embeddings and output of each layer.
Which one should I choose?
Thanks very much!

> hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) 鈥?Tuple of torch.FloatTensor (one for the output of the embeddings + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:",,,,1,,,,,,
1743,https://github.com/huggingface/transformers/issues/9608,9608,[],closed,2021-01-15 03:14:01+00:00,,2,Convert ckpt from TFTrainer to huggingface format.,"I trained models with ```Trainer(pytorch)``` and ```TfTrainer(tensorflow)```, repectively.
With ```Trainer```, everything is ok. Saved models are directly applicable to huggingface pipeline (eg, AutoModel('model_name')).

But with save models from ```TFTrainer``` (ckpt format), I can not do that with ```AutoModel``` and ```TFAutoModel``` .
I can restart the training, so files do not have a problem.
I guess the problem is ckpt file all contains weight and other parameters related to optimizer.

How can I transform my ckpt file to huggingface-applicable format like ```tf_model.h5``` or convert to pytorch?

@jplu ",,,1,,,,,,,
2485,https://github.com/huggingface/transformers/issues/10947,10947,[],closed,2021-03-29 09:36:16+00:00,,2,Save model error: list index out of range after pass input_processing call,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.4.0
- Platform: Linux-4.19.0-14-cloud-amd64-x86_64-with-debian-10.8
- Python version: 3.7.10
- PyTorch version (GPU?): 1.7.0 (True)
- Tensorflow version (GPU?): 2.4.1 (True)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: Yes

### Who can help
Maybe @LysandreJik or @jplu 
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.


Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @LysandreJik

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

Model hub:

- for issues with a model report at https://discuss.huggingface.co/ and tag the model's creator.

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

## Information

Model I am using albert:

The problem arises when using:
* [ ] the official example scripts: (give details below)
* [x] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [x] my own task or dataset: (give details below)

## Code for Tensorflow
* albert_zh: https://github.com/brightmart/albert_zh

```python
strategy = tf.distribute.MirroredStrategy()
max_seq_len = 128
cache_folder = '/tmp'
pretrain_model = 'voidful/albert_chinese_tiny'
albert_config = AlbertConfig.from_json_file(albert_zh / 'albert_config' / 'albert_config_tiny.json')

def sms_classifier_model(pretrain_model, config, max_seq_len, cache_folder):
    input_ids = tf.keras.layers.Input(shape=(max_seq_len, ), name='input_ids', dtype=tf.int32)
    input_token_type_ids = tf.keras.layers.Input(shape=(max_seq_len, ), name='token_type_ids', dtype=tf.int32)
    input_attention_mask = tf.keras.layers.Input(shape=(max_seq_len, ), name='attention_mask', dtype=tf.int32)
        
    albert_model = TFAlbertForSequenceClassification.from_pretrained(
        pretrain_model, 
        config=config,
        from_pt=True,
        cache_dir=cache_folder)
        
    x = albert_model([input_ids, input_token_type_ids, input_attention_mask])
    output = tf.keras.activations.softmax(x[0])
    model = tf.keras.models.Model(
        inputs=[input_ids, input_token_type_ids, input_attention_mask],
        outputs={'target': output}, name='sms_classifier')
    return model 

K.clear_session()
albert_config.hidden_act = 'gelu_new'
albert_config.num_labels = 4

with strategy.scope():    
    albert_model = sms_classifier_model(pretrain_model, albert_config, 128, cached_pretarin_model_folder)

with strategy.scope():    
    albert_model.compile(optimizer=tf.keras.optimizers.Adam(), 
                         loss=tf.keras.losses.CategoricalCrossentropy(),
                         metrics=tf.keras.metrics.CategoricalAccuracy())

albert_model.fit(
        x=training_dataset,
        validation_data=validation_dataset,
        steps_per_epoch=200,
        validation_steps=100,
        epochs=2,
        verbose=1,
        use_multiprocessing=True)

albert_model.save('/tmp/albert_model')
```
## Error Message
```python
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-33-9fbc26d706ab> in <module>
      1 albert_model.save(
----> 2     str(saved_tf_model_folder / f'{run_id}')
      3 )

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)
   2000     # pylint: enable=line-too-long
   2001     save.save_model(self, filepath, overwrite, include_optimizer, save_format,
-> 2002                     signatures, options, save_traces)
   2003 
   2004   def save_weights(self,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)
    155   else:
    156     saved_model_save.save(model, filepath, overwrite, include_optimizer,
--> 157                           signatures, options, save_traces)
    158 
    159 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)
     87     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access
     88       with utils.keras_option_scope(save_traces):
---> 89         save_lib.save(model, filepath, signatures, options)
     90 
     91   if not include_optimizer:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)
   1031 
   1032   _, exported_graph, object_saver, asset_info = _build_meta_graph(
-> 1033       obj, signatures, options, meta_graph_def)
   1034   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION
   1035 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, signatures, options, meta_graph_def)
   1196 
   1197   with save_context.save_context(options):
-> 1198     return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
   1131   if signatures is None:
   1132     signatures = signature_serialization.find_function_to_export(
-> 1133         checkpoint_graph_view)
   1134 
   1135   signatures, wrapped_functions = (

/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)
     73   # If the user did not specify signatures, check the root object for a function
     74   # that can be made into a signature.
---> 75   functions = saveable_view.list_functions(saveable_view.root)
     76   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)
     77   if signature is not None:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in list_functions(self, obj, extra_functions)
    149     if obj_functions is None:
    150       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access
--> 151           self._serialization_cache)
    152       self._functions[obj] = obj_functions
    153     if extra_functions:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _list_functions_for_serialization(self, serialization_cache)
   2611     self.predict_function = None
   2612     functions = super(
-> 2613         Model, self)._list_functions_for_serialization(serialization_cache)
   2614     self.train_function = train_function
   2615     self.test_function = test_function

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _list_functions_for_serialization(self, serialization_cache)
   3085   def _list_functions_for_serialization(self, serialization_cache):
   3086     return (self._trackable_saved_model_saver
-> 3087             .list_functions_for_serialization(serialization_cache))
   3088 
   3089   def __getstate__(self):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py in list_functions_for_serialization(self, serialization_cache)
     92       return {}
     93 
---> 94     fns = self.functions_to_serialize(serialization_cache)
     95 
     96     # The parent AutoTrackable class saves all user-defined tf.functions, and

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in functions_to_serialize(self, serialization_cache)
     77   def functions_to_serialize(self, serialization_cache):
     78     return (self._get_serialized_attributes(
---> 79         serialization_cache).functions_to_serialize)
     80 
     81   def _get_serialized_attributes(self, serialization_cache):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)
     93 
     94     object_dict, function_dict = self._get_serialized_attributes_internal(
---> 95         serialization_cache)
     96 
     97     serialized_attr.set_and_validate_objects(object_dict)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)
     55     objects, functions = (
     56         super(ModelSavedModelSaver, self)._get_serialized_attributes_internal(
---> 57             serialization_cache))
     58     functions['_default_save_signature'] = default_signature
     59     return objects, functions

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)
    102     """"""Returns dictionary of serialized attributes.""""""
    103     objects = save_impl.wrap_layer_objects(self.obj, serialization_cache)
--> 104     functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
    105     # Attribute validator requires that the default save signature is added to
    106     # function dict, even if the value is None.

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in wrap_layer_functions(layer, serialization_cache)
    163   call_fn_with_losses = call_collection.add_function(
    164       _wrap_call_and_conditional_losses(layer),
--> 165       '{}_layer_call_and_return_conditional_losses'.format(layer.name))
    166   call_fn = call_collection.add_function(
    167       _extract_outputs_from_fn(layer, call_fn_with_losses),

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in add_function(self, call_fn, name)
    503       # Manually add traces for layers that have keyword arguments and have
    504       # a fully defined input signature.
--> 505       self.add_trace(*self._input_signature)
    506     return fn
    507 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in add_trace(self, *args, **kwargs)
    418             fn.get_concrete_function(*args, **kwargs)
    419 
--> 420         trace_with_training(True)
    421         trace_with_training(False)
    422       else:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in trace_with_training(value, fn)
    416           utils.set_training_arg(value, self._training_arg_index, args, kwargs)
    417           with K.deprecated_internal_learning_phase_scope(value):
--> 418             fn.get_concrete_function(*args, **kwargs)
    419 
    420         trace_with_training(True)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in get_concrete_function(self, *args, **kwargs)
    548     if not self.call_collection.tracing:
    549       self.call_collection.add_trace(*args, **kwargs)
--> 550     return super(LayerCall, self).get_concrete_function(*args, **kwargs)
    551 
    552 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)
   1297       ValueError: if this object has not yet been called on concrete values.
   1298     """"""
-> 1299     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
   1300     concrete._garbage_collector.release()  # pylint: disable=protected-access
   1301     return concrete

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   1203       if self._stateful_fn is None:
   1204         initializers = []
-> 1205         self._initialize(args, kwargs, add_initializers_to=initializers)
   1206         self._initialize_uninitialized_variables(initializers)
   1207 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    724     self._concrete_stateful_fn = (
    725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 726             *args, **kwds))
    727 
    728     def invalid_creator_scope(*unused_args, **unused_kwds):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2967       args, kwargs = None, None
   2968     with self._lock:
-> 2969       graph_function, _ = self._maybe_define_function(args, kwargs)
   2970     return graph_function
   2971 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3359 
   3360           self._function_cache.missed.add(call_context_key)
-> 3361           graph_function = self._create_graph_function(args, kwargs)
   3362           self._function_cache.primary[cache_key] = graph_function
   3363 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3204             arg_names=arg_names,
   3205             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3206             capture_by_value=self._capture_by_value),
   3207         self._function_attributes,
   3208         function_spec=self.function_spec,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    988         _, original_func = tf_decorator.unwrap(python_func)
    989 
--> 990       func_outputs = python_func(*func_args, **func_kwargs)
    991 
    992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    632             xla_context.Exit()
    633         else:
--> 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    635         return out
    636 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in wrapper(*args, **kwargs)
    525       with autocast_variable.enable_auto_cast_variables(
    526           layer._compute_dtype_object):  # pylint: disable=protected-access
--> 527         ret = method(*args, **kwargs)
    528     _restore_layer_losses(original_losses)
    529     return ret

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in wrap_with_training_arg(*args, **kwargs)
    169     return control_flow_util.smart_cond(
    170         training, lambda: replace_training_and_call(True),
--> 171         lambda: replace_training_and_call(False))
    172 
    173   # Create arg spec for decorated function. If 'training' is not defined in the

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/control_flow_util.py in smart_cond(pred, true_fn, false_fn, name)
    113         pred, true_fn=true_fn, false_fn=false_fn, name=name)
    114   return smart_module.smart_cond(
--> 115       pred, true_fn=true_fn, false_fn=false_fn, name=name)
    116 
    117 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     52   if pred_value is not None:
     53     if pred_value:
---> 54       return true_fn()
     55     else:
     56       return false_fn()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in <lambda>()
    168 
    169     return control_flow_util.smart_cond(
--> 170         training, lambda: replace_training_and_call(True),
    171         lambda: replace_training_and_call(False))
    172 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in replace_training_and_call(training)
    165     def replace_training_and_call(training):
    166       set_training_arg(training, training_arg_index, args, kwargs)
--> 167       return wrapped_call(*args, **kwargs)
    168 
    169     return control_flow_util.smart_cond(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in call_and_return_conditional_losses(inputs, *args, **kwargs)
    568   def call_and_return_conditional_losses(inputs, *args, **kwargs):
    569     """"""Returns layer (call_output, conditional losses) tuple.""""""
--> 570     call_output = layer_call(inputs, *args, **kwargs)
    571     if version_utils.is_v1_layer_or_model(layer):
    572       conditional_losses = layer.get_losses_for(inputs)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)
    423     """"""
    424     return self._run_internal_graph(
--> 425         inputs, training=training, mask=mask)
    426 
    427   def compute_output_shape(self, input_shape):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)
    558 
    559         args, kwargs = node.map_arguments(tensor_dict)
--> 560         outputs = node.layer(*args, **kwargs)
    561 
    562         # Update tensor_dict.

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1010         with autocast_variable.enable_auto_cast_variables(
   1011             self._compute_dtype_object):
-> 1012           outputs = call_fn(inputs, *args, **kwargs)
   1013 
   1014         if self._activity_regularizer:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in return_outputs_and_add_losses(*args, **kwargs)
     71     inputs = args[inputs_arg_index]
     72     args = args[inputs_arg_index + 1:]
---> 73     outputs, losses = fn(inputs, *args, **kwargs)
     74     layer.add_loss(losses, inputs=inputs)
     75 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in wrap_with_training_arg(*args, **kwargs)
    169     return control_flow_util.smart_cond(
    170         training, lambda: replace_training_and_call(True),
--> 171         lambda: replace_training_and_call(False))
    172 
    173   # Create arg spec for decorated function. If 'training' is not defined in the

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/control_flow_util.py in smart_cond(pred, true_fn, false_fn, name)
    113         pred, true_fn=true_fn, false_fn=false_fn, name=name)
    114   return smart_module.smart_cond(
--> 115       pred, true_fn=true_fn, false_fn=false_fn, name=name)
    116 
    117 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     52   if pred_value is not None:
     53     if pred_value:
---> 54       return true_fn()
     55     else:
     56       return false_fn()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in <lambda>()
    168 
    169     return control_flow_util.smart_cond(
--> 170         training, lambda: replace_training_and_call(True),
    171         lambda: replace_training_and_call(False))
    172 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in replace_training_and_call(training)
    165     def replace_training_and_call(training):
    166       set_training_arg(training, training_arg_index, args, kwargs)
--> 167       return wrapped_call(*args, **kwargs)
    168 
    169     return control_flow_util.smart_cond(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in __call__(self, *args, **kwargs)
    542   def __call__(self, *args, **kwargs):
    543     if not self.call_collection.tracing:
--> 544       self.call_collection.add_trace(*args, **kwargs)
    545     return super(LayerCall, self).__call__(*args, **kwargs)
    546 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in add_trace(self, *args, **kwargs)
    418             fn.get_concrete_function(*args, **kwargs)
    419 
--> 420         trace_with_training(True)
    421         trace_with_training(False)
    422       else:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in trace_with_training(value, fn)
    416           utils.set_training_arg(value, self._training_arg_index, args, kwargs)
    417           with K.deprecated_internal_learning_phase_scope(value):
--> 418             fn.get_concrete_function(*args, **kwargs)
    419 
    420         trace_with_training(True)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in get_concrete_function(self, *args, **kwargs)
    548     if not self.call_collection.tracing:
    549       self.call_collection.add_trace(*args, **kwargs)
--> 550     return super(LayerCall, self).get_concrete_function(*args, **kwargs)
    551 
    552 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)
   1297       ValueError: if this object has not yet been called on concrete values.
   1298     """"""
-> 1299     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
   1300     concrete._garbage_collector.release()  # pylint: disable=protected-access
   1301     return concrete

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   1215       # run the first trace but we should fail if variables are created.
   1216       concrete = self._stateful_fn._get_concrete_function_garbage_collected(  # pylint: disable=protected-access
-> 1217           *args, **kwargs)
   1218       if self._created_variables:
   1219         raise ValueError(""Creating variables on a non-first call to a function""

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   3017       args, kwargs = None, None
   3018     with self._lock:
-> 3019       graph_function, _ = self._maybe_define_function(args, kwargs)
   3020       seen_names = set()
   3021       captured = object_identity.ObjectIdentitySet(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3359 
   3360           self._function_cache.missed.add(call_context_key)
-> 3361           graph_function = self._create_graph_function(args, kwargs)
   3362           self._function_cache.primary[cache_key] = graph_function
   3363 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3204             arg_names=arg_names,
   3205             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3206             capture_by_value=self._capture_by_value),
   3207         self._function_attributes,
   3208         function_spec=self.function_spec,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    988         _, original_func = tf_decorator.unwrap(python_func)
    989 
--> 990       func_outputs = python_func(*func_args, **func_kwargs)
    991 
    992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    632             xla_context.Exit()
    633         else:
--> 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    635         return out
    636 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in wrapper(*args, **kwargs)
    525       with autocast_variable.enable_auto_cast_variables(
    526           layer._compute_dtype_object):  # pylint: disable=protected-access
--> 527         ret = method(*args, **kwargs)
    528     _restore_layer_losses(original_losses)
    529     return ret

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in wrap_with_training_arg(*args, **kwargs)
    169     return control_flow_util.smart_cond(
    170         training, lambda: replace_training_and_call(True),
--> 171         lambda: replace_training_and_call(False))
    172 
    173   # Create arg spec for decorated function. If 'training' is not defined in the

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/control_flow_util.py in smart_cond(pred, true_fn, false_fn, name)
    113         pred, true_fn=true_fn, false_fn=false_fn, name=name)
    114   return smart_module.smart_cond(
--> 115       pred, true_fn=true_fn, false_fn=false_fn, name=name)
    116 
    117 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     52   if pred_value is not None:
     53     if pred_value:
---> 54       return true_fn()
     55     else:
     56       return false_fn()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in <lambda>()
    168 
    169     return control_flow_util.smart_cond(
--> 170         training, lambda: replace_training_and_call(True),
    171         lambda: replace_training_and_call(False))
    172 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in replace_training_and_call(training)
    165     def replace_training_and_call(training):
    166       set_training_arg(training, training_arg_index, args, kwargs)
--> 167       return wrapped_call(*args, **kwargs)
    168 
    169     return control_flow_util.smart_cond(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in call_and_return_conditional_losses(inputs, *args, **kwargs)
    568   def call_and_return_conditional_losses(inputs, *args, **kwargs):
    569     """"""Returns layer (call_output, conditional losses) tuple.""""""
--> 570     call_output = layer_call(inputs, *args, **kwargs)
    571     if version_utils.is_v1_layer_or_model(layer):
    572       conditional_losses = layer.get_losses_for(inputs)

/opt/conda/lib/python3.7/site-packages/transformers/models/albert/modeling_tf_albert.py in call(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training, **kwargs)
   1144             labels=labels,
   1145             training=training,
-> 1146             kwargs_call=kwargs,
   1147         )
   1148         outputs = self.albert(

/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_utils.py in input_processing(func, config, input_ids, **kwargs)
    372                     output[tensor_name] = input
    373                 else:
--> 374                     output[parameter_names[i]] = input
    375             elif isinstance(input, allowed_types) or input is None:
    376                 output[parameter_names[i]] = input

IndexError: list index out of range
```
<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->

In version <= 4.0.0 use save method was without the error, but after `input_processing` function comment that the error happened.
Does any advice to fix the problem?

",,,1,,,,,,,
2623,https://github.com/huggingface/transformers/issues/11194,11194,[],closed,2021-04-12 02:12:36+00:00,,1,Transfer learning on bert,"We take pretrained bert model and passing our dataset we save model on .bin file and we take predictions.
But how can we retrain a model with new data set on top of generated bin file 
Please help me on this issue ",,,1,,,,,,,
2836,https://github.com/huggingface/transformers/issues/11581,11581,[],closed,2021-05-04 13:56:58+00:00,,3,unable to save model1.h5 .I am using huggingface distilbert,"Hi Team,
I was trying to save model but i got error.
model.save(""model1.h5"")
but it didn't work.
I tried different method like model.save(""model1"") so it will save as .pb file. But when i was trying to load I am getting below error. 
please let me know how to solved it. how can save model after using distil-bert with my model.
~\.conda\envs\bot2\lib\site-packages\tensorflow\python\keras\engine\training.py in get_config(self)
   2229 
   2230   def get_config(self):
-> 2231     raise NotImplementedError
   2232 
   2233   @classmethod

NotImplementedError: ",,,1,,,,,,,
2899,https://github.com/huggingface/transformers/issues/11710,11710,[],closed,2021-05-13 07:49:34+00:00,,3,AssertionError: internal model should be a reference to self.model,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.2.2 
- Platform: Linux
- Python version: 3.6
- PyTorch version (GPU?): 1.7 CPU
- Tensorflow version (GPU?):
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help

@sgugger 

<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @Rocketknight1

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

Model hub:

- for issues with a model report at https://discuss.huggingface.co/ and tag the model's creator.

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

## Information

Model I am using (Bert, XLNet ...):

The problem arises when using:
* [ ] the official example scripts: (give details below)
When I run `trainer.train()` for the second time in Jyputer, it throws error:
```
AssertionError                            Traceback (most recent call last)
<ipython-input-7-3435b262f1ae> in <module>
----> 1 trainer.train()

~/data/apps/anaconda3/lib/python3.7/site-packages/transformers/trainer.py in train(self, model_path, trial)
    933 
    934             self.control = self.callback_handler.on_epoch_end(self.args, self.state, self.control)
--> 935             self._maybe_log_save_evaluate(tr_loss, model, trial, epoch)
    936 
    937             if self.args.tpu_metrics_debug or self.args.debug:

~/data/apps/anaconda3/lib/python3.7/site-packages/transformers/trainer.py in _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch)
   1006 
   1007         if self.control.should_save:
-> 1008             self._save_checkpoint(model, trial, metrics=metrics)
   1009             self.control = self.callback_handler.on_save(self.args, self.state, self.control)
   1010 

~/data/apps/anaconda3/lib/python3.7/site-packages/transformers/trainer.py in _save_checkpoint(self, model, trial, metrics)
   1012         # In all cases, including ddp/dp/deepspeed, self.model is always a reference to the model we
   1013         # want to save.
-> 1014         assert _model_unwrap(model) is self.model, ""internal model should be a reference to self.model""
   1015 
   1016         # Save model checkpoint

AssertionError: internal model should be a reference to self.model

```
https://huggingface.co/transformers/training.html

The tasks I am working on is:
* [ ] sequence classification
* [ ] my own task

## To reproduce

Steps to reproduce the behavior:

1. run `trainer.train()`
2. run `trainer.train()` again.

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->
",,,1,,,,,,,
3114,https://github.com/huggingface/transformers/issues/12072,12072,[],closed,2021-06-08 17:04:56+00:00,,3,Inconsistent behavior on CPU vs. GPU ,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.5.1
- Platform: Linux-4.19.0-16-cloud-amd64-x86_64-with-debian-10.9
- Python version: 3.7.10
- PyTorch version (GPU?): 1.8.1+cu111 (True)
- Tensorflow version (GPU?): 2.5.0 (False)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No

### Who can help
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @Rocketknight1

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

Model hub:

- for issues with a model report at https://discuss.huggingface.co/ and tag the model's creator.

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

## Information

Model I am using (Bert, XLNet ...): AutoModel

## To reproduce

Steps to reproduce the behavior:

Hi all - I've been struggling with inconsistent behavior on CPU vs. GPU. 

When running on CPU the following code works as expected:
```Python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer


def predict(model, tokenizer, test_str, device):
    input_ids = tokenizer(test_str, return_tensors='pt', padding=True).to(device)
    model.to(device)
    model.eval()
    with torch.no_grad():
        pred = model(**input_ids)
    logits = pred.logits.cpu()
    return logits


device = 'cpu'
model_dir = 'test_dir'
model_type = 'roberta-base'
test_str = [
    'Hello! I am a test string!',
    ]

model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=1)
tokenizer = AutoTokenizer.from_pretrained(model_type)

# save model
model.save_pretrained(model_dir)

pred1 = predict(model, tokenizer, test_str, device)
print(pred1)

model = AutoModelForSequenceClassification.from_pretrained(model_dir)
pred2 = predict(model, tokenizer, test_str, device)
print(pred2)
```
Output:
```
# Obviously output is random, however is identical
tensor([[-0.0238]])
tensor([[-0.0238]])
```

But when I change the to cuda by changing the device
```python
device = 'cuda'
```
I get a significantly different output:
```
tensor([[-0.3194]])
tensor([[-0.3414]])
```

Weirdly the above doesn't happen if I increase the length of my test string:
```
test_str = [
    'Hello! I am a test string! Hello! I am a test string! Hello! I am a test string! Hello! I am a test string! ',
    ]
```
I'm pretty sure I'm missing something obvious - any help is appreciated! 馃檹

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

I expect the output of the loaded model to be identical not only on CPU but also on GPU.
",,,1,,,,,,,
3920,https://github.com/huggingface/transformers/issues/13610,13610,[],closed,2021-09-16 16:25:55+00:00,,5,How to use model.save() in tf2 when using TFBertModel,"tensorflow==2.3.1

transformers==4.2.1

My model define as:
```
import tensorflow as tf
from tensorflow.keras import Model 
from tensorflow.keras.layers import *
from transformers import TFAutoModel

input_ids = Input(shape=(3000), name='INPUT_input_ids', dtype=tf.int32)
input_mask = Input(shape=(3000), name='INPUT_input_mask', dtype=tf.int32)
segment_ids = Input(shape=(3000), name='INPUT_segment_ids', dtype=tf.int32)
passage_mask = Input(shape=(10), name='INPUT_passage_mask', dtype=tf.int32)
input_ids_reshape = K.reshape(input_ids,(-1, 300))
input_mask_reshape = K.reshape(input_mask,(-1, 300))
segment_ids_reshape = K.reshape(segment_ids,(-1, 300))
transformer = TFAutoModel.from_pretrained('hfl/chinese-roberta-wwm-ext', from_pt=False)
transformer_output = transformer([input_ids_reshape, input_mask_reshape, segment_ids_reshape])[0]
......
model = Model(
    inputs  = [input_ids, input_mask, segment_ids, passage_mask], 
    outputs = [start_prob, end_prob]
)
```
I try to save model in this way:
```
model.save(path)
```
but I got error
```
/lib/python3.6/site-packages/transformers/modeling_tf_utils.py in input_processing(func, config, input_ids, **kwargs)
    364                     output[tensor_name] = input
    365                 else:
--> 366                     output[parameter_names[i]] = input
    367             elif isinstance(input, allowed_types) or input is None:
    368                 output[parameter_names[i]] = input

IndexError: list index out of range
```
model.predict() and model.save_weights() is working.

How to use model.save() with huggingface-transformers? OR How to write model with huggingface-transformers? I just want to use transformers as a keras layer in my model.",,,1,,,,,,,
3977,https://github.com/huggingface/transformers/issues/13742,13742,[],open,2021-09-25 16:12:41+00:00,,4,Can't save model in saved_model format when finetune bert in tensorflow2,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.10.3
- Platform: centos7
- Python version: 3.8
- PyTorch version (GPU?): -
- Tensorflow version (GPU?): 2.4.3
- Using GPU in script?: Error exists in CPU and GPU
- Using distributed or parallel set-up in script?: Error exists with or without Tensorflow MirroredStrategy

### Who can help
@LysandreJik @Rocketknight1
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.


Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @Rocketknight1

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

Model hub:

- for issues with a model report at https://discuss.huggingface.co/ and tag the model's creator.

HF projects:

- datasets: [different repo](https://github.com/huggingface/datasets)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

## Information

Model I am using (Bert, XLNet ...): roberta


The problem arises when using:
* [ ] the official example scripts: (give details below)
* [x] my own modified scripts: (give details below)

The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)
* [x] my own task or dataset: (give details below)

## To reproduce

Steps to reproduce the behavior:
```python
class TFBertForMultilabelClassification(TFBertPreTrainedModel):

    def __init__(self, config, *inputs, **kwargs):
        super(TFBertForMultilabelClassification, self).__init__(config, *inputs, **kwargs)
        self.num_labels = config.num_labels
        self.bert = TFBertMainLayer(config, name='bert')
        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)
        self.classifier = tf.keras.layers.Dense(config.num_labels,
                                                kernel_initializer=get_initializer(config.initializer_range),
                                                name='classifier',
                                                activation='sigmoid')#--------------------- sigmoid婵€娲诲嚱鏁?

    def call(self, inputs, **kwargs):
        outputs = self.bert(inputs, **kwargs)
        pooled_output = outputs[1]
        pooled_output = self.dropout(pooled_output, training=kwargs.get('training', False))
        logits = self.classifier(pooled_output)
        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here
        return outputs  # logits, (hidden_states), (attentions)

model = TFBertForMultilabelClassification.from_pretrained(""bert-base-uncased"")
model.save(""/tmp/model"")
```

Error messages:

```
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForMultilabelClassification: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertForMultilabelClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForMultilabelClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFBertForMultilabelClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_2326', 'classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-222-0c8afc02744c> in <module>
     20 
     21 model = TFBertForMultilabelClassification.from_pretrained(""bert-base-uncased"")
---> 22 model.save(""test"")

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)
   1994     """"""
   1995     # pylint: enable=line-too-long
-> 1996     save.save_model(self, filepath, overwrite, include_optimizer, save_format,
   1997                     signatures, options, save_traces)
   1998 

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)
    154         model, filepath, overwrite, include_optimizer)
    155   else:
--> 156     saved_model_save.save(model, filepath, overwrite, include_optimizer,
    157                           signatures, options, save_traces)
    158 

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)
     87     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access
     88       with utils.keras_option_scope(save_traces):
---> 89         save_lib.save(model, filepath, signatures, options)
     90 
     91   if not include_optimizer:

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)
   1030   meta_graph_def = saved_model.meta_graphs.add()
   1031 
-> 1032   _, exported_graph, object_saver, asset_info = _build_meta_graph(
   1033       obj, signatures, options, meta_graph_def)
   1034   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, signatures, options, meta_graph_def)
   1196 
   1197   with save_context.save_context(options):
-> 1198     return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
   1145   # Note we run this twice since, while constructing the view the first time
   1146   # there can be side effects of creating variables.
-> 1147   _ = _SaveableView(checkpoint_graph_view, options)
   1148   saveable_view = _SaveableView(checkpoint_graph_view, options,
   1149                                 wrapped_functions)

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in __init__(self, checkpoint_view, options, wrapped_functions)
    223           #  variables on first run.
    224           concrete_functions = (
--> 225               function._list_all_concrete_functions_for_serialization())  # pylint: disable=protected-access
    226         else:
    227           concrete_functions = [function]

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _list_all_concrete_functions_for_serialization(self)
   1160       A list of instances of `ConcreteFunction`.
   1161     """"""
-> 1162     concrete_functions = self._list_all_concrete_functions()
   1163     seen_signatures = []
   1164     for concrete_function in concrete_functions:

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _list_all_concrete_functions(self)
   1142     """"""Returns all concrete functions.""""""
   1143     if self.input_signature is not None:
-> 1144       self.get_concrete_function()
   1145     concrete_functions = []
   1146     # pylint: disable=protected-access

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)
   1297       ValueError: if this object has not yet been called on concrete values.
   1298     """"""
-> 1299     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
   1300     concrete._garbage_collector.release()  # pylint: disable=protected-access
   1301     return concrete

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   1203       if self._stateful_fn is None:
   1204         initializers = []
-> 1205         self._initialize(args, kwargs, add_initializers_to=initializers)
   1206         self._initialize_uninitialized_variables(initializers)
   1207 

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    723     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    724     self._concrete_stateful_fn = (
--> 725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    726             *args, **kwds))
    727 

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2967       args, kwargs = None, None
   2968     with self._lock:
-> 2969       graph_function, _ = self._maybe_define_function(args, kwargs)
   2970     return graph_function
   2971 

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3359 
   3360           self._function_cache.missed.add(call_context_key)
-> 3361           graph_function = self._create_graph_function(args, kwargs)
   3362           self._function_cache.primary[cache_key] = graph_function
   3363 

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3194     arg_names = base_arg_names + missing_arg_names
   3195     graph_function = ConcreteFunction(
-> 3196         func_graph_module.func_graph_from_py_func(
   3197             self._name,
   3198             self._python_function,

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    988         _, original_func = tf_decorator.unwrap(python_func)
    989 
--> 990       func_outputs = python_func(*func_args, **func_kwargs)
    991 
    992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    632             xla_context.Exit()
    633         else:
--> 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    635         return out
    636 

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/eager/function.py in bound_method_wrapper(*args, **kwargs)
   3885     # However, the replacer is still responsible for attaching self properly.
   3886     # TODO(mdan): Is it possible to do it here instead?
-> 3887     return wrapped_fn(*args, **kwargs)
   3888   weak_bound_method_wrapper = weakref.ref(bound_method_wrapper)
   3889 

/usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, ""ag_error_metadata""):
--> 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

TypeError: in user code:

    /usr/local/Caskroom/miniconda/base/envs/tms/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:682 serving  *
        return self.serving_output(output)

    TypeError: tf__serving_output() takes 1 positional argument but 2 were given
```

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

model can save in saved_model format without error

<!-- A clear and concise description of what you would expect to happen. -->
",,,1,,,,,,,
307,https://github.com/huggingface/transformers/issues/6734,6734,[],closed,2020-08-25 21:38:05+00:00,,1,"id2lang in tokenization_xlm.py should be int, and removing hardcoding","In https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_xlm.py#L78 we have:
```
        ""id2lang"": {""0"": ""de"", ""1"": ""en""},
        ""lang2id"": {""de"": 0, ""en"": 1},
```
and then:
```
        lang2id (:obj:`Dict[str, int]`, `optional`, defaults to :obj:`None`):
            Dictionary mapping languages string identifiers to their IDs.
        id2lang (:obj:`Dict[int, str`, `optional`, defaults to :obj:`None`):
```

So it should be:
```
        ""id2lang"": {0: ""de"", 1: ""en""},
        ""lang2id"": {""de"": 0, ""en"": 1},
```
All other entries need this change too.

The problem hasn't been detected until now since they were used to only count the number of languages it seems.

I need to pass src/tgt languages to the tokenizer I'm porting from fairseq, so I was looking at how to do that and `id2lang` seems to fit the purpose. But I actually need to look them up by `int` id, that's how I saw the problem.

But I'm also not sure why do we need to hardcode the reversal, when it can be done in 1 line of code? Which would also remove this assertion code:

```
        self.lang2id = lang2id
        self.id2lang = id2lang
        if lang2id is not None and id2lang is not None:
            assert len(lang2id) == len(id2lang)
```

Further we don't even need to hardcode the ids. Replace:
```
       ""id2lang"": {0: ""de"", 1: ""en""},
```
with:
```
       ""id2lang"": [""de"", ""en""]
```
So all we need is one of the two entries, and now generate the 2 lookup dicts on the fly.

And since it's no longer `id2lang` semantically, probably renaming it to just `langs` would be more appropriate.

I think I will use this approach regardless of the outcome of this issue.

Thanks.",,1,,,,,,,,
1198,https://github.com/huggingface/transformers/issues/8551,8551,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-11-15 20:55:26+00:00,,7,"""special token {} has to be either str or AddedToken but got:","Hello, two weeks ago I fine-tune the Roberta language model on my data using hugging face (codes given below) and save output at the google drive. Now when using it for semantic search with sentence transformer. I am getting the error that


**""TypeError: special token bos_token has to be either str or AddedToken but got: <class 'dict'>""**

```

from sentence_transformers import SentenceTransformer
from sentence_transformers import models, losses
import scipy.spatial
import pickle as pkl
word_embedding_model = models.RoBERTa(""/content/drive/My Drive/Ottawa_citit"")

# Apply mean pooling to get one fixed sized sentence vector
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),
                               pooling_mode_mean_tokens=True,
                               pooling_mode_cls_token=False,
                               pooling_mode_max_tokens=False)

model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

 Previously fine-tuned Roberta

`!python ""/content/transformers/examples/contrib/legacy/run_language_modeling.py"" \
    --output_dir ""/content/drive/My Drive/Ottawa_citit"" \
    --model_name_or_path roberta-base \
    --do_train \
    --per_gpu_train_batch_size 8 \
    --seed 42 \
    --train_data_file ""/content/input_text.txt"" \
    --block_size 256 \
    --line_by_line \
    --learning_rate 6e-4 \
    --num_train_epochs 3 \
    --save_total_limit 2 \
    --save_steps 200 \
    --weight_decay 0.01 \
     --mlm`
```",,1,,,,,,,,
1641,https://github.com/huggingface/transformers/issues/9420,9420,[],closed,2021-01-05 16:20:53+00:00,,1,Transformer models for semantic parsing,"Hi! Thank you for your awesome work!

I want to perform semantic parsing. Unfortunately, I couldn't find any examples on hugging face repo for that. Could you please let me know how I should proceed? I suppose I could use a Seq2Seq EncoderDecoder model like BERT2BERT and finetune it for semantic parsing. Or do you think there is a better way? For more context, I have natural language grounding descriptions and  I want to generate logical parse tree from it. In literature, there are a few tree transformer-based techniques and Seq2Tree technique which I think hugging face do not support yet (or does it?). 

Thanks :)",,1,,,,,,,,
1949,https://github.com/huggingface/transformers/issues/9992,9992,"[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNjQ4NjIxOTg1', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/Feature%20request', 'name': 'Feature request', 'color': 'FBCA04', 'default': False, 'description': 'Request for a new feature'}]",open,2021-02-04 02:34:49+00:00,,1,Adversarial/amnesic heads,"# 馃殌 Feature request

Task heads that backpropagate deliberately reversed gradients to the encoder. A flag requesting this behavior when constructing a task head.

## Motivation

Transfer learning experiments lend themselves to questions about the extent to which two tasks rely on the same information about a word/sentence, and to experiments probing whether and how word encodings contain/correspond to syntax trees, lemmas, frequencies, and other objects of linguistic/psycholinguistic study.

A difficulty is that a pretrained model, without fine-tuning, may already encode certain information too thoroughly and accessibly for intermediate training to make much of a difference. For example, BERT's masked language modeling objective produces word encodings in which syntax information is readily accessible. Intermediate training on a syntax task requires training a task head to extract this information, of course, but it will result in very little reorganization of the encoder itself.

Adversarial training, such as the amnesic probing of Elazar et al. 2020, can avoid this pitfall. Intermediate training can aim to burn particular information *out* of the encodings, and measure how much this impairs trainability of the target task. Strictly reversing the sense of the training data won't do it though; getting all the answers exactly wrong requires just as much domain knowledge as getting them all right does. And randomizing the labels on training data may just result in a feckless task head, one that discards useful information passed to it from the encoder, rather than affecting the encoder itself. 

Ideally, then, the task head would be trained toward correctly reproducing gold-standard labels, but would flip all its gradients before backpropagating them to the shared encoder, thus training it not to produce precisely the signals that the task head found most informative. The following work by Cory Shain illustrates flipping gradients in this way (although it's not applied to shared-encoder transfer learning, but rather to development of encoders that disentangle semantics from syntax).

	https://docs.google.com/presentation/d/1E89yZ8jXXeSARDLmlksOCJo83QZdNbd7phBrR_dRogg/edit#slide=id.g79452223cd_0_19
	https://github.com/coryshain/synsemnet

## Your contribution

I am deeply unfamiliar with pytorch, unfortunately, and utterly ignorant of tensorflow. I can't offer much.",,1,,,,,,,,
2447,https://github.com/huggingface/transformers/issues/10881,10881,[],closed,2021-03-24 08:33:58+00:00,,8,MlFlow log artefacts,"## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->

- `transformers` version: 4.4.2
- Platform: Darwin-20.3.0-x86_64-i386-64bit
- Python version: 3.7.4
- PyTorch version (GPU?): 1.3.1 (False)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

### Who can help

@sgugger 
<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @
 If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.
 Please tag fewer than 3 people.

Models:

- albert, bert, xlm: @LysandreJik
- blenderbot, bart, marian, pegasus, encoderdecoder,  t5: @patrickvonplaten, @patil-suraj
- longformer, reformer, transfoxl, xlnet: @patrickvonplaten
- fsmt: @stas00
- funnel: @sgugger
- gpt2: @patrickvonplaten, @LysandreJik
- rag: @patrickvonplaten, @lhoestq
- tensorflow: @LysandreJik

Library:

- benchmarks: @patrickvonplaten
- deepspeed: @stas00
- ray/raytune: @richardliaw, @amogkam
- text generation: @patrickvonplaten
- tokenizers: @LysandreJik
- trainer: @sgugger
- pipelines: @LysandreJik

Documentation: @sgugger

Model hub:

- for issues with a model report at https://discuss.huggingface.co/ and tag the model's creator.

HF projects:

- nlp datasets: [different repo](https://github.com/huggingface/nlp)
- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)

Examples:

- maintained examples (not research project or legacy): @sgugger, @patil-suraj
- research_projects/bert-loses-patience: @JetRunner
- research_projects/distillation: @VictorSanh

 -->

## Information

Model I am using (Bert, XLNet ...): Bert

The problem arises when using:
* [x] the official example scripts: (give details below)
* [ ] my own modified scripts: (give details below)

The tasks I am working on is:
* [x] an official GLUE/SQUaD task: NER
* [ ] my own task or dataset: (give details below)

## To reproduce

The bug is for the PR #8016.

Steps to reproduce the behavior:

1. MlFlow installed and  the following env variables exported
```
export HF_MLFLOW_LOG_ARTIFACTS=TRUE
export MLFLOW_S3_ENDPOINT_URL=<custom endpont>
export MLFLOW_TRACKING_URI=<custom uri>
export MLFLOW_TRACKING_TOKEN=<custom token>
```
2. Run the token classification example with the following command
```
python run_ner.py \
  --model_name_or_path bert-base-uncased \
  --dataset_name conll2003 \
  --output_dir /tmp/test-ner \
  --do_train \
  --do_eval
```

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

## Expected behavior

<!-- A clear and concise description of what you would expect to happen. -->

When the training finishes, before the evaluation is performed, the `integrations.MLflowCallback` executes the method `on_train_end`, where if the env variable `HF_MLFLOW_LOG_ARTIFACTS` is set to `TRUE`, it logs the model artifacts to mlflow. 

The problem is, however, when the method `on_train_end` is called and the following line is executed: `self._ml_flow.log_artifacts(args.output_dir)`, the model is not stored on the `args.output_dir`. The model artefacts are stored once the `trainer.save_model()` is called, which is after the training ending. There is no callback in the `trainer.save_model()` that can be called from a `TrainerCallback` to save the model. There is a method `TrainierCallback.on_save()` method, that is called `trainer._maybe_log_save_evaluate()`, but even then the model is not available on the `output_dir`.

Possible solutions would be to extend the `TrainierCallback` with `on_model_save()` callback method, insert the callback in the `trainer.save_model()`. 
Or, a workaround I have now is to change `on_train_end ` with `on_evaluate` in `integrations.MLflowCallback`, that is called after the model is saved in the example script. However, this is not the right solution since it depends on having set the `do_eval` parameter, and it is not semantically correct.


",,1,,,,,,,,
2672,https://github.com/huggingface/transformers/issues/11283,11283,[],closed,2021-04-16 14:43:10+00:00,,12,Beam search decoding and language model integration for Wav2Vec2ForCTC models,"1. AFAIK, `Wav2Vec2ForCTCTokenizer.decode` method only provides greedy decoding. Is there a Beamsearch implementation for CTC available yet? 
2. Also, as it is a common norm in ASR modelling, language models are also generally added on top of the acoustic model. It would also be nice to have a possibility of appending a pretrained Language model which gets taken into consideration at the beamsearch decoding time. Not sure if there's an out-of-box solution implemented for that yet?

I'm also aware of efforts to integrate a language model in #10794 and have had a look at the notebook [here](https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb). Although it is a nice, simple way to integrate an LM, it is suboptimal when considering CTC semantics. A more appropriate approach would be the one described in [this](https://arxiv.org/pdf/1408.2873.pdf) paper and explained in [this](https://distill.pub/2017/ctc/) distilpub blog. Would be great to have these features added (if they are already not there and I somehow missed them).",,1,,,,,,,,
3094,https://github.com/huggingface/transformers/issues/12038,12038,[],closed,2021-06-05 13:30:06+00:00,,1,Support for pointer-generator architectures.,"# 馃殌 Feature request

Is there interest in adding pointer generator architecture support to huggingface? These are currently supported in [fairseq](https://github.com/pytorch/fairseq/blob/master/examples/pointer_generator/README.md), and in general should not be terrible to add for most encoder-decoder seq2seq tasks and modeks.

## Motivation

Pointer-generator architectures generally give SOTA results for extractive summarization, as well as for semantic parsing. (See for instance this paper [https://arxiv.org/pdf/2001.11458.pdf](url)).

## Your contribution
If there is interest but not bandwidth from huggingface members, I could try to add pointer-generator support for a specific architecture such as the T5 and see how hard it would be to port over fairseq's implementation for instance.

Note: apologies also if I've missed where huggingface supports it already.
",,1,,,,,,,,
351,https://github.com/huggingface/transformers/issues/6826,6826,"[{'id': 1314768611, 'node_id': 'MDU6TGFiZWwxMzE0NzY4NjEx', 'url': 'https://api.github.com/repos/huggingface/transformers/labels/wontfix', 'name': 'wontfix', 'color': 'ffffff', 'default': True, 'description': None}]",closed,2020-08-30 06:55:07+00:00,,1,Loading a converted pytorch model in huggingface transformers properly,"# 鉂?Questions & Help

<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,
     new models and benchmarks, and migration questions. For all other questions,
     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .
     You can also try Stack Overflow (SO) where a whole community of PyTorch and
     Tensorflow enthusiast can help you out. In this case, make sure to tag your
     question with the right deep learning framework as well as the
     huggingface-transformers tag: 
     https://stackoverflow.com/questions/tagged/huggingface-transformers 
     -->

## Details
<!-- Description of your issue -->
I converted a pre-trained tf model to pytorch using the following function.
```
def convert_tf_checkpoint_to_pytorch(*, tf_checkpoint_path, albert_config_file, pytorch_dump_path):
    # Initialise PyTorch model
    config = AlbertConfig.from_json_file(albert_config_file)
    print(""Building PyTorch model from configuration: {}"".format(str(config)))
    model = AlbertForPreTraining(config)

    # Load weights from tf checkpoint
    load_tf_weights_in_albert(model, config, tf_checkpoint_path)

    # Save pytorch-model
    print(""Save PyTorch model to {}"".format(pytorch_dump_path))
    torch.save(model.state_dict(), pytorch_dump_path)
```
I am loading the converted model and encoding sentences in the following way:
```
def vectorize_sentence(text):
    albert_tokenizer = AlbertTokenizer.from_pretrained(""albert-base-v2"")
    config = AlbertConfig.from_pretrained(config_path, output_hidden_states=True)
    model = TFAlbertModel.from_pretrained(pytorch_dir, config=config, from_pt=True)
    e = albert_tokenizer.encode(text, max_length=512)
    model_input = tf.constant(e)[None, :]  # Batch size 1
    output = model(model_input)

    v = [0] * 768
    # generate sentence vectors by averaging the word vectors
    for i in range(1, len(model_input[0]) - 1):
        v = v + output[0][0][i].numpy()

    vector = v/len(model_input[0])
    return vector
```
However while loading the model, a warning comes up: 

> Some weights or buffers of the PyTorch model TFAlbertModel were not
> initialized from the TF 2.0 model and are newly initialized:
> ['predictions.LayerNorm.bias', 'predictions.dense.weight',
> 'predictions.LayerNorm.weight', 'sop_classifier.classifier.bias',
> 'predictions.dense.bias', 'sop_classifier.classifier.weight',
> 'predictions.decoder.bias', 'predictions.bias',
> 'predictions.decoder.weight'] You should probably TRAIN this model on
> a down-stream task to be able to use it for predictions and inference.


Can anyone tell me if I am doing anything wrong? What does the warning mean? I saw issue #5588. Don't know if my issue is the same as this.

<!-- You should first ask your question on the forum or SO, and only if
     you didn't get an answer ask it here on GitHub. -->
**A link to original question on the forum/Stack Overflow**:
https://stackoverflow.com/questions/63648380/loading-a-converted-pytorch-model-in-huggingface-transformers-properly",1,,,,,,,,,
