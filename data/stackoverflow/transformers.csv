Id,PostTypeId,AcceptedAnswerId,ParentId,CreationDate,DeletionDate,Score,ViewCount,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditorDisplayName,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate,CommunityOwnedDate,ContentLicense
"62869640","1","","","2020-07-13 05:20:22","","1","2311","<p>I want to do chinese Textual Similarity with huggingface:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = TFBertForSequenceClassification.from_pretrained('bert-base-chinese')
</code></pre>
<p>It doesn't work, system report errors:</p>
<pre><code>Some weights of the model checkpoint at bert-base-chinese were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier', 'dropout_37']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>But I can use huggingface to do name entity:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = TFBertForTokenClassification.from_pretrained(&quot;bert-base-chinese&quot;)
</code></pre>
<p>Does that mean huggingface haven't done chinese sequenceclassification? If my judge is right, how to sove this problem with colab with only 12G memoryï¼Ÿ</p>
","7241796","","","","","2021-08-29 20:14:38","why TFBertForSequenceClassification.from_pretrained('bert-base-chinese') can't use?","<huggingface-transformers>","1","3","1","","","CC BY-SA 4.0"
"62882030","1","","","2020-07-13 18:27:08","","1","747","<p>I am trying to do named entity recognition in Python using BERT, and installed transformers v 3.0.2 from huggingface using <code>pip install transformers</code>
. Then when I try to run this code:</p>
<pre><code>import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertConfig

from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

torch.__version__
</code></pre>
<p>I get this error:</p>
<pre><code>ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tokenizers/tokenizers.cpython-38-darwin.so, 2): Symbol not found: ____chkstk_darwin
  Referenced from: /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tokenizers/tokenizers.cpython-38-darwin.so (which was built for Mac OS X 10.15)
  Expected in: /usr/lib/libSystem.B.dylib
 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tokenizers/tokenizers.cpython-38-darwin.so
</code></pre>
<p>The error occurs in this line: <code>from transformers import BertTokenizer, BertConfig</code> but I'm not sure how to fix this.</p>
","8232711","","6664872","","2020-07-14 08:25:06","2020-07-29 20:06:40","Python ImportError: from transformers import BertTokenizer, BertConfig","<python><python-import><importerror><huggingface-transformers><huggingface-tokenizers>","1","1","1","","","CC BY-SA 4.0"
"68800382","1","","","2021-08-16 09:30:42","","0","24","<p>I've been trying to get distilbert to work and I've downloaded the model and used AutoTokenizer.from_pretrained() and AutoModelForSequenceClassification.from_pretrained(). I tried for a couple days now to pass the parameters from the &quot;Possible class names&quot; on the huggingface model card page: <a href=""https://huggingface.co/typeform/distilbert-base-uncased-mnli?candidateLabels=positive%2C+negative%2C+neutral&amp;multiClass=true&amp;text=which+stocks+will+go+down+during+new+years"" rel=""nofollow noreferrer"">https://huggingface.co/typeform/distilbert-base-uncased-mnli?candidateLabels=positive%2C+negative%2C+neutral&amp;multiClass=true&amp;text=which+stocks+will+go+down+during+new+years</a></p>
<p>I tried:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained('.')
model = AutoModelForSequenceClassification.from_pretrained('.')

text = &quot;Dummy text&quot;
text += &quot;[SEP]Positive[SEP]Neutral[SEP]Negative&quot;
encodedInput = tokenizer(text, return_tensors=&quot;pt&quot;)
output = model(**encodedInput)
print(output)
</code></pre>
<p>It's supposed to output entailment values for &quot;Positive&quot;, &quot;Neutral&quot;, and &quot;Negative&quot;.
Anyone know how to do it? I'm using pytorch.</p>
","12522233","","","","","2021-08-16 16:41:36","How to pass possible class names to distilbert","<python><machine-learning><pytorch><huggingface-transformers><transformer>","1","0","","","","CC BY-SA 4.0"
"68794998","1","","","2021-08-15 20:06:12","","0","8","<p>I have just trained my own tokenizer from scratch, which is a Word Piece model like BERT, and I have saved it.</p>
<p>From there, I am now wanting to train my own language model from scratch using the tokenizer I trained beforehand.</p>
<p>However, referring to the code below, what do I change my <code>model_checkpoint</code> to?</p>
<pre><code>model_checkpoint = &quot;gpt2&quot;
tokenizer_checkpoint = &quot;drive/wordpiece-like-tokenizer&quot;
</code></pre>
<p>I trained a Word Piece model like BERT, so should <code>gpt2</code> be changed to something else?</p>
<p>Thanks.</p>
","16098918","","","","","2021-08-15 20:06:12","What model checkpoint do I use if I trained a Word Piece tokenizer?","<python><machine-learning><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"68806265","1","","","2021-08-16 16:27:34","","0","30","<p>I'm following this tutorial to train some models:</p>
<p><a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/training.html</a></p>
<p>I'd like to track not only the evaluation loss and accuracy but also the train loss and accuracy, to monitor overfitting. While running the code in Jupyter, I do see all of htis:</p>
<pre><code>Epoch   Training Loss   Validation Loss Accuracy    Glue
1   0.096500    0.928782    {'accuracy': 0.625} {'accuracy': 0.625, 'f1': 0.0}
2   0.096500    1.203832    {'accuracy': 0.625} {'accuracy': 0.625, 'f1': 0.0}
3   0.096500    1.643788    {'accuracy': 0.625} {'accuracy': 0.625, 'f1': 0.0}
</code></pre>
<p>but when I go into trainer.state.log_history, that stuff is not there. This really doesn't make sense to me.</p>
<pre><code>for obj in trainer.state.log_history:
    print(obj)

{'loss': 0.0965, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25, 'step': 1}
{'eval_loss': 0.9287818074226379, 'eval_accuracy': {'accuracy': 0.625}, 'eval_glue': {'accuracy': 0.625, 'f1': 0.0}, 'eval_runtime': 1.3266, 'eval_samples_per_second': 6.03, 'eval_steps_per_second': 0.754, 'epoch': 1.0, 'step': 4}
{'eval_loss': 1.2038320302963257, 'eval_accuracy': {'accuracy': 0.625}, 'eval_glue': {'accuracy': 0.625, 'f1': 0.0}, 'eval_runtime': 1.3187, 'eval_samples_per_second': 6.067, 'eval_steps_per_second': 0.758, 'epoch': 2.0, 'step': 8}
{'eval_loss': 1.6437877416610718, 'eval_accuracy': {'accuracy': 0.625}, 'eval_glue': {'accuracy': 0.625, 'f1': 0.0}, 'eval_runtime': 1.3931, 'eval_samples_per_second': 5.742, 'eval_steps_per_second': 0.718, 'epoch': 3.0, 'step': 12}
{'train_runtime': 20.9407, 'train_samples_per_second': 1.146, 'train_steps_per_second': 0.573, 'total_flos': 6314665328640.0, 'train_loss': 0.07855576276779175, 'epoch': 3.0, 'step': 12}
</code></pre>
<p>How do I get these back in an object, and not a printout?</p>
<p>Thanks</p>
<p>Edit: Reproducable code below:</p>
<pre><code>import numpy as np
from datasets import load_metric, load_dataset
from transformers import TrainingArguments, AutoModelForSequenceClassification, Trainer, AutoTokenizer
from datasets import list_metrics

raw_datasets = load_dataset(&quot;imdb&quot;)

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

small_train_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42).select(range(8))
small_eval_dataset = tokenized_datasets[&quot;test&quot;].shuffle(seed=42).select(range(8))
model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=2)
    
training_args = TrainingArguments(&quot;IntroToBERT&quot;, evaluation_strategy=&quot;epoch&quot;)
training_args.logging_strategy = 'step'
training_args.logging_first_step = True
training_args.logging_steps = 1
training_args.num_train_epochs = 3
training_args.per_device_train_batch_size = 2
training_args.eval_steps = 1

metrics = {}
for metric in ['accuracy','glue']:
    metrics[metric] = load_metric(metric,'mrpc')


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    out = {}
    for metric in metrics.keys():
        out[metric] = metrics[metric].compute(predictions=predictions, references=labels)
    return out

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train() 

# here the printout is as shown

for obj in trainer.state.log_history:
    print(obj)

# here the logging data is displayed
</code></pre>
","4493798","","4493798","","2021-08-16 17:41:19","2021-08-16 17:41:19","HuggingFace Trainer logging train data","<pytorch><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"62936905","1","","","2020-07-16 14:23:58","","1","289","<p>I'm using the HuggingFace <code>facebook/bart-large-cnn</code> pretrained model for text summarization via <code>AutoModel</code> and <code>AutoTokenizer</code>. Both model and tokenizers loads fine:</p>
<pre><code>import os
import torch
from transformers import AutoTokenizer, AutoModel

torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'

tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/bart-large-cnn&quot;,
                                          cache_dir=os.getenv(&quot;cache_dir&quot;, &quot;model&quot;))

model = AutoModel.from_pretrained(&quot;facebook/bart-large-cnn&quot;,
                                  cache_dir=os.getenv(&quot;cache_dir&quot;, &quot;model&quot;)).to(torch_device)

FRANCE_ARTICLE = ' Marseille...'  # @noqa

dct = tokenizer.batch_encode_plus(
    [FRANCE_ARTICLE],
    max_length=1024,
    padding=&quot;max_length&quot;,
    truncation=True,
    return_tensors=&quot;pt&quot;,
)

max_length = 140
min_length = 55

hypotheses_batch = model.generate(
    input_ids=dct[&quot;input_ids&quot;].to(torch_device),
    attention_mask=dct[&quot;attention_mask&quot;].to(torch_device),
    num_beams=4,
    length_penalty=2.0,
    max_length=max_length + 2,
    min_length=min_length + 1,
    no_repeat_ngram_size=3,
    do_sample=False,
    early_stopping=True,
    decoder_start_token_id=model.config.eos_token_id,
)

decoded = [
    tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in hypotheses_batch
]

print(decoded)
</code></pre>
<p>But I get this error when calling the decode on the tokenizer in <code>tokenizer.batch_encode_plus</code>:</p>
<pre><code>Traceback (most recent call last):
  File &quot;src/summarization/run.py&quot;, line 42, in &lt;module&gt;
    summary_ids = model.generate(article_input_ids,num_beams=4,length_penalty=2.0,max_length=142,min_length=56,no_repeat_ngram_size=3)
  File &quot;/usr/local/lib/python3.7/site-packages/torch/autograd/grad_mode.py&quot;, line 15, in decorate_context
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/site-packages/transformers/generation_utils.py&quot;, line 379, in generate
    assert hasattr(self, &quot;get_encoder&quot;), &quot;{} should have a 'get_encoder' function defined&quot;.format(self)
AssertionError: BartModel(
  (shared): Embedding(50264, 1024, padding_idx=1)
  (encoder): BartEncoder(
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layers): ModuleList(
      (0): EncoderLayer(
...
      )
    )
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
) should have a 'get_encoder' function defined
</code></pre>
","758836","","","","","2020-07-16 14:23:58","HuggingFace BartModel should have a 'get_encoder' function defined","<python><huggingface-transformers><huggingface-tokenizers>","0","0","1","","","CC BY-SA 4.0"
"68803994","1","","","2021-08-16 13:53:02","","0","15","<h2>Goal</h2>
<ul>
<li>Form many survey answers getting a summary = (average answer of customers) without reading thousands of answers</li>
</ul>
<h2>ERROR</h2>
<ul>
<li>When I am running it on a Wikipedia article ex.: Assassins' creed just the History <a href=""https://en.wikipedia.org/wiki/Assassin%27s_Creed#History"" rel=""nofollow noreferrer"">section</a> It gives back reasonable results. No special characters and  understandable statements</li>
<li>My dataset sometime people just writhe &quot;hjkfahfbtw&quot; as answer, not full sentences, So I closed down all answers if it was not closed down with a dot.</li>
<li>My answers are about 100'000 string long for each question. I haven't cleaned or done anything with them I just copied them to ''' in between 2x3 coma as a string''' and made it equal with a variable</li>
</ul>
<h2>Question</h2>
<ul>
<li>How do I get Longer +512 String long output text without this many special character results just normal English text?</li>
<li>The many special character appears mostly after 512 characters. So probably when input set above <code>max_length=512</code> than it appears mostly (but I am not sure)? But the goal would be to get longer summaries like 1000 stings.</li>
<li>I don't just want to skip too long sentences with <code>truncation=True</code> in <code>summarizer(text5, min_length=800, max_length=1000, truncation=True)</code>. A previosue error was solved by this but (that previouse error: &quot;Token indices sequence length is longer than the specified maximum sequence length for this model (4746 &gt; 512). Running this sequence through the model will result in indexing errors&quot;)</li>
</ul>
<h2>1. CODE [Assassin History Chapter]</h2>
<pre><code>import tensorflow as tf # VERSION CPU: 2.3.1
# transformers VESRION: 4.9.2
from transformers import pipeline

summarizer = pipeline(&quot;summarization&quot;, model=&quot;t5-base&quot;, tokenizer=&quot;t5-base&quot;, framework=&quot;tf&quot;,) 

text = &quot;The Assassin's Creed series...WHOLE HISTORY PART OFTHE WIKIPEDIA ARTICLE...Auto Online.&quot;

summarizer(text, min_length=25, max_length=200)

summarizer(text, min_length=800, max_length=1000)
</code></pre>
<h2>1. OUTPUT [Assassin History Chapter]</h2>
<p><code>summarizer(text, min_length=25, max_length=200)</code></p>
<p>[{'summary_text': &quot;the Assassin's Creed series originated out of ideas for a sequel for Prince of Persia: The Sands of Time, which was made for PlayStation 2, GameCube, Xbox and other platforms . the team decided on taking the gameplay from the original game into an open world approach, taking advantage of the improved processing power to render larger spaces and crowds . after Syndicate, the team realized that the series needed a major re-invention across both gameplay and narrative .&quot;}]</p>
<p><code>summarizer(text, min_length=800, max_length=1000)</code></p>
<p>[{'summary_text': 'the Assassin's Creed series originated out of ideas for a sequel for Prince of Persia: The Sands of Time, which was made for PlayStation 2, GameCube, Xbox and other platforms . the team decided on taking the gameplay from the original game into an open world approach, taking advantage of the improved processing power to render larger spaces and crowds . after release of the first game in 2007, the team realized that the series needed a major re-invention across both gameplay and narrative . this led to . ..  . also . (. - .-  .   . n ., - n s - n. ,   s  --  \xad\xad\xad \xad -\xad\xad- \xad-\xad .\xad\xadn\xad\xads\xad\xad. \xadâ€”\xad\xad...\xad\xad[n\xad s\xad n\xads-\xads \xad  '- -\xad\xad\xad_\xad\xad[[[\xad[[]][[<em>[[[\xad\xad][\xad</em>[\xad]\xad__\xad[<strong><em>[</em>\xad_â€”[[-[[.[._[.\xad[.â€”[.-</strong>â€”â€”â€”\xadâ€”â€”<em>\xadâ€”</em>â€”\xad-â€”â€” '\xad\xadâ€”â€“\xad\xadâ€“â€”\xadâ€“\xadâ€“â€“\xadâ€” \xadâ€”\xadâ€”\xad_\xad-â€”\xad_\xad_ <strong><em>-\xad â€”â€” â€” â€”\xad â€”â€”â€“ â€”-\xadâ€”- â€”â€“â€“ â€“ 'â€” -â€“ -â€” â€“ â€“- â€“\xad â€“â€” \xadâ€“ â€™ â€” â€˜\xad <em>\xad â€™\xad ' '</em></em> _ . â€˜\xad\xad, . Â« .â€” . [\xadâ€” â€˜</strong>_____________________ <em>â€” <strong>= . * ________________ â€” [</strong> *</em>_ = = . = = === == ' == = '=== * * * . + = = * * + == * = = + =  == + =  * *  + &amp; &amp; + _ + , = _ = ,= , * , + .= _&amp; _ * &amp; *  _= = * s = &amp;= &amp; = a = ; = ! = : .&amp; ./ . and .* . 1 . 1. . 2. . 2 . 4. . 3. . 3 . 5. . 4 . 5 .; .:. ; &amp;. &amp;&amp; ,... and. _. ... &amp; ( &amp;, &amp;; _ [ &amp; [ _ ( .) .( .â€™ .s. '&amp; - ( -&amp; '. a .' . &quot; .&quot; . $ .# . # . @ .__&amp;..&amp;.&amp;&amp;&amp;# &amp;## '###&amp;&amp; ### #&amp;# ##&amp;#&amp; #&amp;&amp; *&amp; * <em>&amp;&amp;</em> &amp; # #&amp; # ' #&amp; *## * ' <em>&amp;#</em> ' ( '''}]</p>
<h2>2. CODE [MY SURVEY DATA]</h2>
<pre><code>import tensorflow as tf # VERSION CPU: 2.3.1
from transformers import pipeline

summarizer = pipeline(&quot;summarization&quot;, model=&quot;t5-base&quot;, tokenizer=&quot;t5-base&quot;, framework=&quot;tf&quot;,) 

survey_data = &quot;&quot;&quot;
...ALL THE TEXT HERE IN 1 SINGLE LINE LIKE 100'000 STRING...
&quot;&quot;&quot;

summarizer(survey_data, min_length=400, max_length=500)
</code></pre>
<h2>2. OUTPUT [MY SURVEY DATA]</h2>
<p>Example without <code>truncation=True</code></p>
<p>[{'summary_text': 'i'm not a huge fan of cup cakes, but i do enjoy them . i would like to see more of what you're doing on this site . I'd be interested in a lot more of the stuff you've been selling . it's a great place to start if you can't find a sweet cake. you'll be able to buy more of your products if they're available . but you'd have to be careful what you are doing . n - . &quot; - - n an - &quot; s s ' . &quot; -. -- .- '- -n ' &quot; '&quot; nn '' '. 'n n' -&quot; .. .&quot; / &quot;&quot;&quot;&quot; -/ &quot; &quot;&quot; &quot;&quot; &quot; &quot; / '/&quot; s&quot;&quot; &quot; &quot;n&quot; &quot; ./ ....&quot; .' n/ &quot; n. /// /&quot; &quot;/&quot;&quot;- s' &quot;&quot;- &quot; &quot; &quot;- &quot;- &amp; /'&quot;&quot;/ &quot;' &amp;' &quot;' &quot; &quot;'&quot; &quot;n &amp;/&quot;' &quot;n &quot; &amp; &quot; i&quot; &quot;'' &quot;/ &amp;&quot; i '&amp; '( &quot; e /- &quot;&quot;/ n&quot; t &quot; t /. &quot; &quot;//. ? &quot; sn&quot;// &quot;n/ i/ &quot;/ &quot;-/ s/ -, &quot; a /( / ( /n// (//(//-//'//)//&quot;/ ( &quot;/((/( &quot;(((&quot;/('/ &quot;(/&quot; (/&quot;(/'(/.//?&quot;/''/' &quot;( &quot;/../.)&quot;/.&quot;&quot; (&quot; &quot;(. &quot;/ ((('(( ((/)&quot; &quot;[[(([[['[(['([('[[.''('''[''))''-''&amp;''&quot;(( '[/ &quot;[(/ (''&quot;)))&quot;&quot;( &quot;[' &quot;[. &quot;[&amp;/.'&quot; (( &quot; &quot;[] &quot;( .) &quot; &quot;(&quot; &quot;-&quot; &quot;) &quot;()&quot; '&quot; &quot;&amp; .(&quot; &amp;(() . (( &amp;) 's'' (() &quot; ( ( ( ) s((- &quot; ' (( ') &quot; <em>((&amp; &quot;( ( (( ( ) &quot;'( )&quot; ( ( &quot;(- &quot;(&quot;) &quot;(')&quot;(&quot;( &quot;&quot;() ((&quot;&quot;'()((&quot;)&quot;('&quot;[&quot;([&quot;[[&quot;&quot;[(&quot;['&quot;'[&amp;'[&quot;''''</em>'' [((.'( ('(&amp;)'(-'['(.)) &quot;[('. &quot;([/()&quot;)&quot;&quot; (&quot;((n''s) &quot;&quot;[ &quot; &quot;&amp; &quot;&quot; ( &quot; &quot;&quot;) &quot; &quot;'}]</p>
<p>An example with <code>summarizer(text, min_length=400, max_length=500, truncation=True)</code></p>
<p>[{'summary_text': &quot;i'm not the target audience for non-sugar based products . it's difficult to compete with foodchain retailers already established and constantly giving away or making 1c on ice cream. i would like a more cheap option for cakes and I know that it is hard but some more cheap version of cummies . I HIGHLY recommend a dedicated effort towards artists and indie developers . so things like muffines and other sweets and bigger products as well .. . ( .s \xad\xad\xad \xad .\xad\xad. \xadn\xad\xad-\xad\xad[\xad\xad\xad\xad*\xad\xad_\xad\xad&amp;\xad\xads\xad\xadâ€“\xad\xadÂ»\xad\xad?\xad\xadâ€”\xad\xad...\xad\xad,\xad -\xad n\xad \xad  \xad__\xad <em>\xad</em> \xad.\xad s\xad &amp;\xad ,\xad\xadn-\xad \xad-\xadâ€“ \xad- \xadâ€“--\xad---<em>\xad-</em><strong>-\xadâ€”</strong>â€”\xad_â€”<em>\xadâ€”â€”â€”\xadâ€”\xadâ€” â€”\xadâ€“â€”\xad â€“\xadâ€”â€“\xadâ€“â€“â€“\xad</em>â€“\xad â€” â€“â€“ â€“ â€˜\xad\xadâ€™\xad\xad â€˜\xadâ€“ â€˜â€˜\xadâ€“â€˜\xad\xadâ€˜\xadâ€”â€˜â€“\xadâ€“â€“â€˜â€“â€“ â€˜â€“â€“â€™â€“â€“-â€“â€“â€”â€“â€“\xadâ€“â€“\xad-â€“\xadâ€™â€“\xadâ€˜â€˜â€“â€˜â€™â€“â€˜â€˜â€˜â€™â€™â€“â€™\xadâ€“â€™â€˜â€“â€™â€”â€“â€˜â€”â€“â€”â€”â€“-â€”â€“â€™-â€“â€˜-â€“â€”-â€“-â€™â€“-â€˜â€“- â€˜â€˜â€“ â€™â€“ â€˜â€™â€“ â€”â€“ .â€“â€“...â€“â€“ (&quot;}]</p>
","10270590","","10270590","","2021-08-16 14:47:04","2021-08-16 14:47:04","t5-base Survey data text summary analytics - How to go above 512 output characters without many special characters in the output text","<python><tensorflow><nlp><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68829277","1","68834507","","2021-08-18 08:51:46","","0","19","<p>I try to do transfer learning using a <code>HuggingFace</code> pretrained BERT model. I want to use tensorflow API with it. I do not understand why the last line produces an error</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel

model_name = &quot;distilbert-base-uncased&quot;
text = &quot;this is a test&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)    
text_tensor = tokenizer.encode(text, return_tensors=&quot;tf&quot;)

model = AutoModel.from_pretrained(model_name).to(&quot;cuda&quot;)
output = model(text_tensor) # ERROR!!, but why?
</code></pre>
","4383566","","","","","2021-08-18 14:39:37","Transfer Learning using HuggingFace and Tensorflow with AutoModel does not work","<tensorflow2.0><bert-language-model><huggingface-transformers><transfer-learning>","1","1","","","","CC BY-SA 4.0"
"62863374","1","","","2020-07-12 16:03:53","","2","893","<p>I am fine-tuning a BERT model from huggingface transformers for Named Entity Recognition Task. The input to the model is a single word and output is a tag of that word. I have created a custom generator function (data_generator) from where I am getting data while training. I have freezed the bert layer in training mode and added some layers on top of it to predict the tag of the given word.</p>
<p>The code is this:</p>
<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.layers import Input, Dense, Activation, Dropout, LSTM, GlobalMaxPool1D
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences

from transformers import BertTokenizer, TFBertModel, BertConfig

# Load the BERT tokenizer.
print('Loading BERT tokenizer...')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

bert = 'bert-base-uncased'

config = BertConfig(dropout=0.2, attention_dropout=0.2)
config.output_hidden_states = False
transformer_model = TFBertModel.from_pretrained(bert, config = config)

input_ids_in = Input(shape=(max_len,), name='input_token', dtype='int32')
input_masks_in = Input(shape=(max_len,), name='masked_token', dtype='int32')
embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]

X = LSTM(50, return_sequences=True)(embedding_layer)
X = GlobalMaxPool1D()(X)
X = Dense(50, activation='relu')(X)
X = Dropout(0.2)(X)
X = Dense(num_labels, activation='softmax')(X)

model = Model(inputs=[input_ids_in, input_masks_in], outputs = X)

for layer in model.layers[:3]:
    layer.trainable = False

model.compile(loss='categorical_crossentropy', optimizer='adam')

train_gen = data_generator(sentences, tags, tag2ix, max_len, number_sent_per_batch)
model.fit(train_gen, epochs=1, steps_per_epoch=steps, verbose=1)

</code></pre>
<p>The generator function is this :</p>
<pre class=""lang-py prettyprint-override""><code># data generator, intended to be used in a call to model.fit_generator()

def data_generator(sentences, tags, tag2ix, max_len, num_samples_per_batch):
    
  X1, X2, y = list(), list(), list()
  n=0
    
  while 1:
    for i in range(len(sentences)):
      n += 1
        
      encoded_dict = tokenizer.encode_plus(
        
          sentences[i],                      # Sentence to encode.
          add_special_tokens = True, # Add '[CLS]' and '[SEP]'
          max_length = max_len,           # Pad &amp; truncate all sentences.
          pad_to_max_length = True,
          return_attention_mask = True,   # Construct attn. masks.
          #return_tensors = 'tf',     
          truncation=True
          )
        
      for idx in encoded_dict['input_ids']:
                    
        #idx = [encoded_dict['input_ids'][j]]
        # Add the encoded sentence to the list.
        idx = pad_sequences([[idx]], maxlen=max_len, padding='post')[0]
        X1.append(idx)

      for att_mask in encoded_dict['attention_mask']:
            
        #att_mask = [encoded_dict['attention_mask'][j]]
        # And its attention mask (simply differentiates padding from non-padding).
        att_mask = pad_sequences([[att_mask]], maxlen=max_len, padding='post')[0]
        X2.append(att_mask)
      
      for k in tags[i]:
            
        out = to_categorical([tag2ix[k]], num_classes=num_labels)[0]
        y.append(out)
            
      if n == num_samples_per_batch:
                    
        yield [[array(X1), array(X2)], array(y)]
        X1, X2, y = list(), list(), list()
        n=0
            
</code></pre>
<p>The error I am getting is this :</p>
<pre><code>--------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-64-a19b18bb0230&gt; in &lt;module&gt;()
----&gt; 1 model.fit(train_gen, epochs=1, steps_per_epoch=steps, verbose=1)
      2 #, validation_data=val_gen, validation_steps=val_steps

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:541 train_step  **
        self.trainable_variables)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1804 _minimize
        trainable_variables))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:521 _aggregate_gradients
        filtered_grads_and_vars = _filter_grads(grads_and_vars)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1219 _filter_grads
        ([v.name for _, v in grads_and_vars],))

    ValueError: No gradients provided for any variable: ['lstm_2/lstm_cell_2/kernel:0', 'lstm_2/lstm_cell_2/recurrent_kernel:0', 'lstm_2/lstm_cell_2/bias:0', 'dense_8/kernel:0', 'dense_8/bias:0', 'dense_9/kernel:0', 'dense_9/bias:0'].
</code></pre>
<p>I have gone through many links like :</p>
<p><a href=""https://github.com/tensorflow/tensorflow/issues/1511"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/1511</a></p>
<p><a href=""https://github.com/tensorflow/tensorflow/issues/27949"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/27949</a></p>
<p>and many more.</p>
<p>There are many solutions provided in these github issues but couldn't find the solution of my error. I think mine is an error in the code not in the tensorflow library. That's why I don't know where I am getting wrong. Please help. Thanks in advance.</p>
<p>And I also want to know what is the reason of this error, as I have seen people getting same error on different problems (see above links). I am a beginner in tensorflow, so if you know this please let me know.</p>
","10432635","","10432635","","2020-07-13 20:16:57","2020-07-13 20:16:57","'ValueError : No gradients provided for any variable' in Tensorflow 2.2.0","<python><tensorflow><keras><deep-learning><huggingface-transformers>","0","6","","","","CC BY-SA 4.0"
"68787955","1","","","2021-08-15 00:36:24","","0","63","<p>I try to use the <code>Trainer</code> function from HuggingFace's transformers to train the model and use <code>ray tune</code> for hyperparameters searching. I can run the code in Google Colab without the error, but when I use a remote machine to run the code via ssh, It gives an error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;main.py&quot;, line 130, in &lt;module&gt;
    backend=&quot;ray&quot;)
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/transformers/trainer.py&quot;, line 1668, in hyperparameter_search
    best_run = run_hp_search(self, n_trials, direction, **kwargs)
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/transformers/integrations.py&quot;, line 236, in run_hp_search_ray
    **kwargs,
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/tune/tune.py&quot;, line 297, in run
    _ray_auto_init()
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/tune/tune.py&quot;, line 670, in _ray_auto_init
    ray.init()
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/_private/client_mode_hook.py&quot;, line 82, in wrapper
    return func(*args, **kwargs)
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/worker.py&quot;, line 940, in init
    hook()
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/tune/registry.py&quot;, line 197, in flush
    self.references[k] = ray.put(v)
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/_private/client_mode_hook.py&quot;, line 82, in wrapper
    return func(*args, **kwargs)
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/worker.py&quot;, line 1597, in put
    object_ref = worker.put_object(value)
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/worker.py&quot;, line 287, in put_object
    serialized_value = self.get_serialization_context().serialize(value)
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/serialization.py&quot;, line 331, in serialize
    return self._serialize_to_msgpack(value)
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/serialization.py&quot;, line 311, in _serialize_to_msgpack
    self._serialize_to_pickle5(metadata, python_objects)
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/serialization.py&quot;, line 271, in _serialize_to_pickle5
    raise e
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/serialization.py&quot;, line 268, in _serialize_to_pickle5
    value, protocol=5, buffer_callback=writer.buffer_callback)
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/cloudpickle/cloudpickle_fast.py&quot;, line 73, in dumps
    cp.dump(obj)
  File &quot;/cs/research/external/home/zhengzho/.local/lib/python3.6/site-packages/ray/cloudpickle/cloudpickle_fast.py&quot;, line 580, in dump
    return Pickler.dump(self, obj)
TypeError: can't pickle _thread.RLock objects
</code></pre>
<p>Thanks a lot!</p>
","15236206","","","","","2021-08-15 00:50:23","""can't pickle _thread.RLock objects"" when using HuggingFace Trainer with Ray Tune","<python><huggingface-transformers><ray>","1","0","","","","CC BY-SA 4.0"
"68871329","1","","","2021-08-21 08:21:22","","0","33","<p>I am training the binary classfier using BERT model implement in hugging face library</p>
<pre><code>training_args = TrainingArguments(
   &quot;deleted_tweets_trainer&quot;,                  
   num_train_epochs = 1,            
   #logging_steps=100,    
   evaluation_strategy='steps',       
   remove_unused_columns = True    
)
</code></pre>
<p>I am using Colab TPU still the training time is a lot, 38 hours for 60 hours cleaned tweets.</p>
<p>Is there any way to optimise the training?</p>
","14562845","","","","","2021-08-23 05:39:33","Slow training of BERT model Hugging face","<python><google-colaboratory><bert-language-model><huggingface-transformers><tpu>","1","0","","","","CC BY-SA 4.0"
"62935940","1","","","2020-07-16 13:32:45","","2","424","<p>In general, I need to run DistilBERT in a browser. At first, I converted DistilBERT from huggingface to TensorFlow .pb format. However, I do not understand how to inference it.</p>
<p>Conversion code:</p>
<pre><code>from transformers import TFAutoModel, AutoTokenizer

model = TFAutoModel.from_pretrained('distilbert-base-uncased')
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
dir = &quot;distilbert_savedmodel&quot;
model._set_inputs(tf.TensorSpec([1, 384], tf.int32))
tf.saved_model.save(model, dir)
</code></pre>
<p>Inference code:</p>
<pre><code>encoded = tokenizer.encode('Hello, world!', add_special_tokens=True, return_tensors=&quot;tf&quot;)
model = tf.keras.models.load_model(dir)
model(encoded)
</code></pre>
<p>The error:</p>
<pre><code>ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (1 total):
    * Tensor(&quot;inputs:0&quot;, shape=(1, 1, 6), dtype=int32)
  Keyword arguments: {'training': False}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (1 total):
    * {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids')}
  Keyword arguments: {'training': False}

Option 2:
  Positional arguments (1 total):
    * {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='inputs/input_ids')}
  Keyword arguments: {'training': True}

Option 3:
  Positional arguments (1 total):
    * {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='inputs/input_ids')}
  Keyword arguments: {'training': False}

Option 4:
  Positional arguments (1 total):
    * {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids')}
  Keyword arguments: {'training': True}
</code></pre>
<p>Notebook link: <a href=""https://colab.research.google.com/drive/1otfNIYv8DRo2OZ0D2IpdoywrL0pN9k0I?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1otfNIYv8DRo2OZ0D2IpdoywrL0pN9k0I?usp=sharing</a></p>
<p>P. S. I am a novice at TensorFlow.</p>
","9181499","","","","","2021-01-17 14:08:53","How to inference .pb model converted from huggingface?","<tensorflow><tensorflow2.0><tensorflow.js><huggingface-transformers>","1","2","1","","","CC BY-SA 4.0"
"66998668","1","","","2021-04-08 06:50:12","","2","440","<p>As you see in the following python console, I can import <code>T5Tokenizer</code> from <code>transformers</code>. However, for <code>simpletransformers.t5</code> I get an error:</p>
<pre><code>&gt;&gt;&gt; from transformers import T5Model, T5Tokenizer
&gt;&gt;&gt; from simpletransformers.t5 import T5Model, T5Args                                       
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/simpletransformers/t5/__init$
_.py&quot;, line 2, in &lt;module&gt;
    from simpletransformers.t5.t5_model import T5Model
  File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/simpletransformers/t5/t5_mod$
l.py&quot;, line 20, in &lt;module&gt;
    from transformers.models.t5 import T5Config, T5ForConditionalGeneration, T5Tokenizer
ImportError: cannot import name 'T5Tokenizer' from 'transformers.models.t5' (unknown locatio
n)
</code></pre>
","2651073","","","","","2021-07-28 11:28:59","cannot import name 'T5Tokenizer' from 'transformers.models.t5'","<python><huggingface-transformers><simpletransformers>","1","2","","","","CC BY-SA 4.0"
"68875496","1","69153519","","2021-08-21 17:54:13","","2","80","<p>I am working on Fine-Tuning Pretrained Model on custom (using HuggingFace) dataset I will copy all code correctly from the one youtube video everything is ok but in this cell/code:</p>
<pre><code>with training_args.strategy.scope():
    model=TFDistilBertForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)

    trainer = TFTrainer(model=model,     # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=test_dataset)           # evaluation dataset


    trainer.train()
</code></pre>
<p>It will give me this error:</p>
<pre><code>TypeError: '&gt;' not supported between instances of 'NoneType' and 'int'
</code></pre>
","16620010","","10219955","","2021-08-25 04:45:21","2021-09-12 16:55:42","HUGGINGFACE TypeError: '>' not supported between instances of 'NoneType' and 'int'","<deep-learning><data-science><huggingface-transformers><huggingface-tokenizers><huggingface-datasets>","1","1","","","","CC BY-SA 4.0"
"62863859","1","","","2020-07-12 16:49:13","","2","978","<p>Why this works in google colab but doesn't work on docker?</p>
<p>So this is my Dockerfile.</p>
<pre><code>FROM python:3.7
RUN pip install -q transformers tensorflow 
RUN pip install ipython
ENTRYPOINT [&quot;/bin/bash&quot;]
</code></pre>
<p>And I'm executing this.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import *
nlp = pipeline(
    'question-answering', 
    model='mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',
    tokenizer=(
        'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',  
        {&quot;use_fast&quot;: False}
    )
)
</code></pre>
<p>But I get this error</p>
<pre><code>   ...:                                                                                                                                                                             
Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [00:00&lt;00:00, 325kB/s]
Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 242k/242k [00:00&lt;00:00, 796kB/s]
Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00&lt;00:00, 70.1kB/s]
Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:00&lt;00:00, 99.6kB/s]
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
/usr/local/lib/python3.7/site-packages/transformers/modeling_tf_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    461                 if resolved_archive_file is None:
--&gt; 462                     raise EnvironmentError
    463             except EnvironmentError:

OSError: 

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
&lt;ipython-input-1-1f9fed95967a&gt; in &lt;module&gt;
      5     tokenizer=(
      6         'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',
----&gt; 7         {&quot;use_fast&quot;: False}
      8     )
      9 )

/usr/local/lib/python3.7/site-packages/transformers/pipelines.py in pipeline(task, model, config, tokenizer, framework, **kwargs)
   1882                 &quot;Trying to load the model with Tensorflow.&quot;
   1883             )
-&gt; 1884         model = model_class.from_pretrained(model, config=config, **model_kwargs)
   1885 
   1886     return task_class(model=model, tokenizer=tokenizer, modelcard=modelcard, framework=framework, task=task, **kwargs)

/usr/local/lib/python3.7/site-packages/transformers/modeling_tf_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1207         for config_class, model_class in TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING.items():
   1208             if isinstance(config, config_class):
-&gt; 1209                 return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
   1210         raise ValueError(
   1211             &quot;Unrecognized configuration class {} for this kind of TFAutoModel: {}.\n&quot;

/usr/local/lib/python3.7/site-packages/transformers/modeling_tf_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    467                     f&quot;- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {TF2_WEIGHTS_NAME}, {WEIGHTS_NAME}.\n\n&quot;
    468                 )
--&gt; 469                 raise EnvironmentError(msg)
    470             if resolved_archive_file == archive_file:
    471                 logger.info(&quot;loading weights file {}&quot;.format(archive_file))

OSError: Can't load weights for 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'. Make sure that:

- 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.
</code></pre>
<p>However this works perfectly in <a href=""https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/Using_Spanish_BERT_fine_tuned_for_Q%26A_pipelines.ipynb#scrollTo=keZfuUAUlYNT"" rel=""nofollow noreferrer"">google colab</a>. This Google Colab doesn't require GPU to be ran, so why wouldn't it work in docker? What dependencies could I be missing? It doesn't see in the error message that dependencies could be missing, more than the model is no there but look:
And yes, this model exists <a href=""https://huggingface.co/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es"" rel=""nofollow noreferrer""><code>&quot;mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es&quot;</code></a> in hugging.co</p>
","10000823","","","","","2020-07-12 16:49:13","Running transformers on docker","<huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"62879960","1","","","2020-07-13 16:16:19","","2","852","<p>I am trying to set up a TensorFlow fine-tune framework for a question-answering project. Using hugging-face/transformer as the prototype, but cannot run through the trainer.</p>
<p>The experiment is conducted at Databricks, the pre-trained model loaded is base-bert, train and dev sets are downloaded from hugging-face examples SQUAD 2.0 <a href=""https://github.com/huggingface/transformers/tree/master/examples/question-answering"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/question-answering</a></p>
<p>The error log complains about the unexpected keyword argument '<strong>is_impossible</strong>', which is a SQUAD 2 data format feature.</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = TFAutoModelForQuestionAnswering.from_pretrained('bert-base-uncased')

processor = SquadV2Processor()

train_examples = processor.get_train_examples(data_dir, train_file_name)
eval_examples = processor.get_dev_examples(data_dir, dev_file_name)

train_dataset = (squad_convert_examples_to_features(
            examples=train_examples,
            tokenizer=tokenizer,
            max_seq_length=max_seq_length,
            doc_stride=doc_stride,
            max_query_length=max_query_length,
            is_training=True,
            return_dataset=&quot;tf&quot;
        )) 

eval_dataset = (squad_convert_examples_to_features(
            examples=eval_examples,
            tokenizer=tokenizer,
            max_seq_length=max_seq_length,
            doc_stride=doc_stride,
            max_query_length=max_query_length,
            is_training=False,
            return_dataset=&quot;tf&quot;
        )) 

training_args = (TFTrainingArguments(
    output_dir=output_dir, 
    num_train_epochs=2,
    do_train=True, 
    per_device_train_batch_size = 8, 
    per_device_eval_batch_size = 16,
    logging_steps=10, 
    learning_rate=3e-5))

trainer = TFTrainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)

trainer.train()
</code></pre>
<h2><strong>Error Log as below:</strong></h2>
<blockquote>
<hr />
<p>TypeError                                 Traceback (most recent call
last)  in 
----&gt; 1 trainer.train()</p>
<p>/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py
in train(self)
410             if self.args.past_index &gt;= 0:
411                 self._past = None
--&gt; 412             for step, training_loss in enumerate(self._training_steps(train_ds, optimizer)):
413                 self.global_step = iterations.numpy()
414                 self.epoch_logging = epoch_iter - 1 + (step + 1) / steps_per_epoch</p>
<p>/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py
in _training_steps(self, ds, optimizer)
457         Returns a generator over training steps (i.e. parameters update).
458         &quot;&quot;&quot;
--&gt; 459         for i, loss in enumerate(self._accumulate_next_gradients(ds)):
460             if i % self.args.gradient_accumulation_steps == 0:
461                 self._apply_gradients(optimizer)</p>
<p>/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py
in _accumulate_next_gradients(self, ds)
490         while True:
491             try:
--&gt; 492                 yield _accumulate_next()
493             except tf.errors.OutOfRangeError:
494                 break</p>
<p>/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py
in <strong>call</strong>(self, *args, **kwds)
566         xla_context.Exit()
567     else:
--&gt; 568       result = self._call(*args, **kwds)
569
570     if tracing_count == self._get_tracing_count():</p>
<p>/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py
in _call(self, *args, **kwds)
613       # This is the first call of <strong>call</strong>, so we have to initialize.
614       initializers = []
--&gt; 615       self._initialize(args, kwds, add_initializers_to=initializers)
616     finally:
617       # At this point we know that the initialization is complete (or less</p>
<p>/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py
in _initialize(self, args, kwds, add_initializers_to)
495     self._concrete_stateful_fn = (
496         self._stateful_fn._get_concrete_function_internal_garbage_collected(</p>
<h1>pylint: disable=protected-access</h1>
<p>--&gt; 497             *args, **kwds))
498
499     def invalid_creator_scope(*unused_args, **unused_kwds):</p>
<p>/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py
in _get_concrete_function_internal_garbage_collected(self, *args,
**kwargs)
2387       args, kwargs = None, None
2388     with self._lock:
-&gt; 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)
2390     return graph_function
2391</p>
<p>/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py
in _maybe_define_function(self, args, kwargs)
2701
2702       self._function_cache.missed.add(call_context_key)
-&gt; 2703       graph_function = self._create_graph_function(args, kwargs)
2704       self._function_cache.primary[cache_key] = graph_function
2705       return graph_function, args, kwargs</p>
<p>/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py
in _create_graph_function(self, args, kwargs,
override_flat_arg_shapes)
2591             arg_names=arg_names,
2592             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2593             capture_by_value=self._capture_by_value),
2594         self._function_attributes,
2595         # Tell the ConcreteFunction to clean up its graph once it goes out of</p>
<p>/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py
in func_graph_from_py_func(name, python_func, args, kwargs, signature,
func_graph, autograph, autograph_options, add_control_dependencies,
arg_names, op_return_value, collections, capture_by_value,
override_flat_arg_shapes)
976                                           converted_func)
977
--&gt; 978       func_outputs = python_func(*func_args, **func_kwargs)
979
980       # invariant: <code>func_outputs</code> contains only Tensors, CompositeTensors,</p>
<p>/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py
in wrapped_fn(*args, **kwds)
437         # <strong>wrapped</strong> allows AutoGraph to swap in a converted function. We give
438         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 439         return weak_wrapped_fn().<strong>wrapped</strong>(*args, **kwds)
440     weak_wrapped_fn = weakref.ref(wrapped_fn)
441</p>
<p>/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py
in wrapper(*args, **kwargs)
966           except Exception as e:  # pylint:disable=broad-except
967             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
969             else:
970               raise</p>
<p>TypeError: in converted code:</p>
<pre><code>/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py:488
</code></pre>
<p>_accumulate_next  *
return self._accumulate_gradients(per_replica_features, per_replica_labels)
/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py:498
_accumulate_gradients  *
per_replica_loss = self.args.strategy.experimental_run_v2(
/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/distribute/one_device_strategy.py:180 experimental_run_v2
return super(OneDeviceStrategy, self).experimental_run_v2(fn, args, kwargs)
/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py:511
_forward  *
per_example_loss, _ = self._run_model(features, labels, True)
/databricks/python/lib/python3.7/site-packages/transformers/trainer_tf.py:532
_run_model  *
outputs = self.model(features, training=training, **labels)[:2]
/local_disk0/pythonVirtualEnvDirs/virtualEnv-f56565e5-e45b-447a-b7df-50daf9109495/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:778
<strong>call</strong>
outputs = call_fn(cast_inputs, *args, **kwargs)</p>
<pre><code>TypeError: tf__call() got an unexpected keyword argument 'is_impossible'
</code></pre>
</blockquote>
","3381299","","100297","","2020-08-03 11:14:27","2020-08-03 11:14:27","Fail to run trainer.train() with huggingface transformer","<tensorflow><bert-language-model><huggingface-transformers><squad>","0","3","","","","CC BY-SA 4.0"
"68813979","1","68814093","","2021-08-17 08:22:37","","2","24","<p>I am getting desperate as I have no clue what is the problem over here. I want to translate a list of sentences from german to english. This is my code:</p>
<pre><code>
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)

batch = tokenizer(
    list(data_bert[:100]),
    padding=True,
    truncation=True,
    max_length=250,
    return_tensors=&quot;pt&quot;
)



results = model(batch)
</code></pre>
<p>And I am getting this error:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/miniconda3/envs/textmallet/lib/python3.9/site-packages/transformers/tokenization_utils_base.py in __getattr__(self, item)
    247         try:
--&gt; 248             return self.data[item]
    249         except KeyError:

KeyError: 'size'

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
/tmp/ipykernel_26502/2652187977.py in &lt;module&gt;
     14 
     15 
---&gt; 16 results = model(batch)
     17 

~/miniconda3/envs/textmallet/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/textmallet/lib/python3.9/site-packages/transformers/models/marian/modeling_marian.py in forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
   1274                 )
   1275 
-&gt; 1276         outputs = self.model(
   1277             input_ids,
   1278             attention_mask=attention_mask,
</code></pre>
<p>I have no clue what could be the precise issue over here. If someone can help me out I d be really thankful.</p>
","10604508","","","","","2021-08-17 09:18:12","Bert Transformer ""Size Error"" while Machine Traslation","<python-3.x><translation><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68897807","1","","","2021-08-23 19:03:32","","1","54","<p>I am trying to do semantic search but pre-trained model is not accurate on Italian grocery data.</p>
<p>eg.</p>
<pre><code>Query: latte al cioccolato  #chocolate milk

Top 3 most similar sentences in the corpus:
Milka  cioccolato al latte 100 g (Score: 0.7714)   #Milka milk chocolate 100 g
Alpro, Cioccolato bevanda a base di soia 1 ltr (Score: 0.5586)  #Alpro, Chocolate soy drink 1 ltr(soya milk)
Danone, HiPRO 25g Proteine gusto cioccolato 330 ml (Score: 0.4569) #Danone, HiPRO 25g Protein chocolate flavor 330 ml(protein chocolate milk) 
</code></pre>
<p>here in the above example, the problem is pre-trained BERT model is not returning context similarity. the result should be in the following order.</p>
<p><strong>Expected result:</strong></p>
<pre><code>Query: latte al cioccolato  #chocolate milk

Top 3 most similar sentences in the corpus:
Alpro, Cioccolato bevanda a base di soia 1 ltr (Score: 0.99)  #Alpro, Chocolate soy drink 1 ltr(soya milk)
Danone, HiPRO 25g Proteine gusto cioccolato 330 ml (Score: 0.95) #Danone, HiPRO 25g Protein chocolate flavor 330 ml(protein chocolate milk)
Milka  cioccolato al latte 100 g (Score: 0.40)   #Milka milk chocolate 100 g
</code></pre>
<p><strong>Fine-Tune Try:</strong></p>
<pre><code>!pip install sentence-transformers
import scipy
import numpy as np
from sentence_transformers import models, SentenceTransformer
model = SentenceTransformer('distiluse-base-multilingual-cased') # workes with Arabic, Chinese, Dutch, English, French, German, Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish


#Fine-Tuning 
import pandas as pd
df = pd.DataFrame({
    &quot;message&quot;:[
          &quot;latte al cioccolato&quot;  ,
          &quot;Alpro, Cioccolato bevanda a base di soia 1 ltr &quot;, #Alpro, Chocolate soy drink 1 ltr
          &quot;Milka  cioccolato al latte 100 g&quot;, #Milka milk chocolate 100 g
          &quot;Danone, HiPRO 25g Proteine gusto cioccolato 330 ml&quot;, #Danone, HiPRO 25g Protein chocolate flavor 330 ml
         ],
    &quot;lbl&quot;:[&quot;liquid&quot;,&quot;liquid&quot;,&quot;chocolate&quot;,&quot;liquid&quot;]
})
df


X=list(df['message'])
y=list(df['lbl'])


y=list(pd.get_dummies(y,drop_first=True)['liquid'])


from transformers import AutoTokenizer, AutoModel
  
tokenizer = AutoTokenizer.from_pretrained(&quot;kiri-ai/distiluse-base-multilingual-cased-et&quot;)
encodings = tokenizer(X, truncation=True, padding=True)


import tensorflow as tf

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(encodings),
    y
))



from transformers import AutoModel, TFTrainer, TFTrainingArguments

training_args = TFTrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=2,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=16,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)



with training_args.strategy.scope():
    model = AutoModel.from_pretrained(&quot;kiri-ai/distiluse-base-multilingual-cased-et&quot;)

trainer = TFTrainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset         # training dataset
)

trainer.train()
</code></pre>
","1385107","","","","","2021-08-23 19:03:32","how to fine-tune ""distiluse-base-multilingual-cased"" model for text similarity customisation","<nlp><bert-language-model><huggingface-transformers><fine-tune>","0","7","","","","CC BY-SA 4.0"
"68817989","1","68819822","","2021-08-17 13:11:18","","1","51","<p>I searched a lot for this but havent still got a clear idea so I hope you can help me out:</p>
<p>I am trying to translate german texts to english! I udes this code:</p>
<pre><code>
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)

batch = tokenizer(
    list(data_bert[:100]),
    padding=True,
    truncation=True,
    max_length=250,
    return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;]

results = model(batch)  
</code></pre>
<p>Which returned me a size error! I fixed this problem (thanks to the community: <a href=""https://github.com/huggingface/transformers/issues/5480"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/5480</a>) with switching the last line of code to:</p>
<pre><code>results = model(input_ids = batch,decoder_input_ids=batch)
</code></pre>
<p>Now my output looks like a really long array. What is this output precisely? Are these some sort of word embeddings? And if yes: How shall I go on with converting these embeddings to the texts in the english language? Thanks alot!</p>
","10604508","","6117017","","2021-08-17 13:29:52","2021-08-17 16:04:09","Bert model output interpretation","<translation><bert-language-model><huggingface-transformers><word-embedding>","2","0","","","","CC BY-SA 4.0"
"68918962","1","68919603","","2021-08-25 07:56:05","","1","63","<p>I am trying to predict with the NER model, as in the tutorial from huggingface (it contains only the training+evaluation part).</p>
<p>I am following this exact tutorial here : <a href=""https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb"" rel=""nofollow noreferrer"">https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb</a></p>
<p>The training works flawlessly, but the problems that I have begin when I try to predict on a simple sample.</p>
<pre><code>model_checkpoint = &quot;distilbert-base-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
loaded_model = AutoModel.from_pretrained('./my_model_own_custom_training.pth', from_tf=False)



input_sentence = &quot;John Nash is a great mathematician, he lives in France&quot;
tokenized_input_sentence = tokenizer([input_sentence],
                                     truncation=True, 
                                     is_split_into_words=False,
                                     return_tensors='pt')
predictions = loaded_model(tokenized_input_sentence[&quot;input_ids&quot;])[0]
</code></pre>
<p>Predictions is of shape <code>(1,13,768)</code></p>
<p>How can I arrive at the final result of the form <code>[JOHN &lt;-&gt; â€˜B-PERâ€™, â€¦ France &lt;-&gt; â€œB-LOCâ€]</code>, where <code>B-PER</code> and <code>B-LOC</code> are two ground truth labels, representing the tag for a person and location respectively?</p>
<p>The result of the prediction is:</p>
<pre><code>torch.Size([1, 13, 768])
</code></pre>
<p>If I write:</p>
<pre><code>print(predictions.argmax(axis=2))
tensor([613, 705, 244, 620, 206, 206, 206, 620, 620, 620, 477, 693, 308])
</code></pre>
<p>I get the tensor above.</p>
<p>However I would have expected to get the tensor representing the ground truth <code>[0â€¦8]</code> labels from the ground truth annotations.</p>
<p>Summary when loading the model :</p>
<blockquote>
<pre><code>loading configuration file ./my_model_own_custom_training.pth/config.json
Model config DistilBertConfig {
â€œname_or_path&quot;: â€œdistilbert-base-uncasedâ€,
â€œactivationâ€: â€œgeluâ€,
â€œarchitecturesâ€: [
â€œDistilBertForTokenClassificationâ€
],
â€œattention_dropoutâ€: 0.1,
â€œdimâ€: 768,
â€œdropoutâ€: 0.1,
â€œhidden_dimâ€: 3072,
â€œid2labelâ€: {
â€œ0â€: â€œLABEL_0â€,
â€œ1â€: â€œLABEL_1â€,
â€œ2â€: â€œLABEL_2â€,
â€œ3â€: â€œLABEL_3â€,
â€œ4â€: â€œLABEL_4â€,
â€œ5â€: â€œLABEL_5â€,
â€œ6â€: â€œLABEL_6â€,
â€œ7â€: â€œLABEL_7â€,
â€œ8â€: â€œLABEL_8â€
},
â€œinitializer_rangeâ€: 0.02,
â€œlabel2idâ€: {
â€œLABEL_0â€: 0,
â€œLABEL_1â€: 1,
â€œLABEL_2â€: 2,
â€œLABEL_3â€: 3,
â€œLABEL_4â€: 4,
â€œLABEL_5â€: 5,
â€œLABEL_6â€: 6,
â€œLABEL_7â€: 7,
â€œLABEL_8â€: 8
},
â€œmax_position_embeddingsâ€: 512,
â€œmodel_typeâ€: â€œdistilbertâ€,
â€œn_headsâ€: 12,
â€œn_layersâ€: 6,
â€œpad_token_idâ€: 0,
â€œqa_dropoutâ€: 0.1,
â€œseq_classif_dropoutâ€: 0.2,
â€œsinusoidal_pos_embdsâ€: false,
&quot;tie_weightsâ€: true,
â€œtransformers_versionâ€: â€œ4.8.1â€,
â€œvocab_sizeâ€: 30522
}
</code></pre>
</blockquote>
","6117017","","6117017","","2021-08-25 13:27:16","2021-08-25 13:27:16","HuggingFace-Transformers --- NER single sentence/sample prediction","<python-3.x><pytorch><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"68859300","1","","","2021-08-20 08:38:32","","0","31","<p>I am asked to specify the <code>decoder_input_ids</code>, which is a parameter for the <code>__call__</code> method of the model when running <code>model.fit()</code>.</p>
<p>Why do I need the <code>decoder_input_ids</code>? Should I call the model first before running <code>model.fit()</code>?</p>
<pre class=""lang-py prettyprint-override""><code>model = TFT5ForConditionalGeneration.from_pretrained('t5-base', return_dict=True)
model.compile(
   optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
   metrics=tf.metrics.SparseCategoricalAccuracy(),
)

tf.autograph.experimental.do_not_convert(
    func=model.fit(x=input_ids, y=label_ids, batch_size=8, epochs=3, verbose='auto', shuffle=True)
)
</code></pre>
<p>Error message:</p>
<blockquote>
<p>ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds</p>
</blockquote>
<p><code>input_ids</code> and <code>label_ids</code> are both <code>tf.Tensors</code></p>
","16711335","","8479387","","2021-08-21 03:45:49","2021-08-21 14:34:05","How to pass decoder_input_ids argument into model.fit() for TFT5ForConditionalGeneration model?","<python><tensorflow2.0><tf.keras><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68911203","1","","","2021-08-24 16:49:26","","1","23","<p>I am trying to use Big Bird Pegasus to summarize various long texts. The output is repeating the same concept in each sentence.</p>
<p>Here is my code using a news article I copied from NPR. The text is longer than the 4096 token limit, so it takes the first few thousand words from my input.</p>
<pre><code>from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM

model = BigBirdPegasusForConditionalGeneration.from_pretrained(&quot;google/bigbird-pegasus-large-arxiv&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/bigbird-pegasus-large-arxiv&quot;)

src_text = '''
On a sleepy cul-de-sac amid the bucolic vineyards and grassy hills of California's Sonoma Valley, a $4 million house has become the epicenter of a summer-long spat between angry neighbors and a new venture capital-backed startup buying up homes around the nation. The company is called Pacaso. It says it's the fastest company in American history to achieve the &quot;unicorn&quot; status of a billion-dollar valuation â€” but its quarrels in wine country, one of the first regions where it's begun operations, foreshadow business troubles ahead.
Brad Day and his wife, Holly Kulak, were first introduced to Pacaso in May after a romantic sunset dinner in their yard. &quot;And we just saw this drone, coming up and over our backyard,&quot; Day says. &quot;And we're like, what is that?&quot;
Pacaso denies directing or paying a drone operator to film the neighborhood. But its website does have drone photos of the house in question, located at 1405 Old Winery Court. It says it bought the photos after the fact.
Nonetheless, after the drone incident, Day and Kulak got suspicious about what was going on in their neighborhood. About a week later, their neighbors told them they were moving and selling their house to a limited liability corporation, or LLC. But they were super vague about it.
Day and Kulak began speaking with other residents on their cul-de-sac. One of them, Nancy Gardner, had learned from a friend in nearby Napa Valley about a new company called Pacaso that was buying houses in the area. The company was co-founded by a Napa resident, and it converts houses into LLCs. Pacaso then sells shares of these corporate houses to multiple investors. Gardner Googled Pacaso, and, sure enough, the house on their cul-de-sac was on its website. The company had named the house &quot;Chardonnay&quot; and was now selling investors the chance to buy a one-eighth share of it for $606,000.
Pacaso was founded in October 2020 by Austin Allison and Spencer Rascoff, two former executives at Zillow. The company is based in San Francisco, and as is typical of tech startups in the Silicon Valley area, its founders tell a lofty story about their business that's about more than just making money. The company says the motivation for the venture began when Allison and his wife, both based in Napa, bought a second home in Lake Tahoe. The night after they closed on the house, Allison says in a promotional video, he and his wife sat around a fire &quot;thinking how appreciative we were to be second homeowners. And, from that moment, I've always been inspired about making the dream of second home ownership possible for more people.&quot;
To make second home ownership possible for more people â€” and, of course, make money â€” Pacaso uses a &quot;fractional home ownership&quot; model. They buy a house, lightly refurbish it, furnish it and then create an LLC for it. They then divvy up ownership of this corporatized house into eight fractions and sell those shares on their website.
If you buy a share in a house, you're able to stay in it 44 nights per year in increments that can't exceed 14 consecutive days per visit. You can also &quot;gift&quot; these stays to friends or family. Pacaso offers an app to handle the logistics of booking stays. It oversees management, maintenance and cleaning of the property. In exchange for all this, it charges 12% of the home's purchase price upfront and monthly fees going forward. If you buy a share in a house, you have to hold on to it for a year. After that, you can sell it and profit from any appreciation in the home's value (or be on the hook for any depreciation).
When Day, Kulak and their neighbors learned about Pacaso's business model, they were appalled. They saw the venture capital-backed company as invading their community and converting their neighbor's house into a revolving carousel of vacationers. They imagined endless parties, noise and cars overflowing their cul-de-sac. They worried those staying at &quot;Chardonnay&quot; would drive too fast and fail to heed local concerns about wildfires and droughts. But, most of all, they feared the Pacaso house and more like it would destroy their sense of community and turn their neighborhood into an &quot;adult Disneyland.&quot;
The county, Day says, had designated their neighborhood an &quot;exclusion zone,&quot; which bans Airbnb-style, short-term rentals to preserve the &quot;residential character&quot; of communities. But Pacaso argues that its clients are not short-term renters. They are co-owners of an LLC. This also means they don't have to pay the typical taxes on short-term rentals. Likewise, in the nearby town of St. Helena, Pacaso was trying to circumnavigate a city ban against timeshares with the same argument. Day says he and his neighbors saw Pacaso's newfangled business model as nothing more than a &quot;glorified timeshare&quot; with a legal strategy aimed at &quot;skirting regulations that are designed to keep communities intact.&quot;
The cul-de-sac sprang into action. It formed an organization called Sonomans Together Opposing Pacaso, which, not coincidentally, has the acronym STOP. It contacted the county Board of Supervisors. It created an anti-Pacaso website and circulated an online petition. It flooded the local newspaper with op-eds and letters to the editor. It lobbied local real estate agents not to work with Pacaso. &quot;It feels like we're waging a war by land, air and sea,&quot; Day says.
Protest signs festoon the neighborhood's lawns, fences and cars. They say things such as &quot;Stop Pacaso&quot; and &quot;Not here, Pacaso!&quot; Day's favorite sign reads, &quot;The Pacaso house is the big one on the right with no soul.&quot;
The signs, of course, make the prospect of buying a share in the Pacaso house awkward, to say the least. Alfred Miller, however, bought a share in &quot;Chardonnay&quot; before ever seeing it in person. Miller is a risk management consultant based in Los Angeles. He believes in Pacaso's business model. And he likes wine and Sonoma's climate. As he researched &quot;Chardonnay&quot; online, he liked the modern architecture and pool, and he decided he'd buy a one-eighth share of the house. It wasn't until a couple weeks after he made the purchase that he first drove up to Sonoma and witnessed the spectacle around his new investment.
&quot;So, imagine me as a new owner driving up, and I get to the corner of Old Winery Court,&quot; Miller says. &quot;There's a full-on, professionally printed sign that says 'No Pacaso.' '' Miller then turned right onto Old Winery Court &quot;and the more I drive into the neighborhood, the more signs I see. Brad Day has three vehicles in front of his house, and each vehicle has an anti-Pacaso sign on it. I pull into the driveway â€” there are two signs on each side of the property. I mean, it was not what I would call very welcoming.&quot;
As it did on Old Winery Court, controversy erupted in Napa after the company bought a home worth $1.13 million. That's about 35% higher than Napa's median home price. Pacaso insists it only buys luxury and ultra-luxury houses, and it therefore isn't competing with local middle-class families in the housing market. But this home, located two blocks from a high school, didn't quite fit its talking points. Some Napans were pissed. Pacaso says the house was the victim of trespassing and &quot;illegal signage.&quot; Pacaso even claims it had to file a police report after a local wrote to the company and said, &quot;I will burn down any home you buy in Napa. This is no joke.&quot;
Pacaso's CEO, who lives in Napa, saw firsthand how angry Napans were, and the company responded. In June, Pacaso agreed to sell the Napa home in a traditional manner &quot;to a whole home buyer&quot; rather than convert it into a corporation and sell it to multiple people. The company also pledged to beef up its &quot;Owner Code Of Conduct&quot; to include &quot;decibel limits on all home sound systems,&quot; create a &quot;local liaison&quot; dedicated to assisting neighbors, not buy any homes in the area for under $2 million, and, for each house sold in Napa and Sonoma counties, donate $20,000 to a local nonprofit dedicated to affordable housing.
But while it has been trying to placate local communities with business reforms, Pacaso has waged a court battle with the town of St. Helena over whether its homes should be classified as timeshares. Pacaso is dead set against that classification. One reason might be that timeshares have a bad rap: While they're a popular way to go on vacations, their costs and associated fees tend to make them money losers rather than a profitable investment.
Potentially even more damaging to Pacaso's ambitions, however: Timeshares are banned in many vacation communities around the nation. Hence, Pacaso has strong reasons to insist its homes are not timeshares.
&quot;Unlike a timeshare model, the co-owners that Pacaso serves collectively own real estate, not time,&quot; says Ellen Haberle, director of community and government relations for Pacaso.
St. Helena disagrees, declaring Pacaso homes are not allowed in the town because of a city ordinance against timesharing. &quot;Simply calling them co-ownership arrangements does not change that fact,&quot; City Attorney Ethan Walsh said. In response to the ban, Pacaso sued the town in federal court. The lawsuit is still pending.
Pacaso says it plans to expand across North America and Europe. Given the company's billion-dollar valuation, investors seem to believe that many people will be attracted to its model of fractional second home ownership. But local residents will likely continue to fight the unicorn stampeding into their towns.
'''

device = 'cuda' if torch.cuda.is_available() else 'cpu'
batch = tokenizer(src_text, truncation=True, padding='longest', return_tensors=&quot;pt&quot;).to(device)
translated = model.generate(**batch)
tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)
</code></pre>
<p>Here is the output. It doesn't mention anything but the topic of fractional ownership. I repeated for another input text and got similar results - each sentence was a slight variation of each other.</p>
<pre><code>tgt_text
['the notion of a fractional ownership in a real property was introduced in the 19th century.&lt;n&gt; fractional ownership in a real property was defined to be the fraction of the value of the property minus the cost of its construction.&lt;n&gt; the fractional ownership of a real property was defined to be the fraction of the value of the property minus the cost of its construction.&lt;n&gt; the fractional ownership of a home is defined to be the fraction of the value of the home minus the cost of its construction. &lt;n&gt; the notion of a fractional ownership in a real property was introduced in the 19th century.&lt;n&gt; the fractional ownership of a home is the fraction of the value of the home minus the cost of its construction.&lt;n&gt; the fractional ownership of a real property was defined to be the fraction of the value of the home minus the cost of its construction.&lt;n&gt; the fractional ownership of a home is the fraction of the value of the home minus the cost of its construction.&lt;n&gt; the fractional ownership of a real property was defined to be the fraction of the value of the home minus the cost of its construction.&lt;n&gt; the fractional ownership of a home was defined to be the fraction of the value of the home minus the cost of its construction.&lt;n&gt; the']
</code></pre>
","7323998","","7323998","","2021-08-25 01:27:20","2021-08-25 01:27:20","Big Bird Pegasus Summarization output is repeating itself","<python><nlp><huggingface-transformers><summarization>","0","0","","","","CC BY-SA 4.0"
"68927854","1","","","2021-08-25 18:02:46","","0","25","<p>Training the WordLevel tokenizer I receive strange vocabulary. Bellow is my code:</p>
<pre><code>data = [
    &quot;Beautiful is better than ugly.&quot;
    &quot;Explicit is better than implicit.&quot;
    &quot;Simple is better than complex.&quot;
    &quot;Complex is better than complicated.&quot;
    &quot;Flat is better than nested.&quot;
    &quot;Sparse is better than dense.&quot;
    &quot;Readability counts.&quot;
]

from tokenizers.models import WordLevel
from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers

tokenizer = Tokenizer(models.WordLevel())

trainer = trainers.WordLevelTrainer(
    vocab_size=100000,
)

tokenizer.train_from_iterator(data, trainer=trainer)

tokenizer.get_vocab()
</code></pre>
<p>The output is the following:</p>
<pre><code>{'Beautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.': 0}
</code></pre>
<p>Please explain what I'm doing wrong...</p>
","5592430","","","","","2021-08-25 21:46:24","Transformers: WordLevel tokenizer produces strange vocabulary","<python><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"68939622","1","","","2021-08-26 13:39:32","","0","21","<p>gRPC was expecting a byte format value. The query request is not liking the tensor type into the gRPC. I am pretty new to gRPC/APIs.</p>
<pre><code>Traceback (most recent call last):
  File &quot;basic_grpc_example.py&quot;, line 120, in &lt;module&gt;
    request = client.PrimalRequest(
TypeError: &lt;tf.Tensor: shape=(1, 512), dtype=int32, numpy=
array([[  101,  1053,  2015,  2966,  2373,  1997,  4 has type tensorflow.python.framework.ops.EagerTensor, but expected one of: bytes, unicode
</code></pre>
<p>Have tried using .tobytes() &amp; byte casting, but it is not liking the tensor format for some reason, Any help is appreciated.</p>
","16755173","","16755173","","2021-08-26 17:23:51","2021-08-26 17:23:51","Converting tensorflow.python.framework.ops.EagerTensor to : bytes, unicode","<huggingface-transformers><distilbert>","0","0","","","","CC BY-SA 4.0"
"67089849","1","67089951","","2021-04-14 10:20:20","","2","1185","<p>I am just using the huggingface transformer library and get the following message when running run_lm_finetuning.py: AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len'. Anyone else with this problem or an idea how to fix it? Thanks!</p>
<p>My full experiment run:
mkdir experiments</p>
<p>for epoch in 5
do
python run_lm_finetuning.py <br />
--model_name_or_path distilgpt2 <br />
--model_type gpt2 <br />
--train_data_file small_dataset_train_preprocessed.txt <br />
--output_dir experiments/epochs_$epoch <br />
--do_train <br />
--overwrite_output_dir <br />
--per_device_train_batch_size 4 <br />
--num_train_epochs $epoch
done</p>
","15221534","","","","","2021-04-14 10:27:39","AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len'","<tokenize><huggingface-transformers><transformer><huggingface-tokenizers><gpt-2>","1","0","1","","","CC BY-SA 4.0"
"67106576","1","","","2021-04-15 10:17:14","","0","32","<p>I trying to load model fo Bert from local directory on my computer by given the path to the model. But this error is appearing  :</p>
<pre><code>Traceback (most recent call last):
  File &quot;vectorize_with_FBbc_orAug.py&quot;, line 54, in &lt;module&gt;
    emb, mdlname = get_flaubert_layer(data)
  File &quot;vectorize_with_FBbc_orAug.py&quot;, line 24, in get_flaubert_layer
    flaubert, log = FlaubertModel.from_pretrained(path)
TypeError: 'FlaubertModel' object is not iterable
Traceback (most recent call last):
  File &quot;vectorize_with_FBunc_orAug.py&quot;, line 53, in &lt;module&gt;
    emb, mdlname = get_flaubert_layer(data)
  File &quot;vectorize_with_FBunc_orAug.py&quot;, line 23, in get_flaubert_layer
    flaubert, log = FlaubertModel.from_pretrained(path)
TypeError: 'FlaubertModel' object is not iterable

</code></pre>
<p>the script looks like this :</p>
<pre><code>def get_flaubert_layer(texte):

    modelname = &quot;flaubert-base-uncased&quot;
    path = './flaubert/flaubert-base-uncased/'

    flaubert, log = FlaubertModel.from_pretrained(path)
    flaubert_tokenizer = FlaubertTokenizer.from_pretrained(path)
    tokenized = texte.apply((lambda x: flaubert_tokenizer.encode(x, add_special_tokens=True)))
    max_len = 0
    for i in tokenized.values:
        if len(i) &gt; max_len:
            max_len = len(i)
    padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized.values])
    token_ids = torch.tensor(padded)
    with torch.no_grad():
        last_layer = flaubert(token_ids)[0][:,0,:].numpy()
        
    return last_layer, modelname
    
</code></pre>
<p>I am trying to figure out why I have the error maybe someone can seet it ?
I actually rename my directory like in the transformers library.</p>
","7294253","","6664872","","2021-04-15 11:36:35","2021-04-15 11:36:35","TypeError: 'FlaubertModel' object is not iterable","<python><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"67110646","1","","","2021-04-15 14:43:50","","0","26","<p>Hope you are doing well!</p>
<p>I am trying to use the transfomers library of huggingface.
I want to optimize the speed</p>
<p>Screen of speed test using list comprehension:
<img src=""https://i.stack.imgur.com/B45Jy.png"" alt=""1"" />
The text in the list is around 500 characters each.</p>
<p>I don't understand why this</p>
<pre><code>[classifier(review, labels,
       multi_label=True) for review in list]
</code></pre>
<p>is faster than</p>
<pre><code>classifier(list, labels, multi_label=True)
         
</code></pre>
<p>Do you have any idea why it does this?</p>
<p>Thank you !</p>
","15648526","","6664872","","2021-04-17 14:12:05","2021-04-17 14:12:05","Hugging face classifier speed (list comprehension)","<list><performance><optimization><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"67116382","1","","","2021-04-15 21:41:44","","0","17","<p>When attempting to pip install Huggingfaces's transformers, I receive the following error and I have been unable to correct it. I am on Windows 10, using python 3.9.4.</p>
<p>UnavailableInvalidChannel: The channel is not accessible or is invalid.
channel name: pip install git+https://github.com/huggingface/transformers
channel url: <a href=""https://conda.anaconda.org/pip"" rel=""nofollow noreferrer"">https://conda.anaconda.org/pip</a> install git <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers</a>
error code: 404</p>
<p>You will need to adjust your conda configuration to proceed.
Use <code>conda config --show channels</code> to view your configuration's current state,
and use <code>conda config --show-sources</code> to view config file locations.</p>
","15652203","","","","","2021-04-15 21:41:44","Error Downloading Huggingfaces's transformers package: ""UnavailableInvalidChannel:The channel is not accessible or is invalid.""","<python><pip><nlp><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"67138037","1","","","2021-04-17 12:12:33","","1","464","<p>Iâ€™m very new to HuggingFace, Iâ€™ve come around this error â€œ<strong>TextInputSequence must be str</strong>â€ on a notebook which is helping me a lot to do some practice on various hugging face models. <strong>The boilerplate code on the notebook is throwing this error (I guess) due to some changes in huggingfaceâ€™s API</strong> or something. So I was wondering if someone could suggest some changes that I can make to the code to resolve the error.</p>
<p><em><strong>The error can easily be reproduced by just running all the cells of the notebook.</strong></em></p>
<p><strong>Link</strong>: <a href=""https://colab.research.google.com/drive/1K9H753cX0tD0lsoXvyHsDhrTtbnzq1bL?usp=sharing&amp;authuser=1#scrollTo=d5YvAzA5QJER"" rel=""nofollow noreferrer"">Colab Notebook</a></p>
<h3>This is the line that is throwing the error-<img src=""https://i.stack.imgur.com/SKUT5.png"" alt=""the code throwing error2"" /></h3>
<h3>Here is the error-<a href=""https://i.stack.imgur.com/miF9L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/miF9L.png"" alt=""error trace"" /></a></h3>
","7610724","","","","","2021-07-08 16:04:14","""TextInputSequence must be strâ€ error on Hugging Face Transformers","<deep-learning><nlp><pytorch><huggingface-transformers><huggingface-tokenizers>","1","2","","","","CC BY-SA 4.0"
"68907519","1","69064907","","2021-08-24 12:23:04","","2","83","<p>I am Playing around with Bert Pretrained Models (bert-large-uncased-whole-word-masking)
I used Huggingface to try it I first Used this Piece of Code</p>
<pre><code>m = TFBertLMHeadModel.from_pretrained(&quot;bert-large-cased-whole-word-masking&quot;)
logits = m(tokenizer(&quot;hello world [MASK] like it&quot;,return_tensors=&quot;tf&quot;)[&quot;input_ids&quot;]).logits
</code></pre>
<p>I then used Argmax to get max probabilities after applying softmax,
Things works fine Until now.</p>
<p>When I used padding with max_length = 100 The model started making false prediction and not working well and all predicted tokens were the same i.e 119-Token ID</p>
<p>Code I used for Argmax</p>
<pre><code>tf.argmax(tf.keras.activations.softmax(m(tokenizer(&quot;hello world [MASK] like it&quot;,return_tensors=&quot;tf&quot;,max_length=,padding=&quot;max_length&quot;)[&quot;input_ids&quot;]).logits)[0],axis=-1)
</code></pre>
<p>Output Before using padding</p>
<pre><code>&lt;tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 9800, 19082,  1362,   146,  1176,  1122,   119])&gt;
</code></pre>
<p>Output After using padding with max_length of 100</p>
<pre><code>&lt;tf.Tensor: shape=(100,), dtype=int64, numpy=
array([119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119])&gt;
</code></pre>
<p>I wonder if this problem prevail even training a new model as It is mandatory to set Input shape for training new model I Padded and tokenized the data but, now I want to know if this problem continues with it too.</p>
","16679678","","6664872","","2021-08-24 21:08:17","2021-09-05 15:49:31","Bert with Padding and Masked Token Predicton","<tensorflow><keras><bert-language-model><huggingface-transformers><language-model>","1","3","1","","","CC BY-SA 4.0"
"68941974","1","","","2021-08-26 16:09:01","","0","22","<p>I am have trained DistilBERT via fastai and huggingface for a sequence classification problem. I found a useful tutorial that gave a good example on how to do this with binary classification. The code is below:</p>
<pre><code># !pip install torch==1.9.0
# !pip install torchtext==0.10
# !pip install transformers==4.7
# !pip install fastai==2.4

from fastai.text.all import *
from sklearn.model_selection import train_test_split
import pandas as pd
import glob
from transformers import AutoTokenizer, AutoModelForSequenceClassification


hf_tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;)
hf_model = AutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)

&quot;&quot;&quot;
train_df and val_df looks like this:

      label text
4240    5   whoa interesting.
13      7   you could you could we just
4639    4   you set the goal,
28      1   because ive already agreed to that
66      8   oh hey freshman thats you gona need
&quot;&quot;&quot;

print(list(train_df.label.value_counts().index))
&quot;&quot;&quot;
[4, 1, 5, 6, 7, 0, 2, 3, 8]
&quot;&quot;&quot;

class HF_Dataset(torch.utils.data.Dataset):
    def __init__(self, df, hf_tokenizer):
        self.df = df
        self.hf_tokenizer = hf_tokenizer
        
        self.label_map = {
            0:0,
            1:0,
            2:0,
            3:0,
            4:1,
            5:1,
            6:1,
            7:1,
            8:1
        }
        
    def __len__(self):
        return len(self.df)

    def decode(self, token_ids):
        return ' '.join([hf_tokenizer.decode(x) for x in tokenizer_outputs['input_ids']])
    
    def decode_to_original(self, token_ids):
        return self.hf_tokenizer.decode(token_ids.squeeze())

    def __getitem__(self, index):
        label, text = self.df.iloc[index]
        label = self.label_map[label]
        label = torch.tensor(label)

        tokenizer_output = self.hf_tokenizer(text, return_tensors=&quot;pt&quot;, padding='max_length', truncation=True, max_length=512)
        
        tokenizer_output['input_ids'].squeeze_()
        tokenizer_output['attention_mask'].squeeze_()
        
        return tokenizer_output, label
        

train_dataset = HF_Dataset(train_df, hf_tokenizer)
valid_dataset = HF_Dataset(valid_df, hf_tokenizer)

train_dl = DataLoader(train_dataset, bs=16, shuffle=True)
valid_dl = DataLoader(valid_dataset, bs=16)
dls = DataLoaders(train_dl, valid_dl)
hf_model(**batched_data)


class HF_Model(nn.Module):
  
    def __init__(self, hf_model):
        super().__init__()
        
        self.hf_model = hf_model
        
    def forward(self, tokenizer_outputs):
        
        model_output = self.hf_model(**tokenizer_outputs)
        
        return model_output.logits
        
model = HF_Model(hf_model)
# Manually popping the model onto the gpu since the data is in a dictionary format
# (doesn't automatically place model + data on gpu otherwise)
learn = Learner(dls, model, loss_func=nn.CrossEntropyLoss(), metrics=[accuracy])
learn.fit_one_cycle(3, 1e-4)
</code></pre>
<p>This trains like a normal fastiai pipeline. However, after training (<code>fit_one_cycle</code>), all further attempts to work with the <code>learn</code> object break.</p>
<p>When I run:</p>
<pre><code>interp = ClassificationInterpretation.from_learner(learn)
interp.plot_confusion_matrix(title='Confusion matrix')
</code></pre>
<p>I get:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-51-93b05a3209d9&gt; in &lt;module&gt;
      1 # predict the validation set with our model
----&gt; 2 interp = ClassificationInterpretation.from_learner(learn)
      3 interp.plot_confusion_matrix(title='Confusion matrix')

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/interpret.py in from_learner(cls, learn, ds_idx, dl, act)
     27         &quot;Construct interpretation object from a learner&quot;
     28         if dl is None: dl = learn.dls[ds_idx].new(shuffled=False, drop_last=False)
---&gt; 29         return cls(dl, *learn.get_preds(dl=dl, with_input=True, with_loss=True, with_decoded=True, act=None))
     30 
     31     def top_losses(self, k=None, largest=True):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in get_preds(self, ds_idx, dl, with_input, with_decoded, with_loss, act, inner, reorder, cbs, **kwargs)
    258                 res[pred_i] = act(res[pred_i])
    259                 if with_decoded: res.insert(pred_i+2, getattr(self.loss_func, 'decodes', noop)(res[pred_i]))
--&gt; 260             if reorder and hasattr(dl, 'get_idxs'): res = nested_reorder(res, tensor(idxs).argsort())
    261             return tuple(res)
    262         self._end_cleanup()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/torch_core.py in nested_reorder(t, idxs)
    710     &quot;Reorder all tensors in `t` using `idxs`&quot;
    711     if isinstance(t, (Tensor,L)): return t[idxs]
--&gt; 712     elif is_listy(t): return type(t)(nested_reorder(t_, idxs) for t_ in t)
    713     if t is None: return t
    714     raise TypeError(f&quot;Expected tensor, tuple, list or L but got {type(t)}&quot;)

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/torch_core.py in &lt;genexpr&gt;(.0)
    710     &quot;Reorder all tensors in `t` using `idxs`&quot;
    711     if isinstance(t, (Tensor,L)): return t[idxs]
--&gt; 712     elif is_listy(t): return type(t)(nested_reorder(t_, idxs) for t_ in t)
    713     if t is None: return t
    714     raise TypeError(f&quot;Expected tensor, tuple, list or L but got {type(t)}&quot;)

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/torch_core.py in nested_reorder(t, idxs)
    712     elif is_listy(t): return type(t)(nested_reorder(t_, idxs) for t_ in t)
    713     if t is None: return t
--&gt; 714     raise TypeError(f&quot;Expected tensor, tuple, list or L but got {type(t)}&quot;)
    715 
    716 # Cell

TypeError: Expected tensor, tuple, list or L but got &lt;class 'dict'&gt;
</code></pre>
<p>When I run:</p>
<pre><code>learn.export('export')
</code></pre>
<p>I get:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-52-4de7bc9fac6f&gt; in &lt;module&gt;
----&gt; 1 learn.export('export')

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in export(self, fname, pickle_module, pickle_protocol)
    367     self._end_cleanup()
    368     old_dbunch = self.dls
--&gt; 369     self.dls = self.dls.new_empty()
    370     state = self.opt.state_dict() if self.opt is not None else None
    371     self.opt = None

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/data/core.py in new_empty(self)
    144     def __getitem__(self, i): return self.loaders[i]
    145     def new_empty(self):
--&gt; 146         loaders = [dl.new(dl.dataset.new_empty()) for dl in self.loaders]
    147         return type(self)(*loaders, path=self.path, device=self.device)
    148 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/data/core.py in &lt;listcomp&gt;(.0)
    144     def __getitem__(self, i): return self.loaders[i]
    145     def new_empty(self):
--&gt; 146         loaders = [dl.new(dl.dataset.new_empty()) for dl in self.loaders]
    147         return type(self)(*loaders, path=self.path, device=self.device)
    148 

AttributeError: 'HF_Dataset' object has no attribute 'new_empty'

</code></pre>
<p>When I run:</p>
<pre><code>sample_text = test_df.text.values[0]
# sample text = 'hey how are you'
learn.predict(sample_text)
</code></pre>
<p>I get:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-53-7e8410893154&gt; in &lt;module&gt;
----&gt; 1 learn.predict(sample_text)

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in predict(self, item, rm_type_tfms, with_input)
    264     def predict(self, item, rm_type_tfms=None, with_input=False):
    265         dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)
--&gt; 266         inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)
    267         i = getattr(self.dls, 'n_inp', -1)
    268         inp = (inp,) if i==1 else tuplify(inp)

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in get_preds(self, ds_idx, dl, with_input, with_decoded, with_loss, act, inner, reorder, cbs, **kwargs)
    251         if with_loss: ctx_mgrs.append(self.loss_not_reduced())
    252         with ContextManagers(ctx_mgrs):
--&gt; 253             self._do_epoch_validate(dl=dl)
    254             if act is None: act = getattr(self.loss_func, 'activation', noop)
    255             res = cb.all_tensors()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_epoch_validate(self, ds_idx, dl)
    201         if dl is None: dl = self.dls[ds_idx]
    202         self.dl = dl
--&gt; 203         with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)
    204 
    205     def _do_epoch(self):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in all_batches(self)
    167     def all_batches(self):
    168         self.n_iter = len(self.dl)
--&gt; 169         for o in enumerate(self.dl): self.one_batch(*o)
    170 
    171     def _do_one_batch(self):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in one_batch(self, i, b)
    192         b = self._set_device(b)
    193         self._split(b)
--&gt; 194         self._with_events(self._do_one_batch, 'batch', CancelBatchException)
    195 
    196     def _do_epoch_train(self):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_one_batch(self)
    170 
    171     def _do_one_batch(self):
--&gt; 172         self.pred = self.model(*self.xb)
    173         self('after_pred')
    174         if len(self.yb):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

&lt;ipython-input-16-3f4e8a42b8ff&gt; in forward(self, tokenizer_outputs)
      9     def forward(self, tokenizer_outputs):
     10 
---&gt; 11         model_output = self.hf_model(**tokenizer_outputs)
     12 
     13         return model_output.logits

TypeError: DistilBertForSequenceClassification object argument after ** must be a mapping, not str
</code></pre>
<p>The fact that all of these normal fastai procedures are breaking indicates there must be some general problem I am running into. What do I need to do to treat <code>learn</code> like one normally does in a fastai pipeline?</p>
<p>For reference:</p>
<pre><code>print(fastai.__version__)
print(torch.__version__)
print(transformers.__version__)

2.4
1.9.0+cu102
4.7.0
</code></pre>
<p>Running on Ubuntu 18.04.</p>
","11666502","","","","","2021-08-26 16:09:01","How to work with huggingface transformers in fastai2","<python><deep-learning><pytorch><huggingface-transformers><fast-ai>","0","0","","","","CC BY-SA 4.0"
"67137640","1","","","2021-04-17 11:27:39","","0","32","<p>I am super confused on how to create a seq2seq NLP model based on a transformer with BERT as the encoder.</p>
<p>Please kindly advise if the process below is even correct?
(If you know any materials which would help me figure this out, please kindly share...)</p>
<hr />
<p>[encoer]</p>
<p>-create seq2seq transformer layer</p>
<p>-define BERT-attention</p>
<p>-pool the encoder</p>
<p>[decoder]</p>
<p>-create transformer decoder</p>
<p>[pool]</p>
<p>-concatenate encoder and decoder</p>
<hr />
<p>I would really appreciate any advice or tips! (even just a reference)</p>
","14386833","","","","","2021-04-17 11:27:39","how to create a seq2seq NLP model based on a transformer with BERT as the encoder?","<nlp><bert-language-model><huggingface-transformers><encoder><seq2seq>","0","2","","","","CC BY-SA 4.0"
"67142267","1","","","2021-04-17 19:38:41","","0","209","<p>The following code is used to determine the impact of input words on the most probable output unit.</p>
<pre><code>def _register_embedding_list_hook(model, embeddings_list):
    def forward_hook(module, inputs, output):
        embeddings_list.append(output.squeeze(0).clone().cpu().detach().numpy())
    embedding_layer = model.bert.embeddings.word_embeddings
    handle = embedding_layer.register_forward_hook(forward_hook)
    return handle

def _register_embedding_gradient_hooks(model, embeddings_gradients):
    def hook_layers(module, grad_in, grad_out):
        embeddings_gradients.append(grad_out[0])
    embedding_layer = model.bert.embeddings.word_embeddings
    hook = embedding_layer.register_backward_hook(hook_layers)
    return hook

def saliency_map(model, input_ids, segment_ids, input_mask):
    torch.enable_grad()
    model.eval()
    embeddings_list = []
    handle = _register_embedding_list_hook(model, embeddings_list)
    embeddings_gradients = []
    hook = _register_embedding_gradient_hooks(model, embeddings_gradients)

    model.zero_grad()
    A = model(input_ids, token_type_ids=segment_ids, attention_mask=input_mask)
    pred_label_ids = np.argmax(A.logits[0].detach().numpy())
    A.logits[0][pred_label_ids].backward()
    handle.remove()
    hook.remove()

    saliency_grad = embeddings_gradients[0].detach().cpu().numpy()        
    saliency_grad = np.sum(saliency_grad[0] * embeddings_list[0], axis=1)
    norm = np.linalg.norm(saliency_grad, ord=1)
    saliency_grad = [e / norm for e in saliency_grad] 
    
    return saliency_grad
</code></pre>
<p>which is used in the following way (for a sentiment analysis model):</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained(&quot;textattack/bert-base-uncased-imdb&quot;)
model = AutoModelForSequenceClassification.from_pretrained(&quot;textattack/bert-base-uncased-imdb&quot;)

tokens = tokenizer('A really bad movie')

input_ids = torch.tensor([tokens['input_ids']], dtype=torch.long)
token_type_ids = torch.tensor([tokens['token_type_ids']], dtype=torch.long)
attention_ids = torch.tensor([tokens['attention_mask']], dtype=torch.long)

saliency_scores = saliency_map(model, input_ids, 
                                token_type_ids, 
                                attention_ids)
</code></pre>
<p>But it produces the following scores for the tokens which are nonsense since for example &quot;bad&quot; has a negative effect on the predicted class (which is negative). What's wrong with this code?</p>
<p><a href=""https://i.stack.imgur.com/9KuEc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9KuEc.png"" alt=""enter image description here"" /></a></p>
<p>Here are some more examples:</p>
<p><a href=""https://i.stack.imgur.com/djV4C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/djV4C.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/o4pPV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o4pPV.png"" alt=""enter image description here"" /></a></p>
","5617507","","5617507","","2021-04-19 11:51:50","2021-04-19 11:51:50","Gradient-based saliency of input words in a pytorch model from transformers library","<pytorch><gradient><huggingface-transformers>","0","11","","","","CC BY-SA 4.0"
"68849573","1","","","2021-08-19 14:15:48","","1","49","<p>Is fine-tuning a pre-trained transformer model a easier model an â€˜easierâ€™ task than training a transformer from scratch (BERT, GPT-2) in terms of GPU needs and GPU memory usage?</p>
<p>To clarify further, Iâ€™ve read how to train most transformer models, one would require multi-GPU training. However, is it possible to fine-tune some of these models on a single-GPU?
Why is this the case?</p>
<p>Is it because we can use with smaller batches, the fine-tuning time is not as much as training from scratch?</p>
","16706366","","6573902","","2021-08-19 18:14:16","2021-08-19 18:14:16","Transformer model fine-tuning on single GPU","<machine-learning><tensorflow2.0><huggingface-transformers><fine-tune>","1","0","","","","CC BY-SA 4.0"
"67080085","1","","","2021-04-13 17:58:33","","0","162","<p>Company firewall seems to prevent me from just using</p>
<pre><code>model = AutoModel.from_pretrained(&quot;sentence-transformers/bert-base-nli-stsb-mean-tokens&quot;)
</code></pre>
<p>so I need to download this model locally and then read it into Python.
Couldn't find the direct AWS link, seems to be typically in this form: but did not work</p>
<pre><code>https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-nli-stsb-mean-tokens-pytorch_model.bin
</code></pre>
<p>Tried these similar questions/solutions <a href=""https://stackoverflow.com/questions/62317931/how-to-predownload-a-transformers-model/64280935#64280935"">here</a> but did not work, since I can't run the first line to download from pretrained in Python, I need an external solution</p>
","11302142","","","","","2021-08-08 13:58:02","How to download BERT model locally, without use of package?","<python><nlp><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"67153058","1","67170170","","2021-04-18 20:24:00","","1","281","<p>I am trying to use Huggingface transformer api to load a locally downloaded M-BERT model but it is throwing an exception.
I clone this repo: <a href=""https://huggingface.co/bert-base-multilingual-cased"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-multilingual-cased</a></p>
<pre><code>bert = TFBertModel.from_pretrained(&quot;input/bert-base-multilingual-cased&quot;)
</code></pre>
<p>The directory structure is:</p>
<p><a href=""https://i.stack.imgur.com/rDj4T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rDj4T.png"" alt=""Directory structure"" /></a></p>
<p>But I am getting this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 1277, in from_pretrained
    missing_keys, unexpected_keys = load_tf_weights(model, resolved_archive_file, load_weight_prefix)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 467, in load_tf_weights
    with h5py.File(resolved_archive_file, &quot;r&quot;) as f:
  File &quot;/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py&quot;, line 408, in __init__
    swmr=swmr)
  File &quot;/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py&quot;, line 173, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File &quot;h5py/_objects.pyx&quot;, line 54, in h5py._objects.with_phil.wrapper
  File &quot;h5py/_objects.pyx&quot;, line 55, in h5py._objects.with_phil.wrapper
  File &quot;h5py/h5f.pyx&quot;, line 88, in h5py.h5f.open
OSError: Unable to open file (file signature not found)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;train.py&quot;, line 81, in &lt;module&gt;
    __main__()
  File &quot;train.py&quot;, line 59, in __main__
    model = create_model(num_classes)
  File &quot;/content/drive/My Drive/msc-project/code/model.py&quot;, line 26, in create_model
    bert = TFBertModel.from_pretrained(&quot;input/bert-base-multilingual-cased&quot;)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 1280, in from_pretrained
    &quot;Unable to load weights from h5 file. &quot;
OSError: Unable to load weights from h5 file. If you tried to load a TF 2.0 model from a PyTorch checkpoint, please set from_pt=True. 
</code></pre>
<p>Where am I going wrong?
Need help!
Thanks in advance.</p>
","9963435","","","","","2021-04-19 22:36:41","Cannot load BERT from local disk","<python><tensorflow><bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"58620282","1","","","2019-10-30 07:19:24","","8","2274","<p>I am attempting to update the pre-trained BERT model using an in house corpus. I have looked at the Huggingface transformer docs and I am a little stuck as you will see below.My goal is to compute simple similarities between sentences using the cosine distance but I need to update the pre-trained model for my specific use case. </p>

<p>If you look at the code below, which is precisely from the Huggingface docs. I am attempting to ""retrain"" or update the model and I assumed that special_token_1 and special_token_2 represent ""new sentences"" from my ""in house"" data or corpus. Is this correct? In summary, I like the already pre-trained BERT model but I would like to update it or retrain it using another in house dataset. Any leads will be appreciated. </p>

<pre><code>import tensorflow as tf
import tensorflow_datasets
from transformers import *

model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

SPECIAL_TOKEN_1=""dogs are very cute""
SPECIAL_TOKEN_2=""dogs are cute but i like cats better and my 
brother thinks they are more cute""

tokenizer.add_tokens([SPECIAL_TOKEN_1, SPECIAL_TOKEN_2])
model.resize_token_embeddings(len(tokenizer))
#Train our model
model.train()
model.eval()
</code></pre>
","8291021","","1150683","","2020-01-16 08:55:46","2020-10-26 08:07:26","Updating a BERT model through Huggingface transformers","<tensorflow><nlp><pytorch><spacy><huggingface-transformers>","1","8","","","","CC BY-SA 4.0"
"67157185","1","67158033","","2021-04-19 06:49:03","","0","42","<p>On the huggingface site documentation, it says 'The output directory where the model predictions and checkpoints will be written'. I don't quite understand what it means. Do I have to create any file for that?</p>
","14653046","","","","","2021-04-19 07:56:03","What does 'output_dir' mean in transformers.TrainingArguments?","<python><bert-language-model><huggingface-transformers><ray-tune>","1","0","","","","CC BY-SA 4.0"
"67157289","1","","","2021-04-19 06:58:21","","1","129","<p>I am using <a href=""https://github.com/deepset-ai/haystack"" rel=""nofollow noreferrer"">deepset/haystack</a> and communicating with elastic search. Using OpenDistroElasticsearchDocumentStore method works fine with username,pasword access to aws elastic search. Doesnt seem to work with role based access when deployed in ec2. Please suggest me a solution to access aws elastic search using python elastic search package given a role access</p>
","9071233","","252627","","2021-05-03 15:50:24","2021-05-03 15:50:24","Access aws elastic search role based using python elastic search package","<python><elasticsearch><huggingface-transformers><aws-elasticsearch><haystack>","1","0","","","","CC BY-SA 4.0"
"67158554","1","67187807","","2021-04-19 08:32:28","","1","164","<p>I would like to fine-tune already fine-tuned BertForSequenceClassification model with new dataset containing just 1 additional label which hasn't been seen by model before.</p>
<p>By that, I would like to add 1 new label to the set of labels that model is currently able of classifying properly.</p>
<p>Moreover, I don't want classifier weights to be randomly initialized, I'd like to keep them intact and just update them accordingly to the dataset examples while increasing the size of classifier layer by 1.</p>
<p>The dataset used for further fine-tuning could look like this:</p>
<pre><code>sentece,label
intent example 1,new_label
intent example 2,new_label
...
intent example 10,new_label
</code></pre>
<p>My model's current classifier layer looks like this:</p>
<pre><code>Linear(in_features=768, out_features=135, bias=True)
</code></pre>
<p>How could I achieve it?<br>
Is it even a good approach?</p>
","14661154","","6664872","","2021-04-21 00:20:14","2021-04-21 00:20:14","Fine-tuning model's classifier layer with new label","<pytorch><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"67257008","1","67479054","","2021-04-25 18:40:25","","2","1249","<p>I am trying to run a model on TPU as given in <a href=""https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb"" rel=""nofollow noreferrer"">colab notebook</a>. The model was working fine, but today I could not run the model.</p>
<p>I used the following code to install pytorch-xla.</p>
<pre><code>VERSION = &quot;nightly&quot;  #@param [&quot;1.5&quot; , &quot;20200325&quot;, &quot;nightly&quot;]
!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
!python pytorch-xla-env-setup.py --version $VERSION
</code></pre>
<p>I try to install required libraries as below:</p>
<pre><code>!pip install -U nlp
!pip install sentencepiece
!pip install numpy --upgrade
</code></pre>
<p>However, when I try the following</p>
<pre><code>import nlp
</code></pre>
<p>It gives the following error:</p>
<pre><code>OSError: libmkl_intel_lp64.so.1: cannot open shared object file: No such file or directory
</code></pre>
<p>I searched the error and I tried the followings, but still does not work. Any ideas how to fix it? Note: It was working a few days ago, however, today it is not.</p>
<pre><code>!pip install mkl
#!export PATH=&quot;$PATH:/opt/intel/bin&quot;
#!export LD_LIBRARY_PATH=&quot;$PATH:opt/intel/mkl/lib/intel64_lin/&quot;
!export LID_LIBRAEY_PATH=&quot;$LID_LIBRARY_PATH:/opt/intel/mkl/lib/intel64_lin/&quot;
</code></pre>
","14301445","","14301445","","2021-04-25 19:42:36","2021-05-11 00:20:57","OSError: libmkl_intel_lp64.so.1: cannot open shared object file: No such file or directory","<pytorch><google-colaboratory><kaggle><huggingface-transformers><tpu>","1","5","","","","CC BY-SA 4.0"
"68925863","1","68927119","","2021-08-25 15:35:47","","0","75","<p>I need to work with the pretrained BERT model (<code>'dbmdz/bert-base-italian-xxl-cased'</code>) from Huggingface with Tensorflow (at <a href=""https://huggingface.co/dbmdz/bert-base-italian-xxl-cased"" rel=""nofollow noreferrer"">this</a> link).</p>
<p>After reading this on the website,</p>
<blockquote>
<p>Currently only PyTorch-Transformers compatible weights are available. If you need access to TensorFlow checkpoints, please raise an issue!</p>
</blockquote>
<p>I raised the issue and promptly a download link to an archive containing the following files was given to me. The files are the following ones:</p>
<pre class=""lang-sh prettyprint-override""><code>$ ls bert-base-italian-xxl-cased/
config.json                    model.ckpt.index               vocab.txt
model.ckpt.data-00000-of-00001 model.ckpt.meta
</code></pre>
<p>I'm now trying to load the model and work with it but everything I tried failed.</p>
<p>I tried following <a href=""https://discuss.huggingface.co/t/load-weight-from-local-ckpt-file/3854"" rel=""nofollow noreferrer"">this</a> suggestion from an Huggingface discussion site:</p>
<pre class=""lang-py prettyprint-override""><code>bert_folder = str(Config.MODELS_CONFIG.BERT_CHECKPOINT_DIR) # folder in which I have the files extracted from the archive
from transformers import BertConfig, TFBertModel
config = BertConfig.from_pretrained(bert_folder) # this gets loaded correctly
</code></pre>
<p>After this point I tried several combinations in order to load the model but always unsuccessfully.</p>
<p>eg:</p>
<pre><code>model = TFBertModel.from_pretrained(&quot;../../models/pretrained/bert-base-italian-xxl-cased/model.ckpt.index&quot;, config=config)

model = TFBertModel.from_pretrained(&quot;../../models/pretrained/bert-base-italian-xxl-cased/model.ckpt.index&quot;, config=config, from_pt=True)

model = TFBertModel.from_pretrained(&quot;../../models/pretrained/bert-base-italian-xxl-cased/model.ckpt.index&quot;, config=config, from_pt=True)

model = TFBertModel.from_pretrained(&quot;../../models/pretrained/bert-base-italian-xxl-cased&quot;, config=config, local_files_only=True)

</code></pre>
<p>Always results in this error:</p>
<pre><code>404 Client Error: Not Found for url: https://huggingface.co/models/pretrained/bert-base-italian-xxl-cased/model.ckpt.index/resolve/main/tf_model.h5
...
...
OSError: Can't load weights for '../../models/pretrained/bert-base-italian-xxl-cased/model.ckpt.index'. Make sure that:

- '../../models/pretrained/bert-base-italian-xxl-cased/model.ckpt.index' is a correct model identifier listed on 'https://huggingface.co/models'

- or '../../models/pretrained/bert-base-italian-xxl-cased/model.ckpt.index' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.
</code></pre>
<p>So my question is: <strong>How can I load this pre-trained BERT model from those files and use it in tensorflow?</strong></p>
","11579184","","","","","2021-08-25 17:16:46","Problem building tensorflow model from huggingface weights","<python><tensorflow><keras><tensorflow2.0><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67004233","1","","","2021-04-08 12:42:28","","0","218","<h1>Basic Overview</h1>
<p>I am working on pre-training a Masked Language Model (Vanilla <code>Longformer</code> to be precise) pretrained on a custom dataset. I am using Huggingface's <code>transformers</code> lib but when fine-tuning my MLM on a supervised task, this error appears:-</p>
<pre class=""lang-py prettyprint-override""><code>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&lt;ipython-input-175-83ded7c85bb3&gt; in &lt;module&gt;()
     45     )
     46 
---&gt; 47 train_results = trainer.train()

4 frames

/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
   1118                         tr_loss += self.training_step(model, inputs)
   1119                 else:
-&gt; 1120                     tr_loss += self.training_step(model, inputs)
   1121                 self._total_flos += float(self.floating_point_ops(inputs))
   1122 

/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in training_step(self, model, inputs)
   1522                 loss = self.compute_loss(model, inputs)
   1523         else:
-&gt; 1524             loss = self.compute_loss(model, inputs)
   1525 
   1526         if self.args.n_gpu &gt; 1:

/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in compute_loss(self, model, inputs, return_outputs)
   1554         else:
   1555             labels = None
-&gt; 1556         outputs = model(**inputs)
   1557         # Save past state if it exists
   1558         # TODO: this needs to be fixed and made cleaner later.

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    887             result = self._slow_forward(*input, **kwargs)
    888         else:
--&gt; 889             result = self.forward(*input, **kwargs)
    890         for hook in itertools.chain(
    891                 _global_forward_hooks.values(),

/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py in forward(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1841         if global_attention_mask is None:
   1842             logger.info(&quot;Initializing global attention on CLS token...&quot;)
-&gt; 1843             global_attention_mask = torch.zeros_like(input_ids)
   1844             # global attention on cls token
   1845             global_attention_mask[:, 0] = 1

TypeError: zeros_like(): argument 'input' (position 1) must be Tensor, not NoneType
</code></pre>
<p>Now, this seems to stem from the inputs given to the model most probably - Huggingface has upgraded to now handling direct <code>datasets</code> object and has dropped out the previous BPE classes.</p>
<p>for my data, I dumped my both <code>train</code> and <code>validation</code> NumPy arrays in two separate files:-</p>
<pre class=""lang-py prettyprint-override""><code>src{tgt
....{17
</code></pre>
<p>where <code>{</code> ---&gt; delimiter and <code>src</code> and <code>tgt</code> are the columns. this structure is for both train and val files.
Importantly, my <code>src</code> = string; <code>tgt</code> = number (numeric label). This is a sequence classification task.</p>
<p>Next up, I construct the <code>datasets</code> object from the files using the CSV script:-</p>
<pre class=""lang-py prettyprint-override""><code>from datasets import load_dataset
train_dataset = load_dataset('csv', data_files=&quot;HF_train.txt&quot;, delimiter='{')
val_dataset = load_dataset('csv', data_files=&quot;HF_val.txt&quot;, delimiter='{')
</code></pre>
<p>After this step completes, I import the tokenizer --&gt; <code>tokenizer = AutoTokenizer.from_pretrained('......')</code> from my pretrained Language Model with <code>truncation &amp; padding = True</code></p>
<h1>The suspicious part of the code</h1>
<p>Now, time for tokenization. I used the <code>.map()</code> method to apply the tokenization function on my whole dataset. this is how my tokenization function looks like:-</p>
<pre class=""lang-py prettyprint-override""><code>def tok(example):
  encodings = tokenizer(example['src'], truncation=True)
  return encodings
</code></pre>
<p>The reason I apply it only to <code>src</code> is because my labels are numeric - no tokenization there and only the &quot;X&quot; value which is a long string.</p>
<p>This is how I use that function to apply on my datasets (using <code>.map()</code>):-</p>
<pre class=""lang-py prettyprint-override""><code>train_encoded_dataset = train_dataset.map(tok, batched=True)
val_encoded_dataset = val_dataset.map(tok, batched=True)
</code></pre>
<p>Most probably, this is the part I have messed up because I have no idea how to use the <code>datasets</code> object.</p>
<p>This how my datasets object looks likes, hopefully you might be able to understand it's structure more than I do:-</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; train_dataset
&gt;&gt;&gt; DatasetDict({
    train: Dataset({
        features: ['src', 'tgt'],
        num_rows: 4572
    })
})

&gt;&gt;&gt; train_dataset['train']
&gt;&gt;&gt; Dataset({
    features: ['src', 'tgt'],
    num_rows: 4572
})

&gt;&gt;&gt; train_dataset['train']['src']
&gt;&gt;&gt; [..Giant list of all sequences.present..in.dataset --&gt; **(untokenized)**]

&gt;&gt;&gt;train_dataset['train'][0]
&gt;&gt;&gt;{'src': 'Kasam.....',
 'tgt': 13}

</code></pre>
<p>Now, I explore the so-called <strong>tokenized</strong> dataset (<code>train_encoded_dataset</code>):-</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; train_encoded_dataset
&gt;&gt;&gt; DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'src', 'tgt'],
        num_rows: 4572
    })
})

&gt;&gt;&gt; train_encoded_dataset['train']
&gt;&gt;&gt; Dataset({
     features: ['attention_mask', 'input_ids', 'src', 'tgt'],
     num_rows: 4572
 })

&gt;&gt;&gt; print(train_encoded_dataset['train'][0])
&gt;&gt;&gt; {'attention_mask': [1, 1, 1, 1, 1, 1, 1,...Long list of several numbers than increase..], 'src': 'Kasa....Long string which is a focument', 'tgt': 13}     #tgt being label

</code></pre>
<p>After this, I pass this dataset into <code>Trainer</code>:-</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted',zero_division=1)  #none gives score for each class
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

training_args = TrainingArguments(
    output_dir='/content/results/',          # output directory
    overwrite_output_dir = True,
    num_train_epochs=16,              # total number of training epochs
    per_device_train_batch_size=32,  # batch size per device during training
    per_device_eval_batch_size=32,   # batch size for evaluation
    warmup_steps=600,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='/content/logs',            # directory for storing logs
    logging_steps=10,
    evaluation_strategy='epoch',
    learning_rate=1e-6,
    #fp16 = True,
    load_best_model_at_end = True,
    metric_for_best_model = 'eval_loss',
    greater_is_better = False,
    seed = 101,
    save_total_limit=5,
)

model = AutoModelForSequenceClassification.from_pretrained(&quot;.....&quot;, num_labels=20)

trainer = Trainer(
    model=model,                         # the instantiated Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset['train'],         # training dataset
    eval_dataset=val_dataset['train'],             # evaluation dataset
    compute_metrics=compute_metrics
    )

train_results = trainer.train()
</code></pre>
<p>Which leads to the error posted above.</p>
<p>Now I am not sure where the problem has arisen (apart from tokenization). could anyone point it out?</p>
<p><strong>Update1:</strong> Constructing the dataset using the <code>from_dict</code> method (for native conversion of numpy arrays to <code>datasets</code> object) yields the same error.</p>
<p><strong>Update2:</strong> Apparently, with a few changes, I am getting a new error:-
This is the new <code>tok</code> function:</p>
<pre class=""lang-py prettyprint-override""><code>def tok(example):
  encodings = tokenizer(example['src'], truncation=True, padding=True)
  return encodings
</code></pre>
<p>with padding and truncation added again. After proper training args (passing the tokenized rather than un-tokenized)</p>
<pre class=""lang-py prettyprint-override""><code>trainer = Trainer(
    model=model,                         # the instantiated Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_encoded_dataset,         # training dataset
    eval_dataset=val_encoded_dataset,             # evaluation dataset
    compute_metrics=compute_metrics
    )
</code></pre>
<p>yields this:-</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

&lt;ipython-input-78-6068ea33d5d4&gt; in &lt;module&gt;()
     45     )
     46 
---&gt; 47 train_results = trainer.train()

4 frames

/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
   1099             self.control = self.callback_handler.on_epoch_begin(self.args, self.state, self.control)
   1100 
-&gt; 1101             for step, inputs in enumerate(epoch_iterator):
   1102 
   1103                 # Skip past any already trained steps if resuming training

/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py in __next__(self)
    515             if self._sampler_iter is None:
    516                 self._reset()
--&gt; 517             data = self._next_data()
    518             self._num_yielded += 1
    519             if self._dataset_kind == _DatasetKind.Iterable and \

/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py in _next_data(self)
    555     def _next_data(self):
    556         index = self._next_index()  # may raise StopIteration
--&gt; 557         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    558         if self._pin_memory:
    559             data = _utils.pin_memory.pin_memory(data)

/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     45         else:
     46             data = self.dataset[possibly_batched_index]
---&gt; 47         return self.collate_fn(data)

/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py in default_data_collator(features)
     78                 batch[k] = torch.stack([f[k] for f in features])
     79             else:
---&gt; 80                 batch[k] = torch.tensor([f[k] for f in features])
     81 
     82     return batch

ValueError: expected sequence of length 2033 at dim 1 (got 2036)
</code></pre>
<p>Which is unexpected. will dig more about this.</p>
","8648710","","8648710","","2021-04-10 17:28:14","2021-04-10 17:28:14","TypeError: zeros_like(): argument 'input' when fine-tuning on MLM","<python><machine-learning><huggingface-transformers><huggingface-tokenizers>","0","0","1","","","CC BY-SA 4.0"
"67097467","1","67097860","","2021-04-14 18:48:10","","1","246","<p>I have recently read about Bert and want to use BertForMaskedLM for fill_mask task. I know about Bert architecture. Also, as far as I know, BertForMaskedLM  is built from Bert with a language modeling head on top, but I have no idea about what <em>language modeling head</em> means here. Can anyone give me a brief explanation.</p>
","14479895","","","","","2021-04-14 19:18:53","About BertForMaskedLM","<nlp><bert-language-model><huggingface-transformers><language-model>","1","0","","","","CC BY-SA 4.0"
"67269076","1","","","2021-04-26 15:02:57","","2","85","<p>I'm currently working on a text summarizer powered by the Huggingface transformers library. The summarization process has to be done on premise, as such I have the following code (close to documentation):</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig
model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')
tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-6-6')

inputs = tokenizer([myTextToSummarize], max_length=1024, return_tensors='pt')
summary_ids = model.generate(inputs['input_ids'], num_beams=4, early_stopping=True)
[tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]
</code></pre>
<p>My problem is that I cannot load the model in memory and have my server expose an API which can directly use <code>model</code> and <code>tokenizer</code>, I would like both of them to be initialized in a first process, and made available in a second one (one that will expose an HTTP API). I saw that you can export the model on the filesystem, but again, I don't have access to it (locked k8s environment), and I'd need to store it in a specific database.</p>
<p>Is it possible to export both the <code>model</code>and the <code>tokenizer</code> as string/buffer/something storable in a Database ?</p>
<p>Thanks a lot</p>
","2514387","","","","","2021-04-26 15:02:57","Huggingface transformer export tokenizer and model","<huggingface-transformers><huggingface-tokenizers>","0","2","","","","CC BY-SA 4.0"
"67193312","1","","","2021-04-21 09:47:54","","1","645","<p>I am learning how to use the Huggingface Transformers library, building a binary classification BERT model, on the Kaggle Twitter Disaster Dataset.</p>
<p>Upon entering the training loop, I get the following error, during the forward() function execution:</p>
<pre><code>Epoch 1/50
----------
Aici incepe train_epoch

/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))

----Checkpoint train_epoch 2----
----Checkpoint train_epoch 2----
----forward checkpoint 1----

---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

&lt;ipython-input-175-fd9f98819b6f&gt; in &lt;module&gt;()
     23     device,
     24     scheduler,
---&gt; 25     df_train.shape[0]
     26   )
     27     print(f'Train loss {train_loss} Accuracy:{train_acc}')

4 frames

&lt;ipython-input-173-bfbecd87c5ec&gt; in train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)
     21         targets = d['targets'].to(device)
     22         print('----Checkpoint train_epoch 2----')
---&gt; 23         outputs = model(input_ids=input_ids,attention_mask=attention_mask)
     24         print('----Checkpoint train_epoch 3----')
     25         _,preds = torch.max(outputs,dim=1)

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    887             result = self._slow_forward(*input, **kwargs)
    888         else:
--&gt; 889             result = self.forward(*input, **kwargs)
    890         for hook in itertools.chain(
    891                 _global_forward_hooks.values(),

&lt;ipython-input-171-e754ea3edc36&gt; in forward(self, input_ids, attention_mask)
     16                 input_ids=input_ids,
     17                 attention_mask=attention_mask,
---&gt; 18                 return_dict=False)
     19 
     20         print('----forward checkpoint 2-----')

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    887             result = self._slow_forward(*input, **kwargs)
    888         else:
--&gt; 889             result = self.forward(*input, **kwargs)
    890         for hook in itertools.chain(
    891                 _global_forward_hooks.values(),

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    923         elif input_ids is not None:
    924             input_shape = input_ids.size()
--&gt; 925             batch_size, seq_length = input_shape
    926         elif inputs_embeds is not None:
    927             input_shape = inputs_embeds.size()[:-1]

ValueError: too many values to unpack (expected 2)
</code></pre>
<p>At first, I thought it was related to the return_dict=False change that they added, but I was wrong.
The code for the classifier and training loop is down below</p>
<p>Classifier:</p>
<pre><code>class DisasterClassifier(nn.Module):
    def __init__(self, n_classes):
        super(DisasterClassifier,self).__init__()
        self.bert=BertModel.from_pretrained(PRE_TRAINED_MODEL,return_dict=False)
        self.drop=nn.Dropout(p=0.3) # in timpul antrenarii, valori aleatorii sunt inlocuite cu 0, cu probabilitate p -&gt; regularization and preventing the co-adaptation of neurons
        self.out = nn.Linear(self.bert.config.hidden_size,n_classes)
        
    def forward(self,input_ids,attention_mask):
        print('----forward checkpoint 1----')
        bertOutput = self.bert(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=False)
        
        print('----forward checkpoint 2-----')
        output = self.drop(bertOutput['pooler_output'])
        return self.out(output)`
</code></pre>
<p>Training epoch:</p>
<pre><code>optimizer = AdamW(model.parameters(),lr = 2e-5,correct_bias=False)
total_steps = len(train_data_loader)*EPOCHS
scheduler = get_linear_schedule_with_warmup(
                                            optimizer,
                                            num_warmup_steps=0,
                                            num_training_steps=total_steps)
loss_fn = nn.CrossEntropyLoss().to(device)

def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):
 print('Aici incepe train_epoch') 
 model = model.train()
 losses =[]
 correct_predictions = 0
    
 for d in data_loader:
        print('----Checkpoint train_epoch 2----')
        input_ids = d['input_ids'].to(device)
        attention_mask=d['attention_mask'].to(device)
        targets = d['targets'].to(device)
        print('----Checkpoint train_epoch 2----')
        outputs = model(input_ids=input_ids,attention_mask=attention_mask)
        print('----Checkpoint train_epoch 3----')
        _,preds = torch.max(outputs,dim=1)
        loss = loss_fn(outputs, targets)
    
        correct_predictions += torch.sum(preds == targets)
        losses.append(loss.item())

        #backpropagation steps
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters,max_norm=1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

   
 return (correct_predictions.double() / n_examples), np.mean(losses)
</code></pre>
<p>And the training loop:</p>
<pre><code>history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):
    print(f'Epoch {epoch + 1}/{EPOCHS}')
    print('-' * 10)
    
    # train_acc,train_loss = train_epoch(model,
    #                                    train_data_loader,
    #                                    loss_fn,
    #                                    optimizer,
    #                                    device,
    #                                    scheduler,
    #                                    len(df_train))
    
    train_acc, train_loss = train_epoch(
    model,
    train_data_loader,    
    loss_fn, 
    optimizer, 
    device, 
    scheduler, 
    df_train.shape[0]
  )
    print(f'Train loss {train_loss} Accuracy:{train_acc}')
    
    val_acc, val_loss = eval_model(model,val_data_loader,loss_fn,device,len(df_val))
    print(f'Validation loss {val_loss} Accuracy:{val_acc}')
    print()
        
    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['val_acc'].append(val_acc)
    history['val_loss'].append(val_loss)

    if val_acc &gt; best_accuracy:
        torch.save(model.state_dict(), 'best_model_state.bin')
        best_accuracy = val_acc
</code></pre>
<p>Has anybody encountered a similar situation?</p>
","15717625","","15717625","","2021-04-21 12:53:05","2021-05-17 14:12:04","Huggingface Transformers returning 'ValueError: too many values to unpack (expected 2)', upon training a Bert binary classification model","<python><nlp><classification><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"67195895","1","","","2021-04-21 12:24:52","","1","54","<pre><code>dic = [] 
for step, batch in tqdm(enumerate(train_dataloader)):
inpt = batch[0].to(device)
msks = batch[1].to(device)
#Run the sentences through the model
outputs = model_obj(inpt, msks)
dic.append( {
    'hidden_states': outputs[2],
    'pooled_output': outputs[1]})
</code></pre>
<p>I want to save the model output in each iteration but I got the below error for a small set of datasets.
<strong>RuntimeError: CUDA out of memory.</strong>
notice that without the below code my model works correctly.</p>
<p><code>dic.append( { 'hidden_states': outputs[2], 'pooled_output': outputs[1]})</code></p>
<p>How can I save these outputs in each iteration?</p>
","12700672","","","","","2021-04-21 12:40:17","save model output in pytorch","<nlp><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67221901","1","","","2021-04-22 23:09:25","","0","118","<p>I'm using the huggingface library to generate text using the pre-trained distilgpt2 model. In particular, I am making use of the <a href=""https://huggingface.co/transformers/main_classes/model.html?highlight=sample#transformers.generation_utils.GenerationMixin.beam_search"" rel=""nofollow noreferrer"">beam_search</a> function, as I would like to include a LogitsProcessorList (which you can't use with the <a href=""https://huggingface.co/transformers/main_classes/model.html?highlight=sample#transformers.generation_utils.GenerationMixin.generate"" rel=""nofollow noreferrer"">generate</a> function).</p>
<p>The relevant portion of my code looks like this:</p>
<pre><code>beam_scorer = BeamSearchScorer(
            batch_size=btchsze,
            max_length=15,  # not sure why lengths under 20 fail
            num_beams=num_seq,
            device=model.device,
        )
j = input_ids.tile((num_seq*btchsze,1))
next_output = model.beam_search(
            j, 
            beam_scorer,
            eos_token_id=tokenizer.encode('.')[0],
            logits_processor=logits_processor
        )
</code></pre>
<p>However, the beam_search function throws this error when I try to generate using a max_length of less than 20:</p>
<pre><code>~/anaconda3/envs/techtweets37/lib/python3.7/site-packages/transformers-4.4.2-py3.8.egg/transformers/generation_beam_search.py in finalize(self, input_ids, final_beam_scores, final_beam_tokens, final_beam_indices, pad_token_id, eos_token_id)
    326         # fill with hypotheses and eos_token_id if the latter fits in
    327         for i, hypo in enumerate(best):
--&gt; 328             decoded[i, : sent_lengths[i]] = hypo
    329             if sent_lengths[i] &lt; self.max_length:
    330                 decoded[i, sent_lengths[i]] = eos_token_id

RuntimeError: The expanded size of the tensor (15) must match the existing size (20) at non-singleton dimension 0.  Target sizes: [15].  Tensor sizes: [20]
</code></pre>
<p>I can't seem to figure out where 20 is coming from: it's the same even if the input length is longer or shorter, even if I use a different batch size or number of beams. There's nothing I've defined as length 20, nor can I find any default. The max length of the sequence does effect the results of the beam search, so I'd like to figure this out and be able to set a shorter max length.</p>
","3837352","","","","","2021-04-23 10:29:16","Mismatched tensor size error when generating text with beam_search (huggingface library)","<python><nlp><pytorch><huggingface-transformers><gpt-2>","1","0","","","","CC BY-SA 4.0"
"67286034","1","67303291","","2021-04-27 15:25:10","","1","364","<p>I have a labeled dataset in a pandas dataframe.</p>
<pre><code>&gt;&gt;&gt; df.dtypes
title          object
headline       object
byline         object
dateline       object
text           object
copyright    category
country      category
industry     category
topic        category
file           object
dtype: object
</code></pre>
<p>I am building a model to predict <code>topic</code> based on <code>text</code>. While <code>text</code> is a large string, <code>topic</code> is a list of strings. For example:</p>
<pre><code>&gt;&gt;&gt; df['topic'].head(5)
0    ['ECONOMIC PERFORMANCE', 'ECONOMICS', 'EQUITY ...
1      ['CAPACITY/FACILITIES', 'CORPORATE/INDUSTRIAL']
2    ['PERFORMANCE', 'ACCOUNTS/EARNINGS', 'CORPORAT...
3    ['PERFORMANCE', 'ACCOUNTS/EARNINGS', 'CORPORAT...
4    ['STRATEGY/PLANS', 'NEW PRODUCTS/SERVICES', 'C...
</code></pre>
<p>Before I put this through a model, I have to tokenize this whole dataframe, yet when running it through transformer's <code>Autotokenizer</code> I get getting an error.</p>
<pre><code>import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import AutoTokenizer
import tensorflow_hub as hub
import tensorflow_text as text
from sklearn.model_selection import train_test_split

def preprocess_text(df):

    # Remove punctuations and numbers
    df['text'] = df['text'].str.replace('[^a-zA-Z]', ' ', regex=True)

    # Single character removal
    df['text'] = df['text'].str.replace(r&quot;\s+[a-zA-Z]\s+&quot;, ' ', regex=True)

    # Removing multiple spaces
    df['text'] = df['text'].str.replace(r'\s+', ' ', regex=True)

    # Remove NaNs
    df['text'] = df['text'].fillna('')
    df['topic'] = df['topic'].cat.add_categories('').fillna('')

    return df

# Load tokenizer and logger
tf.get_logger().setLevel('ERROR')
tokenizer = AutoTokenizer.from_pretrained('roberta-large')

# Load dataframe with just text and topic columns
# Only loading first 100 rows for testing purposes
df = pd.DataFrame()
for chunk in pd.read_csv(r'C:\Users\pfortier\Documents\Reuters\test.csv', sep='|', chunksize=100,
                dtype={'topic': 'category', 'country': 'category', 'industry': 'category', 'copyright': 'category'}):
    df = chunk
    break
df = preprocess_text(df)

# Split dataset into train, test, val (70, 15, 15)
train, test = train_test_split(df, test_size=0.15)
train, val = train_test_split(train, test_size=0.15)

# Tokenize datasets
train = tokenizer(train, return_tensors='tf', truncation=True, padding=True, max_length=128)
val = tokenizer(val, return_tensors='tf', truncation=True, padding=True, max_length=128)
test = tokenizer(test, return_tensors='tf', truncation=True, padding=True, max_length=128)
</code></pre>
<p>I get this error:</p>
<pre><code>AssertionError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).
</code></pre>
<p>on the line <code>train = tokenizer(train, return_tensors='tf', truncation=True, padding=True, max_length=128)</code>.</p>
<p>Does this mean I have to turn my df into a list?</p>
","7396306","","","","","2021-04-28 15:33:11","Tokenizing a dataframe using Tensorflow and Transformers","<python><dataframe><tensorflow><tokenize><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68860984","1","","","2021-08-20 10:45:04","","1","58","<p>I am using the zero shot classification pipeline provided by huggingface. I am trying to perform multiprocessing to parallelize the question answering. This is what I have tried till now</p>
<pre><code>from pathos.multiprocessing import ProcessingPool as Pool
import multiprocess.context as ctx
from functools import partial
ctx._force_start_method('spawn') 

os.environ[&quot;TOKENIZERS_PARALLELISM&quot;] = &quot;false&quot;
os.environ[&quot;CUDA_LAUNCH_BLOCKING&quot;]=&quot;1&quot;

classifier = pipeline(&quot;zero-shot-classification&quot;, model=&quot;typeform/distilbert-base-uncased-mnli&quot;, device=0)

def get_label_score(classifier, labels,sentence):
    aa = classifier(sentence, labels)
    return aa['labels'][0],aa['scores'][0]

if __name__ == '__main__':
    pool = Pool(8)
    results = pool.map(
      partial(get_label_score, classifier,tag_values),
      d_lst)
    pool.close()
    pool.terminate()
    pool.join()
</code></pre>
<p>This does not improve the timing versus a for loop and results in OOM for list size greater than 10K sentences</p>
<p>I have also tried using executor.map this also does not improve the timings.</p>
<p>Please assist on how I can speed up inference</p>
","10020032","","","","","2021-08-20 11:04:03","Multiprocessing/Multithreading for huggingface pipeline","<python><multithreading><pytorch><multiprocessing><huggingface-transformers>","0","5","","","","CC BY-SA 4.0"
"68928299","1","68928610","","2021-08-25 18:41:52","","0","28","<p>I am looking to implement DistilBERT via fastai and huggingface for a mutliclass sequence classification problem. I found a useful tutorial that gave a good example on how to do this with binary classification. The code is below:</p>
<pre><code># !pip install torch==1.9.0
# !pip install torchtext==0.10
# !pip install transformers==4.7
# !pip install fastai==2.4

from fastai.text.all import *
from sklearn.model_selection import train_test_split
import pandas as pd
import glob
from transformers import AutoTokenizer, AutoModelForSequenceClassification


hf_tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;)
hf_model = AutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)

&quot;&quot;&quot;
train_df and val_df looks like this:

      label text
4240    5   whoa interesting.
13      7   you could you could we just
4639    4   you set the goal,
28      1   because ive already agreed to that
66      8   oh hey freshman thats you gona need
&quot;&quot;&quot;

print(list(train_df.label.value_counts().index))
&quot;&quot;&quot;
[4, 1, 5, 6, 7, 0, 2, 3, 8]
&quot;&quot;&quot;

class HF_Dataset(torch.utils.data.Dataset):
    def __init__(self, df, hf_tokenizer):
        self.df = df
        self.hf_tokenizer = hf_tokenizer
        
        self.label_map = {
            0:0,
            1:0,
            2:0,
            3:0,
            4:1,
            5:1,
            6:1,
            7:1,
            8:1
        }
        
    def __len__(self):
        return len(self.df)

    def decode(self, token_ids):
        return ' '.join([hf_tokenizer.decode(x) for x in tokenizer_outputs['input_ids']])
    
    def decode_to_original(self, token_ids):
        return self.hf_tokenizer.decode(token_ids.squeeze())

    def __getitem__(self, index):
        label, text = self.df.iloc[index]
        label = self.label_map[label]
        label = torch.tensor(label)

        tokenizer_output = self.hf_tokenizer(text, return_tensors=&quot;pt&quot;, padding='max_length', truncation=True, max_length=512)
        
        tokenizer_output['input_ids'].squeeze_()
        tokenizer_output['attention_mask'].squeeze_()
        
        return tokenizer_output, label
        

train_dataset = HF_Dataset(train_df, hf_tokenizer)
valid_dataset = HF_Dataset(valid_df, hf_tokenizer)

train_dl = DataLoader(train_dataset, bs=16, shuffle=True)
valid_dl = DataLoader(valid_dataset, bs=16)
dls = DataLoaders(train_dl, valid_dl)
hf_model(**batched_data)


class HF_Model(nn.Module):
  
    def __init__(self, hf_model):
        super().__init__()
        
        self.hf_model = hf_model
        
    def forward(self, tokenizer_outputs):
        
        model_output = self.hf_model(**tokenizer_outputs)
        
        return model_output.logits
        
model = HF_Model(hf_model)
# Manually popping the model onto the gpu since the data is in a dictionary format
# (doesn't automatically place model + data on gpu otherwise)
learn = Learner(dls, model, loss_func=nn.CrossEntropyLoss(), metrics=[accuracy])
learn.fit_one_cycle(3, 1e-4)
</code></pre>
<p>This works fine. However, I mapped my multiclass labels to 2 labels to allow this to work. I actually have 9 classes. I tried adjusting the label mapping scheme in <code>HF_Dataset()</code> class to match my actual labels like below:</p>
<pre><code>class HF_Dataset(torch.utils.data.Dataset):
    def __init__(self, df, hf_tokenizer):
        self.df = df
        self.hf_tokenizer = hf_tokenizer
        
        self.label_map = {
            0:0,
            1:1,
            2:2,
            3:3,
            4:4,
            5:5,
            6:6,
            7:7,
            8:8
        }
        
    def __len__(self):
        return len(self.df)

    def decode(self, token_ids):
        return ' '.join([hf_tokenizer.decode(x) for x in tokenizer_outputs['input_ids']])
    
    def decode_to_original(self, token_ids):
        return self.hf_tokenizer.decode(token_ids.squeeze())

    def __getitem__(self, index):
        label, text = self.df.iloc[index]
        label = self.label_map[label]
        label = torch.tensor(label)

        tokenizer_output = self.hf_tokenizer(text, return_tensors=&quot;pt&quot;, padding='max_length', truncation=True, max_length=512)
        
        tokenizer_output['input_ids'].squeeze_()
        tokenizer_output['attention_mask'].squeeze_()
        
        return tokenizer_output, label
</code></pre>
<p>Every line works until <code>learn.fit_one_cycle</code>.</p>
<p>Here is the full stack trace from this line:</p>
<pre><code>
 0.00% [0/3 00:00&lt;00:00]
epoch   train_loss  valid_loss  accuracy    time

 0.00% [0/519 00:00&lt;00:00]
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-21-0ec2ff9e12e1&gt; in &lt;module&gt;
----&gt; 1 learn.fit_one_cycle(3, 1e-4)

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/callback/schedule.py in fit_one_cycle(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)
    111     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),
    112               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}
--&gt; 113     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)
    114 
    115 # Cell

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt)
    219             self.opt.set_hypers(lr=self.lr if lr is None else lr)
    220             self.n_epoch = n_epoch
--&gt; 221             self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)
    222 
    223     def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_fit(self)
    210         for epoch in range(self.n_epoch):
    211             self.epoch=epoch
--&gt; 212             self._with_events(self._do_epoch, 'epoch', CancelEpochException)
    213 
    214     def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_epoch(self)
    204 
    205     def _do_epoch(self):
--&gt; 206         self._do_epoch_train()
    207         self._do_epoch_validate()
    208 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_epoch_train(self)
    196     def _do_epoch_train(self):
    197         self.dl = self.dls.train
--&gt; 198         self._with_events(self.all_batches, 'train', CancelTrainException)
    199 
    200     def _do_epoch_validate(self, ds_idx=1, dl=None):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in all_batches(self)
    167     def all_batches(self):
    168         self.n_iter = len(self.dl)
--&gt; 169         for o in enumerate(self.dl): self.one_batch(*o)
    170 
    171     def _do_one_batch(self):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in one_batch(self, i, b)
    192         b = self._set_device(b)
    193         self._split(b)
--&gt; 194         self._with_events(self._do_one_batch, 'batch', CancelBatchException)
    195 
    196     def _do_epoch_train(self):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_one_batch(self)
    173         self('after_pred')
    174         if len(self.yb):
--&gt; 175             self.loss_grad = self.loss_func(self.pred, *self.yb)
    176             self.loss = self.loss_grad.clone()
    177         self('after_loss')

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/loss.py in forward(self, input, target)
   1119     def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
   1120         return F.cross_entropy(input, target, weight=self.weight,
-&gt; 1121                                ignore_index=self.ignore_index, reduction=self.reduction)
   1122 
   1123 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)
   2822     if size_average is not None or reduce is not None:
   2823         reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 2824     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2825 
   2826 

IndexError: Target 6 is out of bounds.
</code></pre>
<p>This seems like it should be a simple fix. Do I need to adjust something in the model architecture to allow it to accept 9 labels? Or do I need to one hot encode my labels? If so, is there a solution prebuilt to do this in the pipeline?</p>
","11666502","","11666502","","2021-08-25 19:01:37","2021-08-25 19:09:25","multiclass sequence classifiaction with fastai and huggingface","<python><deep-learning><pytorch><huggingface-transformers><fast-ai>","1","2","","","","CC BY-SA 4.0"
"67112206","1","","","2021-04-15 16:17:36","","4","87","<p>Title. I'm currently trying to run import a module that uses transformers but it throws the following error:</p>
<pre><code>(tf2venv) dante@dante-Inspiron-5570:~/projects/classification$ inv process-pdf test.pdf
Using TensorFlow backend.
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])
/home/dante/projects/classification/venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])
Using TensorFlow backend.
Traceback (most recent call last):
  File &quot;/home/dante/projects/classification/venv/bin/inv&quot;, line 8, in &lt;module&gt;
    sys.exit(program.run())
  File &quot;/home/dante/projects/classification/venv/lib/python3.7/site-packages/invoke/program.py&quot;, line 373, in run
    self.parse_collection()
  File &quot;/home/dante/projects/classification/venv/lib/python3.7/site-packages/invoke/program.py&quot;, line 465, in parse_collection
    self.load_collection()
  File &quot;/home/dante/projects/classification/venv/lib/python3.7/site-packages/invoke/program.py&quot;, line 696, in load_collection
    module, parent = loader.load(coll_name)
  File &quot;/home/dante/projects/classification/venv/lib/python3.7/site-packages/invoke/loader.py&quot;, line 76, in load
    module = imp.load_module(name, fd, path, desc)
  File &quot;/home/dante/.pyenv/versions/3.7.0/lib/python3.7/imp.py&quot;, line 235, in load_module
    return load_source(name, filename, file)
  File &quot;/home/dante/.pyenv/versions/3.7.0/lib/python3.7/imp.py&quot;, line 172, in load_source
    module = _load(spec)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 696, in _load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/home/dante/projects/classification/tasks.py&quot;, line 10, in &lt;module&gt;
    from app import ClassifyDocument
  File &quot;/home/dante/projects/classification/app.py&quot;, line 15, in &lt;module&gt;
    from docType_classification import classify, grouping, utils
  File &quot;/home/dante/projects/classification/docType_classification/classify.py&quot;, line 13, in &lt;module&gt;
    import common.hybrid as hybrid
  File &quot;/home/dante/projects/classification/common/hybrid.py&quot;, line 3, in &lt;module&gt;
    import transformers
ModuleNotFoundError: No module named 'transformers'
</code></pre>
<p>The code in <code>common/hybrid.py</code> is as follows:</p>
<pre><code>import transformers
from tokenizers import BertWordPieceTokenizer
import tqdm
import numpy as np

def build_tokenizer():
    # load the real tokenizer
    tokenizer = transformers.DistilBertTokenizer.from_pretrained(
        &quot;distilbert-base-uncased&quot;
    )
    # Save the loaded tokenizer locally
    tokenizer.save_pretrained(&quot;.&quot;)
    # Reload it with the huggingface tokenizers library
    hugging_face_tokenizer = BertWordPieceTokenizer(&quot;vocab.txt&quot;, lowercase=False)
    return hugging_face_tokenizer


def encode(texts, tokenizer, chunk_size=256, maxlen=512):
    tokenizer.enable_truncation(max_length=maxlen)
    tokenizer.enable_padding(length=maxlen)
    all_ids = []

    print(len(texts))
    for i in tqdm(range(0, len(texts), chunk_size)):
        text_chunk = texts[i : i + chunk_size].tolist()
        encs = tokenizer.encode_batch(text_chunk)
        all_ids.extend([enc.ids for enc in encs])

    return np.array(all_ids)
</code></pre>
<p>It is imported in <code>classify.py</code> as:</p>
<pre><code>import common.hybrid as hybrid
</code></pre>
<p>I'm able to compile and run this file with</p>
<pre><code>python3 common/hybrid.py
</code></pre>
<p>without any errors.</p>
<p>When running an invoke task with</p>
<pre><code>invoke process-data
</code></pre>
<p>the file tasks.py is located in the root project directory.</p>
<p>I get the <code>ModuleNotFoundError</code> as soon as it reaches the transformers import.</p>
<p>Note that even when adding</p>
<pre><code>import tensorflow
</code></pre>
<p>above the transformers import, this is imported correctly and the error isn't thrown until</p>
<pre><code>import transformers
</code></pre>
<p><code>pip freeze</code> output:</p>
<pre><code>absl-py==0.12.0
appdirs==1.4.4
astunparse==1.6.3
attrs==20.3.0
backcall==0.2.0
bearbones==2.300
black==20.8b1
boto3==1.9.85
botocore==1.12.253
cachetools==4.2.1
certifi==2020.12.5
cfgv==3.2.0
chardet==3.0.4
click==7.1.2
decorator==5.0.6
distlib==0.3.1
docutils==0.15.2
fancycompleter==0.9.1
filelock==3.0.12
flake8==3.9.0
fuzzysearch==0.7.3
gast==0.3.3
google-auth==1.28.1
google-auth-oauthlib==0.4.4
google-pasta==0.2.0
grpcio==1.37.0
h5py==2.10.0
identify==2.2.3
idna==2.8
invoke==1.5.0
ipython==7.14.0
ipython-genutils==0.2.0
isort==5.8.0
jedi==0.18.0
jmespath==0.10.0
joblib==1.0.1
Keras==2.3.1
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.2
lxml==4.6.3
Markdown==3.3.4
mccabe==0.6.1
more-itertools==8.7.0
mypy-extensions==0.4.3
nodeenv==1.6.0
numpy==1.20.2
oauthlib==3.1.0
opt-einsum==3.3.0
packaging==20.9
pandas==1.1.5
parso==0.8.2
pathspec==0.8.1
pdbpp==0.10.2
pdf2image==1.10.0
pdftotext==2.1.5
pexpect==4.8.0
pickleshare==0.7.5
pikepdf==1.7.1
Pillow==8.2.0
pipdeptree==2.0.0
pluggy==0.13.1
pre-commit==2.12.0
prompt-toolkit==3.0.18
protobuf==3.15.8
ptyprocess==0.7.0
py==1.10.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycodestyle==2.7.0
pyflakes==2.3.1
Pygments==2.8.1
pyparsing==2.4.7
PyPDF2==1.26.0
pyrepl==0.9.0
pytest==5.3.5
python-dateutil==2.8.1
pytz==2021.1
PyYAML==5.4.1
redis==3.3.11
regex==2021.4.4
requests==2.21.0
requests-oauthlib==1.3.0
rsa==4.7.2
s3transfer==0.1.13
sacremoses==0.0.44
scipy==1.4.1
sentencepiece==0.1.95
six==1.15.0
tenacity==6.0.0
tensorboard==2.2.2
tensorboard-plugin-wit==1.8.0
tensorflow==2.2.0
tensorflow-estimator==2.2.0
termcolor==1.1.0
tokenizers==0.10.2
toml==0.10.2
tqdm==4.60.0
traitlets==5.0.5
transformers==4.4.2
typed-ast==1.4.3
typing-extensions==3.7.4.3
urllib3==1.24.1
virtualenv==20.4.3
wcwidth==0.2.5
Werkzeug==1.0.1
wmctrl==0.3
wrapt==1.12.1
</code></pre>
<p>other info:</p>
<pre><code>(tf2venv) dante@dante-Inspiron-5570:~/projects/classification$ which python
/home/dante/projects/classification/tf2venv/bin/python
(tf2venv) dante@dante-Inspiron-5570:~/projects/classification$ which inv
/home/dante/projects/classification/tf2venv/bin/inv
(tf2venv) dante@dante-Inspiron-5570:~/projects/classification$ python3 --version
Python 3.8.0
</code></pre>
<p>Note that there are no circular imports and I've tried various versions of transformers(v3-4)</p>
<p>Everything was installed with <code>pip3</code>, the <code>venv</code> was created with</p>
<pre><code>python3 -m venv tf2venv
</code></pre>
<p>I've tried deleting the <code>venv</code> and reinstalling various times but nothing works. Is there something missing that is causing this ModuleNotFoundError with transformers?</p>
<p>My <code>requirements.txt</code> is</p>
<pre><code>bearbones&gt;=2
fuzzysearch~=0.7.3
ipython~=7.14.0
Keras~=2.3.0
pdf2image~=1.10.0
pikepdf~=1.7.0
tenacity~=6.0.0
tensorflow==2.2.0
transformers==3.0.2
pandas~=1.1.5
pytest~=5.3.2
pdftotext~=2.1.4

</code></pre>
","15617695","","8323650","","2021-04-15 22:07:32","2021-04-15 22:07:32","ModuleNotFoundError with python transformers library despite it being installed in venv when running invoke task","<python-3.x><tensorflow><huggingface-transformers><python-venv><pyinvoke>","0","2","0","","","CC BY-SA 4.0"
"67129129","1","","","2021-04-16 16:43:20","","0","57","<p>I want to solve a sequence-to-sequence text generation task (e.g. question answering, language translation, etc.).</p>
<p>For the purposes of this question, you may assume that I already have the input part already handled. (I already have a tensor of dimensions batch_size x num_input_tokens x input_dim representing the input sequences. Also, all input sequences in my problem are of the same length, so no masking is required on the input side of things).</p>
<p>Now, I want to generate the output sequences using nn.TransformerDecoder. I'm aware of Pytorch's official tutorial <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noreferrer"">SEQUENCE-TO-SEQUENCE MODELING WITH NN.TRANSFORMER AND TORCHTEXT</a>. Unfortunately, the official tutorial doesn't meet my needs, for the following reasons:</p>
<ul>
<li>nn.TransformerDecoder is not used in the example.</li>
<li>The example is about language modeling, not text generation. There is no forward loop that generates text word by word.</li>
</ul>
<p>I've searched around the web and I've found a few things, but nothing like a simple and minimal working example that directly applies to my problem setting. Concretely, on the output side of things I need the following:</p>
<ul>
<li>I want to generate output sequences in batch. I've found codes on GitHub where people appear to be doing text generation, but they do it for a single sequence at a time, not a batch of multiple sequences.</li>
<li>The output sequences may have different lengths.</li>
<li>I want to train my model with the teacher-forcing strategy and batches of multiple sequences. Given that in training I know the lengths of the sequences in advance, you may assume that I already have my batches padded with zeroes. However, I still need to figure out how to implement the forward function of my model, with a generation loop that uses nn.TransformerDecoder. Basically, I need to figure out how to iterate word-wise over my batch of output sequences, masking out the future words in each step (so that the model doesn't cheat by trivially predicting the next words).</li>
<li>Then, I need a similar forward function for inference mode. I need to figure out how to implement the generation loop to do basically the same as in training mode, except that instead of teacher-forcing I want to implement greedy search (i.e. use the tokens with highest predicted probability at iteration i as the next input for iteration i+1).</li>
</ul>
<p>I already know how to do all this using LSTMs. Below you can see the forward function of a model that I implemented in the past to do exactly what I just said with an LSTM. The same forward function is used for both training and inference, depending on the value of the variable 'mode':</p>
<pre><code>  def forward(
      self,
      image_local_features,
      question_vectors,
      answers=None,
      max_answer_length=None,
      mode='train',
  ):
    if mode == 'train':
      batch_size, max_answer_length = answers.shape
      assert answers is not None
    else:
      batch_size = image_local_features.size(0)
      assert max_answer_length is not None
    
    y = self.embedding_table(self.start_idx).expand(batch_size, -1)
    o = torch.zeros(batch_size, self.hidden_size).to(DEVICE)
    h = self.W_h(question_vectors)
    c = self.W_c(question_vectors)

    if mode == 'train':
      answer_embeddings = self.embedding_table(answers.permute(1,0))
      assert answer_embeddings.shape == (max_answer_length, batch_size, self.embed_size)

    output = []

    for t in range(max_answer_length):
      y_bar = torch.cat((y,o),1)
      assert y_bar.shape == (batch_size, self.embed_size + self.hidden_size)
      assert h.shape == (batch_size, self.hidden_size)
      assert c.shape == (batch_size, self.hidden_size)
      h, c = self.lstm_cell(y_bar, (h, c))
      e = (self.W_attn(image_local_features) * h.unsqueeze(1)).sum(-1)
      att = torch.softmax(e,-1)
      a = (image_local_features * att.unsqueeze(2)).sum(1)
      assert a.shape == (batch_size, self.image_local_feat_size)
      u = torch.cat((a,h),1)
      assert u.shape == (batch_size, self.hidden_size + self.image_local_feat_size)
      v = self.W_u(u)
      o = self.dropout(torch.tanh(v))
      assert o.shape == (batch_size, self.hidden_size)
      output.append(self.W_vocab(o))
      if mode == 'train':
        y = answer_embeddings[t] # teacher-forcing
      else:
        y = self.embedding_table(torch.argmax(output[t], 1)) # greedy search
      assert y.shape == (batch_size, self.embed_size)

    output = torch.stack(output, 1)
    assert output.shape == (batch_size, max_answer_length, self.vocab_size)
    return output
</code></pre>
<p>Another way to phrase my question would be: how can I reimplement what I did with LSTMs using nn.TransformerDecoder instead?</p>
<p>Any minimal working / <em>hello world</em> example that shows how to do batch training and batch inference with nn.TransformerDecoder for text generation will be very appreciated.</p>
<hr />
<p><em>Note</em>: alternatively, if there is a straightforward way of accomplishing the same with an out-of-the-box solution from <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">hugginface</a>, that would be awesome too.</p>
","2801404","","","","","2021-04-16 16:43:20","PyTorch: minimal working example or tutorial showing how to use nn.TransformerDecoder for batch text generation in training and inference modes?","<nlp><pytorch><huggingface-transformers><transformer>","0","0","","","","CC BY-SA 4.0"
"67150015","1","","","2021-04-18 15:01:47","","0","167","<p>I was trying out the NER tutorial <code>Token Classification with W-NUT Emerging Entities</code> (<a href=""https://huggingface.co/transformers/custom_datasets.html#tok-ner"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/custom_datasets.html#tok-ner</a>) in google colab using the <code>Annotated Corpus for Named Entity Recognition</code> data on Kaggle (<a href=""https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus?select=ner_dataset.csv"" rel=""nofollow noreferrer"">https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus?select=ner_dataset.csv</a>).</p>
<p>I will outline my process in detail to facilitate an understanding of what I was doing and to let the community help me figure out the source of the indexing assignment error.</p>
<p>To load the data from google drive where I have saved it, I used the following code</p>
<pre><code># import pandas library
import pandas as pd

# columns to select
cols_to_select = [&quot;Sentence #&quot;, &quot;Word&quot;, &quot;Tag&quot;]

# google drive data path
data_path =  '/content/drive/MyDrive/Colab Notebooks/ner/ner_dataset.csv'

# load the data from google colab
dataset = pd.read_csv(data_path, encoding=&quot;latin-1&quot;)[cols_to_select].fillna(method = 'ffill')
</code></pre>
<p>I run the following code to parse the sentences and tags</p>
<pre><code>class SentenceGetter(object):
    
    def __init__(self, data):
        self.n_sent = 1
        self.data = data
        self.empty = False
        agg_func = lambda s: [(w, t) for w, t in zip(s[&quot;Word&quot;].values.tolist(),
                                                     s[&quot;Tag&quot;].values.tolist())]
        self.grouped = self.data.groupby(&quot;Sentence #&quot;).apply(agg_func)
        self.sentences = [s for s in self.grouped]
    
    def retrieve(self):
        try:
            s = self.grouped[&quot;Sentence: {}&quot;.format(self.n_sent)]
            self.n_sent += 1
            return s
        except:
            return None
</code></pre>
<pre><code>
# get full data
getter = SentenceGetter(dataset)

# get sentences
sentences = [[s[0] for s in sent] for sent in getter.sentences]

# get tags/labels
tags = [[s[1] for s in sent] for sent in getter.sentences]

# take a look at the data
print(sentences[0][0:5], tags[0][0:5], sep='\n')
</code></pre>
<p>I then split the data into train, val, and test sets</p>
<pre><code># import the sklearn module
from sklearn.model_selection import train_test_split

# split data in to temp and test sets
temp_texts, test_texts, temp_tags, test_tags = train_test_split(sentences,
                                                                tags,
                                                                test_size=0.20,
                                                                random_state=15)

# split data into train and validation sets
train_texts, val_texts, train_tags, val_tags = train_test_split(temp_texts,
                                                                temp_tags, 
                                                                test_size=0.20,
                                                                random_state=15)
</code></pre>
<p>After splitting the data, I created encodings for tags and the tokens</p>
<pre><code>unique_tags=dataset.Tag.unique()

# create tags to id
tag2id = {tag: id for id, tag in enumerate(unique_tags)}

# create id to tags
id2tag = {id: tag for tag, id in tag2id.items()}
</code></pre>
<p>I then installed the transformer library in colab</p>
<pre><code># install the transformers library
! pip install transformers
</code></pre>
<p>Next I imported the small bert model</p>
<pre><code># import the transformers module
from transformers import BertTokenizerFast

# import the small bert model
model_name = &quot;google/bert_uncased_L-4_H-512_A-8&quot;
tokenizer = BertTokenizerFast.from_pretrained(model_name)
</code></pre>
<p>I then created the encodings for the tokens</p>
<pre><code># create train set encodings
train_encodings = tokenizer(train_texts, 
                            is_split_into_words=True, 
                            return_offsets_mapping=True, 
                            padding=True,
                            max_length=128,
                            truncation=True)

# create validation set encodings
val_encodings = tokenizer(val_texts,
                          is_split_into_words=True, 
                          return_offsets_mapping=True, 
                          padding=True,
                          max_length=128,
                          truncation=True)

# create test set encodings
test_encodings = tokenizer(test_texts,
                          is_split_into_words=True,
                          return_offsets_mapping=True, 
                          padding=True,
                          max_length=128,
                          truncation=True)
</code></pre>
<p>In the tutorial, it uses offset-mapping to handle the problem that arise with word-piece tokenization, specifically, the mismatch between tokens and labels. It is when  running the offset-mapping code in the tutorial that I get the error.  Below is the offset mapping function used in the tutorial:</p>
<pre><code># the offset function
import numpy as np

def encode_tags(tags, encodings):
    labels = [[tag2id[tag] for tag in doc] for doc in tags]
    encoded_labels = []
    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):
        # create an empty array of -100
        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100
        arr_offset = np.array(doc_offset)

        # set labels whose first offset position is 0 and the second is not 0
        doc_enc_labels[(arr_offset[:,0] == 0) &amp; (arr_offset[:,1] != 0)] = doc_labels
        encoded_labels.append(doc_enc_labels.tolist())

    return encoded_labels

# return the encoded labels
train_labels = encode_tags(train_tags, train_encodings)
val_labels = encode_tags(val_tags, val_encodings)
test_labels = encode_tags(test_tags, test_encodings)
</code></pre>
<p>After running the above code, it gives me the following error, and I can't figure out where the source of the error lies. Any help and pointers would be appreciated.</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-19-afdff0186eb3&gt; in &lt;module&gt;()
     17 
     18 # return the encoded labels
---&gt; 19 train_labels = encode_tags(train_tags, train_encodings)
     20 val_labels = encode_tags(val_tags, val_encodings)
     21 test_labels = encode_tags(test_tags, test_encodings)

&lt;ipython-input-19-afdff0186eb3&gt; in encode_tags(tags, encodings)
     11 
     12         # set labels whose first offset position is 0 and the second is not 0
---&gt; 13         doc_enc_labels[(arr_offset[:,0] == 0) &amp; (arr_offset[:,1] != 0)] = doc_labels
     14         encoded_labels.append(doc_enc_labels.tolist())
     15 

ValueError: NumPy boolean array indexing assignment cannot assign 38 input values to the 37 output values where the mask is true
</code></pre>
","12096670","","","","","2021-07-23 10:42:31","Huggingface Transformers NER - Offset Mapping Causing ValueError in NumPy boolean array indexing assignment","<python-3.x><numpy><huggingface-transformers><ner>","0","0","","","","CC BY-SA 4.0"
"67288454","1","","","2021-04-27 18:07:17","","0","68","<p>I'm using Trainer &amp; TrainingArguments to train GPT2 Model, but it seems that this does not work well.</p>
<p>My datasets have the ids of the tokens of my corpus and the mask of each text, to indicate where to apply the attention:</p>
<pre><code>Dataset({
features: ['attention_mask', 'input_ids', 'labels'],
num_rows: 2012860
}))
</code></pre>
<p>I am doing the training with Trainer &amp; TrainingArguments, passing my model and my previous dataset as follows. But nowhere do I specify anything about the attention_mask:</p>
<pre><code>training_args = TrainingArguments(
output_dir=path_save_checkpoints,
overwrite_output_dir=True,
num_train_epochs=1,
per_device_train_batch_size = 4,
gradient_accumulation_steps = 4,
logging_steps = 5_000, save_steps=5_000,
fp16=True,
deepspeed=&quot;ds_config.json&quot;,
remove_unused_columns = True,
debug = True
)

trainer = Trainer(
model=model,
args=training_args,
data_collator=data_collator,
train_dataset=dataset,
tokenizer=tokenizer,
)

trainer.train()
</code></pre>
<p>How should I tell the Trainer to use this feature (attention_mask)?
If you take a look at the file /transformers/trainer.py there is no reference to &quot;attention&quot; or &quot;mask&quot;.</p>
<p>Thanks in advance!</p>
","2742509","","","","","2021-04-27 18:07:17","Train GPT2 with Trainer & TrainingArguments using/specifying attention_mask","<huggingface-transformers><attention-model><gpt-2>","0","0","","","","CC BY-SA 4.0"
"68835630","1","68839872","","2021-08-18 15:54:42","","0","39","<p>I am trying to solve a multilabel text classification problem using BERT from huggingface transformers library. The model is defined as follows:</p>
<pre><code>def create_model(encoder, nb_classes=3, lr=1e-5):

    # inputs
    input_ids = tf.keras.Input(shape=(512,), ragged=False,
                               dtype=tf.int32, name='input_ids')
    input_attention_mask = tf.keras.Input(shape=(512,), ragged=False,
                                          dtype=tf.int32, name='attention_mask')
    # transformer
    output = encoder({'input_ids': input_ids, 
                      'attention_mask': input_attention_mask})[0]
    Y = tf.keras.layers.BatchNormalization()(output)
    Y = tf.keras.layers.Dense(nb_classes, activation='sigmoid')(Y)

    # compilation
    model = tf.keras.Model(inputs=[input_ids, input_attention_mask], 
                           outputs=[Y])
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)

    # losses
    # loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)
    # loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)

    model.compile(optimizer=optimizer, 
                  loss=multilabel_loss, metrics=['acc'])
    model.summary()
    return model
</code></pre>
<p>As you can see, I tried to use tf.keras.losses, but it did not work (throwing <code>AttributeError: 'Tensor' object has no attribute 'nested_row_splits'</code>), so I defined a simple cross entropy by hand:</p>
<pre><code>def multilabel_loss(y_true, y_pred):
    y_pred = tf.convert_to_tensor(y_pred)
    y_true = tf.cast(y_true, y_pred.dtype)
    cross_entropy = -tf.reduce_sum((y_true*tf.math.log(y_pred + 1e-8) + (1 - y_true) * tf.math.log(1 - y_pred + 1e-8)),
                                   name='xentropy')
    return cross_entropy
</code></pre>
<p>The model is created with strategy.scope() as shown below, using 'distil-bert-uncased' as a checkpoint:</p>
<pre><code>with strategy.scope():
    encoder = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
    #encoder = TFRobertaForSequenceClassification.from_pretrained(checkpoint)
    model = create_model(encoder)
</code></pre>
<p>The labels are binary arrays:</p>
<pre><code>163350    [0, 0, 1]
118940    [0, 0, 1]
65243     [0, 0, 1]
30011     [0, 0, 1]
189713    [0, 1, 0]
</code></pre>
<p>They are combined with tokenized texts into a tf.dataset in a next function:</p>
<pre><code>def tf_text_data_prep(df):
    &quot;&quot;&quot;
    input: takes pandas dataframe
    output: returns tokenized tf.Dataset
    &quot;&quot;&quot;
    hugging_ds = Dataset.from_pandas(df)
    tokenized_ds = hugging_ds.map(
                      tokenize_function,
                      batched=True,
                      num_proc=strategy.num_replicas_in_sync,
                      remove_columns=[&quot;Text&quot;, '__index_level_0__'],
                      load_from_cache_file=True 
                      )
    
    # Convert to tensorflow
    tf_dataset = tokenized_ds.with_format(&quot;tensorflow&quot;)
    features = {x: tf_dataset[x].to_tensor() for x in tokenizer.model_input_names}
    tf_data = tf.data.Dataset.from_tensor_slices((features, tf_dataset[&quot;label&quot;]))
    return tf_data
</code></pre>
<p>The problem is when I launch the training, I get the error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-62-720b4634d50e&gt; in &lt;module&gt;()
----&gt; 1 get_ipython().run_cell_magic('time', '', 'steps_per_epoch = int(BUFFER_SIZE // BATCH_SIZE)\nprint(\n    f&quot;Model Params:\\nbatch_size: {BATCH_SIZE}\\nEpochs: {EPOCHS}\\n&quot;\n    f&quot;Step p. Epoch: {steps_per_epoch}\\n&quot;\n    f&quot;Initial Learning rate: {INITAL_LEARNING_RATE}&quot;\n)\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=1,\n)')

12 frames
/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell)
   2115             magic_arg_s = self.var_expand(line, stack_depth)
   2116             with self.builtin_trap:
-&gt; 2117                 result = fn(magic_arg_s, cell)
   2118             return result
   2119 

&lt;decorator-gen-53&gt; in time(self, line, cell, local_ns)

/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py in &lt;lambda&gt;(f, *a, **k)
    186     # but it's overkill for just that one bit of state.
    187     def magic_deco(arg):
--&gt; 188         call = lambda f, *a, **k: f(*a, **k)
    189 
    190         if callable(arg):

/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py in time(self, line, cell, local_ns)
   1191         else:
   1192             st = clock2()
-&gt; 1193             exec(code, glob, local_ns)
   1194             end = clock2()
   1195             out = None

&lt;timed exec&gt; in &lt;module&gt;()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1176                 _r=1):
   1177               callbacks.on_train_batch_begin(step)
-&gt; 1178               tmp_logs = self.train_function(iterator)
   1179               if data_handler.should_sync:
   1180                 context.async_wait()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    887 
    888       with OptionalXlaContext(self._jit_compile):
--&gt; 889         result = self._call(*args, **kwds)
    890 
    891       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    931       # This is the first call of __call__, so we have to initialize.
    932       initializers = []
--&gt; 933       self._initialize(args, kwds, add_initializers_to=initializers)
    934     finally:
    935       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    762     self._concrete_stateful_fn = (
    763         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 764             *args, **kwds))
    765 
    766     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   3048       args, kwargs = None, None
   3049     with self._lock:
-&gt; 3050       graph_function, _ = self._maybe_define_function(args, kwargs)
   3051     return graph_function
   3052 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3442 
   3443           self._function_cache.missed.add(call_context_key)
-&gt; 3444           graph_function = self._create_graph_function(args, kwargs)
   3445           self._function_cache.primary[cache_key] = graph_function
   3446 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3287             arg_names=arg_names,
   3288             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3289             capture_by_value=self._capture_by_value),
   3290         self._function_attributes,
   3291         function_spec=self.function_spec,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    997         _, original_func = tf_decorator.unwrap(python_func)
    998 
--&gt; 999       func_outputs = python_func(*func_args, **func_kwargs)
   1000 
   1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    670         # the function a weak reference to itself to avoid a reference cycle.
    671         with OptionalXlaContext(compile_with_xla):
--&gt; 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    673         return out
    674 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    984           except Exception as e:  # pylint:disable=broad-except
    985             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 986               raise e.ag_error_metadata.to_exception(e)
    987             else:
    988               raise

TypeError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:850 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:840 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:833 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 train_step
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:460 update_state
        metric_obj.update_state(y_t, y_p, sample_weight=mask)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/metrics_utils.py:86 decorated
        update_op = update_state_fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:177 update_state_fn
        return ag_update_state(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:659 update_state  **
        [y_true, y_pred], sample_weight)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/metrics_utils.py:546 ragged_assert_compatible_and_get_flat_values
        raise TypeError('One of the inputs does not have acceptable types.')

    TypeError: One of the inputs does not have acceptable types.
</code></pre>
<p>This same approach worked for ordinary binary classification, but not for multilabel.
I'd appreciate any help regarding the error or the approach in general.</p>
","10868870","","","","","2021-08-19 16:44:03","TypeError when trying to apply custom loss in a multilabel classification problem","<python><tensorflow><machine-learning><keras><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"68846383","1","","","2021-08-19 10:46:08","","0","20","<p>I know that GPT's attention is Masked Multihead Attention. I have a pretrained GPT model and I want to assign its weights for the Encoder of EncoderDecoderModel. Like:</p>
<pre><code>model = EncoderDecoderModel.from_encoder_decoder_pretrained(&quot;pretrained_gpt_name&quot;,&quot;pretrained_for_decoder&quot; , tie_encoder_decoder=True)
# Change masked attention to self attention here to make encoder bidirectional and copy weights.
</code></pre>
<p>I am quite new with Transformers and pytorch.</p>
","14479895","","","","","2021-08-19 10:46:08","Laverage Pretrained GPT for EncoderDecoderModel","<nlp><pytorch><huggingface-transformers><encoder-decoder><gpt>","0","0","","","","CC BY-SA 4.0"
"68850172","1","68854088","","2021-08-19 14:56:28","","0","126","<p>I am running a sentence transformer model and trying to truncate my tokens, but it doesn't appear to be working. My code is</p>
<pre><code>from transformers import AutoModel, AutoTokenizer
model_name = &quot;sentence-transformers/paraphrase-MiniLM-L6-v2&quot;
model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
    
text_tokens = tokenizer(text, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
text_embedding = model(**text_tokens)[&quot;pooler_output&quot;]
</code></pre>
<p>I keep getting the following warning:</p>
<pre><code>Token indices sequence length is longer than the specified maximum sequence length 
for this model (909 &gt; 512). Running this sequence through the model will result in 
indexing errors
</code></pre>
<p>I am wondering why setting <code>truncation=True</code> is not truncating my text to the desired length?</p>
","12096670","","","","","2021-08-19 20:18:00","Token indices sequence length Issue","<python><huggingface-transformers><sentence-transformers>","1","1","","","","CC BY-SA 4.0"
"63139394","1","","","2020-07-28 17:12:07","","1","899","<p>I am trying to use pre-trained model from transformers to predict on CPU with multiprocessing. Below code code gets stuck at step-4. Any thoughts on how to resolve this?
I referred to below but no luck:
<a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/notes/multiprocessing.html</a></p>
<pre class=""lang-py prettyprint-override""><code>
import os
import torch
from transformers import BertForQuestionAnswering
from transformers import BertTokenizer
from torch.multiprocessing import Pool, Process, set_start_method

def answer_question(question, answer_text, model, tokenizer):
    input_ids = tokenizer.encode(question, answer_text)
    print('Query has {:,} tokens.\n'.format(len(input_ids)))
    sep_index = input_ids.index(tokenizer.sep_token_id)
    print(&quot;.. step -- 1&quot;)
    num_seg_a = sep_index + 1
    print(&quot;.. step -- 2&quot;)
    num_seg_b = len(input_ids) - num_seg_a
    print(&quot;.. step -- 3&quot;)
    segment_ids = [0]*num_seg_a + [1]*num_seg_b
    print(&quot;.. step -- 4&quot;)
    assert len(segment_ids) == len(input_ids)
    # ======== Evaluate ========
    # Run our example question through the model.
    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.
                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text
    print(&quot;.. step -- 5&quot;)
    # ======== Reconstruct Answer ========
    # Find the tokens with the highest `start` and `end` scores.
    answer_start = torch.argmax(start_scores)
    answer_end = torch.argmax(end_scores)
    print(&quot;.. step -- 6&quot;)
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    print(&quot;.. step -- 7&quot;)
    answer = tokens[answer_start]
    print(&quot;.. step -- 8&quot;)
    for i in range(answer_start + 1, answer_end + 1):
        if tokens[i][0:2] == '##':
            answer += tokens[i][2:]
        else:
            answer += ' ' + tokens[i]
    print('Answer: &quot;' + answer + '&quot;')

if __name__ == &quot;__main__&quot;:
    size = 2
    processes = []
    print(&quot;reading bert model&quot;)
    bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
    print(&quot;reading bert tokenizer&quot;)
    bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
    q_list = [&quot;How many parameters does BERT-large have?&quot;,&quot;How many parameters does BERT-large have?&quot;]
    a_list = [&quot;BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.&quot;, &quot;BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.&quot;]   

    for a in zip(q_list, a_list):
        print(a)
        q = Process(target = answer_question, args= (a[0], a[1], bert_model, bert_tokenizer))
        q.start()
        processes.append(q)

    for q in processes:
        q.join()
        


</code></pre>
","2289986","","3607203","","2020-07-31 08:12:35","2020-10-12 12:39:32","Running transformers predictions on CPU with multiprocessing","<pytorch><python-multiprocessing><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"67194634","1","67246204","","2021-04-21 11:07:07","","1","482","<p>I'm using transformers and I already have loaded a model and It works fine:</p>
<pre><code>from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer

task='sentiment'
MODEL = &quot;cardiffnlp/twitter-roberta-base-{task}&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL)

# PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL) 
model.save_pretrained(MODEL)
</code></pre>
<p>but If I try to load another task like &quot;emotion&quot; or &quot;hate&quot;, I get this error:</p>
<pre><code>from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer

task='emotion'
MODEL = &quot;cardiffnlp/twitter-roberta-base-{task}&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL)

# PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL)  ## Here I get the error
model.save_pretrained(MODEL)
</code></pre>
<p>This error:</p>
<pre><code>OSError: Can't load weights for 'cardiffnlp/twitter-roberta-base-emotion'. Make sure that:

- 'cardiffnlp/twitter-roberta-base-emotion' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'cardiffnlp/twitter-roberta-base-emotion' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.
</code></pre>
<p>I have checked it and these models actually exists are Hugging Face models, as you can see <a href=""https://huggingface.co/models?filter=arxiv:2010.12421"" rel=""nofollow noreferrer"">here</a>, so I dont get why is not working.</p>
<p>Edit: I have noticed that the first time I run it, It works with all the tasks (hate, emotion, sentiment) but If I try to run it again, then I get the error.</p>
","15259559","","15259559","","2021-04-21 14:43:04","2021-04-24 18:34:34","Error loading weights from a Hugging Face model","<python><error-handling><model><huggingface-transformers>","1","3","","","","CC BY-SA 4.0"
"67216646","1","","","2021-04-22 15:48:51","","1","257","<p>I'm trying to use both <code>BertTokenizer</code> and <code>BertForTokenClassification</code> offline. The goal of the project is to use these pretrained models and apply them to my own dataset for an NLP task. Due to network and security limitations, I am not able to install the <code>transformers</code> library from HuggingFace. I am only able to use PyTorch.</p>
<p>At this point, I have downloaded and saved the following <code>bert-base-uncased</code> files from the HuggingFace website to a local directory:</p>
<ul>
<li>config.json</li>
<li>python_model.bin</li>
<li>vocab.txt</li>
</ul>
<p>I've used the <code>transformers</code> library before, so I'm familiar with initializing the models from local files using something like <code>BertTokenizer.from_pretrained('/path/to/local')</code>. However, since I'm not able to install the package and call the model classes, I don't know how to use these downloaded local files in a similar manner. How do I use these local files to use <code>BertTokenizer</code> and <code>BertForTokenClassification</code>?</p>
<p>I've been instructed to use the following link to implement this: <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/saving_loading_models.html</a></p>
","12489798","","12489798","","2021-04-22 17:29:44","2021-04-22 17:29:44","BERT tokenizer and model without transformers library","<python><pytorch><bert-language-model><huggingface-transformers>","0","6","","","","CC BY-SA 4.0"
"63141267","1","63177834","","2020-07-28 19:14:25","","4","4377","<p>This is literally all the code that I am trying to run:</p>
<pre><code>from transformers import AutoModelWithLMHead, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)
model = AutoModelWithLMHead.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)
</code></pre>
<p>I am getting this error:</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-14-aad2e7a08a74&gt; in &lt;module&gt;
----&gt; 1 from transformers import AutoModelWithLMHead, AutoTokenizer
      2 import torch
      3 
      4 tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)
      5 model = AutoModelWithLMHead.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)

ImportError: cannot import name 'AutoModelWithLMHead' from 'transformers' (c:\python38\lib\site-packages\transformers\__init__.py)
</code></pre>
<p>What do I do about it?</p>
","11037848","","","","","2020-07-30 16:53:17","ImportError: cannot import name 'AutoModelWithLMHead' from 'transformers'","<python><pytorch><huggingface-transformers>","1","7","","","","CC BY-SA 4.0"
"63240385","1","","","2020-08-04 04:40:35","","0","1064","<p>I am getting <code>&quot;ValueError: You have to specify either input_ids or inputs_embeds&quot;</code> from a seemingly straightforward training example:</p>
<pre><code>Iteration:   0%|                                                                                                                                                             | 0/6694 [00:00&lt;?, ?it/s]
Epoch:   0%|                                                                                                                                                                    | 0/3 [00:00&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;train_masked_lm.py&quot;, line 33, in &lt;module&gt;
    trainer.train()
  File &quot;/home/zm/anaconda3/envs/electra/lib/python3.7/site-packages/transformers/trainer.py&quot;, line 503, in train
    tr_loss += self._training_step(model, inputs, optimizer)
  File &quot;/home/zm/anaconda3/envs/electra/lib/python3.7/site-packages/transformers/trainer.py&quot;, line 629, in _training_step
    outputs = model(**inputs)
  File &quot;/home/zm/anaconda3/envs/electra/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/home/zm/anaconda3/envs/electra/lib/python3.7/site-packages/transformers/modeling_electra.py&quot;, line 639, in forward
    return_tuple,
  File &quot;/home/zm/anaconda3/envs/electra/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/home/zm/anaconda3/envs/electra/lib/python3.7/site-packages/transformers/modeling_electra.py&quot;, line 349, in forward
    raise ValueError(&quot;You have to specify either input_ids or inputs_embeds&quot;)
ValueError: You have to specify either input_ids or inputs_embeds
</code></pre>
<p>My goal is to take a pre-trained model and train it a bit further based on additional data. New to transformers. Must be doing something wrong. Please help!</p>
<p>I adapted <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">https://huggingface.co/blog/how-to-train</a> as follows:</p>
<pre><code>from transformers import (
  ElectraForMaskedLM,
  ElectraTokenizer,
  Trainer,
  TrainingArguments,
  LineByLineTextDataset
)

model = ElectraForMaskedLM.from_pretrained('google/electra-base-generator')
tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-generator')

def to_dataset(input_file):
  return LineByLineTextDataset(file_path=input_file, tokenizer=tokenizer, block_size=128)


training_args = TrainingArguments(
  output_dir='./output',
  overwrite_output_dir=True,
  num_train_epochs=3,
  per_device_train_batch_size=64,
  per_device_eval_batch_size=64,
  save_steps=10000,
  warmup_steps=500,
  logging_dir='./logs',
)

trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=to_dataset('...../lines.txt'), # \n-separated lines of text (sentences)
)
trainer.train()
</code></pre>
<p>The aforementioned error fires off a few seconds after the script starts and is the very first output.</p>
","1445604","","1445604","","2020-08-04 20:50:27","2020-08-04 20:50:27","""ValueError: You have to specify either input_ids or inputs_embeds"" when using Trainer","<huggingface-transformers><huggingface-tokenizers>","0","2","","","","CC BY-SA 4.0"
"63244792","1","","","2020-08-04 10:17:55","","1","739","<p>This is my first time working on text classification. I am working on binary text classification with CamemBert using fast-bert library which is mostly inspired from fastai.</p>
<p>When I run the code below</p>
<pre><code>from fast_bert.data_cls import BertDataBunch
from fast_bert.learner_cls import BertLearner

databunch = BertDataBunch(DATA_PATH,LABEL_PATH,
                          tokenizer='camembert-base',
                          train_file='train.csv',
                          val_file='val.csv',
                          label_file='labels.csv',
                          text_col='text',
                          label_col='label',
                          batch_size_per_gpu=8, 
                          max_seq_length=512,
                          multi_gpu=multi_gpu,
                          multi_label=False,
                          model_type='camembert-base')

learner = BertLearner.from_pretrained_model(
                        databunch,
                        pretrained_path='camembert-base', #'/content/drive/My Drive/model/model_out'
                        metrics=metrics,
                        device=device_cuda,
                        logger=logger,
                        output_dir=OUTPUT_DIR,
                        finetuned_wgts_path=None, #WGTS_PATH
                        warmup_steps=300,
                        multi_gpu=multi_gpu,
                        is_fp16=True,
                        multi_label=False,
                        logging_steps=50)

learner.fit(epochs=10,
            lr=9e-5,
            validate=True,
            schedule_type=&quot;warmup_cosine&quot;,
            optimizer_type=&quot;adamw&quot;)
</code></pre>
<p>Everything works fine until training.
I get this error message when I try to train my model:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-13-9b5c6ad7c8f0&gt; in &lt;module&gt;()
      3             validate=True,
      4             schedule_type=&quot;warmup_cosine&quot;,
----&gt; 5             optimizer_type=&quot;adamw&quot;)

2 frames
/usr/local/lib/python3.6/dist-packages/fast_bert/learner_cls.py in fit(self, epochs, lr, validate, return_results, schedule_type, optimizer_type)
    421             # Evaluate the model against validation set after every epoch
    422             if validate:
--&gt; 423                 results = self.validate()
    424                 for key, value in results.items():
    425                     self.logger.info(

/usr/local/lib/python3.6/dist-packages/fast_bert/learner_cls.py in validate(self, quiet, loss_only)
    515             for metric in self.metrics:
    516                 validation_scores[metric[&quot;name&quot;]] = metric[&quot;function&quot;](
--&gt; 517                     all_logits, all_labels
    518                 )
    519             results.update(validation_scores)

/usr/local/lib/python3.6/dist-packages/fast_bert/metrics.py in fbeta(y_pred, y_true, thresh, beta, eps, sigmoid)
     56     y_pred = (y_pred &gt; thresh).float()
     57     y_true = y_true.float()
---&gt; 58     TP = (y_pred * y_true).sum(dim=1)
     59     prec = TP / (y_pred.sum(dim=1) + eps)
     60     rec = TP / (y_true.sum(dim=1) + eps)

RuntimeError: The size of tensor a (2) must match the size of tensor b (39) at non-singleton dimension 1 
</code></pre>
<p>How can I fix this ?
Thanks</p>
","13849474","","10886420","","2020-08-04 12:04:15","2020-08-11 11:07:29","The size of tensor a (2) must match the size of tensor b (39) at non-singleton dimension 1","<nlp><pytorch><bert-language-model><huggingface-transformers><fast-ai>","1","1","","","","CC BY-SA 4.0"
"67282155","1","68828797","","2021-04-27 11:27:42","","0","168","<p>I want to use a pre-trained BERT model in order to use it on a text classification task (I'm using Huggingface library). However, the pre-trained model was trained on domains that are different than mine, and I have a large unannotated dataset that can be used for fine-tuning it. If I use only my tagged examples and fine-tune it &quot;on the go&quot; while training on the specific task (BertForSequenceClassification), the dataset is too small for adapting the language model for the specific domain. What it the best way to do so?
Thanks!</p>
","15775762","","","","","2021-08-18 08:16:31","What is the simplest way to continue training a pre-trained BERT model, on a specific domain?","<nlp><text-classification><bert-language-model><huggingface-transformers><pytorch-lightning>","1","1","","","","CC BY-SA 4.0"
"63030692","1","66593509","","2020-07-22 09:07:00","","4","1955","<p>I want to use BertForMaskedLM or BertModel to calculate perplexity of a sentence, so I write code like this:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertForMaskedLM
# Load pre-trained model (weights)
with torch.no_grad():
    model = BertForMaskedLM.from_pretrained('hfl/chinese-bert-wwm-ext')
    model.eval()
    # Load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')
    sentence = &quot;æˆ‘ä¸ä¼šå¿˜è®°å’Œä½ ä¸€èµ·å¥‹æ–—çš„æ—¶å…‰ã€‚&quot;
    tokenize_input = tokenizer.tokenize(sentence)
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
    sen_len = len(tokenize_input)
    sentence_loss = 0.

    for i, word in enumerate(tokenize_input):
        # add mask to i-th character of the sentence
        tokenize_input[i] = '[MASK]'
        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])

        output = model(mask_input)

        prediction_scores = output[0]
        softmax = nn.Softmax(dim=0)
        ps = softmax(prediction_scores[0, i]).log()
        word_loss = ps[tensor_input[0, i]]
        sentence_loss += word_loss.item()

        tokenize_input[i] = word
    ppl = np.exp(-sentence_loss/sen_len)
    print(ppl)
</code></pre>
<p>I think this code is right, but I also notice BertForMaskedLM's paramaters <code>masked_lm_labels</code>, so could I use this paramaters to calculate PPL of a sentence easiler?
I know the input_ids argument is the masked input, the masked_lm_labels argument is the desired output. But I couldn't understand the actual meaning of its output loss, its code like this:</p>
<pre><code>if masked_lm_labels is not None:
    loss_fct = CrossEntropyLoss()  # -100 index = padding token
    masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), 
    masked_lm_labels.view(-1))
    outputs = (masked_lm_loss,) + outputs
</code></pre>
","10814749","","6664872","","2020-07-22 10:08:35","2021-03-15 11:14:55","How do I use BertForMaskedLM or BertModel to calculate perplexity of a sentence?","<nlp><pytorch><transformer><huggingface-transformers><bert-language-model>","1","0","5","","","CC BY-SA 4.0"
"63358768","1","63581521","","2020-08-11 13:04:45","","1","917","<p>BERT model for Language Model and Sequence classification includes an extra projection layer between the last transformer and the classification layer (it contains a linear layer of size <code>hidden_dim x hidden_dim</code>, a dropout layer and a <code>tanh</code> activation). This was not described in the paper originally but was clarified <a href=""https://github.com/google-research/bert/issues/43"" rel=""nofollow noreferrer"">here</a>. This intermediate layer is pre-trained together with the rest of the transformers.</p>
<p>In huggingface's <code>BertModel</code>, this layer is called <code>pooler</code>.</p>
<p>According to <a href=""https://arxiv.org/pdf/1912.05372.pdf#subsection.5.1"" rel=""nofollow noreferrer"">the paper</a>, FlauBERT model (XLMModel fine-tuned on French corpus) also includes this pooler layer: &quot;The classification head is composed of the following layers, in order: dropout, linear,tanhactivation, dropout, and linear.&quot;. However, when loading a FlauBERT model with huggingface (<em>e.g</em>, with <code>FlaubertModel.from_pretrained(...)</code>, or <code>FlaubertForSequenceClassification.from_pretrained(...)</code>), the model seem to include no such layer.</p>
<p>Hence the question: why is there no pooler layer in huggingfaces' FlauBERT model ?</p>
","5510226","","5510226","","2020-08-25 13:49:17","2020-08-25 14:51:08","Why is there no pooler layer in huggingfaces' FlauBERT model?","<bert-language-model><huggingface-transformers>","2","2","","","","CC BY-SA 4.0"
"68906740","1","","","2021-08-24 11:28:08","","0","36","<p>I have outputs from an encoder and want to experiment (comparing outputs) with various decoders. I am using the hugging face transformers library for this. It is mentioned in the documentation here -<a href=""https://huggingface.co/transformers/v2.11.0/model_doc/bert.html?highlight=decoder#bertmodel"" rel=""nofollow noreferrer"">Bert Model Hugging Face</a> that this can be done by changing the argument in configuration is_decoder=True. But I am unable to run it. Following is my code (encoder part I added so the problem can be recreated) -</p>
<pre><code>    from transformers import BertTokenizer, TFBertModel, BertModel, BertConfig
    
    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert_encoding = TFBertModel.from_pretrained('bert-base-uncased')
    
    question = 'Do you really like drinking coffee?'

    tokenized_input = bert_tokenizer(question, return_tensors=&quot;tf&quot;, padding=True)  
    
    encoded_input = bert_encoding(tokenized_input, output_hidden_states=True) # working fine giving 4 output = last_hidden_state, pooler_output, hidden_states, attentions
    
    configuration = BertConfig(is_decoder=True , add_cross_attention=True)    
    decoder = BertModel(configuration)
    print(decoder.config) # can verify is_decoder = True from the printed output

    last_hidden_states = encoded_input.last_hidden_state # shape is [1, 9, 768]
    output = decoder(encoder_hidden_states = last_hidden_states)
</code></pre>
<p>I am getting the following error -</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-17-a429e86fd56b&gt; in &lt;module&gt;()
      1 last_hidden_states = encoded_input.last_hidden_state
----&gt; 2 output = decoder(encoder_hidden_states = last_hidden_states)

1 frames
/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    942             batch_size, seq_length = input_shape
    943         else:
--&gt; 944             raise ValueError(&quot;You have to specify either input_ids or inputs_embeds&quot;)
    945 
    946         device = input_ids.device if input_ids is not None else inputs_embeds.device

ValueError: You have to specify either input_ids or inputs_embeds
</code></pre>
<p>In my problem statement, I don't have input_ids.</p>
<p>I also tried to use tokenized_input as input_ids in the above code. I get output [1, 9, 768] instead of [batch_size, sequence_length]. Does it mean my decoder is not working?</p>
<p>Thank you for your help.</p>
","9208618","","","","","2021-08-24 11:28:08","Unable to use is_decoder = True for hugging face transformers","<python><huggingface-transformers><decoder><encoder-decoder>","0","1","","","","CC BY-SA 4.0"
"67132964","1","","","2021-04-16 22:40:27","","1","69","<p>I am here because I am encountering a outmost obscure problem. Right when I begin by creating my model I encounter that my gpu usage spikes and then my python code crashes. This only happens when I try to use any of the models 'from_pretrained' only, I haven't had issues with neither Tensorflow nor PyTourch by themselves (this behavior is only native to transformers)</p>
<p>For example:
The problem arises when running this line of code, right at the beginning of my script ;</p>
<pre><code>model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')
</code></pre>
<p>I get the following messages, which are pretty standard but as you can see in the bottom the code simply stops.</p>
<pre><code>&lt;
2021-04-16 16:16:35.330093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-04-16 16:16:38.495667: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-04-16 16:16:38.519178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 computeCapability: 6.1
coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s
2021-04-16 16:16:38.519500: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-04-16 16:16:38.528695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-04-16 16:16:38.528923: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-04-16 16:16:38.533582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-04-16 16:16:38.535368: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-04-16 16:16:38.540093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-04-16 16:16:38.543728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-04-16 16:16:38.544662: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-04-16 16:16:38.544888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-04-16 16:16:38.545436: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-16 16:16:38.546588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 computeCapability: 6.1
coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s
2021-04-16 16:16:38.547283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-04-16 16:16:39.115250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-16 16:16:39.115490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306] 0
2021-04-16 16:16:39.115592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0: N
2021-04-16 16:16:39.115856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4634 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-04-16 16:16:39.419407: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-04-16 16:16:39.709427: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll

Process finished with exit code -1073741819 (0xC0000005)
</code></pre>
<p>Has any one else seen this? There something I am missing here?
Thank you for your help.</p>
<p>Here are the details of my system.</p>
<p>transformers version: Latest
Platform: Windows
Python version: 3.7
PyTorch version (GPU?): Latest
Tensorflow version (GPU?): Latest
Using GPU in script?: Yes, GeForce GTX 1060 computeCapability: 6.1
Using distributed or parallel set-up in script?: No
Models I encountered this error on:</p>
<p>Models that are related to this issue:
albert, bert, xlm:</p>
<p>--UPDATE:
I further narrow the issue down to all the TF[ModelName] pre trained models.</p>
","7942047","","7942047","","2021-04-17 15:52:45","2021-04-17 15:52:45","Python crashes when loading Bert model from pretrained","<python><tensorflow><bert-language-model><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"67203417","1","","","2021-04-21 20:59:54","","0","163","<p>I would like to use AllenNLP Interpret (code + demo) with a PyTorch classification model trained with HuggingFace (electra base discriminator). Yet, it is not obvious to me, how I can convert my model, and use it in a local allen-nlp demo server.</p>
<p>How should I proceed ?</p>
<p>Thanks in advance</p>
","15727016","","","","","2021-04-22 23:23:30","Using AllenNLP Interpret with a HuggingFace model","<nlp><huggingface-transformers><allennlp>","1","0","","","","CC BY-SA 4.0"
"63478947","1","63500593","","2020-08-19 01:57:17","","4","953","<p>For example, I want to train a BERT model from scratch but using the existing configuration. Is the following code the correct way to do so?</p>
<pre class=""lang-py prettyprint-override""><code>model = BertModel.from_pretrained('bert-base-cased')
model.init_weights()
</code></pre>
<p>Because I think the <code>init_weights</code> method will re-initialize all the weights.</p>
<p>Second question, if I want to change a bit the configuration, such as the number of hidden layers.</p>
<pre class=""lang-py prettyprint-override""><code>model = BertModel.from_pretrained('bert-base-cased', num_hidden_layers=10)
model.init_weights()
</code></pre>
<p>I wonder if the above is the correct way to do so. Because they don't appear to have an error when I run the above code.</p>
","2211979","","13769010","","2020-08-20 12:53:41","2020-08-20 17:00:26","Correct Way to Fine-Tune/Train HuggingFace's Model from scratch (PyTorch)","<python><pytorch><bert-language-model><huggingface-transformers>","1","1","3","","","CC BY-SA 4.0"
"67274470","1","","","2021-04-26 22:17:39","","1","306","<p>I am working on tensorflow 2 and I used Hugface's pre-trained BERT with Tensorflow 2 to do a text classification with it, here I want to create neural network with it:
from tensorflow.keras import layers</p>
<pre><code>from transformers import AutoModel

encoder = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)

## create neural network input

input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)

token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)

attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)

embedding = encoder(

input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask

)[0]
</code></pre>
<p>got the error: AttributeError: 'KerasTensor' object has no attribute 'size'</p>
","15771617","","","","","2021-05-10 05:18:14","AttributeError: 'KerasTensor' object has no attribute 'size with hugging-face bert.AutoModel model with tensorflow","<python><tensorflow><bert-language-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"67291062","1","67392339","","2021-04-27 21:43:01","","0","73","<p>I am using the <code>wandb</code> with my HuggingFace code. I would like to log the loss and other metrics. Now I have two questions</p>
<ul>
<li>How does <code>wandb</code> decide when to log the loss? Is this decided by <code>logging_steps</code> in <code>TrainingArguments(...)</code>ï¼Ÿ</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>training_args = TrainingArguments(output_dir=&quot;test&quot;, 
                                  learning_rate=lr,
                                  num_train_epochs=n_epoch,
                                  seed=seed,
                                  per_device_train_batch_size=2,
                                  per_device_eval_batch_size=2,
                                  logging_strategy=&quot;steps&quot;,
                                  logging_steps=5,
                                  report_to=&quot;wandb&quot;)
</code></pre>
<ul>
<li>How do I make sure <code>wandb</code> log other metrics (for example, adding validation metrics after each epoch)? Does this happen automatically?</li>
</ul>
","7784797","","","","","2021-05-04 21:16:13","Control the logging frequency and contents when using wandb with HuggingFace","<huggingface-transformers><wandb>","1","0","","","","CC BY-SA 4.0"
"61107371","1","61107682","","2020-04-08 18:18:06","","0","919","<p>How should I interpret the partial words with '##'s in them returned by the Transformer NER pipelines? Other tools like Flair and SpaCy return the word and their tag. I have worked with the CONLL dataset before and never noticed anything like this. Moreover, why are words being divided like this?</p>

<p>Example from the HuggingFace:</p>

<pre><code>from transformers import pipeline

nlp = pipeline(""ner"")

sequence = ""Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very"" \
           ""close to the Manhattan Bridge which is visible from the window.""

print(nlp(sequence))
</code></pre>

<p>Output:</p>

<pre><code>[
    {'word': 'Hu', 'score': 0.9995632767677307, 'entity': 'I-ORG'},
    {'word': '##gging', 'score': 0.9915938973426819, 'entity': 'I-ORG'},
    {'word': 'Face', 'score': 0.9982671737670898, 'entity': 'I-ORG'},
    {'word': 'Inc', 'score': 0.9994403719902039, 'entity': 'I-ORG'},
    {'word': 'New', 'score': 0.9994346499443054, 'entity': 'I-LOC'},
    {'word': 'York', 'score': 0.9993270635604858, 'entity': 'I-LOC'},
    {'word': 'City', 'score': 0.9993864893913269, 'entity': 'I-LOC'},
    {'word': 'D', 'score': 0.9825621843338013, 'entity': 'I-LOC'},
    {'word': '##UM', 'score': 0.936983048915863, 'entity': 'I-LOC'},
    {'word': '##BO', 'score': 0.8987102508544922, 'entity': 'I-LOC'},
    {'word': 'Manhattan', 'score': 0.9758241176605225, 'entity': 'I-LOC'},
    {'word': 'Bridge', 'score': 0.990249514579773, 'entity': 'I-LOC'}
]
</code></pre>
","1753315","","","","","2020-04-08 18:35:43","Transformer Pipeline for NER returns partial words with ##s","<python><pytorch><named-entity-recognition><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61132448","1","","","2020-04-10 00:48:11","","0","237","<p>Huggingface tranformers has a pipeline for question answering tuning on the Squad dataset.</p>

<p>What would I need to do to develop a pipeline for a question asking pipeline? This would use the context, question and answer to generate questions with answers from a context. Are there any examples for creating new hunggingface pipelines?</p>
","7509632","","","","","2020-04-10 08:03:07","Question asking pipeline for Huggingface transformers","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61913010","1","61913108","","2020-05-20 12:20:43","","4","7125","<p>I have installed <code>pytorch</code> with <code>conda</code> and <code>transformers</code> with <code>pip</code>.</p>

<p>I can <code>import transformers</code> without a problem but when I try to <code>import pipeline from transformers</code> I get an exception:</p>

<pre><code>from transformers import pipeline
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-4-69a9fd07ccac&gt; in &lt;module&gt;
----&gt; 1 from transformers import pipeline

ImportError: cannot import name 'pipeline' from 'transformers' (C:\Users\Alienware\Anaconda3\envs\tf2\lib\site-packages\transformers\__init__.py)
</code></pre>

<p>This is a view of the directory where it searches for the <strong>init</strong>.py file:</p>

<p><a href=""https://i.stack.imgur.com/cDIK7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cDIK7.png"" alt=""enter image description here""></a></p>

<p>What is causing the problem and how can I resolve it?</p>
","8270077","","1000551","","2020-05-21 07:40:24","2021-06-23 10:01:26","Can not import pipeline from transformers","<python><python-3.x><pipeline><huggingface-transformers><anaconda3>","2","0","","","","CC BY-SA 4.0"
"60839967","1","60847207","","2020-03-24 21:57:34","","2","872","<p>In the HuggingFace TensorFlow 2.0 BERT library, the <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""nofollow noreferrer"">documentation</a> states that:</p>

<blockquote>
  <p>TF 2.0 models accepts two formats as inputs:</p>
  
  <ul>
  <li><p>having all inputs as keyword arguments (like PyTorch models), or</p></li>
  <li><p>having all inputs as a list, tuple or dict in the first positional
  arguments.</p></li>
  </ul>
</blockquote>

<p>I'm trying to use the first of these two to call a BERT model I created:</p>

<pre><code>from transformers import BertTokenizer, TFBertModel
import tensorflow as tf

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

text = ['This is a sentence', 
        'The sky is blue and the grass is green', 
        'More words are here']
labels = [0, 1, 0]
tokenized_text = bert_tokenizer.batch_encode_plus(batch_text_or_text_pairs=text,
                                                  pad_to_max_length=True,
                                                  return_tensors='tf')
dataset = tf.data.Dataset.from_tensor_slices((tokenized_text['input_ids'],
                                              tokenized_text['attention_mask'],
                                              tokenized_text['token_type_ids'],
                                              tf.constant(labels))).batch(3)
sample = next(iter(dataset))

result1 = bert_model(inputs=(sample[0], sample[1], sample[2]))  # works fine
result2 = bert_model(inputs={'input_ids': sample[0], 
                             'attention_mask': sample[1], 
                             'token_type_ids': sample[2]})  # also fine
result3 = bert_model(input_ids=sample[0], 
                     attention_mask=sample[1], 
                     token_type_ids=sample[2])  # raises an error
</code></pre>

<p>But when I execute the last line, I get an error:</p>

<pre><code>TypeError: __call__() missing 1 required positional argument: 'inputs'
</code></pre>

<p>Could someone please explain how to properly use the keyword argument style of inputs?</p>
","424306","","424306","","2020-03-24 22:28:21","2020-03-25 10:46:20","Keyword arguments in BERT call function","<tensorflow><nlp><arguments><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"60969176","1","","","2020-04-01 10:49:47","","1","670","<p>I am trying to train huggingface's implementation of the GPT2 model from scratch (meaning I am using their architecture but not using pre-trained weights) but I noticed by looking into the code here <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_gpt2.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_gpt2.py</a> that there doesnâ€™t seem to be an implementation for a causal mask.</p>

<p>I could write an ugly <code>for loop</code> and feed to the network my training sequences one token at a time  which would not be unefficient. I could also chop up each of my examples token by token, pad them and feed it like a batch, which is probably faster but doesnâ€™t feel super satisfying.</p>

<p>Has anyone of you worked closely with huggingfaceâ€™s transformers before ? Do you know if there is an implementation of casal mask that I missed, or another way to do what I am describing ?</p>

<p>PS : Yes, I have already read huggingfaceâ€™s blogpost on training from scratch, but itâ€™s mostly incomplete and the relevant parts concerning training are left out.</p>
","7896124","","","","","2020-04-01 10:49:47","Training huggingface's GPT2 from scratch : how to implement causal mask?","<nlp><huggingface-transformers><gpt>","0","2","","","","CC BY-SA 4.0"
"60876394","1","60883003","","2020-03-26 21:27:21","","4","1213","<p>I'm using the <a href=""https://huggingface.co/transformers/index.html"" rel=""nofollow noreferrer"">Huggingface Transformer</a> package and BERT with PyTorch. I'm trying to do 4-way sentiment classification and am using <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">BertForSequenceClassification</a> to build a model that eventually leads to a 4-way softmax at the end.</p>

<p>My understanding from reading the BERT paper is that the final dense vector for the input <code>CLS</code> token serves as a representation of the whole text string:</p>

<blockquote>
  <p>The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.</p>
</blockquote>

<p>So, does <code>BertForSequenceClassification</code> actually train and use this vector to perform the final classification?</p>

<p>The reason I ask is because when I <code>print(model)</code>, it is not obvious to me that the <code>CLS</code> vector is being used. </p>

<pre class=""lang-py prettyprint-override""><code>model = BertForSequenceClassification.from_pretrained(
    model_config,
    num_labels=num_labels,
    output_attentions=False,
    output_hidden_states=False
)

print(model)
</code></pre>

<p>Here is the bottom of the output:</p>

<pre><code>        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
</code></pre>

<p>I see that there is a pooling layer <code>BertPooler</code> leading to a <code>Dropout</code> leading to a <code>Linear</code> which presumably performs the final 4-way softmax. However, the use of the <code>BertPooler</code> is not clear to me. Is it operating on only the hidden state of <code>CLS</code>, or is it doing some kind of pooling over hidden states of all the input tokens?</p>

<p>Thanks for any help.</p>
","4561314","","","","","2020-03-27 09:14:00","Does BertForSequenceClassification classify on the CLS vector?","<python><machine-learning><pytorch><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61134980","1","61136334","","2020-04-10 06:18:09","","1","128","<p>i have used huggingface BERT for sentence classification with very good results, but now i want to apply it to another use case. Below is the kind of dataset(not exact) i have in mind.</p>

<pre><code> set_df.head()
</code></pre>

<pre><code>    sentence                                subject                   object
0   my big red dog has a big fat bone       my big red dog          big fat bone
1   The Queen of Spades lives in a Castle   The Queen of spades     lives in a castle

</code></pre>

<p>I have a train dataset with these three columns, and i want it to be able to bisect the test sentences into its constituents. i have looked into the different pre-trained models in BERT, but i havent  gotten any success. Am i using the wrong tool?</p>
","13119815","","3607203","","2020-04-10 08:07:16","2020-07-27 13:44:42","Trying to adapt Pre-Trained BERT to another use case of semantic separation of sentences","<python><nlp><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"60807799","1","","","2020-03-23 04:03:57","","2","158","<p>What should we do exactly during the preprocessing step for GPT2? Are there any guidelines?</p>

<p>Would this be just fine for the preprocessing step?</p>

<pre><code>1. Remove any \n from sentence
2. Remove extra spaces from sentence
3. Leave everything else that is part of the sentence but not exactly words (e.g. urls, non-english words that may be added in an english sentence, emojis, etc...)
</code></pre>

<p>Wouldn't it be better to remove extra punctuation or any non-english character? </p>
","881660","","881660","","2020-03-25 17:56:39","2020-03-25 17:56:39","GPT2 huggingface's transformer preprocessing","<huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"61058171","1","","","2020-04-06 10:55:39","","0","666","<p>I face this problem when i try to run bart_sum from huggingface transformers. I'm not sure what this module use. I have tried !pip install transformers, and the !python setup.py develop inside the transformers directory, and then !pip install -r requirements.txt inside the examples directory. Here's a screenshot: <a href=""https://i.stack.imgur.com/oWkQD.png"" rel=""nofollow noreferrer"">The error</a></p>

<p>Could someone please help me out?</p>
","9244879","","","","","2020-04-06 23:21:21","No module named 'transformer_base'","<nlp><summarization><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"61191642","1","","","2020-04-13 15:52:48","","1","88","<p>I'm reading the documentation of the transformer's library from huggingface and I'm a bit confused about the BertModel's forward output.<br>
It's stated in the documents that there are two outputs(and two optional ones) that are:  </p>

<ul>
<li>last hidden state</li>
<li>pooled output  </li>
</ul>

<p>and it's suggested in the documentation of the latter one that:</p>

<blockquote>
  <p>This output is usually not a good summary of the semantic content of the input, youâ€™re often better with averaging or pooling the sequence of hidden-states for the whole input sequence.</p>
</blockquote>

<p>So, It sounds to me like the first output can be a better representation of the input's semantic. However, in the code for <code>BertForSequenceClassification</code> the second output is used and to feed the fully connected layer. Why is that?<br>
This is a part of the code:</p>

<pre><code> pooled_output = outputs[1]
 pooled_output = self.dropout(pooled_output)
 logits = self.classifier(pooled_output)
</code></pre>
","5384986","","","","","2020-04-14 07:21:51","Confused about transformers' documentation","<python-3.x><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61222878","1","","","2020-04-15 06:48:03","","0","243","<p>I'm trying to get generated text from the TFGPT2Model in the Transformers library. I can see the output tensor, but I'm not able to decode it. Is the tokenizer not compatible with the TF model for decoding?</p>

<p>Code is:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

from transformers import (
    TFGPT2Model,
    GPT2Tokenizer,
    GPT2Config,
)

model_name = ""gpt2-medium""
config = GPT2Config.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = TFGPT2Model.from_pretrained(model_name, config=config)

input_ids = tf.constant(tokenizer.encode(""Hello, my dog is cute"", 
                 add_special_tokens=True))[None, :]  # Batch size 1
outputs = model(input_ids)
print(outputs[0])
result = tokenizer.decode(outputs[0])
print(result)
</code></pre>

<p>The resulting output is:</p>

<pre><code>$ python run_tf_gpt2.py
2020-04-16 23:43:11.753181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6                                            
2020-04-16 23:43:11.777487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
2020-04-16 23:43:27.617982: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.         
2020-04-16 23:43:27.693316: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-04-16 23:43:27.824075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA n
ode, so returning NUMA node zero                                                                                                     
...
...
2020-04-16 23:43:38.149860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10565 MB memory) -&gt; physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:25:00.0, compute capability: 3.7)
2020-04-16 23:43:38.150217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-16 23:43:38.150913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10565 MB memory) -&gt; physical GPU (device: 2, name: Tesla K80, pci bus id: 0000:26:00.0, compute capability: 3.7)
2020-04-16 23:43:44.438587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
tf.Tensor(
[[[ 0.671073    0.60760975 -0.10744217 ... -0.51132596 -0.3369941
    0.23458953]
  [ 0.6403012   0.00396247  0.7443729  ...  0.2058892  -0.43869907
    0.2180479 ]
  [ 0.5131284  -0.35192695  0.12285632 ... -0.30060387 -1.0279727
    0.13515341]
  [ 0.3083361  -0.05588413  1.0543617  ... -0.11589152 -1.0487361
    0.05204075]
  [ 0.70787597 -0.40516227  0.4160383  ...  0.44217822 -0.34975922
    0.02535546]
  [-0.03940453 -0.1243843   0.40204537 ...  0.04586177 -0.48230025
    0.5768887 ]]], shape=(1, 6, 1024), dtype=float32)
Traceback (most recent call last):
  File ""run_tf_gpt2.py"", line 19, in &lt;module&gt;
    result = tokenizer.decode(outputs[0])
  File ""/home/.../transformers/src/transformers/tokenization_utils.py"", line 1605, in decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File ""/home/.../transformers/src/transformers/tokenization_utils.py"", line 1575, in convert_ids_to_tokens
    index = int(index)
  File ""/home/.../venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 853, in __int__
    return int(self._numpy())
TypeError: only size-1 arrays can be converted to Python scalars

</code></pre>

<p>(I removed all the TF messages and modified paths of my environment)</p>
","107726","","107726","","2020-04-17 06:45:39","2020-04-30 10:46:06","How can you decode output sequences from TFGPT2Model?","<huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"61286574","1","","","2020-04-18 08:58:12","","1","160","<p>I am trying to measure token importance for <code>BERT</code> via comparing token embedding grad value. So, to get the grad, I've copied the <code>2.8.0</code> forward of BertModel and changed it a bit: </p>

<p><code>huggingface transformers 2.8.0 BERT</code> <a href=""https://github.com/huggingface/transformers/blob/11c3257a18c4b5e1a3c1746eefd96f180358397b/src/transformers/modeling_bert.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/11c3257a18c4b5e1a3c1746eefd96f180358397b/src/transformers/modeling_bert.py</a></p>

<p>Code:</p>

<pre><code>        embedding_output = self.embeddings(
            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
        )
        embedding_output = embedding_output.requires_grad_(True) # my code
        encoder_outputs = self.encoder(
            embedding_output,
            attention_mask=extended_attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_extended_attention_mask,
        )
        sequence_output = encoder_outputs[0]
        sequence_output.mean().backward() # my code
        assert(embedding_output.grad is not None) # my code
</code></pre>

<p><code>Colab</code> link: <a href=""https://colab.research.google.com/drive/1MggBUaDWAAZNuXbTDM11E8jvdMGEkuRD"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1MggBUaDWAAZNuXbTDM11E8jvdMGEkuRD</a>
But it gives assertion error. I do not understand why and it seems to be a bug for me.
Please, help!</p>
","9181499","","9181499","","2020-04-18 09:11:07","2020-04-22 10:32:58","BERT token importance measuring issue. Grad is none","<deep-learning><pytorch><transformer><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"61558613","1","","","2020-05-02 11:28:46","","0","438","<p>So I ran a custom Roberta Token Classification, and I have a tokenized output id that looks like this, each of the output ID's has a corresponding label.</p>

<pre><code>  Ä lives   
  Ä o
  Ä n
  Ä Cali
</code></pre>

<p>Each of these tokens comes with its prediction label. Detokenizing the text is trivial, but it loses the match with the prediction labels.</p>

<pre><code>detokenized = ''.join(tokens).replace('Ä ', ' ')

</code></pre>

<p>However, the challenge is to match the labels to the detokenized text. if anyone can point me to a script or method to use to solve this problem, would be really nice. Also below is my prediction script which was modified from a BERT script.</p>

<pre><code>tags, t_words = [], []                                         #tokenize
for row in test_df:
  tokenized_sentence = tokenizer.encode(row)
  input_ids = torch.tensor([tokenized_sentence]).cuda()

  with torch.no_grad():                                                 #predict
    output = model(input_ids)
    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)

     # join split tokens
    new_tokens, new_labels = [], []
    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0] )
    for token, label_idx in zip(tokens, label_indices[0]):
        new_labels.append(tag_values[label_idx])
        new_tokens.append(token)
    tags.append(new_labels)
    t_words.append(new_tokens)

</code></pre>

<p>but is there any way to have the prediction labels match the detokenized inputs, else the entire attempt at prediction is pointless.  </p>
","13119815","","","","","2020-05-02 11:28:46","Roberta detokenization without losing labels","<python><nlp><pytorch><tokenize><huggingface-transformers>","0","15","","","","CC BY-SA 4.0"
"61134275","1","61136018","","2020-04-10 04:58:26","","7","3790","<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModel, AutoTokenizer

tokenizer1 = AutoTokenizer.from_pretrained(""roberta-base"")
tokenizer2 = AutoTokenizer.from_pretrained(""bert-base-cased"")

sequence = ""A Titan RTX has 24GB of VRAM""
print(tokenizer1.tokenize(sequence))
print(tokenizer2.tokenize(sequence))
</code></pre>

<p>Output:</p>

<p>['A', 'Ä Titan', 'Ä RTX', 'Ä has', 'Ä 24', 'GB', 'Ä of', 'Ä VR', 'AM']</p>

<p>['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']</p>

<p>Bert model uses WordPiece tokenizer. Any word that does not occur in the WordPiece vocabulary is broken down into sub-words greedily. For example, 'RTX' is broken into 'R', '##T' and '##X' where ## indicates it is a subtoken. </p>

<p>Roberta uses BPE tokenizer but I'm unable to understand </p>

<p>a) how BPE tokenizer works? </p>

<p>b) what does G represents in each of tokens?</p>
","6151940","","3607203","","2020-04-10 07:19:09","2020-04-10 07:45:44","Difficulty in understanding the tokenizer used in Roberta model","<nlp><pytorch><huggingface-transformers><bert-language-model>","1","0","1","","","CC BY-SA 4.0"
"61482810","1","61487842","","2020-04-28 14:35:17","","1","978","<p>In his article 'Language Model Fine-Tuning For Pre-Trained Transformers' Thilina Rajapakse (<a href=""https://medium.com/skilai/language-model-fine-tuning-for-pre-trained-transformers-b7262774a7ee"" rel=""nofollow noreferrer"">https://medium.com/skilai/language-model-fine-tuning-for-pre-trained-transformers-b7262774a7ee</a>)
provides the following code snippet for fine-tuning a pre-trained model using the library <code>simpletransformers</code>:</p>

<pre><code>from simpletransformers.language_modeling import LanguageModelingModel
import logging


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger(""transformers"")
transformers_logger.setLevel(logging.WARNING)

train_args = {
    ""reprocess_input_data"": True,
    ""overwrite_output_dir"": True,
}

model = LanguageModelingModel('bert', 'bert-base-cased', args=train_args)

model.train_model(""data/train.txt"", eval_file=""data/text.txt"")

model.eval_model(""data/test.txt"")
</code></pre>

<p>He then adds:</p>

<blockquote>
  <p>We assume that you have combined all the text in your dataset into two
  text files train.txt and test.txt which can be found in the data/
  directory.</p>
</blockquote>

<p>I have 2 questions:</p>

<p><strong>Question 1</strong></p>

<p>Does the highlighted sentence above implies that the entire corpus will be merged into one text file?  So assuming that the Training Corpus is comprised of 1,000,000 text files, are we supposed to merge them all in one text file with code like this?</p>

<pre><code>import fileinput
with open(outfilename, 'w') as fout, fileinput.input(filenames) as fin:
    for line in fin:
        fout.write(line)
</code></pre>

<p><strong>Question 2</strong></p>

<p>I presume that I can use the pretrained model: <code>bert-base-multilingual-cased</code>.  Correct?</p>
","8270077","","10871073","","2020-08-03 12:12:31","2020-08-03 12:12:31","Fine tuning a pretrained language model with Simple Transformers","<python-3.x><huggingface-transformers><language-model><simpletransformers>","1","0","","","","CC BY-SA 4.0"
"61221810","1","61225369","","2020-04-15 05:17:51","","1","703","<p>I want to apply Roberta model for  text similarity. Given a pair of sentences,the input should be in the format <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</code>. I figure out two possible ways to generate the input ids namely</p>

<p>a)</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained('roberta-base')

list1 = tokenizer.encode('Very severe pain in hands')

list2 = tokenizer.encode('Numbness of upper limb')

sequence = list1+[2]+list2[1:]

</code></pre>

<p>In this case, sequence is <code>[0, 12178, 3814, 2400, 11, 1420, 2, 2, 234, 4179, 1825, 9, 2853, 29654, 2]</code></p>

<p>b)</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained('roberta-base')

list1 = tokenizer.encode('Very severe pain in hands', add_special_tokens=False)

list2 = tokenizer.encode('Numbness of upper limb', add_special_tokens=False)

sequence = [0]+list1+[2,2]+list2+[2]
</code></pre>

<p>In this case, sequence is <code>[0, 25101, 3814, 2400, 11, 1420, 2, 2, 487, 4179, 1825, 9, 2853, 29654, 2]</code></p>

<p>Here <code>0</code> represents <code>&lt;s&gt;</code> token and 2 represents <code>&lt;/s&gt;</code> token. I'm not sure which is the correct way to encode the given two sentences for calculating sentence similarity using Roberta model.</p>
","6151940","","3607203","","2020-04-15 09:26:55","2021-05-08 04:55:01","Confusion in Pre-processing text for Roberta Model","<nlp><pytorch><huggingface-transformers><bert-language-model>","1","0","1","","","CC BY-SA 4.0"
"61322251","1","","","2020-04-20 12:12:51","","7","3913","<p>I got a strange error when trying to encode question-answer pairs for BERT using the <code>encode_plus</code> method provided in the Transformers library.</p>
<p>I am using data from <a href=""https://www.kaggle.com/c/google-quest-challenge/data"" rel=""noreferrer"">this Kaggle competition</a>. Given a question title, question body and answer, the model must predict 30 values (regression problem). My goal is to get the following encoding as input to BERT:</p>
<p>[CLS] question_title question_body [SEP] answer [SEP]</p>
<p>However, when I try to use</p>
<pre><code>tokenizer = transformers.BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<p>and encode only the second input from train.csv as follows:</p>
<pre><code>inputs = tokenizer.encode_plus(
            df_train[&quot;question_title&quot;].values[1] + &quot; &quot; + df_train[&quot;question_body&quot;].values[1], # first sequence to be encoded
            df_train[&quot;answer&quot;].values[1], # second sequence to be encoded
            add_special_tokens=True, # [CLS] and 2x [SEP] 
            max_len = 512,
            pad_to_max_length=True
            )
</code></pre>
<p>I get the following error:</p>
<pre><code>Token indices sequence length is longer than the specified maximum sequence length for this model (46 &gt; 512). Running this sequence through the model will result in indexing errors
</code></pre>
<p>It says that the length of the token indices is longer than the specified maximum sequence length, but this is not true (as you can see, 46 is not &gt; 512).</p>
<p>This happens for several of the rows in <code>df_train</code>. Am I doing something wrong here?</p>
","7762882","","-1","","2020-06-20 09:12:55","2020-04-30 20:50:06","Token indices sequence length error when using encode_plus method","<nlp><tokenize><huggingface-transformers><bert-language-model>","1","2","","","","CC BY-SA 4.0"
"61806929","1","","","2020-05-14 20:33:45","","1","158","<p>I tried to test <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py"" rel=""nofollow noreferrer""><code>run_language_modeling.py</code></a> with a small test file and it run out of memory after using more than 32 GB of RAM. Why does it need so much RAM or what am I doing wrong?</p>

<p>Command line:</p>

<pre><code>python run_language_modeling.py --output_dir foo --model_type gpt2 --model_name_or_path gpt2 --do_train --train_data_file test.txt --no_cuda --eval_data_file test.txt
</code></pre>

<p>Testfile size: 29600 bytes, 546 lines.</p>

<p>With the original OpenAI implementation I have no problem to run the training script.</p>
","893159","","","","","2020-05-14 20:33:45","Huge memory usage when running huggingface transformers run_language_modeling.py with GPT2","<machine-learning><torch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"60772384","1","","","2020-03-20 10:28:34","","1","247","<p>I'm interested in huggingface's distillBERT work, by going through their code (<a href=""https://github.com/huggingface/transformers/blob/master/examples/distillation/train.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/examples/distillation/train.py</a>), I found that if use roBERTa as student model, they will freeze the position embedding, I'm wondering what is this for? </p>

<pre><code>def freeze_pos_embeddings(student, args):
    if args.student_type == ""roberta"":
        student.roberta.embeddings.position_embeddings.weight.requires_grad = False
    elif args.student_type == ""gpt2"":
        student.transformer.wpe.weight.requires_grad = False
</code></pre>

<p>I understand the reason of freezing token_type_embeddings, because roBERTa never used segment embeddings, but why position embeddings?<br>
Thanks a lot for the help!</p>
","13093915","","975097","","2020-04-25 00:01:49","2020-04-25 00:01:49","Why freeze position embedding when distill roBERTa?","<huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"61656822","1","61727773","","2020-05-07 11:38:06","","2","2724","<p><strong>Summary:</strong></p>

<p>I want to fine-tune BERT for sentence classification on a custom dataset. I have followed some examples I have found, like <a href=""https://stackoverflow.com/questions/59978959/how-to-use-hugging-face-transformers-library-in-tensorflow-for-text-classificati"">this one</a>, which was very helpful. I have also looked at <a href=""https://gist.github.com/papapabi/124c6ac406e6bbd1f28df732e953ac6d"" rel=""nofollow noreferrer"">this gist</a>. </p>

<p><strong>The problem I have is that when running inference for some samples, the output has other dimensions than I would expect.</strong></p>

<p>When I run inference for 23 samples, I get a tuple with a numpy array of dimensions (1472, 42), where 42 is the number of classes. I would expect dimensions (23, 42).</p>

<p><strong>Code and Other Details:</strong></p>

<p>I run the inference on the trained model using Keras like this:</p>

<pre><code>preds = model.predict(features)
</code></pre>

<p>Where <em>features</em> is tokenized and converted to a Dataset:</p>

<pre><code>for sample, ground_truth in tests:
    test_examples.append(InputExample(text=sample, category_index=ground_truth))

features = convert_examples_to_tf_dataset(test_examples, tokenizer)
</code></pre>

<p>Where <code>sample</code> can be e.g. <code>""A test sentence I want classified""</code> and <code>ground_truth</code> can be e.g. <code>12</code> which is the encoded label. Because I do inference, what I supply as ground truth shouldn't matter of course.</p>

<p>The <code>convert_examples_to_tf_dataset</code>-function looks as follows (which I found in <a href=""https://gist.github.com/papapabi/124c6ac406e6bbd1f28df732e953ac6d"" rel=""nofollow noreferrer"">this gist</a>):</p>

<pre><code>def convert_examples_to_tf_dataset(
    examples: List[Tuple[str, int]],
    tokenizer,
    max_length=64,
):
    """"""
    Loads data into a tf.data.Dataset for finetuning a given model.

    Args:
        examples: List of tuples representing the examples to be fed
        tokenizer: Instance of a tokenizer that will tokenize the examples
        max_length: Maximum string length

    Returns:
        a ``tf.data.Dataset`` containing the condensed features of the provided sentences
    """"""
    features = [] # -&gt; will hold InputFeatures to be converted later

    for e in examples:
        # Documentation is really strong for this method, so please take a look at it
        input_dict = tokenizer.encode_plus(
            e.text,
            add_special_tokens=True,
            max_length=max_length, # truncates if len(s) &gt; max_length
            return_token_type_ids=True,
            return_attention_mask=True,
            pad_to_max_length=True, # pads to the right by default
        )

        # input ids = token indices in the tokenizer's internal dict
        # token_type_ids = binary mask identifying different sequences in the model
        # attention_mask = binary mask indicating the positions of padded tokens so the model does not attend to them

        input_ids, token_type_ids, attention_mask = (input_dict[""input_ids""],
            input_dict[""token_type_ids""], input_dict['attention_mask'])

        features.append(
            InputFeatures(
                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.category_index
            )
        )

    def gen():
        for f in features:
            yield (
                {
                    ""input_ids"": f.input_ids,
                    ""attention_mask"": f.attention_mask,
                    ""token_type_ids"": f.token_type_ids,
                },
                f.label,
            )

    return tf.data.Dataset.from_generator(
        gen,
        ({""input_ids"": tf.int32, ""attention_mask"": tf.int32, ""token_type_ids"": tf.int32}, tf.int64),
        (
            {
                ""input_ids"": tf.TensorShape([None]),
                ""attention_mask"": tf.TensorShape([None]),
                ""token_type_ids"": tf.TensorShape([None]),
            },
            tf.TensorShape([]),
        ),
    )

with tf.device('/cpu:0'):
    train_data = convert_examples_to_tf_dataset(train_examples, tokenizer)
    train_data = train_data.shuffle(buffer_size=len(train_examples), reshuffle_each_iteration=True) \
                           .batch(BATCH_SIZE) \
                           .repeat(-1)

    val_data = convert_examples_to_tf_dataset(val_examples, tokenizer)
    val_data = val_data.shuffle(buffer_size=len(val_examples), reshuffle_each_iteration=True) \
                           .batch(BATCH_SIZE) \
                           .repeat(-1)
</code></pre>

<p>It works as I would expect and running <code>print(list(features.as_numpy_iterator())[1])</code> yields the following:</p>

<pre><code>({'input_ids': array([  101, 11639, 19962, 23288, 13264, 35372, 10410,   102,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
      dtype=int32), 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
      dtype=int32)}, 6705)
</code></pre>

<p>So far everything looks like I would expect. And it seems like the tokenizer is working as it should; 3 arrays of length 64 (which corresponds to my set max-length), and a label as an integer.</p>

<p>The model has been trained as follows:</p>

<pre><code>config = BertConfig.from_pretrained(
    'bert-base-multilingual-cased',
    num_labels=len(label_encoder.classes_),
    output_hidden_states=False,
    output_attentions=False
)
model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', config=config)

# train_data is then a tf.data.Dataset we can pass to model.fit()
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-05, epsilon=1e-08)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')
model.compile(optimizer=optimizer,
              loss=loss,
              metrics=[metric])

model.summary()

history = model.fit(train_data,
                    epochs=EPOCHS,
                    steps_per_epoch=train_steps,
                    validation_data=val_data,
                    validation_steps=val_steps,
                    shuffle=True,
                    )
</code></pre>

<p><strong>Results</strong></p>

<p>The problem now is that when running a prediction <code>preds = model.predict(features)</code>, the output dimensions does not correspond to what the <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification"" rel=""nofollow noreferrer"">documentation</a> says: <code>logits (Numpy array or tf.Tensor of shape (batch_size, config.num_labels)):</code>. <strong>What I get is a tuple containing a numpy array with dimensions: (1472,42).</strong></p>

<p>42 makes sense as this is my number of classes. I sent 23 samples for the test, and 23 x 64 = 1472. 64 is my max sentence length, so it kind of sounds familiar. Is this output incorrect? How can I convert this output to an actual class prediction for each input sample? I get 1472 predictions when I would expect 23.</p>

<p>Please let me know if I can provide more details that could help solve this.</p>
","7186880","","7186880","","2020-05-11 10:45:51","2020-05-11 10:45:51","Tensorflow 2.0 Hugging Face Transformers, TFBertForSequenceClassification, Unexpected Output Dimensions in Inference","<python><tensorflow><machine-learning><nlp><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"61707371","1","61711554","","2020-05-10 04:57:44","","2","381","<p>I use transformers tokenizer, and created mask using API: get_special_tokens_mask.<br>
<a href=""https://github.com/ishikawa-takumi/transformers-sample/blob/master/tokenizer.ipynb"" rel=""nofollow noreferrer"">My Code</a></p>

<p>In <a href=""https://huggingface.co/transformers/model_doc/roberta.html"" rel=""nofollow noreferrer"">RoBERTa Doc</a>, returns of this API is ""A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token"". But I seem that this API returns ""0 for a sequence token, 1 for a special token"".<br>
Is it all right?</p>
","11056172","","","","","2020-05-10 11:56:07","About get_special_tokens_mask in huggingface-transformers","<tokenize><huggingface-transformers>","1","1","1","","","CC BY-SA 4.0"
"61862805","1","","","2020-05-18 05:10:51","","0","61","<p>Would running the running HuggingFace Transformers with different sequence length on TPU cause new computation graph to be made each time ? And thus causing XLA re-compilation every time ? </p>

<p>Also, while training , does that mean all batches should be padded to the over-all maximum length in the entire dataset ? If I use <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py"" rel=""nofollow noreferrer"">the trainer</a>, will it do it automatically for me ?</p>

<p>Or does the ParallelLoader of PyTorch/XLA do it ?</p>
","9084080","","","","","2020-10-20 15:44:00","Does running HuggingFace Transformers with different sequence length on TPU causes XLA re-compilation every time?","<pytorch><huggingface-transformers><tpu>","1","0","","","","CC BY-SA 4.0"
"61916760","1","61929387","","2020-05-20 15:19:58","","0","404","<p>I have installed the latest version of transformers and I was able to use its simple syntax to make sentiment prediction of English phrases:</p>

<pre><code>from transformers import pipeline
sentimentAnalysis = pipeline(""sentiment-analysis"")
print(sentimentAnalysis(""Transformers piplines are easy to use""))
HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_â€¦

HBox(children=(FloatProgress(value=0.0, description='Downloading', max=629.0, style=ProgressStyle(description_â€¦

HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_â€¦

HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267844284.0, style=ProgressStyle(descriâ€¦

[{'label': 'POSITIVE', 'score': 0.9305251240730286}]

print(sentimentAnalysis(""Transformers piplines are extremely easy to use""))

[{'label': 'POSITIVE', 'score': 0.9820092916488647}]
</code></pre>

<p>However, when I tried it on a non-English language (here is Greek) I did not get the results I expected.</p>

<p>The following phrase translates in English as: <code>'This food is disgusting'</code> and I would expect I very low sentiment score which is not what I got:</p>

<pre><code>print(sentimentAnalysis(""Î‘Ï…Ï„ÏŒ Ï„Î¿ Ï†Î±Î³Î·Ï„ÏŒ ÎµÎ¯Î½Î±Î¹ Î±Î·Î´Î¹Î±ÏƒÏ„Î¹ÎºÏŒ""))
[{'label': 'POSITIVE', 'score': 0.7899578213691711}]
</code></pre>

<p>Here is an attempt to use the best multilingual model:</p>

<p><a href=""https://i.stack.imgur.com/X35N3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X35N3.png"" alt=""enter image description here""></a></p>

<p>Somewhat better but still widely out of target.</p>

<p>Is there something I can do about it?</p>
","8270077","","8270077","","2020-05-21 12:46:14","2020-05-21 12:46:14","Using huggingface transformers with a non English language","<python-3.x><multilingual><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61863504","1","","","2020-05-18 06:20:20","","2","1536","<p>I am trying to fine tune the <a href=""https://huggingface.co/transformers/model_doc/bart.html#bartforconditionalgeneration"" rel=""nofollow noreferrer"">BART model</a> to generate news headlines. </p>

<p>I am taking the dataset from <a href=""https://www.kaggle.com/sunnysai12345/news-summary"" rel=""nofollow noreferrer"">Kaggle News Summary</a></p>

<p>Fine tuning <a href=""https://colab.research.google.com/drive/1H2u5lUIr4HIiZu5pPKfxgTxhb1zMhF8C?usp=sharing"" rel=""nofollow noreferrer"">Colab notebook</a></p>

<p>However, in the validation section when trying to generate the text from encoded tokens, i am running into error</p>

<pre><code>---------------------------------------------------------------------------
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-22-059727516ce4&gt; in &lt;module&gt;()
      1 for epoch in range(1):
----&gt; 2     predictions, actuals = validate(epoch)
      3     writer(predictions, actuals)
      4     print('Output Files generated for review')

4 frames
&lt;ipython-input-21-6738e7a3f1f9&gt; in validate(epoch)
     10             mask = data['source_mask'].to(device, dtype = torch.long)
     11 
---&gt; 12             generate_ids = model.generate(input_ids = ids,attention_mask = mask, num_beams=4,repetition_penalty=2.5,length_penalty=2.0,early_stopping=True)
     13             preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generate_ids]
     14             target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]

/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)
     13         def decorate_context(*args, **kwargs):
     14             with self:
---&gt; 15                 return func(*args, **kwargs)
     16         return decorate_context
     17 

/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py in generate(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)
    914 
    915         # We cannot generate if the model does not have a LM head
--&gt; 916         if self.get_output_embeddings() is None:
    917             raise AttributeError(
    918                 ""You tried to generate sequences with a model that does not have a LM Head.""

/usr/local/lib/python3.6/dist-packages/transformers/modeling_bart.py in get_output_embeddings(self)
   1021 
   1022     def get_output_embeddings(self):
-&gt; 1023         return _make_linear_from_emb(self.model.shared)  # make it on the fly
   1024 
   1025 

/usr/local/lib/python3.6/dist-packages/transformers/modeling_bart.py in _make_linear_from_emb(emb)
    148     vocab_size, emb_size = emb.weight.shape
    149     lin_layer = nn.Linear(vocab_size, emb_size, bias=False)
--&gt; 150     lin_layer.weight.data = emb.weight.data
    151     return lin_layer
    152 

RuntimeError: Attempted to call `variable.set_data(tensor)`, but `variable` and `tensor` have incompatible tensor type.
</code></pre>

<p>It will be great if i could get some guidance from the group. Thanks! </p>
","9198765","","9198765","","2020-05-18 06:27:50","2020-05-18 06:27:50","Fine tuning BART to generate Summary","<python><nlp><pytorch><summarization><huggingface-transformers>","0","4","2","","","CC BY-SA 4.0"
"61166514","1","","","2020-04-12 03:32:30","","3","1615","<p>Using HuggingFace's <a href=""https://huggingface.co/transformers/usage.html#masked-language-modeling"" rel=""nofollow noreferrer"">pipeline tool</a>, I was surprised to find that there was a significant difference in output when using the fast vs slow tokenizer. </p>

<p>Specifically, when I run the fill-mask pipeline, the probabilities assigned to the words that would fill in the mask are not the same for the fast and slow tokenizer. Moreover, while the predictions of the fast tokenizer remain constant regardless of the number and length of sentences input, the same is not true for the slow tokenizer.</p>

<p>Here's a minimal example:</p>

<pre><code>from transformers import pipeline

slow = pipeline('fill-mask', model='bert-base-cased', \
                tokenizer=('bert-base-cased', {""use_fast"": False}))

fast = pipeline('fill-mask', model='bert-base-cased', \
                tokenizer=('bert-base-cased', {""use_fast"": True}))

s1 = ""This is a short and sweet [MASK].""  # ""example""
s2 = ""This is [MASK].""  # ""shorter""

slow([s1, s2])
fast([s1, s2])
slow([s2])
fast([s2])
</code></pre>

<p>Each pipeline call yields the top-5 tokens that could fill in for <code>[MASK]</code>, along with their probabilities. I've ommitted the actual outputs for brevity, but the probabilities assigned to each word that fill in <code>[MASK]</code> for <code>s2</code> are not the same across all of the examples. The final 3 examples give the same probabilities, but the first yields different probabilities. The differences are so great that the top-5 are not consistent across the two groups.</p>

<p>The cause behind this, as I can tell, is that the fast and slow tokenizers return different outputs. The fast tokenizer standardizes sequence length to 512 by padding with 0s, and then creates an attention mask that blocks out the padding. In contrast, the slow tokenizer only pads to the length of the longest sequence, and does not create such an attention mask. Instead, it sets the token type id of the padding to 1 (rather than 0, which is the type of the non-padding tokens). By my understanding of HuggingFace's implementation (found <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertForMaskedLM"" rel=""nofollow noreferrer"">here</a>), these are not equivalent.</p>

<p>Does anyone know if this is intentional?</p>
","13291187","","6664872","","2020-06-17 00:39:21","2020-09-09 12:45:35","Fast and slow tokenizers yield different results","<python><nlp><huggingface-transformers><bert-language-model><huggingface-tokenizers>","1","1","1","","","CC BY-SA 4.0"
"61323621","1","","","2020-04-20 13:26:37","","3","2199","<blockquote>
  <p>Returns last_hidden_state (torch.FloatTensor of shape (batch_size,
  sequence_length, hidden_size)): Sequence of hidden-states at the
  output of the last layer of the model.</p>
  
  <p>pooler_output (torch.FloatTensor: of shape (batch_size, hidden_size)):
  Last layer hidden-state of the first token of the sequence
  (classification token) further processed by a Linear layer and a Tanh
  activation function. The Linear layer weights are trained from the
  next sentence prediction (classification) objective during
  pre-training.</p>
  
  <p>This output is usually not a good summary of the semantic content of
  the input, youâ€™re often better with averaging or pooling the sequence
  of hidden-states for the whole input sequence.</p>
  
  <p>hidden_states (tuple(torch.FloatTensor), optional, returned when
  config.output_hidden_states=True): Tuple of torch.FloatTensor (one for
  the output of the embeddings + one for the output of each layer) of
  shape (batch_size, sequence_length, hidden_size).</p>
  
  <p>Hidden-states of the model at the output of each layer plus the
  initial embedding outputs.</p>
  
  <p>attentions (tuple(torch.FloatTensor), optional, returned when
  config.output_attentions=True): Tuple of torch.FloatTensor (one for
  each layer) of shape (batch_size, num_heads, sequence_length,
  sequence_length).</p>
  
  <p>Attentions weights after the attention softmax, used to compute the
  weighted average in the self-attention heads.</p>
</blockquote>

<p>This is from <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertmodel"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html#bertmodel</a>. Although the description in the document is clear, I still don't understand the <strong>hidden_states</strong> of returns. There is a tuple, one for the output of the embeddings, and the other for the output of each layer.
Please tell me how to distinguish them, or what is the meaning of them? Thanks very much!![wink~</p>
","13362493","","975097","","2020-04-25 00:02:22","2021-08-10 17:15:48","How to understand hidden_states of the returns in BertModel?(huggingface-transformers)","<nlp><pytorch><huggingface-transformers><bert-language-model><electrate>","2","1","","","","CC BY-SA 4.0"
"62741335","1","","","2020-07-05 13:29:55","","1","212","<p>With pytorch-lightning and transformers, I finetuned a Bert model on german service tickets. The dataset has the following size:</p>
<pre><code>FULL Dataset: (1220, 2)
TRAIN Dataset: (854, 2)
VAL Dataset: (366, 2)
</code></pre>
<p>Every ticket can be in exactly 1 out of 10 categories. This is why my model is initilized in def <strong>init</strong> like:</p>
<pre><code>#changing the configuration to X lables instead of 2
self.bert = transformers.BertModel.from_pretrained(MODEL_NAME)
self.drop = th.nn.Dropout(p=0.1)
self.out = th.nn.Linear(self.bert.config.hidden_size, NUM_LABELS)
self.softmax = th.nn.Softmax(dim=1)
self.loss = th.nn.CrossEntropyLoss(reduction=&quot;none&quot;)
</code></pre>
<p>This produceds a probability distributuion over the 10 classes for each sample. As the forward function is:</p>
<pre><code> def forward(self, input_ids, mask):
    _, pooled_output = self.bert(
                                input_ids=input_ids,
                                attention_mask=mask
                        )
    output= self.drop(pooled_output)
    output = self.out(output)
    return self.softmax(output)
</code></pre>
<p>As a loss function torch.nn.CrossEntropyLoss is defined and within the training_step called. With a Batch_Size of 16, logits.shape = [16,10] and batch['targets'].shape = [16] and batch['targets'] = [1,5,2,4,8,6,9,0,0,1,2,7,7,7,5,3]. <strong>Is CrossEntropyLoss the right loss function?</strong> And is the optimizer even working?</p>
<pre><code> def training_step(self, batch, batch_idx):
    logits = self.forward(batch['input_ids'], batch['mask']).squeeze()
    loss = self.loss(logits, batch['targets']).mean()
    return {'loss': loss, 'log': {'train_loss': loss}}
</code></pre>
<p>Same goes for validation_step:</p>
<pre><code>  def validation_step(self,batch, batch_idx):
      logits = self.forward(batch['input_ids'], batch['mask']).squeeze()
      acc = (logits.argmax(-1) == batch['targets']).float()
      loss = self.loss(logits, batch['targets'])
      return {'loss': loss, 'acc': acc}
</code></pre>
<p>In the end, the model produces the same probabilities no matter what input sequence it gets. Is this underfitting?</p>
<p>A example is:</p>
<pre><code>model.eval()
text = &quot;Warum kann mein Kollege in SAP keine Transaktionen mehr     ausfÃ¼hren?&quot;
input = tokenizer(            
   text,
   None,
   add_special_tokens=True,
   max_length=200,
   pad_to_max_length=True,
   return_token_type_ids=True,
   truncation=True,
   padding='max_length',
   return_tensors=&quot;pt&quot;
   )

input = input.to(device)

out = model(input_ids=input['input_ids'], mask=input['attention_mask'])
text, out

    --&gt; ('Warum kann mein Kollege in SAP keine Transaktionen mehr ausfÃ¼hren?',
 tensor([[2.9374e-03, 3.1926e-03, 8.7949e-03, 3.0573e-01, 2.6428e-04, 5.2946e-02,
      2.4758e-01, 6.2161e-03, 3.6930e-01, 3.0384e-03]], device='cuda:0',
    grad_fn=&lt;SoftmaxBackward&gt;))
</code></pre>
<p>And another example is:</p>
<pre><code>    ('Auf meinem Telefon erscheinen keine Nummern mehr.',
 tensor([[2.9374e-03, 3.1926e-03, 8.7949e-03, 3.0573e-01, 2.6428e-04, 5.2946e-02,
          2.4758e-01, 6.2161e-03, 3.6930e-01, 3.0384e-03]], device='cuda:0',
        grad_fn=&lt;SoftmaxBackward&gt;))
</code></pre>
<p>It is german, but as you can see, the predictions match exactly. Which is a shame :D And my problem.</p>
","13728592","","4685471","","2020-12-03 02:25:54","2020-12-03 02:25:54","Finetuning (German) Bert. Is it underfitting?","<pytorch><huggingface-transformers><pytorch-lightning>","0","2","","","","CC BY-SA 4.0"
"61969783","1","","","2020-05-23 09:11:11","","4","3590","<p>I am trying <code>BertForSequenceClassification</code> for a simple article classification task.</p>

<p>No matter how I train it (freeze all layers but the classification layer, all layers trainable, last <code>k</code> layers trainable), I always get an almost randomized accuracy score. My model doesn't go above 24-26% training accuracy (I only have 5 classes in my dataset).</p>

<p>I'm not sure what did I do wrong while designing/training the model. I tried the model with multiple datasets, every time it gives the same random baseline accuracy.</p>

<p>Dataset I used: BBC Articles (5 classes) </p>

<p><a href=""https://github.com/zabir-nabil/pytorch-nlp/tree/master/bbc"" rel=""nofollow noreferrer"">https://github.com/zabir-nabil/pytorch-nlp/tree/master/bbc</a></p>

<blockquote>
  <p>Consists of 2225 documents from the BBC news website corresponding to
  stories in five topical areas from 2004-2005. Natural Classes: 5
  (business, entertainment, politics, sport, tech)</p>
</blockquote>

<p>I added the model part and the training part which are the most important portion (to avoid any irrelevant details). I added the full source-code + data too if that's useful for reproducibility.</p>

<p>My guess is there is something wrong with the I way I designed the network or the way I'm passing the attention_masks/ labels to the model. Also, the token length 512 should not be a problem as most of the texts has length &lt; 512 (the mean length is &lt; 300).</p>

<p><strong>Model code:</strong></p>

<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn

class BertClassifier(nn.Module):
    def __init__(self):
        super(BertClassifier, self).__init__()
        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5)
        # as we have 5 classes

        # we want our output as probability so, in the evaluation mode, we'll pass the logits to a softmax layer
        self.softmax = torch.nn.Softmax(dim = 1) # last dimension
    def forward(self, x, attn_mask = None, labels = None):

        if self.training == True:
            # print(x.shape)
            loss = self.bert(x, attention_mask = attn_mask, labels = labels)
            # print(x[0].shape)

            return loss

        if self.training == False: # in evaluation mode
            x = self.bert(x)
            x = self.softmax(x[0])

            return x
    def freeze_layers(self, last_trainable = 1): 
        # we freeze all the layers except the last classification layer + few transformer blocks
        for layer in list(self.bert.parameters())[:-last_trainable]:
            layer.requires_grad = False


# create our model

bertclassifier = BertClassifier()
</code></pre>

<p><strong>Training code:</strong></p>

<pre class=""lang-py prettyprint-override""><code>device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") # cuda for gpu acceleration

# optimizer

optimizer = torch.optim.Adam(bertclassifier.parameters(), lr=0.001)


epochs = 15

bertclassifier.to(device) # taking the model to GPU if possible

# metrics

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

train_losses = []

train_metrics = {'acc': [], 'f1': []}
test_metrics = {'acc': [], 'f1': []}

# progress bar

from tqdm import tqdm_notebook

for e in tqdm_notebook(range(epochs)):
    train_loss = 0.0
    train_acc = 0.0
    train_f1 = 0.0
    batch_cnt = 0

    bertclassifier.train()

    print(f'epoch: {e+1}')

    for i_batch, (X, X_mask, y) in tqdm_notebook(enumerate(bbc_dataloader_train)):
        X = X.to(device)
        X_mask = X_mask.to(device)
        y = y.to(device)


        optimizer.zero_grad()

        loss, y_pred = bertclassifier(X, X_mask, y)

        train_loss += loss.item()
        loss.backward()
        optimizer.step()

        y_pred = torch.argmax(y_pred, dim = -1)

        # update metrics
        train_acc += accuracy_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy())
        train_f1 += f1_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average = 'micro')
        batch_cnt += 1

    print(f'train loss: {train_loss/batch_cnt}')
    train_losses.append(train_loss/batch_cnt)
    train_metrics['acc'].append(train_acc/batch_cnt)
    train_metrics['f1'].append(train_f1/batch_cnt)


    test_loss = 0.0
    test_acc = 0.0
    test_f1 = 0.0
    batch_cnt = 0

    bertclassifier.eval()
    with torch.no_grad():
        for i_batch, (X, y) in enumerate(bbc_dataloader_test):
            X = X.to(device)
            y = y.to(device)

            y_pred = bertclassifier(X) # in eval model we get the softmax output so, don't need to index


            y_pred = torch.argmax(y_pred, dim = -1)

            # update metrics
            test_acc += accuracy_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy())
            test_f1 += f1_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average = 'micro')
            batch_cnt += 1

    test_metrics['acc'].append(test_acc/batch_cnt)
    test_metrics['f1'].append(test_f1/batch_cnt)
</code></pre>

<p>Full source-code with the dataset is available here: <a href=""https://github.com/zabir-nabil/pytorch-nlp/blob/master/bert-article-classification.ipynb"" rel=""nofollow noreferrer"">https://github.com/zabir-nabil/pytorch-nlp/blob/master/bert-article-classification.ipynb</a></p>

<p>Update:</p>

<p>After observing the prediction, it seems model almost always predicts 0:</p>

<pre><code>bertclassifier.eval()
with torch.no_grad():
    for i_batch, (X, y) in enumerate(bbc_dataloader_test):
        X = X.to(device)
        y = y.to(device)

        y_pred = bertclassifier(X) # in eval model we get the softmax output so, don't need to index


        y_pred = torch.argmax(y_pred, dim = -1)

        print(y)
        print(y_pred)
        print('--------------------')
</code></pre>

<pre><code>tensor([4, 2, 2, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 0, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 4, 4, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 2, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 1, 4, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 2, 4, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 0, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 2, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 1, 2, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 4, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 2, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 4, 2, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 4, 4, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 1, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 2, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 1, 4, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 4, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 4, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 1, 1, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 2, 4, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 2, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 1, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 2, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 2, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 2, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 0, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 1, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 4, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 2, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 2, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 2, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 2, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 0, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 0, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 2, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 4, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 0, 4, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 2, 0, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 3, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 1, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 3, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 3, 0, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 2, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 0, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 0, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 1, 1, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 1, 0, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 4, 1, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 2, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 3, 4, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([3, 0, 4, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 1, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 4, 3, 1], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 3, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 3, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 0, 3, 4], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 1, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([1, 2, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([2, 0, 4, 2], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([4, 2, 4, 0], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
tensor([0, 0, 3, 3], device='cuda:0')
tensor([0, 0, 0, 0], device='cuda:0')
--------------------
...
...
</code></pre>

<p>Actually, the model is always predicting the same output <code>[0.2270, 0.1855, 0.2131, 0.1877, 0.1867]</code> for any input, it's like it didn't learn anything at all.</p>

<p>It's weird because my dataset is not imbalanced.</p>

<pre><code>Counter({'politics': 417,
         'business': 510,
         'entertainment': 386,
         'tech': 401,
         'sport': 511})
</code></pre>
","4622046","","4622046","","2020-05-23 11:31:59","2020-06-06 16:34:59","huggingface bert showing poor accuracy / f1 score [pytorch]","<pytorch><huggingface-transformers><bert-language-model>","1","4","3","","","CC BY-SA 4.0"
"61443480","1","61451916","","2020-04-26 15:37:47","","3","4449","<p>It's not entirely clear from the documentation, but I can see that <code>BertTokenizer</code> is initialised with <code>pad_token='[PAD]'</code>, so I assume when you encode with <code>add_special_tokens=True</code> then it would automatically pad it. Given that <code>pad_token_id=0</code>, I can't see any <code>0</code>s in the <code>token_ids</code> however:</p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
tokens = tokenizer.tokenize(text)
token_ids = tokenizer.encode(text, add_special_tokens=True, max_length=2048)

# Print the original sentence.
print('Original: ', text)

# Print the sentence split into tokens.
print('\nTokenized: ', tokens)

# Print the sentence mapped to token ids.
print('\nToken IDs: ', token_ids)
</code></pre>

<p>Output:</p>

<pre><code>Original:  Toronto's key stock index ended higher in brisk trading on Thursday, extending Wednesday's rally despite being weighed down by losses on Wall Street.
The TSE 300 Composite Index rose 29.80 points to close at 5828.62, outperforming the Dow Jones Industrial Average which slumped 21.27 points to finish at 6658.60.
Toronto added to Wednesday's 55-point rally while investors took profits in New York after the Dow's 92-point gains, said MMS International analyst Katherine Beattie.
""That shows that the markets are very fragile,"" Beattie said. ""They (investors) want to take advantage of any strength to sell,"" she said.
Toronto was also buoyed by its heavyweight gold group which jumped nearly 2.2 percent, aided by firmer COMEX gold prices. The key June contract rose $1.00 to $344.30.
Ten of Toronto's 14 sub-indices posted gains, led by golds, transportation, forestry products and consumer products.
The weak side included conglomerates, base metals and utilities.
Trading was heavy at 100 million shares worth C$1.54 billion ($1.1 billion).
Advancing stocks outnumbered declines 556 to 395, with 276 issues flat.
Among hot stocks, Bre-X Minerals Ltd. rose 0.13 to 2.30 on 5.0 million shares as investors continued to consider the viability of its Busang gold discovery in Indonesia.
Kenting Energy Services Inc. rose 0.25 to 9.05 after Precision Drilling Corp. amended its takeover offer
Bakery and foodstuffs maker George Weston Ltd. jumped 4.50 to close at 74.50, the TSE's top gainer.


Tokenized:  ['toronto', ""'"", 's', 'key', 'stock', 'index', 'ended', 'higher', 'in', 'brisk', 'trading', 'on', 'thursday', ',', 'extending', 'wednesday', ""'"", 's', 'rally', 'despite', 'being', 'weighed', 'down', 'by', 'losses', 'on', 'wall', 'street', '.', 'the', 'ts', '##e', '300', 'composite', 'index', 'rose', '29', '.', '80', 'points', 'to', 'close', 'at', '58', '##28', '.', '62', ',', 'out', '##per', '##form', '##ing', 'the', 'dow', 'jones', 'industrial', 'average', 'which', 'slumped', '21', '.', '27', 'points', 'to', 'finish', 'at', '66', '##58', '.', '60', '.', 'toronto', 'added', 'to', 'wednesday', ""'"", 's', '55', '-', 'point', 'rally', 'while', 'investors', 'took', 'profits', 'in', 'new', 'york', 'after', 'the', 'dow', ""'"", 's', '92', '-', 'point', 'gains', ',', 'said', 'mm', '##s', 'international', 'analyst', 'katherine', 'beat', '##tie', '.', '""', 'that', 'shows', 'that', 'the', 'markets', 'are', 'very', 'fragile', ',', '""', 'beat', '##tie', 'said', '.', '""', 'they', '(', 'investors', ')', 'want', 'to', 'take', 'advantage', 'of', 'any', 'strength', 'to', 'sell', ',', '""', 'she', 'said', '.', 'toronto', 'was', 'also', 'bu', '##oy', '##ed', 'by', 'its', 'heavyweight', 'gold', 'group', 'which', 'jumped', 'nearly', '2', '.', '2', 'percent', ',', 'aided', 'by', 'firm', '##er', 'come', '##x', 'gold', 'prices', '.', 'the', 'key', 'june', 'contract', 'rose', '$', '1', '.', '00', 'to', '$', '344', '.', '30', '.', 'ten', 'of', 'toronto', ""'"", 's', '14', 'sub', '-', 'indices', 'posted', 'gains', ',', 'led', 'by', 'gold', '##s', ',', 'transportation', ',', 'forestry', 'products', 'and', 'consumer', 'products', '.', 'the', 'weak', 'side', 'included', 'conglomerate', '##s', ',', 'base', 'metals', 'and', 'utilities', '.', 'trading', 'was', 'heavy', 'at', '100', 'million', 'shares', 'worth', 'c', '$', '1', '.', '54', 'billion', '(', '$', '1', '.', '1', 'billion', ')', '.', 'advancing', 'stocks', 'outnumbered', 'declines', '55', '##6', 'to', '395', ',', 'with', '276', 'issues', 'flat', '.', 'among', 'hot', 'stocks', ',', 'br', '##e', '-', 'x', 'minerals', 'ltd', '.', 'rose', '0', '.', '13', 'to', '2', '.', '30', 'on', '5', '.', '0', 'million', 'shares', 'as', 'investors', 'continued', 'to', 'consider', 'the', 'via', '##bility', 'of', 'its', 'bus', '##ang', 'gold', 'discovery', 'in', 'indonesia', '.', 'kent', '##ing', 'energy', 'services', 'inc', '.', 'rose', '0', '.', '25', 'to', '9', '.', '05', 'after', 'precision', 'drilling', 'corp', '.', 'amended', 'its', 'takeover', 'offer', 'bakery', 'and', 'foods', '##tu', '##ffs', 'maker', 'george', 'weston', 'ltd', '.', 'jumped', '4', '.', '50', 'to', 'close', 'at', '74', '.', '50', ',', 'the', 'ts', '##e', ""'"", 's', 'top', 'gain', '##er', '.']

Token IDs:  [101, 4361, 1005, 1055, 3145, 4518, 5950, 3092, 3020, 1999, 28022, 6202, 2006, 9432, 1010, 8402, 9317, 1005, 1055, 8320, 2750, 2108, 12781, 2091, 2011, 6409, 2006, 2813, 2395, 1012, 1996, 24529, 2063, 3998, 12490, 5950, 3123, 2756, 1012, 3770, 2685, 2000, 2485, 2012, 5388, 22407, 1012, 5786, 1010, 2041, 4842, 14192, 2075, 1996, 23268, 3557, 3919, 2779, 2029, 14319, 2538, 1012, 2676, 2685, 2000, 3926, 2012, 5764, 27814, 1012, 3438, 1012, 4361, 2794, 2000, 9317, 1005, 1055, 4583, 1011, 2391, 8320, 2096, 9387, 2165, 11372, 1999, 2047, 2259, 2044, 1996, 23268, 1005, 1055, 6227, 1011, 2391, 12154, 1010, 2056, 3461, 2015, 2248, 12941, 9477, 3786, 9515, 1012, 1000, 2008, 3065, 2008, 1996, 6089, 2024, 2200, 13072, 1010, 1000, 3786, 9515, 2056, 1012, 1000, 2027, 1006, 9387, 1007, 2215, 2000, 2202, 5056, 1997, 2151, 3997, 2000, 5271, 1010, 1000, 2016, 2056, 1012, 4361, 2001, 2036, 20934, 6977, 2098, 2011, 2049, 8366, 2751, 2177, 2029, 5598, 3053, 1016, 1012, 1016, 3867, 1010, 11553, 2011, 3813, 2121, 2272, 2595, 2751, 7597, 1012, 1996, 3145, 2238, 3206, 3123, 1002, 1015, 1012, 4002, 2000, 1002, 29386, 1012, 2382, 1012, 2702, 1997, 4361, 1005, 1055, 2403, 4942, 1011, 29299, 6866, 12154, 1010, 2419, 2011, 2751, 2015, 1010, 5193, 1010, 13116, 3688, 1998, 7325, 3688, 1012, 1996, 5410, 2217, 2443, 22453, 2015, 1010, 2918, 11970, 1998, 16548, 1012, 6202, 2001, 3082, 2012, 2531, 2454, 6661, 4276, 1039, 1002, 1015, 1012, 5139, 4551, 1006, 1002, 1015, 1012, 1015, 4551, 1007, 1012, 10787, 15768, 21943, 26451, 4583, 2575, 2000, 24673, 1010, 2007, 25113, 3314, 4257, 1012, 2426, 2980, 15768, 1010, 7987, 2063, 1011, 1060, 13246, 5183, 1012, 3123, 1014, 1012, 2410, 2000, 1016, 1012, 2382, 2006, 1019, 1012, 1014, 2454, 6661, 2004, 9387, 2506, 2000, 5136, 1996, 3081, 8553, 1997, 2049, 3902, 5654, 2751, 5456, 1999, 6239, 1012, 5982, 2075, 2943, 2578, 4297, 1012, 3123, 1014, 1012, 2423, 2000, 1023, 1012, 5709, 2044, 11718, 15827, 13058, 1012, 13266, 2049, 15336, 3749, 18112, 1998, 9440, 8525, 21807, 9338, 2577, 12755, 5183, 1012, 5598, 1018, 1012, 2753, 2000, 2485, 2012, 6356, 1012, 2753, 1010, 1996, 24529, 2063, 1005, 1055, 2327, 5114, 2121, 1012, 102]
</code></pre>
","6378530","","","","","2020-04-27 04:50:04","Huggingface's BERT tokenizer not adding pad token","<tokenize><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"61443541","1","","","2020-04-26 15:41:08","","0","615","<p>BERT provides an option to include pre-trained language models from Hugging Face in pipline. <a href=""https://rasa.com/docs/rasa/nlu/components/#hftransformersnlp"" rel=""nofollow noreferrer"">As per the doc:</a></p>

<pre><code>  - name: HFTransformersNLP
    # Name of the language model to use
    model_name: ""bert""
    # Pre-Trained weights to be loaded
    model_weights: ""bert-base-uncased""

    # An optional path to a specific directory to download and cache the pre-trained model weights.
    # The `default` cache_dir is the same as https://huggingface.co/transformers/serialization.html#cache-directory .
    cache_dir: null
</code></pre>

<p>Following this I configured my pipeline as:</p>

<pre><code>- name: ""HFTransformersNLP""
      # Name of the language model to use
      model_name: ""bert""
      # Pre-Trained weights to be loaded
      model_weights: ""bert-base-uncased""
      cache_dir: ""C:/Project ABC/cache/""
</code></pre>

<p>But the problem is that on running the training steps, the model keeps failing with:</p>

<blockquote>
  <p>OSError: Model name 'bert-base-uncased' was not found in tokenizers
  model name list (bert-base-uncased, bert-large-uncased,
  bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,
  bert-base-multilingual-cased, bert-base-chinese,
  bert-base-german-cased, bert-large-uncased-whole-word-masking,
  bert-large-cased-whole-word-masking,
  bert-large-uncased-whole-word-masking-finetuned-squad,
  bert-large-cased-whole-word-masking-finetuned-squad,
  bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased,
  bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1,
  bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed
  'bert-base-uncased' was a path, a model identifier, or url to a
  directory containing vocabulary files named ['vocab.txt'] but couldn't
  find such vocabulary files at this path or url.</p>
</blockquote>

<p>I did some research and it looks like that there might be issue in downloading the files from internet,  so I manually downloaded the files config.json, pytorch_model.bin and placed it in C:/Project ABC/cache/ still I am getting the same error message. Any idea how to resolve this, not giving cache directory is failing too with the same error.</p>
","4543398","","4543398","","2020-04-29 04:51:43","2020-04-29 04:51:43","Issue with incorporating BERT in RASA Pipeline","<chatbot><rasa-nlu><rasa><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"62961194","1","62981360","","2020-07-17 20:14:29","","4","1119","<p><strong>Background:</strong></p>
<p>Following along with this <a href=""https://stackoverflow.com/questions/60876394/does-bertforsequenceclassification-classify-on-the-cls-vector"">question</a> when using bert to classify sequences the model uses the &quot;[CLS]&quot; token representing the classification task. According to the paper:</p>
<blockquote>
<p>The first token of every sequence is always a special classification
token ([CLS]). The final hidden state corresponding to this token is
used as the aggregate sequence representation for classification
tasks.</p>
</blockquote>
<p>Looking at the huggingfaces repo their BertForSequenceClassification utilizes the bert pooler method:</p>
<pre><code>class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We &quot;pool&quot; the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
</code></pre>
<p>We can see they take the first token (CLS) and use this as a representation for the whole sentence. Specifically they perform <code>hidden_states[:, 0]</code> which looks a lot like its taking the first element from each state rather than taking the first tokens hidden state?</p>
<p><strong>My Question:</strong></p>
<p>What I don't understand is how do they encode the information from the entire sentence into this token? Is the CLS token a regular token which has its own embedding vector that &quot;learns&quot; the sentence level representation? Why can't we just use the average of the hidden states (the output of the encoder) and use this to classify?</p>
<p><strong>EDIT</strong>: After thinking a little about it: Because we use the CLS tokens hidden state to predict, is the CLS tokens embedding being trained on the task of classification as this is the token being used to classify (thus being the major contributor to the error which gets propagated to its weights?)</p>
","1726404","","6664872","","2020-07-19 14:11:14","2020-07-19 14:11:14","How does BertForSequenceClassification classify on the CLS vector?","<python><transformer><huggingface-transformers><bert-language-model>","1","0","2","","","CC BY-SA 4.0"
"61567599","1","61704348","","2020-05-02 23:18:17","","7","1005","<p>The <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""noreferrer"">HuggingFace BERT TensorFlow implementation</a> allows us to feed in a precomputed embedding in place of the embedding lookup that is native to BERT. This is done using the model's <code>call</code> method's optional parameter <code>inputs_embeds</code> (in place of <code>input_ids</code>). To test this out, I wanted to make sure that if I <em>did</em> feed in BERT's embedding lookup, I would get the same result as having fed in the <code>input_ids</code> themselves.</p>

<p>The result of BERT's embedding lookup can be obtained by setting the BERT configuration parameter <code>output_hidden_states</code> to <code>True</code> and extracting the first tensor from the last output of the <code>call</code> method. (The remaining 12 outputs correspond to each of the 12 hidden layers.)</p>

<p>Thus, I wrote the following code to test my hypothesis:</p>

<pre><code>import tensorflow as tf
from transformers import BertConfig, BertTokenizer, TFBertModel

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

input_ids = tf.constant(bert_tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True))[None, :]
attention_mask = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])
token_type_ids = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])

config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)
bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)

result = bert_model(inputs={'input_ids': input_ids, 
                            'attention_mask': attention_mask, 
                             'token_type_ids': token_type_ids})
inputs_embeds = result[-1][0]
result2 = bert_model(inputs={'inputs_embeds': inputs_embeds, 
                            'attention_mask': attention_mask, 
                             'token_type_ids': token_type_ids})

print(tf.reduce_sum(tf.abs(result[0] - result2[0])))  # 458.2522, should be 0
</code></pre>

<p>Again, the output of the <code>call</code> method is a tuple. The first element of this tuple is the output of the last layer of BERT. Thus, I expected <code>result[0]</code> and <code>result2[0]</code> to match. <strong>Why is this not the case?</strong></p>

<p>I am using Python 3.6.10 with <code>tensorflow</code> version 2.1.0 and <code>transformers</code> version 2.5.1.</p>

<p><strong>EDIT</strong>: Looking at some of the <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_bert.html"" rel=""noreferrer"">HuggingFace code</a>, it seems that the raw embeddings that are looked up when <code>input_ids</code> is given or assigned when <code>inputs_embeds</code> is given are added to the positional embeddings and token type embeddings before being fed into subsequent layers. If this is the case, then it <em>may</em> be possible that what I'm getting from <code>result[-1][0]</code> is the raw embedding plus the positional and token type embeddings. This would mean that they are erroneously getting added in again when I feed <code>result[-1][0]</code> as <code>inputs_embeds</code> in order to calculate <code>result2</code>.</p>

<p><strong>Could someone please tell me if this is the case and if so, please explain how to get the positional and token type embeddings, so I can subtract them out?</strong> Below is what I came up with for positional embeddings based on the equations given <a href=""https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3"" rel=""noreferrer"">here</a> (but according to the <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">BERT paper</a>, the positional embeddings may actually be learned, so I'm not sure if these are valid):</p>

<pre><code>import numpy as np

positional_embeddings = np.stack([np.zeros(shape=(len(sent),768)) for sent in input_ids])
for s in range(len(positional_embeddings)):
    for i in range(len(positional_embeddings[s])):
        for j in range(len(positional_embeddings[s][i])):
            if j % 2 == 0:
                positional_embeddings[s][i][j] = np.sin(i/np.power(10000., j/768.))
            else:
                positional_embeddings[s][i][j] = np.cos(i/np.power(10000., (j-1.)/768.))
positional_embeddings = tf.constant(positional_embeddings)
inputs_embeds += positional_embeddings
</code></pre>
","424306","","424306","","2020-05-09 20:59:10","2020-05-09 22:01:57","HuggingFace BERT `inputs_embeds` giving unexpected result","<python><tensorflow><nlp><huggingface-transformers><bert-language-model>","1","0","2","","","CC BY-SA 4.0"
"60990897","1","","","2020-04-02 11:50:03","","3","809","<p>I am able to use hugging face's mask filling pipeline to predict 1 masked token in a sentence using the below:</p>

<pre><code>!pip install -q transformers
from __future__ import print_function
import ipywidgets as widgets
from transformers import pipeline

nlp_fill = pipeline('fill-mask')
nlp_fill(""I am going to guess &lt;mask&gt; in this sentence"")
</code></pre>

<p>But does anyone have an opinion on what is the best way to do this if I want to predict 2 masked tokens? e.g. if the sentence is instead <code>""I am going to &lt;mask&gt; &lt;mask&gt; in this sentence""</code>?  </p>

<p>If i try and put this exact sentence into nlp_fill I get the error <code>""ValueError: only one element tensors can be converted to Python scalars""</code> so it doesn't work automatically.</p>

<p>Any help would be much appreciated!</p>
","3472360","","","","","2020-12-03 00:57:54","Best way of using hugging face's Mask Filling for more than 1 masked token at a time","<python><neural-network><nlp><huggingface-transformers>","1","3","","","","CC BY-SA 4.0"
"61168513","1","","","2020-04-12 08:07:13","","-2","485","<p>I've been using huggingface to make predictions for masked tokens and it works great. I noticed that for each prediction it gives a ""score"" and would like to be given the ""score"" for some tokens that it did not predict but that I provide. </p>

<p>For example, if my sentence is <code>""I ate bacon and &lt;mask&gt; for breakfast""</code> I can use <code>pipeline('fill-mask')</code> to get back predictions and their scores e.g. it might give me back [""eggs"", 0.1].  But what I would like to do is provide my own guess and then get back the score it assigns to my own guess. e.g. i might want to know what score it gives to the word ""pancakes"" in the situation. </p>

<p>Is this possible?  </p>
","3472360","","","","","2021-03-19 13:13:36","Using huggingface fill-mask pipeline to get the ""score"" for a result it didn't suggest","<python><neural-network><nlp><huggingface-transformers><spacy-transformers>","2","0","2","","","CC BY-SA 4.0"
"63899303","1","63956740","","2020-09-15 09:50:12","","0","2248","<p>I have a NLP model trained on Pytorch to be run in Jetson Xavier. I installed Jetson stats to monitor usage of CPU and GPU. When I run the Python script, only CPU cores work on-load, GPU bar does not increase. I have searched on Google about that with keywords of &quot; How to check if pytorch is using the GPU?&quot; and checked results on stackoverflow.com etc. According to their advices to someone else facing similar issue, cuda is available and there is cuda device in my Jetson Xavier. However, I donâ€™t understand why GPU bar does not change, CPU core bars go to the ends.</p>
<p>I donâ€™t want to use CPU, it takes so long to compute. In my opinion, it uses CPU, not GPU. How can I be sure and if it uses CPU, how can I change it to GPU?</p>
<p><strong>Note:</strong> Model is taken from huggingface transformers library. I have tried to use cuda() method on the model. (model.cuda()) In this scenario, GPU is used but I can not get an output from model and raises exception.</p>
<p>Here is the code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline
import torch

BERT_DIR = &quot;savasy/bert-base-turkish-squad&quot;    

tokenizer = AutoTokenizer.from_pretrained(BERT_DIR)
model = AutoModelForQuestionAnswering.from_pretrained(BERT_DIR)
nlp=pipeline(&quot;question-answering&quot;, model=model, tokenizer=tokenizer)


def infer(question,corpus):
    try:
        ans = nlp(question=question, context=corpus)
        return ans[&quot;answer&quot;], ans[&quot;score&quot;]
    except:
        ans = None
        pass

    return None, 0
</code></pre>
","11549726","","681865","","2020-09-15 10:27:32","2020-09-18 13:30:28","Pytorch NLP model doesnâ€™t use GPU when making inference","<python><pytorch><huggingface-transformers><nvidia-jetson>","2","0","","","","CC BY-SA 4.0"
"63899305","1","63909021","","2020-09-15 09:50:24","","0","127","<p>I am trying to run <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">transformers</a> from <a href=""https://huggingface.co/transformers/index.html"" rel=""nofollow noreferrer"">huggingface</a> in Google Cloud Run.</p>
<p>My first idea was to run one of the dockerfiles provided by huggingface, but it seems that is not possible.</p>
<p>Any ideas on how to get around this error?</p>
<pre><code>Step 6/9 : WORKDIR /workspace
 ---&gt; Running in xxx
Removing intermediate container xxx
 ---&gt; xxx
Step 7/9 : COPY . transformers/
 ---&gt; xxx
Step 8/9 : RUN cd transformers/ &amp;&amp;     python3 -m pip install --no-cache-dir .
 ---&gt; Running in xxx
â†[91mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.
The command '/bin/sh -c cd transformers/ &amp;&amp;     python3 -m pip install --no-cache-dir .' returned a non-zero code: 1
ERROR
ERROR: build step 0 &quot;gcr.io/cloud-builders/docker&quot; failed: step exited with non-zero status: 1
â†[0m
-------------------------------------------------------------------------------------------------------------------------------------------------------------------

ERROR: (gcloud.builds.submit) build xxx completed with status &quot;FAILURE&quot;
</code></pre>
<p>Dockerfile from <a href=""https://github.com/huggingface/transformers/tree/master/docker/transformers-tensorflow-gpu"" rel=""nofollow noreferrer"">huggingface</a>:</p>
<pre><code>FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04
LABEL maintainer=&quot;Hugging Face&quot;
LABEL repository=&quot;transformers&quot;

RUN apt update &amp;&amp; \
    apt install -y bash \
                   build-essential \
                   git \
                   curl \
                   ca-certificates \
                   python3 \
                   python3-pip &amp;&amp; \
    rm -rf /var/lib/apt/lists

RUN python3 -m pip install --no-cache-dir --upgrade pip &amp;&amp; \
    python3 -m pip install --no-cache-dir \
    mkl \
    tensorflow

WORKDIR /workspace
COPY . transformers/
RUN cd transformers/ &amp;&amp; \
    python3 -m pip install --no-cache-dir .

CMD [&quot;/bin/bash&quot;]
</code></pre>
<p>.dockerignore file from Google Cloud Run <a href=""https://cloud.google.com/run/docs/quickstarts/build-and-deploy"" rel=""nofollow noreferrer"">documentation</a>:</p>
<pre><code>Dockerfile
README.md
*.pyc
*.pyo
*.pyd
__pycache__
.pytest_cache
</code></pre>
<p>---- Edit:</p>
<p>Managed to get working based on the answer from Dustin. I basically:</p>
<ul>
<li>left the Dockerfile in the root folder, together with the transformers folder.</li>
<li>updated the COPY line from the dockerfile to:</li>
</ul>
<pre><code>COPY . ./
</code></pre>
","13505025","","13505025","","2020-09-16 04:06:12","2020-09-16 04:06:12","Docker error when containerizing app in Google Cloud Run","<google-cloud-platform><dockerfile><google-cloud-run><huggingface-transformers><google-cloud-sdk>","1","0","","","","CC BY-SA 4.0"
"63902401","1","","","2020-09-15 13:00:28","","0","547","<p>I have a huge data set of tweet texts (almost 3 billion tweets). I finetuned a BERT model with an annotated data set for a classification task.</p>
<p>Is there a way to make predictions more efficient and faster? I have access to one GPU only.</p>
<p>At the moment I use the following code:</p>
<pre><code>import pandas as pd

import sys

from transformers import pipeline

judge = pipeline(
task=&quot;sentiment-analysis&quot;,
model=&quot;/trained_disasterlabels&quot;,
tokenizer='bert-base-uncased',
device=0)

id_disaster=sys.argv[1]

path=&quot;/disaster_data/ids_&quot;+id_disaster+&quot;_text&quot;

open(path+&quot;_labelpred&quot;, 'w').close()

n=200
with open(path,&quot;rb&quot;) as f:
    batch=[]
    for line in f:
        batch.append(line.decode().rstrip(&quot;\n&quot;))
        if len(batch)==n:

            preds=judge(batch)
            preds_labels=[x['label'].replace(&quot;LABEL_&quot;,&quot;&quot;) for x in preds]
            preds_probs=[round(x['score'],4) for x in preds]
            
            valsdf=pd.DataFrame({&quot;labels&quot;:preds_labels,&quot;probs&quot;:preds_probs})
            
            valsdf.to_csv(path+&quot;_labelpred&quot;, mode='a', header=False,index=False)
            
            batch=[]

    preds=judge(batch)
    preds_labels=[x['label'].replace(&quot;LABEL_&quot;,&quot;&quot;) for x in preds]
    preds_probs=[round(x['score'],4) for x in preds]
    
    valsdf=pd.DataFrame({&quot;labels&quot;:preds_labels,&quot;probs&quot;:preds_probs})
    
    valsdf.to_csv(path+&quot;_labelpred&quot;, mode='a', header=False,index=False)
</code></pre>
<p>Before I also experimented with the following:</p>
<pre><code>model=BertForSequenceClassification.from_pretrained(&quot;/trained_disasterlabels&quot;)

model.eval()

model.to('cuda')

tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')

import torch.nn.functional as F 

path=&quot;/disaster_data/ids_&quot;+id_disaster+&quot;_text&quot;

open(path+&quot;_labelpred&quot;, 'w').close()

n=100
 
with open(path,&quot;rb&quot;) as f:
    batch=[]
    for line in f:
        batch.append(line.decode().rstrip(&quot;\n&quot;))
        if len(batch)==n:
           input_ids = tokenizer.batch_encode_plus(batch, add_special_tokens=True,return_tensors = 'pt', padding=True, truncation=True)
            
            input_ids.to('cuda')
            
            with torch.no_grad():
                last_hidden_states = model(**input_ids)
            
            temp_cpu = F.softmax(last_hidden_states[0],dim=1).detach().cpu().numpy()    

            valsdf=pd.DataFrame(temp_cpu.tolist(),columns=[&quot;prob&quot;,&quot;labelpred&quot;])
                        
            batch=[]
            
    input_ids = tokenizer.batch_encode_plus(batch, add_special_tokens=True,return_tensors = 'pt', padding=True, truncation=True)

    with torch.no_grad():
        last_hidden_states = model(**input_ids)  
        
    temp_cpu = F.softmax(last_hidden_states[0],dim=1).detach().cpu().numpy()
            
    valsdf=pd.DataFrame(temp_cpu.tolist(),columns=[&quot;labelpred&quot;,&quot;prob&quot;])
    
    valsdf.to_csv(path+&quot;_labelpred&quot;, mode='a', header=False,index=False)
           
    batch=[]
</code></pre>
","7546231","","681865","","2020-09-15 13:03:32","2020-09-15 13:03:32","Efficient predictions from pytorch model (transformers library) for NLP tasks with a GPU","<python-3.x><nlp><pytorch><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"61227950","1","","","2020-04-15 11:49:22","","1","137","<pre><code>model = BertModel.from_pretrained('bert-base-uncased', config=BertConfig.from_pretrained('bert-base-uncased',output_hidden_states=True))
outputs = model(input_ids) 
hidden_states = outputs[2]
</code></pre>

<p>hidden_states is a tuple of 13 <code>torch.FloatTensors</code>. Each tensor is of size: <code>(batch_size, sequence_length, hidden_size)</code>.
According to the documentation, the 13 tensors are the hidden states of the embedding and the 12 encoder layers. </p>

<p>My question: </p>

<p>Is <code>hidden_states[0]</code> the embedding layer while <code>hidden_states[12]</code> is the 12th encoder layer or</p>

<p>Is <code>hidden_states[0]</code> the embedding layer while <code>hidden_states[12]</code> is the 1st encoder layer or</p>

<p>Is <code>hidden_states[0]</code> the 12th encoder layer while <code>hidden_states[12]</code> is the embedding layer or</p>

<p>Is <code>hidden_states[0]</code> the 1st encoder layer while <code>hidden_states[12]</code> is the embedding layer </p>

<p>I havent found this found clearly stated anywhere else.</p>
","6711091","","","","","2020-04-16 11:28:24","How are contents of hidden_states tuple in BertModel in the transformers library arranged","<pytorch><python-3.7><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"61482518","1","","","2020-04-28 14:22:07","","0","278","<p>I'm trying to execute <a href=""https://github.com/huggingface/transformers/tree/master/examples/ner"" rel=""nofollow noreferrer"">this script</a> using <code>run_ner.py</code> but everything I tried to continue fine tuning from checkpoint failed. Any ideas?</p>

<p>I run it using Google Colab. Hereafter the cell content I run:</p>

<pre><code>%cd ""/content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27""
%pip install .
%pip install --upgrade .
%pip install seqeval
from fastai import * 
from transformers import *
%cd ""/content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner""

!python ""/content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/run_ner.py"" --data_dir ./ \
                                                                                                          --model_type bert \
                                                                                                          --labels ./labels.txt \
                                                                                                          --model_name_or_path ""/content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/bert-base-256/checkpoint-10000"" \
                                                                                                          --output_dir ""/content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/bert-base-256/check"" \
                                                                                                          --max_seq_length ""256"" \
                                                                                                          --num_train_epochs ""5"" \
                                                                                                          --per_gpu_train_batch_size ""4"" \
                                                                                                          --save_steps ""10000"" \
                                                                                                          --seed ""1"" \
                                                                                                          --do_train --do_eval --do_predict
</code></pre>

<p>As you can see, I already tried to substitute model_name_or_path parameter value (that was ""bert-base-cased"") with checkpoint directory but several errors occurred, asking for the right model name and missing files.</p>

<pre><code>04/28/2020 15:16:36 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/bert-base-256/checkpoint-10000' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/bert-base-256/checkpoint-10000' is a path, a model identifier, or url to a directory containing tokenizer files.
04/28/2020 15:16:36 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/bert-base-256/checkpoint-10000/vocab.txt. We won't load it.
04/28/2020 15:16:36 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/bert-base-256/checkpoint-10000/added_tokens.json. We won't load it.
04/28/2020 15:16:36 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/bert-base-256/checkpoint-10000/special_tokens_map.json. We won't load it.
04/28/2020 15:16:36 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/bert-base-256/checkpoint-10000/tokenizer_config.json. We won't load it.
Traceback (most recent call last):
File ""/content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/run_ner.py"", line 290, in &lt;module&gt;
main()
File ""/content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/run_ner.py"", line 149, in main
use_fast=model_args.use_fast,
File ""/usr/local/lib/python3.6/dist-packages/transformers/tokenization_auto.py"", line 197, in from_pretrained
return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
File ""/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py"", line 868, in from_pretrained
return cls._from_pretrained(*inputs, **kwargs)
File ""/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py"", line 971, in _from_pretrained
list(cls.vocab_files_names.values()),
OSError: Model name '/content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/bert-base-256/checkpoint-10000' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed '/content/drive/My Drive/Colab Notebooks/NER/Batteria/transformers-master_2020_04_27/examples/ner/bert-base-256/checkpoint-10000' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.
</code></pre>

<p>Thank you in advance.</p>
","13425873","","13425873","","2020-04-28 15:22:31","2020-05-09 15:43:39","How can I continue finetuning from checkpoint using the NER script?","<huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"64223792","1","","","2020-10-06 10:18:13","","0","45","<p>I'm working with transformer based model, but unable to formulate this learning rate scheduler with PyTorch</p>
<pre class=""lang-sh prettyprint-override""><code>lrate = d_model ^ 0.5 * min( step_num ^ 0.5, step_num * warmup_steps ^ -1.5)
</code></pre>
<p>Thanks</p>
","13093875","","13093875","","2020-10-06 10:21:44","2020-10-06 10:21:44","How to formulate this particular learning rate scheduler in PyTorch?","<nlp><pytorch><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"63907100","1","63910923","","2020-09-15 17:31:42","","0","914","<p>This issue has been posted a handful of times in SO, but I still can't figure out what is the problem with my code, especially because it comes from a tutorial in <a href=""https://medium.com/atheros/text-classification-with-transformers-in-tensorflow-2-bert-2f4f16eff5ad"" rel=""nofollow noreferrer"">medium</a> and the author makes the code available on google <a href=""https://colab.research.google.com/drive/1934Mm2cwSSfT5bvi78-AExAl-hSfxCbq#scrollTo=kVlHGAbfjvjM"" rel=""nofollow noreferrer"">colab</a></p>
<p>I have seen other users having problem with wrong variable types <a href=""https://stackoverflow.com/questions/52582275/tf-data-with-multiple-inputs-outputs-in-keras"">#56304986</a> (which is not my case, as my model input is the output of <code>tokenizer</code>) and even seen the function I am trying to use (<code>tf.data.Dataset.from_tensor_slices</code>) being suggested as a solution <a href=""https://stackoverflow.com/questions/56304986/valueerror-cant-convert-non-rectangular-python-sequence-to-tensor"">#56304986</a>.</p>
<p>The line yielding error is:</p>
<pre><code># train dataset
ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)
</code></pre>
<p>where the method <code>encode_examples</code> is defined as (I have inserted an <code>assert</code> line into the <code>encode_examples</code> method to be sure my problem was not unmatching lenghts):</p>
<pre><code>def encode_examples(ds, limit=-1):
    # prepare list, so that we can build up final TensorFlow dataset from slices.
    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    if (limit &gt; 0):
        ds = ds.take(limit)

    for review, label in tfds.as_numpy(ds):

            bert_input = convert_example_to_feature(review.decode())

            ii = bert_input['input_ids']
            tti = bert_input['token_type_ids']
            am = bert_input['attention_mask']

            assert len(ii) == len(tti) == len(am), &quot;unmatching lengths!&quot;

            input_ids_list.append(ii)
            token_type_ids_list.append(tti)
            attention_mask_list.append(am)
            label_list.append([label])

    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)
</code></pre>
<p>The data is loaded like this (here i changed the dataset to get only 10% of the training data so I could speed up the debugging)</p>
<pre><code>(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split = ['train[:10%]','test[10%:15%]'], as_supervised=True, with_info=True)
</code></pre>
<p>And the other two calls(<code>convert_example_to_feature</code> and <code>map_example_to_dict</code>) and the tokenizer are as follow:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
def convert_example_to_feature(text):
    # combine step for tokenization, WordPiece vector mapping, adding special tokens as well as truncating reviews longer than the max length
    return tokenizer.encode_plus(text,
                                 add_special_tokens = True, # add [CLS], [SEP]
                                 #max_length = max_length, # max length of the text that can go to BERT
                                 pad_to_max_length = True, # add [PAD] tokens
                                 return_attention_mask = True,)# add attention mask to not focus on pad tokens

def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):
    return ({&quot;input_ids&quot;: input_ids,
            &quot;token_type_ids&quot;: token_type_ids,
            &quot;attention_mask&quot;: attention_masks,
            }, label)
</code></pre>
<p>I suspect the error might have something to do with different versions of TensorFlow (I am using 2.3), but unfortunately I couldn't run the snippets in the google.colab notebook for memory reasons.</p>
<p>Does anyone know where what is the problem with my code? Thanks for your time and attention.</p>
","4075155","","10396469","","2021-02-17 07:05:10","2021-02-17 07:05:19","ValueError: Can't convert non-rectangular Python sequence to Tensor when using tf.data.Dataset.from_tensor_slices","<python><tensorflow><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"63924567","1","63939998","","2020-09-16 16:44:47","","2","395","<p>I am trying to fine-tune gpt2 with a custom dataset of mine. I created a basic example with the documentation from hugging-face transformers. I receive the mentioned error. I know what it means: (basically it is calling backward on a non-scalar tensor) but since I almost use only API calls, I have no idea how to fix this issue. Any suggestions?</p>
<pre><code>from pathlib import Path
from absl import flags, app
import IPython
import torch
from transformers import GPT2LMHeadModel, Trainer,  TrainingArguments
from data_reader import GetDataAsPython

# this is my custom data, but i get the same error for the basic case below
# data = GetDataAsPython('data.json')
# data = [data_point.GetText2Text() for data_point in data]
# print(&quot;Number of data samples is&quot;, len(data))

data = [&quot;this is a trial text&quot;, &quot;this is another trial text&quot;]

train_texts = data

from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

special_tokens_dict = {'pad_token': '&lt;PAD&gt;'}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
train_encodigs = tokenizer(train_texts, truncation=True, padding=True)


class BugFixDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    
    def __getitem__(self, index):
        item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

train_dataset = BugFixDataset(train_encodigs)

training_args = TrainingArguments(
    output_dir='./results',          
    num_train_epochs=3,              
    per_device_train_batch_size=1,  
    per_device_eval_batch_size=1,   
    warmup_steps=500,                
    weight_decay=0.01,               
    logging_dir='./logs',
    logging_steps=10,
)

model = GPT2LMHeadModel.from_pretrained('gpt2', return_dict=True)
model.resize_token_embeddings(len(tokenizer))

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()
</code></pre>
","8010568","","","","","2020-09-17 14:44:44","GPT2 on Hugging face(pytorch transformers) RuntimeError: grad can be implicitly created only for scalar outputs","<python><nlp><pytorch><huggingface-transformers><huggingface-tokenizers>","1","0","1","","","CC BY-SA 4.0"
"61798573","1","61800323","","2020-05-14 13:27:42","","15","6494","<p>Running the below code downloads a model - does anyone know what folder it downloads it to?</p>

<pre><code>!pip install -q transformers
from transformers import pipeline
model = pipeline('fill-mask')
</code></pre>
","3472360","","","","","2021-06-13 02:48:01","Where does hugging face's transformers save models?","<huggingface-transformers>","3","0","3","","","CC BY-SA 4.0"
"62567796","1","","","2020-06-25 04:17:15","","2","203","<p>The way i think is that,</p>
<p>The attention mask (not padding mask) for decoder of an autoregressive transformer model is a subsequent generated mask like as</p>
<pre class=""lang-sh prettyprint-override""><code>[0, 1, 1, 1, 1]
[0, 0, 1, 1, 1]
[0, 0, 0, 1, 1]
[0, 0, 0, 0, 1]
</code></pre>
<p>While there's no attention mask being fed to the decoder of non-autoregressive transformer model, there's only a padding mask.</p>
<p>Am i correct?</p>
","13093875","","","","","2020-06-25 04:17:15","Is the difference between autoregressive transformer and non-autoregressive transformer is only the attention masks in decoder?","<pytorch><transformer><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"61326892","1","61339096","","2020-04-20 16:04:42","","0","400","<p>I am trying to access the gradient of the loss in DistilBERT with respect to each attention weight in the first layer. I could access the computed gradient value of the output weight matrix via the following code when <code>requires_grad=True</code> </p>

<pre><code>loss.backward()
for name, param in model.named_parameters():
    if name == 'transformer.layer.0.attention.out_lin.weight':
       print(param.grad)  #shape is [768,768]
</code></pre>

<p>where <code>model</code> is the loaded distilbert model.
My question is how to get the gradient with respect to [SEP] or [CLS] or other tokens' attention? I need it to reproduce the figure about the ""Gradient-based feature importance estimates for attention to [SEP]"" in the following link: 
<a href=""https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062"" rel=""nofollow noreferrer"">https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062</a></p>

<p>A similar question for the same purpose has been asked in the following, but it is not my issue:
<a href=""https://stackoverflow.com/questions/61286574/bert-token-importance-measuring-issue-grad-is-none"">BERT token importance measuring issue. Grad is none</a> </p>
","5996916","","","","","2020-04-21 08:26:26","Gradient of the loss of DistilBERT for measuring token importance","<pytorch><transformer><attention-model><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"63904821","1","64529095","","2020-09-15 15:12:10","","0","1438","<p>I am using huggingface transformer models for <strong>text-summarization</strong>.
Currently I am testing different models such as <strong>T5</strong> and <strong>Pegasus</strong>.
Now these models were trained for summarizing Big Texts into very short like a maximum of two sentences. Now I have the task, that I want summarizations, that are about half the size of the text, ergo the generated summaries are too small for my purpose.</p>
<p>My question now is, if there is a way to tell the model that another sentence came before?
Kind of similar to the logic inside stateful RNNs (although I know they work completly different).
If yes, I could summarize small windows over the sentences always with the information which content came before.</p>
<p>Is that just a thing of my mind? I cant believe that I am the only one, who wants to create shorter summaries, but not only 1 or two sentence long ones.</p>
<p>Thank you</p>
","13804443","","","","","2020-10-25 21:39:29","Using Transformer for Text-Summarization","<tensorflow><pytorch><huggingface-transformers><transformer><summarization>","1","0","","","","CC BY-SA 4.0"
"63387831","1","63403438","","2020-08-13 03:44:15","","0","406","<p>SPECS:
OS: Windows 10
CUDA: 10.1
GPU: RTX 2060 6G VRAM (x2)
RAM: 32GB
tutorial: <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">https://huggingface.co/blog/how-to-train</a></p>
<p>Hello I am trying to train my own language model and I have had some memory issues. I have tried to run some of this code in Pycharm on my computer and then trying to replicate in my Collab Pro Notebook.</p>
<h2>First, my code</h2>
<pre><code>from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForMaskedLM, LineByLineTextDataset
from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments

config = RobertaConfig(vocab_size=60000, max_position_embeddings=514, num_attention_heads=12, num_hidden_layers=6,
                       type_vocab_size=1)

tokenizer = RobertaTokenizerFast.from_pretrained(&quot;./MODEL DIRECTORY&quot;, max_len=512)

model = RobertaForMaskedLM(config=config)

print(&quot;making dataset&quot;)

dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=&quot;./total_text.txt&quot;, block_size=128)

print(&quot;making c&quot;)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

training_args = TrainingArguments(output_dir=&quot;./MODEL DIRECTORY&quot;, overwrite_output_dir=True, num_train_epochs=1,
                                  per_gpu_train_batch_size=64, save_steps=10000, save_total_limit=2)
print(&quot;Building trainer&quot;)
trainer = Trainer(model=model, args=training_args, data_collator=data_collator, train_dataset=dataset,
                  prediction_loss_only=True)
trainer.train()

trainer.save_model(&quot;./MODEL DIRECTORY&quot;)

</code></pre>
<p><code>&quot;./total_text.txt&quot;</code> being a 1.7GB text file.</p>
<h2>PyCharm Attempt</h2>
<p>This code on pycharm builds the dataset and then would throw an error saying that my preferred gpu was running out of memory, and that Torch was already using 3.7GiB of memory.</p>
<p>I tried:</p>
<ul>
<li>import gc doing a gc clear to try to flush what ever was going on my gpu</li>
<li>Decreasing my batch size for my gpu (training only happened on a batch size of 8 resulting in 200,000+ epochs that all took 1.17 seconds)</li>
<li>Setting my <code>os.environ[&quot;CUDA_VISIBLE_OBJECTS&quot;] =&quot;&quot;</code> so that torch would have to use my CPU and not my GPU. Still threw same gpu memory error...</li>
</ul>
<p>So succumbing to the fact that torch, for the time being, was forcing itself to use my gpu, I decided to go to Collab.</p>
<h2>Collab Attempt</h2>
<p>Collab has different issues with my code. It does not have the memory to build the dataset, and crashes due to RAM shortages. I purchased a Pro account and then increased the usable RAM to 25GB, still memory shortages.</p>
<p>Cheers!</p>
","10882883","","6664872","","2020-08-13 17:56:22","2020-08-13 21:27:20","Memory Issue while following LM tutorial","<python><pytorch><google-colaboratory><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"61962710","1","","","2020-05-22 19:42:05","","5","2105","<p>I want to fine tune BERT on a specific domain. I have texts of that domain in text files. How can I use these to fine tune BERT?
I am looking <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforpretraining"" rel=""noreferrer"">here</a> currently.</p>

<p>My main objective is to get sentence embeddings using BERT.</p>
","11103062","","","","","2021-07-06 08:39:14","How to fine tune BERT on unlabeled data?","<nlp><pytorch><huggingface-transformers><bert-language-model>","4","0","2","","","CC BY-SA 4.0"
"62565480","1","","","2020-06-24 23:16:45","","4","183","<p>I have the following goal, which I have been trying to achieve with the Huggingface Library but I encountered some roadblocks.</p>
<p><strong>The Problem:</strong></p>
<p>I want to generate sentences in a differentiable way at training time. Why am I doing this? I want to apply a discriminator to this output to generate sentences with certain properties, which are &quot;enforced&quot; by the discriminator. These sentences will also be conditioned on a input sentence, so I need a Encoder Decoder Model.</p>
<p>To get around the non differentiability of argmax, I simply take the softmax output of the decoder and multiply it with my embedding matrix. Then I am taking this embedded input and feed it into a transformer discriminator, which simply classifies the input as original/fake. Then I backpropagate through the encoder decoder. Just as one would do it with a normal GAN.</p>
<p>So far I have tried to use the <code>EncoderDecoderModel</code> from Huggingface. This class has a method named generate, which generates sentences in a non differentiable way (greedy or beam-search). So I dug through the source code and tried to build my own differentiable generate method. I didn't get it to work though.</p>
<p><strong>Questions:</strong></p>
<ul>
<li>Is there a reasonably easy way to do this with the Huggingface Library, as I really want to use the pretrained models and everything else that comes with it?</li>
<li>Is there a way to invoke the forward method of the decoder and only generate one new token, not the whole sequence again?</li>
</ul>
<p>Thanks for your help, I would really appreciate it, I have been stuck on this for quiet a while now.</p>
","6144440","","","","","2021-03-02 18:15:19","Differentially generate sentences with Huggingface Library for adversarial training (GANs)","<nlp><pytorch><huggingface-transformers><generative-adversarial-network><discriminator>","0","0","","","","CC BY-SA 4.0"
"63911955","1","","","2020-09-16 01:38:21","","1","1724","<p>Tryin to train GPT-2 on a very large text, in order to generate text from <strong>specific domain</strong>.<br />
Working with tensorflow2 .</p>
<p>For example, let's say I have all of Harry Potter books :)<br />
And I want to train the GPT-2 on them, so I could later generate text from the Harry Potter domain.</p>
<pre><code>from tensorflow.keras.utils import get_file
from transformers import GPT2Tokenizer, TFGPT2Model

text = '...'
# Length of text: 474429 characters
# 84 unique characters

tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')
model = TFGPT2Model.from_pretrained('gpt2-medium')

encoded_input = tokenizer(text, return_tensors='tf') # ERROR
output = model(encoded_input)

input_ids = tokenizer.encode('severus snape', return_tensors='tf')
greedy_output = model.generate(input_ids, max_length=50)
print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))
</code></pre>
<blockquote>
<p>ERROR: Token indices sequence length is longer than the specified
maximum sequence length for this model (149887 &gt; 1024). Running this
sequence through the model will result in indexing errors</p>
</blockquote>
<p>So how would I make it work?<br />
How to feed the model a large new text to train on?</p>
<p><strong>EDIT:</strong><br />
when Trying to concat, tokenizer works, but model doesn't:</p>
<pre><code>from textwrap import wrap
text_batches = wrap(text, 1000)

encoded_input = None

for tb in text_batches:
    current = tokenizer(tb, return_tensors='tf')
  
    if encoded_input == None:
        encoded_input = current
    else:
        encoded_input['input_ids']      = tf.concat([encoded_input['input_ids'], current['input_ids']], axis=-1)
        encoded_input['attention_mask'] = tf.concat([encoded_input['attention_mask'], current['attention_mask']], axis=1)

output = model(encoded_input) # ERROR
</code></pre>
<blockquote>
<p>ERROR:  InvalidArgumentError: indices[0,1024] = 1024 is not in [0,
1024) [Op:ResourceGather]</p>
</blockquote>
<p>What am I missing?</p>
","4048691","","4048691","","2020-09-16 03:13:53","2020-09-16 03:13:53","Fine tune GPT-2 on large text for generate a domain text","<tensorflow><keras><deep-learning><nlp><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"64217601","1","","","2020-10-05 23:57:33","","3","684","<p>What's the right way to return a limited number of layers using the longformer API?</p>
<p>Unlike this case in basic <a href=""https://stackoverflow.com/questions/63461262/bert-sentence-embeddings-from-transformers"">BERT</a>, it's not clear to me from the return type how to get only the last N layers.</p>
<p>So, I run this:</p>
<pre><code>from transformers import LongformerTokenizer, LongformerModel

text = &quot;word &quot; * 4096 # long document!

tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')
model = LongformerModel.from_pretrained(&quot;allenai/longformer-base-4096&quot;)

encoded_input = tokenizer(text, return_tensors=&quot;pt&quot;, max_length=4096, truncation=True)
output = model(**encoded_input)
</code></pre>
<p>And I get dimensions like so from my return:</p>
<pre><code>&gt;&gt;&gt; output[0].shape
torch.Size([1, 4096, 768])

&gt;&gt;&gt; output[1].shape
torch.Size([1, 768])
</code></pre>
<p>You can see the shape of [0] is curiously similar to my number of tokens.  I believe that slicing this would just give me fewer tokens, not just the last N layers.</p>
<h2>Update from answer below</h2>
<p>Even asking for <code>output_hidden_states</code>, the dimensions still look off, and it's not clear to me
how to reduce these to vector sized, 1-d embedding.  Here's what I mean:</p>
<pre><code>encoded_input = tokenizer(text, return_tensors=&quot;pt&quot;, max_length=4096, truncation=True)
output = model(**encoded_input, output_hidden_states=True)
</code></pre>
<p>Ok, now let's look into output[2], the third item of the tuple:</p>
<pre><code>&gt;&gt;&gt; len(output[2])
13
</code></pre>
<p>Suppose we want to see the last 3 of the 13 layers:</p>
<pre><code>&gt;&gt;&gt; [pair[0].shape for pair in output[2][-3:]]
[torch.Size([4096, 768]), torch.Size([4096, 768]), torch.Size([4096, 768])]
</code></pre>
<p>So we see each of the 13 layers is shaped (4096 x 768), and they look like:</p>
<pre><code>&gt;&gt;&gt; [pair[0] for pair in output[2][-3:]]
[tensor([[-0.1494,  0.0190,  0.0389,  ..., -0.0470,  0.0259,  0.0609],
</code></pre>
<p>We still have a size of 4096, in that it corresponds to my token count:</p>
<pre><code>&gt;&gt;&gt; np.mean(np.stack([pair[0].detach().numpy() for pair in output[2][-3:]]), axis=0).shape
(4096, 768)
</code></pre>
<p>Averaging these together does not seem like it would give a valid embedding (for comparisons like cosine similarity).</p>
","1052117","","1052117","","2020-10-06 16:34:05","2021-07-19 06:42:28","The last layers of longformer for document embeddings","<word-embedding><huggingface-transformers>","2","0","1","","","CC BY-SA 4.0"
"63920887","1","","","2020-09-16 13:15:38","","1","135","<p>In the documentation on text generation (<a href=""https://huggingface.co/transformers/main_classes/model.html#generative-models"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/model.html#generative-models</a>) there is the option to put</p>
<pre><code>bad_words_ids (List[int], optional) â€“ List of token ids that are not allowed to be generated. In order to get the tokens of the words that should not appear in the generated text, use tokenizer.encode(bad_word, add_prefix_space=True).
</code></pre>
<p>Is there also the option to put something along the lines of &quot;allowed_words_ids&quot;? The idea would be to restrict the language of the generated texts.</p>
","7546231","","","","","2020-09-16 13:15:38","Whitelist tokens for text generation (XLNet, GPT-2) in huggingface-transformers","<python-3.x><nlp><pytorch><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"65779562","1","","","2021-01-18 17:46:22","","1","195","<p>I want to fine-tune a pre-trained huggingface model for a particular domain. From <a href=""https://stackoverflow.com/questions/64712375/fine-tune-bert-for-specific-domain-unsupervised"">this</a> answer I know I can do it using <a href=""http://run_mlm.py"" rel=""nofollow noreferrer"">run_mlm.py</a> but I can't understan which format should I use for my text file. I tried to use a simple structure with one document per line and I get the following error:</p>
<pre><code>python run_mlm.py --model_name_or_path &quot;neuralmind/bert-base-portuguese-cased&quot; --train_file ../data/full_corpus.csv  --output models/  --do_train


Traceback (most recent call last):
  File &quot;run_mlm.py&quot;, line 449, in &lt;module&gt;
    main()
  File &quot;run_mlm.py&quot;, line 384, in main
    load_from_cache_file=not data_args.overwrite_cache,
  File &quot;/mnt/sdb/data-mwon/paperChega/env2/lib/python3.6/site-packages/datasets/dataset_dict.py&quot;, line 303, in map
    for k, dataset in self.items()
  File &quot;/mnt/sdb/data-mwon/paperChega/env2/lib/python3.6/site-packages/datasets/dataset_dict.py&quot;, line 303, in &lt;dictcomp&gt;
    for k, dataset in self.items()
  File &quot;/mnt/sdb/data-mwon/paperChega/env2/lib/python3.6/site-packages/datasets/arrow_dataset.py&quot;, line 1260, in map
    update_data=update_data,
  File &quot;/mnt/sdb/data-mwon/paperChega/env2/lib/python3.6/site-packages/datasets/arrow_dataset.py&quot;, line 157, in wrapper
    out: Union[&quot;Dataset&quot;, &quot;DatasetDict&quot;] = func(self, *args, **kwargs)
  File &quot;/mnt/sdb/data-mwon/paperChega/env2/lib/python3.6/site-packages/datasets/fingerprint.py&quot;, line 163, in wrapper
    out = func(self, *args, **kwargs)
  File &quot;/mnt/sdb/data-mwon/paperChega/env2/lib/python3.6/site-packages/datasets/arrow_dataset.py&quot;, line 1529, in _map_single
    writer.write_batch(batch)
  File &quot;/mnt/sdb/data-mwon/paperChega/env2/lib/python3.6/site-packages/datasets/arrow_writer.py&quot;, line 278, in write_batch
    pa_table = pa.Table.from_pydict(typed_sequence_examples)
  File &quot;pyarrow/table.pxi&quot;, line 1474, in pyarrow.lib.Table.from_pydict
  File &quot;pyarrow/array.pxi&quot;, line 322, in pyarrow.lib.asarray
  File &quot;pyarrow/array.pxi&quot;, line 222, in pyarrow.lib.array
  File &quot;pyarrow/array.pxi&quot;, line 110, in pyarrow.lib._handle_arrow_array_protocol
  File &quot;/mnt/sdb/data-mwon/paperChega/env2/lib/python3.6/site-packages/datasets/arrow_writer.py&quot;, line 100, in __arrow_array__
    if trying_type and out[0].as_py() != self.data[0]:
  File &quot;pyarrow/array.pxi&quot;, line 1058, in pyarrow.lib.Array.__getitem__
  File &quot;pyarrow/array.pxi&quot;, line 540, in pyarrow.lib._normalize_index
IndexError: index out of bounds
</code></pre>
","710734","","","","","2021-01-18 17:46:22","How to use run_mlm with unlabeled text","<huggingface-transformers>","0","0","0","","","CC BY-SA 4.0"
"63902455","1","","","2020-09-15 13:03:35","","0","192","<p>I'm trying to load a pre trained model using transformers lib (by hugging-face):</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')
</code></pre>
<p>Using local machine, it starts to download the model.
But with docker I get the following:</p>
<pre><code>OSError: Model name 'gpt2-medium' was not found in tokenizers model 
name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We 
assumed 'gpt2-medium' was a path, a model identifier, or url to a 
directory containing vocabulary files named ['vocab.json', 
'merges.txt'] but couldn't find such vocabulary files at this path or url.
</code></pre>
<p>Any idea why it happens?</p>
","3209762","","","","","2020-10-14 14:40:15","Transformers Lib - Load Pre-Trained Model Using Docker","<docker><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"64105840","1","","","2020-09-28 16:12:20","","1","367","<p>I am working on a binary text classification problem. How do I apply SMOTE or <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler"" rel=""nofollow noreferrer""><code>WeightedRandomSampler</code></a> for the imbalance in my dataset? My code currently looks like this:</p>
<pre><code>class GDataset(Dataset):

  def __init__(self, passage, targets, tokenizer, max_len):
    self.passage = passage
    self.targets = targets
    self.tokenizer = tokenizer
    self.max_len = max_len
  
  def __len__(self):
    return len(self.passage)
  
  def __getitem__(self, item):
    passage = str(self.passage[item])
    target = self.targets[item]
    
    if (target == 1) and self.transform: # minority class
            x = self.transform(x)

    encoding = self.tokenizer.encode_plus(
      passage,
      add_special_tokens=True,
      max_length=self.max_len,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt',
    )
    return
      'passage_text': passage,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'targets': torch.tensor(target, dtype=torch.long
</code></pre>
<p>How can I use other balancing techniques?</p>
","12434737","","6879826","","2021-02-13 11:40:46","2021-02-13 11:40:46","How to handle imbalanced classes in transformers pytorch binary classification","<python><pytorch><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"64233392","1","","","2020-10-06 20:30:30","","0","26","<p>I want to do an experiment with bert zero-layer vectors (input vectors), which I understand are of dimension 128.</p>
<p>I can not find <strong>where I can get a file with the tokens and their vectors.</strong>
Is there such a thing?
Is there a file in the Glove/word2vec format but of BERT's input vectors?</p>
<p>I'd appreciate any help with this!!
Thanks,</p>
","14385885","","","","","2020-10-06 20:30:30","BERT zero layer fixed word embeddings","<pytorch><word-embedding><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","2","","2020-10-07 12:03:13","","CC BY-SA 4.0"
"64446355","1","64465408","","2020-10-20 13:37:34","","1","809","<p>I just wonder if the tokenizer is somehow affected or changed if fine tune a BERT model and save it. Do I need to save the tokenizer locally too to reload it when using the saved BERT model later?</p>
<p>I just do:</p>
<pre><code>bert_model.save_pretrained('./Fine_tune_BERT/')
</code></pre>
<p>then later</p>
<pre><code>bert_model = TFBertModel.from_pretrained('./Fine_tune_BERT/')
</code></pre>
<p>But do i need to saver the tokenizer too? Or could I just use it in the normal way like:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
</code></pre>
","9451356","","","","","2020-10-21 14:09:19","huggingface - save fine tuned model locally - and tokenizer too?","<bert-language-model><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"65779837","1","","","2021-01-18 18:05:48","","0","87","<p>I've bumped into an issue when trying to run a python script and for simplicity let's call it <code>my_tokenizer.py</code> and its content is just importing <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">hugging face's transformers</a>. Unfortunately, trying to run it from the working directory leads to ImportError and it seems it is caused due to the name of the file that is in the working directory and has the same name as the file that <code>transformer</code> package uses somewhere in its internals.</p>
<p>Having 2 files in the working directory:</p>
<ul>
<li><code>/project/my_tokenizer.py</code> (contains only line with import &quot;import transformers&quot;)</li>
<li><code>/project/tokenizers.py</code> (empty file)</li>
</ul>
<p>and running <code>python my_tokenizer.py</code> leads to following ImportError:</p>
<pre><code>Traceback (most recent call last):
  File &quot;project/my_tokenizer.py&quot;, line 1, in &lt;module&gt;
    import transformers
  File &quot;/Users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/__init__.py&quot;, line 54, in &lt;module&gt;
    from .data import (
  File &quot;/Users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/data/__init__.py&quot;, line 6, in &lt;module&gt;
    from .processors import (
  File &quot;/Users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/data/processors/__init__.py&quot;, line 5, in &lt;module&gt;
    from .glue import glue_convert_examples_to_features, glue_output_modes, glue_processors, glue_tasks_num_labels
  File &quot;/Users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/data/processors/glue.py&quot;, line 24, in &lt;module&gt;
    from ...tokenization_utils import PreTrainedTokenizer
  File &quot;/Users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/tokenization_utils.py&quot;, line 26, in &lt;module&gt;
    from .tokenization_utils_base import (
  File &quot;/Users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/tokenization_utils_base.py&quot;, line 31, in &lt;module&gt;
    from tokenizers import AddedToken
ImportError: cannot import name 'AddedToken' from 'tokenizers' (/Users/radoslawslowinski/project/tokenizers.py)

</code></pre>
<p>Although I could just rename my file from <code>project/tokenizers.py</code> to something else, I'd like to know why it occurs.</p>
","7376885","","7376885","","2021-01-19 08:21:15","2021-02-26 03:52:07","ImportError caused by file with the same name in working dir and file from imported package","<python><python-3.x><huggingface-transformers><huggingface-tokenizers>","2","0","","","","CC BY-SA 4.0"
"67190212","1","67195818","","2021-04-21 06:15:39","","1","124","<p>I'm training BertForSequenceClassification for a classification task. My dataset consists of 'contains adverse effect' (1) and 'does not contain adverse effect' (0). The dataset contains all of the 1s and then the 0s after (the data isn't shuffled). For training I've shuffled my data and get the logits. From what I've understood, the logits are the probability distributions before softmax. An example logit is [-4.673831, 4.7095485]. Does the first value correspond to the label 1 (contains AE) because it appears first in the dataset, or label 0. Any help would be appreciated thanks.</p>
","14653046","","6664872","","2021-04-21 12:20:10","2021-04-21 13:56:21","How does the BERT model select the label ordering?","<pytorch><bert-language-model><huggingface-transformers><logits>","1","0","","","","CC BY-SA 4.0"
"67283668","1","","","2021-04-27 13:06:54","","2","498","<p>I am trying to train a classification model with a custom dataset using Huggingface Transformers, but I keep getting errors. Last error seems solvable but I somehow I do not understand how.
What am I doing wrong?</p>
<p>I encode data with</p>
<pre><code>model_name = &quot;dbmdz/bert-base-italian-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case = True)

def encode_data(texts):
    return tokenizer.batch_encode_plus(
                texts, 
                add_special_tokens=True, 
                return_attention_mask=True, 
                padding = True,
                truncation=True,
                max_length=200,
                return_tensors='pt'
            )
</code></pre>
<p>Then I create my datasets with</p>
<pre><code>import torch

class my_Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = torch.tensor(labels)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx]
        print(item)
        return item

    def __len__(self):
        return len(self.labels)
</code></pre>
<p>So I have</p>
<pre><code>encoded_data_train = encode_data(df_train['text'].tolist())
encoded_data_val = encode_data(df_val['text'].tolist())
encoded_data_test = encode_data(df_test['text'].tolist())
dataset_train = my_Dataset(encoded_data_train, df_train['labels'].tolist())
dataset_val = my_Dataset(encoded_data_val, df_val['labels'].tolist())
dataset_test = my_Dataset(encoded_data_test, df_test['labels'].tolist())
</code></pre>
<p>Then I initiate my Trainer with</p>
<pre><code>from transformers import AutoConfig, TrainingArguments, DataCollatorWithPadding, Trainer

training_args = TrainingArguments(
    output_dir='/trial',
    learning_rate=1e-6,
    do_train=True,
    do_eval=True,
    evaluation_strategy='epoch',
    num_train_epochs=10,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=0,
    weight_decay=0.2,
    logging_dir=&quot;./logs&quot;,
)

num_labels = len(label_dict)
model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels = num_labels)

trainer = Trainer(
  model=model,
  args=training_args,
  data_collator=DataCollatorWithPadding(tokenizer),
  tokenizer= tokenizer,
  train_dataset=dataset_train,
  eval_dataset=dataset_val,
)
</code></pre>
<p>and finally I train</p>
<pre><code>trainer.train()
</code></pre>
<p>Here is the error I get</p>
<pre><code>AttributeErrorTraceback (most recent call last)
&lt;ipython-input-22-5d018b4b061d&gt; in &lt;module&gt;
----&gt; 1 trainer.train()

/opt/conda/lib/python3.8/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
   1032             self.control = self.callback_handler.on_epoch_begin(self.args, self.state, self.control)
   1033 
-&gt; 1034             for step, inputs in enumerate(epoch_iterator):
   1035 
   1036                 # Skip past any already trained steps if resuming training

/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py in __next__(self)
    433         if self._sampler_iter is None:
    434             self._reset()
--&gt; 435         data = self._next_data()
    436         self._num_yielded += 1
    437         if self._dataset_kind == _DatasetKind.Iterable and \

/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py in _next_data(self)
    473     def _next_data(self):
    474         index = self._next_index()  # may raise StopIteration
--&gt; 475         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    476         if self._pin_memory:
    477             data = _utils.pin_memory.pin_memory(data)

/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     45         else:
     46             data = self.dataset[possibly_batched_index]
---&gt; 47         return self.collate_fn(data)

/opt/conda/lib/python3.8/site-packages/transformers/data/data_collator.py in __call__(self, features)
    116 
    117     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -&gt; Dict[str, torch.Tensor]:
--&gt; 118         batch = self.tokenizer.pad(
    119             features,
    120             padding=self.padding,

/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py in pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)
   2558         if self.model_input_names[0] not in encoded_inputs:
   2559             raise ValueError(
-&gt; 2560                 &quot;You should supply an encoding or a list of encodings to this method&quot;
   2561                 f&quot;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&quot;
   2562             )

AttributeError: 'list' object has no attribute 'keys'
</code></pre>
<p>What I am doing wrong?
I also tried using</p>
<pre><code>import torch
from torch.utils.data import TensorDataset

dataset_train = TensorDataset(encoded_data_train['input_ids'], encoded_data_train['attention_mask'], torch.tensor(df_train['labels'].tolist()))
dataset_test = TensorDataset(encoded_data_test['input_ids'], encoded_data_test['attention_mask'], torch.tensor(df_test['labels'].tolist()))
dataset_val = TensorDataset(encoded_data_val['input_ids'], encoded_data_val['attention_mask'], torch.tensor(df_val['labels'].tolist()))
</code></pre>
<p>getting the same error. I am using torch == 1.7.1 and transformers == 4.4.2</p>
<p>EDIT FOLLOWING FIRST COMMENT.
Here is an example of 5-dimensional <code>encoded_data_train</code></p>
<pre><code>{'input_ids': tensor([[  102,   927,  9534, 30936,  2729, 29505,   123, 11805,  7427, 10587,
          9703,   927,  9534, 30936,  2719, 10118,  2321,   784,   366,   113,
          3627,  7763,  9433,   223,   148, 30937,  4051,  3400,  4011, 20005,
          6079,   784,   366,  7809, 11967,   192,  3497,   784,   366,  7809,
         11967,   192,  3497,   784,   366,  7809, 11967,   192,  3497,   784,
           366,  7809, 11967,   192,  3497,   714,   927,  9534, 30936,  2729,
         29505,   123, 11805,  7427,   260,   480,  1556,   152,  7113, 20734,
           151,   143,   784,   366,   113,  3627,  7763, 19638,   159,  1233,
          1674,  5442,   119,  9433,   223,   148, 30937,   135,   642,   829,
          2250,   223,   743,   151,   143, 14572, 13799,  1767, 28915, 12057,
         12342,   784,   366,   113,  9703,   927,  9534, 30936,  9480, 10125,
          8418,  3726,  8379,  2955,   119,  1006, 30946,  8897,   123,  6423,
           115,  1601,   544, 30938,  3013,   160, 30941,   137,   124, 14118,
         30936,   193,  2701, 19214,  1457,  2701,  1864,   409, 19727, 13305,
          6423,   115, 10389, 13908,   127,  4092, 14079,  1601,  2009, 24286,
         23419,   103],
        [  102, 10587,  2130,   182,  8022,  2719, 10118,   132, 30976, 30943,
         17961,  5123,  3292,  3627, 11532,  2719, 10118,   132, 30976, 30943,
         17961,  5123,  3292,  3627, 11532,  2719, 10118,   201, 17961,  5123,
          3292,  3627, 11532,  6354,   480,  1556, 28951, 17586,   113, 12699,
           135,   480,  1556,  7347,   677,   135,  3110,   103,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0],
        [  102,  2719, 10118,  6729,  6530, 10754, 11752, 10272, 11752,   119,
          4200,   209, 30944, 19919,  2201,  5754,   642,   838, 15657,  6156,
         30941,   148, 30937,  2201,  7305,   642,  6331,  3348, 30937,   170,
           148, 30937,  2463,   103,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0],
        [  102,   780, 30938, 18834,  2336,  2719, 10118,  8823,   784,   366,
           113,   135,  1543,  2080,  1233, 20734,   316,  1767,  1542,  2771,
           152, 25899,   119,  8823,   119,  4472,   784,   366,   113,   137,
          1031,   510,  7763,   123, 21478,  3200,   111,   985,   119,  1670,
          4999,   290, 30941,   119,  6951, 12042,   106,  1542,   135,   245,
         30942, 26609,   199,   983,   119,   261, 28040,  8142,   148, 30937,
           150,   143,   917,  1621,  7161,   111, 26609,  8217,  3723, 12510,
           290, 30941,   119,  8886, 30934,  9798,   106,   204, 30942,  5807,
           155,  1176,   213, 12057,   189,   387,  4953,   214,  2643,  4429,
           123, 11224,  3096,   193,   143,  8823,   387,  2353,  2009,   193,
           982,   176, 18789,   299,  8292,   553,  9798,  8886, 30934, 20853,
           490,  4802, 19222,   642,  3829,  1455, 26321,   167,   148, 30937,
         11498,   123,   103,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0],
        [  102, 10587,   491,  5462,  7664, 22790,  2719, 10118,  8498,   408,
         24484,   112,   491,  5462,  7664, 22790,  3671,   135,   341,  1011,
           299, 18239,   113,   143,   575,  8498,   265,   669,   113,  3850,
         16465,   480,   283, 28951,   810, 21223,   103,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])}
</code></pre>
<p>and accordingly the result of <code>dataset_train.__getitem__(0)</code></p>
<pre><code>{'input_ids': tensor([  102,   927,  9534, 30936,  2729, 29505,   123, 11805,  7427, 10587,
         9703,   927,  9534, 30936,  2719, 10118,  2321,   784,   366,   113,
         3627,  7763,  9433,   223,   148, 30937,  4051,  3400,  4011, 20005,
         6079,   784,   366,  7809, 11967,   192,  3497,   784,   366,  7809,
        11967,   192,  3497,   784,   366,  7809, 11967,   192,  3497,   784,
          366,  7809, 11967,   192,  3497,   714,   927,  9534, 30936,  2729,
        29505,   123, 11805,  7427,   260,   480,  1556,   152,  7113, 20734,
          151,   143,   784,   366,   113,  3627,  7763, 19638,   159,  1233,
         1674,  5442,   119,  9433,   223,   148, 30937,   135,   642,   829,
         2250,   223,   743,   151,   143, 14572, 13799,  1767, 28915, 12057,
        12342,   784,   366,   113,  9703,   927,  9534, 30936,  9480, 10125,
         8418,  3726,  8379,  2955,   119,  1006, 30946,  8897,   123,  6423,
          115,  1601,   544, 30938,  3013,   160, 30941,   137,   124, 14118,
        30936,   193,  2701, 19214,  1457,  2701,  1864,   409, 19727, 13305,
         6423,   115, 10389, 13908,   127,  4092, 14079,  1601,  2009, 24286,
        23419,   103]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(5)}
</code></pre>
","13083446","","13083446","","2021-04-27 15:14:58","2021-04-27 15:14:58","Unable to use custom dataset: AttributeError: 'list' object has no attribute 'keys'","<python-3.x><bert-language-model><huggingface-transformers>","0","3","1","","","CC BY-SA 4.0"
"61196578","1","","","2020-04-13 20:55:50","","1","523","<p>I am trying to train a BERT language model from scratch using <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">Huggingface API</a>. For that I need to build a tokenizer that tokenize the text data based on white spaces only, nothing else. I understand that there are multiple tonkenizers available in Huggingface (such as <code>BPE</code>, <code>WordPiece</code>) that produce good results for language models but for my use case I want to tokenize text input based on whitespace only and generate vocabs that should not have any kind of special characters viz ""##"" in front of words. </p>

<p>For example: the input <code>Hello, y'all! How are you?</code> should be tonkenized as:</p>

<p><code>Hello,</code>, <code>y'all!</code>, <code>How</code>, <code>are</code>, <code>you?</code></p>

<p>I checked the documentation <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">[1]</a> and <a href=""https://github.com/huggingface/tokenizers"" rel=""nofollow noreferrer"">[2]</a> but did not find a way to achieve this. </p>
","12575714","","","","","2020-04-13 20:55:50","Whitespace tokenizer for training BERT language model from scratch with Huggingface","<pytorch><transformer><huggingface-transformers>","0","5","0","","","CC BY-SA 4.0"
"60996241","1","","","2020-04-02 16:18:46","","1","105","<p>Let's consider two sentences:</p>

<pre><code>""why isn't Alex's text tokenizing? The house on the left is the Smiths' house""
</code></pre>

<p>Now let's tokenize and decode:</p>

<pre><code>from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(""why isn't Alex's text tokenizing? The house on the left is the Smiths' house"")))
</code></pre>

<p>We get:</p>

<pre><code>""why isn't alex's text tokenizing? the house on the left is the smiths'house""
</code></pre>

<p><strong>My question is how dealing with missing space in some possessives like <em>smiths'house</em>?</strong></p>

<p>For me, it seems that the process of tokenization in Transformers is done not right. Let's consider output of</p>

<pre><code>tokenizer.tokenize(""why isn't Alex's text tokenizing? The house on the left is the Smiths' house"")
</code></pre>

<p>we get:</p>

<pre><code>['why', 'isn', ""'"", 't', 'alex', ""'"", 's', 'text', 'token', '##izing', '?', 'the', 'house', 'on', 'the', 'left', 'is', 'the', 'smith', '##s', ""'"", 'house']
</code></pre>

<p>So in this step, we already have lost important information about the last apostrophe. It would be much better if tokenization was done in the another way:</p>

<pre><code>['why', 'isn', ""##'"", '##t', 'alex', ""##'"", '##s', 'text', 'token', '##izing', '?', 'the', 'house', 'on', 'the', 'left', 'is', 'the', 'smith', '##s', ""##'"", 'house']
</code></pre>

<p>In this way, tokenization keeps all information about apostrophes, and we will not have problems with possessives.</p>
","2975438","","","","","2020-04-02 16:18:46","Transformers and BERT: dealing with possessives and apostrophes when encode","<python><nlp><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"61452697","1","61452773","","2020-04-27 06:10:12","","0","206","<p>I want to perform author classification on the Reuters 50 50 dataset, where the max token length is 1600+ tokens and there are 50 classes/authors in total.</p>

<p>With <code>max_length=1700</code> and <code>batch_size=1</code>, I'm getting <code>RuntimeError: CUDA out of memory</code>. This error can be prevented by setting <code>max_length=512</code>, but this has the unwanted effect of truncating the texts.</p>

<p><strong>Tokenizing and encoding:</strong></p>

<pre><code>from keras.preprocessing.sequence import pad_sequences
MAX_LEN = 1700
def get_encodings(texts):
    token_ids = []
    attention_masks = []
    for text in texts:
        token_id = tokenizer.encode(text, add_special_tokens=True, max_length=MAX_LEN)
        token_ids.append(token_id)
    return token_ids

def pad_encodings(encodings):
    return pad_sequences(encodings, maxlen=MAX_LEN, dtype=""long"", 
                          value=0, truncating=""post"", padding=""post"")

def get_attention_masks(padded_encodings):
    attention_masks = []
    for encoding in padded_encodings:
        attention_mask = [int(token_id &gt; 0) for token_id in encoding]
        attention_masks.append(attention_mask)
    return attention_masks


train_encodings = get_encodings(train_df.text.values)
train_encodings = pad_encodings(train_encodings)
train_attention_masks = get_attention_masks(train_encodings)

test_encodings = get_encodings(test_df.text.values)
test_encodings = pad_encodings(test_encodings)
test_attention_masks = get_attention_masks(test_encodings)
</code></pre>

<p><strong>Packing into Dataset and Dataloader:</strong></p>

<pre><code>X_train = torch.tensor(train_encodings)
y_train = torch.tensor(train_df.author_id.values)
train_masks = torch.tensor(train_attention_masks)

X_test = torch.tensor(test_encodings)
y_test = torch.tensor(test_df.author_id.values)
test_masks = torch.tensor(test_attention_masks)

batch_size = 1

# Create the DataLoader for our training set.
train_data = TensorDataset(X_train, train_masks, y_train)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

validation_data = TensorDataset(X_test, test_masks, y_test)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)
</code></pre>

<p><strong>Model setup:</strong></p>

<pre><code>if torch.cuda.is_available():    
    device = torch.device(""cuda"")
else:
    device = torch.device(""cpu"")

config = BertConfig.from_pretrained(
    'bert-base-uncased',
    num_labels = 50,
    output_attentions = False,
    output_hidden_states = False,
    max_position_embeddings=MAX_LEN
)

model = BertForSequenceClassification(config)

model.to(device)


optimizer = AdamW(model.parameters(),
                  lr = 2e-5, 
                  eps = 1e-8 
                )
</code></pre>

<p><strong>Training:</strong></p>

<pre><code>for epoch_i in range(0, epochs):

    model.train()

    for step, batch in enumerate(train_dataloader):

        b_texts = batch[0].to(device)
        b_attention_masks = batch[1].to(device)
        b_authors = batch[2].to(device)

        model.zero_grad()        

        outputs = model(b_texts, 
                        token_type_ids=None, 
                        attention_mask=b_attention_masks, 
                        labels=b_authors) &lt;------- ERROR HERE
</code></pre>

<p><strong>Error:</strong></p>

<pre><code>RuntimeError: CUDA out of memory. Tried to allocate 6.00 GiB (GPU 0; 7.93 GiB total capacity; 1.96 GiB already allocated; 5.43 GiB free; 536.50 KiB cached)
</code></pre>
","6378530","","","","","2020-05-04 13:55:42","How to use BertForSequenceClassification for token max_length set at 1700?","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61579248","1","","","2020-05-03 18:01:22","","1","666","<p>I can't figure out how to use transformers-cli on Windows. I got it working on Google Colab, and am using it in the meantime.</p>

<p>[EDIT]</p>

<p>Here's the process that I'm going through, what I expect, and what is happening:</p>

<p><strong>I'm on a Windows System (brackets are the exact commands I'm typing into CMD)</strong></p>

<ol>
<li>I install transformers==2.8.0 (pip install transformers==2.8.0)</li>
<li>I try to run transformers-cli as explained on Huggingface's website (transformers-cli) <a href=""https://huggingface.co/transformers/model_sharing.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_sharing.html</a></li>
</ol>

<p><strong>I get:</strong> </p>

<pre><code>'transformers-cli' is not recognized as an internal or external command,
operable program or batch file.
</code></pre>

<p>I don't know if I have to add some directory to my PATH or perhaps the CLI isn't available on Windows?</p>

<p><strong>I repeat the exact same process on Google Colab, and it works as expected. I get:</strong></p>

<pre><code>usage: transformers-cli &lt;command&gt; [&lt;args&gt;]

positional arguments:
  {convert,download,env,run,serve,login,whoami,logout,s3,upload}
                        transformers-cli command helpers
    convert             CLI tool to run convert model from original author
                        checkpoints to Transformers PyTorch checkpoints.
    run                 Run a pipeline through the CLI
    serve               CLI tool to run inference requests through REST and
                        GraphQL endpoints.
    login               Log in using the same credentials as on huggingface.co
    whoami              Find out which huggingface.co account you are logged
                        in as.
    logout              Log out
    s3                  {ls, rm} Commands to interact with the files you
                        upload on S3.
    upload              Upload a model to S3.

optional arguments:
  -h, --help            show this help message and exit
</code></pre>
","9310350","","9310350","","2020-05-04 17:54:30","2020-05-04 21:51:38","Using transformers-cli on Windows?","<huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"61019485","1","61019599","","2020-04-03 19:54:41","","0","860","<p>I'm trying to develop a binary classifier with Huggingface's BertModel and Pytorch. 
The classifier module is something like this:  </p>

<pre><code>class SSTClassifierModel(nn.Module):

  def __init__(self, num_classes = 2, hidden_size = 768):
    super(SSTClassifierModel, self).__init__()
    self.number_of_classes = num_classes
    self.dropout = nn.Dropout(0.01)
    self.hidden_size = hidden_size
    self.bert = BertModel.from_pretrained('bert-base-uncased')
    self.classifier = nn.Linear(hidden_size, num_classes)

  def forward(self, input_ids, att_masks,token_type_ids,  labels):
    _, embedding = self.bert(input_ids, token_type_ids, att_masks)
    output = self.classifier(self.dropout(embedding))
    return output
</code></pre>

<p>The way I train the model is as follows:  </p>

<pre><code>loss_function = BCELoss()
model.train()
for epoch in range(NO_OF_EPOCHS):
  for step, batch in enumerate(train_dataloader):
        input_ids = batch[0].to(device)
        input_mask = batch[1].to(device)
        token_type_ids = batch[2].to(device)
        labels = batch[3].to(device)
        # assuming batch size = 3, labels is something like:
        # tensor([[0],[1],[1]])
        model.zero_grad()        
        model_output = model(input_ids,  
                             input_mask, 
                             token_type_ids,
                             labels)
        # model output is something like: (with batch size = 3) 
        # tensor([[ 0.3566, -0.0333],
                 #[ 0.1154,  0.2842],
                 #[-0.0016,  0.3767]], grad_fn=&lt;AddmmBackward&gt;)

        loss = loss_function(model_output.view(-1,2) , labels.view(-1))
</code></pre>

<p>I'm doing the <code>.view()</code>s because of the Huggingface's source code for <code>BertForSequenceClassification</code> <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertForSequenceClassification"" rel=""nofollow noreferrer"">here</a> which uses the exact same way to compute the loss. But I get this error:</p>

<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in binary_cross_entropy(input, target, weight, size_average, reduce, reduction)
   2068     if input.numel() != target.numel():
   2069         raise ValueError(""Target and input must have the same number of elements. target nelement ({}) ""
-&gt; 2070                          ""!= input nelement ({})"".format(target.numel(), input.numel()))
   2071 
   2072     if weight is not None:

ValueError: Target and input must have the same number of elements. target nelement (3) != input nelement (6)
</code></pre>

<p>Is there something wrong with my labels? or my model's output? I'm really stuck here. The documentation for Pytorch's BCELoss says:  </p>

<p><em>Input: (N,âˆ—) where âˆ— means, any number of additional dimensions<br>
Target: (N,âˆ—), same shape as the input</em></p>

<p>How should I make my labels the same shape as the model output? I feel like there's something huge that I'm missing but I can't find it. </p>
","5384986","","","","","2020-04-03 20:14:28","Pytorch cross entropy input dimensions","<python><pytorch><python-3.7><cross-entropy><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61825698","1","61828910","","2020-05-15 18:07:16","","2","138","<p>I have a text classification task at hand and I want to use roberta pre-trained model from transformers library.</p>

<p>Here's the docs : <a href=""https://huggingface.co/transformers/model_doc/roberta.html#transformers.TFRobertaForSequenceClassification"" rel=""nofollow noreferrer"">TFRobertaForSequenceClassification</a></p>

<p>As per the documentation to train we have to use,</p>

<pre><code>from transformers import RobertaTokenizer, TFRobertaForSequenceClassification

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')

model.compile('adam', loss='sparse_categorical_crossentropy')
model.fit(x, y)
</code></pre>

<p>So where should I specify number of target labels for sequence classification?</p>
","11816060","","","","","2020-05-15 21:50:07","How to specify number of target classes for TFRobertaSequenceClassification?","<python><machine-learning><deep-learning><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61016422","1","","","2020-04-03 16:33:16","","0","1388","<p>I am not sure if this is the best place to submit that kind of question, perhaps CrossValdation would be a better place. </p>

<p>I am working on a text multiclass classification problem.
I built a model based on BERT concept implemented in PyTorch (huggingface transformer library). The model performs pretty well, except when the input sentence has an OCR error or equivalently it is misspelled. </p>

<p>For instance, if the input is ""NALIBU DRINK"" the Bert tokenizer generates ['na', '##lib', '##u', 'drink'] and model's prediction is completely wrong. On the other hand, if I correct the first character, so my input is ""MALIBU DRINK"", the Bert tokenizer generates two tokens ['malibu', 'drink'] and the model makes a correct prediction with very high confidence. </p>

<p>Is there any way to enhance Bert tokenizer to be able to work with misspelled words? </p>
","1877600","","1877600","","2020-04-03 22:39:23","2020-04-06 23:40:00","Text classification using BERT - how to handle misspelled words","<pytorch><text-classification><huggingface-transformers><bert-language-model><misspelling>","1","0","","","","CC BY-SA 4.0"
"61774933","1","61800495","","2020-05-13 12:40:10","","1","314","<p>I am new to the Transformers concept and I am going through some tutorials and writing my own code to understand the Squad 2.0 dataset Question Answering using the transformer models. In the hugging face website, I came across 2 different links</p>

<ul>
<li><a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">https://huggingface.co/models</a></li>
<li><a href=""https://huggingface.co/transformers/pretrained_models.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/pretrained_models.html</a></li>
</ul>

<p>I want to know the difference between these 2 websites. Does one link have just a pre-trained model and the other have a pre-trained and fine-tuned model? </p>

<p>Now if I want to use, let's say an Albert Model For Question Answering and train with my Squad 2.0 training dataset on that and evaluate the model, to which of the link should I further?</p>
","3152686","","","","","2020-05-14 14:54:06","Understanding the Hugging face transformers","<pre-trained-model><huggingface-transformers><bert-language-model><question-answering><squad>","1","0","","","","CC BY-SA 4.0"
"61776977","1","63379055","","2020-05-13 14:15:49","","4","1022","<p>I'm building a multiclass text classification model using Keras and Bert (HuggingFace), but I have a very imbalanced dataset. I've used SMOTE from Sklearn in order to generate additional samples for the underbalanced classes (I have 45 in total), which works fine when I use the input ids from the Bert Tokenizer.</p>

<p>However, I would like to be able to also use smote for the input masks ids, in order to allow the model to determine where the padded values are.</p>

<p>My question is how can I use smote for both input ids and mask ids? I've done the following so far, and the model doesn't complain, but I'm not sure if the resampled masks match the resampled input ids row for row.
 Smote requires two inputs, inputs and labels, so I've duplicated the process with the same random state, and just returned the required elements:</p>

<pre><code>def smote(input_ids, input_masks, labels):

    smote = SMOTE(""not majority"", random_state=27)

    input_ids_resampled, labels_resampled = smote.fit_sample(input_ids, labels)
    input_masks_resampled, _ = smote.fit_sample(input_masks, labels)

    return input_ids_resampled, input_masks_resampled, labels_resampled
</code></pre>

<p>Is this acceptable? Is there a better way to do this?</p>
","7605543","","","","","2020-12-29 16:54:21","SMOTE with multiple bert inputs","<python><keras><scikit-learn><huggingface-transformers><smote>","2","0","1","","","CC BY-SA 4.0"
"61988776","1","61990477","","2020-05-24 16:14:58","","2","2738","<p>I'm fine-tuning GPT-2 model for language generation task using huggingface Transformers library-pytorch, and I need to calculate an evaluation score(perplexity) for the fine-tuned model. But I'm not sure how that can be done using loss. I would like to know how perplexity can be calculated for the model with sum_loss or mean loss or any other suggestions are also welcome. Any help is appriciated.</p>

<p><strong>Edit:</strong></p>

<pre><code>outputs = model(article_tens, labels=article_tens)

        loss, prediction_scores = outputs[:2]                        
        loss.backward()
        sum_loss = sum_loss + loss.detach().data
</code></pre>

<p>given above is how I calculate loss for each batch of data for the fine-tuning task.</p>

<pre><code>sum loss 1529.43408203125
loss 4.632936000823975
prediction_scores tensor([[[-11.2291,  -9.2614, -11.8575,  ..., -18.1927, -17.7286, -11.9215],
         [-67.2786, -63.5928, -70.7110,  ..., -75.3516, -73.8672, -67.6637],
         [-81.1397, -80.0295, -82.9357,  ..., -83.7913, -85.7201, -78.9877],
         ...,
         [-85.3213, -82.5135, -86.5459,  ..., -90.9160, -90.4393, -82.3141],
         [-44.2260, -43.1702, -49.2296,  ..., -58.9839, -55.2058, -42.3312],
         [-63.2842, -59.7334, -61.8444,  ..., -75.0798, -75.7507, -54.0160]]],
       device='cuda:0', grad_fn=&lt;UnsafeViewBackward&gt;)
</code></pre>

<p>The above given is when loss is printed for a batch only</p>
","10598769","","10598769","","2020-05-25 01:31:26","2020-05-25 01:31:26","How to calculate perplexity for a language model using Pytorch","<nlp><pytorch><huggingface-transformers>","1","0","3","","","CC BY-SA 4.0"
"64102036","1","","","2020-09-28 12:16:34","","1","337","<p>I follow the guide below to use FP16 in PyTorch.
<a href=""https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/"" rel=""nofollow noreferrer"">https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/</a></p>
<p>Basically, I'm using BART in HuggingFace for generation</p>
<ol>
<li>During the training phase, I'm able to get 2x speedup and less GPU memory consumption</li>
</ol>
<p>But.</p>
<ol>
<li>I found out there is no speedup when I call <code>model.generate</code> under <code>torch.cuda.amp.autocast()</code>.</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>with torch.cuda.amp.autocast():
   model.generate(...)
</code></pre>
<ol start=""2"">
<li>When I save the model by:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>model.save_pretrained(&quot;model_folder&quot;)
</code></pre>
<p>the size does not decrease to half. But I have to call <code>model.half()</code> before saving in order to make the model half size.</p>
<p>Thus, my questions:</p>
<ul>
<li>Is the issue in <code>1.</code> expected or there should be something I did wrong?</li>
<li>Is the operation I did in <code>2.</code> proper?</li>
</ul>
","2211979","","","","","2020-09-28 12:16:34","Does using FP16 help accelerate generation? (HuggingFace BART)","<pytorch><huggingface-transformers><seq2seq>","0","0","1","","","CC BY-SA 4.0"
"61232399","1","","","2020-04-15 15:22:01","","1","243","<p>I have trained a custom BPE tokenizer for RoBERTa using <a href=""https://github.com/huggingface/tokenizers"" rel=""nofollow noreferrer"">tokenizers</a>.</p>

<p>I trained custom model on masked LM task using skeleton provided at <a href=""https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py"" rel=""nofollow noreferrer"">run_language_modeling.py</a></p>

<p>Model reaches perplexity of <code>3.2832</code> on an held out eval set.</p>

<p>Here is where what is confusing me when decoding model's predictions:</p>

<p>Following works fine when using pre-trained model</p>

<pre><code>&gt;&gt; from transformers import pipeline
&gt;&gt; nlp = pipeline('fill-mask', model='roberta-base')
&gt;&gt; nlp(""has the &lt;mask&gt; ever had checkup"")
[{'sequence': '&lt;s&gt; has the cat ever had checkup&lt;/s&gt;',
  'score': 0.11192905157804489,
  'token': 4758},
 {'sequence': '&lt;s&gt; has the baby ever had checkup&lt;/s&gt;',
  'score': 0.08717110008001328,
  'token': 1928},
 {'sequence': '&lt;s&gt; has the dog ever had checkup&lt;/s&gt;',
  'score': 0.07775705307722092,
  'token': 2335},
 {'sequence': '&lt;s&gt; has the man ever had checkup&lt;/s&gt;',
  'score': 0.04057956114411354,
  'token': 313},
 {'sequence': '&lt;s&gt; has the woman ever had checkup&lt;/s&gt;',
  'score': 0.031859494745731354,
  'token': 693}]
</code></pre>

<p>However when using custom roberta trained model with customer BPE</p>

<pre><code>&gt;&gt; from transformers import RobertaForMaskedLM, RobertaTokenizer
&gt;&gt; model = RobertaForMaskedLM.from_pretrained(path_to_model)
&gt;&gt; tokenizer = RobertaTokenizer.from_pretrained(path_to_tokenizer)
&gt;&gt; nlp = pipeline('fill-mask', model=model, tokenizer=tokenizer)
&gt;&gt; nlp(""has the &lt;mask&gt; ever had checkup"")
[{'sequence': '&lt;s&gt; the  had never had checkup&lt;/s&gt;',
  'score': 0.08322840183973312,
  'token': 225},
 {'sequence': '&lt;s&gt; the - had never had checkup&lt;/s&gt;',
  'score': 0.07046554237604141,
  'token': 311},
 {'sequence': '&lt;s&gt; the o had never had checkup&lt;/s&gt;',
  'score': 0.020223652943968773,
  'token': 293},
 {'sequence': '&lt;s&gt; the _ had never had checkup&lt;/s&gt;',
  'score': 0.013033385388553143,
  'token': 1246},
 {'sequence': '&lt;s&gt; the r had never had checkup&lt;/s&gt;',
  'score': 0.011952929198741913,
  'token': 346}]
</code></pre>

<p>My question is: Even though my custom model predicted the correct replacement for <code>&lt;mask&gt;</code> token, it's only sub-word token. So how would one get the full word back just like pre-trained models ? <em>e.g.</em> <code>cat</code> is split into <code>ca</code> &amp; <code>t</code> so if you mask <code>cat</code> then my custom model predicts only <code>ca</code> not the remaining <code>t</code> part. </p>

<p>Following config.json was used to train:</p>

<pre><code>{
  ""architectures"": [
    ""RobertaForMaskedLM""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""bos_token_id"": null,
  ""do_sample"": false,
  ""eos_token_ids"": null,
  ""finetuning_task"": ""mlm"",
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""id2label"": {
    ""0"": ""LABEL_0"",
    ""1"": ""LABEL_1""
  },
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""is_decoder"": false,
  ""label2id"": {
    ""LABEL_0"": 0,
    ""LABEL_1"": 1
  },
  ""layer_norm_eps"": 1e-12,
  ""length_penalty"": 1.0,
  ""max_length"": 100,
  ""max_position_embeddings"": 514,
  ""model_type"": ""roberta"",
  ""num_attention_heads"": 12,
  ""num_beams"": 1,
  ""num_hidden_layers"": 6,
  ""num_labels"": 2,
  ""num_return_sequences"": 1,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""output_past"": true,
  ""pad_token_id"": null,
  ""pruned_heads"": {},
  ""repetition_penalty"": 1.0,
  ""temperature"": 1.0,
  ""top_k"": 50,
  ""top_p"": 1.0,
  ""torchscript"": false,
  ""type_vocab_size"": 2,
  ""use_bfloat16"": false,
  ""vocab_size"": 30522
}
</code></pre>

<p>And for training custom tokenizer:</p>

<blockquote>
  <p>some have mentioned to keep order of special tokens same.</p>
</blockquote>

<pre><code>from tokenizers import ByteLevelBPETokenizer

# Initialize a tokenizer
tokenizer = ByteLevelBPETokenizer()

# Customize training
tokenizer.train(files=[""/home/data/BPE/txt_data/72148_tokens.txt"",
                       ""/home/data/BPE/txt_data/70551_tokens.txt"",
                       ""/home/data/BPE/txt_data/70553_tokens.txt"",
                       ""/home/data/BPE/txt_data/78452_tokens.txt"",
                       ""/home/data/BPE/txt_data/74177_tokens.txt"",
                       ""/home/data/BPE/txt_data/71260_tokens.txt"",
                       ""/home/data/BPE/txt_data/71250_tokens.txt"",], vocab_size=30522, min_frequency=10, special_tokens=[
    ""&lt;s&gt;"",
    ""&lt;pad&gt;"",
    ""&lt;/s&gt;"",
    ""&lt;unk&gt;"",
    ""&lt;mask&gt;"",
])

# Save files to disk
tokenizer.save(""/home/data/BPE/"", ""30k_v3_roberta"")
</code></pre>
","2931946","","6664872","","2020-06-16 13:31:31","2020-06-16 13:31:31","Decoding predictions for masked language modeling task using custom BPE","<huggingface-transformers><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"61374361","1","61374602","","2020-04-22 20:16:35","","2","430","<p>I'm trying to do a transfer learning with BertForSequenceClassification <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification</a></p>

<p>This is my simple NN model for classification.</p>

<pre><code>from transformers import BertTokenizer, BertForSequenceClassification
class NN(nn.Module):
    def __init__(self):
        super(NN, self).__init__()
        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 17)
    def forward(self, x):
        return self.bert(x)

a = NN()
</code></pre>

<p>Once I print my model I get this:</p>

<pre><code>NN(
  (bert): BertForSequenceClassification(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Linear(in_features=768, out_features=17, bias=True)
  )
)
</code></pre>

<p>I want to make only the last Linear layer trainable but I can't access the layers of my model. I have tried iterating model as a list and features but both gives error that there is no such attribute.</p>

<pre><code>a.features
</code></pre>

<p>How can make all my layers frozen except the last linear layer?</p>
","13348699","","","","","2020-04-22 20:31:52","Making transformers BertForSequenceClassification initial layers non-trainable for pytorch training","<pytorch><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"61416197","1","","","2020-04-24 19:38:46","","5","1736","<p>I was curious if it is possible to use transfer learning in text generation, and re-train/pre-train it on a specific kind of text.   </p>

<p>For example, having a pre-trained BERT model and a small corpus of medical (or any ""type"") text, make a language model that is able to generate medical text. The assumption is that you do not have a huge amount of ""medical texts"" and that is why you have to use transfer learning.</p>

<p>Putting it as a pipeline, I would describe this as:  </p>

<ol>
<li>Using a pre-trained BERT tokenizer.  </li>
<li>Obtaining new tokens from my new text and adding them to the existing pre-trained language model (i.e., vanilla BERT).  </li>
<li>Re-training the pre-trained BERT model on the custom corpus with the combined tokenizer.  </li>
<li>Generating text that resembles the text within the small custom corpus.</li>
</ol>

<p>Does this sound familiar? Is it possible with hugging-face?</p>
","2227026","","5112804","","2020-04-26 18:27:02","2020-10-27 13:10:17","Pretraining a language model on a small custom corpus","<deep-learning><transfer-learning><huggingface-transformers><language-model><bert-language-model>","1","2","1","","","CC BY-SA 4.0"
"61880247","1","","","2020-05-18 22:44:01","","1","150","<p>Does anyone know if HuggingFace's T5 model (small) comes with mono-language vocabulary? The T5 paper by Google indicates that their vocabulary is trained on English and 3 other languages. Is there a version of this vocabulary that contains English only vocabulary? </p>
","13570797","","","","","2020-05-19 07:49:32","Does Huggingface's T5 Model Vocabulary include English-only version?","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65906965","1","","","2021-01-26 18:16:59","","2","787","<p>I am trying to encode documents sentence-wise with a huggingface transformer module. I'm using the very small <code>google/bert_uncased_L-2_H-128_A-2</code> pretrained model with the following code:</p>
<pre><code>def pre_encode_wikipedia(model, tokenizer, device, save_path):
  
  document_data_list = []

  for iteration, document in enumerate(wikipedia_small['text']):
    torch.cuda.empty_cache()

    sentence_embeds_per_doc = [torch.randn(128)]
    attention_mask_per_doc = [1]
    special_tokens_per_doc = [1]

    doc_split = nltk.sent_tokenize(document)
    doc_tokenized = tokenizer.batch_encode_plus(doc_split, padding='longest', truncation=True, max_length=512, return_tensors='pt')

    for key, value in doc_tokenized.items():
      doc_tokenized[key] = doc_tokenized[key].to(device)

    with torch.no_grad():  
      doc_encoded = model(**doc_tokenized)

    for sentence in doc_encoded['last_hidden_state']:
      sentence[0].to('cpu')
      sentence_embeds_per_doc.append(sentence[0])
      attention_mask_per_doc.append(1)
      special_tokens_per_doc.append(0)

    sentence_embeds = torch.stack(sentence_embeds_per_doc)
    attention_mask = torch.FloatTensor(attention_mask_per_doc)
    special_tokens_mask = torch.FloatTensor(special_tokens_per_doc)

    document_data = torch.utils.data.TensorDataset(*[sentence_embeds, attention_mask, special_tokens_mask])
    torch.save(document_data, f'{save_path}{time.strftime(&quot;%Y%m%d-%H%M%S&quot;)}{iteration}.pt')
    print(f&quot;Document at {iteration} encoded and saved.&quot;)
</code></pre>
<p>After about 200-300 iterations on my local GTX 1060 3GB I get an error saying that my <strong>CUDA memory is full</strong>. Running this code on Colab with more GPU RAM gives me a few thousand iterations.</p>
<p>Things I've tried:</p>
<ul>
<li>Adding <code>torch.cuda.empty_cache()</code> to the start of every iteration to clear out previously held tensors</li>
<li>Wrapping the model in <code>torch.no_grad()</code> to disable the computation graph</li>
<li>Setting <code>model.eval()</code> to disable any stochastic properties that might take up memory</li>
<li>Sending the output straight to CPU in hopes to free up memory</li>
</ul>
<p>I'm baffled as to why my memory keeps overflowing. I've trained several models of bigger sizes, applying all the standard practices of a training loop (<code>optimizer.zero_grad()</code>, etc.) I've never had this problem. Why does it appear during this seemingly trivial task?</p>
<p><strong>Edit #1</strong>
Changing <code>sentence[0].to('cpu')</code> to <code>cpu_sentence = sentence[0].to('cpu')</code> gave me a few thousand iterations before VRAM usage suddenly spiked, causing the run to crash:<a href=""https://i.stack.imgur.com/ahUZb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ahUZb.png"" alt=""enter image description here"" /></a></p>
","10265940","","10265940","","2021-01-27 17:16:33","2021-01-27 17:16:33","PyTorch GPU memory leak during inference","<nlp><pytorch><gpu><bert-language-model><huggingface-transformers>","1","4","","","","CC BY-SA 4.0"
"65917197","1","","","2021-01-27 10:27:05","","1","59","<p>Huggingface has two libraries <strong>Pytorch_transformers 1.2.0 and transformers 4.</strong> and others? There are some papers using the code from <strong>pytorch_transformers</strong> and I am trying to implement some production based solutions on <strong>pytorch_transformers</strong>? Are the <strong>huggingface</strong> maintaining the &quot;<strong>pytorch_transformers</strong>&quot; library?</p>
","12409005","","","","","2021-01-27 10:45:51","Is the pytorch_transformers from huggingface maintained?","<pytorch><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"65918679","1","","","2021-01-27 11:56:37","","0","592","<p>I'm a beginner to this field and am stuck. I am following this tutorial (<a href=""https://towardsdatascience.com/multi-label-multi-class-text-classification-with-bert-transformer-and-keras-c6355eccb63a"" rel=""nofollow noreferrer"">https://towardsdatascience.com/multi-label-multi-class-text-classification-with-bert-transformer-and-keras-c6355eccb63a</a>) to build a multi-label classification using huggingface tranformers.</p>
<p>Following is the code I'm using to train my model.</p>
<pre><code># Name of the BERT model to use
model_name = 'bert-base-uncased'
# Max length of tokens
max_length = 100

PATH = 'uncased_L-12_H-768_A-12/'

# Load transformers config and set output_hidden_states to False
config = BertConfig.from_pretrained(PATH)
config.output_hidden_states = False

# Load BERT tokenizer
tokenizer = BertTokenizerFast.from_pretrained(PATH, local_files_only=True, config = config)
# tokenizer = BertTokenizer.from_pretrained(PATH, local_files_only=True, config = config)

# Load the Transformers BERT model
transformer_model = TFBertModel.from_pretrained(PATH, config = config,from_pt=True)

#######################################
### ------- Build the model ------- ###

# Load the MainLayer
bert = transformer_model.layers[0]

# Build your model input
input_ids = Input(shape=(None,), name='input_ids', dtype='int32')
# attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32') 
# inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}
inputs = {'input_ids': input_ids}

# Load the Transformers BERT model as a layer in a Keras model
bert_model = bert(inputs)[1]
dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')
pooled_output = dropout(bert_model, training=False)


# Then build your model output
issue = Dense(units=len(data.U_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='issue')(pooled_output)
outputs = {'issue': issue}

# And combine it all in a model object
model = Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')

# Take a look at the model
model.summary()


#######################################
### ------- Train the model ------- ###

# Set an optimizer
optimizer = Adam(
    learning_rate=5e-05,
    epsilon=1e-08,
    decay=0.01,
    clipnorm=1.0)

# Set loss and metrics
loss = {'issue': CategoricalCrossentropy(from_logits = True)}
# loss = {'issue': CategoricalCrossentropy()}
metric = {'issue': CategoricalAccuracy('accuracy')}

# Compile the model
model.compile(
    optimizer = optimizer,
    loss = loss, 
    metrics = metric)

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(data['U_H_Label'])

# Ready output data for the model
y_issue = to_categorical(le.transform(data['U_H_Label']))

# Tokenize the input (takes some time)
x = tokenizer(
    text=data['Input_Data'].to_list(),
    add_special_tokens=True,
    max_length=max_length,
    truncation=True,
    padding=True, 
    return_tensors='tf',
    return_token_type_ids = False,
    return_attention_mask = True,
    verbose = True)

# Fit the model
history = model.fit(
    # x={'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']},
    x={'input_ids': x['input_ids']},
    y={'issue': y_issue},
    validation_split=0.2,
    batch_size=64,
    epochs=10)
</code></pre>
<p>When I use the model.predict() function, I think I get logit scores for each class, and would like to convert them to probability scores ranging from 0 to 1.</p>
<p>I have read in multiple blogs that a softmax function is what I have to use, but am not able to relate on where and how. If anyone could please tell me what line of code would be required, I'd be grateful!</p>
","7796146","","","","","2021-09-14 17:45:20","huggingface transformers convert logit scores to probability","<python><text-classification><bert-language-model><multilabel-classification><huggingface-transformers>","0","6","","","","CC BY-SA 4.0"
"60843698","1","60846776","","2020-03-25 06:08:45","","1","842","<p>I am using the following code to summarize an article from using huggingface-transformer's pipeline. Using this code:</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
summarizer = pipeline(task=""summarization"" )
summary = summarizer(text)
print(summary[0]['summary_text'])
</code></pre>

<p>How can I define a ratio between the summary and the original article? For example, 20% of the original article?</p>

<p>EDIT 1: I implemented the solution you suggested, but got the following error. This is the code I used: </p>

<pre><code>summarizer(text, min_length = int(0.1 * len(text)), max_length = int(0.2 * len(text)))
print(summary[0]['summary_text'])
</code></pre>

<p>The error I got: </p>

<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-9-bc11c5d8eb66&gt; in &lt;module&gt;()
----&gt; 1 summarizer(text, min_length = int(0.1 * len(text)), max_length = int(0.2 * len(text)))
      2 print(summary[0]['summary_text'])

13 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1482         # remove once script supports set_grad_enabled
   1483         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1484     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1485 
   1486 

RuntimeError: index out of range: Tried to access index 1026 out of table with 1025 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:418
</code></pre>
","8165980","","8165980","","2020-03-25 12:26:00","2020-03-25 12:26:00","How to define ration of summary with hugging face transformers pipeline?","<pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65778596","1","","","2021-01-18 16:38:44","","0","143","<p>I wrote the following code and now I want to optimize the hyperparameters using Ray Tune, but I do not know how to do this.
I want to use the Population Based Training method and the set of hyperparameters I want is as follows:</p>
<pre><code>{
  &quot;per_gpu_batch_size&quot;: [16, 32, 64],
  &quot;weight_decay&quot;: (0, 0.3),
  &quot;learning_rate&quot;: (1e-5, 5e-5),
  &quot;num_epochs&quot;: [2, 3, 4, 5]
}
</code></pre>
<p>And the implemented code is as follows:</p>
<pre><code>    import tensorflow as tf 
import tensorflow_hub as hub
import pandas as pd 
from sklearn.model_selection import train_test_split
import numpy as np   
import keras 
from tqdm import tqdm 
import pickle 
from keras.models import Model 
import keras.backend as K 
from sklearn.metrics import confusion_matrix,f1_score,classification_report 
import matplotlib.pyplot as plt 
from keras.callbacks import ModelCheckpoint 
import itertools 
from keras.models import load_model 
from sklearn.utils import shuffle
from transformers import * 
from transformers import BertTokenizer, TFBertModel, BertConfig
from random import shuffle
from sklearn.utils import shuffle
 
filename1= ('/content/drive/My Drive/Ø±ÙˆÙ†ÙˆØ´Øª webtext.train.csv')
with open(filename1) as file:
  dr1=pd.read_csv(file)
  dr1 = shuffle(dr1)
  dr1 = dr1.sample(frac=1).reset_index(drop=True)
train_webtext=pd.DataFrame(dr1,columns=[&quot;text&quot;])
train_webtext=train_webtext.loc[:2499]

filename2= ('/content/drive/My Drive/Ø±ÙˆÙ†ÙˆØ´Øª xl-1542M-k40.train.csv')
with open(filename2) as file:
  dr2=pd.read_csv(file)
  dr2 = shuffle(dr2)
  dr2 = dr2.sample(frac=1).reset_index(drop=True)
train_gen=pd.DataFrame(dr2,columns=[&quot;text&quot;])
train_gen=train_gen.loc[:2499]
labels1 = [0]*len(train_webtext)+[1]*len(train_gen)


filename3= ('/content/drive/My Drive/Ø±ÙˆÙ†ÙˆØ´Øª webtext.valid.csv')
with open(filename3) as file:
  dr3=pd.read_csv(file)
valid_webtext=pd.DataFrame(dr3,columns=[&quot;text&quot;])

 
filename4=('/content/drive/My Drive/Ø±ÙˆÙ†ÙˆØ´Øª xl-1542M-k40.valid.csv')
with open(filename4) as file:
  dr4=pd.read_csv(file)
valid_gen=pd.DataFrame(dr4,columns=[&quot;text&quot;])
labels2 = [0]*len(valid_webtext)+[1]*len(valid_gen)

data = pd.concat([train_webtext,train_gen,valid_webtext,valid_gen])

sentences=data['text']
labels=labels1+labels2
len(sentences),len(labels)
    bert_tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
    bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)
    
    input_ids=[]
    attention_masks=[]
    
    for sent in sentences:
        bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =64,pad_to_max_length = True,return_attention_mask = True)
        input_ids.append(bert_inp['input_ids'])
        attention_masks.append(bert_inp['attention_mask'])
        

input_ids=np.asarray(input_ids)
attention_masks=np.array(attention_masks)
labels=np.array(labels)

print('Preparing the pickle file.....')

pickle_inp_path='/content/drive/MyDrive/ber_inp_w5000.pkl'
pickle_mask_path='/content/drive/MyDrive/ber_mask_w5000.pkl'
pickle_label_path='/content/drive/MyDrive/ber_label_w5000.pkl'

pickle.dump((input_ids),open(pickle_inp_path,'wb'))
pickle.dump((attention_masks),open(pickle_mask_path,'wb'))
pickle.dump((labels),open(pickle_label_path,'wb'))


print('Pickle files saved as ',pickle_inp_path,pickle_mask_path,pickle_label_path)


print('Loading the saved pickle files..')

input_ids=pickle.load(open(pickle_inp_path, 'rb'))
attention_masks=pickle.load(open(pickle_mask_path, 'rb'))
labels=pickle.load(open(pickle_label_path, 'rb'))

print('Input shape {} Attention mask shape {} Input label shape {}'.format(input_ids.shape,attention_masks.shape,labels.shape))
#split
train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.6666666666666667)
print('Train inp shape {} Val input shape {}\nTrain label shape {} Val label shape {}\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp.shape,val_inp.shape,train_label.shape,val_label.shape,train_mask.shape,val_mask.shape))
#


print('\nBert Model',bert_model.summary())

log_dir='tensorboard_data/tb_bert'
model_save_path='./models/bert_model.h5'

callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]

print('\nBert Model',bert_model.summary())

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)

bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])

history=bert_model.fit([train_inp,train_mask],train_label,batch_size=64,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)
</code></pre>
<p><a href=""https://colab.research.google.com/drive/1tQgAKgcKQzheoh503OzhS4N9NtfFgmjF?usp=sharing#scrollTo=ip6shHkNICTs"" rel=""nofollow noreferrer"">This link is for optimizing hyperparameters using pytorch</a></p>
<p>Thank you in advance of your guidance.</p>
","14326809","","3607203","","2021-01-21 10:35:04","2021-01-21 10:35:04","Hyperparameter Optimization for ðŸ¤—Transformers in tensorflow with pbt (population-based training)","<python><tensorflow><huggingface-transformers><hyperparameters><ray-tune>","0","0","","","","CC BY-SA 4.0"
"61043681","1","","","2020-04-05 13:52:27","","1","222","<p>I would like to use SuperGLUE tasks with huggingface-transformers. Looking at this page:</p>

<p><a href=""https://github.com/huggingface/transformers/blob/master/examples/README.md"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/examples/README.md</a></p>

<p>The only useful script is ""run_glue.py"". But I'm searching for ""run_superglue.py"", that I suppose it doesn't exist.</p>

<p>Did anyone try to use SuperGLUE tasks with huggingface-transformers? Maybe modifying ""run_glue.py"" adapting it to SuperGLUE tasks? Thanks</p>
","1705496","","","","","2020-04-05 13:52:27","How to use SuperGLUE with huggingface-transformers","<huggingface-transformers>","0","0","1","","","CC BY-SA 4.0"
"62080818","1","","","2020-05-29 07:28:31","","0","888","<p>I am trying to convert my python file which uses <strong>transformers</strong>, <strong>spacy</strong>, and <strong>torch</strong> as packages, into an .exe file. I have tried both cx_freeze and pyinstaller but every time I try to run my generated exe I face the following errors respectively.</p>

<p>For Pyinstaller:</p>

<pre><code>    C:\Users\aishv\output&gt;qa.exe
To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
Traceback (most recent call last):
  File ""main.py"", line 1, in &lt;module&gt;
  File ""&lt;frozen importlib._bootstrap&gt;"", line 983, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 967, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 677, in _load_unlocked
  File ""c:\users\aishv\anaconda3\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 623, in exec_module
    exec(bytecode, module.__dict__)
  File ""site-packages\transformers\__init__.py"", line 42, in &lt;module&gt;
  File ""&lt;frozen importlib._bootstrap&gt;"", line 983, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 967, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 677, in _load_unlocked
  File ""c:\users\aishv\anaconda3\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 623, in exec_module
    exec(bytecode, module.__dict__)
  File ""site-packages\transformers\tokenization_auto.py"", line 28, in &lt;module&gt;
  File ""&lt;frozen importlib._bootstrap&gt;"", line 983, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 967, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 677, in _load_unlocked
  File ""c:\users\aishv\anaconda3\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 623, in exec_module
    exec(bytecode, module.__dict__)
  File ""site-packages\transformers\tokenization_xlm.py"", line 27, in &lt;module&gt;
  File ""&lt;frozen importlib._bootstrap&gt;"", line 983, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 967, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 677, in _load_unlocked
  File ""c:\users\aishv\anaconda3\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 623, in exec_module
    exec(bytecode, module.__dict__)
  File ""site-packages\sacremoses\__init__.py"", line 2, in &lt;module&gt;
    """"""This directory is meant for IPython extensions.""""""
  File ""&lt;frozen importlib._bootstrap&gt;"", line 983, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 967, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 677, in _load_unlocked
  File ""c:\users\aishv\anaconda3\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 623, in exec_module
    exec(bytecode, module.__dict__)
  File ""site-packages\sacremoses\tokenize.py"", line 16, in &lt;module&gt;
  File ""site-packages\sacremoses\tokenize.py"", line 23, in MosesTokenizer
  File ""site-packages\sacremoses\corpus.py"", line 63, in chars
  File ""pkgutil.py"", line 637, in get_data
  File ""c:\users\aishv\anaconda3\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 471, in get_data
    with open(path, 'rb') as fp:
FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\aishv\\AppData\\Local\\Temp\\_MEI228322\\sacremoses\\data\\perluniprops\\IsN.txt'
[23284] Failed to execute script main
</code></pre>

<p>For cx_freeze:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\cx_Freeze\initscripts\__startup__.py"", line 40, in run
    module.run()
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\cx_Freeze\initscripts\Console.py"", line 37, in run
    exec(code, {'__name__': '__main__'})
  File ""main.py"", line 1, in &lt;module&gt;
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\transformers\__init__.py"", line 20, in &lt;module&gt;
    from .file_utils import (TRANSFORMERS_CACHE, PYTORCH_TRANSFORMERS_CACHE, PYTORCH_PRETRAINED_BERT_CACHE,
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\transformers\file_utils.py"", line 20, in &lt;module&gt;
    import boto3
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\boto3\__init__.py"", line 16, in &lt;module&gt;
    from boto3.session import Session
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\boto3\session.py"", line 17, in &lt;module&gt;
    import botocore.session
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\botocore\session.py"", line 30, in &lt;module&gt;
    import botocore.client
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\botocore\client.py"", line 16, in &lt;module&gt;
    from botocore import waiter, xform_name
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\botocore\waiter.py"", line 18, in &lt;module&gt;
    from botocore.docs.docstring import WaiterDocstring
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\botocore\docs\__init__.py"", line 15, in &lt;module&gt;
    from botocore.docs.service import ServiceDocumenter
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\botocore\docs\service.py"", line 18, in &lt;module&gt;
    from botocore.docs.bcdoc.restdoc import DocumentStructure
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\botocore\docs\bcdoc\restdoc.py"", line 16, in &lt;module&gt;
    from botocore.docs.bcdoc.docstringparser import DocStringParser
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\botocore\docs\bcdoc\docstringparser.py"", line 16, in &lt;module&gt;
    class DocStringParser(six.moves.html_parser.HTMLParser):
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\botocore\vendored\six.py"", line 92, in __get__
    result = self._resolve()
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\botocore\vendored\six.py"", line 115, in _resolve
    return _import_module(self.mod)
  File ""C:\Users\aishv\AppData\Local\Programs\Python\Python37\lib\site-packages\botocore\vendored\six.py"", line 82, in _import_module
    __import__(name)
ModuleNotFoundError: No module named 'html.parser'
</code></pre>

<p>Please help me in this situation.
Here is my setup file for Cx_freeze:</p>

<pre><code>from cx_Freeze import setup, Executable
setup(name='Qa',
        version='0.1',
        description='Application',
        executables = [Executable(""main.py"")])
</code></pre>
","11960101","","11960101","","2020-05-29 07:39:22","2020-09-10 10:54:45","Python to an exe File","<pytorch><pyinstaller><cx-freeze><torch><huggingface-transformers>","2","2","","","","CC BY-SA 4.0"
"57248098","1","60574502","","2019-07-29 06:16:27","","1","614","<p>I want to use GPT-2 to make a text classifier model. I am not really sure what head should I add after I extracted features through the GPT-2. for eample I have a sequence.</p>

<pre><code>import pytorch_transformers as pt 
import torch
text=test.iloc[1,1]
text
'If a fire wanted fanning, it could readily be fanned with a newspaper, and as the government grew weaker, I have no doubt that leather and iron acquired durability in proportion, for, in a very short time, there was not a pair of bellows in all Rotterdam that ever stood in need of a stitch or required the assistance of a hammer.'
len(text)

74
tokenizer = pt.GPT2Tokenizer.from_pretrained('gpt2')
model = pt.GPT2Model.from_pretrained('gpt2')
zz = tokenizer.tokenize(text)
z1=torch.tensor([tokenizer.convert_tokens_to_ids(zz)])
z1
tensor([[ 1532,   257,  2046,  2227,  4336,   768,    11,   340,   714, 14704,
           307,   277,  3577,   351,   257,  7533,    11,   290,   355,   262,
          1230,  6348, 17642,    11,   314,   423,   645,  4719,   326, 11620,
           290,  6953,  9477, 26578,   287,  9823,    11,   329,    11,   287,
           257,   845,  1790,   640,    11,   612,   373,   407,   257,  5166,
           286,  8966,  1666,   287,   477, 18481,   353, 11043,   326,  1683,
          6204,   287,   761,   286,   257, 24695,   393,  2672,   262,  6829,
           286,   257, 15554,    13]])
output,hidden=model(z1)
ouput.shape
torch.Size([1, 74, 768])
</code></pre>

<p>the output of GPT2 is n x m x 768 for me, which n is the batch size,m is the number of tokens in the seqence(for example I can pad/truncate to 128.), so I can not do what as the paper said for a classification task just add a fully connected layer in the tail.And I searched on google, few GPT-2 classification task is mensioned.
I am not sure what is correct. Should I do flatten/max pooling/average pooling before the  fully connected layer or something else?</p>
","4469265","","1150683","","2020-01-16 10:25:31","2020-12-01 19:43:38","using huggingface's pytorch- transformers GPT-2 for classifcation tasks","<python><nlp><pytorch><language-model><huggingface-transformers>","2","2","","","","CC BY-SA 4.0"
"64106747","1","","","2020-09-28 17:18:42","","2","337","<p>I am doing some research into HuggingFace's functionalities for transfer learning (specifically, for named entity recognition). To preface, I am a bit new to transformer architectures. I briefly walked through their example off of their website:</p>
<pre><code>from transformers import pipeline

nlp = pipeline(&quot;ner&quot;)

sequence = &quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot; \
       &quot;close to the Manhattan Bridge which is visible from the window.&quot;

print(nlp(sequence))
</code></pre>
<p>What I would like to do is save and run this locally without having to download the &quot;ner&quot; model every time (which is over 1 GB in size). In their documentation, I see that you can save the pipeline using the &quot;pipeline.save_pretrained()&quot; function to a local folder. The results of this are various files which I am storing into a specific folder.</p>
<p>My question would be how can I load this model back up into a script to continue classifying as in the example above after saving? The output of &quot;pipeline.save_pretrained()&quot; is multiple files.</p>
<p>Here is what I have tried so far:</p>
<p>1: Following the documentation about pipeline</p>
<pre><code>pipe = transformers.TokenClassificationPipeline(model=&quot;pytorch_model.bin&quot;, tokenizer='tokenizer_config.json')
</code></pre>
<p>The error I got was: 'str' object has no attribute &quot;config&quot;</p>
<p>2: Following HuggingFace example on ner:</p>
<pre><code>from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch

model = AutoModelForTokenClassification.from_pretrained(&quot;path to folder following .save_pretrained()&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;path to folder following .save_pretrained()&quot;)

label_list = [
&quot;O&quot;,       # Outside of a named entity
&quot;B-MISC&quot;,  # Beginning of a miscellaneous entity right after another miscellaneous entity
&quot;I-MISC&quot;,  # Miscellaneous entity
&quot;B-PER&quot;,   # Beginning of a person's name right after another person's name
&quot;I-PER&quot;,   # Person's name
&quot;B-ORG&quot;,   # Beginning of an organisation right after another organisation
&quot;I-ORG&quot;,   # Organisation
&quot;B-LOC&quot;,   # Beginning of a location right after another location
&quot;I-LOC&quot;    # Location
]

sequence = &quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot; \
       &quot;close to the Manhattan Bridge.&quot;

# Bit of a hack to get the tokens with the special tokens
tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))
inputs = tokenizer.encode(sequence, return_tensors=&quot;pt&quot;)

outputs = model(inputs)[0]
predictions = torch.argmax(outputs, dim=2)

print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].tolist())])
</code></pre>
<p>This yields an error: list index out of range</p>
<p>I also tried printing out just predictions which is not returning the text format of the tokens along with their entities.</p>
<p>Any help would be much appreciated!</p>
","8027995","","","","","2020-09-28 17:18:42","Loading saved NER back into HuggingFace pipeline?","<nlp><named-entity-recognition><huggingface-transformers><huggingface-tokenizers>","0","2","","","","CC BY-SA 4.0"
"64445784","1","64450767","","2020-10-20 13:04:26","","1","402","<p>I have trained a BERT model using ktrain (tensorflow wrapper) to recognize emotion on text, it works but it suffers from really slow inference. That makes my model not suitable for a production environment. I have done some research and it seems pruning could help.</p>
<p>Tensorflow provides some options for pruning e.g.  tf.contrib.model_pruning. The problem is that it is not a not a widely used technique and I can not find a simple enough example that could help me to understand how to use it. Can someone help?</p>
<p>I provide my working code below for reference.</p>
<pre><code>import pandas as pd
import numpy as np
import preprocessor as p
import emoji
import re
import ktrain
from ktrain import text
from unidecode import unidecode
import nltk

#text preprocessing class
class TextPreprocessing:
    def __init__(self):
        p.set_options(p.OPT.MENTION, p.OPT.URL)
  
    def _punctuation(self,val): 
        val = re.sub(r'[^\w\s]',' ',val)
        val = re.sub('_', ' ',val)
        return val
  
    def _whitespace(self,val):
        return &quot; &quot;.join(val.split())
  
    def _removenumbers(self,val):
        val = re.sub('[0-9]+', '', val)
        return val
  
    def _remove_unicode(self, text):
        text = unidecode(text).encode(&quot;ascii&quot;)
        text = str(text, &quot;ascii&quot;)
        return text  
    
    def _split_to_sentences(self, body_text):
        sentences = re.split(r&quot;(?&lt;!\w\.\w.)(?&lt;![A-Z][a-z]\.)(?&lt;=\.|\?)\s&quot;, body_text)
        return sentences
    
    def _clean_text(self,val):
        val = val.lower()
        val = self._removenumbers(val)
        val = p.clean(val)
        val = ' '.join(self._punctuation(emoji.demojize(val)).split())
        val = self._remove_unicode(val)
        val = self._whitespace(val)
        return val
  
    def text_preprocessor(self, body_text):

        body_text_df = pd.DataFrame({&quot;body_text&quot;: body_text},index=[1])

        sentence_split_df = body_text_df.copy()

        sentence_split_df[&quot;body_text&quot;] = sentence_split_df[&quot;body_text&quot;].apply(
            self._split_to_sentences)

        lst_col = &quot;body_text&quot;
        sentence_split_df = pd.DataFrame(
            {
                col: np.repeat(
                    sentence_split_df[col].values, sentence_split_df[lst_col].str.len(
                    )
                )
                for col in sentence_split_df.columns.drop(lst_col)
            }
        ).assign(**{lst_col: np.concatenate(sentence_split_df[lst_col].values)})[
            sentence_split_df.columns
        ]
        
        body_text_df[&quot;body_text&quot;] = body_text_df[&quot;body_text&quot;].apply(self._clean_text)

        final_df = (
            pd.concat([sentence_split_df, body_text_df])
            .reset_index()
            .drop(columns=[&quot;index&quot;])
        )
        
        return final_df[&quot;body_text&quot;]

#instantiate data preprocessing object
text1 = TextPreprocessing()

#import data
data_train = pd.read_csv('data_train_v5.csv', encoding='utf8', engine='python')
data_test = pd.read_csv('data_test_v5.csv', encoding='utf8', engine='python')

#clean the data
data_train['Text'] = data_train['Text'].apply(text1._clean_text)
data_test['Text'] = data_test['Text'].apply(text1._clean_text)

X_train = data_train.Text.tolist()
X_test = data_test.Text.tolist()

y_train = data_train.Emotion.tolist()
y_test = data_test.Emotion.tolist()

data = data_train.append(data_test, ignore_index=True)

class_names = ['joy','sadness','fear','anger','neutral']

encoding = {
    'joy': 0,
    'sadness': 1,
    'fear': 2,
    'anger': 3,
    'neutral': 4
}

# Integer values for each class
y_train = [encoding[x] for x in y_train]
y_test = [encoding[x] for x in y_test]

trn, val, preproc = text.texts_from_array(x_train=X_train, y_train=y_train,
                                                                       x_test=X_test, y_test=y_test,
                                                                       class_names=class_names,
                                                                       preprocess_mode='distilbert',
                                                                       maxlen=350)

model = text.text_classifier('distilbert', train_data=trn, preproc=preproc)

learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)

predictor = ktrain.get_predictor(learner.model, preproc)

#save the model on a file for later use
predictor.save(&quot;models/bert_model&quot;)

message = &quot;This is a happy message&quot;

#cleaning - takes 5ms to run
clean = text1._clean_text(message)

#prediction - takes 325 ms to run
predictor.predict_proba(clean)
</code></pre>
","11536058","","11536058","","2020-10-20 16:16:47","2020-10-20 17:52:50","How to apply pruning on a BERT model?","<python><tensorflow><nlp><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66020205","1","","","2021-02-03 01:56:48","","1","115","<p>I am trying to replicate the results of <a href=""https://www.buildgpt3.com/post/41/"" rel=""nofollow noreferrer"">this demo</a>, whose author primes GPT-3 with <a href=""https://twitter.com/siddkaramcheti/status/1286168606896603136?lang=es"" rel=""nofollow noreferrer"">just</a> the following text:</p>
<pre><code>gpt.add_example(Example('apple', 'slice, eat, mash, cook, bake, juice'))
gpt.add_example(Example('book', 'read, open, close, write on'))
gpt.add_example(Example('spoon', 'lift, grasp, scoop, slice'))
gpt.add_example(Example('apple', 'pound, grasp, lift'))
</code></pre>
<p>I only have access to GPT-2, via the Huggingface Transformer. How can I prime GPT-2 large on Huggingface to replicate the above examples? The issue is that, with <a href=""https://transformer.huggingface.co/doc/gpt2-large"" rel=""nofollow noreferrer"">this</a>, one doesn't get to prime with the input and corresponding output separately (as the author of the GPT-3 demo did above).</p>
<p>Similarly, <a href=""https://www.kaggle.com/nageshsingh/huggingface-transformer-basic-usage"" rel=""nofollow noreferrer"">this tutorial</a> describes using Huggingface, but there's no example which clearly shows how you can prime it using input vs output examples.</p>
<p>Does anyone know how to do this?</p>
<hr />
<p>Desired output:
use GPT-2 to return something like, for input &quot;potato&quot;, output &quot;peel, slice, cook, mash, bake&quot; (as in the GPT-3 demo: <a href=""https://www.buildgpt3.com/post/41/"" rel=""nofollow noreferrer"">https://www.buildgpt3.com/post/41/</a>). Obviously the exact list of output verbs won't be the same as GPT-2 and GPT-3 are not identical models.</p>
","12384851","","","","","2021-02-03 10:17:22","Huggingface Transformer Priming","<python><huggingface-transformers><gpt-2><gpt-3>","1","0","","","","CC BY-SA 4.0"
"60997438","1","","","2020-04-02 17:23:43","","2","228","<p>I've been experimenting with stacking language models recently and noticed something interesting: the output embeddings of BERT and XLNet are not the same as the input embeddings. For example, this code snippet:</p>

<pre><code>bert = transformers.BertForMaskedLM.from_pretrained(""bert-base-cased"")
tok = transformers.BertTokenizer.from_pretrained(""bert-base-cased"")

sent = torch.tensor(tok.encode(""I went to the store the other day, it was very rewarding.""))
enc = bert.get_input_embeddings()(sent)
dec = bert.get_output_embeddings()(enc)

print(tok.decode(dec.softmax(-1).argmax(-1)))
</code></pre>

<p>Outputs this for me:</p>

<pre><code>,,,,,,,,,,,,,,,,,
</code></pre>

<p>I would have expected the (formatted) input sequence to be returned since I was under the impression that the input and output token embeddings were tied.</p>

<p>What's interesting is that most other models do not exhibit this behavior. For example, if you run the same code snippet on GPT2, Albert or Roberta, it outputs the input sequence.</p>

<p>Is this a bug? Or is it expected for BERT/XLNet?</p>
","2457019","","6573902","","2020-12-13 06:33:49","2020-12-13 06:33:49","How to revert BERT/XLNet embeddings?","<python><nlp><pytorch><huggingface-transformers><transformer>","1","0","","","","CC BY-SA 4.0"
"63595007","1","","","2020-08-26 09:50:43","","1","170","<p>My code is as follows:</p>
<pre><code>batch_size=8
sequence_length=25
vocab_size=100
import tensorflow as tf
from transformers import T5Config, TFT5ForConditionalGeneration
configT5 = T5Config(
    vocab_size=vocab_size,
    d_ff =512, 
)  
model = TFT5ForConditionalGeneration(configT5)

model.compile(
    optimizer = tf.keras.optimizers.Adam(),
    loss = tf.keras.losses.SparseCategoricalCrossentropy()
)
input = tf.random.uniform([batch_size,sequence_length],0,vocab_size,dtype=tf.int32)
labels = tf.random.uniform([batch_size,sequence_length],0,vocab_size,dtype=tf.int32)
input = {'inputs': input, 'decoder_input_ids': input}
model.fit(input, labels)
</code></pre>
<p>It generates an error:</p>
<blockquote>
<p>logits and labels must have the same first dimension, got logits shape
[1600,64] and labels shape [200]    [[node
sparse_categorical_crossentropy_3/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits
(defined at C:\Users\FA.PROJECTOR-MSK\Google Ð”Ð¸ÑÐº\Colab
Notebooks\PoetryTransformer\experiments\TFT5.py:30) ]]
[Op:__inference_train_function_25173]  Function call stack:
train_function</p>
</blockquote>
<p>I dont understand - why the model returns a tensor of [1600, 64]. According to <a href=""https://huggingface.co/transformers/model_doc/t5.html#tft5forconditionalgeneration"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/t5.html#tft5forconditionalgeneration</a> model returns [batch_size, sequence_len, vocab_size].</p>
","5561472","","","","","2020-11-29 12:32:37","How to train Huggingface TFT5ForConditionalGeneration model?","<tensorflow><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63606049","1","","","2020-08-26 21:35:17","","0","514","<p>I have some questions about fine-tuning causal language model using transformers and PyTorch.</p>
<p>My main goal is to fine-tune XLNet. However, I found the most of posts online was targeting at text classification, like this <a href=""https://mccormickml.com/2019/09/19/XLNet-fine-tuning/"" rel=""nofollow noreferrer"">post</a>. I was wondering, is there any way to fine-tune the model, without using the <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py"" rel=""nofollow noreferrer""><code>run_language_model.py</code></a> from transformers' GitHub?</p>
<p>Here is a piece of my code trying to fine-tune XLNet:</p>
<pre class=""lang-py prettyprint-override""><code>model = XLNetLMHeadModel.from_pretrained(&quot;xlnet-base-cased&quot;)
tokenizer = XLNetTokenizer.from_pretrained(&quot;xlnet-base-cased&quot;, do_lower_case=True)
LOSS = torch.nn.CrossEntrypoLoss()
batch_texts = [&quot;this is sentence 1&quot;, &quot;i have another sentence like this&quot;, &quot;the final sentence&quot;]
encodings = tokenizer.encode_plus(batch_texts, add_special_tokens=True,
                                  return_tensors=True, return_attention_mask=True)
outputs = model(encodings[&quot;input_ids&quot;], encodings[&quot;attention_mask&quot;])
loss = LOSS(outputs[0], target_ids)
loss.backward()
# ignoring the rest of codes...
</code></pre>
<p>I got stuck at the last two lines. At first, when using this LM model, it seems I don't have any <code>labels</code> as the supervised learning usually do; Second, as the language model which is to minimize the loss (cross-entropy here), I need a <code>target_ids</code> to compute the loss and perplexity with <code>input_ids</code>.</p>
<p>Here are my follow-up questions:</p>
<ol>
<li>How should I deal with this <code>labels</code> during the model fitting?</li>
<li>Should I set something like <code>target_ids=encodings[&quot;input_ids&quot;].copy()</code> to compute cross-entropy loss and perplexity?</li>
<li>If not, how should set this <code>target_ids</code>?</li>
<li>From the perplexity page from transformers' <a href=""https://huggingface.co/transformers/perplexity.html"" rel=""nofollow noreferrer"">documentation</a>, how should I adapt its method for non-fixed length of input text?</li>
<li>I saw another <a href=""https://huggingface.co/transformers/task_summary.html#text-generation"" rel=""nofollow noreferrer"">post</a> from the documentation saying that it requires padding text for causal language modeling. However, from the link in 3), there is no such sign for padding text. Which one should I follow?</li>
</ol>
<p>Any suggestions and advice will be appreciated!</p>
","4826046","","","","","2020-08-26 21:35:17","fine tune causal language model using transformers and pytorch","<python-3.x><pytorch><huggingface-transformers><language-model>","0","5","","","","CC BY-SA 4.0"
"65925640","1","66193869","","2021-01-27 18:58:04","","0","184","<p>I have a basic conceptual doubt. When i train a bert model on sentence say:</p>
<pre><code>Train: &quot;went to get loan from bank&quot; 
Test :&quot;received education loan from bank&quot;
</code></pre>
<p>How does the test sentence assigns the weights for each token because i however dont pass exact sentence for testing and there is a slight addition  of words like &quot;education&quot; which change the context slightly</p>
<p>Assuming such context is not trained in my model how the weights are assigned for each token in my bert before i fine tune further</p>
<p>If i confuse with my question, simply put i am trying to understand how the weights get assigned during testing if a slight variation in context occurs that was not trained on.</p>
","7965040","","","","","2021-02-14 09:05:17","Assigning weights during testing the bert model","<bert-language-model><huggingface-transformers><transformer><language-model>","1","0","","","","CC BY-SA 4.0"
"66022010","1","","","2021-02-03 05:55:18","","0","41","<p>I am new to text summarization and I am trying to figure out what .pb files are and how to create them. For reference, any specifics to the PEGASUS, BART, or T5 models would be appreciated.</p>
","14882176","","","","","2021-02-03 05:55:18","What are .pb files and what data do they contain?","<python><tensorflow><machine-learning><huggingface-transformers>","0","1","","2021-02-13 06:35:32","","CC BY-SA 4.0"
"63607919","1","63608550","","2020-08-27 01:38:31","","0","1289","<p>I have a small dataset for sentiment analysis. The classifier will be a simple KNN but I wanted to get the word embedding with the <code>Bert</code> model from the <code>transformers</code> library. Note that I just found out about this library - I am still learning.</p>
<p>So looking at online example, I am trying to understand the dimensions that are returned from the model.</p>
<p>Example:</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokens = tokenizer.encode([&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;])
print(tokens)

tokens = tokenizer.encode(&quot;Hello, my dog is cute&quot;, &quot;He is really nice&quot;)
print(tokens)

tokens = tokenizer.encode([&quot;Hello, my dog is cute&quot;])
print(tokens)

tokens = tokenizer.encode(&quot;Hello, my dog is cute&quot;)
print(tokens)
</code></pre>
<p>The output is the following:</p>
<pre><code>[101, 100, 100, 102]

[101, 7592, 1010, 2026, 3899, 2003, 10140, 102, 2002, 2003, 2428, 3835, 102]

[101, 100, 102]

[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]
</code></pre>
<p>I can't seem to find the docs for <code>encode()</code> - I have no idea why it returns different stuff when the input is passed as a list. What is this doing?</p>
<p>Additionally, is there a method to pass a word token and get the actual word back - to troubleshoot the above?</p>
<p>Thank you in advance</p>
","4557607","","4557607","","2020-08-27 01:44:22","2020-08-27 03:05:28","Tokens returned in transformers Bert model from encode()","<python><machine-learning><nlp><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63608183","1","63608360","","2020-08-27 02:18:41","","2","345","<p>Due to the code &quot;torch.tensor,&quot; I am getting the error &quot;Tensor object is not callable&quot; when I add &quot;input.&quot; Does anyone know how I can fix this?</p>
<pre><code>import torch
from torch.nn import functional as F
from transformers import GPT2Tokenizer, GPT2LMHeadModel


tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

text0 = &quot;In order to&quot;
text = tokenizer.encode(&quot;In order to&quot;)
input, past = torch.tensor([text]), None


logits, past = model(input, past = past)
logits = logits[0,-1]
probabilities = torch.nn.functional.softmax(logits)
best_logits, best_indices = logits.topk(5)
best_words = [tokenizer.decode([idx.item()]) for idx in best_indices]
text.append(best_indices[0].item())
best_probabilities = probabilities[best_indices].tolist()

for i in range(5):
    f = ('Generated {}: {}'.format(i, best_words[i]))
    print(f)


option = input(&quot;Pick a Option:&quot;)
z = text0.append(option)
print(z)
</code></pre>
<p>Error stacktrace:</p>
<pre><code>TypeError                                 Traceback (most recent call last)

&lt;ipython-input-2-82e8d88e81c1&gt; in &lt;module&gt;()
     25 
     26 
---&gt; 27 option = input(&quot;Pick a Option:&quot;)
     28 z = text0.append(option)
     29 print(z)

TypeError: 'Tensor' object is not callable
</code></pre>
","13624094","","6664872","","2020-08-27 02:34:53","2020-08-27 02:41:20","Torch Tensor & Input Conflicting: ""Tensor Object Is Not Callable""","<python><pytorch><google-colaboratory><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63939072","1","67001959","","2020-09-17 13:25:58","","0","401","<p>I have trained and saved some NER models using</p>
<pre><code>torch.save(model)
</code></pre>
<p>I need to load these model files (extension <code>.pt</code>) for evaluation using</p>
<pre><code>torch.load('PATH_TO_MODEL.pt')
</code></pre>
<p>And I get the following error: <code>'BertConfig' object has no attribute 'return_dict'</code></p>
<p>For the same, I updated my transformer package to the latest one, but the error persists.</p>
<p>This is the stack trace:</p>
<pre><code>Traceback (most recent call last):
File &quot;/home/systematicReviews/train_mtl_3.py&quot;, line 523, in &lt;module&gt;
test_loss, test_cr, test_cr_fine = evaluate_i(test_model, optimizer, scheduler, validation_dataloader, args, device)
File &quot;/home/systematicReviews/train_mtl_3.py&quot;, line 180, in evaluate_i
e_loss_coarse, e_output, e_labels, e_loss_fine, e_f_output, e_f_labels, mask, e_cumulative_loss  = defModel(args, e_input_ids, attention_mask=e_input_mask, P_labels=e_labels, P_f_labels=e_f_labels)
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 541, in __call__
result = self.forward(*input, **kwargs)
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 150, in forward
return self.module(*inputs[0], **kwargs[0])
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 541, in __call__
result = self.forward(*input, **kwargs)
File &quot;/home/systematicReviews/models/mtl/model.py&quot;, line 122, in forward
attention_mask = attention_mask
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 541, in __call__
result = self.forward(*input, **kwargs)
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/transformers/modeling_bert.py&quot;, line 784, in forward
return_dict = return_dict if return_dict is not None else self.config.use_return_dict
File &quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/transformers/configuration_utils.py&quot;, line 219, in use_return_dict
return self.return_dict and not self.torchscript
AttributeError: 'BertConfig' object has no attribute 'return_dict'
</code></pre>
<p>Here is some more information about my system:</p>
<pre><code>- `transformers` version: 3.1.0
- Platform: Linux-4.4.0-186-generic-x86_64-with-debian-stretch-sid
- Python version: 3.6.9
- PyTorch version (GPU?): 1.3.1 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No
</code></pre>
<p>It worked pretty fine until now, but suddenly this bug appears. Any help or hint is appreciated.</p>
","9965155","","","","","2021-04-08 10:19:26","Loading saved NER transformers model causes AttributeError?","<torch><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"64365122","1","","","2020-10-15 04:58:18","","1","1289","<p>I have pre-trained a bert model with custom corpus then got vocab file, checkpoints, model.bin, tfrecords, etc.</p>
<p>Then I loaded the model as below :</p>
<pre class=""lang-py prettyprint-override""><code># Load pre-trained model (weights)
model = BertModel.from_pretrained('/content/drive/My Drive/Anirban_test_pytorch')
</code></pre>
<p>But when I am trying to use the model for any task (like q and a, prediction of mask word etc.) then getti9ng below error</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
nlp = pipeline(&quot;fill-mask&quot;, model=&quot;model&quot;)
nlp(f&quot;This is the best thing I've {nlp.tokenizer.mask_token} in my life.&quot;)
</code></pre>
<p>ERROR:</p>
<p>OSError: Can't load config for 'model'. Make sure that:</p>
<ul>
<li><p>'model' is a correct model identifier listed on 'https://huggingface.co/models'</p>
</li>
<li><p>or 'model' is the correct path to a directory containing a config.json file</p>
</li>
</ul>
<p>Can you please help me?</p>
","14275521","","6664872","","2020-10-15 08:04:19","2020-10-15 08:04:19","Huggingface pre trained bert model is not working","<pytorch><bert-language-model><huggingface-transformers>","0","6","","","","CC BY-SA 4.0"
"63392315","1","","","2020-08-13 09:39:36","","2","561","<p>I have tried <a href=""https://stackoverflow.com/questions/63312859/how-to-change-huggingface-transformers-default-cache-directory"">this</a> but it's not working for me.  I am using this <a href=""https://github.com/barissayil/SentimentAnalysis"" rel=""nofollow noreferrer"">Git repo</a>. I am building a desktop app and don't want users to download model. I want to ship models with build. I know transformers library looks for models in <code>cache/torch/transformers</code>. If it's not there then download it. I also know you can pass <code>cache_dir</code> parameter in <code>pre_trained</code>.
I am trying this.</p>
<pre><code>cache = os.path.join(os.path.abspath(os.getcwd()), 'Transformation/Annotators/New Sentiment Analysis/transformers')
os.environ['TRANSFORMERS_CACHE'] = cache


if args.model_name_or_path is None:
    args.model_name_or_path = 'barissayil/bert-sentiment-analysis-sst'
#Configuration for the desired transformer model
config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=cache)
</code></pre>
<p>I have tried the solution in above mentioned question and tried cache_dir also. The transformer folder is in same directory with analyze.py. The whole repo and transformer folder is in New Sentiment Analysis directory.</p>
","6848463","","472495","","2020-09-26 15:56:28","2020-09-26 15:56:28","Transformer library cache path is not changing","<python><sentiment-analysis><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60914793","1","60938998","","2020-03-29 13:08:48","","2","557","<p>I used the <code>never_split</code> option and tried to retain some tokens. But the tokenizer still divide them into wordpieces. </p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', never_split=['lol'])
tokenizer.tokenize(""lol That's funny"")
['lo', '##l', 'that', ""'"", 's', 'funny']
</code></pre>

<p>Do I miss anything here?</p>
","2236600","","","","","2020-03-30 20:29:37","Argument ""never_split"" not working on bert tokenizer","<python><tensorflow><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61580961","1","61581357","","2020-05-03 20:09:32","","3","916","<p>I have some questions regarding of SpanBert loading using transformers packages.</p>

<p>I downloaded the pre-trained file from <a href=""https://github.com/facebookresearch/SpanBERT"" rel=""nofollow noreferrer"">SpanBert</a> GitHub Repo and <code>vocab.txt</code> from Bert. Here is the code I used for loading:</p>

<pre class=""lang-py prettyprint-override""><code>model = BertModel.from_pretrained(config_file=config_file,
                                  pretrained_model_name_or_path=model_file,
                                  vocab_file=vocab_file)
model.to(""cuda"")
</code></pre>

<p>where </p>

<ul>
<li><code>config_file</code> -> <code>config.json</code></li>
<li><code>model_file</code> -> <code>pytorch_model.bin</code></li>
<li><code>vocab_file</code> -> <code>vocab.txt</code></li>
</ul>

<p>But I got the <code>UnicodeDecoderError</code> with the above code saying that <code>'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte</code></p>

<p>I also tried loading SpanBert with the method mentioned <a href=""https://huggingface.co/SpanBERT/spanbert-large-cased"" rel=""nofollow noreferrer"">here</a>. But it returned <code>OSError: file SpanBERT/spanbert-base-cased not found</code>.</p>

<p>Do you have any suggestions on loading the pre-trained model correctly? Any suggestions are much appreciated. Thanks!</p>
","4826046","","","","","2020-10-27 12:29:23","Unable to load SpanBert model with transformers package","<python><huggingface-transformers><bert-language-model>","1","1","3","","","CC BY-SA 4.0"
"64514791","1","","","2020-10-24 15:04:54","","0","119","<p>I am facing the original error &quot;RuntimeError: bool value of Tensor with more than one value is ambiguous&quot; while executing the 'generate' function within BART model from transformers. The Python version is 3.6.5, Transformers version 2.3.1
Following is the code used:</p>
<pre class=""lang-py prettyprint-override""><code># Summary generator model initiation
from transformers import BartTokenizer, BartForConditionalGeneration
bart_model = 'facebook/bart-large-cnn'
sum_tokenizer = BartTokenizer.from_pretrained(bart_model)
sum_model = BartForConditionalGeneration.from_pretrained(bart_model)

ARTICLE_TO_SUMMARIZE = &quot;My friends are cool but they eat too many carbs.&quot;
bart_sum_input_ids = sum_tokenizer.batch_encode_plus([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')
# Generate Summary
summary_ids = sum_model.generate(bart_sum_input_ids['input_ids'], num_beams=4, max_length=5, early_stopping=True)
bart_sum = sum_tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)
</code></pre>
<p>Please help with identifying the issue and solving it.
Full stack trace:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-3-413492f5e837&gt; in &lt;module&gt;()
      3 
      4 # Generate Summary
----&gt; 5 summary_ids = sum_model.generate(bart_sum_input_ids['input_ids'], num_beams=4, max_length=5, early_stopping=True)
      6 bart_sum = sum_tokenizer.decode(sum_ids.squeeze(), skip_special_tokens=True)

/mnt/disks/user/anaconda3/lib/python3.6/site-packages/torch/autograd/grad_mode.py in decorate_no_grad(*args, **kwargs)
     44         def decorate_no_grad(*args, **kwargs):
     45             with self:
---&gt; 46                 return func(*args, **kwargs)
     47         return decorate_no_grad
     48 

/mnt/disks/user/anaconda3/lib/python3.6/site-packages/transformers/generation_utils.py in generate(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_kwargs)
    346         # create attention mask if necessary
    347         # TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140
--&gt; 348         if (attention_mask is None) and (pad_token_id is not None) and (pad_token_id in input_ids):
    349             attention_mask = input_ids.ne(pad_token_id).long()
    350         elif attention_mask is None:

RuntimeError: bool value of Tensor with more than one value is ambiguous
</code></pre>
","13321507","","6664872","","2020-11-02 06:08:49","2020-11-02 06:08:49","Error_Running_Generate_Function_BART_module","<python><huggingface-transformers>","0","11","","","","CC BY-SA 4.0"
"65913053","1","65931610","","2021-01-27 04:52:56","","0","202","<p>I'm trying to send proxy address to <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">sentence transformers</a> but am not able to figure out the right way.</p>
<pre><code>from sentence_transformers import SentenceTransformer
embedder = SentenceTransformer('paraphrase-distilroberta-base-v1')
</code></pre>
<p>Usually, this piece will download the model, but for a system with proxy it is not working. Based on the <a href=""https://huggingface.co/transformers/main_classes/configuration.html?highlight=proxy"" rel=""nofollow noreferrer"">Huggingface Transformer Configuration</a>, I found that I have to use the proxy using, <code>proxies (Dict[str, str], optional)</code> â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}</code>.</p>
<p>Let's say my proxy is &quot;http://myproxy.mydomain.com:1123&quot;.  How do I use the proxy?</p>
","2614539","","","","","2021-01-28 05:50:25","How to set proxy in SentenceTransformers?","<python><proxy><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"64171762","1","","","2020-10-02 12:43:54","","0","12","<p>Background: I downloaded a weight that is trained using <code>pytorch_pretrained_bert==0.4.0</code> plus <code>apex</code>, and now I want to load it to a framework that has no apex support and using <code>transformers==3.1.0</code> (which is the new name of <code>pytorch_pretrained_bert</code>).</p>
<p>To test if things go right, I wrote the following snippet:</p>
<pre><code>import torch
import numpy as np
try:
    from transformers import BertForNextSentencePrediction
except:
    from pytorch_pretrained_bert import BertForNextSentencePrediction


doc = torch.tensor(np.arange(10)).reshape([1, 10])
seg = torch.tensor([0]* 5 + [1] * 5).reshape([1, 10])
mask = torch.tensor([1]* 5 + [0] * 5).reshape([1, 10])

model = BertForNextSentencePrediction.from_pretrained(&quot;/path/to/my/ckpt&quot;)
model.eval()
scores = model(input_ids=doc, token_type_ids=seg, attention_mask=mask,
print(scores)
</code></pre>
<p>and then run it on the following environment:</p>
<ol>
<li><code>pytorch==1.2, pytorch_pretrained_bert==0.4.0</code>, no <code>apex</code>
installed</li>
<li><code>pytorch==1.2, pytorch_pretrained_bert==0.4.0</code>,
<strong>with</strong> <code>apex</code> installed --&gt; same env with the one where the downloaded checkpoint was trained</li>
<li><code>pytorch==1.5, transformers==3.1.0</code>, no <code>apex</code> installed --&gt; same env with the
framework</li>
</ol>
<p>with (1) and (3), I got exactly same output: <code>tensor([[-0.2002,  0.0512]], grad_fn=&lt;AddmmBackward&gt;)</code>
yet with (2), I got a different result: <code>tensor([[ 0.4332, -0.0907]], grad_fn=&lt;AddmmBackward&gt;)</code></p>
<p>I'm not that sure how removing apex would change the inference results...by browsing through the documents, I assume it's because the ckpt weights were stored in float16 which somehow interrupted differently when apex is removed?
If so, I wonder if there is a way to &quot;revert&quot; this f16 precision weights back to f32 format?</p>
<p>Correct me if my assumption was wrong. Thanks!</p>
","13112529","","","","","2020-10-02 12:43:54","Pytorch: revert weight trained using apex back to normal float32","<apex><torch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"64520322","1","","","2020-10-25 04:21:37","","1","201","<p>I am running a simple comment classification task on google colab. I am using DistilBERT for contextual embeddings.I use only 4000 training sample cause the notebook keeps on crashing.
When I run the cell for obtaining the embeddings, I keep a tab on how the RAM utilisation increases. I am seeing that it oscillates from somewhere between 3gb to 8gb.
Should not it be just increasing? Can anyone explain how this works at lower level.</p>
<p>Here is my code, the cell block at last is where I am seeing the above said thing.</p>
<pre class=""lang-py prettyprint-override""><code># For DistilBERT:
model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')

## Want BERT instead of distilBERT? Uncomment the following line:
#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')

# Load pretrained model/tokenizer
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

max_len=80
tokenized = sample['comment_text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True,max_length= max_len)))

padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])

attention_mask = np.where(padded != 0, 1, 0)
attention_mask.shape

input_ids = torch.tensor(padded)  
attention_mask = torch.tensor(attention_mask)

**with torch.no_grad():
    last_hidden_states = model(input_ids, attention_mask=attention_mask)**
</code></pre>
","8276925","","5123537","","2020-10-25 22:21:26","2020-10-25 22:21:26","Fluctuating RAM in google colab while running a BERT model","<deep-learning><nlp><ram><bert-language-model><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"64646867","1","64647354","","2020-11-02 13:52:37","","3","3430","<p>Once I have downloaded a pre-trained model on a Colab Notebook, it disappears after I reset the notebook variables.
Is there a way I can download the model to use it for a second occasion?</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
</code></pre>
","13448993","","6664872","","2020-11-02 14:00:08","2020-11-02 14:23:24","Downloading huggingface pre-trained models","<python><nlp><google-colaboratory><huggingface-transformers>","1","0","2","2020-11-03 00:55:03","","CC BY-SA 4.0"
"64646890","1","64665758","","2020-11-02 13:53:54","","0","124","<p>I have downloaded the HuggingFace BERT model from the transformer repository found <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">here</a> and would like to train the model on custom NER labels by using the run_ner.py script as it is referenced <a href=""https://huggingface.co/transformers/task_summary.html"" rel=""nofollow noreferrer"">here</a> in the section &quot;Named Entity Recognition&quot;.</p>
<p>I define model (&quot;bert-base-german-cased&quot;), data_dir (&quot;Data/sentence_data.txt&quot;) and labels (&quot;Data/labels.txt)&quot; as defaults in the code.</p>
<p>Now I'm using this input for the command line:</p>
<pre><code>python run_ner.py --output_dir=&quot;Models&quot; --num_train_epochs=3 --logging_steps=100 --do_train --do_eval --do_predict
</code></pre>
<p>But all it does is telling me:</p>
<pre><code>Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.w
eight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>After that it just stops, not ending the script, but simply waiting.</p>
<p>Does anyone know what could be the problem here? Am I missing a parameter?</p>
<p>My sentence_data.txt in CoNLL format looks like this (small snippet):</p>
<pre><code>Strafverfahren O
gegen O
; O
wegen O
Diebstahls O
hat O
das O
Amtsgericht Ort
Leipzig Ort
- O
Strafrichter O
</code></pre>
<p>And that's how I defined my labels in labels.txt:</p>
<pre><code>&quot;Date&quot;, &quot;Delikt&quot;, &quot;Strafe_Tatbestand&quot;, &quot;Schadensbetrag&quot;, &quot;GestÃ¤ndnis_ja&quot;, &quot;Vorstrafe_ja&quot;, &quot;Vorstrafe_nein&quot;, &quot;Ort&quot;,
&quot;Strafe_Gesamtfreiheitsstrafe_Dauer&quot;, &quot;Strafe_Gesamtsatz_Dauer&quot;, &quot;Strafe_Gesamtsatz_Betrag&quot;
</code></pre>
","13440007","","6664872","","2020-11-03 09:57:45","2020-11-03 15:26:53","Train BERT with CLI commands","<python><machine-learning><nlp><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60743907","1","","","2020-03-18 16:42:22","","2","798","<p>Huggigface BERT implementation has a hack to remove the pooler from optimizer.</p>

<p><a href=""https://github.com/huggingface/transformers/blob/b832d5bb8a6dfc5965015b828e577677eace601e/examples/run_squad.py#L927"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/b832d5bb8a6dfc5965015b828e577677eace601e/examples/run_squad.py#L927</a></p>

<pre><code># hack to remove pooler, which is not used
# thus it produce None grad that break apex
param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]
</code></pre>

<p>We are trying to run pretrining on huggingface bert models. The code always diverges later during the training if this pooler hack is not applied. I also see the pooler layer being used during classification.</p>

<pre><code>pooled_output = outputs[1]
pooled_output = self.dropout(pooled_output)
logits = self.classifier(pooled_output)
</code></pre>

<p>The pooler layer is a FFN with tanh activation</p>

<pre><code>class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We ""pool"" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
</code></pre>

<p><strong>My question is why this pooler hack solves numeric instability?</strong></p>

<p>Problem seen with pooler</p>

<p><a href=""https://i.stack.imgur.com/veu0Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/veu0Q.png"" alt=""diminishing loss scaler""></a></p>
","1513792","","975097","","2020-04-25 00:01:00","2020-04-25 00:01:00","Why does huggingface bert pooler hack make mixed precission training stable?","<apex><pre-trained-model><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"64631665","1","64640570","","2020-11-01 12:05:03","","1","145","<p>I am a newbie to <strong>huggingface transformers</strong> and facing the below issue in training a <code>RobertaForMaskedLM</code> LM from scratch:</p>
<p>First, I have trained and saved a <code>ByteLevelBPETokenizer</code> as follows:</p>
<pre><code>tokenizer = ByteLevelBPETokenizer()
print('Saving tokenizer at:', training_file)
tokenizer.train(files=training_file, vocab_size=VOCAB_SIZE, min_frequency=2, 
special_tokens=[&quot;&lt;s&gt;&quot;,&quot;&lt;pad&gt;&quot;,&quot;&lt;/s&gt;&quot;,&quot;&lt;unk&gt;&quot;,&quot;&lt;mask&gt;&quot;])
tokenizer.save_model(tokenizer_mdl_dir)
</code></pre>
<p>Then, trained <code>RobertaForMaskedLM</code> using this tokenizer by creating a <code>RobertaTokenizer</code> as follows:</p>
<pre><code>roberta_tokenizer = RobertaTokenizer(tokenizer_mdl + &quot;/vocab.json&quot;, tokenizer_mdl + &quot;/merges.txt&quot;)
</code></pre>
<p>But now, when I try to test the trained LM using a fill-mask pipeline,</p>
<pre><code>fill_mask_pipeline = pipeline(&quot;fill-mask&quot;, model=roberta_model, tokenizer=roberta_tokenizer)
</code></pre>
<p>I got the below error:</p>
<blockquote>
<p>PipelineException: No mask_token () found on the input</p>
</blockquote>
<p>So, I realized, the tokenizer that I have loaded, is tokenizing the <code>&lt;mask&gt;</code> token as well. But I couldn't understand why it is doing so. Please help me understand this.</p>
<p>After trying several things, I loaded the tokenizer differently,</p>
<pre><code>roberta_tokenizer = RobertaTokenizer.from_pretrained(tokenizer_mdl)
</code></pre>
<p>And, now the <code>fill_mask_pipeline</code> runs without errors. So, what is the difference between loading a tokenizer using <code>RobertaTokenizer()</code> and using the <code>.from_pretrained()</code> method?</p>
","2565484","","","","","2020-11-02 07:31:39","What is the difference in RobertaTokenizer() and from_pretrained() way of initialising RobertaTokenizer?","<pytorch><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"63626014","1","63636417","","2020-08-28 00:57:18","","2","257","<p>Every time I run GPT-2, I am receiving this message. Is there a way I can get this to go away?</p>
<pre><code>Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
","13624094","","1243762","","2020-11-29 11:52:40","2020-11-29 11:52:40","How to Get Rid of GPT-2 Warning Message?","<python><huggingface-transformers><gpt-2>","1","1","","","","CC BY-SA 4.0"
"60833301","1","60833512","","2020-03-24 14:42:13","","0","527","<p>I am trying to use a GPT2 architecture for musical applications and consequently need to train it from scratch. After a bit of googling I found that the issue #1714 from huggingface's github already had ""solved"" the question. When I try the to run the propose solution :</p>

<pre><code>from transformers import GPT2Config, GPT2Model

NUMLAYER = 4
NUMHEAD = 4
SIZEREDUCTION = 10 #the factor by which we reduce the size of the velocity argument.
VELSIZE = int(np.floor(127/SIZEREDUCTION)) + 1 
SEQLEN=40 #size of data sequences.
EMBEDSIZE = 5 

config = GPT2Config(vocab_size = VELSIZE, n_positions = SEQLEN, n_embd = EMBEDSIZE, n_layer = NUMLAYER, n_ctx = SEQLEN, n_head = NUMHEAD)  
model = GPT2Model(config)
</code></pre>

<p>I get the following error : </p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-7-b043a7a2425f&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/cnelias/Desktop/PHD/Swing project/code/script/GPT2.py', wdir='C:/Users/cnelias/Desktop/PHD/Swing project/code/script')

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 786, in runfile
    execfile(filename, namespace)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/cnelias/Desktop/PHD/Swing project/code/script/GPT2.py"", line 191, in &lt;module&gt;
    model = GPT2Model(config)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in __init__
    self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in &lt;listcomp&gt;
    self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 223, in __init__
    self.attn = Attention(nx, n_ctx, config, scale)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 109, in __init__
    assert n_state % config.n_head == 0
</code></pre>

<p>What does it mean and how can I solve it ?</p>

<p>Also more generally, is there a documentation on how to do a forward call with the GPT2 ? Can I define my own <code>train()</code> function or do I have to use the model's build-in function ? Am I forced to use a <code>Dataset</code> to do the training or can I feed it individual tensors ? 
I looked for it but couldn't find answer to these on the doc, but maybe I missed something.</p>

<p>PS : I already read the blogpost fron huggingface.co, but it omits too much informations and details to be usefull for my application.</p>
","7896124","","1243762","","2020-11-29 12:10:24","2020-11-29 12:10:24","Train huggingface's GPT2 from scratch : assert n_state % config.n_head == 0 error","<python><nlp><huggingface-transformers><transformer><gpt-2>","1","0","","","","CC BY-SA 4.0"
"64635072","1","","","2020-11-01 17:52:48","","1","378","<p>I'm running run_clm.py to fine-tune gpt-2 form the huggingface library, following the language_modeling example:</p>
<pre><code>!python run_clm.py \
    --model_name_or_path gpt2 \
    --train_file train.txt \
    --validation_file test.txt \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm
</code></pre>
<p>This is the output, the process seemed to be started but there was the <strong>^C</strong> appeared to stop the process:</p>
<pre><code>The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .
The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .
***** Running training *****
  Num examples = 2318
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed &amp; accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 870
  0% 0/870 [00:00&lt;?, ?it/s]^C
</code></pre>
<p>Here's my environment info:</p>
<ul>
<li>transformers version: 3.4.0</li>
<li>Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic</li>
<li>Python version: 3.6.9</li>
<li>Tensorflow version: 1.14</li>
<li>Using GPU in script?: yes</li>
</ul>
<p>What would be the possible triggers of the early stopping?</p>
","8713984","","1243762","","2020-11-29 12:09:14","2020-11-29 12:09:14","huggingface transformers run_clm.py stops early","<huggingface-transformers><gpt-2>","0","0","","","","CC BY-SA 4.0"
"64526841","1","","","2020-10-25 17:42:57","","3","910","<p>I'm trying to solve a multilabel classification task of 10 classes with a relatively balanced training set consists of ~25K samples and an evaluation set consists of ~5K samples.</p>
<p>I'm using the huggingface:</p>
<pre><code>model = transformers.BertForSequenceClassification.from_pretrained(...
</code></pre>
<p>and obtain quite nice results (ROC AUC = 0.98).</p>
<p>However, I'm witnessing some odd behavior which I don't seem to make sense of -</p>
<p>I add the following lines of code:</p>
<pre><code>for param in model.bert.parameters():
    param.requires_grad = False
</code></pre>
<p>while making sure that the other layers of the model are learned, that is:</p>
<pre><code>[param[0] for param in model.named_parameters() if param[1].requires_grad == True]
gives
['classifier.weight', 'classifier.bias']
</code></pre>
<p>Training the model when configured like so, yields some embarrassingly poor results (ROC AUC = 0.59).</p>
<p>I was working under the assumption that an out-of-the-box pre-trained BERT model (without any fine-tuning) should serve as a relatively good feature extractor for the classification layers. So, where do I got it wrong?</p>
","14517235","","1031417","","2020-11-08 19:53:01","2020-11-08 19:53:01","Can I use BERT as a feature extractor without any finetuning on my specific data set?","<pytorch><bert-language-model><huggingface-transformers>","2","1","2","","","CC BY-SA 4.0"
"60940110","1","","","2020-03-30 21:50:45","","1","225","<p>To Whom It May Concern,</p>

<p>After training a <code>binary classification model</code> via <code>examples/run_glue.py</code>, I have a the following files.</p>

<pre><code>merges.txt
tokenizer_config.json
config.json
pytorch_model.bin
training_args.bin
special_tokens_map.json
vocab.json
</code></pre>

<p>I can evaluate this model via the same script and the <code>--do_eval</code> arg; however, I would like to use this model to do <code>inference</code> not <code>evaluation</code>, as I don't have the labels for the unseen data I will be feeding into it. What is the best way to accomplish this using the main library? I currently use <a href=""https://github.com/kaushaltrivedi/fast-bert"" rel=""nofollow noreferrer"">fast-bert</a>. However, I'm transitioning this into a production pipeline and consequently can't stray from the main library anymore.</p>

<p>Thanks in advance!</p>

<p>Chris</p>
","8061776","","","","","2020-03-30 21:50:45","How to perform text classification inference using the huggingface transformer's library","<python><pytorch><huggingface-transformers>","0","0","0","","","CC BY-SA 4.0"
"65897844","1","","","2021-01-26 08:16:49","","0","48","<p>I am trying to use <a href=""https://huggingface.co/aubmindlab/aragpt2-base"" rel=""nofollow noreferrer"">GPT2</a> for Arabic text classification task as follows:</p>
<pre><code>    tokenizer = GPT2Tokenizer.from_pretrained(model_path)
    model = GPT2ForSequenceClassification.from_pretrained(model_path, 
                                                          num_labels=len(lab2ind)) 
</code></pre>
<p>However, when I use the tokenizer it converts the Arabic characters to symbols like this
<code>'Ä Ã™Ä¥Ã˜ÂªÃ™Ä¬Ã˜Â±'</code></p>
","15025011","","15025011","","2021-01-26 18:02:05","2021-01-26 18:02:05","Why using GPT2Tokenizer convert Arabic characters to symbols?","<pytorch><tokenize><huggingface-transformers><huggingface-tokenizers><gpt-2>","0","3","","","","CC BY-SA 4.0"
"64631124","1","","","2020-11-01 11:01:35","","1","229","<p>I load the Roberta model by TFRobertaModel.frompretrained('Roberta-base') and train it using Keras. I have other layers on top of the Roberta and I need to initialize the bare Roberta with all parameters. I run my code on Colab, and since a few weeks age when loading the Roberta I used to receive the following warning, but still, everything was ok and the model was training properly although â€˜lm_headâ€™ weights were not initialized:</p>
<pre><code>Some weights of the model checkpoint at Roberta-base were not used when initializing ROBERTA: [â€˜lm_headâ€™]
</code></pre>
<p>but now, I think the version of transformers on colab has been changed because I get the new warning with the same code, indicating more encoder and bias layers are not initialized and this leads to no decrease in loss function:</p>
<pre><code>Some layers from the model checkpoint at roberta-base were not used when initializing ROBERTA: ['lm_head', 'encoder/layer_._3/attention/self/value/bias:0', 'encoder/layer_._10/attention/self/value/bias:0', 'encoder/layer_._10/attention/self/key/kernel:0', 'pooler/dense/bias:0', 'encoder/layer_._9/attention/self/query/kernel:0', 'encoder/layer_._10/attention/self/query/kernel:0', 'encoder/layer_._7/attention/output/dense/bias:0', 'embeddings/position_embeddings/embeddings:0', 'encoder/layer_._6/intermediate/dense/kernel:0', 'encoder/layer_._11/intermediate/dense/kernel:0', 'encoder/layer_._8/intermediate/dense/bias:0', 'encoder/layer_._10/attention/self/value/kernel:0', 'encoder/layer_._7/output/dense/bias:0', 'encoder/layer_._6/attention/self/value/bias:0', 'encoder/layer_._8/attention/output/dense/kernel:0', 'encoder/layer_._10/intermediate/dense/kernel:0', 'encoder/layer_._5/attention/self/value/kernel:0', 'encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'encoder/layer_._7/attention/self/query/kernel:0', 'encoder/layer_._6/attention/self/query/kernel:0', 'encoder/layer_._6/attention/self/key/bias:0', 'encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'encoder/layer_._2/output/dense/kernel:0', 'encoder/layer_._11/intermediate/dense/bias:0', 'encoder/layer_._6/output/dense/kernel:0', 'encoder/layer_._2/intermediate/dense/kernel:0', 'encoder/layer_._3/intermediate/dense/kernel:0', 'encoder/layer_._10/output/LayerNorm/beta:0', 'encoder/layer_._6/attention/self/query/bias:0', 'encoder/layer_._6/attention/output/LayerNorm/beta:0', 'encoder/layer_._9/attention/self/value/bias:0', 'encoder/layer_._8/attention/self/query/kernel:0', 'encoder/layer_._0/output/LayerNorm/gamma:0', 'encoder/layer_._11/attention/output/dense/bias:0', 'encoder/layer_._7/attention/self/value/bias:0', 'encoder/layer_._0/attention/output/dense/kernel:0', 'encoder/layer_._9/intermediate/dense/bias:0', 'encoder/layer_._2/attention/self/query/kernel:0', 'encoder/layer_._0/attention/self/key/bias:0', 'encoder/layer_._8/attention/output/LayerNorm/beta:0', 'encoder/layer_._1/attention/self/value/kernel:0', 'encoder/layer_._6/output/LayerNorm/gamma:0', 'encoder/layer_._1/attention/output/dense/bias:0', 'encoder/layer_._3/attention/self/query/bias:0', 'encoder/layer_._3/output/dense/bias:0', 'encoder/layer_._1/attention/self/key/kernel:0', 'encoder/layer_._8/attention/self/key/kernel:0', 'encoder/layer_._9/intermediate/dense/kernel:0', 'encoder/layer_._3/output/dense/kernel:0', 'encoder/layer_._2/output/LayerNorm/beta:0', 'encoder/layer_._7/attention/self/key/bias:0', 'encoder/layer_._5/attention/self/key/kernel:0', 'encoder/layer_._5/attention/self/query/bias:0', 'encoder/layer_._2/attention/output/dense/bias:0', 'encoder/layer_._4/intermediate/dense/kernel:0', 'encoder/layer_._1/intermediate/dense/bias:0', 'encoder/layer_._4/attention/self/value/kernel:0', 'encoder/layer_._11/attention/self/key/bias:0', 'encoder/layer_._5/output/dense/kernel:0', 'encoder/layer_._1/output/dense/bias:0', 'encoder/layer_._0/attention/self/value/bias:0', 'encoder/layer_._6/attention/self/key/kernel:0', 'encoder/layer_._9/attention/self/key/bias:0', 'encoder/layer_._7/output/LayerNorm/gamma:0', 'encoder/layer_._8/attention/output/dense/bias:0', 'encoder/layer_._10/attention/output/dense/bias:0', 'encoder/layer_._0/intermediate/dense/kernel:0', 'encoder/layer_._5/intermediate/dense/kernel:0', 'encoder/layer_._11/attention/self/value/kernel:0', 'encoder/layer_._8/attention/self/key/bias:0', 'encoder/layer_._8/output/dense/bias:0', 'encoder/layer_._8/intermediate/dense/kernel:0', 'encoder/layer_._7/attention/output/LayerNorm/beta:0', 'encoder/layer_._2/output/dense/bias:0', 'encoder/layer_._3/attention/output/dense/bias:0', 'encoder/layer_._0/output/dense/bias:0', 'encoder/layer_._9/attention/self/key/kernel:0', 'encoder/layer_._11/output/dense/bias:0', 'encoder/layer_._7/attention/self/query/bias:0', 'encoder/layer_._10/attention/self/key/bias:0', 'encoder/layer_._2/attention/output/dense/kernel:0', 'encoder/layer_._2/attention/self/query/bias:0', 'encoder/layer_._9/attention/output/dense/kernel:0', 'encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'encoder/layer_._9/output/LayerNorm/gamma:0', 'encoder/layer_._0/attention/output/LayerNorm/beta:0', 'encoder/layer_._1/intermediate/dense/kernel:0', 'encoder/layer_._1/output/dense/kernel:0', 'encoder/layer_._1/attention/self/key/bias:0', 'encoder/layer_._2/attention/self/value/kernel:0', 'encoder/layer_._9/attention/self/value/kernel:0', 'encoder/layer_._10/intermediate/dense/bias:0', 'encoder/layer_._4/intermediate/dense/bias:0', 'encoder/layer_._6/output/LayerNorm/beta:0', 'encoder/layer_._7/output/LayerNorm/beta:0', 'encoder/layer_._11/attention/self/query/bias:0', 'encoder/layer_._0/intermediate/dense/bias:0', 'encoder/layer_._11/attention/output/dense/kernel:0', 'encoder/layer_._5/attention/self/query/kernel:0', 'encoder/layer_._8/attention/self/value/kernel:0', 'encoder/layer_._11/output/LayerNorm/beta:0', 'encoder/layer_._9/output/dense/bias:0', 'encoder/layer_._4/output/dense/bias:0', 'encoder/layer_._2/attention/self/key/bias:0', 'encoder/layer_._3/attention/self/query/kernel:0', 'encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'encoder/layer_._1/attention/output/LayerNorm/beta:0', 'encoder/layer_._1/output/LayerNorm/beta:0', 'encoder/layer_._10/attention/output/LayerNorm/beta:0', 'encoder/layer_._3/attention/self/value/kernel:0', 'encoder/layer_._10/attention/self/query/bias:0', 'encoder/layer_._3/attention/self/key/bias:0', 'pooler/dense/kernel:0', 'encoder/layer_._1/attention/self/value/bias:0', 'encoder/layer_._7/attention/self/key/kernel:0', 'encoder/layer_._1/attention/output/dense/kernel:0', 'encoder/layer_._4/attention/self/key/kernel:0', 'encoder/layer_._8/output/dense/kernel:0', 'encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'encoder/layer_._0/attention/self/value/kernel:0', 'encoder/layer_._3/attention/self/key/kernel:0', 'encoder/layer_._0/attention/self/query/kernel:0', 'encoder/layer_._3/intermediate/dense/bias:0', 'encoder/layer_._7/output/dense/kernel:0', 'encoder/layer_._10/output/dense/kernel:0', 'encoder/layer_._7/intermediate/dense/bias:0', 'embeddings/word_embeddings/weight:0', 'encoder/layer_._3/attention/output/LayerNorm/beta:0', 'encoder/layer_._0/attention/self/key/kernel:0', 'encoder/layer_._4/output/dense/kernel:0', 'encoder/layer_._5/output/LayerNorm/gamma:0', 'encoder/layer_._9/attention/output/dense/bias:0', 'encoder/layer_._0/attention/output/dense/bias:0', 'encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'encoder/layer_._9/attention/output/LayerNorm/beta:0', 'encoder/layer_._11/output/LayerNorm/gamma:0', 'encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'encoder/layer_._6/intermediate/dense/bias:0', 'encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'encoder/layer_._5/output/dense/bias:0', 'encoder/layer_._0/output/dense/kernel:0', 'encoder/layer_._6/attention/output/dense/kernel:0', 'encoder/layer_._6/attention/output/dense/bias:0', 'encoder/layer_._1/attention/self/query/kernel:0', 'encoder/layer_._0/attention/self/query/bias:0', 'encoder/layer_._11/attention/self/value/bias:0', 'encoder/layer_._2/intermediate/dense/bias:0', 'embeddings/LayerNorm/beta:0', 'encoder/layer_._4/attention/output/dense/kernel:0', 'encoder/layer_._3/output/LayerNorm/beta:0', 'encoder/layer_._8/output/LayerNorm/gamma:0', 'encoder/layer_._10/attention/output/dense/kernel:0', 'encoder/layer_._11/output/dense/kernel:0', 'encoder/layer_._2/attention/output/LayerNorm/beta:0', 'encoder/layer_._7/attention/output/dense/kernel:0', 'encoder/layer_._9/attention/self/query/bias:0', 'encoder/layer_._4/attention/self/key/bias:0', 'encoder/layer_._2/output/LayerNorm/gamma:0', 'encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'encoder/layer_._1/attention/self/query/bias:0', 'encoder/layer_._5/attention/output/LayerNorm/beta:0', 'encoder/layer_._10/output/dense/bias:0', 'encoder/layer_._8/output/LayerNorm/beta:0', 'encoder/layer_._5/output/LayerNorm/beta:0', 'embeddings/token_type_embeddings/embeddings:0', 'encoder/layer_._5/attention/output/dense/bias:0', 'encoder/layer_._4/output/LayerNorm/beta:0', 'encoder/layer_._4/attention/self/query/kernel:0', 'encoder/layer_._5/attention/output/dense/kernel:0', 'encoder/layer_._7/attention/self/value/kernel:0', 'encoder/layer_._7/intermediate/dense/kernel:0', 'encoder/layer_._11/attention/self/key/kernel:0', 'encoder/layer_._3/output/LayerNorm/gamma:0', 'encoder/layer_._10/output/LayerNorm/gamma:0', 'encoder/layer_._8/attention/self/query/bias:0', 'encoder/layer_._3/attention/output/dense/kernel:0', 'encoder/layer_._4/output/LayerNorm/gamma:0', 'encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'encoder/layer_._4/attention/self/value/bias:0', 'encoder/layer_._11/attention/self/query/kernel:0', 'encoder/layer_._4/attention/output/dense/bias:0', 'encoder/layer_._4/attention/output/LayerNorm/beta:0', 'encoder/layer_._5/attention/self/key/bias:0', 'encoder/layer_._6/attention/self/value/kernel:0', 'encoder/layer_._5/attention/self/value/bias:0', 'encoder/layer_._11/attention/output/LayerNorm/beta:0', 'encoder/layer_._1/output/LayerNorm/gamma:0', 'encoder/layer_._2/attention/self/value/bias:0', 'encoder/layer_._9/output/dense/kernel:0', 'encoder/layer_._2/attention/self/key/kernel:0', 'encoder/layer_._9/output/LayerNorm/beta:0', 'encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'encoder/layer_._5/intermediate/dense/bias:0', 'embeddings/LayerNorm/gamma:0', 'encoder/layer_._0/output/LayerNorm/beta:0', 'encoder/layer_._6/output/dense/bias:0', 'encoder/layer_._8/attention/self/value/bias:0', 'encoder/layer_._4/attention/self/query/bias:0']
</code></pre>
<p>can anyone help me with my question: How I can load the Roberta and initialize all of its weights properly?</p>
","10696048","","6664872","","2020-11-02 20:11:08","2021-05-12 16:37:02","Load Roberta model with all weights","<python><tensorflow><keras><huggingface-transformers><roberta-language-model>","0","2","","","","CC BY-SA 4.0"
"62907901","1","","","2020-07-15 05:01:53","","0","1112","<p>Currently I am using transformers(3.0.2) and python(3.7.3) which encountered the below error:</p>
<blockquote>
<p><strong>cannot import name 'TFBertForQuestionAnswering' from 'transformers'</strong></p>
</blockquote>
<pre><code>from transformers import BertTokenizer, TFBertForQuestionAnswering

model = TFBertForQuestionAnswering.from_pretrained('bert-base-cased')
f = open(model_path, &quot;wb&quot;)
pickle.dump(model, f)
</code></pre>
<p>How do resolve this issue?</p>
","2269222","","","","","2021-04-20 08:13:09","cannot import name 'TFBertForQuestionAnswering' from 'transformers'","<python><pip><huggingface-transformers><question-answering>","2","1","","","","CC BY-SA 4.0"
"63434097","1","","","2020-08-16 07:06:24","","0","296","<p>How to use Huggingface create_optimizer method ?
My code is as follows:</p>
<pre><code>import tensorflow as tf
from transformers import RobertaConfig, TFRobertaForMaskedLM, create_optimizer
config = RobertaConfig()  
optimizer,lr = create_optimizer(1e-4,1000000,10000,0.1,1e-6,0.01)
training_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model = TFRobertaForMaskedLM(config)
model.compile(optimizer=optimizer, loss=training_loss)
input = tf.random.uniform(shape=[1,25], maxval=100, dtype=tf.int32)
hist = model.fit(input, input, epochs=1, steps_per_epoch=1,verbose=0)
</code></pre>
<p>I am getting an error:</p>
<blockquote>
<p>TypeError: apply_gradients() got an unexpected keyword argument
'experimental_aggregate_gradients'</p>
</blockquote>
<p>I tried with tensorflow 2.3.0 and 2.2.0, transformers 3.0.2.</p>
","5561472","","","","","2020-08-20 09:22:35","Why Huggingface create_optimizer method not working?","<tensorflow><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63218778","1","","","2020-08-02 17:02:08","","3","1155","
<p>I am relatively new to PyTorch and Huggingface-transformers and experimented with DistillBertForSequenceClassification on this <a href=""https://www.kaggle.com/c/nlp-getting-started"" rel=""nofollow noreferrer"">Kaggle-Dataset</a>.</p>
<pre><code>from transformers import DistilBertForSequenceClassification
import torch.optim as optim
import torch.nn as nn
from transformers import get_linear_schedule_with_warmup

n_epochs = 5 # or whatever
batch_size = 32 # or whatever

bert_distil = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')
#bert_distil.classifier = nn.Sequential(nn.Linear(in_features=768, out_features=1), nn.Sigmoid())
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(bert_distil.parameters(), lr=0.1)

X_train = []
Y_train = []

for row in train_df.iterrows():
    seq = tokenizer.encode(preprocess_text(row[1]['text']),  add_special_tokens=True, pad_to_max_length=True)
    X_train.append(torch.tensor(seq).unsqueeze(0))
    Y_train.append(torch.tensor([row[1]['target']]).unsqueeze(0))
X_train = torch.cat(X_train)
Y_train = torch.cat(Y_train)

running_loss = 0.0
bert_distil.cuda()
bert_distil.train(True)
for epoch in range(n_epochs):
    permutation = torch.randperm(len(X_train))
    j = 0
    for i in range(0,len(X_train), batch_size):
        optimizer.zero_grad()
        indices = permutation[i:i+batch_size]
        batch_x, batch_y = X_train[indices], Y_train[indices]
        batch_x.cuda()
        batch_y.cuda()
        outputs = bert_distil.forward(batch_x.cuda())
        loss = criterion(outputs[0],batch_y.squeeze().cuda())
        loss.requires_grad = True
   
        loss.backward()
        optimizer.step()
   
        running_loss += loss.item()  
        j+=1
        if j == 20:   
            #print(outputs[0])
            print('[%d, %5d] running loss: %.3f loss: %.3f ' %
              (epoch + 1, i*1, running_loss / 20, loss.item()))
            running_loss = 0.0
            j = 0
</code></pre>
<blockquote>
<p>[1,   608] running loss: 0.689 loss: 0.687
[1,  1248] running loss: 0.693 loss: 0.694
[1,  1888] running loss: 0.693 loss: 0.683
[1,  2528] running loss: 0.689 loss: 0.701
[1,  3168] running loss: 0.690 loss: 0.684
[1,  3808] running loss: 0.689 loss: 0.688
[1,  4448] running loss: 0.689 loss: 0.692  etc...</p>
</blockquote>
<p>Regardless on what I tried, loss did never decrease, or even increase, nor did the prediction get better. It seems to me that I forgot something so that weights are actually not updated. Someone has an idea?
O</p>
<p><strong>what I tried</strong></p>
<ul>
<li>Different loss functions
<ul>
<li>BCE</li>
<li>CrossEntropy</li>
<li>even MSE-loss</li>
</ul>
</li>
<li>One-Hot Encoding vs A single neuron output</li>
<li>Different learning rates, and optimizers</li>
<li>I even changed all the targets to only one single label, but even then, the network did'nt converge.</li>
</ul>
","4764124","","","","","2021-02-25 03:29:12","Fine-Tuning DistilBertForSequenceClassification: Is not learning, why is loss not changing? Weights not updated?","<nlp><pytorch><text-classification><loss-function><huggingface-transformers>","3","4","3","","","CC BY-SA 4.0"
"60867353","1","60989317","","2020-03-26 12:32:02","","1","1707","<p><strong>Situation</strong>:  I am currently working on visualizing the results of a huggingface transformers machine learning model I have been building using the <a href=""https://github.com/marcotcr/lime"" rel=""nofollow noreferrer"">LIME package</a> following <a href=""https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html"" rel=""nofollow noreferrer"">this tutorial</a>. </p>

<p><strong>Complication</strong>: My code is set up and runs well until I create the LIME <em>explainer</em> object. At this point I get a memory error.</p>

<p><strong>Question</strong>: What am I doing wrong? Why am I running into a memory error?</p>

<p><strong>Code</strong>: Here is my code (you should be able to just copy-paste this into google colab and run the whole thing)</p>

<pre class=""lang-py prettyprint-override""><code>########################## LOAD PACKAGES ######################
# Install new packages in our environment
!pip install lime
!pip install wget
!pip install transformers

# Import general libraries
import sklearn
import sklearn.ensemble
import sklearn.metrics
import numpy as np
import pandas as pd

# Import libraries specific to this notebook
import lime
import wget
import os
from __future__ import print_function
from transformers import FeatureExtractionPipeline, BertModel, BertTokenizer, BertConfig
from lime.lime_text import LimeTextExplainer

# Let the notebook know to plot inline
%matplotlib inline

########################## LOAD DATA ##########################
# Get URL
url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'

# Download the file (if we haven't already)
if not os.path.exists('./cola_public_1.1.zip'):
    wget.download(url, './cola_public_1.1.zip')

# Unzip the dataset (if we haven't already)
if not os.path.exists('./cola_public/'):
    !unzip cola_public_1.1.zip

# Load the dataset into a pandas dataframe.
df_cola = pd.read_csv(""./cola_public/raw/in_domain_train.tsv"", delimiter='\t', 
                      header=None, names=['sentence_source', 'label', 
                                          'label_notes', 'sentence'])

# Only look at the first 50 observations for debugging
df_cola = df_cola.head(50)

###################### TRAIN TEST SPLIT ######################
# Apply the train test split
x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(
    df_cola.sentence, df_cola.label, test_size=0.2, random_state=42
)

###################### CREATE LIME CLASSIFIER ######################
# Create a function to extract vectors from a single sentence
def vector_extractor(sentence):

    # Create a basic BERT model, config and tokenizer for the pipeline
    configuration = BertConfig()
    configuration.max_len = 64
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',
                                              do_lower_case=True, 
                                              max_length=64,
                                              pad_to_max_length=True)
    model = BertModel.from_pretrained('bert-base-uncased',config=configuration)

    # Create the pipeline
    vector_extractor = FeatureExtractionPipeline(model=model,
                                                 tokenizer=tokenizer,
                                                 device=0)

    # The pipeline gives us all tokens in the final layer - we want the CLS token
    vector = vector_extractor(sentence)
    vector = vector[0][0]

    # Return the vector
    return vector

# Adjust the format of our sentences (from pandas series to python list)
x_train = x_train.values.tolist()
x_test = x_test.values.tolist()

# First we vectorize our train features for the classifier
x_train_vectorized = [vector_extractor(x) for x in x_train]

# Create and fit the random forest classifier
rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100)
rf.fit(x_train_vectorized, y_train)

# Define the lime_classifier function
def lime_classifier(sentences): 

    # Turn all the sentences into vectors
    vectors = [vector_extractor(x) for x in sentences]

    # Get predictions for all 
    predictions = rf.predict_proba(vectors)

    # Return the probabilies as a 2D-array
    return predictions  

########################### APPLY LIME ##########################
# Create the general explainer object
explainer = LimeTextExplainer()

# ""Fit"" the explainer object to a specific observation
exp = explainer.explain_instance(x_test[1], 
                                 lime_classifier, 
                                 num_features=6)
</code></pre>
","4465454","","","","","2020-04-02 10:21:25","Using LIME for BERT transformer visualization results in memory error","<python><machine-learning><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63113763","1","","","2020-07-27 10:50:41","","1","261","<p>I want to use hidden_states of bert as an input for the next layer and build it using keras.Model. But bert returns only last layer and pooler output.</p>
<p>Here's the code that I've tried:</p>
<pre><code>def _def_input():
    input_ids = Input(batch_shape=(None, 256), name='input_ids', dtype='int32')
    input_type_ids = Input(batch_shape=(None, 256), name='input_type_ids', dtype='int32')
    attention_mask = Input(batch_shape=(None, 256), name='attention_mask', dtype='int32')

    return [input_ids, input_type_ids, attention_mask]

config = BertConfig.from_pretrained(&quot;bert-base-multilingual-cased&quot;, output_hidden_states=True)
model = TFBertModel.from_pretrained(&quot;bert-base-multilingual-cased&quot;, config=config)
inputs = _def_input()

out = model({'input_ids': inputs[0],
                 'token_type_ids': inputs[1],
                 'attention_mask': inputs[2]})
print(f'Out len: {len(out)}')
print(f'Out: {out}')
print(model.config)
</code></pre>
<p>And here's the output:</p>
<pre><code>Out len: 2
Out: (&lt;tf.Tensor 'tf_bert_model_10/Identity:0' shape=(None, 256, 768) dtype=float32&gt;, &lt;tf.Tensor 'tf_bert_model_10/Identity_1:0' shape=(None, 768) dtype=float32&gt;)
BertConfig {
  &quot;architectures&quot;: [
    &quot;BertForMaskedLM&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;directionality&quot;: &quot;bidi&quot;,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;output_hidden_states&quot;: true,
  &quot;pad_token_id&quot;: 0,
  &quot;pooler_fc_size&quot;: 768,
  &quot;pooler_num_attention_heads&quot;: 12,
  &quot;pooler_num_fc_layers&quot;: 3,
  &quot;pooler_size_per_head&quot;: 128,
  &quot;pooler_type&quot;: &quot;first_token_transform&quot;,
  &quot;type_vocab_size&quot;: 2,
  &quot;vocab_size&quot;: 119547
}
</code></pre>
","7947467","","","","","2020-08-17 22:04:47","How to use TFBertModel's hidden states as part of custom Keras model?","<python><tensorflow><keras><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63323464","1","","","2020-08-09 07:06:02","","2","908","<p>I got confused by which hidden state should I use as the output of fine-tuned Roberta transformer models.</p>
<pre><code>from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer
config = AutoConfig.from_pretrained(&quot;roberta-base&quot;)
config.output_hidden_states = True

tok = AutoTokenizer.from_pretrained(&quot;roberta-base&quot;)
model = AutoModelForMaskedLM.from_pretrained(&quot;roberta-base&quot;, config=config)

inp = &quot;alright let s do this  &quot;

sentence = tok.encode(inp, padding='max_length', max_length=512, truncation=True, return_tensors='pt')

output = model(sentence)
</code></pre>
<p>According to Huggingface documentation for <code>RobertaForMaskedLM</code>:</p>
<p>Returns a tuple of:</p>
<ol>
<li>masked_lm_loss (optional)</li>
<li>prediction_scores</li>
<li>hidden_states (optional)</li>
<li>attentions (optional)</li>
</ol>
<p>By passing the config to enable hidden_states output, the <code>output</code> is a tuple of (<code>prediction_scores</code>, <code>hidden_states</code>)</p>
<p>My question is:
should I use <code>output[-1][0]</code> or <code>output[-1][-1]</code> as the final output embedding from the fine-tuned Roberta Model? My understanding is that <code>output[-1][0]</code> is the initial embedding feeding into the Roberta Model, and <code>output[-1][-1]</code> is the final embedding output.</p>
","8787039","","","","","2020-08-10 04:14:48","how to get the correct embedding from Roberta transformers?","<bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63221913","1","63232334","","2020-08-02 23:20:14","","7","2708","<p>I'm looking at the documentation for <a href=""https://huggingface.co/transformers/task_summary.html#named-entity-recognition"" rel=""noreferrer"">Huggingface pipeline for Named Entity Recognition</a>, and it's not clear to me how these results are meant to be used in an actual entity recognition model.</p>
<p>For instance, given the example in documentation:</p>
<pre><code>&gt;&gt;&gt; from transformers import pipeline

&gt;&gt;&gt; nlp = pipeline(&quot;ner&quot;)

&gt;&gt;&gt; sequence = &quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot;
...            &quot;close to the Manhattan Bridge which is visible from the window.&quot;

This outputs a list of all words that have been identified as an entity from the 9 classes     defined above. Here is the expected results:

print(nlp(sequence))

[
{'word': 'Hu', 'score': 0.9995632767677307, 'entity': 'I-ORG'},
{'word': '##gging', 'score': 0.9915938973426819, 'entity': 'I-ORG'},
{'word': 'Face', 'score': 0.9982671737670898, 'entity': 'I-ORG'},
{'word': 'Inc', 'score': 0.9994403719902039, 'entity': 'I-ORG'},
{'word': 'New', 'score': 0.9994346499443054, 'entity': 'I-LOC'},
{'word': 'York', 'score': 0.9993270635604858, 'entity': 'I-LOC'},
{'word': 'City', 'score': 0.9993864893913269, 'entity': 'I-LOC'},
{'word': 'D', 'score': 0.9825621843338013, 'entity': 'I-LOC'},
{'word': '##UM', 'score': 0.936983048915863, 'entity': 'I-LOC'},
{'word': '##BO', 'score': 0.8987102508544922, 'entity': 'I-LOC'},
{'word': 'Manhattan', 'score': 0.9758241176605225, 'entity': 'I-LOC'},
{'word': 'Bridge', 'score': 0.990249514579773, 'entity': 'I-LOC'}
]
</code></pre>
<p>While this alone is impressive, it isn't clear to me the correct way to get  &quot;DUMBO&quot; from:</p>
<pre><code>{'word': 'D', 'score': 0.9825621843338013, 'entity': 'I-LOC'},
{'word': '##UM', 'score': 0.936983048915863, 'entity': 'I-LOC'},
{'word': '##BO', 'score': 0.8987102508544922, 'entity': 'I-LOC'},
</code></pre>
<p>---or even to the cleaner multiple token matches, like distinguishing &quot;New York City&quot; from simply the city of &quot;York.&quot;</p>
<p>While I can imagine heuristic methods, what's the correct intended way to join these tokens back into correct labels given your inputs?</p>
","1052117","","","","","2020-08-03 15:26:54","Named Entity Recognition with Huggingface transformers, mapping back to complete entities","<huggingface-transformers>","1","1","1","","","CC BY-SA 4.0"
"63312859","1","63314437","","2020-08-08 07:28:58","","6","7228","<p>The default cache directory is lack of disk capacity, I need change the configure of the default cache directory.</p>
","2281101","","","","","2020-10-21 07:06:43","How to change huggingface transformers default cache directory","<huggingface-transformers>","2","0","2","","","CC BY-SA 4.0"
"60868295","1","","","2020-03-26 13:23:38","","1","446","<p>We are interested in the bert vectors for each token. With bert vector we mean the word vector for a specific token in berts output layer. So we would like to find out which token produces which bert vector. We wrote some code but we are not sure if it is correct or how to test it.</p>

<p>So in the code we process a sentence with bert. We construct a list of position ids and hand them to the model. Afterwards we use the same position ids to map the tokens to the output layer. Then there is some code that produces calculates the character offsets of each vector in the input sentence.</p>

<p>Is this the correct way how to use position_ids to generate </p>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertModel, BertConfig, BertTokenizer
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

def sentence_to_vector(input_sentence):
    tokens_encoded = tokenizer.encode(input_sentence, add_special_tokens=True)
    input_ids = torch.tensor(tokens_encoded).unsqueeze(0)  # Batch size 1

    seq_length = input_ids.size(1)

    # code to construct position_ids from here: 
    # https://github.com/huggingface/transformers/blob/8da280ebbeca5ebd7561fd05af78c65df9161f92/pytorch_pretrained_bert/modeling.py#L188:L189
    position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)  
    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
    outputs = model(input_ids, position_ids=position_ids)

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    # from the BertModel documentation (example at the bottom):
    # The last hidden-state is the first element of the output tuple
    # https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel

    #ttv = {}  # token to vector
    #for i in position_ids[0]:
    #    ttv[tokens[i]] = outputs[0][0][position_ids[0][i]]

    data = []
    last_offset = 0
    for i in range(0, len(position_ids[0])):
        token = tokens[position_ids[0][i]]
        vector = outputs[0][0][position_ids[0][i]]
        pos_begin = None
        pos_end = None
        if not token == ""[CLS]"" and not token == ""[SEP]"":
            pos_begin = input_sentence.find(token, last_offset)
            pos_end = pos_begin + len(token)
            last_offset = pos_end
        data.append({
            ""token"": token,
            ""pos_begin"": pos_begin,
            ""pos_end"": pos_end,
            ""vector"": vector
        })
    return data

input_sentence = ""do the chicken dance!""
data = sentence_to_vector(input_sentence)

for token in data:
    print(token[""token""] + ""\t"" + str(token[""pos_begin""]) + ""\t"" + str(token[""pos_end""]) + ""\t"" + str(token[""vector""][0:3]) + ""..."" )
</code></pre>
","5271642","","3607203","","2020-03-26 15:15:29","2020-03-26 15:15:29","Get position of token in berts output layer","<huggingface-transformers>","0","0","1","","","CC BY-SA 4.0"
"63633633","1","","","2020-08-28 12:16:06","","1","309","<p>A few weeks ago, the following ran successfully:</p>
<pre><code>bert_model = BertModel.from_pretrained(&quot;bert-base-cased&quot;, from_tf=True)
</code></pre>
<p>I've just re-run the exact same code and I'm now getting this error:</p>
<blockquote>
<p>OSError: Unable to open file (file signature not found)</p>
</blockquote>
<p>I tried adding <code>, force_download=True</code> to the parameters but still get the same error.</p>
<p>What's causing this and what do I need to change to fix it?</p>
","9751001","","","","","2021-05-11 12:59:04","""OSError: Unable to open file (file signature not found)"" trying to run ""bert_model = BertModel.from_pretrained(""bert-base-cased"", from_tf=True)""","<python><bert-language-model><huggingface-transformers>","0","4","1","","","CC BY-SA 4.0"
"60942088","1","","","2020-03-31 01:32:14","","0","282","<p>I am using run_squad.py <a href=""https://github.com/huggingface/transformers/blob/master/examples/run_squad.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/examples/run_squad.py</a> from Huggingface Transformers for fine-tuning on BioASQ Question Answering dataset.</p>

<p>I have converted the tensorflow weights provided by the authors of BioBERT <a href=""https://github.com/dmis-lab/bioasq-biobert"" rel=""nofollow noreferrer"">https://github.com/dmis-lab/bioasq-biobert</a> to Pytorch as discussed here <a href=""https://github.com/huggingface/transformers/issues/312"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/312</a>. </p>

<p>Further, I am using the preprocessed data of BioASQ <a href=""https://github.com/dmis-lab/bioasq-biobert"" rel=""nofollow noreferrer"">https://github.com/dmis-lab/bioasq-biobert</a> which is converted to the SQuAD form.
However, when I am running the run_squad.py script with the below parameters</p>

<pre><code> --model_type bert \
  --model_name_or_path /scratch/oe7/uk1594/BioBERT/BioBERT-PyTorch/BioBERTv1.1-SQuADv1.1-Factoid-PyTorch/ \
  --do_train \
  --do_eval \
  --save_steps 1000 \
  --train_file $data/BioASQ-train-factoid-6b.json \
  --predict_file $data/BioASQ-test-factoid-6b-1.json \
  --per_gpu_train_batch_size 12 \
  --learning_rate 3e-5 \
  --num_train_epochs 2.0 \
  --max_seq_length 384 \
  --doc_stride 128 \
  --output_dir /scratch/oe7/uk1594/BioBERT/BioBERT-PyTorch/QA_output_squad/BioASQ-factoid-6b/BioASQ-factoid-6b-1-issue-23mar/


I get the below error:

03/23/2020 12:53:12 - INFO - transformers.modeling_utils -   loading weights file /scratch/oe7/uk1594/BioBERT/BioBERT-PyTorch/QA_output_squad/BioASQ-factoid-6b/BioASQ-factoid-6b-1-issue-23mar/pytorch_model.bin
03/23/2020 12:53:15 - INFO - __main__ -   Creating features from dataset file at .

  0%|          | 0/1 [00:00&lt;?, ?it/s]
  0%|          | 0/1 [00:00&lt;?, ?it/s]
Traceback (most recent call last):
  File ""run_squad.py"", line 856, in &lt;module&gt;
    main()
  File ""run_squad.py"", line 845, in main
    result = evaluate(args, model, tokenizer, prefix=global_step)
  File ""run_squad.py"", line 299, in evaluate
    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)
  File ""run_squad.py"", line 475, in load_and_cache_examples
    examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)
  File ""/scratch/oe7/uk1594/lib/python3.7/site-packages/transformers/data/processors/squad.py"", line 522, in get_dev_examples
    return self._create_examples(input_data, ""dev"")
  File ""/scratch/oe7/uk1594/lib/python3.7/site-packages/transformers/data/processors/squad.py"", line 549, in _create_examples
    answers = qa[""answers""]
KeyError: 'answers'


</code></pre>

<p>Really appreciate your help.</p>

<p>Thanks a lot for your guidance.</p>

<p>The evaluaton dataset is looks like this:</p>

<pre><code>{
  ""version"": ""BioASQ6b"", 
  ""data"": [
    {
      ""title"": ""BioASQ6b"", 
      ""paragraphs"": [
        {
          ""context"": ""emMAW: computing minimal absent words in external memory. Motivation: The biological significance of minimal absent words has been investigated in genomes of organisms from all domains of life. For instance, three minimal absent words of the human genome were found in Ebola virus genomes"",
          ""qas"": [
            {
              ""question"": ""Which algorithm is available for computing minimal absent words using external memory?"", 
              ""id"": ""5a6a3335b750ff4455000025_000""
            }
          ]
        }
    ]
}
]
}



</code></pre>
","8720570","","8720570","","2020-04-02 07:41:37","2021-01-25 09:59:30","KeyError: 'answers' error when using BioASQ dataset using Huggingface Transformers","<tensorflow><pytorch><huggingface-transformers><question-answering><squad>","1","3","","","","CC BY-SA 4.0"
"60923314","1","60926442","","2020-03-30 02:42:25","","0","3264","<p>below is the result of my fine-tuning.</p>

<pre><code>Training Loss   Valid. Loss Valid. Accur.   Training Time   Validation Time
epoch                   
1   0.16    0.11    0.96    0:02:11 0:00:05
2   0.07    0.13    0.96    0:02:19 0:00:05
3   0.03    0.14    0.97    0:02:22 0:00:05
4   0.02    0.16    0.96    0:02:21 0:00:05
</code></pre>

<p>next i tried to use the model to predict labels from a csv file. i created a label column, set the type to int64 and run the prediction.</p>

<pre><code>print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))
model.eval()
# Tracking variables 
predictions , true_labels = [], []
# Predict 
for batch in prediction_dataloader:
  # Add batch to GPU
  batch = tuple(t.to(device) for t in batch)

  # Unpack the inputs from our dataloader
  b_input_ids, b_input_mask, b_labels = batch

  # Telling the model not to compute or store gradients, saving memory and 
  # speeding up prediction
  with torch.no_grad():
      # Forward pass, calculate logit predictions
      outputs = model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)

  logits = outputs[0]

  # Move logits and labels to CPU
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()

  # Store predictions and true labels
  predictions.append(logits)
  true_labels.append(label_ids)


</code></pre>

<p>however, while i am able to print out the predictions[4.235, -4.805] etc, and the true_labels[NaN,NaN.....], i am unable to actually get the predicted labels{0 or 1}. Am i missing something here?</p>
","13119815","","","","","2020-03-30 08:14:51","I fine tuned a pre-trained BERT for sentence classification, but i cant get it to predict for new sentences","<python><machine-learning><nlp><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61157314","1","61226181","","2020-04-11 13:09:53","","1","304","<p>I am running the following code on colab taken from the example here: <a href=""https://huggingface.co/transformers/model_doc/albert.html#albertformaskedlm"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/albert.html#albertformaskedlm</a></p>

<pre><code>import os
import torch
import torch_xla
import torch_xla.core.xla_model as xm

assert os.environ['COLAB_TPU_ADDR']

dev = xm.xla_device()

from transformers import AlbertTokenizer, AlbertForMaskedLM
import torch

tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = AlbertForMaskedLM.from_pretrained('albert-base-v2').to(dev)
input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True)).unsqueeze(0)  # Batch size 1

data = input_ids.to(dev)

outputs = model(data, masked_lm_labels=data)
loss, prediction_scores = outputs[:2]
</code></pre>

<p>I haven't done anything to the example code except move <code>input_ids</code> and <code>model</code> onto the TPU device using <code>.to(dev)</code>. It seems everything is moved to the TPU no problem as when I input <code>data</code> I get the following output: <code>tensor([[    2, 10975,    15,    51,  1952,    25, 10901,     3]], device='xla:1')</code></p>

<p>However when I run this code I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-5-f756487db8f7&gt; in &lt;module&gt;()
      1 
----&gt; 2 outputs = model(data, masked_lm_labels=data)
      3 loss, prediction_scores = outputs[:2]

9 frames
/usr/local/lib/python3.6/dist-packages/transformers/modeling_albert.py in forward(self, hidden_states, attention_mask, head_mask)
    277         attention_output = self.attention(hidden_states, attention_mask, head_mask)
    278         ffn_output = self.ffn(attention_output[0])
--&gt; 279         ffn_output = self.activation(ffn_output)
    280         ffn_output = self.ffn_output(ffn_output)
    281         hidden_states = self.full_layer_layer_norm(ffn_output + attention_output[0])

RuntimeError: Unknown device
</code></pre>

<p>Anyone know what's going on?</p>
","10613991","","","","","2020-04-15 10:27:01","RuntimeError: Unknown device when trying to run AlbertForMaskedLM on colab tpu","<nlp><pytorch><tpu><huggingface-transformers><tensorflow-xla>","1","0","","","","CC BY-SA 4.0"
"60982890","1","","","2020-04-02 01:27:21","","1","360","<p>I am trying to add a layer after the pretrained ALBERT model. I want to use the ALBERT pretrained model to generate tokens.</p>

<pre><code>import tensorflow as tf
from transformers import AlbertTokenizer, TFAlbertForMaskedLM


tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = TFAlbertForMaskedLM.from_pretrained('albert-base-v2')
input_ids = tf.constant(tokenizer.encode(""This is a test! Nice to meet you![MASK]love you""))[
    None, :]  # Batch size 1
print(input_ids)
outputs = model(input_ids)
prediction_scores = outputs[0]
outputTokens = tf.math.argmax(prediction_scores, axis=2)
outputTokens = tf.keras.backend.eval(outputTokens[0])
outputTokens = tokenizer.decode(outputTokens)
print(outputTokens)

tokenModel = tf.keras.Sequential([
    tf.keras.layers.Dense(128,input_shape=input_ids.shape),
    model(),
    tf.keras.layers.Softmax()
])
tokenModel.summary()

</code></pre>

<p>Which outputs:</p>

<pre><code>/test.py"", line 19, in &lt;module&gt;
    model(),
TypeError: __call__() missing 1 required positional argument: 'inputs'
</code></pre>

<p>What should be the model like? I can't add the ALBERT model as the first layer of the model in TensorFlow.</p>
","4843407","","","","","2020-04-02 01:27:21","How to add more layers to the Huggingface pretrained ALBERT model?","<tensorflow><huggingface-transformers>","0","1","2","","","CC BY-SA 4.0"
"61551797","1","61632692","","2020-05-01 22:29:44","","3","1590","<p>I am running <a href=""https://github.com/huggingface/transformers/blob/master/examples/run_glue.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/examples/run_glue.py</a> to perform finetuning on a binary classification task (CoLA). 
I'd like to monitor both the training and evaluation losses to prevent overfitting.</p>

<p>Currently the library is at 2.8.0, and I did the install from source.</p>

<p>When I run the example with </p>

<pre><code>python run_glue.py --model_name_or_path bert-base-uncased 
                   --task_name CoLA 
                   --do_train 
                   --do_eval
                   --data_dir my_dir 
                   --max_seq_length 128
                   --per_gpu_train_batch_size 8
                   --per_gpu_eval_batch_size 8 
                   --learning_rate 2e-5
                   --num_train_epochs 3.0
                   --output_dir ./outputs
                   --logging_steps 5
</code></pre>

<p>In the stdout logs I see lines with one single value for the loss, such as</p>

<blockquote>
  <p>{""learning_rate"": 3.3333333333333333e-06, ""loss"": 0.47537623047828675,
  ""step"": 25}</p>
</blockquote>

<p>By peeking in <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py</a> I see that training and evaluation losses are computed there (looks to me that code was recently refactored).</p>

<p>I have thus replaced <a href=""https://github.com/huggingface/transformers/blob/abb1fa3f374811ea09d0bc3440d820c50735008d/src/transformers/trainer.py#L314"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/abb1fa3f374811ea09d0bc3440d820c50735008d/src/transformers/trainer.py#L314</a> with</p>

<pre><code> cr_loss = self._training_step(model, inputs, optimizer)
 tr_loss += cr_loss
</code></pre>

<p>and added after line <a href=""https://github.com/huggingface/transformers/blob/abb1fa3f374811ea09d0bc3440d820c50735008d/src/transformers/trainer.py#L345"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/abb1fa3f374811ea09d0bc3440d820c50735008d/src/transformers/trainer.py#L345</a> </p>

<pre><code>logs[""training loss""] = cr_loss
</code></pre>

<p>with this I get:</p>

<pre><code>0502 14:12:18.644119 23632 summary.py:47] Summary name training loss is illegal; using training_loss instead.  
                          | 4/10  [00:02&lt;00:04,  1.49it/s]  
{""learning_rate"":  3.3333333333333333e-06, ""loss"": 0.47537623047828675, ""training loss"": 0.5451719760894775, ""step"": 25}
</code></pre>

<p>Is this OK, or am I doing anything wrong here? </p>

<p>What's the best way to monitor in stdout <em>both</em> the averaged training and evaluation loss for a given logging interval during finetuning? </p>
","4240413","","4240413","","2020-05-11 12:46:06","2020-10-24 11:12:53","How can I monitor both training and eval loss when finetuning BERT on a GLUE task?","<python><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61555097","1","","","2020-05-02 05:54:21","","4","3878","<p>I have my encode function that looks like this:</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertModel

MODEL = 'bert-base-multilingual-uncased'
tokenizer = BertTokenizer.from_pretrained(MODEL)

def encode(texts, tokenizer=tokenizer, maxlen=10):
#     import pdb; pdb.set_trace()
    inputs = tokenizer.encode_plus(
        texts,
        return_tensors='tf',
        return_attention_masks=True, 
        return_token_type_ids=True,
        pad_to_max_length=True,
        max_length=maxlen
    )

    return inputs['input_ids'], inputs[""token_type_ids""], inputs[""attention_mask""]
</code></pre>

<p>I want to get my data encoded on the fly by doing this:</p>

<pre class=""lang-py prettyprint-override""><code>x_train = (tf.data.Dataset.from_tensor_slices(df_train.comment_text.astype(str).values)
           .map(encode))
</code></pre>

<p>However, this chucks the error:</p>

<pre><code>ValueError: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.
</code></pre>

<p>Now from my understanding when I set a breakpoint inside <code>encode</code> it was because I was sending a non-numpy array. How do I get huggingface transformers to play nice with tensorflow strings as inputs?</p>

<p>If you need a dummy dataframe here it is:</p>

<pre><code>df_train = pd.DataFrame({'comment_text': ['Today was a good day']*5})
</code></pre>

<h2>What I tried</h2>

<p>So I tried to use <code>from_generator</code> so that I can parse in the strings to the <code>encode_plus</code> function. However, this does not work with TPUs.</p>

<pre class=""lang-py prettyprint-override""><code>AUTO = tf.data.experimental.AUTOTUNE

def get_gen(df):
    def gen():
        for i in range(len(df)):
            yield encode(df.loc[i, 'comment_text']) , df.loc[i, 'toxic']
    return gen

shapes = ((tf.TensorShape([maxlen]), tf.TensorShape([maxlen]), tf.TensorShape([maxlen])), tf.TensorShape([]))

train_dataset = tf.data.Dataset.from_generator(
    get_gen(df_train),
    ((tf.int32, tf.int32, tf.int32), tf.int32),
    shapes
)
train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(AUTO)
</code></pre>

<h2>Version Info:</h2>

<p><code>transformers.__version__, tf.__version__</code> => <code>('2.7.0', '2.1.0')</code></p>
","2530674","","2530674","","2020-05-03 05:03:24","2020-06-04 08:53:17","Mapping text data through huggingface tokenizer","<tensorflow><tensorflow-datasets><huggingface-transformers>","2","4","1","","","CC BY-SA 4.0"
"66664184","1","","","2021-03-16 22:16:27","","1","233","<p>I am trying to use transformers package in Python.  When I import it as follows</p>
<pre><code>from transformers import * 
</code></pre>
<p>I get the following error</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-1-52f597fbeb0b&gt; in &lt;module&gt;
----&gt; 1 from transformers import *

~\anaconda3\envs\tf\lib\site-packages\transformers\__init__.py in __getattr__(self, name)
   2306             if name == &quot;__version__&quot;:
   2307                 return __version__
   2308             return super().__getattr__(name)
   2309 
   2310     sys.modules[__name__] = _LazyModule(__name__, _import_structure)

~\anaconda3\envs\tf\lib\site-packages\transformers\file_utils.py in __getattr__(self, name)
   1655         elif name in self._class_to_module.keys():
   1656             module = self._get_module(self._class_to_module[name])
   1657             value = getattr(module, name)
   1658         else:
   1659             raise AttributeError(f&quot;module {self.__name__} has no attribute {name}&quot;)

~\anaconda3\envs\tf\lib\site-packages\transformers\file_utils.py in __getattr__(self, name)
   1657             value = getattr(module, name)
   1658         else:
   1659             raise AttributeError(f&quot;module {self.__name__} has no attribute {name}&quot;)
   1660 
   1661         setattr(self, name, value)

AttributeError: module transformers.models.ibert has no attribute IBertLayer
</code></pre>
<p>Is there any solution to fix this issue?</p>
","2350549","","","","","2021-03-16 22:16:27","AttributeError: module transformers.models.ibert has no attribute IBertLayer","<python><huggingface-transformers><summarization>","0","1","","","","CC BY-SA 4.0"
"60780181","1","60780342","","2020-03-20 19:02:08","","0","276","<p>I am trying to access the output embeddings from several different layers of the pretrained ""DistilBERT"" model. (""distilbert-base-uncased"")</p>

<pre><code>bert_output = model(input_ids, attention_mask=attention_mask)
</code></pre>

<p>The bert_output seems to return only the embedding values of the last layer for the input tokens.</p>
","7586741","","","","","2020-03-20 20:59:36","Access the output of several layers of pretrained DistilBERT model","<python><nlp><pytorch><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67457480","1","","","2021-05-09 12:05:21","","1","512","<p>I'm using the huggingface Trainer with BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;) model.</p>
<p>Simplified, it looks like this:</p>
<pre><code>model = BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;)
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)

training_args = TrainingArguments(
        output_dir=&quot;bert_results&quot;,
        num_train_epochs=3,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=32,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir=&quot;bert_results/logs&quot;,
        logging_steps=10
        )

trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
        )
</code></pre>
<p>The logs contain the loss for each 10 steps, but I can't seem to find the training accuracy. Does anyone know how to get the accuracy, for example by changing the verbosity of the logger? I can't seem to find anything about it online.</p>
<p>Thanks,
CptBaas</p>
","12456230","","","","","2021-05-12 20:32:49","How to get the accuracy per epoch or step for the huggingface.transformers Trainer?","<python><tensorflow><logging><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66693724","1","66702446","","2021-03-18 15:04:05","","4","281","<p>i'm trying to convert T5 model to onnx using the <a href=""https://github.com/Ki6an/fastT5"" rel=""nofollow noreferrer"">fastT5</a> library, but
getting an error while running the following code</p>
<pre><code>from fastT5 import export_and_get_onnx_model
from transformers import AutoTokenizer

model_name = 't5-small'
model = export_and_get_onnx_model(model_name)

tokenizer = AutoTokenizer.from_pretrained(model_name)
t_input = &quot;translate English to French: The universe is a dark forest.&quot;
token = tokenizer(t_input, return_tensors='pt')

tokens = model.generate(input_ids=token['input_ids'],
               attention_mask=token['attention_mask'],
               num_beams=2)

output = tokenizer.decode(tokens.squeeze(), skip_special_tokens=True)
print(output)
</code></pre>
<p>the error:</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:244: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if causal_mask.shape[1] &lt; attention_mask.shape[1]:
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-16-80094b7c4f6f&gt; in &lt;module&gt;()
      7                     input_names=decoder_input_names,
      8                     output_names=decoder_output_names,
----&gt; 9                     dynamic_axes=dyn_axis_params,
     10                     )

24 frames
/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py in forward(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)
    497                 position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)
    498 
--&gt; 499         scores += position_bias
    500         attn_weights = F.softmax(scores.float(), dim=-1).type_as(
    501             scores

RuntimeError: output with shape [5, 8, 1, 2] doesn't match the broadcast shape [5, 8, 2, 2]
</code></pre>
<p>can someone please help me solve the issue?
<br/>
thank you.</p>
","15151212","","15151212","","2021-03-24 17:41:44","2021-03-24 17:41:44","while exporting T5 model to onnx using fastT5 getting ""RuntimeError:output with shape [5, 8, 1, 2] doesn't match the broadcast shape [5, 8, 2, 2]""","<python><tensorflow><pytorch><huggingface-transformers><onnx>","1","0","","","","CC BY-SA 4.0"
"60904170","1","","","2020-03-28 17:16:37","","4","1326","<p>I want to fine tune a GPT-2 model using Huggingfaceâ€™s Transformers. Preferably the medium model but large if possible. Currently, I have a RTX 2080 Ti with 11GB of memory and I can train the small model just fine.</p>

<p>My question is: will I run into any issues if I added an old Tesla K80 (24GB) to my machine and distributed the training? I cannot find information about using different capacity GPUs during training and issues I could run into.</p>

<p>Will my model size limit essentially be sum of all available GPU memory? (35GB?)</p>

<p>Iâ€™m not interested in doing this in AWS.</p>
","107726","","","","","2020-03-28 17:16:37","Multi GPU training for Transformers with different GPUs","<machine-learning><huggingface-transformers>","0","4","1","","","CC BY-SA 4.0"
"63236864","1","","","2020-08-03 20:58:51","","0","382","<p>Say I have two tokenized BERT sequences:</p>
<pre class=""lang-py prettyprint-override""><code>seq1 = tensor([[ 101,  2023,  2003,  1996, 23032,   102]])
seq2 = tensor([[ 101, 2023, 2003, 6019, 1015,  102]])
</code></pre>
<p>This is produced with huggingface's tokenizer:</p>
<pre class=""lang-py prettyprint-override""><code>seq = torch.tensor(tokenizer.encode(text=query, add_special_tokens=True)).unsqueeze(0)
</code></pre>
<p>What is the best way to combined the tokenized sequences to get one final sequence, where the [sep] tokens are auto-incremented?</p>
<p>For example:</p>
<pre class=""lang-py prettyprint-override""><code>combined = tensor([[ 101,  2023,  2003,  1996, 23032,   102,  2023,  2003,  6019,  1015,
           102]])
</code></pre>
<p>It seems like I should loop through and increment the special tokens but that also seems hacky.</p>
","2573069","","2573069","","2020-08-03 21:22:34","2020-08-03 21:58:51","How to combine two tokenized bert sequences","<python><tokenize><bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"66703229","1","66713260","","2021-03-19 06:02:29","","0","315","<p>In the Transformers library, what is the maximum input length of words and/or sentences of the Pegasus model? I read in the Pegasus research paper that the max was 512 tokens, but how many words and/or sentences is that? Also, can you increase the maximum number of 512 tokens?</p>
","14882176","","","","","2021-03-19 17:50:24","Maximum Input Length of words/sentences of the Pegasus Model in the Transformers library","<python><machine-learning><nlp><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66712357","1","","","2021-03-19 16:50:44","","1","387","<p>I am trying to use my fine-tuned BERT for visualization on my local machine. The model parameters are saved in a file called <code>trained_model.pt</code>. When I try to load and use it, I get the following error:</p>
<pre><code>import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenized input
text = &quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;
tokenized_text = tokenizer.tokenize(text)

# Mask a token that we will try to predict back with `BertForMaskedLM`
masked_index = 8
tokenized_text[masked_index] = '[MASK]'
assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']

# Convert token to vocabulary indices
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Convert inputs to PyTorch tensors
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])

# Load pre-trained model (weights)
model = torch.load('trained_model.pt', map_location=torch.device('cpu'))
model.eval()

# Predict hidden states features for each layer
with torch.no_grad():
    encoded_layers, _ = model(tokens_tensor, segments_tensors, output_all_encoded_layers=False)
</code></pre>
<hr />
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-32-65859554cc9e&gt; in &lt;module&gt;
      5 # Predict hidden states features for each layer
      6 with torch.no_grad():
----&gt; 7     encoded_layers, _ = model(tokens_tensor, segments_tensors, output_all_encoded_layers=False)

/opt/anaconda3/envs/bert/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

/opt/anaconda3/envs/bert/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)
    151         for t in chain(self.module.parameters(), self.module.buffers()):
    152             if t.device != self.src_device_obj:
--&gt; 153                 raise RuntimeError(&quot;module must have its parameters and buffers &quot;
    154                                    &quot;on device {} (device_ids[0]) but found one of &quot;
    155                                    &quot;them on device: {}&quot;.format(self.src_device_obj, t.device))

RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu
</code></pre>
<p>Can anyone explain me, how this is resolved? ..by the way I get problems with DataParallel <strong>all the time</strong>. Not sure if the error is caused by that, but this feature seems really buggy to me.</p>
","12486121","","4238408","","2021-03-19 16:53:23","2021-03-20 21:27:14","Using BERT on cpu: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu","<python><pytorch><bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"63335442","1","","","2020-08-10 06:55:05","","0","747","<p>I am trying to fine-tune BART on the XSum datasets using the fine-tuning script on the Readme and I keep getting this error:</p>
<pre><code>RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB 
(GPU 0; 14.73 GiB total capacity; 13.67 GiB already allocated; 
15.88 MiB free; 13.72 GiB reserved in total by PyTorch)
</code></pre>
<p>Here is the link to the collab.</p>
<p>Can someone possibly help?</p>
<p><a href=""https://colab.research.google.com/drive/1d192bELyuUI0oPbZDYnCyAWE7QL8Ki96?authuser=3#scrollTo=dc48Udgnq7ED"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1d192bELyuUI0oPbZDYnCyAWE7QL8Ki96?authuser=3#scrollTo=dc48Udgnq7ED</a></p>
","14078176","","2440114","","2020-08-11 15:17:33","2020-08-11 15:17:33","How do I deal with CUDA out of memory while finetuning BART","<huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"63112907","1","65016424","","2020-07-27 09:58:30","","2","285","<p>I've trained a BERT classifier using HuggingFace <code>transformers.TFBertForSequenceClassification</code> classifier. It's working fine, but when using the model.predict() method, it gives a tuple as output which are not normalized between [0, 1]. E.g. I trained the model to classify news articles into fraud and non-fraud category. Then I fed the following 4 test data to the model for prediction:</p>
<pre><code>articles = ['He was involved in the insider trading scandal.', 
            'Johnny was a good boy. May his soul rest in peace', 
            'The fraudster stole money using debit card pin', 
            'Sun rises in the east']
</code></pre>
<p>The outputs are:</p>
<pre><code>[[-2.8615277,  2.6811066],
 [ 2.8651822, -2.564444 ],
 [-2.8276567,  2.4451752],
 [ 2.770451 , -2.3713884]]
</code></pre>
<p>For me label-0 is for non-fraud, and label-1 is for fraud, so that's working fine. But how do I prepare the scoring confidence from here? Does normalization using softmax make sense in this context? Also, if I want to look at those predictions where the model is kind of indecisive, how would I do that? In that case would both the values be very close to each other?</p>
","4245859","","-1","","2020-07-29 15:11:26","2020-11-26 05:08:09","How to normalize output from BERT classifier","<python-3.x><nlp><classification><tf.keras><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"61127158","1","61136147","","2020-04-09 17:49:06","","2","358","<p>I want to use hugging face's fill-mask pipeline to guess a masked token and then extract just the guessed token as a word.  This code should do that:</p>

<pre><code>!pip install -q transformers
model = pipeline('fill-mask')
outcome = model(""Kubernetes is a container orchestration &lt;mask&gt;"")[0]

#Prints: ""Kubernetes is a container orchestration platform"" 
print(outcome['sequence']) 

token = outcome['token'] 

#Prints: 1761
print(token)

#Prints: Ä platform 
print(model.tokenizer.convert_ids_to_tokens(token))
</code></pre>

<p>But I am finding that it gives me back <code>""Ä platform""</code> instead of <code>""platform""</code> - does anyone know why this is or what can be going on here?</p>
","3472360","","","","","2021-08-14 15:42:20","Identifying the word picked by hugging face pipeline fill-mask","<python><neural-network><nlp><huggingface-transformers>","2","0","1","","","CC BY-SA 4.0"
"66015068","1","66025567","","2021-02-02 17:53:37","","1","46","<p>I am trying to implement a QA system using models from huggingface. One thing I do not understand is, when I don't specify which pre-trained model I am using for question-answering, is the model chosen at random?</p>
<pre><code>from transformers import pipeline

# Allocate a pipeline for question-answering

question_answerer = pipeline('question-answering')
question_answerer({

     'question': 'What is the name of the repository ?',
     'context': 'Pipeline have been included in the huggingface/transformers repository'

})
</code></pre>
<p>Output:</p>
<p><code>{'score': 0.5135612454720828, 'start': 35, 'end': 59, 'answer': 'huggingface/transformers'}</code></p>
<p>I know how to specify a model by adding the name of the model (bert-base-uncased for example) as a model parameter, but which one is it using when you are not specifying anything? Does it use a combination of all models on huggingface? I could not find the answer.</p>
","8972207","","8893595","","2021-02-03 10:58:19","2021-02-03 10:58:19","Is the pretrained model selected at random when not specified from transformers","<python><nlp><huggingface-transformers><nlg>","1","0","","","","CC BY-SA 4.0"
"66722287","1","","","2021-03-20 13:42:43","","0","24","<p>I'm trying to make the model predict a word from a sentence using pretrained Huggingface's BERT as feature extractor. The model look like this</p>
<pre><code>class BertAutoEncoder(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        decoder_layer = nn.TransformerDecoderLayer(768, 2, 1024, dropout=0.1)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, 2)
        self.fc = nn.Linear(768, vocab_size)

    def forward(self, memory, embedded_word):
        output = self.transformer_decoder(embedded_word, memory)
        output = self.fc(output)
        return output
</code></pre>
<p>And when train/evaluate I call the model like this</p>
<pre><code>bert = BertModel.from_pretrained('bert-base-uncased')
bert.requires_grad_(False)
...
memory = bert(**src).last_hidden_state.transpose(0, 1)
embeded_word = bert.embeddings(trg.data['input_ids'][:, :-1], token_type_ids=trg.data['token_type_ids'][:, :-1]).transpose(0, 1)
output = model(memory, embeded_word)
</code></pre>
<p>The loss reduced nicely but turned out the model only predict <code>&lt;eos&gt;</code> token.</p>
<p>I tried train the model with 1 batch of 32 samples and it did work when loss reduced pass <code>8e-6</code> but when I trained it with all data the loss could go way beyond that but none of the saved models work. Even the one with eval or train loss around <code>4e-6</code> - <code>8e-6</code>.</p>
<p>Surprisingly the model would work if I use a separate decoder's Embedding like this</p>
<pre><code>class BertAutoEncoderOld(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        decoder_layer = nn.TransformerDecoderLayer(768, 2, 1024, dropout=0.1)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, 2)
        self.decoder = nn.Embedding(vocab_size, 768)
        self.pos_decoder = PositionalEncoding(768, 0.5)
        self.fc = nn.Linear(768, vocab_size)

    def forward(self, memory, word):
        tgt = self.decoder(word.data['input_ids'][:, :-1].transpose(0, 1))
        tgt = self.pos_decoder(tgt)
        output = self.transformer_decoder(tgt, memory)
        output = self.fc(output)
        return output
</code></pre>
<p>But I was asked to make it work with one Embedding and I have no idea how.</p>
<p>I tried</p>
<ul>
<li>Reduce/increase batch from 32 to 8-64</li>
<li>Also tried 2 and 1024 batch size</li>
<li>Remove <code>&lt;eos&gt;</code> token and change it's attention mask to 0</li>
</ul>
<p>But none of those work.</p>
<p>What did I do wrong and how to fix it?</p>
<p>Thanks</p>
<h1>Edit per @emily qeustion</h1>
<p>I change the data itself in collate function</p>
<pre><code>text.data['attention_mask'][text.data['input_ids'] == 102] = 0
text.data['input_ids'][text.data['input_ids'] == 102] = 0
word.data['attention_mask'][word.data['input_ids'] == 102] = 0
word.data['input_ids'][word.data['input_ids'] == 102] = 0
</code></pre>
<p>It only used in Bert though.</p>
","9477338","","9477338","","2021-03-21 08:04:36","2021-03-21 08:04:36","A language model with only one embedding layer in both encode and decode only predict <eos>","<nlp><pytorch><huggingface-transformers><transformer>","0","2","","","","CC BY-SA 4.0"
"66731885","1","66949667","","2021-03-21 11:43:29","","0","24","<p>I was reading the renowned paper <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">'Attention is all you need'</a>. Though I am clear with most of the major concepts, got buggy with a few points
<a href=""https://i.stack.imgur.com/8wE8A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8wE8A.png"" alt=""enter image description here"" /></a></p>
<ol>
<li>How Encoder pass the attention matrix calculated using the input to Decoder? Like what I understood is it only passes the Key &amp; Value matrix to the decoder</li>
<li>From where do we get shifted output for the decoder while testing?</li>
<li>As it is able to output just one token at a time, is this transformer run for multiple iterations to generate output sequence. If yes, then, how to know when to stop?</li>
<li>Are weights trained in Multi-Head Attention in the decoder as it already gets Q,K &amp; V from encoder &amp; masked multi-head attention</li>
</ol>
<p>Any help is appreciated</p>
","8422170","","","","","2021-04-05 07:35:57","How Encoder passes Attention Matrix to Decoder in Tranformers 'Attention is all you need'?","<machine-learning><nlp><artificial-intelligence><huggingface-transformers><attention-model>","1","0","","","","CC BY-SA 4.0"
"66729711","1","","","2021-03-21 07:09:24","","0","186","<p>Trying to run <a href=""https://colab.research.google.com/github/AndreasMadsen/python-textualheatmap/blob/master/notebooks/huggingface_bert_example.ipynb#scrollTo=IMyHY55SC24O"" rel=""nofollow noreferrer"">this TextualHeatmap example</a>, we encounter <code>'TFEmbeddings' object has no attribute 'word_embeddings'</code> error in the following code snippet from the HuggingFace <em>transformers</em> library. Any help is appreciated.</p>
<pre><code>from transformers import TFDistilBertForMaskedLM, DistilBertTokenizer 
dbert_model = TFDistilBertForMaskedLM.from_pretrained('distilbert-base-uncased') 
dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') 
dbert_embmat = dbert_model.distilbert.embeddings.word_embeddings
</code></pre>
","5617507","","5617507","","2021-03-21 07:21:40","2021-03-21 07:21:40","HuggingFace 'TFEmbeddings' object has no attribute 'word_embeddings'","<huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"66729915","1","","","2021-03-21 07:41:38","","1","61","<p>I tried the examples in <a href=""https://github.com/huggingface/transformers/blob/v4.3.2/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/v4.3.2/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py</a>, however, I got the AttributeError that type object 'EncoderDecoderModel' has no attribute 'from_encoder_decoder_pretrained'. How to address this problem?</p>
","15443731","","","","","2021-08-19 10:43:06","huggingface EncoderDecoderModel has no attribute 'from_encoder_decoder_pretrained'","<huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"66746307","1","","","2021-03-22 12:43:40","","4","2100","<p>I am following this tutorial: <a href=""https://huggingface.co/transformers/torchscript.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/torchscript.html</a>
to create a trace of my custom BERT model, however when running the exact same <code>dummy_input</code> I receive an error:</p>
<pre><code>TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. 
We cant record the data flow of Python values, so this value will be treated as a constant in the future. 
</code></pre>
<p>Having loaded in my model and tokenizer, the code to create the trace is the following:</p>
<pre><code>text = &quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;
tokenized_text = tokenizer.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = '[MASK]'
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

traced_model = torch.jit.trace(model, dummy_input)
</code></pre>
<p>The <code>dummy_input</code> is a list of tensors so I'm not sure where the <code>Boolean</code> type is coming into play here. Does anyone understand why this error is occurring and whether the Boolean conversion is happening?</p>
<p>Many Thanks</p>
","14973252","","10886420","","2021-03-22 17:24:47","2021-03-22 17:24:47","Torch JIT Trace = TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect","<tensorflow><pytorch><huggingface-transformers><torchscript>","1","0","","","","CC BY-SA 4.0"
"66747954","1","66748673","","2021-03-22 14:23:08","","0","476","<p>Trying to tokenize and encode data to feed to a neural network.</p>
<p>I only have 25GB RAM and everytime I try to run the code below my google colab crashes. Any idea how to prevent his from happening? â€œYour session crashed after using all available RAMâ€</p>
<p>I thought tokenize/encoding chunks of 50000 sentences would work but unfortunately not.
The code works on a dataset with length 1.3 million. The current dataset has a length of  5 million.</p>
<pre><code>max_q_len = 128
max_a_len = 64    
trainq_list = train_q.tolist()    
batch_size = 50000
    
def batch_encode(text, max_seq_len):
      for i in range(0, len(trainq_list), batch_size):
        encoded_sent = tokenizer.batch_encode_plus(
            text,
            max_length = max_seq_len,
            pad_to_max_length=True,
            truncation=True,
            return_token_type_ids=False
        )
      return encoded_sent

    # tokenize and encode sequences in the training set
    tokensq_train = batch_encode(trainq_list, max_q_len)
</code></pre>
<p>The tokenizer comes from HuggingFace:</p>
<pre><code>tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')
</code></pre>
","14829523","","","","","2021-03-22 15:02:59","Tokenizing & encoding dataset uses too much RAM","<python><nlp><pytorch><huggingface-transformers><huggingface-tokenizers>","1","0","1","","","CC BY-SA 4.0"
"66707770","1","66708577","","2021-03-19 11:58:10","","0","189","<p>I fine-tuned the BERT base model on my own dataset following the script here:</p>
<p><a href=""https://github.com/cedrickchee/pytorch-pretrained-BERT/tree/master/examples/lm_finetuning"" rel=""nofollow noreferrer"">https://github.com/cedrickchee/pytorch-pretrained-BERT/tree/master/examples/lm_finetuning</a></p>
<p>I saved the model as a <code>.pt</code> file and I want to use it now for a sentence similarity task. Unfortunately, it is not clear to me, how to load the fine-tuned model. I tried the following:</p>
<pre><code>model = BertModel.from_pretrained('trained_model.pt')
model.eval()
</code></pre>
<p>This doesn't work. It says:</p>
<pre><code>ReadError: not a gzip file
</code></pre>
<p>So apparently, loading a <code>.pt</code> file with the <code>from_pretrained</code> method is not possible. Can anyone help me out here? Thank's a lot!! :)</p>
<p>Edit: I saved the model in a s3 bucket as follows:</p>
<pre><code># Convert model to buffer
buffer = io.BytesIO()
torch.save(model, buffer)
# Save in s3 bucket
output_model_file = output_folder + &quot;trained_model.pt&quot;
s3_.put_object(Bucket=&quot;power-plant-embeddings&quot;, Key=output_model_file, Body=buffer.getvalue())
</code></pre>
","12486121","","12486121","","2021-03-19 12:25:03","2021-03-19 12:53:09","How to use fine-tuned BERT model for sentence encoding?","<python><nlp><pytorch><bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"67472548","1","","","2021-05-10 14:43:28","","0","58","<p>I load a dataset containing columns of text, I want to translate them.</p>
<p>To speed the process I tried to use transformer datasets.</p>
<pre><code>model_size = &quot;base&quot;
model_name = f&quot;persiannlp/mt5-{model_size}-parsinlu-translation_en_fa&quot;
tokenizer = MT5Tokenizer.from_pretrained(model_name)
model = MT5ForConditionalGeneration.from_pretrained(model_name)
dataset = load_dataset('csv', data_files=dfname, split='train')

dataset = dataset.map(lambda e: tokenizer(e['input_text'], padding='longest'))


dataset.set_format(type='torch', columns=['input_ids'])

# map for generating translation
#dataset = dataset.map(lambda e: {&quot;trans&quot;:model.generate(e['input_ids'])})




dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)
for batch in tqdm(dataloader):
    input_ids = batch[&quot;input_ids&quot;]
    res = model.generate(input_ids)
    target = tokenizer.batch_decode(res, skip_special_tokens=True)

</code></pre>
<p>First, I tried to call <code>model.generate</code> in another <code>map</code> which give this error (commented in code):</p>
<pre><code>File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/transformers/generation_utils.py&quot;, line 378, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs[&quot;encoder_outputs&quot;]: ModelOutput = encoder(input_ids, return_dict=True, **encoder_kwargs)
  File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py&quot;, line 877, in forward
    batch_size, seq_length = input_shape
ValueError: not enough values to unpack (expected 2, got 1)

</code></pre>
<p>Then I tried to call it in a loop, but it gives the following error for the loop:</p>
<pre><code>Traceback (most recent call last):
  File &quot;prepare_natural.py&quot;, line 146, in &lt;module&gt;
    for batch in tqdm(dataloader):
  File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/tqdm/std.py&quot;, line 1129, in __iter__
    for obj in iterable:
  File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py&quot;, line 435, in __next__
    data = self._next_data()
  File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py&quot;, line 475, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 47, in fetch
    return self.collate_fn(data)
  File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 73, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 73, in &lt;dictcomp&gt;
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File &quot;/home/pouramini/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 55, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [17] at entry 0 and [15] at entry 1


</code></pre>
","2651073","","2651073","","2021-05-10 19:28:57","2021-05-10 19:28:57","How can I use map or a loop for decoding a transformer dataset?","<python><pytorch><huggingface-transformers>","0","3","1","","","CC BY-SA 4.0"
"61390271","1","","","2020-04-23 14:56:38","","2","1893","<p>I have prepared a custom dataset for training my own custom model for text summarization. I wish to use BART as it is the state of art now. I am using Transformer Library of HuggingFace using pytorch. </p>

<p>But I don't know how to train a model on my own.I tried running run_train.sh on the following link:
<a href=""https://github.com/huggingface/transformers/tree/master/examples/summarization/bart"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/summarization/bart</a></p>

<p>I have loaded the CNN data into Google Colab. Still it is showing errors.</p>

<p>Can someone help me with how to do custom text summarization using transfer learning?
Any other models are also helpful.</p>
","12246100","","","","","2020-08-14 19:20:32","How to train BART for text summarization using custom datset?","<python><nlp><pytorch><summarization><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"67455420","1","","","2021-05-09 07:52:51","","0","86","<p>I try to translate a dataframe from English to Persian. I use a pertrained langauge model for this purpose, but it's slow, how can I speed it up?</p>
<pre><code>model_size = &quot;base&quot;
model_name = f&quot;persiannlp/mt5-{model_size}-parsinlu-translation_en_fa&quot;
tokenizer = MT5Tokenizer.from_pretrained(model_name)
model = MT5ForConditionalGeneration.from_pretrained(model_name)



with open(fname, 'w') as tsvfile:
    writer = csv.writer(tsvfile, delimiter='\t')
    #writer.writerow([&quot;prefix&quot;, &quot;input_text&quot;, &quot;target_text&quot;])    
    writer.writerow([&quot;input_text&quot;, &quot;target_text&quot;])    
    for i, row in tqdm(df.iterrows(), total=maxlen): 
        prompt = row[&quot;input_text&quot;]
        target = row[&quot;target_text&quot;]
        if trans:
            input_ids = tokenizer.encode(prompt, return_tensors=&quot;pt&quot;)
            res = model.generate(input_ids)
            prompt = tokenizer.batch_decode(res, skip_special_tokens=True)
            input_ids = tokenizer.encode(target, return_tensors=&quot;pt&quot;)
            res = model.generate(input_ids)
            target = tokenizer.batch_decode(res, skip_special_tokens=True)
        writer.writerow([prompt, target])                

print(&quot;saved in &quot;, fname)        
</code></pre>
","2651073","","286934","","2021-05-09 09:04:46","2021-05-09 09:04:46","How to speed up translation with transformers?","<python><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"59240668","1","","","2019-12-08 22:56:52","","6","307","<p>I've been struggling with huggingface's DistilBERT model for some time now, since the documentation seems very unclear and their examples (e.g. <a href=""https://github.com/huggingface/transformers/blob/master/notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb</a> and <a href=""https://github.com/huggingface/transformers/tree/master/examples/distillation"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/distillation</a>) are extremely thick and the thing they are showcasing doesn't seem well documented.</p>

<p>I'm wondering if anyone here has any experience and knows of some good code example for basic in-python usage of their models. Namely:</p>

<ul>
<li><p>How to properly decode the output of the model into actual text (no matter how I change its shape the tokenizer seems willing to decode it and always yields some sequence of <code>[UNK]</code> tokens)</p></li>
<li><p>How to actually use their schedulers+optimizers to train a model for a simple text to text task.</p></li>
</ul>
","7156008","","931175","","2020-03-10 05:35:05","2021-04-17 13:11:22","Text generation using huggingface's distilbert models","<machine-learning><nlp><pytorch><huggingface-transformers><distilbert>","1","0","1","","","CC BY-SA 4.0"
"66797042","1","","","2021-03-25 09:59:39","","0","101","<p>I am trying to use Google's T5 for language translation. However, it is not working for German to English.</p>
<p>English to German works fine:</p>
<pre><code>self.tokenizer = AutoTokenizer.from_pretrained(&quot;t5-small&quot;)
self.model = AutoModelForSeq2SeqLM.from_pretrained(&quot;t5-small&quot;)
inputs = self.tokenizer.encode(&quot;translate English to German: &quot; + text, return_tensors=&quot;pt&quot;, max_length=512, truncation=True)
summary_ids = self.model.generate(inputs, max_length=512, min_length=5, length_penalty=5., num_beams=2)
summary = self.tokenizer.decode(summary_ids[0])
</code></pre>
<p>However, changing encoding to &quot;German to English&quot; is not working.</p>
<p>Is this model not intended to be able to translate German to English, or am I using it wrong?</p>
","3869554","","","","","2021-03-25 09:59:39","Using Google's T5 for translation from German to English","<python><nlp><huggingface-transformers><language-translation>","0","0","","","","CC BY-SA 4.0"
"66656622","1","66661028","","2021-03-16 13:54:25","","0","509","<p>when I'm trying to simply <code>import transformers</code> I receive this error:</p>
<p>ImportError: cannot import name 'version' from 'packaging' (C:\Users\miria\packaging.py)</p>
<p>Can anyone help me solve this?</p>
<p><a href=""https://i.stack.imgur.com/RHMPL.png"" rel=""nofollow noreferrer"">traceback</a></p>
","15408120","","15408120","","2021-03-16 18:07:58","2021-03-16 18:14:04","Python ImportError: cannot import name 'version' from 'packaging' (transformers)","<python><bert-language-model><huggingface-transformers><transformer>","1","5","","","","CC BY-SA 4.0"
"67455305","1","","","2021-05-09 07:35:31","","1","50","<p>I am trying out a project where I use the T5EncoderModel from HuggingFace in order to obtain hidden representations of my input sentences. I have 100K sentences which I tokenize and pad as follows:</p>
<pre><code> for sentence in dataset[original]:
        sentence = tokenizer(sentence, max_length=40, padding='max_length', return_tensors='tf', truncation= True)
        original_sentences.append(sentence.input_ids)
        org_mask.append(sentence.attention_mask)

</code></pre>
<p>This gives me the right outputs and tokenizes everything decently. The problem I achieve is when I am trying to actually train the model. The setup is a bit complex and is taken from <a href=""https://keras.io/examples/vision/semantic_image_clustering/"" rel=""nofollow noreferrer"">https://keras.io/examples/vision/semantic_image_clustering/</a> which I am trying to apply to text.</p>
<p>The set-up for training is as follows:</p>
<pre><code>def create_encoder(rep_dim):
    encoder = TFT5EncoderModel.from_pretrained('t5-small', output_hidden_states=True)
    encoder.trainable = True
     
    original_input = Input(shape=(max_length), name = 'originalIn', dtype=tf.int32)
    augmented_input = Input(shape=(max_length), name = 'originalIn', dtype=tf.int32)
 


    concat = keras.layers.Concatenate(axis=1)([original_input, augmented_input])

    #Take 0-index because it returns a TFBERTmodel type, and 0 returns a tensor
    encoded = encoder(input_ids=concat)[0]
    
    #This outputs shape: [sentences, max_length, encoded_dims]
    
    
    output = Dense(rep_dim, activation='relu')(encoded)
    
    return encoder
</code></pre>
<p>This function is fed into the ReprensentationLearner class from the above link as such:</p>
<pre><code>class RepresentationLearner(keras.Model):
    def __init__(
        self,
        encoder,
        projection_units,
        temperature=0.8,
        dropout_rate=0.1,
        l2_normalize=False,
        **kwargs
    ):
        super(RepresentationLearner, self).__init__(**kwargs)
        self.encoder = encoder
        # Create projection head.
        self.projector = keras.Sequential(
            [
                layers.Dropout(dropout_rate),
                layers.Dense(units=projection_units, use_bias=False),
                layers.BatchNormalization(),
                layers.ReLU(),
            ]
        )
        self.temperature = temperature
        self.l2_normalize = l2_normalize
        self.loss_tracker = keras.metrics.Mean(name=&quot;loss&quot;)

    @property
    def metrics(self):
        return [self.loss_tracker]

    def compute_contrastive_loss(self, feature_vectors, batch_size):
        num_augmentations = tf.shape(feature_vectors)[0] // batch_size
        if self.l2_normalize:
            feature_vectors = tf.math.l2_normalize(feature_vectors, -1)
        # The logits shape is [num_augmentations * batch_size, num_augmentations * batch_size].
        logits = (
            tf.linalg.matmul(feature_vectors, feature_vectors, transpose_b=True)
            / self.temperature
        )
        # Apply log-max trick for numerical stability.
        logits_max = tf.math.reduce_max(logits, axis=1)
        logits = logits - logits_max
        # The shape of targets is [num_augmentations * batch_size, num_augmentations * batch_size].
        # targets is a matrix consits of num_augmentations submatrices of shape [batch_size * batch_size].
        # Each [batch_size * batch_size] submatrix is an identity matrix (diagonal entries are ones).
        targets = tf.tile(tf.eye(batch_size), [num_augmentations, num_augmentations])
        # Compute cross entropy loss
        return keras.losses.categorical_crossentropy(
            y_true=targets, y_pred=logits, from_logits=True
        )

    def call(self, inputs):
        features = self.encoder(inputs[0])[0]
        # Apply projection head.
        return self.projector(features[0])

    def train_step(self, inputs):
        batch_size = tf.shape(inputs)[0]
        # Run the forward pass and compute the contrastive loss
        with tf.GradientTape() as tape:
            feature_vectors = self(inputs, training=True)
            loss = self.compute_contrastive_loss(feature_vectors, batch_size)
        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # Update loss tracker metric
        self.loss_tracker.update_state(loss)
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}

    def test_step(self, inputs):
        batch_size = tf.shape(inputs)[0]
        feature_vectors = self(inputs, training=False)
        loss = self.compute_contrastive_loss(feature_vectors, batch_size)
        self.loss_tracker.update_state(loss)
        return {&quot;loss&quot;: self.loss_tracker.result()}

</code></pre>
<p>In order to train it, I use the Colab TPU and train it as such:</p>
<pre><code>with strategy.scope():
  encoder = create_encoder(rep_dim)
  training_model = RepresentationLearner(encoder=encoder, projection_units=128, temperature=0.1)


  lr_scheduler = keras.experimental.CosineDecay(initial_learning_rate=0.001, decay_steps=500, alpha=0.1)
  training_model.compile(optimizer=tfa.optimizers.AdamW(learning_rate=lr_scheduler, weight_decay=0.0001))
  history = training_model.fit(x = [original_train, augmented_train], batch_size=32*8, epocs = 10)
  
  training_model.save_weights('representation_learner.h5', overwrite=True)

</code></pre>
<p>Note that I am giving my model two inputs. When I predict on my input data, I get all zeros, and I can not seem to understand why. I predict as follows:</p>
<pre><code>training_model.load_weights('representation_learner.h5')
feature_vectors= training_model.predict([[original_train, augmented_train]], verbose = 1)
</code></pre>
<p>And the output is:</p>
<pre><code>array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)

</code></pre>
<p>With a way too large shape of (1000000, 128)</p>
","12027232","","","","","2021-05-09 07:35:31","T5 Encoder model output all zeros?","<tensorflow><machine-learning><keras><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"66756552","1","","","2021-03-23 02:44:23","","0","89","<p>I'd like to fine-tune a transformer model from huggingface on specific dialogue exchanges in order to teach it to chat like a specific speaker. For example, train the model to speak like Harry Potter, given all his dialogue exchanges from movies, or speak like any character / individual given dialogue exchanges between them and another person.</p>
<p>I've seen huggingface's <a href=""https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313"" rel=""nofollow noreferrer"">conversational ai article</a>, but it seems more complicated than necessary, by including 'personalities', and by not using the huggingface libraries.</p>
<p>I've looked into huggingface's <a href=""https://huggingface.co/transformers/model_doc/dialogpt.html"" rel=""nofollow noreferrer"">DialoGPT</a>, which says that to fine-tune it we should use 'causal language modeling training', and that it should be fine-tuned in the same way as GPT-2. I was unable to find any examples of that being done using huggingface transformers. It refers back to the paper, which also didn't help me much.</p>
<p>I've previously fine-tuned BART for conditional generation using huggingface, which was very easy and intuitive. I was wondering if some similar method existed to fine-tune GPT/GPT-2/DialoGPT given example dialogues.</p>
<p>Thanks in advance</p>
","2632659","","","","","2021-06-14 06:25:24","Fine-Tune Casual Language Model","<chatbot><huggingface-transformers><language-model>","0","0","","","","CC BY-SA 4.0"
"66767358","1","","","2021-03-23 16:35:02","","0","38","<pre class=""lang-py prettyprint-override""><code>def convert_data_to_examples(train, test, review, sentiment): 
    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case
                                                          text_a = x[review], 
                                                          label = x[sentiment]), axis = 1)

    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case
                                                          text_a = x[review], 
                                                          label = x[sentiment]), axis = 1,)

  
    return train_InputExamples, validation_InputExamples

train_InputExamples, validation_InputExamples = convert_data_to_examples(train,  test, 'review',  'sentiment')
</code></pre>
<p><strong>NameError: name 'InputExample' is not defined</strong></p>
<p>After running this part of code its gives error. Please tell me how to solve this error.</p>
","8602792","","6664872","","2021-03-23 17:23:18","2021-03-23 17:23:18","Training roberta model on imdb movie reviews dataset giving this error?","<python-3.x><huggingface-transformers><roberta-language-model>","1","1","","","","CC BY-SA 4.0"
"64583794","1","","","2020-10-29 01:47:21","","1","522","<p>I want to get translations of one batch of sentences using pretrained model.</p>
<pre><code>model = AutoModelWithLMHead.from_pretrained(&quot;Helsinki-NLP/opus-mt-es-en&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-es-en&quot;)
batch_input_str = ((&quot;Mary spends $20 on pizza&quot;), (&quot;She likes eating it&quot;), (&quot;The pizza was great&quot;))
encoded = (tokenizer.batch_encode_plus(batch_input_str, pad_to_max_length=True))
</code></pre>
<p>The <code>encoded</code>is like:</p>
<pre><code>{'input_ids': [[4963, 10154, 5021, 9, 25, 1326, 2255, 35, 17462, 0], [552, 3996, 2274, 9, 129, 75, 2223, 25, 1370, 0], [42, 17462, 12378, 9, 25, 5807, 1949, 0, 65000, 65000]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]}
</code></pre>
<p>Then, should I just pass the <code>encoded</code> to</p>
<pre><code>output = model.generate(a)
</code></pre>
<p>And then use</p>
<pre><code>res = tokenizer.decode(output)
</code></pre>
<p>?</p>
<p>Thanks!</p>
","13482179","","","","","2020-10-29 03:30:14","How to get translations of one batch of sentences after batch_encode_plus?","<nlp><translation><huggingface-transformers><transformer>","1","0","","","","CC BY-SA 4.0"
"63211463","1","63211626","","2020-08-02 01:11:19","","0","308","<p>I have the following code taken directly from <a href=""https://towardsdatascience.com/simple-abstractive-text-summarization-with-pretrained-t5-text-to-text-transfer-transformer-10f6d602c426"" rel=""nofollow noreferrer"">here</a> with some pretty little modifications:</p>
<pre><code>import pandas as pd
import torch
import json 
from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config
from torch import cuda

df = pd.read_pickle('df_final.pkl')

model = T5ForConditionalGeneration.from_pretrained('t5-base')
tokenizer = T5Tokenizer.from_pretrained('t5-base')
device = 'cuda' if cuda.is_available() else 'cpu'

text = ''.join(df[(df['col1'] == 'type') &amp; (df['col2'] == 2)].col3.to_list())

preprocess_text = text.strip().replace(&quot;\n&quot;,&quot;&quot;)
t5_prepared_Text = &quot;summarize: &quot;+preprocess_text
#print (&quot;original text preprocessed: \n&quot;, preprocess_text)

tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=&quot;pt&quot;, max_length = 500000).to(device)


# summmarize 
summary_ids = model.generate(tokenized_text,
                                    num_beams=4,
                                    no_repeat_ngram_size=2,
                                    min_length=30,
                                    max_length=100,
                                    early_stopping=True)

output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print (&quot;\n\nSummarized text: \n&quot;,output)
</code></pre>
<p>When executing the <code>model_generate()</code> part i get an error like this:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-12-e8e9819a85dc&gt; in &lt;module&gt;
     12                                     min_length=30,
     13                                     max_length=100,
---&gt; 14                                     early_stopping=True).to(device)
     15 
     16 output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

~\Anaconda3\lib\site-packages\torch\autograd\grad_mode.py in decorate_no_grad(*args, **kwargs)
     47         def decorate_no_grad(*args, **kwargs):
     48             with self:
---&gt; 49                 return func(*args, **kwargs)
     50         return decorate_no_grad
     51 

~\Anaconda3\lib\site-packages\transformers\generation_utils.py in generate(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)
    383             encoder = self.get_encoder()
    384 
--&gt; 385             encoder_outputs: tuple = encoder(input_ids, attention_mask=attention_mask)
    386 
    387         # Expand input ids if num_beams &gt; 1 or num_return_sequences &gt; 1

~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

~\Anaconda3\lib\site-packages\transformers\modeling_t5.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_value_states, use_cache, output_attentions, output_hidden_states, return_dict)
    701         if inputs_embeds is None:
    702             assert self.embed_tokens is not None, &quot;You have to intialize the model with valid token embeddings&quot;
--&gt; 703             inputs_embeds = self.embed_tokens(input_ids)
    704 
    705         batch_size, seq_length = input_shape

~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

~\Anaconda3\lib\site-packages\torch\nn\modules\sparse.py in forward(self, input)
    112         return F.embedding(
    113             input, self.weight, self.padding_idx, self.max_norm,
--&gt; 114             self.norm_type, self.scale_grad_by_freq, self.sparse)
    115 
    116     def extra_repr(self):

~\Anaconda3\lib\site-packages\torch\nn\functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1482         # remove once script supports set_grad_enabled
   1483         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1484     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1485 
   1486 

RuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select
â€‹
</code></pre>
<p>I've searched this error and fouund some other threads like <a href=""https://stackoverflow.com/questions/55278566/runtimeerror-expected-object-of-backend-cuda-but-got-backend-cpu-for-argument"">this</a> one and <a href=""https://stackoverflow.com/questions/58799486/expected-object-of-device-type-cuda-but-got-device-type-cpu-in-pytorch"">this</a> one but they didn't help me much since their case seems to be completely different. In my case there are no custom instances or classes created, so i don't know how to fix this or where the error come from.</p>
<p>Could you please tell me where is the error coming from and how could i fix it?</p>
<p>Thank you very much in advance.</p>
","9542954","","","","","2020-08-02 01:50:33","Error Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select","<neural-network><pytorch><gpu><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66655023","1","66658405","","2021-03-16 12:14:14","","1","196","<p>I am trying to follow this example in the huggingface documentation here <a href=""https://huggingface.co/transformers/model_doc/longformer.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/longformer.html</a>:</p>
<pre><code>import torch
from transformers import LongformerModel, LongformerTokenizer
model = LongformerModel.from_pretrained('allenai/longformer-base-4096')
tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')
SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document
input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1
# Attention mask values -- 0: no attention, 1: local attention, 2: global attention
attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention
global_attention_mask = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to global attention to be deactivated for all tokens
global_attention_mask[:, [1, 4, 21,]] = 1  # Set global attention to random tokens for the sake of this example
                                    # Usually, set global attention based on the task. For example,
                                    # classification: the &lt;s&gt; token
                                    # QA: question tokens
                                    # LM: potentially on the beginning of sentences and paragraphs
outputs = model(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask, output_hidden_states= True)
sequence_output = outputs[0].last_hidden_state
pooled_output = outputs.pooler_output
</code></pre>
<p>I suppose that this would return a document embedding for the sample text.
However, I run into the following error:</p>
<pre><code>AttributeError: 'Tensor' object has no attribute 'last_hidden_state'
</code></pre>
<p>Why isnt it possible to call last_hidden_state?</p>
","14521644","","6664872","","2021-03-16 15:34:06","2021-04-02 13:20:31","Longformer get last_hidden_state","<python><nlp><pytorch><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"66742778","1","","","2021-03-22 08:46:58","","0","143","<p>I have fine-tuned a machine translation model and I'm trying to load my pytorch_model.bin model checkpoint that was saved during training and predict the translation of a word.
How do I convert from transformers.modeling_outputs.Seq2SeqModelOutput to normal text?</p>
<pre><code>model = AutoModel.from_pretrained('/content/drive/MyDrive/model', cache_dir=None)

 tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/model', cache_dir=None)

model.eval()

inputs2 = tokenizer('word', return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;]

inputs2.data #results in =&gt;
tensor([[ 1415,   259, 54622,     1]])

outputs = model(input_ids=inputs2, decoder_input_ids=inputs2)
type(outputs) #results in =&gt;transformers.modeling_outputs.Seq2SeqModelOutput

**output_str = #translation of the input word**
</code></pre>
","8626329","","","","","2021-03-22 08:46:58","How do I decode my finetuned model's output into text?","<python><torch><huggingface-transformers><seq2seq><huggingface-tokenizers>","0","4","","","","CC BY-SA 4.0"
"66820943","1","66867147","","2021-03-26 16:39:46","","0","387","<p>I am working on a word-level classification task on multilingual data, I am using XLM-R, I know that XLM-R uses <code>sentencepiece</code>  as tokenizers which sometimes tokenizes words into subword.</p>
<blockquote>
<p>For example the sentence &quot;deception master&quot; is tokenized as <code>de</code> <code>ception</code> <code>master</code>, the word deception has been tokenized into two sub-words.</p>
</blockquote>
<p>How can I get the embedding of <code>deception</code>. I can take the mean of the subwords to get the embedding of the word as done <a href=""https://github.com/uhh-lt/bert-sense/blob/bfecb3c0e677d36ccfab4e2131ef9183995efaef/BERT_Model.py#L342"" rel=""nofollow noreferrer"">here</a>.  But I have to implement my code in TensorFlow and TensorFlow computational graph doesn't support NumPy.</p>
<p>I could store the final hidden embeddings after taking the mean of the subwords into a NumPy array and give this array as input to the model, but I want to fine-tune the transformer.</p>
<p>How to get the word embeddings from the sub-word embeddings given by the transformer</p>
","10531776","","10531776","","2021-03-28 04:43:04","2021-03-30 08:16:29","How to get word embeddings from the pretrained transformers","<python><tensorflow><huggingface-transformers><transfer-learning><transformer>","1","8","1","","","CC BY-SA 4.0"
"61513052","1","61693167","","2020-04-29 22:51:34","","0","271","<p>I have a codebase which was working fine but today when I was trying to run, I observed that <code>tokenizer.encode_plus</code> stopped returning <code>attention_mask</code>. Is it removed in the latest release? Or, do I need to do something else?</p>

<p>The following piece of code was working for me.</p>

<pre><code>encoded_dict = tokenizer.encode_plus(
                truncated_query,
                span_doc_tokens,
                max_length=max_seq_length,
                return_overflowing_tokens=True,
                pad_to_max_length=True,
                stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens,
                truncation_strategy=""only_second"",
                return_token_type_ids=True,
                return_attention_mask=True
            )
</code></pre>

<p>But now, I get only <code>dict_keys(['input_ids', 'token_type_ids'])</code> from encode_plus. Also, I realized that the returned <code>input_ids</code> are not padded to <code>max_length</code>.</p>
","5352399","","6664872","","2020-06-16 13:30:48","2020-06-16 13:30:48","attention_mask is missing in the returned dict from tokenizer.encode_plus","<huggingface-transformers><huggingface-tokenizers>","1","1","","","","CC BY-SA 4.0"
"59315138","1","60255556","","2019-12-13 01:58:57","","1","665","<p>I am using Hugging Face's Transformer library to work with different NLP models. Following code does masking with XLNet. It outputs a tensor with numbers. How do I convert the output to words again?  </p>

<pre><code>import torch
from transformers import XLNetModel,  XLNetTokenizer, XLNetLMHeadModel

tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')
model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased')

# We show how to setup inputs to predict a next token using a bi-directional context.
input_ids = torch.tensor(tokenizer.encode(""I went to &lt;mask&gt; York and saw the &lt;mask&gt; &lt;mask&gt; building."")).unsqueeze(0)  # We will predict the masked token
print(input_ids)

perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)
perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token

target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  # Shape [1, 1, seq_length] =&gt; let's predict one token
target_mapping[0, 0, -1] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)

outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)
next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]
</code></pre>

<p>The current output I get is: </p>

<p>tensor([[[ -5.1466, -17.3758, -17.3392,  ..., -12.2839, -12.6421, -12.4505]]],
       grad_fn=AddBackward0)</p>
","987653","","1676393","","2020-02-18 10:51:49","2020-02-18 10:51:49","How to get words from output of XLNet using Transformers library","<nlp><masking><transformer><language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66767832","1","67000813","","2021-03-23 17:03:19","","1","1513","<p>I am trying to download the tokenizer from Huggingface for BERT.</p>
<p>I am executing:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
</code></pre>
<p>Error:</p>
<pre><code>&lt;Path&gt;\tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1663                         resume_download=resume_download,
   1664                         local_files_only=local_files_only,
-&gt; 1665                         use_auth_token=use_auth_token,
   1666                     )
   1667 

&lt;Path&gt;\file_utils.py in cached_path(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)
   1140             user_agent=user_agent,
   1141             use_auth_token=use_auth_token,
-&gt; 1142             local_files_only=local_files_only,
   1143         )
   1144     elif os.path.exists(url_or_filename):

&lt;Path&gt;\file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)
   1347                 else:
   1348                     raise ValueError(
-&gt; 1349                         &quot;Connection error, and we cannot find the requested files in the cached path.&quot;
   1350                         &quot; Please try again or make sure your Internet connection is on.&quot;
   1351                     )

ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.
</code></pre>
<p>Based on a similar discussion on <a href=""https://github.com/huggingface/transformers/issues/8690"" rel=""nofollow noreferrer"">github in huggingface's repo</a>, I gather that the file that the above call wants to download is: <a href=""https://huggingface.co/bert-base-uncased/resolve/main/config.json"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-uncased/resolve/main/config.json</a></p>
<p>While I can access that json file perfectly well on my browser, I can not download it via requests.
The error I get is:</p>
<pre><code>&gt;&gt; import requests as r
&gt;&gt; r.get('https://huggingface.co/bert-base-uncased/resolve/main/config.json')
...
requests.exceptions.SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by SSLError(SSLError(&quot;bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])&quot;)))
</code></pre>
<p>While examining the certificate of the page - <a href=""https://huggingface.co/bert-base-uncased/resolve/main/config.json"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-uncased/resolve/main/config.json</a>, I see that it is signed by my IT department not the standard CA root I would expect to find.
Based on discussion <a href=""https://stackoverflow.com/questions/5846652/can-proxy-change-ssl-certificate"">here</a>, it looks like it is plausible for SSL proxies to do something like this.</p>
<p>My IT department's certificate is in the trusted authorities list. But requests does not seem to be considering that list for trusting certificates.</p>
<p>Taking a cue from <a href=""https://stackoverflow.com/questions/30405867/how-to-get-python-requests-to-trust-a-self-signed-ssl-certificate"">a stack-overflow discussion on how to let requests trust a self-signed certificate</a> I have also tried append cacert.pem (file pointed to by curl-config --ca) with the ROOT certificate that appears for the huggingface and adding the path of this pem to REQUESTS_CA_BUNDLE</p>
<pre><code>export REQUESTS_CA_BUNDLE=/mnt/&lt;path&gt;/wsl-anaconda/ssl/cacert.pem
</code></pre>
<p>But it did not help at all.</p>
<p>Would you know how I can let requests know that it is OK to trust my IT department's certificate ?</p>
<p>P.S: If it matters, I am working on windows and am facing this in WSL as well.</p>
","4441239","","","","","2021-04-08 09:10:11","BertTokenizer.from_pretrained errors out with ""Connection error""","<python><ssl><ssl-certificate><huggingface-transformers>","1","6","1","","","CC BY-SA 4.0"
"66127970","1","","","2021-02-09 22:09:47","","1","188","<p>I am looking for methods to classify documents. For ex. I have a bunch of documents with text and I want to label the document on whether it belongs to sports, food, politics etc.
Can I use BERT (for documents with words &gt; 500) for this or are there any other models that do this task efficiently?</p>
","15179309","","931175","","2021-02-10 09:05:55","2021-02-10 09:05:55","Document classification using pretrained models like BERT","<nlp><bert-language-model><huggingface-transformers><document-classification>","1","1","","","","CC BY-SA 4.0"
"66821505","1","66833123","","2021-03-26 17:16:42","","1","184","<p>Hello together currently IÂ´m trying to develop a model for contradicition detection. Using and fine-tuning a BERT Model I already got quite statisfactionary result but I think with with some other features I could get a better accuracy. I oriented myself on this <a href=""https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db"" rel=""nofollow noreferrer"">Tutorial</a>. After fine-tuning, my model looks like this:</p>
<pre><code>==== Embedding Layer ====

bert.embeddings.word_embeddings.weight                  (30000, 768)
bert.embeddings.position_embeddings.weight                (512, 768)
bert.embeddings.token_type_embeddings.weight                (2, 768)
bert.embeddings.LayerNorm.weight                              (768,)
bert.embeddings.LayerNorm.bias                                (768,)

==== First Transformer ====

bert.encoder.layer.0.attention.self.query.weight          (768, 768)
bert.encoder.layer.0.attention.self.query.bias                (768,)
bert.encoder.layer.0.attention.self.key.weight            (768, 768)
bert.encoder.layer.0.attention.self.key.bias                  (768,)
bert.encoder.layer.0.attention.self.value.weight          (768, 768)
bert.encoder.layer.0.attention.self.value.bias                (768,)
bert.encoder.layer.0.attention.output.dense.weight        (768, 768)
bert.encoder.layer.0.attention.output.dense.bias              (768,)
bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)
bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)
bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)
bert.encoder.layer.0.intermediate.dense.bias                 (3072,)
bert.encoder.layer.0.output.dense.weight                 (768, 3072)
bert.encoder.layer.0.output.dense.bias                        (768,)
bert.encoder.layer.0.output.LayerNorm.weight                  (768,)
bert.encoder.layer.0.output.LayerNorm.bias                    (768,)

==== Output Layer ====

bert.pooler.dense.weight                                  (768, 768)
bert.pooler.dense.bias                                        (768,)
classifier.weight                                           (2, 768)
classifier.bias                                                 (2,)
</code></pre>
<p>My next step would be to get the [CLS] token from this model, combine it with a few hand crafted features and feed them into a different model (MLP) for classfification. Any hints how to do this?</p>
","12581584","","","","","2021-03-27 19:28:06","Extracting Features from BertForSequenceClassification","<python><nlp><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66822496","1","","","2021-03-26 18:31:04","","1","4160","<p>I am trying to import BertTokenizer from the transformers library as follows:</p>
<pre><code>import transformers
from transformers import BertTokenizer
from transformers.modeling_bert import BertModel, BertForMaskedLM
</code></pre>
<p>However, I get the following error:</p>
<p><a href=""https://i.stack.imgur.com/rge7T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rge7T.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/kfrff.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kfrff.png"" alt=""enter image description here"" /></a></p>
<p>I am using transformers version 3.5.1 because I had a problem with the updated version which can be found <a href=""https://datascience.stackexchange.com/questions/87186/bert-dropout-argument-input-position-1-must-be-tensor-not-str"">here</a>.</p>
<p>Does anyone know how to fix this? Apart from updating the transformers library to its latest version (that will unfortunately cause more errors). <br></p>
<p>Any help is appreciated!</p>
","14998051","","","","","2021-05-08 13:27:16","No module named 'transformers.models' while trying to import BertTokenizer","<python><importerror><bert-language-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"66828699","1","","","2021-03-27 06:41:47","","0","292","<p>I am getting the following error:</p>
<pre><code>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File &quot;./run_hyperparameter_search.py&quot;, line 74, in &lt;module&gt;
    trainer = Trainer(
  File &quot;/ext3/miniconda3/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 273, in __init__
    model = self.call_model_init()
  File &quot;/ext3/miniconda3/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 737, in call_model_init
    raise RuntimeError(&quot;model_init should have 0 or 1 argument.&quot;)
RuntimeError: model_init should have 0 or 1 argument.
~                                                                       
</code></pre>
<p>Here is what I call in my hugging face trainer:</p>
<pre><code>#Initialising the model
trainer = Trainer(
    args = training_args,
    tokenizer = tokenizer,
    train_dataset = train_data,
    eval_dataset = val_data,
    # maybe there is a () in the init, but not in compute metrics for sure. Will test
    model_init = finetuning_utils.model_init(),
    compute_metrics = finetuning_utils.compute_metrics,
)
</code></pre>
<p>The problem is apparently in the model_init.</p>
<p>Here is what the finetuning_utils.model_init() encompasses:</p>
<pre><code>def model_init():
    &quot;&quot;&quot;Returns an initialized model for use in a Hugging Face Trainer.&quot;&quot;&quot;
    ## TODO: Return a pretrained RoBERTa model for sequence classification.
    ## See https://huggingface.co/transformers/model_doc/roberta.html#robertaforsequenceclassification.
    model = RobertaForSequenceClassification.from_pretrained(&quot;roberta-base&quot;)
    #model = model.to('cuda')
    return model
</code></pre>
<p>Please help with the error.</p>
","12905187","","6664872","","2021-03-27 15:46:12","2021-04-17 10:33:57","Hugging Face Trainer: Error in the model init","<nlp><huggingface-transformers>","1","4","","","","CC BY-SA 4.0"
"59435020","1","59520904","","2019-12-21 09:24:46","","8","2759","<p>It is relatively easy to get a token's probability according to a language model, as the snippet below shows. You can get the output of a model, restrict yourself to the output of the masked token, and then find the probability of your requested token in the output vector. However, this only works with single-token words, e.g. words that are themselves in the tokenizer's vocabulary. When a word does not exist in the vocabulary, the tokenizer will chunk it up into pieces that it <em>does</em> know (see the bottom of the example). But since the input sentence consists of only one masked position, and the requested token has more tokens than that, how can we get its probability? Ultimately I am looking for a solution that works regardless of the number of subword units a word has.</p>

<p>In the code below I have added many comments explaining what is going on, as well as printing out the given output of print statements. You'll see that predicting tokens such as 'love' and 'hate' is straightforward because they are in the tokenizer's vocabulary. 'reprimand' is not, though, so it cannot be predicted in a single masked position - it consists of three subword units. So how can we predict 'reprimand' in the masked position?</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForMaskedLM
import torch

# init model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval()
# init softmax to get probabilities later on
sm = torch.nn.Softmax(dim=0)
torch.set_grad_enabled(False)

# set sentence with MASK token, convert to token_ids
sentence = f""I {tokenizer.mask_token} you""
token_ids = tokenizer.encode(sentence, return_tensors='pt')
print(token_ids)
# tensor([[ 101, 1045,  103, 2017,  102]])
# get the position of the masked token
masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero().item()

# forward
output = model(token_ids)
last_hidden_state = output[0].squeeze(0)
# only get output for masked token
# output is the size of the vocabulary
mask_hidden_state = last_hidden_state[masked_position]
# convert to probabilities (softmax)
# giving a probability for each item in the vocabulary
probs = sm(mask_hidden_state)

# get probability of token 'hate'
hate_id = tokenizer.convert_tokens_to_ids('hate')
print('hate probability', probs[hate_id].item())
# hate probability 0.008057191967964172

# get probability of token 'love'
love_id = tokenizer.convert_tokens_to_ids('love')
print('love probability', probs[love_id].item())
# love probability 0.6704086065292358

# get probability of token 'reprimand' (?)
reprimand_id = tokenizer.convert_tokens_to_ids('reprimand')
# reprimand is not in the vocabulary, so it needs to be split into subword units
print(tokenizer.convert_ids_to_tokens(reprimand_id))
# [UNK]

reprimand_id = tokenizer.encode('reprimand', add_special_tokens=False)
print(tokenizer.convert_ids_to_tokens(reprimand_id))
# ['rep', '##rim', '##and']
# but how do we now get the probability of a multi-token word in a single-token position?
</code></pre>
","1150683","","1150683","","2019-12-24 09:58:30","2020-02-11 00:30:39","Get probability of multi-token word in MASK position","<python><pytorch><transformer><bert-language-model><huggingface-transformers>","2","0","2","","","CC BY-SA 4.0"
"66121700","1","","","2021-02-09 15:11:38","","0","46","<p>I'm tring to exploit create a Question Answer system in French language.
In order to accomplish my goal I exploit the code found on the colab code at the link: <a href=""https://colab.research.google.com/drive/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-</a></p>
<p>What is differnent in my code is the loading of the pretrained model</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering
source_model = 'etalab-ia/camembert-base-squadFR-fquad-piaf'
model_fr = AutoModelForQuestionAnswering.from_pretrained(source_model)
tokenizer_fr = AutoTokenizer.from_pretrained(source_model, use_fast=False)
</code></pre>
<p>The finetuned model for question answer is related to the following link:
<a href=""https://huggingface.co/etalab-ia/camembert-base-squadFR-fquad-piaf"" rel=""nofollow noreferrer"">https://huggingface.co/etalab-ia/camembert-base-squadFR-fquad-piaf</a></p>
<p>The error I receive is &quot;IndexError: index out of range in self&quot; and is related to the
<em>def answer_question(question, answer_text)</em> function, which raise expection in the code point</p>
<pre><code># ======== Evaluate ========
# Run our example through the model.
outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.
                token_type_ids=torch.tensor([segment_ids]),
                return_dict=True) 
</code></pre>
<p>I tried other model from finetuned cambert, but they didn't work.
I tried with mrm8488/bert-multi-cased-finetuned-xquadv1 (<a href=""https://huggingface.co/mrm8488/bert-multi-cased-finetuned-xquadv1"" rel=""nofollow noreferrer"">https://huggingface.co/mrm8488/bert-multi-cased-finetuned-xquadv1</a> ), which is finetuned on different languages than french, but the results are not so promising (although I'm exploiting the zero-shot cross language property of mBERT explained here: <a href=""https://arxiv.org/abs/1906.01502"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1906.01502</a> which shows the powerful of mBERT of similar topological languages)</p>
<p>Any idea how to cope with the specific problem of camembert?</p>
","15176806","","7941251","","2021-09-01 18:30:19","2021-09-01 18:30:19","Problem with finetuned QuestionAnswer Camembert using Hugging Face","<python><nlp><huggingface-transformers><nlu>","0","1","","","","CC BY-SA 4.0"
"66148641","1","","","2021-02-11 04:09:11","","0","874","<p>I am using HuggingFace models for <code>TokenClassification</code> task. I have the following label2id mapping. I am using version 3.3.0 of the library</p>
<pre><code>label2id = {
    &quot;B-ADD&quot;: 4,
    &quot;B-ARRESTED&quot;: 7,
    &quot;B-CRIME&quot;: 2,
    &quot;B-INCIDENT_DATE&quot;: 3,
    &quot;B-SUSPECT&quot;: 9,
    &quot;B-VICTIMS&quot;: 1,
    &quot;B-WPN&quot;: 5,
    &quot;I-ADD&quot;: 8,
    &quot;I-ARRESTED&quot;: 13,
    &quot;I-CRIME&quot;: 11,
    &quot;I-INCIDENT_DATE&quot;: 10,
    &quot;I-SUSPECT&quot;: 14,
    &quot;I-VICTIMS&quot;: 12,
    &quot;I-WPN&quot;: 6,
    &quot;O&quot;: 0
  }
</code></pre>
<p>The following scenario works well and the model gets loaded correctly.</p>
<pre><code>from transformers import AutoModelForTokenClassification, AutoTokenizer, AutoConfig

pretrained_model_name = &quot;bert-base-cased&quot;
config = AutoConfig.from_pretrained(pretrained_model_name)

id2label = {y:x for x,y in label2id.items()}
config.label2id = label2id
config.id2label = id2label
config._num_labels = len(label2id)

model = AutoModelForTokenClassification.from_pretrained(pretrained_model_name, config=config)

model
</code></pre>
<p>I get the following output. The last layer have been correctly initialized with 15 neurons (numer of token category to predict).</p>
<pre><code>.....................
      (dropout): Dropout(p=0.1, inplace=False)
      (classifier): Linear(in_features=768, out_features=15, bias=True)
    )
</code></pre>
<p>but if I changed the <code>pretrained_model_name</code> to <code>&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</code>, I am getting the following error</p>
<pre><code>loading weights file https://cdn.huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english/pytorch_model.bin from cache at C:\Users\anu10961/.cache\torch\transformers\4b02c1fe04cf7f7e6972536150e9fb329c7b3d5720b82afdac509bd750c705d2.6dcb154688bb97608a563afbf68ba07ae6f7beafd9bd98b5a043cd269fcc02b4
All model checkpoint weights were used when initializing BertForTokenClassification.

All the weights of BertForTokenClassification were initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-15-2969a8092bf4&gt; in &lt;module&gt;
----&gt; 1 model = AutoModelForTokenClassification.from_pretrained(pretrained_model_name, config=config)

C:\ProgramData\Anaconda3\envs\arcgis183\lib\site-packages\transformers\modeling_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1372         if type(config) in MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING.keys():
   1373             return MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING[type(config)].from_pretrained(
-&gt; 1374                 pretrained_model_name_or_path, *model_args, config=config, **kwargs
   1375             )
   1376 

C:\ProgramData\Anaconda3\envs\arcgis183\lib\site-packages\transformers\modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1047                 raise RuntimeError(
   1048                     &quot;Error(s) in loading state_dict for {}:\n\t{}&quot;.format(
-&gt; 1049                         model.__class__.__name__, &quot;\n\t&quot;.join(error_msgs)
   1050                     )
   1051                 )

RuntimeError: Error(s) in loading state_dict for BertForTokenClassification:
    size mismatch for classifier.weight: copying a param with shape torch.Size([9, 1024]) from checkpoint, the shape in current model is torch.Size([15, 1024]).
    size mismatch for classifier.bias: copying a param with shape torch.Size([9]) from checkpoint, the shape in current model is torch.Size([15]).
</code></pre>
<p>The only difference I could see is the model <code>dbmdz/bert-large-cased-finetuned-conll03-english</code> is already finetuned on <code>TokenClassification</code> task and it model config has these <code>label2id</code> mappings</p>
<pre><code>label2id = {
    &quot;B-LOC&quot;: 7,
    &quot;B-MISC&quot;: 1,
    &quot;B-ORG&quot;: 5,
    &quot;B-PER&quot;: 3,
    &quot;I-LOC&quot;: 8,
    &quot;I-MISC&quot;: 2,
    &quot;I-ORG&quot;: 6,
    &quot;I-PER&quot;: 4,
    &quot;O&quot;: 0
  }
</code></pre>
<p>But still I feel that we can change the last layer of this model and use it for my specific task (although I need to train the model first before using it for inferencing)</p>
","1364646","","1364646","","2021-02-11 04:28:41","2021-02-11 12:02:12","Changing config and loading Hugging Face model fine-tuned on a downstream task","<python><pytorch><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66845379","1","","","2021-03-28 19:27:50","","1","343","<p>I am using a fine-tuned Huggingface model (on my company data) with the <a href=""https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TextClassificationPipeline"" rel=""nofollow noreferrer"">TextClassificationPipeline</a> to make class predictions. Now the labels that this <code>Pipeline</code> predicts defaults to <code>LABEL_0</code>, <code>LABEL_1</code> and so on. Is there a way to supply the label mappings to the <code>TextClassificationPipeline</code> object so that the output may reflect the same?</p>
<blockquote>
<p>Env:</p>
<ul>
<li>tensorflow==2.3.1</li>
<li>transformers==4.3.2</li>
</ul>
</blockquote>
<p>Sample Code:</p>
<pre><code>import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}

from transformers import TextClassificationPipeline, TFAutoModelForSequenceClassification, AutoTokenizer

MODEL_DIR = &quot;path\to\my\fine-tuned\model&quot;

# Feature extraction pipeline
model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)

pipeline = TextClassificationPipeline(model=model,
                                      tokenizer=tokenizer,
                                      framework='tf',
                                      device=0)

result = pipeline(&quot;It was a good watch. But a little boring.&quot;)[0]
</code></pre>
<p>Output:</p>
<pre><code>In [2]: result
Out[2]: {'label': 'LABEL_1', 'score': 0.8864616751670837}
</code></pre>
","10944913","","10944913","","2021-03-28 20:58:26","2021-03-28 20:58:26","How to set the label names when using the Huggingface TextClassificationPipeline?","<nlp><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66797173","1","67461785","","2021-03-25 10:06:55","","2","241","<p>I am using transformers pipeline to perform sentiment analysis on sample texts from 6 different languages. I tested the code in my local Jupyterhub and it worked fine. But when I wrap it in a flask application and create a docker image out of it, the execution is hanging at the pipeline inference line and its taking forever to return the sentiment scores.</p>
<ul>
<li>mac os catalina 10.15.7 (no GPU)</li>
<li>Python version : 3.8</li>
<li>Transformers package : 4.4.2</li>
<li>torch version : 1.6.0</li>
</ul>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
model_name = &quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
results = classifier([&quot;We are very happy to show you the Transformers library.&quot;, &quot;We hope you don't hate it.&quot;])
print([i['score'] for i in results])
</code></pre>
<p>The above code works fine in Jupyter notebook and it has provided me the expected result</p>
<pre><code>[0.7495927810668945,0.2365245819091797]
</code></pre>
<p>So now if I create a docker image with flask wrapper its getting stuck at the <code>results = classifier([input_data])</code> line and the execution is running forever.</p>
<p>My folder structure is as follows:</p>
<pre><code>- src
    |-- app
         |--main.py
    |-- Dockerfile
    |-- requirements.txt
</code></pre>
<p>I used the below <code>Dockerfile</code> to create the image</p>
<pre><code>FROM tiangolo/uwsgi-nginx-flask:python3.8
COPY ./requirements.txt /requirements.txt
COPY ./app /app
WORKDIR /app
RUN pip install -r /requirements.txt
RUN echo &quot;uwsgi_read_timeout 1200s;&quot; &gt; /etc/nginx/conf.d/custom_timeout.conf
</code></pre>
<p>And my <code>requirements.txt</code> file is as follows:</p>
<pre><code>pandas==1.1.5
transformers==4.4.2
torch==1.6.0
</code></pre>
<p>My <code>main.py</code> script look like this :</p>
<pre><code>from flask import Flask, json, request, jsonify
import traceback
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline


app = Flask(__name__)
app.config[&quot;JSON_SORT_KEYS&quot;] = False

model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
nlp = pipeline('sentiment-analysis', model=model_path, tokenizer=model_path)

@app.route(&quot;/&quot;)
def hello():
    return &quot;Model: Sentiment pipeline test&quot;


@app.route(&quot;/predict&quot;, methods=['POST'])
def predict():

    json_request = request.get_json(silent=True)
    input_list = [i['text'] for i in json_request[&quot;input_data&quot;]]
    
    results = nlp(input_list)         ##########  Getting stuck here
    for result in results:
        print(f&quot;label: {result['label']}, with score: {round(result['score'], 4)}&quot;)
    score_list = [round(i['score'], 4) for i in results]
    
    return jsonify(score_list)

if __name__ == &quot;__main__&quot;:
    app.run(host='0.0.0.0', debug=False, port=80)
</code></pre>
<p>My input payload is of the form</p>
<pre><code>{&quot;input_data&quot; : [{&quot;text&quot; : &quot;We are very happy to show you the Transformers library.&quot;},
                 {&quot;text&quot; : &quot;We hope you don't hate it.&quot;}]}
</code></pre>
<p>I tried looking into the transformers github issues but couldn't find one. I execution works fine even when using the flask development server but it runs forever when I wrap it and create a docker image. I am not sure if I am missing any additional dependency to be included while creating the docker image.</p>
<p>Thanks.</p>
","10422855","","400617","","2021-03-25 14:01:34","2021-05-09 19:48:30","Issue while using transformers package inside the docker image","<python><docker><pytorch><huggingface-transformers>","2","0","1","","","CC BY-SA 4.0"
"66828095","1","","","2021-03-27 04:47:54","","0","321","<p>The Hugging Face <code>transformers</code> library provide a tokenizer <code>GPT2Tokenizer</code> which is already pretrained. However, I want to train a tokenizer from scratch while using the same config as <code>GPT2Tokenizer</code> other than the <code>vocab_size</code>. This will be used to train a GPT model of another language from scratch.</p>
<p>Is it possible to retrain a <code>GPT2Tokenizer</code> using your own list of sentences/words?</p>
<p>Tried running the following code</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pre_tokenizer = Whitespace()
tokenizer.train_from_iterator(my_list_of_words, vocab_size=10000, min_frequency=1, 
    special_tokens=[
        &quot;&lt;s&gt;&quot;,
        &quot;&lt;/s&gt;&quot;,
        &quot;&lt;unk&gt;&quot;,
        &quot;&lt;mask&gt;&quot;,
        &quot;&lt;pad&gt;&quot;,
    ]
)
</code></pre>
<p>but its giving me the error</p>
<blockquote>
<p>AttributeError: 'GPT2Tokenizer' object has no attribute 'train_from_iterator'</p>
</blockquote>
<p>What is the correct method to use in this case? I am using <code>transformers</code> 4.4.2 and <code>tokenizers</code> 0.10.1. Thanks!</p>
","741099","","","","","2021-03-27 04:47:54","How to Train a Hugging Face GPT2Tokenizer using your own words?","<python><nlp><tokenize><huggingface-transformers><huggingface-tokenizers>","0","2","","","","CC BY-SA 4.0"
"66828446","1","","","2021-03-27 05:58:06","","1","177","<p>The default behavior of <code>Trainer(...)</code> in HuggingFace when evaluating model is disabling Dropout. Concretely, <code>y_pred</code> for <code>M</code> runs will be exactly the same</p>
<pre class=""lang-py prettyprint-override""><code>for i in range(M):
    logits, labels, metrics = trainer.predict(tokenized_datasets[&quot;eval&quot;])
    y_pred = np.argmax(logits, axis=2)
    ...
</code></pre>
<p>Now I am trying to apply Monte Carlo Dropout trick introduced <a href=""https://datascience.stackexchange.com/questions/44065/what-is-monte-carlo-dropout"">this this answer</a>. This requires to turn the Dropout <strong>on</strong> while making predictions on the validation set.</p>
<p>I am wondering how I achieve this goal. Any input is appreciated.</p>
","7784797","","","","","2021-03-27 15:59:15","Make predictions on on HuggingFace's BERT with Dropout on","<huggingface-transformers><dropout>","1","0","","","","CC BY-SA 4.0"
"66834205","1","","","2021-03-27 17:28:48","","0","828","<p>While trying to finetune a Huggingface <code>GPT2LMHeadModel</code> model for casual language modeling (given a sequence of words, predict the next word) using Pytorch Lightning, I am getting an error during training:</p>
<blockquote>
<p>AttributeError: 'str' object has no attribute 'size'</p>
</blockquote>
<p>What went wrong with our training code? Is this due to the incorrect use of <code>DataCollatorForLanguageModeling</code> in a Pytorch <code>DataLoader</code>?</p>
<p><strong>Reproducible Example:</strong></p>
<pre class=""lang-py prettyprint-override""><code>import os
from pathlib import Path
import torch
import pytorch_lightning as pl
from transformers import (
    GPT2Config,
    GPT2LMHeadModel,
    GPT2Tokenizer,
    DataCollatorForLanguageModeling,
)
from transformers.optimization import AdamW
from tokenizers import ByteLevelBPETokenizer
from torch.utils.data import (
    DataLoader,
    Dataset,
)

TOKENIZER_DIRPATH = os.path.join(&quot;..&quot;, &quot;data&quot;)


def tokenize_data():
    tokenizer = ByteLevelBPETokenizer()
    tokenizer.train(
        files=os.path.join(TOKENIZER_DIRPATH, &quot;words.txt&quot;),
        vocab_size=50000,
        min_frequency=2,
        special_tokens=[&quot;&lt;s&gt;&quot;, &quot;&lt;/s&gt;&quot;, &quot;&lt;unk&gt;&quot;, &quot;&lt;mask&gt;&quot;, &quot;&lt;pad&gt;&quot;,],
    )
    tokenizer.save_model(&quot;../data&quot;)


class MyDataset(Dataset):
    def __init__(self):
        tokenizer = GPT2Tokenizer(
            os.path.join(TOKENIZER_DIRPATH, &quot;vocab.json&quot;),
            os.path.join(TOKENIZER_DIRPATH, &quot;merges.txt&quot;),
        )

        src_file = Path(os.path.join(TOKENIZER_DIRPATH, &quot;words.txt&quot;))
        lines = src_file.read_text(encoding=&quot;utf-8&quot;).splitlines()
        self.examples = [tokenizer.encode(line) for line in lines]

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return torch.tensor(self.examples[i])


class MyDataModule(pl.LightningDataModule):
    def __init__(self):
        super().__init__()
        self.tokenizer = GPT2Tokenizer(
            os.path.join(TOKENIZER_DIRPATH, &quot;vocab.json&quot;),
            os.path.join(TOKENIZER_DIRPATH, &quot;merges.txt&quot;),
        )

    def setup(self, stage):
        self.train_dataset = MyDataset()

    def train_dataloader(self):
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer, mlm=False
        )
        train_dataloader = DataLoader(self.train_dataset, collate_fn=data_collator)
        return train_dataloader


class MyModel(pl.LightningModule):
    def __init__(self, learning_rate, adam_beta1, adam_beta2, adam_epsilon):
        super().__init__()
        self.save_hyperparameters()
        config = GPT2Config()
        self.model = GPT2LMHeadModel(config)

    def forward(self, x):
        return self.model(x).logits

    def training_step(self, batch, batch_idx):
        input_ids, labels = batch
        loss = self.model(input_ids, labels=labels).loss
        self.log(&quot;train_loss&quot;, loss, on_epoch=True)
        return loss

    def configure_optimizers(self):
        optimizer = AdamW(
            self.parameters(),
            self.hparams.learning_rate,
            betas=(self.hparams.adam_beta1, self.hparams.adam_beta2),
            eps=self.hparams.adam_epsilon,
        )
        return optimizer


tokenize_data()
dm = MyDataModule()
model = MyModel(
    learning_rate=5e-5, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-8,
)

trainer = pl.Trainer()
trainer.fit(model, dm)
</code></pre>
<p><strong>Error Traceback:</strong></p>
<pre><code>Epoch 0:   0%|                                                                                                                                                                                                                                             | 0/9 [00:00&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;test_gpt.py&quot;, line 102, in &lt;module&gt;
    trainer.fit(model, dm)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py&quot;, line 499, in fit
    self.dispatch()
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py&quot;, line 546, in dispatch
    self.accelerator.start_training(self)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py&quot;, line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py&quot;, line 114, in start_training
    self._results = trainer.run_train()
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py&quot;, line 637, in run_train
    self.train_loop.run_training_epoch()
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py&quot;, line 493, in run_training_epoch
    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py&quot;, line 655, in run_training_batch
    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py&quot;, line 426, in optimizer_step
    model_ref.optimizer_step(
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py&quot;, line 1387, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py&quot;, line 214, in step
    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py&quot;, line 134, in __optimizer_step
    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py&quot;, line 277, in optimizer_step
    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py&quot;, line 282, in run_optimizer_step
    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py&quot;, line 163, in optimizer_step
    optimizer.step(closure=lambda_closure, **kwargs)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/transformers/optimization.py&quot;, line 318, in step
    loss = closure()
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py&quot;, line 649, in train_step_and_backward_closure
    result = self.training_step_and_backward(
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py&quot;, line 743, in training_step_and_backward
    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py&quot;, line 293, in training_step
    training_step_output = self.trainer.accelerator.training_step(args)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py&quot;, line 156, in training_step
    return self.training_type_plugin.training_step(*args)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py&quot;, line 125, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File &quot;test_gpt.py&quot;, line 81, in training_step
    loss = self.model(input_ids, labels=labels).loss
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py&quot;, line 904, in forward
    transformer_outputs = self.transformer(
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/opt/anaconda3/envs/test_huggingface/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py&quot;, line 633, in forward
    input_shape = input_ids.size()
AttributeError: 'str' object has no attribute 'size'
</code></pre>
<p><strong>Conda Packages:</strong></p>
<pre><code>pytorch                   1.7.0           py3.8_cuda10.2.89_cudnn7.6.5_0    pytorch
pytorch-lightning         1.2.5              pyhd8ed1ab_0    conda-forge
tokenizers                0.10.1                   pypi_0    pypi
transformers              4.4.2                    pypi_0    pypi
</code></pre>
","741099","","741099","","2021-03-27 17:36:34","2021-05-04 12:06:18","Huggingface error during training: AttributeError: 'str' object has no attribute 'size'","<python><pytorch><huggingface-transformers><huggingface-tokenizers><pytorch-lightning>","1","2","","","","CC BY-SA 4.0"
"66855905","1","","","2021-03-29 14:17:43","","2","956","<p>I'm trying to use a tensorflow model with huggingface transformers. No success so far... Any hint would be appreciated !</p>
<pre><code>Python 3.7.4 (default, Aug 13 2019, 20:35:49)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import tensorflow
&gt;&gt;&gt; tensorflow.__version__
'2.0.0'
&gt;&gt;&gt; from transformers import TFBertModel
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
ImportError: cannot import name 'TFBertModel' from 'transformers' (unknown location)
&gt;&gt;&gt;
</code></pre>
","1731620","","","","","2021-03-29 14:17:43","ImportError: cannot import name 'TFBertModel' from 'transformers' (unknown location)","<tensorflow><huggingface-transformers>","0","1","0","","","CC BY-SA 4.0"
"66877374","1","","","2021-03-30 19:32:27","","0","35","<p>I am going through <a href=""https://huggingface.co/transformers/custom_datasets.html"" rel=""nofollow noreferrer"">Hugging Face's Tutorial on Fine-Tuning BERT with a custom dataset</a> but am unable to follow along as Google Colab has been executing for over an hour (so far) just trying to load the data.</p>
<p>The tutorial uses the IMDB Review dataset which I downloaded to my Google Drive account (in order to mimic how I will be getting data for my actual project). The dataset contains 50000 movie reviews in which each movie review is saved as a standalone txt file.</p>
<p>The code that I am trying to execute is the following (taken from the tutorial):</p>
<pre><code>from pathlib import Path

def read_imdb_split(split_dir):
    split_dir = Path(split_dir)
    texts = []
    labels = []
    for label_dir in [&quot;pos&quot;, &quot;neg&quot;]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is &quot;neg&quot; else 1)

    return texts, labels

train_texts, train_labels = read_imdb_split('/content/drive/MyDrive/aclImdb/train')
test_texts, test_labels = read_imdb_split('/content/drive/MyDrive/aclImdb/test')
</code></pre>
<p>It is only 210 MB so I do not understand how it could possibly be taking so long. Is it normal for it to be taking this long? What can I do?</p>
<p>I will also mention that I have Colab Pro and am using a GPU with High-RAM.</p>
","7254514","","7254514","","2021-03-30 19:40:08","2021-03-30 19:40:08","Loading text data (210 MB) from google drive to google colab is excruciatingly slow","<google-drive-api><google-colaboratory><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"66774033","1","","","2021-03-24 02:46:31","","0","56","<p>I want apply n-grams masked to masked Language Model in pre-train model using pytorch, Is there source code about it? or Just I must to Implementation it?</p>
<p>This is huggingface's code about <code>datacollator</code>.
<a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py</a></p>
<p>Thanks very much!</p>
","11951220","","","","","2021-04-01 02:39:48","Is there a like `datacollator` code can apply n-grams masked to masked Language Model using pytorch?","<bert-language-model><huggingface-transformers><pre-trained-model>","0","2","","","","CC BY-SA 4.0"
"66778632","1","","","2021-03-24 09:58:34","","0","283","<p>I'm trying to fine tune GPT-2 for the task of if I give five consecutive numbers, what are the next consecutive numbers. For example if <code>input_text = &quot;one | two | three | four | five&quot;</code>, <code>output_text = &quot;six | seven... | ten&quot;</code>.</p>
<p>The important bits of the model that I have used via the huggingface API is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>class Model(pl.LightningModule):
    def __init__(self, 
                 tokenizer, 
                 lr: float) -&gt; None:
        super().__init__()
        self.lr = lr
        self.tokenizer = Tokenizer(tokenizer)
        self.model = GPT2LMHeadModel.from_pretrained('gpt2')
        
    def common_step(self, batch: Tuple[List[str], List[str]]) -&gt; torch.FloatTensor:
        questions, answers = batch
        combined = [input + &quot; &lt;EOS&gt; &quot; + output for input, output in zip(questions, answers)]
        tokens = {k: v.to(self.device) for k, v in self.tokenizer(combined).items()}
        
        labels = tokens[&quot;input_ids&quot;].clone()
        labels[tokens[&quot;attention_mask&quot;]==0] = -100

        outputs = self.model(
            input_ids=tokens[&quot;input_ids&quot;], 
            attention_mask=tokens[&quot;attention_mask&quot;],
            labels=labels, 
            return_dict=True
        )
        
        return outputs[&quot;loss&quot;]
    
    def training_step(self, batch: Tuple[List[str], List[str]], *args) -&gt; torch.FloatTensor:
        loss = self.common_step(batch)
        return loss
        
    def generate_examples(self, batch):
        questions, answers = batch
        combined = [question + &quot; &lt;EOS&gt; &quot; for question in questions]
        tokens = {k: v.to(self.device) for k, v in self.tokenizer(combined).items()}

        generated = self.model.generate(
            input_ids=tokens[&quot;input_ids&quot;], 
            attention_mask=tokens[&quot;attention_mask&quot;], 
        )

        print(questions[0])
        print(&quot;=&quot;*30)
        print(self.tokenizer.decode(generated[0]))
</code></pre>
<p>The output I can is trying to spit out numbers but unfortunately looks like this. The output starts from where the  tag is shown, otherwise its just copying stuff. Note that GPT-2 tokenizer does not have :</p>
<pre><code>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
&lt;|endoftext|&gt;five thousand, five hundred and ninety-one| five thousand, five hundred and ninety-two| five thousand, five hundred and ninety-three| five thousand, five hundred and ninety-four| five thousand, five hundred and ninety-five &lt;EOS&gt; &lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt; fifteen thousand, four hundred and thirty-six| ten thousand, six hundred and sixty-seven| fifteen thousand and sixtyâ€‘eight| 15 thousand and eighty-nine| fifteen hundred and seventy&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;
</code></pre>
<p>So the question is <strong>why is it generating a possible candidate after a bunch of &lt;|endoftext|&gt; tokens</strong>. In the training set combine the input and output via a &quot;&quot; word (its not an actual token) and the output is there straight away without any padding.</p>
<p>Is it to do with the tokenizer that I've used which I've defined below?</p>
<pre class=""lang-py prettyprint-override""><code># make sure GPT2 appends EOS in begin and end
def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):
    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]
    return outputs
    
GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens
gpt2_tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
# set pad_token_id to unk_token_id -&gt; be careful here as unk_token_id == eos_token_id == bos_token_id
gpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token
</code></pre>
<p>A working example on colab can be found <a href=""https://colab.research.google.com/drive/1tpIsMizfJbJmXLRCWuoHlrwk9G_EZsah?usp=sharing"" rel=""nofollow noreferrer"">here</a>.</p>
","2530674","","","","","2021-03-24 09:58:34","Fine Tuning GPT-2","<nlp><pytorch><huggingface-transformers><gpt-2>","0","0","","","","CC BY-SA 4.0"
"66872821","1","","","2021-03-30 14:23:58","","2","271","<p>How to concatenate BERT-like sentence representation and word embeddings - Keras &amp; huggingface</p>
<p>I am following this Keras tutorial to combine Hugging Face transformers with other layers:
<a href=""https://keras.io/examples/nlp/text_extraction_with_bert/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/text_extraction_with_bert/</a></p>
<p>I want to concatenate the transformer embedding layer (included in the tutorial) with some regular word embeddings:</p>
<pre><code>encoder = TFBertModel.from_pretrained(&quot;bert-base-uncased&quot;)

input_ids = layers.Input(shape=(max_transformer_len,), dtype=tf.int32)
token_type_ids = layers.Input(shape=(max_transformer_len,), dtype=tf.int32)
attention_mask = layers.Input(shape=(max_transformer_len,), dtype=tf.int32)
embedding1 = encoder(
    input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]`

input_wordembedding = Input(shape=(max_sentence_len,), dtype='int32', 
                 name='we_input')
embedding2 = Embedding(output_dim=wordembedding_VECTOR_SIZE, 
                 input_dim=wordembedding_VOCAB_SIZE,
                 input_length=max_sentence_len,
                 weights = [emb_matrix],
                 name='emb1')(we_input)

z = Concatenate(name='merged')([embedding1, embedding2])
</code></pre>
<p>My problem is that the layer <code>embedding1</code> has sub-words representations and <code>embedding2</code> has words representations. Then, I want to do a max-pooling of the sub-words in the <code>embedding1</code> layer. In this way, I will obtain word representations with a transformer model.</p>
<p>Does anyone know how to approach this problem in Keras? If it is impossible to solve it with Keras, is it doable in PyTorch? How?</p>
","9522041","","","","","2021-03-30 14:23:58","How to concatenate BERT-like sentence representation and word embeddings - Keras & huggingface","<keras><nlp><pytorch><word-embedding><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"69060023","1","","","2021-09-05 01:35:19","","0","25","<p>I'm using BertForMaskedLM in Jupyter Notebooks. <code>output = model(masked_arr)</code> wouldn't run after 89-95 iterations. Each time I try running the cell it would fail after a different number of iterations in that range. (I tested this by making print statements before the line as well as a count of the iteration.) I'm on M1 Macbook Air 2020. I'm wondering if this is an issue with BertForMaskedLM/I'm using it the wrong way/hardware issue. The cell would show still running and not show any error.</p>
<pre><code>model = BertForMaskedLM.from_pretrained('bert-base-uncased')

imp = {}
for count, id_to_mask in enumerate(unique_ids):
    # mask all occurences of word
    masked_arr = torch.tensor([[103 if x==id_to_mask else x for x in input_ids]])
    # pass masked document into BertForMaskedLM
    output = model(masked_arr)
    # find indices of word in document
    idxs = np.where(input_ids==id_to_mask)[0]
    # find probability of word from bert output
    prob = output.logits[0][idxs][:,id_to_mask]
    # summation for all occurencesof word
    sum_prob = prob.sum()
    # debug
    print(count)
    # store in dict
    imp[id_to_mask] = sum_prob
</code></pre>
","10747624","","","","","2021-09-05 01:35:19","Huggingface BertForMaskedLM fails after 90+ iterations","<jupyter-notebook><huggingface-transformers><apple-m1>","0","6","","","","CC BY-SA 4.0"
"66811473","1","","","2021-03-26 05:27:15","","1","57","<p>I get this error message:</p>
<blockquote>
<p>AssertionError: You tried to initiate a model of type 'encoder-decoder' with a pretrained model of type 'encoder_decoder'</p>
</blockquote>
<p>when I try to run the following code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import LongformerTokenizer, EncoderDecoderModel

model = EncoderDecoderModel.from_pretrained(&quot;patrickvonplaten/longformer2roberta-cnn_dailymail-fp16&quot;)
tokenizer = LongformerTokenizer.from_pretrained(&quot;allenai/longformer-base-4096&quot;) 

article = &quot;&quot;&quot;(CNN)James Holmes made his introduction to the world in a Colorado cinema filled with spectators watching a midnight showing of the new Batman movie, The matter will be settled by the jury. CNN\'s Ana Cabrera and Sara Weisfeldt contributed to this report from Denver.&quot;&quot;&quot;

input_ids = tokenizer(article, return_tensors=&quot;pt&quot;).input_ids
output_ids = model.generate(input_ids)

print(tokenizer.decode(output_ids[0], skip_special_tokens=True))
</code></pre>
","15483763","","3607203","","2021-03-26 10:12:58","2021-03-26 10:12:58","AssertionError: You tried to initiate a model of type 'encoder-decoder' with a pretrained model of type 'encoder_decoder'","<nlp><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"66835293","1","","","2021-03-27 19:25:48","","0","27","<p>Deployed <a href=""https://github.com/Sonal778/summarizer"" rel=""nofollow noreferrer"">this</a> model on play-with-docker and build was successful but on Curl Post request I'm getting error.</p>
<p>Error Log -</p>
<blockquote>
 500
<p>Internal Server Error <h1>Internal Server Error</h1> <p>The
server encountered an internal error and was unable to complete your
request. Either the server is overloaded or there is an error in the
application.</p> curl: (6) Could not resolve host: \ curl: (6) Could
not resolve host: application curl: (6) Could not resolve host: <br />
curl: (3) [globbing] unmatched close brace/bracket in column 52</p>
</blockquote>
<p>But the strange part is, as I exit from cmd I can see my print commands processed properly even the <em>output</em> in logs.</p>
","13000354","","","","","2021-03-27 19:25:48","Error in output in ML model on Docker using Flask","<python><docker><machine-learning><flask><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"66870809","1","","","2021-03-30 12:17:50","","0","86","<p><a href=""https://gist.github.com/fvisin/578089ae098424590d3f25567b6ee255"" rel=""nofollow noreferrer"">This snippet</a> is a nice and easy-use CLI that renames checkpoint variables of a saved model. The
problem is that it was built to work with Tensorflow v1. In order to make it work with Tensorflow 2, i need to know if there's any alternative to <code>tensorflow.contrib.framework.list_variables</code> method, since it was deprecated and not migrated to TF2.</p>
<p>The main reason that i want to make use of this code is that i am trying to convert a TF checkpoint to a Hugginface (pytorch) checkpoint. Unfortunately i can't now because one of the variables was named differently.</p>
<p>Thanks in advance!</p>
","6529926","","","","","2021-03-30 12:17:50","tf.contrib.framework.list_variables in TF2","<python><tensorflow><tensorflow2.0><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"69072624","1","","","2021-09-06 09:57:53","","1","40","<p>i use tokenizers to train a Tokenizer and save the model like this</p>
<pre><code>tokenizer = Tokenizer(BPE())
tokenizer.pre_tokenizer = Whitespace()
tokenizer.decoder = ByteLevelDecoder()
trainer = BpeTrainer(vocab_size=25000, show_progress=True, initial_alphabet=ByteLevel.alphabet())

tokenizer.train(files=[&quot;/content/drive/MyDrive/Work/NLP/bert_practice/data/doc.txt&quot;], trainer=trainer)

tokenizer.model.save('/content/drive/MyDrive/Work/NLP/bert_practice/data/tokenizer')

['/content/drive/MyDrive/Work/NLP/bert_practice/data/tokenizer/vocab.json',
 '/content/drive/MyDrive/Work/NLP/bert_practice/data/tokenizer/merges.txt']
</code></pre>
<p>and it works well:</p>
<pre><code>tokenizer.encode(&quot;ä¸œé£Žæ—¥äº§2021æ¬¾åŠ²å®¢æ­£å¼ä¸Šå¸‚&quot;).tokens
['ä¸œé£Žæ—¥äº§', '2021æ¬¾', 'åŠ²å®¢', 'æ­£å¼ä¸Šå¸‚']
</code></pre>
<p>but when i load the model by transformers's BertTokenizer like this:</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer(
    vocab_file=&quot;/content/drive/MyDrive/Work/NLP/bert_practice/data/tokenizer/vocab.json&quot;,
    #merges_file=&quot;/content/drive/MyDrive/Work/NLP/bert_practice/data/tokenizer/merges.txt&quot;,
)
</code></pre>
<p>it always predict '[UNK]' as follows:</p>
<pre><code>tokenizer.tokenize(&quot;å¥¥è¿ªA5æœ‰ç€å¹´è½»æ—¶å°šçš„å¤–è§‚,åŠ¨åŠ›å¼ºã€æ“æŽ§ä¹Ÿå¾ˆæ£’&quot;)
['[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]',
 '[UNK]']
</code></pre>
<p>Could anyone figure out the problem?Any suggestion to solve this will be very helpful.</p>
","8300481","","","","","2021-09-07 09:29:25","how to use BertTokenizer to load Tokenizer model?","<tokenize><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"66824985","1","66833572","","2021-03-26 22:20:15","","1","148","<p>I am trying to tokenize some numerical strings using a <code>WordLevel</code>/<code>BPE</code> tokenizer, create a data collator and eventually use it in a PyTorch DataLoader to train a new model from scratch.</p>
<p>However, I am getting an error</p>
<blockquote>
<p>AttributeError: 'ByteLevelBPETokenizer' object has no attribute 'pad_token_id'</p>
</blockquote>
<p>when running the following code</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import DataCollatorForLanguageModeling
from tokenizers import ByteLevelBPETokenizer
from tokenizers.pre_tokenizers import Whitespace
from torch.utils.data import DataLoader, TensorDataset

data = ['4814 4832 4761 4523 4999 4860 4699 5024 4788 &lt;unk&gt;']

# Tokenizer
tokenizer = ByteLevelBPETokenizer()
tokenizer.pre_tokenizer = Whitespace()
tokenizer.train_from_iterator(data, vocab_size=1000, min_frequency=1, 
    special_tokens=[
        &quot;&lt;s&gt;&quot;,
        &quot;&lt;/s&gt;&quot;,
        &quot;&lt;unk&gt;&quot;,
        &quot;&lt;mask&gt;&quot;,
    ])

# Data Collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False
)

train_dataset = TensorDataset(torch.tensor(tokenizer(data, ......)))

# DataLoader
train_dataloader = DataLoader(
    train_dataset, 
    collate_fn=data_collator
)
</code></pre>
<p>Is this error due to not having configured the <code>pad_token_id</code> for the tokenizer? If so, how can we do this?</p>
<p>Thanks!</p>
<p><strong>Error trace:</strong></p>
<pre class=""lang-py prettyprint-override""><code>AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &quot;/opt/anaconda3/envs/x/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py&quot;, line 198, in _worker_loop
    data = fetcher.fetch(index)
  File &quot;/opt/anaconda3/envs/x/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 47, in fetch
    return self.collate_fn(data)
  File &quot;/opt/anaconda3/envs/x/lib/python3.8/site-packages/transformers/data/data_collator.py&quot;, line 351, in __call__
    if self.tokenizer.pad_token_id is not None:
AttributeError: 'ByteLevelBPETokenizer' object has no attribute 'pad_token_id'
</code></pre>
<p><strong>Conda packages</strong></p>
<pre><code>pytorch                   1.7.0           py3.8_cuda10.2.89_cudnn7.6.5_0    pytorch
pytorch-lightning         1.2.5              pyhd8ed1ab_0    conda-forge
tokenizers                0.10.1                   pypi_0    pypi
transformers              4.4.2                    pypi_0    pypi
</code></pre>
","3023615","","","","","2021-03-27 16:25:19","Huggingface error: AttributeError: 'ByteLevelBPETokenizer' object has no attribute 'pad_token_id'","<python><pytorch><tokenize><huggingface-transformers><huggingface-tokenizers>","1","2","","","","CC BY-SA 4.0"
"66889503","1","","","2021-03-31 14:11:33","","0","112","<p>Instead of printing the evaluation loss every epoch I would like to output it after every n-batches.</p>
<p>I have around 150'000 batches per epoch. I would like to output the evaluation loss every 50'000 batches.</p>
<p>Is this even possible? I am using pytorch and a pretrained bert model from huggingface.</p>
<p>My train loop:</p>
<pre><code>best_valid_loss = float('inf')
train_losses=[]
valid_losses=[]

for epoch in range(params['epochs']):
     
      print('\n Epoch {:} / {:}'.format(epoch + 1, params['epochs']))
      
      #train model
      train_loss = train(scheduler, optimizer)
      
      #evaluate model
      valid_loss = evaluate()
      
      #save the best model
      if valid_loss &lt; best_valid_loss:
          best_valid_loss = valid_loss
          torch.save(model.state_dict(), model_file)
      
      # append training and validation loss
      train_losses.append(train_loss)
      valid_losses.append(valid_loss)
      
      print(f'\nTraining Loss: {train_loss:.3f}')
      print(f'Validation Loss: {valid_loss:.3f}')
</code></pre>
","14829523","","14829523","","2021-03-31 19:56:24","2021-03-31 19:56:24","Output evaluation loss after every n-batches instead of epochs with pytorch","<python><pytorch><huggingface-transformers>","1","4","","","","CC BY-SA 4.0"
"69061884","1","","","2021-09-05 08:48:25","","0","39","<p>I would like to train Hugging face MBart model to be able to translate between Hungarian and English or any other unsupported language (it currently supports 25 languages ).
I have read the original documentation and a <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">tutorial</a> on how to train a language model on their website but I am not sure how translate that to what I want to do or if that is of any benefit in the first place. I would appreciate any idea or reference.</p>
","6436355","","","","","2021-09-05 08:48:25","Training and fine tuning MBart on a specefic (unsupported yet) language","<python><nlp><huggingface-transformers><machine-translation>","0","0","","","","CC BY-SA 4.0"
"69065131","1","","","2021-09-05 16:15:39","","0","31","<p>I am trying to use <a href=""https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment"" rel=""nofollow noreferrer"">this</a> huggingface model and have been following the example provided, but I am getting an error when loading the tokenizer:</p>
<pre><code>from transformers import AutoTokenizer

task = 'sentiment'
MODEL = f&quot;cardiffnlp/twitter-roberta-base-{task}&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL)
</code></pre>
<blockquote>
<p>OSError: Can't load tokenizer for 'cardiffnlp/twitter-roberta-base-sentiment'. Make sure that:</p>
<ul>
<li><p>'cardiffnlp/twitter-roberta-base-sentiment' is a correct model identifier listed on 'https://huggingface.co/models'</p>
</li>
<li><p>or 'cardiffnlp/twitter-roberta-base-sentiment' is the correct path to a directory containing relevant tokenizer files</p>
</li>
</ul>
</blockquote>
<p>What I find very weird is that I was able to run my script several times but ran into an error after some time, while I don't recall changing anything in the meantime. Does anyone know what's the solution here?</p>
<hr />
<p>EDIT: Here is my entire script:</p>
<pre><code>from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
import numpy as np
from scipy.special import softmax
import csv
import urllib.request

task = 'sentiment'
MODEL = f&quot;nlptown/bert-base-multilingual-uncased-{task}&quot;

tokenizer = AutoTokenizer.from_pretrained(MODEL)

labels = ['very_negative', 'negative', 'neutral', 'positive', 'very_positive']

model = AutoModelForSequenceClassification.from_pretrained(MODEL)
model.save_pretrained(MODEL)

text = &quot;I love you&quot;
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0].detach().numpy()
scores = softmax(scores)

print(scores)
</code></pre>
<p>The error seems to start happening when I run <code>model.save_pretrained(MODEL)</code>, but this might be a coincidence.</p>
","16463112","","16463112","","2021-09-05 16:29:27","2021-09-05 16:29:27","OSError when loading tokenizer for huggingface model","<python><huggingface-transformers>","0","5","","","","CC BY-SA 4.0"
"69077472","1","","","2021-09-06 16:02:45","","0","82","<p>I pretrained a custom NLP model in PyTorch and saved the last checkpoint as &quot;model.pt&quot;. Now I want to integrate this model in the Huggingface Transformers environment, so it can be finetuned with different heads etc.
How do I convert this PyTorch-model to a Huggingface-model? As far as I understand it, I have to somehow generate a set of configuration files?</p>
","12410833","","6664872","","2021-09-06 20:18:41","2021-09-06 20:18:41","How can I convert a custom PyTorch model (model.pt) to a Huggingface Transformers model, so I can load it with from_pretrained()?","<pytorch><huggingface-transformers>","0","4","","","","CC BY-SA 4.0"
"66906652","1","","","2021-04-01 14:43:41","","1","493","<p><strong>How to download hugging face sentiment-analysis pipeline to use it offline?</strong> I'm unable to use hugging face sentiment analysis pipeline without internet. How to download that pipeline?</p>
<p>The basic code for sentiment analysis using hugging face is</p>
<pre><code>from transformers import pipeline
classifier = pipeline('sentiment-analysis') #This code will download the pipeline
classifier('We are very happy to show you the ðŸ¤— Transformers library.')
</code></pre>
<p>And the output is</p>
<pre><code>[{'label': 'POSITIVE', 'score': 0.9997795224189758}]
</code></pre>
","15224778","","","","","2021-04-01 22:02:33","How to download hugging face sentiment-analysis pipeline to use it offline?","<deep-learning><nlp><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"66901602","1","66905568","","2021-04-01 09:07:34","","1","145","<p>I am following Rostylav's tutorial found <a href=""https://colab.research.google.com/drive/15wa925dj7jvdvrz8_z3vU7btqAFQLVlG#scrollTo=7KrNfVNueNhR"" rel=""nofollow noreferrer"">here</a> and am runnning into an error I dont quite understand:</p>
<pre><code>AttributeError                            
Traceback (most recent call last)
&lt;ipython-input-22-523c0d2a27d3&gt; in &lt;module&gt;()
----&gt; 1 main(trn_df, val_df)

&lt;ipython-input-20-1f17c050b9e5&gt; in main(df_trn, df_val)
     59     # Training
     60     if args.do_train:
---&gt; 61         train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)
     62 
     63         global_step, tr_loss = train(args, train_dataset, model, tokenizer)

&lt;ipython-input-18-3c4f1599e14e&gt; in load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate)
     40 
     41 def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):
---&gt; 42     return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)
     43 
     44 def set_seed(args):

&lt;ipython-input-18-3c4f1599e14e&gt; in __init__(self, tokenizer, args, df, block_size)
      8     def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):
      9 
---&gt; 10         block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)
     11 
     12         directory = args.cache_dir

AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len'
</code></pre>
<p>This is the class I believe is causing the error, however I am not able to understand what Tokenize.max_len is supposed to do so I can try to fix it:</p>
<pre><code>   class ConversationDataset(Dataset):

    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):

        block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)

        directory = args.cache_dir
        cached_features_file = os.path.join(
            directory, args.model_type + &quot;_cached_lm_&quot; + str(block_size)
        )

        if os.path.exists(cached_features_file) and not args.overwrite_cache:
            logger.info(&quot;Loading features from cached file %s&quot;, cached_features_file)
            with open(cached_features_file, &quot;rb&quot;) as handle:
                self.examples = pickle.load(handle)
        else:
            logger.info(&quot;Creating features from dataset file at %s&quot;, directory)

            self.examples = []
            for _, row in df.iterrows():
                conv = construct_conv(row, tokenizer)
                self.examples.append(conv)

            logger.info(&quot;Saving features into cached file %s&quot;, cached_features_file)
            with open(cached_features_file, &quot;wb&quot;) as handle:
                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, item):
        return torch.tensor(self.examples[item], dtype=torch.long)
 
# Cacheing and storing of data/checkpoints

def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):
    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)
</code></pre>
<p>Thank you for reading!</p>
","6010395","","","","","2021-04-01 13:38:17","What is tokenizer.max len doing in this class definition?","<python><google-colaboratory><huggingface-transformers><huggingface-tokenizers><gpt-2>","1","0","","","","CC BY-SA 4.0"
"64669365","1","","","2020-11-03 19:29:48","","3","2782","<p>I am using Huggingface BERT for an NLP task. My texts contain names of companies which are split up into subwords.</p>
<pre><code>tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
tokenizer.encode_plus(&quot;Somespecialcompany&quot;)
output: {'input_ids': [101, 2070, 13102, 8586, 4818, 9006, 9739, 2100, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre>
<p>Now I would like to add those names to the tokenizer IDs so they are not split up.</p>
<pre><code>tokenizer.add_tokens(&quot;Somespecialcompany&quot;)
output: 1
</code></pre>
<p>This extends the lenght of the tokenizer from 30522 to 30523.</p>
<p>The desired output would therefore be the new ID:</p>
<pre><code>tokenizer.encode_plus(&quot;Somespecialcompany&quot;)
output: 30522
</code></pre>
<p>But the output is the same as before:</p>
<pre><code>output: {'input_ids': [101, 2070, 13102, 8586, 4818, 9006, 9739, 2100, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre>
<p>So my questions is; what is the right way of adding new tokens to the tokenizer so I can use them with tokenizer.encode_plus() and tokenizer.batch_encode_plus()?</p>
","11324008","","","","","2020-11-05 16:17:44","Huggingface BERT Tokenizer add new token","<bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","1","","","","CC BY-SA 4.0"
"66761988","1","","","2021-03-23 11:11:20","","1","122","<p>I am trying to fine-tune model for token classification (named entity recognition) with dataset consisting of 9 unique tokens.</p>
<p>I am mainly interested in 2 models from hugging face transformers library, which are:<br></p>
<ol>
<li><a href=""https://huggingface.co/DeepPavlov/bert-base-bg-cs-pl-ru-cased"" rel=""nofollow noreferrer"">https://huggingface.co/DeepPavlov/bert-base-bg-cs-pl-ru-cased</a></li>
<li><a href=""https://huggingface.co/dkleczek/bert-base-polish-cased-v1"" rel=""nofollow noreferrer"">https://huggingface.co/dkleczek/bert-base-polish-cased-v1</a></li>
</ol>
<p>The first one works fine, there is no mismatch between classifier's output features number and tensor size. <br></p>
<pre><code>(classifier): Linear(in_features=768, out_features=9, bias=True)
...
torch.Size([320, 9])
</code></pre>
<p>Fine-tuning the second one leads to assertion errors which are probably caused by mismatch between classifier's output features number and tensor size.</p>
<pre><code>(classifier): Linear(in_features=768, out_features=9, bias=True)
...
torch.Size([336, 2])
</code></pre>
<p>I have compared both model's parameters and they are entirely the same with exception of <code>word_embeddings</code> number which I believe is not relevant in this case.</p>
<p>I would appreciate any tips on how I could solve that issue or indications towards things that I may be missing.</p>
<p>Model initialisation step:</p>
<pre><code>    model = AutoModelForTokenClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(&quot;.ckpt&quot; in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
</code></pre>
<p>Whole script that I am using - <code>run_ner.py</code>:</p>
<pre class=""lang-py prettyprint-override""><code>
#!/usr/bin/env python
# coding=utf-8
# Copyright 2020 The HuggingFace Team All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
&quot;&quot;&quot;
Fine-tuning the library models for token classification.
&quot;&quot;&quot;
# You can also adapt this script on your own token classification task and datasets. Pointers for this are left as
# comments.

import logging
import os
import sys
from dataclasses import dataclass, field
from typing import Optional

import numpy as np
from datasets import ClassLabel, load_dataset, load_metric

import transformers
from transformers import (
    AutoConfig,
    AutoModelForTokenClassification,
    AutoTokenizer,
    DataCollatorForTokenClassification,
    HfArgumentParser,
    PreTrainedTokenizerFast,
    Trainer,
    TrainingArguments,
    set_seed,
)
from transformers.trainer_utils import get_last_checkpoint, is_main_process
from transformers.utils import check_min_version


# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
#check_min_version(&quot;4.5.0.dev0&quot;)

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:
    &quot;&quot;&quot;
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    &quot;&quot;&quot;

    model_name_or_path: str = field(
        metadata={&quot;help&quot;: &quot;Path to pretrained model or model identifier from huggingface.co/models&quot;}
    )
    config_name: Optional[str] = field(
        default=None, metadata={&quot;help&quot;: &quot;Pretrained config name or path if not the same as model_name&quot;}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={&quot;help&quot;: &quot;Pretrained tokenizer name or path if not the same as model_name&quot;}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={&quot;help&quot;: &quot;Where do you want to store the pretrained models downloaded from huggingface.co&quot;},
    )
    model_revision: str = field(
        default=&quot;main&quot;,
        metadata={&quot;help&quot;: &quot;The specific model version to use (can be a branch name, tag name or commit id).&quot;},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            &quot;help&quot;: &quot;Will use the token generated when running `transformers-cli login` (necessary to use this script &quot;
            &quot;with private models).&quot;
        },
    )


@dataclass
class DataTrainingArguments:
    &quot;&quot;&quot;
    Arguments pertaining to what data we are going to input our model for training and eval.
    &quot;&quot;&quot;

    task_name: Optional[str] = field(default=&quot;ner&quot;, metadata={&quot;help&quot;: &quot;The name of the task (ner, pos...).&quot;})
    dataset_name: Optional[str] = field(
        default=None, metadata={&quot;help&quot;: &quot;The name of the dataset to use (via the datasets library).&quot;}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={&quot;help&quot;: &quot;The configuration name of the dataset to use (via the datasets library).&quot;}
    )
    train_file: Optional[str] = field(
        default=None, metadata={&quot;help&quot;: &quot;The input training data file (a csv or JSON file).&quot;}
    )
    validation_file: Optional[str] = field(
        default=None,
        metadata={&quot;help&quot;: &quot;An optional input evaluation data file to evaluate on (a csv or JSON file).&quot;},
    )
    test_file: Optional[str] = field(
        default=None,
        metadata={&quot;help&quot;: &quot;An optional input test data file to predict on (a csv or JSON file).&quot;},
    )
    overwrite_cache: bool = field(
        default=False, metadata={&quot;help&quot;: &quot;Overwrite the cached training and evaluation sets&quot;}
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={&quot;help&quot;: &quot;The number of processes to use for the preprocessing.&quot;},
    )
    pad_to_max_length: bool = field(
        default=False,
        metadata={
            &quot;help&quot;: &quot;Whether to pad all samples to model maximum sentence length. &quot;
            &quot;If False, will pad the samples dynamically when batching to the maximum length in the batch. More &quot;
            &quot;efficient on GPU but very bad for TPU.&quot;
        },
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            &quot;help&quot;: &quot;For debugging purposes or quicker training, truncate the number of training examples to this &quot;
            &quot;value if set.&quot;
        },
    )
    max_val_samples: Optional[int] = field(
        default=None,
        metadata={
            &quot;help&quot;: &quot;For debugging purposes or quicker training, truncate the number of validation examples to this &quot;
            &quot;value if set.&quot;
        },
    )
    max_test_samples: Optional[int] = field(
        default=None,
        metadata={
            &quot;help&quot;: &quot;For debugging purposes or quicker training, truncate the number of test examples to this &quot;
            &quot;value if set.&quot;
        },
    )
    label_all_tokens: bool = field(
        default=False,
        metadata={
            &quot;help&quot;: &quot;Whether to put the label for one word on all tokens of generated by that word or just on the &quot;
            &quot;one (in which case the other tokens will have a padding index).&quot;
        },
    )
    return_entity_level_metrics: bool = field(
        default=False,
        metadata={&quot;help&quot;: &quot;Whether to return all the entity levels during evaluation or just the overall ones.&quot;},
    )

    def __post_init__(self):
        if self.dataset_name is None and self.train_file is None and self.validation_file is None:
            raise ValueError(&quot;Need either a dataset name or a training/validation file.&quot;)
        else:
            if self.train_file is not None:
                extension = self.train_file.split(&quot;.&quot;)[-1]
                assert extension in [&quot;csv&quot;, &quot;json&quot;], &quot;`train_file` should be a csv or a json file.&quot;
            if self.validation_file is not None:
                extension = self.validation_file.split(&quot;.&quot;)[-1]
                assert extension in [&quot;csv&quot;, &quot;json&quot;], &quot;`validation_file` should be a csv or a json file.&quot;
        self.task_name = self.task_name.lower()


def main():
    # See all possible arguments in src/transformers/training_args.py
    # or by passing the --help flag to this script.
    # We now keep distinct sets of args, for a cleaner separation of concerns.

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(&quot;.json&quot;):
        # If we pass only one argument to the script and it's the path to a json file,
        # let's parse it to get our arguments.
        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # Detecting last checkpoint.
    last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) &gt; 0:
            raise ValueError(
                f&quot;Output directory ({training_args.output_dir}) already exists and is not empty. &quot;
                &quot;Use --overwrite_output_dir to overcome.&quot;
            )
        elif last_checkpoint is not None:
            logger.info(
                f&quot;Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change &quot;
                &quot;the `--output_dir` or add `--overwrite_output_dir` to train from scratch.&quot;
            )

    # Setup logging
    logging.basicConfig(
        format=&quot;%(asctime)s - %(levelname)s - %(name)s -   %(message)s&quot;,
        datefmt=&quot;%m/%d/%Y %H:%M:%S&quot;,
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)

    # Log on each process the small summary:
    logger.warning(
        f&quot;Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}&quot;
        + f&quot;distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}&quot;
    )
    # Set the verbosity to info of the Transformers logger (on main process only):
    if is_main_process(training_args.local_rank):
        transformers.utils.logging.set_verbosity_info()
        transformers.utils.logging.enable_default_handler()
        transformers.utils.logging.enable_explicit_format()
    logger.info(&quot;Training/evaluation parameters %s&quot;, training_args)

    # Set seed before initializing model.
    set_seed(training_args.seed)

    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)
    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/
    # (the dataset will be downloaded automatically from the datasets Hub).
    #
    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called
    # 'text' is found. You can easily tweak this behavior (see below).
    #
    # In distributed training, the load_dataset function guarantee that only one local process can concurrently
    # download the dataset.
    if data_args.dataset_name is not None:
        # Downloading and loading a dataset from the hub.
        datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)
    else:
        data_files = {}
        if data_args.train_file is not None:
            data_files[&quot;train&quot;] = data_args.train_file
        if data_args.validation_file is not None:
            data_files[&quot;validation&quot;] = data_args.validation_file
        if data_args.test_file is not None:
            data_files[&quot;test&quot;] = data_args.test_file
        extension = data_args.train_file.split(&quot;.&quot;)[-1]
        datasets = load_dataset(extension, data_files=data_files)
    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at
    # https://huggingface.co/docs/datasets/loading_datasets.html.

    if training_args.do_train:
        column_names = datasets[&quot;train&quot;].column_names
        features = datasets[&quot;train&quot;].features
    else:
        column_names = datasets[&quot;validation&quot;].column_names
        features = datasets[&quot;validation&quot;].features
    text_column_name = &quot;tokens&quot; if &quot;tokens&quot; in column_names else column_names[0]
    label_column_name = (
        f&quot;{data_args.task_name}_tags&quot; if f&quot;{data_args.task_name}_tags&quot; in column_names else column_names[1]
    )

    # In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the
    # unique labels.
    def get_label_list(labels):
        unique_labels = set()
        for label in labels:
            unique_labels = unique_labels | set(label)
        label_list = list(unique_labels)
        label_list.sort()
        return label_list

    if isinstance(features[label_column_name].feature, ClassLabel):
        label_list = features[label_column_name].feature.names
        # No need to convert the labels since they are already ints.
        label_to_id = {i: i for i in range(len(label_list))}
    else:
        label_list = get_label_list(datasets[&quot;train&quot;][label_column_name])
        label_to_id = {l: i for i, l in enumerate(label_list)}
    num_labels = len(label_list)

    label2id = {label: i for i, label in enumerate(label_list)}
    print(&quot;\nlabel_list\n&quot;, label_list)
    print(&quot;\nlabel_to_id\n&quot;, label_to_id)
    print(&quot;\nnum_labels\n&quot;, num_labels)

    # label2id=label_to_id,
    id2label={id: label for label, id in label_to_id.items()}
    #print(label2id)
    #print(id2label)

    
    # Load pretrained model and tokenizer
    #
    # Distributed training:
    # The .from_pretrained methods guarantee that only one local process can concurrently
    # download model &amp; vocab.
    config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        num_labels=num_labels,

        label2id=label_to_id,
        id2label={id: label for label, id in label_to_id.items()},

        finetuning_task=data_args.task_name,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
        cache_dir=model_args.cache_dir,
        use_fast=True,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    model = AutoModelForTokenClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(&quot;.ckpt&quot; in model_args.model_name_or_path),
        #from_tf=True,
        config=config,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )

    # Tokenizer check: this script requires a fast tokenizer.
    if not isinstance(tokenizer, PreTrainedTokenizerFast):
        raise ValueError(
            &quot;This example script only works for models that have a fast tokenizer. Checkout the big table of models &quot;
            &quot;at https://huggingface.co/transformers/index.html#bigtable to find the model types that meet this &quot;
            &quot;requirement&quot;
        )

    # Preprocessing the dataset
    # Padding strategy
    padding = &quot;max_length&quot; if data_args.pad_to_max_length else False

    # Tokenize all texts and align the labels with them.
    def tokenize_and_align_labels(examples):
        # print(&quot;examples\n&quot;, examples)
        tokenized_inputs = tokenizer(
            examples[text_column_name],
            padding=padding,
            truncation=True,
            # We use this argument because the texts in our dataset are lists of words (with a label for each word).
            is_split_into_words=True,
        )
        labels = []
        for i, label in enumerate(examples[label_column_name]):
            # print(&quot;\nlabel in enumerate(examples[label_column_name])\n&quot;, label)
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            previous_word_idx = None
            label_ids = []
            for word_idx in word_ids:
                # Special tokens have a word id that is None. We set the label to -100 so they are automatically
                # ignored in the loss function.
                if word_idx is None:
                    label_ids.append(-100)
                # We set the label for the first token of each word.
                elif word_idx != previous_word_idx:
                    label_ids.append(label_to_id[label[word_idx]])
                    # label_ids.append(label2id[label[word_idx]])

                # For the other tokens in a word, we set the label to either the current label or -100, depending on
                # the label_all_tokens flag.
                else:
                    label_ids.append(label_to_id[label[word_idx]] if data_args.label_all_tokens else -100)
                    # label_ids.append(label2id[label[word_idx]] if data_args.label_all_tokens else -100)

                previous_word_idx = word_idx

            labels.append(label_ids)
        tokenized_inputs[&quot;labels&quot;] = labels
        # print(&quot;label_to_id\n&quot;, label_to_id)
        # print(&quot;\n\n tokenized_inputs\n&quot;, tokenized_inputs)
        return tokenized_inputs

    if training_args.do_train:
        if &quot;train&quot; not in datasets:
            raise ValueError(&quot;--do_train requires a train dataset&quot;)
        train_dataset = datasets[&quot;train&quot;]
        if data_args.max_train_samples is not None:
            train_dataset = train_dataset.select(range(data_args.max_train_samples))
        train_dataset = train_dataset.map(
            tokenize_and_align_labels,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            load_from_cache_file=not data_args.overwrite_cache,
        )

    if training_args.do_eval:
        if &quot;validation&quot; not in datasets:
            raise ValueError(&quot;--do_eval requires a validation dataset&quot;)
        eval_dataset = datasets[&quot;validation&quot;]
        if data_args.max_val_samples is not None:
            eval_dataset = eval_dataset.select(range(data_args.max_val_samples))
        eval_dataset = eval_dataset.map(
            tokenize_and_align_labels,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            load_from_cache_file=not data_args.overwrite_cache,
        )

    if training_args.do_predict:
        if &quot;test&quot; not in datasets:
            raise ValueError(&quot;--do_predict requires a test dataset&quot;)
        test_dataset = datasets[&quot;test&quot;]
        if data_args.max_test_samples is not None:
            test_dataset = test_dataset.select(range(data_args.max_test_samples))
        test_dataset = test_dataset.map(
            tokenize_and_align_labels,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            load_from_cache_file=not data_args.overwrite_cache,
        )

    # Data collator
    data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)

    # Metrics
    metric = load_metric(&quot;seqeval&quot;)

    def compute_metrics(p):
        predictions, labels = p
        predictions = np.argmax(predictions, axis=2)

        # Remove ignored index (special tokens)
        true_predictions = [
            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        true_labels = [
            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]

        results = metric.compute(predictions=true_predictions, references=true_labels)
        if data_args.return_entity_level_metrics:
            # Unpack nested dictionaries
            final_results = {}
            for key, value in results.items():
                if isinstance(value, dict):
                    for n, v in value.items():
                        final_results[f&quot;{key}_{n}&quot;] = v
                else:
                    final_results[key] = value
            return final_results
        else:
            return {
                &quot;precision&quot;: results[&quot;overall_precision&quot;],
                &quot;recall&quot;: results[&quot;overall_recall&quot;],
                &quot;f1&quot;: results[&quot;overall_f1&quot;],
                &quot;accuracy&quot;: results[&quot;overall_accuracy&quot;],
            }

    # Initialize our Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    print(&quot;\nmodel parameters\n&quot;, model.parameters)

    # Training
    if training_args.do_train:
        if last_checkpoint is not None:
            checkpoint = last_checkpoint
        elif os.path.isdir(model_args.model_name_or_path):
            checkpoint = model_args.model_name_or_path
        else:
            checkpoint = None
        train_result = trainer.train(resume_from_checkpoint=checkpoint)
        metrics = train_result.metrics
        trainer.save_model()  # Saves the tokenizer too for easy upload

        max_train_samples = (
            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
        )
        metrics[&quot;train_samples&quot;] = min(max_train_samples, len(train_dataset))

        trainer.log_metrics(&quot;train&quot;, metrics)
        trainer.save_metrics(&quot;train&quot;, metrics)
        trainer.save_state()

    # Evaluation
    if training_args.do_eval:
        logger.info(&quot;*** Evaluate ***&quot;)

        metrics = trainer.evaluate()

        max_val_samples = data_args.max_val_samples if data_args.max_val_samples is not None else len(eval_dataset)
        metrics[&quot;eval_samples&quot;] = min(max_val_samples, len(eval_dataset))

        trainer.log_metrics(&quot;eval&quot;, metrics)
        trainer.save_metrics(&quot;eval&quot;, metrics)

    # Predict
    if training_args.do_predict:
        logger.info(&quot;*** Predict ***&quot;)

        predictions, labels, metrics = trainer.predict(test_dataset)
        predictions = np.argmax(predictions, axis=2)

        # Remove ignored index (special tokens)
        true_predictions = [
            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]

        trainer.log_metrics(&quot;test&quot;, metrics)
        trainer.save_metrics(&quot;test&quot;, metrics)

        # Save predictions
        output_test_predictions_file = os.path.join(training_args.output_dir, &quot;test_predictions.txt&quot;)
        if trainer.is_world_process_zero():
            with open(output_test_predictions_file, &quot;w&quot;) as writer:
                for prediction in true_predictions:
                    writer.write(&quot; &quot;.join(prediction) + &quot;\n&quot;)


def _mp_fn(index):
    # For xla_spawn (TPUs)
    main()


if __name__ == &quot;__main__&quot;:
    main()

</code></pre>
<p><code>environment.yml</code></p>
<pre><code>name: test_3_1
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - _libgcc_mutex=0.1=conda_forge
  - _openmp_mutex=4.5=1_llvm
  - blas=1.0=mkl
  - bzip2=1.0.8=h7f98852_4
  - ca-certificates=2020.12.5=ha878542_0
  - certifi=2020.12.5=py38h578d9bd_1
  - cudatoolkit=11.1.1=h6406543_8
  - ffmpeg=4.3=hf484d3e_0
  - freetype=2.10.4=h0708190_1
  - gmp=6.2.1=h58526e2_0
  - gnutls=3.6.13=h85f3911_1
  - jpeg=9b=h024ee3a_2
  - lame=3.100=h7f98852_1001
  - lcms2=2.11=h396b838_0
  - ld_impl_linux-64=2.33.1=h53a641e_7
  - libffi=3.3=he6710b0_2
  - libgcc-ng=9.3.0=h2828fa1_18
  - libiconv=1.16=h516909a_0
  - libpng=1.6.37=h21135ba_2
  - libstdcxx-ng=9.3.0=h6de172a_18
  - libtiff=4.1.0=h2733197_1
  - libuv=1.41.0=h7f98852_0
  - llvm-openmp=11.0.1=h4bd325d_0
  - lz4-c=1.9.3=h9c3ff4c_0
  - mkl=2020.4=h726a3e6_304
  - mkl-service=2.3.0=py38h1e0a361_2
  - mkl_fft=1.3.0=py38h5c078b8_1
  - mkl_random=1.2.0=py38hc5bc63f_1
  - ncurses=6.2=he6710b0_1
  - nettle=3.6=he412f7d_0
  - ninja=1.10.2=h4bd325d_0
  - numpy-base=1.19.2=py38hfa32c7d_0
  - olefile=0.46=pyh9f0ad1d_1
  - openh264=2.1.1=h780b84a_0
  - openssl=1.1.1j=h7f98852_0
  - pillow=8.1.2=py38he98fc37_0
  - pip=21.0.1=py38h06a4308_0
  - python=3.8.8=hdb3f193_4
  - python_abi=3.8=1_cp38
  - pytorch=1.8.0=py3.8_cuda11.1_cudnn8.0.5_0
  - readline=8.1=h27cfd23_0
  - setuptools=52.0.0=py38h06a4308_0
  - six=1.15.0=pyh9f0ad1d_0
  - sqlite=3.35.1=hdfb4753_0
  - tk=8.6.10=hbc83047_0
  - torchaudio=0.8.0=py38
  - torchvision=0.9.0=py38_cu111
  - typing_extensions=3.7.4.3=py_0
  - wheel=0.36.2=pyhd3eb1b0_0
  - xz=5.2.5=h7b6447c_0
  - zlib=1.2.11=h7b6447c_3
  - zstd=1.4.9=ha95c52a_0
  - pip:
    - chardet==4.0.0
    - click==7.1.2
    - datasets==1.4.1
    - dill==0.3.3
    - filelock==3.0.12
    - fsspec==0.8.7
    - huggingface-hub==0.0.2
    - idna==2.10
    - joblib==1.0.1
    - multiprocess==0.70.11.1
    - numpy==1.20.1
    - packaging==20.9
    - pandas==1.2.3
    - pyarrow==3.0.0
    - pyparsing==2.4.7
    - python-dateutil==2.8.1
    - pytz==2021.1
    - regex==2020.11.13
    - requests==2.25.1
    - sacremoses==0.0.43
    - scikit-learn==0.24.1
    - scipy==1.6.1
    - seqeval==1.2.2
    - threadpoolctl==2.1.0
    - tokenizers==0.10.1
    - tqdm==4.49.0
    - transformers==4.4.1
    - urllib3==1.26.4
    - xxhash==2.0.0
</code></pre>
<p>1 line sample of dataset - <code>single_line.json</code>:<br>
<strong>Below sample consists 3 unique labels instead of 9.</strong></p>
<pre class=""lang-json prettyprint-override""><code>{&quot;words&quot;: [&quot;AstronomiÄ™&quot;, &quot;moÅ¼na&quot;, &quot;inaczej&quot;, &quot;okreÅ›liÄ‡&quot;, &quot;jako&quot;, &quot;naukÄ™&quot;, &quot;o&quot;, &quot;wszelkich&quot;, &quot;obiektach&quot;, &quot;i&quot;, &quot;zjawiskach&quot;, &quot;znajdujÄ…cych&quot;, &quot;siÄ™&quot;, &quot;poza&quot;, &quot;ZiemiÄ…&quot;, &quot;.&quot;], &quot;ner&quot;: [&quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;B-LOC&quot;, &quot;I-LOC&quot;, &quot;O&quot;]}
</code></pre>
<p>Execution calls:</p>
<ol>
<li>Works.</li>
</ol>
<pre class=""lang-sh prettyprint-override""><code>python run_ner.py \
 --model_name_or_path &quot;DeepPavlov/bert-base-bg-cs-pl-ru-cased&quot; \
 --train_file single_line.json \
 --validation_file single_line.json \
 --test_file single_line.json \
 --do_train \
 --do_eval \
 --do_predict \
 --output_dir model_test_deeppavlov \
 --save_steps 50000 \
 --num_train_epochs 1
</code></pre>
<ol start=""2"">
<li>Crashes.</li>
</ol>
<pre class=""lang-sh prettyprint-override""><code>python run_ner.py \
 --model_name_or_path bert-base-polish-cased-v1 \
 --train_file single_line.json \
 --validation_file single_line.json \
 --test_file single_line.json \
 --do_train \
 --do_eval \
 --do_predict \
 --output_dir model_test_polish_bert \
 --save_steps 50000 \
 --num_train_epochs 1
</code></pre>
<p>In order to use <code>bert-base-polish-cased-v1</code> model, it may require to be downloaded.</p>
<pre class=""lang-sh prettyprint-override""><code>git clone https://huggingface.co/dkleczek/bert-base-polish-cased-v1
</code></pre>
","14661154","","14661154","","2021-03-24 10:09:55","2021-05-27 12:20:02","Tensor size does not match classifier's output features number","<pytorch><huggingface-transformers>","1","8","","","","CC BY-SA 4.0"
"66791849","1","","","2021-03-25 01:36:24","","0","22","<pre><code>python run_clm.py \
    --model_name_or_path ctrl \
    --train_file df_finetune_train.csv \
    --validation_file df_finetune_test.csv \
    --do_train \
    --do_eval \
    --preprocessing_num_workers 72 \
    --block_size 256 \
    --output_dir ./finetuned
</code></pre>
<p>I am trying to fine tune the ctrl model on my own dataset, where each row represents a sample.
However, I got the warning info below.</p>
<blockquote>
<p>[WARNING|tokenization_utils_base.py:3213] 2021-03-25 01:32:22,323 &gt;&gt;
Token indices sequence length is longer than the specified maximum
sequence length for this model (934 &gt; 256). Running this sequence
through the model will result in indexing errors</p>
</blockquote>
<p>What is the cause for this ? any solutions ?</p>
","6407393","","","","","2021-03-25 01:36:24","Transformers fine tune model warning","<huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"66841694","1","","","2021-03-28 12:59:43","","0","485","<p>I would like to train / fine-tune the BERT model with some own datasets which contain just raw data from a specific domain, similar to BioBERT. In the first step I just want to train BERT with this raw data, without fine-tuning with any specific NLP task. So, I want to build a base which I can potentially use for fine-tuning with annotated train data for specific tasks, such as NER, etc.
Is this possible in the way I plan to do it?</p>
<p>I tried to use the example scripts of Huggingface: <a href=""https://github.com/huggingface/transformers/tree/master/examples/language-modeling"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/language-modeling</a></p>
<p>I used the run_mlm script in the following way:</p>
<pre><code>python run_mlm.py \
    --model_name_or_path bert-base-uncased\
    --train_file /path/to/text_file.txt \
    --do_train \
    --output_dir /out
</code></pre>
<p>I get some output but how can I evaluate the resulting model?
There are some intrinsic evaluation methods that I wanted to try but I failed, like word similarity or word analogy.</p>
<p>If you could help me with my approach I would be very thankful.</p>
","8478545","","","","","2021-04-01 08:13:55","How to train BERT with custom (raw text) domain-specific dataset using Huggingface?","<python><nlp><pytorch><bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"66133626","1","","","2021-02-10 08:52:46","","1","360","<pre><code>import pandas as pd
from ast import literal_eval

from cdqa.utils.filters import filter_paragraphs
from cdqa.utils.download import download_model, download_bnpp_data
from cdqa.pipeline.cdqa_sklearn import QAPipeline

# Download data and models
download_bnpp_data(dir='./data/bnpp_newsroom_v1.1/')
download_model(model='bert-squad_1.1', dir='./models')

# Loading data and filtering / preprocessing the documents
df = pd.read_csv('data/bnpp_newsroom_v1.1/bnpp_newsroom-v1.1.csv', converters={'paragraphs': literal_eval})
df = filter_paragraphs(df)

# Loading QAPipeline with CPU version of BERT Reader pretrained on SQuAD 1.1
cdqa_pipeline = QAPipeline(reader='C:/models/bert_qa.joblib')

# Fitting the retriever to the list of documents in the dataframe
cdqa_pipeline.fit_retriever(X=df)


</code></pre>
<p>I was trying to load model but the  cdqa_pipeline = QAPipeline(reader='C:/models/bert_qa.joblib') line throws an error saying AttributeError: module 'transformers.modeling_bert' has no attribute 'gelu'</p>
<p>I was using transformers version 3.5</p>
","4080047","","","","","2021-08-13 19:11:23","AttributeError: module 'transformers.modeling_bert' has no attribute 'gelu'","<python><pytorch><torch><huggingface-transformers><question-answering>","1","2","","","","CC BY-SA 4.0"
"66909773","1","","","2021-04-01 18:26:56","","0","258","<p><strong>cannot import 'AutoModelForSequenceClassification' from 'transformers'</strong></p>
<p>The code is</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

t = AutoTokenizer.from_pretrained('/some/directory')
m = AutoModelForSequenceClassification.from_pretrained('/some/directory')
c2 = pipeline(task = 'sentiment-analysis', model=m, tokenizer=t)
</code></pre>
<p>The error is</p>
<pre><code>cannot import 'AutoModelForSequenceClassification' from 'transformers'
</code></pre>
","15224778","","6664872","","2021-04-01 22:32:52","2021-04-13 02:58:46","cannot import 'AutoModelForSequenceClassification' from 'transformers'","<nlp><huggingface-transformers><transformer>","1","2","","","","CC BY-SA 4.0"
"66860788","1","","","2021-03-29 19:55:41","","0","69","<p>I already fine-tuned a BERT model ( with the huggingface library) for a classification task to predict a post category in two types (1 and 0, for example). But, I would need to retrieve the &quot;relevant tokens&quot; for the documents that are predicted as category 1 (for example). I know that I can use the traditional TF-IDF approach once I have all the posts labeled as 1 (for example) with my BERT model. But I have the following question:
is it possible to do the same task with the architecture of the fine-tunned BERT model? I mean, access to the last layer of the encoder (the prediction layer), and with the attention mechanism, get the &quot;relevant&quot; tokens that make that te prediction are 1 (for example)?
Is it possible to do that? Does someone know a tutorial o something similar?</p>
","8083784","","","","","2021-04-01 07:33:44","Retrieve the ""relevant tokens"" with a BERT model (already fine-tuned)","<keyword><bert-language-model><huggingface-transformers><attention-model>","1","2","","","","CC BY-SA 4.0"
"69053582","1","","","2021-09-04 08:20:01","","2","34","<p>I'm testing the ONNX model with one identical input for multiple inference calls, but it produces different results every time?</p>
<p>For details, please refer to the below Colab script.</p>
<p><a href=""https://colab.research.google.com/drive/1cBd0MkQ804FXjWtOME1EB1-UiTXe1elp#scrollTo=bRLuTOjO2YQU"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1cBd0MkQ804FXjWtOME1EB1-UiTXe1elp#scrollTo=bRLuTOjO2YQU</a></p>
","10616533","","","","","2021-09-04 08:20:01","ONNX model inference produces different results for the same input","<pytorch><huggingface-transformers><onnx><onnxruntime>","0","1","1","","","CC BY-SA 4.0"
"68156539","1","","","2021-06-28 01:22:47","","0","35","<p>I use TFBertModel for text similarity task, the code that worked 2 months ago, now gives an error and does not work properly, probably because of the versions of colab libraries that are changing over time. The error he gave was for Bert, who said that keras.tensor was used instead of tf.tensor. I used this command:
<strong>tf.compat.v1.disable_eager_execution()</strong>
to solve this problem. The problem was solved, but now it gives this error to the fit() function:
<strong>ValueError: Variable &lt;tf.Variable 'tf_bert_model/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32&gt; has <code>None</code> for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.</strong>
How can I fix this error?</p>
<pre><code>    tf.compat.v1.disable_eager_execution()
    bert = TFBertModel.from_pretrained('bert-base-multilingual-cased')
    input_ids_1 = keras.layers.Input(shape=(300,), name='input_ids_1', dtype=tf.int32)
    input_masks_ids_1 =  keras.layers.Input(shape=(300,), name='masked_token_1', dtype=tf.int32)
    input_token_ids_1 = keras.layers.Input(shape=(300,), name='token_ids_1', dtype=tf.int32)
    bert_1 = bert(input_ids_1, input_masks_ids_1,input_token_ids_1 )
    bert_1 = bert_1[0]
    bert_1=keras.layers.Dropout(0.2)(bert_1)
    bert_1 = keras.layers.GlobalAveragePooling1D()(bert_1)
    input_ids_2 = keras.layers.Input(shape=(300,), name='input_ids_2', dtype='int32')
    input_masks_ids_2 = keras.layers.Input(shape=(300,), name='masked_token_2', dtype='int32')
    input_token_ids_2 = keras.layers.Input(shape=(300,), name='token_ids_2', dtype='int32')
    bert_2 = bert(input_ids_2,input_masks_ids_2,input_token_ids_2)
    bert_2 = bert_2[0]
    bert_2=keras.layers.Dropout(0.2)(bert_2)  
    bert_2 = keras.layers.GlobalAveragePooling1D()(bert_2)
    concat=keras.layers.concatenate([bert_1,bert_2])
    dense = keras.layers.Dense(256,activation='relu')(concat)
    classifier = keras.layers.Dense(1,activation='sigmoid')(dense)
    model = keras.Model(inputs=[[input_ids_1,input_masks_ids_1,input_token_ids_1,input_ids_2,input_masks_ids_2,input_token_ids_2]], outputs=classifier)
    model.summary()
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
    loss = tf.keras.losses.mse
    metric = tf.keras.metrics.mae
    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])
    model_fit = model.fit([x_train_1[0],x_train_1[1],x_train_1[2],x_train_2[0],x_train_2[1],x_train_2[2]], y_train_1, batch_size=8, epochs=1,validation_data=([x_val_1[0],x_val_1[1],x_val_1[2],x_val_2[0],x_val_2[1],x_val_2[2]], y_val_1))
</code></pre>
","16330318","","16330318","","2021-06-29 16:25:20","2021-06-29 16:25:20","fit()function error in Fine-tune Bert model","<tensorflow><keras><bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"66900855","1","","","2021-04-01 08:18:49","","1","1102","<p>I get an error saying that the input should be of type Tensor, not tuple. I do not know how to work around this problem, as I am already implementing the return_dict=False method as stated in the migration plan.</p>
<p>My model is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>class XLNetClassifier(torch.nn.Module):
    def __init__(self, dropout_rate=0.1):
        super(XLNetClassifier, self).__init__()
        self.XLNet = XLNetModel.from_pretrained('xlnet-base-cased', return_dict=False)
        self.d1 = torch.nn.Dropout(dropout_rate)
        self.l1 = torch.nn.Linear(768, 64)
        self.bn1 = torch.nn.LayerNorm(64)
        self.d2 = torch.nn.Dropout(dropout_rate)
        self.l2 = torch.nn.Linear(64, 3)
        
    def forward(self, input_ids, attention_mask):
        x = self.XLNet(input_ids=input_ids, attention_masks = attention_mask)
        x = self.d1(x)
        x = self.l1(x)
        x = self.bn1(x)
        x = torch.nn.Tanh()(x)
        x = self.d2(x)
        x = self.l2(x)
        
        return x
</code></pre>
<p>The error occurs when calling the dropout.</p>
","15528696","","6664872","","2021-04-01 13:15:41","2021-04-01 13:15:41","dropout(): argument 'input' (position 1) must be Tensor, not tuple when using XLNet with HuggingfCE","<tensor><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68178377","1","","","2021-06-29 12:05:03","","0","32","<p>Please advise how to confirm if ReduceLROnPlateau has been actually applied and what learning rates have been applied at each epoch.</p>
<p>The patience of ReduceLROnPlateau is set to 2 and monitor <code>var_loss</code>.</p>
<pre><code>self._history = self.model.fit(
    self.X.shuffle(1000).batch(self.batch_size).prefetch(1),
    epochs=self.num_epochs,
    batch_size=self.batch_size,
    validation_data=self.V.shuffle(1000).batch(self.batch_size).prefetch(1),
    callbacks=[
        EaryStoppingCallback(patience=self.early_stop_patience),
        ReduceLRCallback(patience=self.reduce_lr_patience),     # &lt;---- set to 2
        TensorBoardCallback(self.log_directory)
    ]
)
</code></pre>
<pre><code>class ReduceLRCallback(tf.keras.callbacks.ReduceLROnPlateau):
    &quot;&quot;&quot;Reduce learning rate when a metric has stopped improving.
    See https://keras.io/api/callbacks/reduce_lr_on_plateau/
    &quot;&quot;&quot;
    def __init__(self, patience=3):
        assert patience &gt; 0
        super().__init__(
            monitor=&quot;val_loss&quot;,
            factor=0.3,
            patience=patience,
        )
</code></pre>
<p>The training <code>val_loss</code> have increased more than twice but have not seen any information if ReduceLROnPlateau has been applied.</p>
<pre><code>_________________________________________________________________
Epoch 1/20
3990/3990 [==============================] - 860s 214ms/step - loss: 0.1705 - accuracy: 0.9386 - val_loss: 0.1626 - val_accuracy: 0.9456
Epoch 2/20
3990/3990 [==============================] - 847s 212ms/step - loss: 0.1618 - accuracy: 0.9412 - val_loss: 0.1433 - val_accuracy: 0.9456
Epoch 3/20
3990/3990 [==============================] - 846s 212ms/step - loss: 0.1593 - accuracy: 0.9425 - val_loss: 0.1478 - val_accuracy: 0.9438
Epoch 4/20
3990/3990 [==============================] - 846s 212ms/step - loss: 0.1567 - accuracy: 0.9427 - val_loss: 0.1428 - val_accuracy: 0.9468
Epoch 5/20
3990/3990 [==============================] - 846s 212ms/step - loss: 0.1558 - accuracy: 0.9425 - val_loss: 0.1502 - val_accuracy: 0.9425
Epoch 6/20
3990/3990 [==============================] - 843s 211ms/step - loss: 0.1554 - accuracy: 0.9433 - val_loss: 0.1453 - val_accuracy: 0.9456
Epoch 7/20
3990/3990 [==============================] - 843s 211ms/step - loss: 0.1482 - accuracy: 0.9454 - val_loss: 0.1362 - val_accuracy: 0.9477
Epoch 8/20
3990/3990 [==============================] - 843s 211ms/step - loss: 0.1475 - accuracy: 0.9449 - val_loss: 0.1373 - val_accuracy: 0.9471
Epoch 9/20
3990/3990 [==============================] - 845s 212ms/step - loss: 0.1468 - accuracy: 0.9460 - val_loss: 0.1362 - val_accuracy: 0.9485
Epoch 10/20
3990/3990 [==============================] - 843s 211ms/step - loss: 0.1448 - accuracy: 0.9462 - val_loss: 0.1344 - val_accuracy: 0.9489
Epoch 11/20
3990/3990 [==============================] - 846s 212ms/step - loss: 0.1447 - accuracy: 0.9458 - val_loss: 0.1346 - val_accuracy: 0.9483
Epoch 12/20
3990/3990 [==============================] - 843s 211ms/step - loss: 0.1444 - accuracy: 0.9460 - val_loss: 0.1342 - val_accuracy: 0.9483
</code></pre>
","4281353","","","","","2021-06-29 21:23:10","Keras ReduceLROnPlateau - How to check if it is applied","<tensorflow><keras><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"68178517","1","","","2021-06-29 12:15:00","","0","21","<p>I am using the <code>hugginface</code> transformers package and am loading a pretrained <code>OpenAIGPTTokenizer</code> tokenizer from a vocab file and merges file.</p>
<p>I want to add some domain-specific words to the vocabulary of this pretrained tokenizer (words such as <code>&quot;sms&quot;</code>, <code>&quot;idk&quot;</code> and <code>&quot;lol&quot;</code>)</p>
<p>When I tokenise a word that is currently in the vocabulary, the resulting token has a <code>&lt;/w&gt;</code> symbol appended which I assume means end-of-word.</p>
<pre><code>&gt;&gt;&gt;tokenizer.tokenize(&quot;hello there lol&quot;)
[&quot;hello&lt;/w&gt;&quot;, &quot;there&lt;/w&gt;&quot;, &quot;lo&quot;, &quot;l&lt;/w&gt;&quot;]
</code></pre>
<p>However, if I try and add a token to the tokenizer, it doesn't have a <code>&lt;/w&gt;</code> symbol at the end. Although, at least the tokenizer recognises that <code>&quot;lol&quot;</code> is its own token.</p>
<pre><code>&gt;&gt;&gt;tokenizer.tokenize(&quot;hello there lol&quot;)
[&quot;hello&lt;/w&gt;&quot;, &quot;there&lt;/w&gt;&quot;, &quot;lol&quot;]
</code></pre>
<p>I don't think this is the correct way to go about extending the vocabulary. What should I do?</p>
<p>Thanks</p>
","6017833","","","","","2021-06-29 12:15:00","Adding domain specific vocabulary to Huggingface pretrained tokenizer","<python><spacy><tokenize><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68192213","1","","","2021-06-30 09:36:11","","0","42","<p>while saving model i use <code>model.save()</code> but it gave error
'''Layer PositionalEncoding has arguments in <code>__init__</code> and therefore must override <code>get_config</code></p>
<pre><code>
  def __init__(self, position, d_model):
    super(PositionalEncoding, self).__init__()
    self.pos_encoding = self.positional_encoding(position, d_model)

  def get_angles(self, position, i, d_model):
    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
    return position * angles

  def positional_encoding(self, position, d_model):
    angle_rads = self.get_angles(
        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
        d_model=d_model)
    # apply sin to even index in the array
    sines = tf.math.sin(angle_rads[:, 0::2])
    # apply cos to odd index in the array
    cosines = tf.math.cos(angle_rads[:, 1::2])

    pos_encoding = tf.concat([sines, cosines], axis=-1)
    pos_encoding = pos_encoding[tf.newaxis, ...]
    return tf.cast(pos_encoding, tf.float32)

  def call(self, inputs):
    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]


sample_pos_encoding = PositionalEncoding(50, 512)```
</code></pre>
","15144358","","","","","2021-06-30 09:36:11","Layer PositionalEncoding has arguments in `__init__` and therefore must override `get_config`","<python><tensorflow><keras><model><huggingface-transformers>","0","6","","","","CC BY-SA 4.0"
"68198199","1","","","2021-06-30 16:04:20","","1","152","<p>Using spacy v3, I try to train a classifier using camemBert and got <code>CUDA out of memory</code> problem.
To resolve this issue I read that I should decrease the batch size but I'm confused which parameter should I change between :</p>
<ul>
<li>[nlp] batch_size</li>
<li>[components.transformer] max_batch_items</li>
<li>[corpora.train or dev] max_length</li>
<li>[trainning.batcher] size</li>
<li>[trainning.batcher] buffer</li>
</ul>
<p>I tried to understand the difference between each parameter :</p>
<ol>
<li><strong>[nlp] batch_size</strong></li>
</ol>
<blockquote>
<p>Default batch size for pipe and evaluate. Defaults to 1000.</p>
</blockquote>
<p>Are those functions used in the training / evaluating process ?<br />
In the quickstart widget (<a href=""https://spacy.io/usage/training#quickstart"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#quickstart</a>), why are the value different according to the hardware ? 1000 for CPU and 128 for GPU.<br />
During the training process, will be the evaluation slower if this value is low ?</p>
<ol start=""2"">
<li><strong>[components.transformer] max_batch_items</strong></li>
</ol>
<blockquote>
<p>Maximum size of a padded batch. Defaults to 4096.</p>
</blockquote>
<p>According to the warning message : <code>Token indices sequence length is longer than the specified maximum sequence length for this model (556 &gt; 512). Running this sequence through the model will result in indexing errors</code> explained here (<a href=""https://github.com/explosion/spaCy/issues/6939"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/issues/6939</a>),
Camembert model has a specified maximum sequence length of 512.</p>
<p>Is the parameter max_batch_item overloaded to this value ? Should I change the value to 512 ?</p>
<ol start=""3"">
<li><strong>[corpora.train or dev] max_length</strong></li>
</ol>
<p>In my comprehension, this value should be equal or lower to the maximum sequence length.
In the quickstart widget this value is set to 500 for the training set and 0 for the dev set.
If set to 0, will it be overloaded to the maximum sequence length of the transformer model ?</p>
<ol start=""4"">
<li><strong>[trainning.batcher] size for spacy.batch_by_padded.v1</strong></li>
</ol>
<blockquote>
<p>The largest padded size to batch sequences into. Can also be a block referencing a schedule, e.g. compounding.</p>
</blockquote>
<p>If I don't use compounding, in what this parameter is different of the max_lentgh ?</p>
<p>Here are some parts of my config file</p>
<pre><code>[nlp]
lang = &quot;fr&quot;
pipeline = [&quot;transformer&quot;,&quot;textcat&quot;]
# Default batch size to use with nlp.pipe and nlp.evaluate
batch_size = 256
...

[components.transformer]
factory = &quot;transformer&quot;
# Maximum size of a padded batch. Defaults to 4096.
max_batch_items = 4096
...

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
# Limitations on training document length
max_length = 512
...

[training.batcher]
@batchers = &quot;spacy.batch_by_padded.v1&quot;
discard_oversize = true
# The largest padded size to batch sequences into. Can also be a block referencing a schedule, e.g. compounding.
size = 500
# The number of sequences to accumulate before sorting by length. A larger buffer will result in more even sizing, but if the buffer is very large, the iteration order will be less random, which can result in suboptimal training.
buffer = 128
get_length = null
...

</code></pre>
","4636923","","","","","2021-07-21 21:58:48","Using spacy v3 which parameter should I change in the config file to resolve CUDA out of memory problem ? batch_size vs max_length vs batcher.size","<machine-learning><huggingface-transformers><spacy-3><spacy-transformers>","1","1","","","","CC BY-SA 4.0"
"66920911","1","","","2021-04-02 14:35:52","","0","115","<p>I am trying to control the allocation and deallocation of a Pytorch model (<a href=""https://huggingface.co/transformers/master/model_doc/bart.html#transformers.BartForConditionalGeneration"" rel=""nofollow noreferrer"">BART</a> from the transformers library). I would like to deallocate the model when it's done. This is what i tried:</p>
<pre class=""lang-py prettyprint-override""><code>import transformers as t
import tracemalloc
import gc
import torch


class BartSummarize:
    def __init__(self):
        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        self._model = t.BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(self.device)
        self._tokenizer = t.BartTokenizer.from_pretrained('facebook/bart-large-cnn')

    def __del__(self):
        print('I am deleted')

    def run(self):
        ARTICLE_TO_SUMMARIZE = &quot;My friends are cool but they eat too many carbs.&quot;
        inputs = self._tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt').to(self.device)
        summary_ids = self._model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)
        return [self._tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]

if __name__ == '__main__':
    tracemalloc.start()    
    BARTModel = BartSummarize()
    data = BARTModel.run()
    current, peak = tracemalloc.get_traced_memory()
    print(f&quot;Current memory BEFORE is {current / 10**6}MB; Peak was {peak / 10**6}MB&quot;)
    del BARTModel
    gc.collect()
    current, peak = tracemalloc.get_traced_memory()
    print(f&quot;Current memory AFTER usage is {current / 10**6}MB; Peak was {peak / 10**6}MB&quot;)
</code></pre>
<p>This is what I get when I run everything:</p>
<pre class=""lang-sh prettyprint-override""><code>Current memory BEFORE is 90.008281MB; Peak was 90.010476MB
Current memory AFTER usage is 89.883413MB; Peak was 90.010476MB
I am deleted
</code></pre>
<p>I assumed that when the garbage collector is called, <code>BartModel</code> gets deleted from memory and <code>I am deleted</code> is printed, then finally I get the last measure of memory allocation. Clearly, the order of execution is different, why is that? Why <code>I am deleted</code> comes as last, and not second? Is there a way to control the deallocation?</p>
","9847918","","9847918","","2021-04-02 15:17:24","2021-04-02 15:17:24","Pytorch: can't control garbage collector and model deallocation","<python><memory-management><pytorch><garbage-collection><huggingface-transformers>","0","7","","","","CC BY-SA 4.0"
"68175501","1","","","2021-06-29 08:52:47","","0","44","<p>i saved my model like this :</p>
<pre><code>ktrain.get_predictor(learner.model,preproc=trans).save('model')
</code></pre>
<p>i want to load my model and use it and to do something like :</p>
<pre><code>predictor = ktrain.load_predictor('/model')
model = ktrain.get_predictor(predictor.model, predictor.preproc)
predictions = model.predict('&lt;anything you want to predict&gt;')
</code></pre>
<p>now, i have a folder &quot;model&quot; which contains 6 files : vocab.txt , tokenizer_config.json, tf_model.preproc , special_tokens_map.json , model.h5 and config.json</p>
<p>but when i run my code that return an error :</p>
<pre><code>  File &quot;api.py&quot;, line 11, in &lt;module&gt;
    predictor = ktrain.load_predictor(&quot;/model&quot;)
  File &quot;C:\Users\omayma\AppData\Roaming\Python\Python38\site-packages\ktrain\core.py&quot;, line 1541, in load_predictor
    raise Exception('Failed to load .preproc file in either the post v0.16.x loction (%s) or pre v0.16.x location (%s)' % (os.path.join(fpath, U.PREPROC_NAME), fpath+'.preproc'))
Exception: Failed to load .preproc file in either the post v0.16.x loction (/model\tf_model.preproc) or pre v0.16.x location (/model.preproc)
</code></pre>
","15864132","","4685471","","2021-06-29 11:32:34","2021-06-29 11:32:34","is there a method to load model finetuned?","<tensorflow><deep-learning><bert-language-model><huggingface-transformers><ktrain>","0","0","","","","CC BY-SA 4.0"
"68207323","1","","","2021-07-01 09:01:57","","1","68","<p>Trying to use <a href=""https://huggingface.co/tuner007/pegasus_paraphrase"" rel=""nofollow noreferrer"">tuner007/pegasus_paraphrase</a>. Followed the examples in <a href=""https://huggingface.co/transformers/model_doc/pegasus.html"" rel=""nofollow noreferrer"">Pegasus</a>.</p>
<blockquote>
<p>The Pegasus model was proposed in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019.</p>
</blockquote>
<p><strong>Problem:</strong></p>
<p>PegasusTokenizer cannot be instantiated as <code>PegasusTokenizer.from_pretrained(model_name)</code> returns <code>None</code>. Using the 'google/pegasus-xsum' as the model name caused the same.</p>
<pre><code>from transformers import PegasusForConditionalGeneration, PegasusTokenizer
model_name = 'tuner007/pegasus_paraphrase'
tokenizer = PegasusTokenizer.from_pretrained(model_name)

type(tokenizer)
---
NoneType
</code></pre>
<p>Please suggest how to work it around.</p>
","4281353","","4420967","","2021-07-01 13:15:32","2021-09-16 00:13:36","huggingface - pegasus PegasusTokenizer is None","<huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"66867213","1","","","2021-03-30 08:20:57","","2","76","<p>I am unsure of how the evaluation for large document summarisation is conducted for the recently introduced <a href=""http://scholar.google.com.sg/scholar_url?url=http://proceedings.mlr.press/v119/zhang20ae/zhang20ae.pdf&amp;hl=en&amp;sa=X&amp;ei=It5iYPzDPKuM6rQP-6OsuAc&amp;scisig=AAGBfm3TMc7EP3WzoI9nibV1Z0HLzzeb2w&amp;nossl=1&amp;oi=scholarr"" rel=""nofollow noreferrer"">PEGASUS model</a> for single document summarisation.</p>
<p>The author's show evaluation against large document datasets like Big Patent, PubMed etc with document lengths exceeding that of the input size to the transformer models.</p>
<p>To quote from the paper, they did talk about this but didn't really elaborate further.</p>
<blockquote>
<p>CNN/DailyMail, Multi-News, arXiv, PubMed, BIG- PATENT datasets contain input documents longer than the maximum input length (<code>L_input = 512 tokens</code>) in pre- training. This would present a problem for position em- beddings which would never be updated for longer input lengths, but we confirm the postulation that sinusoidal po- sitional encodings (Vaswani et al., 2017) generalize well when fine-tuning PEGASUSLARGE beyond the input lengths observed in training up to <code>L_input = 1024 tokens</code>. Since average input length in BIGPATENT, arXiv, PubMed and Multi-News are well beyond 1024 tokens, further scaling up <code>L_input</code> or applying a two-stage approach (Liu et al., 2018) may improve performance even more, although this is out- side the scope of this work.</p>
</blockquote>
<p>They did mention that the input length is up till 1024 tokens. In the PEGASUS Large model on huggingface the max input tokens is also 1024.</p>
<p>I am not sure how they managed to extend their document summarisations for more than 1024 tokens.</p>
<p>I would also like to do similar for my own long document summarisations that I want to try.</p>
","9851155","","","","","2021-03-30 08:20:57","PEGASUS pre-training for summarisation tasks","<nlp><huggingface-transformers><transformer><summarization><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"66879527","1","","","2021-03-30 22:58:28","","0","120","<p>I am trying to process a CSV file from streamlit frontend which has a list of URLs which I am pre-processing using nltk  to pass to a hugging face transformer for summarization. I want to create a background task using asyncio and ProcessPoolExecutor  for this and return the taskid to the UI for polling the results which are stored individually for each URL in a text file on the backend storage. I cannot figure out why my model is not being invoked when I use a background async task at all, it doesn't output any errors or logs.When I call the model in a synchronous fashion for the first URL in the file it returns an output, not sure why it doesnt get invoked when its in the async event loop. Here is my main.py which receives a CSV file and invokes the inference function.</p>
<pre><code>class Job(BaseModel):
uid: UUID = Field(default_factory=uuid4)
status: str = &quot;in_progress&quot;
processed_urls: List[str] = Field(default_factory=list)

app = FastAPI()
nlp = NLP()
jobs: Dict[UUID, Job] = {}


@app.get(&quot;/&quot;)
def read_root():
return {&quot;message&quot;: &quot;Welcome from the API&quot;}

@app.post(&quot;/{contentType}&quot;, status_code=HTTPStatus.ACCEPTED)

async def get_summary(background_tasks: BackgroundTasks, contentType: 
str, file: UploadFile = File(...)):

df = pd.read_excel(file.file.read(), index_col=None, header=None)
model_name = config.MODEL_NAMES[contentType]
start = time.time()
name = f&quot;/storage/{str(uuid.uuid4())}.txt&quot;
new_task = Job()
jobs[new_task.uid] = new_task
background_tasks.add_task(generate_remaining_summaries, new_task.uid, 
model_name, name, df)
return new_task

async def generate_remaining_summaries(uid: UUID, model_name, name, df):
executor = ProcessPoolExecutor()
event_loop = asyncio.get_event_loop()
jobs[uid].result = await event_loop.run_in_executor(
    executor, partial(generate_summary, uid, model_name, name, df))
jobs[uid].status = &quot;complete&quot;


def generate_summary(task_id: UUID, model_name, name, df):
logger.info(&quot;model_name in generate_summary &quot; + model_name)
for ind in range(len(df)):
    url = df.iat[ind + 1, 0]
    
    article_text = get_text(url)
    summary = nlp.inference(model_name, article_text)
    name = name.split(&quot;.&quot;)[0]
    name = f&quot;{name.split('_')[0]}_{ind}.txt&quot;
    logger.info(&quot;name &quot; + name)
    with open(name, 'w+') as file1:
        for listItem in summary:
            file1.write('%s\n' % listItem)
    jobs[task_id].processed_urls.append(name)
jobs[task_id].status = &quot;completed&quot;


 @app.get(&quot;/work/{uid}/status&quot;)
 async def status_handler(uid: UUID):
 return jobs[uid]


if __name__ == &quot;__main__&quot;:
 uvicorn.run(&quot;main:app&quot;, host=&quot;0.0.0.0&quot;, port=8080) 

&gt; Inference.py

import transformers
from transformers import BartTokenizer, BartForConditionalGeneration

import nltk
import torch
import logging.config
import os
import config
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

nltk.download('punkt')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
 transformers.logging.set_verbosity_info()


def nest_sentences(document):
 nested = []
 sent = []
 length = 0
 for sentence in nltk.sent_tokenize(document):
    length += len(sentence)
    if length &lt; 1024:
        sent.append(sentence)
    else:
        nested.append(sent)
        sent = [sentence]
        length = len(sentence)

  if sent:
    nested.append(sent)

  return nested


 class NLP:
   def __init__(self):
    self.cache_dir = os.environ[&quot;MODEL_DIR&quot;] + &quot;facebook/bart-large-cnn/&quot;
    self.tokenizer = BartTokenizer.from_pretrained(&quot;facebook/bart-large-cnn&quot;, 
    cache_dir=self.cache_dir)
    self.model = BartForConditionalGeneration.from_pretrained(&quot;facebook/bart-large-cnn&quot;, 
    cache_dir=self.cache_dir)
    self.device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

  def generate_summary(self, nested_sentences):
    logger.info(&quot;Inside inference before generate summary&quot;)
    
    summaries = []
    for nested in nested_sentences:
        input_tokenized = self.tokenizer.encode(' '.join(nested), truncation=True, 
     return_tensors='pt')
        input_tokenized = input_tokenized.to(self.device)
        summary_ids = self.model.to(self.device).generate(input_tokenized,
                                                        length_penalty=3.0)
        output = [self.tokenizer.decode(g, skip_special_tokens=True, 
                 clean_up_tokenization_spaces=False) for g in
                  summary_ids]
        summaries.append(output)

       summaries = [sentence for sublist in summaries for sentence in sublist]
       return summaries

def inference(self, model_name, article_text):
    
    nested = nest_sentences(article_text)
    logger.info(nested)
    summarized_text = self.generate_summary(nested)
    logger.info(&quot;Inside inference summarized text&quot;)
    logger.info(summarized_text)
    nested_summ = nest_sentences(' '.join(summarized_text))
    return self.generate_summary(nested_summ)
</code></pre>
","15517445","","15517445","","2021-04-01 01:07:26","2021-04-01 01:07:26","Is there a reason why my huggingface model is not generating the summary in an asyncio event loop?","<python><python-asyncio><fastapi><huggingface-transformers><background-task>","0","4","","","","CC BY-SA 4.0"
"68165942","1","","","2021-06-28 15:17:59","","0","21","<p>I am using the transformer sentimental analysis pipeline to get if a text is positive or negative like in the Huggingface webpage:</p>
<pre><code>&gt;&gt;&gt; results = classifier([&quot;We are very happy to show you the ðŸ¤— Transformers library.&quot;,&quot;We hope you don't hate it.&quot;])

&gt;&gt;&gt; for result in results:
    print(f&quot;label: {result['label']}, with score: {round(result['score'], 4)}&quot;)
</code></pre>
<p>And I get this as a result:</p>
<blockquote>
<p>label: POSITIVE, with score: 0.9998</p>
</blockquote>
<p>One is the label and the other the probability of the label. What I want to get is the score of how good or bad is a text. For example, is not the same to say &quot;you are good&quot;  and &quot;you are great&quot;, and either two are positive.</p>
<p>I saw that you can get labels between 1 and 5 but I also want to distinguish between those that are within the labels with a sentiment score between -1 and 1. At the end I want something like the following:</p>
<ul>
<li><p>&quot;You are very good&quot; - Label: 5 stars, score: 0.8</p>
</li>
<li><p>&quot;You are bad&quot; - Label: 1 stars, score: -0.8</p>
</li>
<li><p>&quot;You are great&quot; - Label: 5 stars, score: 0.9</p>
</li>
<li><p>&quot;You are good&quot; - Label: 4 star, score: 0.65</p>
</li>
</ul>
","15575118","","","","","2021-06-28 15:17:59","How do you know how good or bad is a text with sentimental analysis using transformers?","<python><pytorch><sentiment-analysis><huggingface-transformers>","0","3","1","","","CC BY-SA 4.0"
"68214370","1","","","2021-07-01 17:12:56","","2","63","<p>The following code fails for me when I do not have a working internet connection, with the error message that I have detailed below. I have tried a lot of things including configuring and checking the <code>.flair</code> folder and the <code>.cache</code> folder that the <code>transformers</code> package uses. I also checked that <code>distilbert-base-uncased</code> is a current model identifier on <a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">https://huggingface.co/models</a> as the error message suggested. What can I do to fix this? Details below.</p>
<p><strong>Code that fails</strong></p>
<pre><code>from flair.models import TextClassifier
en_classifier = TextClassifier.load('en-sentiment')
</code></pre>
<p><strong>Error:</strong></p>
<pre><code>Can't load tokenizer for 'distilbert-base-uncased'. Make sure that:

- 'distilbert-base-uncased' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'distilbert-base-uncased' is the correct path to a directory containing relevant tokenizer files
</code></pre>
<p><strong>Additional Details:</strong></p>
<p>Flair version: <code>0.8.0.post1'</code></p>
<p>Environment variables needed because my app needs to run standalone with no internet connection:</p>
<pre><code>TRANSFORMERS_OFFLINE=1
HF_DATASETS_OFFLINE=1
</code></pre>
<p>I have the model files located in the <code>.flair</code> directory that is configured by <code>flair.cache_root</code>. The code that works for me when I load other models is:</p>
<pre><code>from flair.models import TextClassifier
rnn_classifier = TextClassifier.load('sentiment-fast')
</code></pre>
<p>even this works:</p>
<pre><code>from flair.models import SequenceTagger
tagger = SequenceTagger.load('ner')
</code></pre>
<p><code>.flair</code> directory has:</p>
<ul>
<li><code>sentiment-en-mix-distillbert_4.pt</code></li>
<li><code>sentiment-en-mix-ft-rnn.pt</code></li>
<li><code>ner-english</code> (folder)</li>
<li><code>pos-english</code> (folder)</li>
</ul>
<p><code>~/.cache/huggingface/transformers</code> directory has:</p>
<ul>
<li><code>0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99</code></li>
<li><code>0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.json</code></li>
<li><code>75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4</code></li>
<li><code>75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.json</code></li>
<li><code>8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79</code></li>
<li><code>8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.json</code></li>
</ul>
","5319613","","","","","2021-07-01 17:12:56","Flair en-sentiment model loading fails when not connected to the internet","<python><nlp><huggingface-transformers><flair>","0","0","","","","CC BY-SA 4.0"
"68207474","1","","","2021-07-01 09:12:20","","0","78","<p>I am using the question-answering pipeline provided by <code>huggingface</code>. I am trying to perform multiprocessing to parallelize the question answering. This is what I have tried till now</p>
<pre><code>from transformers import pipeline
from torch.multiprocessing import Pool, Process, set_start_method
set_start_method('spawn', force = True)

model_name = &quot;deepset/roberta-base-squad2&quot;
reader = pipeline('question-answering', model=model_name, tokenizer=model_name, device = -1)


def get_answer(input_dict):
  return reader(input_dict)

input_list = []
for i in range(3):
  QA_input = {
          'question': val_questions[i],
          'context': val_contexts[i]
          }
  input_list.append(QA_input)
  
if __name__ == '__main__':
    result =[]
    multi_pool = Pool(processes=3)
    predictions = multi_pool.map(get_answer, input_list)
    multi_pool.close() 
    multi_pool.join()
    print(predictions)
</code></pre>
<p>This code runs without any error, but the execution does not get past <code>predictions = multi_pool.map(get_answer, input_list)</code> line. The execution gets stuck at this line and does not progress. When I remove <code>set_start_method('spawn', force = True)</code> and run the code on CPU. The code works perfectly, But it does not work with GPU. What am I doing wrong. Is there anything that should be changed to make the above code suitable for running on a GPU?</p>
","6077443","","6077443","","2021-07-01 09:27:10","2021-08-20 09:23:50","Multiprocessing for huggingface pipeline : Execution does not end","<python><pytorch><multiprocessing><huggingface-transformers><question-answering>","0","0","","","","CC BY-SA 4.0"
"68234881","1","","","2021-07-03 09:22:08","","0","24","<p>Please advise how I can get the output shape or layer details (e.g. output, output_shape) form the pre-trained model.</p>
<p>Simply get the <code>layer.output_shape</code> below causes <code>AttributeError: The layer has never been called and thus has no defined output shape.</code>. Please help understand what causes the error.</p>
<pre><code>model_name = 'distilbert-base-uncased'
max_sequence_length = MAX_SEQUENTH_LENGTH
num_labels = NUM_LABELS

from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

def tokenize(sentences):
    return tokenizer(
        sentences,
        truncation=True,
        padding=True,
        max_length=max_sequence_length,
        return_tensors=&quot;tf&quot;
    )
tokens = tokenize(&quot;I say hello&quot;)


from transformers import TFDistilBertModel
# Use TFDistilBertModel as TFDistilBertForSequenceClassification has classification heads added.
# base = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
base = TFDistilBertModel.from_pretrained(model_name)

# Freeze the base model weights.
for layer in base.layers:
    layer.trainable = False
base.summary()
---
Model: &quot;tf_distil_bert_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
distilbert (TFDistilBertMain multiple                  66362880  
=================================================================
Total params: 66,362,880
Trainable params: 0
Non-trainable params: 66,362,880
</code></pre>
<pre><code>base.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    metrics=['accuracy']
)

for layer in base.layers:
    print(layer.output_shape)for layer in base.layers:
    print(layer.output_shape)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-89-c8756f2a58fc&gt; in &lt;module&gt;()
      1 for layer in base.layers:
----&gt; 2     print(layer.output_shape)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in output_shape(self)
   2136     &quot;&quot;&quot;
   2137     if not self._inbound_nodes:
-&gt; 2138       raise AttributeError('The layer has never been called '
   2139                            'and thus has no defined output shape.')
   2140     all_output_shapes = set(

AttributeError: The layer has never been called and thus has no defined output shape.
</code></pre>
<h2>Workaround</h2>
<pre><code>base_model_output = base(tokens)
base_model_output.last_hidden_state.shape
---
TensorShape([2, 6, 768])
</code></pre>
","4281353","","4281353","","2021-07-03 09:27:37","2021-07-03 09:27:37","Huggingface/Keras - how to get the output shape of the pre-trained model","<tensorflow><keras><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68203089","1","","","2021-07-01 00:55:03","","0","35","<p>LayoutLM builds itself on top of BERT as the baseline, but I want to substitute BERT for MobileBERT because BERT is too large. Unfortunately, the Huggingface Transformers library doesn't give you the option to change the baseline model for LayoutLM. How should I go about swapping BERT for MobileBERT? I'm aware they have very different configurations.</p>
<p>I'm aware this is a very broad question and a wide topic, but I can't find anything about it online. How would I go about it and where should I start?</p>
","12815420","","","","","2021-07-05 03:20:32","Is it possible to substitute BERT with MobileBERT under the hood of LayoutLM?","<nlp><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68225540","1","","","2021-07-02 12:50:06","","0","38","<p>I am really new to Machine Learning and I am not so well versed in coding in general. However there is need to look through the customers feedback at our store, that average quite a lot each year, yet we cannot tell % of positive, negative and neutral.</p>
<p>Currently I am trying to train a Bert Model to do simple multi labeled sentiment analysis. The input is our store's customers feedback. The customers feedback is not always so clearly defined since customers do tend to tell long and long about their experience and their sentiment is not always so clear. However we managed to get positive, negative and neutral, each set 2247 samples.</p>
<p>But when I try to train it the training accuracy is around 0.4% which is super low. Validation score is around 60%. F1-score is around 60% for each of the label. I wonder what can be done to improve this training accuracy. I have been stuck for a while. Please take a look at my codes and help me out with this.</p>
<p>I have tried changing learning rate (tried all learning rate Bert suggested and 1e-5),changing Max LEN, changing amount of EPOCH, changing drop out rate (0.1, 0.2, 0.3, 0.4, 0.5), but so far nothing yielded results.</p>
<pre><code>#read dataset
df = pd.read_csv(&quot;data.csv&quot;,header=None, names=['content', 'sentiment'], sep='\;', lineterminator='\r',encoding = &quot;ISO-8859-1&quot;,engine=&quot;python&quot;)

from sklearn.utils import shuffle

df = shuffle(df)

df['sentiment'] = df['sentiment'].replace(to_replace = [-1, 0, 1], value = [0, 1, 2])

df.head()

#Load pretrained FinBert model and get bert tokenizer from it
PRE_TRAINED_MODEL_NAME = 'TurkuNLP/bert-base-finnish-cased-v1'
tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

#Choose sequence Length
token_lens = []

for txt in df.content:

 tokens = tokenizer.encode(txt, max_length=512)

 token_lens.append(len(tokens))

sns.distplot(token_lens)

plt.xlim([0, 256]);

plt.xlabel('Token count');

MAX_LEN = 260


#Make a PyTorch dataset
class FIDataset(Dataset):

 def __init__(self, texts, targets, tokenizer, max_len):

   self.texts = texts

   self.targets = targets

   self.tokenizer = tokenizer

   self.max_len = max_len

 def __len__(self):

   return len(self.texts)

 def __getitem__(self, item):

   text = str(self.texts[item])

   target = self.targets[item]

   encoding = self.tokenizer.encode_plus(

     text,

     add_special_tokens=True,

     max_length=self.max_len,

     return_token_type_ids=False,

     pad_to_max_length=True,

     return_attention_mask=True,

     return_tensors='pt',

   )

   return {

     'text': text,

     'input_ids': encoding['input_ids'].flatten(),

     'attention_mask': encoding['attention_mask'].flatten(),

     'targets': torch.tensor(target, dtype=torch.long)

   }

#split test and train
df_train, df_test = train_test_split(

 df,

 test_size=0.1,

 random_state=RANDOM_SEED

)

df_val, df_test = train_test_split(

 df_test,

 test_size=0.5,

 random_state=RANDOM_SEED

)

df_train.shape, df_val.shape, df_test.shape

#data loader function
def create_data_loader(df, tokenizer, max_len, batch_size):

 ds = FIDataset(

   texts=df.content.to_numpy(),

   targets=df.sentiment.to_numpy(),

   tokenizer=tokenizer,

   max_len=max_len

 )

 return DataLoader(

   ds,

   batch_size=batch_size,

   num_workers=4

 )

#Load data into train, test, val
BATCH_SIZE = 16

train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)

val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)

test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)


# Sentiment Classifier based on Bert model just loaded
class SentimentClassifier(nn.Module):

 def __init__(self, n_classes):

   super(SentimentClassifier, self).__init__()

   self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)

   self.drop = nn.Dropout(p=0.1)

   self.out = nn.Linear(self.bert.config.hidden_size, n_classes)

 def forward(self, input_ids, attention_mask):
   returned = self.bert(
       
       input_ids=input_ids,
       attention_mask=attention_mask
   )
   pooled_output = returned[&quot;pooler_output&quot;]
   output = self.drop(pooled_output)
   
   return self.out(output)


#Create a Classifier instance and move to GPU
model = SentimentClassifier(3)

model = model.to(device)


#Optimize with AdamW
EPOCHS = 5

optimizer = AdamW(model.parameters(), lr= 2e-5, correct_bias=False)

total_steps = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(

 optimizer,

 num_warmup_steps=0,

 num_training_steps=total_steps

)

loss_fn = nn.CrossEntropyLoss().to(device)

#Train each Epoch function
def train_epoch(

 model,

 data_loader,

 loss_fn,

 optimizer,

 device,

 scheduler,

 n_examples

):
 
 model = model.train()

 losses = []

 correct_predictions = 0

 for d in data_loader:

   input_ids = d[&quot;input_ids&quot;].to(device)

   attention_mask = d[&quot;attention_mask&quot;].to(device)

   targets = d[&quot;targets&quot;].to(device)

   outputs = model(

     input_ids=input_ids,

     attention_mask=attention_mask

   )

   _, preds = torch.max(outputs, dim=1)

   loss = loss_fn(outputs, targets)

   correct_predictions += torch.sum(preds == targets)

   losses.append(loss.item())

   loss.backward()

   nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

   optimizer.step()

   scheduler.step()

   optimizer.zero_grad()
   
   return correct_predictions.double() / n_examples, np.mean(losses)

#Eval model function
def eval_model(model, data_loader, loss_fn, device, n_examples):

 model = model.eval()

 losses = []

 correct_predictions = 0

 with torch.no_grad():
       
   torch.cuda.empty_cache()

   for d in data_loader:

     input_ids = d[&quot;input_ids&quot;].to(device)

     attention_mask = d[&quot;attention_mask&quot;].to(device)

     targets = d[&quot;targets&quot;].to(device)

     outputs = model(

       input_ids=input_ids,

       attention_mask=attention_mask

     )

     _, preds = torch.max(outputs, dim=1)

     loss = loss_fn(outputs, targets)

     correct_predictions += torch.sum(preds == targets)

     losses.append(loss.item())
       
   
   
 return correct_predictions.double() / n_examples, np.mean(losses)

#training loop through each epochs

import torch

torch.cuda.empty_cache()

history = defaultdict(list)

best_accuracy = 0

if __name__ == '__main__':  

   for epoch in range(EPOCHS):

     print(f'Epoch {epoch + 1}/{EPOCHS}')

     print('-' * 10)

     train_acc, train_loss = train_epoch(

       model,

       train_data_loader,

       loss_fn,

       optimizer,

       device,

       scheduler,

       len(df_train)

     )

     print(f'Train loss {train_loss} accuracy {train_acc}')

     val_acc, val_loss = eval_model(

       model,

       val_data_loader,

       loss_fn,

       device,

       len(df_val)

     )

     print(f'Val   loss {val_loss} accuracy {val_acc}')

     print()

     history['train_acc'].append(train_acc)

     history['train_loss'].append(train_loss)

     history['val_acc'].append(val_acc)

     history['val_loss'].append(val_loss)

     if val_acc &gt; best_accuracy:

       torch.save(model.state_dict(), 'best_model_state.bin')

       best_accuracy = val_acc

</code></pre>
<p>-- Edit: I have printed out preds and targets as well as train and val accuracy</p>
<p><a href=""https://i.stack.imgur.com/7Afiz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Afiz.png"" alt=""training and val accuracy"" /></a></p>
","13719914","","13719914","","2021-07-05 06:08:22","2021-07-05 07:26:12","Almost non-existent training accuracy and low test accuracy","<pytorch><sentiment-analysis><bert-language-model><huggingface-transformers><multiclass-classification>","1","0","","","","CC BY-SA 4.0"
"68226106","1","","","2021-07-02 13:31:57","","1","34","<p>After quantizing the BERT model, it works without any issue. But if I save the quantized model and load, it does not work. It shows an error message: 'LinearPackedParams' object has no attribute '_modules&quot;. I have used the same device to save and load the quantized model.</p>
<pre><code>model = SentenceTransformer('bert-base-nli-mean-tokens')
model.encode(sentences)
quantized_model = torch.quantization.quantize_dynamic(
                model, {torch.nn.Linear}, dtype=torch.qint8)
quantized_model.encode(sentences) ```



torch.save(quantized_model, 
       &quot;/PATH/TO/DESTINATION/Base_bert_quant.pt&quot;)
model=torch.load(&quot;/SAME/PATH/Base_bert_quant.pt&quot;)
model.encode(sentences) #shows the error
</code></pre>
","3190883","","","","","2021-07-02 13:31:57","Error loading quantized BERT model from local repository","<bert-language-model><huggingface-transformers><sentence-transformers>","0","0","","","","CC BY-SA 4.0"
"68239361","1","68379175","","2021-07-03 19:26:39","","0","176","<p>Im trying to use huggingface transformers library in my python project. I am a first time python programmer, and I am stuck on this error message, even though tensorflow has been installed on my machine:</p>
<pre class=""lang-python prettyprint-override""><code>&gt;&gt;&gt; from transformers import pipeline

None of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
</code></pre>
<p>I have discovered that tensorflow does not exist, even though I have installed it via <code>pip</code>. I have tried uninstalling it and reinstalling it, and but when I try to import the package, it just comes back as a <code>ModuleNotFoundError</code></p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import tensorflow

Traceback (most recent call last):
  File &quot;&lt;pyshell#2&gt;&quot;, line 1, in &lt;module&gt;
    import tensorflow
  File &quot;C:\Users\######\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\tensorflow\__init__.py&quot;, line 41, in &lt;module&gt;
    from tensorflow.python.tools import module_util as _module_util
ModuleNotFoundError: No module named 'tensorflow.python
</code></pre>
<p>I have tried uninstalling and re-installing using <code>pip</code> and <code>conda</code>. I even tried installing <code>pytorch</code> using the same methods. It always says that the package was succesfully installed, and yet the error persists.</p>
<p>I am using Python 3.9 and my OS is Windows 10. I dont know what I am doing wrong. But I know that a solution will definitely not be to uninstall and reinstall a package.</p>
<p><strong>Pip version (<code>pip -V</code>):</strong></p>
<pre><code>pip 21.1.3 from c:\users\######\appdata\local\programs\python\python39\lib\site-packages\pip (python 3.9)
</code></pre>
<p><strong>Python version (<code>python -V</code>):</strong></p>
<pre><code>Python 3.9.5
</code></pre>
<p><strong>Python path list</strong>
I tried comparing the output of <code>sys.path</code> with the output of <code>pip -V</code>.
The closest path I saw for the <code>pip -V</code> path is down at the bottom, however I did not find the exact directory.</p>
<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.path

['', 'C:\\windows\\system32', 'C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0', 'C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0\\python39.zip', 'C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0\\DLLs', 'C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0\\lib', 'C:\\Users\\######\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0', 'C:\\Users\\######\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages', 'C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0', 'C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0\\lib\\site-packages']
</code></pre>
<p>Closest path:</p>
<pre><code>C:\Users\######\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages
</code></pre>
","14695522","","14695522","","2021-07-04 03:08:09","2021-07-14 13:37:44","Cant install tensorflow for huggingface transformers library","<python><tensorflow><pytorch><package><huggingface-transformers>","1","8","","","","CC BY-SA 4.0"
"68172891","1","","","2021-06-29 05:04:15","","1","60","<p>What is the objective of the Dropout layer in the TFDistilBertForSequenceClassification pretrained model and why the last layer is not softmax or sigmoid?</p>
<p>The TFDistilBertForSequenceClassification pretrained model has Dropout layer at the last.</p>
<pre><code>model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')
model.compile(
    optimizer=optimizer, 
    # loss=self.model.compute_loss,
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics = [&quot;accuracy&quot;]  
)
model.summary()

Layer (type)                 Output Shape              Param #   
=================================================================
distilbert (TFDistilBertMain multiple                  66362880  
_________________________________________________________________
pre_classifier (Dense)       multiple                  590592    
_________________________________________________________________
classifier (Dense)           multiple                  1538      
_________________________________________________________________
dropout_99 (Dropout)         multiple                  0         &lt;------------
=================================================================
Total params: 66,955,010
Trainable params: 592,130
Non-trainable params: 66,362,880
</code></pre>
<p>To feed output into the categorical or logistic cross-entropy loss layer, I believe the output is from softmax or sigmoid. However the last layer is dropout. Please help understand the reason.</p>
<h1>Reference</h1>
<p><a href=""https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-native-pytorch-tensorflow"" rel=""nofollow noreferrer"">Fine-tuning with native PyTorch/TensorFlow</a></p>
<pre><code>from transformers import TFDistilBertForSequenceClassification

model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)
</code></pre>
","4281353","","","","","2021-06-29 05:04:15","Huggingface - Dropout layer in the TFDistilBertForSequenceClassification pretrained model","<tensorflow><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"68198796","1","","","2021-06-30 16:53:58","","0","22","<p>I was just wondering whether it would be possible to see all the predicted tokens for masked language modelling? Specifically, all the tokens with a low probability.</p>
<p>For example, consider this masked language model:</p>
<pre><code>unmasker(&quot;I am feeling &lt;mask&gt; today&quot;)
</code></pre>
<pre><code>[{'score': 0.5322356820106506,
  'sequence': 'I am feeling good today',
  'token': 4,
  'token_str': good'},
 {'score': 0.1725485771894455,
  'sequence': 'I am feeling happy today!',
  'token': 328,
  'token_str': 'happy'},
 {'score': 0.1252109706401825,
  'sequence': 'I am feeling sad today.&quot;',
  'token': 72,
  'token_str': 'sad&quot;'},
 {'score': 0.01904081553220749,
  'sequence': 'I am feeling angry today!&quot;',
  'token': 2901,
  'token_str': 'angry'},
 {'score': 0.012199202552437782,
  'sequence': 'I am feeling fun todayâ€¦',
  'token': 1174,
  'token_str': 'fun'}]
</code></pre>
<p>As you can see from my output, the top tokens are &quot;good&quot;, &quot;happy&quot;, &quot;sad&quot;, &quot;angry&quot; and &quot;fun&quot;. However, would it be possible to see all the predicted tokens beyond the top 5?</p>
<p>I just want to see all a list of all the predicted tokens: the ones which have the lowest probability - if this is possible.</p>
<p>I don't want to see the top 5 predicted; I want to see all of them.</p>
<p>Thanks.</p>
","","user14946125","","","","2021-06-30 16:58:26","Is it possible to see all the token rankings for masked language modelling?","<python><nlp><bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"68245499","1","","","2021-07-04 14:03:53","","0","52","<p>I wrote the text classification code with two classes using the Roberta model and now I want to draw the confusion matrix.
How to go about plotting the confusion matrix based of a Roberta model?</p>
<pre><code>RobertaTokenizer = RobertaTokenizer.from_pretrained('roberta-base',do_lower_case=False)
roberta_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=2)

input_ids=[]
attention_masks=[]

for sent in sentences:
    bert_inp=RobertaTokenizer.encode_plus(sent,add_special_tokens = True,max_length =128,pad_to_max_length = True,return_attention_mask = True)
    input_ids.append(bert_inp['input_ids'])
    attention_masks.append(bert_inp['attention_mask'])
    

input_ids=np.asarray(input_ids)
attention_masks=np.array(attention_masks)
labels=np.array(labels)
#split
train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.5)
print('Train inp shape {} Val input shape {}\nTrain label shape {} Val label shape {}\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp.shape,val_inp.shape,train_label.shape,val_label.shape,train_mask.shape,val_mask.shape))
#
log_dir='tensorboard_data/tb_roberta'
model_save_path='/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py'

callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]

print('\nBert Model',roberta_model.summary())

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)

roberta_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])

history=roberta_model.fit([train_inp,train_mask],train_label,batch_size=16,epochs=2,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)
</code></pre>
","14326809","","","","","2021-07-27 10:41:39","Plot Confusion Matrix from Roberta Model","<tensorflow><nlp><huggingface-transformers><confusion-matrix><roberta-language-model>","1","1","","","","CC BY-SA 4.0"
"60243099","1","60244493","","2020-02-15 21:01:25","","3","2105","<p>Using the vanilla configuration of base BERT model in the huggingface implementation, I get a tuple of length 2.</p>

<pre><code>import torch

import transformers
from transformers import AutoModel,AutoTokenizer

bert_name=""bert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(bert_name)
BERT = AutoModel.from_pretrained(bert_name)

e=tokenizer.encode('I am hoping for the best', add_special_tokens=True)

q=BERT(torch.tensor([e]))

print (len(q)) #Output: 2
</code></pre>

<p>The first element is what I expect to receive - the 768 dimension embedding of each input token.</p>

<pre><code>print (e) #Output : [101, 1045, 2572, 5327, 2005, 1996, 2190, 102] 
print (q[0].shape) #Output : torch.Size([1, 8, 768])
</code></pre>

<p>But what is the second element in the tuple?</p>

<pre><code>print (q[1].shape) # torch.Size([1, 768])
</code></pre>

<p>It has the same size as the encoding of each token.
But what is it?</p>

<p>Maybe a copy of the [CLS] token, a representation for the classification of the entire encoded text?</p>

<p>Let's check.</p>

<pre><code>a= q[0][:,0,:]
b=q[1]

print (torch.eq(a,b)) #Output : Tensor([[False, False, False, .... False]])
</code></pre>

<p>Nope!</p>

<p>What about a copy the embedding of the last token (for whatever reason)?</p>

<pre><code>c= q[0][:,-1,:]
b=q[1]

print (torch.eq(a,c)) #Output : Tensor([[False, False, False, .... False]])
</code></pre>

<p>So, also not that.</p>

<p>The documentation talks about how changing the <code>config</code> can result in more tuple elements (like hidden states), but I did not find any description of this ""mysterious"" tuple element outputted by the default configuration.</p>

<p>Any ideas as to what is it and what is its usage?    </p>
","2182857","","","","","2020-02-16 00:44:36","What is the meaning of the second output of Huggingface's Bert?","<python><deep-learning><pytorch><huggingface-transformers>","1","0","3","","","CC BY-SA 4.0"
"68215125","1","","","2021-07-01 18:15:34","","0","70","<p>I'm a newbie going through the hugging face library trying out the Translation models for a data entry task and translating text from English to Italian.</p>
<p>The code I tried based on the documentation:</p>
<pre><code>from transformers import MarianTokenizer, MarianMTModel
from typing import List

#src = 'en'  # source language
#trg = 'it'  # target language
#saved the model locally.
#model_name = f'Helsinki-NLP/opus-mt-{src}-{trg}'
#model.save_pretrained(&quot;./model_en_to_it&quot;)
#tokenizer.save_pretrained(&quot;./tokenizer_en_to_it&quot;)


model = MarianMTModel.from_pretrained('./model_en_to_it')
tokenizer = MarianTokenizer.from_pretrained('./tokenizer_en_to_it')

#Next, trying to iterate over each column - 'english_text' of the dataset and 
#translate the text from English to Italian and append the translated text to the 
#list 'italian'.
 
italian = []
for i in range(len(data)):   
    batch = tokenizer(dataset['english_text'][i], 
                      return_tensors=&quot;pt&quot;,truncation=True, 
                      padding = True)
    gen = model.generate(**batch)
    italian.append(tokenizer.batch_decode(gen, skip_special_tokens=True))
</code></pre>
<p>Two concerns over here:</p>
<ol>
<li>Translates and appends only partial text i.e., it truncates the paragraph if it exceeds a certain length. How to translate the text given any length?</li>
<li>I have near about 10k data and it is taking a hell of a lot of time.</li>
</ol>
<p>Even if any one of the problem could be solved, that's helpful. Would love to learn</p>
","14172829","","","","","2021-07-06 07:08:42","Translating text from english to Italian using hugging face Helsinki models not fully translating","<python-3.x><neural-network><nlp><huggingface-transformers><machine-translation>","1","0","","","","CC BY-SA 4.0"
"68257999","1","68438614","","2021-07-05 14:39:49","","0","37","<p>I have lot of text which has the counting in words as well in different languages (different datasets but one data has one language so no mixing of language).</p>
<p>like</p>
<pre><code>I have one apple 
I have two kids
</code></pre>
<p>and
I want it to convert as</p>
<pre><code>I have 1 apple
I have 2 kids
</code></pre>
<p>Is it possible to change that to the numbers / integers using huggingface or any ? or any suggestion for such problem would help! English Dutch French German supporting.</p>
","11341120","","11341120","","2021-07-19 10:00:23","2021-07-19 10:02:55","How to convert word to numerics using huggingface or spacy or any python based workflow","<python><nltk><spacy><huggingface-transformers><huggingface-tokenizers>","2","0","","","","CC BY-SA 4.0"
"68185061","1","68518389","","2021-06-29 20:10:15","","1","234","<p>I need to translate large amounts of text from a database. Therefore, I've been dealing with transformers and models for a few days. I'm absolutely no data science expert and unfortunately I don't get any further.</p>
<p>The problem starts with longer text. The 2nd issue is the usual-maximum token size (512) of the sequencers. Just truncating is not really an option. <a href=""https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f"" rel=""nofollow noreferrer"">Here</a> I did  find a work-around, but it does not work properly and the result is a word salad on longer texts (&gt;300 sequences)</p>
<p>Here an Example <em>(please ignore the warnings, this is another issues - which does not hurt currently that much)</em>;</p>
<p>If i take the Example Sentence 2 (55 seq) or 5 times (163 sequences) - <strong>no issues.</strong></p>
<p>But it get messed up with e.g. 433 sequences (the 3rd green text block in the screenshot).</p>
<p><a href=""https://i.stack.imgur.com/IzYKf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IzYKf.png"" alt=""enter image description here"" /></a></p>
<p>With more than 510 sequences, I tried to split it up in chunks as in the upper described link. But the result here is as well pretty strange.</p>
<p>I am pretty sure - that I have more than just one mistake and underestimated this topic.
But I see no alternative (free/cheap) way for translating big amount of text.</p>
<p>Can you guys help me out? Which (thinking) errors do you see and how would you suggest to solve the issues? Thank you very much.</p>
<p><a href=""https://i.stack.imgur.com/S8jMW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S8jMW.png"" alt=""enter image description here"" /></a></p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

if torch.cuda.is_available():  
  dev = &quot;cuda&quot;
else:  
  dev = &quot;cpu&quot; 
device = torch.device(dev)
 
mname = 'Helsinki-NLP/opus-mt-de-en'
tokenizer = AutoTokenizer.from_pretrained(mname)
model = AutoModelForSeq2SeqLM.from_pretrained(mname)
model.to(device)

chunksize = 512

text_short = &quot;Nach nur sieben Seiten appellierte man an die WÃ¤hlerinnen und WÃ¤hler, sich richtig zu entscheiden, nÃ¤mlich fÃ¼r Frieden, Freiheit, Sozialismus. &quot;
text_long = text_short
#this loop is just for debugging/testing and simulating long text
for x in range(30):
    text_long = text_long + text_short

tokens = tokenizer.encode_plus(text_long, return_tensors=&quot;pt&quot;, add_special_tokens=True, padding=False, truncation=False).to(device)
str_len = len(tokens['input_ids'][0])

if str_len &gt; 510:
    # split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)
    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))
    mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))

    cnt = 1
    for tensor in input_id_chunks:
        print('\033[96m' + 'chunk ' + str(cnt) + ': ' + str(len(tensor)) + '\033[93m')
        cnt += 1
    
    # loop through each chunk
    # https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f
    for i in range(len(input_id_chunks)):
        # add CLS and SEP tokens to input IDs
        input_id_chunks[i] = torch.cat([
            torch.tensor([101]).to(device), input_id_chunks[i], torch.tensor([102]).to(device)
        ])
        # add attention tokens to attention mask
        mask_chunks[i] = torch.cat([
            torch.tensor([1]).to(device), mask_chunks[i], torch.tensor([1]).to(device)
        ])
        # get required padding length
        pad_len = chunksize - input_id_chunks[i].shape[0]
        # check if tensor length satisfies required chunk size
        if pad_len &gt; 0:
            # if padding length is more than 0, we must add padding
            input_id_chunks[i] = torch.cat([
                input_id_chunks[i], torch.Tensor([0] * pad_len).to(device)
            ])
            mask_chunks[i] = torch.cat([
                mask_chunks[i], torch.Tensor([0] * pad_len).to(device)
            ])
   
    input_ids = torch.stack(input_id_chunks)
    attention_mask = torch.stack(mask_chunks)
    input_dict = {'input_ids': input_ids.long(), 'attention_mask': attention_mask.int()}
    
    outputs = model.generate(**input_dict)
    #this doesnt work - following error comes to the console --&gt; &quot;host_softmax&quot; not implemented for 'Long'
    #probs = torch.nn.functional.softmax(outputs[0], dim=-1)
    # probs
    # probs = probs.mean(dim=0)
    # probs
  
else:
    tokens[&quot;input_ids&quot;] = tokens[&quot;input_ids&quot;][:, :512] #truncating normally not necessary
    tokens[&quot;attention_mask&quot;] = tokens[&quot;attention_mask&quot;][:, :512]
    outputs = model.generate(**tokens)

decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print('\033[94m' + str(str_len))
print('\033[92m' + decoded)
</code></pre>
<p>Remark; following libs are necessary:</p>
<blockquote>
<p>pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio===0.9.0 -f <a href=""https://download.pytorch.org/whl/torch_stable.html"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/torch_stable.html</a></p>
</blockquote>
<blockquote>
<p>pip install transformers</p>
</blockquote>
<blockquote>
<p>pip install sentencepiece</p>
</blockquote>
","14226613","","14226613","","2021-06-29 20:46:20","2021-07-25 12:17:46","Strange results with huggingface transformer[marianmt] translation of larger text","<python><translation><huggingface-transformers><huggingface-tokenizers>","1","2","","","","CC BY-SA 4.0"
"68196815","1","68197081","","2021-06-30 14:35:01","","-1","108","<p>I want to use the huggingface datasets library from within a Jupyter notebook.</p>
<p>This should be as simple as installing it (<code>pip install datasets</code>, in bash within a venv) and importing it (<code>import datasets</code>, in Python or notebook).</p>
<p>All works well when I test it in the standard Python interactive shell, however, when trying in a Jupyter notebook, it says:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-6-652e886d387f&gt; in &lt;module&gt;
----&gt; 1 import datasets

ModuleNotFoundError: No module named 'datasets'
</code></pre>
<p>At first, I thought it might be the case that the notebook kernel uses a different virtual environment, but I verified from within the notebook that the package is installed:</p>
<p><code>!pip install datasets</code></p>
<pre><code>Requirement already satisfied: datasets in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (1.8.0)
Requirement already satisfied: numpy&gt;=1.17 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (1.21.0)
Requirement already satisfied: xxhash in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (2.0.2)
Requirement already satisfied: pyarrow&lt;4.0.0,&gt;=1.0.0 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (3.0.0)
Requirement already satisfied: pandas in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (1.2.5)
Requirement already satisfied: fsspec in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (2021.6.1)
Requirement already satisfied: packaging in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (20.9)
Requirement already satisfied: dill in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (0.3.4)
Requirement already satisfied: requests&gt;=2.19.0 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (2.25.1)
Requirement already satisfied: tqdm&lt;4.50.0,&gt;=4.27 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (4.49.0)
Requirement already satisfied: multiprocess in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (0.70.12.2)
Requirement already satisfied: huggingface-hub&lt;0.1.0 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from datasets) (0.0.13)
Requirement already satisfied: pytz&gt;=2017.3 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from pandas-&gt;datasets) (2021.1)
Requirement already satisfied: python-dateutil&gt;=2.7.3 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from pandas-&gt;datasets) (2.8.1)
Requirement already satisfied: pyparsing&gt;=2.0.2 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from packaging-&gt;datasets) (2.4.7)
Requirement already satisfied: certifi&gt;=2017.4.17 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;datasets) (2021.5.30)
Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;datasets) (4.0.0)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;datasets) (1.26.6)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;datasets) (2.10)
Requirement already satisfied: typing-extensions in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from huggingface-hub&lt;0.1.0-&gt;datasets) (3.10.0.0)
Requirement already satisfied: filelock in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from huggingface-hub&lt;0.1.0-&gt;datasets) (3.0.12)
Requirement already satisfied: six&gt;=1.5 in /home/yoga/venvs/text_embeddings/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;datasets) (1.16.0)
</code></pre>
<p>and</p>
<p><code>!pip freeze</code></p>
<pre><code>certifi==2021.5.30
chardet==4.0.0
datasets==1.8.0
dill==0.3.4
filelock==3.0.12
fsspec==2021.6.1
huggingface-hub==0.0.13
idna==2.10
multiprocess==0.70.12.2
numpy==1.21.0
packaging==20.9
pandas==1.2.5
pyarrow==3.0.0
pyparsing==2.4.7
python-dateutil==2.8.1
pytz==2021.1
requests==2.25.1
six==1.16.0
tqdm==4.49.0
typing-extensions==3.10.0.0
urllib3==1.26.6
xxhash==2.0.2
</code></pre>
<p>Any ideas? Do I need to configure the notebook in a special way, or is there a problem with the datasets module? Thanks!</p>
<hr />
<p><strong>Edit:</strong> Following the answer below, this makes the error go away:</p>
<pre><code>datasets_dir=r&quot;/home/yoga/venvs/text_embeddings/lib/python3.8/site-packages/datasets&quot;

import sys
sys.path.append(datasets_dir)

import datasets
</code></pre>
<p>But is there a way that works without setting this path explicitely? (Or can somebody explain why this is necessary here?)</p>
","16351166","","16351166","","2021-06-30 15:04:11","2021-06-30 17:30:38","ModuleNotFoundError huggingface datasets in Jupyter notebook","<python><jupyter-notebook><huggingface-transformers><huggingface-datasets>","1","4","","","","CC BY-SA 4.0"
"68236419","1","","","2021-07-03 12:51:33","","0","22","<p>I'm currently trying to convert character-level spans to token-level spans and am wondering if there's a functionality in the library that I may not be taking advantage of.</p>
<p>The data that I'm currently using consists of &quot;proper&quot; text (I say &quot;proper&quot; as in it's written as if it's a normal document, not with things like extra whitespaces for easier split operations) and annotated entities. The entities are annotated at the character level but I would like to obtain the tokenized subword-level span.</p>
<p>My plan was to first convert character-level spans to word-level spans, then convert that to subword-level spans. A piece of code that I wrote looks like this:</p>
<pre><code>new_text = []
for word in original_text.split():
    if (len(word) &gt; 1) and (word[-1] in ['.', ',', ';', ':']):
        new_text.append(word[:-1] + ' ' + word[-1])
    else:
        new_text.append(word)

new_text = ' '.join(new_text).split()

word2char_span = {}
start_idx = 0
for idx, word in enumerate(new_text):
    char_start = start_idx
    char_end = char_start + len(word)
    word2char_span[idx] = (char_start, char_end)
    start_idx += len(word) + 1
</code></pre>
<p>This seems to work well but one edge case I didn't think of is parentheses. To give a more concrete example, one paragraph-entity pair looks like this:</p>
<pre><code>&gt;&gt;&gt; original_text = &quot;RDH12, a retinol dehydrogenase causing Leber's congenital amaurosis, is also involved in \
steroid metabolism. Three retinol dehydrogenases (RDHs) were tested for steroid converting abilities: human and \
murine RDH 12 and human RDH13. RDH12 is involved in retinal degeneration in Leber's congenital amaurosis (LCA). \
We show that murine Rdh12 and human RDH13 do not reveal activity towards the checked steroids, but that human type \
12 RDH reduces dihydrotestosterone to androstanediol, and is thus also involved in steroid metabolism. Furthermore, \
we analyzed both expression and subcellular localization of these enzymes.&quot;
&gt;&gt;&gt; entity_span = [139, 143]
&gt;&gt;&gt; print(original_text[139:143])
'RDHs'
</code></pre>
<p>This example actually returns a <code>KeyError</code> when I try to refer to <code>(139, 143)</code> because the adjustment code I wrote takes <code>(RDHs)</code> as the entity rather than <code>RDHs</code>. I don't want to hardcode parentheses handling either because there are some entities where the parentheses are included.</p>
<p>I feel like there should be a simpler approach to this issue and I'm overthinking things a bit. Any feedback on how I could achieve what I want is appreciated.</p>
","7766024","","","","","2021-07-03 13:31:46","Converting character-level spans to token-level spans","<python><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"68259014","1","","","2021-07-05 15:59:55","","0","29","<p>Working with pretrained BERT model for regression task. I have idea that dataset has useful not text feature, which can help to improve result. Is it possible to modify my model so that the text field is processed by BERT taking into account this useful feature?</p>
<pre class=""lang-py prettyprint-override""><code>class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()

        config = AutoConfig.from_pretrained(model_name)
        config.update({&quot;output_hidden_states&quot;:True, 
                       &quot;hidden_dropout_prob&quot;: 0.0,
                       &quot;layer_norm_eps&quot;: 1e-7})                       
        
        self.roberta = AutoModel.from_pretrained(model_name, config=config)  
            
        self.attention = torch.nn.Sequential(            
            torch.nn.Linear(768, 512),            
            torch.nn.Tanh(),                       
            torch.nn.Linear(512, 1),
            torch.nn.Softmax(dim=1)
        )        

        self.regressor = torch.nn.Sequential(                        
            torch.nn.Linear(768, 1)                        
        )
        

    def forward(self, input_ids, attention_mask):
        roberta_output = self.roberta(input_ids=input_ids,
                                      attention_mask=attention_mask)        

        last_layer_hidden_states = roberta_output.hidden_states[-1]
        weights = self.attention(last_layer_hidden_states)
        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        
        return self.regressor(context_vector)
</code></pre>
<p><code>df</code> - some dataset with a 'text' feature processed by Bert and useful feature 'stat' (<code>float</code>). I what to use the 'stat' feature to improve my predictions.</p>
","5592430","","3607203","","2021-07-07 12:15:07","2021-07-07 12:15:07","Transformers with additional external data","<python><bert-language-model><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68228474","1","","","2021-07-02 16:17:40","","0","27","<p>I need to load pre-trained SciBERT from the Huggingface TensorFlowÂ backend.</p>
","9120020","","","","","2021-07-02 16:17:40","How can a per-trained TensorFlowÂ model be loaded from a checkpoint in the Huggingface TensorFlowÂ backend?","<tensorflow><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68233265","1","","","2021-07-03 04:32:15","","2","235","<h1>Question</h1>
<p>Please help understand the cause of the issue below and how to build a Keras model for fine-tuning on top of the pre-trained model from the huggingface.</p>
<h2>Objective</h2>
<p>Create a custom model for DistilBERT fine tuning on top of <a href=""https://huggingface.co/transformers/model_doc/distilbert.html#tfdistilbertforsequenceclassification"" rel=""nofollow noreferrer"">TFDistilBertForSequenceClassification</a> from Huggingface.</p>
<h3>Input shape to the model</h3>
<p>From the shape of the tokenizer output, I assumed it is <code>(2, None, 256)</code> as <code>[input_ids, attention_mask]</code> would go into the model.</p>
<p>The output of the tokenizer.</p>
<pre><code>from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

max_sequence_length = 256
tokens = tokenizer(
    &quot; &quot;.join([&quot;token&quot;] * max_sequence_length), 
    truncation=True,
    padding=True,
    max_length=max_sequence_length,
    return_tensors=&quot;tf&quot;
)
print(tokens)
---
{
  'input_ids':      &lt;tf.Tensor: shape=(1, 256), dtype=int32, numpy=array([[  101, 19204, 19204, 19204, 19204, 19204, 19204, 19204, 19204, ...]], dtype=int32)&gt;, 
  'attention_mask': &lt;tf.Tensor: shape=(1, 256), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]], dtype=int32)&gt;
}
</code></pre>
<h3>Pretrained Model</h3>
<pre><code>model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')
for layer in model.layers:
    if layer.name == &quot;distilbert&quot;:
        layer.trainable = False
model.summary()
---
Model: &quot;tf_distil_bert_for_sequence_classification_4&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
distilbert (TFDistilBertMain multiple                  66362880  
_________________________________________________________________
pre_classifier (Dense)       multiple                  590592    
_________________________________________________________________
classifier (Dense)           multiple                  1538      
_________________________________________________________________
dropout_99 (Dropout)         multiple                  0         
=================================================================
Total params: 66,955,010
Trainable params: 592,130
Non-trainable params: 66,362,880
</code></pre>
<h2>Custom model</h2>
<p>Added a Keras Dense layer on top of the pretrained model using Sequential.</p>
<pre><code>seq = Sequential([
   model,
   Dense(
       name=&quot;output_softmax&quot;, 
       units=2, 
       activation=&quot;softmax&quot;
   )
])
seq.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam()
)
</code></pre>
<h1>Problem</h1>
<p><a href=""https://keras.io/api/layers/base_layer/#layer-class"" rel=""nofollow noreferrer"">The base Layer class</a> says build method creates the weights.</p>
<blockquote>
<p>build(self, input_shape): This method can be used to create weights that depend on the shape(s) of the input(s), using add_weight(). <strong>call</strong>() will automatically build the layer (if it has not been built yet) by calling build().</p>
</blockquote>
<p>Run the method but got the error.</p>
<pre><code>seq.build(input_shape=(2, None, max_sequence_length))
---
...
ValueError: You cannot build your model by calling `build` if your layers do not support float-type inputs. Instead, in order to instantiate and build your model, `call` your model on real tensor data (of the correct type).
</code></pre>
<p>As per the error message, feed the tokenizer output to the model and got another error.</p>
<pre><code>seq(tokens)
---
TypeError: Failed to convert 'TFSequenceClassifierOutput(loss=None, logits=TensorShape([1, 2]), hidden_states=None, attentions=None)' to a shape: ''logits''could not be converted to a dimension. A shape should either be single dimension (e.g. 10), or an iterable of dimensions (e.g. [1, 10, None]).
</code></pre>
<h1>Envirionment</h1>
<pre><code>python --version
---
Python 3.7.10

print(tf.__version__)
---
2.5.0

print(transformers.__version__)
---
4.8.2
</code></pre>
","4281353","","9215780","","2021-07-03 09:29:11","2021-08-06 17:21:54","Huggingface fine-tuning - how to build a custom model on top of pre-trained","<tensorflow><keras><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68233646","1","","","2021-07-03 06:02:01","","0","82","<p>I am encountering a <code>ValueError</code> in my Python code when trying to fine-tune <a href=""https://huggingface.co/gpt2"" rel=""nofollow noreferrer"">Hugging Face's distribution of the GPT-2 model</a>. Specifically:</p>
<pre><code>ValueError: Dimensions must be equal, but are 64 and 0 for
'{{node Equal_1}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Cast_18, Cast_19)'
with input shapes: [64,0,1024], [2,0,12,1024].
</code></pre>
<p>I have around 100 text files that I concatenate into a string variable called <code>raw_text</code> and then pass into the following function to create training and testing TensorFlow datasets:</p>
<pre class=""lang-py prettyprint-override""><code>def to_datasets(raw_text):
    # split the raw text in smaller sequences
    seqs = [
        raw_text[SEQ_LEN * i:SEQ_LEN * (i + 1)]
        for i in range(len(raw_text) // SEQ_LEN)
    ]

    # set up Hugging Face GPT-2 tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    tokenizer.pad_token = tokenizer.eos_token

    # tokenize the character sequences
    tokenized_seqs = [
        tokenizer(seq, padding=&quot;max_length&quot;, return_tensors=&quot;tf&quot;)[&quot;input_ids&quot;]
        for seq in seqs
    ]

    # convert tokenized sequences into TensorFlow datasets
    trn_seqs = tf.data.Dataset \
        .from_tensor_slices(tokenized_seqs[:int(len(tokenized_seqs) * TRAIN_PERCENT)])
    tst_seqs = tf.data.Dataset \
        .from_tensor_slices(tokenized_seqs[int(len(tokenized_seqs) * TRAIN_PERCENT):])

    def input_and_target(x):
        return x[:-1], x[1:]

    # map into (input, target) tuples, shuffle order of elements, and batch
    trn_dataset = trn_seqs.map(input_and_target) \
        .shuffle(SHUFFLE_BUFFER_SIZE) \
        .batch(BATCH_SIZE, drop_remainder=True)
    tst_dataset = tst_seqs.map(input_and_target) \
        .shuffle(SHUFFLE_BUFFER_SIZE) \
        .batch(BATCH_SIZE, drop_remainder=True)

    return trn_dataset, tst_dataset
</code></pre>
<p>I then try to train my model, calling <code>train_model(*to_datasets(raw_text))</code>:</p>
<pre class=""lang-py prettyprint-override""><code>def train_model(trn_dataset, tst_dataset):
    # import Hugging Face GPT-2 model
    model = TFGPT2Model.from_pretrained(&quot;gpt2&quot;)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=tf.metrics.SparseCategoricalAccuracy()
    )

    model.fit(
        trn_dataset,
        epochs=EPOCHS,
        initial_epoch=0,
        validation_data=tst_dataset
    )
</code></pre>
<p>The <code>ValueError</code> is triggered on the <code>model.fit()</code> call. The variables in all-caps are settings pulled in from a JSON file. Currently, they are set to:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;BATCH_SIZE&quot;:64,
    &quot;SHUFFLE_BUFFER_SIZE&quot;:10000,
    &quot;EPOCHS&quot;:500,
    &quot;SEQ_LEN&quot;:2048,
    &quot;TRAIN_PERCENT&quot;:0.9
}
</code></pre>
<p>Any information regarding what this error means or ideas on how to resolve it would be greatly appreciated. Thank you!</p>
","6272434","","","","","2021-07-29 19:55:56","ValueError when trying to fine-tune GPT-2 model in TensorFlow","<tensorflow><huggingface-transformers><transformer><pre-trained-model><gpt-2>","1","0","0","","","CC BY-SA 4.0"
"68277635","1","","","2021-07-06 21:41:27","","0","24","<p>I am using the python huggingface <code>transformers</code> library for a <code>text-generation</code> model. I need to know how to implement the <code>stopping_criteria</code> parameter in the <code>generator()</code> function I am using.</p>
<p>I found the <code>stopping_criteria</code> parameter in this documentation:
<a href=""https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TextGenerationPipeline"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TextGenerationPipeline</a></p>
<p>The problem is, I just dont know how to implement it.</p>
<p>My Code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125M')
stl = StoppingCriteria(['###'])
res = generator(prompt, do_sample=True,stopping_criteria = stl)
</code></pre>
","14695522","","14695522","","2021-07-07 17:55:51","2021-07-07 17:55:51","How to implement `stopping_criteria` parameter in transformers library?","<python><generator><documentation><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68166081","1","","","2021-06-28 15:27:53","","0","140","<p>i saved my model like this :</p>
<pre><code>ktrain.get_predictor(learner.model,preproc=trans).save('model')
</code></pre>
<p>i want to load my model and use it and to do something like :</p>
<pre><code>predictor = ktrain.load(folder)
x = &quot;hello wold&quot;
prediction = predictor(x) 
</code></pre>
<p>now, i have a folder &quot;model&quot; which contains 5 files : vocab.txt , tokenizer_config.json, tf_model.preproc , special_tokens_map.json and config.json
thank you please help me to load and use my predictor</p>
","15864132","","","","","2021-06-28 17:22:58","is there a method to load predictor from model file ktrain?","<tensorflow><keras><bert-language-model><huggingface-transformers><ktrain>","1","0","","","","CC BY-SA 4.0"
"68204547","1","","","2021-07-01 05:00:39","","0","35","<p>I am trying to find a way to understand if a given text sequence is Analytical (sentence which is necessarily true), contradictory (sentence which is necessarily false, or Synthetic, a sentence that may be true or false depending).</p>
<p>To help me break this problem down, I've first attempted to see if I could find if a sentence is true or false. Using zero shot learning.</p>
<pre><code>from transformers import pipeline

classifier = pipeline(&quot;zero-shot-classification&quot;)

sequence1 = &quot;The Earth is flat. &quot;
sequence2 = &quot;The Earth is round. &quot;
candidate_labels = [&quot;true&quot;, &quot;false&quot;]

print(classifier(sequence1, candidate_labels, multi_label=True))
print(classifier(sequence2, candidate_labels, multi_label=True))
</code></pre>
<pre><code>Output: {'sequence': 'The Earth is flat. ', 'labels': ['false', 'true'], 'scores': [0.993715226650238, 0.8926827907562256]}
{'sequence': 'The Earth is round. ', 'labels': ['true', 'false'], 'scores': [0.9893032908439636, 0.25639939308166504]}
</code></pre>
<p>Given the outputs, zero-shot learning has predicted that the first sequence is more false than true, while the second sequence is more true than false; I further pushed this into labelling a sentence as contradictory, analytical or synthetic.</p>
<p>For example, given the contradictory sequence: &quot;A man is a butterfly.&quot;, the zero-shot learning produces the following results:</p>
<pre><code>from transformers import pipeline

classifier = pipeline(&quot;zero-shot-classification&quot;)

sequence1 = &quot; A man is a butterfly.&quot;
candidate_labels = [&quot;analytic&quot;, &quot;contradictory&quot;, &quot;synthetic&quot;]

print(classifier(sequence1, candidate_labels, multi_label=True))
</code></pre>
<pre><code>Output: {'sequence': ' A man is a butterfly.', 'labels': ['contradictory', 'synthetic', 'analytic'], 'scores': [0.9467607736587524, 0.029895156621932983, 0.02475137822329998]}
</code></pre>
<p>The result classifies the sequence as 0.94% contradictory. My question is, for semantical understanding in text, would this approach work for labelling sequences?</p>
","13022556","","","","","2021-07-01 05:00:39","Is there a way to find different types or sentences using Python?","<python><machine-learning><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68313263","1","","","2021-07-09 08:03:04","","0","83","<p>When I want to use huggingface's pretrained models such as mbart to conduct multilingual experiments, the meaning of paramaters <code>decoder_start_token_id</code> and <code>forced_bos_token_id</code> confuse me. I find codes like:</p>
<pre><code># While generating the target text set the decoder_start_token_id to the target language id. 
# The following example shows how to translate English to Romanian 
# using the facebook/mbart-large-en-ro model.
from transformers import MBartForConditionalGeneration, MBartTokenizer

tokenizer = MBartTokenizer.from_pretrained(&quot;facebook/mbart-large-en-ro&quot;, src_lang=&quot;en_XX&quot;)
article = &quot;UN Chief Says There Is No Military Solution in Syria&quot;
inputs = tokenizer(article, return_tensors=&quot;pt&quot;)
translated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id[&quot;ro_RO&quot;])
tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
</code></pre>
<p>and:</p>
<pre><code># To generate using the mBART-50 multilingual translation models, 
# eos_token_id is used as the decoder_start_token_id and the target language id is forced as the first generated token. 
# To force the target language id as the first generated token, 
# pass the forced_bos_token_id parameter to the generate method. 
# The following example shows how to translate between Hindi to French and Arabic to English 
# using the facebook/mbart-50-large-many-to-many checkpoint.
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

article_hi = &quot;à¤¸à¤‚à¤¯à¥à¤•à¥à¤¤ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤•à¥‡ à¤ªà¥à¤°à¤®à¥à¤– à¤•à¤¾ à¤•à¤¹à¤¨à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤¸à¥€à¤°à¤¿à¤¯à¤¾ à¤®à¥‡à¤‚ à¤•à¥‹à¤ˆ à¤¸à¥ˆà¤¨à¥à¤¯ à¤¸à¤®à¤¾à¤§à¤¾à¤¨ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ&quot;
article_ar = &quot;Ø§Ù„Ø£Ù…ÙŠÙ† Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© ÙŠÙ‚ÙˆÙ„ Ø¥Ù†Ù‡ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ù„ Ø¹Ø³ÙƒØ±ÙŠ ÙÙŠ Ø³ÙˆØ±ÙŠØ§.&quot;

model = MBartForConditionalGeneration.from_pretrained(&quot;facebook/mbart-large-50-many-to-many-mmt&quot;)
tokenizer = MBart50TokenizerFast.from_pretrained(&quot;facebook/mbart-large-50-many-to-many-mmt&quot;)

# translate Hindi to French
tokenizer.src_lang = &quot;hi_IN&quot;
encoded_hi = tokenizer(article_hi, return_tensors=&quot;pt&quot;)
generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.lang_code_to_id[&quot;fr_XX&quot;])
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
# =&gt; &quot;Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire en Syria.&quot;

# translate Arabic to English
tokenizer.src_lang = &quot;ar_AR&quot;
encoded_ar = tokenizer(article_ar, return_tensors=&quot;pt&quot;)
generated_tokens = model.generate(**encoded_ar, forced_bos_token_id=tokenizer.lang_code_to_id[&quot;en_XX&quot;])
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
# =&gt; &quot;The Secretary-General of the United Nations says there is no military solution in Syria.&quot;
</code></pre>
<p>While the annotation of this two paramaters are:</p>
<pre><code>decoder_start_token_id (:obj:`int`, `optional`): 
If an encoder-decoder model starts decoding with a different token than `bos`, 
the id of that token.

forced_bos_token_id (:obj:`int`, `optional`): 
The id of the token to force as the first generated token after the :obj:`decoder_start_token_id`.
Useful for multilingual models like :doc:`mBART &lt;../model_doc/mbart&gt;` where 
the first generated token needs to be the target language token.
</code></pre>
<p>And for different varients of mbart, such as <code>facebook/mbart-large-cc25</code> and <code>facebook/mbart-large-50</code>, which one should we specify to generate response of specific language?</p>
","10814749","","10814749","","2021-07-09 08:09:08","2021-07-14 08:04:25","How to understand decoder_start_token_id and forced_bos_token_id in mbart?","<pytorch><multilingual><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68242979","1","","","2021-07-04 08:35:55","","0","30","<p>I am trying to train a DistilBert sentiment analysis model. I am using <code>criterion = nn.BCEWithLogitsLoss(reduction='mean')</code> and <code>optimizer = optim.AdamW(model.parameters(), lr = 1e-3)</code></p>
<p>My batch size is 15. Here's the <a href=""https://colab.research.google.com/github/JeremiahKamama/Spacy-Analysis/blob/main/analysis.ipynb#scrollTo=Ct-OsTggfYyn"" rel=""nofollow noreferrer"">rest of the code</a>. Where do I need to change to rectify the error?</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-538-b4d602b96318&gt; in &lt;module&gt;()
----&gt; 1 train_loss, train_acc = train(model, train_iterator, optimizer,criterion)

3 frames
&lt;ipython-input-537-a155d1e33041&gt; in train(model, train_iterator, optimizer, criterion)
     12         predictions = model(batch.text).squeeze(1)
     13 
---&gt; 14         loss = criterion(predictions, batch.label)
     15 
     16         acc = binary_accuracy(predictions, batch.label)#.unsqueeze(1))

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py in forward(self, input, target)
    714                                                   self.weight,
    715                                                   pos_weight=self.pos_weight,
--&gt; 716                                                   reduction=self.reduction)
    717 
    718 

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in binary_cross_entropy_with_logits(input, target, weight, size_average, reduce, reduction, pos_weight)
   2956 
   2957     if not (target.size() == input.size()):
-&gt; 2958         raise ValueError(&quot;Target size ({}) must be the same as input size ({})&quot;.format(target.size(), input.size()))
   2959 
   2960     return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)

ValueError: Target size (torch.Size([15])) must be the same as input size (torch.Size([15, 2]))
</code></pre>
","11651479","","","","","2021-07-04 08:35:55","Target size (torch.Size([15])) must be the same as input size (torch.Size([15, 2]))","<python><machine-learning><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68301034","1","","","2021-07-08 11:45:30","","1","43","<p>The aim is to extract the sub-tree (phrases) from the sentence if the 'nsubj' exists in the given sentence.</p>
<p>Here is the code which I am using:</p>
<pre><code>import spacy    
nlp = spacy.load('en')
piano_doc = nlp('The alarm clock is, to many high school students, a wailing monstrosity whose purpose is to torture all who are sleep-deprived')
    for token in piano_doc:
        if token.dep_ == 'nsubj':    
            print (token.text, token.tag_, token.head.text, token.dep_)
            subtree = token.subtree
            print([(t.text) for t in subtree])
            print('*' * 50)
</code></pre>
<p>The output we get is:
clock NN is nsubj</p>
<p>['The', 'alarm', 'clock']</p>
<hr />
<p>purpose NN is nsubj</p>
<p>['whose', 'purpose']</p>
<hr />
<p>who WP are nsubj</p>
<p>['who']</p>
<hr />
<p>But the output i am expecting in the case of nsubj is the whole subtree i.e.</p>
<hr />
<p>purpose NN is nsubj</p>
<p>['whose', 'purpose','is','to','torture']</p>
<hr />
<p>who WP are nsubj</p>
<p>['who' ,'are' ,'sleep-deprived']</p>
","13034631","","","","","2021-07-09 04:21:41","Extract subtree (Phrase) for nsubj elements using spacy","<python><nlp><spacy><huggingface-transformers><dependency-parsing>","1","2","","","","CC BY-SA 4.0"
"68315780","1","68381359","","2021-07-09 11:08:29","","0","70","<p>I've recently been trying to get hands on experience with the transformer library from Hugging Face. Since I'm an absolute noob when it comes to using Pytorch (and Deep Learning in general), I started with the introduction that can be found <a href=""https://huggingface.co/course/chapter3?fw=pt"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Here is the code to install dependencies :</p>
<pre><code>#!pip install transformers
!pip install transformers[sentencepiece] # includes transformers dependencies
!pip install datasets # datasets from huggingface hub
!pip install tqdm
</code></pre>
<p>Here's the code they propose to use to fine-tune BERT the MNPR dataset (used in the GLUE benchmark). This dataset includes two sentences per &quot;sample&quot;, so in the tokenizer we have to use <code>sentence1</code> and <code>sentence2</code>.</p>
<pre><code>from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
from torch.utils.data import DataLoader
from transformers import AutoModelForSequenceClassification
from transformers import AdamW
from transformers import get_scheduler
import torch
from tqdm.auto import tqdm

raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)
checkpoint = &quot;bert-base-uncased&quot;

# functions defining how the tokenizer works
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
  return tokenizer(example[&quot;sentence1&quot;], example[&quot;sentence2&quot;], truncation=True)

# tokenizer will use dynamic padding (https://huggingface.co/course/chapter3/2?fw=pt)
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# remove unecessary columns from data and format in torch tensors
tokenized_datasets = tokenized_datasets.remove_columns(
  [&quot;sentence1&quot;, &quot;sentence2&quot;, &quot;idx&quot;]
)
tokenized_datasets = tokenized_datasets.rename_column(&quot;label&quot;, &quot;labels&quot;)
tokenized_datasets.set_format(&quot;torch&quot;)

train_dataloader = DataLoader(
  tokenized_datasets[&quot;train&quot;], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
  tokenized_datasets[&quot;validation&quot;], batch_size=8, collate_fn=data_collator
)

# loading model and training requirements
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
  &quot;linear&quot;,
  optimizer=optimizer,
  num_warmup_steps=0,
  num_training_steps=num_training_steps
)
print(num_training_steps)

device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
model.to(device)

progress_bar = tqdm(range(num_training_steps))

# training loop:
model.train()
for epoch in range(num_epochs):
  for batch in train_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    outputs = model(**batch)
    loss = outputs.loss
    loss.backward()

    optimizer.step()
    lr_scheduler.step()
    optimizer.zero_grad()
    progress_bar.update(1)
    # assert 1==0
</code></pre>
<p>This works perfectly fine for me in Google Colab. I wanted to do the same thing with another dataset <code>sst2</code>. The code I use is very similar to the one above. The only few lines of code that change are the lines to import the data and the tokenizer (we have one sentence per feature instead of two). I have double-checked and the tokenizer works fine. Here is my code :</p>
<pre><code># imports
import torch
from datasets import load_dataset # datasets from huggingface
# tokenization
from transformers import AutoTokenizer, DataCollatorWithPadding
from torch.utils.data import DataLoader
# training
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
from tqdm.auto import tqdm

# Hyperparameters
batch_size = 8
learning_rate = 5e-5
num_epochs = 3
num_warmup_steps = 0

# load dataset and choosing checkpoint
raw_datasets = load_dataset(&quot;glue&quot;, &quot;sst2&quot;)
checkpoint = &quot;bert-base-uncased&quot;
# load tokenizer
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# tokenization of dataset
def tokenize_function(example):
  return tokenizer(example[&quot;sentence&quot;], truncation=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns([&quot;sentence&quot;, &quot;idx&quot;])
tokenized_datasets = tokenized_datasets.rename_column(&quot;label&quot;, &quot;labels&quot;) 
tokenized_datasets.set_format(&quot;torch&quot;)

# setting DataLoader
train_dataloader = DataLoader(
  tokenized_datasets[&quot;train&quot;], shuffle=True, batch_size=batch_size, collate_fn=data_collator
)
eval_dataloader = DataLoader(
  tokenized_datasets[&quot;validation&quot;], batch_size=batch_size, collate_fn=data_collator
)

# import model
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)

# setup training loop
optimizer = AdamW(model.parameters(), lr=learning_rate)

num_training_steps = num_epochs * len(train_dataloader)
print(num_training_steps)
lr_scheduler = get_scheduler(
    &quot;linear&quot;,
    optimizer=optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)
# chose device (GPU or CPU)
device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
model.to(device)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
  for batch in train_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()} 
    for k,v in batch.items():
      print(f&quot;key={k},v.dtype={v.dtype}, type(v)={type(v)}&quot;)
    outputs = model(**batch)
    loss = outputs.loss
    loss.backward()
        
    optimizer.step()
    lr_scheduler.step()
    optimizer.zero_grad()
    progress_bar.update(1)
</code></pre>
<p>And here's the error I get :</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-11-7893d7715ac2&gt; in &lt;module&gt;()
     69     outputs = model(**batch)
     70     loss = outputs.loss
---&gt; 71     loss.backward()
     72 
     73     optimizer.step()

1 frames
/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    147     Variable._execution_engine.run_backward(
    148         tensors, grad_tensors_, retain_graph, create_graph, inputs,
--&gt; 149         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
    150 
    151 

RuntimeError: Found dtype Long but expected Float
</code></pre>
<p>This seems like a very silly mistake, but like I said I'm an absolute pytorch noob and it's difficult for me to know where to start solving this issue. I have checked the type of the values in <code>batch.items()</code> and in both cases, they are all <code>torch.int64</code> (or <code>torch.long</code>). I tried to change the <code>attention_mask</code> and <code>input_ids</code> values to <code>torch.float32</code>, but I got the same error message.</p>
<p>Thanks in advance.</p>
<p>Python version and packages :</p>
<ul>
<li>python 3.7.20</li>
<li>Pytorch 1.9.0+cu102</li>
<li>transformers 4.8.2</li>
<li>GPU : Tesla T4 (also tried with tesla P4)</li>
</ul>
","16222904","","","","","2021-07-14 17:03:38","Wrong tensor type when trying to do the HuggingFace tutorial (pytorch)","<python><nlp><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68197664","1","","","2021-06-30 15:28:26","","0","51","<p>I'm quite new to the whole HuggingFace pipeline world, and I have stumbled upon something which I can't figure out. I have googled quite a bit for an answer, but haven't found anything yet, so any help would be great.
I am trying to get just the score from the HF pipeline sentiment classifier, not the label, as I want to apply the scores to a dataframe containing many cells of text.
I know how to achieve this on just a single sentence, namely like so:</p>
<pre><code>from transformers import pipeline
classifier = pipeline(&quot;sentiment-analysis&quot;)

result = classifier(&quot;This is a positive sentence&quot;)[0]
(result['score'])
</code></pre>
<p>This gives me the following output:</p>
<blockquote>
<p>0.9994597434997559</p>
</blockquote>
<p>I know how to apply the classifier to my dataframe. However, when I adapt the code above to the dataframe, like so:</p>
<pre><code>result = df['text'].apply(lambda x: classifier(x[:512]))[0]
df['sentiment'] = result['score']
</code></pre>
<p>My code fails on the second line, with the following error:</p>
<pre><code>TypeError: list indices must be integers or slices, not str
</code></pre>
<p>Does anyone know how to fix this? I have tried a few things, but I haven't been able to figure it out so far. Any help would be immensely appreciated!</p>
","1339136","","","","","2021-06-30 15:28:26","How to take just the score from HuggingFace Pipeline Sentiment Analysis","<python><sentiment-analysis><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68209677","1","","","2021-07-01 11:52:16","","0","27","<p>The code below is taken from a <a href=""https://huggingface.co/course/chapter3/4?fw=pt"" rel=""nofollow noreferrer"">tutorial</a> by huggingface:</p>
<pre><code>from datasets import load_metric

metric= load_metric(&quot;glue&quot;, &quot;mrpc&quot;)
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)
    
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch[&quot;labels&quot;])

metric.compute()
</code></pre>
<p>Inside the loop <code>for batch in eval_dataloader:</code>, how can I know which indices from the dataset this batch includes?</p>
<p>The DataLoader is created earlier using</p>
<pre><code>eval_dataloader = DataLoader(
    tokenized_datasets[&quot;validation&quot;], batch_size=8, collate_fn=data_collator
)
</code></pre>
<p>Note that it's without the shuffling flag, so it's possible to count manually using batch size, but how to do it with shuffling? Is it possible to to make it a field of the batch when creating the dataset and dataloader?</p>
","165753","","165753","","2021-07-01 17:24:20","2021-07-01 17:24:20","get batch indices when iterating DataLoader over a huggingface Dataset","<nlp><pytorch><huggingface-transformers><pytorch-dataloader><huggingface-datasets>","0","0","","","","CC BY-SA 4.0"
"68403128","1","68616941","","2021-07-16 03:39:38","","0","49","<p>I am trying to create a search relevance model where I take the dot product between query vector and resulting documents. I add a positional bias term on top to take into account the fact that position 1 is more likely to be clicked on. The final (unnormalised) log likelihood calculation is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>        query = self.query_model(query_input_ids, query_attention_mask)
        docs = self.doc_model(doc_input_ids, doc_attention_mask)
        positional_bias = self.position_model()
        
        if optimizer_idx is not None:
            if optimizer_idx == 0:
                docs = docs.detach()
                positional_bias = positional_bias.clone().detach()
            elif optimizer_idx == 1:
                query = query.detach()
                positional_bias = positional_bias.clone().detach()
            else:
                query = query.detach()
                docs = docs.detach()
                
        similarity = (docs @ query.unsqueeze(-1)).squeeze()

        click_log_lik = (similarity + positional_bias)\
                .reshape(doc_mask.shape)\
                .masked_fill_((1 - doc_mask).bool(), float(&quot;-inf&quot;))
</code></pre>
<p>The query and doc model is simply a distilbert model with a projection layer on top of CLS token. The models can be seen here: <a href=""https://pastebin.com/g21g9MG3"" rel=""nofollow noreferrer"">https://pastebin.com/g21g9MG3</a></p>
<p>When inspecting the first gradient descent step, it has <code>nan</code>s, but only for the query model and not the doc model. <strong>My hypothesis</strong> is that normalizing the return values for doc and query models (<code>return F.normalize(out, dim=-1)</code>) is somehow playing up with the gradients.</p>
<p>Does anyone know <s>1. If my hypothesis is true</s> and more importantly 2. <strong>How can I rectify nan gradients?</strong>.</p>
<h2 id=""additional-info-el5r"">Additional Info:</h2>
<ul>
<li>None of the losses are inf or nan.</li>
<li>query is BS x 768</li>
<li>docs is BS x DOC_RESULTS x 768</li>
<li>positional_bias is DOC_RESULTS</li>
<li>DOC_RESULTS is 10 in my case.</li>
<li>The <code>masked_fill</code> in the last line is because occasionally I have less than 10 data points for a query.</li>
</ul>
<h2 id=""update-1-t6xj"">Update 1</h2>
<p>The following changes made no difference to nans:</p>
<ul>
<li>Changing <code>masked_fill</code> from <code>-inf</code> to <code>1e5</code>.</li>
<li>Changing the projection from <code>F.normalize(out, dim=-1)</code> to <code>out / 100</code>.</li>
<li>Removed positional bias altogether with again no luck.</li>
</ul>
","2530674","","2530674","","2021-07-16 06:35:18","2021-08-02 05:56:50","Getting nans for gradient","<tensorflow><deep-learning><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68312300","1","","","2021-07-09 06:39:58","","0","41","<p>I used &quot;<code>!sudo apt-get install git-lfs</code>&quot;, and I'm on windows, am I wrong somewhere: how can I get pas this error message?</p>
<p>Text:</p>
<pre class=""lang-sh prettyprint-override""><code>[ ] model.push_to_hub (MY_MODEL_NAME, use_auth_token=HUGGINGFACE_API_KEY)
    tokenizer.push_to_hub(MY MODEL_NAME, use_auth_token-HUGGINGFACE_API_KEY)

07/09/2021 01:13:08 - INFO huggingface_hub.repository - git version 2.17.1
Sorry, no usage text found for &quot;git-lfs&quot;
--------------------------------------------------------------------
Called ProcessError                Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/huggingface hub/repository.py in git commit(self, commit_message)
    396                 encoding=&quot;utf-8&quot;,
--&gt; 397                 cwd-self.local dir,
    398              )
------------------------------------ 5 frames ----------------------
Called ProcessError: Command '['git', 'commit', '-m', ' add model'] returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

OSError                             Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/huggingface_hub/repository.py in git commit(self, commit_message)

    401                 raise EnvironmentError(exc.stderr)
    402              else:
--&gt; 403                 raise EnvironmentError (exc.stdout)
    404
    405     def git push(self) -&gt; str:

OSError: On branch main
Your branch is ahead of origin/main' by 1 commit. 
  (use &quot;git push&quot; to publish your local commits)

nothing to commit, working tree clean
</code></pre>
<p>Screenshot:</p>
<p><a href=""https://i.stack.imgur.com/qcXOQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qcXOQ.jpg"" alt=""Screenshot"" /></a></p>
","16412367","","6309","","2021-07-09 07:17:53","2021-07-09 07:17:53","How to push trained NLP model to huggingface.co via git-lfs?","<python><git><nlp><huggingface-transformers><gpt-2>","0","3","1","","","CC BY-SA 4.0"
"68421125","1","68421933","","2021-07-17 13:52:11","","0","27","<p>This is how I build the model for classification task:</p>
<pre><code>    def bert_for_classification(transformer_model_name, max_sequence_length, num_labels):
        config = ElectraConfig.from_pretrained(
            transformer_model_name,
            num_labels=num_labels,
            output_hidden_states=False,
            output_attentions=False
        )
        model = TFElectraForSequenceClassification.from_pretrained(transformer_model_name, config=config)
        # This is the input for the tokens themselves(words from the dataset after encoding):
        input_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int32, name='input_ids')

        # attention_mask - is a binary mask which tells BERT which tokens to attend and which not to attend.
        # Encoder will add the 0 tokens to the some sequence which smaller than MAX_SEQUENCE_LENGTH,
        # and attention_mask, in this case, tells BERT where is the token from the original data and where is 0 pad
        # token:
        attention_mask = tf.keras.layers.Input((max_sequence_length,), dtype=tf.int32, name='attention_mask')

        # Use previous inputs as BERT inputs:
        output = model([input_ids, attention_mask])[0]
        output = tf.keras.layers.Dense(num_labels, activation='softmax')(output)
        model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)

        model.compile(loss=keras.losses.CategoricalCrossentropy(),
                      optimizer=keras.optimizers.Adam(3e-05, epsilon=1e-08),
                      metrics=['accuracy'])

        return model
</code></pre>
<p>After I trained this model I save it using <code>model.save_weights('model.hd5')</code>
But it turns out there are two files that are saved: <code>model.hd5.index</code> and <code>model.hd5.data-00000-of-00001</code></p>
<p>How should I load this model from the disk?</p>
","5516760","","","","","2021-07-17 15:35:10","huggingface transformer with tensorflow saves two files as model weights","<python><tensorflow><tensorflow2.0><huggingface-transformers>","1","3","","","","CC BY-SA 4.0"
"68350133","1","","","2021-07-12 15:36:29","","1","28","<p>Trying the pretrained Faceboook DETR model for object detection using the HuggingFace implementation.
The sample code listed below from <a href=""https://huggingface.co/facebook/detr-resnet-50"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/detr-resnet-50</a> is straightforward.</p>
<pre><code>from transformers import DetrFeatureExtractor, DetrForObjectDetection
from PIL import Image
import requests
import numpy as np 

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50')
model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')

inputs = feature_extractor(images=image, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)

# model predicts bounding boxes and corresponding COCO classes
logits = outputs.logits
bboxes = outputs.pred_boxes
</code></pre>
<p>I can use</p>
<pre><code>threshod = 0.7
labels =['background', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
       'train', 'truck', 'boat', 'traffic light', 'fire hydrant',
       'street sign', 'stop sign', 'parking meter', 'bench', 'bird',
       'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',
       'giraffe', 'hat', 'backpack', 'umbrella', 'shoe', 'eye glasses',
       'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',
       'sports ball', 'kite', 'baseball bat', 'baseball glove',
       'skateboard', 'surfboard', 'tennis racket', 'bottle', 'plate',
       'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',
       'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog',
       'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',
       'mirror', 'dining table', 'window', 'desk', 'toilet', 'door', 'tv',
       'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',
       'oven', 'toaster', 'sink', 'refrigerator', 'blender', 'book',
       'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
       'toothbrush']

np_softmax = (logits.softmax(-1)[0, :, :-1]).detach().numpy()
classes = []
probability = []
idx = []
for i, j in enumerate(np_softmax):
  if np.max(j) &gt; threshold:
    classes.append(labels[np.argmax(j)])
    probability.append(np.max(j))
    idx.append(i)
</code></pre>
<p>to retrieve the detected classes. But I did not fully understand the coordinates in the bboxes.
This is a torch tensor with 100 bounded boxes coordinates of 4 dimensions. With <code>idx</code> I can get the index of the classes so I can get their corresponding boxes. Seems the coordinates are normalized because they are all between 0 and 1. I have a difficulty to remap the coordinates into pixels so I can draw the bounded boxes on the original images. Could not find documentation on this, any suggestions?  Thanks</p>
","5016733","","","","","2021-08-14 19:52:41","Facebook DETR resnet 50 in HuggingFace Hub","<huggingface-transformers><resnet>","2","0","","","","CC BY-SA 4.0"
"68377527","1","","","2021-07-14 11:49:31","","0","9","<p>I am working on training Wav2Vec2 Model. However, I am not able to find any way to get the timestamp of each character. Please help</p>
","1591392","","","","","2021-07-14 11:49:31","How to get character wise timestamp in Wav2Vec2?","<speech-recognition><huggingface-transformers><transcription>","0","0","","","","CC BY-SA 4.0"
"68430210","1","","","2021-07-18 14:48:29","","0","48","<p>I'm attempting to receive a features vector of short wav (audio) files using wav2vec by using <a href=""https://huggingface.co/transformers/model_doc/wav2vec2.html"" rel=""nofollow noreferrer"">Hugging Face Transformers</a>.</p>
<p>However, for unknown reasons, no matter which approach I use to control the output size, the results do not meet my requirements.</p>
<p>Ideally, I'd like to get all of the vectors to be the same length (e.g. 60K).
I try to get it with the following command:</p>
<pre class=""lang-py prettyprint-override""><code>feature_extractor(input_audio, sampling_rate=16000, return_tensors=&quot;np&quot;, padding=&quot;max_length&quot;,
                                    max_length=60000).input_values
</code></pre>
<p>That command helped me create a minimal boundary of the data size by padding all the vectors into a minimum of 60K length, but I was surprised to see vectors with 120K values created as well.</p>
<p>Then I remove the padding parameter in the hope of obtaining vectors with no padding but an upper boundary of 60K.
Based on the <code>max_length</code> documentation:</p>
<blockquote>
<p>Maximum length of the returned list and optionally padding length</p>
</blockquote>
<p>So I executed this line:</p>
<pre class=""lang-py prettyprint-override""><code>feature_extractor(input_audio, sampling_rate=16000, return_tensors=&quot;np&quot;,
                                    max_length=60000).input_values
</code></pre>
<p>Unexpectedly, I receive vectors ranging in length from 20K to 120K. Not limited at all.</p>
<hr />
<p>To reproduce my bug and results, I've included a snippet of code and a link to relevant audio data.</p>
<pre class=""lang-py prettyprint-override""><code>import librosa
import numpy as np
from transformers import Wav2Vec2FeatureExtractor
from pathlib import Path

    p = Path(dataset_path)
    audio_files = [i.parents[0] / i.name for i in p.glob('**/*.wav')]
    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-base-960h')
    for file in (audio_files):
        input_audio, _ = librosa.load(file,
                                      sr=16000)
        features_with_padding = feature_extractor(input_audio, sampling_rate=16000,
                                return_tensors=&quot;np&quot;, padding=&quot;max_length&quot;, max_length=60000).input_values                                
        features_without_padding = feature_extractor(input_audio, sampling_rate=16000,
                                  return_tensors=&quot;np&quot;, max_length=60000).input_values
        print(features_with_padding.shape, features_without_padding.shape)
</code></pre>
<p>In <a href=""https://drive.google.com/drive/folders/1j-BNp8D8yN16exgoacgDtDrnMF-jQP91?usp=sharing"" rel=""nofollow noreferrer"">this</a> drive folder, I attached 2 wav files that create about 80K length vector.</p>
<p>How could I create a one-size feature vector with a wav2vec transformer?</p>
","11079284","","11079284","","2021-07-18 20:44:50","2021-07-20 08:10:29","How to limit the size of the features vector in Wav2Vec?","<python><numpy><huggingface-transformers><transformer>","1","0","","","","CC BY-SA 4.0"
"68444252","1","68451130","","2021-07-19 16:49:40","","2","190","<p>I have a function that will load a pre-trained model from huggingface and fine-tune it for sentiment analysis then calculates the F1 score and returns the result.
The problem is when I call this function multiple times with the exact same arguments, it will give the exact same metric score which is expected, except for the first time which is different, how is that possible?</p>
<p>This is my function which is written based on <a href=""https://huggingface.co/course/chapter3/3?fw=pt"" rel=""nofollow noreferrer"">this tutorial</a> in huggingface:</p>
<pre class=""lang-py prettyprint-override""><code>import uuid

import numpy as np

from datasets import (
    load_dataset,
    load_metric,
    DatasetDict,
    concatenate_datasets
)

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

CHECKPOINT = &quot;distilbert-base-uncased&quot;
SAVING_FOLDER = &quot;sst2&quot;
def custom_train(datasets, checkpoint=CHECKPOINT, saving_folder=SAVING_FOLDER):

    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    
    def tokenize_function(example):
        return tokenizer(example[&quot;sentence&quot;], truncation=True)

    tokenized_datasets = datasets.map(tokenize_function, batched=True)
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    saving_folder = f&quot;{SAVING_FOLDER}_{str(uuid.uuid1())}&quot;
    training_args = TrainingArguments(saving_folder)

    trainer = Trainer(
        model,
        training_args,
        train_dataset=tokenized_datasets[&quot;train&quot;],
        eval_dataset=tokenized_datasets[&quot;validation&quot;],
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    trainer.train()
    
    predictions = trainer.predict(tokenized_datasets[&quot;test&quot;])
    print(predictions.predictions.shape, predictions.label_ids.shape)
    preds = np.argmax(predictions.predictions, axis=-1)
    
    metric_fun = load_metric(&quot;f1&quot;)
    metric_result = metric_fun.compute(predictions=preds, references=predictions.label_ids)
    
    return metric_result
</code></pre>
<p>And then I will run this function several times with the same datasets, and append the result of the returned F1 score each time:</p>
<pre class=""lang-py prettyprint-override""><code>raw_datasets = load_dataset(&quot;glue&quot;, &quot;sst2&quot;)

small_datasets = DatasetDict({
    &quot;train&quot;: raw_datasets[&quot;train&quot;].select(range(100)).flatten_indices(),
    &quot;validation&quot;: raw_datasets[&quot;validation&quot;].select(range(100)).flatten_indices(),
    &quot;test&quot;: raw_datasets[&quot;validation&quot;].select(range(100, 200)).flatten_indices(),
})

results = []
for i in range(4):
    result = custom_train(small_datasets)
    results.append(result)
</code></pre>
<p>And then when I check the results list:</p>
<pre><code>[{'f1': 0.7755102040816325}, {'f1': 0.5797101449275361}, {'f1': 0.5797101449275361}, {'f1': 0.5797101449275361}]
</code></pre>
<p>Something that may come to mind is that when I load a pre-trained model, the head will be initialized with random weights and that is why the results are different, if that is the case, why only the first one is different and the others are exactly the same?</p>
","9690045","","","","","2021-07-20 07:31:10","Multiple training with huggingface transformers will give exactly the same result except for the first time","<python><machine-learning><deep-learning><nlp><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"68473592","1","","","2021-07-21 16:55:13","","0","37","<p>when I use:</p>
<pre><code>modelname = 'deepset/bert-base-cased-squad2'
model = BertForQuestionAnswering.from_pretrained(modelname)
tokenizer = AutoTokenizer.from_pretrained(modelname)
nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)

result = nlp({'question': question,'context': context}) 
</code></pre>
<p>it doesn't crash. However when i use encode_plus():</p>
<pre><code>modelname = 'deepset/bert-base-cased-squad2'
model = BertForQuestionAnswering.from_pretrained(modelname)
tokenizer = AutoTokenizer.from_pretrained(modelname)

inputs= tokenizer.encode_plus(question,context,return_tensors='pt') 
</code></pre>
<p>I have this error:<br />
The size of tensor a (629) must match the size of tensor b (512) at non-singleton dimension 1</p>
<p>which I understand but why I don't have the same error in the first case? Can someone explain the difference?</p>
","16090334","","16090334","","2021-07-21 17:00:30","2021-08-12 06:23:29","Bert using transformer's pipeline and encode_plus function","<python><nlp><bert-language-model><huggingface-transformers><question-answering>","1","1","","","","CC BY-SA 4.0"
"68414535","1","","","2021-07-16 19:45:52","","0","29","<p>What is the most correct way to add an additional layer on top of Huggin</p>
<pre><code>from transformers import BertModel
class CustomBERTModel(nn.Module):
    def __init__(self, ...):
          super(CustomBERTModel, self).__init__()
          self.bert = BertModel.from_pretrained(&quot;dbmdz/bert-base-italian-xxl-cased&quot;)
          ### a transformer layer
          self.transformer = nn.Transformer(...)
          self.fc = nn.Linear(...)

    def forward(self, src, src_attention_mask, src_token_type_ids, labels):
        src = self.bert(input_ids=src, attention_mask=src_attention_mask, token_type_ids=src_token_type_ids)
        output = self.transformer(...)
        return self.fc(output)
</code></pre>
<p>Is this implementation correct? One concern might be the transformer layer here is the pytorch transformer. But maybe it is better to use the hugging face transformer instead? So what is the correct way to add an transform layer on top of hugging face Bert?</p>
<p>Thanks a lot.</p>
","8183106","","","","","2021-07-16 19:45:52","How to add a transformer on top of hugging face Bert model","<nlp><bert-language-model><huggingface-transformers><transformer>","0","1","","","","CC BY-SA 4.0"
"68414910","1","","","2021-07-16 20:26:41","","0","7","<p>I am trying to use NER (Natural Entity Recognition) in order to extract information from receipts. I have clustered textboxes, so my data for a single receipt includes the OCR-read text data as well as the location of the text in the image. However, I have limited labelled data(&lt;100 receipts), so in order to get a decent model I understand I should be using something pretrained.</p>
<p>But I don't understand how I can include the spatial information (the pixel coordinates of the text boxes); can I simply append them onto the text feature vectors (even though the encoder pretrained models will have only been trained on text data)?</p>
","16416762","","","","","2021-07-16 20:26:41","How to use a pretrained model when concatenating info to feature vectors","<nlp><huggingface-transformers><pre-trained-model>","0","0","","","","CC BY-SA 4.0"
"68483855","1","","","2021-07-22 11:15:49","","0","18","<p>I have a fine tuned xlm-roberta-base model for a binary classification task like below:</p>
<p><code>model = XLMRobertaForSequenceClassification.from_pretrained( &quot;xlm-roberta-base&quot;, num_labels=2, ) </code></p>
<p>I want to re-train the model using maskedlm where both input and label are sentences then train it again for binary classification task but I don't know if its possible and if it's possible what is the syntax to do so. Right now I can't load my XLMRobertaForSequenceClassification into the maskedLM model.</p>
<p><code>model = RobertaForMaskedLM.from_pretrained(&quot;xlm-roberta-base&quot;) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) print(&quot;device is &quot;, device) model.load_state_dict(torch.load('fine_tuned_model.pt', map_location=torch.device('cpu')))</code></p>
<p>Any help appreciated.
thanks</p>
","1898270","","","","","2021-07-22 11:15:49","Roberta re-finetuning a fine tuned model for a different task","<bert-language-model><huggingface-transformers><pre-trained-model><roberta-language-model>","0","1","","","","CC BY-SA 4.0"
"68484170","1","","","2021-07-22 11:37:29","","0","27","<p>So, let's say I have a following csv dataset. I have to use pre-trained BERT question-answering model to train , predict and finally evaluate. As, I am new to this it would be helpful to see similar project to understand and work myself on my project or any guidance would be helpful too.</p>
<p>I already tried training the model individually (checking one article at a time), that works. I need guidance on how to work with CSV dataset and evaluation.</p>
<p><a href=""https://i.stack.imgur.com/oAGU1.png"" rel=""nofollow noreferrer"">Here is the format of the dataset</a></p>
","16502388","","16502388","","2021-07-22 11:44:11","2021-07-22 11:44:11","How to use pre-trained BERT question-answering model for text extraction in Python?","<csv><text-extraction><bert-language-model><huggingface-transformers><question-answering>","0","3","","","","CC BY-SA 4.0"
"68313471","1","","","2021-07-09 08:18:43","","2","147","<p>I want to use spacy-transformers in a corporate environment with limited internet access, so i have to download transformer models from the huggingfaces hub manually and get them to work in spacy.</p>
<p>In this example i tried to use the transformer pipline component from the en_core_web_trf pretrained model:</p>

<pre><code>import spacy
import spacy_transformers

nlp_trf = spacy.load(&quot;en_core_web_trf&quot;) # load roberta pretrained model
transformer= nlp_trf.get_pipe(&quot;transformer&quot;) # get transformer pipeline component
transformer.to_disk(&quot;transfomer_pretrained&quot;) # save pipeline component to disk

nlp = spacy.blank(&quot;en&quot;) 
trf = nlp.add_pipe(&quot;transformer&quot;)
trf.from_disk(&quot;transformer_pretrained&quot;, exclude=[&quot;vocab&quot;]) # load transformer pipeline component from disk

</code></pre>
<p>I get following error message:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-23-c66c45181d83&gt; in &lt;module&gt;
      1 #trf.model.initialize([nlp.make_doc(&quot;hello world&quot;)])
----&gt; 2 trf.from_disk(&quot;models/transformer_pretrained&quot;, exclude=[&quot;vocab&quot;])
      3 nlp.pipe_names

C:\_Development\Python37\site-packages\spacy_transformers\pipeline_component.py in from_disk(self, path, exclude)
    400             &quot;model&quot;: load_model,
    401         }
--&gt; 402         util.from_disk(path, deserialize, exclude)
    403         return self

C:\_Development\Python37\site-packages\spacy\util.py in from_disk(path, readers, exclude)
   1172         # Split to support file names like meta.json
   1173         if key.split(&quot;.&quot;)[0] not in exclude:
-&gt; 1174             reader(path / key)
   1175     return path
   1176 

C:\_Development\Python37\site-packages\spacy_transformers\pipeline_component.py in load_model(p)
    390             p = Path(p).absolute()
    391             tokenizer, transformer = huggingface_from_pretrained(
--&gt; 392                 p, self.model.attrs[&quot;tokenizer_config&quot;]
    393             )
    394             self.model.attrs[&quot;tokenizer&quot;] = tokenizer

C:\_Development\Python37\site-packages\spacy_transformers\util.py in huggingface_from_pretrained(source, config)
     29     else:
     30         str_path = source
---&gt; 31     tokenizer = AutoTokenizer.from_pretrained(str_path, **config)
     32     transformer = AutoModel.from_pretrained(str_path)
     33     ops = get_current_ops()

C:\_Development\Python37\site-packages\transformers\models\auto\tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    388         kwargs[&quot;_from_auto&quot;] = True
    389         if not isinstance(config, PretrainedConfig):
--&gt; 390             config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
    391 
    392         use_fast = kwargs.pop(&quot;use_fast&quot;, True)

C:\_Development\Python37\site-packages\transformers\models\auto\configuration_auto.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    396         &quot;&quot;&quot;
    397         kwargs[&quot;_from_auto&quot;] = True
--&gt; 398         config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
    399         if &quot;model_type&quot; in config_dict:
    400             config_class = CONFIG_MAPPING[config_dict[&quot;model_type&quot;]]

C:\_Development\Python37\site-packages\transformers\configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    464                 local_files_only=local_files_only,
    465                 use_auth_token=use_auth_token,
--&gt; 466                 user_agent=user_agent,
    467             )
    468             # Load config dict

C:\_Development\Python37\site-packages\transformers\file_utils.py in cached_path(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)
   1171             user_agent=user_agent,
   1172             use_auth_token=use_auth_token,
-&gt; 1173             local_files_only=local_files_only,
   1174         )
   1175     elif os.path.exists(url_or_filename):

C:\_Development\Python37\site-packages\transformers\file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)
   1387                 else:
   1388                     raise ValueError(
-&gt; 1389                         &quot;Connection error, and we cannot find the requested files in the cached path.&quot;
   1390                         &quot; Please try again or make sure your Internet connection is on.&quot;
   1391                     )

ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.
</code></pre>
<p>As the error message states, the requested files cannot be found in the cached path. Can somebody explain to me which files i have to put in the chache path? Or a another way to pre download models and use them in spacy.</p>
<p>Versions:</p>
<p>spacy               3.0.5</p>
<p>spacy-transformers  1.0.2</p>
<p>transformers        4.5.1</p>
","16412791","","16412791","","2021-07-09 09:53:13","2021-07-09 09:53:13","Is there a way to use spacy-transformers from disk (offline)","<python><spacy><huggingface-transformers><spacy-3><spacy-transformers>","0","1","","","","CC BY-SA 4.0"
"68343073","1","68403617","","2021-07-12 06:54:41","","0","127","<p>I am trying to generate summary of long PDF. So, what I did, first I converted my pdf to text using <code>pdfminer.six</code> library. Next, I used 2 functions which were provided in a discuss <a href=""https://github.com/huggingface/transformers/issues/4224#issuecomment-694650789"" rel=""nofollow noreferrer"">here</a>.</p>
<p>The code:</p>
<pre><code>bart_tokenizer = BartTokenizer.from_pretrained(&quot;facebook/bart-large&quot;)
bart_model = BartModel.from_pretrained(&quot;facebook/bart-large&quot;, return_dict=True)

# generate chunks of text \ sentences &lt;= 1024 tokens
def nest_sentences(document):
  nested = []
  sent = []
  length = 0
  for sentence in nltk.sent_tokenize(document):
    length += len(sentence)
    if length &lt; 1024:
      sent.append(sentence)
    else:
      nested.append(sent)
      sent = [sentence]
      length = len(sentence)

  if sent:
    nested.append(sent)
  return nested

# generate summary on text with &lt;= 1024 tokens
def generate_summary(nested_sentences):
  device = 'cuda'
  summaries = []
  for nested in nested_sentences:
    input_tokenized = bart_tokenizer.encode(' '.join(nested), truncation=True, return_tensors='pt')
    input_tokenized = input_tokenized.to(device)
    summary_ids = bart_model.to(device).generate(
        input_tokenized,
        length_penalty=3.0,
        min_length=30,
        max_length=100,
    )
    output = [bart_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]
    summaries.append(output)
  summaries = [sentence for sublist in summaries for sentence in sublist]
  return summaries
</code></pre>
<p>Then, to get the summary, I do:</p>
<pre><code>nested_sentences = nest_sentences(text)
</code></pre>
<p>Where, <code>text</code> is a text of string having length around 10K which I converted using pdf library.</p>
<pre><code>summary = generate_summary(nested_sentences)
</code></pre>
<p>Then, I get the following error:</p>
<pre><code>---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-15-d5aa7709bb5f&gt; in &lt;module&gt;()
----&gt; 1 summary = generate_summary(nested_sentences)

3 frames

&lt;ipython-input-11-8554509269e0&gt; in generate_summary(nested_sentences)
     28         length_penalty=3.0,
     29         min_length=30,
---&gt; 30         max_length=100,
     31     )
     32     output = [bart_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]

/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)
     26         def decorate_context(*args, **kwargs):
     27             with self.__class__():
---&gt; 28                 return func(*args, **kwargs)
     29         return cast(F, decorate_context)
     30 

/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py in generate(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)
   1061                 return_dict_in_generate=return_dict_in_generate,
   1062                 synced_gpus=synced_gpus,
-&gt; 1063                 **model_kwargs,
   1064             )
   1065 

/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py in beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)
   1799                 continue  # don't waste resources running the code we don't need
   1800 
-&gt; 1801             next_token_logits = outputs.logits[:, -1, :]
   1802 
   1803             # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`

AttributeError: 'Seq2SeqModelOutput' object has no attribute 'logits'


</code></pre>
<p>I cannot find anything related to this error, so I would really appreciate it if anyone could help or is there any better approach to generate summary for long texts?</p>
<p>Thank you in advance!</p>
","11685381","","","","","2021-07-16 04:55:22","'Seq2SeqModelOutput' object has no attribute 'logits' BART transformers","<nlp><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68384132","1","","","2021-07-14 19:36:31","","0","41","<p>I am trying to freeze some layers of my masked language model using the following code:</p>
<pre><code>for param in model.bert.parameters():
    param.requires_grad = False
</code></pre>
<p>However, when I execute the code above, I get this error:</p>
<pre><code>AttributeError: 'RobertaForMaskedLM' object has no attribute 'bert'
</code></pre>
<p>In my code, I have the following imports for my masked language model, but I am unsure what is causing the error above:</p>
<pre><code>from transformers import AutoModelForMaskedLM
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
</code></pre>
<p>So far, I have tried to replace <code>bert</code> with <code>model</code> in my code, but that did not work.</p>
<p>Any help would be good.</p>
<p>Thanks.</p>
","16098918","","","","","2021-08-19 09:15:46","BERT: AttributeError: 'RobertaForMaskedLM' object has no attribute 'bert'","<python><bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"60287465","1","","","2020-02-18 18:39:53","","1","821","<p>Hi I'm trying to use 'fmikaelian/flaubert-base-uncased-squad' for question answering. I understand that I should load the model and the tokenizers. I'm not sure how should I do this. </p>

<p>My code is basically far</p>

<pre><code>from transformers import pipeline, BertTokenizer

nlp = pipeline('question-answering', \
model='fmikaelian/flaubert-base-uncased-squad', \
tokenizer='fmikaelian/flaubert-base-uncased-squad')
</code></pre>

<p>Most probably this can be solve with a two liner.</p>

<p>Many thanks</p>

<p><strong><em>EDIT</em></strong></p>

<p>I have also tried to use automodels but it seems those are not there:</p>

<pre><code>OSError: Model name 'flaubert-base-uncased-squad' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed 'flaubert-base-uncased-squad' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url.
</code></pre>

<p><strong><em>EDIT II</em></strong>
I tried following the approach suggested with the following code that loads models that have been saved from S3:</p>

<pre class=""lang-py prettyprint-override""><code>tokenizer_ = FlaubertTokenizer.from_pretrained(MODELS)
model_ = FlaubertModel.from_pretrained(MODELS)


p = transformers.QuestionAnsweringPipeline(
    model=transformers.AutoModel.from_pretrained(MODELS), 
    tokenizer=transformers.AutoTokenizer.from_pretrained(MODELS)
)

question_=""Quel est le montant de la garantie?""
language_=""French""
context_=""le montant de la garantie est â‚¬ 1000""

output=p({'question':question_, 'context': context_})
print(output)
</code></pre>

<p>Unfortunately I have been getting the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    exitcode = _main(fd)
Traceback (most recent call last):
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\spawn.py"", line 114, in _main
  File ""question_extraction.py"", line 61, in &lt;module&gt;
        prepare(preparation_data)
output=p({'question':question_, 'context': context_})  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\spawn.py"", line 225, in prepare

      File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\site-packages\transformers\pipelines.py"", line 802, in __call__
_fixup_main_from_path(data['init_main_from_path'])
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\spawn.py"", line 277, in _fixup_main_from_path
    run_name=""__mp_main__"")
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\... ...\Box Sync\nlp - 2...\NLP\src\question_extraction.py"", line 61, in &lt;module&gt;
    output=p({'question':question_, 'context': context_})
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\site-packages\transformers\pipelines.py"", line 802, in __call__
    for example in examples
      File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\site-packages\transformers\pipelines.py"", line 802, in &lt;listcomp&gt;
for example in examples
for example in examples  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\site-packages\transformers\pipelines.py"", line 802, in &lt;listcomp&gt;

      File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\site-packages\transformers\data\processors\squad.py"", line 304, in squad_convert_examples_to_features
for example in examples
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\site-packages\transformers\data\processors\squad.py"", line 304, in squad_convert_examples_to_features
        with Pool(threads, initializer=squad_convert_example_to_features_init, initargs=(tokenizer,)) as p:with Pool(threads, initializer=squad_convert_example_to_features_init, initargs=(tokenizer,)) as p:

  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\context.py"", line 119, in Pool
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\context.py"", line 119, in Pool
        context=self.get_context())context=self.get_context())

  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\pool.py"", line 174, in __init__
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\pool.py"", line 174, in __init__
        self._repopulate_pool()self._repopulate_pool()

  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\pool.py"", line 239, in _repopulate_pool
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\pool.py"", line 239, in _repopulate_pool
    w.start()
    w.start()
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\process.py"", line 105, in start
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\process.py"", line 105, in start
    self._popen = self._Popen(self)
      File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\context.py"", line 322, in _Popen
self._popen = self._Popen(self)
      File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\context.py"", line 322, in _Popen
return Popen(process_obj)
return Popen(process_obj)  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\popen_spawn_win32.py"", line 65, in __init__

  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\popen_spawn_win32.py"", line 33, in __init__
        prep_data = spawn.get_preparation_data(process_obj._name)reduction.dump(process_obj, to_child)

  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\spawn.py"", line 143, in get_preparation_data
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\reduction.py"", line 60, in dump
    _check_not_importing_main()
  File ""C:\Users\... ...\AppData\Local\Continuum\Anaconda3\envs\nlp_nlp\lib\multiprocessing\spawn.py"", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.
    ForkingPickler(file, protocol).dump(obj)
BrokenPipeError: [Errno 32] Broken pipe
</code></pre>

<p><strong>*EDIT IV *</strong></p>

<p>I solved the previous EDIT error by placing the functions inside the ""<strong>main</strong>"".
Unfortunately when I run the following code:</p>

<pre class=""lang-py prettyprint-override""><code>tokenizer_ = FlaubertTokenizer.from_pretrained(MODELS)
model_ = FlaubertModel.from_pretrained(MODELS)

def question_extraction(text, question, model, tokenizer, language=""French"", verbose=False):

    if language==""French"":
        nlp = pipeline('question-answering', \
        model=model, \
        tokenizer=tokenizer)
    else:
        nlp=pipeline('question-answering')

    output=nlp({'question':question, 'context': text})

    answer, score = output.answer, output.score 

    if verbose==True:
        print(""Q: "", question ,""\n"",\
              ""A:"", answer,""\n"", \
              ""Confidence (%):"", ""{0:.2f}"".format(str(score*100) )
              )

    return answer, score

if __name__==""__main__"":
    question_=""Quel est le montant de la garantie?""
    language_=""French""
    text=""le montant de la garantie est â‚¬ 1000""

    answer, score=question_extraction(text, question_, model_, tokenizer_, language_, verbose= True)
</code></pre>

<p>I'm getting the following error:</p>

<pre><code>C:\...\NLP\src&gt;python question_extraction.py
OK
OK
convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00&lt;00:00,  4.66it/s]
add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00&lt;?, ?it/s]
Traceback (most recent call last):
  File ""question_extraction.py"", line 77, in &lt;module&gt;
    answer, score=question_extraction(text, question_, model_, tokenizer_, language_, verbose= True)
  File ""question_extraction.py"", line 60, in question_extraction
    output=nlp({'question':question, 'context': text})
  File ""C:\...\transformers\pipelines.py"", line 818, in __call__
    start, end = self.model(**fw_args)
ValueError: not enough values to unpack (expected 2, got 1)

</code></pre>
","5768398","","5768398","","2020-02-25 15:49:05","2020-02-25 15:49:05","Pipeline Loading Models and Tokenizers for Q&A","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68450133","1","","","2021-07-20 05:53:41","","0","82","<p>I'm learning about using Pytorch Learning for different NLP tasks.
I tried to implement a token classification example that I found in <a href=""https://www.kaggle.com/eriknovak/pytorch-roberta-named-entity-recognition/notebook"" rel=""nofollow noreferrer"">Kaggle</a> using PL API, but when I run my code after two epochs my model converges to only predict the O (other, not entity) class. unlike the example in Kaggle who were able to learn significantly better how to predict different entities.
I rerun the Kaggle example locally and I was able to get similar results</p>
<p>What am I missing with my model? It looks like my implementation of the model optimize to learn something else.</p>
<p>This is the code that I'm using:</p>
<pre><code>class NERClassifier(pl.LightningModule):

def __init__(self, train_ds, val_ds, test_ds):
    super().__init__()
    self.train_ds, self.val_ds, self.test_ds = train_ds, val_ds, test_ds
    self.num_labels = self.train_ds.features['ner_tags'].feature.num_classes
    self.model = transformers.RobertaForTokenClassification.from_pretrained(FLAGS.model, num_labels=self.num_labels)
    self.loss = th.nn.CrossEntropyLoss(reduction='mean')
    self.confusion_matrix = th.zeros(self.num_labels, self.num_labels)

def prepare_data(self):
    tokenizer = transformers.RobertaTokenizer.from_pretrained(FLAGS.model)

    def _prepare_ds(ds):
        ds = ds.map(_tokenize)
        ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
        return ds

    def _tokenize(x):
        encodings = tokenizer(x['tokens'], truncation=True, padding='max_length', is_split_into_words=True)
        labels = x['ner_tags'] + [0] * (tokenizer.model_max_length - len(x['ner_tags']))
        return {**encodings, 'labels': labels}

    def _prepare():
        return map(_prepare_ds, [self.train_ds, self.val_ds, self.test_ds])

    def _update_model():
        labels = self.train_ds.features['ner_tags'].feature
        label2id = {k: labels.str2int(k) for k in labels.names}
        id2label = {v: k for k, v in label2id.items()}
        self.model.config.id2label = id2label
        self.model.config.label2id = label2id

    self.train_ds, self.val_ds, self.test_ds = _prepare()
    _update_model()

def train_dataloader(self):
    return DataLoader(self.train_ds, sampler=RandomSampler(self.train_ds),
                      batch_size=FLAGS.batch_size, pin_memory=True,
                      num_workers=FLAGS.num_workers)

def val_dataloader(self):
    return DataLoader(self.val_ds, batch_size=FLAGS.batch_size,
                      sampler=RandomSampler(self.val_ds),
                      pin_memory=True, num_workers=FLAGS.num_workers)

def test_dataloader(self):
    return DataLoader(self.test_ds, batch_size=FLAGS.batch_size,
                      sampler=RandomSampler(self.test_ds),
                      pin_memory=True, num_workers=FLAGS.num_workers)

def configure_optimizers(self):
    return th.optim.AdamW(self.parameters(), lr=FLAGS.lr, eps=FLAGS.eps)

def forward(self, batch, batch_idx):
    loss, logits = self.model(**batch, return_dict=False)
    return loss, logits

def training_step(self, batch, batch_idx):
    print('start training step')
    loss, logits = self.forward(batch, batch_idx)
    self.logger.experiment.add_scalar('train_loss', loss)
    return {'loss': loss, 'logits': logits}

def training_epoch_end(self, outputs):
    print('training epoch end')
    loss = th.mean(th.stack([o['loss'].float() for o in outputs]))
    self.logger.experiment.add_scalar('epoc_train_loss', loss, self.current_epoch)

def validation_step(self, batch, batch_idx):
    print('start validation step')
    loss, logits = self.forward(batch, batch_idx)
    labels_hat = th.argmax(logits, dim=2)
    tags = batch['attention_mask'].sum(dim=1)
    labels = batch['labels']
    self._update_confusion_matrix(labels, labels_hat, tags)
    self.logger.experiment.add_scalar('val_loss', loss)
    return {'loss': loss, 'logits': logits, 'labels_hat': labels_hat}

def validation_epoch_end(self, outputs):
    print('validation epoch end')
    loss = th.mean(th.stack([o['loss'].float() for o in outputs]))
    self.logger.experiment.add_scalar('epoc_val_loss', loss, self.current_epoch)

def on_validation_epoch_end(self) -&gt; None:
    print('end validation epoch')
    labels = self.model.config.id2label
    labels = list(labels.values())
    image_tensor = get_figure_from_cm(self.confusion_matrix, labels)
    self.logger.experiment.add_figure(f'confusion matrix_{self.current_epoch}', image_tensor, self.current_epoch)

def on_validation_end(self):
    print('on_validation_end!!!!!')

def _update_confusion_matrix(self, labels, labels_hat, tags):
    for label, label_hat, tag in zip(labels, labels_hat, tags):
        true_labels = label[:tag]
        predicted_labels = label_hat[:tag]
        for true, pred in zip(true_labels, predicted_labels):
            self.confusion_matrix[true.item()][pred.item()] += 1
</code></pre>
","1071842","","","","","2021-07-20 05:53:41","Token classification using Transformer and PL only predict one token","<python><pytorch><huggingface-transformers><named-entity-recognition><pytorch-lightning>","0","0","","","","CC BY-SA 4.0"
"68491678","1","","","2021-07-22 21:27:40","","0","36","<p><strong>Question: How to train XLNet on a &quot;NER-Like&quot; Task Successfully?</strong></p>
<p>I'm trying to do a &quot;Named-Entity-Recognition(NER)-like&quot; training on Huggingface's <code>TFXLNetForTokenClassification</code> model. Essentially, I am trying to label what are actions in a sentence.</p>
<p>My input sentence is: <code>text = &quot;Please click on this, and then press the save button&quot;</code></p>
<p>After running <code>tokenize(text)</code>:</p>
<pre><code>inputs = {'input_ids': &lt;tf.Tensor: shape=(1, 13), dtype=int32, numpy=
array([[1431, 1962,   31,   52,   19,   21,  137, 1320,   18, 1537, 3167,
           4,    3]], dtype=int32)&gt;, 'token_type_ids': &lt;tf.Tensor: shape=(1, 13), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]], dtype=int32)&gt;, 'attention_mask': &lt;tf.Tensor: shape=(1, 13), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)&gt;}
</code></pre>
<p>The label is: <code>label = tf.reshape(tf.constant([1,2,2,2,0,0,1,2,2,2,2,0,0]), (1, tf.size(input_ids)))</code>, where <code>1</code> corresponds to the beginning of an action, <code>2</code> corresponds to the entity of the same action, and <code>0</code> is not an action.</p>
<p>I create a <code>tf.data.Dataset</code> to pass into my model:</p>
<pre><code>train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(inputs),
    labels))
</code></pre>
<p>After starting training:</p>
<pre><code>action_model.compile(optimizer = 'adam')
action_model.fit(train_dataset, epochs=1)
</code></pre>
<p>It gives me an error:</p>
<pre><code>...
tf.transpose(inputs[&quot;input_ids&quot;], perm=(1, 0))
...
ValueError: Dimension must be 1 but is 2 for '{{node action_model_6/transformer/transpose}} = Transpose[T=DT_INT32, Tperm=DT_INT32](data_1, action_model_6/transformer/transpose/perm)' with input shapes: [13], [2].
</code></pre>
<p>However, when I try to run the function <code>tf.transpose(inputs[&quot;input_ids&quot;], perm=(1, 0))</code> on my sentence in isolation, it works perfectly.</p>
<p>Also, when I call the model by itself:</p>
<pre><code>x = dict(inputs)
action_model(x)
</code></pre>
<p>It gives me the corresponding <code>TFXLNetForTokenClassificationOutput</code> output class.</p>
<p>Appendix:
Full Source Code</p>
<pre><code>from transformers import XLNetTokenizer, TFXLNetForTokenClassification
import tensorflow as tf

class ActionModel(TFXLNetForTokenClassification):
    def __init__(self, *args, log_dir=None, cache_dir= None, **kwargs):
        super().__init__(*args, **kwargs)
        self.loss_tracker= tf.keras.metrics.Mean(name='loss')
        
    @tf.function
    def train_step(self, data):
        x = data[0]
        y_true = data[1]
        with tf.GradientTape() as tape:
            outputs = self(x, training=True) # &lt;------ It fails here
            logits = outputs['logits']
            loss = tf.reduce_mean(outputs['loss'])

            grads = tape.gradient(loss, self.trainable_variables)

        y_true = tf.reshape(y_true, [-1, 1])

        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        self.loss_tracker.update_state(loss)       
        self.compiled_metrics.update_state(y_true, logits)
        metrics = {m.name: m.result() for m in self.metrics}
        lr = self.optimizer._decayed_lr(tf.float32)
        metrics.update({'lr': lr})
        
        return metrics

tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')
action_model = ActionModel.from_pretrained('xlnet-base-cased')
text = &quot;Please click on this, and then press the save button&quot;
inputs = tokenizer(text, return_tensors=&quot;tf&quot;)
input_ids = inputs[&quot;input_ids&quot;]
labels = tf.reshape(tf.constant([1,2,2,2,0,0,1,2,2,2,2,0,0]), (-1, tf.size(input_ids))) # Batch size 1

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(inputs),
    labels))

action_model.compile(optimizer = 'adam')
action_model.fit(train_dataset, epochs=1)
</code></pre>
<p>Full Error Message</p>
<pre><code>ValueError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:855 train_function  *
        return step_function(self, iterator)
    &lt;ipython-input-47-6adc35b145c8&gt;:16 train_step  *
        outputs = self(x, training=True)
    /usr/local/lib/python3.7/dist-packages/transformers/models/xlnet/modeling_tf_xlnet.py:1757 call  *
        transformer_outputs = self.transformer(
    /usr/local/lib/python3.7/dist-packages/transformers/models/xlnet/modeling_tf_xlnet.py:630 call  *
        inputs[&quot;input_ids&quot;] = tf.transpose(inputs[&quot;input_ids&quot;], perm=(1, 0))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper  **
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:2227 transpose_v2
        return transpose(a=a, perm=perm, name=name, conjugate=conjugate)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:2308 transpose
        return transpose_fn(a, perm, name=name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py:11653 transpose
        &quot;Transpose&quot;, x=x, perm=perm, name=name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:601 _create_op_internal
        compute_device)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:3565 _create_op_internal
        op_def=op_def)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:2042 __init__
        control_input_ops, op_def)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1883 _create_c_op
        raise ValueError(str(e))

    ValueError: Dimension must be 1 but is 2 for '{{node action_model_6/transformer/transpose}} = Transpose[T=DT_INT32, Tperm=DT_INT32](data_1, action_model_6/transformer/transpose/perm)' with input shapes: [13], [2].
</code></pre>
","14330199","","","","","2021-07-22 21:27:40","XLNet: Custom Training for Named Entity Recognition - Huggingface Transformers - Tensorflow","<tensorflow><keras><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68492369","1","","","2021-07-22 23:08:41","","1","105","<p>I am using Hugginface's Trainer.
How to adjust the learning rate after N number of epochs?
For example, I have an initial learning rate set to <code>lr=2e-6</code>, and I would like to change the learning rate to <code>lr=1e-6</code> after the first epoch and stay on it the rest of the training.</p>
<p>I tried this so far:</p>
<pre><code>optimizer = AdamW(model.parameters(),
              lr = 2e-5,
              eps = 1e-8
            )

epochs = 5
batch_number = len(small_train_dataset) / 8
total_steps = batch_number * epochs


scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps,
                                            last_epoch=-1
                                            )
</code></pre>
<p>I know that there is <a href=""https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR</a> but here it drops learning rate every epoch but that is not what i want to do. I want it to drop after 1 epoch and then stay on it rest of the training process.</p>
","13240679","","6664872","","2021-09-18 18:02:33","2021-09-18 18:02:33","How to adjust the learning rate after N number of epochs?","<python><nlp><pytorch><huggingface-transformers>","3","0","","","","CC BY-SA 4.0"
"68492518","1","","","2021-07-22 23:37:27","","0","5","<p>The description for <code>TFXLNetForTokenClassification</code> is &quot;XLNet Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.&quot;</p>
<p>The default <code>num_layers</code> for <code>TFXLNetForTokenClassification</code> is 2. If we import a pre-trained model, then the logits layer only has 2 labels, and there doesn't seem to be a good way to expand to &gt;2 labels and keep the pre-train weights. Setting <code>self.config.num_layer=3</code> doesn't do the trick. How do we add another linear model (i.e. token classification head) on top to classify more than 2 labels?</p>
<p><a href=""https://huggingface.co/transformers/model_doc/xlnet.html#tfxlnetfortokenclassification"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/xlnet.html#tfxlnetfortokenclassification</a></p>
","14330199","","","","","2021-07-22 23:37:27","XLNet: Editing Last Layer for TFXLNetForTokenClassification","<huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68486020","1","","","2021-07-22 13:49:58","","0","29","<p>Working with ML task I use Trainer and load_dataset to fune-tune model on custom corpus.
But during train the strange error is appeared. It seems to me that stackoverflow doesn't allow to upload csv file. Below is my solution and info about used csv file:</p>
<pre><code>import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from transformers import AutoTokenizer, AutoModel
import torch

from datasets import load_dataset

text = 'excerpt'
raw_datasets = load_dataset('csv', data_files='./corpus.csv')

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)

def tokenize_function(examples):
    return tokenizer(examples[text], padding=&quot;max_length&quot;, truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

from sklearn.model_selection import ShuffleSplit
rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=0)

for train_index, test_index in rs.split(range(0, 2834)):
    pass
    
small_train_dataset = tokenized_datasets[&quot;train&quot;].select(train_index) 
small_eval_dataset = tokenized_datasets[&quot;train&quot;].select(test_index) 

from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification
model = AutoModelForMaskedLM.from_pretrained(&quot;bert-base-cased&quot;)


from transformers import TrainingArguments
training_args = TrainingArguments(&quot;test_trainer&quot;)

from transformers import Trainer

trainer = Trainer(
    model=model, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset
)

trainer.train()
</code></pre>
<h1 id=""csv-file-rpma"">CSV file:</h1>
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 2834 entries, 0 to 2833
Data columns (total 1 columns):
 #   Column   Non-Null Count  Dtype 
---  ------   --------------  ----- 
 0   excerpt  2834 non-null   object
dtypes: object(1)
memory usage: 22.3+ KB
</code></pre>
<p>Here excerpt feature contains text values.
The debug output for the mistake the following: KeyError: 'loss'. Please explain what is wrong.</p>
","5592430","","","","","2021-07-22 13:49:58","Transformers: Fine-tuning is failed on dataset built from csv file","<python><bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"68502532","1","","","2021-07-23 16:41:32","","0","46","<p>I have trained a masked language model using my own dataset, which contains sentences with emojis (trained on 20,000 entries).</p>
<p>Now, when I make predictions, I want emojis to be in the output, however, most of the predicted tokens are words, so I think that the emojis are right at the bottom of the list somewhere, as they must be less frequent tokens compared to the words.</p>
<p>So far, this is my output - you can see that one emoji has been predicted, but the rest of the predictions are words:</p>
<pre><code>mask_filler(&quot;I am so good today, &lt;mask&gt;&quot;, top_k=5)

[{'score': 0.2953376770019531,
  'sequence': 'I am so good today, friend',
  'token': 72,
  'token_str': 'friend'},
 {'score': 0.18523386120796204,
  'sequence': 'I am so good today ðŸ™‚',
  'token': 328,
  'token_str': 'ðŸ™‚'},
 {'score': 0.1431082785129547,
  'sequence': 'I am so good today, mate',
  'token': 2901,
  'token_str': 'mate'},
 {'score': 0.13269349932670593,
  'sequence': 'I am so good today, father',
  'token': 4,
  'token_str': 'father'},
 {'score': 0.030341114848852158,
  'sequence': 'I am so good today, mother',
  'token': 44660,
  'token_str': 'mother'},
</code></pre>
<p>Therefore, I was wondering if there is any code or functions that can filter the predictions, so that there are only emojis in the output, removing any predicted tokens that are words.</p>
<p>I have got one emoji to show in the output, but I think the rest of the emojis are less frequent tokens, so they are not appearing at the top when I make predictions.</p>
<p>So, is it possible to filter out the word tokens in favour of only emojis?</p>
<p>Thanks.</p>
","16098918","","","","","2021-07-25 11:56:50","BERT: Is it possible to filter the predicted tokens in masked language modelling?","<python><machine-learning><bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"68322542","1","68451347","","2021-07-09 20:46:22","","1","250","<p>I need to build a transformer-based architecture in Tensorflow following the encoder-decoder approach where the encoder is a preexisting Huggingface Distilbert model and the decoder is a CNN.</p>
<p>Inputs: a text containing texts with several phrases in a row. Outputs: codes according to taxonomic criteria. My data file has 7387 pairs text-label in TSV format:</p>
<pre><code>text \t code
This is example text number one. It might contain some other phrases. \t C21
This is example text number two. It might contain some other phrases. \t J45.1
This is example text number three. It might contain some other phrases. \t A27
</code></pre>
<p>The remainder of the code is this:</p>
<pre><code>        text_file = &quot;data/datafile.tsv&quot;
        with open(text_file) as f:
                lines = f.read().split(&quot;\n&quot;)[:-1]
                text_and_code_pairs = []
                for line in lines:
                        text, code = line.split(&quot;\t&quot;)
                        text_and_code_pairs.append((text, code))


        random.shuffle(text_and_code_pairs)
        num_val_samples = int(0.10 * len(text_and_code_pairs))
        num_train_samples = len(text_and_code_pairs) - 3 * num_val_samples
        train_pairs = text_and_code_pairs[:num_train_samples]
        val_pairs = text_and_code_pairs[num_train_samples : num_train_samples + num_val_samples]
        test_pairs = text_and_code_pairs[num_train_samples + num_val_samples :]

        train_texts = [fst for (fst,snd) in train_pairs]
        train_labels = [snd for (fst,snd) in train_pairs]
        val_texts = [fst for (fst,snd) in val_pairs]
        val_labels = [snd for (fst,snd) in val_pairs]
        test_texts = [fst for (fst,snd) in test_pairs]
        test_labels = [snd for (fst,snd) in test_pairs]

        distilbert_encoder = TFDistilBertModel.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;)
        tokenizer = DistilBertTokenizerFast.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;)

        train_encodings = tokenizer(train_texts, truncation=True, padding=True)
        val_encodings = tokenizer(val_texts, truncation=True, padding=True)
        test_encodings = tokenizer(test_texts, truncation=True, padding=True)

        train_dataset = tf.data.Dataset.from_tensor_slices((
                dict(train_encodings),
                train_labels
        ))
        val_dataset = tf.data.Dataset.from_tensor_slices((
                dict(val_encodings),
                val_labels
        ))
        test_dataset = tf.data.Dataset.from_tensor_slices((
                dict(test_encodings),
                test_labels
        ))

        model = build_model(distilbert_encoder)
        model.fit(train_dataset.batch(64), validation_data=val_dataset, epochs=3, batch_size=64)
        model.predict(test_dataset, verbose=1)
</code></pre>
<p>Lastly, the <code>build_model</code> function:</p>
<pre><code>def build_model(transformer, max_len=512):
        model = tf.keras.models.Sequential()
        # Encoder
        inputs = layers.Input(shape=(max_len,), dtype=tf.int32)
        distilbert = transformer(inputs)
        # LAYER - something missing here?
        # Decoder
        conv1D = tf.keras.layers.Conv1D(filters=5, kernel_size=10)(distilbert)
        pooling = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1D)
        flat = tf.keras.layers.Flatten()(pooling)
        fc = tf.keras.layers.Dense(1255, activation='relu')(flat)
        softmax = tf.keras.layers.Dense(1255, activation='softmax')(fc)
        model = tf.keras.models.Model(inputs = inputs, outputs = softmax)
        model.compile(tf.keras.optimizers.Adam(learning_rate=5e-5), loss=&quot;categorical_crossentropy&quot;, metrics=['accuracy'])
        print(model.summary())
        return model
</code></pre>
<p>I managed to narrow down the possible locations of my problem. After changing from sequential to functional Keras API, I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;keras_transformer.py&quot;, line 99, in &lt;module&gt;
    main()
  File &quot;keras_transformer.py&quot;, line 94, in main
    model = build_model(distilbert_encoder)
  File &quot;keras_transformer.py&quot;, line 23, in build_model
    conv1D = tf.keras.layers.Conv1D(filters=5, kernel_size=10)(distilbert)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 897, in __call__
    self._maybe_build(inputs)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 2416, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py&quot;, line 152, in build
    input_shape = tensor_shape.TensorShape(input_shape)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 771, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 771, in &lt;listcomp&gt;
    self._dims = [as_dimension(d) for d in dims_iter]
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 716, in as_dimension
    return Dimension(value)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 200, in __init__
    None)
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
TypeError: Dimension value must be integer or None or have an __index__ method, got 'last_hidden_state'
</code></pre>
<p>It seems that the error lies in the connection between the output of the transformer and the input of the convolutional layer. Am I supposed to include another layer between them so as to adapt the output of the transformer? If so, what would be the best option?I'm using tensorflow==2.2.0, transformers==4.5.1 and Python 3.6.9</p>
","8167370","","8167370","","2021-07-17 17:24:55","2021-07-20 07:51:03","Problem connecting transformer output to CNN input in Keras","<tensorflow><keras><conv-neural-network><huggingface-transformers><transformer>","2","0","2","","","CC BY-SA 4.0"
"68374748","1","","","2021-07-14 08:39:52","","0","24","<p>I used this code to load weights</p>
<pre><code>from transformers import DebertaTokenizer, DebertaModel
import torch

tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')
model = DebertaModel.from_pretrained('microsoft/deberta-base')
</code></pre>
<p>after that i want to optimize and use loss function using compile function</p>
<pre><code>model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=tf.metrics.SparseCategoricalAccuracy(),
)


</code></pre>
<p>I got this error
AttributeError: 'DebertaModel' object has no attribute 'compile'</p>
","7237926","","","","","2021-08-25 14:49:44","how to use deberta model from hugging face and use .compile() and . summary() with it","<python><nlp><bert-language-model><huggingface-transformers><huggingface-datasets>","1","3","","","","CC BY-SA 4.0"
"68388413","1","","","2021-07-15 05:30:20","","2","62","<p>I am using Hugging Face <code>mrm8488/longformer-base-4096-finetuned-squadv2</code> pre-trained model
<a href=""https://huggingface.co/mrm8488/longformer-base-4096-finetuned-squadv2"" rel=""nofollow noreferrer"">https://huggingface.co/mrm8488/longformer-base-4096-finetuned-squadv2</a>.</p>
<p>I want to generate sentence level embedding. I have a data-frame which has a text column.</p>
<p>I am using this code:</p>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
ckpt = &quot;mrm8488/longformer-base-4096-finetuned-squadv2&quot;
tokenizer = AutoTokenizer.from_pretrained(ckpt)
model = AutoModelForQuestionAnswering.from_pretrained(ckpt)

text = &quot;Huggingface has democratized NLP. Huge thanks to Huggingface for this.&quot; # I will pas text-column here from my data-frame
#question = &quot;What has Huggingface done ?&quot;
encoding = tokenizer(question, text, return_tensors=&quot;pt&quot;)
# I don't want to use it for Question-Answer use-case. I just need the sentence embeddings
input_ids = encoding[&quot;input_ids&quot;]

# default is local attention everywhere
# the forward method will automatically set global attention on question tokens
attention_mask = encoding[&quot;attention_mask&quot;] 
</code></pre>
<p>How can I do modification  in the above code to generate embedding for sentences. ?</p>
<p>I have the following examples:</p>
<pre><code>                           Text
i've added notes to the claim and it's been escalated for final review
after submitting the request you'll receive an email confirming the open request.
hello my name is person and i'll be assisting you
this is sam and i'll be assisting you for date.
I'll return the amount as asap.
ill return it to you.
</code></pre>
","9907733","","","","","2021-09-05 13:52:47","How to generate sentence embedding using long-former model","<python-3.x><deep-learning><embedding><huggingface-transformers><transformer>","1","0","1","","","CC BY-SA 4.0"
"68481189","1","68486285","","2021-07-22 07:59:10","","0","24","<p>I am trying to import AutoTokenizer and AutoModelWithLMHead, but I am getting the following error:</p>
<p>ImportError: cannot import name 'AutoTokenizer' from partially initialized module 'transformers' (most likely due to a circular import)</p>
<p>First, I install transformers: <code>pip install transformers</code> then implemented the following code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelWithLMHead

tokenizer = AutoTokenizer.from_pretrained(&quot;t5-base&quot;)
model = AutoModelWithLMHead.from_pretrained(&quot;t5-base&quot;)
</code></pre>
","13022556","","","","","2021-07-22 14:06:48","Huggingface AutoTokenizer cannot be referenced when importing Transformers","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68511456","1","","","2021-07-24 15:36:22","","0","30","<p>We have a dataset with a few thousand of sentences, each having several labels.</p>
<p>For example:</p>
<ul>
<li>&quot;you are a bad guy&quot;</li>
<li>[provocation, judgement]</li>
</ul>
<p>Both labels are valid and accepted.</p>
<p>Currently we train BERT on the first one and it works pretty well, except that we need to train it on both to fit our use case.</p>
<p>Our first idea was to duplicate the lines of the dataset. So what we have:</p>
<p>Line 123.1</p>
<ul>
<li>&quot;you are a bad guy&quot;</li>
<li>provocation</li>
</ul>
<p>Line 123.2</p>
<ul>
<li>&quot;you are 1 bad guy&quot;</li>
<li>judgment</li>
</ul>
<p>Our concern are that:</p>
<ul>
<li>it may dilute the weight gained during the training from this sentence (not such a big deal probably?)</li>
<li>it will not work for the test set, unless we create from scratch a training scenario where we acknowledge that both labels are to be considered as a success</li>
</ul>
<p>Is there 1 more relevant way of working for this use case?</p>
","9927519","","","","","2021-07-24 15:36:22","Is it possible to train BERT for sentence classification when there is multiple labels possibilities for one line of the dataset?","<python><tensorflow><text-classification><bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"68442608","1","","","2021-07-19 14:54:15","","0","26","<p>I am using HuggingFace <code>Trainer</code> to fine-tune a multi-class text classification model using <code>distill-bert</code>. Is there a way I can get probabilities instead of class labels?</p>
","6509765","","","","","2021-07-19 14:54:15","how can I get class probabilities when using HuggingFace `Trainer`?","<pytorch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68457765","1","","","2021-07-20 15:43:24","","0","23","<p>I'm trying to find the best output of summary of a text using <code>pipline</code>
but, sometimes Im reciving this warning:</p>
<pre><code>Your max_length is set to 100, but you input_length is only 79. You might consider decreasing
max_length manually, e.g. summarizer('...', max_length=50) 
</code></pre>
<p>I saw in the documentation  (<a href=""https://huggingface.co/transformers/_modules/transformers/pipelines/text2text_generation.html#SummarizationPipeline"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/_modules/transformers/pipelines/text2text_generation.html#SummarizationPipeline</a>) that this is the input_legth from ther class:</p>
<pre><code>input_length = tf.shape(inputs[&quot;input_ids&quot;])[-1].numpy()
</code></pre>
<p>Is someone knows how can I get/know what is the <code>input_length</code> from the very beginning while I have an imput of few srtrings?</p>
","12620237","","12620237","","2021-07-20 15:50:23","2021-07-20 15:50:23","Transformers.pipelines: extract the input_length from summarization methode","<python><numpy><huggingface-transformers><summarization>","0","0","","","","CC BY-SA 4.0"
"68470573","1","","","2021-07-21 13:40:16","","0","11","<p>I'm trying to follow <a href=""https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb"" rel=""nofollow noreferrer"">this notebook</a> but I get stuck at loading my SQuAD dataset.</p>
<pre><code>import transformers
from datasets import load_dataset, load_metric

dataset = load_dataset('json', data_files={'train': 'squad/nl_squad_train_clean.json', 'test': 'squad/nl_squad_train_clean.json'}, field='data')
</code></pre>
<p>Gives the following error <code>ArrowInvalid: JSON parse error: Column(/paragraphs/[]/qas/[]/answers/[]) changed from object to array in row 0</code>.</p>
<p>Does anyone know how to fix this? If needed I can post the complete stack trace.</p>
","15158631","","","","","2021-07-21 13:40:16","JSON parse error when trying to load my own SQuAD dataset using Huggingface Transformers","<python><json><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"68499238","1","","","2021-07-23 12:30:43","","0","142","<h2 id=""environment-info-f7cj"">Environment info</h2>
<ul>
<li><code>transformers</code> version: 4.9.0</li>
<li>Platform: Linux-4.15.0-151-generic-x86_64-with-glibc2.27</li>
<li>Python version: 3.9.2</li>
<li>PyTorch version (GPU?): 1.7.1+cu101 (False)</li>
<li>Tensorflow version (GPU?): 2.5.0 (False)</li>
<li>Flax version (CPU?/GPU?/TPU?): 0.3.4 (cpu)</li>
<li>Jax version: 0.2.17</li>
<li>JaxLib version: 0.1.69</li>
<li>Using GPU in script?: no</li>
<li>Using distributed or parallel set-up in script?: no</li>
</ul>
<h2 id=""details-i0la"">Details</h2>
<p>I am attempting to use a fresh installation of transformers library, but after successfully completing the installation with pip, I am not able to run the test script: <code>python -c &quot;from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))&quot;</code></p>
<p>Instead, I see the following output:</p>
<blockquote>
<p>/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/gensim/similarities/<strong>init</strong>.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package &lt;https://pypi.org/proje$
t/python-Levenshtein/&gt; is unavailable. Install Levenhstein (e.g. <code>pip install python-Levenshtein</code>) to suppress this warning.<br />
warnings.warn(msg)<br />
Traceback (most recent call last):<br />
File &quot;&quot;, line 1, in <br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/file_utils.py&quot;, line 1977, in <strong>getattr</strong><br />
module = self._get_module(self._class_to_module[name])<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/file_utils.py&quot;, line 1986, in _get_module<br />
return importlib.import_module(&quot;.&quot; + module_name, self.<strong>name</strong>)<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/importlib/<strong>init</strong>.py&quot;, line 127, in import_module<br />
return _bootstrap._gcd_import(name[level:], package, level)<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/pipelines/<strong>init</strong>.py&quot;, line 25, in <br />
from ..models.auto.configuration_auto import AutoConfig<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/models/<strong>init</strong>.py&quot;, line 19, in <br />
from . import (<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/models/layoutlm/<strong>init</strong>.py&quot;, line 22, in <br />
from .configuration_layoutlm import LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP, LayoutLMConfig<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/models/layoutlm/configuration_layoutlm.py&quot;, line 19, in <br />
from ..bert.configuration_bert import BertConfig<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/models/bert/configuration_bert.py&quot;, line 21, in <br />
from ...onnx import OnnxConfig<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/onnx/<strong>init</strong>.py&quot;, line 16, in <br />
from .config import EXTERNAL_DATA_FORMAT_SIZE_LIMIT, OnnxConfig, OnnxConfigWithPast<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/onnx/config.py&quot;, line 18, in <br />
from transformers import PretrainedConfig, PreTrainedTokenizer, TensorType<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/file_utils.py&quot;, line 1977, in <strong>getattr</strong><br />
module = self._get_module(self._class_to_module[name])<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/file_utils.py&quot;, line 1986, in _get_module<br />
return importlib.import_module(&quot;.&quot; + module_name, self.<strong>name</strong>)<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/importlib/<strong>init</strong>.py&quot;, line 127, in import_module<br />
return _bootstrap._gcd_import(name[level:], package, level)<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/tokenization_utils.py&quot;, line 26, in <br />
from .tokenization_utils_base import (<br />
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/tokenization_utils_base.py&quot;, line 74, in <br />
from tokenizers import AddedToken<br />
File &quot;/home/shushan/tokenization_experiments/tokenizers.py&quot;, line 26, in <br />
from transformers import BertTokenizer
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/file_utils.py&quot;, line 1978, in <strong>getattr</strong>
value = getattr(module, name)
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/file_utils.py&quot;, line 1977, in <strong>getattr</strong>
module = self._get_module(self._class_to_module[name])
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/file_utils.py&quot;, line 1986, in _get_module
return importlib.import_module(&quot;.&quot; + module_name, self.<strong>name</strong>)
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/importlib/<strong>init</strong>.py&quot;, line 127, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File &quot;/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py&quot;, line 23, in 
from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace
ImportError: cannot import name 'PreTrainedTokenizer' from partially initialized module 'transformers.tokenization_utils' (most likely due to a circular import) (/home/shushan/.conda/envs/ccg_parser/lib/python3.9/site-packages/transformer
s/tokenization_utils.py)</p>
</blockquote>
<p>I have attempted uninstalling transformers and re-installing them, but I couldn't find any more information as to what is wrong, or how to go about fixing this issue I am seeing. The only suspicious behavior is that the output of <code>transformers-cli env</code> command above specifies the pytorch version does not work with GPU, while in reality, I have an installation of pytorch that works with gpu. Can you help?
Thanks in advance
Shushan</p>
","1056822","","","","","2021-07-24 21:37:38","Cannot import pipeline after successful transformers installation","<python><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68528187","1","","","2021-07-26 10:21:09","","2","103","<p>I have the following problem to load a transformer model. The strange thing is that it work on google colab or even when I tried on another computer, it seems to be version / cache problem but I didn't found it.</p>
<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim
model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')
</code></pre>
<pre><code>---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
&lt;ipython-input-8-0b8b6a3eea75&gt; in &lt;module&gt;
      1 from sentence_transformers import SentenceTransformer
      2 from sentence_transformers.util import cos_sim
----&gt; 3 model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')
      4 

~\AppData\Local\Programs\Python\Python39\lib\site-packages\sentence_transformers\SentenceTransformer.py in __init__(self, model_name_or_path, modules, device, cache_folder)
     88 
     89             if os.path.exists(os.path.join(model_path, 'modules.json')):    #Load as SentenceTransformer model
---&gt; 90                 modules = self._load_sbert_model(model_path)
     91             else:   #Load with AutoModel
     92                 modules = self._load_auto_model(model_path)

~\AppData\Local\Programs\Python\Python39\lib\site-packages\sentence_transformers\SentenceTransformer.py in _load_sbert_model(self, model_path)
    820         for module_config in modules_config:
    821             module_class = import_from_string(module_config['type'])
--&gt; 822             module = module_class.load(os.path.join(model_path, module_config['path']))
    823             modules[module_config['name']] = module
    824 

~\AppData\Local\Programs\Python\Python39\lib\site-packages\sentence_transformers\models\Transformer.py in load(input_path)
    122         with open(sbert_config_path) as fIn:
    123             config = json.load(fIn)
--&gt; 124         return Transformer(model_name_or_path=input_path, **config)
    125 
    126 

~\AppData\Local\Programs\Python\Python39\lib\site-packages\sentence_transformers\models\Transformer.py in __init__(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)
     28         config = AutoConfig.from_pretrained(model_name_or_path, **model_args, cache_dir=cache_dir)
     29         self.auto_model = AutoModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)
---&gt; 30         self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path if tokenizer_name_or_path is not None else model_name_or_path, cache_dir=cache_dir, **tokenizer_args)
     31 
     32         #No max_seq_length set. Try to infer from model

~\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\models\auto\tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    566             tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]
    567             if tokenizer_class_fast and (use_fast or tokenizer_class_py is None):
--&gt; 568                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    569             else:
    570                 if tokenizer_class_py is not None:

~\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1730                 logger.info(f&quot;loading file {file_path} from cache at {resolved_vocab_files[file_id]}&quot;)
   1731 
-&gt; 1732         return cls._from_pretrained(
   1733             resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs
   1734         )

~\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)
   1848         # Instantiate tokenizer.
   1849         try:
-&gt; 1850             tokenizer = cls(*init_inputs, **init_kwargs)
   1851         except OSError:
   1852             raise OSError(

~\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\models\xlm_roberta\tokenization_xlm_roberta_fast.py in __init__(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)
    132         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token
    133 
--&gt; 134         super().__init__(
    135             vocab_file,
    136             tokenizer_file=tokenizer_file,

~\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\tokenization_utils_fast.py in __init__(self, *args, **kwargs)
    105         elif fast_tokenizer_file is not None and not from_slow:
    106             # We have a serialization from tokenizers which let us directly build the backend
--&gt; 107             fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
    108         elif slow_tokenizer is not None:
    109             # We need to convert a slow tokenizer to build the backend

Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 1 column 317584
</code></pre>
<p>To give your more details, i also got another problem only on this computer with another model :</p>
<pre class=""lang-py prettyprint-override""><code>model = SentenceTransformer('etalab-ia/dpr-question_encoder-fr_qa-camembert')
</code></pre>
<pre><code>ValueError: unable to parse C:\Users\david.rouyre/.cache\torch\sentence_transformers\etalab-ia_dpr-question_encoder-fr_qa-camembert\tokenizer_config.json as a URL or as a local path
</code></pre>
<p>So i checked in the cache path and there was not <code>tokenizer_config.json</code>, only <code>tokenizer.json</code> (by renaming the file it worked)</p>
<p>The package : (same version in colab)</p>
<pre><code>Name: sentence-transformers
Version: 2.0.0
Summary: Sentence Embeddings using BERT / RoBERTa / XLM-R
Home-page: https://github.com/UKPLab/sentence-transformers
Author: Nils Reimers
Author-email: info@nils-reimers.de
License: Apache License 2.0
Location: c:\users\david.rouyre\appdata\local\programs\python\python39\lib\site-packages
Requires: transformers, tqdm, torch, torchvision, numpy, scikit-learn, scipy, nltk, sentencepiece, huggingface-hub
Required-by:
</code></pre>
<p>I tried to clear the cache and uninstall with pip all dependencies (transformers, tqdm, torch, torchvision, numpy, scikit-learn, scipy, nltk, sentencepiece, huggingface-hub), uninstall sentence-transformers and reinstalling it.</p>
","15878672","","7802200","","2021-08-04 15:29:49","2021-08-04 15:29:49","Can't load transformers models","<python><huggingface-transformers><huggingface-tokenizers>","0","3","","","","CC BY-SA 4.0"
"68541428","1","","","2021-07-27 08:28:32","","0","56","<p>I am running this example from huggingface website and it works well. I would like to know how can I extract the complete sentence of the answer.</p>
<pre><code>import transformers
import numpy as np
import torch
import json
from transformers import BertForQuestionAnswering,AutoTokenizer, AutoModelForQuestionAnswering
from transformers import BertTokenizerenter code here
path=r&quot;\Huggingface_BertModel\bert-large-uncased-whole-word-masking-finetuned-squad&quot;
model=BertForQuestionAnswering.from_pretrained(path)
tokenizer=BertTokenizer.from_pretrained(path)
content_text=[&quot;setences as list&quot;]
answers=&quot;.&quot;.join(content_text)

tokenizer = AutoTokenizer.from_pretrained(path)
model=BertForQuestionAnswering.from_pretrained(path)
questions=[&quot;what is fatca schema version?&quot;]
for question in questions:
    inputs = tokenizer(question, answers, add_special_tokens=True, return_tensors=&quot;pt&quot;)    
    input_ids = inputs[&quot;input_ids&quot;].tolist()[0] 
    text_tokens = tokenizer.convert_ids_to_tokens(input_ids) 
    outputs = model(**inputs)
    
    answer_start = torch.argmax(outputs.start_logits)  # Get the most likely beginning of answer with the argmax of the score
    answer_end = torch.argmax(outputs.end_logits) + 1  # Get the most likely end of answer with the argmax of the score

    print(answer_start,answer_end)

    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

print(f&quot;Question: {question}&quot;)
print(f&quot;Answer: {answer}&quot;)
</code></pre>
","4343929","","7802200","","2021-08-04 15:29:27","2021-08-04 15:29:27","extract complete sentence using transformers BertForQuestionAnswering and BertTokenizer","<pytorch><huggingface-transformers><bert-language-model><question-answering>","0","1","","","","CC BY-SA 4.0"
"68260614","1","","","2021-07-05 18:22:22","","0","76","<p>I have fine-tuned PEGASUS model for abstractive  summarization using <a href=""https://gist.github.com/jiahao87/50cec29725824da7ff6dd9314b53c4b3"" rel=""nofollow noreferrer"">this script</a> which uses huggingface.
The output model is in pytorch.</p>
<p>Is there a way to transorm it into tensorflow model so I can use it in a javascript backend?</p>
","15517911","","","","","2021-07-08 08:27:14","PEGASUS From pytorch to tensorflow","<tensorflow><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68461204","1","","","2021-07-20 20:52:44","","0","163","<p>I have some custom data I want to use to <em><strong>further pre-train</strong></em> the BERT model. Iâ€™ve tried the two following approaches so far:</p>
<ol>
<li>Starting with a pre-trained BERT checkpoint and continuing the pre-training with Masked Language Modeling (<code>MLM</code>) + Next Sentence Prediction (<code>NSP</code>) heads (e.g. using <em><strong>BertForPreTraining</strong></em> model)</li>
<li>Starting with a pre-trained BERT model with the <code>MLM</code> objective (e.g. using the <em><strong>BertForMaskedLM</strong></em> model assuming we donâ€™t need NSP for the pretraining part.)</li>
</ol>
<p>But Iâ€™m still confused that if using either <em>BertForPreTraining</em> or <em>BertForMaskedLM</em> actually does the continual pre-training on BERT or these are just two models for fine-tuning that use MLM+NSP and MLM for fine-tuning BERT, respectively. Is there even any difference between fine-tuning BERT with MLM+NSP or continually pre-train it using these two heads or this is something we need to test?</p>
<p>I've reviewed similar questions such as <a href=""https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp"">this one</a> but still, I want to make sure that whether technically there's a difference between continual pre-training a model from an initial checkpoint and fine-tuning it using the same objective/head.</p>
","2347063","","","","","2021-07-22 10:13:01","Continual pre-training vs. Fine-tuning a language model with MLM","<deep-learning><nlp><huggingface-transformers><bert-language-model><pre-trained-model>","1","0","","","","CC BY-SA 4.0"
"68489759","1","68511793","","2021-07-22 18:16:27","","1","64","<p>I have a csv data as below.</p>
<pre><code>**token**      **label**
0.45&quot;      length
1-12       size
2.6&quot;       length
8-9-78     size
6mm        length
</code></pre>
<p>Whenever I get the text as below</p>
<pre><code>6mm 8-9-78 silver head
</code></pre>
<p>I should be able to say <code>length = 6mm</code> and <code>size = 8-9-78</code>. I'm new to NLP world, I'm trying to solve this using Huggingface NER. I have gone through various articles. I'm not getting how to train with my own data. Which <code>model/tokeniser</code> should I make use of? Or should I build my own? Any help would be appreciated.</p>
","2422930","","2422930","","2021-07-23 10:07:39","2021-08-27 11:28:22","Huggingface NER with custom data","<python><nlp><huggingface-transformers><ner>","2","0","","","","CC BY-SA 4.0"
"68545934","1","","","2021-07-27 13:37:41","","0","28","<p>My set up looks as follows:</p>
<pre><code>MODEL_CHECKPOINT = &quot;distilroberta-base&quot;
tokenizer = AutoTokenizer.from_pretrained(PATH_TO_MY_MODEL, max_len=512, add_prefix_space=True)
model = AutoModelForTokenClassification.from_pretrained(MODEL_CHEKPOINT, num_labels=32)
ner_pipeline = pipeline(task=&quot;ner&quot;, tokenizer=tokenizer, model=model)
</code></pre>
<p>However I can obtain the NER predictions for an arbitrary length documents. I wonder how is it implemented internally (maybe sliding window approach?)</p>
","1039328","","","","","2021-07-28 10:00:49","Does the Hugginface NER pipeline internally deal with long documents?","<python><machine-learning><nlp><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60297908","1","60315567","","2020-02-19 10:13:53","","2","392","<p>I finetuned two separate bert model (bert-base-uncased) on sentiment analysis and pos tagging tasks. Now, I want to feed the output of the pos tagger (batch, seqlength, hiddensize) as input to the sentiment model.The original bert-base-uncased model is in 'bertModel/' folder which contains 'model.bin' and 'config.json'. Here is my code:</p>

<pre><code>class DeepSequentialModel(nn.Module):
def __init__(self, sentiment_model_file, postag_model_file, device):
    super(DeepSequentialModel, self).__init__()

    self.sentiment_model = SentimentModel().to(device)
    self.sentiment_model.load_state_dict(torch.load(sentiment_model_file, map_location=device))
    self.postag_model = PosTagModel().to(device)
    self.postag_model.load_state_dict(torch.load(postag_model_file, map_location=device))

    self.classificationLayer = nn.Linear(768, 1)

def forward(self, seq, attn_masks):
    postag_context = self.postag_model(seq, attn_masks)
    sent_context = self.sentiment_model(postag_context, attn_masks)
    logits = self.classificationLayer(sent_context)
    return logits

class PosTagModel(nn.Module):
def __init__(self,):
    super(PosTagModel, self).__init__()
    self.bert_layer = BertModel.from_pretrained('bertModel/')
    self.classificationLayer = nn.Linear(768, 43)

def forward(self, seq, attn_masks):
    cont_reps, _ = self.bert_layer(seq, attention_mask=attn_masks)
    return cont_reps

class SentimentModel(nn.Module):
def __init__(self,):
    super(SentimentModel, self).__init__()
    self.bert_layer = BertModel.from_pretrained('bertModel/')
    self.cls_layer = nn.Linear(768, 1)

def forward(self, input, attn_masks):
    cont_reps, _ = self.bert_layer(encoder_hidden_states=input, encoder_attention_mask=attn_masks)
    cls_rep = cont_reps[:, 0]
    return cls_rep
</code></pre>

<p>But I get the below error. I appreciate it if someone could help me. Thanks!</p>

<pre><code>    cont_reps, _ = self.bert_layer(encoder_hidden_states=input, encoder_attention_mask=attn_masks)
    result = self.forward(*input, **kwargs)
    TypeError: forward() got an unexpected keyword argument 'encoder_hidden_states'
</code></pre>
","8175919","","","","","2020-02-20 08:20:53","How to feed the output of a finetuned bert model as inpunt to another finetuned bert model?","<pytorch><pre-trained-model><bert-language-model><huggingface-transformers>","1","3","","","","CC BY-SA 4.0"
"68585678","1","68769749","","2021-07-30 05:07:10","","2","201","<p>I want a summary of a PyTorch model downloaded from huggingface:</p>
<pre><code>from torchinfo import summary
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
summary(model, input_size=(16, 512), dtypes=['torch.IntTensor'])
</code></pre>
<p>(See <a href=""https://stackoverflow.com/a/68577755/1349673"">SO</a> for why the <code>dtypes</code> is needed.)</p>
<p>However, I am getting the error <code>Expected all tensors to be on the same device, ...</code> even though I have not provided any tensors. See the output below.</p>
<p>How can I fix this?</p>
<pre><code>

---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in forward_pass(model, x, batch_dim, cache_forward_pass, device, **kwargs)
    257             if isinstance(x, (list, tuple)):
--&gt; 258                 _ = model.to(device)(*x, **kwargs)
    259             elif isinstance(x, dict):

11 frames

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1530             output_hidden_states=output_hidden_states,
-&gt; 1531             return_dict=return_dict,
   1532         )

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    988             inputs_embeds=inputs_embeds,
--&gt; 989             past_key_values_length=past_key_values_length,
    990         )

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)
    214         if inputs_embeds is None:
--&gt; 215             inputs_embeds = self.word_embeddings(input_ids)
    216         token_type_embeddings = self.token_type_embeddings(token_type_ids)

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py in forward(self, input)
    159             input, self.weight, self.padding_idx, self.max_norm,
--&gt; 160             self.norm_type, self.scale_grad_by_freq, self.sparse)
    161 

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2042         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2043     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2044 

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)


The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-13-d6f4e53beef7&gt; in &lt;module&gt;()
      3 else:
      4     # Can't get this working. See https://stackoverflow.com/questions/68577198/pytorch-summary-fails-with-huggingface-model
----&gt; 5     summary(model, input_size=(16, 512), dtypes=['torch.IntTensor'])
      6     print(model)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in summary(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, row_settings, verbose, **kwargs)
    190     )
    191     summary_list = forward_pass(
--&gt; 192         model, x, batch_dim, cache_forward_pass, device, **kwargs
    193     )
    194     formatting = FormattingOptions(depth, verbose, col_names, col_width, row_settings)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in forward_pass(model, x, batch_dim, cache_forward_pass, device, **kwargs)
    268             &quot;Failed to run torchinfo. See above stack traces for more details. &quot;
    269             f&quot;Executed layers up to: {executed_layers}&quot;
--&gt; 270         ) from e
    271     finally:
    272         if hooks is not None:

RuntimeError: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []

</code></pre>
<p>Output from <code>transformers-cli</code>:</p>
<pre><code>- `transformers` version: 4.9.1
- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.11
- PyTorch version (GPU?): 1.9.0+cu102 (True)
- Tensorflow version (GPU?): 2.5.0 (True)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: &lt;fill in&gt;
- Using distributed or parallel set-up in script?: &lt;fill in&gt;
</code></pre>
","1349673","","1349673","","2021-07-31 07:53:20","2021-08-13 09:17:32","pytorch summary fails with huggingface model II: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu","<python><pytorch><huggingface-transformers>","1","5","","","","CC BY-SA 4.0"
"68596995","1","","","2021-07-30 20:52:48","","1","43","<p>My custom BERT model's architecture:</p>
<pre><code>class BertArticleClassifier(nn.Module):
    def __init__(self, n_classes, freeze_bert_weights=False):
        super(BertArticleClassifier, self).__init__()

        self.bert = AutoModel.from_pretrained('bert-base-uncased')

        if freeze_bert_weights:
            for param in self.bert.parameters():
                param.requires_grad = False

        self.dropout = nn.Dropout(0.1)
        self.fc_1 = nn.Linear(768, 256)
        self.leaky_relu = nn.LeakyReLU()
        self.fc_out = nn.Linear(256, n_classes)

    def forward(self, input_ids, attention_mask):
        output = self.bert(input_ids, attention_mask)
        return self.fc_out(self.leaky_relu(self.fc_1(self.dropout(output['pooler_output']))))
</code></pre>
<p><code>self.bert</code> is a model from transformers library.</p>
<p>Training script:</p>
<pre><code>def train_my_model(model, optimizer, criterion, scheduler, epochs, dataloader_train, dataloader_validation, device, pretrained_weights=None):
    if pretrained_weights:
        torch.save(model.state_dict(), pretrained_weights)

    for epoch in tqdm(range(1, epochs + 1)):
        model.train()
        loss_train_total = 0
        progress_bar = tqdm(dataloader_train, desc=f'Epoch {epoch :1d}', leave=False, disable=False)

        for batch in progress_bar:
            optimizer.zero_grad()

            batch = tuple(batch[b].to(device) for b in batch)
            input_ids, mask, labels = batch

            predictions = model(input_ids, mask)
            loss = criterion(predictions, labels)
            loss.backward()
            loss_train_total += loss.item()

            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            optimizer.step()
            scheduler.step()

            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})

        torch.save(model.state_dict(), f'models_data/bert_my_model/finetuned_BERT_epoch_{epoch}.model')
        tqdm.write(f'\nEpoch {epoch}')
        loss_train_avg = loss_train_total / len(dataloader_train)
        tqdm.write(f'Training loss: {loss_train_avg}')
        val_loss, predictions, true_vals = evaluate(model, dataloader_validation, criterion, device)
        val_f1 = f1_score_func(predictions, true_vals)
        tqdm.write(f'Validation loss: {val_loss}')
        tqdm.write(f'F1 Score (Weighted): {val_f1}')
</code></pre>
<p>Optimizer and Criterion:</p>
<pre><code>optimizer = AdamW(model.parameters(),
                      lr=1e-4,
                      eps=1e-6)

class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)
</code></pre>
<p>After 5 epochs I get the same validation loss ~3.1. I know that my data is preprocessed in the correct way because if I train this transformers <code>BertForSequenceClassification</code> model, the model is learning, but the problem with that approach is that I cannot tweak the loss function to accept the class weights, so that is the reason for creating my own custom model.</p>
<p>As you can see in the model's <code>forward</code> method, I extract the <code>output['pooler_output']</code> piece, and disregard the loss (which is returned alongside the <code>output['pooler_output']</code> element). The problem which I may deduced is that when in the training loop I call <code>loss.backward()</code>, maybe the model's weights aren't updating, because transformers BERT model's return their own loss as an output.</p>
<p>What am I doing wrong?</p>
","14153160","","4621513","","2021-07-30 20:54:37","2021-07-30 20:54:37","Why is BERT model with pytorch native approach not learning?","<python><deep-learning><pytorch><bert-language-model><huggingface-transformers>","0","4","","","","CC BY-SA 4.0"
"68345621","1","","","2021-07-12 10:15:22","","1","26","<p>I am using a BERT like model, which has a limit for input's length.
I am looking to encode a long input and feed into BERT.
Most common solution I know of is sliding-window to add context to input's segments.</p>
<p>For example:</p>
<pre><code>model_max_size = 5
stride = 2

input = [1, ..., 12]

output = [
  [1, 2, 3, 4, 5], -&gt; [1, 2, 3, 4, 5]
  [4, 5, 6, 7, 8], -&gt; [6, 7, 8]
  [7, 8, 9, 10, 11], -&gt; [9, 10, 11]
  [10, 11, 12] -&gt; [12]
]
</code></pre>
<p>Is there a known good strategy?
Do you send each input into consecutive windows and average their outputs?
Any already built in implementation for this?</p>
<p>HuggingFace tokenizer has the stride and return_overflowing_tokens feature but it's not quite it as it works only for the first sliding window.</p>
<p>*I know there are other models accepting longer input (e.g. LongFormer, BigBird etc.) but I need to use this specific one.</p>
<p>Thanks!</p>
","2568505","","","","","2021-07-12 10:15:22","HuggingFace transformers - encoding long input with context","<nlp><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","1","","","","CC BY-SA 4.0"
"68494108","1","68519700","","2021-07-23 04:37:06","","0","98","<p>I'm using this simple script, using the example blog post. However, it fails because of <code>wandb</code>. It was of no use to make <code>wandb</code> OFFLINE as well.</p>
<pre><code>from datasets import load_dataset, load_metric
from transformers import (AutoModelForSequenceClassification, AutoTokenizer,
                          Trainer, TrainingArguments)
import wandb


wandb.init()

tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
dataset = load_dataset('glue', 'mrpc')
metric = load_metric('glue', 'mrpc')

def encode(examples):
    outputs = tokenizer(
        examples['sentence1'], examples['sentence2'], truncation=True)
    return outputs

encoded_dataset = dataset.map(encode, batched=True)

def model_init():
    return AutoModelForSequenceClassification.from_pretrained(
        'distilbert-base-uncased', return_dict=True)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = predictions.argmax(axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Evaluate during training and a bit more often
# than the default to be able to prune bad trials early.
# Disabling tqdm is a matter of preference.
training_args = TrainingArguments(
    &quot;test&quot;, eval_steps=500, disable_tqdm=True,
    evaluation_strategy='steps',)

trainer = Trainer(
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=encoded_dataset[&quot;train&quot;],
    eval_dataset=encoded_dataset[&quot;validation&quot;],
    model_init=model_init,
    compute_metrics=compute_metrics,
)

def my_hp_space(trial):
    return {
        &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 1e-4, 1e-2, log=True),
        &quot;weight_decay&quot;: trial.suggest_float(&quot;weight_decay&quot;, 0.1, 0.3),
        &quot;num_train_epochs&quot;: trial.suggest_int(&quot;num_train_epochs&quot;, 5, 10),
        &quot;seed&quot;: trial.suggest_int(&quot;seed&quot;, 20, 40),
        &quot;per_device_train_batch_size&quot;: trial.suggest_categorical(&quot;per_device_train_batch_size&quot;, [32, 64]),
    }


trainer.hyperparameter_search(
    direction=&quot;maximize&quot;,
    backend=&quot;optuna&quot;,
    n_trials=10,
    hp_space=my_hp_space
)
</code></pre>
<p><code>Trail 0</code> finishes successfully, but next <code>Trail 1</code> crashes with following error:</p>
<pre><code>  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/integrations.py&quot;, line 138, in _objective
    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1376, in train
    self.log(metrics)
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1688, in log
    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/trainer_callback.py&quot;, line 371, in on_log
    return self.call_event(&quot;on_log&quot;, args, state, control, logs=logs)
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/trainer_callback.py&quot;, line 378, in call_event
    result = getattr(callback, event)(
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/transformers/integrations.py&quot;, line 754, in on_log
    self._wandb.log({**logs, &quot;train/global_step&quot;: state.global_step})
  File &quot;/home/user123/anaconda3/envs/iza/lib/python3.8/site-packages/wandb/sdk/lib/preinit.py&quot;, line 38, in preinit_wrapper
    raise wandb.Error(&quot;You must call wandb.init() before {}()&quot;.format(name))
wandb.errors.Error: You must call wandb.init() before wandb.log()
</code></pre>
<p>Any help is highly appreciated.</p>
","1349516","","","","","2021-07-25 14:48:22","Hyperparam search on huggingface with optuna fails with wandb error","<python><huggingface-transformers><hyperparameters><optuna>","1","8","","","","CC BY-SA 4.0"
"68523070","1","","","2021-07-25 22:56:21","","0","69","<p>Transformers also provide their own schedulers for learning rates like <code>get_constant_schedule</code>, <code>get_constant_schedule_with_warmup</code>, etc. They are again returning <code>torch.optim.lr_scheduler.LambdaLR</code> (torch scheduler). Is the <code>warmup_steps</code> the only difference between the two?</p>
<p>How can we create a custom transformer-based scheduler similar to other torch schedulers like <code>lr_scheduler.MultiplicativeLR</code>, <code>lr_scheduler.StepLR</code>, <code>lr_scheduler.ExponentialLR</code>?</p>
","3306097","","","","","2021-07-26 18:55:32","Difference between transformers schedulers and Pytorch schedulers","<python><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68536546","1","68550539","","2021-07-26 21:32:04","","1","37","<p>I am trying to use a simple <code>pipeline</code> offline. I am only allowed to download files directly from the web.</p>
<p>I went to <a href=""https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/tree/main</a> and downloaded all the files in a local folder <code>C:\\Users\\me\\mymodel</code></p>
<p>However, when I tried to load the model I get a strange error</p>
<pre><code>from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model= &quot;C:\\Users\\me\\mymodel&quot;,
                      tokenizer = &quot;C:\\Users\\me\\mymodel&quot;)

ValueError: unable to parse C:\Users\me\mymodel\modelcard.json as a URL or as a local path
</code></pre>
<p>What is the issue here?
Thanks!</p>
","1609428","","","","","2021-07-27 19:09:06","using pipelines with a local model","<python><tensorflow2.0><huggingface-transformers>","2","1","","","","CC BY-SA 4.0"
"68614278","1","68671819","","2021-08-01 20:52:48","","2","77","<p>I am trying to save some disk space to use the CommonVoice French dataset (19G) on Google Colab as my Notebook always crashes out of disk space. I saw that from the <a href=""https://huggingface.co/docs/datasets/dataset_streaming.html"" rel=""nofollow noreferrer"">HuggingFace</a> documentation that we can load a dataset in a streaming mode so we can <code>iterate over it directly without having to download the entire dataset.</code>. I tried to use that mode in Google Colab, but can't make it work - and I haven't found anything on SO about this issue.</p>
<pre class=""lang-py prettyprint-override""><code>!pip install datasets
!pip install 'datasets[streaming]'
!pip install aiohttp

common_voice_train = load_dataset(&quot;common_voice&quot;, &quot;fr&quot;, split=&quot;train&quot;, streaming=True)
</code></pre>
<p>Then, I get the following error:</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-24-489f8a0ca4e4&gt; in &lt;module&gt;()
----&gt; 1 common_voice_train = load_dataset(&quot;common_voice&quot;, &quot;fr&quot;, split=&quot;train&quot;, streaming=True)

/usr/local/lib/python3.7/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)
    811         if not config.AIOHTTP_AVAILABLE:
    812             raise ImportError(
--&gt; 813                 f&quot;To be able to use dataset streaming, you need to install dependencies like aiohttp &quot;
    814                 f'using &quot;pip install \'datasets[streaming]\'&quot; or &quot;pip install aiohttp&quot; for instance'
    815             )

ImportError: To be able to use dataset streaming, you need to install dependencies like aiohttp using &quot;pip install 'datasets[streaming]'&quot; or &quot;pip install aiohttp&quot; for instance

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
&quot;Open Examples&quot; button below.
---------------------------------------------------------------------------
</code></pre>
<p>Is there a reason why Google Colab wouldn't allow streaming to load a dataset?</p>
<p>Otherwise, what am I missing?</p>
","3339641","","","","","2021-08-05 18:29:30","How to load a dataset in streaming mode on Google Colab?","<python><google-colaboratory><huggingface-transformers><huggingface-datasets>","1","3","","","","CC BY-SA 4.0"
"68604289","1","68656887","","2021-07-31 17:14:49","","1","133","<p>I cloned this repository/documentation <a href=""https://huggingface.co/EleutherAI/gpt-neo-125M"" rel=""nofollow noreferrer"">https://huggingface.co/EleutherAI/gpt-neo-125M</a></p>
<p>I get the below error whether I run it on google collab or locally. I also installed transformers using this</p>
<pre><code>pip install git+https://github.com/huggingface/transformers
</code></pre>
<p>and made sure the configuration file is named as config.json</p>
<pre><code>      5 tokenizer = AutoTokenizer.from_pretrained(&quot;gpt-neo-125M/&quot;,from_tf=True)
----&gt; 6 model = AutoModelForCausalLM.from_pretrained(&quot;gpt-neo-125M&quot;,from_tf=True)
      7 
      8 

3 frames
/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py in __getattr__(self, name)

AttributeError: module transformers has no attribute TFGPTNeoForCausalLM

</code></pre>
<p>Full code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM 

tokenizer = AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;,from_tf=True)

model = AutoModelForCausalLM.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;,from_tf=True)

</code></pre>
<p>transformers-cli env results:</p>
<ul>
<li><code>transformers</code> version: 4.10.0.dev0</li>
<li>Platform: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.29</li>
<li>Python version: 3.8.5</li>
<li>PyTorch version (GPU?): 1.9.0+cpu (False)</li>
<li>Tensorflow version (GPU?): 2.5.0 (False)</li>
<li>Flax version (CPU?/GPU?/TPU?): not installed (NA)</li>
<li>Jax version: not installed</li>
<li>JaxLib version: not installed</li>
<li>Using GPU in script?: </li>
<li>Using distributed or parallel set-up in script?: </li>
</ul>
<p>Both collab and locally have TensorFlow 2.5.0 version</p>
","10605020","","10605020","","2021-08-01 09:27:54","2021-08-04 19:14:11","AttributeError: module transformers has no attribute TFGPTNeoForCausalLM","<python><pytorch><huggingface-transformers><gpt><gpt-3>","2","0","","","","CC BY-SA 4.0"
"68625465","1","","","2021-08-02 17:02:42","","0","169","<p>I am trying to use the HuggingFace library to fine-tune the T5 transformer model using a custom dataset. HF provide an example of <a href=""https://huggingface.co/transformers/master/custom_datasets.html"" rel=""nofollow noreferrer"">fine-tuning with custom data</a> but this is for distilbert model, not the T5 model I want to use. From their example it says I need to implement <strong>len</strong> and <strong>getitem</strong> methods in my dataset subclass, but there doesn't seem to be much documentation about what to change when using t5 instead of distilbert. Here is the tokenizer code followed by my attempt at changing <strong>getitem</strong></p>
<p><a href=""https://i.stack.imgur.com/sqiNI.png"" rel=""nofollow noreferrer""><strong>getitem</strong> method code</a></p>
<p>and the resulting error from trainer.train() which says &quot; KeyError: 'labels' &quot;</p>
<p><a href=""https://i.stack.imgur.com/TBMid.png"" rel=""nofollow noreferrer"">trainer.train() error message</a></p>
<p>I have seen the following <a href=""https://stackoverflow.com/questions/67691530/key-error-while-fine-tunning-t5-for-summarization-with-huggingface"">discussion</a> which seems to relate to this problem, but the answer offered still produces an error in trainer.train() which I can also post if useful.</p>
<p>Using the original example code from the &quot;fine-tuning with custom data&quot; then the dataset class is:</p>
<p><a href=""https://i.stack.imgur.com/t9ibj.png"" rel=""nofollow noreferrer"">original code from hf distilbert example applied to T5</a></p>
<p>but then the error with the trainer changes:</p>
<p><a href=""https://i.stack.imgur.com/PrGLK.png"" rel=""nofollow noreferrer"">trainer error using hf distilbert example applied to T5</a></p>
<p>which is what originally got me looking around for solutions. So using &quot;fine-tuning with custom data&quot; doesn't seem to be as simple as changing the model and the tokenizer (and the input/output data you are training on) when switching from say distilbert to a text to text model like T5. distilbert doesn't have any output text to train on, so I would have thought (but what do I know?) it would be different to T5 but I can't find documentation on how? At the bottom of this <a href=""https://stackoverflow.com/questions/67691530/key-error-while-fine-tunning-t5-for-summarization-with-huggingface"">question</a> seems to point to a direction to follow but once again I don't know (much!)</p>
<p>I think I may have solved the problem (at least the trainer runs and completes). The distilbert model doesn't have output text, it has flags that are provided to the dataset class as a list of integers. The T5 model has output text, so you assign the output encodings and rely upon DataCollatorForSeq2Seq() to prepare the data/featurs that the T5 model expects. See changes (for T5) with commented out HF code (for distilbert) below:</p>
<p><a href=""https://i.stack.imgur.com/JSl2f.png"" rel=""nofollow noreferrer"">Changes for T5 - commented out distilbert code</a></p>
<p>Raised an issue to HuggingFace and they advised that the fine-tuning with custom datasets example on their website was out of date and that I needed to work off their maintained <a href=""https://github.com/huggingface/transformers/tree/master/examples"" rel=""nofollow noreferrer"">examples</a>.</p>
","10451062","","10451062","","2021-08-05 16:58:06","2021-08-05 16:58:06","HuggingFace T5 transformer model - how to prep a custom dataset for fine-tuning?","<python><nlp><huggingface-transformers><huggingface-tokenizers>","0","3","","","","CC BY-SA 4.0"
"68616730","1","","","2021-08-02 05:27:29","","0","77","<p>I am using Hugging-face pre-trained <code>LongformerModel</code> model. I am using to extract embedding for sentence. I want to change the <code>token length</code>, <code>max sentence length</code> parameter but I am not able to do so. Here is the code.</p>
<pre><code>model = LongformerModel.from_pretrained('allenai/longformer-base-4096',output_hidden_states = True)
tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')

model.eval()

text=[&quot; I like to play cricket&quot;]

input_ids = torch.tensor(tokenizer.encode(text,max_length=20,padding=True,add_special_tokens=True)).unsqueeze(0)

print(tokenizer.encode(text,max_length=20,padding=True,add_special_tokens=True))

# [0, 38, 101, 7, 310, 5630, 2]
</code></pre>
<p><strong>I expected encoder to give me list of size 20 with padding as I have passed a parameter <code>max_length=20.</code> But it returned list of size 7 only?</strong></p>
<pre><code>attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)
attention_mask[:, [0,-1]] = 2
outputs = model(input_ids, attention_mask=attention_mask, return_dict=True)

hidden_states = outputs[2]

print (&quot;Number of layers:&quot;, len(hidden_states), &quot;  (initial embeddings + 12 BERT layers)&quot;)
        layer_i = 0

print (&quot;Number of batches:&quot;, len(hidden_states[layer_i]))
        batch_i = 0

print (&quot;Number of tokens:&quot;, len(hidden_states[layer_i][batch_i]))
        token_i = 0

print (&quot;Number of hidden units:&quot;, len(hidden_states[layer_i][batch_i][token_i]))
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Number of layers: 13   (initial embeddings + 12 BERT layers)
Number of batches: 1
Number of tokens: 512 # How can I change this parameter to pick up my sentence length during run-time
Number of hidden units: 768
</code></pre>
<p><strong>How can I reduce number of tokens to sentence length instead of 512 ? Every-time I input a new sentence, it should pick up that length.</strong></p>
","9907733","","9907733","","2021-08-02 06:44:34","2021-08-24 22:28:08","How to change parameters of pre-trained longformer model from huggingface","<python-3.x><deep-learning><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"68617212","1","","","2021-08-02 06:26:41","","1","23","<p>Is there any ways to pass two evaluation datasets to a HuggingFace Trainer object so that the trained model can be evaluated on two different sets (say in-distribution and out-of-distribution sets) during training? Here is the instantiation of the object, which accepts just one <code>eval_dataset</code>:</p>
<pre><code>trainer = Trainer(
    model,
    args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer
)
</code></pre>
","5617507","","","","","2021-08-31 12:55:37","Passing two evaluation datasets to HuggingFace Trainer objects","<huggingface-transformers>","2","0","1","","","CC BY-SA 4.0"
"60345277","1","60390257","","2020-02-21 19:42:58","","0","662","<p>I'm having trouble migrating my code from <code>pytorch_pretrained_bert</code> to <code>pytorch_transformers</code>. I'm attempting to run a cosine similarity exercise. I want to extract text embeddings values of the second-to-last of the 12 hidden embedding layer. </p>

<pre><code>
import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel
#from pytorch_transofmers import BertTokenizer, BertModel
import pandas as pd
import numpy as np

model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# This is done by default in the pytorch_transformers
model.eval() 

input_query = ""This is my test input query text""
marked_text = ""[CLS] "" + input_query + "" [SEP]""
tokenized_text = tokenizer.tokenize(marked_text)
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
segments_ids = [1] * len(tokenized_text)
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
with torch.no_grad():
    encoded_layers, _ = model(tokens_tensor, segments_tensors)
    sentence_embedding = torch.mean(encoded_layers[10], 1)

</code></pre>

<p>Using the pytorch_pretrained_bert works perfectly fine with the above code. My <code>encoded_layers</code> object is a list of 12 hidden layer tensors, allowing me to pick and reduce the 11th layer by taking an average, resulting in <code>sentence_embedding</code> object I can run cosine similarities against.</p>

<p>However, when I migrate my code to the <code>pytorch_transformers</code> library, the resulting <code>encoded_layers</code> object is no longer the full list of 12 hidden layers, but a single torch tensor object of shape <code>torch.Size([1, 7, 768])</code>, which results in the following error when I attempt to create the <code>sentence_embedding</code> object:</p>

<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-23-7f877a7d2f9c&gt; in &lt;module&gt;
      9         encoded_layers, _ = model(tokens_tensor, segments_tensors)
     10         test = encoded_layers[0]
---&gt; 11         sentence_embedding = torch.mean(test[10], 1)
     12 

IndexError: index 10 is out of bounds for dimension 0 with size 7
</code></pre>

<p>The migration documentation (<a href=""https://huggingface.co/transformers/migration.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/migration.html</a>) states that I should take the first element of the <code>encoded_layers</code> object as a replacement but that does not provide me with access to the second to last hidden layer of embeddings. </p>

<p>How can I access it?</p>

<p>Thank you!</p>
","5660632","","3607203","","2020-02-22 08:11:24","2020-02-25 08:22:34","Migrating from `pytorch-pretrained-bert` to `pytorch-transformers` issue regarding model() output","<python><cosine-similarity><pre-trained-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"68521791","1","","","2021-07-25 19:12:36","","0","49","<p>Please consider the minimal example below:</p>
<pre><code>import pandas as pd
import numpy as np

data = pd.DataFrame({'text' : ['this is good!', 'amazing stuff',
                                'not good at all', 'not very nice'],
                     'label': [1,1,0,0]})
    
data
Out[3]: 
              text  label
0    this is good!      1
1    amazing stuff      1
2  not good at all      0
3    not very nice      0
</code></pre>
<p>As you can see, I have a small dataset containing text and label (positive vs. negative).</p>
<p>I need some help to train a classifying using <code>huggingface</code> and <code>bert/distilbert</code>. What would be the minimal code to do so? I was not able to find the solution by looking at the official documentation (I am looking for a <code>tensorflow</code> solution).</p>
<p>Thanks!</p>
","1609428","","","","","2021-07-25 19:12:36","classifying text in a pandas dataframe with huggingface and tensorflow","<python><pandas><tensorflow2.0><text-classification><huggingface-transformers>","0","0","1","","","CC BY-SA 4.0"
"68648702","1","","","2021-08-04 09:22:11","","0","15","<p>I am trying to fine-tune some pre-trained models that I have on the SUPERGLUE benchmark. For most the tasks which contain (question, answer) pairs, it is pretty straight forward: just concatenate the question answer pairs with a [SEP] token and tokenize.</p>
<p>I am struggling with the other tasks that do not follow that format; for example: COPA task has (question, choice1, choice2, question_type). I have read in the ROBERTA paper that the authors concatenate the s1 = question + &quot;because&quot; + choice1, s2= question + &quot;because&quot; + choice2 and choose the one that yields the highest scalar value. I am not entirely sure how to do this with huggingface transformers and the tokenizers that are available. Does anyone have any idea how to make this work?</p>
<blockquote>
<p>For classification tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the
sentences with a [SEP] token, feed the fused input to BERT, and use a logistic regression classifier
that sees the representation corresponding to [CLS]. For WiC, we also concatenate the representation
of the marked word. For COPA, MultiRC, and ReCoRD, for each answer choice, we similarly
concatenate the context with that answer choice and feed the resulting sequence into BERT to produce
an answer representation. For COPA, we project these representations into a scalar, and take as the
answer the choice with the highest associated scalar. For MultiRC, because each question can have
more than one correct answer, we feed each answer representation into a logistic regression classifier.
For ReCoRD, we also evaluate the probability of each candidate independent of other candidates,
and take the most likely candidate as the modelâ€™s prediction. For WSC, which is a span-based task,
we use a model inspired by Tenney et al. (2019). Given the BERT representation for each word in the
original sentence, we get span representations of the pronoun and noun phrase via a self-attention
span-pooling operator (Lee et al., 2017), before feeding it into a logistic regression classifier</p>
</blockquote>
","9443671","","","","","2021-08-04 09:22:11","How to Fine-Tune Transformers on the SUPERGLUE benchmark (COPA/ReCoRD/ etc..)?","<machine-learning><nlp><pytorch><dataset><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68698065","1","","","2021-08-08 05:38:15","","3","99","<p>I'm trying to execute the named entity recognition example using BERT and pytorch following the Hugging Face page: <a href=""https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities"" rel=""nofollow noreferrer"">Token Classification with W-NUT Emerging Entities</a>.</p>
<p>There was <a href=""http://Huggingface%20BERT%20NER%20Example%20Batch_Size%20error"" rel=""nofollow noreferrer"">a related question</a> on stackoverflow, but the error message is different from my case.</p>
<p><code>cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:29</code></p>
<p><strong>I have trouble with fixing the above cuda runtime error.</strong></p>
<p>How can I execute the sample code on google colab with <strong>the run time type, GPU</strong>?</p>
<h2>Error</h2>
<pre><code>trainer.train()

# Error Message
/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    147     Variable._execution_engine.run_backward(
    148         tensors, grad_tensors_, retain_graph, create_graph, inputs,
--&gt; 149         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
    150 
    151 

RuntimeError: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:29
</code></pre>
<h2>Code</h2>
<p>I didn't change the original data and code introduced on the tutorial, <a href=""https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities"" rel=""nofollow noreferrer"">Token Classification with W-NUT Emerging Entities</a>.</p>
<p>Access from the browser to Token Classification with W-NUT Emerging Entities code:
<a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/custom_datasets.ipynb"" rel=""nofollow noreferrer"">custom_datasets.ipynb - Colaboratory</a></p>
<pre><code>from pathlib import Path
import re

def read_wnut(file_path):
    file_path = Path(file_path)

    raw_text = file_path.read_text().strip()
    raw_docs = re.split(r'\n\t?\n', raw_text)
    token_docs = []
    tag_docs = []
    for doc in raw_docs:
        tokens = []
        tags = []
        for line in doc.split('\n'):
            token, tag = line.split('\t')
            tokens.append(token)
            tags.append(tag)
        token_docs.append(tokens)
        tag_docs.append(tags)

    return token_docs, tag_docs

texts, tags = read_wnut('wnut17train.conll')
</code></pre>
<pre><code>from sklearn.model_selection import train_test_split
train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)
unique_tags = set(tag for doc in tags for tag in doc)
tag2id = {tag: id for id, tag in enumerate(unique_tags)}
id2tag = {id: tag for tag, id in tag2id.items()}
</code></pre>
<pre><code>from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')
train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)
val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)
</code></pre>
<pre><code>import numpy as np

def encode_tags(tags, encodings):
    labels = [[tag2id[tag] for tag in doc] for doc in tags]
    encoded_labels = []
    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):
        # create an empty array of -100
        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100
        arr_offset = np.array(doc_offset)

        # set labels whose first offset position is 0 and the second is not 0
        doc_enc_labels[(arr_offset[:,0] == 0) &amp; (arr_offset[:,1] != 0)] = doc_labels
        encoded_labels.append(doc_enc_labels.tolist())

    return encoded_labels

train_labels = encode_tags(train_tags, train_encodings)
val_labels = encode_tags(val_tags, val_encodings)
</code></pre>
<pre><code>import torch
import os

#os.environ['CUDA_LAUNCH_BLOCKING'] = &quot;1&quot;
torch.backends.cudnn.enabled = False
# check if CUDA is available
train_on_gpu = torch.cuda.is_available()
# torch.backends.cudnn.enabled

if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')

class WNUTDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_encodings.pop(&quot;offset_mapping&quot;) # we don't want to pass this to the model
val_encodings.pop(&quot;offset_mapping&quot;)
train_dataset = WNUTDataset(train_encodings, train_labels)
val_dataset = WNUTDataset(val_encodings, val_labels)
</code></pre>
<pre><code>from transformers import DistilBertForTokenClassification
model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))
</code></pre>
<pre><code>from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, DistilBertForTokenClassification
from sklearn.metrics import precision_recall_fscore_support
import tensorflow as tf

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

model = DistilBertForTokenClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)

trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,             # evaluation dataset
    compute_metrics=compute_metrics
)

trainer.train()
</code></pre>
<h2>What I did</h2>
<p>I checked cuda and GPU related settings.</p>
<pre><code>#os.environ['CUDA_LAUNCH_BLOCKING'] = &quot;1&quot;
torch.backends.cudnn.enabled = False
# check if CUDA is available
train_on_gpu = torch.cuda.is_available()
# torch.backends.cudnn.enabled

if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')

#output
CUDA is available!  Training on GPU ...

training_args.device

#output
device(type='cuda', index=0)
</code></pre>
<h2>Responce to an answer</h2>
<p>When I comment out the part,</p>
<pre><code>#os.environ['CUDA_LAUNCH_BLOCKING'] = &quot;1&quot;
#torch.backends.cudnn.enabled = False
</code></pre>
<p>The error message changed to the below when I didn't reset runtime.</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py in _make_grads(outputs, grads)
     49                 if out.numel() != 1:
     50                     raise RuntimeError(&quot;grad can be implicitly created only for scalar outputs&quot;)
---&gt; 51                 new_grads.append(torch.ones_like(out, memory_format=torch.preserve_format))
     52             else:
     53                 new_grads.append(None)

RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
</code></pre>
<p>If I reset runtime, the message was the same.</p>
<pre><code>RuntimeError: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:29
</code></pre>
","14497686","","14497686","","2021-08-08 07:16:13","2021-08-08 07:16:13","How can I fix cuda runtime error on google colab?","<python><pytorch><gpu><google-colaboratory><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"68703608","1","","","2021-08-08 18:23:10","","1","43","<p>Iâ€™m trying to fine-tune a model to perform text summarization. Iâ€™m using <code>AutoModelForSeq2SeqLM.from_pretrained()</code>, so the following applies to several models (e.g. T5, ProphetNet, BART).</p>
<p>Iâ€™ve created a class called <code>CustomDataset</code>, which is a subclass of <code>torch.utils.Dataset</code>. That class contains one field: <code>samples</code> - a list of dictionaries that have <code>encodings</code> and <code>labels</code> keys. Each of the values in each of those dictionaries is a <code>torch.Tensor</code>. Hereâ€™s what an entry in <code>samples</code> looks like:</p>
<p><code>{'encoding': tensor([[21603, 10, 188, 563, 1]]), 'label': tensor([[ 1919, 22003, 22, 7, 1]])}</code></p>
<p>Hereâ€™s how Iâ€™m attempting to fine-tune the model using Trainer:</p>
<pre><code>model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

training_args = TrainingArguments(&quot;test_trainer&quot;)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=data,
)
trainer.train()
</code></pre>
<p>The error Iâ€™m getting gets thrown on line 63 in <code>transformers\data\data_collator.py</code>. Hereâ€™s that line of code:</p>
<p><code>label = first[&quot;label&quot;].item() if isinstance(first[&quot;label&quot;], torch.Tensor) else first[&quot;label&quot;]</code></p>
<p>Hereâ€™s the error message:
<code>ValueError: only one element tensors can be converted to Python scalars</code></p>
<p>I understand why the error message specifically is being thrown - the <code>first[&quot;label&quot;]</code> tensor isnâ€™t a one-element tensor, and hence <code>item()</code> canâ€™t be called on it. Thatâ€™s not why Iâ€™m asking this question, though.</p>
<p>Iâ€™m assuming that Iâ€™m not passing the data correctly, but it seems to me that <code>Trainer</code> should take care of <code>input_ids</code> and <code>decoder_input_ids</code> on its own. Iâ€™ve tried to set those manually (passing the <code>encodings</code> as <code>input_ids</code> and the <code>labels</code> as <code>decoder_input_ids</code>) and the model can successfully perform inference, but I havenâ€™t managed to fine-tune it. Where am I making a mistake and how do I fix it?</p>
","7305715","","7305715","","2021-08-08 21:26:21","2021-08-09 20:33:25","HuggingFace text summarization input data format issue","<python><pytorch><artificial-intelligence><huggingface-transformers><summarization>","1","0","","","","CC BY-SA 4.0"
"68557028","1","68563703","","2021-07-28 08:35:11","","0","56","<p>I am training a model using HuggingFace Trainer class. The following code does a decent job:</p>
<pre><code>!pip install datasets
!pip install transformers

from datasets import load_dataset
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer

dataset = load_dataset('glue', 'mnli')
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)

def preprocess_function(examples):
  return tokenizer(examples[&quot;premise&quot;], examples[&quot;hypothesis&quot;], truncation=True, padding=True)
encoded_dataset = dataset.map(preprocess_function, batched=True)

args = TrainingArguments(
    &quot;test-glue&quot;,
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    remove_unused_columns=True
  )

trainer = Trainer(
    model,
    args,
    train_dataset=encoded_dataset[&quot;train&quot;],
    tokenizer=tokenizer
)
trainer.train()
</code></pre>
<p>However, setting <code>remove_unused_columns=False</code> results in the following error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py in convert_to_tensors(self, tensor_type, prepend_batch_axis)
    704                 if not is_tensor(value):
--&gt; 705                     tensor = as_tensor(value)
    706 

ValueError: too many dimensions 'str'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
8 frames
/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py in convert_to_tensors(self, tensor_type, prepend_batch_axis)
    720                     )
    721                 raise ValueError(
--&gt; 722                     &quot;Unable to create tensor, you should probably activate truncation and/or padding &quot;
    723                     &quot;with 'padding=True' 'truncation=True' to have batched tensors with the same length.&quot;
    724                 )

ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.
</code></pre>
<p>Any suggestions are highly appreciated.</p>
","5617507","","","","","2021-07-28 15:58:36","Setting `remove_unused_columns=False` causes error in HuggingFace Trainer class","<pytorch><huggingface-transformers><huggingface-tokenizers><huggingface-datasets>","1","2","","","","CC BY-SA 4.0"
"68633563","1","","","2021-08-03 09:30:40","","1","18","<p>Let's say all layers in bert are frozen except the last one, and I have different bert models like this one trained for different purposes - thereby, having different weights for the last unfrozen layer, whilst having the same other layers. Now, passing inputs to all models will have redundancy as all layers except the last one are frozen, so their outputs will be the same for all. So, is there a way I can skip passing inputs for each model and instead get the outputs of the 11th layer(12total-1unfrozen) from 1 model and then pass those values as inputs to the 12th layer in the other models? If this is not possible, is there a way to break down bert layers so as to achieve this task?</p>
","8517728","","","","","2021-08-03 09:30:40","BERT - Pass input to an intermediate layer","<machine-learning><nlp><huggingface-transformers><bert-language-model>","0","1","","","","CC BY-SA 4.0"
"62276011","1","","","2020-06-09 06:02:14","","2","140","<p>I trained a RoBERTa model following this colab - <a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=XaFAsB_fnU3K"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=XaFAsB_fnU3K</a></p>

<p>Here is how my data looked: </p>

<pre><code>Merkel bemoans lack of rain as Germany fears for its forests .\n
Germanyâ€™s forests, covering a third of its territory and as much a part of its cultural landscape as its physical one, are in danger.\n
An aerial view shows a forest near Gummersbach, Germany, April 24, 2020, following an unusually warm, dry winter after a summer of record temperatures leaving forests dried out.\n
Picture taken with a drone.\n
The last two exceptionally hot and dry summers have weakened millions of trees, undermining their defences against the bark beetle, which can be fatal to ancient woodlands.\n
And after an exceptionally dry April, with summer still two months away, a forest fire has already had to be put out near the town of Gummersbach in western Germany this week.\n
â€œWeâ€™re already noticing these days that itâ€™s not raining enough in many areas. 
</code></pre>

<p>After training the model I used <code>pipeline</code> from the transforms library for the fill_mask task</p>

<pre><code>from transformers import pipeline

fill_mask = pipeline(
    ""fill-mask"",
    model=""./output"",
    tokenizer=""./output""
fill_mask(""Merkel bemoans lack of rain as &lt;mask&gt; fears for its forests"")
)
</code></pre>

<p>These are the results: </p>

<pre><code>[{'sequence': '&lt;s&gt; Merkel bemoans lack of rain as. fears for its forests&lt;/s&gt;',
  'score': 0.040456026792526245,
  'token': 18},
 {'sequence': '&lt;s&gt; Merkel bemoans lack of rain as, fears for its forests&lt;/s&gt;',
  'score': 0.03502459451556206,
  'token': 16},
 {'sequence': '&lt;s&gt; Merkel bemoans lack of rain as the fears for its forests&lt;/s&gt;',
  'score': 0.03497963398694992,
  'token': 269},
 {'sequence': '&lt;s&gt; Merkel bemoans lack of rain as\n fears for its forests&lt;/s&gt;',
  'score': 0.03180328756570816,
  'token': 203},
 {'sequence': '&lt;s&gt; Merkel bemoans lack of rain as to fears for its forests&lt;/s&gt;',
  'score': 0.020796578377485275,
  'token': 288}]
</code></pre>

<p>As you can see there is no meaningful word(s) returned only punctuations and one other word (to) which doesn't make sense. What am i doing wrong here? Do I have to remove all punctuations?</p>
","8165980","","","","","2020-06-13 06:32:35","Training RoBerta using transformers on masked language task giving weird results?","<pytorch><transformer><huggingface-transformers><bert-language-model>","0","1","","","","CC BY-SA 4.0"
"68524992","1","","","2021-07-26 05:43:22","","0","32","<p>I have a question about training BERT classification(or pretrained model).</p>
<p>BERT classifier model usually constructed 2 models. BERT model and classifier.</p>
<p>Many BERT fine tuning example code is training BERT model and classifier layer at once.
But I think, classifier is training first and BERT weight should not updated. After classifier trained, training all model layers.</p>
<p>Example</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertForSequenceClassification
model = BertForSequenceClassification()
...

# training1
for name, param in model.named_parameters():
    if 'classifier' in name:
        param.requires_grad = True # only classifier update
    else:
        param.requires_grad = False # tied other layer

...
# And after training1, we can using BERT model that is trained only classfier.
model = BertForSequenceClassification()
model.load_state_dict(torch.load({model only trained classifier})
for name, param in model.named_parameters():
    param.requires_grad = True # training all 

# training BERT Classification model
</code></pre>
<p>Why BERT Classification model training at once?
Thank you.</p>
","12855826","","","","","2021-07-26 05:43:22","BERT finetuning : Is it right to train BERT Classification model at once?","<machine-learning><nlp><huggingface-transformers><bert-language-model>","0","3","","","","CC BY-SA 4.0"
"68577198","1","68577755","","2021-07-29 13:51:38","","1","135","<p>I want a summary of a <code>PyTorch</code> model downloaded from huggingface.</p>
<p>Am I doing something wrong here?</p>
<pre><code>from torchinfo import summary
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
summary(model, input_size=(16, 512))
</code></pre>
<p>Gives the error:</p>
<pre><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in forward_pass(model, x, batch_dim, cache_forward_pass, device, **kwargs)
    257             if isinstance(x, (list, tuple)):
--&gt; 258                 _ = model.to(device)(*x, **kwargs)
    259             elif isinstance(x, dict):

11 frames

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1530             output_hidden_states=output_hidden_states,
-&gt; 1531             return_dict=return_dict,
   1532         )

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    988             inputs_embeds=inputs_embeds,
--&gt; 989             past_key_values_length=past_key_values_length,
    990         )

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)
    214         if inputs_embeds is None:
--&gt; 215             inputs_embeds = self.word_embeddings(input_ids)
    216         token_type_embeddings = self.token_type_embeddings(token_type_ids)

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-&gt; 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py in forward(self, input)
    159             input, self.weight, self.padding_idx, self.max_norm,
--&gt; 160             self.norm_type, self.scale_grad_by_freq, self.sparse)
    161 

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2042         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2043     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2044 

RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)


The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-8-4f70d4e6fa82&gt; in &lt;module&gt;()
      5 else:
      6     # Can't get this working
----&gt; 7     summary(model, input_size=(16, 512)) #, device='cpu')
      8     #print(model)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in summary(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, row_settings, verbose, **kwargs)
    190     )
    191     summary_list = forward_pass(
--&gt; 192         model, x, batch_dim, cache_forward_pass, device, **kwargs
    193     )
    194     formatting = FormattingOptions(depth, verbose, col_names, col_width, row_settings)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in forward_pass(model, x, batch_dim, cache_forward_pass, device, **kwargs)
    268             &quot;Failed to run torchinfo. See above stack traces for more details. &quot;
    269             f&quot;Executed layers up to: {executed_layers}&quot;
--&gt; 270         ) from e
    271     finally:
    272         if hooks is not None:

RuntimeError: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []
</code></pre>
","1349673","","","","","2021-07-29 16:49:01","pytorch summary fails with huggingface model","<python><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68668874","1","","","2021-08-05 14:50:13","","-1","40","<p>I have deeppavlov fine-tuned model. Is there a way to convert to a model that transformers can work with (<a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers</a>)?</p>
","16601464","","","","","2021-08-10 16:41:35","Deeppavlov tuned model to hugging face model","<pytorch><huggingface-transformers><deeppavlov>","1","0","","","","CC BY-SA 4.0"
"68687968","1","","","2021-08-06 22:14:50","","1","31","<p>This <a href=""https://github.com/allenai/longformer/issues/192"" rel=""nofollow noreferrer"">issue</a> discusses about the difference between HuggingFace LED and AllenAI LED. What is the correct way of loading AllenAI's pretrained model <code>led-base-16384</code>?</p>
<p><code>Approach 1 using HuggingFace LED:</code>
Using <code>transformers v4.9.1</code></p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  
tokenizer = AutoTokenizer.from_pretrained(&quot;allenai/led-base-16384&quot;)

model = AutoModelForSeq2SeqLM.from_pretrained(&quot;allenai/led-base-16384&quot;, gradient_checkpointing=True)
</code></pre>
<p><code>Approach 2 using AllenAI LED:</code>
Using transformer version suggested in <code>allenai/longformer/requirements.txt</code>:
<code>git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers</code></p>
<pre><code>from transformers import AutoTokenizer
from longformer.longformer_encoder_decoder import LongformerEncoderDecoderForConditionalGeneration
tokenizer = AutoTokenizer.from_pretrained(&quot;allenai/led-base-16384&quot;)
model = LongformerEncoderDecoderForConditionalGeneration.from_pretrained(&quot;allenai/led-base-16384&quot;, gradient_checkpointing=True)
</code></pre>
<p><code>Results:</code>
Approach 1 seems to work but I am not sure whether it is correct because we are loading AllenAI's pretrained LED using HuggingLED which has a different attention window size.</p>
<p>Approach 2 produces the error:</p>
<pre><code>  File &quot;/Users/krishna/opt/anaconda3/envs/CiteKP/lib/python3.8/site-packages/transformers/configuration_utils.py&quot;, line 353, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;LED_download.py&quot;, line 11, in &lt;module&gt;
    tokenizer = AutoTokenizer.from_pretrained(&quot;allenai/led-base-16384&quot;)
  File &quot;/Users/krishna/opt/anaconda3/envs/CiteKP/lib/python3.8/site-packages/transformers/tokenization_auto.py&quot;, line 209, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File &quot;/Users/krishna/opt/anaconda3/envs/CiteKP/lib/python3.8/site-packages/transformers/configuration_auto.py&quot;, line 272, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File &quot;/Users/krishna/opt/anaconda3/envs/CiteKP/lib/python3.8/site-packages/transformers/configuration_utils.py&quot;, line 362, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'allenai/led-base-16384'. Make sure that:

- 'allenai/led-base-16384' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'allenai/led-base-16384' is the correct path to a directory containing a config.json file
</code></pre>
<p>I think the error is because the latest pretrained model <code>allenai/led-base-16384</code> is not compatible with the transformers version (i.e. <code>v3.1.0</code>) mentioned in their <code>requirements.txt</code>?</p>
","3306097","","","","","2021-08-06 22:14:50","Correct way of loading pretrained model led-base-16384","<python><machine-learning><pytorch><huggingface-transformers><huggingface-tokenizers>","0","3","","","","CC BY-SA 4.0"
"66017404","1","","","2021-02-02 20:47:43","","1","948","<p>The title. At this point I basically tried all combinations of TF 2.0, 2.1, tensorflow-gpu (both 2.1 and 2.0) and the latest huggingface build. Plus creating new environments from scratch for each try.</p>
<p>I can either:</p>
<ul>
<li>Install tensorflow-gpu (2.0 or 2.1) -&gt; TF does find my GPU, but then huggingface cannot find a single TF model. For example, I can <code>import AutoModel</code> just fine, but I cannot <code>import TFAutoModel</code> (error: <code>ImportError: cannot import name 'TFAutoModel' from 'transformers' (unknown location)</code>). This happens with everything TF* that I tried.</li>
</ul>
<p>OR</p>
<ul>
<li>Install tensorflow (2.0 or 2.1) -&gt; TF does NOT find my GPU, but I can <code>import TFAutoModel</code> without an issue.</li>
</ul>
<p>I have also installed the required CUDA and cuDNN versions and have a GPU with enough compute power yadda yadda.</p>
<p>TL;DR: How to get huggingface TF models to work with TF GPU?</p>
","2897989","","","","","2021-02-03 08:05:47","Anybody got huggingface transformers to work in a Conda env?","<python><tensorflow><anaconda><conda><huggingface-transformers>","1","4","","","","CC BY-SA 4.0"
"60382793","1","60384804","","2020-02-24 19:37:01","","6","2246","<p>I was reading the <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">BERT paper</a> and was not clear regarding the inputs to the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""noreferrer"">transformer</a> encoder and decoder. </p>

<p>For learning masked language model (Cloze task), the paper says that 15% of the tokens are masked and the network is trained to predict the masked tokens. Since this is the case, what are the inputs to the transformer encoder and decoder?</p>

<p><a href=""https://i.stack.imgur.com/jIIuo.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jIIuo.png"" alt=""BERT input representation (from the paper)""></a></p>

<p>Is the input to the transformer encoder this input representation (see image above). If so, what is the decoder input?</p>

<p>Further, how is the output loss computed? Is it a softmax for only the masked locations? For this, the same linear layer is used for all masked tokens?</p>
","3253481","","","","","2020-12-06 21:00:17","What are the inputs to the transformer encoder and decoder in BERT?","<python><deep-learning><nlp><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"60397610","1","","","2020-02-25 15:05:50","","7","2152","<p>I had a question about the language model finetuning code on the Hugging Face repository.
It seems that the forward method of the BERT model takes as input an argument called <a href=""https://github.com/huggingface/transformers/blob/e693cd1e877aa191d3317faed33e87d1558c9406/src/transformers/modeling_bert.py#L639"" rel=""nofollow noreferrer"">attention_mask</a>. </p>

<p>The <a href=""https://huggingface.co/transformers/glossary.html#attention-mask"" rel=""nofollow noreferrer"">documentation</a> says that the attention mask is an optional argument used when batching sequences together. This argument indicates to the model which tokens should be attended to, and which should not.  For example, the tokenizer encoding methods return this attention mask, a binary tensor indicating the position of the padded indices so that the model does not attend to them, which makes sense.</p>

<p>If I'm not wrong, however, I do not see the usage of this attention mask in the code for language model finetuning. Indeed during the forward pass, only the input ids are given as input, <a href=""https://github.com/huggingface/transformers/blob/e693cd1e877aa191d3317faed33e87d1558c9406/examples/run_language_modeling.py#L353"" rel=""nofollow noreferrer"">cf this code</a>. </p>

<p>My questions is: does this mean that we do not kill the attention on the padding tokens during the training ? Does it make sense to take them into account ? 
Or maybe I missed something in the code ?</p>

<p>Thank you very much for your answer :) </p>

<p>EDIT</p>

<p>I noticed that the way Hugging Face is building the dataset leads to no padding needs at all (see <a href=""https://github.com/huggingface/transformers/blob/8bcb37bfb80d77e06001f989ad982c9961a69c31/examples/run_language_modeling.py#L112"" rel=""nofollow noreferrer"">this code</a> )</p>
","12960873","","12960873","","2020-02-27 17:33:30","2020-02-27 17:33:30","Use of attention_mask during the forward pass in lm finetuning","<huggingface-transformers>","1","0","2","","","CC BY-SA 4.0"
"68619985","1","","","2021-08-02 10:23:11","","0","19","<p>I am so close to achieving masked language modelling using emojis, but I have stumbled upon an issue, preventing me from reaching my goal.</p>
<p>So far, I have trained a masked language model on a Twitter dataset, with each tweet containing one emoji. Then, I used the following code to add the emojis as special tokens:</p>
<pre><code>num_added_toks = tokenizer.add_tokens(['ðŸ˜ƒ',
'ðŸ˜„',
'ðŸ˜',
'ðŸ˜†',
'ðŸ˜…',
'ðŸ˜‚',
'ðŸ¤£',
'ðŸ§”ðŸ¿â€â™‚ï¸'])
print('We have added', num_added_toks, 'tokens')
model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer
</code></pre>
<p>From adding the special tokens, I added 3311 different emojis successfully, which increased the embedding to (53575, 768) as shown below:</p>
<pre><code>We have added 3311 tokens

Embedding(53575, 768)
</code></pre>
<p>Now, here's the issue I am facing... When I add the  token to a sentence and input the top_k as the total number of embeddings, which is 53575, not a single emoji shows up in the predictions.</p>
<p>I used this line of code:</p>
<pre><code>mask_filler(&quot;Are you happy today &lt;mask&gt;&quot;, top_k=53575)
</code></pre>
<p>As you can see in the code above, the top_k is 53575, the total number of embeddings which should include the 3311 emojis I added, right?</p>
<p>However, when I make the predictions and scroll through the list of 53575, not a single emoji is there!</p>
<p>Why is this is happening? Like, I have added the emojis to the vocabulary, but they are simply not there when making predictions.</p>
<p>SEE FULL CODE HERE: <a href=""https://github.com/saucyhambon/MLM-EMOJIS/blob/main/mlm_emojis.ipynb"" rel=""nofollow noreferrer"">https://github.com/saucyhambon/MLM-EMOJIS/blob/main/mlm_emojis.ipynb</a></p>
","16098918","","","","","2021-08-02 10:23:11","Why are special tokens, which I added, not appearing as predictions in masked language modelling?","<python><machine-learning><huggingface-transformers><bert-language-model><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"68642673","1","","","2021-08-03 20:51:43","","0","18","<p>I have a question about sample_weights. Typically, you can pass in the <code>sample_weights</code> as the third element of a tuple when constructing the Tensorflow Dataset (<a href=""https://www.tensorflow.org/guide/keras/train_and_evaluate#sample_weights"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras/train_and_evaluate#sample_weights</a>). However, for the class <code>T5ForConditionalGeneration</code>, the <code>call</code> method (which I assume is what is called on the model is called) only takes the following parameters:</p>
<pre><code> def call(
    self,
    input_ids=None,
    attention_mask=None,
    decoder_input_ids=None,
    decoder_attention_mask=None,
    head_mask=None,
    decoder_head_mask=None,
    encoder_outputs=None,
    past_key_values=None,
    inputs_embeds=None,
    decoder_inputs_embeds=None,
    labels=None,
    use_cache=None,
    output_attentions=None,
    output_hidden_states=None,
    return_dict=None,
    training=False,
    **kwargs,
):
</code></pre>
<p>Source: <a href=""https://huggingface.co/transformers/_modules/transformers/models/t5/modeling_tf_t5.html#TFT5Model"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/_modules/transformers/models/t5/modeling_tf_t5.html#TFT5Model</a></p>
<p>I donâ€™t see a way for T5 to consider the sample weights. How do I pass in the sample weights to a T5ForConditionalGeneration model? Thanks!</p>
","15284739","","","","","2021-08-03 20:51:43","Adding Sample Weights to HuggingFace's TFT5ForConditionalGeneration Model","<tensorflow><tensorflow-datasets><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"60573957","1","","","2020-03-07 02:49:56","","0","211","<p>Consider the following code taken from <a href=""https://medium.com/tensorflow/using-tensorflow-2-for-state-of-the-art-natural-language-processing-102445cda54a"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>import tensorflow
import transformers
import tensorflow_datasets
from tensorflow.keras import callbacks, losses, metrics, optimizers

data = tensorflow_datasets.load(""glue/mrpc"")
train_dataset = data[""train""]
validation_dataset = data[""validation""]

bert_model = transformers.TFBertModel.from_pretrained(""bert-base-cased"")
bert_tokenizer = transformers.BertTokenizer.from_pretrained(""bert-base-cased"")

tensorboard_callback = callbacks.TensorBoard(write_images=True, embeddings_freq=1)
bert_model.compile(optimizer=optimizers.Adam(learning_rate=0.01), loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[metrics.SparseCategoricalAccuracy(""accuracy"")])
bert_history = bert_model.fit(train_dataset, epochs=10, validation_data=(validation_dataset), callbacks=[tensorboard_callback])
</code></pre>

<p>When I run it, it gives the following error:</p>

<pre><code>ValueError: in converted code:

    C:\tools\miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py:677 map_fn
        batch_size=None)
    C:\tools\miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py:2469 _standardize_tensors
        exception_prefix='target')
    C:\tools\miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_utils.py:539 standardize_input_data
        str(data)[:200] + '...')

    ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), for inputs ['output_1', 'output_2'] but instead got the following list of 1 arrays: [&lt;tf.Tensor 'args_3:0' shape=() dtype=int64&gt;]...
</code></pre>

<p>And I don't understand, because it should work. Any suggestions?</p>
","12228795","","","","","2020-03-07 02:49:56","ValueError when calling ""fit"" on BERT transformer on Tensorflow 2.1","<python><tensorflow><keras><tensorflow-datasets><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"68579096","1","","","2021-07-29 15:46:39","","0","65","<p>I am trying to run a notebook that uses the huggingface library dataset class. I've loaded a dataset and am trying to apply a map() function to it.</p>
<p>Here is my code:</p>
<pre><code>model_name_or_path = &quot;facebook/wav2vec2-base-100k-voxpopuli&quot;
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path,)
target_sampling_rate = feature_extractor.sampling_rate
print(feature_extractor)

Wav2Vec2FeatureExtractor {
  &quot;do_normalize&quot;: true,
  &quot;feature_extractor_type&quot;: &quot;Wav2Vec2FeatureExtractor&quot;,
  &quot;feature_size&quot;: 1,
  &quot;padding_side&quot;: &quot;right&quot;,
  &quot;padding_value&quot;: 0,
  &quot;return_attention_mask&quot;: false,
  &quot;sampling_rate&quot;: 16000
}
</code></pre>
<pre><code>def speech_file_to_array_fn(path):
    speech_array, sampling_rate = torchaudio.load(path)
    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)
    speech = resampler(speech_array).squeeze().numpy()
    return speech

def label_to_id(label, label_list):

    if len(label_list) &gt; 0:
        return label_list.index(label) if label in label_list else -1

    return label

def preprocess_function(examples):
    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]
    print(speech_list)
    target_list = [label_to_id(label, label_list) for label in examples[output_column]]
    print(type(speech_list))
    result = feature_extractor(speech_list, sampling_rate=target_sampling_rate)

    result[&quot;labels&quot;] = list(target_list)

    return result

# Remove this part
max_samples = 100
train_dataset = train_dataset.select(range(max_samples))
eval_dataset = eval_dataset.select(range(max_samples))

train_dataset = train_dataset.map(
    preprocess_function,
    batch_size=10,
    batched=True,
    num_proc=4
)
eval_dataset = eval_dataset.map(
    preprocess_function,
    batch_size=10,
    batched=True,
    num_proc=4
)
</code></pre>
<p>After running the last lines, (the map() function), here is the error:</p>
<pre><code>
/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order)
/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order)
/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order)
/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order)
---------------------------------------------------------------------------
RemoteTraceback                           Traceback (most recent call last)
RemoteTraceback: 
&quot;&quot;&quot;
TypeError: float() argument must be a string or a number, not 'list'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/multiprocess/pool.py&quot;, line 121, in worker
    result = (True, func(*args, **kwds))
  File &quot;/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py&quot;, line 185, in wrapper
    out: Union[&quot;Dataset&quot;, &quot;DatasetDict&quot;] = func(self, *args, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/datasets/fingerprint.py&quot;, line 397, in wrapper
    out = func(self, *args, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py&quot;, line 2020, in _map_single
    offset=offset,
  File &quot;/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py&quot;, line 1906, in apply_function_on_filtered_inputs
    function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)
  File &quot;&lt;ipython-input-105-3cb463e63163&gt;&quot;, line 19, in preprocess_function
    result = feature_extractor(speech_list, sampling_rate=target_sampling_rate)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py&quot;, line 211, in __call__
    padded_inputs[&quot;input_values&quot;] = self.zero_mean_unit_var_norm(
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py&quot;, line 87, in zero_mean_unit_var_norm
    if isinstance(input_values[0], np.ndarray):
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py&quot;, line 87, in &lt;listcomp&gt;
    if isinstance(input_values[0], np.ndarray):
ValueError: setting an array element with a sequence.
&quot;&quot;&quot;

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-109-28e13eb2d6de&gt; in &lt;module&gt;()
      3     batch_size=10,
      4     batched=True,
----&gt; 5     num_proc=4
      6 )
      7 eval_dataset = eval_dataset.map(

11 frames
/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)
   1742                 logger.info(&quot;Spawning {} processes&quot;.format(num_proc))
   1743                 results = [pool.apply_async(self.__class__._map_single, kwds=kwds) for kwds in kwds_per_shard]
-&gt; 1744                 transformed_shards = [r.get() for r in results]
   1745                 logger.info(&quot;Concatenating {} shards from multiprocessing&quot;.format(num_proc))
   1746                 result = concatenate_datasets(transformed_shards)

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in &lt;listcomp&gt;(.0)
   1742                 logger.info(&quot;Spawning {} processes&quot;.format(num_proc))
   1743                 results = [pool.apply_async(self.__class__._map_single, kwds=kwds) for kwds in kwds_per_shard]
-&gt; 1744                 transformed_shards = [r.get() for r in results]
   1745                 logger.info(&quot;Concatenating {} shards from multiprocessing&quot;.format(num_proc))
   1746                 result = concatenate_datasets(transformed_shards)

/usr/local/lib/python3.7/dist-packages/multiprocess/pool.py in get(self, timeout)
    655             return self._value
    656         else:
--&gt; 657             raise self._value
    658 
    659     def _set(self, i, obj):

/usr/local/lib/python3.7/dist-packages/multiprocess/pool.py in worker()
    119         job, i, func, args, kwds = task
    120         try:
--&gt; 121             result = (True, func(*args, **kwds))
    122         except Exception as e:
    123             if wrap_exception and func is not _helper_reraises_exception:

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in wrapper()
    183         }
    184         # apply actual function
--&gt; 185         out: Union[&quot;Dataset&quot;, &quot;DatasetDict&quot;] = func(self, *args, **kwargs)
    186         datasets: List[&quot;Dataset&quot;] = list(out.values()) if isinstance(out, dict) else [out]
    187         # re-apply format to the output

/usr/local/lib/python3.7/dist-packages/datasets/fingerprint.py in wrapper()
    395             # Call actual function
    396 
--&gt; 397             out = func(self, *args, **kwargs)
    398 
    399             # Update fingerprint of in-place transforms + update in-place history of transforms

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in _map_single()
   2018                                 indices,
   2019                                 check_same_num_examples=len(input_dataset.list_indexes()) &gt; 0,
-&gt; 2020                                 offset=offset,
   2021                             )
   2022                         except NumExamplesMismatch:

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in apply_function_on_filtered_inputs()
   1904                 effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset
   1905             processed_inputs = (
-&gt; 1906                 function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)
   1907             )
   1908             if update_data is None:

&lt;ipython-input-105-3cb463e63163&gt; in preprocess_function()
     17     target_list = [label_to_id(label, label_list) for label in examples[output_column]]
     18 
---&gt; 19     result = feature_extractor(speech_list, sampling_rate=target_sampling_rate)
     20     result[&quot;labels&quot;] = list(target_list)
     21 

/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py in __call__()
    209         # zero-mean and unit-variance normalization
    210         if self.do_normalize:
--&gt; 211             padded_inputs[&quot;input_values&quot;] = self.zero_mean_unit_var_norm(
    212                 padded_inputs[&quot;input_values&quot;], input_lengths=input_lengths
    213             )

/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py in zero_mean_unit_var_norm()
     85         &quot;&quot;&quot;
     86         print(input_values)
---&gt; 87         if isinstance(input_values[0], np.ndarray):
     88             input_values = [x.astype(np.float32) for x in input_values]
     89 

/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py in &lt;listcomp&gt;()
     85         &quot;&quot;&quot;
---&gt; 86         if isinstance(input_values[0], np.ndarray):
     87             input_values = [x.astype(np.float32) for x in input_values]
     88 

ValueError: setting an array element with a sequence.
</code></pre>
<p>I'm not sure what is going on here. When I print <code>speech_list</code> from the <code>preprocess_function()</code> function, I get this:</p>
<pre><code>
[array([[-1.3845960e-03, -1.5129161e-03, -1.3279491e-03, ...,
        -9.3758357e-04, -5.0248392e-04, -2.3690595e-04],
       [ 5.0415384e-04,  7.3929900e-06, -5.8541872e-04, ...,
         2.6963546e-04,  4.4448639e-04,  7.5516611e-04]], dtype=float32), array([[ 0.00750456,  0.00959514,  0.00922188, ..., -0.0017352 ,
        -0.0019784 , -0.00042148],
       [-0.0046173 ,  0.00029656,  0.01085352, ...,  0.00995941,
         0.0046006 ,  0.00151875]], dtype=float32), array([[ 0.00032558,  0.00044874, -0.00064546, ...,  0.00199648,
         0.00220139,  0.00113442],
       [ 0.01374926,  0.02029924,  0.02300985, ...,  0.02447655,
         0.024421  ,  0.00926847]], dtype=float32), array([[ 3.1322680e-04,  2.9084622e-05, -1.7249165e-04, ...,
        -1.0999236e-03, -1.4311116e-03, -3.1127129e-04],
       [-1.9904135e-03, -2.2752464e-03, -1.9129037e-03, ...,
        -7.8604842e-04, -1.6195733e-03, -3.5395977e-04]], dtype=float32), array([[-0.0018726 , -0.00167636, -0.0016572 , ..., -0.00041437,
         0.00060199,  0.0006947 ],
       [ 0.00442896,  0.0041303 ,  0.00259148, ...,  0.00126941,
         0.0004518 , -0.00026673]], dtype=float32), array([[-0.00154839, -0.00183026, -0.00170901, ..., -0.00169933,
        -0.00238513, -0.00154379],
       [ 0.00048418,  0.00074115,  0.00099551, ..., -0.05256891,
        -0.03463165, -0.01582825]], dtype=float32), array([[-4.3763156e-04, -1.5511583e-04,  1.5612959e-04, ...,
        -1.0198121e-04,  2.6510053e-05,  5.8304349e-06],
       [-2.4142796e-03, -2.7431613e-03, -1.9503339e-03, ...,
         1.9912045e-03,  1.8718862e-03,  3.3789902e-04]], dtype=float32), array([[ 2.5531935e-04,  2.9120210e-04,  1.8021779e-05, ...,
         9.6951338e-04,  1.1847753e-03,  3.6130843e-04],
       [-4.2422273e-04, -9.5154933e-04, -1.1366532e-03, ...,
         1.3966652e-03,  1.4367601e-03,  3.6545223e-04]], dtype=float32), array([[ 0.00049792,  0.00055293,  0.00043075, ..., -0.00584954,
        -0.00827   , -0.00197146],
       [-0.00125196, -0.00177683, -0.00116915, ..., -0.00643045,
        -0.00696308, -0.00153378]], dtype=float32), array([[-0.00286428, -0.00418009, -0.00461933, ...,  0.00096886,
         0.00105958,  0.00106084],
       [-0.00322456, -0.00440617, -0.00480009, ..., -0.00011426,
         0.0002051 ,  0.00059317]], dtype=float32)]
</code></pre>
<p>which is type <code>&lt;class 'list'&gt;</code>.</p>
<p>I see the error says it doesn't accept lists, only strings or numbers, but this doesn't make sense? The docs aren't very clear about input to <code>Wav2Vec2FeatureExtractor</code>.</p>
<p><a href=""https://huggingface.co/transformers/model_doc/wav2vec2.html#transformers.Wav2Vec2FeatureExtractor"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/wav2vec2.html#transformers.Wav2Vec2FeatureExtractor</a></p>
<p>Please advise.</p>
","11666502","","7802200","","2021-08-04 15:28:57","2021-08-04 15:28:57","Setting an array with a sequence using Huggingface dataset map()","<python><deep-learning><pytorch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68589222","1","68592068","","2021-07-30 10:12:43","","0","71","<p>I downloaded the file first using:</p>
<pre><code>!curl -L -O https://github.com/huggingface/transformers/blob/master/examples/legacy/question-answering/run_squad.py
</code></pre>
<p>Then used following code:</p>
<pre><code>!python run_squad.py  \
    --model_type bert   \
    --model_name_or_path bert-base-uncased  \
    --output_dir models/bert/ \
    --data_dir data/squad   \
    --overwrite_output_dir \
    --overwrite_cache \
    --do_train  \
    --train_file /content/train.json   \
    --version_2_with_negative \
    --do_lower_case  \
    --do_eval   \
    --predict_file /content/val.json   \
    --per_gpu_train_batch_size 2   \
    --learning_rate 3e-5   \
    --num_train_epochs 2.0   \
    --max_seq_length 384   \
    --doc_stride 128   \
    --threads 10   \
    --save_steps 5000 
</code></pre>
<p>Also tried following:</p>
<pre><code>!python run_squad.py \
  --model_type bert \
  --model_name_or_path bert-base-cased \
  --do_train \
  --do_eval \
  --do_lower_case \
  --train_file /content/train.json \
  --predict_file /content/val.json \
  --per_gpu_train_batch_size 12 \
  --learning_rate 3e-5 \
  --num_train_epochs 2.0 \
  --max_seq_length 584 \
  --doc_stride 128 \
  --output_dir /content/
</code></pre>
<p>The error says in both the codes:</p>
<blockquote>
<p>File &quot;run_squad.py&quot;, line 7

^ SyntaxError: invalid syntax</p>
</blockquote>
<p>What exactly is the issue? How can I run the <code>.py</code> file?</p>
","16502388","","6818619","","2021-08-02 11:44:25","2021-08-02 11:44:25","How to run 'run_squad.py' on google colab? It gives 'invalid syntax' error","<google-colaboratory><stanford-nlp><bert-language-model><huggingface-transformers><question-answering>","1","0","","","","CC BY-SA 4.0"
"60355127","1","","","2020-02-22 17:59:10","","1","1823","<p>I fine tuned a huggingface transformer using Keras (with ktrain) and then reloaded the model in Pytorch.</p>

<p>I want to access the third to last layer (<code>pre_classifier</code>), so I removed the two last layers:</p>

<pre><code>BERT2 = torch.nn.Sequential(*(list(BERT.children())[:-2])) 
</code></pre>

<p>Running an encoded sentence through this yields the following error message:</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-38-640702475573&gt; in &lt;module&gt;
----&gt; 1 ans2=BERT2(torch.tensor([e1]))
      2 print (ans2)

C:\ProgramData\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

C:\ProgramData\Anaconda3\lib\site-packages\torch\nn\modules\container.py in forward(self, input)
     90     def forward(self, input):
     91         for module in self._modules.values():
---&gt; 92             input = module(input)
     93         return input
     94 

C:\ProgramData\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

C:\ProgramData\Anaconda3\lib\site-packages\torch\nn\modules\linear.py in forward(self, input)
     85 
     86     def forward(self, input):
---&gt; 87         return F.linear(input, self.weight, self.bias)
     88 
     89     def extra_repr(self):

C:\ProgramData\Anaconda3\lib\site-packages\torch\nn\functional.py in linear(input, weight, bias)
   1366         - Output: :math:`(N, *, out\_features)`
   1367     """"""
-&gt; 1368     if input.dim() == 2 and bias is not None:
   1369         # fused op is marginally faster
   1370         ret = torch.addmm(bias, input, weight.t())

AttributeError: 'tuple' object has no attribute 'dim'
</code></pre>

<p>Meanwhile deleting the classifier entirely (all three layers)</p>

<pre><code>BERT3 = torch.nn.Sequential(*(list(BERT.children())[:-3])) 
</code></pre>

<p>Yields the expected tensor (within a size 1 tuple) with the expected shape (<code>[sentence_num,token_num,768]</code>).</p>

<p>Why does the removal of two (but not three) layers breaks the model?
And how can I access the <code>pre_classifier</code> results?</p>

<p>It is not accessible by setting <code>config</code> with <code>output_hidden_states=True</code> as this flag returns the hidden values of the BERT transformer stack, not those of the classifier layers downstream to it.</p>

<p>--</p>

<h2>PS</h2>

<p>The code used to initialize the BERT model:</p>

<pre><code>def collect_data_for_FT():

    from sklearn.datasets import fetch_20newsgroups
    train_data = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)
    test_data =  fetch_20newsgroups(subset='test', shuffle=True, random_state=42)

    print('size of training set: %s' % (len(train_b['data'])))
    print('size of validation set: %s' % (len(test_b['data'])))
    print('classes: %s' % (train_b.target_names))

    x_train = train_data.data
    y_train = train_data.target
    x_test = test_data.data
    y_test = test_data.target

    return(x_train,y_train,x_test,y_test)


 bert_name = 'distilbert-base-uncased'
    from transformers import DistilBertForSequenceClassification,AutoConfig,AutoTokenizer
import os
dir_path = os.getcwd()
dir_path=os.path.join(dir_path,'models')

config = AutoConfig.from_pretrained(bert_name,num_labels=20) # change model configuration to access hidden values.

try:
    BERT = DistilBertForSequenceClassification.from_pretrained(dir_path,config=config)
    print (""Finetuned predictor loaded"")
except:
    import tensorflow.keras as keras
    print (""No finetuned predictor found.\nTraining."")
    (x_train,y_train,x_test,y_test)=collect_data_for_FT()
    ####
    # prework:
    import ktrain
    from ktrain import text
    t = text.Transformer(bert_name, maxlen=500, classes=train_b.target_names)
    trn = t.preprocess_train(x_train, y_train)
    val = t.preprocess_test(x_test, y_test)
    pre_trained_model = t.get_classifier()
    learner = ktrain.get_learner(pre_trained_model, train_data=trn, val_data=val, batch_size=6)    
    ####

    ####
    # Find best learning rate
    learner.lr_find()
    learner.lr_plot()
    ####

    learner.fit_onecycle(2e-4, 4) # choosen based on the learning rate/loss plot.

    ####
    # prepare and save:
    predictor = ktrain.get_predictor(learner.model, preproc=t)
    predictor.save('my_distilbertbase_predictor')
    predictor.model.save_pretrained(dir_path)
    ####
    BERT = DistilBertForSequenceClassification.from_pretrained(os.path.join(dir_path), from_tf=True,config=config) # re-load tensorflow to pytorch
    BERT.save_pretrained(dir_path) # save as a ""full blooded"" pytorch model
    BERT = DistilBertForSequenceClassification.from_pretrained(dir_path,config=config)  # re-load
    from tensorflow.keras import backend as K
    K.clear_session() # loading from tensorflow takes up space and the GPU. This releases it/
</code></pre>
","2182857","","2182857","","2020-02-24 22:16:57","2020-02-24 22:16:57","Removing last 2 layers from a BERT classifier results in "" 'tuple' object has no attribute 'dim' "" error. Why?","<keras><pytorch><tf.keras><huggingface-transformers>","0","4","","","","CC BY-SA 4.0"
"68664138","1","","","2021-08-05 09:37:28","","0","49","<p>I have recently started experimenting with the <code>transformers</code> library from <code>Hugging Face</code>.
In this small project I would like to <strong>fine-tune a GPT2 model to generate text</strong> using the <code>Trainer</code> API. In the example below I use a dummy dataset, but nonetheless I was able to replicate the error.</p>
<pre><code>import torch
from torch.utils.data import random_split, Dataset
from transformers import GPT2Tokenizer, GPT2Model, Trainer, \
    TrainingArguments


class MyDataset(Dataset):

    def __init__(self, txt_list, tokenizer, max_length):
        self.tokenizer = tokenizer 
        self.input_ids = []
        self.attn_masks = []

        for txt in txt_list:
            &quot;&quot;&quot;
            This loop will iterate through each entry in the text corpus.
            For each bit of text it will prepend it with the start of text token,
            then append the end of text token and pad to the maximum length with the 
            pad token.
            &quot;&quot;&quot;
            encodings_dict = tokenizer(
                '&lt;|startoftext|&gt;' + txt + '&lt;|endoftext|&gt;',
                truncation=True,
                max_length=max_length,
                padding=&quot;max_length&quot;)

            &quot;&quot;&quot;
            Each iteration then appends either the encoded tensor to a list,
            or the attention mask for that encoding to a list. The attention mask is
            a binary list of 1's or 0's which determine whether the langauge model
            should take that token into consideration or not. 
            &quot;&quot;&quot;
            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
            self.attn_masks.append(
                torch.tensor(encodings_dict['attention_mask']))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.attn_masks[idx]


def dummy_data_collator(features):
    batch = {}
    batch['input_ids'] = torch.stack([f[0] for f in features])
    batch['attention_mask'] = torch.stack([f[1] for f in features])

    return batch


if __name__ == '__main__':
    txt_list = [
        'Lorem ipsum dolor sit amet, consectetur adipiscing elit.',
        'Vestibulum tempus lorem arcu, eget consectetur augue pretium a.',
        'Suspendisse id pellentesque erat.',
        'Pellentesque quis ante ut risus sollicitudin maximus scelerisque ut urna.',
        'Nam tempus quis magna ac convallis. Praesent convallis egestas libero, ac sollicitudin libero dignissim at.',
        'Etiam efficitur eget dolor nec iaculis.'
    ]

    # Instantiate italian GPT2 tokenizer.
    tokenizer = GPT2Tokenizer.from_pretrained('LorenzoDeMattei/GePpeTto',
                                              bos_token='&lt;|startoftext|&gt;',
                                              eos_token='&lt;|endoftext|&gt;',
                                              pad_token='&lt;|pad|&gt;')

    # Identify the longest text to know how long to pad our sentences out to.
    max_length = max(
        [len(tokenizer.encode(txt)) for txt in txt_list])

    # Create the PyTorch dataset.
    dataset = MyDataset(txt_list, tokenizer, max_length)

    # Split into training and validation sets.
    val_size = int(0.1 * len(dataset))
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    # Fine-tune the model using the ðŸ¤— Trainer API
    model = GPT2Model.from_pretrained('LorenzoDeMattei/GePpeTto')

    training_args = TrainingArguments(
        output_dir='./results/',
        num_train_epochs=4,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir='./log/',
        evaluation_strategy='epoch'
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=dummy_data_collator,
    )

    trainer.train()

</code></pre>
<p>When running it I get the following error message</p>
<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-23-94c97283687f&gt; in &lt;module&gt;
     18     )
     19 
---&gt; 20 trainer.train()

~\...\lib\site-packages\transformers\trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
   1267                         tr_loss += self.training_step(model, inputs)
   1268                 else:
-&gt; 1269                     tr_loss += self.training_step(model, inputs)
   1270                 self.current_flos += float(self.floating_point_ops(inputs))
   1271 

~\...\site-packages\transformers\trainer.py in training_step(self, model, inputs)
   1760                 loss = self.compute_loss(model, inputs)
   1761         else:
-&gt; 1762             loss = self.compute_loss(model, inputs)
   1763 
   1764         if self.args.n_gpu &gt; 1:

~\...\site-packages\transformers\trainer.py in compute_loss(self, model, inputs, return_outputs)
   1792         else:
   1793             labels = None
-&gt; 1794         outputs = model(**inputs)
   1795         # Save past state if it exists
   1796         # TODO: this needs to be fixed and made cleaner later.

~\...\site-packages\torch\nn\modules\module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~\...\site-packages\transformers\models\gpt2\modeling_gpt2.py in forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)
    731 
    732         if inputs_embeds is None:
--&gt; 733             inputs_embeds = self.wte(input_ids)
    734         position_embeds = self.wpe(position_ids)
    735         hidden_states = inputs_embeds + position_embeds

~\...\site-packages\torch\nn\modules\module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~\...\site-packages\torch\nn\modules\sparse.py in forward(self, input)
    156 
    157     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 158         return F.embedding(
    159             input, self.weight, self.padding_idx, self.max_norm,
    160             self.norm_type, self.scale_grad_by_freq, self.sparse)

~\...\site-packages\torch\nn\functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2041         # remove once script supports set_grad_enabled
   2042         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2043     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2044 
   2045 

IndexError: index out of range in self
</code></pre>
<p>By debugging at <code>return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)</code> I find the following:</p>
<pre><code>weight.shape
&gt;&gt;&gt; torch.Size([30000, 768])

input.shape
&gt;&gt;&gt; torch.Size([6, 37])

torch.max(input)
&gt;&gt;&gt; tensor(30000)
</code></pre>
<p>I suppose the problem is that the embedding size is not large enough, since, as far as I understood, it should be that <code>weight.shape[0] - 1</code> &gt;= <code>torch.max(input)</code> .</p>
<p>I cannot figure out what's the root cause of this error, anyone can help me?
Thank you!</p>
","10224502","","","","","2021-08-05 09:37:28","IndexError: Text Generation with GPT2 and Hugging Face(pytorch)","<python><pytorch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"60579343","1","","","2020-03-07 15:48:08","","2","2755","<p>The following code is without batch:</p>

<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.eval()
context=torch.tensor([tokenizer.encode(""This is"")])
output, past = model(context)
token = torch.argmax(output[..., -1, :])
print(tokenizer.decode(token.item()))

output: ' a'
</code></pre>

<p>This is working fine. Now, I extended this to batch setting:</p>

<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.eval()

context=[torch.tensor(tokenizer.encode(""This is "")),torch.tensor(tokenizer.encode(""Hello How are ""))]
context=pad_sequence(context,batch_first=True)

mask=torch.tensor([[1,1,0],[1,1,1]])
output, past = model(context,attention_mask=mask)
token = torch.argmax(output[..., -1, :],dim=1)
tokenizer.decode(token)

output: '\n you'
</code></pre>

<p>Here <code>\n</code> is next token for the first context and <code>you</code> is next token for second context of the batch.
But The expected next token for the first context is <code>a</code>, since all the settings are same. Furthermore, if you reduce the second context to 2 token you will get <code>a</code> in this batch setting. So clearly, model can not understand the padding.
Also, the attention mask does not work. Because,
after padding the next token of sequence <code>this is</code> is 0 (zero). And according to the attention mask (<code>[1,1,0]</code>), this zero should be avoided and only the tokens <code>this</code> and <code>is</code> should be attended. The proofs that this attention masking is not working are:</p>

<ul>
<li><p>Use attention mask [1,1,1], that means attend even on the padding zero, you get the same output
which is <code>\n</code>.</p></li>
<li><p>Use the the string <code>this is!</code>. Here <code>!</code> has the zero index in the vocabulary matrix. Again you get the same output which is <code>\n</code>.</p></li>
</ul>

<p>Only time, it is possible to get desirable output is without the batch settings and attention mask ( now it seems, it does not matter because it has no effect anyway)</p>

<p>Then I found <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.pad_token"" rel=""nofollow noreferrer"">this</a>, which suggested to use pad_token. So I used like following:</p>

<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
from torch.nn.utils.rnn import pad_sequence  

tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"",pad_token=""&lt;PAD&gt;"")
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.eval()

context=[torch.tensor(tokenizer.encode(""This is &lt;PAD&gt; "")),torch.tensor(tokenizer.encode(""Hello How are""))]
context=torch.stack(context)
print(context)
mask=torch.tensor([[1,1,0],[1,1,1]])

output, past = model(context,attention_mask=mask)
token = torch.argmax(output[..., -1, :],dim=1)
tokenizer.decode(token)

output: 'The you'
</code></pre>

<p>Here <code>The</code> is next token for the first context and <code>you</code> is next token for second context of the batch. This is also not working. Because <code>The</code> is not expected for the first context.</p>

<p>How do I use variable length sequence in batch setting in gpt/gpt2 model?</p>
","3363813","","3607203","","2020-03-08 10:14:00","2020-03-10 05:47:23","padding and attention mask does not work as intended in batch input in GPT language model","<python><pytorch><language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60586863","1","","","2020-03-08 11:08:34","","2","48","<p>I'm using the transformers library of HuggingFace. As far as I know changing the number of hidden layers in the config file leads to loading the first x layers of the pre-trained BERT. I want to load the even layers (or the last x layers) of the pre-trained BERT and then fine-tune them for a classification task. </p>

<p>An example for classification tasks can be found here : <a href=""https://github.com/huggingface/transformers/blob/master/examples/run_glue.py"" rel=""nofollow noreferrer"">run_glue.py</a></p>

<p>Thanks in advance</p>
","8007968","","","","","2020-03-08 11:08:34","loading even layers of pre-trained BERT for classification","<machine-learning><deep-learning><huggingface-transformers>","0","2","1","","","CC BY-SA 4.0"
"68747152","1","68964812","","2021-08-11 18:20:06","","2","49","<p>I am new to AI models and currently experimenting with the QandA model. Particularly I am interested in following 2 models.
<strong>1. from transformers import BertForQuestionAnswering</strong><br />
<strong>2. from simpletransformers.question_answering import QuestionAnsweringModel</strong></p>
<p>Using option 1 <strong>BertForQuestionAnswering</strong> I am getting the desired results. However I can ask only one question at a time. Also I am not getting the probability of the answer.</p>
<p>below is the code for <strong>BertForQuestionAnswering</strong> from transformers.</p>
<pre><code>from transformers import BertTokenizer, BertForQuestionAnswering
import torch

tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')


input_ids = tokenizer.encode('Sky color?', 'Today the sky is blue. and it is cold out there')
tokens = tokenizer.convert_ids_to_tokens(input_ids)
sep_index = input_ids.index(tokenizer.sep_token_id)
num_seg_a = sep_index + 1
num_seg_b = len(input_ids) - num_seg_a
segment_ids = [0]*num_seg_a + [1]*num_seg_b
assert len(segment_ids) == len(input_ids)
outputs = model(torch.tensor([input_ids]), 
                            token_type_ids=torch.tensor([segment_ids]), 
                            return_dict=True)
start_scores = outputs.start_logits
end_scores = outputs.end_logits
answer_start = torch.argmax(start_scores)
answer_end = torch.argmax(end_scores)
answer = ' '.join(tokens[answer_start:answer_end+1])
print(answer)
</code></pre>
<p>Here is the output: <strong>blue</strong></p>
<p>Where as using option 2 <strong>QuestionAnsweringModel</strong> from simpletransformers, I can put multiple questions at a time and also getting probability of the answer.</p>
<p>below is the code for <strong>QuestionAnsweringModel</strong> from simpletransformers</p>
<pre><code>from simpletransformers.question_answering import QuestionAnsweringModel
model = QuestionAnsweringModel('distilbert', 'distilbert-base-uncased-distilled-squad', use_cuda=False)

question_data = {
        'qas': [{
            'question': 'Sky color?',
            'id': 0,
        },
        {
            'question': 'weather?',
            'id': 1,
        }
        ],
        'context': 'Today the sky is blue. and it is cold out there'
    }

prediction = model.predict([question_data])
output = {'result': list(prediction)}
print(output)
</code></pre>
<p>Here is the output:</p>
<pre><code>{
   &quot;result&quot;:[
      [
         {
            &quot;id&quot;:0,
            &quot;answer&quot;:[&quot;blue&quot;, &quot;the sky is blue&quot;, &quot;blue.&quot;]
         },
         {
            &quot;id&quot;:1,
            &quot;answer&quot;:[&quot;cold&quot;, &quot;it is cold&quot;, &quot;cold out there&quot;]
         }
      ],
      [
         {
            &quot;id&quot;:0,
            &quot;probability&quot;:[0.8834650211919095,0.0653234009794176,0.031404456093241565]
         },
         {
            &quot;id&quot;:1,
            &quot;probability&quot;:[0.6851319220199236,0.18145769901523698,0.05004994980074798]
         }
      ]
   ]
}
</code></pre>
<p>As you can see, For the same context I can ask multiple questions at a time and get the probability for each answer.</p>
<p>Is there a way I can get similar output for the BERT model in option#1.
I need a way to set multiple questions to a context and also need probability for each answer in the response.</p>
<p>Any help would be greatly appreciated.</p>
","2627292","","","","","2021-08-28 13:27:32","How to get probability of an answer using BERT model and is there a way to ask multiple questions for a context","<python><bert-language-model><huggingface-transformers><simpletransformers>","1","0","1","","","CC BY-SA 4.0"
"68749434","1","","","2021-08-11 22:06:05","","0","19","<p>So trying to run the script 'run_squad.py' provided by huggingface.
Used the code to convert csv to json squad 2.0 format which gives the correct format when checked.</p>
<p>code: <a href=""https://i.stack.imgur.com/pYydX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pYydX.png"" alt=""code where run_squad from huggingface is being run"" /></a></p>
<p>But when trying to run the script, it gives error &quot;list out of range&quot;, [<img src=""https://i.stack.imgur.com/e9Pve.png"" alt=""error screenshot"" />what could be the problem?</p>
","16502388","","","","","2021-08-11 22:06:05","Getting ""list out of range"" error while running run_squad.py from huggingface for question answering. How to solve?","<python><nlp><stanford-nlp><huggingface-transformers><bert-language-model>","0","0","","","","CC BY-SA 4.0"
"60710606","1","60710917","","2020-03-16 17:33:30","","0","44","<p>I'm trying to extract vector-representations of text using BERT in the transformers libray, and have stumbled on the following part of the <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertmodel"" rel=""nofollow noreferrer"">documentation</a> for the ""BERTModel"" class:</p>

<p><a href=""https://i.stack.imgur.com/74fta.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/74fta.png"" alt=""enter image description here""></a></p>

<p>Can anybody explain this in more detail? A forward-pass makes intuitive sense to me (am trying to get final hidden states after all), and I can't find any additional information on what ""pre and post processing"" means in this context.</p>

<p>Thanks up front!</p>
","4465454","","","","","2020-03-16 17:54:31","Why should I call a BERT module instance rather than the forward method?","<bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68757944","1","68758723","","2021-08-12 12:57:42","","0","40","<p>I would like to fine-tune a pre-trained transformers model on Question Answering. The model was pre-trained on large engineering &amp; science related corpora.</p>
<p>I have been provided a &quot;checkpoint.pt&quot; file containing the weights of the model. They have also provided me with a &quot;bert_config.json&quot; file but I am not sure if this is the correct configuration file.</p>
<pre><code>from transformers import AutoModel, AutoTokenizer, AutoConfig

MODEL_PATH = &quot;./checkpoint.pt&quot;
config = AutoConfig.from_pretrained(&quot;./bert_config.json&quot;)
model = AutoModel.from_pretrained(MODEL_PATH, config=config)
</code></pre>
<p>The reason I believe that bert_config.json doesn't match &quot;./checkpoint.pt&quot; file is that,  when I load the model with the code above, I get the error that goes as below.</p>
<blockquote>
<p>Some weights of the model checkpoint at ./aerobert/phase2_ckpt_4302592.pt were not used when initializing BertModel: ['files', 'optimizer', 'model', 'master params']</p>
<ul>
<li>This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).</li>
<li>This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at ./aerobert/phase2_ckpt_4302592.pt and are newly initialized: ['encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', ............</li>
</ul>
</blockquote>
<p>If I am correct in assuming that &quot;bert_config.json&quot; is not the correct one, is there a way to load this model correctly without the config.json file?</p>
<p>Is there a way to see the model architecture from the saved weights of checkpoint.pt file?</p>
","13681936","","13681936","","2021-08-12 14:42:41","2021-08-12 15:39:32","Is there a way to use a pre-trained transformers model without the configuration file?","<python><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60574112","1","","","2020-03-07 03:28:06","","1","1261","<p>I am experimenting on the use of transformer embeddings in sentence classification tasks <strong>without finetuning them</strong>. I have used BERT embeddings and those experiments gave me very good results. Now I want to use GPT-2 embeddings (without fine-tuning). So I have two questions,</p>

<ol>
<li>Can I use GPT-2 embeddings like that (because I know Gpt-2 is
trained on the left to right) </li>
<li>Is there any example uses of GPT-2 in
    classification tasks other than generation tasks?</li>
<li>If I can use GPT-2embeddings, how should I do it?</li>
</ol>
","5915270","","1243762","","2020-11-29 11:50:22","2020-11-29 11:50:22","Can we use GPT-2 sentence embedding for classification tasks?","<nlp><huggingface-transformers><gpt-2>","1","6","","","","CC BY-SA 4.0"
"68754733","1","","","2021-08-12 09:22:08","","0","23","<p>I installed the huggingface <code>Accelerate</code> library to train the BERT model in multiple GPUs. I installed the <code>Accelerate</code> library using <code>pip</code> in the <code>Anaconda</code> environment. But I couldnt the <code>accelerate</code> command in powershell.</p>
<p>The following <code>accelerate</code> command</p>
<pre><code>accelerate config
</code></pre>
<p>throws the following error:</p>
<pre><code>accelerate : The term 'accelerate' is not recognized as the name of a cmdlet, function, script file, or operable
program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.
At line:1 char:1
+ accelerate config
+ ~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (accelerate:String) [], CommandNotFoundException
    + FullyQualifiedErrorId : CommandNotFoundException
</code></pre>
<p>I used the example mentioned in their <a href=""https://github.com/huggingface/accelerate/blob/main/examples/nlp_example.py"" rel=""nofollow noreferrer"">GitHub Page</a>. As the <code>accelerate</code> command was not working from poershell, I used the <code>torch.distributed.launch</code> to run the script as follows:</p>
<pre><code> python -m torch.distributed.launch --nproc_per_node 1 --use_env ./nlp_example.py
</code></pre>
<p>Since I was using Windows OS, it gave the following error:</p>
<pre><code>RuntimeError: Distributed package doesn't have NCCL built in
</code></pre>
<p>My doubt is, will it to possible to change the backend to use <code>gloo</code>, rather than 'NCCL' in <code>Accelerate</code> package?</p>
<p>Kindly help. Thank you.</p>
","2825570","","","","","2021-08-12 09:22:08","Using 'gloo' backend with Huggingface Accelerate","<python><pytorch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68759885","1","68819794","","2021-08-12 15:02:53","","0","34","<p>I'm working on HuggingFace Transformers and using toy example from here:
<a href=""https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainer"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainer</a></p>
<p>What I actually need: ability to print input, output, grad and loss at every step.
It is trivial using Pytorch training loop, but it is not obvious using HuggingFace <code>Trainer</code>.
At the current moment I have next idea: create a <code>CustomCallback</code> like this:</p>
<pre><code>class MyCallback(TrainerCallback):
    &quot;A callback that prints a grad at every step&quot;

    def on_step_begin(self, args, state, control, **kwargs):
        print(&quot;next step&quot;)
        print(kwargs['model'].classifier.out_proj.weight.grad.norm())

args = TrainingArguments(
    output_dir='test_dir',
    overwrite_output_dir=True,
    num_train_epochs=1,
    logging_steps=100,
    report_to=&quot;none&quot;,
    fp16=True,
    disable_tqdm=True,
)


trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    callbacks=[MyCallback],
)

trainer.train()
</code></pre>
<p>This way I can print grad and weights for any model layer.
But I still can't figure out how to print input/output (for example, I want to check them on <code>nan</code>) and loss?</p>
<p>P.S. I also read something about <code>forward_hook</code> but still can't find good code examples for it.</p>
","4960953","","","","","2021-08-17 15:07:06","Print input / output / grad / loss at every step/epoch when training Transformers HuggingFace model","<python><logging><neural-network><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68693068","1","","","2021-08-07 14:05:23","","0","28","<p>When viewing the top predicted tokens in masked language modelling (MLM), is it possible to use <code>top_k</code> with <code>k=len(vocab)</code>?</p>
<p>So far, I have used this following line of code:</p>
<pre><code>mask_filler(&quot;The capital of [MASK] is Paris&quot;, top_k=5)
</code></pre>
<p>Would it be possible to incorporate <code>k=len(vocab)</code> into the line of code above to see the predicted tokens in my vocabulary or not?</p>
","16098918","","4685471","","2021-08-07 19:00:02","2021-08-07 19:00:02","Can k=len(vocab) be used with top_k when viewing predicted tokens?","<python><machine-learning><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","2","","","","CC BY-SA 4.0"
"68742863","1","68885046","","2021-08-11 13:20:44","","5","290","<p>I'm trying to fine-tune the ReformerModelWithLMHead (google/reformer-enwik8) for NER. I used the padding sequence length same as in the encode method (max_length = max([len(string) for string in list_of_strings])) along with attention_masks. And I got this error:</p>
<p><strong>ValueError: If training, make sure that config.axial_pos_shape factors: (128, 512) multiply to sequence length. Got prod((128, 512)) != sequence_length: 2248. You might want to consider padding your sequence length to 65536 or changing config.axial_pos_shape.</strong></p>
<ul>
<li>When I changed the sequence length to 65536, my colab session crashed by getting all the inputs of 65536 lengths.</li>
<li>According to the second option(changing config.axial_pos_shape), I cannot change it.</li>
</ul>
<p>I would like to know, Is there any chance to change config.axial_pos_shape while fine-tuning the model? Or I'm missing something in encoding the input strings for reformer-enwik8?</p>
<p>Thanks!</p>
<p><strong>Question Update: I have tried the following methods:</strong></p>
<ol>
<li>By giving paramteres at the time of model instantiation:</li>
</ol>
<blockquote>
<p>model = transformers.ReformerModelWithLMHead.from_pretrained(&quot;google/reformer-enwik8&quot;, num_labels=9, max_position_embeddings=1024, axial_pos_shape=[16,64], axial_pos_embds_dim=[32,96],hidden_size=128)</p>
</blockquote>
<p>It gives me the following error:</p>
<blockquote>
<p>RuntimeError: Error(s) in loading state_dict for ReformerModelWithLMHead:
size mismatch for reformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([258, 1024]) from checkpoint, the shape in current model is torch.Size([258, 128]).
size mismatch for reformer.embeddings.position_embeddings.weights.0: copying a param with shape torch.Size([128, 1, 256]) from checkpoint, the shape in current model is torch.Size([16, 1, 32]).</p>
</blockquote>
<p>This is quite a long error.</p>
<ol start=""2"">
<li>Then I tried this code to update the config:</li>
</ol>
<blockquote>
<p>model1 = transformers.ReformerModelWithLMHead.from_pretrained('google/reformer-enwik8', num_labels = 9)</p>
</blockquote>
<h4>Reshape Axial Position Embeddings layer to match desired max seq length</h4>
<pre><code>model1.reformer.embeddings.position_embeddings.weights[1] = torch.nn.Parameter(model1.reformer.embeddings.position_embeddings.weights[1][0][:128])
</code></pre>
<h4>Update the config file to match custom max seq length</h4>
<pre><code>model1.config.axial_pos_shape = 16,128
model1.config.max_position_embeddings = 16*128 #2048
model1.config.axial_pos_embds_dim= 32,96
model1.config.hidden_size = 128
output_model_path = &quot;model&quot;
model1.save_pretrained(output_model_path)
</code></pre>
<p>By this implementation, I am getting this error:</p>
<blockquote>
<p>RuntimeError: The expanded size of the tensor (512) must match the existing size (128) at non-singleton dimension 2.  Target sizes: [1, 128, 512, 768].  Tensor sizes: [128, 768]</p>
</blockquote>
<p>Because updated size/shape doesn't match with the original config parameters of pretrained model. The original parameters are: axial_pos_shape = 128,512 max_position_embeddings = 128*512 #65536 axial_pos_embds_dim= 256,768 hidden_size = 1024</p>
<p>Is it the right way I'm changing the config parameters or do I have to do something else?</p>
<p>Is there any example where ReformerModelWithLMHead('google/reformer-enwik8') model fine-tuned.</p>
<p>My main code implementation is as follow:</p>
<pre><code>class REFORMER(torch.nn.Module):
def __init__(self):
    super(REFORMER, self).__init__()
    self.l1 = transformers.ReformerModelWithLMHead.from_pretrained(&quot;google/reformer-enwik8&quot;, num_labels=9)

def forward(self, input_ids, attention_masks, labels):
    output_1= self.l1(input_ids, attention_masks, labels = labels)
    return output_1


model = REFORMER()

def train(epoch):
    model.train()
    for _, data in enumerate(training_loader,0):
        ids = data['input_ids'][0]   # input_ids from encode method of the model https://huggingface.co/google/reformer-enwik8#:~:text=import%20torch%0A%0A%23%20Encoding-,def%20encode,-(list_of_strings%2C%20pad_token_id%3D0
        input_shape = ids.size()
        targets = data['tags']
        print(&quot;tags: &quot;, targets, targets.size())
        least_common_mult_chunk_length = 65536 
        padding_length = least_common_mult_chunk_length - input_shape[-1] % least_common_mult_chunk_length
        #pad input 
        input_ids, inputs_embeds, attention_mask, position_ids, input_shape = _pad_to_mult_of_chunk_length(self=model.l1,
                input_ids=ids,
                inputs_embeds=None,
                attention_mask=None,
                position_ids=None,
                input_shape=input_shape,
                padding_length=padding_length,
                padded_seq_length=None,
                device=None,
            )
        outputs = model(input_ids, attention_mask, labels=targets) # sending inputs to the forward method
        print(outputs)
        loss = outputs.loss
        logits = outputs.logits
        if _%500==0:
           print(f'Epoch: {epoch}, Loss:  {loss}')

for epoch in range(1):
    train(epoch)
</code></pre>
","4930086","","4930086","","2021-08-18 15:04:04","2021-08-22 21:36:37","Error while trying to fine-tune the ReformerModelWithLMHead (google/reformer-enwik8) for NER","<python><nlp><pytorch><huggingface-transformers><ner>","2","2","","","","CC BY-SA 4.0"
"60610280","1","60615939","","2020-03-10 01:02:54","","13","6880","<p>I'm working on a text classification problem (e.g. sentiment analysis), where I need to classify a text string into one of five classes.</p>
<p>I just started using the <a href=""https://huggingface.co/transformers/index.html"" rel=""noreferrer"">Huggingface Transformer</a> package and BERT with PyTorch. What I need is a classifier with a softmax layer on top so that I can do 5-way classification. Confusingly, there seem to be two relevant options in the Transformer package: <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""noreferrer"">BertForSequenceClassification</a> and <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertformultiplechoice"" rel=""noreferrer"">BertForMultipleChoice</a>.</p>
<p><strong>Which one should I use for my 5-way classification task? What are the appropriate use cases for them?</strong></p>
<p>The documentation for <strong>BertForSequenceClassification</strong> doesn't mention softmax at all, although it does mention cross-entropy. I am not sure if this class is only for 2-class classification (i.e. logistic regression).</p>
<blockquote>
<p><em>Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.</em></p>
<ul>
<li><em><strong>labels</strong> (torch.LongTensor of shape (batch_size,), optional, defaults to None) â€“ Labels for computing the sequence classification/regression loss. Indices should be in [0, ..., config.num_labels - 1]. If config.num_labels == 1 a regression loss is computed (Mean-Square loss), If config.num_labels &gt; 1 a classification loss is computed (Cross-Entropy).</em></li>
</ul>
</blockquote>
<p>The documentation for <strong>BertForMultipleChoice</strong> mentions softmax, but the way the labels are described, it sound like this class is for multi-label classification (that is, a binary classification for multiple labels).</p>
<blockquote>
<p><em>Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.</em></p>
<ul>
<li><em><strong>labels</strong> (torch.LongTensor of shape (batch_size,), optional, defaults to None) â€“ Labels for computing the multiple choice classification loss. Indices should be in [0, ..., num_choices] where num_choices is the size of the second dimension of the input tensors.</em></li>
</ul>
</blockquote>
<p>Thank you for any help.</p>
","4561314","","-1","","2020-06-20 09:12:55","2020-03-10 10:41:23","BertForSequenceClassification vs. BertForMultipleChoice for sentence multi-class classification","<python><machine-learning><pytorch><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68783979","1","","","2021-08-14 14:02:04","","0","19","<p>I want to input conversation data as an input to the gpt2 model from huggingface transformers.</p>
<p>====Example====</p>
<p>A: Where did you meet David?<br>
B: I met him at the central park.<Br>
A: Weren't he quite strange that day?</p>
<p>=&gt; predicted B: Not at all, why?</p>
<p>===============</p>
<p>Like the upper example, I want to input some conversation data to the transformer and get a reply from the pretrained model(gpt2). Can anybody tell me how?</p>
","14067076","","","","","2021-08-14 14:02:04","Does anyone knows how to input a text content in huggingface gpt2?","<huggingface-transformers><gpt-2>","0","1","","","","CC BY-SA 4.0"
"67034683","1","","","2021-04-10 13:07:03","","0","118","<p>I have multi_label text classification task with 3 labels. I want apply Bert on my model. In the first segment of code I preprocessed my dataset . in the second segment of my code I want to apply a function to my data for converting to Bert tokens.</p>
<p>I get error as following, can anyone help me please?</p>
<pre><code>from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

TOP_K = 20000
MAX_SEQUENCE_LENGTH = 500


tokenizer = Tokenizer()
    # Create vocabulary with training texts.
tokenizer = text.Tokenizer(num_words=TOP_K)
tokenizer.fit_on_texts(X_train0)

    # Vectorize training and validation texts.
x_train = tokenizer.texts_to_sequences(X_train0)
x_val = tokenizer.texts_to_sequences(X_val0)

    # Get max sequence length.
max_length = len(max(x_train, key=len))

if max_length &gt; MAX_SEQUENCE_LENGTH:
    max_length = MAX_SEQUENCE_LENGTH

    # Fix sequence length to max value. Sequences shorter than the length are
    # padded in the beginning and sequences longer are truncated
    # at the beginning.
x_train = sequence.pad_sequences(x_train, maxlen=max_length)
x_val = sequence.pad_sequences(x_val, maxlen=max_length)
    #return x_train, x_val, tokenizer.word_index

</code></pre>
<p>The code below is about converting the data to Bert Token</p>
<pre><code># Initialise Bert Tokenizer
bert_tokenizer_transformer = BertTokenizer.from_pretrained('bert-base-cased')
maxlen = 31
def create_input_array(df, tokenizer, args):
    sentences = df.text_prepd.values

    input_ids = []
    attention_masks = []
    token_type_ids = []

    for sent in tqdm(sentences):
        # `encode_plus` will:
        #   (1) Tokenize the sentence.
        #   (2) Prepend the `[CLS]` token to the start.
        #   (3) Append the `[SEP]` token to the end.
        #   (4) Map tokens to their IDs.
        #   (5) Pad or truncate the sentence to `max_length`
        #   (6) Create attention masks for [PAD] tokens.
        encoded_dict = tokenizer.encode_plus(
            sent,  # Sentence to encode.
            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
            max_length=args.max_length,  # Pad &amp; truncate all sentences.
                pad_to_max_length=True,
                return_attention_mask=True,  # Construct attn. masks.
                return_tensors='tf',  # Return tf tensors.
            )

        # Add the encoded sentence to the list.
        input_ids.append(encoded_dict['input_ids'])

        # And its attention mask (simply differentiates padding from non-padding).
        attention_masks.append(encoded_dict['attention_mask'])

        token_type_ids.append(encoded_dict['token_type_ids'])

    input_ids = tf.convert_to_tensor(input_ids)
    attention_masks = tf.convert_to_tensor(attention_masks)
    token_type_ids = tf.convert_to_tensor(token_type_ids)

    return input_ids, attention_masks, token_type_ids
#Convert Data to Bert Inputs




train_inputs = [create_input_array(x_train[:], tokenizer=tokenizer, args=args)]
val_inputs = [create_input_array(x_val[:], tokenizer=tokenizer, args=args)]



</code></pre>
<p>The Error that I get is as following:</p>
<pre><code>
NameError                                 Traceback (most recent call last)
&lt;ipython-input-74-593f5b32429c&gt; in &lt;module&gt;()
      1 #Convert Data to Bert Inputs
      2 
----&gt; 3 train_inputs = [create_input_array(x_train[:], tokenizer=tokenizer, args=args)]
      4 val_inputs = [create_input_array(x_val[:], tokenizer=tokenizer, args=args)]

NameError: name 'args' is not defined

</code></pre>
","14269252","","","","","2021-04-10 13:07:03","Multi label classification with Bert - Python - Keras","<python><nlp><bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"67047188","1","","","2021-04-11 16:01:43","","1","124","<p>I have a NLP project where a collection of words are encoded currently by <code>w2v</code>, to compare to other collections of words. I'd like to try <code>transformers</code> which could give a better encoding than <code>w2v</code>. However, due to the nature of the data, I won't need positional encoding at all (due to the fact that the collection of words have no order). <code>Is there a pretrained transformer that won't do positional encoding</code>?</p>
","8787039","","","","","2021-04-13 18:07:16","need a ""bag of words"" type of transformer","<nlp><word2vec><huggingface-transformers><transformer>","1","3","","","","CC BY-SA 4.0"
"68761188","1","","","2021-08-12 16:33:15","","0","36","<p>I have just followed <a href=""https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb"" rel=""nofollow noreferrer"">this</a> tutorial on how to train my own tokenizer.</p>
<p>Now, from training my tokenizer, I have wrapped it inside a Transformers object, so that I can use it with the transformers library:</p>
<pre><code>from transformers import BertTokenizerFast

new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
</code></pre>
<p>Then, I try to save my tokenizer using this code:</p>
<pre><code>tokenizer.save_pretrained('/content/drive/MyDrive/Tokenzier')
</code></pre>
<p>But I get this error:</p>
<pre><code>AttributeError: 'tokenizers.Tokenizer' object has no attribute 'save_pretrained'
</code></pre>
<p>Am I saving the tokenizer wrongly?</p>
<p>If so, what is the correct approach to save it to my local files, so that I can use it later?</p>
","16098918","","4685471","","2021-08-13 00:10:18","2021-09-06 14:58:35","How to save a tokenizer after training it?","<python><machine-learning><bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","2","","","","CC BY-SA 4.0"
"58532911","1","58558888","","2019-10-24 01:28:33","","1","2053","<p>When using the <code>torch.nn.modules.transformer.Transformer</code> module/object, the first layer is the <code>encoder.layers.0.self_attn</code> layer that is a <code>MultiheadAttention</code> layer, i.e. </p>

<pre><code>from torch.nn.modules.transformer import Transformer
bumblebee = Transformer()

bumblee.parameters
</code></pre>

<p>[out]:</p>

<pre><code>&lt;bound method Module.parameters of Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
</code></pre>

<p>And if we print out the size of the layer, we see:</p>

<pre><code>for name in bumblebee.encoder.state_dict():
    print(name, '\t', bumblebee.encoder.state_dict()[name].shape)
</code></pre>

<p>[out]:</p>

<pre><code>layers.0.self_attn.in_proj_weight    torch.Size([1536, 512])
layers.0.self_attn.in_proj_bias      torch.Size([1536])
layers.0.self_attn.out_proj.weight   torch.Size([512, 512])
layers.0.self_attn.out_proj.bias     torch.Size([512])
layers.0.linear1.weight      torch.Size([2048, 512])
layers.0.linear1.bias    torch.Size([2048])
layers.0.linear2.weight      torch.Size([512, 2048])
layers.0.linear2.bias    torch.Size([512])
layers.0.norm1.weight    torch.Size([512])
layers.0.norm1.bias      torch.Size([512])
layers.0.norm2.weight    torch.Size([512])
layers.0.norm2.bias      torch.Size([512])
</code></pre>

<p>It seems like 1536 is 512 * 3 and somehow the <code>layers.0.self_attn.in_proj_weight</code> parameter might be storing all three QKV tensors in the transformer architecture in one matrix. </p>

<p>From <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py#L649"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py#L649</a> </p>

<pre><code>class MultiheadAttention(Module):
    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):
        super(MultiheadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim

        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, ""embed_dim must be divisible by num_heads""

        if self._qkv_same_embed_dim is False:
            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))
            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))
            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))
        else:
            self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))
</code></pre>

<p>And the note in the docstring of the <code>MultiheadAttention</code> says:</p>

<blockquote>
  <p>Note: if kdim and vdim are None, they will be set to embed_dim such that
  query, key, and value have the same number of features.</p>
</blockquote>

<p>Is that correct? </p>
","610569","","1150683","","2020-01-16 10:25:46","2020-01-16 10:25:46","Why is the input size of the MultiheadAttention in Pytorch Transformer module 1536?","<pytorch><tensor><transformer><attention-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68779111","1","","","2021-08-13 23:01:24","","0","23","<p>I am using the pre-trained <code>google/bigbird-pegasus-large-arxiv</code> model.</p>
<p>But I receive the following update during the forward pass.</p>
<pre><code>Attention type 'block_sparse' is not possible if sequence_length: 458 &lt;= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3.Changing attention type to 'original_full'...
</code></pre>
<p>I understand the update and I am aware of benefit of time and memory it saves while using <code>block_sparse</code> than <code>original_full</code>.</p>
<p>So, how should I go about selecting the suitable <code>block_size</code> and  <code>num_random_blocks</code> when I know that there is a lot of variation in the sequence length of my inputs?</p>
","3306097","","","","","2021-08-13 23:01:24","Attention type 'block_sparse' is not possible if sequence_length: 458 <= num global tokens:","<machine-learning><pytorch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"58454157","1","58456379","","2019-10-18 15:42:38","","11","16150","<p>Training a BERT model using PyTorch transformers (following the tutorial <a href=""https://mccormickml.com/2019/07/22/BERT-fine-tuning/"" rel=""noreferrer"">here</a>).</p>

<p>Following statement in the tutorial</p>

<pre><code>loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
</code></pre>

<p>leads to</p>

<pre><code>TypeError: forward() got an unexpected keyword argument 'labels'
</code></pre>

<p>Here is the full error,</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-53-56aa2f57dcaf&gt; in &lt;module&gt;
     26         optimizer.zero_grad()
     27         # Forward pass
---&gt; 28         loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
     29         train_loss_set.append(loss.item())
     30         # Backward pass

~/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

TypeError: forward() got an unexpected keyword argument 'labels'
</code></pre>

<p>I cant seem to figure out what kind of argument the forward() function expects.</p>

<p>There is a similar problem <a href=""https://github.com/allenai/allennlp/issues/2528"" rel=""noreferrer"">here</a>, but I still do not get what the solution is.</p>

<p>System information:</p>

<ul>
<li>OS: Ubuntu 16.04 LTS</li>
<li>Python version: 3.6.x</li>
<li>Torch version: 1.3.0</li>
<li>Torch Vision version: 0.4.1</li>
<li>PyTorch transformers version: 1.2.0</li>
</ul>
","9965155","","1150683","","2020-01-16 10:23:55","2020-09-28 06:17:53","PyTorch BERT TypeError: forward() got an unexpected keyword argument 'labels'","<python><pytorch><bert-language-model><huggingface-transformers>","1","2","1","","","CC BY-SA 4.0"
"68726545","1","","","2021-08-10 12:07:05","","0","26","<p>I am following this tutorial here: <a href=""https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb"" rel=""nofollow noreferrer"">https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb</a></p>
<p>So, using this code, I add my custom dataset:</p>
<pre><code>from datasets import load_dataset
dataset = load_dataset('csv', data_files=['/content/drive/MyDrive/mydata.csv'])
</code></pre>
<p>Then, I use this code to take a look at the dataset:</p>
<pre><code>dataset
</code></pre>
<p>Access an element:</p>
<pre><code>dataset['train'][1]
</code></pre>
<p>Access a slice directory:</p>
<pre><code>dataset['train'][:5]
</code></pre>
<p><strong>After executing the above code successfully, I try to execute this here:</strong></p>
<pre><code>new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)
</code></pre>
<p><strong>However, I get this error:</strong></p>
<pre><code>KeyError: &quot;Invalid key: slice(0, 1000, None). Please first select a split. For example: `my_dataset_dictionary['train'][slice(0, 1000, None)]`. Available splits: ['train']&quot;
</code></pre>
<p>How do I fix this?</p>
<p>I am trying to train my own tokenizer, and this seems to be an issue.</p>
<p>Any help would be appreciated!</p>
","16098918","","","","","2021-08-10 12:07:05","Key error when feeding the training corpus to the train_new_from_iterator method","<python><bert-language-model><huggingface-transformers><huggingface-tokenizers><huggingface-datasets>","0","0","","","","CC BY-SA 4.0"
"67058709","1","67138993","","2021-04-12 12:51:18","","2","224","<p>My question here is no how to add new tokens, or how to train using a domain-specific corpus, I'm already doing that.</p>
<p>The thing is, am I supposed to add the domain-specific tokens before the MLM training, or I just let Bert figure out the context? If I choose to not include the tokens, am I going to get a poor task-specific model like NER?</p>
<p>To give you more background of my situation, I'm training a Bert model on medical text using Portuguese language, so, deceased names, drug names, and other stuff are present in my corpus, but I'm not sure I have to add those tokens before the training.</p>
<p>I saw this one: <a href=""https://stackoverflow.com/questions/64816669/using-pretrained-bert-model-to-add-additional-words-that-are-not-recognized-by-t"">Using Pretrained BERT model to add additional words that are not recognized by the model</a></p>
<p>But the doubts remain, as other sources say otherwise.</p>
<p>Thanks in advance.</p>
","7938007","","6664872","","2021-04-14 09:52:50","2021-04-17 14:01:19","BERT - Is that needed to add new tokens to be trained in a domain specific environment?","<nlp><bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","6","","","","CC BY-SA 4.0"
"60463829","1","","","2020-02-29 09:49:46","","9","1277","<p>I am working on a TextClassification problem, for which I am trying to traing my model on TFBertForSequenceClassification given in huggingface-transformers library.</p>

<p>I followed the example given on their <a href=""https://github.com/huggingface/transformers#quick-tour-tf-20-training-and-pytorch-interoperability"" rel=""nofollow noreferrer"">github</a> page, I am able to run the sample code with given sample data using <code>tensorflow_datasets.load('glue/mrpc')</code>.
However, I am unable to find an example on how to load my own custom data and pass it in 
<code>model.fit(train_dataset, epochs=2, steps_per_epoch=115, validation_data=valid_dataset, validation_steps=7)</code>. </p>

<p>How can I define my own X, do tokenization of my X and prepare train_dataset with my X and Y. Where X represents my input text and Y represents classification category of given X.</p>

<p>Sample Training dataframe : </p>

<pre><code>    text    category_index
0   Assorted Print Joggers - Pack of 2 ,/ Gray Pri...   0
1   ""Buckle"" ( Matt ) for 35 mm Width Belt  0
2   (Gagam 07) Barcelona Football Jersey Home 17 1...   2
3   (Pack of 3 Pair) Flocklined Reusable Rubber Ha...   1
4   (Summer special Offer)Firststep new born baby ...   0
</code></pre>
","6517236","","100297","","2020-08-03 01:00:29","2021-07-20 13:17:18","Training TFBertForSequenceClassification with custom X and Y data","<nlp><pytorch><tensorflow2.0><huggingface-transformers><bert-language-model>","4","0","1","","","CC BY-SA 4.0"
"60492839","1","60493083","","2020-03-02 16:20:07","","21","9645","<p>I am using the HuggingFace Transformers package to access pretrained models. As my use case needs functionality for both English and Arabic, I am using the <a href=""https://github.com/google-research/bert/blob/master/multilingual.md"" rel=""noreferrer"">bert-base-multilingual-cased</a> pretrained model. I need to be able to compare the similarity of sentences using something such as cosine similarity. To use  this, I first need to get an embedding vector for each sentence, and can then compute the cosine similarity.</p>

<p>Firstly, what is the best way to extratc the semantic embedding from the BERT model? Would taking the last hidden state of the model after being fed the sentence suffice?</p>

<pre><code>import torch
from transformers import BertModel, BertTokenizer

model_class = BertModel
tokenizer_class = BertTokenizer
pretrained_weights = 'bert-base-multilingual-cased'

tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

sentence = 'this is a test sentence'

input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)])
with torch.no_grad():
    output_tuple = model(input_ids)
    last_hidden_states = output_tuple[0]

print(last_hidden_states.size(), last_hidden_states)
</code></pre>

<p>Secondly, if this is a sufficient way to get embeddings from my sentence, I now have another problem where the embedding vectors have different lengths depending on the length of the original sentence. The shapes output are <code>[1, n, vocab_size]</code>, where <code>n</code> can have any value. </p>

<p>In order to compute two vectors' cosine similarity, they need to be the same  length. How can I do this here? Could something as naive as first summing across <code>axis=1</code> still work? What other options do I have? </p>
","4564080","","4564080","","2020-03-02 16:25:55","2021-03-05 17:24:51","How to compare sentence similarities using embeddings from BERT","<python><vector><nlp><cosine-similarity><huggingface-transformers>","2","0","8","","","CC BY-SA 4.0"
"68624392","1","68893434","","2021-08-02 15:39:25","","0","163","<p>I am trying to train a model using huggingface's wav2vec for audio classification. I keep getting this error:</p>
<pre><code>The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: name, emotion, path.
***** Running training *****
  Num examples = 2708
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 64
  Gradient Accumulation steps = 2
  Total optimization steps = 42
 [ 2/42 : &lt; :, Epoch 0.02/1]
Step    Training Loss   Validation Loss

RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py&quot;, line 61, in _worker
    output = module(*input, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;&lt;ipython-input-81-dd9fe3ea0f13&gt;&quot;, line 77, in forward
    return_dict=return_dict,
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py&quot;, line 1073, in forward
    return_dict=return_dict,
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py&quot;, line 732, in forward
    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py&quot;, line 574, in forward
    hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py&quot;, line 510, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/functional.py&quot;, line 1555, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.17 GiB total capacity; 10.49 GiB already allocated; 11.44 MiB free; 10.68 GiB reserved in total by PyTorch)
</code></pre>
<p>I'm on an AWS ubuntu deep learning AMI ec2.</p>
<p>I've been researching this a lot. I've already tried:</p>
<ul>
<li>reducing the batch size (I want 4, but I've gone down to 1 with no change in error)</li>
<li>adding:
<pre><code>import gc
gc.collect()
torch.cuda.empty_cache()
</code></pre>
</li>
<li>removing all wav files in my dataset that are longer than 6 seconds</li>
</ul>
<p>Is there anything else I can do? I'm on a p2.8xlarge dataset with 105 GiB mounted.</p>
<p>Running <code>torch.cuda.memory_summary(device=None, abbreviated=False) </code> gives me:</p>
<pre><code>|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 3            |        cudaMalloc retries: 4         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |    7550 MB |   10852 MB |  209624 MB |  202073 MB |\n|       from large pool |    7544 MB |   10781 MB |  209325 MB |  201780 MB |\n|       from small pool |       5 MB |      87 MB |     298 MB |     293 MB |\n|---------------------------------------------------------------------------|\n| Active memory         |    7550 MB |   10852 MB |  209624 MB |  202073 MB |\n|       from large pool |    7544 MB |   10781 MB |  209325 MB |  201780 MB |\n|       from small pool |       5 MB |      87 MB |     298 MB |     293 MB |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |   10936 MB |   10960 MB |   63236 MB |   52300 MB |\n|       from large pool |   10928 MB |   10954 MB |   63124 MB |   52196 MB |\n|       from small pool |       8 MB |      98 MB |     112 MB |     104 MB |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |  443755 KB |    1309 MB |  155426 MB |  154992 MB |\n|       from large pool |  443551 KB |    1306 MB |  155081 MB |  154648 MB |\n|       from small pool |     204 KB |      12 MB |     344 MB |     344 MB |\n|---------------------------------------------------------------------------|\n| Allocations           |    1940    |    2622    |   32288    |   30348    |\n|       from large pool |    1036    |    1618    |   21855    |   20819    |\n|       from small pool |     904    |    1203    |   10433    |    9529    |\n|---------------------------------------------------------------------------|\n| Active allocs         |    1940    |    2622    |   32288    |   30348    |\n|       from large pool |    1036    |    1618    |   21855    |   20819    |\n|       from small pool |     904    |    1203    |   10433    |    9529    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |     495    |     495    |    2169    |    1674    |\n|       from large pool |     491    |     491    |    2113    |    1622    |\n|       from small pool |       4    |      49    |      56    |      52    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |     179    |     335    |   15998    |   15819    |\n|       from large pool |     165    |     272    |   12420    |   12255    |\n|       from small pool |      14    |      63    |    3578    |    3564    |\n|===========================================================================|\n'
</code></pre>
<p>After reducing data only to inputs that are less tahn 2 seconds in length, it trains a lot further but still errors with this:</p>
<pre><code>The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path, emotion, name.
***** Running training *****
  Num examples = 1411
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 64
  Gradient Accumulation steps = 2
  Total optimization steps = 22
/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 [11/22 01:12 &lt; 01:28, 0.12 it/s, Epoch 0.44/1]
Step    Training Loss   Validation Loss Accuracy
10  2.428100    2.257138    0.300283
The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path, emotion, name.
***** Running Evaluation *****
  Num examples = 353
  Batch size = 32
Saving model checkpoint to trainingArgs/checkpoint-10
Configuration saved in trainingArgs/checkpoint-10/config.json
Model weights saved in trainingArgs/checkpoint-10/pytorch_model.bin
Configuration saved in trainingArgs/checkpoint-10/preprocessor_config.json
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)
    378             with _open_zipfile_writer(opened_file) as opened_zipfile:
--&gt; 379                 _save(obj, opened_zipfile, pickle_module, pickle_protocol)
    380                 return

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in _save(obj, zip_file, pickle_module, pickle_protocol)
    498         num_bytes = storage.size() * storage.element_size()
--&gt; 499         zip_file.write_record(name, storage.data_ptr(), num_bytes)
    500 

OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-25-3435b262f1ae&gt; in &lt;module&gt;
----&gt; 1 trainer.train()

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1334                     self.control = self.callback_handler.on_step_end(args, self.state, self.control)
   1335 
-&gt; 1336                     self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
   1337                 else:
   1338                     self.control = self.callback_handler.on_substep_end(args, self.state, self.control)

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)
   1441 
   1442         if self.control.should_save:
-&gt; 1443             self._save_checkpoint(model, trial, metrics=metrics)
   1444             self.control = self.callback_handler.on_save(self.args, self.state, self.control)
   1445 

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in _save_checkpoint(self, model, trial, metrics)
   1531         elif self.args.should_save and not self.deepspeed:
   1532             # deepspeed.save_checkpoint above saves model/optim/sched
-&gt; 1533             torch.save(self.optimizer.state_dict(), os.path.join(output_dir, &quot;optimizer.pt&quot;))
   1534             with warnings.catch_warnings(record=True) as caught_warnings:
   1535                 torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, &quot;scheduler.pt&quot;))

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)
    378             with _open_zipfile_writer(opened_file) as opened_zipfile:
    379                 _save(obj, opened_zipfile, pickle_module, pickle_protocol)
--&gt; 380                 return
    381         _legacy_save(obj, opened_file, pickle_module, pickle_protocol)
    382 

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in __exit__(self, *args)
    257 
    258     def __exit__(self, *args) -&gt; None:
--&gt; 259         self.file_like.write_end_of_file()
    260         self.buffer.flush()
    261 

RuntimeError: [enforce fail at inline_container.cc:298] . unexpected pos 1849920000 vs 1849919888
</code></pre>
<p>When I run <code>!free</code> in the notebook, I get:</p>
<pre><code>The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.
              total        used        free      shared  buff/cache   available
Mem:      503392908     6223452   478499292      346492    18670164   492641984
Swap:             0           0           0
</code></pre>
<p>For training code, I am essentially running this colab notebook as an example:
<a href=""https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb#scrollTo=6M8bNvLLJnG1"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb#scrollTo=6M8bNvLLJnG1</a></p>
<p>All that I am changing is the incoming data/labels, which I have intentionally fit into the same directory structure used in the tutorial notebook. The tutorial notebook runs fine for some reason, even though my data has comparable size/num classes.</p>
","11666502","","11666502","","2021-08-02 19:38:05","2021-08-23 13:17:51","Running out of memory with pytorch","<deep-learning><pytorch><huggingface-transformers>","1","6","","","","CC BY-SA 4.0"
"68784423","1","","","2021-08-14 14:56:26","","1","22","<p>I try to use and adapt a notebook based on huggingface models: <em><strong>Text Classification on GLUE</strong></em> (<a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=YZbiBDuGIrId"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=YZbiBDuGIrId</a>)</p>
<p>My goal is to classify a sentence (16 classes predefined).</p>
<p>So I followed the notebook and did. My data looks like below.</p>
<pre><code>id  data     label              langue

0   text_1   label_1            FranÃ§ais
 
0   text_2   label_2            FranÃ§ais

1   text_3   label_3            FranÃ§ais


import pandas as pd
import numpy as np
from datasets import load_dataset, load_metric, DatasetDict, Features, Value, ClassLabel, Dataset
</code></pre>
<p>I have a labeldict like this</p>
<pre><code>{'label_1': 0,
 'label_2': 1,
 ...}

dataset = load_dataset('csv', sep=&quot;|&quot;, data_files={&quot;train&quot; : train_paths, &quot;test&quot; : test_paths})
</code></pre>
<p>output:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['id', 'data', 'label', 'langue'],
        num_rows: ...
    })
    test: Dataset({
        features: ['id', 'data', 'label', 'langue'],
        num_rows: ...
    })
})
</code></pre>
<p>Did all before in the notebook and when I try to do this:</p>
<pre><code>trainer = Trainer(
    model,
    args,
    train_dataset=encoded_dataset[&quot;train&quot;],
    eval_dataset=encoded_dataset[validation_key],
    tokenizer=tokenizer,
    compute_metrics= compute_metrics,
    callbacks=[MLflowCallback()]
)

trainer.train()
</code></pre>
<p>I have the error: <code>The following columns in the training set  don't have a corresponding argument in </code>CamembertForSequenceClassification.forward<code> and have been ignored: langue, id, data.</code> <code>IndexError: tuple index out of range</code></p>
<p>What can I do ?</p>
","14728691","","14728691","","2021-08-14 15:02:16","2021-08-14 15:02:16","CamembertForSequenceClassification : training is not working","<bert-language-model><huggingface-transformers><huggingface-tokenizers><huggingface-datasets>","0","1","","","","CC BY-SA 4.0"
"68729645","1","","","2021-08-10 15:30:00","","0","51","<p>When using GPT2 we can simply pass on the 'labels' parameter to get the loss as follows:</p>
<pre><code>import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2', return_dict=True)

inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs, labels=inputs[&quot;input_ids&quot;])
loss = outputs.loss
</code></pre>
<p>But, not able to find out how to get the same loss in an ONNX inference session. I am using the below code which only returns the 'last_hidden_state':</p>
<pre><code>import onnxruntime as ort

from transformers import GPT2TokenizerFast
#tokenizer = GPT2TokenizerFast.from_pretrained(&quot;gpt2&quot;)

ort_session = ort.InferenceSession(&quot;onnx/gpt2/model.onnx&quot;)

inputs = tokenizer(&quot;Using BERT in ONNX!&quot;, return_tensors=&quot;np&quot;)
outputs = ort_session.run([&quot;last_hidden_state&quot;], dict(inputs))
</code></pre>
","10836319","","220700","","2021-08-13 18:48:01","2021-08-13 18:48:01","How to get the language modeling loss by passing 'labels' while using ONNX inference session?","<pytorch><huggingface-transformers><language-model><onnxruntime><gpt-2>","1","0","","","","CC BY-SA 4.0"
"68732271","1","","","2021-08-10 19:01:33","","0","294","<p>I am trying to run BART language model for a text generation task.</p>
<p>My code was working fine when I used for another encoder-decoder model (T5), but with bart I am getting this error:</p>
<pre><code>File &quot;train_bart.py&quot;, line 89, in train
    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)                                                     cs-lab-host1&quot; 12:39 10-Aug-21
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 1308, in forward
    return_dict=return_dict,
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 1196, in forward
    return_dict=return_dict,
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 985, in forward
    attention_mask, input_shape, inputs_embeds, past_key_values_length
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 866, in _prepare_decoder_attent
ion_mask
    ).to(self.device)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
</code></pre>
<p>And this is where error happens:</p>
<pre><code>for _, data in tqdm(enumerate(loader, 0), total=len(loader), desc='Processing batches..'):
    y = data['target_ids'].to(device, dtype = torch.long)
    y_ids = y[:, :-1].contiguous()
    lm_labels = y[:, 1:].clone().detach()
    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100
    ids = data['source_ids'].to(device, dtype = torch.long)
    mask = data['source_mask'].to(device, dtype = torch.long)

    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)
    loss = outputs[0]
</code></pre>
<p><code>loader</code> is the tokenized and processed data.</p>
","3261292","","","","","2021-08-11 15:29:20","RuntimeError: CUDA error: device-side assert triggered - BART model","<pytorch><huggingface-transformers><language-model>","2","0","","","","CC BY-SA 4.0"
"60624441","1","","","2020-03-10 19:11:48","","0","186","<pre><code>&gt;&gt;&gt; from transformers import GPT2Tokenizer, GPT2Model
&gt;&gt;&gt; model = GPT2Model.from_pretrained(""gpt2"",output_attentions=True)
&gt;&gt;&gt; tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
&gt;&gt;&gt; text = ""a,b,c""
&gt;&gt;&gt; inputs = tokenizer.encode_plus(text,return_tensors='pt',add_special_tokens=True)
&gt;&gt;&gt; input_ids = inputs['input_ids']
&gt;&gt;&gt; attention = model(input_ids)[-1]
&gt;&gt;&gt; attention[0].shape
torch.Size([1, 12, 5, 5])
&gt;&gt;&gt; import transformers
&gt;&gt;&gt; m2 = transformers.AutoModelWithLMHead.from_pretrained(""gpt2"")
&gt;&gt;&gt; at2 = m2(input_ids)[-1]
&gt;&gt;&gt; at2[0].shape
torch.Size([2, 1, 12, 5, 64])
</code></pre>

<p>For your reference, <code>attention</code> is a tuple and <code>attention[0]</code> is for its first layer.</p>

<p>I can map everything except for <code>2</code> in <code>torch.Size([2, 1, 12, 5, 64])</code> vs <code>torch.Size([1, 12, 5, 5])</code>. What does that <code>2</code> mean?</p>

<p>I get these definitions from <code>bertviz</code> github repo:</p>

<pre><code>            attention: list of ``torch.FloatTensor``(one for each layer) of shape
                ``(batch_size(must be 1), num_heads, sequence_length, sequence_length)``
</code></pre>
","734263","","","","","2020-03-10 20:28:52","GPT2Model and GPT2Model with LM Head had different attention weight dimensions","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68781875","1","","","2021-08-14 09:07:33","","0","81","<p>I am encountering an error when training my language model from scratch, having trained a tokenizer beforehand.</p>
<p>I have just trained my tokenizer from scratch on a WordPiece model like BERT, following this notebook: <a href=""https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb"" rel=""nofollow noreferrer"">https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb</a></p>
<p>I then saved my model using this code:</p>
<pre><code>new_tokenizer.save_pretrained(&quot;/content/drive/MyDrive/my-new-tokenizer&quot;)
</code></pre>
<p>Thus, the folder structure of <code>my-new-tokenizer</code> looks something like this:</p>
<pre><code>vocab.txt
tokenizer.json
tokenizer_config.json
special_tokens_map.json
</code></pre>
<p>After training my tokenizer from scratch, I followed the notebook to train a language model from scratch - this notebook: <a href=""https://github.com/huggingface/notebooks/blob/master/examples/language_modeling_from_scratch.ipynb"" rel=""nofollow noreferrer"">https://github.com/huggingface/notebooks/blob/master/examples/language_modeling_from_scratch.ipynb</a></p>
<p>Then executed and used the following code from that notebook:</p>
<pre><code>from datasets import load_dataset

You can replace the dataset above with any dataset hosted on [the hub](https://huggingface.co/datasets) or use your own files. Just uncomment the following cell and replace the paths with values that will lead to your files:

datasets = load_dataset('csv', data_files={'train': ['/content/drive/MyDrive/data.csv'],
                                              'validation': '/content/drive/MyDrive/data.csv'})

You can also load datasets from a csv or a JSON file, see the [full documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) for more information.

To access an actual element, you need to select a split first, then give an index:

datasets[&quot;train&quot;][10]

To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset.

from datasets import ClassLabel
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples &lt;= len(dataset), &quot;Can't pick more elements than there are in the dataset.&quot;
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    display(HTML(df.to_html()))

show_random_elements(datasets[&quot;train&quot;])

As we can see, some of the texts are a full paragraph of a Wikipedia article while others are just titles or empty lines.

## Causal Language modeling

For causal language modeling (CLM) we are going to take all the texts in our dataset and concatenate them after they are tokenized. Then we will split them in examples of a certain sequence length. This way the model will receive chunks of contiguous text that may look like:
</code></pre>
<p>part of text 1</p>
<pre><code>or 
</code></pre>
<p>end of text 1 [BOS_TOKEN] beginning of text 2</p>
<pre><code>depending on whether they span over several of the original texts in the dataset or not. The labels will be the same as the inputs, shifted to the left.

We will use the [`gpt2`](https://huggingface.co/gpt2) architecture for this example. You can pick any of the checkpoints listed [here](https://huggingface.co/models?filter=causal-lm) instead. For the tokenizer, you can replace the checkpoint by the one you trained yourself.

model_checkpoint = &quot;gpt2&quot;
tokenizer_checkpoint = &quot;/content/drive/MyDrive/Train Tokenizer and LM /Tokenizer/my-new-tokenizer&quot;

To tokenize all our texts with the same vocabulary that was used when training the model, we have to download a pretrained tokenizer. This is all done by the `AutoTokenizer` class:

from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

We can now call the tokenizer on all our texts. This is very simple, using the [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method from the Datasets library. First we define a function that call the tokenizer on our texts:

def tokenize_function(examples):
    return tokenizer(examples[&quot;Tweets&quot;])

Then we apply it to all the splits in our `datasets` object, using `batched=True` and 4 processes to speed up the preprocessing. We won't need the `text` column afterward, so we discard it.

tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[&quot;Tweets&quot;])

If we now look at an element of our datasets, we will see the text have been replaced by the `input_ids` the model will need:

tokenized_datasets[&quot;train&quot;][1]

Now for the harder part: we need to concatenate all our texts together then split the result in small chunks of a certain `block_size`. To do this, we will use the `map` method again, with the option `batched=True`. This option actually lets us change the number of examples in the datasets by returning a different number of examples than we got. This way, we can create our new samples from a batch of examples.

First, we grab the maximum length our model was pretrained with. This might be a big too big to fit in your GPU RAM, so here we take a bit less at just 128.

# block_size = tokenizer.model_max_length
block_size = 128

Then we write the preprocessing function that will group our texts:

def group_texts(examples):
    # Concatenate all texts.
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
        # customize this part to your needs.
    total_length = (total_length // block_size) * block_size
    # Split by chunks of max_len.
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result[&quot;labels&quot;] = result[&quot;input_ids&quot;].copy()
    return result

First note that we duplicate the inputs for our labels. This is because the model of the ðŸ¤— Transformers library apply the shifting to the right, so we don't need to do it manually.

Also note that by default, the `map` method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of `block_size` every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:

lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=4,
)

And we can check our datasets have changed: now the samples contain chunks of `block_size` contiguous tokens, potentially spanning over several of our original texts.

tokenizer.decode(lm_datasets[&quot;train&quot;][1][&quot;input_ids&quot;])

Now that the data has been cleaned, we're ready to instantiate our `Trainer`. First we create the model using the same config as our checkpoint, but initialized with random weights:

from transformers import AutoConfig, AutoModelForCausalLM

config = AutoConfig.from_pretrained(model_checkpoint)
model = AutoModelForCausalLM.from_config(config)

And we will needsome `TrainingArguments`:

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    &quot;test-clm&quot;,
    evaluation_strategy = &quot;epoch&quot;,
    learning_rate=2e-5,
    weight_decay=0.01,
)

The last two arguments are to setup everything so we can push the model to the [Hub](https://huggingface.co/models) at the end of training. Remove the two of them if you didn't follow the installation steps at the top of the notebook, otherwise you can change the value of `push_to_hub_model_id` to something you would prefer.

We pass along all of those to the `Trainer` class:

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_datasets[&quot;train&quot;],
    eval_dataset=lm_datasets[&quot;validation&quot;],
)
</code></pre>
<p>And we can train our model:</p>
<pre><code>trainer.train()
</code></pre>
<p><strong>I then get this error:</strong></p>
<pre><code>RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
</code></pre>
<p><strong>I THINK THE ERROR HAS SOMETHING TO DO WITH THE FOLLOWING CODE:</strong></p>
<pre><code>model_checkpoint = &quot;gpt2&quot;
tokenizer_checkpoint = &quot;/content/drive/MyDrive/my-new-tokenizer&quot;
</code></pre>
<p><strong>I TRAINED MY TOKENIZER ON A WORD PIECE MODEL LIKE BERT, SO SHOULD THE MODEL CHECKPOINT BE DIFFERENT?</strong></p>
<p>THANKS!</p>
","16098918","","","","","2021-08-14 09:07:33","RuntimeError: CUDA error - when training my model?","<python><machine-learning><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"66996270","1","","","2021-04-08 01:42:46","","1","183","<p>I'm implementing BART on HuggingFace, see reference: <a href=""https://huggingface.co/transformers/model_doc/bart.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bart.html</a></p>
<p>Here is the code from their documentation that works in creating a generated summary:</p>
<pre><code>from transformers import BartModel, BartTokenizer, BartForConditionalGeneration

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')

def baseBart(ARTICLE_TO_SUMMARIZE):
  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')
  # Generate Summary
  summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=25, early_stopping=True)
  return [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]
</code></pre>
<p>I need to impose conciseness with my summaries so I am setting <code>max_length=25</code>. In doing so though, I'm getting incomplete sentences such as these two examples:</p>
<blockquote>
<p>EX1: The opacity at the left lung base appears stable from prior exam.
There is elevation of the left hemidi</p>
<p>EX 2: There is normal mineralization and alignment. No fracture or
osseous lesion is identified. The ankle mort</p>
</blockquote>
<p>How do I make sure that the predicted summary is only coherent sentences with complete thoughts and remains concise. If possible, I'd prefer to not perform a regex on the summarized output and cut off any text after the last period, but actually have the BART model produce sentences within the the maximum length.</p>
<p>I tried setting <code>truncation=True</code> in the model but that didn't work.</p>
","1744357","","1744357","","2021-04-08 14:38:32","2021-04-08 14:38:32","Limiting BART HuggingFace Model to complete sentences of maximum length","<huggingface-transformers><summarization>","0","0","1","","","CC BY-SA 4.0"
"60530393","1","","","2020-03-04 16:18:38","","0","305","<p>i tried to train a NER model with GilBERTo (tokenizer and Model) the italian version of RoBERTa (from CamemBert), but i have the error below, 
someone could help me to understand it and maybe a possible solution?</p>

<p>the solution replicate the: <a href=""https://github.com/monologg/R-BERT"" rel=""nofollow noreferrer"">GitHub: R-BERT entity relation</a>
(I just change the utils.py with: </p>

<pre><code>def load_tokenizer(args):
    tokenizer = AutoTokenizer.from_pretrained(""idb-ita/gilberto-uncased-from-camembert"", do_lower_case=True)
    #tokenizer = MODEL_CLASSES[args.model_type][2].from_pretrained(args.model_name_or_path)
    tokenizer.add_special_tokens({""additional_special_tokens"": ADDITIONAL_SPECIAL_TOKENS})
    return tokenizer
</code></pre>

<p>)
and then i use model ""roberta"" in main.py with just change the Tokenizer to GilBERTo. And i used a different Dataset with the same format like the original project.
Note: i disable the GPU for size problem in cache upload, and i use transformers 2.4.1
thanks a lot!</p>

<pre><code>03/04/2020 16:33:01 - INFO - trainer -   ***** Config loaded *****
03/04/2020 16:33:01 - INFO - transformers.modeling_utils -   loading weights file ./model\pytorch_model.bin
03/04/2020 16:33:01 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at C:\Users\ADMIN\.cache\torch\transformers\228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
Traceback (most recent call last):
  File ""C:\Users\ADMIN\R-BERT\trainer.py"", line 196, in load_model
    self.model = self.model_class.from_pretrained(self.args.model_dir, config=self.bert_config, args=self.args)
  File ""C:\Users\ADMIN\Miniconda3\envs\envTransformers\lib\site-packages\transformers\modeling_utils.py"", line 463, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File ""C:\Users\ADMIN\R-BERT\model.py"", line 31, in __init__
    self.bert = PRETRAINED_MODEL_MAP[args.model_type].from_pretrained(args.model_name_or_path, config=bert_config)  # Load pretrained bert
  File ""C:\Users\ADMIN\Miniconda3\envs\envTransformers\lib\site-packages\transformers\modeling_utils.py"", line 555, in from_pretrained
    model.__class__.__name__, ""\n\t"".join(error_msgs)
RuntimeError: Error(s) in loading state_dict for RobertaModel:
        size mismatch for roberta.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50265, 768]) from checkpoint, the shape in current model is torch.Size([30522, 768]).
        size mismatch for roberta.embeddings.position_embeddings.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).
        size mismatch for roberta.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 60, in &lt;module&gt;
    trainer.load_model()
  File ""C:\Users\ADMIN\R-BERT\trainer.py"", line 200, in load_model
    raise Exception(""Some model files might be missing..."")
Exception: Some model files might be missing...
</code></pre>
","12878375","","12878375","","2020-03-06 17:12:54","2020-03-06 17:12:54","GilBERTo (version Italian) from RoBERTa","<python><transformer><ner><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"68691450","1","","","2021-08-07 10:19:53","","2","48","<p>This question is the same with <a href=""https://datascience.stackexchange.com/questions/99815/how-can-i-check-a-confusion-matrix-after-fine-tuning-with-custom-datasets"">How can I check a confusion_matrix after fine-tuning with custom datasets?</a>, on Data Science Stack Exchange.</p>
<h2>Background</h2>
<p>I would like to check a confusion_matrix, including precision, recall, and f1-score like below after fine-tuning with custom datasets.</p>
<p>Fine tuning process and the task are <a href=""https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews"" rel=""nofollow noreferrer"">Sequence Classification with IMDb Reviews</a> on the <a href=""https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainer"" rel=""nofollow noreferrer"">Fine-tuning with custom datasets tutorial on Hugging face</a>.</p>
<p>After finishing the fine-tune with Trainer, how can I check a confusion_matrix in this case?</p>
<p>An image of confusion_matrix, including precision, recall, and f1-score <a href=""http://www.renom.jp/notebooks/product/renom_dl/trainer/notebook.html"" rel=""nofollow noreferrer"">original site</a>: just for example output image</p>
<pre><code>predictions = np.argmax(trainer.test(test_x), axis=1)

# Confusion matrix and classification report.
print(classification_report(test_y, predictions))

            precision    recall  f1-score   support

          0       0.75      0.79      0.77      1000
          1       0.81      0.87      0.84      1000
          2       0.63      0.61      0.62      1000
          3       0.55      0.47      0.50      1000
          4       0.66      0.66      0.66      1000
          5       0.62      0.64      0.63      1000
          6       0.74      0.83      0.78      1000
          7       0.80      0.74      0.77      1000
          8       0.85      0.81      0.83      1000
          9       0.79      0.80      0.80      1000

avg / total       0.72      0.72      0.72     10000
</code></pre>
<h2>Code</h2>
<pre class=""lang-py prettyprint-override""><code>from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

model = DistilBertForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)

trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()
</code></pre>
<h2>What I did so far</h2>
<p>Data set Preparation for <a href=""https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews"" rel=""nofollow noreferrer"">Sequence Classification with IMDb Reviews</a>, and I'm fine-tuning with Trainer.</p>
<pre><code>from pathlib import Path

def read_imdb_split(split_dir):
    split_dir = Path(split_dir)
    texts = []
    labels = []
    for label_dir in [&quot;pos&quot;, &quot;neg&quot;]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is &quot;neg&quot; else 1)

    return texts, labels

train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')

from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)

from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

import torch

class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = IMDbDataset(train_encodings, train_labels)
val_dataset = IMDbDataset(val_encodings, val_labels)
test_dataset = IMDbDataset(test_encodings, test_labels)
</code></pre>
","14497686","","4685471","","2021-08-07 11:00:25","2021-08-26 06:40:48","How can I check a confusion_matrix after fine-tuning with custom datasets?","<python-3.x><machine-learning><nlp><huggingface-transformers><transformer>","1","1","","","","CC BY-SA 4.0"
"64208317","1","","","2020-10-05 12:06:21","","0","26","<p>I'm playing with a RAG example from facebook (huggingface) <a href=""https://huggingface.co/facebook/rag-token-nq#usage"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/rag-token-nq#usage</a>.</p>
<p>Here a very nice explanation of it: <a href=""https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/"" rel=""nofollow noreferrer"">https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/</a></p>
<p>The code is very simple but the dataset it downloads in this step is a little big (75GB):</p>
<pre><code>retriever = RagRetriever.from_pretrained(&quot;facebook/rag-token-nq&quot;, index_name=&quot;exact&quot;, use_dummy_dataset=True)
</code></pre>
<p>It downloads the dataset in <code>/root/.cache/huggingface/datasets/</code>, something that I'd like to change if possible. This is the output of that line of code is:</p>
<pre><code>Downloading and preparing dataset wiki_dpr/psgs_w100.nq.no_index (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wiki_dpr/
</code></pre>
<p>My question is: how I can change the folder to where download the dataset used by RagRetriever.from_pretrained (the 75GB one) to another one different to <code>root/.cache/huggingface/datasets/ .</code></p>
<p>Thanks!.</p>
","3918511","","","","","2020-10-05 12:06:21","How to download the pretrained dataset of huggingface RagRetriever to a custom directory","<pytorch><huggingface-transformers><huggingface-tokenizers>","0","3","","2020-10-05 14:30:24","","CC BY-SA 4.0"
"60459292","1","60480429","","2020-02-28 21:11:27","","0","373","<p>I am processing a batch of sentences with different lengths, so I am planning to take advantage of the padding + attention_mask functionality in gpt2 for that. </p>

<p>At the same time, for each sentence I need to add a suffix phrase and run N different inferences. For instance, given the sentence ""I like to drink coke"", I may need to run two different inferences: ""I like to drink coke. Coke is good"" and ""I like to drink coke. Drink is good"". Thus, I am trying to improve the inference time for this by using the ""past"" functionality: <a href=""https://huggingface.co/transformers/quickstart.html#using-the-past"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/quickstart.html#using-the-past</a> so I just process the original sentence (e.g. ""I like to drink coke"") once, and then I somehow expand the result to be able to be used with two other sentences: ""Coke is good"" and ""Drink is good"". </p>

<p>Below you will find a simple code that is trying to represent how I was trying to do this. For simplicity I'm just adding a single suffix phrase per sentence (...but I still hope my original idea is possible though):</p>

<pre><code>from transformers.tokenization_gpt2 import GPT2Tokenizer
from transformers.modeling_gpt2 import GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='&lt;|endoftext|&gt;')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Complete phrases are: ""I like to drink soda without sugar"" and ""Go watch TV alone, I am not going""
docs = [""I like to drink soda"", ""Go watch TV""]
docs_tensors = tokenizer.batch_encode_plus(
    [d for d in docs], pad_to_max_length=True, return_tensors='pt')

docs_next = [""without sugar"", ""alone, I am not going""]
docs_next_tensors = tokenizer.batch_encode_plus(
    [d for d in docs_next], pad_to_max_length=True, return_tensors='pt')

# predicting the first part of each phrase
_, past = model(docs_tensors['input_ids'], attention_mask=docs_tensors['attention_mask'])

# predicting the rest of the phrase
logits, _ = model(docs_next_tensors['input_ids'], attention_mask=docs_next_tensors['attention_mask'], past=past)
logits = logits[:, -1]
_, top_indices_results = logits.topk(30)
</code></pre>

<p>The error I am getting is the following:</p>

<pre><code>Traceback (most recent call last):
  File ""/Applications/PyCharm CE.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py"", line 1434, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Applications/PyCharm CE.app/Contents/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/damiox/Workspace/xxLtd/yy/stress-test-withpast2.py"", line 26, in &lt;module&gt;
    logits, _ = model(docs_next_tensors['input_ids'], attention_mask=docs_next_tensors['attention_mask'], past=past)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/transformers/modeling_gpt2.py"", line 593, in forward
    inputs_embeds=inputs_embeds,
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/transformers/modeling_gpt2.py"", line 476, in forward
    hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/transformers/modeling_gpt2.py"", line 226, in forward
    self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/transformers/modeling_gpt2.py"", line 189, in forward
    attn_outputs = self._attn(query, key, value, attention_mask, head_mask)
  File ""/Users/damiox/.local/share/virtualenvs/yy-uMxmjV2h/lib/python3.7/site-packages/transformers/modeling_gpt2.py"", line 150, in _attn
    w = w + attention_mask
RuntimeError: The size of tensor a (11) must match the size of tensor b (6) at non-singleton dimension 3

Process finished with exit code 1
</code></pre>

<p>Initially I thought this was related to <a href=""https://github.com/huggingface/transformers/issues/3031"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/3031</a> - so I re-built latest master to try the fix, but I still experience the issue. </p>
","881660","","881660","","2020-02-28 21:30:46","2020-03-01 22:35:41","Using past and attention_mask at the same time for gpt2","<python><pytorch><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"63785319","1","63791977","","2020-09-07 23:23:32","","6","2720","<p>I'm following a <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb"" rel=""nofollow noreferrer"">PyTorch tutorial</a> which uses the BERT NLP model (feature extractor) from the Huggingface Transformers library. There are two pieces of interrelated code for gradient updates that I don't understand.</p>
<p>(1) <code>torch.no_grad()</code></p>
<p>The tutorial has a class where the <code>forward()</code> function creates a <code>torch.no_grad()</code> block around a call to the BERT feature extractor, like this:</p>
<pre class=""lang-py prettyprint-override""><code>bert = BertModel.from_pretrained('bert-base-uncased')

class BERTGRUSentiment(nn.Module):
    
    def __init__(self, bert):
        super().__init__()
        self.bert = bert
        
    def forward(self, text):
        with torch.no_grad():
            embedded = self.bert(text)[0]
</code></pre>
<p>(2) <code>param.requires_grad = False</code></p>
<p>There is another portion in the same tutorial where the BERT parameters are frozen.</p>
<pre class=""lang-py prettyprint-override""><code>for name, param in model.named_parameters():                
    if name.startswith('bert'):
        param.requires_grad = False
</code></pre>
<p><strong>When would I need (1) and/or (2)?</strong></p>
<ul>
<li>If I want to train with a frozen BERT, would I need to enable both?</li>
<li>If I want to train to let BERT be updated, would I need to disable both?</li>
</ul>
<p>Additionaly, I ran all four combinations and found:</p>
<pre><code>   with torch.no_grad   requires_grad = False  Parameters  Ran
   ------------------   ---------------------  ----------  ---
a. Yes                  Yes                      3M        Successfully
b. Yes                  No                     112M        Successfully
c. No                   Yes                      3M        Successfully
d. No                   No                     112M        CUDA out of memory
</code></pre>
<p><strong>Can someone please explain what's going on?</strong> Why am I getting <code>CUDA out of memory</code> for (d) but not (b)? Both have 112M learnable parameters.</p>
","4561314","","4561314","","2020-09-08 16:54:24","2020-09-08 16:54:24","PyTorch torch.no_grad() versus requires_grad=False","<python><machine-learning><pytorch><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68738894","1","","","2021-08-11 08:53:55","","0","30","<p>I am trying to use the pipeline function from the <code>transformers</code> package. It says in the documentation that <code>feature_extractor()</code> is an optional parameter and when I call a pre-trained DEIT model I continually get this error message despite not calling the feature extractors.</p>
<p>For context, I am trying to build a pipeline so that I am able to use a pre-trained image transformer for data augmentation.</p>
<pre><code>pipeline('feature-extraction', model = 'facebook/deit-small-distilled-patch16-224')
</code></pre>
<pre><code>Some weights of the model checkpoint at facebook/deit-small-distilled-patch16-224 were not used when initializing DeiTModel: ['distillation_classifier.bias', 'cls_classifier.weight', 'distillation_classifier.weight', 'cls_classifier.bias']
- This IS expected if you are initializing DeiTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DeiTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DeiTModel were not initialized from the model checkpoint at facebook/deit-small-distilled-patch16-224 and are newly initialized: ['deit.pooler.dense.weight', 'deit.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-104-12087829eb1f&gt; in &lt;module&gt;()
----&gt; 1 pipeline('feature-extraction', model = 'facebook/deit-small-distilled-patch16-224', feature_extractor='facebook/deit-small-distilled-patch16-224')

/usr/local/lib/python3.7/dist-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)
    507         kwargs[&quot;feature_extractor&quot;] = feature_extractor
    508 
--&gt; 509     return task_class(model=model, framework=framework, task=task, **kwargs)

TypeError: __init__() got an unexpected keyword argument 'feature_extractor'
</code></pre>
","13388123","","","","","2021-08-11 08:53:55","__init__() got an unexpected keyword argument 'feature_extractor'","<python><tensorflow><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"60551906","1","60567136","","2020-03-05 18:29:46","","0","816","<p>I'm running a code using Tensorflow's BERT in HuggingFace's transformers based on this tutorial:</p>

<p><a href=""https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/"" rel=""nofollow noreferrer"">Text Classification with BERT Tokenizer and TF 2.0 in Python</a></p>

<p>However, instead of creating my own neural net, I'm using transformers and:</p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model0 = TFBertForSequenceClassification.from_pretrained('bert-base-cased')
</code></pre>

<p>I am able to generate the following data for training:</p>

<pre><code>(&lt;tf.Tensor: id=6582, shape=(20, 70), dtype=int32, numpy=
 array([[  191, 19888,  1186,     0, ...,     0,     0,     0,     0],
        [ 7353,  1200,  2180,  1197, ...,     0,     0,     0,     0],
        [  164,   112, 12890,  5589, ...,     0,     0,     0,     0],
        [  164,   112, 21718, 19009, ...,     0,     0,     0,     0],
        ...,
        [ 7998,  3101,   164,   112, ...,     0,     0,     0,     0],
        [  164,   112,   154,  4746, ...,     0,     0,     0,     0],
        [  164,   112,  1842, 23228, ...,  1162,   112,   166,     0],
        [  164,   112,   140,  3814, ...,  7443,   119,   112,   166]], dtype=int32)&gt;,
 &lt;tf.Tensor: id=6583, shape=(20,), dtype=int32, numpy=array([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=int32)&gt;)
</code></pre>

<p>But as long as I've seen, there must exist a problem with vocabulary file, that is not defined. I also get the following warning when running:</p>

<pre><code>train2=[]
for i in range(0,train.shape[0]):
    out=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(str(train.iloc[i,1])))
    print(i)
    train2.append(out)

WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (6935 &gt; 512). Running this sequence through the model will result in indexing errors
WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (3574 &gt; 512). Running this sequence through the model will result in indexing errors
</code></pre>

<p>The <code>model0</code> is successfully created:</p>

<pre><code>Model: ""tf_bert_for_sequence_classification""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
bert (TFBertMainLayer)       multiple                  108310272 
_________________________________________________________________
dropout_37 (Dropout)         multiple                  0         
_________________________________________________________________
classifier (Dense)           multiple                  1538      
=================================================================
Total params: 108,311,810
Trainable params: 108,311,810
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<p>Then on:</p>

<pre><code>model0.fit(train_data, epochs=2, steps_per_epoch=30,validation_data=test_data, validation_steps=7)
</code></pre>

<p>I get the following error:</p>

<pre><code>Train for 1 steps
Epoch 1/2
1/1 [==============================] - 21s 21s/step
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-53-61d611c37004&gt; in &lt;module&gt;
----&gt; 1 history = model0.fit(train_data, epochs=2, steps_per_epoch=1)#,validation_data=test_data, validation_steps=7)

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--&gt; 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--&gt; 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--&gt; 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---&gt; 86                               distributed_function(input_fn))
     87 
     88   return execution_function

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--&gt; 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    518         # Lifting succeeded, so variables are initialized and we can run the
    519         # stateless function.
--&gt; 520         return self._stateless_fn(*args, **kwds)
    521     else:
    522       canon_args, canon_kwds = \

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   1821     """"""Calls a graph function specialized to the inputs.""""""
   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-&gt; 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824 
   1825   @property

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-&gt; 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-&gt; 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--&gt; 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---&gt; 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~/.local/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  indices[0,624] = 624 is not in [0, 512)
     [[node tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embedding_lookup (defined at /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
  (1) Invalid argument:  indices[0,624] = 624 is not in [0, 512)
     [[node tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embedding_lookup (defined at /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
     [[GroupCrossDeviceControlEdges_0/Adam/Adam/Const/_867]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_function_36559]

Function call stack:
distributed_function -&gt; distributed_function
</code></pre>

<p>My data consists of a column of 2 classes and the other column are phrases.</p>

<p>What can I do ?</p>
","6901690","","6901690","","2020-03-05 19:10:18","2020-03-06 15:28:47","Tensorflow - HuggingFace - Invalid argument: indices[0,624] = 624 is not in [0, 512)","<tensorflow><huggingface-transformers>","1","6","","","","CC BY-SA 4.0"
"54873721","1","","","2019-02-25 19:55:37","","5","2436","<p>I have been using the PyTorch implementation of Google's <a href=""https://github.com/google-research/bert#fine-tuning-with-bert"" rel=""nofollow noreferrer"">BERT</a> by <a href=""https://github.com/huggingface/pytorch-pretrained-BERT"" rel=""nofollow noreferrer"">HuggingFace</a> for the MADE 1.0 dataset for quite some time now. Up until last time (11-Feb), I had been using the library and getting an <strong>F-Score</strong> of <strong>0.81</strong> for my Named Entity Recognition task by Fine Tuning the model. But this week when I ran the exact same code which had compiled and run earlier, it threw an error when executing this statement:</p>

<pre><code>input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=""long"", truncating=""post"", padding=""post"")
</code></pre>

<blockquote>
  <p>ValueError: Token indices sequence length is longer than the specified
  maximum  sequence length for this BERT model (632 > 512). Running this
  sequence through BERT will result in indexing errors</p>
</blockquote>

<p>The full code is available in this <a href=""https://colab.research.google.com/drive/1JxWdw1BjXZCFC2a8IwtZxvvq4rFGcxas"" rel=""nofollow noreferrer"">colab notebook</a>.</p>

<p>To get around this error I modified the above statement to the one below by taking the first 512 tokens of any sequence and made the necessary changes to add the index of [SEP] to the end of the truncated/padded sequence as required by BERT.</p>

<pre><code>input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt[:512]) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=""long"", truncating=""post"", padding=""post"")
</code></pre>

<p>The result shouldn't have changed because I am only considering the first 512 tokens in the sequence and later truncating to 75 as my (MAX_LEN=75) but my <strong>F-Score</strong> has dropped to <strong>0.40</strong> and my <strong>precision</strong> to <strong>0.27</strong> while the <strong>Recall</strong> remains the same <strong>(0.85)</strong>. I am unable to share the dataset as I have signed a confidentiality clause but I can assure all the preprocessing as required by BERT has been done and all extended tokens like (Johanson --> Johan ##son) have been tagged with X and replaced later after the prediction as said in the <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">BERT Paper</a>.</p>

<p>Has anyone else faced a similar issue or can elaborate on what might be the issue or what changes the PyTorch (Huggingface) people have done on their end recently?</p>
","11115418","","1150683","","2020-01-16 08:57:14","2020-02-23 07:10:23","PyTorch Huggingface BERT-NLP for Named Entity Recognition","<python><nlp><data-science><named-entity-recognition><huggingface-transformers>","2","0","1","","","CC BY-SA 4.0"
"62014720","1","","","2020-05-26 04:32:29","","1","524","<p>When I'm trying to evaluate GPT2 model for text generation task, I printed loss and perplexity as given below in the code, but it is not defined in the code by me.</p>

<pre><code>with torch.no_grad():

    for _ in range(length):
        outputs = model(generated)
        next_token_logits = outputs[0][:, -1, :] 
        next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1) 

        generated = torch.cat((generated, next_token), dim=1)

print(f""loss {loss}"")
print(f""perplexity {perplexity}"")

...
</code></pre>

<p>part from the output:</p>

<pre><code>loss 3.644557237625122
perplexity 38.26582717895508
</code></pre>

<p>This prints out very realistic values for loss and perplexity and I'm not sure from where this is coming from, because if I change it to loss1 and perplexity1 it would give me this error</p>

<blockquote>
<pre><code>     38 
---&gt; 39 print(f""loss {loss1}"")
     40 print(f""perplexity {perplexity}"")
     41 
NameError: name 'loss1' is not defined
</code></pre>
</blockquote>

<p>Although here, for every generation, the loss and the perplexity will be the same.</p>

<p>So, I'm so confused as to how this is calculated automatically and would like some clarification since I'm very new to Pytorch and ML. Any help is appreciated. </p>
","10598769","","","","","2020-05-26 04:32:29","calculating loss and perplexity when evaluating GPT2 model even when not defined","<python><nlp><pytorch><huggingface-transformers>","0","3","1","","","CC BY-SA 4.0"
"62064368","1","","","2020-05-28 12:10:16","","1","640","<p>I followed the isntructions to fine-tune the pretrained BERT model with a customized corpus, as shown here <a href=""https://github.com/huggingface/transformers/tree/master/examples/language-modeling"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/language-modeling</a>. During the process of training, there are checkpoints generated under the specified output_dir, however, when I wanted to continue training from one of the checkpoints (--model_name_or_path=/path-to-ckpt/), it returns the traceback that</p>

<pre><code>Traceback (most recent call last):
  File ""run_language_modeling.py"", line 277, in &lt;module&gt;
    main()
  File ""run_language_modeling.py"", line 186, in main
    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)
  File ""H:\Anaconda3\envs\env_name\lib\site-packages\transformers\tokenization_auto.py"", line 203, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File ""H:\Anaconda3\envs\env_name\lib\site-packages\transformers\tokenization_utils.py"", line 902, in from_pretrained
    return cls._from_pretrained(*inputs, **kwargs)
  File ""H:\Anaconda3\envs\env_name\lib\site-packages\transformers\tokenization_utils.py"", line 1007, in _from_pretrained
    list(cls.vocab_files_names.values()),
OSError: Model name 'C:\\path-to-ckpt\\checkpoint-17500' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed 'C:\\path-to-ckpt\\checkpoint-17500' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.
</code></pre>

<p>As I comprehend, there is a mismatch between the files saved in the checkpoint directory and the files needed to load the model, furthermore, it could also be that there is no explicit argument to specify continue training from checkpoints. However, I am not sure whether I complicated things or there is alternative way to do so.</p>
","9873133","","","","","2020-05-28 12:10:16","Continue fine-tuning from saved checkpoints for run_language_modeling.py","<huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"60539758","1","61757072","","2020-03-05 06:57:57","","3","1752","<p>(also posted in <a href=""https://github.com/dmis-lab/biobert/issues/98"" rel=""nofollow noreferrer"">https://github.com/dmis-lab/biobert/issues/98</a>)</p>

<p>Hi, does anyone know how to load biobert as a keras layer using the huggingface transformers (version 2.4.1)? I tried several possibilities but none of these worked. All that I found out is how to use the pytorch version but I am interested in the keras layer version. Below are two of my attempts (I saved the biobert files into folder ""biobert_v1.1_pubmed"").</p>

<h3>Attempt 1:</h3>

<pre><code>biobert_model = TFBertModel.from_pretrained('bert-base-uncased')
biobert_model.load_weights('biobert_v1.1_pubmed/model.ckpt-1000000')
</code></pre>

<p>Error message:</p>

<pre><code>AssertionError: Some objects had attributes which were not restored:
    : ['tf_bert_model_4/bert/embeddings/word_embeddings/weight']
    : ['tf_bert_model_4/bert/embeddings/position_embeddings/embeddings']
   (and many more lines like above...)
</code></pre>

<h3>Attempt 2:</h3>

<pre><code>biobert_model = TFBertModel.from_pretrained(""biobert_v1.1_pubmed/model.ckpt-1000000"", config='biobert_v1.1_pubmed/bert_config.json')
</code></pre>

<p>Error message:</p>

<pre><code>NotImplementedError: Weights may only be loaded based on topology into Models when loading TensorFlow-formatted weights (got by_name=True to load_weights).
</code></pre>

<p>Any help appreciated! My experience with huggingface's transformers library is almost zero. I also tried to load the following two models but it seems they only support the pytorch version.</p>

<ul>
<li><a href=""https://huggingface.co/monologg/biobert_v1.1_pubmed"" rel=""nofollow noreferrer"">https://huggingface.co/monologg/biobert_v1.1_pubmed</a></li>
<li><a href=""https://huggingface.co/adamlin/NCBI_BERT_pubmed_mimic_uncased_base_transformers"" rel=""nofollow noreferrer"">https://huggingface.co/adamlin/NCBI_BERT_pubmed_mimic_uncased_base_transformers</a></li>
</ul>
","13010579","","13010579","","2020-03-11 07:42:46","2020-05-12 16:37:15","biobert for keras version of huggingface transformers","<keras><nlp><keras-layer><huggingface-transformers>","1","4","2","","","CC BY-SA 4.0"
"63538113","1","","","2020-08-22 15:48:04","","0","315","<p>I am playing around with an NLP problem (sentence classification) and decided to use HuggingFace's TFBertModel along with Conv1D, Flatten, and Dense layers. I am using the functional API and my model compiles. However, during model.fit(), I get a shape error at the output Dense layer.</p>
<p>Model definition:</p>
<pre><code># Build model with a max length of 50 words in a sentence
max_len = 50
def build_model():
    bert_encoder = TFBertModel.from_pretrained(model_name)
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_mask&quot;)
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_type_ids&quot;)
    
    # Create a conv1d model. The model may not really be useful or make sense, but that's OK (for now).
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    conv_layer = tf.keras.layers.Conv1D(32, 3, activation='relu')(embedding)
    dense_layer = tf.keras.layers.Dense(24, activation='relu')(conv_layer)
    flatten_layer = tf.keras.layers.Flatten()(dense_layer)
    output_layer = tf.keras.layers.Dense(3, activation='softmax')(flatten_layer)
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output_layer)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', 
                  metrics=['accuracy'])
    return model

# View model architecture
model = build_model()
model.summary()

Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_word_ids (InputLayer)     [(None, 50)]         0                                            
__________________________________________________________________________________________________
input_mask (InputLayer)         [(None, 50)]         0                                            
__________________________________________________________________________________________________
input_type_ids (InputLayer)     [(None, 50)]         0                                            
__________________________________________________________________________________________________
tf_bert_model (TFBertModel)     ((None, 50, 768), (N 177853440   input_word_ids[0][0]             
                                                                 input_mask[0][0]                 
                                                                 input_type_ids[0][0]             
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 48, 32)       73760       tf_bert_model[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 48, 24)       792         conv1d[0][0]                     
__________________________________________________________________________________________________
flatten (Flatten)               (None, 1152)         0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 3)            3459        flatten[0][0]                    
==================================================================================================
Total params: 177,931,451
Trainable params: 177,931,451
Non-trainable params: 0
__________________________________________________________________________________________________

# Fit model on input data
model.fit(train_input, train['label'].values, epochs = 3, verbose = 1, batch_size = 16, 
          validation_split = 0.2)
</code></pre>
<p>And this is the error message:</p>
<blockquote>
<p>ValueError: Input 0 of layer dense_1 is incompatible with the layer: expected axis -1 of input shape to have value 1152 but received
input with shape [16, 6168]</p>
</blockquote>
<p>I am unable to understand how the input shape to layer dense_1 (the output dense layer) can be 6168? As per the model summary, it should always be 1152.</p>
","2396316","","","","","2020-08-22 16:31:28","TensorFlow input shape error at Dense output layer is contradictory to what model.summary() says","<python><tensorflow><tf.keras><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"62069350","1","62141748","","2020-05-28 16:08:25","","0","211","<p>I'm trying to finetune the pretrained Transformer-XL model <code>transfo-xl-wt103</code> for a language modeling task. Therfore, I use the model class <code>TransfoXLLMHeadModel</code>.</p>

<p>To iterate over my dataset I use the <code>LMOrderedIterator</code> from the file <a href=""https://github.com/huggingface/transformers/blob/5e737018e1fcb22c8b76052058279552a8d6c806/src/transformers/tokenization_transfo_xl.py#L467"" rel=""nofollow noreferrer"">tokenization_transfo_xl.py</a> which yields a tensor with the <code>data</code> and its <code>target</code> for each batch (and the sequence length).</p>

<p>Let's assume the following data with <code>batch_size = 1</code> and <code>bptt = 8</code>:</p>

<pre><code>data = tensor([[1,2,3,4,5,6,7,8]])
target = tensor([[2,3,4,5,6,7,8,9]])
mems # from the previous output
</code></pre>

<p><strong>My question is:</strong> I currently pass this data into the model like this:</p>

<pre><code>output = model(input_ids=data, labels=target, mems=mems)
</code></pre>

<p>Is this correct?</p>

<p>I am wondering because the documentation says for the <code>labels</code> parameter:</p>

<blockquote>
  <p>labels (:obj:<code>torch.LongTensor</code> of shape :obj:<code>(batch_size, sequence_length)</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
              Labels for language modeling.
              Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set <code>lm_labels = input_ids</code></p>
</blockquote>

<p>So what is it about the parameter <code>lm_labels</code>? I only see <code>labels</code> defined in the <code>forward</code> method.</p>

<p>And when the labels ""are shifted"" inside the model, does this mean I have to pass <code>data</code> twice (additionally instead of <code>targets</code>) because its shifted inside? But how does the model then know the next token to predict?</p>

<p>I also read through <a href=""https://github.com/huggingface/transformers/issues/3711"" rel=""nofollow noreferrer"">this bug</a> and the fix in <a href=""https://github.com/huggingface/transformers/pull/3716"" rel=""nofollow noreferrer"">this pull request</a> but I don't quite understand how to treat the model now (before vs. after fix)</p>

<p>Thanks in advance for some help!</p>

<p><strong>Edit</strong>: <a href=""https://github.com/huggingface/transformers/issues/4698"" rel=""nofollow noreferrer"">Link</a> to issue on Github</p>
","9478384","","9478384","","2020-06-03 05:34:45","2020-06-03 05:34:45","Transformer-XL: Input and labels for Language Modeling","<huggingface-transformers><language-model>","1","0","","","","CC BY-SA 4.0"
"60513592","1","64732429","","2020-03-03 18:37:00","","2","2654","<p>I see there exits two configs of the T5model - <strong>T5Model</strong> and <strong>TFT5WithLMHeadModel</strong>. I want to test this for translation tasks (eg. en-de) as they have shown in the google's original repo. Is there a way I can use this model from hugging face to test out translation tasks. I did not see any examples related to this on the documentation side and was wondering how to provide the input and get the results. </p>

<p>Any help appreciated</p>
","5165216","","","","","2020-11-07 21:07:14","How to use huggingface T5 model to test translation task?","<python-3.x><tensorflow2.0><huggingface-transformers>","2","3","","","","CC BY-SA 4.0"
"61979944","1","","","2020-05-23 23:42:20","","3","2462","<p>I am using arabic Bert and passing my training datasets as batches, the error message that I get is that Tensor can not be treated as numpy array but could not detach to numpy using detach().numpy()</p>

<pre><code>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
    outputs = self.distribute_strategy.run(
&lt;ipython-input-9-271b41658d5b&gt;:46 call  *
    x = self.embed_with_bert(inputs)
&lt;ipython-input-9-271b41658d5b&gt;:39 embed_with_bert  *
    embds = self.bert_layer(all_tokens[:,0,:].detach().numpy(),

AttributeError: 'Tensor' object has no attribute 'detach'
</code></pre>

<p>also when I change the tokenizer to 'tf'</p>

<p>I get the following error:</p>

<pre><code>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
    outputs = self.distribute_strategy.run(
&lt;ipython-input-17-b2127b5212bc&gt;:46 call  *
    x = self.embed_with_bert(inputs)
&lt;ipython-input-53-b99c90611f94&gt;:39 embed_with_bert  *
    embds = self.bert_layer([all_tokens[:,0,:],
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:550 __call__  *
    result = self.forward(*input, **kwargs)
/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py:691 forward  *
    input_shape = input_ids.size()

AttributeError: 'list' object has no attribute 'size'
</code></pre>

<p>tried this also</p>

<pre><code>embds = self.bert_layer(tf.unstack(all_tokens[:,0,:]),
                            tf.unstack(all_tokens[:,1,:]),
                            tf.unstack(all_tokens[:,2,:])) 
</code></pre>

<p>but it generated:</p>

<pre><code>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
    outputs = self.distribute_strategy.run(
&lt;ipython-input-64-e1ff853b33fb&gt;:55 call  *
    x = self.embed_with_bert(inputs)
&lt;ipython-input-29-d3665857b399&gt;:44 embed_with_bert  *
    embds = self.bert_layer(tf.unstack(all_tokens[:,0,:]),tf.unstack(all_tokens[:,1,:]),tf.unstack(all_tokens[:,2,:])) #[:,0,:],all_tokens[:,1,:],all_tokens[:,2,:])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1510 unstack  **
    raise ValueError(""Cannot infer num from shape %s"" % value_shape)

ValueError: Cannot infer num from shape (None, None)
</code></pre>

<p>Example: --> all_tokens are tensor sent as batch to the model but now I want to use it as numpy array to fix the error?</p>

<pre><code>embds = self.bert_layer(all_tokens[:,0,:],
                               all_tokens[:,1,:],
                               all_tokens[:,2,:])
</code></pre>

<p>though: </p>

<p>this work fine [t is generated from the tokenizer of hugging-face:</p>

<pre><code>a = bert_layer(t[""input_ids""], t[""attention_mask""], t[""token_type_ids""])
</code></pre>
","7060584","","7060584","","2020-05-29 18:33:03","2020-05-29 18:33:03","'Tensor' object has no attribute 'detach'","<numpy><tensorflow><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"62317723","1","","","2020-06-11 05:33:25","","6","4039","<p>Is there a way to know the mapping from the tokens back to the original words in the <code>tokenizer.decode()</code> function?<br>
For example:</p>

<pre class=""lang-py prettyprint-override""><code>from transformers.tokenization_roberta import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True)

str = ""This is a tokenization example""
tokenized = tokenizer.tokenize(str) 
## ['this', 'Ä is', 'Ä a', 'Ä token', 'ization', 'Ä example']

encoded = tokenizer.encode_plus(str) 
## encoded['input_ids']=[0, 42, 16, 10, 19233, 1938, 1246, 2]

decoded = tokenizer.decode(encoded['input_ids']) 
## '&lt;s&gt; this is a tokenization example&lt;/s&gt;'
</code></pre>

<p>And the objective is to have a function that maps each token in the <code>decode</code> process to the correct input word, for here it will be:<br>
<code>desired_output = [[1],[2],[3],[4,5],[6]]</code><br> As <code>this</code> corresponds to id <code>42</code>, while <code>token</code> and <code>ization</code> corresponds to ids <code>[19244,1938]</code> which are at indexes <code>4,5</code> of the <code>input_ids</code> array.</p>
","7034613","","6664872","","2020-06-12 11:06:20","2020-06-13 12:22:46","Tokens to Words mapping in the tokenizer decode step huggingface?","<pytorch><tokenize><huggingface-transformers>","1","2","1","","","CC BY-SA 4.0"
"62644862","1","","","2020-06-29 18:52:30","","0","34","<p>I am trying to understand <a href=""https://gist.github.com/papapabi/124c6ac406e6bbd1f28df732e953ac6d"" rel=""nofollow noreferrer"">this example</a> on using Bert for sequence classification with a custom example. My question is fairly simple: what is the need for:</p>
<pre><code>train_data = train_data.shuffle(buffer_size=num_examples, reshuffle_each_iteration=True) \
        .batch(BATCH_SIZE) \
        .repeat(-1)
</code></pre>
<p>If I don't shuffle the data, I get an error:</p>
<pre><code>ValueError: slice index 0 of dimension 0 out of bounds.
for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0,
 ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' 
with input shapes: [0], [1], [1], [1] and with computed input tensors: 
input[1] = &lt;0&gt;, input[2] = &lt;1&gt;, input[3] = &lt;1&gt;.

</code></pre>
<p>So, to me, it seems that shuffling is not only shuffling but doing something else and I don't exactly understand what.</p>
","7466576","","","","","2020-06-29 18:52:30","What is the need for shuffling in tf.data.Dataset when fitting data into TFBertForSequenceClassification","<python><tensorflow><keras><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"62658346","1","","","2020-06-30 13:15:40","","1","139","<p>IÂ´m working on a framework where I have multiple text classification tasks (as shown in the example for binary text classification). For these tasks IÂ´m using the powerful <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface-transformer library</a>. My code looks actually like this for the binary classification task.</p>
<pre><code>class BertForBinaryDocumentClassification(BertPreTrainedModel):

def __init__(self, config):
    super().__init__(config)
    self.bert = BertModel.from_pretrained('bert-base-uncased')
    self.dropout = nn.Dropout(0.1)
    self.classifier = nn.Linear(in_features=config.hidden_size, out_features=config.num_labels)
    self.init_weights()

def forward(self, input_ids, token_type_ids=None, attention_mask=None,head_mask=None, labels=None):
    outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, head_mask=head_mask)
    pooled_output = outputs[0]
    pooled_output = self.dropout(pooled_output)
    logits = self.classifier(pooled_output)

    outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here

    loss_fct = BCELoss()
    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
    outputs = (loss,) + outputs

    return outputs  # (loss), logits, (hidden_states), (attentions)
</code></pre>
<p>Now IÂ´m takle performance problems because for each classifiction task I use the whole BERT model with the proper classification head (binary and multiclass). One model is about 400-500Mb.</p>
<p>Now IÂ´m wondering if I could split these networks in the BERT model, which produces the sentence embedding and send these encoded sentence embeddings to standalone classification networks with the final linear layer as shown in the code below.</p>
<p>The problem is now that IÂ´m not really sure how to tackle this problem. I was reading of freezing the last layer and send the intermediate embedding to the classification network. One thing is that I wonder if it still represents the whole information of the BERT layers from 1 to 11?</p>
<p>Another thing is that IÂ´m not really sure how the standalone classification layer should look like. I think it should look like this way.</p>
<pre><code>class BinaryClassifier(nn.Module):

def __init__(self, vocab_size, embedding_dim, context_size):
    super(BinaryClassifier, self).__init__()
    self.classifier = nn.Linear(in_features=config.hidden_size, out_features=config.num_labels)
    self.init_weights()

def forward(self, inputs):
    embeds = self.embeddings(inputs).view((1, -1))
    logits = self.classifier(embeds)
    outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here

    loss_fct = BCELoss()
    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
    outputs = (loss,) + outputs

    return outputs  # (loss), logits, (hidden_states), (attentions)
</code></pre>
<p>Can anybody tell me if my approach is correct and if not how to tackle this task? Thanks in advance.</p>
","5500658","","","","","2020-06-30 13:15:40","How to split a fine-tuned huggingface transformer model into two networks?","<python><pytorch><huggingface-transformers>","0","3","2","","","CC BY-SA 4.0"
"68650155","1","","","2021-08-04 11:07:08","","1","28","<p>I'm working on a machine with multiple GPUs (indexed 0 to 7).</p>
<p>I would like to deploy and train a HuggingFace pretrained model (RoBERTa) on one of them and be able to specify on which one, say GPU number 7.</p>
<p>I went over <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">transformers library docs</a> and looked for which parameter sets the GPU index but didn't find any.</p>
<p>How do I set this GPUs configuration?</p>
","10442092","","681865","","2021-08-04 12:12:24","2021-08-04 12:12:24","How to chose which GPU to run on with HuggingFace?","<huggingface-transformers>","0","1","1","","","CC BY-SA 4.0"
"68682282","1","","","2021-08-06 13:20:27","","0","13","<p>I see that some HuggingFace Pytorch pre-trained models and autotokenizers work well for SMILES representations of molecules. Are there any Tensorflow versions that would be suitable for this task?</p>
<p>Here is an example SMILES representation of a molecule:</p>
<p><code>'CC(=O)O'</code></p>
","8753030","","","","","2021-08-06 13:20:27","Is there a HuggingFace Tensorflow model and tokenizer that is suitable for molecules?","<tensorflow><deep-learning><huggingface-transformers><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"60378466","1","60390369","","2020-02-24 14:54:01","","1","701","<p>In BERT pretraining, the [CLS] token is embedded into the input of a classifier tasked with the Next Sentence Prediction task (or, in some BERT variants, with other tasks, such as ALBERT's Sentence Order Prediction); this helps in the pretraining of the entire transformer, and it also helps to make the [CLS] position readily available for retraining to other ""sentence scale"" tasks.</p>

<p>I wonder whether [SEP] could also be retrained in the same manner.
While [CLS] will probably be easier to retrain as the transformer is already trained to imbue its embedding with meaning from across the sentence, while [SEP] does not have these ""connections"" (one would assume), this might still work with sufficient fine-tuning. </p>

<p>With this one could retrain the same model for two different classification tasks,   one using [CLS] and one using [SEP].</p>

<p>Am I missing anything? 
Is there a reason why this would not work? </p>
","9084663","","","","","2020-02-25 08:30:36","If BERT's [CLS] can be retrained for a variety of sentence classification objectives, what about [SEP]?","<transformer><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60567168","1","","","2020-03-06 15:30:36","","1","45","<p>I have a specific generation problem involving a dataset built from a very small vocabulary. Ideally, my use case will be much more straightforward if I can simply provide that vocabulary in a fixed set of tokens. I know that with the BertTokenizer, for example, I can provide a <code>vocab.txt</code> file and avoid any further tokenization of this basic vocabulary, and I'm wondering if there's a way to get GPT-2 to do the same? The only thing I can think of right now is creating a hacked <code>PretrainedTokenizer</code> subclass, but perhaps someone has a better idea?</p>

<p>Any thoughts appreciated.</p>

<p>UPDATE: Okay, so it turns out I can just swap out <code>BertTokenizer</code> and <code>BertWordpieceTokenizer</code> when creating the <code>GPT2LMHeadModel</code>. (Thanks HuggingFace for a well-designed, modular codebase!)</p>
","4321521","","1243762","","2020-11-29 11:58:32","2020-11-29 11:58:32","Use BertTokenizer with HuggingFace GPT-2","<nlp><huggingface-transformers><gpt-2>","0","0","","","","CC BY-SA 4.0"
"65517232","1","65625106","","2020-12-31 06:01:53","","0","62","<p>After training the model, I tried to make predictions, but an error occurred and I don't know how to fix it.</p>
<p>The model was constructed using electra.</p>
<p><strong>here is my model</strong></p>
<pre><code>electra = TFElectraModel.from_pretrained(&quot;monologg/koelectra-base-v3-discriminator&quot;, from_pt=True)
input_ids = tf.keras.Input(shape=(MAX_LEN,), name='input_ids', dtype=tf.int32)
mask = tf.keras.Input(shape=(MAX_LEN,), name='attention_mask', dtype=tf.int32)
token = tf.keras.Input(shape=(MAX_LEN,), name='token_type_ids', dtype=tf.int32)
embeddings = electra(input_ids, attention_mask = mask, token_type_ids= token)[0]
X = tf.keras.layers.GlobalMaxPool1D()(embeddings)
X = tf.keras.layers.BatchNormalization()(X)
X = tf.keras.layers.Dense(128, activation='relu')(X)
X = tf.keras.layers.Dropout(0.1)(X)
y = tf.keras.layers.Dense(3, activation='softmax', name='outputs')(X)
model = tf.keras.Model(inputs=[input_ids, mask, token], outputs=y)
model.layers[2].trainable=False
model.summary()
</code></pre>
<p><strong>and here is summary</strong></p>
<pre><code>__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 25)]         0                                            
__________________________________________________________________________________________________
attention_mask (InputLayer)     [(None, 25)]         0                                            
__________________________________________________________________________________________________
token_type_ids (InputLayer)     [(None, 25)]         0                                            
__________________________________________________________________________________________________
tf_electra_model_4 (TFElectraMo TFBaseModelOutput(la 112330752   input_ids[0][0]                  
                                                                 attention_mask[0][0]             
                                                                 token_type_ids[0][0]             
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 768)          0           tf_electra_model_4[3][0]         
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 768)          3072        global_max_pooling1d_6[0][0]     
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 128)          98432       batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dropout_390 (Dropout)           (None, 128)          0           dense_18[0][0]                   
__________________________________________________________________________________________________
outputs (Dense)                 (None, 3)            387         dropout_390[0][0]                
==================================================================================================
Total params: 112,432,643
Trainable params: 112,431,107
Non-trainable params: 1,536
__________________________________________________________________________________________________
</code></pre>
<p><strong>This is the code to make train data set.</strong></p>
<pre><code>input_ids = []
attention_masks = []
token_type_ids = []
train_data_labels = []

for train_sent, train_label in tqdm(zip(train_data[&quot;content&quot;], train_data[&quot;label&quot;]), total=len(train_data)):
    try:
        input_id, attention_mask, token_type_id = Electra_tokenizer(train_sent, MAX_LEN)
        input_ids.append(input_id)
        attention_masks.append(attention_mask)
        token_type_ids.append(token_type_id)
        train_data_labels.append(train_label)

    except Exception as e:
        print(e)
        print(train_sent)
        pass

train_input_ids = np.array(input_ids, dtype=int)
train_attention_masks = np.array(attention_masks, dtype=int)
train_type_ids = np.array(token_type_ids, dtype=int)
intent_train_inputs = (train_input_ids, train_attention_masks, train_type_ids)
intent_train_data_labels = np.asarray(train_data_labels, dtype=np.int32)
</code></pre>
<p><strong>this is train data set shape</strong></p>
<pre><code>tf.Tensor([ 3 75 25], shape=(3,), dtype=int32)
</code></pre>
<p><strong>With this train data, the model train works fine but execute the following code to predict, an error occurs.</strong></p>
<pre><code>sample_text = 'this is sample text'
input_id, attention_mask, token_type_id = Electra_tokenizer(sample_text, MAX_LEN)
sample_text = (input_id, attention_mask, token_type_id)
model(sample_text) #or model.predict(sample_text)
</code></pre>
<p><strong>here is error</strong></p>
<pre><code>Layer model_15 expects 3 input(s), but it received 75 input tensors. Inputs received: [&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;, &lt;tf.Tensor: ....
</code></pre>
<p>It's the same shape as when i train, but why do i get an error and ask for help on how to fix it.</p>
<p>hope you have a great year ahead. Happy New Year.</p>
","14916190","","14916190","","2020-12-31 06:59:23","2021-01-08 07:42:54","An error occurs when predict with the same data as when performing train (expects 3 input(s), but it received 75 input tensors.)","<python><tensorflow><machine-learning><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"64800623","1","64801036","","2020-11-12 08:50:15","","0","275","<p>I've installed <em>SpacyBert</em> and <em>SpacyCake</em> to extract keyword phrases from a corpus of text. I've checked all the dependencies and have everything installed but I am getting the below error. Any ideas?</p>
<pre><code>File &quot;/usr/local/lib/python3.8/site-packages/spacy/language.py&quot;, line 449, in __call__ doc = proc(doc, **component_cfg.get(name, {})) File &quot;/usr/local/lib/python3.8/site-packages/spacycake/__init__.py&quot;, line 105, in __call__ second_part = torch.matmul( RuntimeError: cannot perform reduction function max on tensor with no elements because the operation does not have an identity
</code></pre>
<p>I have the correct language model downloaded so not sure what could be causing the issue. The models I have tested with:</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_md&quot;) and
nlp = spacy.load(&quot;en&quot;)
</code></pre>
<p>Spacy is working fine because I can perform other NLP tasks but it's just when I try using:</p>
<pre><code>cake = bake(nlp, from_pretrained='bert-base-cased', top_k=3)
nlp.add_pipe(cake, last=True)

doc = nlp(&quot;This is a test but obviously you need to place a bigger document here to extract meaningful keyphrases&quot;)
print(doc._.extracted_phrases)
</code></pre>
","1020496","","1020496","","2020-11-12 09:05:43","2020-11-12 09:17:10","SpacyBert/SpacyCake cannot perform reduction function max on tensor","<python><spacy><huggingface-transformers>","1","3","","2020-11-15 11:28:25","","CC BY-SA 4.0"
"63832094","1","","","2020-09-10 14:40:12","","0","115","<p>I'm a novice in writing neural networks. I have just started using BERT models, while running BERT for text summarization using the examples in <a href=""https://pypi.org/project/bert-extractive-summarizer/"" rel=""nofollow noreferrer"">bert extractive summarizer</a> I get the following error with the pretrained model getting halted at 57%.</p>
<p><code> OSError: Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin' to download pretrained weights.</code></p>
<p>How do I resolve this error.
Thanks.</p>
","14255103","","","","","2020-09-10 14:40:12","I get error while downloading BERT models for summarization","<python><pytorch><artificial-intelligence><bert-language-model><huggingface-transformers>","0","5","","","","CC BY-SA 4.0"
"63280435","1","63295226","","2020-08-06 09:14:29","","0","3680","<p><a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"" rel=""nofollow noreferrer""><code>encode_plus</code></a> in huggingface's transformers library allows truncation of the input sequence. Two parameters are relevant: <code>truncation</code> and <code>max_length</code>. I'm passing a paired input sequence to <code>encode_plus</code> and need to truncate the input sequence simply in a &quot;cut off&quot; manner, i.e., if the whole sequence consisting of both inputs <code>text</code> and <code>text_pair</code> is longer than <code>max_length</code> it should just be truncated correspondingly from the right.</p>
<p>It seems that neither of the truncation strategies allows to do this, instead <code>longest_first</code> removes tokens from the longest sequence (which could be either text or text_pair, but not just simply from the right or end of the sequence, e.g., if text is longer that text_pair, it seems this would remove tokens from text first), <code>only_first</code> and <code>only_second</code> remove tokens from only the first or second (hence, also not simply from the end), and <code>do_not_truncate</code> does not truncate at all. Or did I misunderstood this and actually <code>longest_first</code> might be what I'm looking for?</p>
","1455800","","","","","2020-08-07 04:29:28","huggingface transformers: truncation strategy in encode_plus","<pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"64805769","1","","","2020-11-12 14:24:53","","0","351","<p>I am currently fine-tuning a BERT model on a sequence classification task. To do this, I am using the transformers framework. This requires a Batch input in a Trainer: <a href=""https://huggingface.co/transformers/_modules/transformers/trainer.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/_modules/transformers/trainer.html</a></p>
<p>The way fine-tuning works, is described here: <a href=""https://huggingface.co/transformers/custom_datasets.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/custom_datasets.html</a>
I think the Batch needs to look like I created it, but for some reason I keep getting errors. The picture shows a single item from the dataset.</p>
<p>If I add the labels as tensor, a part of the model that converts labels to tensors gives an error. But when I add the labels as list I get: Expected input batch_size (16) to match target batch_size (2016).
What is the correct way to give a Batch to the BERT model?</p>
<p><a href=""https://i.stack.imgur.com/d8hop.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d8hop.png"" alt=""What my dataSet object looks like"" /></a></p>
<p>Here is how I initialise the model:</p>
<pre><code>training_args = TrainingArguments(
     output_dir='C:',          # output directory
     num_train_epochs=3,              # total number of training epochs
     per_device_train_batch_size=16,  # batch size per device during training
     per_device_eval_batch_size=64,   # batch size for evaluation
     warmup_steps=500,                # number of warmup steps for learning rate scheduler
     weight_decay=0.01,               # strength of weight decay
     logging_dir='C:',            # directory for storing logs
     logging_steps=10,
 )

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;)
data_collator = DataCollatorForTokenClassification(tokenizer)
 
trainer = Trainer(
    model=model,                           # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                    # training arguments, defined above
    data_collator=data_collator, #
    train_dataset=train_dataset,           # training dataset
    eval_dataset=test_dataset              # evaluation dataset
 )

trainer.train()   
</code></pre>
","9803315","","9803315","","2020-11-13 10:09:12","2020-11-13 10:09:12","Fine-tuning BERT on SequenceClassification using Transformers framework","<python><nlp><bert-language-model><huggingface-transformers>","0","4","","","","CC BY-SA 4.0"
"66996561","1","","","2021-04-08 02:27:15","","0","105","<p>I'm trying to configure Rasa to use models pretrained at HuggingFace. This is my setup</p>
<pre><code>language: fr
pipeline:
    - name: LanguageModelTokenizer
    - name: LanguageModelFeaturizer
      model_name: &quot;bert&quot;
      model_weights: &quot;setu4993/LaBSE&quot;
      cache_dir: &quot;/rasa/model&quot;
</code></pre>
<p><a href=""https://huggingface.co/setu4993/LaBSE"" rel=""nofollow noreferrer"">The HuggingFace model is here</a></p>
<p>After I run â€œrasa trainâ€, it says OSError: Model name â€œsetu4993/LaBSEâ€ was not found. Any idea how to integrate this huggingface model to Rasa?</p>
<p>Can we include multiple HuggingFace pipeline in Rasa configuration? Example</p>
<pre><code>pipeline:
    - model_name: &quot;bert&quot;
...
    - model_name: &quot;xlnet&quot;
</code></pre>
","857068","","","","","2021-04-08 02:27:15","Rasa integration pipeline with HuggingFace","<huggingface-transformers><rasa>","0","0","","","","CC BY-SA 4.0"
"64815481","1","","","2020-11-13 04:25:20","","2","177","<h2>The Problem</h2>
<p>I have been trying to train TFBertForMaskedLM model with tensorflow. But when i use model.fit() always encounter some question.Hope someone can help and propose some solution.</p>
<h2>Reference Paper and sample output</h2>
<p>The Paper title is &quot;Conditional Bert for Contextual Augmentation&quot;. In short, just change type_token_ids to label_ids. if the label of sentence is 5, length is 10 and max_sequence_length = 16. It will process output as follows:</p>
<pre><code>input_ids = [101, 523, 791, 3189, 677, 5221, 524, 1920, 686, 102, 0, 0, 0, 0, 0, 0]
attention_mask = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]
token_type_ids = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0]
labels = [-100, -100, 791, -100, 677, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
</code></pre>
<h2>Environment</h2>
<ul>
<li>tensorflow == 2.2.0</li>
<li>huggingface == 3.5.0</li>
<li>datasets == 1.1.2</li>
<li>dataset total label is 5. (1~5)</li>
<li>GPU : GCP P100 * 1</li>
</ul>
<h2>Dataset output (max_sequence_length=128, batch_size=1)</h2>
<pre class=""lang-py prettyprint-override""><code>{'attention_mask': &lt;tf.Tensor: shape=(128,), dtype=int32, numpy=
 array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)&gt;,
 'input_ids': &lt;tf.Tensor: shape=(128,), dtype=int32, numpy=
 array([  101,   523,   791,  3189,   677,  5221,   524,  1920,   686,
         4518,  6240,   103,  2466,  2204,  2695,   100,   519,  5064,
         1918,   736,  2336,   520,   103,  2695,  1564,  4923,  8013,
          678,  6734,  8038,  8532,   131,   120,   120,  8373,   119,
          103,  9989,   103,  8450,   120,   103,   120, 12990,  8921,
         8165,   102,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0], dtype=int32)&gt;,
 'labels': &lt;tf.Tensor: shape=(128,), dtype=int32, numpy=
 array([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        4634, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        4158, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, 8429, -100,  119, -100, -100,  100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100], dtype=int32)&gt;,
 'token_type_ids': &lt;tf.Tensor: shape=(128,), dtype=int32, numpy=
 array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)&gt;}
</code></pre>
<h2>Model code</h2>
<pre class=""lang-py prettyprint-override""><code>from transformers import AdamWeightDecay, TFBertForMaskedLM, BertConfig

def create_model():
    configuration = BertConfig.from_pretrained('bert-base-chinese')
    model = TFBertForMaskedLM.from_pretrained('bert-base-chinese',
                                              config=configuration)
    model.bert.embeddings.token_type_embeddings = tf.keras.layers.Embedding(5, 768, 
                                                                            embeddings_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))
    return model
model = create_model()

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metrics = [tf.keras.metrics.Mean(), tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]

model.compile(optimizer = optimizer,
              loss = loss,
              metrics = metrics)

model.fit(tf_sms_dataset, 
          epochs=1,
          verbose=1)
</code></pre>
<h2>Warning Message when use TFBertForMaskedLM</h2>
<pre><code>Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertForMaskedLM: ['nsp___cls']
- This IS expected if you are initializing TFBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-chinese.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.
</code></pre>
<h2>Error Message</h2>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-42-99b78906fef7&gt; in &lt;module&gt;()
      5 model.fit(tf_sms_dataset, 
      6           epochs=1,
----&gt; 7           verbose=1)

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    846                 batch_size=batch_size):
    847               callbacks.on_train_batch_begin(step)
--&gt; 848               tmp_logs = train_function(iterator)
    849               # Catch OutOfRangeError for Datasets of unknown size.
    850               # This blocks until the batch has finished executing.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    578         xla_context.Exit()
    579     else:
--&gt; 580       result = self._call(*args, **kwds)
    581 
    582     if tracing_count == self._get_tracing_count():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    625       # This is the first call of __call__, so we have to initialize.
    626       initializers = []
--&gt; 627       self._initialize(args, kwds, add_initializers_to=initializers)
    628     finally:
    629       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    504     self._concrete_stateful_fn = (
    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 506             *args, **kwds))
    507 
    508     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2444       args, kwargs = None, None
   2445     with self._lock:
-&gt; 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2447     return graph_function
   2448 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)
-&gt; 2777       graph_function = self._create_graph_function(args, kwargs)
   2778       self._function_cache.primary[cache_key] = graph_function
   2779       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665             arg_names=arg_names,
   2666             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2667             capture_by_value=self._capture_by_value),
   2668         self._function_attributes,
   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--&gt; 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    440         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    442     weak_wrapped_fn = weakref.ref(wrapped_fn)
    443 
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:541 train_step  **
        self.trainable_variables)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1804 _minimize
        trainable_variables))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:521 _aggregate_gradients
        filtered_grads_and_vars = _filter_grads(grads_and_vars)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1219 _filter_grads
        ([v.name for _, v in grads_and_vars],))

    ValueError: No gradients provided for any variable: ['tf_bert_for_masked_lm_2/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_masked_lm_2/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_masked_lm_2/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_masked_lm_2/bert/embeddings/LayerNorm/beta:0', 'tf_bert_for_masked_lm_2/bert/embeddings/embedding_1/embeddings:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_for_masked_lm_2/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_f...
</code></pre>
<p>Have Someone can help. I will thanks a lot.</p>
<h2>Other Test</h2>
<p>I used english sentence to test. example as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import TFBertForMaskedLM, BertConfig

def create_model():
    configuration = BertConfig.from_pretrained('bert-base-uncased')
    model = TFBertForMaskedLM.from_pretrained('bert-base-uncased',
                                              config=configuration)
    return model
    
model = create_model()
eng_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
token_info = eng_tokenizer(text=&quot;We are very happy to show you the ðŸ¤— Transformers library.&quot;, padding='max_length', max_length=20)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metrics = [tf.keras.metrics.Mean(), tf.keras.metrics.SparseCategoricalAccuracy(&quot;acc&quot;)]

dataset = tf.data.Dataset.from_tensor_slices(dict(token_info))
dataset = dataset.batch(1).prefetch(tf.data.experimental.AUTOTUNE)

model.compile(optimizer = optimizer,
              loss = model.compute_loss,
              metrics = metrics)

model.fit(dataset)
</code></pre>
<p>token_info output dataset</p>
<pre><code>{
  'input_ids': [101, 2057, 2024, 2200, 103, 2000, 2265, 2017, 103, 100, 19081, 3075, 1012, 102, 0, 0, 0, 0, 0, 0]
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  'labels': [-100, -100, -100, -100, 3407, -100, -100, -100, 1996, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
}
</code></pre>
<p>Get same error.....</p>
<pre><code>ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:541 train_step  **
        self.trainable_variables)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1804 _minimize
        trainable_variables))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:521 _aggregate_gradients
        filtered_grads_and_vars = _filter_grads(grads_and_vars)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1219 _filter_grads
        ([v.name for _, v in grads_and_vars],))

    ValueError: No gradients provided for any variable: ['tf_bert_for_masked_lm_2/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_masked_lm_2/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_masked_lm_2/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_for_masked_lm_2/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_masked_lm_2/bert/embeddings/LayerNorm/beta:0', 
</code></pre>
<p>I'm not sure if there is a problem with the integration of fit() into the model?</p>
","14630202","","","","","2020-11-13 04:25:20","Finetune TFBertForMaskedLM model.fit() ValueError","<tensorflow2.0><huggingface-transformers><huggingface-tokenizers>","0","0","1","","","CC BY-SA 4.0"
"64818504","1","","","2020-11-13 09:32:47","","1","974","<p>I have a small text dataset for translation which I want to fine-tune with <code>t5-small</code>, Here is the code which I am trying to use to finetune.</p>
<pre><code>import numpy as np
import tensorflow as tf
from transformers import TFT5ForConditionalGeneration, T5Tokenizer

model = TFT5ForConditionalGeneration.from_pretrained('t5-small')
tokenizer = T5Tokenizer.from_pretrained('t5-small')

def data_gen():
  for _ in range(256):
    x = np.random.randint(1,tokenizer.vocab_size, model.config.n_positions)
    attention = np.ones_like(x)
    yield ((x, attention), (x, attention))

output_type = ((tf.int32, tf.int32), (tf.int32, tf.int32)) 
ds = tf.data.Dataset.from_generator(data_gen, output_type).batch(2)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=optimizer, loss=loss)

model.fit(ds, epochs=3, steps_per_epoch=128)
</code></pre>
<p>but while running this code I am getting the following error.</p>
<pre><code>All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.

All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.
Epoch 1/3
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-12-12c0ab7ab337&gt; in &lt;module&gt;()
     19 model.compile(optimizer=optimizer, loss=loss)
     20 
---&gt; 21 model.fit(ds, epochs=3, steps_per_epoch=128)

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_t5.py:1285 call  *
        encoder_outputs = self.encoder(
    /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_t5.py:618 call  *
        input_shape = shape_list(input_ids)
    /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_utils.py:1026 shape_list  *
        static = x.shape.as_list()
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1190 as_list  **
        raise ValueError(&quot;as_list() is not defined on an unknown TensorShape.&quot;)

    ValueError: as_list() is not defined on an unknown TensorShape
</code></pre>
<pre><code>Python 3.6.9
tensorflow==2.3.0
tensorflow-addons==0.8.3
tensorflow-datasets==4.0.1
tensorflow-estimator==2.3.0
tensorflow-gcs-config==2.3.0
tensorflow-hub==0.10.0
tensorflow-metadata==0.24.0
tensorflow-privacy==0.2.2
tensorflow-probability==0.11.0
transformers==3.5.0
</code></pre>
","6082378","","6082378","","2020-11-13 09:48:51","2020-11-13 09:48:51","How to finetune the huggingface T5 model on custom data?","<python><tensorflow><tf.keras><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"62651381","1","","","2020-06-30 06:19:50","","0","86","<p>In Distilbert config files, there is a option that is &quot;sinusoidal_pos_embds&quot;, and is set to be &quot;true&quot;. Is that means distilbert utilize the sinusoidal position embeddings in practice?</p>
","5481214","","","","","2020-06-30 07:10:24","What type of position embedding is used in Distilbert?","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63281940","1","","","2020-08-06 10:46:05","","1","123","<p>I have a deep learning model that uses BERT layer from Huggingface library (<code>TF=2.0</code>, <code>Transformers=2.8.0</code>).</p>
<p>The model consists of: BERT embeddings -&gt; Attention -&gt; Softmax. Also I am using <code>tf.keras.callbacks.ModelCheckpoint</code> to save the best model during training.</p>
<p>I'm trying to slice the model to get the attention weights.</p>
<p>My problem is that, if I try to access the output of any layer after loading the saved model using <code>output1 = model.layers[3].output</code>, I get this error:</p>
<blockquote>
<p>AttributeError: Layer tf_bert_model has no inbound nodes.</p>
</blockquote>
<p>but if I do the same on the original model without saving it (instantly after finishing model.fit()), I have no problem.</p>
<p>This is how I load the saved model:</p>
<pre><code>model = tf.keras.models.load_model(model_dir)
</code></pre>
<p>Is there a problem with this way?, giving that it works for prediction.</p>
","3261292","","","","","2020-10-21 15:52:44","AttributeError: Layer tf_bert_model has no inbound nodes","<python><tensorflow><keras><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"64839614","1","","","2020-11-14 23:13:13","","0","411","<p>I'm trying to get reproducible results of T5 transformer model:</p>
<pre><code>import torch
from transformers import T5ForConditionalGeneration,T5Tokenizer


def set_seed(seed):
  torch.manual_seed(seed)
  if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

set_seed(42)

t5model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_paraphraser')
tokenizer = T5Tokenizer.from_pretrained('t5-base')

device = torch.device(&quot;cpu&quot;)
print (&quot;device &quot;,device)
t5model = t5model.to(device)

max_len = 256

text =  &quot;paraphrase: &quot; + txt + &quot; &lt;/s&gt;&quot;

encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=&quot;pt&quot;)
input_ids, attention_masks = encoding[&quot;input_ids&quot;].to(device), encoding[&quot;attention_mask&quot;].to(device)

beam_outputs = t5model.generate(
    input_ids=input_ids, attention_mask=attention_masks,
    do_sample=True,
    max_length=max_len,
    top_k=50,
    top_p=0.98,
    early_stopping=True,
    num_return_sequences=10,
)
</code></pre>
<p>Though I set a seed number, <code>t5model.generate</code> gives me different results each time I run it.</p>
<p>What is the right way to set the seed number, in order to get the same results of <code>t5model.generate</code> after multiple executions?</p>
","11622712","","6664872","","2020-11-15 23:10:29","2020-11-16 15:00:37","How to get reproducible results of T5 transformer model","<python><nlp><torch><huggingface-transformers><transformer>","1","0","","","","CC BY-SA 4.0"
"65557918","1","65599896","","2021-01-04 05:09:48","","2","1701","<p>I am trying to implement the XLNET on Google Collaboratory. But I get the following issue.</p>
<pre><code>ImportError: 
XLNetTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment.
</code></pre>
<p>I have also tried the following steps:</p>
<pre><code>!pip install -U transformers
!pip install sentencepiece

from transformers import XLNetTokenizer
tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased-spiece.model')
</code></pre>
<p>Thank you for your help in advance.</p>
","8068733","","","","","2021-01-06 17:12:14","XLNetTokenizer requires the SentencePiece library but it was not found in your environment","<google-colaboratory><huggingface-transformers><transformer><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"62669261","1","62688252","","2020-07-01 03:32:24","","5","5789","<p>I would like to create a minibatch by encoding multiple sentences using transform.BertTokenizer. It seems working for a single sentence. How to make it work for several sentences?</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# tokenize a single sentence seems working
tokenizer.encode('this is the first sentence')
&gt;&gt;&gt; [2023, 2003, 1996, 2034, 6251]

# tokenize two sentences
tokenizer.encode(['this is the first sentence', 'another setence'])
&gt;&gt;&gt; [100, 100] # expecting 7 tokens
</code></pre>
","1487336","","1487336","","2020-07-02 02:33:02","2021-03-28 11:39:44","How to encode multiple setence using transformers.BertTokenizer?","<word-embedding><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"65551516","1","","","2021-01-03 15:26:40","","1","185","<p>I've fine tuned a distilgpt2 model using my own text using <code>run_language_modeling.py</code> and its working fine after training and <code>run_generation.py</code> script produces the expected results.</p>
<p>Now I want to convert this to a Tensorflow Lite model and did so by using the following</p>
<pre><code>from transformers import *

CHECKPOINT_PATH = '/content/drive/My Drive/gpt2_finetuned_models/checkpoint-2500'

model = GPT2LMHeadModel.from_pretrained(&quot;distilgpt2&quot;)
model.save_pretrained(CHECKPOINT_PATH)
model = TFGPT2LMHeadModel.from_pretrained(CHECKPOINT_PATH, from_pt=True) 
</code></pre>
<p>But I dont think I'm doing this right as after conversion, when I write</p>
<pre><code>print(model.inputs)
print(model.outputs)
</code></pre>
<p>I get</p>
<pre><code>None
None
</code></pre>
<p>But I still went ahead with the TFLite conversion using :</p>
<pre><code>import tensorflow as tf

input_spec = tf.TensorSpec([1, 64], tf.int32)
model._set_inputs(input_spec, training=False)

converter = tf.lite.TFLiteConverter.from_keras_model(model)

# FP16 quantization:
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]

tflite_model = converter.convert()

open(&quot;/content/gpt2-fp16.tflite&quot;, &quot;wb&quot;).write(tflite_model)
</code></pre>
<p>But does not work and when using the generated <code>tflite</code> model I get the error:</p>
<blockquote>
<p>tensorflow/lite/kernels/kernel_util.cc:249 d1 == d2 || d1 == 1 || d2 == 1 was not true.</p>
</blockquote>
<p>Which I'm sure has something to to with my model not converting properly and getting <code>None</code> for input/output.</p>
<p>Does anyone have any idea how to fix this?</p>
<p>Thanks</p>
","843036","","","","","2021-01-03 15:26:40","Cannot convert from a fine-tuned GPT-2 model to a Tensorflow Lite model","<tensorflow><tensorflow-lite><huggingface-transformers><gpt-2>","0","3","","","","CC BY-SA 4.0"
"65553639","1","","","2021-01-03 18:51:39","","1","116","<p>I am enjoying experimenting with different transformers from the excellent 'Huggingface' library. However, I receive the following error message when I attempt to use any kind of 'roberta'/ 'xlm' transformers. My Python code seems to work just fine with bert-base and bert-large models , so I want to understand how I might need to adjust it to work with these variants.</p>
<p>Exception: WordPiece error: Missing [UNK] token from the vocabulary</p>
<p>My code adds a fine-tuning layer on top of the pre-trained BERT model. All the bert models I have used previously have no problem tokenizing and processing the English language text data I am analysing. My Python knowledge is growing but I would describe it as solid basics but patchy above this level.  Please help me to better understand the issue arsing here so I can make the necessary adjustments, With thanks - Mark</p>
<p>Here is the full error message, if that helps.</p>
<pre><code>---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
&lt;ipython-input-61-d42d72a742f6&gt; in &lt;module&gt;()
      5     pad_to_max_length=True,
      6     truncation=True,
----&gt; 7     return_token_type_ids=False
      8 )
      9 


2 frames

/usr/local/lib/python3.6/dist-packages/tokenizers/implementations/base_tokenizer.py in encode_batch(self, inputs, is_pretokenized, add_special_tokens)
    247             raise ValueError(&quot;encode_batch: `inputs` can't be `None`&quot;)
    248 
--&gt; 249         return self._tokenizer.encode_batch(inputs, is_pretokenized, add_special_tokens)
    250 
    251     def decode(self, ids: List[int], skip_special_tokens: Optional[bool] = True) -&gt; str:

Exception: WordPiece error: Missing [UNK] token from the vocabulary
</code></pre>
","14845575","","","","","2021-01-03 18:51:39","error received after loading Roberta and XLM_Roberta models from Huggingface","<nlp><text-classification><huggingface-transformers><xlm><roberta>","0","2","","","","CC BY-SA 4.0"
"62782001","1","62848765","","2020-07-07 18:52:36","","0","1919","<p>I am trying to POS_TAG French using the Hugging Face Transformers library. In English I was able to do so given a sentence like e.g:</p>
<blockquote>
<p>The weather is really great. So let us go for a walk.</p>
</blockquote>
<p>the result is:</p>
<pre><code>    token   feature
0   The     DET
1   weather NOUN
2   is      AUX
3   really  ADV
4   great   ADJ
5   .       PUNCT
6   So      ADV
7   let     VERB
8   us      PRON
9   go      VERB
10  for     ADP
11  a       DET
12  walk    NOUN
13  .       PUNCT
</code></pre>
<p>Does anyone have an idea how a similar thing could be achieved for French?</p>
<p>This is the code I used for the English version in a Jupyter notebook:</p>
<pre><code>!git clone https://github.com/bhoov/spacyface.git
!python -m spacy download en_core_web_sm

from transformers import pipeline
import numpy as np
import pandas as pd

nlp = pipeline('feature-extraction')
sequence = &quot;The weather is really great. So let us go for a walk.&quot;
result = nlp(sequence)
# Just displays the size of the embeddings. The sequence
# In this case there are 16 tokens and the embedding size is 768
np.array(result).shape

import sys
sys.path.append('spacyface')

from spacyface.aligner import BertAligner

alnr = BertAligner.from_pretrained(&quot;bert-base-cased&quot;)
tokens = alnr.meta_tokenize(sequence)
token_data = [{'token': tok.token, 'feature': tok.pos} for tok in tokens]
pd.DataFrame(token_data)
</code></pre>
<p>The output of this notebook is above.</p>
","2735286","","2735286","","2020-07-08 09:10:57","2020-07-11 11:48:08","Using Hugging Face Transformers library how can you POS_TAG French text","<python><nlp><huggingface-transformers><bert-language-model>","1","7","","","","CC BY-SA 4.0"
"62317931","1","62318184","","2020-06-11 05:52:57","","4","6110","<p>I want to perform a text generation task in a flask app and host it on a web server however when downloading the GPT models the elastic beanstalk managed EC2 instance crashes because the download takes too much time and memory</p>

<pre><code>from transformers.tokenization_openai import OpenAIGPTTokenizer
from transformers.modeling_tf_openai import TFOpenAIGPTLMHeadModel
model = TFOpenAIGPTLMHeadModel.from_pretrained(""openai-gpt"")
tokenizer = OpenAIGPTTokenizer.from_pretrained(""openai-gpt"")
</code></pre>

<p>These are the lines in question causing the issue. GPT is approx 445 MB. I am using the transformers library. Instead of downloading the model at this line I was wondering if I could pickle the model and then bundle it as part of the repository. Is that possible with this library? Otherwise how can I preload this model to avoid the issues I am having?</p>
","7981821","","","","","2021-08-05 03:45:00","How to predownload a transformers model","<machine-learning><flask><amazon-elastic-beanstalk><transformer><huggingface-transformers>","3","0","3","","","CC BY-SA 4.0"
"63845748","1","63880613","","2020-09-11 10:59:58","","0","29","<p>I want to train a BERT like model for Hebrew, where fore very word I know:</p>
<ol>
<li>Lemma</li>
<li>Gender</li>
<li>Number</li>
<li>Voice</li>
</ol>
<p>And I would like to train a model where for each token these features are concatenated
Embedding(Token) = E1(Lemma):E2(Gender):E3(Number):E4(Voice)</p>
<p>Is there a way to do such a thing with the current huggingface transformers library?</p>
","5561875","","","","","2020-09-14 08:21:22","Train Model with Token Features","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65541788","1","65699717","","2021-01-02 17:06:02","","4","612","<p>Currently Helsinki-NLP/opus-mt-es-en model takes around 1.5sec for inference from transformer. How can that be reduced?
Also when trying to convert it to onxx runtime getting this error:</p>
<blockquote>
<p>ValueError: Unrecognized configuration class &lt;class 'transformers.models.marian.configuration_marian.MarianConfig'&gt; for this kind of AutoModel: AutoModel.
Model type should be one of RetriBertConfig, MT5Config, T5Config, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaConfig, BartConfig, LongformerConfig, RobertaConfig, LayoutLMConfig, SqueezeBertConfig, BertConfig, OpenAIGPTConfig, GPT2Config, MobileBertConfig, TransfoXLConfig, XLNetConfig, FlaubertConfig, FSMTConfig, XLMConfig, CTRLConfig, ElectraConfig, ReformerConfig, FunnelConfig, LxmertConfig, BertGenerationConfig, DebertaConfig, DPRConfig, XLMProphetNetConfig, ProphetNetConfig, MPNetConfig, TapasConfig.</p>
</blockquote>
<p>Is it possible to convert this to onxx runtime?</p>
","7086926","","6331369","","2021-01-02 17:14:00","2021-01-13 10:10:17","How to reduce the inference time of Helsinki-NLP/opus-mt-es-en (translation model) from transformer","<pytorch><huggingface-transformers><machine-translation>","1","0","","","","CC BY-SA 4.0"
"62988081","1","","","2020-07-20 03:08:42","","1","948","<p>I trained a RoBERTa model from scratch using <code>transformers</code>, but I can't check the training loss during training using</p>
<p><a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb</a></p>
<p>In the notebook, loss is printed every 500 steps, but there is no training loss logged during training:</p>
<pre><code>Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 20703/20711 [4:42:54&lt;00:07,  1.14it/s][A  
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 20704/20711 [4:42:54&lt;00:05,  1.24it/s][A  
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 20705/20711 [4:42:55&lt;00:05,  1.20it/s][A  
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 20706/20711 [4:42:56&lt;00:04,  1.18it/s][A  
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 20707/20711 [4:42:57&lt;00:03,  1.19it/s][A  
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 20708/20711 [4:42:58&lt;00:02,  1.16it/s][A  
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 20709/20711 [4:42:59&lt;00:01,  1.14it/s][A  
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 20710/20711 [4:43:00&lt;00:00,  1.13it/s][A  
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20711/20711 [4:43:00&lt;00:00,  1.45it/s][A  
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20711/20711 [4:43:00&lt;00:00,  1.22it/s]  
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [61:14:16&lt;00:00, 16952.06s/it]  
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [61:14:16&lt;00:00, 16958.16s/it]

compress roberta.20200717.zip on ./pretrained
save roberta.20200717.zip on minio(petcharts)
</code></pre>
<p>No values are printed for the loss, so I don't know if the training converged well or not. How can I monitor the loss during training?</p>
","10748392","","4240413","","2020-11-21 12:41:35","2020-11-21 12:41:35","How can I check the loss when training RoBERTa in huggingface/transformers?","<logging><huggingface-transformers><loss><roberta-language-model>","1","0","1","","","CC BY-SA 4.0"
"63876450","1","63876494","","2020-09-13 23:23:43","","0","35","<p>I'm running the following using the huggingface implementation:</p>
<pre><code>t1 = &quot;My example sentence is really great.&quot;

tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')
model = TransfoXLLMHeadModel.from_pretrained(&quot;transfo-xl-wt103&quot;)

encoded_input = tokenizer(t1, return_tensors='pt', add_space_before_punct_symbol=True) 
output = model(**encoded_input)
tmp = output[0].detach().numpy()
print(tmp.shape)

&gt;&gt;&gt; (1, 7, 267735)
</code></pre>
<p>With the goal of getting output embeddings that I'll use downstream.</p>
<p>The last dimension is /substantially/ larger than I expected, and it looks like it is the size of the entire <code>vocab_size</code> rather than a reduction based on the <a href=""https://arxiv.org/pdf/1901.02860.pdf."" rel=""nofollow noreferrer"">ECL from the paper</a> (which potentially I am misinterpreting).</p>
<p>What argument would I provide the <code>model</code> to reduce this layer size to a smaller dimensional space, something more like the basic BERT at 400 or 768 and still obtain good performance based on the pretrained embeddings?</p>
","1052117","","","","","2020-09-13 23:31:32","Reduce the output layer size from XLTransformers","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"64823332","1","64866990","","2020-11-13 15:12:04","","4","308","<p>I want to get the gradient of an embedding layer from a pytorch/huggingface model. Here's a minimal working example:</p>
<pre><code>from transformers import pipeline

nlp = pipeline(&quot;zero-shot-classification&quot;, model=&quot;facebook/bart-large-mnli&quot;)

responses = [&quot;I'm having a great day!!&quot;]
hypothesis_template = 'This person feels {}'
candidate_labels = ['happy', 'sad']
nlp(responses, candidate_labels, hypothesis_template=hypothesis_template)
</code></pre>
<p>I can extract the logits just fine,</p>
<pre><code>inputs = nlp._parse_and_tokenize(responses, candidate_labels, hypothesis_template)
predictions = nlp.model(**inputs, return_dict=True, output_hidden_states=True)
predictions['logits']   
</code></pre>
<p>and the model returns a layer I'm interested in. I tried to retain the gradient and backprop with respect to a single logit I'm interested in:</p>
<pre class=""lang-py prettyprint-override""><code>layer = predictions['encoder_hidden_states'][0]
layer.retain_grad()
predictions['logits'][0][2].backward(retain_graph=True)
</code></pre>
<p>However, <code>layer.grad == None</code> no matter what I try. The other named parameters of the model have their gradients computed, so I'm not sure what I'm doing wrong. How do I get the grad of the encoder_hidden_states?</p>
","249341","","","","","2020-11-16 23:08:50","Gradients returning None in huggingface module","<python><nlp><pytorch><huggingface-transformers>","1","5","","","","CC BY-SA 4.0"
"62904242","1","","","2020-07-14 21:26:50","","2","348","<p>I have a pytorch lightning code that works perfectly for a binary classification task when used with bert-base-uncased or roberta-base but doesn't work with roberta-large i.e the training loss doesn't come down.</p>
<p>I have no clue why this is happening.
I'm looking for reasons for such an issue.</p>
<p>Edit:
I'm training on MNLI dataset (only entailment and contradiction classes)
The model is predicting the same class for all the examples.</p>
<p>Thanks</p>
","6392022","","6392022","","2020-07-16 04:53:06","2020-07-16 07:07:34","Training loss is not decreasing for roberta-large model but working perfectly fine for roberta-base, bert-base-uncased","<huggingface-transformers>","1","3","","","","CC BY-SA 4.0"
"62995253","1","","","2020-07-20 12:19:17","","0","79","<p>I'm trying to install the transformers library on HPC. I do:</p>
<pre><code>git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e . --user
</code></pre>
<p>All three of these work as expected, with the last output being:</p>
<pre><code>Successfully installed dataclasses-0.7 numpy-1.19.0 tokenizers-0.8.1rc2 transformers
</code></pre>
<p>Then, I try <code>python -c &quot;import transformers&quot;</code> but I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/maths/btech/mt1170727/transformers/src/transformers/__init__.py&quot;, line 23, in &lt;module&gt;
    from .configuration_albert import ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, AlbertConfig
  File &quot;/home/maths/btech/mt1170727/transformers/src/transformers/configuration_albert.py&quot;, line 18, in &lt;module&gt;
    from .configuration_utils import PretrainedConfig
  File &quot;/home/maths/btech/mt1170727/transformers/src/transformers/configuration_utils.py&quot;, line 25, in &lt;module&gt;
    from .file_utils import CONFIG_NAME, cached_path, hf_bucket_url, is_remote_url
  File &quot;/home/maths/btech/mt1170727/transformers/src/transformers/file_utils.py&quot;, line 37, in &lt;module&gt;
    import torch
  File &quot;/home/soft/PYTHON/3.6.0/ucs4/gnu/447/lib/python3.6/site-packages/torch/__init__.py&quot;, line 125, in &lt;module&gt;
    _load_global_deps()
  File &quot;/home/soft/PYTHON/3.6.0/ucs4/gnu/447/lib/python3.6/site-packages/torch/__init__.py&quot;, line 83, in _load_global_deps
    ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)
  File &quot;/home/soft/PYTHON/3.6.0/ucs4/gnu/447/lib/python3.6/ctypes/__init__.py&quot;, line 344, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: libnvToolsExt.so.1: cannot open shared object file: No such file or directory
</code></pre>
<p>I have done as was written in the <a href=""https://huggingface.co/transformers/installation.html#installing-from-source"" rel=""nofollow noreferrer"">documentation</a>, and can't see why I'm facing this error. Any help would be great. Thanks...</p>
","11074646","","6664872","","2020-07-20 13:35:40","2020-07-20 13:35:40","Installing `transformers` on HPC Cluster","<pip><centos><hpc><huggingface-transformers>","0","5","","","","CC BY-SA 4.0"
"65822014","1","","","2021-01-21 06:11:18","","0","249","<p>Runtime error while finetuning a pretrained <a href=""https://huggingface.co/gpt2-medium"" rel=""nofollow noreferrer"">GPT2-medium</a> model using <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">Huggingface</a> library in SageMaker - <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html"" rel=""nofollow noreferrer"">ml.p3.8xlarge</a> instance.</p>
<p>The <code>finetuning_gpt2_script.py</code> contains the below,</p>
<p>Libraries:</p>
<pre><code>from transformers import Trainer, TrainingArguments
from transformers import EarlyStoppingCallback
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import TextDataset,DataCollatorForLanguageModeling
</code></pre>
<p>Pretrained Models:</p>
<pre><code>gpt2_model = GPT2LMHeadModel.from_pretrained(&quot;gpt2-medium&quot;)
gpt2_tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2-medium&quot;)
</code></pre>
<p>Train and Test Data Construction:</p>
<pre><code>train_dataset = TextDataset(
          tokenizer=gpt2_tokenizer,
          file_path=train_path,
          block_size=128)
    
test_dataset = TextDataset(
          tokenizer=gpt2_tokenizer,
          file_path=test_path,
          block_size=128)
    
data_collator = DataCollatorForLanguageModeling(
        tokenizer=gpt2_tokenizer, mlm=False,
    )
</code></pre>
<p><code>train_path</code> &amp; <code>test_path</code> are unstructured text data file of size 1.45 Million and 200K lines of data</p>
<p>Training arguments:</p>
<pre><code>training_args = TrainingArguments(
        output_dir=&quot;./gpt2-finetuned-models&quot;, #The output directory
        overwrite_output_dir=True, #overwrite the content of the output directory
        num_train_epochs=1, # number of training epochs
        per_device_train_batch_size=8, # batch size for training #32
        per_device_eval_batch_size=8,  # batch size for evaluation #64
        save_steps=100, # after # steps model is saved
        warmup_steps=500,# number of warmup steps for learning rate scheduler
        prediction_loss_only=True,
        metric_for_best_model = &quot;eval_loss&quot;,
        load_best_model_at_end = True,
        evaluation_strategy=&quot;epoch&quot;,
    )
</code></pre>
<p><code>training_args</code> are the training arguments constructed to train the model.</p>
<p>Trainer:</p>
<pre><code>trainer = Trainer(
        model=gpt2_model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        callbacks = [early_stop_callback],
    )
early_stop_callback = EarlyStoppingCallback(early_stopping_patience  = 3)
</code></pre>
<p>Training:</p>
<pre><code>trainer.train()
trainer.save_model(model_path)
</code></pre>
<p>Here, the training is done for only 1 epoch in 4 GPUS using <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html"" rel=""nofollow noreferrer"">ml.p3.8xlarge</a> instance.</p>
<p>The training is done by torch-distribution like below,</p>
<pre><code>python -m torch.distributed.launch finetuning_gpt2_script.py
</code></pre>
<p>While training at the end of the epoch, observed the below error,</p>
<p><code>RuntimeError: Input tensor at index 3 has invalid shape [2, 2, 16, 128, 64] but expected [2, 4, 16, 128, 64]</code></p>
<ol>
<li>Is the <code>RuntimeError</code> because of the way the <code>train_dataset</code> and <code>test_dataset</code>constructed using <code>TextData</code> ?</li>
<li>Am I doing wrong in the <code>torch-distribution</code> ?</li>
</ol>
","1793799","","","","","2021-01-21 08:34:05","RuntimeError: Input tensor at index 3 has invalid shape [2, 2, 16, 128, 64] but expected [2, 4, 16, 128, 64]","<python><pytorch><amazon-sagemaker><huggingface-transformers><gpt-2>","1","0","","","","CC BY-SA 4.0"
"63201036","1","63692060","","2020-08-01 01:54:28","","4","3374","<p>I want to add additional <code>Dense</code> layer after pretrained <code>TFDistilBertModel</code>, <code>TFXLNetModel</code> and <code>TFRobertaModel</code> Huggingface models. I have already seen how I can do this with the <code>TFBertModel</code>, e.g. <a href=""https://www.kaggle.com/dhruv1234/huggingface-tfbertmodel"" rel=""nofollow noreferrer"">in this notebook</a>:</p>
<pre><code>output = bert_model([input_ids,attention_masks])
output = output[1]
output = tf.keras.layers.Dense(32,activation='relu')(output)
</code></pre>
<p>So, here I need to use the second item(i.e. item with index <code>1</code>) of the <code>BERT</code> output tuple. According to the <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""nofollow noreferrer"">docs</a> <code>TFBertModel</code> has <code>pooler_output</code> at this tuple index. But the other three models don't have <code>pooler_output</code>.</p>
<p>So, how can I add additional layers to the other three model outputs?</p>
","2274492","","","","","2020-09-22 07:56:04","Add additional layers to the Huggingface transformers","<python><tensorflow><keras><nlp><huggingface-transformers>","1","0","2","","","CC BY-SA 4.0"
"63413414","1","63422347","","2020-08-14 13:09:15","","1","976","<p>I am feeding sentences to a BERT model (Hugging Face library). These sentences get tokenized with a pretrained tokenizer. I know that you can use the decode function to go back from tokens to strings.</p>
<pre><code>string = tokenizer.decode(...)
</code></pre>
<p>However, the reconstruction is not perfect. If you use an uncased pretrained model, the uppercase letters get lost. Also, if the tokenizer splits a word into 2 tokens, the second token will start with '##'. For example, the word 'coronavirus' gets split into 2 tokens: 'corona' and '##virus'.</p>
<p>So my question is: is there a way to get the indices of the substring from which every token is created?
For example, take the string &quot;Tokyo to report nearly 370 new coronavirus cases, setting new single-day record&quot;. The 9th token is the token corresponding to 'virus'.</p>
<pre><code>['[CLS]', 'tokyo', 'to', 'report', 'nearly', '370', 'new', 'corona', '##virus', 'cases', ',', 'setting', 'new', 'single', '-', 'day', 'record', '[SEP]']
</code></pre>
<p>I want something that tells me that the token '##virus' comes from the 'virus' substring in the original string, which is located between the indices 37 and 41 of the original string.</p>
<pre><code>sentence = &quot;Tokyo to report nearly 370 new coronavirus cases, setting new single-day record&quot;
print(sentence[37:42]) # --&gt; outputs 'virus
</code></pre>
","14105340","","14105340","","2020-08-14 13:28:32","2020-08-15 03:39:16","Is there a way to get the location of the substring from which a certain token has been produced in BERT?","<tokenize><bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"66041246","1","","","2021-02-04 07:41:47","","0","69","<p>I follow this tutorial <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">How to train a new language model from scratch using Transformers and Tokenizers</a>.</p>
<p>In Section 2. Train a tokenizer, after training by my own Vietnamese text data, I look at the .vocab file generated, all the tokens become like this:</p>
<pre><code>&quot;Ä Ã„Ä³&quot;:268,&quot;nh&quot;:269,&quot;Ã¡Â»Â§&quot;:270,&quot;ÃƒÅ‚&quot;:271,&quot;Ä ch&quot;:272,&quot;iÃ¡Â»&quot;:273,&quot;ÃƒÂ¡&quot;:274,&quot;Ä l&quot;:275,&quot;Ä b&quot;:276,&quot;Ã†Â°&quot;:277,&quot;Ä h&quot;:278,&quot;Ã¡ÂºÂ¿&quot;:279,
</code></pre>
<p>any idea to fix this?</p>
","1501120","","6664872","","2021-02-20 13:23:05","2021-02-20 13:23:05","Encoding error: Train BERT from scratch in Vietnamese language","<encoding><tokenize><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","1","","","","CC BY-SA 4.0"
"60881999","1","","","2020-03-27 08:03:16","","2","467","<p>I am using <code>tensorflow 2.1.0</code> and  <code>transformers 2.5.1</code> on MacOS with <code>python 3.7</code>. I am bulding a Keras model using <code>TFBertForSequenceClassification</code>:</p>

<pre><code>model = TFBertForSequenceClassification.from_pretrained('bert-base-cased',
                                                        num_labels=number_label)
model.compile(optimizer=optimizer,
                  loss=loss, 
                  metrics=[metric])
</code></pre>

<p>and I can explore the structure:</p>

<pre><code>model.summary()
</code></pre>

<p>and we can see the following</p>

<pre><code>Model: ""tf_bert_for_sequence_classification""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
bert (TFBertMainLayer)       multiple                  108310272 
_________________________________________________________________
dropout_37 (Dropout)         multiple                  0         
_________________________________________________________________
classifier (Dense)           multiple                  1538      
=================================================================
Total params: 108,311,810
Trainable params: 108,311,810
Non-trainable params: 0
</code></pre>

<p>As we can see from above we only see the 3 main layers and we don't have access to more details.</p>

<p>After fitting the  model (not sure why but if we don't do that the input and outpu variable will be empty) we can access:</p>

<pre><code>model.inputs

{'attention_mask': &lt;tf.Tensor 'attention_mask:0' shape=(None, 128) dtype=int32&gt;,
 'input_ids': &lt;tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32&gt;,
 'token_type_ids': &lt;tf.Tensor 'token_type_ids:0' shape=(None, 128) dtype=int32&gt;}

model.outputs

[&lt;tf.Tensor 'tf_bert_for_sequence_classification/Identity:0' shape=(None, 2) dtype=float32&gt;]
</code></pre>

<p>This is a good start and now I would like to explore the keras layers:</p>

<pre><code>model.layers

[&lt;transformers.modeling_tf_bert.TFBertMainLayer at 0x1a415fd7d0&gt;,
 &lt;tensorflow.python.keras.layers.core.Dropout at 0x1a445c3550&gt;,
 &lt;tensorflow.python.keras.layers.core.Dense at 0x1a445c3890&gt;]
</code></pre>

<p>but now if I try to get more info, it is always empty:</p>

<pre><code>for layer in model.layers:
    print(layer.name, layer._inbound_nodes, layer._outbound_nodes)

bert [] []
dropout_37 [] []
classifier [] []
</code></pre>

<p>I tried some other method like <code>inbound_nodes</code> but it is always empty!</p>

<p>Is there some way to be able to inspect in more details the layer of some complicated model like BERT ?
What is the reason we got empty information ?</p>

<p>I also tried:</p>

<pre><code> tf.keras.utils.plot_model(model,
                          'model.png',
                          show_shapes=True)
</code></pre>

<p>but what I get is not very hepfull:</p>

<p><a href=""https://i.stack.imgur.com/qbdsY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qbdsY.png"" alt=""enter image description here""></a></p>

<p>Same using TensorBoard that give me one graph and many disconnected elements
<a href=""https://i.stack.imgur.com/tALBX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tALBX.png"" alt=""enter image description here""></a></p>

<p>Yes as an option, I good look at the code directly: <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_bert.py"" rel=""nofollow noreferrer"">code</a> but I tought it will be possible programatically to inspect the structure of such complex model.</p>
","6430839","","","","","2020-03-27 08:03:16","TFBertForSequenceClassification Keras model.layers information details are empty ? How to inspect the model?","<python><tensorflow><keras><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"63407347","1","","","2020-08-14 06:00:09","","-1","116","<p>Since my custom QA dataset is small, does it makes sense to mix it with the Squad and fine tune BERT on this &quot;augmented&quot; squad</p>
","14008819","","","","","2020-08-14 07:25:39","Using BERT with custom QA dataset","<python><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63932600","1","","","2020-09-17 06:43:14","","0","363","<p>Is there any way to add entities to a spacy doc object using BERT's offsets ? Problem is my whole pipeline is spacy dependent and i am using the latest PUBMEDBERT for which spacy doesnt provide support.</p>
<p>So at times  the offsets of entities given by pubmedbert dont result into a valid SPAN for spacy as the tokenization is completely different.</p>
<p>what work have i done till now to solve my problem ?
I made a custom tokenizer by asking spacy to split on punctuation, similar to bert but there are certain cases wherein i just cant make a rule. for example:-</p>
<pre><code>text = '''assessment
Exdtve age-rel mclr degn, left eye, with actv chrdl neovas
Mar-10-2020
assessment'''
</code></pre>
<p>Pubmedbert predicted 13:17 to be an entity i.e. dtve
but on adding the span as entity in spacy doc object it results NONE as it is not a valid span.</p>
<pre><code>span = doc.char_span(row['start'], row['end'], row['ent'])
doc.ents = list(doc.ents) + [span]
TypeError: object of type 'NoneType' has no len()
</code></pre>
<p>Consider row['start'] to be 13, row['end'] to be 17 and row['ent'] to be label</p>
<p>how can i solve this problem ? is there anyway i can just add entities in spacy doc object using starting and ending offset given by pubmedbert</p>
<p>would really appreciate any help on this, Thank you.</p>
","13612551","","13612551","","2020-09-18 07:25:55","2020-09-18 13:36:02","Adding entites to spacy doc object using BERT's offsets","<python><data-science><spacy><bert-language-model><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"65763465","1","","","2021-01-17 16:58:08","","1","1007","<p>I am working on a binary classification task and would like to try adding lstm layer on top of the last hidden layer of huggingface BERT model, however, I couldn't reach the last hidden layer. Is it possible to combine BERT with LSTM?</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = BertTokenizer.from_pretrained(model_path)
tain_inputs, train_labels, train_masks = data_prepare_BERT(
    train_file, lab2ind, tokenizer, content_col, label_col, 
    max_seq_length)
validation_inputs, validation_labels, validation_masks = data_prepare_BERT(
    dev_file, lab2ind, tokenizer, content_col, label_col,max_seq_length)

# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.
model = BertForSequenceClassification.from_pretrained(
    model_path, num_labels=len(lab2ind))
</code></pre>
","15025011","","5652313","","2021-01-18 12:33:45","2021-01-18 12:43:57","How to add LSTM layer on top of Huggingface BERT model","<nlp><pytorch><recurrent-neural-network><bert-language-model><huggingface-transformers>","1","5","","","","CC BY-SA 4.0"
"62671668","1","62766944","","2020-07-01 07:25:04","","1","1017","<p>If I am using the tensorflow version of huggingface transformer, how do I freeze the weights of the pretrained encoder so that only the weights of the head layer are optimized?</p>
<p>For the PyTorch implementation, it is done through</p>
<pre><code>for param in model.base_model.parameters():
    param.requires_grad = False
</code></pre>
<p>Would like to do the same for tensorflow implementation.</p>
","13732618","","","","","2021-06-29 05:18:14","How to freeze TFBertForSequenceClassification pre trained model?","<tensorflow><huggingface-transformers>","4","0","1","","","CC BY-SA 4.0"
"60918676","1","","","2020-03-29 18:10:55","","1","216","<p>I'm trying to use a small pre-trained GPT2 model on wikipedia text. I try to use as much text as I can as the input for the gpt2 model. The model summarizes the text for me. How do I use the whole wiki article for input. As it is now I'm limited to something like 768 tokens. A typical wiki article is longer than that. Is there a trick to using text passages longer than 768 tokens?</p>
","859559","","","","","2020-04-03 05:07:27","GPT2 input size with wiki articles","<python-3.x><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60848403","1","","","2020-03-25 11:59:39","","-1","105","<p>I am trying to follow <a href=""https://github.com/huggingface/transformers#quick-tour-of-the-fine-tuningusage-scripts"" rel=""nofollow noreferrer"">this</a> instructions.
I downloaded the Glue dataset, and I am trying to run this command</p>

<pre><code>python ./examples/run_glue.py \
    --model_type bert \
    --model_name_or_path bert-base-uncased \
    --task_name MRPC \
    --do_train \
    --do_eval \
    --do_lower_case \
    --data_dir C:/Git/RemoteDGX/MRPC/glue_data/MRPC \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/MRPC/
</code></pre>

<p>I am running the command from pycharm, so I use this configuration.</p>

<p>When I press the run command:</p>

<pre><code>C:\Git\PythonEnv\Scripts\python.exe C:/Git/RemoteDGX/transformers/examples/run_glue.py --model_type bert --model_name_or_path bert-base-uncased --task_name MRPC --do_train --do_eval --do_lower_case --data_dir C:/Git/RemoteDGX/MRPC/glue_data/MRPC --max_seq_length 128 --per_gpu_eval_batch_size=8 --per_gpu_train_batch_size=8 --learning_rate 2e-5 --num_train_epochs 3.0 --output_dir /tmp/MRPC/
</code></pre>

<p>But I am getting this error:</p>

<pre><code>ImportError: cannot import name 'MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING' from 'transformers' 
(C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\transformers\__init__.py)
</code></pre>

<p>By the error, I see that the interperter is trying to find transformers in (C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\transformers__init__.py.</p>

<p>What am I doing wrong? I set the configuration accordingly</p>
","12918011","","","","","2020-03-26 08:20:07","Pycharm interperter uses the wrong path","<python><pycharm><huggingface-transformers>","1","3","","","","CC BY-SA 4.0"
"61022109","1","","","2020-04-03 23:57:54","","1","741","<p>Usually, a transformers tokenizer encodes an input as a dictionary.</p>

<pre class=""lang-py prettyprint-override""><code>{""input_ids"": tf.int32, ""attention_mask"": tf.int32, ""token_type_ids"": tf.int32}
</code></pre>

<p>And to archive better performance handling with a large dataset, it is a good practice to implement a pipeline which includes using <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"" rel=""nofollow noreferrer""><code>Dataset.map</code></a> to apply a tokenizer function to each element of an input dataset. Exactly the same as done in the Tensorflow tutorial: <a href=""https://www.tensorflow.org/tutorials/load_data/text#encode_examples"" rel=""nofollow noreferrer"">Load text</a>. </p>

<p>However, the <a href=""https://www.tensorflow.org/api_docs/python/tf/py_function"" rel=""nofollow noreferrer""><code>tf.py_function</code></a> (used to wrap the map python function) doesn't support returning a dictionary of tensors as shown above.</p>

<p>For instance, if the tokenizer (encoder) in the <a href=""https://www.tensorflow.org/tutorials/load_data/text#encode_examples"" rel=""nofollow noreferrer"">Load text</a> returns the following dictionary:</p>

<pre class=""lang-py prettyprint-override""><code>{
    ""input_ids"": [ 101, 13366,  2131,  1035,  6819,  2094,  1035,  102 ],
    ""attention_mask"": [ 1, 1, 1, 1, 1, 1, 1, 1 ]
}
</code></pre>

<p>how can someone set the <code>Tout</code> parameter of the <a href=""https://www.tensorflow.org/api_docs/python/tf/py_function"" rel=""nofollow noreferrer""><code>tf.py_function</code></a> to get the desired dictionary of tensors:</p>

<pre class=""lang-py prettyprint-override""><code>{
    'input_ids': &lt;tf.Tensor: shape=(16,), dtype=int32, numpy = array(
    [ 101, 13366,  2131,  1035,  6819,  2094,  1035,  102 ], dtype=int32)&gt;

    'attention_mask': &lt;tf.Tensor: shape=(16,), dtype=int32, numpy=array(
     [ 1, 1, 1, 1, 1, 1, 1, 1 ], dtype=int32)&gt;
}
</code></pre>

<p>?</p>
","8414280","","","","","2020-04-04 04:51:48","How to return a dictionary of tensors from tf.py_function?","<python-3.x><tensorflow2.0><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"62784202","1","","","2020-07-07 21:29:10","","0","473","<p>I am following instructions on this webpage (<a href=""https://github.com/huggingface/transformers/tree/master/examples/language-modeling"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/language-modeling</a>) to fine tune the pretrained BERT model on my own corpus. I managed to run the example using the WikiText-103 dataset (<a href=""https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip</a>) that, once unpacked, takes about 550MB. I managed to run this example with 48GB memory and a K80.</p>
<p>I then tried to do this using my own corpus, with is 20 times larger. I know that with the same hardware config it would not be possible, so I tested chunking my corpus into different equal sizes to figure out what would be the maximum corpus size that the language model fine tuning can run on a 128GB memory + K80 setting.</p>
<p>As indicated in the comment below, I also used the following settings:</p>
<ul>
<li>use_fast=True to use a 'faster' tokenizer</li>
<li>toggle '--line_by_line'</li>
</ul>
<p>But with these settings, the maximum corpus I can fit is a 3.5GB chunk, for 128GB memory and K80. Beyond this, the program stops with OOM when it is doing <code>INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file</code>, i.e., it has not even started the GPU processing yet.</p>
<p>I would like to understand a little how the language model fine tuning code works - what is the dataset-memory ratio, and if it is possible to configure it so it does not use so much memory for processing the dataset.</p>
<p>Many thanks</p>
","1783398","","1783398","","2020-07-08 14:53:42","2020-07-08 14:53:42","Language model fine tuning BERT with run_language_modeling.py - Reduce memory usage?","<huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"60937617","1","61909224","","2020-03-30 18:58:03","","5","4596","<p>I've been looking to use Hugging Face's Pipelines for NER (named entity recognition). However, it is returning the entity labels in inside-outside-beginning (IOB) format but <a href=""https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)"" rel=""nofollow noreferrer"">without the IOB labels</a>. So I'm not able to map the output of the pipeline back to my original text. Moreover, the outputs are masked in BERT tokenization format (the default model is BERT-large).</p>

<p>For example: </p>

<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
nlp_bert_lg = pipeline('ner')
print(nlp_bert_lg('Hugging Face is a French company based in New York.'))
</code></pre>

<p>The output is:</p>

<pre><code>[{'word': 'Hu', 'score': 0.9968873858451843, 'entity': 'I-ORG'},
{'word': '##gging', 'score': 0.9329522848129272, 'entity': 'I-ORG'},
{'word': 'Face', 'score': 0.9781811237335205, 'entity': 'I-ORG'},
{'word': 'French', 'score': 0.9981815814971924, 'entity': 'I-MISC'},
{'word': 'New', 'score': 0.9987512826919556, 'entity': 'I-LOC'},
{'word': 'York', 'score': 0.9976728558540344, 'entity': 'I-LOC'}]
</code></pre>

<p>As you can see, New York is broken up into two tags.</p>

<p>How can I map Hugging Face's NER Pipeline back to my original text?</p>

<p>Transformers version: 2.7</p>
","2573069","","2573069","","2020-04-14 14:32:16","2020-12-18 07:13:25","How to reconstruct text entities with Hugging Face's transformers pipelines without IOB tags?","<nlp><tokenize><transformer><ner><huggingface-transformers>","2","2","2","","","CC BY-SA 4.0"
"61306391","1","61670209","","2020-04-19 14:54:36","","0","518","<p>I have a question regarding the usage of ALBERT with the SQuAD 2.0 huggingface-transformers script.</p>

<p>In the github page, there are no specific instructions in how to run the script using ALBERT, so I used the same specifications used to run the script with BERT. 
However, the final results achieved are (exact_match = 30.632527583593028, f1 = 36.36948708435092), far from the (f1 = 88.52, exact_match = 81.22) that are achieved by BERT and that are reported on the github page. So I think that I may be doing something wrong.</p>

<p>This is the code that I ran in the command line: </p>

<pre><code>python run_squad.py \
   --model_type albert \
   --model_name_or_path albert-base-v2 \
   --do_train   --do_eval \
   --train_file train-v2.0.json \
   --predict_file dev-v2.0.json \
   --per_gpu_train_batch_size 5 \
   --learning_rate 3e-5 \
   --num_train_epochs 2.0 \
   --max_seq_length 384 \
   --doc_stride 128 \
   --output_dir /aneves/teste2/output/
</code></pre>

<p>The only difference between this one and the one from the transformers page is the model_name, in which they use 'bert_base_uncased', and the per_gpu_train_batch_size which is 12 but I had to use 5 due to memory constrains in my GPU. </p>

<p>Am I forgetting some option when I run the script or are the results achieved because of the per_gpu_train_batch_size being set to 5 instead of 12?</p>

<p>Thanks!</p>
","13354784","","","","","2020-05-08 04:58:59","Running SQuAD script using ALBERT (huggingface-transformers)","<python><deep-learning><huggingface-transformers><squad>","2","0","","","","CC BY-SA 4.0"
"61680408","1","","","2020-05-08 13:35:10","","0","518","<p>My VS Code Editor for Python is not able to import transformers even though I have done a conda install and giving me the following error</p>

<pre><code>Traceback (most recent call last):
  File ""c:/Users/I323017/Documents/Studies/question_answering_kinnal/src/main.py"", line 3, in &lt;module&gt;
    import transformers
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\transformers\__init__.py"", line 107, in &lt;module&gt;
    from .pipelines import (
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\transformers\pipelines.py"", line 40, in &lt;module&gt;
    from .tokenization_auto import AutoTokenizer
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\transformers\tokenization_auto.py"", line 49, in &lt;module&gt;
    from .tokenization_flaubert import FlaubertTokenizer
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\transformers\tokenization_flaubert.py"", line 23, in &lt;module&gt;
    from .tokenization_xlm import XLMTokenizer
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\transformers\tokenization_xlm.py"", line 26, in &lt;module&gt;
    import sacremoses as sm
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\sacremoses\__init__.py"", line 2, in &lt;module&gt;
    from sacremoses.tokenize import *
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\sacremoses\tokenize.py"", line 10, in &lt;module&gt;
    from sacremoses.util import is_cjk
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\sacremoses\util.py"", line 11, in &lt;module&gt;
    from joblib import Parallel, delayed
ModuleNotFoundError: No module named 'joblib'
</code></pre>

<p>May I know the problem here?</p>
","3152686","","3152686","","2020-05-08 13:41:05","2021-04-14 18:10:49","Huggingface Transformers not getting imported in VS Code","<python><visual-studio-code><python-import><vscode-settings><huggingface-transformers>","3","0","","","","CC BY-SA 4.0"
"61832308","1","61832614","","2020-05-16 05:31:46","","0","250","<p>I am trying to convert a tf checkpoint to a pytorch checkpoint using <code>transformers-cli</code> as following</p>

<p><code>transformers-cli convert model_type bert --tf_checkpoint bio_bert_large_1000k.ckpt --config bert_config_bio_58k_large.json --pytorch_dump_output pytorch_model.bin</code></p>

<p>and am getting the following error</p>

<pre><code>usage: transformers-cli &lt;command&gt; [&lt;args&gt;] convert [-h] --model_type
                                               MODEL_TYPE --tf_checkpoint
                                               TF_CHECKPOINT
                                               --pytorch_dump_output
                                               PYTORCH_DUMP_OUTPUT
                                               [--config CONFIG]
                                               [--finetuning_task_name FINETUNING_TASK_NAME]
transformers-cli &lt;command&gt; [&lt;args&gt;] convert: error: the following arguments are required: --model_type
</code></pre>

<p>What am I doing wrong?</p>
","9074767","","","","","2020-05-16 06:09:44","transformers-cli error: the following arguments are required: --model_type","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62337074","1","","","2020-06-12 03:36:05","","1","173","<p>I am using BERT for Named Entity Recognition. Initially I had only 18 labels, and I trained the model using the 18 labels and saved the model. Now I added 2 more new labels, and when I updated the previously saved model I am getting the following error:</p>

<pre class=""lang-sh prettyprint-override""><code>C:/w/1/s/windows/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:106: block: [0,0,0
], thread: [0,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
CUDA error: device-side assert triggered
Traceback (most recent call last):
  File ""C:\Users\jk2446\Desktop\jeril\repos\jk2446-phoenix\apps\utilities\utils.py"", line 48, in catch_errors
    return func(*args, **kwargs)
  File ""C:\Users\jk2446\Desktop\jeril\repos\jk2446-phoenix\apps\utilities\bert_utils.py"", line 414, in start_training
    loss.backward()
  File ""C:\Users\jk2446\AppData\Roaming\Python\Python36\site-packages\torch\tensor.py"", line 166, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""C:\Users\jk2446\AppData\Roaming\Python\Python36\site-packages\torch\autograd\__init__.py"", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
</code></pre>

<p>The following is my code:</p>

<pre class=""lang-py prettyprint-override""><code>model = BertForTokenClassification.from_pretrained(model_dir)
# inititalizing the model to use GPU
if torch.cuda.is_available():
    __ = model.cuda()
    torch.cuda.empty_cache()

# finetuning the model
FULL_FINETUNING = True
if FULL_FINETUNING:
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(
            nd in n for nd in no_decay)],
         'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(
            nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]
else:
    param_optimizer = list(model.classifier.named_parameters())
    optimizer_grouped_parameters = [
        {""params"": [p for n, p in param_optimizer]}]
optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)

model.train()
tr_loss = 0
nb_tr_examples, nb_tr_steps = 0, 0
for step, batch in enumerate(train_dataloader):
    # add batch to gpu
    batch = tuple(t.to(device) for t in batch)
    b_input_ids, b_input_mask, b_labels = batch
    b_input_ids, b_input_mask, b_labels = b_input_ids.long(
    ), b_input_mask.long(), b_labels.long()
    # forward pass
    loss, scores = model(b_input_ids, token_type_ids=None,
                         attention_mask=b_input_mask, labels=b_labels)
    # backward pass
    loss.backward()
    # track train loss
    tr_loss += loss.item()
    nb_tr_examples += b_input_ids.size(0)
    nb_tr_steps += 1
    # gradient clipping
    torch.nn.utils.clip_grad_norm_(
        parameters=model.parameters(), max_norm=max_grad_norm)
    # update parameters
    optimizer.step()
    model.zero_grad()
# print train loss per epoch
train_loss = tr_loss / nb_tr_steps
print(""Train loss: {}"".format(train_loss))
</code></pre>

<p>Is there a way to update the already trained BERT model with the new labels? Kindly help.</p>
","2825570","","3607203","","2020-06-12 11:36:47","2020-06-12 11:36:47","Adding new labels to an already trained BERT model","<huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"60832547","1","60834242","","2020-03-24 13:58:05","","5","2625","<p>I see some github comments saying the output of the model() call's loss is in the form of perplexity:
<a href=""https://github.com/huggingface/transformers/issues/473"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/473</a></p>

<p>But when I look at the relevant code...
<a href=""https://huggingface.co/transformers/_modules/transformers/modeling_openai.html#OpenAIGPTLMHeadModel.forward"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/_modules/transformers/modeling_openai.html#OpenAIGPTLMHeadModel.forward</a></p>

<pre><code>    if labels is not None:
        # Shift so that tokens &lt; n predict n
        shift_logits = lm_logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        # Flatten the tokens
        loss_fct = CrossEntropyLoss()
        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        outputs = (loss,) + outputs

    return outputs  # (loss), lm_logits, (all hidden states), (all attentions)
</code></pre>

<p>I see cross entropy being calculated, but no transformation into perplexity. Where does the loss finally get transformed? Or is there a transformation already there that I'm not understanding?</p>
","947659","","306739","","2020-07-01 06:39:18","2020-07-01 06:39:18","Where is perplexity calculated in the Huggingface gpt2 language model code?","<machine-learning><huggingface-transformers><gpt><perplexity>","1","2","2","","","CC BY-SA 4.0"
"62598130","1","","","2020-06-26 15:36:31","","2","2209","<p>I have sentences that I vectorize using sentence_vector() method of BiobertEmbedding python module (<a href=""https://pypi.org/project/biobert-embedding/"" rel=""nofollow noreferrer"">https://pypi.org/project/biobert-embedding/</a>). For some group of sentences I have no problem but for some others I have the following error message :</p>
<blockquote>
<p>File
&quot;/home/nobunaga/.local/lib/python3.6/site-packages/biobert_embedding/embedding.py&quot;,
line 133, in sentence_vector
encoded_layers = self.eval_fwdprop_biobert(tokenized_text)   File &quot;/home/nobunaga/.local/lib/python3.6/site-packages/biobert_embedding/embedding.py&quot;,
line 82, in eval_fwdprop_biobert
encoded_layers, _ = self.model(tokens_tensor, segments_tensors)   File
&quot;/home/nobunaga/.local/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;,
line 547, in __call__
result = self.forward(*input, **kwargs)   File &quot;/home/nobunaga/.local/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py&quot;,
line 730, in forward
embedding_output = self.embeddings(input_ids, token_type_ids)   File
&quot;/home/nobunaga/.local/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;,
line 547, in __call__
result = self.forward(*input, **kwargs)   File &quot;/home/nobunaga/.local/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py&quot;,
line 268, in forward
position_embeddings = self.position_embeddings(position_ids)   File
&quot;/home/nobunaga/.local/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;,
line 547, in __call__
result = self.forward(*input, **kwargs)   File &quot;/home/nobunaga/.local/lib/python3.6/site-packages/torch/nn/modules/sparse.py&quot;,
line 114, in forward
self.norm_type, self.scale_grad_by_freq, self.sparse)   File &quot;/home/nobunaga/.local/lib/python3.6/site-packages/torch/nn/functional.py&quot;,
line 1467, in embedding
return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) RuntimeError: index out of range: Tried to
access index 512 out of table with 511 rows. at
/pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237</p>
</blockquote>
<p>I discovered that for some group of sentences, the problem was related to tags like <code>&lt;tb&gt;</code> for instance. But for others, even when tags are removed, the error message is still there.<br />
(Unfortunately I can't share the code for confidentiality reasons)</p>
<p>Do you have any ideas of what could be the problem?</p>
<p>Thank you by advance</p>
<p>EDIT : you are right cronoik, it will be better with an example.</p>
<p>Example :</p>
<pre><code>sentences = [&quot;This is the first sentence.&quot;, &quot;This is the second sentence.&quot;, &quot;This is the third sentence.&quot;

biobert = BiobertEmbedding(model_path='./biobert_v1.1_pubmed_pytorch_model')

vectors = [biobert.sentence_vector(doc) for doc in sentences]
</code></pre>
<p>This last line of code is what caused the error message in my opinion.</p>
","11240107","","11240107","","2020-06-26 17:04:23","2020-06-27 22:39:21","Pytorch error ""RuntimeError: index out of range: Tried to access index 512 out of table with 511 rows""","<python-3.x><pytorch><vectorization><word-embedding><huggingface-transformers>","2","1","","","","CC BY-SA 4.0"
"61888473","1","","","2020-05-19 10:11:06","","0","55","<p>I am doing a  binary classification task. It consists in classifying Amazon reviews. I am using FlauBERT( french version of BERT) for language modelling in order to produce my vectors. So Following the steps provide in github, I try to finetune FlauBERT on my task using the library ""transformers"" of hugging face but I am encountering this error and I cannot seems to find the solution. I read others post on it but so far, everything I tried failed. I checked the encodage of the data I used and they are encode in ""utf-8"". I change the encodage when reading the files to cp1252 but still sames answer. Apperently the problem lies with my data but I can seem to figure out how to overcame this error.</p>

<pre><code>Traceback (most recent call last):
  File ""transformers/examples/run_flue.py"", line 782, in &lt;module&gt;
    main()
  File ""transformers/examples/run_flue.py"", line 737, in main
    cache_dir=args.cache_dir if args.cache_dir else None,
  File ""/home/anaconda3/envs/env/lib/python3.6/site-packages/transformers/configuration_utils.py"", line 188, in from_pretrained
    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File ""/home/anaconda3/envs/env/lib/python3.6/site-packages/transformers/configuration_utils.py"", line 240, in get_config_dict
    config_dict = cls._dict_from_json_file(resolved_config_file)
  File ""/home/anaconda3/envs/env/lib/python3.6/site-packages/transformers/configuration_utils.py"", line 329, in _dict_from_json_file
    text = reader.read()
  File ""/home/anaconda3/envs/env/lib/python3.6/codecs.py"", line 321, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte


</code></pre>
","7294253","","681865","","2020-05-19 10:19:26","2020-05-19 10:19:26","Transformers errors (encoding errors) when trying to finetune FlauBERT","<python><python-3.x><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"62350438","1","","","2020-06-12 18:34:25","","0","544","<p>I am trying to train a BERT model from scratch using this blog post:
<a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">https://huggingface.co/blog/how-to-train</a></p>

<p>I am trying to train my model on random walks from graph data. Essentially the nodes are the words and going from one node to the next forms the sentence. Because of this I don't want to break up any of the words (nodes) into subparts like you would for normal language because the nodes are just represented by numbers. So I tried creating my own tokenizer by first creating a custom vocab.json file that lists all of the words by frequency in a dictionary and then wrote a custom tokenizer:</p>

<pre><code>from transformers.tokenization_utils import PreTrainedTokenizer
class RandomWalkTokenizer(PreTrainedTokenizer):

    #copied rest from BertTokenizer

    def batch_encode_plus(self, text, **kwargs):
        """"""
        text: must be a list of lines you want to encode
        """"""
        tokenized_lines = []

        for i in text:
            tokenized_lines.append(self._tokenize(i))
        return {'input_ids': tokenized_lines}

    def _tokenize(self, text):
        if type(text) == str:
            tokenized_text = []

            for i in text.split(' '):
                tokenized_text.append(self.vocab.get(i, ""[UNK]""))


        return tokenized_text
</code></pre>

<p>Then create the dataset by: </p>

<pre><code>from transformers import LineByLineTextDataset

dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=""./walks.txt"",
    block_size=128,
)
</code></pre>

<p>With walks.txt just being lines of space separated numbers like: </p>

<pre><code>10096244043 10079425660 10111609222 10111609222
2462116941 10015483987 2462116941 10012741942
</code></pre>

<p>The lines are much longer but follow the same pattern</p>

<p>Start the model with:</p>

<pre><code>from transformers import RobertaConfig

config = RobertaConfig(
    vocab_size=tokenizer.vocab_size,
    max_position_embeddings=114,
    num_attention_heads=12,
    num_hidden_layers=2,
    type_vocab_size=1
)
from transformers import RobertaForMaskedLM

model = RobertaForMaskedLM(config=config)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=""./outBert"",
    overwrite_output_dir=True,
    num_train_epochs=1,
    save_steps=10_000,
    save_total_limit=2,
    #per_gpu_train_batch_size=64,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    prediction_loss_only=True,
)

trainer.train()

</code></pre>

<p>But I keep getting:</p>

<pre><code>ValueError: You have to specify either input_ids or inputs_embeds
</code></pre>

<p>I think my fundamental issue is that I'm not sure if I'm creating the tokenizer correctly and I'm not sure how I should be putting in the masks into the training dataset when it gets created. Any tips on how to create a simple tokenizer that doesn't break down any words into subpieces would be much appreciated. </p>
","8305682","","8305682","","2020-06-16 17:19:05","2020-06-16 17:19:05","HuggingFace Bert on Graph Data","<python><huggingface-transformers><graphml><bert-language-model>","0","6","","","","CC BY-SA 4.0"
"61000500","1","61386745","","2020-04-02 20:25:20","","1","2504","<p>I'm attempting to fine-tune the HuggingFace TFBertModel to be able to classify some text to a single label. I have the model up and running, however the accuracy is extremely low from the start. My expectation is that the accuracy would be high given that it is using the BERT pre-trained weights as a starting point. I was hoping to get some advice on where I'm going wrong.</p>

<p>I'm using the bbc-text dataset from <a href=""https://www.kaggle.com/yufengdev/bbc-text-categorization"" rel=""nofollow noreferrer"">here</a>:</p>

<p><strong>Load Data</strong></p>

<pre><code>df = pd.read_csv(open(&lt;s3 url&gt;),encoding='utf-8', error_bad_lines=False)
df = df.sample(frac=1)
df = df.dropna(how='any')
</code></pre>

<p><strong>Value Counts</strong></p>

<pre><code>sport            511
business         510
politics         417
tech             401
entertainment    386
Name: label, dtype: int64
</code></pre>

<p><strong>Preprocessing</strong></p>

<pre><code>def preprocess_text(sen):
# Convert html entities to normal
sentence = unescape(sen)

# Remove html tags
sentence = remove_tags(sentence)

# Remove newline chars
sentence = remove_newlinechars(sentence)

# Remove punctuations and numbers
sentence = re.sub('[^a-zA-Z]', ' ', sentence)

# Convert to lowercase
sentence = sentence.lower()

return sentence


def remove_newlinechars(text):
    return "" "".join(text.splitlines()) 

def remove_tags(text):
    TAG_RE = re.compile(r'&lt;[^&gt;]+&gt;')
    return TAG_RE.sub('', text)

df['text_prepd'] = df['text'].apply(preprocess_text)
</code></pre>

<p><strong>Split Data</strong></p>

<pre><code>train, val = train_test_split(df, test_size=0.30, shuffle=True, stratify=df['label'])
</code></pre>

<p><strong>Encode Labels</strong></p>

<pre><code>from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_train = np.asarray(le.fit_transform(train['label']))
y_val = np.asarray(le.fit_transform(val['label']))

</code></pre>

<p><strong>Define BERT input function</strong></p>

<pre><code># Initialise Bert Tokenizer
bert_tokenizer_transformer = BertTokenizer.from_pretrained('bert-base-cased')

def create_input_array(df, tokenizer, args):
    sentences = df.text_prepd.values

    input_ids = []
    attention_masks = []
    token_type_ids = []

    for sent in tqdm(sentences):
        # `encode_plus` will:
        #   (1) Tokenize the sentence.
        #   (2) Prepend the `[CLS]` token to the start.
        #   (3) Append the `[SEP]` token to the end.
        #   (4) Map tokens to their IDs.
        #   (5) Pad or truncate the sentence to `max_length`
        #   (6) Create attention masks for [PAD] tokens.
        encoded_dict = tokenizer.encode_plus(
            sent,  # Sentence to encode.
            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
            max_length=args.max_seq_len,  # Pad &amp; truncate all sentences.
                pad_to_max_length=True,
                return_attention_mask=True,  # Construct attn. masks.
                return_tensors='tf',  # Return tf tensors.
            )

        # Add the encoded sentence to the list.
        input_ids.append(encoded_dict['input_ids'])

        # And its attention mask (simply differentiates padding from non-padding).
        attention_masks.append(encoded_dict['attention_mask'])

        token_type_ids.append(encoded_dict['token_type_ids'])

    input_ids = tf.convert_to_tensor(input_ids)
    attention_masks = tf.convert_to_tensor(attention_masks)
    token_type_ids = tf.convert_to_tensor(token_type_ids)

    return input_ids, attention_masks, token_type_ids

</code></pre>

<p><strong>Convert Data to Bert Inputs</strong></p>

<pre><code>train_inputs = [create_input_array(train[:], tokenizer=tokenizer, args=args)]
val_inputs = [create_input_array(val[:], tokenizer=tokenizer, args=args)]
</code></pre>

<p>For <code>train_inputs, y_train</code> and <code>val_inputs, y_val</code> I then apply the below function which reshapes and converts to numpy arrays. The returned list from this function is then passed as arguments to the keras fit method. I realise this is a bit overkill converting to tf.tensors then to numpy, but I don't think this has an impact. I was originally trying to use tf.datasets but switched to numpy.</p>

<pre><code>def convert_inputs_to_tf_dataset(inputs,y, args):
    # args.max_seq_len = 256
    ids = inputs[0][1]
    masks = inputs[0][1]
    token_types = inputs[0][2]

    ids = tf.reshape(ids, (-1, args.max_seq_len))
    print(""Input ids shape: "", ids.shape)
    masks = tf.reshape(masks, (-1, args.max_seq_len))
    print(""Input Masks shape: "", masks.shape)
    token_types = tf.reshape(token_types, (-1, args.max_seq_len))
    print(""Token type ids shape: "", token_types.shape)

    ids=ids.numpy()
    masks = masks.numpy()
    token_types = token_types.numpy()

    return [ids, masks, token_types, y]
</code></pre>

<p><strong>Keras Model</strong></p>

<pre><code># args.max_seq_len = 256
# n_classes = 6
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', trainable=True, num_labels=n_classes)

input_ids_layer = Input(shape=(args.max_seq_len, ), dtype=np.int32)
input_mask_layer = Input(shape=(args.max_seq_len, ), dtype=np.int32)
input_token_type_layer = Input(shape=(args.max_seq_len,), dtype=np.int32)

bert_layer = model([input_ids_layer, input_mask_layer, input_token_type_layer])[0]
flat_layer = Flatten()(bert_layer)
dropout= Dropout(0.3)(flat_layer)
dense_output = Dense(n_classes, activation='softmax')(dropout)

model_ = Model(inputs=[input_ids_layer, input_mask_layer, input_token_type_layer], outputs=dense_output)

</code></pre>

<p><strong>Compile and Fit</strong></p>

<pre><code>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer='adam', loss=loss, metrics=[metric])
model.fit(inputs=..., outputs=..., validation_data=..., epochs=50, batch_size = 32, metrics=metric, verbose=1)


Epoch 32/50
1401/1401 [==============================] - 42s 30ms/sample - loss: 1.6103 - accuracy: 0.2327 - val_loss: 1.6042 -
 val_accuracy: 0.2308
</code></pre>

<p>As I'm using BERT, only a few epochs are necessary, so I was expecting something much higher than 23% after 32 epochs.</p>
","7605543","","7605543","","2020-04-22 18:03:52","2021-06-10 08:21:15","Tensorflow/Keras/BERT MultiClass Text Classification Accuracy","<python><tensorflow><machine-learning><keras><huggingface-transformers>","2","7","","","","CC BY-SA 4.0"
"62125405","1","62126272","","2020-06-01 03:49:58","","0","946","<p>I am using the bert on the SMILE dataset. I have written following code can you guide me where I am getting wrong.
I have written training code which is evaluating correctly but the when I try to run evaluate code for validation it is giving error. I tried to pass the parameters directly to cuda. still I am facing the issue</p>

<pre><code>'''

def evaluate(dataloader_val):

  print(""in evaluate"")
  model.eval()

  loss_val_total = 0
  predictions, true_value = [],[]

  for batch in dataloader_val:

    print(""in for loop of dataloader"")
    barch = tuple(b.to(device) for b in batch)

    inputs = {
               'input_ids':  batch[0],
                'attention_mask': batch[1],
                 'labels' : batch[2],
    }

    with torch.no_grad():
      outputs = model(**inputs)

    loss = outputs[0]
    logits = outputs[1]
    loss_val_total += loss.item()

    print(""before logit"")

    logits = logits.to(device)
    print(""in the for batch evaluate: "",logits)
    label_ids = inputs['labels'].to(device)
    true_vals.append(label_ids)

  loss_val_avg = loss_val_total/len(dataloader_val)

  predictions = np.concatenate(predictions, axis = 0)
  true_vals = np.concatenate(true_vals,axis = 0)

  return loss_val_avg, predictions, true_vals
'''
</code></pre>

<p>and another function is</p>

<pre><code>'''
for epoch in tqdm(range(1, epochs+1)):
  model.train()

  loss_train_total = 0

  progress_bar = tqdm(dataloader_train,
                      desc = 'Epoch {:1d}'.format(epoch),
                      leave = False,
                      disable = False)
  for batch in progress_bar:

    model.zero_grad()

    batch = tuple(b.to(device) for b in batch)

    inputs = {

            'input_ids'      : batch[0],
            'attention_mask' : batch[1],
            'labels'         : batch[2]
    }

    outputs = model(**inputs)

    loss = outputs[0]
    loss_train_total += loss.item()
    loss.backward()

    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    optimizer.step()
    scheduler.step()

    progress_bar.set_postfix({'training_loss' : '{:.3f}'.format(loss.item()/len(batch))})

  torch.save(model.state_dict(), f'/content/drive/My Drive/Bert/Coursera/SMILE/Bert_ft_epoch{epoch}.model')

  tqdm.write(f'\n Epoch {epoch}')

  loss_train_avg = loss_train_total / len(dataloader_train)

  tqdm.write(f'Training Loss: {loss_train_avg}')

  val_loss, predictions, true_vals = evaluate(dataloader_val)
  val_f1 = f1_score_func(predictions, true_vals)
  tqdm.write(f'Validation loss : {val_loss}')
  tqdm.write(f'F1 score(weighted): {val_f1}')
'''
</code></pre>
","9624182","","","","","2020-06-01 05:36:16","RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select site:stackoverflow.com","<python><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62142122","1","","","2020-06-01 22:43:30","","1","225","<p>I was trying to build a <a href=""https://arxiv.org/abs/1706.05098"" rel=""nofollow noreferrer"">multi-task deep neural network</a> using <a href=""https://huggingface.co/jplu/tf-xlm-roberta-large"" rel=""nofollow noreferrer"">xlm roberta large model</a> for a multilingual classification problem. my training dataset contains 4 columns : </p>

<ol>
<li><p>ID</p></li>
<li><p>comment_text (according to id number,each users english comment is stored in this column. example comment : ""you are a loser"")</p></li>
<li><p>toxic (this column contains 1/0,0 means not toxic,1 means toxic)</p></li>
<li><p>personal_attack(this column also contains 0/1,,0 means the comment is not a personal attack type comment and 1 means opposite)</p></li>
</ol>

<p>here is my models code : </p>

<pre><code>def build_model(transformer, max_len=512):
    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=""input_word_ids"")
    sequence_output = transformer(input_word_ids)[0]
    cls_token = sequence_output[:, 0, :]
    out = Dense(1, activation='sigmoid',name = 'y_train')(cls_token)
    out1 = Dense(1, activation='sigmoid',name = 'y_aux')(cls_token)
    model = Model(inputs=input_word_ids, outputs=[out, out1])
    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])

    return model
</code></pre>

<p>here is the code for train and test dataset : </p>

<pre><code>train_dataset = (
tf.data.Dataset
.from_tensor_slices((x_train,

{ 'y_train':train.toxic.values,
'y_aux':train.identity_attack.values}

))
.repeat()
.shuffle(2048)
.batch(BATCH_SIZE)
.prefetch(AUTO)
)

test_dataset = (
    tf.data.Dataset
    .from_tensor_slices(x_test)
    .batch(BATCH_SIZE)
)
</code></pre>

<p>then for training model i used this code : </p>

<pre><code>EPOCHS = 3
n_steps = x_train.shape[0] // BATCH_SIZE
train_history = model.fit(
    train_dataset,
    steps_per_epoch=n_steps,
    epochs=EPOCHS
)
</code></pre>

<p>i don't wish to perform validation so just train_dataset was given for model.fit()</p>

<p>after 3 epoch i get performance like this : </p>

<pre><code>Epoch 3/3
1658/1658 [==============================] - 887s 535ms/step - loss: 0.0591 - y_train_loss: 0.0175 - y_aux_loss: 0.0416 - y_train_accuracy: 0.9940 - y_aux_accuracy: 0.9821
</code></pre>

<p>now in my test set i have 1 columns : </p>

<ol>
<li>comments( this column contains comments of non english language (remember in train set we only had English comments and here in test set all comments are non english)</li>
</ol>

<p>so i expect my model to predict on these test set whether the given test set comment is toxic or not?
as you can see from 3rd epoch result that i am calculating y_train_accuracy: 0.9940 - y_aux_accuracy: 0.9821</p>

<p>now i want my model to predict y_test or toxic/not toxic only</p>

<p>for that i tried : </p>

<pre><code>sub['toxic'] = model.predict(test_dataset, verbose=1)
</code></pre>

<p>sub is a dataframe that contains all the id of test set and using <strong>test_dataset</strong> i was trying to predict each and every test set comments but i get this error : </p>

<pre><code>499/499 [==============================] - 126s 253ms/step
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-23-1dc84858379e&gt; in &lt;module&gt;
----&gt; 1 sub['toxic'] = model.predict(test_dataset, verbose=1)
      2 sub.to_csv('submission.csv', index=False)

/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __setitem__(self, key, value)
   2936         else:
   2937             # set column
-&gt; 2938             self._set_item(key, value)
   2939 
   2940     def _setitem_slice(self, key, value):

/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in _set_item(self, key, value)
   2998 
   2999         self._ensure_valid_index(value)
-&gt; 3000         value = self._sanitize_column(key, value)
   3001         NDFrame._set_item(self, key, value)
   3002 

/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in _sanitize_column(self, key, value, broadcast)
   3634 
   3635             # turn me into an ndarray
-&gt; 3636             value = sanitize_index(value, self.index, copy=False)
   3637             if not isinstance(value, (np.ndarray, Index)):
   3638                 if isinstance(value, list) and len(value) &gt; 0:

/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_index(data, index, copy)
    609 
    610     if len(data) != len(index):
--&gt; 611         raise ValueError(""Length of values does not match length of index"")
    612 
    613     if isinstance(data, ABCIndexClass) and not copy:

ValueError: Length of values does not match length of index
</code></pre>

<p>now i have 4 questions :</p>

<ol>
<li><p>is my implementation correct?</p></li>
<li><p>why i am getting that error? if i treat this problem as simple multilingual classification task like compute 1 loss for y true then i get no error at all,so where i am having trouble?</p></li>
<li><p>how can i solve the issue?</p></li>
<li><p>as it is my first time with multi task learning using huggingface transformers,what are your suggestions for updating my model so that it can generalize better?</p></li>
</ol>
","7563212","","","","","2020-06-01 22:43:30","how to make a multi-task deep neural network baseline using huggingface transformers?","<python><deep-learning><nlp><tensorflow2.0><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"62345557","1","","","2020-06-12 13:46:12","","2","1013","<p>I am trying to use torch multiprocessing to parallelize the predictions from two separate huggingface distillbert classification models. It seems to be deadlocked at the prediction step.  I am using python 3.6.5, torch 1.5.0 and huggingface transformers version 2.11.0.
The output from running the code is </p>

<pre><code>Tree enc done
Begin tree prediction&lt;------(Comment: Both begin tree
End tree predictions&lt;-------  and end tree predictions)
0.03125429153442383
Dn prediction
Dn enc done
Begin dn predictions&lt;------(Comment: Both begin dn
End dn predictions&lt;-------  and end dn predictions)
0.029727697372436523
----------Done sequential predictions-------------

--------Start Parallel predictions--------------
Tree prediction
Tree enc done
Begin tree prediction. &lt;------(Comment: Process is deadlocked after this)
Dn prediction
Dn enc done
Begin dn predictions. &lt;-------(Comment: Process is deadlocked after this)
</code></pre>

<p>During parallel predictions it seems to be deadlocking and not printing out ""End tree predictions"" and ""End dn predictions"". Not sure why this is happening. 
The code is </p>

<pre><code>import torch
import torch.multiprocessing as mp
import time
import transformers
from transformers import  DistilBertForSequenceClassification

# Load the BERT tokenizer.
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

tree_model = DistilBertForSequenceClassification.from_pretrained(
    ""distilbert-base-uncased"",
    num_labels = 2,
    output_attentions = False, 
    output_hidden_states = False
)
tree_model.eval()

dn_model = DistilBertForSequenceClassification.from_pretrained(
    ""distilbert-base-uncased"",
    num_labels = 2,
    output_attentions = False, 
    output_hidden_states = False, 
)
dn_model.eval()


tree_model.share_memory()
dn_model.share_memory()


def predict(sentences =[], tokenizer=tokenizer,models=(tree_model,dn_model,None)):
  MAX_SENTENCE_LENGTH = 16
  start = time.time()
  input_ids = []
  attention_masks = []
  predictions = []

  tree_model = models[0]
  dn_model = models[1]

  if models[0]:
      print(""Tree prediction"")
  if models[1]:
    print(""Dn prediction"")
  for sent in sentences:
    encoded_dict = tokenizer.encode_plus(
                        sent,                      
                        add_special_tokens = True, 
                        max_length = MAX_SENTENCE_LENGTH,
                        pad_to_max_length = True,
                        return_attention_mask = True,   
                        return_tensors = 'pt',     
                   )

    # Add the encoded sentence to the list.    
    input_ids.append(encoded_dict['input_ids'])

    # And its attention mask (simply differentiates padding from non-padding).
    attention_masks.append(encoded_dict['attention_mask'])

  if tree_model:
      print(""Tree enc done"")
  if dn_model:
    print(""Dn enc done"")

  # Convert the lists into tensors.
  new_input_ids = torch.cat(input_ids, dim=0)
  new_attention_masks = torch.cat(attention_masks, dim=0)

  with torch.no_grad():
      # Forward pass, calculate logit predictions
    if tree_model:
      print(""Begin tree prediction"")
      outputs = tree_model(new_input_ids, 
                      attention_mask=new_attention_masks) 
      print(""End tree predictions"")
    else:
      print(""Begin dn predictions"")
      outputs = dn_model(new_input_ids, 
                      attention_mask=new_attention_masks)
      print(""End dn predictions"")
  logits = outputs[0]
  logits = logits.detach().cpu()
  print(time.time()-start)
  predictions = logits
  return predictions



def get_tree_prediction(sentence, tokenizer=tokenizer,models=(tree_model,dn_model, None)):
    return predict(sentences =[sentence], tokenizer=tokenizer,models=models)

def get_dn_prediction(sentence, tokenizer=tokenizer,models=(tree_model,dn_model, None)):
  return predict(sentences =[sentence], tokenizer=tokenizer,models=models)


if __name__ == '__main__':
    sentence = ""hello world""
    processes = []
    get_tree_prediction(sentence, tokenizer, (tree_model,None,None))
    get_dn_prediction(sentence, tokenizer, (None,dn_model,None))
    print(""----------Done sequential predictions-------------"")

    print('\n--------Start Parallel predictions--------------')
    tr_p = mp.Process(target=get_tree_prediction, args=(sentence, tokenizer,
                                                         (tree_model,None,None)))

    tr_p.start()
    processes.append(tr_p)

    dn_p = mp.Process(target=get_dn_prediction, args=(sentence, tokenizer,
                                                       (None,dn_model,None)))
    dn_p.start()
    processes.append(dn_p)

    for p in processes:
        p.join()
</code></pre>
","3245722","","","","","2020-06-12 13:46:12","huggingface distillbert classification using multiprocessing","<pytorch><python-multiprocessing><huggingface-transformers><distilbert>","0","1","","","","CC BY-SA 4.0"
"62351295","1","","","2020-06-12 19:37:35","","0","239","<p>I have seen <a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=o6sASa36Nf-N"" rel=""nofollow noreferrer"">this tutorial on how to train a BERT model</a> from scratch on Hugging Face Transformers library. </p>

<p>I'm trying to train a GPT-2 model on 1.5 GB data on Google Colab. I load up all the data using this code:</p>

<pre><code>dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=""./my-1.5gb-large-file.txt"",
    block_size=128,
)
</code></pre>

<p>The file gets too large, training fails because of limited memory in the GPU. Is there a way I could train my GPT-2 model gradually by splitting the dataset?</p>
","1019952","","","","","2020-06-12 19:37:35","How to gradually train a model on transformers library?","<python><tensorflow><nlp><pytorch><huggingface-transformers>","0","4","","","","CC BY-SA 4.0"
"61990266","1","","","2020-05-24 18:10:51","","1","448","<p>good day,
I have used the wonderful library huggingface transformers to generate text with GPT2 and this works great:</p>

<pre><code>tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
input_ids = torch.tensor(tokenizer.encode(""Once upon a time there was"")).unsqueeze(0)
model = GPT2LMHeadModel.from_pretrained(""gpt2"", pad_token_id=tokenizer.eos_token_id)
greedy_output = model.generate(input_ids, max_length=50)
print(""Output:\n"" + 100 * '-')
print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))
</code></pre>

<p>My issue is that now I want to do the same but with the smaller simpler DistilmBERT model which is also multilingual in 104 languages, so I want to generate text in for example Spanish and English and with this lighter model</p>

<p>I have tried this</p>

<pre><code>tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')
model = DistilBertForMaskedLM.from_pretrained('distilbert-base-multilingual-cased')
input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, masked_lm_labels=input_ids)
loss, prediction_scores = outputs[:2]
</code></pre>

<p>but Im not sure if this is the right model to use. And once I get the outputs, how would I get the continuation of the phrase from that? </p>

<p>After more tests, I can get the generation working well with distilgpt2, the thing is that I would like to do it multilingual using the light multilingual model DistilmBERT  (distilbert-base-multilingual-cased), any tips?</p>

<pre><code>import torch
from transformers import *
from transformers import TFGPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
input_ids = torch.tensor(tokenizer.encode(""Once upon a time"")).unsqueeze(0)
model = GPT2LMHeadModel.from_pretrained(""distilgpt2"", pad_token_id=tokenizer.eos_token_id)
greedy_output = model.generate(input_ids, max_length=50) #greedy search

sample_outputs = model.generate(
    input_ids,
    do_sample=True, 
    max_length=50, 
    top_k=50, 
    top_p=0.95, 
    temperature=1,
    num_return_sequences=3
)

print(""Output:\n"" + 100 * '-')
for i, sample_output in enumerate(sample_outputs):
  print(""{}: {}"".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))`

</code></pre>

<p>thank you for your help :)</p>
","1070745","","3607203","","2020-05-25 12:43:58","2020-06-03 10:18:17","Using DistilBERT for generating sentences of text","<python><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"62352130","1","","","2020-06-12 20:38:42","","1","68","<p>The transformer models take as input token Ids, which are converted into embeddings. I am wondering how to input embeddings directly. </p>

<p>I am asking for both the Pytorch and Keras versions of the models. </p>
","3259896","","3259896","","2020-06-12 22:07:41","2020-06-12 22:07:41","How to feeding hidden state vectors from one transformer directly into a layer of different transformer","<tensorflow><keras><pytorch><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"64793859","1","","","2020-11-11 20:45:18","","0","184","<p>I'm using TFBertForTokenClassification to perform a NER task on the annotated corpus fo NER:
<a href=""https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus"" rel=""nofollow noreferrer"">https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus</a>.
The problem is that the O-Labels are the majority of all labels, then the accuracy is quite high as the model correctly predicts most of them.
So, when I try to predict the labels of a simple sentence, the network predict only the O Label for each token of the sentence, however in several tutorials in which it is used Pytorch (I am using Tensorflow), the predictions are good.
Probably there is a problem in my code, but I cannot figure out where is it.</p>
<p>The code is the following:</p>
<pre class=""lang-py prettyprint-override""><code># Import libraries
import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split
import math
import numpy as np

from transformers import (
    TF2_WEIGHTS_NAME,
    BertConfig,
    BertTokenizer,
    TFBertForTokenClassification,
    create_optimizer)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Config
MAX_LEN= 128
TRAIN_BATCH_SIZE = 32
VALID_BTCH_SIZE = 8
EPOCHS = 10
BERT_MODEL = 'bert-base-uncased'
MODEL_PATH = &quot;model.bin&quot;
TRAINING_FILE = &quot;../input/entity-annotated-corpus/ner_dataset.csv&quot;
TOKENIZER = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Create the padded input, attention masks, token type and labels
def get_train_data(text, tags):

    tokenized_text = []
    target_tags = []

    for index, token in enumerate(text):

        encoded_token = TOKENIZER.encode(
            token,
            add_special_tokens = False
        )

        encoded_token_len = len(encoded_token)

        tokenized_text.extend(encoded_token)
        target_tags.extend([tags[index]] * encoded_token_len)

    #truncation
    tokenized_text = tokenized_text[: MAX_LEN - 2]
    target_tags = target_tags[: MAX_LEN - 2]

    #[101] = [CLS] , [102] = [SEP]
    tokenized_text = [101] + tokenized_text + [102]
    target_tags = [0] + target_tags + [0]
    attention_mask = [1] * len(tokenized_text)
    token_type_ids = [0] * len(tokenized_text)

    #padding
    padding_len = int(MAX_LEN - len(tokenized_text))

    tokenized_text = tokenized_text + ([0] * padding_len)
    target_tags = target_tags + ([0] * padding_len)
    attention_mask = attention_mask + ([0] * padding_len)
    token_type_ids = token_type_ids + ([0] * padding_len)

    return (tokenized_text, target_tags, attention_mask,  token_type_ids)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Extract sentences from dataset
class RetrieveSentence(object):
    
    def __init__(self, data):
        self.n_sent = 1
        self.data = data
        self.empty = False
        function = lambda s: [(w, p, t) for w, p, t in zip(s[&quot;Word&quot;].values.tolist(),
                                                           s[&quot;POS&quot;].values.tolist(),
                                                           s[&quot;Tag&quot;].values.tolist())]
        self.grouped = self.data.groupby(&quot;Sentence #&quot;).apply(function)
        self.sentences = [s for s in self.grouped]
    
    def retrieve(self):
        try:
            s = self.grouped[&quot;Sentence: {}&quot;.format(self.n_sent)]
            self.n_sent += 1
            return s
        except:
            return None
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Load dataset and create one hot encoding for labels
df_data = pd.read_csv(TRAINING_FILE,sep=&quot;,&quot;,encoding=&quot;latin1&quot;).fillna(method='ffill')
Sentences = RetrieveSentence(df_data)

sentences_list = [&quot; &quot;.join([s[0] for s in sent]) for sent in Sentences.sentences]
labels = [ [s[2] for s in sent] for sent in Sentences.sentences]

tags_2_val = list(set(df_data[&quot;Tag&quot;]))
tag_2_idx = {t: i for i, t in enumerate(tags_2_val)}

id_labels = [[tag_2_idx.get(l) for l in lab] for lab in labels]
sentences_list = [sent.split() for sent in sentences_list]

# I removed the sentence n 41770 because it gave index problems
del labels[41770]
del sentences_list[41770]
del id_labels[41770]
</code></pre>
<pre class=""lang-py prettyprint-override""><code>encoded_text = []
encoded_labels = []
attention_masks = []
token_type_ids = []

for i in range(len(sentences_list)):

    text, labels, att_mask, tok_type = get_train_data(text = sentences_list[i], tags = id_labels[i])
    encoded_text.append(text)
    encoded_labels.append(labels)
    attention_masks.append(att_mask)
    token_type_ids.append(tok_type)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Convert from list to np array
encoded_text = np.array(encoded_text)
encoded_labels = np.array(encoded_labels)
attention_masks = np.array(attention_masks)
token_type_ids = np.array(token_type_ids)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Train Test split
X_train, X_valid, Y_train, Y_valid = train_test_split(encoded_text, encoded_labels, random_state=20, test_size=0.1)
Mask_train, Mask_valid, Token_ids_train, Token_ids_valid = train_test_split(attention_masks,token_type_ids ,random_state=20, test_size=0.1)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Aggregate the train and test set, then shuffle and batch the train set
def example_to_features(input_ids,attention_masks,token_type_ids,y):
  return {&quot;input_ids&quot;: input_ids,
          &quot;attention_mask&quot;: attention_masks,
          &quot;token_type_ids&quot;: token_type_ids},y

train_ds = tf.data.Dataset.from_tensor_slices((X_train,Mask_train,Token_ids_train,Y_train)).map(example_to_features).shuffle(1000).batch(32)
test_ds=tf.data.Dataset.from_tensor_slices((X_valid,Mask_valid,Token_ids_valid,Y_valid)).map(example_to_features).batch(1)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Load TFBertForTokenClassification with default config
config = BertConfig.from_pretrained(BERT_MODEL,num_labels=len(tags_2_val))
model = TFBertForTokenClassification.from_pretrained(BERT_MODEL, from_pt=bool(&quot;.bin&quot; in BERT_MODEL), config=config)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Add softmax layer, compute loss, optimizer and fit
model.layers[-1].activation = tf.keras.activations.softmax
model.summary()
optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])
history = model.fit(train_ds, epochs=3, validation_data=test_ds)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Prediction. Spoiler: the label predicted are O-Label
sentence = &quot;Hi , my name is Bob and I live in England&quot;
inputs = TOKENIZER(sentence, return_tensors=&quot;tf&quot;)
input_ids = inputs[&quot;input_ids&quot;]
inputs[&quot;labels&quot;] = tf.reshape(tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))) # Batch size 1
output = model(inputs)
</code></pre>
<p>The code is executed on a Kaggle notebook.
The transformer library version is 3.4.0</p>
<p>Many thanks in advance.</p>
","8265144","","","","","2020-11-11 20:45:18","TFBertForTokenClassification scoring only O labels on a NER task","<python><tensorflow><bert-language-model><huggingface-transformers><ner>","0","2","","","","CC BY-SA 4.0"
"61490051","1","","","2020-04-28 21:10:23","","2","94","<p>Can somebody tell me how exactly the wordpiece model work ? I am having some hard time trying to understand how exactly the wordpiece model is working. I understand the BPE that it is based on merging according the highest frequency pairs. After digging for hours on the internet and reading the paper. It is mentioned that in wordpiece we make the final merge according to what maximize the likelihood of the Language model we created.
How is this language model is created ? Is it created by Probability of Pair equal to Count of Pair / Total Count of Pairs or what ?
What i understand is that we want to measure which token pair minus separate tokens is the largest , like if we have ""de"" = 9 , ""d"" = 15 ""e"" = 12 and ""th"" = 10 , ""t"" = 12 ""h""= 12 , then we choose to merge token ""t"" and ""h"" as its 10-24 > 9-27. Am i right ? Please somebody correct me</p>
","13428524","","","","","2020-04-28 21:10:23","Wordpiece Tokenization Model","<nlp><tokenize><huggingface-transformers><bert-language-model>","0","0","","","","CC BY-SA 4.0"
"64044200","1","64044374","","2020-09-24 10:08:21","","1","525","<p>I am running into issues of evaluating huggingface's BERT model ('bert-base-uncased') on large input sequences.</p>
<pre><code>model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)
token_ids = [101, 1014, 1016, ...] # len(token_ids) == 33286
token_tensors = torch.tensor([token_ids]) # shape == [1, 33286]
segment_tensors = torch.tensor([[1] * len(token_ids)]) # shape == [1, 33286]
model(token_tensors, segment_tensors)

Traceback
self.model(token_tensors, segment_tensors)
  File &quot;/home/.../python3.8/site-packages/torch/nn/modules/module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/.../python3.8/site-packages/transformers/modeling_bert.py&quot;, line 824, in forward
    embedding_output = self.embeddings(
  File &quot;/home/.../python3.8/site-packages/torch/nn/modules/module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/.../python3.8/site-packages/transformers/modeling_bert.py&quot;, line 211, in forward
    embeddings = inputs_embeds + position_embeddings + token_type_embeddings
RuntimeError: The size of tensor a (33286) must match the size of tensor b (512) at non-singleton dimension 1
</code></pre>
<p>I noticed that <code>model.embeddings.positional_embeddings.weight.shape == (512, 768)</code>. I.e. when I restrict the input size to <code>model(token_tensors[:, :10], segment_tensors[:, :10])</code> it works. I am misunderstanding how the the <code>token_tensors</code> and <code>segment_tensors</code> should be shaped. I thought they should be sized <code>(batch_size, sequence_length)</code></p>
<p>Thanks for the help</p>
","5379182","","","","","2020-09-24 10:19:04","Pytorch BERT: Misshaped inputs","<python><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61073049","1","","","2020-04-07 04:52:45","","1","932","<p>I am trying out the transformers pipeline from huggingface </p>

<p><a href=""https://github.com/huggingface/transformers#installation"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers#installation</a></p>

<p><a href=""https://i.stack.imgur.com/j8p4E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j8p4E.png"" alt=""enter image description here""></a></p>

<p>When i run on my system, i get different result</p>

<pre><code>&gt;&gt;&gt; import transformers
&gt;&gt;&gt; transformers.__version__
'2.8.0'


I am running on Python 3.7.6 


&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; nlp = pipeline('sentiment-analysis')
Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00&lt;00:00, 77.3kB/s]
2020-04-08 18:04:33.862653: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-04-08 18:04:33.931454: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa52e6b7be0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-08 18:04:33.931486: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
&gt;&gt;&gt; nlp('We are very happy to include pipeline into the transformers repository.')
[{'label': 'NEGATIVE', 'score': 0.94570833}]
&gt;&gt;&gt; 
</code></pre>

<p>Anything i can look into ?</p>
","97651","","97651","","2020-04-08 10:06:16","2020-04-08 10:06:16","Transformers Pipeline from Huggingface","<python-3.x><huggingface-transformers>","0","7","","","","CC BY-SA 4.0"
"61382917","1","","","2020-04-23 08:42:40","","1","563","<p>I've been trying to play with <a href=""https://pytorch.org/hub/huggingface_pytorch-transformers/"" rel=""nofollow noreferrer"">PYTORCH-TRANSFORMERS</a> pre-trained model. With everything at default in the collab template, using <code>torch.hub.load()</code> from <code>huggingface/pytorch-transformers</code> as 'model' to <code>bert-base-uncased</code></p>
<p>code example</p>
<pre><code>import torch
model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')    # Download model and configuration from S3 and cache.
</code></pre>
<p>and I'm presented with this error</p>
<pre><code>Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-19-ad22a1a34951&gt; in &lt;module&gt;()
      1 import torch
----&gt; 2 model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'somethingelse')    # Download model and configuration from S3 and cache.
      3 model = torch.hub.load('huggingface/pytorch-transformers', 'model', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`
      4 model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased', output_attentions=True)  # Update configuration during loading
      5 assert model.config.output_attentions == True

13 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in decorator(arg)
    177       raise ValueError(
    178           '%s has already been registered to %s' %
--&gt; 179           (registered_name, _GLOBAL_CUSTOM_OBJECTS[registered_name]))
    180 
    181     if arg in _GLOBAL_CUSTOM_NAMES:

ValueError: Custom&gt;TFBertMainLayer has already been registered to &lt;class 'src.transformers.modeling_tf_bert.TFBertMainLayer'&gt;
</code></pre>
<p>I'm not really understanding what's going on.</p>
","9817556","","-1","","2020-06-20 09:12:55","2020-11-22 16:35:44","Custom>TFBertMainLayer has already been registered to <class 'src.transformers.modeling_tf_bert.TFBertMainLayer'> occurring when downloading a model","<python><tensorflow><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"62671054","1","","","2020-07-01 06:42:20","","1","178","<p>I have some problem when trying to run the example code from transformers github page (<a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers</a>) using geany, visual studio code or the terminal. I'm using python 3.6 on Windows. The code of the example is as follows:</p>
<pre><code>import tensorflow as tf
import tensorflow_datasets
from transformers import *

# Load dataset, tokenizer, model from pretrained model/vocabulary
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')
data = tensorflow_datasets.load('glue/mrpc')

# Prepare dataset for GLUE as a tf.data.Dataset instance
train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')
valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')
train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)
valid_dataset = valid_dataset.batch(64)

# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Train and evaluate using tf.keras.Model.fit()
history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,
                    validation_data=valid_dataset, validation_steps=7)

# Load the TensorFlow model in PyTorch for inspection
model.save_pretrained('./save/')
pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)

# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task
sentence_0 = &quot;This research was consistent with his findings.&quot;
sentence_1 = &quot;His findings were compatible with this research.&quot;
sentence_2 = &quot;His findings were not compatible with this research.&quot;
inputs_1 = tokenizer(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')
inputs_2 = tokenizer(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')

pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()
pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()

print(&quot;sentence_1 is&quot;, &quot;a paraphrase&quot; if pred_1 else &quot;not a paraphrase&quot;, &quot;of sentence_0&quot;)
print(&quot;sentence_2 is&quot;, &quot;a paraphrase&quot; if pred_2 else &quot;not a paraphrase&quot;, &quot;of sentence_0&quot;)

input('Press any key to quit')

</code></pre>
<p>When I run that example, the program get stuck for a long time (several minutes to more than an hour) before producing a very long error message for several times:</p>
<pre><code>Traceback (most recent call last):
  File 'C:\Program Files\Python36\lib\site-packages\urllib3\connectionpool.py', line 677, in urlopen
    chunked=chunked,
  File 'C:\Program Files\Python36\lib\site-packages\urllib3\connectionpool.py', line 381, in _make_request
    self._validate_conn(conn)
  File 'C:\Program Files\Python36\lib\site-packages\urllib3\connectionpool.py', line 976, in _validate_conn
    conn.connect()
  File 'C:\Program Files\Python36\lib\site-packages\urllib3\connection.py', line 370, in connect
    ssl_context=context,
  File 'C:\Program Files\Python36\lib\site-packages\urllib3\util\ssl_.py', line 377, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File 'C:\Program Files\Python36\lib\ssl.py', line 407, in wrap_socket
    _context=self, _session=session)
  File 'C:\Program Files\Python36\lib\ssl.py', line 817, in __init__
    self.do_handshake()
  File 'C:\Program Files\Python36\lib\ssl.py', line 1077, in do_handshake
    self._sslobj.do_handshake()
  File 'C:\Program Files\Python36\lib\ssl.py', line 689, in do_handshake
    self._sslobj.do_handshake()
TimeoutError: [WinError 10060] &lt;&lt;Some Chinese charaters&gt;&gt;

During handling of the above exception, another exception occurred:
</code></pre>
<p>The chinese charaters says 'the server didn't response in time, connection failed'. So it's probably a problem with internet. So how can I connect to the server, or download the files required by this program by other means?</p>
","13845284","","13845284","","2020-07-04 05:06:35","2020-07-04 05:06:35","program stucks when running transformers example code","<python><nlp><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"66034490","1","","","2021-02-03 19:33:57","","0","209","<p>I'm trying to run a binary supervised text classification task using BERT, but I'm not sure how to do that.
I have tried to run BERT with the Hugging Face (ðŸ¤—) library, but I have no idea what to do with the output of the process.</p>
<p>After a lot of internet searches I ended up with the following class (according to <a href=""https://towardsdatascience.com/build-a-bert-sci-kit-transformer-59d60ddd54a5"" rel=""nofollow noreferrer"">https://towardsdatascience.com/build-a-bert-sci-kit-transformer-59d60ddd54a5</a>):</p>
<pre class=""lang-py prettyprint-override""><code>class BertTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
        self.model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)
        self.model.eval()
        self.embedding_func = lambda x: x[0][:, 0, :].squeeze()

    def _tokenize(self, text: str):
        # Tokenize the text with the provided tokenizer
        tokenized_text = self.tokenizer.encode_plus(text,
                                                    add_special_tokens=True,
                                                    truncation=True
                                                    )[&quot;input_ids&quot;]

        # Create an attention mask telling BERT to use all words
        attention_mask = [1] * len(tokenized_text)

        # bert takes in a batch so we need to unsqueeze the rows
        return (
            torch.tensor(tokenized_text).unsqueeze(0),
            torch.tensor(attention_mask).unsqueeze(0),
        )

    def _tokenize_and_predict(self, text: str) -&gt; torch.tensor:
        tokenized, attention_mask = self._tokenize(text)

        embeddings = self.model(tokenized, attention_mask)
        return self.embedding_func(embeddings)

    def transform(self, text: List[str]):
        if isinstance(text, pd.Series):
            text = text.tolist()

        with torch.no_grad():
            return torch.stack([self._tokenize_and_predict(string) for string in text])

    def fit(self, X, y=None):
        return self
</code></pre>
<ol>
<li><p>This class suitable for use in Sikict-Learn which is good for me, but I want also to run it with deep learning models using Keras. How can I make this work with Keras's neural networks (such as RNN and CNN)?</p>
</li>
<li><p>From what I understand, the above code takes only the CLS token and not all of the tokens. I don't know if that's alright. Maybe I should use all of them? If so, how can I do that?</p>
</li>
</ol>
<p>Any help would be appreciated.</p>
","12798000","","8893595","","2021-02-04 11:58:49","2021-02-08 15:38:21","Using BERT and Keras's neural network for text classification","<python><pytorch><text-classification><bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"62386631","1","62386694","","2020-06-15 10:47:14","","0","4370","<p>I am trying to import BertModel from transformers, but it fails. This is code I am using</p>

<pre><code>from transformers import BertModel, BertForMaskedLM
</code></pre>

<p>This is the error I get</p>

<pre><code>ImportError: cannot import name 'BertModel' from 'transformers'
</code></pre>

<p>Can anyone help me fix this?</p>
","12988629","","3607203","","2020-06-15 12:44:56","2020-06-21 22:12:48","Cannot import BertModel from transformers","<python><nlp><pytorch><huggingface-transformers><bert-language-model>","2","0","","","","CC BY-SA 4.0"
"61465103","1","61492100","","2020-04-27 17:47:33","","5","6459","<p>(I'm following <a href=""https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"" rel=""nofollow noreferrer"">this</a> pytorch tutorial about BERT word embeddings, and in the tutorial the author is access the intermediate layers of the BERT model.)</p>

<p>What I want is to access the last, lets say, 4 last layers of a single input token of the BERT model in TensorFlow2 using HuggingFace's Transformers library. Because each layer outputs a vector of length 768, so the last 4 layers will have a shape of <code>4*768=3072</code> (for each token).</p>

<p>How can I implement this in TF/keras/TF2, to get the intermediate layers of pretrained model for an input token? (later I will try to get the tokens for each token in a sentence, but for now one token is enough).</p>

<p>I'm using the HuggingFace's BERT model:</p>

<pre><code>!pip install transformers
from transformers import (TFBertModel, BertTokenizer)

bert_model = TFBertModel.from_pretrained(""bert-base-uncased"")  # Automatically loads the config
bert_tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")
sentence_marked = ""hello""
tokenized_text = bert_tokenizer.tokenize(sentence_marked)
indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokenized_text)

print (indexed_tokens)
&gt;&gt; prints [7592]
</code></pre>

<p>The output is a token (<code>[7592]</code>), which should be the input of the for the BERT model.</p>
","3826374","","2099607","","2020-04-29 00:19:38","2020-04-29 00:19:38","How to get intermediate layers' output of pre-trained BERT model in HuggingFace Transformers library?","<tensorflow><keras><tensorflow2.0><huggingface-transformers><bert-language-model>","1","3","2","","","CC BY-SA 4.0"
"62131369","1","","","2020-06-01 11:42:20","","0","22","<p>I am trying to use <strong>ELECTRA</strong> model from <strong>HuggingFace</strong> library. However, I need to get the offsets for <code>ElectraTokenizer</code>, which can be done straightforward, according to <a href=""https://huggingface.co/transformers/_modules/transformers/tokenization_electra.html#ElectraTokenizer"" rel=""nofollow noreferrer"">docs</a>. Does anyone know how can I get them? Any help is appreciated.</p>
","13494387","","","","","2020-06-01 11:42:20","How to get offsets for ElectraTokenizer","<nlp><tokenize><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"61408753","1","","","2020-04-24 12:49:31","","2","268","<p>Does anyone know if it is possible to use the T5 model with hugging face's mask-fill pipeline?  The below is how you can do it using the default model but i can't seem to figure out how to do is using the T5 model specifically? </p>

<pre><code>from transformers import pipeline
nlp_fill = pipeline('fill-mask')
nlp_fill('Hugging Face is a French company based in ' + nlp_fill.tokenizer.mask_token)
</code></pre>

<p>Trying this for example raises the error ""TypeError: must be str, not NoneType"" because nlp_fill.tokenizer.mask_token is None.</p>

<pre><code>nlp_fill = pipeline('fill-mask',model=""t5-base"", tokenizer=""t5-base"")
nlp_fill('Hugging Face is a French company based in ' + nlp_fill.tokenizer.mask_token)
</code></pre>
","3472360","","3472360","","2020-04-24 12:59:00","2020-04-24 12:59:00","Using the T5 model with huggingface's mask-fill pipeline","<neural-network><nlp><pytorch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"61717097","1","61718620","","2020-05-10 18:46:50","","0","2159","<p>I am using a model consisting of an embedding layer and an LSTM to perform sequence labelling, in pytorch + torchtext. I have already tokenised the sentences.</p>

<p>If I use self-trained or other pre-trained word embedding vectors, this is straightforward.</p>

<p>But if I use the Huggingface transformers <code>BertTokenizer.from_pretrained</code> and <code>BertModel.from_pretrained</code> there is a <code>'[CLS]'</code> and <code>'[SEP]'</code> token added to the beginning and end of the sentence, respectively. So the output of the model becomes a sequence that is two elements longer than the label/target sequence.</p>

<p>What I am unsure of is:</p>

<ol>
<li>Are these two tags needed for the <code>BertModel</code> to embed each token of a sentence ""correctly""?</li>
<li>If they are needed, can I take them out after the BERT embedding layer, before the input to the LSTM, so that the lengths are correct in the output?</li>
</ol>
","8487853","","","","","2020-05-10 20:55:46","Sequence Labelling with BERT","<pytorch><lstm><huggingface-transformers><torchtext>","1","0","2","","","CC BY-SA 4.0"
"62072536","1","62072749","","2020-05-28 19:01:44","","0","171","<p>I want to train BERT on a target corpus. I am looking at this <a href=""https://github.com/huggingface/transformers/tree/master/examples/language-modeling#robertabertdistilbert-and-masked-language-modeling"" rel=""nofollow noreferrer"">HuggingFace implementation</a>.
They are using .raw files for the training data. If I have .txt files of my training data, how can I use their implementation?</p>
","11103062","","4240413","","2020-05-28 19:04:24","2020-05-28 19:13:27","How to do language model training on BERT","<nlp><pytorch><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"61685555","1","","","2020-05-08 18:21:08","","1","415","<p>I am trying to execute :</p>

<pre class=""lang-py prettyprint-override""><code>import ktrain

from ktrain import text

MODEL_NAME='distilbert-base-uncased'

t=text.Transformer(MODEL_NAME, maxlen=500, classes=np.unique(y_train))
</code></pre>

<p>I am getting the following error:</p>

<pre><code>*OSError: Model name 'distilbert-base-uncased' was not found in tokenizers model name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). We assumed 'distilbert-base-uncased' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.*
</code></pre>

<p>ktrain version 0.14.6
transformers version 2.8.0</p>

<p>Libraries was installed using pip install.</p>

<p>Any help would be appreciated.</p>
","13500678","","4154062","","2020-05-13 10:12:37","2020-05-13 10:12:37","DISTILBERT_BASE_UNCASED failed to load - Hugging face transformers","<huggingface-transformers><bert-language-model>","0","0","","","","CC BY-SA 4.0"
"62341603","1","","","2020-06-12 09:42:35","","1","487","<p>I'm training dialoGPT on my own dataset, following this <a href=""https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30"" rel=""nofollow noreferrer"">tutorial</a>. </p>

<p>When I follow exactly the tutorial with the provided dataset I have no issues. I changed the example dataset. The only difference between the example and my code is that my dataset is 256397 lines long compared to the tutorialâ€™s 1906 lines. </p>

<p>I am not sure if the error is pertaining to my column labels in my dataset or if its an issue in one of the text values on a particular row, or the size of my data.</p>

<pre><code>06/12/2020 09:23:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
06/12/2020 09:23:10 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/config.json from cache at cached/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5
06/12/2020 09:23:10 - INFO - transformers.configuration_utils -   Model config GPT2Config {
  ""activation_function"": ""gelu_new"",
  ""architectures"": [
    ""GPT2LMHeadModel""
  ],
  ""attn_pdrop"": 0.1,
  ""bos_token_id"": 50256,
  ""embd_pdrop"": 0.1,
  ""eos_token_id"": 50256,
  ""initializer_range"": 0.02,
  ""layer_norm_epsilon"": 1e-05,
  ""model_type"": ""gpt2"",
  ""n_ctx"": 1024,
  ""n_embd"": 768,
  ""n_head"": 12,
  ""n_layer"": 12,
  ""n_positions"": 1024,
  ""resid_pdrop"": 0.1,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""vocab_size"": 50257
}

06/12/2020 09:23:11 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/config.json from cache at cached/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5
06/12/2020 09:23:11 - INFO - transformers.configuration_utils -   Model config GPT2Config {
  ""activation_function"": ""gelu_new"",
  ""architectures"": [
    ""GPT2LMHeadModel""
  ],
  ""attn_pdrop"": 0.1,
  ""bos_token_id"": 50256,
  ""embd_pdrop"": 0.1,
  ""eos_token_id"": 50256,
  ""initializer_range"": 0.02,
  ""layer_norm_epsilon"": 1e-05,
  ""model_type"": ""gpt2"",
  ""n_ctx"": 1024,
  ""n_embd"": 768,
  ""n_head"": 12,
  ""n_layer"": 12,
  ""n_positions"": 1024,
  ""resid_pdrop"": 0.1,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""vocab_size"": 50257
}

06/12/2020 09:23:11 - INFO - transformers.tokenization_utils -   Model name 'microsoft/DialoGPT-small' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'microsoft/DialoGPT-small' is a path, a model identifier, or url to a directory containing tokenizer files.
06/12/2020 09:23:15 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/vocab.json from cache at cached/78725a31b87003f46d5bffc3157ebd6993290e4cfb7002b5f0e52bb0f0d9c2dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
06/12/2020 09:23:15 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/merges.txt from cache at cached/570e31eddfc57062e4d0c5b078d44f97c0e5ac48f83a2958142849b59df6bbe6.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
06/12/2020 09:23:15 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/added_tokens.json from cache at None
06/12/2020 09:23:15 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/special_tokens_map.json from cache at None
06/12/2020 09:23:15 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/tokenizer_config.json from cache at None
06/12/2020 09:23:19 - INFO - filelock -   Lock 140392381680496 acquired on cached/9eab12d0b721ee394e9fe577f35d9b8b22de89e1d4f6a89b8a76d6e1a82bceae.906a78bee3add2ff536ac7ef16753bb3afb3a1cf8c26470f335b7c0e46a21483.lock
06/12/2020 09:23:19 - INFO - transformers.file_utils -   https://cdn.huggingface.co/microsoft/DialoGPT-small/pytorch_model.bin not found in cache or force_download set to True, downloading to /content/drive/My Drive/Colab Notebooks/cached/tmpj1dveq14
Downloading: 100%
351M/351M [00:34&lt;00:00, 10.2MB/s]
06/12/2020 09:23:32 - INFO - transformers.file_utils -   storing https://cdn.huggingface.co/microsoft/DialoGPT-small/pytorch_model.bin in cache at cached/9eab12d0b721ee394e9fe577f35d9b8b22de89e1d4f6a89b8a76d6e1a82bceae.906a78bee3add2ff536ac7ef16753bb3afb3a1cf8c26470f335b7c0e46a21483
06/12/2020 09:23:32 - INFO - transformers.file_utils -   creating metadata file for cached/9eab12d0b721ee394e9fe577f35d9b8b22de89e1d4f6a89b8a76d6e1a82bceae.906a78bee3add2ff536ac7ef16753bb3afb3a1cf8c26470f335b7c0e46a21483

06/12/2020 09:23:33 - INFO - filelock -   Lock 140392381680496 released on cached/9eab12d0b721ee394e9fe577f35d9b8b22de89e1d4f6a89b8a76d6e1a82bceae.906a78bee3add2ff536ac7ef16753bb3afb3a1cf8c26470f335b7c0e46a21483.lock
06/12/2020 09:23:33 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/microsoft/DialoGPT-small/pytorch_model.bin from cache at cached/9eab12d0b721ee394e9fe577f35d9b8b22de89e1d4f6a89b8a76d6e1a82bceae.906a78bee3add2ff536ac7ef16753bb3afb3a1cf8c26470f335b7c0e46a21483
06/12/2020 09:23:39 - INFO - transformers.modeling_utils -   Weights of GPT2LMHeadModel not initialized from pretrained model: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias']
06/12/2020 09:23:54 - INFO - __main__ -   Training/evaluation parameters &lt;__main__.Args object at 0x7fafa60a00f0&gt;
06/12/2020 09:23:54 - INFO - __main__ -   Creating features from dataset file at cached
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-12-523c0d2a27d3&gt; in &lt;module&gt;()
----&gt; 1 main(trn_df, val_df)

7 frames
&lt;ipython-input-11-d6dfa312b1f5&gt; in main(df_trn, df_val)
     59    # Training
     60    if args.do_train:
---&gt; 61        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)
     62 
     63        global_step, tr_loss = train(args, train_dataset, model, tokenizer)

&lt;ipython-input-9-3c4f1599e14e&gt; in load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate)
     40 
     41 def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):
---&gt; 42     return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)
     43 
     44 def set_seed(args):

&lt;ipython-input-9-3c4f1599e14e&gt; in __init__(self, tokenizer, args, df, block_size)
     24             self.examples = []
     25             for _, row in df.iterrows():
---&gt; 26                 conv = construct_conv(row, tokenizer)
     27                 self.examples.append(conv)
     28 

&lt;ipython-input-9-3c4f1599e14e&gt; in construct_conv(row, tokenizer, eos)
      1 def construct_conv(row, tokenizer, eos = True):
      2     flatten = lambda l: [item for sublist in l for item in sublist]
----&gt; 3     conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))
      4     conv = flatten(conv)
      5     return conv

&lt;ipython-input-9-3c4f1599e14e&gt; in &lt;listcomp&gt;(.0)
      1 def construct_conv(row, tokenizer, eos = True):
      2     flatten = lambda l: [item for sublist in l for item in sublist]
----&gt; 3     conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))
      4     conv = flatten(conv)
      5     return conv

/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py in encode(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, pad_to_max_length, return_tensors, **kwargs)
   1432             pad_to_max_length=pad_to_max_length,
   1433             return_tensors=return_tensors,
-&gt; 1434             **kwargs,
   1435         )
   1436 

/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py in encode_plus(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, pad_to_max_length, is_pretokenized, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, **kwargs)
   1574             )
   1575 
-&gt; 1576         first_ids = get_input_ids(text)
   1577         second_ids = get_input_ids(text_pair) if text_pair is not None else None
   1578 

/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py in get_input_ids(text)
   1554             else:
   1555                 raise ValueError(
-&gt; 1556                     ""Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.""
   1557                 )
   1558 

ValueError: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.
</code></pre>
","7514813","","13329963","","2020-06-17 08:29:43","2020-06-17 08:29:43","How should I format my dataset to avoid this? ""Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers""","<huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"61667142","1","61729996","","2020-05-07 20:27:57","","0","244","<p>This is a clarification question. I am trying to train <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">BERT provided by huggingface</a> using standard attention, and evaluate using a different attention definition.</p>

<p>The operation I was thinking about was change <code>bert-base-uncased</code> to the path of my trained model(using standard attention) in the following command, and run <code>--do_eval</code> under the installation of my customized attention version.</p>

<pre><code>export GLUE_DIR=/path/to/glue
export TASK_NAME=MRPC

python ./examples/run_glue.py \
    --model_name_or_path bert-base-uncased \
    --task_name $TASK_NAME \
    --do_eval \
    --data_dir $GLUE_DIR/$TASK_NAME \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/$TASK_NAME/
</code></pre>

<p>However, I was getting unexpected results. So I want to make sure that I was using the right command. Could anyone confirm with me or correct me?</p>

<p>Edited: The version was 2.8.0.</p>
","7756021","","7756021","","2020-05-07 21:32:49","2020-05-11 12:42:29","huggingface-transformers: Train BERT and evaluate it using different attentions","<transformer><huggingface-transformers>","1","6","","","","CC BY-SA 4.0"
"61667186","1","61945612","","2020-05-07 20:30:19","","1","113","<p>The recent implementation of the Reformer in HuggingFace has both what they call LSH Self Attention and Local Self Attention, but the difference is not very clear to me after reading <a href=""https://huggingface.co/transformers/model_doc/reformer.html"" rel=""nofollow noreferrer"">the documentation</a>. Both use bucketing to avoid the quadratic memory requirement of vanilla transformers, but it is not clear how they differ.</p>

<p>Is it the case that local self attention only allows queries to attend to keys sequentially near them (i.e., inside a given window in the sentence), as opposed to the proper LSH hashing that LSH self attention does? Or is it something else?</p>
","348412","","","","","2020-05-21 23:47:07","Reformer local and LSH attention in HuggingFace implementation","<pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61124443","1","61125328","","2020-04-09 15:16:00","","3","1965","<p>The below lets me get 5 suggestions for the masked token, but i'd like to get 10 suggestions - does anyone know if this is possible with hugging face?</p>

<pre><code>!pip install -q transformers
from __future__ import print_function
import ipywidgets as widgets
from transformers import pipeline

nlp_fill = pipeline('fill-mask')
nlp_fill(""I am going to guess &lt;mask&gt; in this sentence"")
</code></pre>
","3472360","","","","","2020-12-04 13:47:56","Using huggingface fill-mask pipeline to get more than 5 suggestions","<python><neural-network><nlp><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"61688282","1","61688470","","2020-05-08 21:21:14","","1","596","<p>Considering a batch of 4 pre-processed sentences (tokenization, numericalizing and padding) shown below:</p>

<pre class=""lang-py prettyprint-override""><code>batch = torch.tensor([
                      [1, 2, 0, 0],
                      [4, 0, 0, 0],
                      [3, 5, 6, 7]
])
</code></pre>

<p>where <code>0</code> states for <code>[PAD]</code> token.</p>

<p>Thus, what would be an efficient approach to generate a <code>padding masking</code> tensor of the same shape as the <code>batch</code> assigning zero at <code>[PAD]</code> positions and assigning one to other input data (sentence tokens)?</p>

<p>In the example above it would be something like:</p>

<pre class=""lang-py prettyprint-override""><code>padding_masking= 
tensor([
                      [1, 1, 0, 0],
                      [1, 0, 0, 0],
                      [1, 1, 1, 1]
])
</code></pre>
","8414280","","","","","2020-05-09 07:46:38","How to get padding mask from input ids?","<pytorch><huggingface-transformers><torchtext>","2","0","","","","CC BY-SA 4.0"
"60847291","1","60848331","","2020-03-25 10:50:01","","1","464","<p>It is the example given in the documentation of transformers pytorch library</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForTokenClassification
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForTokenClassification.from_pretrained('bert-base-uncased', 
                      output_hidden_states=True, output_attentions=True)

input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", 
                         add_special_tokens=True)).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)

loss, scores, hidden_states,attentions = outputs
</code></pre>

<p>Here <code>hidden_states</code> is a tuple of length 13 and contains hidden-states of the model at the output of each layer plus the initial embedding outputs. I would like to know, <strong>whether hidden_states[0] or hidden_states[12] represent the final hidden state vectors</strong>?</p>
","6151940","","3607203","","2020-03-25 11:17:08","2021-02-05 14:04:36","Confusion in understanding the output of BERTforTokenClassification class from Transformers library","<nlp><pytorch><huggingface-transformers><bert-language-model>","1","2","","","","CC BY-SA 4.0"
"61121982","1","","","2020-04-09 13:12:13","","1","830","<p>I am currently generating text from left context using the example script <code>run_generation.py</code> of the huggingface transformers library with gpt-2:</p>

<pre class=""lang-sh prettyprint-override""><code>$ python transformers/examples/run_generation.py \
  --model_type gpt2 \
  --model_name_or_path gpt2 \
  --prompt ""Hi, "" --length 5

=== GENERATED SEQUENCE 1 ===
Hi,  could anyone please inform me
</code></pre>

<p>I would like to generate short complete sentences. Is there any way to tell the model to finish a sentence before <code>length</code> words?</p>

<hr>

<p>Note: I don't mind changing model, but would prefer an auto-regressive one.</p>
","13268010","","1243762","","2020-11-29 11:58:17","2020-11-29 11:58:17","Asking gpt-2 to finish sentence with huggingface transformers","<nlp><pytorch><huggingface-transformers><gpt-2>","1","3","","","","CC BY-SA 4.0"
"62154230","1","","","2020-06-02 14:23:27","","3","670","<p>What's the general tradeoff between choosing BPE vs WordPiece Tokenization? When is one preferable to the other? Are there any differences in model performance between the two? I'm looking for a general overall answer, backed up with specific examples. Thanks!</p>
","190894","","","","","2020-06-02 14:23:27","BPE vs WordPiece Tokenization - when to use / which?","<machine-learning><nlp><lstm><transformer><huggingface-transformers>","0","2","1","","","CC BY-SA 4.0"
"62385092","1","62386837","","2020-06-15 09:19:02","","-1","267","<p>I need to compute words embeddings for a bunch of documents with different language models.
No problem with that, the script is doing fine, except I'm working on a notebook, without GPU and each text needs around 1.5s to be processed which is by far too long (I have thousands of texts to process).</p>

<p>Here is how I'm doing it with pytorch and transformers lib:</p>

<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import CamembertModel, CamembertTokenizer

docs = [text1, text2, ..., text20000]
tok = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertModel.from_pretrained('camembert-base', output_hidden_states=True)
# let try with a batch size of 64 documents
docids = [tok.encode(
  doc, max_length=512, return_tensors='pt', pad_to_max_length=True) for doc in docs[:64]]
ids=torch.cat(tuple(docids))
device = 'cuda' if torch.cuda.is_available() else 'cpu' # cpu in my case...
model = model.to(device)
ids = ids.to(device)
model.eval()
with torch.no_grad():
    out = model(input_ids=ids)
# 103s later...
</code></pre>

<p>Do someone has any idea or suggestions to improve speed?</p>
","1608467","","1608467","","2020-06-15 12:50:53","2020-06-15 12:50:53","How to improve code to speed up word embedding with transformer models?","<nlp><pytorch><word-embedding><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"61736317","1","","","2020-05-11 18:06:43","","1","1148","<p>I'm using huggingface transformer gpt-xl model to generate multiple responses. I'm trying to run it on multiple gpus because gpu memory maxes out with multiple larger responses. I've tried using dataparallel to do this but, looking at nvidia-smi it does not appear that the 2nd gpu is ever used. Here's my code:</p>

<pre><code>import numpy as np
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch
n_gpu=torch.cuda.device_count()
#device = xm.xla_device()
device=torch.device(""cuda:0"")
tokenizer = GPT2Tokenizer.from_pretrained('/spell/GPT2Model/GPT2Model/') #downloaded pre-trained model and tokenizer earlier
model = GPT2LMHeadModel.from_pretrained('/spell/GPT2Model/GPT2Model/')
model.to(device)
model = torch.nn.DataParallel(model, device_ids=[0,1])
encoded_prompt=tokenizer.encode(prompt_text, add_special_tokens=True,return_tensors=""pt"")
encoded_prompt = encoded_prompt.to(device)
outputs = model.module.generate(encoded_prompt,response_length,temperature=.8,num_return_sequences=num_of_responses,repetition_penalty=85,do_sample=True,top_k=80,top_p=.85 )
</code></pre>

<p>program gets oom on dual T4, memory of 2nd gpu never goes above 11M.</p>
","2628337","","","","","2020-05-11 18:06:43","huggingface transformers gpt2 generate multiple GPUs","<pytorch><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"64595546","1","","","2020-10-29 16:43:41","","0","628","<p>I am working on a project for text classification using BERT</p>
<p>I am getting ca. 90% accuracy by using simple transformers.
But I only get like 60% using my own training for-loop (not published here) or using the trainer module from the transformers library.
Both are done with the default parameters of simple transformers.</p>
<p>I am really struggling to understand why there is such a difference in performance</p>
<p>Dataset is from Kaggle: <a href=""https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news"" rel=""nofollow noreferrer"">https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news</a></p>
<p>Imports:</p>
<pre><code>from transformers import BertForSequenceClassification, AdamW, BertTokenizer, get_linear_schedule_with_warmup, Trainer, TrainingArguments
import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
import pandas as pd
from pathlib import Path
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
from torch.nn import functional as F
from collections import defaultdict
import random
from simpletransformers.classification import ClassificationModel
</code></pre>
<p>Data Pre-Processing:</p>
<pre><code>#loading phrase bank dataset
phrase_bank_dataset = &quot;all-data.csv&quot;
phrase_bank_dataset_file = Path(phrase_bank_dataset)
file_loaded = False
while not file_loaded:
  if phrase_bank_dataset_file.exists():
    phrase_bank_dataset = pd.read_csv(phrase_bank_dataset, encoding='latin-1')
    phrase_bank_dataset = phrase_bank_dataset.values.tolist()
    file_loaded = True
    print(&quot;Dataset Loaded&quot;)
  else:
    print(&quot;File not Found&quot;)
#correcting the format of phrase bank dataset
phrase_dataset = pd.DataFrame(columns=[&quot;news&quot;, &quot;sentiment&quot;])
for ele in phrase_bank_dataset:
  news = ele[1]
  #converting sentiment text into numbers
  sentiment = 0 if ele[0] == 'negative' else 1 if ele[0] == 'neutral' else 2
  row = [news, sentiment]
  phrase_dataset.loc[len(phrase_dataset)] = row
print(phrase_dataset)

</code></pre>
<p>Simple Transformers Code:</p>
<pre><code>model = ClassificationModel('bert', 'bert-base-cased', num_labels=3,use_cuda=True)
train,eva = train_test_split(labeled_dataset,test_size = 0.2)

train_df = pd.DataFrame({
    'text': train['news'],
    'label': train['sentiment']
})

eval_df = pd.DataFrame({
    'text': eva['news'],
    'label': eva['sentiment']
})

model.train_model(train_df)

result, model_outputs, wrong_predictions = model.eval_model(eval_df)

lst = []
for arr in model_outputs:
    lst.append(np.argmax(arr))
true = eval_df['label'].tolist()
predicted = lst
sklearn.metrics.accuracy_score(true,predicted)
</code></pre>
<p>Transformers Trainer Code:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-cased', stride = 0.8)
model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 3)

if torch.cuda.is_available():
  print(&quot;\nUsing: &quot;, torch.cuda.get_device_name(0))
  device = torch.device('cuda')
else:
  print(&quot;\nUsing: CPU&quot;)
  device = torch.device('cpu')
model = model.to(device)

#custom dataset class

class NewsSentimentDataset(torch.utils.data.Dataset):
  def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

  def __getitem__(self, idx):
      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
      item['labels'] = torch.tensor(self.labels[idx])
      return item

  def __len__(self):
      return len(self.labels)

#method for tokenizing dataset list

def tokenize_headlines(headlines, labels, tokenizer):

  encodings = tokenizer.batch_encode_plus(
      headlines,
      add_special_tokens = True,
      pad_to_max_length = True,
      return_attention_mask = True
  )

  dataset = NewsSentimentDataset(encodings, labels)
  return dataset

#splitting dataset into training and validation set
all_headlines = phrase_dataset['news'].tolist()
all_labels = phrase_dataset['sentiment'].tolist()

train_headlines, val_headlines, train_labels, val_labels = train_test_split(phrase_headlines, phrase_labels, test_size=.2)

val_dataset = tokenize_headlines(val_headlines, val_labels, tokenizer)
train_dataset = tokenize_headlines(train_headlines, val_labels, tokenizer)

#data loader
train_batch_size = 8
val_batch_size = 8

train_data_loader = DataLoader(train_dataset, batch_size = train_batch_size, sampler=RandomSampler(train_dataset))
val_data_loader = DataLoader(val_dataset, batch_size = val_batch_size, sampler=SequentialSampler(val_dataset))

#optimizer and scheduler
num_epochs = 1
num_steps = len(train_data_loader) * num_epochs
optimizer = AdamW(model.parameters(), lr=4e-5, eps=1e-8)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=30, num_training_steps=num_steps)

#training and evaluation with trainer moduel from huggingfaces

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=1,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=8,   # batch size for evaluation
    warmup_steps=0,                # number of warmup steps for learning rate scheduler
    weight_decay=0,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)


trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset  ,          # evaluation dataset
    compute_metrics=compute_metrics           
)

trainer.train()
trainer.evaluate()

</code></pre>
","13733783","","13733783","","2020-10-30 08:10:32","2020-10-30 08:10:32","Higher Accuracy using SimpleTransformers vs Transformers Library with BERT","<python><machine-learning><pytorch><huggingface-transformers><simpletransformers>","0","5","","","","CC BY-SA 4.0"
"64262302","1","","","2020-10-08 12:13:58","","0","380","<p>I was trying to utilize the <a href=""https://github.com/microsoft/unilm/tree/master/layoutlm"" rel=""nofollow noreferrer"">https://github.com/microsoft/unilm/tree/master/layoutlm</a> for document classification purpose, but was constantly getting <em>&quot;OSError: Unable to load weights from pytorch checkpoint file.&quot;</em></p>
<p>Can someone help me to run and work with layoutLM.</p>
<p>Configuration/Versions:</p>
<ul>
<li>Windows 10</li>
<li>Python - 3.6.5</li>
<li>huggingface-transformers - 3.1.0</li>
<li>pytorch - 1.5.0</li>
<li>tensorflow - 2.3.1</li>
</ul>
<p>command to run the code:</p>
<pre><code>python run_classification.py --data_dir C:\Users\Downloads\unilm-master\unilm-master\layoutlm\examples\classification\data --model_type layoutlm --output_dir C:\Users\Downloads\unilm-master\unilm-master\layoutlm\examples\classification\data --do_eval --model_name_or_path
</code></pre>
<p>I believe there are some issues with the command <code>--model_name_or_path</code>, I have tried the above method and tried downloading the pytorch_model.bin file for layoutlm and specifying it as an argument for <code>--model_name_or_path</code>, but of no help.
C:\Users\Downloads\unilm-master\unilm-master\layoutlm\examples\classification\model\pytorch_model.bin.</p>
<p>And also I doubt if it is because of the discrepancy between Transformer's support and layoutlm support (related to the version of tranformers 3.1.0 or 2.0.0)?</p>
<p>Can someone help me get up to speed with layoutLM.</p>
<p>Help is appreciated.</p>
","14413014","","6664872","","2020-10-08 20:47:33","2020-10-08 20:47:33","Microsoft LayoutLM model error with huggingface","<python-3.x><machine-learning><bert-language-model><huggingface-transformers><document-classification>","0","4","","","","CC BY-SA 4.0"
"61821515","1","","","2020-05-15 14:22:36","","3","672","<p>I want to solve stress prediction task with pretrained russian bert.</p>

<p>Input data looks like this:</p>

<blockquote>
  <p>Ð³Ñ€Ð°Ð¼Ð¼Ð¾Ð² ÑÐ²ÐµÑ€Ñ…Ñƒ|000100000001000</p>
</blockquote>

<p>Zeros mean no stress. Ones represent stress position character.</p>

<p>I want to map it as word -> vowel number index</p>

<p>So it will be like
Ð³Ñ€Ð°Ð¼Ð¼Ð¾Ð² -> 1
ÑÐ²ÐµÑ€Ñ…Ñƒ -> 1</p>

<p>So, for each token, it should be a linear layer with softmax.</p>

<p>I understand this part, but it's hard for me to deal with text preprocessing because text tokenizator can split a word into subword tokens.</p>

<p>Tokenizator takes an input string and returns tokens like this</p>

<pre><code>bert_tokenizer.encode('Ð³Ñ€Ð°Ð¼Ð¼Ð¾Ð² ÑÐ²ÐµÑ€Ñ…Ñƒ')
-&gt;
[101, 44505, 26656, 102]
</code></pre>

<p>So, how to get position mapping between input chars and words?</p>

<p>The desired output should be like [[0, 7], [8, 14]]</p>

<p>I tried to read docs
<a href=""https://huggingface.co/transformers/main_classes/tokenizer.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/tokenizer.html</a></p>

<p>And found convert_ids_to_tokens function
It works like</p>

<pre><code>encoded = bert_tokenizer.encode('Ð³Ñ€Ð°Ð¼Ð¼Ð¾Ð² ÑÐ²ÐµÑ€Ñ…Ñƒ')
bert_tokenizer.convert_ids_to_tokens(encoded)
-&gt;
['[CLS]', 'Ð³Ñ€Ð°Ð¼Ð¼Ð¾Ð²', 'ÑÐ²ÐµÑ€Ñ…Ñƒ', '[SEP]']
</code></pre>

<p>But I'm not sure how to use original string and stress indices to calculate stress position number for token.</p>
","8067584","","","","","2020-12-08 04:13:52","How to extract position input-output indeces from huggingface transformer text tokenizator?","<python><nlp><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62091519","1","","","2020-05-29 17:28:17","","3","175","<p>I know the context supported by GPT2 is 1024, but I assume there's some technique they utilized to train and generate text longer than that in their results. Also, I saw many gpt2-based repos training text with length longer than 1024. But when I tried generating text using run_generation.py to generate text longer than 1024 it throws a runtime error :The size of tensor a (1025) must match the size of tensor b (1024) at non-singleton dimension 3. I have the following questions:</p>

<p>Shouldn't it be possible to generate longer text since a sliding window is used?
Can you please explain what's necessary to generate longer text? What changes will I have to make to the run_generation.py code?</p>
","13643426","","","","","2020-05-29 17:28:17","Gpt2 generation of text larger than 1024","<pytorch><huggingface-transformers>","0","0","1","","","CC BY-SA 4.0"
"62370354","1","","","2020-06-14 08:54:50","","1","2781","<p>I am trying to train a model for real disaster tweets prediction(Kaggle Competition) using the Hugging face bert model for classification of the tweets.</p>

<p>I have followed many tutorials and have used many models of bert but none could run in COlab and thros the error</p>

<p>My Code is: </p>

<pre><code>!pip install transformers
import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.callbacks import ModelCheckpoint
from transformers import DistilBertTokenizer, RobertaTokenizer


train = pd.read_csv(""/content/drive/My Drive/Kaggle_disaster/train.csv"")
test = pd.read_csv(""/content/drive/My Drive/Kaggle_disaster/test.csv"")


roberta = 'distilbert-base-uncased'
tokenizer = DistilBertTokenizer.from_pretrained(roberta, do_lower_case = True, add_special_tokens = True, max_length = 128, pad_to_max_length = True)


def tokenize(sentences, tokenizer):
  input_ids, input_masks, input_segments = [], [], []
  for sentence in sentences:
    inputs = tokenizer.encode_plus(sentence, add_special_tokens = True, max_length = 128, pad_to_max_length = True, return_attention_mask = True, return_token_type_ids = True)
    input_ids.append(inputs['input_ids'])
    input_masks.append(inputs['attention_mask'])
    input_segments.append(inputs['token_type_ids'])
  return np.asarray(input_ids, dtype = ""int32""), np.asarray(input_masks, dtype = ""int32""), np.asarray(input_segments, dtype = ""int32"")


input_ids, input_masks, input_segments = tokenize(train.text.values, tokenizer)

from transformers import TFDistilBertForSequenceClassification, DistilBertConfig, TFDistilBertModel

distil_bert = 'distilbert-base-uncased'

config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)
config.output_hidden_states = False
transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)

input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype=tf.int32)
input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype=tf.int32) 
embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]
X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)
X = tf.keras.layers.GlobalMaxPool1D()(X)
X = tf.keras.layers.Dense(50, activation='relu')(X)
X = tf.keras.layers.Dropout(0.2)(X)
X = tf.keras.layers.Dense(1, activation='sigmoid')(X)
model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)
model.compile(Adam(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])
for layer in model.layers[:3]:
  layer.trainable = False

bert_input = [
    input_ids,
    input_masks
]


checkpoint = ModelCheckpoint('/content/drive/My Drive/disaster_model/model_hugging_face.h5', monitor = 'val_loss', save_best_only= True)


train_history = model.fit(
    bert_input,
    validation_split = 0.2,
    batch_size = 16,
    epochs = 10,
    callbacks = [checkpoint]
)

</code></pre>

<p>On running the above code in colab I get the following error:</p>

<pre><code>Epoch 1/10
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-91-9df711c91040&gt; in &lt;module&gt;()
      9     batch_size = 16,
     10     epochs = 10,
---&gt; 11     callbacks = [checkpoint]
     12 )

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, ""ag_error_metadata""):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:541 train_step  **
        self.trainable_variables)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1804 _minimize
        trainable_variables))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:521 _aggregate_gradients
        filtered_grads_and_vars = _filter_grads(grads_and_vars)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1219 _filter_grads
        ([v.name for _, v in grads_and_vars],))

    ValueError: No gradients provided for any variable: ['tf_distil_bert_model_23/distilbert/embeddings/word_embeddings/weight:0', 'tf_distil_bert_model_23/distilbert/embeddings/position_embeddings/embeddings:0', 'tf_distil_bert_model_23/distilbert/embeddings/LayerNorm/gamma:0', 'tf_distil_bert_model_23/distilbert/embeddings/LayerNorm/beta:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/attention/q_lin/kernel:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/attention/q_lin/bias:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/attention/k_lin/kernel:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/attention/k_lin/bias:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/attention/v_lin/kernel:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/attention/v_lin/bias:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/attention/out_lin/kernel:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/attention/out_lin/bias:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/sa_layer_norm/gamma:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/sa_layer_norm/beta:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/ffn/lin1/kernel:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/ffn/lin1/bias:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/ffn/lin2/kernel:0', 'tf_distil_bert_model_23/distilbert/transformer/layer_._0/ffn/lin2/bias:0', 'tf_...

</code></pre>
","12911777","","","","","2020-10-20 14:10:58","Implementing HuggingFace BERT using tensorflow fro sentence classification","<tensorflow><text-classification><huggingface-transformers><bert-language-model>","2","4","","","","CC BY-SA 4.0"
"62381286","1","62382180","","2020-06-15 04:21:11","","3","528","<p>For a pytorch <a href=""https://pytorch.org/docs/master/generated/torch.nn.Module.html"" rel=""nofollow noreferrer"">module</a>, I suppose I could use <code>.named_children</code>, <code>.named_modules</code>, etc. to obtain a list of the submodules. However, I suppose the list is not given in order, right? An example: </p>

<pre><code>In [19]: import transformers

In [20]: model = transformers.DistilBertForSequenceClassification.from_pretrained('distilb
    ...: ert-base-cased')

In [21]: [name for name, _ in model.named_children()]
Out[21]: ['distilbert', 'pre_classifier', 'classifier', 'dropout']
</code></pre>

<p>The order of <code>.named_children()</code> in the above model is given as distilbert, pre_classifier, classifier, and dropout. However, if you examine the <a href=""https://github.com/huggingface/transformers/blob/9931f817b75ecb2c8bb08b6e9d4cbec4b0933935/src/transformers/modeling_distilbert.py#L641"" rel=""nofollow noreferrer"">code</a>, it is evident that <code>dropout</code> happens before <code>classifier</code>. So how do I get the order of these submodules? </p>
","8525221","","","","","2020-06-15 06:03:31","How to obtain sequence of submodules from a pytorch module?","<pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"64734093","1","","","2020-11-08 01:43:31","","0","53","<p>I am using my customized bert script to train a model. However, everything even I keep the same setting for lr, AdamW weight decay and epoch, and run on the same platform (cuda on SageMaker) with same torch (1.5.0) and transformers (2.11.0) versions,  the results still change a lot in terms of the loss. This make my different experiments not comparable.</p>
<p>Can someone who has experienced this before or have any ideas please advice me on what should I do? I really want to solve this inreproducible issue so that I can continue on my experiments. Super appreciated for your help!</p>
<p>Details as below:</p>
<p>For example, if I set epoch = 4, lr = 1e-5, decay for AdamW as 0.01.
For one run I got this result for the first epoch only showing the last complete 100 batches result:</p>
<pre><code>2020-10-19 03:45:29,032 - utils - INFO - | epoch   1 |  1300/ 1320 batches | lr 2.261e-05 | loss 0.267 | Elapsed 0:12:29
2020-10-19 03:45:40,550 - utils - INFO -   Training epoch took: 0:12:41
2020-10-19 03:45:40,550 - utils - INFO - Validating...
2020-10-19 03:46:14,588 - utils - INFO - | loss 0.019 | Elapsed 0:00:34
              precision    recall  f1-score      support
False          0.906472  0.979875  0.941745  2087.000000
True           0.475000  0.152610  0.231003   249.000000
accuracy       0.891695  0.891695  0.891695     0.891695
macro avg      0.690736  0.566243  0.586374  2336.000000
weighted avg   0.860480  0.891695  0.865986  2336.000000
2020-10-19 03:46:15,403 - utils - INFO - Testing...
2020-10-19 03:46:55,182 - utils - INFO - use model: 1 batch / 1319 step
              precision  recall  f1-score   support
False             0.906   0.984     0.944  2344.000
True              0.413   0.098     0.159   265.000
accuracy          0.894   0.894     0.894     0.894
macro avg         0.659   0.541     0.551  2609.000
weighted avg      0.856   0.894     0.864  2609.000
2020-10-19 03:46:55,188 - utils - INFO - best test F1 score: 0.8638224640164368
</code></pre>
<p>And for the second attempt I got this for the first epoch:</p>
<pre><code>2020-11-07 17:08:08,821 - utils - INFO - | epoch   1 |  1300/ 1320 batches | lr 2.261e-05 | loss 0.286 | Elapsed 0:12:25
2020-11-07 17:08:20,487 - utils - INFO -   Training epoch took: 0:12:37
2020-11-07 17:08:20,487 - utils - INFO - Validating...
2020-11-07 17:08:54,609 - utils - INFO - | loss 0.018 | Elapsed 0:00:34
              precision    recall  f1-score      support
False          0.893408  1.000000  0.943703  2087.000000
True           0.000000  0.000000  0.000000   249.000000
accuracy       0.893408  0.893408  0.893408     0.893408
macro avg      0.446704  0.500000  0.471852  2336.000000
weighted avg   0.798177  0.893408  0.843112  2336.000000
2020-11-07 17:08:55,313 - utils - INFO - Testing...
2020-11-07 17:09:34,934 - utils - INFO - use model: 1 batch / 1319 step
              precision  recall  f1-score   support
False             0.898   1.000     0.946  2344.000
True              0.000   0.000     0.000   265.000
accuracy          0.898   0.898     0.898     0.898
macro avg         0.449   0.500     0.473  2609.000
weighted avg      0.807   0.898     0.850  2609.000
2020-11-07 17:09:34,938 - utils - INFO - best test F1 score: 0.8503599608647853
</code></pre>
<p>Note that, the last used lr rate per 100 batches are the same, while the average loss per 100 batches are slightly different. But this result in the predictions for the validation and testing data set very different.</p>
<p>I already set the seed during my model with this function below:</p>
<pre><code>def set_seed(seed):
    &quot;&quot;&quot; Set all seeds to make results reproducible (deterministic mode).
        When seed is a false-y value or not supplied, disables deterministic mode. &quot;&quot;&quot;
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
</code></pre>
<p>And my model script is like this:</p>
<pre><code>class ReviewClassification(BertPreTrainedModel):
    def __init__(self, config,
                 add_agent_text, agent_text_heads):
        &quot;&quot;&quot;
        :param config: Bert configuration, can set up some parameters, like  output_attention, output_hidden_states
        :param add_agent_text: whether to use the non text feature, and how.
                It can have three options: None, &quot;concat&quot; and &quot;attention&quot;
        :param agent_text_heads: number of the heads in agent attention mechanism. Only useful if add_agent_text are set to
                &quot;attention&quot;
        &quot;&quot;&quot;
        super().__init__(config)
        # self.num_labels = 2
        self.add_agent_text = add_agent_text

        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

        embedding_size = config.hidden_size

        if self.add_agent_text == &quot;concat&quot;:
            embedding_size = 2 * embedding_size
        elif self.add_agent_text == &quot;attention&quot;:
            self.agent_attention = nn.MultiheadAttention(embedding_size, num_heads=agent_text_heads)
        else:
            # don't use the information in Agent text
            pass

        self.classifier = nn.Linear(embedding_size, 1) # self.classifier = nn.Linear(embedding_size, len(LABEL_NAME)) # bias: If set to False, the layer will not learn an additive bias
        self.init_weights()

        print(
            &quot;&quot;&quot;            
            add agent text         :{}
            agent text multi-head  :{}
            &quot;&quot;&quot;.format(self.add_agent_text, agent_text_heads)
        )

    def forward(
            self,
            review_input_ids=None,
            review_attention_mask=None,
            review_token_type_ids=None,
            agent_input_ids=None,
            agent_attention_mask=None,
            agent_token_type_ids=None,
            labels=None,
    ):

        review_outputs = self.bert(
            review_input_ids,
            attention_mask=review_attention_mask,
            token_type_ids=review_token_type_ids,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
        )
        if self.add_agent_text is not None:
            # means that self.add_agent_text is &quot;concat&quot; or &quot;attention&quot;
            # TODO: we can try that agent_outputs do not share the same parameter
            agent_outputs = self.bert(
                agent_input_ids,
                attention_mask=agent_attention_mask,
                token_type_ids=agent_token_type_ids,
                position_ids=None,
                head_mask=None,
                inputs_embeds=None,
            )
     
        if self.add_agent_text == &quot;attention&quot;:
            review_hidden_states = review_outputs[0].transpose(0, 1)  # before trans: (bs, seq_len, hidden_size)

            # want to take it as query, we need the it has the shape (#target_seq_len, batch_size, embedding_size)
            agent_hidden_states = agent_outputs[0].mean(axis=1).unsqueeze(dim=0)  # (1, batch_size, hidden_size)

            attn_output, _ = self.agent_attention(agent_hidden_states, review_hidden_states, review_hidden_states)
            feature = attn_output.squeeze()  # (batch_size, seq_len)
        else:
            feature = review_outputs[1]  # (batch_size, seq_len) -? Should it be (batch_size, hidden_size)

        if self.add_agent_text == &quot;concat&quot;:
            feature = torch.cat([feature, agent_outputs[1]], axis=1)
       
        logits = self.classifier(feature).squeeze()

        outputs = (logits,)  # + outputs[2:]  # add hidden states and attention if they are here


        if labels is not None:            
            loss_fct = nn.BCEWithLogitsLoss().cuda() #pos_weight=pos_weight
            loss = loss_fct(logits, labels)
            outputs = (loss,) + outputs

        return outputs  # (loss, logits, hidden_states, attentions)

</code></pre>
<p>The loss is calculated using BCEWithLogitsLoss() from torch.nn.</p>
<p>The train, validation and test part script is as below:</p>
<pre><code>import time
import pickle
from path import Path
import numpy as np
import pandas as pd

from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix
import torch
import torch.nn as nn

from utils import LABEL_NAME, isnotebook, set_seed, format_time

if isnotebook():
    from tqdm.notebook import tqdm
else:
    from tqdm import tqdm


def model_train(model, train_data_loader, valid_data_loader, test_data_loader,
                logger, optimizer, scheduler, num_epochs, seed, out_dir):
    # move model to gpu
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    if torch.cuda.device_count() &gt; 1:
        model = nn.DataParallel(model)

    num_gpus = torch.cuda.device_count()
    logger.info(&quot;Let's use {} GPUs!&quot;.format(num_gpus))

    # Set the seed value all over the place to make this reproducible.
    set_seed(seed=seed)

    # We'll store a number of quantities such as training and validation loss,
    # validation accuracy, and timings.
    training_stats = []
    print_interval = 100

    # Measure the total training time for the whole run.
    total_t0 = time.time()
    batch_size = train_data_loader.batch_size
    num_batch = len(train_data_loader)
    best_f1_score = {
        &quot;weighted&quot;: 0,
        &quot;averaged&quot;: 0
    }
    best_test_f1_score = 0

    # For each epoch...
    for epoch_i in range(0, num_epochs):

        # ========================================
        #               Training
        # ========================================

        # Perform one full pass over the training set.
        logger.info(&quot;&quot;)
        logger.info('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_epochs))
        logger.info('Training...')

        # Reset the total loss for this epoch.
        total_train_loss = 0

        # Measure how long the training epoch takes.
        t_train = time.time()

        model.train()

        # For each batch of training data...
        for step, batch in tqdm(enumerate(train_data_loader), desc=&quot;Training Iteration&quot;, total=num_batch):
            # Progress update every 100 batches.
            if step % print_interval == 0 and not step == 0:
                # Calculate elapsed time in minutes.
                elapsed = format_time(time.time() - t_train)
                avg_train_loss = total_train_loss / print_interval

                # Report progress.
                logger.info('| epoch {:3d} | {:5d}/{:5d} batches | lr {:.3e} | loss {:5.3f} | Elapsed {:s}'.format(
                    epoch_i+1, step, num_batch, scheduler.get_last_lr()[0], avg_train_loss, elapsed)
                )
                total_train_loss = 0
                training_stats.append(
                    {
                        'epoch': epoch_i + 1,
                        'step': step,
                        'train loss': avg_train_loss,
                    }
                )

            # Unpack this training batch from our dataloader.
            #
            # As we unpack the batch, we'll also copy each tensor to the GPU using the
            # `to` method.
            #
            # `batch` contains four pytorch tensors:
            #   &quot;input_ids&quot;
            #   &quot;attention_mask&quot;
            #   &quot;token_type_ids&quot;
            #   &quot;binarized_labels&quot;

            b_review_input_ids = batch[&quot;review_input_ids&quot;].to(device)
            b_review_attention_mask = batch[&quot;review_attention_mask&quot;].to(device)
            b_review_token_type_ids = batch[&quot;review_token_type_ids&quot;].to(device)
            b_agent_input_ids = batch[&quot;agent_input_ids&quot;].to(device)
            b_agent_attention_mask = batch[&quot;agent_attention_mask&quot;].to(device)
            b_agent_token_type_ids = batch[&quot;agent_token_type_ids&quot;].to(device)

            b_binarized_label = batch[&quot;binarized_label&quot;].to(device)

            model.zero_grad()
            (loss, _) = model(review_input_ids=b_review_input_ids,
                              review_attention_mask=b_review_attention_mask,
                              review_token_type_ids=b_review_token_type_ids,
                              agent_input_ids=b_agent_input_ids,
                              agent_attention_mask=b_agent_attention_mask,
                              agent_token_type_ids=b_agent_token_type_ids,

                              labels=b_binarized_label
                              )

            # Accumulate the training loss over all of the batches so that we can
            # calculate the average loss at the end. `loss` is a Tensor containing a
            # single value; the `.item()` function just returns the Python value
            # from the tensor.

            if num_gpus &gt; 1:
                total_train_loss += loss.mean().item()
                loss.mean().backward()  # use loss.mean().backward() instead of loss.backward() for multiple gpu trainings
            else:
                total_train_loss += loss.item()
                loss.backward()

            # Clip the norm of the gradients to 1.0.
            # This is to help prevent the &quot;exploding gradients&quot; problem.
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Update parameters and take a step using the computed gradient.
            # The optimizer dictates the &quot;update rule&quot;--how the parameters are
            # modified based on their gradients, the learning rate, etc.
            optimizer.step()
            scheduler.step()
        # End of training epoch

        # Measure how long this epoch took.
        training_time = format_time(time.time() - t_train)

        logger.info(&quot;&quot;)
        logger.info(&quot;  Training epoch took: {:s}&quot;.format(training_time))

        # evaluate the model after one epoch.

        # ========================================
        #               Validation
        # ========================================
        # After the completion of each training epoch, measure our performance on
        # our validation set.

        logger.info(&quot;&quot;)
        logger.info(&quot;Validating...&quot;)

        t_valid = time.time()
        model.eval()
        ave_valid_loss, valid_f1_table, cm_table, f1_score = model_validate(model=model, data_loader=valid_data_loader)
        # Measure how long this epoch took.
        validation_time = format_time(time.time() - t_valid)

        logger.info(&quot;&quot;)
        logger.info('| loss {:5.3f} | Elapsed {:s}'.format(ave_valid_loss, validation_time))
        logger.info(&quot;  \n{:s}&quot;.format(valid_f1_table.to_string()))
        logger.info(&quot;&quot;)
        logger.info(&quot;  \n{:s}&quot;.format(cm_table.to_string()))

        # need to store the best model
        for key in best_f1_score.keys():
            if best_f1_score[key] &lt; f1_score[key]:
                # remove the old model:
                file_list = [f for f in out_dir.files() if f.name.endswith(&quot;.pt&quot;) and f.name.startswith(key)]
                for f in file_list:
                    Path.remove(f)
                model_file = out_dir.joinpath('{:s}_epoch_{:02d}-f1_{:.3f}.pt'.format(
                    key, epoch_i + 1, f1_score[key])
                )
                best_f1_score[key] = f1_score[key]
                if num_gpus &gt; 1:
                    torch.save(model.module.state_dict(), model_file)
                else:
                    torch.save(model.state_dict(), model_file)

        # ========================================
        #               Test
        # ========================================
        logger.info(&quot;&quot;)
        logger.info(&quot;Testing...&quot;)

        result_df = model_test(model=model, data_loader=test_data_loader)
    
        y_true = np.array(result_df[&quot;review_label&quot;], dtype=np.bool) # This part may need double check
        y_pred = result_df[&quot;Probability&quot;] &gt; 0.5

        report = classification_report(y_true, y_pred, output_dict=True)
        metrics_df = pd.DataFrame(report).transpose()

        metrics_df = metrics_df.sort_index()

        weighted_f1_score = metrics_df.loc['weighted avg', 'f1-score']
        averaged_f1_score = metrics_df.loc['macro avg', 'f1-score']

        best_test_f1_score = metrics_df.loc['weighted avg', 'f1-score'] \
            if best_test_f1_score &lt; metrics_df.loc['weighted avg', 'f1-score'] else best_test_f1_score

        metrics_df = metrics_df.astype(float).round(3)

        # Calculate confusion matrix
        tn, fp, fn, tp  = confusion_matrix(y_true, y_pred).ravel()
        cm_df = pd.DataFrame(columns = ['Predicted No', 'Predicted Yes'],  
                       index = ['Actual No', 'Actual Yes']) 
        # adding rows to an empty  
        # dataframe at existing index 
        cm_df.loc['Actual No'] = [tn,fp] 
        cm_df.loc['Actual Yes'] = [fn,tp]
        
        logger.info(&quot;use model: {} batch / {} step&quot;.format(epoch_i + 1, step))
        logger.info(&quot;\n&quot; + &quot;=&quot; * 50)
        logger.info(&quot;\n&quot; + metrics_df.to_string())
        logger.info(&quot;\n&quot; + &quot;=&quot; * 50)
        logger.info(&quot;\n&quot; + cm_df.to_string())
        logger.info(&quot;best test F1 score: {}&quot;.format(best_test_f1_score))
        logger.info(&quot;\n&quot; + &quot;=&quot; * 50)
        # Below is to save the result files
        result_filename = &quot;result_df_epoch_&quot; + str(epoch_i + 1) + &quot;.xlsx&quot;
        result_df.to_excel(out_dir.joinpath(result_filename), index=False)

    logger.info(&quot;&quot;)
    logger.info(&quot;Training complete!&quot;)
    logger.info(&quot;Total training took {:} (h:mm:ss)&quot;.format(format_time(time.time() - total_t0)))

    # Save training_stats to csv file
    pd.DataFrame(training_stats).to_csv(out_dir.joinpath(&quot;model_train.log&quot;), index=False)
    return model, optimizer, scheduler


def model_validate(model, data_loader):
    # Put the model in evaluation mode--the dropout layers behave differently
    # during evaluation.
    model.eval()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    if torch.cuda.device_count() &gt; 1:
        model = nn.DataParallel(model)

    label_prop = data_loader.dataset.dataset.label_prop()

    total_valid_loss = 0

    batch_size = data_loader.batch_size
    num_batch = len(data_loader)

    y_pred, y_true = [], []

    # Evaluate data
    for step, batch in tqdm(enumerate(data_loader), desc=&quot;Validation...&quot;, total=num_batch):
        b_review_input_ids = batch[&quot;review_input_ids&quot;].to(device)
        b_review_attention_mask = batch[&quot;review_attention_mask&quot;].to(device)
        b_review_token_type_ids = batch[&quot;review_token_type_ids&quot;].to(device)
        b_agent_input_ids = batch[&quot;agent_input_ids&quot;].to(device)
        b_agent_attention_mask = batch[&quot;agent_attention_mask&quot;].to(device)
        b_agent_token_type_ids = batch[&quot;agent_token_type_ids&quot;].to(device)

        b_binarized_label = batch[&quot;binarized_label&quot;].to(device)

        # Tell pytorch not to bother with constructing the compute graph during
        # the forward pass, since this is only needed for backprop (training).
        with torch.no_grad():
            (loss, logits,) = model(review_input_ids=b_review_input_ids,
                                    review_attention_mask=b_review_attention_mask,
                                    review_token_type_ids=b_review_token_type_ids,
                                    agent_input_ids=b_agent_input_ids,
                                    agent_attention_mask=b_agent_attention_mask,
                                    agent_token_type_ids=b_agent_token_type_ids,

                                    labels=b_binarized_label)

        total_valid_loss += loss.item()
        ### The sigmoid function is used for the two-class logistic regression, 
        ### whereas the softmax function is used for the multiclass logistic regression
        
        # Version 1
        # numpy_probas = logits.detach().cpu().numpy()
        # y_pred.extend(np.argmax(numpy_probas, axis=1).flatten())
        # y_true.extend(b_binarized_label.cpu().numpy())

        # Version 2
        # transfored_logits = F.log_softmax(logits,dim=1)
        # numpy_probas = transfored_logits.detach().cpu().numpy()
        # y_pred.extend(np.argmax(numpy_probas, axis=1).flatten())
        # y_true.extend(b_binarized_label.cpu().numpy())

        # Version 3
        # transfored_logits = torch.sigmoid(logits)
        # numpy_probas = transfored_logits.detach().cpu().numpy()
        # y_pred.extend(np.argmax(numpy_probas, axis=1).flatten())
        # y_true.extend(b_binarized_label.cpu().numpy())

        # New version - for num_label = 1
        transfored_logits = torch.sigmoid(logits)
        numpy_probas = transfored_logits.detach().cpu().numpy()
        y_pred.extend(numpy_probas)
        y_true.extend(b_binarized_label.cpu().numpy())
        
    # End of an epoch of validation

    # put model to train mode again.
    model.train()

    ave_loss = total_valid_loss / (num_batch * batch_size)

    y_pred = np.array(y_pred)
    y_pred[y_pred &lt; 0.5] = 0
    y_pred[y_pred &gt;= 0.5] = 1
    
    # Below is in case the input and target are not the same data format
    y_pred = np.array(y_pred, dtype=np.bool)
    y_true = np.array(y_true, dtype=np.bool)
    
    
    # compute the various f1 score for each label
    report = classification_report(y_true, y_pred, output_dict=True)
    metrics_df = pd.DataFrame(report).transpose()
    # metrics_df = pd.DataFrame(0, index=LABEL_NAME, columns=[&quot;Precision&quot;, &quot;Recall&quot;, &quot;F1&quot;,&quot;support&quot;])
    # metrics_df.Precision = precision_recall_fscore_support(y_true, y_pred)[0]
    # metrics_df.Recall = precision_recall_fscore_support(y_true, y_pred)[1]
    # metrics_df.F1 = precision_recall_fscore_support(y_true, y_pred)[2]
    # metrics_df.support = precision_recall_fscore_support(y_true, y_pred)[3]

    # y_pred = np.array(y_pred)
    # y_pred[y_pred &lt; 0] = 0
    # y_pred[y_pred &gt; 0] = 1
    # y_pred = np.array(y_pred, dtype=np.bool)
    # y_true = np.array(y_true, dtype=np.bool)

    # metrics_df = pd.DataFrame(0, index=LABEL_NAME, columns=[&quot;Precision&quot;, &quot;Recall&quot;, &quot;F1&quot;], dtype=np.float)
    # # or_y_pred = np.zeros(y_pred.shape[0], dtype=np.bool)
    # # or_y_true = np.zeros(y_true.shape[0], dtype=np.bool)
    # for i in range(len(LABEL_NAME)):
    #     metrics_df.iloc[i] = precision_recall_fscore_support(
    #         y_true=y_true[:, i], y_pred=y_pred[:, i], average='binary', zero_division=0)[0:3]

        # or_y_pred = or_y_pred | y_pred[:, i]
        # or_y_true = or_y_true | y_true[:, i]

    metrics_df = metrics_df.sort_index()
    # metrics_df.loc['Weighted Average'] = metrics_df.transpose().dot(label_prop)
    # metrics_df.loc['Average'] = metrics_df.mean()

    # metrics_df.loc['Weighted Average', 'F1'] = 2 / (1/metrics_df.loc['Weighted Average', &quot;Recall&quot;] +
    #                                                 1/metrics_df.loc['Weighted Average', &quot;Precision&quot;])
    # metrics_df.loc['Average', 'F1'] = 2 / (1/metrics_df.loc['Average', &quot;Recall&quot;] +
    #                                        1/metrics_df.loc['Average', &quot;Precision&quot;])

    weighted_f1_score = metrics_df.loc['weighted avg', 'f1-score']
    averaged_f1_score = metrics_df.loc['macro avg', 'f1-score']

    # Calculate confusion matrix
    tn, fp, fn, tp  = confusion_matrix(y_true, y_pred).ravel()
    cm_df = pd.DataFrame(columns = ['Predicted No', 'Predicted Yes'],  
                   index = ['Actual No', 'Actual Yes']) 
    # adding rows to an empty  
    # dataframe at existing index 
    cm_df.loc['Actual No'] = [tn,fp] 
    cm_df.loc['Actual Yes'] = [fn,tp]

    # pooled_f1_score = f1_score(y_pred=or_y_pred, y_true=or_y_true)

    return ave_loss, metrics_df, cm_df,{
        &quot;weighted&quot;: weighted_f1_score,
        &quot;averaged&quot;: averaged_f1_score,
    }


def model_test(model, data_loader):
    # Put the model in evaluation mode--the dropout layers behave differently
    # during evaluation.
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()
    model.to(device)
    if torch.cuda.device_count() &gt; 1:
        model = nn.DataParallel(model)

    num_batch = len(data_loader)
    # Below need to modify if change the input
    review_id, review_label, hmd_text, head_cust_text = [], [], [], []
    agent = []
    pred_logits = []

    # Evaluate data
    for step, batch in tqdm(enumerate(data_loader), desc=&quot;Inference...&quot;, total=num_batch):
        if &quot;anecdote_lead_final&quot; in batch.keys():
            review_label.extend(batch[&quot;anecdote_lead_final&quot;])
        review_id.extend(batch[&quot;_id&quot;].tolist())
        hmd_text.extend(batch[&quot;hmd_comments&quot;])
        head_cust_text.extend(batch[&quot;head_cust&quot;])
        agent.extend(batch[&quot;new_transcript_agent&quot;])

        b_review_input_ids = batch[&quot;review_input_ids&quot;].to(device)
        b_review_attention_mask = batch[&quot;review_attention_mask&quot;].to(device)
        b_review_token_type_ids = batch[&quot;review_token_type_ids&quot;].to(device)
        b_agent_input_ids = batch[&quot;agent_input_ids&quot;].to(device)
        b_agent_attention_mask = batch[&quot;agent_attention_mask&quot;].to(device)
        b_agent_token_type_ids = batch[&quot;agent_token_type_ids&quot;].to(device)


        # Tell pytorch not to bother with constructing the compute graph during
        # the forward pass, since this is only needed for backprop (training).
        with torch.no_grad():
            (logits,) = model(review_input_ids=b_review_input_ids,
                              review_token_type_ids=b_review_token_type_ids,
                              review_attention_mask=b_review_attention_mask,
                              agent_input_ids=b_agent_input_ids,
                              agent_token_type_ids=b_agent_token_type_ids,
                              agent_attention_mask=b_agent_attention_mask
                              )

        if logits.detach().cpu().numpy().size == 1:
            pred_logits.extend(logits.detach().cpu().numpy().reshape(1,))  
        else:
            pred_logits.extend(logits.detach().cpu().numpy())
            
    # End of an epoch of validation
    # put model to train mode again.
    model.train()
    pred_logits = np.array(pred_logits)
    pred_prob = np.exp(pred_logits)
    pred_prob = pred_prob / (1 + pred_prob)
    pred_label = pred_prob.copy()
    pred_label[pred_label &lt; 0.5] = 0
    pred_label[pred_label &gt;= 0.5] = 1
    # compute the f1 score for each tags
    d = {'Probability':pred_prob,'Anecdotes Prediction':pred_label}
    pred_df = pd.DataFrame(d, columns=['Probability','Anecdotes Prediction'])
    result_df = pd.DataFrame(
        {
            &quot;review_id&quot;: review_id,
            &quot;hmd_text&quot;: hmd_text,
            &quot;head_cust_text&quot;: head_cust_text,
            &quot;agent&quot;: agent
        }
    )
    if len(review_label) != 0:
        result_df[&quot;review_label&quot;] =  [x.item() for x in review_label] 
    return pd.concat([result_df, pred_df], axis=1).set_index(&quot;review_id&quot;)

</code></pre>
<p>optimizer and scheduler part are defined as below:</p>
<pre><code>if args.full_finetuning:
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         'weight_decay_rate': args.decay},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]
else:
    param_optimizer = list(model.classifier.named_parameters())
    optimizer_grouped_parameters = [
        {&quot;params&quot;: [p for n, p in param_optimizer]}
    ]

# Note: AdamW is a class from the huggingface library (as opposed to pytorch)
optimizer = AdamW(optimizer_grouped_parameters,  # or param_optimizer
                  lr=args.lr,  # args.learning_rate - default is 5e-5, our notebook had 1e-5
                  eps=1e-8)  # args.adam_epsilon  - default is 1e-8.

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=len(data_loader[&quot;train&quot;]) * args.num_epochs
)
</code></pre>
<p>And to run the model, I use below script:</p>
<pre><code>model_train(model=model, train_data_loader=data_loader[&quot;train&quot;], valid_data_loader=data_loader[&quot;valid&quot;],
            test_data_loader=data_loader[&quot;test&quot;], optimizer=optimizer, scheduler=scheduler,
            num_epochs=args.num_epochs, seed=args.seed, logger=logger, out_dir=out_dir)
</code></pre>
","14318461","","","","","2020-11-08 01:43:31","Inreproducible loss/results when training bert with same setting for >=2 times","<nlp><pytorch><bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"61787853","1","","","2020-05-14 01:45:41","","4","1721","<p>I'm trying to calculate the probability or any type of score for words in a sentence using NLP. I've tried this approach with GPT2 model using Huggingface Transformers library, but, I couldn't get satisfactory results due to the model's unidirectional nature which for me didn't seem to predict within context. So I was wondering whether there is a way, to calculate the above said using BERT since it's Bidirectional.</p>

<p>I've found this <a href=""https://github.com/huggingface/transformers/issues/473#issuecomment-482674742"" rel=""nofollow noreferrer"">post</a> relatable, which I randomly saw the other day but didn't see any answer which would be useful for me as well.</p>

<p>Hope I will be able to receive ideas or a solution for this. Any help is appreciated. Thank you. </p>
","10598769","","","","","2020-05-14 07:53:25","How to get the probability of a particular token(word) in a sentence given the context","<nlp><pytorch><huggingface-transformers><bert-language-model>","1","0","3","","","CC BY-SA 4.0"
"62206826","1","62207015","","2020-06-05 01:47:39","","0","499","<p>I'm new to coding, and could use guidance as to why it is printing oddly like it is. While this is related to NLP, I believe this error could most likely be explained by somebody who has greater knowledge in coding than me. I hope this is the right place to ask this question. Thank you for the help! </p>

<pre><code>from transformers import AutoTokenizer, AutoModelWithLMHead
import torch


tokenizer = AutoTokenizer.from_pretrained(""bert-large-cased-whole-word-masking"")

model = AutoModelWithLMHead.from_pretrained(""bert-large-cased-whole-word-masking"")

sentence = """"""While United States [MASK] heed human rights,""""""


token_ids = tokenizer.encode(sentence, return_tensors='pt')
# print(token_ids)
token_ids_tk = tokenizer.tokenize(sentence, return_tensors='pt')
print(token_ids_tk)


masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero()
masked_pos = [mask.item() for mask in masked_position ]
print (masked_pos)


with torch.no_grad():
    output = model(token_ids)

last_hidden_state = output[0].squeeze()

print (""\n\n"")
print (""sentence :"", sentence)
print (""\n"")
list_of_list =[]
for mask_index in masked_pos:
    mask_hidden_state = last_hidden_state[mask_index]
    idx = torch.topk(mask_hidden_state, k=25, dim=0)[1]
    words = [tokenizer.decode(i.item()).strip() for i in idx]
    list_of_list.append(words)
    print (words)

best_guess = """"
for j in list_of_list:
    best_guess = best_guess+"" ""+j[0]

print (""\nBest guess for fill in the blank :::"",best_guess)
</code></pre>

<p>OUTPUT:</p>

<pre><code>['While', 'United', 'States', '[MASK]', 'he', '##ed', 'human', 'rights', ',']
</code></pre>

<p>[4]</p>

<pre><code>sentence : While United States [MASK] heed human rights,


['m u s t', 'c i t i z e n s', 's h o u l d', 'c a n n o t', 'l a w s', 'd o e s', 'g e n e r a l l y', 'd i d', 'a l w a y s', 'l a w', ',', 'g o v e r n m e n t', 'd o', 'p o l i t i c i a n s', 'm a y', 'd e f e n d e r s', 'c o u n t r i e s', 'c a n', 'o f f i c i a l s', 'g o v e r n m e n t s', 'w i l l', 'G o v e r n m e n t', 'v a l u e s', 'C o n s t i t u t i o n', 'p e o p l e']

Best guess for fill in the blank :::  m u s t
</code></pre>
","13624094","","7182350","","2020-06-13 12:10:02","2020-06-13 12:10:02","Huggingface Bert: Output Printing","<python><nlp><torch><huggingface-transformers><spacy-transformers>","1","1","","","","CC BY-SA 4.0"
"62208388","1","","","2020-06-05 04:53:48","","0","82","<p>I want to freeze intermediate sub-layers in ALBERT. Unlike BERT,
ALBERT's encoder looks like this </p>

<pre><code>(encoder): AlbertTransformer(
      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)
      (albert_layer_groups): ModuleList(
        (0): AlbertLayerGroup(
          (albert_layers): ModuleList(
            (0): AlbertLayer(
              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (attention): AlbertAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0, inplace=False)
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              )
              (ffn): Linear(in_features=768, out_features=3072, bias=True)
              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
        )
      )
    )
</code></pre>

<p>So inside the <code>modulelist</code>, we can easily choose which sublayers to freeze. But here, we have one module. The documentation says that there are 12 repeating layers. Is there any good way to freeze intermediate layers in ALBERT?</p>
","13256890","","13256890","","2020-06-05 06:09:22","2020-06-05 06:18:19","Freezing intermediate layers in ALBERT","<pytorch><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"62435022","1","62436033","","2020-06-17 17:33:24","","2","1058","<p>My question concerns the example, available in the great huggingface/transformers library.</p>
<p>I am using a <a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/trainer/01_text_classification.ipynb#scrollTo=uBzDW1FO63pK"" rel=""nofollow noreferrer"">notebook</a>, provided by library creators as a starting point for my pipeline. It presents a pipeline of finetuning a BERT for Sentence Classification on Glue dataset.</p>
<p>When getting into the code, I noticed a very weird thing, which I cannot explain.</p>
<p>In the example, input data is introduced to the model as the instances of the <code>InputFeatures</code> class from <a href=""https://github.com/huggingface/transformers/blob/011cc0be51cf2eb0a91333f1a731658361e81d89/src/transformers/data/processors/utils.py"" rel=""nofollow noreferrer"">here</a>:</p>
<p>This class has 4 attributes, including the <strong>label</strong> attribute:</p>
<pre class=""lang-py prettyprint-override""><code>class InputFeatures:
    ...
    input_ids: List[int]
    attention_mask: Optional[List[int]] = None
    token_type_ids: Optional[List[int]] = None
    label: Optional[Union[int, float]] = None
</code></pre>
<p>which are later passed as a dictionary of inputs to the <code>forward()</code> method of the model. This is done by the <code>Trainer</code> class, for example in the lines 573-576 <a href=""https://github.com/huggingface/transformers/blob/edcb3ac59ab05d9afbc6b4f7bebfb2e5dfc662d2/src/transformers/trainer.py"" rel=""nofollow noreferrer"">here</a>:</p>
<pre class=""lang-py prettyprint-override""><code>    def _training_step(
        self, model: nn.Module, inputs: Dict[str, torch.Tensor], optimizer: torch.optim.Optimizer
    ) -&gt; float:
        model.train()
        for k, v in inputs.items():
            inputs[k] = v.to(self.args.device)

        outputs = model(**inputs)  
</code></pre>
<p>However, the <code>forward()</code> method expects <strong>labels</strong> (note the plural form) input parameter (taken from <a href=""https://huggingface.co/transformers/_modules/transformers/modeling_distilbert.html#DistilBertForSequenceClassification"" rel=""nofollow noreferrer"">here</a>):</p>
<pre class=""lang-py prettyprint-override""><code>    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        output_attentions=None,
    ):
</code></pre>
<p>So my question is <strong>where does the label become labels</strong> in this pipeline?</p>
<p>To give some extra info on the issue, I created my own pipeline, which uses nothing, related, with Glue data and pipe, basically it relies only on the <code>Trainer</code> class of transformers. I even use another model (Flaubert). I replicated the InputFeature class and my code works for both cases below:</p>
<pre class=""lang-py prettyprint-override""><code>class InputFeature:
    def __init__(self, text, label):
        self.input_ids = text
        self.label = label

class InputFeaturePlural:
    def __init__(self, text, label):
        self.input_ids = text
        self.labels = label
</code></pre>
<p>But it does not work if I name the second attribute as <code>self.labe</code> or by any other names. <strong>Why is it possible to use both attribute names?</strong></p>
<p>It's not like it is extremely important in my case, but I feel uncomfortable passing around the data in the variable, which &quot;changes name&quot; somewhere along the way.</p>
","5944744","","4228275","","2021-04-28 14:04:34","2021-04-28 14:04:34","Where in the code of pytorch or huggingface/transformer label gets ""renamed"" into labels?","<python><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62434075","1","62434868","","2020-06-17 16:41:46","","0","852","<p>I just recently started looking into the huggingface transformer library.
When I tried to get started using the model card code at e.g. <a href=""https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT"" rel=""nofollow noreferrer"">community model</a> </p>

<pre><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")
model = AutoModel.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")
</code></pre>

<p>However, I got the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 2, in &lt;module&gt;
    tokenizer = AutoTokenizer.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")
  File ""/Users/Lukas/miniconda3/envs/nlp/lib/python3.7/site-packages/transformers/tokenization_auto.py"", line 124, in from_pretrained
    ""'xlm', 'roberta', 'ctrl'"".format(pretrained_model_name_or_path))
ValueError: Unrecognized model identifier in emilyalsentzer/Bio_ClinicalBERT. Should contains one of 'bert', 'openai-gpt', 'gpt2', 'transfo-xl', 'xlnet', 'xlm', 'roberta', 'ctrl'
</code></pre>

<p>If I try a different tokenizer such as ""baykenney/bert-base-gpt2detector-topp92"" I get the following error:</p>

<pre><code>OSError: Model name 'baykenney/bert-base-gpt2detector-topp92' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed 'baykenney/bert-base-gpt2detector-topp92' was a path or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.
</code></pre>

<p>Did I miss anything to get started? I feel like the model cards indicate that these three lines of code should should be enough to get started.</p>

<p>I am using Python 3.7 and the transformer library version 2.1.1 and pytorch 1.5. </p>
","5511236","","","","","2020-06-17 17:25:40","Getting started: Huggingface Model Cards","<python><pytorch><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"64901831","1","","","2020-11-18 21:40:15","","6","3021","<p>I am trying to run this example from huggingface website. <a href=""https://huggingface.co/transformers/task_summary.html"" rel=""noreferrer"">https://huggingface.co/transformers/task_summary.html</a>. It seems that the model returns two strings instead of logits! and that leads to an error thrown by torch.argmax()</p>
<pre><code>    from transformers import AutoTokenizer, AutoModelForQuestionAnswering
    import torch
    
    tokenizer = AutoTokenizer.from_pretrained(&quot;bert-large-uncased-whole-word-masking-finetuned-squad&quot;)
    
    model = AutoModelForQuestionAnswering.from_pretrained(&quot;bert-large-uncased-whole-word-masking-finetuned-squad&quot;, return_dict=True)
    
    text = r&quot;&quot;&quot;ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
    architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural
    Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
    TensorFlow 2.0 and PyTorch.
    &quot;&quot;&quot;
    
    questions = [&quot;How many pretrained models are available in ðŸ¤— Transformers?&quot;,
    &quot;What does ðŸ¤— Transformers provide?&quot;,
    &quot;ðŸ¤— Transformers provides interoperability between which frameworks?&quot;]
    
    for question in questions:
      inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=&quot;pt&quot;)
      input_ids = inputs[&quot;input_ids&quot;].tolist()[0] # the list of all indices of words in question + context
    
      text_tokens = tokenizer.convert_ids_to_tokens(input_ids) # Get the tokens for the question + context
      answer_start_scores, answer_end_scores = model(**inputs)
    
      answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score
      answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score
    
      answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
    
      print(f&quot;Question: {question}&quot;)
      print(f&quot;Answer: {answer}&quot;)

</code></pre>
","9536854","","","","","2020-11-18 23:44:04","Huggingface transformer model returns string instead of logits","<huggingface-transformers><question-answering>","1","1","2","","","CC BY-SA 4.0"
"61947796","1","","","2020-05-22 04:18:31","","2","6144","<p>I'm trying to run language model finetuning script (run_language_modeling.py) from huggingface examples with my own tokenizer(just added in several tokens, see the comments). I have problem loading the tokenizer. I think the problem is with AutoTokenizer.from_pretrained('local/path/to/directory'). </p>

<p>Code:</p>

<pre><code>from transformers import *

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
# special_tokens = ['&lt;HASHTAG&gt;', '&lt;URL&gt;', '&lt;AT_USER&gt;', '&lt;EMOTICON-HAPPY&gt;', '&lt;EMOTICON-SAD&gt;']
# tokenizer.add_tokens(special_tokens)
tokenizer.save_pretrained('../twitter/twittertokenizer/')
tmp = AutoTokenizer.from_pretrained('../twitter/twittertokenizer/')
</code></pre>

<p>Error Message:</p>

<pre><code>OSError                                   Traceback (most recent call last)
/z/huggingface_venv/lib/python3.7/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)
    248                 resume_download=resume_download,
--&gt; 249                 local_files_only=local_files_only,
    250             )

/z/huggingface_venv/lib/python3.7/site-packages/transformers/file_utils.py in cached_path(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)
    265         # File, but it doesn't exist.
--&gt; 266         raise EnvironmentError(""file {} not found"".format(url_or_filename))
    267     else:

OSError: file ../twitter/twittertokenizer/config.json not found

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
&lt;ipython-input-32-662067cb1297&gt; in &lt;module&gt;
----&gt; 1 tmp = AutoTokenizer.from_pretrained('../twitter/twittertokenizer/')

/z/huggingface_venv/lib/python3.7/site-packages/transformers/tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    190         config = kwargs.pop(""config"", None)
    191         if not isinstance(config, PretrainedConfig):
--&gt; 192             config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
    193 
    194         if ""bert-base-japanese"" in pretrained_model_name_or_path:

/z/huggingface_venv/lib/python3.7/site-packages/transformers/configuration_auto.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    192         """"""
    193         config_dict, _ = PretrainedConfig.get_config_dict(
--&gt; 194             pretrained_model_name_or_path, pretrained_config_archive_map=ALL_PRETRAINED_CONFIG_ARCHIVE_MAP, **kwargs
    195         )
    196 

/z/huggingface_venv/lib/python3.7/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)
    270                     )
    271                 )
--&gt; 272             raise EnvironmentError(msg)
    273 
    274         except json.JSONDecodeError:

OSError: Can't load '../twitter/twittertokenizer/'. Make sure that:

- '../twitter/twittertokenizer/' is a correct model identifier listed on 'https://huggingface.co/models'

- or '../twitter/twittertokenizer/' is the correct path to a directory containing a 'config.json' file
</code></pre>

<p>If I change  <code>AutoTokenizer</code> to <code>BertTokenizer</code>, the code above can work. Also I can run the script without any problem is I load by shortcut name instead of path. But in the script run_language_modeling.py it uses <code>AutoTokenizer</code>. I'm looking for a way to get it running.</p>

<p>Any idea? Thanks!</p>
","10380477","","8075540","","2020-05-22 11:15:42","2020-11-11 22:45:57","Huggingface AutoTokenizer can't load from local path","<huggingface-transformers>","2","0","1","","","CC BY-SA 4.0"
"64902814","1","","","2020-11-18 23:05:42","","0","152","<p>I have found myself dealing with an enviroment that does not support multiprocessing. How do I run my DistillBert without transformers pipeline?</p>
<p>Here is code right now:</p>
<pre class=""lang-py prettyprint-override""><code>import json
import os
import sys

sys.path.append(&quot;/mnt/access&quot;)
import torch
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from transformers.pipelines import pipeline


def lambda_handler(event, context):
    print(&quot;After:&quot;,os.listdir(&quot;/mnt/access&quot;))
    tokenizer = AutoTokenizer.from_pretrained('/mnt/access/Dis_Save/')
    model = AutoModelForQuestionAnswering.from_pretrained('/mnt/access/Dis_Save/')
    nlp_qa = pipeline('question-answering', tokenizer=tokenizer,model=model)
    context = &quot;tra&quot;
    question = &quot;tra&quot;
    X = nlp_qa(context=context, question=question)

    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
</code></pre>
<p>Error message I get right now:</p>
<pre><code>{
  &quot;errorMessage&quot;: &quot;[Errno 38] Function not implemented&quot;,
  &quot;errorType&quot;: &quot;OSError&quot;,
  &quot;stackTrace&quot;: [
    &quot;  File \&quot;/var/task/lambda_function.py\&quot;, line 18, in lambda_handler\n    X = nlp_qa(context=context, question=question)\n&quot;,
    &quot;  File \&quot;/mnt/access/transformers/pipelines.py\&quot;, line 1776, in __call__\n    features_list = [\n&quot;,
    &quot;  File \&quot;/mnt/access/transformers/pipelines.py\&quot;, line 1777, in &lt;listcomp&gt;\n    squad_convert_examples_to_features(\n&quot;,
    &quot;  File \&quot;/mnt/access/transformers/data/processors/squad.py\&quot;, line 354, in squad_convert_examples_to_features\n    with Pool(threads, initializer=squad_convert_example_to_features_init, initargs=(tokenizer,)) as p:\n&quot;,
    &quot;  File \&quot;/var/lang/lib/python3.8/multiprocessing/context.py\&quot;, line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n&quot;,
    &quot;  File \&quot;/var/lang/lib/python3.8/multiprocessing/pool.py\&quot;, line 191, in __init__\n    self._setup_queues()\n&quot;,
    &quot;  File \&quot;/var/lang/lib/python3.8/multiprocessing/pool.py\&quot;, line 343, in _setup_queues\n    self._inqueue = self._ctx.SimpleQueue()\n&quot;,
    &quot;  File \&quot;/var/lang/lib/python3.8/multiprocessing/context.py\&quot;, line 113, in SimpleQueue\n    return SimpleQueue(ctx=self.get_context())\n&quot;,
    &quot;  File \&quot;/var/lang/lib/python3.8/multiprocessing/queues.py\&quot;, line 336, in __init__\n    self._rlock = ctx.Lock()\n&quot;,
    &quot;  File \&quot;/var/lang/lib/python3.8/multiprocessing/context.py\&quot;, line 68, in Lock\n    return Lock(ctx=self.get_context())\n&quot;,
    &quot;  File \&quot;/var/lang/lib/python3.8/multiprocessing/synchronize.py\&quot;, line 162, in __init__\n    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)\n&quot;,
    &quot;  File \&quot;/var/lang/lib/python3.8/multiprocessing/synchronize.py\&quot;, line 57, in __init__\n    sl = self._semlock = _multiprocessing.SemLock(\n&quot;
  ]
}
</code></pre>
<p>Other code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch
import json
import sys

sys.path.append(&quot;/mnt/access&quot;)

tokenizer = AutoTokenizer.from_pretrained(&quot;/mnt/access/Dis_Save/&quot;)
model = AutoModelForQuestionAnswering.from_pretrained(&quot;/mnt/access/Dis_Save/&quot;, return_dict=True)


def lambda_handler(event, context):
    text = r&quot;&quot;&quot;
ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural
Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
TensorFlow 2.0 and PyTorch.
&quot;&quot;&quot;
    questions = [&quot;How many pretrained models are available in ðŸ¤— Transformers?&quot;,]
    for question in questions:
        inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=&quot;pt&quot;)
        input_ids = inputs[&quot;input_ids&quot;].tolist()[0]
        text_tokens = tokenizer.convert_ids_to_tokens(input_ids)
        answer_start_scores, answer_end_scores = model(**inputs).values()
        answer_start = torch.argmax(
            answer_start_scores
        )  # Get the most likely beginning of answer with the argmax of the score
        answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score
        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
        print(f&quot;Question: {question}&quot;)
        print(f&quot;Answer: {answer}&quot;)
    return {
        'statusCode': 200,
        'body': json.dumps(answer)
    }

</code></pre>
<p><strong>Edit:</strong>
I run the code. It runs well on it's own, however I get an error whne running on API itself:</p>
<pre><code>{
    &quot;errorMessage&quot;: &quot;'tuple' object has no attribute 'values'&quot;,
    &quot;errorType&quot;: &quot;AttributeError&quot;,
    &quot;stackTrace&quot;: [
        &quot;  File \&quot;/var/task/lambda_function.py\&quot;, line 39, in lambda_handler\n    answer_start_scores, answer_end_scores = model(**inputs).values()\n&quot;
    ]
}
</code></pre>
","11037848","","11037848","","2020-11-20 23:19:43","2020-11-20 23:19:43","How to run a transformers bert without pipeline?","<aws-lambda><pipeline><huggingface-transformers>","0","9","","","","CC BY-SA 4.0"
"61465223","1","61475491","","2020-04-27 17:54:15","","1","417","<p>The <a href=""https://github.com/huggingface/transformers/blob/41750a6cff55e401364568868d619747de3db037/src/transformers/tokenization_roberta.py#L154"" rel=""nofollow noreferrer"">Roberta Tokenizer</a> in huggingface-transformers describes Roberta's tokenization
method as such:  </p>

<pre><code>- single sequence: ``&lt;s&gt; X &lt;/s&gt;``
- pair of sequences: ``&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;``
</code></pre>

<p>I'm curious why the tokenization of multiple sequences is not <code>&lt;s&gt; A &lt;/s&gt;&lt;s&gt; B &lt;/s&gt;</code>?</p>

<p>Building upon the above, if I were to encode more than two sequences manually, should I encode them as <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;&lt;/s&gt; C &lt;/s&gt;</code> or as <code>&lt;s&gt; A &lt;/s&gt;&lt;s&gt; B &lt;/s&gt;&lt;s&gt; C &lt;/s&gt;</code></p>
","7260404","","","","","2020-04-28 08:07:03","Roberta Tokenization of multiple sequences","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62093326","1","","","2020-05-29 19:25:43","","1","614","<p>what I am doing: adding special tokens using hugging face and their tokenizer and using keras tensor-flow hub model</p>

<p>I get this error when I add special token to the model:</p>

<pre><code>   tensorflow.python.framework.errors_impl.InvalidArgumentError:  indices[19]=30522 is not in [0, 30522)[[{{nodedncc/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_model/StatefulPartitionedCall/word_embeddings/Gather}]] [Op:__inference_train_function_43492]
</code></pre>

<p>though same code worked fine in google colab only if I am connected GPU only?. </p>

<p>I don't know if there is incompatibility problem in some how? why it only work in colab?</p>

<p>Answer: there is a bug and special tokens only work on GPU</p>
","7060584","","7060584","","2020-06-05 05:32:39","2020-11-28 14:42:32","tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[19] = 30522 is not in [0, 30522)","<python><huggingface-transformers><tensorflow-hub>","0","2","","","","CC BY-SA 4.0"
"65589197","1","","","2021-01-06 02:01:01","","0","162","<p>I tried below code:</p>
<pre><code>import transformers
</code></pre>
<p>and got below error:</p>
<pre><code>import transformers

  File &quot;C:\Users\user1\AppData\Roaming\Python\Python37\site-packages\transformers\__init__.py&quot;, line 22, in &lt;module&gt;
    from .integrations import (  # isort:skip

  File &quot;C:\Users\user1\AppData\Roaming\Python\Python37\site-packages\transformers\integrations.py&quot;, line 58, in &lt;module&gt;
    from .file_utils import is_torch_tpu_available

  File &quot;C:\Users\user1\AppData\Roaming\Python\Python37\site-packages\transformers\file_utils.py&quot;, line 156, in &lt;module&gt;
    import sklearn.metrics  # noqa: F401

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\metrics\__init__.py&quot;, line 7, in &lt;module&gt;
    from .ranking import auc

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\metrics\ranking.py&quot;, line 35, in &lt;module&gt;
    from ..preprocessing import label_binarize

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\preprocessing\__init__.py&quot;, line 6, in &lt;module&gt;
    from ._function_transformer import FunctionTransformer

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\preprocessing\_function_transformer.py&quot;, line 5, in &lt;module&gt;
    from ..utils.testing import assert_allclose_dense_sparse

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\utils\testing.py&quot;, line 718, in &lt;module&gt;
    import pytest

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\pytest.py&quot;, line 6, in &lt;module&gt;
    from _pytest.assertion import register_assert_rewrite

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\_pytest\assertion\__init__.py&quot;, line 7, in &lt;module&gt;
    from _pytest.assertion import rewrite

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\_pytest\assertion\rewrite.py&quot;, line 26, in &lt;module&gt;
    from _pytest.assertion import util

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\_pytest\assertion\util.py&quot;, line 8, in &lt;module&gt;
    import _pytest._code

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\_pytest\_code\__init__.py&quot;, line 2, in &lt;module&gt;
    from .code import Code  # noqa

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\_pytest\_code\code.py&quot;, line 23, in &lt;module&gt;
    import pluggy

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\pluggy\__init__.py&quot;, line 16, in &lt;module&gt;
    from .manager import PluginManager, PluginValidationError

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\pluggy\manager.py&quot;, line 11, in &lt;module&gt;
    import importlib_metadata

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 547, in &lt;module&gt;
    __version__ = version(__name__)

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 509, in version
    return distribution(distribution_name).version

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 482, in distribution
    return Distribution.from_name(distribution_name)

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 183, in from_name
    dist = next(dists, None)

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 425, in &lt;genexpr&gt;
    for path in map(cls._switch_path, paths)

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 449, in _search_path
    if not root.is_dir():

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\pathlib.py&quot;, line 1358, in is_dir
    return S_ISDIR(self.stat().st_mode)

  File &quot;C:\Users\user1\AppData\Local\Continuum\anaconda3\lib\pathlib.py&quot;, line 1168, in stat
    return self._accessor.stat(self)

OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'C:\\Users\\user1\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip:C:\\Users\\user1\\Spark\\python'
</code></pre>
<p>So, here is the error:
<em><strong>OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'C:\Users\user1\Spark\python\lib\py4j-0.10.8.1-src.zip:C:\Users\user1\Spark\python'</strong></em></p>
<p>This is happening only for transformers package and rest like pandas or numpy or torch give no issues.
I see quite a few links giving solutions to this error, but more appear related to syntax of the code (like using double quotes or having an additional backlash).
What am I missing? Thanks in advance for any help.</p>
","4652061","","4652061","","2021-01-06 02:13:45","2021-01-07 10:02:53","WinError 123 The filename, directory name, or volume label syntax is incorrect --> while importing transformers","<python><anaconda><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65589297","1","","","2021-01-06 02:16:43","","0","117","<p>I want to import pipeline from transformers and i had already installed transformers using '!pip install transformers'. But now when I put 'from transformers import pipeline' my kernel keeps on dying. Failry new to coding so help much appreciated!</p>
","14948738","","","","","2021-01-06 02:16:43","Kernel keeps dying when I try to import pipeline from transformers","<python><pandas><pipeline><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"65594307","1","","","2021-01-06 10:44:34","","1","47","<p>I'm totally new to the Hugging Face API and I'm trying to figure out the number of labels I can pass in a single API call. The reason I ask this is I tried using 5 labels some time back and it was giving me an error, but now when I tried with 40 labels it worked perfectly fine. So is there a limit at all for the number of labels in the multi-label approach?</p>
<p>The code is as follows-</p>
<pre><code> payload = json.dumps(
    {&quot;inputs&quot;: corpus
    ,&quot;parameters&quot;: {&quot;candidate_labels&quot;: keywords, &quot;multi_class&quot;: True}
    })

headers = {
    'Authorization': ':)',
    'Content-Type': 'application/json'
}
api_response = requests.request(&quot;POST&quot;, API_URL, headers=headers, data=payload)
</code></pre>
<p>Thank You!</p>
","9573638","","9573638","","2021-01-06 12:43:33","2021-01-06 12:43:33","What is the multi-label limit for BERT based Hugging Face API?","<huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"61708486","1","61732210","","2020-05-10 07:16:33","","18","14169","<p>Here is an example of doing sequence classification using a model to determine if two sequences are paraphrases of each other. The two examples give two different results. Can you help me explain why <code>tokenizer.encode</code> and <code>tokenizer.encode_plus</code> give different results?</p>

<p>Example 1 (with <code>.encode_plus()</code>):</p>

<pre class=""lang-py prettyprint-override""><code>paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=""pt"")
not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=""pt"")

paraphrase_classification_logits = model(**paraphrase)[0]
not_paraphrase_classification_logits = model(**not_paraphrase)[0]
</code></pre>

<p>Example 2 (with <code>.encode()</code>):</p>

<pre class=""lang-py prettyprint-override""><code>paraphrase = tokenizer.encode(sequence_0, sequence_2, return_tensors=""pt"")
not_paraphrase = tokenizer.encode(sequence_0, sequence_1, return_tensors=""pt"")

paraphrase_classification_logits = model(paraphrase)[0]
not_paraphrase_classification_logits = model(not_paraphrase)[0]
</code></pre>
","7241796","","3607203","","2020-05-10 11:59:57","2020-05-11 15:30:47","what's difference between tokenizer.encode and tokenizer.encode_plus in Hugging Face","<huggingface-transformers>","1","1","2","","","CC BY-SA 4.0"
"62436178","1","","","2020-06-17 18:44:19","","1","1906","<p><strong>I'm trying to fine-tune BERT for a text classification task, but I'm getting NaN losses and can't figure out why.</strong></p>

<p>First I define a BERT-tokenizer and then tokenize my text:</p>

<pre><code>from transformers import DistilBertTokenizer, RobertaTokenizer
distil_bert = 'distilbert-base-uncased' 

tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,
                                                max_length=128, pad_to_max_length=True)

def tokenize(sentences, tokenizer):
    input_ids, input_masks, input_segments = [],[],[]
    for sentence in tqdm(sentences):
        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=25, pad_to_max_length=True, 
                                             return_attention_mask=True, return_token_type_ids=True)
        input_ids.append(inputs['input_ids'])
        input_masks.append(inputs['attention_mask'])
        input_segments.append(inputs['token_type_ids'])        

    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')

train = pd.read_csv('train_dataset.csv')
d = train['text']
input_ids, input_masks, input_segments = tokenize(d, tokenizer)
</code></pre>

<p>Next, I load my integer labels which are: 0, 1, 2, 3. </p>

<pre><code>d_y = train['label']
0    0
1    1
2    0
3    2
4    0
5    0
6    0
7    0
8    3
9    1
Name: label, dtype: int64
</code></pre>

<p>Then I load the pretrained Transformer model and put layers on top of it. I use SparseCategoricalCrossEntropy Loss when compiling the model:</p>

<pre><code>from transformers import TFDistilBertForSequenceClassification, DistilBertConfig, AutoTokenizer, TFDistilBertModel

  distil_bert = 'distilbert-base-uncased'
  optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0000001)

  config = DistilBertConfig(num_labels=4, dropout=0.2, attention_dropout=0.2)
  config.output_hidden_states = False
  transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)

  input_ids_in = tf.keras.layers.Input(shape=(25,), name='input_token', dtype='int32')
  input_masks_in = tf.keras.layers.Input(shape=(25,), name='masked_token', dtype='int32') 

  embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]
  X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)
  X = tf.keras.layers.GlobalMaxPool1D()(X)
  X = tf.keras.layers.Dense(50, activation='relu')(X)
  X = tf.keras.layers.Dropout(0.2)(X)
  X = tf.keras.layers.Dense(4, activation='softmax')(X)
  model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)

  for layer in model.layers[:3]:
    layer.trainable = False

  model.compile(optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['sparse_categorical_accuracy'],
    )
</code></pre>

<p><strong>Finally, I run the model using previously tokenized input_ids and input_masks as inputs to the model and get a NAN Loss after the first epoch:</strong></p>

<pre><code>model.fit(x=[input_ids, input_masks], y = d_y, epochs=3)

    Epoch 1/3
20/20 [==============================] - 4s 182ms/step - loss: 0.9714 - sparse_categorical_accuracy: 0.6153
Epoch 2/3
20/20 [==============================] - 0s 19ms/step - loss: nan - sparse_categorical_accuracy: 0.5714
Epoch 3/3
20/20 [==============================] - 0s 20ms/step - loss: nan - sparse_categorical_accuracy: 0.5714
&lt;tensorflow.python.keras.callbacks.History at 0x7fee0e220f60&gt;
</code></pre>

<p><strong>EDIT: The model computes losses on the first epoch but it starts returning NaNs 
at the second epoch. What could be causing that problem???</strong></p>

<p>Does anyone has any ideas about what I am doing wrong? 
All suggestions are welcomed!</p>
","4834762","","4834762","","2020-06-18 09:23:34","2021-08-18 14:12:10","BERT HuggingFace gives NaN Loss","<machine-learning><keras><text-classification><transformer><huggingface-transformers>","4","3","","","","CC BY-SA 4.0"
"61708227","1","","","2020-05-10 06:50:15","","1","359","<p>I want to understand how to train a hugging face transformer model (like BERT, DistilBERT, etc) for the question-answer system and TensorFlow as backend. Following is the logic that I am currently using (but I am not sure whether is it right approach):</p>

<ol>
<li>I am using SQuAD v1.1 dataset. </li>
<li>In SQuAd dataset answer to any question is always present in context. So to put in simple words I am trying to predict start index and end index and answer. </li>
<li><p>I have transformed the dataset for the same purpose. I have added the start index and end index of on word level after performing tokenization. Here is how my dataset looks,
<a href=""https://i.stack.imgur.com/CiLXH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CiLXH.png"" alt=""enter image description here""></a></p></li>
<li><p>Next I am encoded question and context as per hugging face docs guide and returing input_ids, attention_ids and token_type_ids; which will be used as input to model. </p></li>
</ol>

<pre><code>def tokenize(questions, contexts):
  input_ids, input_masks, input_segments = [],[],[]
  for question,context in tqdm_notebook(zip(questions, contexts)):
      inputs = tokenizer.encode_plus(question,context, add_special_tokens=True, max_length=512, pad_to_max_length=True,return_attention_mask=True, return_token_type_ids=True )
      input_ids.append(inputs['input_ids'])
      input_masks.append(inputs['attention_mask'])
      input_segments.append(inputs['token_type_ids'])

  return [np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')]
</code></pre>

<ol start=""5"">
<li>Finally I define a Keras model which takes this three input and predict two value, start and end word index of answer from given context.</li>
</ol>

<pre><code>input_ids_in = tf.keras.layers.Input(shape=(512,), name='input_token', dtype='int32')
input_masks_in = tf.keras.layers.Input(shape=(512,), name='masked_token', dtype='int32')
input_segment_in = tf.keras.layers.Input(shape=(512,), name='segment_token', dtype='int32')
embedding_layer = transformer_model({'inputs':input_ids_in,'attention_mask':input_masks_in,
                                     'token_type_ids':input_segment_in})[0]
X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)
X = tf.keras.layers.GlobalMaxPool1D()(X)

start_branch = tf.keras.layers.Dense(1024, activation='relu')(X)
start_branch = tf.keras.layers.Dropout(0.3)(start_branch)
start_branch_output = tf.keras.layers.Dense(512, activation='softmax', name='start_branch')(start_branch)

end_branch = tf.keras.layers.Dense(1024, activation='relu')(X)
end_branch = tf.keras.layers.Dropout(0.3)(end_branch)
end_branch_output = tf.keras.layers.Dense(512, activation='softmax', name='end_branch')(end_branch)


model = tf.keras.Model(inputs=[input_ids_in, input_masks_in, input_segment_in], outputs = [start_branch_output, end_branch_output])
</code></pre>

<p>I am using last softmax layer with 512 units as it is my max no of words I my aim is to predict index dromit. </p>
","10097045","","","","","2020-05-10 06:50:15","How to train any Hugging face transformer model (eg DistilBERT) for question answer from scratch using Tensorflow backend?","<tensorflow><keras><nlp><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"62300836","1","","","2020-06-10 10:01:46","","0","987","<p>I have no problems using the default model in the sentiment analysis pipeline. </p>

<pre class=""lang-py prettyprint-override""><code># Allocate a pipeline for sentiment-analysis
nlp = pipeline('sentiment-analysis')

nlp('I am a black man.')

&gt;&gt;&gt;[{'label': 'NEGATIVE', 'score': 0.5723695158958435}]
</code></pre>

<p>But, when I try to customise the pipeline a little by adding a specific model. It throws a KeyError. </p>

<pre class=""lang-py prettyprint-override""><code>nlp = pipeline('sentiment-analysis',
               tokenizer = AutoTokenizer.from_pretrained(""DeepPavlov/bert-base-cased-conversational""),
               model = AutoModelWithLMHead.from_pretrained(""DeepPavlov/bert-base-cased-conversational""))

nlp('I am a black man.')



&gt;&gt;&gt;---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-55-af7e46d6c6c9&gt; in &lt;module&gt;
      3                tokenizer = AutoTokenizer.from_pretrained(""DeepPavlov/bert-base-cased-conversational""),
      4             model = AutoModelWithLMHead.from_pretrained(""DeepPavlov/bert-base-cased-conversational""))
----&gt; 5 nlp('I am a black man.')
      6 
      7 

~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines.py in __call__(self, *args, **kwargs)
    721         outputs = super().__call__(*args, **kwargs)
    722         scores = np.exp(outputs) / np.exp(outputs).sum(-1, keepdims=True)
--&gt; 723         return [{""label"": self.model.config.id2label[item.argmax()], ""score"": item.max().item()} for item in scores]
    724 
    725 

~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines.py in &lt;listcomp&gt;(.0)
    721         outputs = super().__call__(*args, **kwargs)
    722         scores = np.exp(outputs) / np.exp(outputs).sum(-1, keepdims=True)
--&gt; 723         return [{""label"": self.model.config.id2label[item.argmax()], ""score"": item.max().item()} for item in scores]
    724 
    725 

KeyError: 58129

</code></pre>
","13719811","","3607203","","2020-06-12 11:48:06","2020-09-30 20:40:52","KeyError when using non-default models in Huggingface transformers pipeline","<huggingface-transformers>","1","2","2","","","CC BY-SA 4.0"
"61072673","1","61074902","","2020-04-07 04:07:38","","0","488","<p>I have been given a large csv each line of which is a set of BERT tokens made with hugging face BertTokenizer (<a href=""https://huggingface.co/transformers/main_classes/tokenizer.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/tokenizer.html</a>).
1 line of this file looks as follows:</p>

<p><code>101, 108, 31278, 90939, 70325, 196, 199, 71436, 10107, 29190, 10107, 106, 16680, 68314, 10153, 17015, 15934, 10104, 108, 10233, 12396, 14945, 10107, 10858, 11405, 13600, 13597, 169, 57343, 64482, 119, 119, 119, 100, 11741, 16381, 10109, 68830, 10110, 20886, 108, 10233, 11127, 21768, 100, 14120, 131, 120, 120, 188, 119, 11170, 120, 12132, 10884, 10157, 11490, 12022, 10113, 10731, 10729, 11565, 14120, 131, 120, 120, 188, 119, 11170, 120, 162, 11211, 11703, 12022, 11211, 10240, 44466, 100886, 102</code></p>

<p>and there are 9 million lines like this</p>

<p>Now, I am trying to get embeddings from these tokens like this:</p>

<pre><code>def embedding:
    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)
    model = BertModel.from_pretrained('bert-base-multilingual-cased')
    input_ids = torch.tensor([101, 108, 31278, 90939, 70325, 196, 199, 71436, 10107, 29190, 10107, 106, 16680, 68314, 10153, 17015, 15934, 10104, 108, 10233, 12396, 14945, 10107, 10858, 11405, 13600, 13597, 169, 57343, 64482, 119, 119, 119, 100, 11741, 16381, 10109, 68830, 10110, 20886, 108, 10233, 11127, 21768, 100, 14120, 131, 120, 120, 188, 119, 11170, 120, 12132, 10884, 10157, 11490, 12022, 10113, 10731, 10729, 11565, 14120, 131, 120, 120, 188, 119, 11170, 120, 162, 11211, 11703, 12022, 11211, 10240, 44466, 100886, 102]).unsqueeze(0)  # Batch size 1
    outputs = model(input_ids)
    last_hidden_states = outputs[0][0][0]  # The last hidden-state is the first element of the output tuple
</code></pre>

<p>Output of this is embedding correspending to the line. The size is 768*1 tensor. Semantically, everything is ok. But, when I do this for the full file the output is 768 * 9,0000,000 <code>torch tensors</code>. So I get a memory error even with large machine with a 768 GB of RAM.
Here is how I call this function: 
<code>tokens['embeddings'] = tokens['text_tokens'].apply(lambda x: embedding(x))</code></p>

<p><code>tokens</code> is the pandas data frame with 9 million lines each of which contains BERT tokens.</p>

<p>Is it possible to reduce the default size of the hidden units, which is 768 here: <a href=""https://huggingface.co/transformers/main_classes/model.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/model.html</a></p>

<p>Thank you for your help.</p>
","4244519","","4244519","","2020-04-07 04:23:30","2020-04-07 07:32:53","Reduce the number of hidden units in hugging face transformers (BERT)","<python><python-3.x><nlp><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61464726","1","61470583","","2020-04-27 17:27:29","","0","53","<p>I am trying to implement a model that uses encoding from multiple pre-trained BERT models on different datasets and gets a combined representation using a fully-connected layer. In this, I want that BERT models should remain fixed and only fully-connected layers should get trained. Is it possible to achieve this in huggingface-transformers? I don't see any flag which allows me to do that. </p>

<p>PS: I don't want to go by the way of dumping the encoding of inputs for each BERT model and use them as inputs. </p>
","2450212","","","","","2020-04-28 00:13:00","How to keep model fixed during training?","<python><deep-learning><nlp><pytorch><huggingface-transformers>","1","5","","","","CC BY-SA 4.0"
"66156592","1","","","2021-02-11 14:25:14","","0","265","<p>I want to use hugging face translation model. I am using code :</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;modelname&quot;)

model = AutoModelForSeq2SeqLM.from_pretrained(&quot;modelname&quot;)
</code></pre>
<p>But I have to specify a proxy, I have tried with just adding my proxi- in the way here is proposed:
<a href=""https://stackoverflow.com/questions/65247333/how-to-specify-a-proxy-in-transformers-pipeline/66156422?noredirect=1#comment116961118_66156422"">How to specify a proxy in transformers pipeline</a> but with no success. Does someone know any other solution? Thank you.</p>
","15005936","","","","","2021-02-11 14:25:14","How to specify a proxy in transformers","<proxy><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"65582498","1","65934307","","2021-01-05 16:10:54","","1","194","<p>I am using <code>layoutlm</code> <a href=""https://github.com/microsoft/unilm/tree/master/layoutlm"" rel=""nofollow noreferrer"">github</a> which require <code>python 3.6</code>, <code>transformer 2.9.0</code>. I created an <code>conda</code> env:</p>
<pre><code>name: env_test

    channels:
    - defaults
    - conda-forge
    dependencies:
    - python=3.6
    - pip=20.3.3
    - pytorch=1.4.0
    - cudatoolkit=10.1
    - pip:
      - transformers==2.9.0
</code></pre>
<p>I have the following test.py code to reproduce the issue:</p>
<pre><code>import sys

import torch
from torch.nn import CrossEntropyLoss

from transformers import (
    BertConfig,
    __version__
)

print (sys.version)
print(torch.__version__)
print(__version__)
CrossEntropyLoss().ignore_index

print(&quot;success!&quot;)
</code></pre>
<p>Importing <code>transformers</code> library results in segmentation fault (core dumped) a when calling <code>CrossEntropyLoss().ignore_index</code>:</p>
<pre><code>$python test.py 
3.6.12 |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56) 
[GCC 7.3.0]
1.4.0
2.9.0
Segmentation fault (core dumped)
</code></pre>
<p>I tried to investigate a bit but I don't really see from where the problem is coming from:</p>
<pre><code>gdb python
GNU gdb (Ubuntu 8.1.1-0ubuntu1) 8.1.1
Copyright (C) 2018 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type &quot;show copying&quot;
and &quot;show warranty&quot; for details.
This GDB was configured as &quot;x86_64-linux-gnu&quot;.
Type &quot;show configuration&quot; for configuration details.
For bug reporting instructions, please see:
&lt;http://www.gnu.org/software/gdb/bugs/&gt;.
Find the GDB manual and other documentation resources online at:
&lt;http://www.gnu.org/software/gdb/documentation/&gt;.
For help, type &quot;help&quot;.
Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;...
Reading symbols from python...done.
(gdb) r test.py 
Starting program: /home/jupyter/.conda-env/env_test/bin/python test.py
warning: Error disabling address space randomization: Operation not permitted
[Thread debugging using libthread_db enabled]
Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;.
3.6.12 |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56) 
[GCC 7.3.0]
1.4.0
2.9.0

Program received signal SIGSEGV, Segmentation fault.
0x00007f97000055fb in ?? ()
(gdb) where
#0  0x00007f97000055fb in ?? ()
#1  0x00007f97f4755729 in void pybind11::cpp_function::initialize&lt;void (*&amp;)(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;), void, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, pybind11::name, pybind11::scope, pybind11::sibling&gt;(void (*&amp;)(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;), void (*)(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;), pybind11::name const&amp;, pybind11::scope const&amp;, pybind11::sibling const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::_FUN(pybind11::detail::function_call&amp;) ()
   from /home/jupyter/.conda-env/env_test/lib/python3.6/site-packages/torch/lib/libtorch_python.so
#2  0x00007f97f436bca6 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /home/jupyter/.conda-env/env_test/lib/python3.6/site-packages/torch/lib/libtorch_python.so
#3  0x000055fbadd73a14 in _PyCFunction_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Objects/methodobject.c:231
#4  0x000055fbaddfba5c in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4851
#5  0x000055fbade1e25a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335
#6  0x000055fbaddf5c1b in _PyFunction_FastCall (globals=&lt;optimized out&gt;, nargs=1, args=&lt;optimized out&gt;, co=&lt;optimized out&gt;) at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4933
#7  fast_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4968
#8  0x000055fbaddfbb35 in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4872
#9  0x000055fbade1e25a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335
#10 0x000055fbaddf5166 in _PyEval_EvalCodeWithName () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166
#11 0x000055fbaddf5e51 in fast_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4992
#12 0x000055fbaddfbb35 in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4872
#13 0x000055fbade1e25a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335
#14 0x000055fbaddf5166 in _PyEval_EvalCodeWithName () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166
#15 0x000055fbaddf5e51 in fast_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4992
#16 0x000055fbaddfbb35 in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4872
#17 0x000055fbade1e25a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335
#18 0x000055fbaddf5166 in _PyEval_EvalCodeWithName () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166
#19 0x000055fbaddf632c in _PyFunction_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:5084
#20 0x000055fbadd73ddf in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2310
#21 0x000055fbadd78873 in _PyObject_Call_Prepend () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2373
#22 0x000055fbadd7381e in PyObject_Call () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2261
#23 0x000055fbaddcc88b in slot_tp_init () at /tmp/build/80754af9/python_1599604603603/work/Objects/typeobject.c:6420
#24 0x000055fbaddfbd97 in type_call () at /tmp/build/80754af9/python_1599604603603/work/Objects/typeobject.c:915
#25 0x000055fbadd73bfb in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2331
#26 0x000055fbaddfbbae in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4875
#27 0x000055fbade1e25a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335
#28 0x000055fbaddf6969 in _PyEval_EvalCodeWithName (qualname=0x0, name=&lt;optimized out&gt;, closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwstep=2, kwcount=&lt;optimized out&gt;, kwargs=0x0, kwnames=0x0, argcount=0, args=0x0, 
    locals=0x7f98035bf1f8, globals=0x7f98035bf1f8, _co=0x7f980357aae0) at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166
#29 PyEval_EvalCodeEx () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4187
#30 0x000055fbaddf770c in PyEval_EvalCode (co=co@entry=0x7f980357aae0, globals=globals@entry=0x7f98035bf1f8, locals=locals@entry=0x7f98035bf1f8) at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:731
#31 0x000055fbade77574 in run_mod () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:1025
#32 0x000055fbade77971 in PyRun_FileExFlags () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:978
#33 0x000055fbade77b73 in PyRun_SimpleFileExFlags () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:419
#34 0x000055fbade77c7d in PyRun_AnyFileExFlags () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:81
#35 0x000055fbade7b663 in run_file (p_cf=0x7fff210dc16c, filename=0x55fbaefa6dc0 L&quot;test.py&quot;, fp=0x55fbaefda800) at /tmp/build/80754af9/python_1599604603603/work/Modules/main.c:340
#36 Py_Main () at /tmp/build/80754af9/python_1599604603603/work/Modules/main.c:811
#37 0x000055fbadd4543e in main () at /tmp/build/80754af9/python_1599604603603/work/Programs/python.c:69
#38 0x00007f9803fd6bf7 in __libc_start_main (main=0x55fbadd45350 &lt;main&gt;, argc=2, argv=0x7fff210dc378, init=&lt;optimized out&gt;, fini=&lt;optimized out&gt;, rtld_fini=&lt;optimized out&gt;, stack_end=0x7fff210dc368) at ../csu/libc-start.c:310
#39 0x000055fbade24d0b in _start () at ../sysdeps/x86_64/elf/start.S:103
(gdb
</code></pre>
<p>I am the following list of packages:</p>
<pre><code>_libgcc_mutex             0.1                        main    defaults
_pytorch_select           0.2                       gpu_0    defaults
blas                      1.0                         mkl    defaults
ca-certificates           2020.12.8            h06a4308_0    defaults
certifi                   2020.12.5        py36h06a4308_0    defaults
cffi                      1.14.4           py36h261ae71_0    defaults
chardet                   4.0.0                    pypi_0    pypi
click                     7.1.2                    pypi_0    pypi
cudatoolkit               10.1.243             h6bb024c_0    defaults
cudnn                     7.6.5                cuda10.1_0    defaults
dataclasses               0.8                      pypi_0    pypi
filelock                  3.0.12                   pypi_0    pypi
idna                      2.10                     pypi_0    pypi
intel-openmp              2020.2                      254    defaults
joblib                    1.0.0                    pypi_0    pypi
ld_impl_linux-64          2.33.1               h53a641e_7    defaults
libedit                   3.1.20191231         h14c3975_1    defaults
libffi                    3.3                  he6710b0_2    defaults
libgcc-ng                 9.1.0                hdf63c60_0    defaults
libstdcxx-ng              9.1.0                hdf63c60_0    defaults
mkl                       2020.2                      256    defaults
mkl-service               2.3.0            py36he8ac12f_0    defaults
mkl_fft                   1.2.0            py36h23d657b_0    defaults
mkl_random                1.1.1            py36h0573a6f_0    defaults
ncurses                   6.2                  he6710b0_1    defaults
ninja                     1.10.2           py36hff7bd54_0    defaults
numpy                     1.19.2           py36h54aff64_0    defaults
numpy-base                1.19.2           py36hfa32c7d_0    defaults
openssl                   1.1.1i               h27cfd23_0    defaults
pip                       20.3.3           py36h06a4308_0    defaults
pycparser                 2.20                       py_2    defaults
python                    3.6.12               hcff3b4d_2    defaults
pytorch                   1.4.0           cuda101py36h02f0884_0    defaults
readline                  8.0                  h7b6447c_0    defaults
regex                     2020.11.13               pypi_0    pypi
requests                  2.25.1                   pypi_0    pypi
sacremoses                0.0.43                   pypi_0    pypi
sentencepiece             0.1.94                   pypi_0    pypi
setuptools                51.0.0           py36h06a4308_2    defaults
six                       1.15.0           py36h06a4308_0    defaults
sqlite                    3.33.0               h62c20be_0    defaults
tk                        8.6.10               hbc83047_0    defaults
tokenizers                0.7.0                    pypi_0    pypi
tqdm                      4.55.1                   pypi_0    pypi
transformers              2.9.0                    pypi_0    pypi
urllib3                   1.26.2                   pypi_0    pypi
wheel                     0.36.2             pyhd3eb1b0_0    defaults
xz                        5.2.5                h7b6447c_0    defaults
zlib                      1.2.11               h7b6447c_3    defaults
</code></pre>
<p>What is responsible fo this core dump (I have a VM with 30 GB of memory) ? Seems to be related to <code>transformers</code>. Some dependency issues not catched by <code>conda</code> ? This piece of code seems to work with the latest version of <code>transformers 4.1.1</code> but this is not compatible with <code>layoutlm</code>. Any suggestions?</p>
","6430839","","","","","2021-01-28 09:25:37","torch.nn.CrossEntropyLoss().ignore_index is crashing when importing transfomers library","<python-3.x><segmentation-fault><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62632043","1","","","2020-06-29 05:53:32","","5","2385","<p>I am practicing with Transformers to summarize text.
Following the tutorial at : <a href=""https://huggingface.co/transformers/usage.html#summarization"" rel=""noreferrer"">https://huggingface.co/transformers/usage.html#summarization</a></p>
<pre><code>from transformers import pipeline

summarizer = pipeline(&quot;summarization&quot;)

ARTICLE = &quot;&quot;&quot; New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.
A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.
Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared &quot;I do&quot; five more times, sometimes only within two weeks of each other.
In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her &quot;first and only&quot; marriage.
Barrientos, now 39, is facing two criminal counts of &quot;offering a false instrument for filing in the first degree,&quot; referring to her false statements on the
2010 marriage license application, according to court documents.
Prosecutors said the marriages were part of an immigration scam.
On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.
After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective
Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.
All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.
Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.
Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.
The case was referred to the Bronx District Attorney\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\'s
Investigation Division. Seven of the men are from so-called &quot;red-flagged&quot; countries, including Egypt, Turkey, Georgia, Pakistan and Mali.
Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.
If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.
&quot;&quot;&quot;

print(summarizer(ARTICLE, max_length=130, min_length=30))
</code></pre>
<p>I get the expected summarized text, but when I try another model (in the tutorial they used T5) :</p>
<pre><code>from transformers import AutoModelWithLMHead, AutoTokenizer

model = AutoModelWithLMHead.from_pretrained(&quot;t5-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;t5-base&quot;)

# T5 uses a max_length of 512 so we cut the article to 512 tokens.
inputs = tokenizer.encode(&quot;summarize: &quot; + ARTICLE, return_tensors=&quot;pt&quot;, max_length=512)
outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
print(outputs)
</code></pre>
<p>I only get this kind of output :</p>
<pre><code>tensor([[    0,    16,   792,     6,  1207,  3483,   235,     7,    65,   118,
          4464,   335,   648,     6,    28,  4169,    13,   160,  5281,     7,
         16198,   344,  5247,    11,  4407,     3,     5,   255,    19,  6141,
            12,   341,    36,  4464,    12,   662,  1076,     6,    11,    44,
            80,    97,     6,   255,  4464,  2641,  1076,    44,   728,     3,
             5]])
</code></pre>
<p>I'd like to get the summarized text with T5 in the end, just like the first tutorial.
I am using Google Colab.</p>
","13832336","","13832336","","2020-06-29 11:39:08","2021-07-27 06:14:09","Huggingface Summarization","<huggingface-transformers><huggingface-tokenizers>","3","1","","","","CC BY-SA 4.0"
"62295463","1","","","2020-06-10 03:43:10","","3","754","<p>I have setup all the requirement packages installed on my VM and i found no nvidia GPU driver  installed, In the requirements doesn't have nvidia GPU driver installation instructions, I want to know which cuda version and it compatible nvidia driver which needs too resolve the below error.</p>

<p>Github link: <a href=""https://github.com/kamalkraj/BERT-NER"" rel=""noreferrer"">github</a></p>

<p><strong>Error logs:</strong></p>

<pre><code>  File ""run_ner.py"", line 594, in &lt;module&gt;
    main()
  File ""run_ner.py"", line 489, in main
    loss = model(input_ids, segment_ids, input_mask, label_ids,valid_ids,l_mask)
  File ""/home/pt3_gcp/BERT-NER/ber_ner/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""run_ner.py"", line 35, in forward
    valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device='cuda')
  File ""/home/pt3_gcp/BERT-NER/ber_ner/lib/python3.7/site-packages/torch/cuda/__init__.py"", line 178, in _lazy_init
    _check_driver()
  File ""/home/pt3_gcp/BERT-NER/ber_ner/lib/python3.7/site-packages/torch/cuda/__init__.py"", line 99, in _check_driver
    http://www.nvidia.com/Download/index.aspx"""""")
AssertionError: 
**Found no NVIDIA driver on your system. Please check that you
have an NVIDIA GPU and installed a driver from
http://www.nvidia.com/Download/index.aspx
**
</code></pre>

<p>After installing latest cuda version from the following link,
<a href=""https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=debnetwork"" rel=""noreferrer"">cuda</a> I got the following error,</p>

<pre><code>06/04/2020 07:38:40 - INFO - __main__ -   ***** Running training *****
06/04/2020 07:38:40 - INFO - __main__ -     Num examples = 14041
06/04/2020 07:38:40 - INFO - __main__ -     Batch size = 32
06/04/2020 07:38:40 - INFO - __main__ -     Num steps = 2190
Epoch:   0%|                                                                                 | 0/5 [00:00&lt;?, ?it/sTHCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=38 : no CUDA-capable device is detectedt/s]
Traceback (most recent call last):
  File ""run_ner.py"", line 594, in &lt;module&gt;
    main()
  File ""run_ner.py"", line 489, in main
    loss = model(input_ids, segment_ids, input_mask, label_ids,valid_ids,l_mask)
  File ""/home/pt3_gcp/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""run_ner.py"", line 35, in forward
    valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device='cuda')
  File ""/home/pt3_gcp/.local/lib/python3.7/site-packages/torch/cuda/__init__.py"", line 179, in _lazy_init
    torch._C._cuda_init()
RuntimeError: cuda runtime error (38) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
</code></pre>
","1872999","","","","","2021-04-17 13:51:28","CUDA Runtime Error: Which Cuda version is compatible to run NER task using BERT-NER","<pytorch><ner><huggingface-transformers><bert-language-model>","1","8","","","","CC BY-SA 4.0"
"61690689","1","61690755","","2020-05-09 01:34:50","","4","827","<p>I was trying to add an additional layer after huggingface bert transformer, so I used <code>BertForSequenceClassification</code> inside my <code>nn.Module</code> Network. But, I see the model is giving me random outputs when compared to loading the model directly.</p>

<p>Model 1:</p>

<pre><code>from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5) # as we have 5 classes

import torch
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

input_ids = torch.tensor(tokenizer.encode(texts[0], add_special_tokens=True, max_length = 512)).unsqueeze(0)  # Batch size 1

print(model(input_ids))

</code></pre>

<p>Out:</p>

<pre><code>(tensor([[ 0.3610, -0.0193, -0.1881, -0.1375, -0.3208]],
        grad_fn=&lt;AddmmBackward&gt;),)
</code></pre>

<p>Model 2:</p>

<pre><code>import torch
from torch import nn

class BertClassifier(nn.Module):
    def __init__(self):
        super(BertClassifier, self).__init__()
        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5)
        # as we have 5 classes

        # we want our output as probability so, in the evaluation mode, we'll pass the logits to a softmax layer
        self.softmax = torch.nn.Softmax(dim = 1) # last dimension
    def forward(self, x):
        print(x.shape)
        x = self.bert(x)

        if self.training == False: # in evaluation mode
            pass
            #x = self.softmax(x)

        return x

# create our model

bertclassifier = BertClassifier()

print(bertclassifier(input_ids))
</code></pre>

<pre><code>torch.Size([1, 512])
torch.Size([1, 5])
(tensor([[-0.3729, -0.2192,  0.1183,  0.0778, -0.2820]],
        grad_fn=&lt;AddmmBackward&gt;),)
</code></pre>

<p>They should be the same model, right. I found a similar issue here but no reasonable explanation <a href=""https://github.com/huggingface/transformers/issues/2770"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/2770</a></p>

<ol>
<li><p>Does Bert has some ranomized parameter if so how to get reproducible output?</p></li>
<li><p>Why the two models give me different outputs? Is there something I'm doing wrong?</p></li>
</ol>
","13445425","","","","","2020-05-09 07:43:47","Bert pre-trained model giving random output each time","<python-3.x><pytorch><huggingface-transformers><bert-language-model>","1","0","1","","","CC BY-SA 4.0"
"62641972","1","","","2020-06-29 15:53:43","","5","1793","<p>I am pretty new to Hugging-Face transformers. I am facing the following issue when I try to load <strong>xlm-roberta-base</strong> model from a given path:</p>
<pre><code>&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(model_path)
&gt;&gt; Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/user/anaconda3/lib/python3.7/site-packages/transformers/tokenization_auto.py&quot;, line 182, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File &quot;/home/user/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py&quot;, line 309, in from_pretrained
    return cls._from_pretrained(*inputs, **kwargs)
  File &quot;/home/user/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py&quot;, line 458, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File &quot;/home/user/anaconda3/lib/python3.7/site-packages/transformers/tokenization_roberta.py&quot;, line 98, in __init__
    **kwargs,
  File &quot;/home/user/anaconda3/lib/python3.7/site-packages/transformers/tokenization_gpt2.py&quot;, line 133, in __init__
    with open(vocab_file, encoding=&quot;utf-8&quot;) as vocab_handle:
TypeError: expected str, bytes or os.PathLike object, not NoneType
</code></pre>
<p>However, if I load it by its name, there is no problem:</p>
<pre><code>&gt;&gt; tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')
</code></pre>
<p>I would appreciate any help.</p>
","11478063","","","","","2021-08-26 09:43:22","Hugging-Face Transformers: Loading model from path error","<huggingface-transformers><huggingface-tokenizers>","3","2","1","","","CC BY-SA 4.0"
"62703391","1","62980607","","2020-07-02 19:05:55","","3","1000","<p>I have a sentence like:  <code>&quot;I like sitting in my new chair and _____ about life&quot;</code>.</p>
<p>And I have a SPECIFIC set of tokens like <code>[&quot;watch&quot;, &quot;run&quot;, &quot;think&quot;, &quot;apple&quot;, &quot;light&quot;]</code></p>
<p>I would like to calculate the probability of each of those tokens to appear as the next word in that incomplete sentence. Hopefully I should get that the probability of <code>&quot;think&quot;</code> is higher that <code>&quot;apple&quot;</code> for instance.</p>
<p>I am working with pytorch-transformers (GPT2LMHeadModel specifically), and a possible solution is to evaluate the score of the full sentence with each of the tokens, but when number of tokens to evaluate is on the order of 100 or 1000 then the computation time starts to be too long.</p>
<p>It must be possible to process the sentence only once and somehow use the hidden states to calculate the probabilities of the set of tokens, but I don't know how to do it.</p>
<p>Any ideas? Thanks in advance</p>
<hr />
<p>EDIT:</p>
<p>The actual code looks like the one below (estimating the probability for the full sentence every time). For every sentence it takes about 0.1 seconds to run the <code>score()</code> method, which turns into hours if I want to evaluate some thousands of words.</p>
<pre><code>from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel
import pandas as pd

model = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
model.eval()
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)


def score(sentence):
    tokenize_input = tokenizer.tokenize(sentence)
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
    loss = model(tensor_input, labels=tensor_input)
    return -loss[0].item()


candidates = [&quot;watch&quot;, &quot;run&quot;, &quot;think&quot;, &quot;apple&quot;, &quot;light&quot;]
sent_template = &quot;I like sitting in my new chair and {} about life&quot;
print({candidate: score(sent_template.format(candidate)) for candidate in candidates})
</code></pre>
","7872025","","7872025","","2020-07-04 07:36:04","2020-08-03 14:50:13","Estimate token probability/logits given a sentence without computing the entire sentence","<python><nlp><huggingface-transformers>","1","4","1","","","CC BY-SA 4.0"
"61741788","1","","","2020-05-12 00:37:11","","0","703","<p>I know for a fact that changing hyperparameters of an LSTM model or selecting different BERT layers causes changes in the classification result. I have tested this out using TensorFlow and Keras. I recently switched to Pytorch to do the same design, but no matter what I change, the result remains the same. Below is the code. Am I doing anything wrong? </p>

<pre class=""lang-py prettyprint-override""><code>
def pad_sents(sents, pad_token):  #Pad list of sentences according to the longest sentence in the batch.
    sents_padded = []
    max_len = max(len(s) for s in sents)
    batch_size = len(sents)

    for s in sents:
        padded = [pad_token] * max_len
        padded[:len(s)] = s
        sents_padded.append(padded)

    return sents_padded

def sents_to_tensor(tokenizer, sents, device):
    tokens_list = [tokenizer.tokenize(str(sent)) for sent in sents]
    sents_lengths = [len(tokens) for tokens in tokens_list]
    tokens_list_padded = pad_sents(tokens_list, '[PAD]')
    sents_lengths = torch.tensor(sents_lengths, device=device)

    masks = []
    for tokens in tokens_list_padded:
        mask = [0 if token=='[PAD]' else 1 for token in tokens]
        masks.append(mask)
    masks_tensor = torch.tensor(masks, dtype=torch.long, device=device)
    tokens_id_list = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokens_list_padded]
    sents_tensor = torch.tensor(tokens_id_list, dtype=torch.long, device=device)

    return sents_tensor, masks_tensor, sents_lengths 


class BERT_LSTM_Model(nn.Module):

    def __init__(self, device, dropout_rate, n_class, lstm_hidden_size=None):
        super(BERT_LSTM_Model, self).__init__()

        self.bert_config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)
        self.bert = BertModel.from_pretrained('bert-base-uncased',config =self.bert_config)
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',config =self.bert_config)

        if not lstm_hidden_size:
            self.lstm_hidden_size = self.bert.config.hidden_size
        else:
            self.lstm_hidden_size = lstm_hidden_size
        self.n_class = n_class
        self.dropout_rate = dropout_rate
        self.lstm = nn.LSTM(self.bert.config.hidden_size, self.lstm_hidden_size, bidirectional=True)
        self.hidden_to_softmax = nn.Linear(self.lstm_hidden_size * 2, n_class, bias=True)
        self.dropout = nn.Dropout(p=self.dropout_rate)
        self.device = device

    def forward(self, sents):
        sents_tensor, masks_tensor, sents_lengths = sents_to_tensor(self.tokenizer, sents, self.device)
        encoded_layers = self.bert(input_ids=sents_tensor, attention_mask=masks_tensor)[2] #,output_all_encoded_layers=False)   #output_hidden_states output_hidden_states=True
        bert_hidden_layer = encoded_layers[12]
        bert_hidden_layer = bert_hidden_layer.permute(1, 0, 2)   #permute rotates the tensor. if tensor.shape = 3,4,5  tensor.permute(1,0,2), then tensor,shape= 4,3,5  (batch_size, sequence_length, hidden_size)

        enc_hiddens, (last_hidden, last_cell) = self.lstm(pack_padded_sequence(bert_hidden_layer, sents_lengths, enforce_sorted=False)) #enforce_sorted=False  #pack_padded_sequence(data and batch_sizes
        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)  # (batch_size, 2*hidden_size)
        output_hidden = self.dropout(output_hidden)
        pre_softmax = self.hidden_to_softmax(output_hidden)

        return pre_softmax


def batch_iter(data, batch_size, shuffle=False, bert=None):
    batch_num = math.ceil(data.shape[0] / batch_size)
    index_array = list(range(data.shape[0]))

    if shuffle:
        data = data.sample(frac=1)

    for i in range(batch_num):
        indices = index_array[i * batch_size: (i + 1) * batch_size]
        examples = data.iloc[indices] 
        targets = list(examples.train_label.values)
        yield sents, targets  # list[list[str]] if not bert else list[str], list[int]


def validation(model, df_val, loss_func, device):
    was_training = model.training
    model.eval()
    train_BERT_tweet = list(df_val.train_BERT_tweet)
    train_label = list(df_val.train_label)
    val_batch_size = 16

    n_batch = int(np.ceil(df_val.shape[0]/val_batch_size))

    total_loss = 0.

    with torch.no_grad():
        for i in range(n_batch):
            sents =  train_BERT_tweet[i*val_batch_size: (i+1)*val_batch_size]
            targets = torch.tensor(train_label[i*val_batch_size: (i+1)*val_batch_size],
                                   dtype=torch.long, device=device)
            batch_size = len(sents)
            pre_softmax = model(sents)
            batch_loss = loss_func(pre_softmax, targets)
            total_loss += batch_loss.item()*batch_size

    if was_training:
        model.train()

    return total_loss/df_val.shape[0]

def train():
    label_name = ['Yes', 'Maybe', 'No']
    if torch.cuda.is_available():
        device = torch.device(""cuda"")
    else:
        device = torch.device(""cpu"")

    start_time = time.time()
    print('Importing data...', file=sys.stderr)
    df_train = pd.read_csv('trainn.csv') #, index_col=0)
    df_val = pd.read_csv('valn.csv')   #, index_col=0)
    train_label = dict(df_train.train_label.value_counts())

    label_max = float(max(train_label.values()))

    train_label_weight = torch.tensor([label_max/train_label[i] for i in range(len(train_label))], device=device)

    print('Done! time elapsed %.2f sec' % (time.time() - start_time), file=sys.stderr)
    print('-' * 80, file=sys.stderr)

    start_time = time.time()
    print('Set up model...', file=sys.stderr)

    model = BERT_LSTM_Model(device=device, dropout_rate=0.2, n_class=len(label_name),lstm_hidden_size=768)
    optimizer = AdamW(model.parameters(), lr=1e-3, correct_bias=False)
    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps=100, t_total=1000)  #changed the last 2 arguments to old ones

    model = model.to(device)
    print('Use device: %s' % device, file=sys.stderr)
    print('Done! time elapsed %.2f sec' % (time.time() - start_time), file=sys.stderr)
    print('-' * 80, file=sys.stderr)

    model.train()

    cn_loss = torch.nn.CrossEntropyLoss(weight=train_label_weight, reduction='mean')
    torch.save(cn_loss, 'loss_func3')  # for later testing

    train_batch_size =16
    valid_niter = 500
    log_every = 10
    model_save_path = 'NonLinear_bert_uncased_model.bin'

    num_trial = 0
    train_iter = patience = cum_loss = report_loss = 0
    cum_examples = report_examples = epoch = 0
    hist_valid_scores = []
    train_time = begin_time = time.time()
    print('Begin Maximum Likelihood training...')

    for epoch in range(20):

        for sents, targets in batch_iter(df_train, batch_size=train_batch_size, shuffle=True):  # for each epoch
            train_iter += 1
            optimizer.zero_grad()
            batch_size = len(sents)
            pre_softmax = model(sents)
            loss = cn_loss(pre_softmax, torch.tensor(targets, dtype=torch.long, device=device))
            loss.backward()
            optimizer.step()
            scheduler.step()
            batch_losses_val = loss.item() * batch_size
            report_loss += batch_losses_val
            cum_loss += batch_losses_val
            report_examples += batch_size
            cum_examples += batch_size

            if train_iter % log_every == 0:
                print('epoch %d, iter %d, avg. loss %.2f, '
                      'cum. examples %d, speed %.2f examples/sec, '
                      'time elapsed %.2f sec' % (epoch, train_iter,
                                                                                         report_loss / report_examples,
                                                                                         cum_examples,
                                                                                         report_examples / (time.time() - train_time),
                                                                                         time.time() - begin_time), file=sys.stderr)

                train_time = time.time()
                report_loss = report_examples = 0.

    #torch.save(model.state_dict(), 'LSTM_bert_uncased_model.bin')

            # perform validation
            if train_iter % valid_niter == 0:
                print('epoch %d, iter %d, cum. loss %.2f, cum. examples %d' % (epoch, train_iter,
                                                                                         cum_loss / cum_examples,
                                                                                         cum_examples), file=sys.stderr)
                cum_loss = cum_examples = 0.

                print('begin validation ...', file=sys.stderr)

                validation_loss = validation(model, df_val, cn_loss, device=device)   # dev batch size can be a bit larger

                print('validation: iter %d, loss %f' % (train_iter, validation_loss), file=sys.stderr)

                is_better = len(hist_valid_scores) == 0 or validation_loss &lt; min(hist_valid_scores)
                hist_valid_scores.append(validation_loss)

                if is_better:
                    patience = 0
                    print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)


                    torch.save(model.state_dict(), 'LSTM_bert_uncased_model.bin')
                    # also save the optimizers' state
                    torch.save(optimizer.state_dict(), model_save_path + '.optim')

                elif patience &lt; 5:
                    patience += 1
                    print('hit patience %d' % patience, file=sys.stderr)

                    if patience == 20:
                        num_trial += 1
                        print('hit #%d trial' % num_trial, file=sys.stderr)
                        if num_trial == 3:
                            print('early stop!', file=sys.stderr)
                            exit(0)

                        # decay lr, and restore from previously best checkpoint
                        print('load previously best model and decay learning rate to %f%%' %
                              (0.1*100), file=sys.stderr)

                        # load model                                       model.load_state_dict(torch.load('LSTM_bert_uncased_model.bin'))
                        model = model.to(device)

                        print('restore parameters of the optimizers', file=sys.stderr)
                        optimizer.load_state_dict(torch.load(model_save_path + '.optim'))

                        # set new lr
                        for param_group in optimizer.param_groups:
                            param_group['lr'] *= 0.5

                        # reset patience
                        patience = 0

                if epoch == 100:
                    print('reached maximum number of epochs!', file=sys.stderr)
                    exit(0)

def test():
    label_name = ['Yes', 'Maybe', 'No']
    if torch.cuda.is_available():
        device = torch.device(""cuda"")
    else:
        device = torch.device(""cpu"")
    model = BERT_LSTM_Model(device=device, dropout_rate=0.3, n_class=len(label_name), lstm_hidden_size=768)

    model.load_state_dict(torch.load('LSTM_bert_uncased_model.bin'))
    model.to(device)
    model.eval()
    df_test = pd.read_csv('testn.csv')
    test_batch_size = 16
    n_batch = int(np.ceil(df_test.shape[0]/test_batch_size))
    cn_loss = torch.load('loss_func3', map_location=lambda storage, loc: storage).to(device)
    train_BERT_tweet = list(df_test.train_BERT_tweet)
    train_label = list(df_test.train_label)
    test_loss = 0.
    prediction = []
    prob = []
    softmax = torch.nn.Softmax(dim=1)

    with torch.no_grad():
        for i in range(n_batch):
            sents = train_BERT_tweet[i*test_batch_size: (i+1)*test_batch_size]
            targets = torch.tensor(train_label[i * test_batch_size: (i + 1) * test_batch_size],
                                   dtype=torch.long, device=device)
            batch_size = len(sents)

            pre_softmax = model(sents)
            batch_loss = cn_loss(pre_softmax, targets)
            test_loss += batch_loss.item()*batch_size
            prob_batch = softmax(pre_softmax)
            prob.append(prob_batch)

            prediction.extend([t.item() for t in list(torch.argmax(prob_batch, dim=1))])


    accuracy = accuracy_score(df_test.train_label.values, prediction)
    matthews = matthews_corrcoef(df_test.train_label.values, prediction)
    f1_macro = f1_score(df_test.train_label.values, prediction, average='macro')
    print('accuracy: %.2f' % accuracy)
    print('matthews coef: %.2f' % matthews)
    print('f1_macro: %.2f' % f1_macro)

TrainingModel = train()
TestingModel = test()
</code></pre>

<p>The data can be accessed from <a href=""https://github.com/Kosisochi/DataSnippet"" rel=""nofollow noreferrer"">https://github.com/Kosisochi/DataSnippet</a>
I didnt know how else to create a synthetic data.</p>

<p>Also, the training and validation loss remains quite high with the lowest being around 0.93. 
I also tried a CNN and the same issue remained. Is there something I'm over looking? thanks for your help.</p>
","6711091","","6711091","","2020-05-13 04:15:41","2020-05-13 04:15:41","Finetuning BERT with LSTM via PyTorch and transformers library. Metrics remain the same with hyperparameter changes","<pytorch><lstm><huggingface-transformers><bert-language-model>","0","2","","","","CC BY-SA 4.0"
"64904840","1","","","2020-11-19 03:09:17","","1","563","<p>During the generation phase in HuggingFace's code:
<a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L88-L100"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L88-L100</a></p>
<p>They pass in a <code>decoder_start_token_id</code>, I'm not sure why they need this. And in the BART config, the <code>decoder_start_token_id</code> is actually <code>2</code> (<a href=""https://huggingface.co/facebook/bart-base/blob/main/config.json"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/bart-base/blob/main/config.json</a>), which is the end of sentence token <code>&lt;/s&gt;</code>.</p>
<p>And I tried a simple example:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import *

import torch
model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')
input_ids = torch.LongTensor([[0, 894, 213, 7, 334, 479, 2]])
res = model.generate(input_ids, num_beams=1, max_length=100)

print(res)

preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True).strip() for g in res]
print(preds)
</code></pre>
<p>The results I obtained:</p>
<pre class=""lang-sh prettyprint-override""><code>tensor([[  2,   0, 894, 213,   7, 334, 479,   2]])
['He go to school.'] 
</code></pre>
<p>Though it does not affect the final &quot;tokenization decoding&quot; results. But it seems weird to me that the first token we generate is actually <code>2</code>(<code>&lt;/s&gt;</code>).</p>
","2211979","","","","","2021-07-06 17:55:28","Why we need a decoder_start_token_id during generation in HuggingFace BART?","<nlp><pytorch><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"65591064","1","","","2021-01-06 06:18:03","","0","255","<p>I want to train a LayoutLM through huggingface transformer, however I need help in creating the training data for LayoutLM from my pdf documents.</p>
","1771338","","","","","2021-01-06 06:18:03","How to prepare custom training data for LayoutLM","<nlp><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"62302499","1","62303090","","2020-06-10 11:31:07","","0","160","<p>I am working with Bert and the library <a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">https://huggingface.co/models</a> hugginface.
I was wondering which of the models available you would choose for debugging?</p>

<p>In other words which models trains/loads the fast on my GPU, to get runs as fast as possible?
Albert, distillbert or?</p>
","6614410","","","","","2020-06-10 12:01:50","Huggingface Bert, Which Bert flavor is the fastest to train for debugging?","<machine-learning><nlp><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"62219426","1","","","2020-06-05 15:56:12","","1","509","<p>I want to add new words to my BPE tokenizer. I know the symbol Ä  means the end of a new token and the majority of tokens in vocabs of pre-trained tokenizers start with Ä . Assume I want to add the word <strong>Salah</strong> to my tokenizer. I tried to add both <strong>Salah</strong> token and <strong>Ä Salah</strong>:
tokenizer.add_tokens(['Salah', 'Ä Salah']) # they get 50265 and 50266 values respectively.
However, when I tokenize a sentence where <strong>Salah</strong> appears, the tokenizer will never return me the second number (neither when using <code>.tokenize</code>nor<code>.encode</code>), for instance:
<code>tokenizer.tokenize('I love Salah and salad')</code> returns <code>['I', 'Ä love', 'Salah', 'Ä and', 'Ä salad']</code>.
The question is: should I use the symbol <code>Ä </code> when adding new tokens or the tokenizer does it itself? Or, probably, it must be specified manually?
Thanks in advance!</p>
","13494387","","1243762","","2020-11-29 12:05:47","2020-11-29 12:05:47","Adding tokens to GPT-2 BPE tokenizer","<python><nlp><tokenize><huggingface-transformers><gpt-2>","0","2","","","","CC BY-SA 4.0"
"63519373","1","","","2020-08-21 08:34:28","","2","470","<p>I tried doing tokenisation using documentation of huggingface transformers</p>
<pre><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
encoded_input = tokenizer(batch_of_sequences)
</code></pre>
<p>Pre Trained Tokenizer gives output of dictionary containing three keys which are</p>
<pre><code>encoded_input = {
'input_ids': [[],[],[]],
'token_type_ids': [[],[],[]],
'attention_mask': [[],[],[]]
}

</code></pre>
<p>Trainer API requires input of Train &amp; Eval Dataset of type <code>torch.utils.data.Dataset</code>.</p>
<p>How can we use this output to create training dataset required for Trainer API?</p>
","14142121","","1513086","","2020-08-21 13:49:09","2021-05-18 18:15:28","How to convert tokenizer output to train_dataset which is required by Trainer API in Huggingface Transformers?","<python><huggingface-transformers>","2","1","","","","CC BY-SA 4.0"
"61569900","1","61704397","","2020-05-03 05:10:34","","3","1011","<p>Prior to passing my tokens through BERT, I would like to perform some processing on their embeddings, (the result of the embedding lookup layer). The <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""nofollow noreferrer"">HuggingFace BERT TensorFlow implementation</a> allows us to access the output of embedding lookup using:</p>

<pre><code>import tensorflow as tf
from transformers import BertConfig, BertTokenizer, TFBertModel

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

input_ids = tf.constant(bert_tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True))[None, :]
attention_mask = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])
token_type_ids = tf.stack([tf.ones(shape=(len(sent),)) for sent in input_ids])

config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)
bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)

result = bert_model(inputs={'input_ids': input_ids, 
                            'attention_mask': attention_mask, 
                            'token_type_ids': token_type_ids})
inputs_embeds = result[-1][0]  # output of embedding lookup
</code></pre>

<p>Subsequently, one can process <code>inputs_embeds</code> and then send this in as an input to the same model using:</p>

<pre><code>inputs_embeds = process(inputs_embeds)  # some processing on inputs_embeds done here (dimensions kept the same)
result = bert_model(inputs={'inputs_embeds': inputs_embeds, 
                            'attention_mask': attention_mask, 
                            'token_type_ids': token_type_ids})
output = result[0]
</code></pre>

<p>where <code>output</code> now contains the output of BERT for the modified input. However, this requires two full passes through BERT. Instead of running BERT all the way through just to perform embedding lookup, I would like to just get the output of the embedding lookup layer. <strong>Is this possible, and if so, how?</strong></p>
","424306","","","","","2020-05-09 22:44:28","Getting embedding lookup result from BERT","<python><tensorflow><nlp><huggingface-transformers><bert-language-model>","1","0","2","","","CC BY-SA 4.0"
"62303645","1","","","2020-06-10 12:31:09","","1","367","<p>I'm trying to train NER using distilbert on CPU. However, training is slow. Is there any way to do some CPU optimization to reduce the training time?</p>
","9353620","","","","","2020-06-10 12:31:09","Hugging Face Transformer multiprocessing training","<python><deep-learning><pytorch><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"64047261","1","","","2020-09-24 13:13:53","","8","2971","<p>I'm trying to train a model using a Trainer, according to the documentation (<a href=""https://huggingface.co/transformers/master/main_classes/trainer.html#transformers.Trainer"" rel=""noreferrer"">https://huggingface.co/transformers/master/main_classes/trainer.html#transformers.Trainer</a>) I can specify a tokenizer:</p>
<blockquote>
<p>tokenizer (PreTrainedTokenizerBase, optional) â€“ The tokenizer used to
preprocess the data. If provided, will be used to automatically pad
the inputs the maximum length when batching inputs, and it will be
saved along the model to make it easier to rerun an interrupted
training or reuse the fine-tuned model.</p>
</blockquote>
<p>So padding should be handled automatically, but when trying to run it I get this error:</p>
<blockquote>
<p>ValueError: Unable to create tensor, you should probably activate
truncation and/or padding with 'padding=True' 'truncation=True' to
have batched tensors with the same length.</p>
</blockquote>
<p>The tokenizer is created this way:</p>
<pre><code>tokenizer = BertTokenizerFast.from_pretrained(pretrained_model)
</code></pre>
<p>And the Trainer like that:</p>
<pre><code>trainer = Trainer(
    tokenizer=tokenizer,
    model=model,
    args=training_args,
    train_dataset=train,
    eval_dataset=dev,
    compute_metrics=compute_metrics
)
</code></pre>
<p>I've tried putting the <code>padding</code> and <code>truncation</code> parameters in the tokenizer, in the Trainer, and in the training_args. Nothing does. Any idea?</p>
","4657751","","","","","2021-02-01 10:43:18","How to make a Trainer pad inputs in a batch with huggingface-transformers?","<python><pytorch><huggingface-transformers>","1","2","2","","","CC BY-SA 4.0"
"65806586","1","65807548","","2021-01-20 09:23:47","","0","221","<p>i'm fairly new to tensorflow and would appreciate answers a lot.
i'm trying to use a transformer model as an embedding layer and feed the data to a custom model.</p>
<pre><code>from transformers import TFAutoModel
from tensorflow.keras import layers
def build_model():
    transformer_model = TFAutoModel.from_pretrained(MODEL_NAME, config=config)
    
    input_ids_in = layers.Input(shape=(MAX_LEN,), name='input_ids', dtype='int32')
    input_masks_in = layers.Input(shape=(MAX_LEN,), name='attention_mask', dtype='int32')

    embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]

    X = layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)
    X = layers.GlobalMaxPool1D()(X)
    X = layers.Dense(64, activation='relu')(X)
    X = layers.Dropout(0.2)(X)
    X = layers.Dense(30, activation='softmax')(X)

    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)

    for layer in model.layers[:3]:
        layer.trainable = False

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

    
model = build_model()
model.summary()
r = model.fit(
            train_ds,
            steps_per_epoch=train_steps,
            epochs=EPOCHS,
            verbose=3)
</code></pre>
<p>I have 30 classes and the labels are not one-hot encoded so im using sparse_categorical_crossentropy as my loss function but i keep getting the following error</p>
<pre><code>ValueError: Shape mismatch: The shape of labels (received (1,)) should equal the shape of logits except for the last dimension (received (10, 30)).
</code></pre>
<p>how can i solve this?
and why is the (10, 30) shape required? i know 30 is because of the last Dense layer with 30 units but why the 10? is it because of the MAX_LENGTH which is 10?</p>
<p>my model summary:</p>
<pre><code>Model: &quot;model_16&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 10)]         0                                            
__________________________________________________________________________________________________
attention_mask (InputLayer)     [(None, 10)]         0                                            
__________________________________________________________________________________________________
tf_bert_model_21 (TFBertModel)  TFBaseModelOutputWit 162841344   input_ids[0][0]                  
                                                                 attention_mask[0][0]             
__________________________________________________________________________________________________
bidirectional_17 (Bidirectional (None, 10, 100)      327600      tf_bert_model_21[0][0]           
__________________________________________________________________________________________________
global_max_pooling1d_15 (Global (None, 100)          0           bidirectional_17[0][0]           
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 64)           6464        global_max_pooling1d_15[0][0]    
__________________________________________________________________________________________________
dropout_867 (Dropout)           (None, 64)           0           dense_32[0][0]                   
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 30)           1950        dropout_867[0][0]                
==================================================================================================
Total params: 163,177,358
Trainable params: 336,014
Non-trainable params: 162,841,344
</code></pre>
","7359978","","","","","2021-01-20 10:21:17","ValueError: Shape mismatch: The shape of labels (received (1,)) should equal the shape of logits except for the last dimension (received (10, 30))","<python><tensorflow><keras><tensorflow2.0><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62109957","1","62110180","","2020-05-30 23:40:13","","0","276","<p>Here's the code in question. </p>

<p><a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491</a></p>

<pre><code>class BertOnlyNSPHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, pooled_output):
        seq_relationship_score = self.seq_relationship(pooled_output)
        return seq_relationship_score
</code></pre>

<p>I think it was just ranking how likely one sentence would follow another? Wouldn't it be one score?</p>
","3259896","","","","","2020-05-31 00:19:18","Why does the BERT NSP head linear layer have two outputs?","<nlp><pytorch><transformer><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"63045229","1","67606464","","2020-07-23 00:54:12","","4","155","<p>I'm currently using Huggingface's Trainer class to train Distillbert for a regression problem using a custom loss function. I'm using their checkpoints to resume training due to the ephemeral nature of compute / unexpected errors.</p>
<p>The issue I'm facing is that each time I resume training from a checkpoint as per their Trainer class via the <code>model_path</code> in the <code>Trainer.train()</code> method, I noticed that the class iterates over the dataloader until it reaches the iteration count as saved in the checkpoint (<a href=""https://github.com/huggingface/transformers/blob/33d7506ea10ca92886fd1bb3b5306a1a720c58fe/src/transformers/trainer.py#L500"" rel=""nofollow noreferrer"">see the lines from the Trainer class that match the issue</a>).</p>
<p>This might usually not be a issue, but due to the nature of my dataloader's collate function and the size of the dataset, iterating for such a duration without any training is pretty expensive and slows down the overall training.</p>
<p>I planned on utilizing a custom sampler class <a href=""https://discuss.pytorch.org/t/resume-iterating-dataloader-from-checkpoint-batch-idx/60683"" rel=""nofollow noreferrer"">something along the lines of this</a> with a parameter to resume the indices from a given location but that too seems quite the hack for the given problem.</p>
<p>What could be an alternative that I could try to save on this wasted compute cycles?</p>
","2925622","","","","","2021-05-19 15:41:06","How to avoid iterating over Dataloader while resuming training in Huggingface Trainer class?","<pytorch><transformer><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"61739662","1","","","2020-05-11 21:26:22","","0","202","<p>I have run the run_train.sh script at and at steps of 75000, the model still couldn't produce a decent summary. </p>

<p>Here is an example summary generated from model trained from run_train.sh which is still so broken:
Kimberlyï¿½ have else  better' she). am New other"" on aag like.) in when, - d you,'] first her A what Kim Thef into here or the said has'm herself Former NBC came In will one Women '- to around thereÂ  can at another Woman Produ ab it had,"" out no up are], new be isar? [. that doit</p>

<p>I do notice the ""(WIP) Rouge Scores"" at <a href=""https://github.com/huggingface/transformers/tree/master/examples/summarization/bart"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/summarization/bart</a>. I wonder if the fine-tune code is finished with model performance verified. </p>
","4490160","","","","","2020-05-26 21:02:34","BART finetune run_train.sh generates models with performance issues","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62119250","1","","","2020-05-31 16:30:18","","4","426","<p>I am trying to understand what merge.txt file infers in tokenizers for RoBERTa model in HuggingFace library. However, nothing is said about it on their website. Any help is appreciated.</p>
","13494387","","","","","2020-08-18 14:48:26","What does merge.txt file mean in BERT-based models in HuggingFace library?","<nlp><tokenize><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"62704534","1","","","2020-07-02 20:26:57","","2","643","<p>while binary classification with a finetuned Bert worked well, I am stuck with the multiclass classification. My dataset (german news articles, with 10 classes) contains roughly 10.000 samples.
Although, training loss and average evaluation loss circle around 2.2.</p>
<p>Some NLP config variables:</p>
<pre><code>DEBUG=True
VERSION = 1
MAX_LEN = 200 #Set the maximum length according to the diagram above
BATCH_SIZE = 16
EPOCHS = 3
LEARNING_RATE = 2e-4
MOMENTUM = 0.9
TRAIN_SIZE = 0.7
NUM_LABELS = len(df_data.Labels.unique())
MODEL_NAME = &quot;dbmdz/bert-base-german-cased&quot;
params = {&quot;debug&quot;: DEBUG, &quot;max_len&quot;: MAX_LEN, &quot;batch_size&quot;: BATCH_SIZE, &quot;epochs&quot;: EPOCHS, &quot;lr&quot;: LEARNING_RATE, &quot;momentum&quot;: MOMENTUM, &quot;model&quot;: MODEL_NAME, &quot;loss&quot;: &quot;BCEWithLogitsLoss&quot;, &quot;optimizer&quot;: &quot;SGD&quot;}
tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)
</code></pre>
<p>The DataLoader:</p>
<pre><code>class NLPDataset(th.utils.data.Dataset):

  def __init__(self, dataframe, tokenizer, max_len):
    self.tokenizer = tokenizer
    self.max_len = max_len
    self.data = dataframe
    self.text = dataframe.Text
    self.targets = dataframe.Labels
    self.len = len(self.text)

  def __getitem__(self,idx):
    text = str(self.text[idx])
    text = &quot; &quot;.join(text.split())

    inputs = self.tokenizer(
        text,
        None,
        add_special_tokens=True,
        max_length=self.max_len,
        pad_to_max_length=True,
        return_token_type_ids=True,
        truncation=True,
        padding='max_length',
    )
    input_ids = inputs['input_ids']
    mask = inputs['attention_mask']
    token_type_ids = inputs[&quot;token_type_ids&quot;]

    return {
        'input_ids': th.tensor(input_ids, dtype=th.long),
        'mask': th.tensor(mask, dtype=th.float),
        'token_type_ids': th.tensor(token_type_ids, dtype=th.long),
        'targets': th.tensor(self.targets[idx], dtype=th.long)
    }
  
  def __len__(self):
    return self.len
</code></pre>
<p>The Pytorch_lightning Module:</p>
<pre><code>class NLPClassifier(pl.LightningModule):

  def __init__(self):
      super().__init__()

      #changing the configuration to 10 lables instead of 2
      config = transformers.AutoConfig.from_pretrained(MODEL_NAME)
      config.num_labels = NUM_LABELS
      self.model = transformers.AutoModelForSequenceClassification.from_config(config)
      self.loss = th.nn.CrossEntropyLoss(reduction=&quot;none&quot;)

  def prepare_data(self):
      # train/val split
      train_dataset = df_data.sample(frac=TRAIN_SIZE)
      val_dataset=df_data.drop(train_dataset.index).sample(frac=1).reset_index(drop=True)
      train_dataset = train_dataset.reset_index(drop=True)

      # Assign CustomDataset Class
      train_set = NLPDataset(train_dataset, tokenizer, MAX_LEN)
      val_set = NLPDataset(val_dataset, tokenizer, MAX_LEN)

      print(&quot;FULL Dataset: {}&quot;.format(df_data.shape))
      print(&quot;TRAIN Dataset: {}&quot;.format(train_dataset.shape))
      print(&quot;VAL Dataset: {}&quot;.format(val_dataset.shape))

      # assign to use in dataloaders
      self.train_ds = train_set
      self.val_ds = val_set
      #self.test_dataset = mnist_test DO TO
  
  def forward(self, input_ids, mask):
      logits, = self.model(input_ids, mask)
      # logits.shape: (16, 10)
      return logits
  
  def training_step(self, batch, batch_idx):
    logits = self.forward(batch['input_ids'], batch['mask']).squeeze()
    loss = self.loss(logits, batch['targets']).mean()
    run.log(name='train_loss', value=loss.tolist())
    return {'loss': loss, 'log': {'train_loss': loss}}
  
  def validation_step(self,batch, batch_idx):
    logits = self.forward(batch['input_ids'], batch['mask']).squeeze()
    print(logits.shape)
    acc = (logits.argmax(-1) == batch['targets']).float()
    loss = self.loss(logits, batch['targets'])
    run.log_list('loss', loss.tolist())
    run.log_list('acc', acc.tolist())
    return {'loss': loss, 'acc': acc}

  def validation_epoch_end(self, outputs):
    loss = th.cat([o['loss'] for o in outputs], 0).mean()
    acc = th.cat([o['acc'] for o in outputs], 0).mean()
    out = {'val_loss': loss, 'val_acc': acc}
    run.log('val_loss', loss.tolist())
    run.log('val_acc', acc.tolist())
    return {**out, 'log': {'val_loss': loss, 'val_acc': acc}}
    
  def train_dataloader(self):
      return th.utils.data.DataLoader(
          self.train_ds,
          batch_size=BATCH_SIZE,
          num_workers=8,
          drop_last=True,
          shuffle=False,
      )
  
  def val_dataloader(self):
      return th.utils.data.DataLoader(
          self.val_ds,
          batch_size=BATCH_SIZE,
          num_workers=8,
          drop_last=False,
          shuffle=False,
      )

  
  def configure_optimizers(self):
      return transformers.AdamW(
            self.model.parameters(),
            lr=LEARNING_RATE,
            #momentum=MOMENTUM,
        )
</code></pre>
<p>The trainer:</p>
<pre><code>model = NLPClassifier()
trainer = pl.Trainer(
   gpus=(1 if th.cuda.is_available() else 0),
   default_root_dir = f&quot;./models/version_{VERSION}&quot;,
   max_epochs=EPOCHS,
   fast_dev_run=DEBUG,
   limit_train_batches=1.0,
   val_check_interval=0.5,
   limit_val_batches=1.0,
   profiler=True,
   #logger=wandb_logger
   )
</code></pre>
<p>trainer.fit(model)</p>
<p>This is a sample loss curve.</p>
<p><a href=""https://i.stack.imgur.com/0MvH7.png"" rel=""nofollow noreferrer"">Sample loss curve</a></p>
<p>My central questions are:</p>
<ol>
<li>Is CrossEntropyLoss correctly used?</li>
<li>Does the optimizer work, as the predictions fairly quickly become the same for every sample.</li>
<li>The learning rate issue did not solve the problem. I tried a range from 1e-2 to 1e-6</li>
</ol>
<p>Thanks for your help. :)</p>
","13728592","","","","","2020-07-02 20:26:57","BERT Fine tuning: High loss and low accuracy in multiclass classification","<pytorch><huggingface-transformers><pytorch-lightning>","0","4","","","","CC BY-SA 4.0"
"62746180","1","62774709","","2020-07-05 20:49:34","","0","472","<p>So I installed the latest version of transformers on Google Colab</p>
<pre><code>!pip install transformers 
</code></pre>
<p>When trying to invoke the conversion file using</p>
<pre><code>!python /usr/local/lib/python3.6/dist-packages/transformers/convert_pytorch_checkpoint_to_tf2.py .py --help  
</code></pre>
<p>Or trying to use</p>
<pre><code>from transformers.file_utils import hf_bucket_url.                                 // works 
from transformers.convert_pytorch_checkpoint_to_tf2 import *.                      // fails

convert_pytorch_checkpoint_to_tf(&quot;gpt2&quot;, pytorch_file, config_file, tf_file).      
</code></pre>
<p>I get this error</p>
<pre><code> ImportError                               Traceback (most recent call last)

&lt;ipython-input-3-dadaf83ecea0&gt; in &lt;module&gt;()
      1 from transformers.file_utils import hf_bucket_url
----&gt; 2 from transformers.convert_pytorch_checkpoint_to_tf2 import *
      3 
      4 convert_pytorch_checkpoint_to_tf(&quot;gpt2&quot;, pytorch_file, config_file, tf_file)
/usr/local/lib/python3.6/dist-packages/transformers/convert_pytorch_checkpoint_to_tf2.py in &lt;module&gt;()
     20 import os
     21 
---&gt; 22 from transformers import (
     23     ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP,
     24     BERT_PRETRAINED_CONFIG_ARCHIVE_MAP,

ImportError: cannot import name 'hf_bucket_url'
</code></pre>
<p>What's going on?</p>
","1019952","","","","","2020-07-07 11:56:01","ImportError: cannot import name 'hf_bucket_url' in HuggingFace Transformers","<tensorflow><pytorch><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"62040309","1","62130091","","2020-05-27 09:54:16","","0","2978","<p>In the code by Hugginface transformers, there are many fine-tuning models have the function <code>init_weight</code>. 
For example(<a href=""https://github.com/huggingface/transformers/blob/a9aa7456ac/src/transformers/modeling_bert.py#L1073-L1082"" rel=""nofollow noreferrer"">here</a>), there is a <code>init_weight</code> function at last.</p>

<pre class=""lang-py prettyprint-override""><code>class BertForSequenceClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        self.init_weights()

</code></pre>

<p>As I know, it will call the following <a href=""https://github.com/huggingface/transformers/blob/a9aa7456ac/src/transformers/modeling_bert.py#L520-L530"" rel=""nofollow noreferrer"">code</a></p>

<pre class=""lang-py prettyprint-override""><code>def _init_weights(self, module):
    """""" Initialize the weights """"""
    if isinstance(module, (nn.Linear, nn.Embedding)):
        # Slightly different from the TF version which uses truncated_normal for initialization
        # cf https://github.com/pytorch/pytorch/pull/5617
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
    elif isinstance(module, BertLayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if isinstance(module, nn.Linear) and module.bias is not None:
        module.bias.data.zero_()
</code></pre>

<p>My question is <strong>If we are loading the pre-trained model, why do we need to initialize the weight for every module?</strong></p>

<p>I guess I must be misunderstanding something here.</p>
","2211979","","","","","2021-07-06 08:00:32","Why we need the init_weight function in BERT pretrained model in Huggingface Transformers?","<python><huggingface-transformers><bert-language-model>","2","0","1","","","CC BY-SA 4.0"
"62327803","1","62328920","","2020-06-11 15:23:32","","2","1456","<p>I was just wondering if it is possibel to extend the HuggingFace <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">BertForSequenceClassification</a> model to more than 2 labels. The docs say, we can pass positional arguments, but it seems like ""labels"" is not working. Does anybody has an idea?</p>

<h2>Model assignment</h2>

<pre class=""lang-py prettyprint-override""><code>labels = th.tensor([0,0,0,0,0,0], dtype=th.long).unsqueeze(0)
print(labels.shape)
modelBERTClass = transformers.BertForSequenceClassification.from_pretrained(
    'bert-base-uncased', 
    labels=labels
    )

l = [module for module in modelBERTClass.modules()]
l
</code></pre>

<h2>Console Output</h2>

<pre><code>torch.Size([1, 6])
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-122-fea9a36402a6&gt; in &lt;module&gt;()
      3 modelBERTClass = transformers.BertForSequenceClassification.from_pretrained(
      4     'bert-base-uncased',
----&gt; 5     labels=labels
      6     )
      7 

/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    653 
    654         # Instantiate model.
--&gt; 655         model = cls(config, *model_args, **model_kwargs)
    656 
    657         if state_dict is None and not from_tf:

TypeError: __init__() got an unexpected keyword argument 'labels'
</code></pre>
","13728592","","","","","2020-06-11 16:22:25","Having 6 labels instead of 2 in Hugging Face BertForSequenceClassification","<python><transformer><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"62206248","1","","","2020-06-05 00:31:11","","3","95","<p>I'm experimenting with text generators, like OpenAI's GPT-2, Hugging Face's transformers, and Facebook's ParlAI, and I'm wondering if I can limit or weight the output to a specified list of words? For example, how can I limit the output to only words that start with the letter 'a'?</p>

<p>One obvious idea is to train on a dataset that is limited by that vocabulary, but I only have a laundry list of words, not a natural corpus that only has those words.</p>
","210173","","","","","2020-06-05 00:31:11","NLP - Specify custom vocabulary / word list for text generation","<nlp><huggingface-transformers><parlai>","0","0","","","","CC BY-SA 4.0"
"63152188","1","63173987","","2020-07-29 11:05:06","","0","1370","<p>I have downloaded the Norwegian BERT-model from <a href=""https://github.com/botxo/nordic_bert"" rel=""nofollow noreferrer"">https://github.com/botxo/nordic_bert</a>, and loaded it in using:</p>
<pre><code>import transformers as t

model_class = t.BertModel
tokenizer_class = t.BertTokenizer

tokenizer = tokenizer_class.from_pretrained(/PATH/TO/MODEL/FOLDER)
model = model_class.from_pretrained(/PATH/TO/MODEL)
model.eval()
</code></pre>
<p>This works very well, however when i try to tokenize a given sentence, some nordic characters such as &quot;Ã¸&quot; and &quot;Ã¦&quot; remain the same, whereas all words having the char &quot;Ã¥&quot; is replaced with &quot;a&quot;.
For instance:</p>
<pre><code>s = &quot;Ã¦ Ã¸ Ã¥ lÃ¸pe fÃ¥ Ã¦rfugl&quot;
print(tokenizer.tokenize(s))
</code></pre>
<p>Yields:</p>
<pre><code>['Ã¦', 'Ã¸', 'a', 'lÃ¸p', '##e', 'fa', 'Ã¦r', '##fugl']
</code></pre>
<p>Thanks</p>
","14015397","","","","","2020-08-05 17:57:21","HuggingFace Transformers: BertTokenizer changing characters","<nlp><huggingface-transformers><huggingface-tokenizers>","1","8","","","","CC BY-SA 4.0"
"63377135","1","","","2020-08-12 13:04:50","","1","489","<p>I am looking, for example, script/notebook to train GPT2 and Reformer model from scratch in German.
Something similar to :</p>
<blockquote>
<p><a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb</a></p>
</blockquote>
<p>I am trying to modify the same notebook but GPT2 doesn't seem to accept LinebyLineDataset or padding.</p>
<p>My Error is:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;timed eval&gt; in &lt;module&gt;

~/anaconda3/envs/thesis_p1/lib/python3.6/site-packages/transformers/trainer.py in train(self, model_path)
    490                 self._past = None
    491 
--&gt; 492             for step, inputs in enumerate(epoch_iterator):
    493 
    494                 # Skip past any already trained steps if resuming training

~/anaconda3/envs/thesis_p1/lib/python3.6/site-packages/tqdm/notebook.py in __iter__(self, *args, **kwargs)
    226     def __iter__(self, *args, **kwargs):
    227         try:
--&gt; 228             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):
    229                 # return super(tqdm...) will not catch exception
    230                 yield obj

~/anaconda3/envs/thesis_p1/lib/python3.6/site-packages/tqdm/std.py in __iter__(self)
   1128 
   1129         try:
-&gt; 1130             for obj in iterable:
   1131                 yield obj
   1132                 # Update and possibly print the progressbar.

~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)
    344     def __next__(self):
    345         index = self._next_index()  # may raise StopIteration
--&gt; 346         data = self.dataset_fetcher.fetch(index)  # may raise StopIteration
    347         if self.pin_memory:
    348             data = _utils.pin_memory.pin_memory(data)

~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     45         else:
     46             data = self.dataset[possibly_batched_index]
---&gt; 47         return self.collate_fn(data)

~/anaconda3/envs/thesis_p1/lib/python3.6/site-packages/transformers/data/data_collator.py in __call__(self, examples)
     79 
     80     def __call__(self, examples: List[torch.Tensor]) -&gt; Dict[str, torch.Tensor]:
---&gt; 81         batch = self._tensorize_batch(examples)
     82         if self.mlm:
     83             inputs, labels = self.mask_tokens(batch)

~/anaconda3/envs/thesis_p1/lib/python3.6/site-packages/transformers/data/data_collator.py in _tensorize_batch(self, examples)
     96             if self.tokenizer._pad_token is None:
     97                 raise ValueError(
---&gt; 98                     &quot;You are attempting to pad samples but the tokenizer you are using&quot;
     99                     f&quot; ({self.tokenizer.__class__.__name__}) does not have one.&quot;
    100                 )

ValueError: You are attempting to pad samples but the tokenizer you are using (GPT2Tokenizer) does not have one.
</code></pre>
<p>Here is my current implementation:</p>
<p>Datasets looks like this (Million lines):</p>
<pre><code>1   &quot;09.05.2019, Flyer: Zeit fÃ¼r Perspektiven - UnterstÃ¼tzung im Haushalt durch professionelle Dienstleistungen&quot;
2   %0A%0ADie Burg Werle (ca. 10 km von hier entfernt) war ein schwer einnehmbarer Schlupfwinkel.
3   %0A%0AHier, abseits der verkehrsreichen StraÃŸen, liegt das idyllische Quellental, ein Naturdenkmal der besonderen Art.
4   Â½ bis 1 Tasse (75â€“150 ml) HEITMANN Reine CitronensÃ¤ure in Â½ Liter Wasser geben und in den Wassertank der Maschine fÃ¼llen.
5   %0% der anfallenden Kosten ergeben sich aus der StraÃŸenbeleuchtung.
6   Â¾ Parken wÃ¤hrend der Ladezeit in FuÃŸgÃ¤ngerzonen, in denen das Be- oder Entladen fÃ¼r bestimmte Zeiten freigegeben ist.
</code></pre>
<p>First I train Sentence Piece Tokenizer:</p>
<pre><code>from pathlib import Path
import sentencepiece as spm
paths = [str(x) for x in Path(&quot;.&quot;).glob(&quot;**/*.txt&quot;)]
arg='--input=deu-de_web-public_2019_1M-sentences.txt --model_prefix=m_test --vocab_size=52000'
spm.SentencePieceTrainer.train(arg)
</code></pre>
<p>Then I load my GPT2 tokenizer as below:</p>
<pre><code>from transformers import GPT2TokenizerFast

tokenizer = GPT2Tokenizer.from_pretrained(&quot;./German&quot;,additional_special_tokens=[&quot;&lt;s&gt;&quot;,&quot;&lt;pad&gt;&quot;,&quot;&lt;/s&gt;&quot;,&quot;&lt;unk&gt;&quot;,&quot;&lt;mask&gt;&quot;], max_len=512)
</code></pre>
<p>Here is my GPT2 config and language Model:</p>
<pre><code>from transformers import GPT2LMHeadModel, GPT2Config

# Initializing a GPT2 configuration
configuration = GPT2Config(vocab_size=52_000)
model = GPT2LMHeadModel(config=configuration)
</code></pre>
<p>The logic for Dataset Preparation:</p>
<pre><code>from transformers import LineByLineTextDataset

dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=&quot;./deu-de_web-public_2019_1M-sentences.txt&quot;,
    block_size=128,
)
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False,
)
</code></pre>
<p>The training logic:</p>
<pre><code>from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=&quot;./output&quot;,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_gpu_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
    prediction_loss_only=True,
)
trainer.train()
</code></pre>
","11937059","","6664872","","2020-08-13 18:09:31","2020-08-13 18:09:31","Training GPT2 and Reformer from scratch","<nlp><huggingface-transformers><huggingface-tokenizers>","0","2","","","","CC BY-SA 4.0"
"61647418","1","","","2020-05-06 23:49:22","","-3","588","<p>I was trying to get the extractive BertSUM summarizer working (<a href=""https://github.com/Alcamech/PreSumm/tree/PreSumm_Raw_Input_Text_Setup"" rel=""nofollow noreferrer"">Paper and Github here</a>) 
but i still get the following message </p>

<pre><code>xent 0 at step -1""
</code></pre>

<p>and no summary is produced. What i'am doing wrong? Can someone please help me with that,  perhaps provide a working example. the above message apppeared when i did the following in google colab:</p>

<p>1 clone requiered GitHub</p>

<pre><code>!git clone https://github.com/Alcamech/PreSumm.git
</code></pre>

<p>2 Change Git-Branch for summarization of raw text data</p>

<pre><code>%cd /content/PreSumm
!git checkout -b  Raw_Input origin/PreSumm_Raw_Input_Text_Setup
!git pull
</code></pre>

<p>3 install requirements</p>

<pre><code>!pip install torch==1.1.0 pytorch_transformers tensorboardX multiprocess pyrouge
</code></pre>

<p>4 install CNN/DM Extractive bertext_cnndm_transformer.pt</p>

<pre><code>!gdown https://drive.google.com/uc?id=1kKWoV0QCbeIuFt85beQgJ4v0lujaXobJ&amp;export=download
!unzip /content/PreSumm/models/bertext_cnndm_transformer.zip
</code></pre>

<p>4.1  Download the Pre-Processed data for CNN/Dailymail</p>

<pre><code>%cd /content/PreSumm/bert_data/
!gdown https://drive.google.com/uc?id=1DN7ClZCCXsk2KegmC6t4ClBwtAf5galI&amp;export=download
!unzip /content/PreSumm/bert_data/bert_data_cnndm_final.zip
</code></pre>

<p>5 change to /src folder</p>

<pre><code>cd /content/PreSumm/src/
</code></pre>

<p>6 run the extractive summarizer</p>

<pre><code>!python /content/PreSumm/src/train.py -task ext -mode test_text -test_from /content/PreSumm/models/bertext_cnndm_transformer.pt -text_src /content/PreSumm/raw_data/temp_ext.raw_src -text_tgt /content/PreSumm/results/result.txt -log_file /content/PreSumm/logs/ext_bert_cnndm
</code></pre>

<p>The Output of Step 6 is:</p>

<pre><code>[2020-05-07 11:20:12,355 INFO] Loading checkpoint from /content/PreSumm/models/bertext_cnndm_transformer.pt
Namespace(accum_count=1, alpha=0.6, batch_size=140, beam_size=5, bert_data_path='../bert_data_new/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='/content/PreSumm/logs/ext_bert_cnndm', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=150, max_ndocs_in_batch=6, max_pos=512, max_tgt_len=140, min_length=15, mode='test_text', model_path='../models/', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=5, seed=666, sep_optim=False, share_emb=False, task='ext', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='/content/PreSumm/models/bertext_cnndm_transformer.pt', test_start_from=-1, text_src='/content/PreSumm/raw_data/temp_ext.raw_src', text_tgt='/content/PreSumm/results/result.txt', train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='-1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)
[2020-05-07 11:20:13,361 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpvck0jwoy
100% 433/433 [00:00&lt;00:00, 309339.74B/s]
[2020-05-07 11:20:13,498 INFO] copying /tmp/tmpvck0jwoy to cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2020-05-07 11:20:13,499 INFO] creating metadata file for ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2020-05-07 11:20:13,499 INFO] removing temp file /tmp/tmpvck0jwoy
[2020-05-07 11:20:13,499 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2020-05-07 11:20:13,500 INFO] Model config {
  ""architectures"": [
    ""BertForMaskedLM""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""finetuning_task"": null,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""layer_norm_eps"": 1e-12,
  ""max_position_embeddings"": 512,
  ""model_type"": ""bert"",
  ""num_attention_heads"": 12,
  ""num_hidden_layers"": 12,
  ""num_labels"": 2,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""pad_token_id"": 0,
  ""pruned_heads"": {},
  ""torchscript"": false,
  ""type_vocab_size"": 2,
  ""vocab_size"": 30522
}

[2020-05-07 11:20:13,571 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp6b78t4_2
100% 440473133/440473133 [00:06&lt;00:00, 71548841.10B/s]
[2020-05-07 11:20:19,804 INFO] copying /tmp/tmp6b78t4_2 to cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-05-07 11:20:21,212 INFO] creating metadata file for ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-05-07 11:20:21,212 INFO] removing temp file /tmp/tmp6b78t4_2
[2020-05-07 11:20:21,267 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
gpu_rank 0
[2020-05-07 11:20:24,645 INFO] * number of parameters: 120512513
[2020-05-07 11:20:24,736 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpyv3mwnb6
100% 231508/231508 [00:00&lt;00:00, 4268647.82B/s]
[2020-05-07 11:20:25,044 INFO] copying /tmp/tmpyv3mwnb6 to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-05-07 11:20:25,045 INFO] creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-05-07 11:20:25,045 INFO] removing temp file /tmp/tmpyv3mwnb6
[2020-05-07 11:20:25,046 INFO] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
  0% 0/2 [00:00&lt;?, ?it/s]
[2020-05-07 11:20:25,115 INFO] Validation xent: 0 at step -1
</code></pre>

<p>and the <em>result.txt</em>-file is empty. </p>

<p><a href=""https://colab.research.google.com/drive/1JhUxedmrAFyoFEQVq5UHGT7XaO-zX15d"" rel=""nofollow noreferrer"">Here</a> is a link to  a copy of my google colab, where you can see the full colde.<br>
I also tried these steps  on the origin-github-repo  <a href=""https://github.com/nlpyang/BertSum"" rel=""nofollow noreferrer"">here</a> and i get the same error.
Thanks for any help.</p>
","4439936","","4439936","","2020-05-07 11:40:50","2020-05-12 20:39:30","BertSumExt is not producing Summaries","<machine-learning><nlp><pytorch><summarization><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"64049513","1","","","2020-09-24 15:19:51","","1","298","<p>I am using a distilled bert model to perform question answering</p>
<p><a href=""https://huggingface.co/distilbert-base-cased-distilled-squad"" rel=""nofollow noreferrer"">https://huggingface.co/distilbert-base-cased-distilled-squad</a></p>
<p>Locally it all works fine, but when trying to use the model in my celery task, the worker thread logs show</p>
<blockquote>
<p>2020-09-24T03:50:05.794556+00:00 app[worker.1]: 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 187M/261M [00:14&lt;00:04, 15.5MB/s]
2020-09-24T03:50:19.779019+00:00 heroku[worker.1]: Process running mem=652M(123.2%)
2020-09-24T03:50:19.781651+00:00 heroku[worker.1]: Error R14 (Memory quota exceeded)</p>
</blockquote>
<p>The code below is where I my import, model loading and task takes place. Removing the imports and loading resolves the memory issue.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-cased-distilled-squad&quot;)
model = AutoModelForQuestionAnswering.from_pretrained(&quot;distilbert-base-cased-distilled-squad&quot;)
@app.task
def evaluateQuestionAnswer():
</code></pre>
<p>I'm wondering if I need to use something like tensorflow serving</p>
<p><a href=""https://www.tensorflow.org/tfx/guide/serving"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tfx/guide/serving</a></p>
<p>Or am I missing something about making models production ready?</p>
<p>Would using docker alleviate this loading problem?</p>
","3050491","","","","","2020-09-24 15:19:51","Using BERT (distilled) model in celery/redis task on heroku","<heroku><redis><nlp><celery><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"62591068","1","62594171","","2020-06-26 08:39:48","","2","219","<p>I am working on a French Question-Answering model using huggingface transformers library. I'm using a pre-trained CamemBERT model which is very similar to RoBERTa but is adapted to french.</p>
<p>Currently, i am able to get the best answer candidate for a question on a text of my own, using the QuestionAnsweringPipeline from the transformers library.</p>
<p>Here is an extract of my code.</p>
<pre><code>QA_model = &quot;illuin/camembert-large-fquad&quot;
CamTokQA = CamembertTokenizer.from_pretrained(QA_model)
CamQA = CamembertForQuestionAnswering.from_pretrained(QA_model)

device_pipeline = 0 if torch.cuda.is_available() else -1
q_a_pipeline = QuestionAnsweringPipeline(model=CamQA,
                                         tokenizer=CamTokQA,
                                         device=device_pipeline)

ctx = open(&quot;text/Sample.txt&quot;, &quot;r&quot;).read()
question = 'Quel est la taille de la personne ?'
res = q_a_pipeline({'question': question, 'context': ctx})
print(res)
</code></pre>
<p>I am currently getting this :<code>{'score': 0.9630325870663725, 'start': 2421, 'end': 2424, 'answer': '{21'} </code>, which is wrong.</p>
<p>Therefore, i would like to get the 5 best candidates for the answer. Does anyone have an idea how to do that ?</p>
","13257959","","","","","2020-06-26 12:02:08","NLP : Get 5 best candidates from QuestionAnsweringPipeline","<nlp><huggingface-transformers><bert-language-model><question-answering>","1","0","2","","","CC BY-SA 4.0"
"64908229","1","","","2020-11-19 08:49:15","","0","213","<p>I am working on ner using BERT. When I try to download vocab.txt file, it says unicode error.</p>
<p>I downloaded this to local:<a href=""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt</a></p>
<pre><code>tokenizer=BertTokenizer(&quot;/vocab.txt&quot;,do_lower_case=False)
</code></pre>
<p>It throws,</p>
<p>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd7 in position 5620: invalid continuation byte</p>
<pre><code> model = BertForTokenClassification.from_pretrained('models/bert-base-cased.bin',num_labels=len(tag2idx))
</code></pre>
<p>Says same error,Not sure what is wrong.</p>
<p>Kindly help.</p>
","7216834","","6664872","","2020-11-27 22:59:06","2020-11-27 22:59:06","UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd7 in position 5620: invalid continuation byte error while loading bert-base-cased-vocab.txt","<bert-language-model><huggingface-transformers><ner><huggingface-tokenizers>","0","1","","","","CC BY-SA 4.0"
"66183163","1","66197119","","2021-02-13 08:23:52","","1","277","<p>I was looking at potentially converting an ml NLP model to the ONNX format in order to take advantage of its speed increase (ONNX Runtime). However, I don't really understand what is fundamentally changed in the new models compared to the old models. Also, I don't know if there are any drawbacks. Any thoughts on this would be very appreciated.</p>
","14882176","","","","","2021-04-20 01:36:35","Does converting a seq2seq NLP model to the ONNX format negatively affect its performance?","<python><machine-learning><nlp><huggingface-transformers><onnx>","1","0","0","","","CC BY-SA 4.0"
"64113822","1","","","2020-09-29 06:08:54","","0","85","<pre><code>from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(&quot;/nfs/storages/bio_corpus/ner/BC2GM/ner_outputs&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;/nfs/storages/bio_corpus/ner/BC2GM/ner_outputs&quot;)

ner_model = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)

sequence = &quot;In this issue of Eurosurveillance, we are publishing two articles on different aspects of the newly emerged 2019-nCoV. One is a research article by Corman et al. on the development of a diagnostic methodology based on RT-PCR of the E and RdRp genes, without the need for virus material; the assays were validated in five international laboratoriesã€‚&quot;

ner_model(sequence)

[{'entity_group': 'B', 'score': 0.9881901144981384, 'word': 'E'},
 {'entity_group': 'B', 'score': 0.9853595495223999, 'word': 'Rd'},
 {'entity_group': 'I', 'score': 0.9730346202850342, 'word': '##Rp genes'}]
</code></pre>
<p>In the codes, the sub word was spited by &quot;##&quot;. please show me how to remove &quot;##&quot; and join 'Rd' and 'Rp genes' as an entity.</p>
<hr />
<pre><code>items = ner_model(sequence)
entities = []
for item in items:
    word = item['word']
    if word.startswith('##'):
        word = entities[len(entities)-1] + word.replace('##','')
        entities.pop()
    entities.append(word)
print(entities)
</code></pre>
","2281101","","2281101","","2020-10-01 00:26:30","2021-08-15 17:46:17","How to join sub words produced by the named entity recognization task on transformer huggingface?","<huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"64310515","1","","","2020-10-12 01:18:20","","0","211","<p>I'm using the Huggingface's Transformers pipeline function to download the model and the tokenizer, my Windows PC downloaded them but I don't know where they are stored on my PC. Can you please help me? <a href=""https://i.stack.imgur.com/eHRNO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eHRNO.png"" alt=""enter image description here"" /></a></p>
<pre><code>from transformers import pipeline

qa_pipeline = pipeline(
    &quot;question-answering&quot;,
    model=&quot;mrm8488/bert-multi-cased-finetuned-xquadv1&quot;,
    tokenizer=&quot;mrm8488/bert-multi-cased-finetuned-xquadv1&quot;
)
</code></pre>
","4805898","","","","","2020-10-12 10:20:27","Transformers pipeline model directory","<python><python-3.x><pipeline><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"64313576","1","","","2020-10-12 07:44:16","","0","398","<p>Training a <code>huggingface transformers</code> NER model according to the documentation, the evaluation loss increases after a few epochs, but the other scores (accuracy, precision, recall, f1) continuously getting better. The behaviour seems unexpected, is there a simple explanation for this effect? Can this depend on the given data?</p>
<p><a href=""https://i.stack.imgur.com/ZDdxz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZDdxz.jpg"" alt=""enter image description here"" /></a></p>
<pre><code>model = TokenClassificationModel.from_pretrained('roberta-base', num_labels=len(tag_values))


model.train()
model.zero_grad()

   for epoch in range(epochs):
      for batch in range(batches):
         -- train --
         ...

      train_loss = model.evaluate(train_data)
      validation_loss = model.evaluate(validation_data)



</code></pre>
","8590398","","","","","2020-11-12 22:45:02","Why does the evaluation loss increases when training a huggingface transformers NER model?","<python><pytorch><huggingface-transformers>","1","5","","","","CC BY-SA 4.0"
"64606333","1","64608214","","2020-10-30 10:09:40","","2","417","<p>Currently I am working on productionize a NER model on Spark. I have a current implementation that is using Huggingface DISTILBERT with the TokenClassification head, but as the performance is a bit slow and costly, I am trying to find ways to optimize.</p>
<p>I have checked SPARKNLP implementation, which lacks a pretrained DISTILBERT and has I think a different approach, so some questions regarding this arose:</p>
<ol>
<li>Huggingface uses the entire BERT model and adds a head for token classification. Is this the same as obtaining the BERT embeddings and just feeding them to another NN?</li>
<li>I ask this because this is the SPARKNLP approach, a class that helps obtaim those embeddings and use it as a feature for another complex NN. Doesnt this lose some of the knowledge inside BERT?</li>
<li>Does SPARKNLP have any optimization regarding SPARK that helps in inference time or is it just another BERT implementation.</li>
</ol>
","1160393","","","","","2020-10-30 12:18:25","BERT embeddings in SPARKNLP or BERT for token classification in huggingface","<nlp><bert-language-model><huggingface-transformers><johnsnowlabs-spark-nlp>","1","0","","","","CC BY-SA 4.0"
"64370953","1","","","2020-10-15 11:49:09","","2","157","<p>I am trying to run a simple benchmark script from the <code>transformers</code> <a href=""https://huggingface.co/transformers/benchmarks.html"" rel=""nofollow noreferrer"">lib from Huggingface</a>, but it fails due to a CUDA error, which leads to another error:</p>
<pre><code>1 / 1
Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
Traceback (most recent call last):
  File &quot;/home/cbarkhof/code-thesis/Experimentation/Benchmarking/benchmarking-models-claartje.py&quot;, line 12, in &lt;module&gt;
    benchmark.run()
  File &quot;/home/cbarkhof/.local/lib/python3.6/site-packages/transformers/benchmark/benchmark_utils.py&quot;, line 674, in run
    memory, inference_summary = self.inference_memory(model_name, batch_size, sequence_length)
ValueError: too many values to unpack (expected 2)
</code></pre>
<p>The script simply follows the example like shown on <a href=""https://huggingface.co/transformers/benchmarks.html"" rel=""nofollow noreferrer"">this page</a>:</p>
<pre><code>from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments

benchmark_args = PyTorchBenchmarkArguments(models=[&quot;bert-base-uncased&quot;],
                                           batch_sizes=[8],
                                           sequence_lengths=[8, 32, 128, 512],
                                           save_to_csv=True,
                                           log_filename='log',
                                           env_info_csv_file='env_info')

benchmark = PyTorchBenchmark(benchmark_args)
benchmark.run()
</code></pre>
<p>If anyone can point me to why this might be happening. Please let me know :). Cheers!</p>
","10299995","","10299995","","2020-10-15 14:10:50","2020-12-21 15:33:38","Simple PytorchBenchmark from transformers lib of Huggingface script gives CUDA initialisation error","<python><pytorch><benchmarking><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62592468","1","62594240","","2020-06-26 10:06:41","","-1","948","<p>I train with BERT (from huggingface) sentiment analysis which is a NLP task.</p>
<p>My question refers to the learning rate.</p>
<pre><code>EPOCHS = 5                                                                                                                                                                                
optimizer = AdamW(model.parameters(), lr=1e-3, correct_bias=True)                  
total_steps = len(train_data_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(                                    
  optimizer,
  num_warmup_steps=0,                                                          
  num_training_steps=total_steps
)
loss_fn = nn.CrossEntropyLoss().to(device)
</code></pre>
<p>Can you please explain how to read 1e-3?</p>
<p>Is this the density of steps or is this a value to decay.</p>
<p>If the latter, is it a linear decay?</p>
<p>If I train with a value 3e-5, which is a recommended value of huggingface for NLP tasks, my model overfits very quickly: loss for training decreases to a minimum, loss for validation increases.</p>
<p>Learning rate 3e-5:</p>
<p><img src=""https://i.stack.imgur.com/yPkqa.png"" alt=""3e-5"" /></p>
<p>If I train with a value of 1e-2, I get a steady improvement in the loss value of validation. but the validation accuracy does not improve after the first epoch. See picture. Why does the validation value not increase, even though the loss falls. Isn't that a contradiction? I thought these two values were an interpretation of each other.</p>
<p>Learning rate 1e-2:</p>
<p><img src=""https://i.stack.imgur.com/lEqAc.png"" alt=""1e-2"" /></p>
<p>What would you recommend?</p>
","13698907","","6664872","","2020-06-26 13:58:58","2020-06-26 13:58:58","learning rate AdamW Optimizer","<deep-learning><nlp><huggingface-transformers><learning-rate>","1","0","","","","CC BY-SA 4.0"
"65625130","1","65725571","","2021-01-08 07:44:43","","1","139","<p>Let us suppose I have a model like:</p>
<pre><code>class BERT_Subject_Classifier(nn.Module):

    def __init__(self,out_classes,hidden1=128,hidden2=32,dropout_val=0.2):
      super(BERT_Subject_Classifier, self).__init__()

      self.hidden1 = hidden1
      self.hidden2 = hidden2
      self.dropout_val = dropout_val
      self.logits = logit
      self.bert = AutoModel.from_pretrained('bert-base-uncased')
      self.out_classes = out_classes
      self.unfreeze_n = unfreeze_n # make the last n layers trainable
      
      self.dropout = nn.Dropout(self.dropout_val)
      self.relu =  nn.ReLU()
      self.fc1 = nn.Linear(768,self.hidden1)
      self.fc2 = nn.Linear(self.hidden1,self.hidden2)
      self.fc3 = nn.Linear(self.hidden2,self.out_classes)

    def forward(self, sent_id, mask):
      _, cls_hs = self.bert(sent_id, attention_mask=mask)
      x = self.fc1(cls_hs)
      x = self.relu(x)
      x = self.dropout(x)
      x = self.fc2(x)
      x = self.dropout(x)
      return self.fc3(x)
</code></pre>
<p>I train my model and for a new data point <code>x = ['My Name is Slim Shady']</code>, I get my label result as  <code>3</code>.</p>
<p>My Question is that how can I check which of the words in the sentence were responsible for the the classification? I mean it could be any collection of words. Is there a library or way to check the functionality? Just like shown in the paper and <a href=""https://www.tensorflow.org/tutorials/text/image_captioning"" rel=""nofollow noreferrer"">Tensorflow Implementation</a> of <code>show Attend and Tell</code>, you can get the areas of images where the model is paying attention to. How can I do it for the Text?</p>
","11725056","","","","","2021-05-19 21:02:02","How to find the (Most important) responsible Words/ Tokens/ embeddings responsible for the label result of a text classification model in PyTorch","<python><deep-learning><pytorch><bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"64550503","1","64552678","","2020-10-27 08:20:04","","9","6570","<p>I am trying to save the tokenizer in huggingface so that I can load it later from a container where I don't need access to the internet.</p>
<pre class=""lang-py prettyprint-override""><code>BASE_MODEL = &quot;distilbert-base-multilingual-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
tokenizer.save_vocabulary(&quot;./models/tokenizer/&quot;)
tokenizer2 = AutoTokenizer.from_pretrained(&quot;./models/tokenizer/&quot;)
</code></pre>
<p>However, the last line is giving the error:</p>
<pre class=""lang-py prettyprint-override""><code>OSError: Can't load config for './models/tokenizer3/'. Make sure that:

- './models/tokenizer3/' is a correct model identifier listed on 'https://huggingface.co/models'

- or './models/tokenizer3/' is the correct path to a directory containing a config.json file
</code></pre>
<p><strong>transformers version: 3.1.0</strong></p>
<p><a href=""https://stackoverflow.com/questions/58417374/how-to-load-the-saved-tokenizer-from-pretrained-model-in-pytorch"">How to load the saved tokenizer from pretrained model in Pytorch</a> didn't help unfortunately.</p>
<h2>Edit 1</h2>
<p>Thanks to @ashwin's answer below I tried <code>save_pretrained</code> instead, and I get the following error:</p>
<pre class=""lang-py prettyprint-override""><code>OSError: Can't load config for './models/tokenizer/'. Make sure that:

- './models/tokenizer/' is a correct model identifier listed on 'https://huggingface.co/models'

- or './models/tokenizer/' is the correct path to a directory containing a config.json file
</code></pre>
<p>the contents of the tokenizer folder is below:
<a href=""https://i.stack.imgur.com/hNYVy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hNYVy.png"" alt=""enter image description here"" /></a></p>
<p>I tried renaming <code>tokenizer_config.json</code> to <code>config.json</code> and then I got the error:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError: Unrecognized model in ./models/tokenizer/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: retribert, t5, mobilebert, distilbert, albert, camembert, xlm-roberta, pegasus, marian, mbart, bart, reformer, longformer, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl, electra, encoder-decoder
</code></pre>
","2530674","","2530674","","2020-10-28 00:57:05","2021-05-16 16:13:51","Huggingface saving tokenizer","<huggingface-transformers><huggingface-tokenizers>","3","0","","","","CC BY-SA 4.0"
"64256985","1","","","2020-10-08 06:34:32","","0","79","<p>I am looking for MBART pretrained model and found that was published in hugging face. <a href=""https://huggingface.co/facebook/mbart-large-cc25#"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/mbart-large-cc25#</a>. But it seems like there is only a pytorch version. When clicking the &quot;List all files in model&quot; link, there is only a pytorch model file</p>
<p><a href=""https://i.stack.imgur.com/7PG7l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7PG7l.png"" alt=""enter image description here"" /></a></p>
<p>Also, when looking the document for MBART, there is no class like &quot;TFMBart&quot;.</p>
<p>I am wondering whether there is a tf2.0 version for this model. If not, can I convert this pytorch version to tf2.0 version?</p>
","14411860","","14411860","","2020-10-08 06:43:45","2020-10-08 06:43:45","Is there MBART tensorflow 2.0 model ready for use?","<tensorflow><pytorch><huggingface-transformers><transformer><pre-trained-model>","0","0","","","","CC BY-SA 4.0"
"64370118","1","64370802","","2020-10-15 11:00:05","","0","66","<p>How to add hugging face lib to a Kaggle notebook. I want to add <a href=""https://huggingface.co/facebook/bart-large-cnn?text=hi"" rel=""nofollow noreferrer"">this one</a> to my notebook . the code sample below does not work in the notebook I have. is there some additional step I have missed?</p>
","","user14455435","","user14455435","2020-10-15 11:11:11","2020-10-15 11:40:03","How to add hugging face lib to a Kaggle notebook","<kaggle><huggingface-transformers>","1","0","","2021-02-11 12:27:34","","CC BY-SA 4.0"
"64382393","1","","","2020-10-16 03:10:42","","0","106","<p>My question relates directly to the text summarization task. I know that there are a bunch of implementations with RNN networks (especially LSTMs) that use sentence-level attention to extract salient sentences of the source, using an attentive LSTM decoder. I have been digging into it to see if this is possible with the Transformer's networks, specifically the Transformer's decoder part, but do not really have an idea how to get this incorporated.</p>
<p>Look, for example, in terms of LSTM decoder, the LSTM encoder can produce contextualized encodings for the sentences that are in the source, then the last hidden state is passed to the LSTM decoder, and <em>at each decoding timestep</em>, the decoder attends to the source sentences (encoded by the encoder) to get an attention score over those. Finally, these scores and the sentence hidden states are combined to form the context vector, which is further processed with the decoder hidden state to predict the right sentence to pick up.</p>
<p>Assume that I have obtained the sentence encodings using Transformer's encoder, I'm just wondering how I can relate the scenario happening in LSTM network's decoder part to the Transformer's network decoder side.</p>
<p>Also, a question, how are these networks (both LSTM and Transformer's) that use sentence-level attention instead of word-level attention trained?</p>
<hr />
<p>Update: the behaviour that I intended to achieve is as follows: I want the Transformer's decoder gets in sentences (instead of tokens which is then regarded as abstractive summarization), compute attention on the source sentences considering the partial summary that has been selected in prior timesteps, and then give a probability distribution over the source sentences denoting how much it is probable that a sentence is being copied into the target. So to make it explicit, I'm looking for an extractive summarizer with decoder.</p>
","5112804","","5112804","","2020-10-16 18:30:21","2020-10-16 18:30:21","Using Transformer's decoder to extract sentences","<neural-network><pytorch><recurrent-neural-network><huggingface-transformers><transformer>","0","2","","","","CC BY-SA 4.0"
"62261602","1","","","2020-06-08 12:00:40","","6","3839","<p>I have a trained transformers NER model that I want to use on a machine not connected to the internet. When loading such a model, currently it downloads cache files to the .cache folder. </p>

<p>To load and run the model offline, you need to copy the files in the .cache folder to the offline machine. However, these files have long, non-descriptive names, which makes it really hard to identify the correct files if you have multiple models you want to use. Any thoughts on this?</p>

<p><a href=""https://i.stack.imgur.com/0CFZj.png"" rel=""noreferrer"">Example of model files</a></p>
","13706152","","","","","2021-08-25 15:20:05","Downloading transformers models to use offline","<python><nlp><pytorch><huggingface-transformers>","2","1","","","","CC BY-SA 4.0"
"63517293","1","","","2020-08-21 05:59:15","","7","7672","<pre><code>def split_data(path):
  df = pd.read_csv(path)
  return train_test_split(df , test_size=0.1, random_state=100)

train, test = split_data(DATA_DIR)
train_texts, train_labels = train['text'].to_list(), train['sentiment'].to_list() 
test_texts, test_labels = test['text'].to_list(), test['sentiment'].to_list() 

train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=100)

from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
valid_encodings = tokenizer(valid_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)
</code></pre>
<p>When I tried to split from the dataframe using BERT tokenizers I got an error us such.</p>
","8634589","","","","","2021-06-26 11:48:52","ValueError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]] - Tokenizing BERT / Distilbert Error","<tokenize><bert-language-model><huggingface-transformers><huggingface-tokenizers><distilbert>","3","0","2","","","CC BY-SA 4.0"
"64537339","1","","","2020-10-26 12:43:10","","1","186","<p>I am trying to use RobertaForCausalLM and/or BertGeneration for causal language modelling / next-word-prediction / left-to-right prediction. I can't seem to figure out where the causal masking is happening? I want to train teacher forcing with the ground-truth labels, but no information from future tokens to be included in the attention mechanism. For that I thought the model would need causal attention masking, but I don't see it being applied anywhere...</p>
<p>If anyone could point me to where this might be happening or why it is unnecessary, that would be helpful.</p>
<p>Thanks!</p>
","10299995","","6664872","","2020-10-27 12:32:36","2020-10-27 12:32:36","What makes BertGeneration and/or RobertaForCausalLM causal models? Where does the causal attention masking happen?","<pytorch><bert-language-model><huggingface-transformers><roberta-language-model>","1","0","1","","","CC BY-SA 4.0"
"64947064","1","","","2020-11-21 19:14:57","","3","1708","<p>I'm trying to use transformer's huggingface pretrained model <code>bert-base-uncased</code>, but I want to increace dropout. There isn't any mention to this in <code>from_pretrained</code> method, but colab ran the object instantiation below without any problem. I saw these dropout parameters in <code>classtransformers.BertConfig</code> documentation.</p>
<p>Am I using bert-base-uncased AND changing dropout in the correct way?</p>
<pre><code>model = BertForSequenceClassification.from_pretrained(
        pretrained_model_name_or_path='bert-base-uncased',
        num_labels=2,
        output_attentions = False,
        output_hidden_states = False,
        attention_probs_dropout_prob=0.5,
        hidden_dropout_prob=0.5
    )
</code></pre>
","11444715","","","","","2020-11-22 21:05:46","Transformers pretrained model with dropout setting","<python><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62452271","1","62452767","","2020-06-18 14:25:48","","5","2130","<p>I am trying to understand BERT vocab <a href=""https://raw.githubusercontent.com/microsoft/SDNet/master/bert_vocab_files/bert-base-uncased-vocab.txt"" rel=""noreferrer"">here</a>. It has 1000 [unusedxxx] tokens. I don't follow the usage of these tokens. I understand other special tokens like [SEP], [CLS], but what is [unused] used for? </p>

<p>Thanks!</p>
","12769533","","","","","2020-06-18 14:48:49","Understanding BERT vocab [unusedxxx] tokens:","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62476760","1","","","2020-06-19 18:48:20","","0","292","<p>I have a large file (1 GB+) with a mix of short and long texts (format: wikitext-2) for fine tuning the masked language model with <em>bert-large-uncased</em>
 as baseline model. I followed the instruction at <a href=""https://github.com/huggingface/transformers/tree/master/examples/language-modeling"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/language-modeling</a>. The process seems to be stuck at a stage ""<code>Creating features from dataset file at &lt;file loc&gt;</code>"". I am unsure what is wrong, is it really stuck or does it take really long for file of this size? </p>

<p>Command looks pretty much this:</p>

<pre><code>export TRAIN_FILE=/path/to/dataset/my.train.raw
export TEST_FILE=/path/to/dataset/my.test.raw

python run_language_modeling.py \
    --output_dir=local_output_dir \
    --model_type=bert \
    --model_name_or_path=local_bert_dir \
    --do_train \
    --train_data_file=$TRAIN_FILE \
    --do_eval \
    --eval_data_file=$TEST_FILE \
    --mlm
</code></pre>

<p>Added: The job is running on CPU</p>
","6782087","","6782087","","2020-06-19 20:10:07","2020-06-21 22:07:44","Huggingface language modeling stuck at data reading phase","<deep-learning><nlp><pytorch><huggingface-transformers>","1","5","","","","CC BY-SA 4.0"
"61844173","1","","","2020-05-16 22:01:42","","3","1127","<p>Hi I am having some serious problems saving and loading a tensorflow model which is combination of hugging face transformers + some custom layers to do classfication. I am using the latest Huggingface transformers tensorflow keras version. The idea is to extract features using distilbert and then run the features through CNN to do classification and extraction. I have got everything to work as far as getting the correct classifications.</p>

<p>The problem is in saving the model once trained and then loading the model again.</p>

<p>I am using tensorflow keras and tensorflow version 2.2</p>

<p>Following is the code to design the model, train it, evaluate it and then save and load it</p>

<pre><code>
    bert_config = DistilBertConfig(dropout=0.2, attention_dropout=0.2, output_hidden_states=False)
    bert_config.output_hidden_states = False
    transformer_model = TFDistilBertModel.from_pretrained(DISTIL_BERT, config=bert_config)

    input_ids_in = tf.keras.layers.Input(shape=(BERT_LENGTH,), name='input_token', dtype='int32')
    input_masks_in = tf.keras.layers.Input(shape=(BERT_LENGTH,), name='masked_token', dtype='int32')

    embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]
    x = tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1,
                             recurrent_dropout=0, recurrent_activation=""sigmoid"",
                             unroll=False, use_bias=True, activation=""tanh""))(embedding_layer)
    x = tf.keras.layers.GlobalMaxPool1D()(x)

    outputs = []
    # lots of code here to define the dense layers to generate the outputs
    # .....
    # .....

    model = Model(inputs=[input_ids_in, input_masks_in], outputs=outputs)
    for model_layer in model.layers[:3]:
        logger.info(f""Setting layer {model_layer.name} to not trainable"")
        model_layer.trainable = False
    rms_optimizer = RMSprop(learning_rate=0.001)
    model.compile(loss=SigmoidFocalCrossEntropy(), optimizer=rms_optimizer)

    # the code to fit the model (which works)
    # then code to evaluate the model (which also works)

    # finally saving the model. This too works.
    tf.keras.models.save_model(model, save_url, overwrite=True, include_optimizer=True, save_format=""tf"")
</code></pre>

<p>However, when I try to load the saved model using the following</p>

<pre><code>    tf.keras.models.load_model(
            path, custom_objects={""Addons&gt;SigmoidFocalCrossEntropy"": SigmoidFocalCrossEntropy})
</code></pre>

<p>I get the following load error</p>

<pre><code>
ValueError: The two structures don't have the same nested structure.

First structure: type=TensorSpec str=TensorSpec(shape=(None, 128), dtype=tf.int32, name='inputs')

Second structure: type=dict str={'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='inputs/input_ids')}

More specifically: Substructure ""type=dict str={'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='inputs/input_ids')}"" is a sequence, while substructure ""type=TensorSpec str=TensorSpec(shape=(None, 128), dtype=tf.int32, name='inputs')"" is not
Entire first structure:
.
Entire second structure:
{'input_ids': .}
</code></pre>

<p>I believe the issue is because TFDistilBertModel layer can be called using a dictionary input from DistilBertTokenizer.encode() and that happens to be the first layer. So the model compiler on load expects that to be the input signature to the call model. However, the inputs defined to the model are two tensors of shape (None, 128)</p>

<p>So how do I tell the load function or the save function to assume the correct signatures? </p>
","13207502","","","","","2020-05-18 17:08:44","Isues with saving and loading tensorflow model which uses hugging face transformer model as its first layer","<tensorflow><machine-learning><keras><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"62490113","1","","","2020-06-20 18:36:23","","4","562","<p>I'm trying to apply a pretrained HuggingFace ALBERT transformer model to my own text classification task, but the loss is not decreasing beyond a certain point.</p>
<p>Here's my code:</p>
<p><strong>There are four labels in my text classification dataset which are:</strong></p>
<pre><code>0, 1, 2, 3
</code></pre>
<p><strong>Define the tokenizer</strong></p>
<pre><code>maxlen=25
albert_path = 'albert-large-v1'
from transformers import AlbertTokenizer, TFAlbertModel, AlbertConfig
tokenizer = AlbertTokenizer.from_pretrained(albert_path, do_lower_case=True, add_special_tokens=True,
                                                max_length=maxlen, pad_to_max_length=True)
</code></pre>
<p><strong>Encode all sentences in text, using the tokenizer</strong></p>
<pre><code>encodings = []
for t in text:
  encodings.append(tokenizer.encode(t, max_length=maxlen, pad_to_max_length=True, add_special_tokens=True))
</code></pre>
<p><strong>Define the pretrained transformer model and add Dense layer on top</strong></p>
<pre><code>    from tensorflow.keras.layers import Input, Flatten, Dropout, Dense
    from tensorflow.keras import Model

    optimizer = tf.keras.optimizers.Adam(learning_rate= 1e-4)
    token_inputs = Input((maxlen), dtype=tf.int32, name='input_word_ids')
    config = AlbertConfig(num_labels=4, dropout=0.2, attention_dropout=0.2)
    albert_model = TFAlbertModel.from_pretrained(pretrained_model_name_or_path=albert_path, config=config)

    X = albert_model(token_inputs)[1] 
    X = Dropout(0.2)(X)
    output_= Dense(4, activation='softmax', name='output')(X)

    bert_model2 = Model(token_inputs,output_)
    print(bert_model2.summary())
    
    bert_model2.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')
</code></pre>
<p><strong>Finally, feed the encoded text and labels to the model</strong></p>
<pre><code>encodings = np.asarray(encodings)
labels = np.asarray(labels)
bert_model2.fit(x=encodings, y = labels, epochs=20, batch_size=128)


Epoch 11/20
5/5 [==============================] - 2s 320ms/step - loss: 1.2923
Epoch 12/20
5/5 [==============================] - 2s 319ms/step - loss: 1.2412
Epoch 13/20
5/5 [==============================] - 2s 322ms/step - loss: 1.3118
Epoch 14/20
5/5 [==============================] - 2s 319ms/step - loss: 1.2531
Epoch 15/20
5/5 [==============================] - 2s 318ms/step - loss: 1.2825
Epoch 16/20
5/5 [==============================] - 2s 322ms/step - loss: 1.2479
Epoch 17/20
5/5 [==============================] - 2s 321ms/step - loss: 1.2623
Epoch 18/20
5/5 [==============================] - 2s 319ms/step - loss: 1.2576
Epoch 19/20
5/5 [==============================] - 2s 321ms/step - loss: 1.3143
Epoch 20/20
5/5 [==============================] - 2s 319ms/step - loss: 1.2716
</code></pre>
<p>Loss has decreased from 6 to around 1.23 but doesn't seem to decrease any further, even after 30+ epochs.</p>
<p>What am I doing wrong?</p>
<p>All advice is greatly appreciated!</p>
","4834762","","","","","2020-06-30 05:42:59","ALBERT not converging - HuggingFace","<machine-learning><nlp><text-classification><transformer><huggingface-transformers>","1","3","1","","","CC BY-SA 4.0"
"64112358","1","","","2020-09-29 03:17:06","","1","464","<p>I was referring to this answer from stackoverflow but I can't get any leads regarding my problem: [https://stackoverflow.com/questions/63141267/importerror-cannot-import-name-automodelwithlmhead-from-transformers][1]</p>
<p>This is the code that I ran:</p>
<pre><code>import transformers
from transformers import AutoModelWithLMHead
</code></pre>
<p>Results:</p>
<pre><code>cannot import name 'AutoModelWithLMHead' from 'transformers' (/Users/xev/opt/anaconda3/lib/python3.7/site-packages/transformers/__init__.py)
</code></pre>
<p>My transformer version is '3.0.2'.
My import for AutoTokenizer is fine.</p>
<p>Would appreciate if there's anyone who can help regarding the Transformers package!
[1]: <a href=""https://stackoverflow.com/questions/63141267/importerror-cannot-import-name-automodelwithlmhead-from-transformers"">ImportError: cannot import name &#39;AutoModelWithLMHead&#39; from &#39;transformers&#39;</a></p>
","14132340","","","","","2021-04-04 17:50:58","Transformer: cannot import name 'AutoModelWithLMHead' from 'transformers'","<python><nlp><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"64650703","1","","","2020-11-02 17:50:45","","0","196","<p>I'm converting a BERT Tensorflow model to hugging face:</p>
<pre><code>    transformers-cli convert \
  --model_type bert \
  --tf_checkpoint C:\Users\sacl\Panasonic-AI\POC\pretraining\content\PatentBERT\model.ckpt-181172 \
  --config C:\Users\sacl\Panasonic-AI\POC\pretraining\content\PatentBERT\bert_config.json \
  --pytorch_dump_output C:\Users\sacl\Panasonic-AI\POC\pretraining\content\PatentBERT\pytorch_model.bin
</code></pre>
<p>I'm getting this error:</p>
<blockquote>
<p>torch.nn.modules.module.ModuleAttributeError: 'BertForPreTraining'
object has no attribute 'bias'</p>
</blockquote>
","7994456","","","","","2020-12-30 09:40:36","Converting Tensorflow checkpoint to hugging face","<tensorflow><bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"62462878","1","62504210","","2020-06-19 03:39:55","","1","300","<p>I am working on a text classification project using <a href=""https://huggingface.co/transformers/glossary.html#token-type-ids"" rel=""nofollow noreferrer"">Huggingface transformers module</a>. The encode_plus function provides the users with a convenient way of generating the input ids, attention masks, token type ids, etc. For instance:</p>

<pre><code>from transformers import BertTokenizer

pretrained_model_name = 'bert-base-cased'
bert_base_tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

sample_text = 'Bamboo poles, â€installation by an unknown building constructor #discoverhongkong #hongkonginsta'

encoding = bert_base_tokenizer.encode_plus(
        cleaned_tweet, hashtag_string,
        max_length=70,
        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
        return_token_type_ids=True,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt',  # Return PyTorch tensors
    )

print('*'*20)
print(encoding['input_ids'])
print(encoding['attention_mask'])
print(encoding['token_type_ids'])
print('*'*20)
</code></pre>

<p>However, my current project requires me to generate <strong>customized ids</strong> for a given text. For instance, for a list of words <code>[HK, US, UK]</code>, I want to generate ids for these words and let other words' ids which do not exist in this list as zero. These ids are used to find embedding in another customized embedding matrix, not from pretrained bert module.</p>

<p>How can I achieve this kind of customized encoder? Any suggestions and solutions are welcomed! Thanks~</p>
","8079220","","8079220","","2020-06-19 03:49:30","2021-06-15 20:42:33","Customize the encode module in huggingface bert model","<nlp><text-classification><huggingface-transformers><bert-language-model>","1","1","","","","CC BY-SA 4.0"
"62538079","1","62542474","","2020-06-23 15:12:56","","1","1702","<p>I am using Anaconda, python 3.7, windows 10.</p>
<p>I tried to install transformers by <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/</a> on my env.
I am aware that I must have either pytorch or TF installed, I have pytorch installed - as seen in anaconda navigator environments.</p>
<p>I would get many kinds of errors, depending on where (anaconda / prompt) I uninstalled and reinstalled pytorch and transformers. Last attempt using
conda install pytorch torchvision cpuonly -c pytorch and
conda install -c conda-forge transformers
I get an error:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

def tok(dataset):
    input_ids = []
    attention_masks = []
    sentences = dataset.Answer2EN.values
    labels = dataset.Class.values
    for sent in sentences:
        encoded_sent = bert_tokenizer.encode(sent, 
                                             add_special_tokens=True,
                                             max_length = 64,
                                             pad_to_max_length =True)
</code></pre>
<blockquote>
<p>TypeError: _tokenize() got an unexpected keyword argument
'pad_to_max_length'</p>
</blockquote>
<p><strong>Does anyone know a secure installation of transformers using Anaconda?</strong>
Thank you</p>
","8766532","","6664872","","2020-06-23 19:37:31","2020-06-23 19:37:31","Hugginface transformers module not recognized by anaconda","<python><python-3.x><anaconda><pytorch><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"64608515","1","","","2020-10-30 12:37:17","","0","706","<p>torch version 1.4.0
I execute run_language_modeling.py and save the model. However, when I load the saved model, &quot;OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a Pytorch model from a TF 2.0 checkpoint, please set from_tf=True&quot; occurs.</p>
<p>If I install torch==1.6.0, it is successful to load model. However, I have to use torch version 1.4.0 and torchvision 0.5.0. How can I load pytorch_model.bin in torch 1.4.0???</p>
<p>++ I tried to train run_language_modeling.py in torch version 1.4.0, but  it cannot import &quot;torch.optim.lr_scheduler&quot; thus the train code cannot be executed.</p>
<p>Thus my question is
[1] How can I load pytorch_model.bin in torch version 1.4.0   / or
[2] How can I train run_language_modeling.py in torch version 1.4.0?</p>
","14272809","","","","","2020-10-30 12:37:17","Transformers - How to load pytorch_model.bin in torch version 1.4.0 (Albert)","<deep-learning><bert-language-model><huggingface-transformers><language-model>","0","0","","","","CC BY-SA 4.0"
"65627663","1","65752411","","2021-01-08 10:52:36","","1","444","<p>I'm using transformers TFBertModel to classify a bunch of input strings, however I'd like to access the CLS embedding in order to be able to rebalance my data.</p>
<p>When I pass a single element of my data to the predict method of my simplified bert model (in order to get the CLS data), I take the first array of the <code>last_hidden_state</code>, and voila. However, when I pass in more than one row of data, the shape of the output changes as expected, but it seems the actual CLS embedding (of the first row that I first passed in) changes too.</p>
<p>My dataset contains the input ids and the masks, and the model:</p>
<pre><code>from transformers import TFBertModel

model = TFBertModel.from_pretrained('bert-base-multilingual-cased', trainable=False, num_labels=len(le.classes_))

input_ids_layer = Input(shape=(256,), dtype=np.int32)
input_mask_layer = Input(shape=(256,), dtype=np.int32)

bert_layer = model([input_ids_layer, input_mask_layer])

model = Model(inputs=[input_ids_layer, input_mask_layer], outputs=bert_layer)
</code></pre>
<p>Then, to get the CLS embeddings I just call the predict method and dig into the result. So for the first row of data (data_x[0] being the input ids, and data_x[1] being the masks)</p>
<p><code>output1 = model.predict([data_x[0][0], data_x[1][0]])</code></p>
<pre><code>TFBaseModelOutputWithPooling([('last_hidden_state',
                               array([[[ 0.35013607, -0.5340336 ,  0.28577858, ..., -0.03405955,
                                        -0.0165604 , -0.36481357]],
                               
                                      [[ 0.34572566, -0.5361709 ,  0.281771  , ..., -0.03687727,
                                        -0.01690093, -0.35451806]],
                               
                                      [[ 0.34878412, -0.5399749 ,  0.28948805, ..., -0.03613809,
                                        -0.01503076, -0.35425758]],
                               
                                      ...,
</code></pre>
<p>My understanding is that the CLS representation of the sentence is the first array of the last_hidden_state i.e:</p>
<pre><code>lhs1 = output1[0]

lhs1.shape
&gt;&gt; (256, 1, 768)

cls1 = lhs1[0][0]

cls1
&gt;&gt;[0.35013607 ... -0.36481357]` (as above)
</code></pre>
<p>So far so good. My confusion arises when I now want to obtain the first 2 of the CLS embeddings from my dataset:</p>
<pre><code>output_both = model.predict([data_x[0][:2], data_x[1][:2]])
lhs_both = output_both[0] # last hidden states

lhs_both.shape
&gt;&gt; (2, 256, 768)

cls_both = lhs_both[0][0] # I thought this would give me two CLS arrays including the first one above

</code></pre>
<p>Inspecting <code>cls_both</code>:</p>
<pre><code>array([[[ 0.11075249, -0.02257648, -0.40831113, ...,  0.18384863,
          0.17032738, -0.05989586],
        [-0.22926208, -0.5627498 ,  0.2617012 , ...,  0.20701236,
          0.3141808 , -0.8650396 ],
        [-0.22352833, -0.49676323, -0.5286081 , ...,  0.23819353,
          0.3742358 , -0.69018203],
        ...,
        [ 0.5120927 , -0.09863365,  0.7378716 , ..., -0.19551781,
          0.45915398,  0.22804889],
        [-0.13397002,  0.1617202 ,  0.15663634, ..., -0.511597  ,
          0.3959382 ,  0.30565232],
        [-0.14100523,  0.22792323, -0.15898004, ..., -0.2690729 ,
          0.4730471 ,  0.18431285]],

       [[-0.20033133, -0.08412935, -0.0411438 , ...,  0.34706163,
          0.1919156 , -0.08740871],
        [-0.12536147, -0.44519228,  1.2984221 , ...,  0.07149828,
          0.7915938 ,  0.08048639],
        [ 0.4596323 , -0.3316555 ,  1.2545322 , ..., -0.02128018,
          0.5344383 ,  0.32054782],
        ...,
        [-0.54777217,  0.23129587,  0.5007771 , ...,  0.70299244,
          0.27277255, -0.2848366 ],
        [-0.49410668,  0.37352908,  0.8732239 , ...,  0.6065303 ,
          0.152081  , -0.9312557 ],
        [-0.33172935, -0.35368383,  0.5942321 , ...,  0.7171531 ,
          0.24436645,  0.08909844]]], dtype=float32)
</code></pre>
<p>I'm not sure how to interpret this - my expectation was to see the first rows CLS <code>cls1</code> contained within <code>cls_both</code>, but as you can see, the first row in the first sub array is different. Can anyone explain this?</p>
<p>Furthermore, if I run <strong>only</strong> the second row through, I get exactly the same CLS token as the first, despite them containing totally different input_ids/masks:</p>
<pre><code>output2 = model.predict([data_x[0][1], data_x[1][1]])
lhs2 = output2[0]
cls2 = lhs2[0][0]


cls2
&gt;&gt;
[ 0.35013607, -0.5340336 ,  0.28577858, ..., -0.03405955,
         -0.0165604 , -0.36481357]]

cls1 == cl2 
&gt;&gt; True
</code></pre>
<h3>Edit</h3>
<p><a href=""https://stackoverflow.com/questions/59330597/bert-sentence-embeddings-how-to-obtain-sentence-embeddings-vector"">BERT sentence embeddings: how to obtain sentence embeddings vector</a></p>
<p>Above post explains that <code>output[0][:,0,:]</code> is the correct way to obtain exactly the CLS tokens which makes things easiers.</p>
<p>When I run three rows through, I get consistent results, but any time I run a single row through, I get the result shown in <code>cls1</code> - why does this not differ each time?</p>
","7605543","","7605543","","2021-01-08 11:23:14","2021-01-16 17:18:38","BERT - Extracting CLS embedding from multiple outputs vs single","<python><pandas><tensorflow><keras><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"64542535","1","64544995","","2020-10-26 18:00:18","","0","51","<p>I'm looking for suitable datasets to test a few new machine learning ideas. Is there any way to see summary statistics (e.g. number of samples; type of data) of HuggingFace datasets?</p>
<p>They provide descriptions here <a href=""https://huggingface.co/datasets"" rel=""nofollow noreferrer"">https://huggingface.co/datasets</a> , but it's a bit hard to filter them.</p>
","2950707","","","","","2020-10-26 21:08:58","How can I see summary statistics (e.g. number of samples; type of data) of HuggingFace datasets?","<tensorflow><pytorch><huggingface-transformers>","1","0","","2020-10-27 16:32:22","","CC BY-SA 4.0"
"62446827","1","","","2020-06-18 09:31:06","","7","1028","<p>I would like to train a encoder decoder model as configured below for a translation task. Could someone guide me as to how I can set-up a training pipeline for such a model? Any links or code snippets would be appreciated to understand.</p>

<pre><code>from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel

# Initializing a BERT bert-base-uncased style configuration
config_encoder = BertConfig()
config_decoder = BertConfig()

config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

# Initializing a Bert2Bert model from the bert-base-uncased style configurations
model = EncoderDecoderModel(config=config)
</code></pre>
","2804512","","","","","2021-04-19 08:03:19","How do I train a encoder-decoder model for a translation task using hugging face transformers?","<huggingface-transformers><machine-translation><encoder-decoder>","1","1","","","","CC BY-SA 4.0"
"62547771","1","","","2020-06-24 04:41:08","","0","1466","<p>Reference github: <a href=""https://github.com/kaushaltrivedi/fast-bert"" rel=""nofollow noreferrer"">fast-bert</a></p>
<p>I used to run the following notebook to predict the multi label classification using Bert model that means I don't need GPU driver instead I can use CPU memory,
here is the reference for jupyter notebook <a href=""https://github.com/kaushaltrivedi/fast-bert/blob/master/sample_notebooks/new-toxic-multilabel.ipynb"" rel=""nofollow noreferrer"">multilabel</a></p>
<p>It's not the memory issue, How to resolve this error?</p>
<p>While increasing CPU with RAM size, target size also getting increasing,</p>
<blockquote>
<p>I choose n1-standard-4 (6 vCPUs, 26 GB memory) machine type.</p>
</blockquote>
<p><strong>Sample code:</strong></p>
<p>I have removed this peace of code I use '<strong>cpu</strong>' instead of '<strong>cuda</strong>'</p>
<pre><code>device = torch.device('cuda')
if torch.cuda.device_count() &gt; 1:
    args.multi_gpu = True
else:
    args.multi_gpu = False
</code></pre>
<p>to</p>
<pre><code>torch.device('cpu')
</code></pre>
<p><strong>Error Logs:</strong></p>
<pre><code>Traceback (most recent call last):-------------------------------------------------------------| 0.00% [0/63 00:00&lt;00:00]
  File &quot;bert/run.py&quot;, line 146, in &lt;module&gt;
    learner.fit(args.num_train_epochs, args.learning_rate, validate=True)
  File &quot;/home/pt4_gcp/.local/lib/python3.7/site-packages/fast_bert/learner_cls.py&quot;, line 397, in fit
    outputs = self.model(**inputs)
  File &quot;/home/pt4_gcp/.local/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/home/pt4_gcp/.local/lib/python3.7/site-packages/fast_bert/modeling.py&quot;, line 205, in forward
    logits.view(-1, self.num_labels), labels.view(-1, self.num_labels)
  File &quot;/home/pt4_gcp/.local/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/home/pt4_gcp/.local/lib/python3.7/site-packages/torch/nn/modules/loss.py&quot;, line 617, in forward
    reduction=self.reduction)
  File &quot;/home/pt4_gcp/.local/lib/python3.7/site-packages/torch/nn/functional.py&quot;, line 2433, in binary_cross_entropy_with_logits
    raise ValueError(&quot;Target size ({}) must be the same as input size ({})&quot;.format(target.size(), input.size()))
ValueError: Target size (torch.Size([64, 3])) must be the same as input size (torch.Size([32, 3]))
</code></pre>
","1872999","","1872999","","2020-06-25 12:32:35","2020-06-26 16:26:10","ValueError: Target size (torch.Size([64, 3])) must be the same as input size (torch.Size([32, 3]))","<google-cloud-platform><nlp><pytorch><huggingface-transformers><bert-language-model>","1","1","","","","CC BY-SA 4.0"
"62487267","1","62504947","","2020-06-20 14:42:44","","-1","279","<p>I would like to use a language model such as Bert to get a feature vector for a certain text describing a medical condition.</p>
<p>As there are many words in the text unknown to most pre-trained models and tokenizers, I wonder which steps are required to achieve this task?</p>
<p>Using a pre-trained model seems beneficial to me since the dataset describing the medical conditions is quite small.</p>
","5511236","","","","","2021-04-25 21:08:36","Fine Tuning Bert on Medical Dataset","<python><nlp><huggingface-transformers>","2","1","","","","CC BY-SA 4.0"
"62502719","1","","","2020-06-21 18:20:17","","3","1015","<p>When I use (with a long test_text and short question):</p>
<pre><code>from transformers import BertTokenizer
import torch
from transformers import BertForQuestionAnswering

tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

input_ids = tokenizer.encode(question, test_text)

print('Query has {:,} tokens.\n'.format(len(input_ids)))

sep_index = input_ids.index(tokenizer.sep_token_id)

num_seg_a = sep_index + 1

num_seg_b = len(input_ids) - num_seg_a

segment_ids = [0]*num_seg_a + [1]*num_seg_b

start_scores, end_scores = model(torch.tensor([input_ids]),
                                token_type_ids=torch.tensor([segment_ids]))
</code></pre>
<p>I get an error with the output</p>
<p>Token indices sequence length is longer than the specified maximum sequence length for this model (3 &gt; 512). Running this sequence through the model will result in indexing errors</p>
<p>Query has 1,244 tokens.</p>
<p>How can I separate test_text into maximized length of chunks knowing that it won't exceed 512 tokens? And then ask the same question for each chunk of text, taking the best answer out of all of them, also going through the text twice with different slice points, in case the answer is cut during a slice.</p>
","","user12975267","","","","2020-06-22 03:46:57","How to slice string depending on length of tokens","<python><python-3.x><tokenize><huggingface-transformers><bert-language-model>","0","12","","","","CC BY-SA 4.0"
"62554953","1","","","2020-06-24 12:23:45","","2","177","<p>I wanted to apply Bert on a sequence of sentences in the following manner, but I am getting a NotImplementedError</p>
<p>How to reproduce :</p>
<pre><code>import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
inputs = tf.keras.Input(shape=(50, 64), dtype='int32')
model = TFBertModel.from_pretrained('bert-base-uncased')
outputs = tf.keras.layers.TimeDistributed(model)(inputs)

NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-5-631f3cd2e8b2&gt; in &lt;module&gt;
----&gt; 1 outputs = tf.keras.layers.TimeDistributed(model)(inputs)
</code></pre>
<p>Whereas the code would work fine for</p>
<pre><code>inputs = tf.keras.Input(shape=(10, 128, 128, 3))
conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))
outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)
</code></pre>
<p>Is there anything I am missing here?</p>
","6906156","","","","","2020-06-24 12:23:45","keras.layers.TimeDistributed with Huggingface Transformer gives NotImplementedError","<tensorflow><keras><tensorflow2.0><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"62728985","1","","","2020-07-04 12:16:06","","2","3384","<p>I want to translate from Chinese to English using HuggingFace's transformers using a pretrained <code>&quot;xlm-mlm-xnli15-1024&quot;</code> model. <a href=""https://huggingface.co/transformers/usage.html#translation"" rel=""nofollow noreferrer"">This tutorial</a> shows how to do it from English to German.</p>
<p>I tried following the tutorial but it doesn't detail how to manually change the language or to decode the result. I am lost on where to start. Sorry that this question could not be more specific.</p>
<p>Here is what I tried:</p>
<pre><code>from transformers import AutoModelWithLMHead, AutoTokenizer
base_model = &quot;xlm-mlm-xnli15-1024&quot;
model = AutoModelWithLMHead.from_pretrained(base_model)
tokenizer = AutoTokenizer.from_pretrained(base_model)

inputs = tokenizer.encode(&quot;translate English to Chinese: Hugging Face is a technology company based in New York and Paris&quot;, return_tensors=&quot;pt&quot;)
outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)

print(tokenizer.decode(outputs.tolist()[0]))
</code></pre>
<pre><code>'&lt;s&gt;translate english to chinese : hugging face is a technology company based in new york and paris &lt;/s&gt;china hug â„¢ â„¢ â„¢ â„¢ â„¢ â„¢ â„¢ â„¢ â„¢ â„¢ â„¢ â„¢ â„¢ â„¢ â„¢ â„¢ â„¢'
</code></pre>
","12360937","","","","","2021-04-25 00:14:04","How do I translate using HuggingFace from Chinese to English?","<nlp><translation><huggingface-transformers><machine-translation><huggingface-tokenizers>","2","0","","","","CC BY-SA 4.0"
"62691279","1","62703850","","2020-07-02 07:35:42","","14","7579","<p>I use pytorch to train huggingface-transformers model, but every epoch, always output the warning:</p>
<pre><code>The current process just got forked. Disabling parallelism to avoid deadlocks... To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
</code></pre>
<p>How to disable this warning?</p>
","4361407","","6664872","","2020-07-02 16:04:59","2021-04-25 15:09:41","How to disable TOKENIZERS_PARALLELISM=(true | false) warning?","<pytorch><huggingface-transformers><huggingface-tokenizers>","3","0","3","","","CC BY-SA 4.0"
"62691434","1","","","2020-07-02 07:44:54","","0","321","<p>I want fine tune on squad with huggingface run_squad.py, but meet the following question:</p>
<p>1, when I use &quot;--do_train&quot; without &quot;True&quot; as following code, after 20 minutes runing,there is no models in output_dir:</p>
<pre><code>!python run_squad.py  \
    --model_type bert   \
    --model_name_or_path bert-base-uncased  \
    --output_dir models/bert/ \
    --data_dir data/squad   \
    --overwrite_output_dir \
    --overwrite_cache \
    --do_train  \
    --train_file train-v2.0.json   \
    --version_2_with_negative \
    --do_lower_case  \
    --do_eval   \
    --predict_file dev-v2.0.json   \
    --per_gpu_train_batch_size 2   \
    --learning_rate 3e-5   \
    --num_train_epochs 2.0   \
    --max_seq_length 384   \
    --doc_stride 128   \
    --threads 10   \
    --save_steps 5000 
</code></pre>
<p>2, when I use &quot;--do_train=True&quot; as following code, the error message is &quot;run_squad.py: error: argument --do_train: ignored explicit argument 'True'&quot;:</p>
<pre><code>!python run_squad.py  \
    --model_type bert   \
    --model_name_or_path bert-base-uncased  \
    --output_dir models/bert/ \
    --data_dir data/squad   \
    --overwrite_output_dir \
    --overwrite_cache \
    --do_train=True  \
    --train_file train-v2.0.json   \
    --version_2_with_negative \
    --do_lower_case  \
    --do_eval   \
    --predict_file dev-v2.0.json   \
    --per_gpu_train_batch_size 2   \
    --learning_rate 3e-5   \
    --num_train_epochs 2.0   \
    --max_seq_length 384   \
    --doc_stride 128   \
    --threads 10   \
    --save_steps 5000 
</code></pre>
<p>3, when I use &quot;--do_train True&quot; as following code, the error message is &quot;run_squad.py: error: unrecognized arguments: True&quot;:</p>
<pre><code>!python run_squad.py  \
    --model_type bert   \
    --model_name_or_path bert-base-uncased  \
    --output_dir models/bert/ \
    --data_dir data/squad   \
    --overwrite_output_dir \
    --overwrite_cache \
    --do_train True  \
    --train_file train-v2.0.json   \
    --version_2_with_negative \
    --do_lower_case  \
    --do_eval   \
    --predict_file dev-v2.0.json   \
    --per_gpu_train_batch_size 2   \
    --learning_rate 3e-5   \
    --num_train_epochs 2.0   \
    --max_seq_length 384   \
    --doc_stride 128   \
    --threads 10   \
    --save_steps 5000 
</code></pre>
<p>I run code in colab with GPU: Tesla P100-PCIE-16GB</p>
<p>Judging by the running time, I think the code didn't through training process, but I don't know how to set parameters in order to let training go.what should I do?</p>
","7241796","","","","","2020-07-02 07:44:54","why run ""python run_squad.py"" doesn't work?","<nlp><google-colaboratory><huggingface-transformers>","0","5","","","","CC BY-SA 4.0"
"62797376","1","","","2020-07-08 14:41:56","","1","2025","<p>I'm doing token-based classification using the pre-trained BERT-model for tensorflow to automatically label cause and effects in sentences.</p>
<p>To access BERT, I'm using the TFBertForTokenClassification-Interface from huggingface: <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertfortokenclassification"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html#tfbertfortokenclassification</a></p>
<p>The sentences I use to train are all converted to tokens (basically a mapping of words to numbers) according to the BERT-tokenizer and then padded to a certain length before training, so when one sentence has only 50 tokens and another one has only 30 the first one is filled up with 50 pad-tokens and the second one with 70 of them to get a universal input sentence-length of 100.</p>
<p>I then train my model to predict on every token which label this token belongs to; whether it is part of the cause, the effect or none of them.</p>
<p>However, during training and evaluation, my model does predictions on the PAD-tokens as well and they are also included in the accuracy of the model. As PAD-tokens are very easy to predict for the model (they always have the same token and they all have the &quot;none&quot; label which means they neither belong to the cause nor the effect of the sentence), they really distort my model's accuracy.</p>
<p>For example, if you have a sentence which has 30 words -&gt; 30 tokens and you pad all sentences to a  length of 100, then this sentence would get a score of 70% even if the model predicted none of the &quot;real&quot; tokens correctly.
This way i'm getting training and validation accuracy of 90+% really quick although the model performs poorly on the real pad-tokens.</p>
<p>I thought that attention-mask is there to solve this problem but this doesn't seem to be the case.</p>
<p>The input-datasets are created as follows:</p>
<pre><code>def example_to_features(input_ids,attention_masks,token_type_ids,label_ids):
  return {&quot;input_ids&quot;: input_ids,
          &quot;attention_mask&quot;: attention_masks},label_ids

train_ds = tf.data.Dataset.from_tensor_slices((input_ids_train,attention_masks_train,token_ids_train,label_ids_train)).map(example_to_features).shuffle(buffer_size=1000).batch(32)
</code></pre>
<p>Model creation:</p>
<pre><code>from transformers import TFBertForTokenClassification

num_epochs = 30

model = TFBertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=3)

model.layers[-1].activation = tf.keras.activations.softmax

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

model.summary()
</code></pre>
<p>And then I train it like this:</p>
<pre><code>history = model.fit(train_ds, epochs=num_epochs, validation_data=validate_ds)
</code></pre>
<p>Has anyone encountered this problem so far or does know how to exclude the predictions on pad-tokens from the model's accuracy during training and evaluation?</p>
","3228384","","","","","2020-07-13 14:30:20","Tensorflow BERT for token-classification - exclude pad-tokens from accuracy while training and testing","<python><tensorflow><named-entity-recognition><huggingface-transformers><bert-language-model>","1","0","1","","","CC BY-SA 4.0"
"64383443","1","64386115","","2020-10-16 05:37:07","","0","235","<p>I try to use hugging face transformers api.
As I import library , I have some questions. If anyone who know the answer, please tell me your knowledge.</p>
<p>transformers library have several models that are trained. transformers provide not only bare model like 'BertModel, RobertaModel, ... but also convenient heads like 'ModelForMultipleChoice' , 'ModelForSequenceClassification', 'ModelForTokenClassification' , ModelForQuestionAnswering.</p>
<p>I wonder what's difference between bare model adding new linear transformation myself and modelforsequenceclassification.
what's different custom model (pretrained model with random intialized linear) and transformers modelforsequenceclassification.</p>
<p>is ModelforSequenceClassification trained from glue data?
I look forward to someone's reply Thanks.</p>
","11054795","","","","","2021-09-01 01:00:28","What's difference RobertaModel, RobertaSequenceClassification (hugging face)","<huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"64774966","1","","","2020-11-10 18:50:11","","0","129","<p>I'm trying to implement a token classifier, but I want to use rich text metadata that I've got, in addition to the text itself.</p>
<p>The model I want to fit is the following:</p>
<pre><code>label = dot_product([hidden_state, metadata], W)
</code></pre>
<p>where the hidden state is the output of <code>TFDistilBertMainLayer</code> (documented <a href=""https://github.com/huggingface/transformers/blob/969859d5f67c7106de4d1098c4891c9b03694bbe/src/transformers/modeling_tf_distilbert.py"" rel=""nofollow noreferrer"">here</a>)</p>
<p>I'm trying to tweak huggingface's token classifier tutorial, which is <a href=""https://huggingface.co/transformers/custom_datasets.html#tok-ner"" rel=""nofollow noreferrer"">here</a></p>
<p>Here is my start at modifying the <code>TFDistilBertForTokenClassification</code> class for my purposes:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import Model, Input
from tensorflow.keras.layers import (concatenate, Dense, Dropout)
from transformers.modeling_tf_outputs import (
    TFSequenceClassifierOutput,
)
from transformers.modeling_tf_utils import (
    TFTokenClassificationLoss,
    get_initializer,
)
from transformers.tokenization_utils import BatchEncoding

from transformers import TFDistilBertPreTrainedModel, TFDistilBertMainLayer

unique_tags = list(range(5))

class my_cool_model(TFDistilBertPreTrainedModel, TFTokenClassificationLoss):
    def __init__(self, config, *inputs, **kwargs):
        super().__init__(config, *inputs, **kwargs)
        self.num_labels = config.num_labels
        self.distilbert = TFDistilBertMainLayer(config, name=&quot;distilbert&quot;)
        inp_p = Input(shape=3)
        self.conc = concatenate([self.distilbert, inp_p])
        self.dropout = tf.keras.layers.Dropout(config.dropout)
        self.classifier = tf.keras.layers.Dense(
            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=&quot;classifier&quot;
        )


    def call(
        self,
        inputs=None,
        attention_mask=None,
        head_mask=None,
        inputs_embeds=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        labels=None,
        training=False,
    ):
        r&quot;&quot;&quot;
        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -
            1]``.
        &quot;&quot;&quot;
        return_dict = return_dict if return_dict is not None else self.distilbert.return_dict
        if isinstance(inputs, (tuple, list)):
            labels = inputs[7] if len(inputs) &gt; 7 else labels
            if len(inputs) &gt; 7:
                inputs = inputs[:7]
        elif isinstance(inputs, (dict, BatchEncoding)):
            labels = inputs.pop(&quot;labels&quot;, labels)

        outputs = self.distilbert(
            inputs,
            attention_mask=attention_mask,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            training=training,
        )

        sequence_output = outputs[0]

        sequence_output = self.dropout(sequence_output, training=training)
        logits = self.classifier(sequence_output)

        loss = None if labels is None else self.compute_loss(labels, logits)

        if not return_dict:
            output = (logits,) + outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return TFTokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

model = my_cool_model.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))
model.summary()
  
</code></pre>
<p>It fails when it gets to the <code>concatenate</code> step, with the following error:</p>
<pre><code>TypeError: 'NoneType' object is not subscriptable
</code></pre>
<p>I think that the problem is that the <code>distilbert</code> object doesn't get treated like a regular tf/keras layer:</p>
<pre><code>self.distilbert
&lt;transformers.modeling_tf_distilbert.TFDistilBertMainLayer object at 0x7feabd9b8d50&gt;
</code></pre>
<p>So, how would I go about building a token classifier that also has metadata here?</p>
","1883795","","","","","2020-11-10 18:50:11","Modifying TFDistilBertForTokenClassification to enable appending metadata as a parallel input stream","<python><tensorflow><deep-learning><nlp><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"62472238","1","","","2020-06-19 14:17:45","","4","5215","<p>I am new to PyTorch and recently, I have been trying to work with Transformers. I am using pretrained tokenizers provided by HuggingFace.
<br> I am successful in downloading and running them. But if I try to save them and load again, then some error occurs. <br> If I use  <code>AutoTokenizer.from_pretrained</code> to download a tokenizer, then it works. </p>

<pre><code>[1]:    tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')
        text = ""Hello there""
        enc = tokenizer.encode_plus(text)
        enc.keys()

Out[1]: dict_keys(['input_ids', 'attention_mask'])
</code></pre>

<p>But if I save it using <code>tokenizer.save_pretrained(""distilroberta-tokenizer"")</code> and try to load it locally, then it fails.</p>

<pre><code>[2]:    tmp = AutoTokenizer.from_pretrained('distilroberta-tokenizer')


---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    238                 resume_download=resume_download,
--&gt; 239                 local_files_only=local_files_only,
    240             )

/opt/conda/lib/python3.7/site-packages/transformers/file_utils.py in cached_path(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)
    266         # File, but it doesn't exist.
--&gt; 267         raise EnvironmentError(""file {} not found"".format(url_or_filename))
    268     else:

OSError: file distilroberta-tokenizer/config.json not found

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
&lt;ipython-input-25-3bd2f7a79271&gt; in &lt;module&gt;
----&gt; 1 tmp = AutoTokenizer.from_pretrained(""distilroberta-tokenizer"")

/opt/conda/lib/python3.7/site-packages/transformers/tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    193         config = kwargs.pop(""config"", None)
    194         if not isinstance(config, PretrainedConfig):
--&gt; 195             config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
    196 
    197         if ""bert-base-japanese"" in pretrained_model_name_or_path:

/opt/conda/lib/python3.7/site-packages/transformers/configuration_auto.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    194 
    195         """"""
--&gt; 196         config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
    197 
    198         if ""model_type"" in config_dict:

/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    250                 f""- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\n\n""
    251             )
--&gt; 252             raise EnvironmentError(msg)
    253 
    254         except json.JSONDecodeError:

OSError: Can't load config for 'distilroberta-tokenizer'. Make sure that:

- 'distilroberta-tokenizer' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'distilroberta-tokenizer' is the correct path to a directory containing a config.json file

</code></pre>

<p>Its saying 'config.josn' is missing form the directory. On checking the directory, I am getting list of these files:</p>

<pre><code>[3]:    !ls distilroberta-tokenizer

Out[3]: merges.txt  special_tokens_map.json  tokenizer_config.json  vocab.json
</code></pre>

<p>I know this problem has been posted earlier but none of them seems to work. I have also tried to follow the <a href=""https://huggingface.co/transformers/model_doc/auto.html#autotokenizer"" rel=""nofollow noreferrer"">docs</a> but still can't make it work.
<br>Any help would be appreciated.</p>
","12175624","","","","","2020-07-04 01:33:58","AutoTokenizer.from_pretrained fails to load locally saved pretrained tokenizer (PyTorch)","<python><deep-learning><pytorch><huggingface-transformers><huggingface-tokenizers>","2","0","","","","CC BY-SA 4.0"
"62472438","1","62544040","","2020-06-19 14:28:32","","2","663","<p>I'm going off of <a href=""https://github.com/cortexlabs/cortex/blob/master/examples/pytorch/text-generator/predictor.py"" rel=""nofollow noreferrer"">https://github.com/cortexlabs/cortex/blob/master/examples/pytorch/text-generator/predictor.py</a></p>

<p>But if I pass <code>num_samples=5</code>, I get:</p>

<pre><code>    generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)
RuntimeError: Sizes of tensors must match except in dimension 1. Got 5 and 1 in dimension 0
</code></pre>

<p>the code is:</p>

<pre><code>def sample_sequence(
    model,
    length,
    context,
    num_samples=1,
    temperature=1,
    top_k=0,
    top_p=0.9,
    repetition_penalty=1.0,
    device=""cpu"",
):
    context = torch.tensor(context, dtype=torch.long, device=device)
    context = context.unsqueeze(0).repeat(num_samples, 1)
    print('context_size', context.shape)
    generated = context
    print('context', context)
    with torch.no_grad():
        for _ in trange(length):
            inputs = {""input_ids"": generated}
            outputs = model(
                **inputs
            )  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)
            next_token_logits = outputs[0][0, -1, :] / (temperature if temperature &gt; 0 else 1.0)

            # reptition penalty from CTRL (https://arxiv.org/abs/1909.05858)
            for _ in set(generated.view(-1).tolist()):
                next_token_logits[_] /= repetition_penalty

            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)
            if temperature == 0:  # greedy sampling:
                next_token = torch.argmax(filtered_logits).unsqueeze(0)
            else:
                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)
            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)
    return generated
</code></pre>
","239879","","","","","2020-06-24 08:07:50","With the HuggingFace transformer, how can I return multiple samples when generating text?","<python><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63020991","1","63050666","","2020-07-21 18:35:48","","1","1197","<p>I am getting this error when trying following code in the jupyter-lab:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
</code></pre>
<p>Amazingly, if I copy that line of code in a <code>code_test.py</code> file, and execute it using <code>python3 code_test.py</code>(both in the terminal and jupyter-lab itself) everything will work fine.</p>
<p>I am using jupyter-lab and which is configured to use a virtual-env(the one containing transformers module).</p>
<p>I have searched for similar problems, but none of proposed solutions worked(such as reinstalling the <strong>transformers</strong> module).</p>
<p><strong>Edited</strong>:</p>
<p>Output of <code>sys.path</code> in jupyter-lab:</p>
<pre class=""lang-py prettyprint-override""><code>['/Users/{my_username}/{path_to_my_project}/code',
 '/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python37.zip',
 '/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7',
 '/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload',
 '',
 '/Users/{my_username}/Library/Python/3.7/lib/python/site-packages',
 '/usr/local/lib/python3.7/site-packages',
 '/Users/{my_username}/Library/Python/3.7/lib/python/site-packages/IPython/extensions',
 '/Users/{{my_username}/.ipython']
</code></pre>
<p>Output of sys.path in <code>code_test.py</code>:</p>
<pre><code>['/Users/{my_username}/{path_to_my_project}/code',
'/Library/Frameworks/Python.framework/Versions/3.7/lib/python37.zip',
'/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7',
'/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload',
'/Users/{my_username}/{path_to_my_project}/code/env/lib/python3.7/site-packages']
</code></pre>
","11784913","","11784913","","2020-07-22 11:44:28","2020-07-23 09:04:13","cannot import name 'pipline' from 'transformers' (unknown location)","<python><jupyter-lab><huggingface-transformers>","1","6","1","","","CC BY-SA 4.0"
"63026701","1","","","2020-07-22 04:07:48","","0","92","<p>I would like to encode articles using Bert. My code runs fine on CPU, but failed on GPU. The GPU is a on a remote server.</p>
<p>My strategy is to tokenize the articles using CPU first. Then move both model and tokens to GPU. I am not sure if this is a good approach. Any suggestion of improvements are welcomed. Thank you in advance.</p>
<pre><code>def assign_gpu(token):
    token_tensor = token['input_ids'].to('cuda')
    token_typeid = token['token_type_ids'].to('cuda')
    attention_mask = token['attention_mask'].to('cuda')
    output = {'input_ids': token_tensor,
              'token_type_ids': token_typeid,
              'attention_mask': attention_mask}
    return output

bs = 16
data_dl = DataLoader(PatentDataset(df), batch_size=bs, shuffle=False)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased/')
model = BertModel.from_pretrained('bert-base-uncased/')
inputs_list = []
for a in data_dl:
     inputs_list.append(tokenizer.batch_encode_plus(a, pad_to_max_length=True, return_tensors='pt'))

# GPU part 
model.cuda()
model.eval()
out_list = []
with torch.no_grad():
    for i, inputs in enumerate(inputs_list):
        inputs = assign_gpu(inputs)
        output = model(**inputs)[0][:, 0, :]
        out_list.append(output)
</code></pre>
<p>The error message is</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-3-f6979d077f60&gt; in &lt;module&gt;
      6     for i, inputs in enumerate(inputs_list):
      7         inputs = assign_gpu(inputs)
----&gt; 8         output = model(**inputs)[0][:, 0, :]
      9         out_list.append(output)

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    530             result = self._slow_forward(*input, **kwargs)
    531         else:
--&gt; 532             result = self.forward(*input, **kwargs)
    533         for hook in self._forward_hooks.values():
    534             hook_result = hook(self, input, result)

~/volta_pypkg/lib/python3.6/site-packages/transformers/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)
    751 
    752         embedding_output = self.embeddings(
--&gt; 753             input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
    754         )
    755         encoder_outputs = self.encoder(

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    530             result = self._slow_forward(*input, **kwargs)
    531         else:
--&gt; 532             result = self.forward(*input, **kwargs)
    533         for hook in self._forward_hooks.values():
    534             hook_result = hook(self, input, result)

~/volta_pypkg/lib/python3.6/site-packages/transformers/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds)
    180         token_type_embeddings = self.token_type_embeddings(token_type_ids)
    181 
--&gt; 182         embeddings = inputs_embeds + position_embeddings + token_type_embeddings
    183         embeddings = self.LayerNorm(embeddings)
    184         embeddings = self.dropout(embeddings)

RuntimeError: CUDA error: device-side assert triggered
</code></pre>
<p>Another issue is that if I rerun the GPU for loop on a new cell in the interactive Jupyter notebook, it reports error on the assign_gpu part. I am not sure if it's caused by my remote server or some error in my code.</p>
<pre><code>inputs = assign_gpu(inputs_list[0])
output = model(**inputs)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-5-ec07b46d1a63&gt; in &lt;module&gt;
----&gt; 1 inputs = assign_gpu(inputs_list[0])
      2 output = model(**inputs)

&lt;ipython-input-1-cb4bdd793d7f&gt; in assign_gpu(token)
     29 
     30 def assign_gpu(token):
---&gt; 31     token_tensor = token['input_ids'].to('cuda')
     32     token_typeid = token['token_type_ids'].to('cuda')
     33     attention_mask = token['attention_mask'].to('cuda')

RuntimeError: CUDA error: device-side assert triggered
</code></pre>
","1487336","","","","","2020-07-22 04:07:48","Bert forward reports error on GPU; but runs fine on CPU","<pytorch><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"63126386","1","63128700","","2020-07-28 02:57:50","","0","609","<p>I know that BERT has total vocabulary size of 30522 which contains some words and subwords. I want to get the initial input embeddings of BERT. So, my requirement is to get the table of size <code>[30522, 768]</code> to which I can index by token id to get its embeddings. Where can I get this table?</p>
","13952588","","100297","","2020-08-03 01:03:21","2020-08-03 01:03:21","Where can I get the pretrained word embeddinngs for BERT?","<embedding><huggingface-transformers><bert-language-model>","1","0","1","","","CC BY-SA 4.0"
"63232732","1","63415069","","2020-08-03 15:51:48","","6","645","<p>I have:</p>
<pre><code>        context = torch.tensor(context, dtype=torch.long, device=self.device)
        context = context.unsqueeze(0)
        generated = context
        with torch.no_grad():
            past_outputs = None
            for i in trange(num_words):
                print(i, num_words)
                inputs = {&quot;input_ids&quot;: generated}

                outputs, past_outputs = self.model(
                    **inputs,
                    past=past_outputs
                )
                next_token_logits = outputs[
                    0, -1, :] / (temperature if temperature &gt; 0 else 1.0)

                # reptition penalty from CTRL
                # (https://arxiv.org/abs/1909.05858)
                for _ in set(generated.view(-1).tolist()):
                    next_token_logits[_] /= repetition_penalty

                filtered_logits = top_k_top_p_filtering(
                    next_token_logits, top_k=top_k, top_p=top_p)
                if temperature == 0:  # greedy sampling:
                    next_token = torch.argmax(filtered_logits).unsqueeze(0)
                else:
                    next_token = torch.multinomial(
                        F.softmax(filtered_logits, dim=-1), num_samples=1)

                generated = torch.cat(
                    (generated, next_token.unsqueeze(0)), dim=1)
</code></pre>
<p>This works for the first iteration, but then I get an error for the next iteration:</p>
<pre><code>  File &quot;/Users/shamoon/Sites/wordblot/packages/ml-server/generator.py&quot;, line 143, in sample_sequence
    past=past_outputs
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/transformers/modeling_gpt2.py&quot;, line 601, in forward
    output_hidden_states=output_hidden_states,
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/transformers/modeling_gpt2.py&quot;, line 470, in forward
    position_embeds = self.wpe(position_ids)
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/torch/nn/modules/sparse.py&quot;, line 114, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/torch/nn/functional.py&quot;, line 1724, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
</code></pre>
<p>Is there something I'm doing wrong?</p>
","239879","","239879","","2020-08-03 20:30:05","2020-08-14 14:49:16","How to use the past with HuggingFace Transformers GPT-2?","<python><pytorch><huggingface-transformers>","2","8","","","","CC BY-SA 4.0"
"63461262","1","63464865","","2020-08-18 03:00:39","","5","6336","<p>I'm trying to get sentence vectors from hidden states in a BERT model.  Looking at the huggingface BertModel instructions <a href=""https://huggingface.co/bert-base-multilingual-cased?text=This%20sentence%20etc"" rel=""noreferrer"">here</a>, which say:</p>
<pre><code>from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained(&quot;bert-base-multilingual-cased&quot;)
text = &quot;Replace me by any text you'd like.&quot;
encoded_input = tokenizer(text, return_tensors='pt') 
output = model(**encoded_input)
</code></pre>
<p>So first note, as it is on the website, this does /not/ run. You get:</p>
<pre><code>&gt;&gt;&gt; Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: 'BertTokenizer' object is not callable
</code></pre>
<p>But it looks like a minor change fixes it, in that you don't call the tokenizer directly, but ask it to encode the input:</p>
<pre><code>encoded_input = tokenizer.encode(text, return_tensors=&quot;pt&quot;)
output = model(encoded_input)
</code></pre>
<p>OK, that aside, the tensors I get, however, have a different shape than I expected:</p>
<pre><code>&gt;&gt;&gt; output[0].shape
torch.Size([1,11,768])
</code></pre>
<p>This is a lot of layers.  Which is the correct layer to use for sentence embeddings?  <code>[0]</code>?  <code>[-1]</code>?  Averaging several?  I have the goal of being able to do cosine similarity with these, so I need a proper 1xN vector rather than an NxK tensor.</p>
<p>I see that the popular <a href=""https://github.com/hanxiao/bert-as-service#building-a-qa-semantic-search-engine-in-3-minutes"" rel=""noreferrer"">bert-as-a-service project</a> appears to use <code>[0]</code></p>
<p>Is this correct? Is there documentation for what each of the layers are?</p>
","1052117","","","","","2021-01-22 17:55:13","BERT sentence embeddings from transformers","<bert-language-model><huggingface-transformers>","2","1","1","","","CC BY-SA 4.0"
"63465109","1","","","2020-08-18 08:52:59","","1","1215","<p>I am trying to install Hugging Face's Transformers using,</p>
<blockquote>
<p>pip install transformers</p>
</blockquote>
<p>but it's giving me that error.
I tried installing the NVIDIA's Apex but it doesn't work on Mac.</p>
<p>P.S: I want to install transformers on mac (no gpu support)</p>
<pre><code>!pip install transformers
</code></pre>
<p>I also installed NVIDIA's apex.</p>
<p>ERROR</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-1-279c49635b32&gt; in &lt;module&gt;
----&gt; 1 import transformers

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/transformers/__init__.py in &lt;module&gt;
     21 
     22 # Configurations
---&gt; 23 from .configuration_albert import ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, AlbertConfig
     24 from .configuration_auto import ALL_PRETRAINED_CONFIG_ARCHIVE_MAP, CONFIG_MAPPING, AutoConfig
     25 from .configuration_bart import BartConfig

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/transformers/configuration_albert.py in &lt;module&gt;
     16 &quot;&quot;&quot; ALBERT model configuration &quot;&quot;&quot;
     17 
---&gt; 18 from .configuration_utils import PretrainedConfig
     19 
     20 

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/transformers/configuration_utils.py in &lt;module&gt;
     23 from typing import Dict, Tuple
     24 
---&gt; 25 from .file_utils import CONFIG_NAME, cached_path, hf_bucket_url, is_remote_url
     26 
     27 

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/transformers/file_utils.py in &lt;module&gt;
    101 
    102 try:
--&gt; 103     from apex import amp  # noqa: F401
    104 
    105     _has_apex = True

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/apex/__init__.py in &lt;module&gt;
     16 from apex.exceptions import (ApexAuthSecret,
     17                              ApexSessionSecret)
---&gt; 18 from apex.interfaces import (ApexImplementation,
     19                              IApex)
     20 from apex.lib.libapex import (groupfinder,

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/apex/interfaces.py in &lt;module&gt;
      8     pass
      9 
---&gt; 10 class ApexImplementation(object):
     11     &quot;&quot;&quot; Class so that we can tell if Apex is installed from other 
     12     applications

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/apex/interfaces.py in ApexImplementation()
     12     applications
     13     &quot;&quot;&quot;
---&gt; 14     implements(IApex)

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/zope/interface/declarations.py in implements(*interfaces)
    704     # the coverage for this block there. :(
    705     if PYTHON3:
--&gt; 706         raise TypeError(_ADVICE_ERROR % 'implementer')
    707     _implements(&quot;implements&quot;, interfaces, classImplements)
    708 

TypeError: Class advice impossible in Python3.  Use the @implementer class decorator instead.
</code></pre>
","9309016","","4518341","","2020-08-18 14:56:30","2020-08-19 10:20:36","TypeError: Class advice impossible in Python3. Use the @implementer class decorator instead","<python><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62525680","1","","","2020-06-23 00:53:56","","5","3350","<p>Currently, I'm building a new transformer-based model with huggingface-transformers, where attention layer is different from the original one. I used <code>run_glue.py</code> to check performance of my model on GLUE benchmark. However, I found that Trainer class of huggingface-transformers saves all the checkpoints that I set, where I can set the maximum number of checkpoints to save. However, I want to save only the weight (or other stuff like optimizers) with <strong>best</strong> performance on validation dataset, and current Trainer class doesn't seem to provide such thing. (If we set the maximum number of checkpoints, then it removes older checkpoints, not ones with worse performances). <a href=""https://github.com/huggingface/transformers/issues/2675"" rel=""noreferrer"">Someone already asked about same question on Github</a>, but I can't figure out how to modify the script and do what I want. Currently, I'm thinking about making a custom Trainer class that inherits original one and change the <code>train()</code> method, and it would be great if there's an easy and simple way to do this. Thanks in advance.</p>
","11195279","","","","","2021-05-20 06:45:26","Save only best weights with huggingface transformers","<deep-learning><nlp><pytorch><huggingface-transformers>","3","0","3","","","CC BY-SA 4.0"
"64985740","1","","","2020-11-24 11:40:40","","1","532","<p>I am trying to build Bert model for Arabic Text classification task using pretrained model from <a href=""https://github.com/alisafaya/Arabic-BERT"" rel=""nofollow noreferrer"">https://github.com/alisafaya/Arabic-BERT</a>
i want to know the exact difference between the two statement:</p>
<pre><code>model_name = 'kuisailab/albert-large-arabic' 
model = AutoModel.from_pretrained(model_name)
model = BertForSequenceClassification .from_pretrained(model_name)
</code></pre>
<p>I fine-tuned the model by adding the following layers on top of the model:</p>
<pre><code>for param in model.parameters():
    param.requires_grad = False

model.classifier = nn.Sequential(
nn.Dropout(0.5),
nn.ReLU(),
nn.Linear(768,512),
nn.Linear(512,2), 
nn.LogSoftmax(dim=1), 
nn.Softmax(dim=1)
)
model = model.to(device)
</code></pre>
<p>and used the optimizer:</p>
<pre><code>optimizer = AdamW(model.parameters(),
                  lr = 2e-5) 
</code></pre>
<p>finally this is my training loop:</p>
<pre><code>model.train()

for idx, row in train_data.iterrows():
    text_parts = preprocess_text(str(row['sentence']))
    label = torch.tensor([row['label']]).long().to(device)

    optimizer.zero_grad()

    overall_output = torch.zeros((1, 2)).float().to(device)
    for part in text_parts:
        if len(part) &gt; 0:
            try:
                input = part.reshape(-1)[:512].reshape(1, -1)
                # print(input.shape)
                overall_output += model(input, labels=label)[1].float().to(device)
            except Exception as e:
                print(str(e))

#     overall_output /= len(text_parts)
    overall_output = F.softmax(overall_output[0], dim=-1)

    if label == 0:
        label = torch.tensor([1.0, 0.0]).float().to(device)
    elif label == 1:
        label = torch.tensor([0.0, 1.0]).float().to(device)

    # print(overall_output, label)

    loss = criterion(overall_output, label)
    total_loss += loss.item()
    
    loss.backward()
    optimizer.step()
</code></pre>
<p>and i get the error:</p>
<pre><code>mat1 dim 1 must match mat2 dim 0
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-33-5c2f0fea6c1f&gt; in &lt;module&gt;()
     39     total_loss += loss.item()
     40 
---&gt; 41     loss.backward()
     42     optimizer.step()
     43 

1 frames
/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
    130     Variable._execution_engine.run_backward(
    131         tensors, grad_tensors_, retain_graph, create_graph,
--&gt; 132         allow_unreachable=True)  # allow_unreachable flag
    133 
    134 

RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn 
</code></pre>
<p>any idea how to solve this error</p>
","13201830","","13201830","","2020-11-24 12:57:40","2020-11-24 12:57:40","""RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn "" error BertFoeSequenceClassification","<nlp><pytorch><bert-language-model><huggingface-transformers><pre-trained-model>","1","9","","","","CC BY-SA 4.0"
"63461821","1","63467606","","2020-08-18 04:19:51","","0","398","<p>I'm looking <a href=""https://huggingface.co/transformers/main_classes/pipelines.html#featureextractionpipeline"" rel=""nofollow noreferrer"">here at the feature extraction pipeline</a>.</p>
<p>I initialize with the following:</p>
<pre><code>from transformers import pipeline 
pipe = pipeline(&quot;feature-extraction&quot;) 
features = pipe(&quot;test&quot;)
</code></pre>
<p>And I get a bunch of features.  What model is this using by default?  How can I initialize this pipeline to use a particular pre-trained model?</p>
<pre><code>len(features)
1
&gt;&gt;&gt; features
[[[0.4122459590435028, 0.10175584256649017, 0.09342928230762482, -0.3119196593761444, -0.3226662278175354, -0.16414110362529755, 0.06356583535671234, -0.03167172893881798, -0.010002809576690197, -1.1153486967086792, -0.3304346203804016, 0.1727224737405777, -0.0904250368475914, -0.04243310168385506, -0.4745883047580719, 0.09118127077817917, 0.4240476191043854, 0.2237153798341751, 0.12108077108860016, -0.16883963346481323, 0.055300742387771606, -0.07225772738456726, 0.4521999955177307, -0.31655701994895935, 0.05917530879378319, -0.0343029648065567, 0.4157347083091736, 0.10791877657175064, -0
...etc
</code></pre>
<p>While the document tells me:</p>
<blockquote>
<p>All models may be used for this pipeline. See a list of all models, including community-contributed models on huggingface.co/models.</p>
</blockquote>
<p>It's not clear to me where to initialize the models in this link.  The API is very terse.</p>
","1052117","","","","","2020-08-18 11:24:59","What features are used in the default transformers pipeline?","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63708496","1","","","2020-09-02 15:14:14","","5","834","<p>Looking to do something similar to</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
input_ids = torch.tensor(tokenizer.encode(&quot;Hello, my dog is cute&quot;)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids)
last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
</code></pre>
<p>(from <a href=""https://github.com/huggingface/transformers/issues/1950"" rel=""noreferrer"">this thread</a>)
using the longformer</p>
<p>the documentation example seems to do something similar, but is confusing (esp. wrt. how to set the attention mask, I assume I'd want to set it to the <code>[CLS]</code> token, the example sets global attention to random values I think)</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; from transformers import LongformerModel, LongformerTokenizer

&gt;&gt;&gt; model = LongformerModel.from_pretrained('allenai/longformer-base-4096', return_dict=True)
&gt;&gt;&gt; tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')

&gt;&gt;&gt; SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document
&gt;&gt;&gt; input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1

&gt;&gt;&gt; # Attention mask values -- 0: no attention, 1: local attention, 2: global attention
&gt;&gt;&gt; attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention
&gt;&gt;&gt; attention_mask[:, [1, 4, 21,]] = 2  # Set global attention based on the task. For example,
...                                     # classification: the &lt;s&gt; token
...                                     # QA: question tokens
...                                     # LM: potentially on the beginning of sentences and paragraphs
&gt;&gt;&gt; outputs = model(input_ids, attention_mask=attention_mask)
&gt;&gt;&gt; sequence_output = outputs.last_hidden_state
&gt;&gt;&gt; pooled_output = outputs.pooler_output
</code></pre>
<p>(from <a href=""https://huggingface.co/transformers/model_doc/longformer.html"" rel=""noreferrer"">here</a>)</p>
","1161523","","3607203","","2020-09-03 12:21:03","2021-03-18 23:37:35","How to extract document embeddings from HuggingFace Longformer","<huggingface-transformers>","1","1","2","","","CC BY-SA 4.0"
"63231544","1","","","2020-08-03 14:41:25","","0","379","<p>This is my first time working on binary text classification, I'm working with CamemBert using  fast-bert library. After fine-tuning, I tried training but got this error :</p>
<pre class=""lang-none prettyprint-override""><code>Error(s) in loading state_dict for CamembertForSequenceClassification:
    size mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([10, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).
    size mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([2]).
</code></pre>
<p>The weird thing is that I tried doing multi-label classification and it worked fine.</p>
<p>How can I fix this?</p>
<p>Here's the config.json file obtained after the fine-tuning:</p>
<pre class=""lang-none prettyprint-override""><code>{
  &quot;architectures&quot;: [
    &quot;CamembertForMultiLabelSequenceClassification&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;bos_token_id&quot;: 5,
  &quot;eos_token_id&quot;: 6,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;id2label&quot;: {
    &quot;0&quot;: &quot;LABEL_0&quot;,
    &quot;1&quot;: &quot;LABEL_1&quot;,
    &quot;2&quot;: &quot;LABEL_2&quot;,
    &quot;3&quot;: &quot;LABEL_3&quot;,
    &quot;4&quot;: &quot;LABEL_4&quot;,
    &quot;5&quot;: &quot;LABEL_5&quot;,
    &quot;6&quot;: &quot;LABEL_6&quot;,
    &quot;7&quot;: &quot;LABEL_7&quot;,
    &quot;8&quot;: &quot;LABEL_8&quot;,
    &quot;9&quot;: &quot;LABEL_9&quot;
  },
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;label2id&quot;: {
    &quot;LABEL_0&quot;: 0,
    &quot;LABEL_1&quot;: 1,
    &quot;LABEL_2&quot;: 2,
    &quot;LABEL_3&quot;: 3,
    &quot;LABEL_4&quot;: 4,
    &quot;LABEL_5&quot;: 5,
    &quot;LABEL_6&quot;: 6,
    &quot;LABEL_7&quot;: 7,
    &quot;LABEL_8&quot;: 8,
    &quot;LABEL_9&quot;: 9
  },
  &quot;layer_norm_eps&quot;: 1e-05,
  &quot;max_position_embeddings&quot;: 514,
  &quot;model_type&quot;: &quot;camembert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;output_past&quot;: true,
  &quot;pad_token_id&quot;: 1,
  &quot;type_vocab_size&quot;: 1,
  &quot;vocab_size&quot;: 32005
}
</code></pre>
","13849474","","13849474","","2020-08-03 19:44:27","2020-08-03 19:44:27","Loading pretrained model fast-bert","<nlp><pytorch><bert-language-model><huggingface-transformers><fast-ai>","0","19","","","","CC BY-SA 4.0"
"65032437","1","","","2020-11-27 05:21:14","","0","619","<p>I tried to use a Google Colab initialised Notebook for Kaggle and found a strange behaviour as it gave me something like:</p>
<pre><code> 16   # text2tensor
---&gt; 17   train_seq,train_mask,train_y = textToTensor(train_text,train_labels,pad_len)
     18   val_seq,val_mask,val_y       = textToTensor(val_text,val_labels,pad_len)
     19 

&lt;ipython-input-9-ee85c4607a30&gt; in textToTensor(text, labels, max_len)
      4   tokens = tokenizer.batch_encode_plus(text.tolist(), max_length=max_len, padding='max_length', truncation=True)
      5 
----&gt; 6   text_seq = torch.tensor(tokens['input_ids'])
      7   text_mask = torch.tensor(tokens['attention_mask'])
      8 

ValueError: expected sequence of length 38 at dim 1 (got 13)
</code></pre>
<p>The error came from the code below:</p>
<pre><code>def textToTensor(text,labels=None,max_len=38):#max_len is 38
  
  tokens = tokenizer.batch_encode_plus(text.tolist(), max_length=max_len, padding='max_length', truncation=True)
  
  text_seq = torch.tensor(tokens['input_ids']) # ERROR CAME FROM HERE
  text_mask = torch.tensor(tokens['attention_mask'])

  text_y = None
  if isinstance(labels,np.ndarray):
    text_y = torch.tensor(labels.tolist())

  return text_seq, text_mask, text_y



train_seq,train_mask,train_y = textToTensor(train_text,train_labels,pad_len)
train_data = TensorDataset(train_seq, train_mask, train_y)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

</code></pre>
<p>I again ran this code on <strong>COLAB</strong> and it ran smoothly. Can it be because of the Version and something like that? Can someone please this?</p>
<p><strong>Kaggle Configs</strong>:</p>
<pre><code>transformers: '2.11.0'
torch: '1.5.1'
python: 3.7.6
</code></pre>
<p><strong>Colab Configs</strong>:</p>
<pre><code>torch: 1.7.0+cu101
transformers: 3.5.1
python: 3.6.9
</code></pre>
<p><strong>EDIT</strong>:
My <code>train_text</code> is a numpy array of texts and <code>train_labels</code> is 1-d numerical array with 4 classes ranging 0-3.</p>
<p>Also: I initialized my tokenizer as:</p>
<pre><code>from transformers import BertTokenizerFast
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
</code></pre>
","11725056","","11725056","","2020-11-28 13:38:48","2020-11-28 13:38:48","Pytorch + BERT+ batch_encode_plus() Code running fine in Colab but producing problems with Kaggle in mismatch input shapes","<pytorch><google-colaboratory><kaggle><huggingface-transformers><huggingface-tokenizers>","0","7","","","","CC BY-SA 4.0"
"62466514","1","62504605","","2020-06-19 08:41:36","","3","1416","<p>Shall we lower case input data for (pre) training a BERT uncased model using huggingface? I looked into this response from Thomas Wolf (<a href=""https://github.com/huggingface/transformers/issues/92#issuecomment-444677920"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/92#issuecomment-444677920</a>) but not entirely sure if he meant that. </p>

<p>What happens if we lowercase the text ? </p>
","6782087","","","","","2020-06-21 21:23:10","Shall we lower case input data for (pre) training a BERT uncased model using huggingface?","<deep-learning><nlp><pytorch><huggingface-transformers>","2","1","","","","CC BY-SA 4.0"
"62545504","1","","","2020-06-23 23:53:18","","0","47","<p>I know we can access the hidden layer from each layer by the following code. However, each hidden output we obtains is the result mixed from 12 head by a fully connected layers. Is there a way to obtain the outputs before they enter the fully connected layers?</p>
<pre><code>from transformers import BertModel, BertConfig

config = BertConfig.from_pretrained(&quot;xxx&quot;, output_hidden_states=True)
model = BertModel.from_pretrained(&quot;xxx&quot;, config=config)

outputs = model(inputs)
print(len(outputs))  # 3

hidden_states = outputs[2]
</code></pre>
<p>Thank you!</p>
","13802302","","","","","2020-06-23 23:53:18","How can I get the hidden layers for each indiviual head?","<python><huggingface-transformers><bert-language-model>","0","3","","","","CC BY-SA 4.0"
"62852940","1","62911442","","2020-07-11 18:19:30","","8","1630","<p>I was trying the hugging face gpt2 model. I have seen the <a href=""https://github.com/huggingface/transformers/blob/master/examples/text-generation/run_generation.py"" rel=""noreferrer""><code>run_generation.py</code> script</a>, which generates a sequence of tokens given a prompt. I am aware that we can use GPT2 for NLG.</p>
<p>In my use case, I wish to determine the probability distribution for (only) the immediate next word following the given prompt. Ideally this distribution would be over the entire vocab.</p>
<p>For example, given the prompt: &quot;How are &quot;, it should give a probability distribution where &quot;you&quot; or &quot;they&quot; have the some high floating point values and other vocab words have very low floating values.</p>
<p>How to do this using hugging face transformers? If it is not possible in hugging face, is there any other transformer model that does this?</p>
","2181238","","2181238","","2020-07-12 11:01:21","2020-07-15 09:11:52","How to get immediate next word probability using GPT2 model?","<transformer><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"68942858","1","","","2021-08-26 17:16:56","","0","18","<p>I am wondering why <code>sep_token_id</code> is same as <code>eos_token_id</code> for <code>allenai/led-base-16384</code> Huggingface model. It creates a confusion during inference whether <code>&lt;/s&gt;</code> token refers to <code>eos_token</code> or <code>sep_token</code>, both ideally would be used in different places in text generation.</p>
<p>Steps to reproduce the issue:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained(&quot;allenai/led-base-16384&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;allenai/led-base-16384&quot;)
print(tokenizer.eos_token_id) # prints 2
print(tokenizer.sep_token_id) # prints 2 again
</code></pre>
","3306097","","","","","2021-08-26 17:16:56","Why sep_token_id is same as eos_token_id for allenai/led-base-16384","<machine-learning><huggingface-transformers><huggingface-tokenizers>","0","1","","","","CC BY-SA 4.0"
"65037368","1","","","2020-11-27 12:27:17","","2","1369","<p>I would like to remove tensorflow and hugging face models from my laptop.
I did find one link <a href=""https://github.com/huggingface/transformers/issues/861"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/861</a>
but is there not command that can remove them because as mentioned in the link manually deleting can cause problems because we don't know which other files are linked to those models or are expecting some model to be present in that location or simply it may cause some error.</p>
","5687866","","5687866","","2020-11-28 07:02:47","2020-11-28 07:02:47","Remove downloaded tensorflow and pytorch(Hugging face) models","<tensorflow><pytorch><huggingface-transformers>","2","2","","","","CC BY-SA 4.0"
"64986361","1","","","2020-11-24 12:19:17","","1","76","<p>I'm currently training Roberta for sentiment classification and having a fixed mini-batch size is killing my performance. To the extent that running the stuff I'm running will take 8-9 days to cover the training set once. Is there a general equation for calculating the memory usage of RoBERTa given n_inputs, input_length and input embedding size? If I have this I should be able to efficiently mini-batch on the fly (it has to be done during runtime due to other libraries that I'm using).</p>
<p>(If it helps in anyway I'm using PyTorch to do this)</p>
","13392352","","13392352","","2020-11-24 14:47:32","2020-11-24 14:47:32","Calculating the memory usage of RoBERTa?","<deep-learning><pytorch><huggingface-transformers>","0","5","","","","CC BY-SA 4.0"
"65025131","1","","","2020-11-26 15:40:14","","0","84","<p>I want to use hugging face pipeline in my hug web service, but the problem is that the service load the pipeline each time, so i want to load the pipeline once only at the begging of service, how can i do this ?</p>
<pre><code>from transformers import pipeline

import hug


classifier = pipeline(&quot;zero-shot-classification&quot;, model='joeddav/xlm-roberta-large-xnli')



@hug.local()
@hug.post('/classifier')
def function(data, labels):

    out= classifier(text , labels)
    return out
</code></pre>
","14522387","","","","","2020-11-26 15:40:14","is there a method to load model once at the beginning of program?","<python><python-3.x><nlp><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"65083581","1","65084300","","2020-12-01 01:38:17","","3","520","<p>I'm using the HuggingFace Transformers BERT model, and I want to compute a summary vector (a.k.a. embedding) over the tokens in a sentence, using either the <code>mean</code> or <code>max</code> function. The complication is that some tokens are <code>[PAD]</code>, so I want to ignore the vectors for those tokens when computing the average or max.</p>
<p>Here's an example. I initially instantiate a <code>BertTokenizer</code> and a <code>BertModel</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import transformers
from transformers import AutoTokenizer, AutoModel

transformer_name = 'bert-base-uncased'

tokenizer = AutoTokenizer.from_pretrained(transformer_name, use_fast=True)

model = AutoModel.from_pretrained(transformer_name)
</code></pre>
<p>I then input some sentences into the tokenizer and get out <code>input_ids</code> and <code>attention_mask</code>. Notably, an <code>attention_mask</code> value of 0 means that the token was a <code>[PAD]</code> that I can ignore.</p>
<pre class=""lang-py prettyprint-override""><code>sentences = ['Deep learning is difficult yet very rewarding.',
             'Deep learning is not easy.',
             'But is rewarding if done right.']
tokenizer_result = tokenizer(sentences, max_length=32, padding=True, return_attention_mask=True, return_tensors='pt')

input_ids = tokenizer_result.input_ids
attention_mask = tokenizer_result.attention_mask

print(input_ids.shape) # torch.Size([3, 11])

print(input_ids)
# tensor([[  101,  2784,  4083,  2003,  3697,  2664,  2200, 10377,  2075,  1012,  102],
#         [  101,  2784,  4083,  2003,  2025,  3733,  1012,   102,     0,     0,    0],
#         [  101,  2021,  2003, 10377,  2075,  2065,  2589,  2157,  1012,   102,   0]])

print(attention_mask.shape) # torch.Size([3, 11])

print(attention_mask)
# tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
#         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],
#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])
</code></pre>
<p>Now, I call the BERT model to get the 768-D token embeddings (the top-layer hidden states).</p>
<pre class=""lang-py prettyprint-override""><code>model_result = model(input_ids, attention_mask=attention_mask, return_dict=True)

token_embeddings = model_result.last_hidden_state
print(token_embeddings.shape) # torch.Size([3, 11, 768])
</code></pre>
<p>So at this point, I have:</p>
<ol>
<li>token embeddings in a [3, 11, 768] matrix: 3 sentences, 11 tokens, 768-D vector for each token.</li>
<li>attention mask in a [3, 11] matrix: 3 sentences, 11 tokens. A 1 value indicates non-<code>[PAD]</code>.</li>
</ol>
<p>How do I compute the <code>mean</code> / <code>max</code> over the vectors for the valid, non-<code>[PAD]</code> tokens?</p>
<p>I tried using the attention mask as a mask and then called <code>torch.max()</code>, but I don't get the right dimensions:</p>
<pre class=""lang-py prettyprint-override""><code>masked_token_embeddings = token_embeddings[attention_mask==1]
print(masked_token_embeddings.shape) # torch.Size([29, 768] &lt;-- WRONG. SHOULD BE [3, 11, 768]

pooled = torch.max(masked_token_embeddings, 1)
print(pooled.values.shape) # torch.Size([29]) &lt;-- WRONG. SHOULD BE [3, 768]
</code></pre>
<p>What I really want is a tensor of shape [3, 768]. That is, a 768-D vector for each of the 3 sentences.</p>
","4561314","","4561314","","2020-12-01 01:49:57","2020-12-01 03:27:48","How to compute mean/max of HuggingFace Transformers BERT token embeddings with attention mask?","<machine-learning><pytorch><bert-language-model><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"62235153","1","62235318","","2020-06-06 17:05:56","","2","794","<p>I want to do a joint-embedding from vgg16 and <code>bert</code> for classification.</p>

<p>The thing with <code>huggingface transformers bert</code> is that it has the classification layer which has <code>num_labels</code> dimension.</p>

<p>But, I want the output from <code>BertPooler</code> (768 dimensions) which I will use as a text-embedding for an extended model.</p>

<pre><code>from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
</code></pre>

<p>This gives the following model:</p>

<pre><code>BertForSequenceClassification(
...
...
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
</code></pre>

<p>How can I get rid of the <code>classifier</code> layer?</p>
","4622046","","4228275","","2020-06-06 17:13:04","2020-06-06 17:29:49","huggingface transformers bert model without classification layer","<pytorch><huggingface-transformers><bert-language-model>","1","2","1","","","CC BY-SA 4.0"
"62255856","1","62259596","","2020-06-08 05:56:15","","1","642","<p>I have a problem statement where I want to predict multiple continuous outputs using a text input. I tried using 'robertaforsequenceclassification' from HuggingFace library. But the documentation states that when the number of outputs in the final layer is more than 1, a cross entropy loss is used automatically as mentioned here: <a href=""https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification</a>.
But I want to use an RMSE loss in a regression setting with two classes in the final layer. How would one go about modifying it?</p>
","3619334","","","","","2020-09-30 18:44:26","How to perform Multi output regression using RoBERTa?","<pytorch><regression><huggingface-transformers><bert-language-model>","1","1","","","","CC BY-SA 4.0"
"62519413","1","","","2020-06-22 16:45:49","","-1","400","<p>Is there a way to choose my custom vocabulary in T5-model while fine-tuning for a text summarization task?</p>
<p>I tried using a sentencepiece model to create my custom tokenizer but the model predicted some tokens which was not present in my tokenizer and hence the tokenizer takes it as an unknown token.</p>
","10046021","","","","","2021-04-17 20:15:39","T5 model custom vocabulary","<python><pytorch><huggingface-transformers>","2","1","","","","CC BY-SA 4.0"
"65095129","1","","","2020-12-01 17:07:31","","1","425","<p>i want to train text to determine if text positive , neutral or negative , so i trained the BERT base model but at the end at the step of learner.validate it return an error, can you help me please ?</p>
<pre><code> !pip install ktrain==0.23.2
 import ktrain

 X=[&quot;hello&quot;,&quot;i'am here..&quot;,&quot;...&quot;]
 y=np.array([1,2,0,2,1,2,0,1,1,...])
 categories=[&quot;neg&quot;,&quot;neut&quot;,&quot;pos&quot;]
 X_train, X_test, y_train, y_test = train_test_split(X[:100], y[:100], test_size=0.33, 
 random_state=42)
 model_name='distilbert-base-multilingual-cased'
 trans = text.Transformer(model_name, maxlen=512, class_names=categories)
 train_data=trans.preprocess_train(X_train,y_train)
 test_data = trans.preprocess_test(X_test, y_test)
 model= trans.get_classifier()
 learner= ktrain.get_learner(model, train_data=train_data,val_data=test_data,batch_size=8)
 learner.fit_onecycle(3e-5,1)
 learner.validate(class_names=categories)
</code></pre>
<p>but i get error :</p>
<pre><code>      ValueError                                Traceback (most recent call last)
     &lt;ipython-input-39-d99399b4f681&gt; in &lt;module&gt;()
     ----&gt; 1 learner.validate(class_names=categories)

     8 frames
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in 
   convert_to_eager_tensor(value, ctx, dtype)
   96       dtype = dtypes.as_dtype(dtype).as_datatype_enum
   97   ctx.ensure_initialized()
   ---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)
   99 
   100 

   ValueError: Attempt to convert a value (TFSequenceClassifierOutput(loss=None, 
   logits=array([[-0.14492714, -0.308572  ,  0.41551754],
   [-0.02239409, -0.22379267,  0.13737921],
   [-0.15169457, -0.26776308,  0.3646287 ],
   [-0.11638386, -0.30610234,  0.42503807],
   [-0.14799587, -0.2751274 ,  0.3999261 ],
   [-0.1224369 , -0.2705566 ,  0.37462863],
   [-0.16992202, -0.26875758,  0.38301754],
   [-0.02906444, -0.22582646,  0.16817996],
   [-0.14451084, -0.3335299 ,  0.45228514],
   [-0.12560453, -0.15705517,  0.3202087 ],
   [-0.1477989 , -0.28774017,  0.40492412],
   [-0.15207294, -0.27476442,  0.41782793],
   [-0.14203653, -0.24748898,  0.4224902 ],
   [-0.02594737, -0.23538935,  0.17150217],
   [-0.10563572, -0.31230217,  0.37387425],
   [-0.13637367, -0.28157175,  0.38459644],
   [-0.14043958, -0.29381162,  0.41240314],
   [-0.03989747, -0.23261254,  0.19984315],
   [-0.12188954, -0.25612894,  0.4106647 ],
   [-0.1576367 , -0.32221746,  0.44524238],
   [-0.14458796, -0.29356796,  0.43222305],
   [-0.0947336 , -0.26198757,  0.30835733],
   [-0.18257669, -0.28770167,  0.43495163],
   [-0.17199922, -0.27865767,  0.3720437 ],
   [-0.12301907, -0.2829709 ,  0.42322537],
   [-0.10598033, -0.2630937 ,  0.36105153],
   [-0.01495049, -0.21938747,  0.14062764],
   [-0.12164614, -0.3101943 ,  0.4182844 ],
   [-0.10304911, -0.24770047,  0.353735  ],
   [-0.16426452, -0.29451552,  0.42003003],
   [-0.16851503, -0.2555803 ,  0.41026738],
   [-0.10907209, -0.26942176,  0.40432337],
   [-0.13107793, -0.25909895,  0.34871536]], dtype=float32), hidden_states=None, attentions=None)) 
    with an unsupported type (&lt;class 'transformers.modeling_tf_outputs.TFSequenceClassifierOutput'&gt;) 
    to a Tensor.
</code></pre>
<p>how we can fix this problem, i find on the internet is that this problem is related to the version of tensorflow but i tried the version of tensorflow 2.1 but i have the same problem.... any ideas please</p>
","14522387","","","","","2020-12-02 19:39:47","is there a method to transform array to tensor tensorflow?","<python-3.x><tensorflow><tensorflow2.0><sentiment-analysis><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"65032735","1","","","2020-11-27 05:59:53","","0","386","<p>I need to get the last layer of embeddings from a BERT model using HuggingFace. The following code works, however is <strong>extremely slow</strong>, how can I <strong>increase the speed</strong>?</p>
<p>This is a toy example, my real data is made of thousands of examples with long texts.</p>
<pre><code>import transformers
import pandas as pd
from transformers import BertModel, BertTokenizer

tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;) 
model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)

def getPrediction(text): 
  encoded_input = tokenizer(text, return_tensors='pt')
  outputs = model(**encoded_input)
  embedding = outputs[0][:, -1]  
  embedding_1ist = embedding.cpu().detach().tolist()[0]
  return embedding_1ist

df = pd.DataFrame({'text':['First text', 'Second text']})
results =  pd.DataFrame(df.apply(lambda x: getPrediction(x.text), axis=1)) 
</code></pre>
","5799243","","","","","2020-11-27 05:59:53","Increase speed Huggingface tokenizer ouput","<pytorch><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","4","","","","CC BY-SA 4.0"
"65084644","1","","","2020-12-01 04:15:31","","0","38","<p>I have run into what may be the most bizarre error of my life.
I have written a function to train a BERT model as follows :</p>
<pre><code>def process_data(path, enc_pos = None, enc_tags = None):
    df = pd.read_csv(path, index_col=0).dropna(subset=['tag'])

    if enc_pos is None:
        enc_pos = LabelEncoder()
        df.loc[:, 'pos'] = enc_pos.fit_transform(df['pos'])
    else:
        df.loc[:, 'pos'] = enc_pos.transform(df['pos'])

    if enc_tags is None:
        enc_tags = LabelEncoder()
        df.loc[:, 'tag'] = enc_tags.fit_transform(df['tag'])
    else:
        df.loc[:, 'tag'] = enc_tags.transform(df['tag'])

    sentences = df.groupby(&quot;sentence&quot;)['word'].apply(list).values
    tags = df.groupby(&quot;sentence&quot;)['tag'].apply(list).values
    pos = df.groupby(&quot;sentence&quot;)['pos'].apply(list).values

    for i in range(df.shape[0]):
        assert len(sentences) == len(tags) == len(pos)

    return sentences, tags, pos, enc_tags, enc_pos

def _train(
    train_sentences,
    valid_sentences,
    train_pos, 
    valid_pos,
    train_tags,
    valid_tags,
    num_pos, 
    num_tags,
    save=False,
    early_stopping=None):

    num_unimproved_steps = 0 

    train_dataset = Dataset(train_sentences, train_pos, train_tags)
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=config.TRAIN_BATCH_SIZE)

    valid_dataset = Dataset(valid_sentences, valid_pos, valid_tags)
    valid_loader = torch.utils.data.DataLoader(
        valid_dataset, batch_size=config.VALID_BATCH_SIZE)


    device = torch.device(config.DEVICE)
    model = NERModel(num_tags, num_pos)
    model.to(device)

    param_optimizer = list(model.named_parameters())
    no_decay = [&quot;bias&quot;, &quot;LayerNorm.bias&quot;, &quot;LayerNorm.weight&quot;]
    optimizer_parameters = [
        {
            &quot;params&quot;: [
                p for n, p in param_optimizer if not any(
                    nd in n for nd in no_decay
                )
            ],
            &quot;weight_decay&quot;: 0.001,
        },
        {
            &quot;params&quot;: [
                p for n, p in param_optimizer if any(
                    nd in n for nd in no_decay
                )
            ],
            &quot;weight_decay&quot;: 0.0,
        },
    ]

    num_steps = int(len(train_sentences)/ config.TRAIN_BATCH_SIZE * config.EPOCHS)
    optimizer = AdamW(optimizer_parameters, lr=7e-5)

    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=0, num_training_steps= num_steps)

    best_loss = np.inf

    train_graphs = []
    valid_graphs = []

    for epoch in range(config.EPOCHS):
        train_loss, train_graph = train_fn(train_loader, model, device, scheduler, optimizer)
        valid_loss, valid_graph = eval_fn(valid_loader, model, device)

        train_graphs.append(train_graph)
        valid_graphs.append(valid_graph)

        print(f&quot;For epoch {epoch}, Train Loss: {train_loss:.04f} , Valid Loss: {valid_loss:.04f}&quot;)

        if valid_loss &lt; best_loss:
            num_unimproved_steps = 0
            best_loss = valid_loss
            if save:
                print(&quot;Validation loss improved. Now saving model ...&quot;)
                torch.save(model.state_dict(), config.MODEL_FILE)
        else: 
            num_unimproved_steps += 1
            if num_unimproved_steps == early_stopping :
                print(f&quot;Early Stopping: The model did not improve in the last {early_stopping} epochs.&quot;)
                break
        

    return best_loss, train_graphs, valid_graphs

# training section 
def train_with_cv(early_stopping=None, n_folds=5):
    print(f&quot;Now starting model training with {n_folds} fold cross validation...&quot;)
    sentences, tags, pos, enc_tags, enc_pos = process_data(config.TRAINING_FILE)
    num_unimproved_steps = 0 

    num_pos = len(enc_pos.classes_)
    num_tags = len(enc_tags.classes_)

    kf = KFold(shuffle=True, random_state=42, n_splits=10)
    cv_losses = []
    fold = 0

    for train_i, valid_i in kf.split(sentences):
        train_sentences = sentences[train_i]
        valid_sentences = sentences[valid_i]

        train_pos = pos[train_i]
        valid_pos = pos[valid_i]

        train_tags = tags[train_i]
        valid_tags = tags[valid_i]


        # (
        #     train_sentences,
        #     valid_sentences,
        #     train_pos, 
        #     valid_pos,
        #     train_tags,
        #     valid_tags
        # ) = train_test_split(
        #     sentences, 
        #     pos,
        #     tags,
        #     test_size = 0.1 
        # )
        print(f&quot;Fold {fold+1} starting ...&quot;)

        loss, _, _ = _train(
            train_sentences=train_sentences, 
            valid_sentences=valid_sentences, 
            train_pos=train_pos, 
            valid_pos=valid_pos, 
            train_tags=train_tags, 
            valid_tags=valid_tags,
            num_pos=num_pos,
            num_tags=num_tags,
            early_stopping=early_stopping
        )

        print(f&quot;Fold {fold} finished with best loss: {loss}&quot;) 
        cv_losses.append(loss) 
        fold +=1 
        if fold == n_folds: 
            break

    return  cv_losses
</code></pre>
<p>If I comment out the K-fold code and uncomment the train_test_split code, the model trains fine and the validation loss decreases as expected to about 0.15. However, when I use K Fold, the validation loss does not improve at all! It is stuck around the 0.4 - 0.45 range.</p>
<p>I even tried printing out the sentences and the dtypes, but they seem to be identical. I don't understand what the difference is here.</p>
<p>Any help would be appreciated!</p>
","9665188","","","","","2020-12-01 04:15:31","Model does not train when using KFold","<python><scikit-learn><nlp><pytorch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"65099455","1","","","2020-12-01 22:23:49","","0","2242","<p>I think my problem here is due to transformer version mismatch... but I would like some help with this...</p>
<p>Previously I used the huggingface library to perform <a href=""https://github.com/huggingface/transformers/blob/master/examples/contrib/legacy/run_language_modeling.py"" rel=""nofollow noreferrer"">language model fine tuning</a>. This takes a corpus, an existing BERT model, and fine tune that model using this corpus. My command was</p>
<pre><code>python run_language_modelling.py --output_dir=lm_finetune --model_type=bert --model_name_or_path=bert-base-uncased --do_train --train_data_file=thread0_wdc20.txt --do_eval --eval_data_file=wiki.test.raw --mlm --save_total_limit=1 --save_steps=2 --line_by_line --num_train_epochs=2
</code></pre>
<p>I fine-tuned the models successfully, and this created a folder that contained the following files:</p>
<pre><code>checkpoint-183236  config.json  eval_results_lm.txt  lm_finetune  pytorch_model.bin  special_tokens_map.json  tokenizer_config.json  training_args.bin  vocab.txt
</code></pre>
<p>And I also successfully loaded this fine-tuned language model for downstream tasks.</p>
<p><strong>The problem is that I don't remember the versions of the libraries I used to do all these - pytorch, transformers, tensorflow...</strong></p>
<p>Recently, I am experimenting something that required me to re-install these libraries. Their versions are now:</p>
<pre><code>tensorflow-gpu 2.2.0
transformers 3.0.2
pytorch 1.4.0
torchtext 0.5.0
</code></pre>
<p>And when I use this environment to reload those previously fine-tuned language models, I get this error:</p>
<pre><code>  File &quot;/home/li1zz/.conda/envs/tensorflow-gpu/lib/python3.6/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/home/li1zz/.conda/envs/tensorflow-gpu/lib/python3.6/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/home/li1zz/wop_matching/src/exp/run_bert_standard.py&quot;, line 313, in &lt;module&gt;
    bert_model = transformers.TFBertModel.from_pretrained(bert_model)
  File &quot;/home/li1zz/.conda/envs/tensorflow-gpu/lib/python3.6/site-packages/transformers/modeling_tf_utils.py&quot;, line 437, in from_pretrained
    [WEIGHTS_NAME, TF2_WEIGHTS_NAME], pretrained_model_name_or_path
OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5'] found in directory /home/li1zz/bert/lm_finetune_proddesc/lm_finetune_part00 or `from_pt` set to False
</code></pre>
<p>Obviously, the file that is missing now is</p>
<pre><code>tf_model.h5
</code></pre>
<p>I don't understand how I had this error - the fine-tuned models worked for sure before. And the only thing I can think of is the version mismatch. I.e., I fine-tuned those models using a version of the libraries that are incompatible with the ones I am now using, as one file is missing.</p>
<p>Can anyone provide some insights to this? Am I using wrong versions of the libraries? How can I fix this without re-doing all the language model finetuning using this new environment again?</p>
","1783398","","","","","2020-12-10 05:21:07","Loading pre-trained BERT model error - Error no file named ['pytorch_model.bin', 'tf_model.h5'] found","<huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"62561471","1","","","2020-06-24 18:09:19","","3","2144","<p>I've successfully used the <a href=""https://huggingface.co/transformers/model_doc/bert.html"" rel=""nofollow noreferrer"">Huggingface Transformers BERT model</a> to do sentence classification  using the <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">BERTForSequenceClassification</a> class and API. I've used it for both 1-sentence sentiment analysis and 2-sentence NLI.</p>
<p>I can see that other models have analogous classes, e.g. <a href=""https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforsequenceclassification"" rel=""nofollow noreferrer"">XLNetForSequenceClassification</a> and <a href=""https://huggingface.co/transformers/model_doc/roberta.html#robertaforsequenceclassification"" rel=""nofollow noreferrer"">RobertaForSequenceClassification</a>. This type of sentence classification usually involves placing a classifier layer on top of a dense vector representing the entirety of the sentence.</p>
<p>Now I'm trying to use the <a href=""https://huggingface.co/transformers/model_doc/gpt2.html"" rel=""nofollow noreferrer"">GPT2</a> and <a href=""https://huggingface.co/transformers/model_doc/t5.html"" rel=""nofollow noreferrer"">T5</a> models. However, when I look at the available classes and API for each one, there is no equivalent &quot;ForSequenceClassification&quot; class. For example, for GPT2 there are <a href=""https://huggingface.co/transformers/model_doc/gpt2.html#gpt2model"" rel=""nofollow noreferrer"">GPT2Model</a>, <a href=""https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel"" rel=""nofollow noreferrer"">GPT2LMHeadModel</a>, and <a href=""https://huggingface.co/transformers/model_doc/gpt2.html#gpt2doubleheadsmodel"" rel=""nofollow noreferrer"">GPT2DoubleHeadsModel</a> classes. Perhaps I'm not familiar enough with the research for GPT2 and T5, but I'm certain that both models are capable of sentence classification.</p>
<p>So my questions are:</p>
<ol>
<li><p>What Huggingface classes for GPT2 and T5 should I use for 1-sentence classification?</p>
</li>
<li><p>What classes should I use for 2-sentence (sentence pair) classification (like natural language inference)?</p>
</li>
</ol>
<p>Thank you for any help.</p>
","4561314","","","","","2020-07-02 21:28:53","Huggingface GPT2 and T5 model APIs for sentence classification?","<python><machine-learning><nlp><pytorch><huggingface-transformers>","2","2","2","","","CC BY-SA 4.0"
"63017931","1","","","2020-07-21 15:30:09","","3","2609","<p>To speed up performace I looked into pytorches <a href=""https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html"" rel=""nofollow noreferrer"">DistributedDataParallel</a> and tried to apply it to transformer <a href=""https://huggingface.co/transformers/main_classes/trainer.html"" rel=""nofollow noreferrer"">Trainer</a>.</p>
<p>The <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel"" rel=""nofollow noreferrer"">pytorch examples for DDP</a> states that this should <strong>at least</strong> be faster:</p>
<blockquote>
<p>DataParallel is single-process, multi-thread, and only works on a single machine, while DistributedDataParallel is multi-process and works for both single- and multi- machine training. DataParallel is usually slower than DistributedDataParallel even on a single machine due to GIL contention across threads, per-iteration replicated model, and additional overhead introduced by scattering inputs and gathering outputs.</p>
</blockquote>
<p>My DataParallel trainer looks like this:</p>
<pre><code>import os
from datetime import datetime
import sys
import torch
from transformers import Trainer, TrainingArguments, BertConfig

training_args = TrainingArguments(
        output_dir=os.path.join(path_storage, 'results', &quot;mlm&quot;),  # output directory
        num_train_epochs=1,  # total # of training epochs
        gradient_accumulation_steps=2,  # for accumulation over multiple steps
        per_device_train_batch_size=4,  # batch size per device during training
        per_device_eval_batch_size=4,  # batch size for evaluation
        logging_dir=os.path.join(path_storage, 'logs', &quot;mlm&quot;),  # directory for storing logs
        evaluate_during_training=False,
        max_steps=20,
    )

mlm_train_dataset = ProteinBertMaskedLMDataset(
        path_vocab, os.path.join(path_storage, &quot;data&quot;, &quot;uniparc&quot;, &quot;uniparc_train_sorted.h5&quot;),
)

mlm_config = BertConfig(
        vocab_size=mlm_train_dataset.tokenizer.vocab_size,
        max_position_embeddings=mlm_train_dataset.input_size
)
mlm_model = ProteinBertForMaskedLM(mlm_config)
trainer = Trainer(
   model=mlm_model,  # the instantiated ðŸ¤— Transformers model to be trained
   args=training_args,  # training arguments, defined above
   train_dataset=mlm_train_dataset,  # training dataset
   data_collator=mlm_train_dataset.collate_fn,
)
print(&quot;build trainer with on device:&quot;, training_args.device, &quot;with n gpus:&quot;, training_args.n_gpu)
start = datetime.now()
trainer.train()
print(f&quot;finished in {datetime.now() - start} seconds&quot;)
</code></pre>
<p>The output:</p>
<pre><code>build trainer with on device: cuda:0 with n gpus: 4
finished in 0:02:47.537038 seconds
</code></pre>
<p>My DistributedDataParallel trainer is build like this:</p>
<pre><code>def create_transformer_trainer(rank, world_size, train_dataset, model):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    os.environ[&quot;RANK&quot;] = str(rank)
    os.environ[&quot;WORLD_SIZE&quot;] = str(world_size)

    training_args = TrainingArguments(
        output_dir=os.path.join(path_storage, 'results', &quot;mlm&quot;),  # output directory
        num_train_epochs=1,  # total # of training epochs
        gradient_accumulation_steps=2,  # for accumulation over multiple steps
        per_device_train_batch_size=4,  # batch size per device during training
        per_device_eval_batch_size=4,  # batch size for evaluation
        logging_dir=os.path.join(path_storage, 'logs', &quot;mlm&quot;),  # directory for storing logs
        local_rank=rank,
        max_steps=20,
    )

    trainer = Trainer(
        model=model,  # the instantiated ðŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
        train_dataset=train_dataset,  # training dataset
        data_collator=train_dataset.collate_fn,
    )
    print(&quot;build trainer with on device:&quot;, training_args.device, &quot;with n gpus:&quot;, training_args.n_gpu)
    start = datetime.now()
    trainer.train()
    print(f&quot;finished in {datetime.now() - start} seconds&quot;)


mlm_train_dataset = ProteinBertMaskedLMDataset(
    path_vocab, os.path.join(path_storage, &quot;data&quot;, &quot;uniparc&quot;, &quot;uniparc_train_sorted.h5&quot;))

mlm_config = BertConfig(
    vocab_size=mlm_train_dataset.tokenizer.vocab_size,
    max_position_embeddings=mlm_train_dataset.input_size
)
mlm_model = ProteinBertForMaskedLM(mlm_config)
torch.multiprocessing.spawn(create_transformer_trainer,
     args=(4, mlm_train_dataset, mlm_model),
     nprocs=4,
     join=True)
</code></pre>
<p>The output:</p>
<pre><code>The current process just got forked. Disabling parallelism to avoid deadlocks...
To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
The current process just got forked. Disabling parallelism to avoid deadlocks...
To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
The current process just got forked. Disabling parallelism to avoid deadlocks...
To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
The current process just got forked. Disabling parallelism to avoid deadlocks...
To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
The current process just got forked. Disabling parallelism to avoid deadlocks...
To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
build trainer with on device: cuda:1 with n gpus: 1
build trainer with on device: cuda:2 with n gpus: 1
build trainer with on device: cuda:3 with n gpus: 1
build trainer with on device: cuda:0 with n gpus: 1
finished in 0:04:15.937331 seconds
finished in 0:04:16.899411 seconds
finished in 0:04:16.938141 seconds
finished in 0:04:17.391887 seconds
</code></pre>
<p>About the inital forking warning: What is exaclty forked and is this expected?</p>
<p>And about the resulting time: Is the trainer incorrectly used since it seemed to be a lot slower than the DataParallel approach?</p>
","13878065","","","","","2021-05-05 09:37:38","using huggingface Trainer with distributed data parallel","<python><pytorch><huggingface-transformers>","1","8","2","","","CC BY-SA 4.0"
"65082609","1","","","2020-11-30 23:26:09","","0","342","<p>while trying to run the example of hugging face for DeBERTa :</p>
<pre><code>from transformers import DebertaTokenizer, DebertaModel
import torch

tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')
model = DebertaModel.from_pretrained('microsoft/deberta-base')

inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
</code></pre>
<p>in the <a href=""https://huggingface.co/transformers/model_doc/deberta.html"" rel=""nofollow noreferrer"">DeBERTa</a> hitting into this problem :</p>
<pre><code>&gt;&gt;&gt; outputs = model(**inputs)
/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/transformers/modeling_deberta.py:574: UserWarning: Output 0 of SplitBackward is a view and is being modified inplace. This view is an output of a function that returns multiple views. Inplace operators on such views are being deprecated and will be forbidden starting from version 1.8. Consider using `unsafe_` version of the function that produced this view or don't modify this view inplace. (Triggered internally at  /pytorch/torch/csrc/autograd/variable.cpp:491.)
  query_layer += self.transpose_for_scores(self.q_bias[None, None, :])
/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/transformers/modeling_deberta.py:575: UserWarning: Output 2 of SplitBackward is a view and is being modified inplace. This view is an output of a function that returns multiple views. Inplace operators on such views are being deprecated and will be forbidden starting from version 1.8. Consider using `unsafe_` version of the function that produced this view or don't modify this view inplace. (Triggered internally at  /pytorch/torch/csrc/autograd/variable.cpp:491.)
  value_layer += self.transpose_for_scores(self.v_bias[None, None, :])
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/transformers/modeling_deberta.py&quot;, line 896, in forward
    return_dict=return_dict,
  File &quot;/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/transformers/modeling_deberta.py&quot;, line 407, in forward
    rel_embeddings=rel_embeddings,
  File &quot;/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/transformers/modeling_deberta.py&quot;, line 330, in forward
    rel_embeddings=rel_embeddings,
  File &quot;/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/transformers/modeling_deberta.py&quot;, line 263, in forward
    rel_embeddings=rel_embeddings,
  File &quot;/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/m/anaconda3/envs/cc35/lib/python3.7/site-packages/transformers/modeling_deberta.py&quot;, line 575, in forward
    value_layer += self.transpose_for_scores(self.v_bias[None, None, :])
RuntimeError: diff_view_meta-&gt;output_nr_ == 0 INTERNAL ASSERT FAILED at &quot;/pytorch/torch/csrc/autograd/variable.cpp&quot;:363, please report a bug to PyTorch.

</code></pre>
<p>the environment has python 3.7 with anaconda :</p>
<pre><code>torch 1.7.0
transformer 3.4.0
torchvision 0.8.1

</code></pre>
<p>2 GPUs are available for me to run the sample as after searching around suggestions was to run with one GPU it might solve the issue, But I have tried it using only one GPU with this command: <code>CUDA_VISIBLE_DEVICES=0 python sample_DeBERTa.py</code>, but still got the same error.</p>
","3926442","","6664872","","2020-12-04 20:22:29","2020-12-04 20:22:29","RuntimeError: diff_view_meta->output_nr_ == 0 INTERNAL ASSERT FAILED hugginface DeBERTa","<python><nlp><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","1","","","","CC BY-SA 4.0"
"65117817","1","","","2020-12-02 23:45:30","","1","291","<p>I am trying to run this my model on Colab Multi core TPU but I really don't know how to do it. I tried <a href=""https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/multi-core-alexnet-fashion-mnist.ipynb"" rel=""nofollow noreferrer"">this tutorial notebook</a> but I got some error and I can't fix it but I think there is maybe simpler wait for to do it.</p>
<p>About my model:</p>
<pre><code>class BERTModel(nn.Module):
    def __init__(self,...):
        super().__init__()
        if ...:
            self.bert_model = XLMRobertaModel.from_pretrained(...)   # huggingface XLM-R
        elif ...:
            self.bert_model = others_model.from_pretrained(...)   # huggingface XLM-R
        
        ... # some other model's parameters
        
    def forward(self,...):
        bert_input = ...
        output = self.bert_model(bert_input)
        
        ... # some function that process on output
        
    def other_function(self,...):
        # just doing some process on output. like concat layers's embedding and return ...
        
class MAINModel(nn.Module):
    def __init__(self,...):
        super().__init__()
        
        print('Using model 1')
        self.bert_model_1 = BERTModel(...)
        
        print('Using model 2')
        self.bert_model_2 = BERTModel(...)
        
        self.linear = nn.Linear(...)
        
    def forward(self,...):
        bert_input = ...
        bert_output = self.bert_model(bert_input)
        linear_output = self.linear(bert_output)
   
        return linear_output
</code></pre>
<p>Can you please tell me how to run a model like my model on Colab TPU? I used Colab PRO to make sure Ram memory is not a big problem. Thanks you so so much.</p>
","10636733","","8841057","","2020-12-03 01:23:59","2020-12-03 22:17:02","Run Pytorch stacked model on Colab TPU","<pytorch><google-colaboratory><huggingface-transformers><tpu><google-cloud-tpu>","1","1","1","","","CC BY-SA 4.0"
"65132144","1","65226930","","2020-12-03 18:42:50","","8","2312","<p>I'm following <a href=""https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"" rel=""noreferrer"">this</a> tutorial that codes a sentiment analysis classifier using BERT with the <a href=""https://huggingface.co/"" rel=""noreferrer"">huggingface</a> library and I'm having a very odd behavior. When trying the BERT model with a sample text I get a string instead of the hidden state. This is the code I'm using:</p>
<pre><code>import transformers
from transformers import BertModel, BertTokenizer

print(transformers.__version__)

PRE_TRAINED_MODEL_NAME = 'bert-base-cased'
PATH_OF_CACHE = &quot;/home/mwon/data-mwon/paperChega/src_classificador/data/hugingface&quot;

tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,cache_dir = PATH_OF_CACHE)

sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'

encoding_sample = tokenizer.encode_plus(
  sample_txt,
  max_length=32,
  add_special_tokens=True, # Add '[CLS]' and '[SEP]'
  return_token_type_ids=False,
  padding=True,
  truncation = True,
  return_attention_mask=True,
  return_tensors='pt',  # Return PyTorch tensors
)

bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,cache_dir = PATH_OF_CACHE)


last_hidden_state, pooled_output = bert_model(
  encoding_sample['input_ids'],
  encoding_sample['attention_mask']
)

print([last_hidden_state,pooled_output])
</code></pre>
<p>that outputs:</p>
<pre><code>4.0.0
['last_hidden_state', 'pooler_output']
 
</code></pre>
","710734","","","","","2020-12-10 05:08:58","BertModel transformers outputs string instead of tensor","<bert-language-model><huggingface-transformers><huggingface-tokenizers>","2","0","2","","","CC BY-SA 4.0"
"65140400","1","65172846","","2020-12-04 08:42:56","","2","1579","<p>Trying to convert a <code>question-generation</code> t5 model to <code>torchscript</code> <a href=""http://%5Bmodel%5D(https://huggingface.co/transformers/torchscript.html#torchscript)."" rel=""nofollow noreferrer"">model</a>, while doing that Running into this error</p>
<p><strong>ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds</strong></p>
<p>here's the code that I ran on colab.</p>
<pre><code>!pip install -U transformers==3.0.0
!python -m nltk.downloader punkt

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

model = AutoModelForSeq2SeqLM.from_pretrained('valhalla/t5-base-qg-hl')

t_input =  'Python is a programming language. It is developed by &lt;hl&gt; Guido Van Rossum &lt;hl&gt;. &lt;/s&gt;'

tokenizer = AutoTokenizer.from_pretrained('valhalla/t5-base-qg-hl', return_tensors = 'pt')

def _tokenize(
    inputs,
    padding=True,
    truncation=True,
    add_special_tokens=True,
    max_length=64
):
    inputs = tokenizer.batch_encode_plus(
        inputs, 
        max_length=max_length,
        add_special_tokens=add_special_tokens,
        truncation=truncation,
        padding=&quot;max_length&quot; if padding else False,
        pad_to_max_length=padding,
        return_tensors=&quot;pt&quot;
    )
    return inputs

token = _tokenize(t_input, padding=True, truncation=True)


traced_model = torch.jit.trace(model, [token['input_ids'], token['attention_mask']] )
torch.jit.save(traced_model, &quot;traced_t5.pt&quot;)
</code></pre>
<p>got this error</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-1-f9b449524ef1&gt; in &lt;module&gt;()
     32 
     33 
---&gt; 34 traced_model = torch.jit.trace(model, [token['input_ids'], token['attention_mask']] )
     35 torch.jit.save(traced_model, &quot;traced_t5.pt&quot;)

7 frames
/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_value_states, use_cache, output_attentions, output_hidden_states)
    682         else:
    683             if self.is_decoder:
--&gt; 684                 raise ValueError(&quot;You have to specify either decoder_input_ids or decoder_inputs_embeds&quot;)
    685             else:
    686                 raise ValueError(&quot;You have to specify either input_ids or inputs_embeds&quot;)

ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds
</code></pre>
<p>how to resolve this issue? or is there a better way for converting the t5 model to <code>torchscript</code>.</p>
<p>thank you.</p>
","13273054","","","","","2021-03-31 15:46:25","ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds","<python><deep-learning><huggingface-transformers><torchscript>","1","2","","","","CC BY-SA 4.0"
"65034771","1","65038457","","2020-11-27 09:19:44","","4","1547","<p>I am using the Scibert pretrained model to get embeddings for various texts. The code is as follows:</p>
<pre><code>from transformers import *

tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', model_max_length=512, truncation=True)
model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')
</code></pre>
<p>I have added both the max length and truncation parameters to tokenizers, but unfortunately, they don't truncate the results.If I run a longer text through the tokenizer:</p>
<pre><code>inputs = tokenizer(&quot;&quot;&quot;long text&quot;&quot;&quot;)
</code></pre>
<p>I get the following error:</p>
<blockquote>
<p>Token indices sequence length is longer than the specified maximum
sequence length for this model (605 &gt; 512). Running this sequence
through the model will result in indexing errors</p>
</blockquote>
<p>Now obviously I can't run this through the model due to having too long sequences of tensors. What is the easiest way to truncate the input to fit the maximum sequence length of 512?</p>
","6692895","","","","","2020-11-27 13:48:13","How to truncate a Bert tokenizer in Transformers library","<python><nlp><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65079466","1","","","2020-11-30 18:53:23","","0","189","<p>I'm trying to fine tuning Bert for document classification.</p>
<p>I started by tokenizing the documents to generate the <code>input_ids</code>, <code>attention_mask</code> and <code>token_type_ids</code> lists to feed my TFBertModel:</p>
<pre><code>def tokenize_sequences(tokenizer, max_length, corpus):
    input_ids = []
    token_type_ids = []
    attention_masks = []

    for i in tqdm(range(len(corpus))):
      encoded = tokenizer.encode_plus(
          corpus[i], 
          max_length=max_length, 
          add_special_tokens=True,
          padding='max_length',
          truncation=True,
          return_token_type_ids=True,
          return_attention_mask=True,  # add attention mask to not focus on pad tokens)
      )
      input_ids.append(encoded[&quot;input_ids&quot;])
      attention_masks.append(encoded[&quot;attention_mask&quot;])
      token_type_ids.append(encoded[&quot;token_type_ids&quot;])

    input_ids = tf.convert_to_tensor(input_ids)
    attention_masks = tf.convert_to_tensor(attention_masks)
    token_type_ids = tf.convert_to_tensor(token_type_ids)
    #print(input_ids.shape, attention_masks.shape, token_type_ids.shape)

    return [input_ids, attention_masks, token_type_ids]
</code></pre>
<p>Then, I tried to fit my model:</p>
<pre><code>x_train = tokenize_sequences(tokenizer, MAXLEN, corpus_train)
model = loadBertModel()

model.fit(
    x_train, y_bin_train,
    epochs=N_EPOCHS,
    verbose=1,
    batch_size=4, 
  )
</code></pre>
<p>And I get this error:</p>
<blockquote>
<p>InvalidArgumentError: indices[3] = [1,5] is out of order. Many sparse ops require sorted indices.
Use <code>tf.sparse.reorder</code> to create a correctly ordered copy.</p>
</blockquote>
<p>I tried to solve the issue following <a href=""https://stackoverflow.com/questions/61961042/indices201-0-8-is-out-of-order-many-sparse-ops-require-sorted-indices-use"">this</a> suggestion. I did this by modifying <code>input_ids, attention_masks, token_type_ids</code> tensors returned by <code>tokenize_sequences</code>.</p>
<pre><code>input_ids = tf.sparse.reorder(input_ids)
attention_masks = tf.sparse.reorder(attention_masks)
token_type_ids = tf.sparse.reorder(token_type_ids)
</code></pre>
<p>But then another error occurred:</p>
<blockquote>
<p>TypeError: Input must be a SparseTensor.</p>
</blockquote>
<p>PS: When I checked the type of my tensors, I noticed that they were <code>&lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;</code>.</p>
<p>Any ideas on how to solve this?</p>
","5936342","","","","","2020-11-30 18:53:23","Tensorflow sparse ops require sorted indices","<python-3.x><tensorflow><keras><bert-language-model><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"65145526","1","","","2020-12-04 14:37:53","","1","311","<p>I'm currently trying to fine-tune DistilGPT-2 (with Pytorch and HuggingFace transformers library) for a code completion task. My corpus is arranged like the following example:</p>
<pre><code>&lt;|startoftext|&gt;
public class FindCityByIdService {
    private CityRepository cityRepository = ...
&lt;|endoftext|&gt;
</code></pre>
<p>My first attempt was to run the following <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py"" rel=""nofollow noreferrer"">script</a> from transformers library:</p>
<pre><code>python run_clm.py 
     --model_type=gpt2 \
     --model_name_or_path distilgpt2 \
     --do_train \
     --train_file $TRAIN_FILE \
     --num_train_epochs 100 \
     --output_dir $OUTPUT_DIR \
     --overwrite_output_dir \
     --save_steps 20000 \
     --per_device_train_batch_size 4 \
</code></pre>
<p>After doing some generation tests, I realized that the model is not predicting <code>\ n</code> for any given context. I imagine that some pre-process stage or something similar is missing. But anyway, what should I do so that <code>\ n</code> be predicted as expected?</p>
<p><a href=""https://discuss.huggingface.co/t/why-new-lines-arent-generated/2543"" rel=""nofollow noreferrer"">HF Forum question</a></p>
<p>Thanks!!</p>
","9291922","","","","","2020-12-13 05:29:01","Why new lines aren't generated with my fine-tuned DistilGPT2 model?","<pytorch><huggingface-transformers><gpt-2>","1","2","","","","CC BY-SA 4.0"
"65023526","1","65052927","","2020-11-26 14:01:21","","2","3257","<p>I'm trying to build a model for document classification. I'm using <code>BERT</code> with <code>PyTorch</code>.</p>
<p>I got the bert model with below code.</p>
<pre><code>bert = AutoModel.from_pretrained('bert-base-uncased')
</code></pre>
<p>This is the code for training.</p>
<pre><code>for epoch in range(epochs):
 
    print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))

    #train model
    train_loss, _ = modhelper.train(proc.train_dataloader)

    #evaluate model
    valid_loss, _ = modhelper.evaluate()

    #save the best model
    if valid_loss &lt; best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(modhelper.model.state_dict(), 'saved_weights.pt')

    # append training and validation loss
    train_losses.append(train_loss)
    valid_losses.append(valid_loss)

    print(f'\nTraining Loss: {train_loss:.3f}')
    print(f'Validation Loss: {valid_loss:.3f}')
</code></pre>
<p>this is my train method, accessible with the object <code>modhelper</code>.</p>
<pre><code>def train(self, train_dataloader):
    self.model.train()
    total_loss, total_accuracy = 0, 0
    
    # empty list to save model predictions
    total_preds=[]
    
        # iterate over batches
    for step, batch in enumerate(train_dataloader):
        
        # progress update after every 50 batches.
        if step % 50 == 0 and not step == 0:
            print('  Batch {:&gt;5,}  of  {:&gt;5,}.'.format(step, len(train_dataloader)))
        
        # push the batch to gpu
        #batch = [r.to(device) for r in batch]
        
        sent_id, mask, labels = batch
        
        # clear previously calculated gradients 
        self.model.zero_grad()        

        print(sent_id.size(), mask.size())
        # get model predictions for the current batch
        preds = self.model(sent_id, mask) #This line throws the error
        
        # compute the loss between actual and predicted values
        self.loss = self.cross_entropy(preds, labels)
        
        # add on to the total loss
        total_loss = total_loss + self.loss.item()
        
        # backward pass to calculate the gradients
        self.loss.backward()
        
        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        
        # update parameters
        self.optimizer.step()
        
        # model predictions are stored on GPU. So, push it to CPU
        #preds=preds.detach().cpu().numpy()
        
        # append the model predictions
        total_preds.append(preds)
      
    # compute the training loss of the epoch
    avg_loss = total_loss / len(train_dataloader)
    
    # predictions are in the form of (no. of batches, size of batch, no. of classes).
    # reshape the predictions in form of (number of samples, no. of classes)
    total_preds  = np.concatenate(total_preds, axis=0)
      
    #returns the loss and predictions
    return avg_loss, total_preds
</code></pre>
<p><code>preds = self.model(sent_id, mask)</code> this line throws the following error(including full traceback).</p>
<pre><code> Epoch 1 / 1
torch.Size([32, 4000]) torch.Size([32, 4000])
Traceback (most recent call last):

File &quot;&lt;ipython-input-39-17211d5a107c&gt;&quot;, line 8, in &lt;module&gt;
train_loss, _ = modhelper.train(proc.train_dataloader)

File &quot;E:\BertTorch\model.py&quot;, line 71, in train
preds = self.model(sent_id, mask)

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\model.py&quot;, line 181, in forward
#pass the inputs to the model

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\venv\lib\site-packages\transformers\modeling_bert.py&quot;, line 837, in forward
embedding_output = self.embeddings(

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\venv\lib\site-packages\transformers\modeling_bert.py&quot;, line 201, in forward
embeddings = inputs_embeds + position_embeddings + token_type_embeddings

RuntimeError: The size of tensor a (4000) must match the size of tensor b (512) at non-singleton dimension 1
</code></pre>
<p>If you observe I've printed the torch size in the code.
<code>print(sent_id.size(), mask.size())</code></p>
<p>The output of that line of code is <code>torch.Size([32, 4000]) torch.Size([32, 4000])</code>.</p>
<p>as we can see that size is the same but it throws the error. Please put your thoughts. Really appreciate it.</p>
<p>please comment if you need further information. I'll be quick to add whatever is required.</p>
","8340027","","6664872","","2020-11-27 22:47:26","2020-11-28 18:08:29","RuntimeError: The size of tensor a (4000) must match the size of tensor b (512) at non-singleton dimension 1","<python><deep-learning><pytorch><bert-language-model><huggingface-transformers>","1","6","1","","","CC BY-SA 4.0"
"65085934","1","","","2020-12-01 06:52:20","","0","56","<p>I have used <code>BERT</code> with <code>HuggingFace</code> and <code>PyTorch</code> and used <code>DataLoader</code>, <code>Serializer</code> for <strong>Training &amp; Evaluation</strong>. Below is the code for that:</p>
<pre><code>! pip install transformers==3.5.1
from transformers import AutoModel, BertTokenizerFast

bert = AutoModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')


def textToTensor(text,labels=None,paddingLength=30):
  
  tokens = tokenizer.batch_encode_plus(text.tolist(), max_length=paddingLength, padding='max_length', truncation=True)
  
  text_seq = torch.tensor(tokens['input_ids'])
  text_mask = torch.tensor(tokens['attention_mask'])

  text_y = None
  if isinstance(labels,np.ndarray): # if we do not have y values
    text_y = torch.tensor(labels.tolist())

  return text_seq, text_mask, text_y


text = test_df['text'].values

seq,mask,_ = textToTensor(text,paddingLength=35)
data = TensorDataset(seq,mask)
dataloader = DataLoader(data,batch_size=1)

for step,batch in enumerate(dataloader):
  batch = [t.to(device) for t in batch]
  sent_id, mask = batch

  with torch.no_grad():
    print(np.argmax(model(sent_id, mask).detach().cpu().numpy(),1))
</code></pre>
<p>It gives me a <code>numpy array</code> as a result and since the <code>batch_size=1</code> and No <code>Serializer</code> is used in this one, I am getting results as single array number as class prediction.</p>
<p>I have two questions:</p>
<p><strong>Are the results strictly according to the index of <code>df['text']</code></strong>?</p>
<p>**How can I get the predictions for a single sentence like <strong>Hello make my prediction. I am a single sentence</strong>?</p>
<p>Can someone please help me making a single prediction?</p>
","11725056","","","","","2020-12-01 06:52:20","how to get Single text prediction from the CustomisedBERT Classification + PyTorch NLP model with/without DataLoader","<pytorch><huggingface-transformers><dataloader><huggingface-tokenizers>","0","6","","","","CC BY-SA 4.0"
"65159768","1","","","2020-12-05 17:15:55","","2","556","<p>I have a question regarding &quot;on-the-fly&quot; tokenization. This question was elicited by reading the &quot;How to train a new language model from scratch using Transformers and Tokenizers&quot; <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">here</a>. Towards the end there is this sentence: &quot;If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step&quot;. I've tried coming up with a solution that would combine both <code>datasets</code> and <code>tokenizers</code>, but did not manage to find a good pattern.</p>
<p>I guess the solution would entail wrapping a dataset into a Pytorch dataset.</p>
<p>As a concrete example from the <a href=""https://huggingface.co/transformers/custom_datasets.html"" rel=""nofollow noreferrer"">docs</a></p>
<pre class=""lang-py prettyprint-override""><code>import torch

class SquadDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        # instead of doing this beforehand, I'd like to do tokenization on the fly
        self.encodings = encodings 

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)

train_dataset = SquadDataset(train_encodings)
</code></pre>
<p>How would one implement this with &quot;on-the-fly&quot; tokenization exploiting the vectorized capabilities of tokenizers?</p>
","9824768","","9824768","","2020-12-08 21:57:02","2021-03-04 09:49:05","On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders","<huggingface-transformers><huggingface-tokenizers><gpt-2>","1","0","1","","","CC BY-SA 4.0"
"65167131","1","","","2020-12-06 10:47:29","","2","378","<p>I have trained a EncoderDecoderModel from huggging face to do english-German translation task. I tried to overfit a small dataset (100 parallel sentences), and use <code>model.generate()</code> then <code>tokenizer.decode()</code> to perform the translation. However, the output seems to be proper German sentences, but it is definitely not the correct translation.</p>
<p>Here are the code for building the model</p>
<pre><code>encoder_config = BertConfig()
decoder_config = BertConfig()
config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)
model = EncoderDecoderModel(config=config)
</code></pre>
<p>Here are the code for testing the model</p>
<pre><code>model.eval()
input_ids = torch.tensor(tokenizer.encode(input_text)).unsqueeze(0)
output_ids = model.generate(input_ids.to('cuda'), decoder_start_token_id=model.config.decoder.pad_token_id)
output_text = tokenizer.decode(output_ids[0])
</code></pre>
<p>Example input: &quot;iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould .&quot;</p>
<p>Ground truth translation: &quot;iron cement ist eine gebrauchs ##AT##-##AT## fertige Paste , die mit einem Spachtel oder den Fingern als Hohlkehle in die Formecken ( Winkel ) der Stahlguss -Kokille aufgetragen wird .&quot;</p>
<p>What the model outputs after trained 100 epochs: &quot;[S] wenn sie den unten stehenden link anklicken, sehen sie ein video uber die erstellung ansprechender illustrationen in quarkxpress&quot; which is totally nonesense.</p>
<p>Where is the problem?</p>
","7759152","","","","","2020-12-06 10:47:29","transformers: how to use hugging face EncoderDecoderModel to do machine translation task?","<python><pytorch><huggingface-transformers><transformer><machine-translation>","0","4","","","","CC BY-SA 4.0"
"67326772","1","","","2021-04-30 00:39:18","","0","157","<p>I am using Windows 10 and pip installed the latest allennlp branch. When I successfully install the package, I encountered the following:</p>
<pre><code>$ allennlp test-install

Traceback (most recent call last):
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;C:\Users\604194\AppData\Local\Continuum\anaconda3\envs\domains\Scripts\allennlp.exe\__main__.py&quot;, line 4, in &lt;module&gt;
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\run.py&quot;, line 15, in &lt;module&gt;
    from allennlp.commands import main  # pylint: disable=wrong-import-position
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\commands\__init__.py&quot;, line 8, in &lt;module&gt;
    from allennlp.commands.configure import Configure
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\commands\configure.py&quot;, line 27, in &lt;module&gt;
    from allennlp.service.config_explorer import make_app
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\service\config_explorer.py&quot;, line 24, in &lt;module&gt;
    from allennlp.common.configuration import configure, choices
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\common\__init__.py&quot;, line 1, in &lt;module&gt;
    from allennlp.common.from_params import FromParams
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\common\from_params.py&quot;, line 48, in &lt;module&gt;
    from allennlp.common.params import Params
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\common\params.py&quot;, line 173, in &lt;module&gt;
    class Params(MutableMapping):
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\common\params.py&quot;, line 236, in Params
    def pop(self, key: str, default: Any = DEFAULT) -&gt; Any:
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\overrides\overrides.py&quot;, line 67, in overrides
    return _overrides(method, check_signature, check_at_runtime)
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\overrides\overrides.py&quot;, line 93, in _overrides
    _validate_method(method, super_class, check_signature)
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\overrides\overrides.py&quot;, line 114, in _validate_method
    ensure_signature_is_compatible(super_method, method, is_static)
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\overrides\signature.py&quot;, line 87, in ensure_signature_is_compatible
    super_sig, sub_sig, super_type_hints, sub_type_hints, is_static, method_name
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\overrides\signature.py&quot;, line 213, in ensure_all_positional_args_defined_in_sub
    f&quot;`{method_name}: {sub_param.name} must be a supertype of `{super_param.annotation}` but is `{sub_param.annotation}`&quot;
TypeError: `Params.pop: key must be a supertype of `&lt;class 'inspect._empty'&gt;` but is `&lt;class 'str'&gt;`
</code></pre>
<p>Any advice on how to resolve this issue? Thanks!</p>
<p>EDIT: Edited my question for more information if needed. Thanks!</p>
","7779709","","7779709","","2021-04-30 00:55:06","2021-04-30 00:55:06","Encountering type error handling when running allennlp test-install","<huggingface-transformers><allennlp>","0","4","","","","CC BY-SA 4.0"
"67328345","1","","","2021-04-30 04:49:44","","0","96","<p>For my use case, I need to use the model.forward() instead of the model.generate() method
i.e instead of the below code</p>
<pre><code>outs = model.model.generate(input_ids=batch['source_ids'],
                                 attention_mask=batch['source_mask'],
                                 output_scores=True,
                                 max_length=model.model_arguments.max_output_seq_length)

preds_cleaned = [model.tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True) for ids in outs]
</code></pre>
<p>I need to use</p>
<pre><code>model_outputs = model.model(
            input_ids=batch[&quot;source_ids&quot;],
            attention_mask=batch[&quot;source_mask&quot;],
            labels=lm_labels.to(device),
            decoder_attention_mask=batch['target_mask']
        )
logits = model_outputs.logits
softmax_logits = m(logits)
max_logits = torch.max(softmax_logits, dim=2)

    
</code></pre>
<p>decoding these logits gives unprocessed text that has many issues like repetition of words at the end etc.
What do I need to do to get the same result as model.generate() ?</p>
","6392022","","","","","2021-04-30 07:48:29","How to use forward() method instead of model.generate() for T5 model","<nlp><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65221079","1","65228537","","2020-12-09 16:43:06","","1","1129","<p>Being new to the &quot;Natural Language Processing&quot; scene, I am experimentally learning and have implemented the following segment of code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch
    
path = &quot;D:/LM/rb/&quot;
tokenizer = RobertaTokenizer.from_pretrained(path)
model = RobertaForSequenceClassification.from_pretrained(path)
    
inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
pred_logits = outputs.logits
print(pred_logits)
probs = pred_logits.softmax(dim=-1).detach().cpu().flatten().numpy().tolist()
print(probs)
</code></pre>
<p>I <em>understand</em> that applying the model returns a <em>&quot;<code>torch.FloatTensor</code> comprising various elements depending on the configuration (RobertaConfig) and inputs&quot;</em>, and that the logits are accessible using <code>.logits</code>. As demonstrated I have applied the <em>.softmax</em> function to the tensor to return normalised probabilities and have converted the result into a list. I am outputted with the following:</p>
<pre><code>[0.5022980570793152, 0.49770188331604004]
</code></pre>
<p>Do these probabilities represent some kind of overall &quot;masked&quot; probability?</p>
<p><strong>What do the first and second index represent in context of the input?</strong></p>
<hr />
<p>EDIT:</p>
<pre><code>model.num_labels
</code></pre>
<p>Output:</p>
<pre><code>2
</code></pre>
<p><a href=""https://stackoverflow.com/users/6664872/cronoik"">@cronoik</a> explains that the model &quot;tries to classify if a sequence belongs to one class or another&quot;</p>
<p>Am I to assume that because there are no trained output layers these classes don't mean anything yet?</p>
<p>For example, I <em>can</em> assume that the probability that the sentence, post analysis, belongs to class 1 is 0.5. However, what is class 1?</p>
<p>Additionally, model cards with pre-trained output layers such as the <a href=""https://huggingface.co/roberta-large-openai-detector"" rel=""nofollow noreferrer"">open-ai detector</a> help differentiate between what is <a href=""https://github.com/openai/gpt-2-output-dataset/blob/master/detector/server.py#L46"" rel=""nofollow noreferrer"">&quot;real&quot;</a> and <a href=""https://github.com/openai/gpt-2-output-dataset/blob/master/detector/server.py#L46"" rel=""nofollow noreferrer"">&quot;fake&quot;</a>, and so I can assume the class that a sentence belongs to. However, how can I confirm these &quot;labels&quot; without some type of &quot;mapping.txt&quot; file?</p>
","5806746","","5806746","","2020-12-10 20:04:58","2020-12-10 23:53:50","What do the logits and probabilities from RobertaForSequenceClassification represent?","<python><nlp><pytorch><text-classification><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65199160","1","","","2020-12-08 12:42:43","","1","541","<p>I'm trying to create my own tokenizer with my own dataset/vocabulary using Sentencepiece and then use it with AlbertTokenizer transformers.</p>
<p>I followed really closely the tutorial on how to train a model from scratch by HuggingFace: <a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=hO5M3vrAhcuj"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=hO5M3vrAhcuj</a></p>
<pre><code>    # import relevant libraries   
    from pathlib import Path
    from tokenizers import SentencePieceBPETokenizer
    from tokenizers.implementations import SentencePieceBPETokenizer
    from tokenizers.processors import BertProcessing
    from transformers import AlbertTokenizer
    

    paths = [str(x) for x in Path(&quot;./data&quot;).glob(&quot;**/*.txt&quot;)]
    

    # Initialize a tokenizer
    tokenizer = SentencePieceBPETokenizer(add_prefix_space=True)
    
    # Customize training
    tokenizer.train(files=paths, 
                    vocab_size=32000, 
                    min_frequency=2, 
                    show_progress=True,
                    special_tokens=['&lt;unk&gt;'],)

    # Saving model
    tokenizer.save_model(&quot;Sent-AlBERT&quot;)


    tokenizer = SentencePieceBPETokenizer(
        &quot;./Sent-AlBERT/vocab.json&quot;,
        &quot;./Sent-AlBERT/merges.txt&quot;,)

    tokenizer.enable_truncation(max_length=512)
</code></pre>
<p>Everything is fine up until this point when I tried to re-create the tokenizer in transformers</p>
<pre><code>    # Re-create our tokenizer in transformers
        tokenizer = AlbertTokenizer.from_pretrained(&quot;./Sent-AlBERT&quot;, do_lower_case=True)
  
</code></pre>
<p>This is the error message I kept receiving:</p>
<pre><code>OSError: Model name './Sent-AlBERT' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed './Sent-AlBERT' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url.
</code></pre>
<p>For some reason, it works with RobertaTokenizerFast but not with AlbertTokenzier.</p>
<p>If anyone could give me a suggestion or any sort of direction on how to use Sentencepiece with AlberTokenizer I would really appreciate.</p>
","13747728","","","","","2020-12-08 12:42:43","""OSError: Model name './XX' was not found in tokenizers model name list"" - cannot load custom tokenizer in Transformers","<python><pytorch><bert-language-model><huggingface-transformers><sentencepiece>","0","2","","","","CC BY-SA 4.0"
"65249790","1","","","2020-12-11 10:43:07","","3","270","<p>I am using fastai with pytorch to fine tune XLMRoberta from huggingface.
I've trained the model and everything is fine on the machine where I trained it.</p>
<p>But when I try to load the model on another machine I get <code>OSError - Not Found - No such file or directory</code> pointing to <code>.cache/torch/transformers/</code>. The issue is the path of a <code>vocab_file</code>.</p>
<p>I've used fastai's <a href=""https://docs.fast.ai/learner.html#Learner.export"" rel=""nofollow noreferrer"">Learner.export</a> to export the model in <code>.pkl</code> file, but I don't believe that issue is related to fastai since I found <a href=""https://github.com/flairNLP/flair/issues/1747"" rel=""nofollow noreferrer"">the same issue</a> appearing in flairNLP.</p>
<p>It appears that the path to the cache folder, where the vocab_file is stored during the training, is embedded in the <code>.pkl</code> file:
<a href=""https://i.stack.imgur.com/ZJoP0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZJoP0.png"" alt=""enter image description here"" /></a></p>
<p>The error comes from transformer's <a href=""https://huggingface.co/transformers/_modules/transformers/tokenization_xlm_roberta.html"" rel=""nofollow noreferrer"">XLMRobertaTokenizer</a> <code>__setstate__</code>:</p>
<pre class=""lang-py prettyprint-override""><code>def __setstate__(self, d):
    self.__dict__ = d
    self.sp_model = spm.SentencePieceProcessor()
    self.sp_model.Load(self.vocab_file)
</code></pre>
<p>which tries to load the vocab_file using the path from the file.</p>
<p>I've tried patching this method using:</p>
<pre class=""lang-py prettyprint-override""><code>pretrained_model_name = &quot;xlm-roberta-base&quot;
vocab_file = XLMRobertaTokenizer.from_pretrained(pretrained_model_name).vocab_file

def _setstate(self, d):
    self.__dict__ = d
    self.sp_model = spm.SentencePieceProcessor()
    self.sp_model.Load(vocab_file)

XLMRobertaTokenizer.__setstate__ = MethodType(_setstate, XLMRobertaTokenizer(vocab_file))
</code></pre>
<p>And that successfully loaded the model but caused other problems like missing model attributes and other unwanted issues.</p>
<p>Can someone please explain why is the path embedded inside the file, is there a way to configure it without reexporting the model or if it has to be reexported how to configure it dynamically using fastai, torch and huggingface.</p>
","10947997","","","","","2021-09-02 07:55:47","Load trained model on another machine - fastai, torch, huggingface","<nlp><pytorch><huggingface-transformers><fast-ai>","1","3","1","","","CC BY-SA 4.0"
"65248532","1","","","2020-12-11 09:12:06","","1","196","<p>As far as I understood, the RoBERTa model implemented by the <em><a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface</a></em> library, uses <strong>BPE</strong> tokenizer. Here is the link for the documentation:</p>
<p><em><a href=""https://huggingface.co/transformers/model_doc/roberta.html"" rel=""nofollow noreferrer"">RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme</a>.</em></p>
<p>However, I have a custom tokenizer based on <strong>WordPiece</strong> tokenization and I used the <strong>BertTokenizer</strong>.</p>
<p>Because my customized tokenizer is much more relevant for my task, I prefer not to use BPE.</p>
<p>When I pre-trained the RoBERTa from scratch (RobertaForMaskedLM) with my custom tokenizer the loss for the MLM task was much better than the loss with BPE. However, when it comes to fine-tuning, the model (RobertaForSequenceClassification) perform poorly. I am almost sure the problem is not about the tokenizer. I wonder if the huggingface library for the RobertaForSequenceClassification is not compatible with my tokenizer.</p>
<p>Details about the fine-tuning:</p>
<p>task: multilabel classification with imbalanced labels.</p>
<p>epochs: 20</p>
<p>loss: BCEWithLogitsLoss()</p>
<p>optimizer: Adam, weight_decay_rate:0.01, lr: 2e-5, correct_bias: True</p>
<p>The F1 and AUC was very low because the output probabilities for the labels was not in accordance with the actual labels (even with a very low threshold) which means the model couldn't learn anything.</p>
<p>*</p>
<blockquote>
<p>Note: The pre-trained and fine-tuned RoBERTa with BPE tokenizer
performs better than the pre-trained and fine-tuned with custom
tokenizer although the loss for MLM with custom tokenizer was better
than BPE.</p>
</blockquote>
<ul>
<li></li>
</ul>
","2997866","","2997866","","2020-12-12 12:39:18","2020-12-12 12:39:18","Using WordPiece tokenization with RoBERTa","<deep-learning><bert-language-model><huggingface-transformers><transfer-learning><roberta-language-model>","0","4","","","","CC BY-SA 4.0"
"65252670","1","","","2020-12-11 13:55:19","","1","185","<p>I am trying to embed some documents including a couple of sentences using huggingface transformers models. I have multi-gpu single-node and I want to do embedding parallel and distributed in all 8 gpus. I tried to use pytorch DistributedDataParallel, but I think all sentences are sending to all GPUs and for all sentences, it is returning one tensor. this is a sample code:</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import time

import argparse
import os

from transformers import AlbertTokenizer, AlbertModel
import numpy
from tqdm import tqdm
from torch.utils.data import DataLoader,TensorDataset

def parse_args():
    parse = argparse.ArgumentParser()
    parse.add_argument(
            '--local_rank',
            dest = 'local_rank',
            type = int,
            default = 0,
            )
    parse.add_argument(&quot;--gpu&quot;, type=str, default='None',
                        help=&quot;choose gpu device.&quot;)
    return parse.parse_args()



def train():
    args = parse_args()
    
    if not args.gpu == 'None':
        device = torch.device(&quot;cuda&quot;)
        os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=args.gpu
    else:
        device = torch.device(&quot;cpu&quot;)
    
    torch.cuda.set_device(args.local_rank)
    
    torch.distributed.init_process_group(
                backend='nccl',
                init_method='env://',
                )
    
    tokenizer = AlbertTokenizer.from_pretrained('albert-xxlarge-v2')
    

    sentences=['I love tea',    
               'He hates tea',
               'We love tea', 
               'python coder',
               'geeksforgeeks',
               'coder in geeksforgeeks']
   
   
    sentence_tokens = []
    for sent in (sentences):
        
        token_id = tokenizer.encode(sent, max_length=128, add_special_tokens=True, pad_to_max_length=True)
        
        sentence_tokens.append(token_id)
    original_sentences = torch.tensor(sentence_tokens)
    train_dataset = TensorDataset(original_sentences)
    
    #setup training sampler
    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,num_replicas=len(sentences))
    
    #setup training data loader with the train sampler setup
    train_dataloader = DataLoader(train_dataset, batch_size=16,sampler=train_sampler, shuffle=False)
    

    model = AlbertModel.from_pretrained('albert-xxlarge-v2', return_dict=True)
    
    model = model.to(device)
    model = nn.parallel.DistributedDataParallel(model,
            device_ids = [args.local_rank, ],
            output_device = args.local_rank,\
            find_unused_parameters=True
            )
    for batch in (train_dataloader):
        
        batch_input_tensors = batch[0].to('cuda')
        outputs = model(batch_input_tensors)
        last_hidden_states = outputs.last_hidden_state
        
        average= torch.mean(last_hidden_states,dim=1)
       
if __name__ == &quot;__main__&quot;:
    train()
</code></pre>
<p>all of sentences are sending to all 8 GPUs and output as last_hidden_states is only one tensor. I got the average of tensor elements because I thought at the end they should be same but they aren't.
how can do it distributed and sentences distribute to GPUs and embed over there? and finally for each sentence or for my final case each Doc I have one tensor as feature vector?
thanks</p>
","13909058","","13909058","","2020-12-11 16:38:48","2020-12-11 16:38:48","Embedding documents on multi-GPU single-node Docker using pre-trained models of huggingface transformers and pytorch DistributedDataParallel","<pytorch><bert-language-model><huggingface-transformers><multi-gpu>","0","0","1","","","CC BY-SA 4.0"
"65017564","1","","","2020-11-26 07:19:20","","1","674","<p>Bert has an Architecture something like <code>encoder -&gt; 12 BertLayer -&gt; Pooling</code>. I want to train the last 40% layers of Bert Model. I can freeze all the layers as:</p>
<pre><code># freeze parameters
bert = AutoModel.from_pretrained('bert-base-uncased')
for param in bert.parameters():
    param.requires_grad = False

</code></pre>
<p>But I want to Train last 40% layers. When I do <code>len(list(bert.parameters()))</code>, it gives me 199. So let us suppose 79 is the 40% of parameters. Can I do something like:</p>
<pre><code>for param in list(bert.parameters())[-79:]: # total  trainable 199 Params: 79 is 40%
    param.requires_grad = False
</code></pre>
<p>I think it will freeze first 60% layers.</p>
<p>Also, can someone tell me that which layers it will freeze according to architecture?</p>
","11725056","","","","","2020-11-26 09:49:54","Train n% last layers of BERT in Pytorch using HuggingFace Library (train Last 5 BERTLAYER out of 12 .)","<deep-learning><nlp><pytorch><torch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65072694","1","65081174","","2020-11-30 11:27:58","","-1","867","<p>I want to make sure my BertModel does not loads pre-trained weights. I am using auto class (hugging face) which loads model automatically.</p>
<p>My question is how do I load bert model without pretrained weights?</p>
","10381012","","","","","2020-11-30 22:40:20","Make sure BERT model does not load pretrained weights?","<pytorch><bert-language-model><huggingface-transformers>","2","3","","","","CC BY-SA 4.0"
"65217033","1","","","2020-12-09 12:43:01","","3","1095","<p>I'm using pytorch and this is the model from huggingface transformers <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">link</a>:</p>
<pre><code>from transformers import BertTokenizerFast, BertForSequenceClassification
bert = BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;,
                                                     num_labels=int(data['class'].nunique()),
                                                     output_attentions=False,
                                                     output_hidden_states=False)
</code></pre>
<p>and in the forward function I'm building, I'm calling <code>x1, x2 = self.bert(sent_id, attention_mask=mask)</code>
Now, as far as I know, x2 is the cls output(which is the output of the first transformer encoder) but yet again, I don't think I understand the output of the model.
but I want the output of all the 12 last transformer encoders.
How can I do that in pytorch ?</p>
","10303779","","10303779","","2020-12-09 12:49:20","2021-06-15 20:26:41","How can i get all outputs of the last transformer encoder in bert pretrained model and not just the cls token output?","<neural-network><pytorch><text-classification><bert-language-model><huggingface-transformers>","2","3","","","","CC BY-SA 4.0"
"62405867","1","62408256","","2020-06-16 09:59:54","","0","808","<p>I'm trying to run the code 'transformers' version of <a href=""https://github.com/VinAIResearch/BERTweet#transformers"" rel=""nofollow noreferrer"">this code</a> to use the new pre-trained BERTweet model and I'm getting an error. </p>

<p>The following lines of code ran successfully in my Google Colab notebook:</p>

<pre><code>
!pip install fairseq
import fairseq
!pip install fastBPE
import fastBPE

# download the pre-trained BERTweet model zipped file
!wget https://public.vinai.io/BERTweet_base_fairseq.tar.gz

# unzip the pre-trained BERTweet model files
!tar -xzvf BERTweet_base_fairseq.tar.gz

!pip install transformers
import transformers

import torch
import argparse

from transformers import RobertaConfig
from transformers import RobertaModel

from fairseq.data.encoders.fastbpe import fastBPE
from fairseq.data import Dictionary

</code></pre>

<p>Then I tried to run the following code:</p>

<pre><code># Load model
config = RobertaConfig.from_pretrained(
    ""/Absolute-path-to/BERTweet_base_transformers/config.json""
)
BERTweet = RobertaModel.from_pretrained(
    ""/Absolute-path-to/BERTweet_base_transformers/model.bin"",
    config=config
)
</code></pre>

<p>...and an error was displayed:</p>

<pre><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    242             if resolved_config_file is None:
--&gt; 243                 raise EnvironmentError
    244             config_dict = cls._dict_from_json_file(resolved_config_file)

OSError: 

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
2 frames
/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    250                 f""- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\n\n""
    251             )
--&gt; 252             raise EnvironmentError(msg)
    253 
    254         except json.JSONDecodeError:

OSError: Can't load config for '/Absolute-path-to/BERTweet_base_transformers/config.json'. Make sure that:

- '/Absolute-path-to/BERTweet_base_transformers/config.json' is a correct model identifier listed on 'https://huggingface.co/models'

- or '/Absolute-path-to/BERTweet_base_transformers/config.json' is the correct path to a directory containing a config.json file

</code></pre>

<p>I'm guessing the issue is that I need to replace '/Absolute-path-to' with something else but if that's the case what should it be replaced with? It's likely a very simple answer and I feel stupid for asking but I need help.</p>
","9751001","","6664872","","2020-10-24 17:16:13","2020-10-24 17:16:13","Error Running ""config = RobertaConfig.from_pretrained( ""/Absolute-path-to/BERTweet_base_transformers/config.json""""","<nlp><google-colaboratory><bert-language-model><huggingface-transformers><roberta-language-model>","1","2","","","","CC BY-SA 4.0"
"65091635","1","65185943","","2020-12-01 13:42:27","","0","943","<p>I am trying to reproduce <a href=""https://stackoverflow.com/questions/61389018/big-loss-and-low-accuracy-on-training-data-in-both-bert-and-albert"">this</a> example using huggingface <code>TFBertModel</code> to do a classification task.</p>
<p>My model is almost the same of the example, but I'm performing multilabel classification. For this reason, I've performed the binarization of my labels using sklearn's <code>MultiLabelBinarizer</code>.</p>
<p>Then, I've adapted my model to have the predictions accordingly.</p>
<pre><code>def loadBertModel(max_length,n_classes):

  bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased')

  input_ids = keras.Input(shape=(max_length,), dtype=np.int32)
  attention_mask = keras.Input(shape=(max_length,), dtype=np.int32)
  token_type_ids = keras.Input(shape=(max_length,), dtype=np.int32)

  _, output = bert_model([input_ids, attention_mask,token_type_ids])
  
  output = keras.layers.Dense(n_classes, activation=&quot;sigmoid&quot;, name=&quot;dense_out_dom&quot;)(output)

  model = keras.Model(
    inputs=[input_ids, attention_mask,token_type_ids],
    outputs=output,
    name='bert_classifier',
  )       

  model.compile(
    optimizer=Adam(lr=2e-5),  
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
  )

  model.summary()
  return model
</code></pre>
<p>Also, I'm using tensorflow's <code>Dataset</code> to produce my model's inputs:</p>
<pre><code>def map_example_to_dict(input_ids, attention_masks, token_type_ids, label): 
  return {
      &quot;input_ids&quot;: input_ids,
      &quot;token_type_ids&quot;: token_type_ids,
      &quot;attention_mask&quot;: attention_masks,
  }, label
  

def tokenize_sequences(tokenizer, max_length, corpus, labels):
  input_ids = []
  token_type_ids = []
  attention_masks = []

  for i in tqdm(range(len(corpus))):
    encoded = tokenizer.encode_plus(
        corpus[i], 
        max_length=max_length, 
        add_special_tokens=True,
        padding='max_length',
        truncation=True,
        return_token_type_ids=True,
        return_attention_mask=True,  # add attention mask to not focus on pad tokens)
        return_tensors=&quot;tf&quot;
    )
    input_ids.append(encoded[&quot;input_ids&quot;])
    attention_masks.append(encoded[&quot;attention_mask&quot;])
    token_type_ids.append(encoded[&quot;token_type_ids&quot;])

  input_ids = tf.convert_to_tensor(input_ids)
  attention_masks = tf.convert_to_tensor(attention_masks)
  token_type_ids = tf.convert_to_tensor(token_type_ids)
  
  labels = labels.toarray()

  return tf.data.Dataset.from_tensor_slices((input_ids, attention_masks, token_type_ids, labels)).map(map_example_to_dict)
</code></pre>
<p>Finally, when I try to fit my model, I have an incoherence concerning the logits and the labels' shapes:</p>
<pre><code>ValueError: logits and labels must have the same shape ((1, 21) vs (21, 1))
</code></pre>
<p>I really don't know if the <code>Dataset</code> transformation is messing with my inputs' shapes or if I'm missing some other detail. Any ideas?</p>
<p>Full stack trace:</p>
<p>ValueError                                Traceback (most recent call last)</p>
<pre><code>&lt;ipython-input-42-19f4c0665eeb&gt; in &lt;module&gt;()
      4       epochs=N_EPOCHS,
      5       verbose=1,
----&gt; 6       batch_size=1,
      7       )

10 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--&gt; 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-&gt; 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = &quot;nonXla&quot;
--&gt; 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--&gt; 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 697             *args, **kwds))
    698 
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-&gt; 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-&gt; 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--&gt; 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:149 __call__
        losses = ag_call(y_true, y_pred)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:253 call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1605 binary_crossentropy
        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4814 binary_crossentropy
        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:174 sigmoid_cross_entropy_with_logits
        (logits.get_shape(), labels.get_shape()))

    ValueError: logits and labels must have the same shape ((1, 21) vs (21, 1))
</code></pre>
","5936342","","5936342","","2020-12-01 15:57:05","2020-12-07 16:51:47","ValueError: logits and labels must have the same shape ((1, 21) vs (21, 1))","<python><tensorflow><keras><huggingface-transformers><huggingface-tokenizers>","1","4","","","","CC BY-SA 4.0"
"65157852","1","","","2020-12-05 14:15:46","","2","558","<p>I am trying to import a pretrained model from Huggingface's transformers library and extend it with a few layers for classification using tensorflow keras. When I directly use transformers model (Method 1), the model trains well and reaches a validation accuracy of 0.93 after 1 epoch. However, when trying to use the model as a layer within a tf.keras model (Method 2), the model can't get above 0.32 accuracy. As far as I can tell based on the documentation, the two approaches should be equivalent. My goal is to get Method 2 working so that I can add more layers to it instead of directly using the logits produced by Huggingface's classifier head but I'm stuck at this stage.</p>
<pre><code>import tensorflow as tf

from transformers import TFRobertaForSequenceClassification
</code></pre>
<p>Method 1:</p>
<pre><code>model = TFRobertaForSequenceClassification.from_pretrained(&quot;roberta-base&quot;, num_labels=6)
</code></pre>
<p>Method 2:</p>
<pre><code>input_ids = tf.keras.Input(shape=(128,), dtype='int32')

attention_mask = tf.keras.Input(shape=(128, ), dtype='int32')

transformer = TFRobertaForSequenceClassification.from_pretrained(&quot;roberta-base&quot;, num_labels=6)

encoded = transformer([input_ids, attention_mask])

logits = encoded[0]

model = tf.keras.models.Model(inputs = [input_ids, attention_mask], outputs = logits)

</code></pre>
<p>Rest of the code for either method is identical,</p>
<pre><code>model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])
</code></pre>
<p>I am using Tensorflow 2.3.0 and have tried with transformers versions 3.5.0 and 4.0.0.</p>
","8840524","","8840524","","2020-12-08 15:22:15","2020-12-08 15:27:39","How to mix tensorflow keras model and transformers","<python><tensorflow><keras><nlp><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62603089","1","","","2020-06-26 21:31:10","","2","1880","<p>I am trying to implement a classification head for the reformer transformer. The classification head works fine, but when I try to change one of the config parameters- config.axial_pos_shape i.e sequence length parameter for the model it throws an error;</p>
<blockquote>
<p>size mismatch for reformer.embeddings.position_embeddings.weights.0: copying a param with shape torch.Size([512, 1, 64]) from checkpoint, the shape in current model is torch.Size([64, 1, 64]).
size mismatch for reformer.embeddings.position_embeddings.weights.1: copying a param with shape torch.Size([1, 1024, 192]) from checkpoint, the shape in current model is torch.Size([1, 128, 192]).</p>
</blockquote>
<p>The config:</p>
<pre><code>{
  &quot;architectures&quot;: [
    &quot;ReformerForSequenceClassification&quot;
  ],
  &quot;attention_head_size&quot;: 64,
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;attn_layers&quot;: [
    &quot;local&quot;,
    &quot;lsh&quot;,
    &quot;local&quot;,
    &quot;lsh&quot;,
    &quot;local&quot;,
    &quot;lsh&quot;
  ],
  &quot;axial_norm_std&quot;: 1.0,
  &quot;axial_pos_embds&quot;: true,
  &quot;axial_pos_embds_dim&quot;: [
    64,
    192
  ],
  &quot;axial_pos_shape&quot;: [
    64,
    256
  ],
  &quot;chunk_size_feed_forward&quot;: 0,
  &quot;chunk_size_lm_head&quot;: 0,
  &quot;eos_token_id&quot;: 2,
  &quot;feed_forward_size&quot;: 512,
  &quot;hash_seed&quot;: null,
  &quot;hidden_act&quot;: &quot;relu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.05,
  &quot;hidden_size&quot;: 256,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;is_decoder&quot;: true,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;local_attention_probs_dropout_prob&quot;: 0.05,
  &quot;local_attn_chunk_length&quot;: 64,
  &quot;local_num_chunks_after&quot;: 0,
  &quot;local_num_chunks_before&quot;: 1,
  &quot;lsh_attention_probs_dropout_prob&quot;: 0.0,
  &quot;lsh_attn_chunk_length&quot;: 64,
  &quot;lsh_num_chunks_after&quot;: 0,
  &quot;lsh_num_chunks_before&quot;: 1,
  &quot;max_position_embeddings&quot;: 8192,
  &quot;model_type&quot;: &quot;reformer&quot;,
  &quot;num_attention_heads&quot;: 2,
  &quot;num_buckets&quot;: [
    64,
    128
  ],
  &quot;num_chunks_after&quot;: 0,
  &quot;num_chunks_before&quot;: 1,
  &quot;num_hashes&quot;: 1,
  &quot;num_hidden_layers&quot;: 6,
  &quot;output_past&quot;: true,
  &quot;pad_token_id&quot;: 0,
  &quot;task_specific_params&quot;: {
    &quot;text-generation&quot;: {
      &quot;do_sample&quot;: true,
      &quot;max_length&quot;: 100
    }
  },
  &quot;vocab_size&quot;: 320
}
</code></pre>
<p>Python Code:</p>
<pre class=""lang-py prettyprint-override""><code>config = ReformerConfig()
config.max_position_embeddings = 8192
config.axial_pos_shape=[64, 128]

#config = ReformerConfig.from_pretrained('./cnp/config.json', output_attention=True)

model = ReformerForSequenceClassification(config)
model.load_state_dict(torch.load(&quot;./cnp/pytorch_model.bin&quot;))
</code></pre>
","12699944","","6664872","","2020-06-27 23:55:05","2020-12-16 16:08:09","Config change for a pre-trained transformer model","<pytorch><huggingface-transformers><pre-trained-model>","1","2","","","","CC BY-SA 4.0"
"62779585","1","","","2020-07-07 16:15:24","","1","470","<p>I have a corpus that is 16 GB large and my ram IS around 16 GB ish. If I load the entire dataset to train the language model RoBERTa from scratch, I am going to have a memory issue. I intend to train my RoBERTa using the script provided from Huggingface's tutorial in their blog post: <a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb</a></p>
<p>However, their blog post suggests the usage of LineByLineTextDatase. However, this loads the dataset eagerly.</p>
<pre><code>class LineByLineTextDataset(Dataset):
    &quot;&quot;&quot;
    This will be superseded by a framework-agnostic approach
    soon.
    &quot;&quot;&quot;

    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):
        assert os.path.isfile(file_path)
        # Here, we do not cache the features, operating under the assumption
        # that we will soon use fast multithreaded tokenizers from the
        # `tokenizers` repo everywhere =)
        logger.info(&quot;Creating features from dataset file at %s&quot;, file_path)

        with open(file_path, encoding=&quot;utf-8&quot;) as f:
            lines = [line for line in f.read().splitlines() if (len(line) &gt; 0 and not line.isspace())]

        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)
        self.examples = batch_encoding[&quot;input_ids&quot;]

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i) -&gt; torch.Tensor:
        return torch.tensor(self.examples[i], dtype=torch.long)
</code></pre>
<p>Unexpectedly, my kernel crashed on the part where they read the line. I wonder if there is a way to make it read lazily. It will be very desirable if the suggested answer can create minimum code change with the posted tutorial since I'm rather new with Huggingface and afraid I won't be able to debug it on my own.</p>
","2872852","","6664872","","2020-11-27 23:12:44","2020-12-06 10:54:02","Train RoBERTa from scratch where dataset is larger than the capacity of RAM?","<huggingface-transformers><roberta-language-model>","1","0","","","","CC BY-SA 4.0"
"62978957","1","62996559","","2020-07-19 10:18:32","","4","2243","<p>I've read post which explains how the sliding window works but I cannot find any information on how it is actually implemented.</p>
<p>From what I understand if the input are too long, sliding window can be used to process the text.</p>
<p>Please correct me if I am wrong.
Say I have a text <em><strong>&quot;In June 2017 Kaggle announced that it passed 1 million registered users&quot;</strong></em>.</p>
<p>Given some <code>stride</code> and <code>max_len</code>, the input can be split into chunks with over lapping words (not considering padding).</p>
<pre><code>In June 2017 Kaggle announced that # chunk 1
announced that it passed 1 million # chunk 2
1 million registered users # chunk 3
</code></pre>
<p>If my questions were <em><strong>&quot;when did Kaggle make the announcement&quot;</strong></em> and <em><strong>&quot;how many registered users&quot;</strong></em> I can use <code>chunk 1</code> and <code>chunk 3</code> and <strong>not use</strong> <code>chunk 2</code> <strong>at all</strong> in the model. Not quiet sure if I should still use <code>chunk 2</code> to train the model</p>
<p>So the input will be:
<code>[CLS]when did Kaggle make the announcement[SEP]In June 2017 Kaggle announced that[SEP]</code>
and
<code>[CLS]how many registered users[SEP]1 million registered users[SEP]</code></p>
<hr>
<p>Then if I have a question with no answers do I feed it into the model with all chunks like and indicate the starting and ending index as <strong>-1</strong>? For example <em><strong>&quot;can pigs fly?&quot;</strong></em></p>
<p><code>[CLS]can pigs fly[SEP]In June 2017 Kaggle announced that[SEP]</code></p>
<p><code>[CLS]can pigs fly[SEP]announced that it passed 1 million[SEP]</code></p>
<p><code>[CLS]can pigs fly[SEP]1 million registered users[SEP]</code></p>
<hr>
<p>As suggested in the comments, II tried to run <code>squad_convert_example_to_features</code> (<a href=""https://github.com/huggingface/transformers/blob/1af58c07064d8f4580909527a8f18de226b226ee/src/transformers/data/processors/squad.py#L134"" rel=""nofollow noreferrer"">source code</a>) to investigate the problem I have above, but it doesn't seem to work, nor there are any documentation. It seems like <code>run_squad.py</code> from huggingface uses <code>squad_convert_example_to_features</code> with the <code>s</code> in <code>example</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor, squad_convert_example_to_features
from transformers import AutoTokenizer, AutoConfig, squad_convert_examples_to_features

FILE_DIR = &quot;.&quot;

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
processor = SquadV2Processor()
examples = processor.get_train_examples(FILE_DIR)

features = squad_convert_example_to_features(
    example=examples[0],
    max_seq_length=384,
    doc_stride=128,
    max_query_length=64,
    is_training=True,
)
</code></pre>
<p>I get the error.</p>
<pre><code>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00&lt;00:00, 159.95it/s]
Traceback (most recent call last):
  File &quot;&lt;input&gt;&quot;, line 25, in &lt;module&gt;
    sub_tokens = tokenizer.tokenize(token)
NameError: name 'tokenizer' is not defined
</code></pre>
<p>The error indicates that there are no <code>tokenizers</code> but it does not allow us to pass a <code>tokenizer</code>. Though it does work if I add a tokenizer while I am inside the function in debug mode. So how exactly do I use the <code>squad_convert_example_to_features</code> function?</p>
","13936333","","13936333","","2020-07-20 12:48:06","2020-07-20 15:56:24","Sliding window for long text in BERT for Question Answering","<nlp><text-classification><huggingface-transformers><question-answering><bert-language-model>","1","4","2","","","CC BY-SA 4.0"
"65242786","1","65244304","","2020-12-10 21:46:59","","1","124","<p>I implemented my custom Bert Binary Classification Model class, by adding a classifier layer on top of Bert Model (attached below). However, the accuracy/metrics are significantly different when I train with the official BertForSequenceClassification model, which makes me wonder if I am missing somehting in my class.</p>
<p>Few Doubts I have:</p>
<p>While loading the official <code>BertForSequenceClassification</code> <code>from_pretrained</code> are the classifiers weight initialized as well from pretrained model or they are randomly initialized? Because in my custom class they are randomly initialized.</p>
<pre class=""lang-py prettyprint-override""><code>class MyCustomBertClassification(nn.Module):
    def __init__(self, encoder='bert-base-uncased',
                        num_labels,
                        hidden_dropout_prob):

    super(MyCustomBertClassification, self).__init__()
    self.config  = AutoConfig.from_pretrained(encoder)
    self.encoder = AutoModel.from_config(self.config)
    self.dropout = nn.Dropout(hidden_dropout_prob)
    self.classifier = nn.Linear(self.config.hidden_size, num_labels)

def forward(self, input_sent):
    outputs = self.encoder(input_ids=input_sent['input_ids'],
                          attention_mask=input_sent['attention_mask'],
                          token_type_ids=input_sent['token_type_ids'],
                          return_dict=True)
    
    pooled_output = self.dropout(outputs[1])
    # for both tasks
    logits = self.classifier(pooled_output)

    return logits
</code></pre>
","2359772","","3607203","","2020-12-11 11:25:27","2020-12-11 11:25:27","Metrics mismatch between BertForSequenceClassification Class and my custom Bert Classification","<pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"62405155","1","62408820","","2020-06-16 09:19:39","","3","4619","<p>I have the following pieces of code and trying to understand the difference between BertWordPieceTokenizer and BertTokenizer.</p>
<h1><strong>BertWordPieceTokenizer (Rust based)</strong></h1>
<pre><code>from tokenizers import BertWordPieceTokenizer

sequence = &quot;Hello, y'all! How are you Tokenizer ðŸ˜ ?&quot;
tokenizer = BertWordPieceTokenizer(&quot;bert-base-uncased-vocab.txt&quot;)
tokenized_sequence = tokenizer.encode(sequence)
print(tokenized_sequence)
&gt;&gt;&gt;Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])

print(tokenized_sequence.tokens)
&gt;&gt;&gt;['[CLS]', 'hello', ',', 'y', &quot;'&quot;, 'all', '!', 'how', 'are', 'you', 'token', '##izer', '[UNK]', '?', '[SEP]']
</code></pre>
<h1>BertTokenizer</h1>
<pre><code>from transformers import BertTokenizer
tokenizer = BertTokenizer(&quot;bert-base-cased-vocab.txt&quot;)
tokenized_sequence = tokenizer.encode(sequence)
print(tokenized_sequence)
#Output: [19082, 117, 194, 112, 1155, 106, 1293, 1132, 1128, 22559, 17260, 100, 136]
</code></pre>
<ol>
<li>Why is encode working differently in both ? In BertWordPieceTokenizer it gives Encoding object while in BertTokenizer it gives the ids of the vocab.</li>
<li>What is the Difference between BertWordPieceTokenizer and BertTokenizer fundamentally, because as I understand BertTokenizer also uses WordPiece under the hood.</li>
</ol>
<p>Thanks</p>
","6043669","","6043669","","2020-06-21 08:56:38","2020-06-21 08:56:38","BertWordPieceTokenizer vs BertTokenizer from HuggingFace","<nlp><huggingface-transformers><bert-language-model><huggingface-tokenizers>","1","0","2","","","CC BY-SA 4.0"
"62980509","1","","","2020-07-19 12:49:23","","0","475","<p>I want to do Named Entity Recognition on scientific articles.</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_cased')
model=TFBertForTokenClassification.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;,from_pt=True,config=config)
</code></pre>
<p>But it gives following warning.</p>
<blockquote>
<p>Some weights of the PyTorch model were not used when initializing the
TF 2.0 model TFBertForTokenClassification: ['classifier.weight',
'classifier.bias'] - This IS expected if you are initializing
TFBertForTokenClassification from a TF 2.0 model trained on another
task or with another architecture (e.g. initializing a
BertForSequenceClassification model from a TFBertForPretraining
model). - This IS NOT expected if you are initializing
TFBertForTokenClassification from a TF 2.0 model that you expect to be
exactly identical (initializing a BertForSequenceClassification model
from a TFBertForSequenceClassification model). Some weights or buffers
of the PyTorch model TFBertForTokenClassification were not initialized
from the TF 2.0 model and are newly initialized:
['cls.predictions.transform.dense.weight',
'cls.predictions.decoder.bias',
'cls.predictions.transform.LayerNorm.weight',
'cls.predictions.transform.LayerNorm.bias',
'cls.predictions.decoder.weight', 'cls.seq_relationship.weight',
'cls.predictions.bias', 'cls.predictions.transform.dense.bias',
'cls.seq_relationship.bias'] You should probably TRAIN this model on a
down-stream task to be able to use it for predictions and inference.</p>
</blockquote>
<p>And when I try to fit the model, it gives follwing warning.</p>
<blockquote>
<p>WARNING:tensorflow:Gradients do not exist for variables
['tf_bert_for_token_classification/bert/pooler/dense/kernel:0',
'tf_bert_for_token_classification/bert/pooler/dense/bias:0'] when
minimizing the loss.</p>
</blockquote>
<p>Does anyone know why it is giving these warnings?</p>
","13958024","","6664872","","2020-07-19 13:20:07","2020-07-19 13:20:07","Why TFBertForTokenClassification.from_pretrained(""allenai/scibert_scivocab_uncased"",from_pt=True,config=config) gives weights warning?","<pytorch><tensorflow2.0><huggingface-transformers><bert-language-model><allennlp>","0","3","","","","CC BY-SA 4.0"
"62982346","1","","","2020-07-19 15:38:16","","2","344","<p>I would like to ask about how to finetune distillbart on gigaword and cnn dailymail with the starting checkpoint <a href=""https://huggingface.co/sshleifer/distilbart-cnn-12-6"" rel=""nofollow noreferrer"">distilbart-cnn-12-6</a>.
I did use the gigaword dataset provided by tensorflow but it replaces numbers by this character: &quot;#&quot;, as a result, my summaries have # instead of numbers, is it normal that it has those # ?
Also is it really possible to finetune distillbart from the checkpoint distilbart-cnn-12-6 with cnn daily mail?</p>
<pre><code>import os
os.environ['PYTHONPATH'] += &quot;:/content/transformers/examples&quot;
%cd &quot;/content/transformers/examples&quot;

!python /content/transformers/examples/seq2seq/finetune.py \
    --learning_rate=3e-5 \
    --fp16 \
    --gpus 1 \
    --do_train \
    --do_predict \
    --n_val 1000 \
    --val_check_interval 0.1 \
    --sortish_sampler \
    --data_dir '/content/dataset' \
    --train_batch_size=4 \
    --eval_batch_size=4 \
    --output_dir=distilbart_1300k_1400k \
    --num_train_epochs 1 \
    --model_name_or_path /content/transformers/examples/distilbart_1200k_1300k/best_tfmr
</code></pre>
<p>here is the link to gigaword: <a href=""https://www.tensorflow.org/datasets/catalog/gigaword"" rel=""nofollow noreferrer"">https://www.tensorflow.org/datasets/catalog/gigaword</a>
and here is the link to cnn dailymail: <a href=""https://www.tensorflow.org/datasets/catalog/cnn_dailymail"" rel=""nofollow noreferrer"">https://www.tensorflow.org/datasets/catalog/cnn_dailymail</a></p>
<p>For the code I followed the instruction of fine tuning distillabart here:
<a href=""https://github.com/Hildweig/transformers/tree/master/examples/seq2seq"" rel=""nofollow noreferrer"">https://github.com/Hildweig/transformers/tree/master/examples/seq2seq</a></p>
<p>For the outputs with gigawords I get something like this:
&quot;foreign exchange rates in hong kong sept. ## #### ; china 's defense minister says he 's ready to work with yugoslavia 's foreign minister on iraq 's role in iraq with bc-me-gen iraq&quot;</p>
","9579719","","6664872","","2020-07-19 16:03:51","2020-07-19 16:03:51","How to finetune distillbart for abstractive summarization using Gigaword or Cnn dailymail?","<transformer><huggingface-transformers><summarization><bert-language-model>","0","2","","","","CC BY-SA 4.0"
"63087586","1","","","2020-07-25 11:07:21","","1","95","<p>In my views.py file of my Django application I'm trying to load the 'transformers' library with the following command:</p>
<pre><code>from transformers import pipeline
</code></pre>
<p>This works in my local environment, but on my Linux server at Linode, when I try to load my website, the page tries to load for 5 minutes then I get a Timeout error. I don't understand what is going on, I know I have installed the library correctly. I have also run the same code in the python shell on my server and it loads fine, it's is just that if I load it in my Django views.py file, no page of my website loads.</p>
<p>My server: Ubuntu 20.04 LTS, Nanode 1GB: 1 CPU, 25GB Storage, 1GB RAM</p>
<p>Library: transformers==3.0.2</p>
<p>I also have the same problem when I try to load tensorflow. All the other libraries are loading fine, like pytorch and pandas etc. I've been trying to solve this problem since more than a week, I've also changed hosts from GCP to Linode, but it's still the same.</p>
<p><strong>Edit:</strong> I created a new server and installed everything from scratch and used a virtualenv this time, but still its the same problem. Following are the installed libraries outputted from <code>pip freeze</code>:</p>
<pre><code>asgiref==3.2.10
certifi==2020.6.20
chardet==3.0.4
click==7.1.2
Django==3.0.7
djangorestframework==3.11.0
filelock==3.0.12
future==0.18.2
idna==2.10
joblib==0.16.0
numpy==1.19.1
packaging==20.4
Pillow==7.2.0
pyparsing==2.4.7
pytz==2020.1
regex==2020.7.14
requests==2.24.0
sacremoses==0.0.43
sentencepiece==0.1.91
six==1.15.0
sqlparse==0.3.1
tokenizers==0.8.1rc1
torch==1.5.1+cpu
torchvision==0.6.1+cpu
tqdm==4.48.0
transformers==3.0.2
urllib3==1.25.10
</code></pre>
<p>I also know transformers library is installed because if I try to import some library that doesn't exist then I simply get an error, like I should. But in this case it just loads forever and doesn't output any error. This is so bizarre.</p>
","4823067","","2108707","","2020-07-25 19:35:42","2020-07-25 19:35:42","Certain python libraries are not loading in my Django application in production, but load fine in local server","<python><django><linux><ubuntu><huggingface-transformers>","1","16","","","","CC BY-SA 4.0"
"67326333","1","67340829","","2021-04-29 23:31:13","","1","141","<p>I am retraining a wav2vec model from hugging face for classification problem. I have 5 classes and the input is a list of tensors [1,400].
Here is how I am getting the model</p>
<pre><code>num_labels = 5
model_name = &quot;Zaid/wav2vec2-large-xlsr-53-arabic-egyptian&quot;
model_config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)  ##needed for the visualizations
tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(model_name)
model = Wav2Vec2ForCTC.from_pretrained(model_name, config=model_config)
</code></pre>
<p>Here is the model updated settings</p>
<pre><code># Freeze the pre trained parameters
for param in model.parameters():
    param.requires_grad = False
criterion = nn.MSELoss().to(device)
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-6)

# Add three new layers at the end of the network
model.classifier = nn.Sequential(
    nn.Linear(768, 256),
    nn.Dropout(0.25),
    nn.ReLU(),
    nn.Linear(256, 64),
    nn.Dropout(0.25),
    nn.ReLU(),
    nn.Linear(64, 2),
    nn.Dropout(0.25),
    nn.Softmax(dim=1)
)
</code></pre>
<p>Then the training loop</p>
<pre><code>print_every = 300

total_loss = 0
all_losses = []
model.train()
for epoch in range(2):
    print(&quot;Epoch number: &quot;, epoch)
    for row in range(16918):
        Input = torch.tensor(trn_ivectors[row]).double()
        label = torch.tensor(trn_labels[row]).long().to(device)
        label = torch.unsqueeze(label,0).to(device)
        #print(&quot;Label&quot;, label.shape)
        Input = torch.unsqueeze(Input,1).to(device)
        #print(Input.shape)
        optimizer.zero_grad()
        
        #Input.requires_grad = True
        Input = F.softmax(Input[0], dim=-1)
        
        if label == 0:
            label = torch.tensor([1.0, 0.0]).float().to(device)
        elif label == 1:
            label = torch.tensor([0.0, 1.0]).float().to(device)

        # print(overall_output, label)

        loss = criterion(Input, label)
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

        if idx % print_every == 0 and idx &gt; 0:
            average_loss = total_loss / print_every
            print(&quot;{}/{}. Average loss: {}&quot;.format(idx, len(train_data), average_loss))
            all_losses.append(average_loss)
            total_loss = 0

torch.save(model.state_dict(), &quot;model_after_train.pt&quot;)
</code></pre>
<p>Unfortunately when I try to train the program it gives me the following error</p>
<pre><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>Please I would appreciate if you could tell me how to fix this error. I have been searching a lot on a way fixing it but didn't fix it</p>
<p>Thanks</p>
","15755552","","","","","2021-04-30 21:47:26","Wav2Vec pytorch element 0 of tensors does not require grad and does not have a grad_fn","<python><deep-learning><nlp><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63419835","1","","","2020-08-14 20:47:47","","1","145","<p>I found interesting model - question generator, but can't run it. I got an error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;qg.py&quot;, line 5, in &lt;module&gt;
    model = AutoModelWithLMHead.from_pretrained(&quot;/home/user/ml-experiments/gamesgen/t5-base-finetuned-question-generation-ap/&quot;)
  File &quot;/home/user/.virtualenvs/hugging/lib/python3.7/site-packages/transformers/modeling_auto.py&quot;, line 806, in from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
  File &quot;/home/user/.virtualenvs/hugging/lib/python3.7/site-packages/transformers/modeling_utils.py&quot;, line 798, in from_pretrained
    import torch_xla.core.xla_model as xm
ModuleNotFoundError: No module named 'torch_xla'
</code></pre>
<p>I briefly googled and found that &quot;torch_xla&quot; is a something that is used to train pytorch model on TPU. But I would like to run it localy on cpu (for inference, of course) and got this error when pytorch tried to load tpu-bound tensors.
How can I fix it?</p>
<p>this is model, which I tried: <a href=""https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap"" rel=""nofollow noreferrer"">https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap</a></p>
","5773233","","","","","2020-08-15 15:41:14","pytorch model saved from TPU run on CPU","<pytorch><huggingface-transformers><xla>","1","1","","","","CC BY-SA 4.0"
"62979062","1","","","2020-07-19 10:28:27","","0","403","<p>I am trying to test this <a href=""https://github.com/ThilinaRajapakse/simpletransformers#minimal-start"" rel=""nofollow noreferrer"">https://github.com/ThilinaRajapakse/simpletransformers#minimal-start</a> NER using simple transformers and facing the following error. can someone guide me with this issue?</p>
<p>I tried to write my own NER using hugging face transformers lib and still face same issue.</p>
<p>Tried this : <a href=""https://github.com/ThilinaRajapakse/simpletransformers/issues/370#issuecomment-627763738"" rel=""nofollow noreferrer"">https://github.com/ThilinaRajapakse/simpletransformers/issues/370#issuecomment-627763738</a> still facing the same issue with python3.7</p>
<p>Error :</p>
<pre><code>File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\site-packages\simpletransformers\ner\ner_model.py&quot;, line 242, in train_model
    train_dataset = self.load_and_cache_examples(train_data)
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\site-packages\simpletransformers\ner\ner_model.py&quot;, line 986, in load_and_cache_examples
    features = convert_examples_to_features(
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\site-packages\simpletransformers\ner\ner_utils.py&quot;, line 242, in convert_examples_to_features
    with Pool(process_count) as p:
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\context.py&quot;, line 119, in Pool
    return Pool(processes, initializer, initargs, maxtasksperchild,
INFO:simpletransformers.ner.ner_model: Converting to features started.
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\pool.py&quot;, line 212, in __init__
Traceback (most recent call last):
    self._repopulate_pool()
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\pool.py&quot;, line 303, in _repopulate_pool
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 116, in spawn_main
    return self._repopulate_pool_static(self._ctx, self.Process,
    exitcode = _main(fd, parent_sentinel)
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 125, in _main
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\pool.py&quot;, line 326, in _repopulate_pool_static
    prepare(preparation_data)
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 236, in prepare
    w.start()
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\process.py&quot;, line 121, in start
    self._popen = self._Popen(self)
    _fixup_main_from_path(data['init_main_from_path'])
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 287, in _fixup_main_from_path
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\context.py&quot;, line 326, in _Popen
    main_content = runpy.run_path(main_path,
    return Popen(process_obj)
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\runpy.py&quot;, line 265, in run_path
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\popen_spawn_win32.py&quot;, line 45, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
    return _run_module_code(code, init_globals, run_name,
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 154, in get_preparation_data
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\runpy.py&quot;, line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
    _check_not_importing_main()
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\runpy.py&quot;, line 87, in _run_code
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 134, in _check_not_importing_main
    exec(code, run_globals)
    raise RuntimeError('''
  File &quot;C:\Users\Harsha\Desktop\git_repos\bert-ner\src\inference_ner_simple_transformers.py&quot;, line 27, in &lt;module&gt;
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The &quot;freeze_support()&quot; line can be omitted if the program
        is not going to be frozen to produce an executable.    model.train_model(train_df)

  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\site-packages\simpletransformers\ner\ner_model.py&quot;, line 242, in train_model
    train_dataset = self.load_and_cache_examples(train_data)
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\site-packages\simpletransformers\ner\ner_model.py&quot;, line 986, in load_and_cache_examples
INFO:simpletransformers.ner.ner_model: Converting to features started.
    features = convert_examples_to_features(
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\site-packages\simpletransformers\ner\ner_utils.py&quot;, line 242, in convert_examples_to_features
Traceback (most recent call last):
    with Pool(process_count) as p:
INFO:simpletransformers.ner.ner_model: Converting to features started.
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\context.py&quot;, line 119, in Pool
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 116, in spawn_main
Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
    return Pool(processes, initializer, initargs, maxtasksperchild,
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\pool.py&quot;, line 212, in __init__
    exitcode = _main(fd, parent_sentinel)
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 125, in _main
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 116, in spawn_main
    self._repopulate_pool()
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\pool.py&quot;, line 303, in _repopulate_pool
    prepare(preparation_data)
    exitcode = _main(fd, parent_sentinel)
    return self._repopulate_pool_static(self._ctx, self.Process,
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 236, in prepare
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 125, in _main
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\pool.py&quot;, line 326, in _repopulate_pool_static
    _fixup_main_from_path(data['init_main_from_path'])
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 287, in _fixup_main_from_path
    w.start()
    prepare(preparation_data)
INFO:simpletransformers.ner.ner_model: Converting to features started.
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\process.py&quot;, line 121, in start
Traceback (most recent call last):
    main_content = runpy.run_path(main_path,
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 236, in prepare
    self._popen = self._Popen(self)
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\runpy.py&quot;, line 265, in run_path
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\context.py&quot;, line 326, in _Popen
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 116, in spawn_main
    _fixup_main_from_path(data['init_main_from_path'])
    return _run_module_code(code, init_globals, run_name,
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 287, in _fixup_main_from_path
    return Popen(process_obj)
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\runpy.py&quot;, line 97, in _run_module_code
    exitcode = _main(fd, parent_sentinel)
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\popen_spawn_win32.py&quot;, line 45, in __init__
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 125, in _main
    main_content = runpy.run_path(main_path,
    _run_code(code, mod_globals, init_globals,
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\runpy.py&quot;, line 265, in run_path
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\runpy.py&quot;, line 87, in _run_code
    prep_data = spawn.get_preparation_data(process_obj._name)
    prepare(preparation_data)
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 154, in get_preparation_data
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 236, in prepare
    return _run_module_code(code, init_globals, run_name,
    exec(code, run_globals)
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\runpy.py&quot;, line 97, in _run_module_code
  File &quot;C:\Users\Harsha\Desktop\git_repos\bert-ner\src\inference_ner_simple_transformers.py&quot;, line 27, in &lt;module&gt;
    _check_not_importing_main()
    _fixup_main_from_path(data['init_main_from_path'])
    _run_code(code, mod_globals, init_globals,
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 134, in _check_not_importing_main
    model.train_model(train_df)
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\multiprocessing\spawn.py&quot;, line 287, in _fixup_main_from_path
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\runpy.py&quot;, line 87, in _run_code
  File &quot;C:\Users\Harsha\Anaconda3\envs\simpt\lib\site-packages\simpletransformers\ner\ner_model.py&quot;, line 242, in train_model
</code></pre>
<p>System Config :
Windows 10
GTX 1070
Python 3.8.3
transformers 2.1.1
pytorch 1.5.1
conda environment
<a href=""https://i.stack.imgur.com/JwgdH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JwgdH.png"" alt=""enter image description here"" /></a></p>
","10656943","","10656943","","2020-07-19 11:06:00","2020-08-05 19:03:22","Running simple transformers to test NER is causing the error. Any solutions?","<python><multiprocessing><huggingface-transformers><simpletransformers>","1","2","","","","CC BY-SA 4.0"
"63076190","1","","","2020-07-24 14:57:47","","4","2333","<p>I am curious about the memory usage of transformers.BertModel. I would like to use the pretrained model to transform text and save the output of token [CLS]. No training, only inference.</p>
<p>My input to bert is 511 tokens. With the batch size being 16, my code runs out of memory. The GPU has 32GB memory. My question is how to estimate the memory usage of Bert.</p>
<p>Strangely the other job having batch size 32 finished successfully, with the same set up. My code are listed below.</p>
<pre><code># Create dataloader
bs = 16
train_comb = ConcatDataset([train_data, valid_data])
train_dl = DataLoader(train_comb, sampler=RandomSampler(train_data), batch_size=bs)

model = BertModel.from_pretrained('/my_dir/bert_base_uncased/',
                                  output_attentions=False,
                                  output_hidden_states=False)
model.cuda()
out_list = []
model.eval()
with torch.no_grad():
    for d in train_dl:
        d = [i.cuda() for i in d].      # d = [input_ids, attention_mask, token_type_ids, labels] 
        inputs, labels = d[:3], d[3]    # input_ids has shape 16 x 511
        output = model(*inputs)[0][:, 0, :]
        out_list.append(output)

outputs = torch.cat(out_list)
</code></pre>
<p>Later I changed the for loop to below</p>
<pre><code>with torch.no_grad():
    for d in train_dl:
        d = [i.cuda() for i in d[:3]]          # don't care about the labels
        out_list.append(model(*d)[0][:, 0, :]) # remove the intermediary variables
    del d
</code></pre>
<p>To summarize, my questions are:</p>
<ol>
<li>How to estimate the memory usage of Bert? I want to use it to estimate the batch size.</li>
<li>My 2nd job having batch size 32 finished successfully. Is it because it has more paddings?</li>
<li>Is there any suggestion on improving the efficiency of the memory usage in my code?</li>
</ol>
","1487336","","","","","2020-07-25 02:13:34","How to calculate the memory requirement of Bert?","<memory-management><out-of-memory><huggingface-transformers>","1","0","2","","","CC BY-SA 4.0"
"64138426","1","64144405","","2020-09-30 13:15:33","","0","1032","<p>I'm in the process of finetuning a BERT model to the long answer task in the Natural Questions dataset. I'm training the model just like a SQuAD model (predicting start and end tokens).</p>
<p>I use Huggingface and PyTorch.</p>
<p>So the targets and labels have a shape/size of <em><strong>[batch, 2]</strong></em>. My problem is that I can't input &quot;multi-targets&quot; which I think is refering to the fact that the last shape is <em><strong>2</strong></em>.</p>
<blockquote>
<p>RuntimeError: multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:18</p>
</blockquote>
<p>Should I choose another loss function or is there another way to bypass this problem?</p>
<p>This code I'm using:</p>
<pre><code>def loss_fn(preds, targets):
    return nn.CrossEntropyLoss()(preds,labels)
</code></pre>
<pre><code>class DecoderModel(nn.Module):

    def __init__(self, model_args, encoder_config, loss_fn):
        super(DecoderModel, self).__init__()
        # ...

    def forward(self, pooled_output, labels):   
        pooled_output = self.dropout(pooled_output)
        logits = self.linear(pooled_output)

        start_logits, end_logits = logits.split(1, dim = -1)
        start_logit = torch.squeeze(start_logits, axis=-1)
        end_logit = torch.squeeze(end_logits, axis=-1)

        # Concatenate into a &quot;label&quot;
        preds = torch.cat((start_logits, end_logits), -1)

        # Calculate loss
        loss = self.loss_fn(
            preds = preds, 
            labels = labels)

        return loss, preds
</code></pre>
<p>The targets properties are:
<em><strong>torch.int64</strong></em> &amp; <em><strong>[3,2]</strong></em></p>
<p>The predictions properties are:
<em><strong>torch.float32</strong></em> &amp; <em><strong>[3,2]</strong></em></p>
<h1>SOLVED - this is my solution</h1>
<pre><code>def loss_fn(preds:list, labels):
    start_token_labels, end_token_labels = labels.split(1, dim = -1)
    start_token_labels = start_token_labels.squeeze(-1)
    end_token_labels = end_token_labels.squeeze(-1)

    print('*'*50)
    print(preds[0].shape) # preds [0] and [1] has the same shape and dtype
    print(preds[0].dtype) # preds [0] and [1] has the same shape and dtype
    print(start_token_labels.shape) # labels [0] and [1] has the same shape and dtype
    print(start_token_labels.dtype) # labels [0] and [1] has the same shape and dtype

    start_loss = nn.CrossEntropyLoss()(preds[0], start_token_labels)
    end_loss = nn.CrossEntropyLoss()(preds[1], end_token_labels)

    avg_loss = (start_loss + end_loss) / 2
    return avg_loss
</code></pre>
<p>Basically I'm splitting the logits (just not concatinating them) and the labels. I then do Cross Entropy loss on both of them and at last taking the average loss between the two. Hope this gives you an idea to solve your own problem!</p>
","11400785","","11400785","","2020-09-30 19:58:43","2020-09-30 19:58:43","Why can't I use Cross Entropy Loss for multilabel?","<python><machine-learning><nlp><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65247333","1","","","2020-12-11 07:27:55","","4","2208","<p>I am using sentiment-analysis pipeline as described <a href=""https://huggingface.co/transformers/quicktour.html"" rel=""nofollow noreferrer"">here</a>.</p>
<pre><code>from transformers import pipeline
classifier = pipeline('sentiment-analysis')
</code></pre>
<p>It's failing with a connection error message</p>
<blockquote>
<p>ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.</p>
</blockquote>
<p>Is there a way to specify a proxy within the pipeline method so that it's able to connect to the internet and download the files needed?</p>
","3232272","","","","","2021-02-11 14:16:10","How to specify a proxy in transformers pipeline","<python><bert-language-model><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"64493308","1","","","2020-10-23 03:08:57","","0","190","<p>I have tried loading a distilbert model in pytorch over 3 different GPUs (GeForce GTX 1080 ti, tesla k80, tesla v100).  According to the pytorch cuda profiler, the memory consumption is identical in all of these GPUs(534MB).  But &quot;nvidia-smi&quot; shows different memory consumption for each of them (GTX 1080 ti- 1181MB, tesla k80 - 898MB, tesla v100- 1714MB).</p>
<p>I chose v100, hoping to accommodate more processes because of it's extra memory.  Because of this, I am not able accommodate any more processes in v100 compared to k80.</p>
<p><strong>Versions</strong>: Python  3.6.11, transformers==2.3.0,
torch==1.6.0</p>
<p>Any help would be appreciated.</p>
<p>Following are the memory consumption in the GPUs.</p>
<p>----------------GTX 1080ti---------------------</p>
<pre><code>2020-10-19 02:11:04,147 - CE - INFO - torch.cuda.max_memory_allocated() : 514.33154296875
2020-10-19 02:11:04,147 - CE - INFO - torch.cuda.memory_allocated() : 514.33154296875
2020-10-19 02:11:04,147 - CE - INFO - torch.cuda.memory_reserved() : 534.0
2020-10-19 02:11:04,148 - CE - INFO - torch.cuda.max_memory_reserved() : 534.0
</code></pre>
<p>The output of &quot;nvidia-smi&quot; :</p>
<pre><code>2020-10-19 02:11:04,221 - CE - INFO - | ID | Name                | Serial          | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |
2020-10-19 02:11:04,222 - CE - INFO - |  0 | GeForce GTX 1080 Ti | [Not Supported] | GPU-58d5d4d3-07a1-81b4-ba67-8d6b46e342fb ||       50C |       15% |          11% ||      11178MB |      1181MB |      9997MB || Disabled     | Disabled       |
</code></pre>
<p>----------------Tesla k80---------------------</p>
<pre><code>2020-10-19 12:15:37,030 - CE - INFO - torch.cuda.max_memory_allocated() : 514.33154296875
2020-10-19 12:15:37,031 - CE - INFO - torch.cuda.memory_allocated() : 514.33154296875
2020-10-19 12:15:37,031 - CE - INFO - torch.cuda.memory_reserved() : 534.0
2020-10-19 12:15:37,031 - CE - INFO - torch.cuda.max_memory_reserved() : 534.0
</code></pre>
<p>The output of &quot;nvidia-smi&quot; :</p>
<pre><code>2020-10-19 12:15:37,081 - CE - INFO - | ID | Name      | Serial        | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |
2020-10-19 12:15:37,081 - CE - INFO - |  0 | Tesla K80 | 0324516191902 | GPU-1e7baee8-174b-2178-7115-cf4a063a8923 ||       50C |        3% |           8% ||      11441MB |       898MB |     10543MB || Disabled     | Disabled       |
</code></pre>
<p>----------------Tesla v100---------------------</p>
<pre><code>2020-10-20 08:18:42,952 - CE - INFO - torch.cuda.max_memory_allocated() : 514.33154296875
2020-10-20 08:18:42,952 - CE - INFO - torch.cuda.memory_allocated() : 514.33154296875
2020-10-20 08:18:42,953 - CE - INFO - torch.cuda.memory_reserved() : 534.0
2020-10-20 08:18:42,953 - CE - INFO - torch.cuda.max_memory_reserved() : 534.0
</code></pre>
<p>The output of &quot;nvidia-smi&quot; :</p>
<pre><code>2020-10-20 08:18:43,020 - CE - INFO - | ID | Name                 | Serial        | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |
2020-10-20 08:18:43,020 - CE - INFO - |  0 | Tesla V100-SXM2-16GB | 0323617004258 | GPU-849088a3-508a-1737-7611-75a087f18085 ||       29C |        0% |          11% ||      16160MB |      1714MB |     14446MB || Enabled      | Disabled       |
</code></pre>
","9155357","","","","","2021-02-05 09:58:43","Different memory allocation on GTX 1080 ti, Tesla k80, Tesla v100 for the same pytorch model","<pytorch><gpu><nvidia><huggingface-transformers><tesla>","0","0","","","","CC BY-SA 4.0"
"62422590","1","62426315","","2020-06-17 06:19:07","","5","1206","<p>I feel confused when using the Roberta tokenizer in Huggingface. </p>

<pre><code>&gt;&gt;&gt; tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
&gt;&gt;&gt; x = tokenizer.tokenize(""The tiger is ___ (big) than the dog."")
['The', 'Ä tiger', 'Ä is', 'Ä ___', 'Ä (', 'big', ')', 'Ä than', 'Ä the', 'Ä dog', '.']
&gt;&gt;&gt; x = tokenizer.tokenize(""The tiger is ___ ( big ) than the dog."")
['The', 'Ä tiger', 'Ä is', 'Ä ___', 'Ä (', 'Ä big', 'Ä )', 'Ä than', 'Ä the', 'Ä dog', '.']
&gt;&gt;&gt; x = tokenizer.encode(""The tiger is ___ (big) than the dog."")
[0, 20, 23921, 16, 2165, 36, 8527, 43, 87, 5, 2335, 4, 2]
&gt;&gt;&gt; x = tokenizer.encode(""The tiger is ___ ( big ) than the dog."")
[0, 20, 23921, 16, 2165, 36, 380, 4839, 87, 5, 2335, 4, 2]
&gt;&gt;&gt;
</code></pre>

<p><strong>Question</strong>: <code>(big)</code> and <code>( big )</code> have different tokenization results, which result in different token id as well. Which one I should use? Does it mean that I should pre-tokenize the input first to make it <code>( big )</code> and go for RobertaTokenization?  Or it doesn't really matter?</p>

<p>Secondly, it seems <code>BertTokenizer</code> has no such confusion:</p>

<pre><code>&gt;&gt;&gt; tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
&gt;&gt;&gt; x = tokenizer.tokenize(""The tiger is ___ (big) than the dog."")
['the', 'tiger', 'is', '_', '_', '_', '(', 'big', ')', 'than', 'the', 'dog', '.']
&gt;&gt;&gt; x = tokenizer.tokenize(""The tiger is ___ ( big ) than the dog."")
['the', 'tiger', 'is', '_', '_', '_', '(', 'big', ')', 'than', 'the', 'dog', '.']
&gt;&gt;&gt;
</code></pre>

<p><code>BertTokenizer</code> gives me the same results using the wordpieces. </p>

<p>Any thoughts to help me better understand the RobertaTokenizer, which I know is using Byte-Pair Encoding?</p>
","2211979","","","","","2020-06-17 09:57:19","Do I need to pre-tokenize the text first before using HuggingFace's RobertaTokenizer? (Different undersanding)","<huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"63077776","1","","","2020-07-24 16:33:48","","1","57","<p>Say Iâ€™ve trained a BERT model for classification. Iâ€™d like to calculate the proportional impact each input token is having on the predicted output.</p>
<p>For example - and this is very general - if I have a model that labels input text as <code>{â€˜about dogsâ€™ : 0, â€˜about catsâ€™ : 1}</code>, the following input sentence:
<code>s = 'this is a sentence about a cat'</code>
should output very close to:
<code>1</code></p>
<p>HOWEVER, what Iâ€™d like is to calculate each inputâ€™s impact on that final prediction, e.g. (assuming weâ€™re tokenizing on the level of words - which is not how it would be done in practice, I know):
<code>{this : .01, is: .005, a : .02, sentence : .0003, about : [some other low prob], a: [another low prob], cat : 0.999999}</code></p>
<p>Intuitively Iâ€™d think this means running a forward pass with the input sentence, then looking at the backprop values? But Iâ€™m not quite sure how youâ€™d do that. Thoughts?</p>
<p><strong>NOTE</strong>
Assume everything is implemented in PyTorch. My current use case is with HuggingFace, but I'd want to generalize this anyway.</p>
","13877925","","","","","2020-07-24 16:33:48","Calculate Impact of Input Tokens on BERT (or other classifier) Output Probability","<python><machine-learning><nlp><classification><huggingface-transformers>","0","3","0","","","CC BY-SA 4.0"
"64500833","1","","","2020-10-23 13:20:49","","0","589","<p>I'm trying to fine-tune huggingface's implementation of distilbert for multi-class classification (100 classes) on a custom dataset following the tutorial at <a href=""https://huggingface.co/transformers/custom_datasets.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/custom_datasets.html</a>.</p>
<p>I'm doing so using Tensorflow, and fine-tuning in native tensorflow, that is, I use the following part of the tutorial for dataset creation:</p>
<pre><code>import tensorflow as tf
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))
</code></pre>
<p>And this one for fine-tuning:</p>
<pre><code>from transformers import TFDistilBertForSequenceClassification
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)
</code></pre>
<p>Everything seems to go fine with fine-tuning, but when I try to predict on the test dataset using <code>model.predict(test_dataset)</code> as argument (with 2000 examples), the model seems to yield one prediction per token rather than one prediction per sequence...</p>
<p>That is, instead of getting an output of shape <code>(1, 2000, 100)</code>, I get an output of shape <code>(1, 1024000, 100)</code>, where 1024000 is the number of test examples (2000) * the sequence length (512).</p>
<p>Any hint on what's going on here?
(Sorry if this is naive, I'm very new to tensorflow).</p>
","6931486","","","","","2020-10-25 15:16:32","fine-tuning huggingface DistilBERT for multi-class classification on custom dataset yields weird output shape on prediction","<tensorflow2.0><predict><huggingface-transformers><distilbert>","1","0","","","","CC BY-SA 4.0"
"64119623","1","","","2020-09-29 12:28:00","","0","70","<p>In my docker container I have a flask app (behind nginx and uwsgi) which instantiates a model from huggingface/transformers.
For some reason, the app continuously restarts when trying to after downloading the models</p>
<p><strong>App:</strong></p>
<pre><code>### app.py
server = Flask(__name__)
cors = CORS(server)
server.config[&quot;CORS_HEADERS&quot;] = &quot;Content-Type&quot;
log.info(&quot;Instantiating model&quot;)
model = AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
pipe = pipeline('ner', model=model, tokenizer=tokenizer)

if __name__ == &quot;__main__&quot;:
    server.run(host=appconf.host, port=appconf.port, debug=appconf.isdev, use_reloader=False)
</code></pre>
<p><strong>Logs:</strong></p>
<pre><code>[2020-09-29 14:13:11,704] {./app.py:14} INFO - Instantiating model
[2020-09-29 14:13:11,708] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:939} DEBUG - Starting new HTTPS connection (1): s3.amazonaws.com:443
[2020-09-29 14:13:12,170] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:433} DEBUG - https://s3.amazonaws.com:443 &quot;HEAD /models.huggingface.co/bert/dbmdz/bert-large-cased-finetuned-conll03-english/config.json HTTP/1.1&quot; 200 0
[2020-09-29 14:13:12,176] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:939} DEBUG - Starting new HTTPS connection (1): cdn.huggingface.co:443
[2020-09-29 14:13:12,317] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:433} DEBUG - https://cdn.huggingface.co:443 &quot;HEAD /dbmdz/bert-large-cased-finetuned-conll03-english/pytorch_model.bin HTTP/1.1&quot; 200 0
[uWSGI] getting INI configuration from /etc/uwsgi/uwsgi.ini
*** Starting uWSGI 2.0.19.1 (64bit) on [Tue Sep 29 14:13:55 2020] ***
...
your server socket listen backlog is limited to 100 connections
your mercy for graceful operations on workers is 60 seconds
mapped 1727064 bytes (1686 KB) for 16 cores
*** Operational MODE: preforking ***
mounting app:server on /
Comment: FROM HERE IT REPEATS
[2020-09-29 14:13:57,254] {./app.py:14} INFO - Instantiating model &lt;-- AGAIN!
[2020-09-29 14:13:57,257] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:939} DEBUG - Starting new HTTPS connection (1): s3.amazonaws.com:443
[2020-09-29 14:13:57,686] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:433} DEBUG - https://s3.amazonaws.com:443 &quot;HEAD /models.huggingface.co/bert/dbmdz/bert-large-cased-finetuned-conll03-english/config.json HTTP/1.1&quot; 200 0
[2020-09-29 14:13:57,693] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:939} DEBUG - Starting new HTTPS connection (1): cdn.huggingface.co:443
[2020-09-29 14:13:57,790] {/usr/local/lib/python3.8/site-packages/urllib3/connectionpool.py:433} DEBUG - https://cdn.huggingface.co:443 &quot;HEAD /dbmdz/bert-large-cased-finetuned-conll03-english/pytorch_model.bin HTTP/1.1&quot; 200 0
</code></pre>
<p>I am sure to have <code>http_proxy</code> and <code>https_proxy</code> set within the Docker container.</p>
<p>Thanks for the help</p>
","5379182","","13328195","","2020-09-29 12:32:16","2020-09-29 13:10:03","Flask app continuously restarting after downloading huggingface models","<python><huggingface-transformers>","1","4","","","","CC BY-SA 4.0"
"64626722","1","","","2020-10-31 21:46:19","","0","746","<p>When running <code>from transformers import BertForSequenceClassification</code>, I am getting the below error stacktrace.</p>
<p><strong>Error stacktrace</strong></p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-16-7a027f32a339&gt; in &lt;module&gt;
----&gt; 1 from transformers import BertForSequenceClassification

/opt/conda/lib/python3.6/site-packages/transformers/__init__.py in &lt;module&gt;
    133 
    134 # Pipelines
--&gt; 135 from .pipelines import (
    136     Conversation,
    137     ConversationalPipeline,

/opt/conda/lib/python3.6/site-packages/transformers/pipelines.py in &lt;module&gt;
     63     import torch
     64 
---&gt; 65     from .modeling_auto import (
     66         MODEL_FOR_MASKED_LM_MAPPING,
     67         MODEL_FOR_QUESTION_ANSWERING_MAPPING,

/opt/conda/lib/python3.6/site-packages/transformers/modeling_auto.py in &lt;module&gt;
     97 )
     98 from .modeling_ctrl import CTRLLMHeadModel, CTRLModel
---&gt; 99 from .modeling_deberta import DebertaForSequenceClassification, DebertaModel
    100 from .modeling_distilbert import (
    101     DistilBertForMaskedLM,

/opt/conda/lib/python3.6/site-packages/transformers/modeling_deberta.py in &lt;module&gt;
     20 import torch
     21 from packaging import version
---&gt; 22 from torch import _softmax_backward_data, nn
     23 from torch.nn import CrossEntropyLoss
     24 

ImportError: cannot import name '_softmax_backward_data'

</code></pre>
<p><strong>OS:</strong> Ubuntu 20 LTS</p>
<p><strong>Using Kaggle python docker container:</strong> <a href=""https://github.com/Kaggle/docker-python"" rel=""nofollow noreferrer"">https://github.com/Kaggle/docker-python</a></p>
<p><strong>Python version:</strong> Python 3.6.6 :: Anaconda, Inc.</p>
<p><strong>requirements.txt:</strong> <a href=""http://www.itextpad.com/timekeeperrequirementstxt1"" rel=""nofollow noreferrer"">http://www.itextpad.com/timekeeperrequirementstxt1</a></p>
","5100213","","","","","2021-08-28 22:52:24","ImportError: cannot import name '_softmax_backward_data'","<python><torch><huggingface-transformers>","1","4","","","","CC BY-SA 4.0"
"63948641","1","63948723","","2020-09-18 02:48:21","","2","44","<p>I completely understand why I would be getting this error, but I am interested to know if there is a way to pad the text to fourteen words. For context purposes, this is a textual heatmap using GPT-2. If you have a simpler idea in mind, I would greatly appreciate that as well. To test the code out for yourself: <a href=""https://colab.research.google.com/drive/1NFwEdkQdQkDQAwAGwhP_fw4_zfyHWvMU#scrollTo=aNXp69VxE6lf"" rel=""nofollow noreferrer"">Google Colaboratory</a>. Thank you in advance for your assistance!</p>
<pre><code>def apply(f):
    text = f
    text = re.sub(r'\W+', ' ', text)
    res = LM().check_probabilities(text, topk=50)
    
    word_list = f.split()
    one = word_list[0]
    two = word_list[1]
    three = word_list[2]
    four = word_list[3]
    five = word_list[4]
    six = five = word_list[5]
    seven = word_list[6]
    eight = word_list[7]
    nine = word_list[8]
    ten = word_list[9]
    eleven = word_list[10]
    twelve = word_list[11]
    thirteen = word_list[12]
    fourteen = word_list[13]

    data = [[
{'token': '[CLR]',
 'meta': ['', '', ''],
 'heat': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},
{'token': ' ',
 'format': True},
{'token': one,
 'meta': res['pred_topk'][0],
 'heat': [0.13271349668502808, 0.4047139883041382, 0.23314827680587769, 1.0, 0.5698219537734985, 0.20001010596752167, 0.41732218861579895, 0.2375192940235138, 0.12837326526641846, 0.3011391758918762, 0.2920743227005005, 0.15121395885944366, 0.4707326292991638, 0.141720250248909, 0.1146061047911644, 0.3309290111064911, 0.2721664309501648, 0.38880598545074463, 0.28752031922340393, 0.30476102232933044, 0.40849509835243225, 0.12109626829624176, 0.236867755651474, 0.15692873299121857, 0.08568184077739716, 0.28222283720970154, 0.10787433385848999, 0.09868176281452179, 0.11645302921533585, 0.27660083770751953, 0.1150846853852272, 0.13137750327587128, 0.2834398150444031, 0.1425863653421402, 0.7729436159133911, 0.15550559759140015, 0.3342195451259613, 0.2743198275566101]},
{'token': ' ',
 'format': True},
{'token': two,
 'meta': res['pred_topk'][1],
 'heat': [0.11053311824798584, 1.0, 0.3417408764362335, 0.5805244445800781, 0.596860408782959, 0.18530210852622986, 0.2305091768503189, 0.19138814508914948, 0.08227257430553436, 0.19505015015602112, 0.10965480655431747, 0.07133453339338303, 0.21702361106872559, 0.07083487510681152, 0.05262206494808197, 0.09487571567296982, 0.07871642708778381, 0.09568451344966888, 0.10381820052862167, 0.11150145530700684, 0.08054117858409882, 0.06160977482795715, 0.13430000841617584, 0.07046942412853241, 0.04503295198082924, 0.10039176791906357, 0.07321848720312119, 0.04508531466126442, 0.04002087190747261, 0.1304282695055008, 0.05149686336517334, 0.05910608172416687, 0.1943625509738922, 0.05612911283969879, 0.2365487962961197, 0.0644913837313652, 0.08357883244752884, 0.10955799371004105]},
{'token': ' ',
 'format': True},
{'token': three,
 'meta': res['pred_topk'][2],
 'heat': [0.13794338703155518, 0.7412312626838684, 0.2688325345516205, 0.3519371747970581, 1.0, 0.3511815071105957, 0.6799001097679138, 0.23039610683918, 0.10480885207653046, 0.29196831583976746, 0.24283158779144287, 0.08086933195590973, 0.3110826909542084, 0.16006161272525787, 0.07783187925815582, 0.23599569499492645, 0.2036796659231186, 0.25475823879241943, 0.39147695899009705, 0.4029639661312103, 0.16113890707492828, 0.08008856326341629, 0.4354044497013092, 0.14515410363674164, 0.05876074731349945, 0.21267741918563843, 0.11644049733877182, 0.08587612956762314, 0.08814962208271027, 0.363741010427475, 0.07122389227151871, 0.07023804634809494, 0.1380654275417328, 0.1375676840543747, 0.7550925016403198, 0.10494624823331833, 0.23596565425395966, 0.12745369970798492]},
{'token': ' ',
 'format': True},
{'token': four,
 'meta': res['pred_topk'][3],
 'heat': [0.09374084323644638, 0.27613726258277893, 0.19584566354751587, 1.0, 0.2668629586696625, 0.12618684768676758, 0.5485848784446716, 0.10671643167734146, 0.05578231066465378, 0.16895149648189545, 0.14708179235458374, 0.08301705121994019, 0.2549331486225128, 0.05449998006224632, 0.0407552570104599, 0.09658133238554001, 0.08113130927085876, 0.10979730635881424, 0.09126582741737366, 0.16856855154037476, 0.10670913755893707, 0.049128126353025436, 0.12720689177513123, 0.10207141935825348, 0.040946654975414276, 0.14924436807632446, 0.07131370157003403, 0.05912680923938751, 0.057828083634376526, 0.2358609288930893, 0.05285044014453888, 0.03720799833536148, 0.08448022603988647, 0.05244402214884758, 0.2379569709300995, 0.07916100323200226, 0.06218649446964264, 0.10799198597669601]},
{'token': ' ',
 'format': True},
{'token': five,
 'meta': res['pred_topk'][4],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949, 0.08953408151865005, 0.13667654991149902, 0.1143374964594841, 0.11026952415704727, 0.05795498564839363, 0.12386422604322433, 0.08859734237194061, 0.042766354978084564, 0.3162827491760254, 0.07349050790071487, 0.09265555441379547, 0.08770584315061569, 0.2039150893688202, 0.05270526185631752, 0.06614900380373001, 0.16070793569087982, 0.05872023105621338, 0.3202408254146576, 0.062171820551157, 0.14679910242557526, 0.08074744045734406]},
{'token': ' ',
 'format': True},
 {'token': six,
 'meta': res['pred_topk'][5],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
 {'token': seven,
 'meta': res['pred_topk'][6],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
 {'token': eight,
 'meta': res['pred_topk'][7],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
 {'token': nine,
 'meta': res['pred_topk'][8],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
  {'token': ten,
 'meta': res['pred_topk'][9],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
  {'token': eleven,
 'meta': res['pred_topk'][10],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
  {'token': twelve,
 'meta': res['pred_topk'][11],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
  {'token': thirteen,
 'meta': res['pred_topk'][12],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
  {'token': fourteen,
 'meta': res['pred_topk'][13],
 'heat': [0.1723620444536209, 1.0, 0.49656736850738525, 0.5609704256057739, 0.6928957104682922, 0.37088003754615784, 0.5890324115753174, 0.1961166113615036, 0.09367834031581879, 0.19113656878471375, 0.13310600817203522, 0.13753651082515717, 0.2627904713153839, 0.08134050667285919, 0.053574152290821075, 0.10540777444839478, 0.09048342704772949]},
{'token': ' ',
 'format': True},
{'token': '[SEP]',
 'meta': ['', '', ''],
 'heat': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}
]]

    from textualheatmap import TextualHeatmap
    heatmap = TextualHeatmap(facet_titles = ['BERT'], show_meta=True, width=3800)
    heatmap.set_data(data)
    print(&quot;    &quot;)
</code></pre>
<p><a href=""https://i.stack.imgur.com/3JT6t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3JT6t.png"" alt=""enter image description here"" /></a></p>
","6246159","","","","","2020-09-18 03:00:14","List Index Out of Range: Can I Pad My Text To Avoid?","<python><google-colaboratory><huggingface-transformers>","1","4","","","","CC BY-SA 4.0"
"63950303","1","","","2020-09-18 06:19:15","","0","311","<p>I am playing around with GPT2 and I have 2 tensors:</p>
<p><strong>O</strong>: An output tensor of shaped (B, S-1, V) where B is the batch size S is the the number of timestep and V is the vocabulary size. This is the output of a generative model and is softmaxed along the 2nd dimension.</p>
<p><strong>L</strong>: A 2D tensor shaped (B, S-1) where each element is the index of the correct token for each timestep for each sample. This is basically the labels.</p>
<p>I want to extract the predicted probability of the corresponding correct token from tensor <strong>O</strong> based on tensor <strong>L</strong> such that I will end up with a 2D tensor shaped (B, S). Is there an efficient way of doing this apart from using loops?</p>
","7678823","","5884955","","2020-09-23 10:06:09","2020-09-23 10:06:09","Retrieve elements from a 3D tensor with a 2D index tensor","<nlp><pytorch><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"64003734","1","","","2020-09-22 05:48:03","","0","207","<p>I was going to download <a href=""https://huggingface.co/cahya/bert-base-indonesian-522M"" rel=""nofollow noreferrer"">this</a> model, and then I was going to save it later to be used with bert-serving. Since bert-serving only supports tensorflow model, I need to download the tensorflow one and not the PyTorch. The PyTorch model downloads just fine, but the I cannot download the tensorflow model. I used this code to download:</p>
<pre><code>from transformers import BertTokenizer, TFBertModel

model_name='cahya/bert-base-indonesian-522M' 
model = TFBertModel.from_pretrained(model_name)
</code></pre>
<p>Here's what I got when running the code on Ubuntu 16.04, python3.5, transformers==2.5.1,</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/username/.local/lib/python3.5/site-packages/transformers/modeling_tf_utils.py&quot;, line 346, in from_pretrained
    assert os.path.isfile(resolved_archive_file), &quot;Error retrieving file {}&quot;.format(resolved_archive_file)
  File &quot;/usr/lib/python3.5/genericpath.py&quot;, line 30, in isfile
    st = os.stat(path)
TypeError: stat: can't specify None for path argument
</code></pre>
<p>And here's what I got when running it on Windows 10, python 3.6.5, transformers 3.1.0:</p>
<pre><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\transformers\modeling_tf_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    579                 if resolved_archive_file is None:
--&gt; 580                     raise EnvironmentError
    581             except EnvironmentError:

OSError:

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
&lt;ipython-input-3-c2f14f761f05&gt; in &lt;module&gt;()
      3 model_name='cahya/bert-base-indonesian-522M'
      4 tokenizer = BertTokenizer.from_pretrained(model_name)
----&gt; 5 model = TFBertModel.from_pretrained(model_name)

C:\ProgramData\Anaconda3\lib\site-packages\transformers\modeling_tf_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    585                     f&quot;- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {TF2_WEIGHTS_NAME}, {WEIGHTS_NAME}.\n\n&quot;
    586                 )
--&gt; 587                 raise EnvironmentError(msg)
    588             if resolved_archive_file == archive_file:
    589                 logger.info(&quot;loading weights file {}&quot;.format(archive_file))

OSError: Can't load weights for 'cahya/bert-base-indonesian-522M'. Make sure that:

- 'cahya/bert-base-indonesian-522M' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'cahya/bert-base-indonesian-522M' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.
</code></pre>
<p>This also happens with other cahya/ models. <a href=""https://huggingface.co/cahya/bert-base-indonesian-522M"" rel=""nofollow noreferrer"">This page</a> says that you can use the tensorflow model. However, based on the error, it seems like the file does not exist over there?
I tried downloading other pretrained model like <code>bert-base-uncased</code> etc. and they download just fine. This issue only happens with cahya/ models.</p>
<p>Am I missing something? or should I report this issue to forum or the github issue?</p>
","5571136","","5571136","","2020-09-22 06:04:37","2020-09-22 13:49:19","Cannot download tensorflow model of cahya/bert-base-indonesian-522M","<bert-language-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"67334513","1","","","2021-04-30 13:09:03","","1","23","<p>Crazy question maybe: but I want to download the gpt-2 model framework but I want the weights to be initialized randomly. So as if the model still has to be finetuned on the reddit content (including json, vocab, meta &amp; index files etc). Is this possible?</p>
<p>Kind regards!</p>
","15221534","","","","","2021-05-04 14:31:08","Is there an 'untrained' gpt model folder?","<huggingface-transformers><transformer><gpt-2>","1","0","","","","CC BY-SA 4.0"
"63193611","1","","","2020-07-31 14:19:14","","1","295","<p>I'm trying to regularize my model with pytorch optimizer using the <code>weight_decay</code> parameter. When the <code>weight_decay</code> value is equal to 0 (which is the default vallue), the <em>training loss</em> and <em>validation loss</em> decrease. But when I try setting the <code>weight_decay</code> to different values (eg. 0.0001, 0.001, 0.01, 0.1...) the <em>validation loss</em> and <em>training loss</em> remain the same during all the epochs (0.70). What could be the problem?</p>
","11777143","","","","","2020-07-31 14:19:14","Changing the pytorch weight decay value prevents the validation loss and training loss from decreasing","<python><pytorch><huggingface-transformers><regularized>","0","3","","","","CC BY-SA 4.0"
"63307009","1","","","2020-08-07 17:58:16","","5","747","<p>I use the HuggingFace's Transformers library for building a <strong>sequence-to-sequence model</strong> based on BART and T5. I carefully read the documentation and the research paper and I can't find what the input to the decoder (decoder_input_ids) should be for sequence-to-sequence tasks.</p>
<p>Should decoder input for both models (BART and T5) be same as lm_labels (output of the LM head) or should it be same as input_ids (input to the encoder)?</p>
","13645516","","13645516","","2020-08-07 18:04:17","2021-07-06 08:27:52","What decoder_input_ids should be for sequence-to-sequence Transformer model?","<nlp><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"64156202","1","64156912","","2020-10-01 13:16:01","","7","5214","<p>I want to add a dense layer on top of the bare BERT Model transformer outputting raw hidden-states, and then fine tune the resulting model. Specifically, I am using <a href=""https://huggingface.co/dbmdz/bert-base-italian-xxl-cased"" rel=""noreferrer"">this</a> base model. This is what the model should do:</p>
<ol>
<li>Encode the sentence (a vector with 768 elements for each token of the sentence)</li>
<li>Keep only the first vector (related to the first token)</li>
<li>Add a dense layer on top of this vector, to get the desired transformation</li>
</ol>
<p>So far, I have successfully encoded the sentences:</p>
<pre><code>from sklearn.neural_network import MLPRegressor

import torch

from transformers import AutoModel, AutoTokenizer

# List of strings
sentences = [...]
# List of numbers
labels = [...]

tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/bert-base-italian-xxl-cased&quot;)
model = AutoModel.from_pretrained(&quot;dbmdz/bert-base-italian-xxl-cased&quot;)

# 2D array, one line per sentence containing the embedding of the first token
encoded_sentences = torch.stack([model(**tokenizer(s, return_tensors='pt'))[0][0][0]
                                 for s in sentences]).detach().numpy()

regr = MLPRegressor()
regr.fit(encoded_sentences, labels)
</code></pre>
<p>In this way I can train a neural network by feeding it with the encoded sentences. However, this approach clearly does not fine tune the base BERT model. Can anybody help me? How can I build a model (possibly in pytorch or using the Huggingface library) that can be entirely fine tuned?</p>
","5296106","","","","","2020-10-01 15:08:25","Add dense layer on top of Huggingface BERT model","<python><python-3.x><neural-network><pytorch><huggingface-transformers>","2","0","4","","","CC BY-SA 4.0"
"63533941","1","","","2020-08-22 08:06:35","","2","335","<p>When I run the following code</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-large-uncased-whole-word-masking-finetuned-squad&quot;)
model = AutoModelForQuestionAnswering.from_pretrained(&quot;bert-large-uncased-whole-word-masking-finetuned-squad&quot;)

text = r&quot;&quot;&quot;
As checked Dis is not yet on boarded to ARB portal, hence we cannot upload the invoices in portal
&quot;&quot;&quot;

questions = [
    &quot;Dis asked if it is possible to post the two invoice in ARB.I have not access so I wanted to check if you would be able to do it.&quot;,
]

for question in questions:
    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=&quot;pt&quot;)
    input_ids = inputs[&quot;input_ids&quot;].tolist()[0]

    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)
    answer_start_scores, answer_end_scores = model(**inputs)

    answer_start = torch.argmax(
        answer_start_scores
    )  # Get the most likely beginning of answer with the argmax of the score
    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score

    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

    print(f&quot;Question: {question}&quot;)
    print(f&quot;Answer: {answer}\n&quot;)
</code></pre>
<p>The answer that I get here is:</p>
<pre><code>Question: Dis asked if it is possible to post the two invoice in ARB.I have not access so I wanted to check if you would be able to do it.
Answer: dis is not yet on boarded to ARB portal
</code></pre>
<p><strong>How do I get a score for this answer? Score here is very similar to what is I get when I run Question-Answer pipeline .</strong></p>
<p>I have to take this approach since Question-Answer pipeline when used is giving me Key Error for the below code</p>
<pre><code>from transformers import pipeline

nlp = pipeline(&quot;question-answering&quot;)

context = r&quot;&quot;&quot;
As checked Dis is not yet on boarded to ARB portal, hence we cannot upload the invoices in portal.
&quot;&quot;&quot;

print(nlp(question=&quot;Dis asked if it is possible to post the two invoice in ARB?&quot;, context=context))
</code></pre>
","2538811","","6664872","","2020-08-25 02:19:18","2020-12-11 11:17:24","How can I get the score from Question-Answer Pipeline? Is there a bug when Question-answer pipeline is used?","<huggingface-transformers>","1","7","1","","","CC BY-SA 4.0"
"64491004","1","","","2020-10-22 21:49:35","","3","735","<p>I am using Pytorch and a pretrained model from the transformers library. However, while finetuning it runs out of GPU memory very quickly and I wonder why.</p>
<p>I've found out that there is a memory leak in forward pass.</p>
<pre><code>show_memory('before call')
outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)
show_memory('after call')

del outputs
gc.collect()
model.zero_grad()
torch.cuda.empty_cache()
show_memory('after clearing')
</code></pre>
<p>Here <code>show_memory</code> function is defined as following:</p>
<pre><code>def show_memory(text=''):
  t = torch.cuda.get_device_properties(0).total_memory
  c = torch.cuda.memory_cached(0)
  a = torch.cuda.memory_allocated(0)
  f = c-a  # free inside cache
  print(f'\n\n{text}\nTotal: {t}\nCached: {c} \nAllocated: {a} \nFree in cache: {f}\n\n')
</code></pre>
<p>And the output is</p>
<pre><code>before call 
Total: 17071734784  
Cached: 3441426432  
Allocated: 3324039680  
Free in cache: 117386752


after call 
Total: 17071734784 
Cached: 6830424064  
Allocated: 6720267264  
Free in cache: 110156800


after clearing 
Total: 17071734784 
Cached: 6109003776  
Allocated: 4876882944  
Free in cache: 1232120832
</code></pre>
<p>So despite I clear everything, there are still almost two gigabytes more data in memory than there was originally. I wonder why it happens and if there is a way to inspect such cases in detail.</p>
","9646320","","681865","","2020-10-22 23:22:11","2020-10-22 23:22:11","Pytorch transformers memory leak in inference","<python><memory><pytorch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"63648380","1","","","2020-08-29 14:57:10","","0","1141","<p>I converted a pre-trained tf model to pytorch using the following function.</p>
<pre><code>def convert_tf_checkpoint_to_pytorch(*, tf_checkpoint_path, albert_config_file, pytorch_dump_path):
    # Initialise PyTorch model
    config = AlbertConfig.from_json_file(albert_config_file)
    print(&quot;Building PyTorch model from configuration: {}&quot;.format(str(config)))
    model = AlbertForPreTraining(config)

    # Load weights from tf checkpoint
    load_tf_weights_in_albert(model, config, tf_checkpoint_path)

    # Save pytorch-model
    print(&quot;Save PyTorch model to {}&quot;.format(pytorch_dump_path))
    torch.save(model.state_dict(), pytorch_dump_path)
</code></pre>
<p>I am loading the converted model and encoding sentences in the following way:</p>
<pre><code>def vectorize_sentence(text):
    albert_tokenizer = AlbertTokenizer.from_pretrained(&quot;albert-base-v2&quot;)
    config = AlbertConfig.from_pretrained(config_path, output_hidden_states=True)
    model = TFAlbertModel.from_pretrained(pytorch_dir, config=config, from_pt=True)
    e = albert_tokenizer.encode(text, max_length=512)
    model_input = tf.constant(e)[None, :]  # Batch size 1
    output = model(model_input)

    v = [0] * 768
    # generate sentence vectors by averaging the word vectors
    for i in range(1, len(model_input[0]) - 1):
        v = v + output[0][0][i].numpy()

    vector = v/len(model_input[0])
    return vector
</code></pre>
<p>However while loading the model, a warning comes up:</p>
<blockquote>
<p>Some weights or buffers of the PyTorch model TFAlbertModel were not
initialized from the TF 2.0 model and are newly initialized:
['predictions.LayerNorm.bias', 'predictions.dense.weight',
'predictions.LayerNorm.weight', 'sop_classifier.classifier.bias',
'predictions.dense.bias', 'sop_classifier.classifier.weight',
'predictions.decoder.bias', 'predictions.bias',
'predictions.decoder.weight'] You should probably TRAIN this model on
a down-stream task to be able to use it for predictions and inference.</p>
</blockquote>
<p>Can anyone tell me if I am doing anything wrong? What does the warning mean? I saw issue #5588 on the github repo of Transformers. Don't know if my issue is the same as this.</p>
","8703627","","","","","2020-10-01 14:19:19","Loading a converted pytorch model in huggingface transformers properly","<python><tensorflow><machine-learning><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67299510","1","67906030","","2021-04-28 11:38:49","","0","73","<p>Using tutorials <a href=""https://huggingface.co/transformers/model_doc/gpt2.html"" rel=""nofollow noreferrer"">here</a> , I wrote the following codes:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')

inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
</code></pre>
<p>So I realize that &quot;inputs&quot;, consists of tokenized items of my sentence.
But how can I get the values of tokenized items? (see for example [&quot;hello&quot;, &quot;,&quot;, &quot;my&quot;, &quot;dog&quot;, &quot;is&quot;, &quot;cute&quot;])</p>
<p>I am asking this because sometimes I think it separetes a word if that word is not in its dictionary (i.e., a word from another language). So I want to check that in my codes.</p>
","2178942","","6664872","","2021-05-12 22:09:27","2021-06-09 14:19:35","Understanding how gpt-2 tokenizes the strings","<python><huggingface-transformers><transformer><gpt-2>","1","1","","","","CC BY-SA 4.0"
"64023547","1","64024615","","2020-09-23 08:04:00","","1","94","<p>I have a <code>BertTokenizer</code> (<code>tokenizer</code>) and a <code>BertModel</code> (<code>model</code>) from the <code>transformers</code> library.
<strong>I have pre-trained the model from scratch</strong> with a few wikipedia articles, just to test how it works.</p>
<p>Once the model is pre-trained, <strong>I want to extract a layer vector representation for a given sentence</strong>. For that, I calculate the average of the 11 hidden (768-sized) vectors. I do this as follows (<code>line</code> is a single <code>String</code>):</p>
<pre><code>padded_sequence = tokenizer(line, padding=True)
        
indexed_tokens = padded_sequence['input_ids']
attention_mask = padded_sequence[&quot;attention_mask&quot;]

tokens_tensor = torch.tensor([indexed_tokens])
attention_mask_tensor = torch.tensor([attention_mask])

outputs = model(tokens_tensor, attention_mask_tensor)
hidden_states = outputs[0]

line_vectorized = hidden_states[0].data.numpy().mean(axis=0)
</code></pre>
<p>So far so good. <strong>I can do this for every sentence individually. But now I want to do it in batch</strong>, ie. I have a bunch of sentences and instead of iterating each sentence I send the appropiate tensor representations to get all vectors at once. I do this as follows (<code>lines</code> is a <code>list of Strings</code>):</p>
<pre><code>padded_sequences = self.tokenizer_PYTORCH(lines, padding=True)
        
indexed_tokens_list = padded_sequences['input_ids']
attention_mask_list = padded_sequences[&quot;attention_mask&quot;]
        
tokens_tensors_list = [torch.tensor([indexed_tokens]) for indexed_tokens in indexed_tokens_list]
attention_mask_tensors_list = [torch.tensor([attention_mask ]) for attention_mask in attention_mask_list ]
        
tokens_tensors = torch.cat((tokens_tensors_list), 0)
attention_mask_tensors = torch.cat((attention_mask_tensors_list ), 0)

outputs = model(tokens_tensors, attention_mask_tensors)
hidden_states = outputs[0]

lines_vectorized = [hidden_states[i].data.numpy().mean(axis=0) for i in range(0, len(hidden_states))]
</code></pre>
<p>The problem is the following: <strong>I have to use padding so that I can appropiately concatenate the token tensors</strong>. That means that the indexed tokens and the attention masks can be larger than in the previous case where the sentences were evaluated individually. <strong>But when I use padding, I get different results for the sentences which have been padded</strong>.</p>
<p><em>EXAMPLE</em>:
I have two sentences (in French but it doesn't matter):</p>
<p><code>sentence_A</code> = &quot;appareil digestif un article de wikipedia l encyclopedie libre&quot;</p>
<p><code>sentence_B</code> = &quot;sauter a la navigation sauter a la recherche cet article est une ebauche concernant la biologie&quot;</p>
<p>When I evaluate the two sentences <strong>individually</strong>, I obtain:</p>
<p><code>sentence_A</code>:</p>
<pre><code>indexed_tokens =  [10002, 3101, 4910, 557, 73, 3215, 9630, 2343, 4200, 8363, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.9304411   0.53798294 -1.6231083 ...]
</code></pre>
<p><code>sentence_B</code>:</p>
<pre><code>indexed_tokens =  [10002, 2217, 6496, 1387, 9876, 2217, 6496, 1387, 4441, 405, 73, 6451, 3, 2190, 5402, 1387, 2971, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.8077076   0.56028104 -1.5135447  ...]
</code></pre>
<p>But when I evaluate the two sentences <strong>in batch</strong>, I obtain:</p>
<p><code>sentence_A</code>:</p>
<pre><code>indexed_tokens =  [10002, 3101, 4910, 557, 73, 3215, 9630, 2343, 4200, 8363, 10000, 10004, 10004, 10004, 10004, 10004, 10004, 10004]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
line_vectorized =  [-1.0473819   0.6090186  -1.727466  ...]
</code></pre>
<p><code>sentence_B</code>:</p>
<pre><code>indexed_tokens =  [10002, 2217, 6496, 1387, 9876, 2217, 6496, 1387, 4441, 405, 73, 6451, 3, 2190, 5402, 1387, 2971, 10000]
attention_mask =  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
line_vectorized =  [-0.8077076   0.56028104 -1.5135447  ...]
</code></pre>
<p>That is, <strong>since <code>sentence_B</code> is larger than <code>sentence_A</code>, <code>sentence_A</code> has been padded and the attention mask has been padded with zeros as well</strong>. The indexed tokens contain now extra tokens (<code>10004</code> which I assume <code>empty</code>).
The vector representation of <code>sentence_B</code> has NOT changed. But <strong>the vector representation of <code>sentence_A</code> HAS CHANGED</strong>.</p>
<p>I would like to know if this is working as intended or not (I assume not).
And I guess I am doing something wrong but I can't figure out what.</p>
<p>Any ideas?</p>
","14190580","","","","","2020-09-23 09:29:56","Inconsistent vector representation using transformers BertModel and BertTokenizer","<python><nlp><bert-language-model><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"63662548","1","63665329","","2020-08-30 21:59:51","","1","639","<p>I am trying to run the following <a href=""https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb"" rel=""nofollow noreferrer"">Hugging Face Transformers tutorial</a> on GCP's AI Platform Notebook with 32 vCPUs, 208 GB RAM, and 2 NVIDIA Tesla T4s.</p>
<p>However, when I try to run the part</p>
<p><code>model = DistillBERTClass()</code></p>
<p><code>model.to(device)</code></p>
<p>I get the following Assertion Error:</p>
<pre><code>AssertionError: The NVIDIA driver on your system is too old (found version 10010).
Please update your GPU driver by downloading and installing a new
version from the URL: http://www.nvidia.com/Download/index.aspx
Alternatively, go to: https://pytorch.org to install
a PyTorch version that has been compiled with your version
of the CUDA driver.
</code></pre>
<p>However, when I run
!nvidia-smi</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   38C    P0    22W /  70W |     10MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |
| N/A   39C    P8    10W /  70W |     10MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
</code></pre>
<p>The version on the NVIDIA driver is compatible with the latest PyTorch version, which I am using.
Has anyone else ran into this error, and is there a way around it?</p>
","3550693","","3550693","","2020-08-31 00:51:28","2020-10-14 11:05:40","GCP AI Platform Notebook driver too old?","<pytorch><nvidia><bert-language-model><huggingface-transformers><gcp-ai-platform-notebook>","2","0","","","","CC BY-SA 4.0"
"64342621","1","64342707","","2020-10-13 20:20:30","","0","474","<p>I have a dataframe which contains survey answers. Three of those columns are open-ended answers. Using HuggingFace NLP I'm using a pre-trained sentiment analysis classifier. Please find the code below:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
model_name = &quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
classifier(&quot;This community is so helpful!&quot;)
</code></pre>
<p>The results for the classifier test is: &quot;[{'label': '5 stars', 'score': 0.800311}]</p>
<p>What I'd like to do is have the classifier run on my open-ended responses and, in new columns in my dataframe, have it include the stars and ranking score.</p>
<p>Any help would be greatly appreciated.</p>
<p>edit: I uploaded the dataset through a local csv. The dataframe column name I want to work with is &quot;Q72&quot;</p>
","12020223","","12020223","","2020-10-13 21:16:34","2020-10-13 21:21:54","How to apply a sentiment classifier to a dataframe","<python><pandas><huggingface-transformers>","1","3","1","2020-10-14 07:41:50","","CC BY-SA 4.0"
"64001128","1","","","2020-09-21 23:23:26","","11","20416","<p>From the documentation <a href=""https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained"" rel=""noreferrer"">for from_pretrained</a>, I understand I don't have to download the pretrained vectors every time, I can save them and load from disk with this syntax:</p>
<pre><code>  - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.
  - (not applicable to all derived classes, deprecated) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.
</code></pre>
<p>So, I went to the model hub:</p>
<ul>
<li><a href=""https://huggingface.co/models"" rel=""noreferrer"">https://huggingface.co/models</a></li>
</ul>
<p>I found the model I wanted:</p>
<ul>
<li><a href=""https://huggingface.co/bert-base-cased"" rel=""noreferrer"">https://huggingface.co/bert-base-cased</a></li>
</ul>
<p>I downloaded it from the link they provided to this repository:</p>
<blockquote>
<p>Pretrained model on English language using a masked language modeling
(MLM) objective. It was introduced in this paper and first released in
this repository. This model is case-sensitive: it makes a difference
between english and English.</p>
</blockquote>
<p>Stored it in:</p>
<pre><code>  /my/local/models/cased_L-12_H-768_A-12/
</code></pre>
<p>Which contains:</p>
<pre><code> ./
 ../
 bert_config.json
 bert_model.ckpt.data-00000-of-00001
 bert_model.ckpt.index
 bert_model.ckpt.meta
 vocab.txt
</code></pre>
<p>So, now I have the following:</p>
<pre><code>  PATH = '/my/local/models/cased_L-12_H-768_A-12/'
  tokenizer = BertTokenizer.from_pretrained(PATH, local_files_only=True)
</code></pre>
<p>And I get this error:</p>
<pre><code>&gt;           raise EnvironmentError(msg)
E           OSError: Can't load config for '/my/local/models/cased_L-12_H-768_A-12/'. Make sure that:
E           
E           - '/my/local/models/cased_L-12_H-768_A-12/' is a correct model identifier listed on 'https://huggingface.co/models'
E           
E           - or '/my/local/models/cased_L-12_H-768_A-12/' is the correct path to a directory containing a config.json file
</code></pre>
<p>Similarly for when I link to the config.json directly:</p>
<pre><code>  PATH = '/my/local/models/cased_L-12_H-768_A-12/bert_config.json'
  tokenizer = BertTokenizer.from_pretrained(PATH, local_files_only=True)

        if state_dict is None and not from_tf:
            try:
                state_dict = torch.load(resolved_archive_file, map_location=&quot;cpu&quot;)
            except Exception:
                raise OSError(
&gt;                   &quot;Unable to load weights from pytorch checkpoint file. &quot;
                    &quot;If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. &quot;
                )
E               OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.
</code></pre>
<p>What should I do differently to get huggingface to use my local pretrained model?</p>
<h1>Update to address the comments</h1>
<pre><code>YOURPATH = '/somewhere/on/disk/'

name = 'transfo-xl-wt103'
tokenizer = TransfoXLTokenizerFast(name)
model = TransfoXLModel.from_pretrained(name)
tokenizer.save_pretrained(YOURPATH)
model.save_pretrained(YOURPATH)

&gt;&gt;&gt; Please note you will not be able to load the save vocabulary in Rust-based TransfoXLTokenizerFast as they don't share the same structure.
('/somewhere/on/disk/vocab.bin', '/somewhere/on/disk/special_tokens_map.json', '/somewhere/on/disk/added_tokens.json')

</code></pre>
<p>So all is saved, but then....</p>
<pre><code>YOURPATH = '/somewhere/on/disk/'
TransfoXLTokenizerFast.from_pretrained('transfo-xl-wt103', cache_dir=YOURPATH, local_files_only=True)

    &quot;Cannot find the requested files in the cached path and outgoing traffic has been&quot;
ValueError: Cannot find the requested files in the cached path and outgoing traffic has been disabled. To enable model look-ups and downloads online, set 'local_files_only' to False.
</code></pre>
","1052117","","1052117","","2020-10-05 18:38:09","2021-07-15 13:15:02","Load a pre-trained model from disk with Huggingface Transformers","<huggingface-transformers>","3","4","1","","","CC BY-SA 4.0"
"67308013","1","","","2021-04-28 21:16:47","","1","55","<p>This is my first question on StackOverflow. I am working on the <strong>CUAD</strong>(Contract Understanding Atticus Dataset) which is a Q&amp;A based dataset. But training 80% of the dataset in one go is impossible due to resource constraints. I am using the boilerplate code provided by HuggingFace Transformer docs for Q&amp;A task <a href=""https://huggingface.co/transformers/examples.html"" rel=""nofollow noreferrer"">here</a>. My hands are tied with Google Colab Pro. So, it's not possible for me to use multiple GPU's in training the dataset. Inspite of using the hyperparameters below, I'm unable to avoid errors due to memory constraints like &quot;CUDA out of Memory&quot; etc.</p>
<pre><code>args = TrainingArguments(
    'cuad-roberta',
    evaluation_strategy = &quot;epoch&quot;,
    learning_rate=3e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    save_steps=5000,
    logging_steps=5000,
    save_total_limit=100,
    gradient_accumulation_steps = 12,
    eval_accumulation_steps = 4,
)
</code></pre>
<p>Under these circumstances, I have divided my training set(80%) into 4 parts with each part holding 25% data. So, using any Q&amp;A supported pretrained model from Transformers, I've trained the first 25% of the training data and then saved the model in a directory of my drive. Then, I have loaded that tokenizer and model from the saved directory and trained the next 25% of my training data on the same model as shown below.</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/models/cuad-25%-roberta-base')
model = AutoModelForQuestionAnswering.from_pretrained('/content/drive/MyDrive/models/cuad-25%-roberta-base')
</code></pre>
<p>I repeated the step two more times to complete training the model on the entire training data.</p>
<p>Now, my question is that, <strong>Is this approach correct in terms of training a model when I have resource constraints? If it is correct, will this approach hurt the performance of my model?</strong> I'm relatively new to ML and NLP so please kindly consider any silly mistakes.</p>
<p>Also, any sources for understanding, visualising or implementing the Q&amp;A task through HuggingFace Transformers would be really helpful.</p>
","8753037","","","","","2021-04-28 21:16:47","Training a model on an entire dataset by dividing the dataset into chunks & loading the model back again untill all chunks of the dataset are trained","<pytorch><huggingface-transformers><transformer><pre-trained-model><question-answering>","0","1","0","","","CC BY-SA 4.0"
"65598192","1","","","2021-01-06 15:01:16","","0","45","<p>I am using the tensorflow version of a pretrained Bert in huggingface to encode batches of sentences with varying batch size.
That is, I need to use Bert repeatedly to encode a number of inputs, where each input has <code>[n_sentences, 512]</code> dimensionality (where 512 is the number of tokens).
N_sentences can vary between 2 and 250 across each input.</p>
<p>This is proving <em>very</em> time consuming: encoding each input/example takes several seconds, especially for larger values of <code>n_sentences</code>.</p>
<p>Is there a way to parallelize the <code>model(input)</code> call (where input has dimensionality [n_sentences, 512]) in Google Colab's TPU (or on GPUs), such that more than one sentence can be encoded at once?</p>
","6931486","","","","","2021-01-06 15:01:16","parallelize call to BERT for large batches","<deep-learning><parallel-processing><bert-language-model><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"64180517","1","","","2020-10-03 03:51:06","","1","1064","<p>I have the following code attempting to use XL transformers to vectorize text:</p>
<pre><code>  text = &quot;Some string about 5000 characters long&quot;

  tokenizer = TransfoXLTokenizerFast.from_pretrained('transfo-xl-wt103', cache_dir=my_local_dir, local_files_only=True)
  model = TransfoXLModel.from_pretrained(&quot;transfo-xl-wt103&quot;, cache_dir=my_local_dir, local_files_only=True)

  encoded_input = tokenizer(text, return_tensors='pt') 
  output = model(**encoded_input)
</code></pre>
<p>This produces:</p>
<pre><code>    output = model(**encoded_input)
  File &quot;/home/user/w/default/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/home/user/w/default/lib/python3.7/site-packages/transformers/modeling_transfo_xl.py&quot;, line 863, in forward
    output_attentions=output_attentions,
  File &quot;/home/user/w/default/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/home/user/w/default/lib/python3.7/site-packages/transformers/modeling_transfo_xl.py&quot;, line 385, in forward
    dec_inp, r, attn_mask=dec_attn_mask, mems=mems, head_mask=head_mask, output_attentions=output_attentions,
  File &quot;/home/user/w/default/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/home/user/w//default/lib/python3.7/site-packages/transformers/modeling_transfo_xl.py&quot;, line 338, in forward
    attn_score = attn_score.float().masked_fill(attn_mask[:, :, :, None], -1e30).type_as(attn_score)
RuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 2007869696 bytes. Error code 12 (Cannot allocate memory)
</code></pre>
<p>I'm a little perplexed by this because this is asking for 2007869696, which is only 2GB and this machine has 64G of RAM. So I both don't understand why it is asking for this, and even more, why it is failing to get it.</p>
<p>Where can I change the setting that controls this and allow this process more RAM?  This is such a small invocation of the example code, and I just see very few places that would even accept this argument.</p>
","1052117","","1052117","","2020-10-03 14:44:39","2020-10-06 15:05:59","Huggingface transformers unusual memory use","<python><pytorch><huggingface-transformers>","1","3","0","","","CC BY-SA 4.0"
"64180606","1","","","2020-10-03 04:10:57","","0","110","<p>I have the following code:</p>
<pre><code>  text = &quot;Some string about 5000 characters long&quot;

  tokenizer = TransfoXLTokenizerFast.from_pretrained('transfo-xl-wt103', cache_dir=my_local_dir, local_files_only=True)
  model = TransfoXLModel.from_pretrained(&quot;transfo-xl-wt103&quot;, cache_dir=my_local_dir, local_files_only=True)

  encoded_input = tokenizer(text, return_tensors='pt') 
  output = model(**encoded_input)
</code></pre>
<p>When I change <code>return_tensors='pt'</code> to <code>return_tensors='np'</code>, I expect the return type of the vectors to change from pytorch to numpy.</p>
<p>I understand from <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"" rel=""nofollow noreferrer"">this documentation</a> that this should be feasible, and that while the input_ids are covered <a href=""https://huggingface.co/transformers/glossary.html#attention-mask"" rel=""nofollow noreferrer"">here</a>, I expect it to be able to return.  However I get:</p>
<pre><code>    output = model(**encoded_input)
  File &quot;/home/user/w/default/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/home/user/w/default/lib/python3.7/site-packages/transformers/modeling_transfo_xl.py&quot;, line 795, in forward
    input_ids = input_ids.transpose(0, 1).contiguous()
AttributeError: 'numpy.ndarray' object has no attribute 'contiguous'
</code></pre>
<p>What's happening?  How should I differently prepare <code>**encoded_input</code> when I want to return numpy vectors versus pytorch vectors?</p>
","1052117","","","","","2020-10-03 04:10:57","Huggingface changing from pytorch to numpy","<python><numpy><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"64937822","1","","","2020-11-20 22:33:55","","1","311","<p>I have a saved transformers model using <code>BertModel.from_pretrained('test_model')</code></p>
<p>I have trained this model using google colab's GPUs</p>
<p>Then, I want to open it, with <code>BertModel.from_pretrained('test_model/')</code>
but I do not  have a GPU in my local PC. I get this:</p>
<pre><code>/home/seiji/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() &gt; 0
</code></pre>
<p>What shoud I do? I have no idea of how can I open it using a CPU. And is it possible?</p>
","11444715","","","","","2020-11-23 09:28:31","Locally opening a transformers saved model","<python><nlp><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"63661017","1","","","2020-08-30 18:50:36","","1","110","<p>I am using <code>transformers</code> <code>TFBertForSequenceClassification.from_pretrained</code> with 'bert-base-multilingual-uncased') and <code>keras</code> to build my model.</p>
<pre><code>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# metric
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

# optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)

# create and compile the Keras model in the context of strategy.scope
model = TFBertForSequenceClassification.from_pretrained(pretrained_weights,
                                                        num_labels=num_labels,
                                                        cache_dir=pretrained_model_dir)
model._name = 'tf_bert_classification'

# compile Keras model
model.compile(optimizer=optimizer,
              loss=loss,
              metrics=[metric])
</code></pre>
<p>I am using <code>SST2</code> data, that are tokenize and the feed to the model for training. The data have the following shape:</p>
<pre><code>    shape: (32,)
    dict structure
       dim: 3
       [input_ids       / attention_mask  / token_type_ids ]
       [(32, 128)       / (32, 128)       / (32, 128)      ]
       [ndarray         / ndarray         / ndarray        ]
</code></pre>
<p>and here an example:</p>
<pre><code>({'input_ids': &lt;tf.Tensor: shape=(32, 128), dtype=int32, numpy=
array([[  101, 21270, 94696, ...,     0,     0,     0],
       [  101,   143, 45100, ...,     0,     0,     0],
       [  101, 24220,   102, ...,     0,     0,     0],
       ...,
       [  101, 11008, 10346, ...,     0,     0,     0],
       [  101, 43062, 15648, ...,     0,     0,     0],
       [  101, 13178, 18418, ...,     0,     0,     0]], dtype=int32)&gt;, 'attention_mask': ....
</code></pre>
<p>As we can see we have <code>input_ids</code>  with shape (32, 128) where 32 is the batch size and 128 is the maxiumum length of the string (max for BERT is 512). We also have <code>attention_mask</code> and <code>token_type_ids</code> with the same structure.</p>
<p>I am able to train a model and to do prediction using <code>model.evaluate(test_dataset)</code>. All good.</p>
<p>The issue that I am having is that when I serve the model on GCP, then it require data in a different input shape and structure! I saw the same if I run the cli on the saved model:</p>
<pre><code>saved_model_cli show --dir $MODEL_LOCAL --tag_set serve --signature_def serving_default

The given SavedModel SignatureDef contains the following input(s):
  inputs['input_ids'] tensor_info:
      dtype: DT_INT32
      shape: (-1, 5)
      name: serving_default_input_ids:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['output_1'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 2)
      name: StatefulPartitionedCall:0
Method name is: tensorflow/serving/predict
</code></pre>
<p>As we can see we only need to give <code>input_ids</code> and not (<code>attention_mask</code> and <code>token_type_ids</code>) and the sape is different. While the batch size is not defined (-1) which expected, the maxium length is 5 instead of 128!It was working 2 months ago and I probably introduce something that created this issue.</p>
<p>I tried few version of <code>Tensorfow</code> (<code>2.2.0</code> and <code>2.3.0</code>) and transformers (<code>2.8.0</code>, <code>2.9.0</code> and <code>3.0.2</code>). I cannot see the Keras'model input and outpu shape (None):</p>
<pre><code>model.inputs


model.outputs


model.summary()

Model: &quot;tf_bert_classification&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
bert (TFBertMainLayer)       multiple                  167356416 
_________________________________________________________________
dropout_37 (Dropout)         multiple                  0         
_________________________________________________________________
classifier (Dense)           multiple                  1538      
=================================================================
Total params: 167,357,954
Trainable params: 167,357,954
Non-trainable params: 0
</code></pre>
<p>Any idea what could explain that the saved model require a different input that the one use for training! I could use the Keras functional API and defined the input shape but I am pretty sure the this code was working before.</p>
","6430839","","","","","2021-01-17 13:46:55","""saved_model_cli show"" on a keras+transformers model display different inputs and shapes that the one used for training","<keras><google-cloud-platform><tensorflow2.0><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"64180222","1","","","2020-10-03 02:44:01","","0","361","<p>I am working on a multilabel text classification task with Bert.</p>
<p>The following is the code for generating an iterable Dataset.</p>
<pre><code>from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

train_set = TensorDataset(X_train_id,X_train_attention, y_train)
test_set = TensorDataset(X_test_id,X_test_attention,y_test)

train_dataloader = DataLoader(
    train_set,
    sampler = RandomSampler(train_set),
    drop_last=True,
    batch_size=13
)

test_dataloader = DataLoader(
    test_set,
    sampler = SequentialSampler(test_set),
    drop_last=True,
    batch_size=13
)
</code></pre>
<p>The following are the the dimensions of the training set:</p>
<p>In[]</p>
<pre><code>print(X_train_id.shape)
print(X_train_attention.shape)
print(y_train.shape)
</code></pre>
<p>Out[]</p>
<pre><code>torch.Size([262754, 512])
torch.Size([262754, 512])
torch.Size([262754, 34])
</code></pre>
<p>There should be 262754 rows each with 512 columns. The output should predict the values from 34 possible labels. I am breaking them down into batches of 13.</p>
<p>Training code</p>
<pre><code>optimizer = AdamW(model.parameters(), lr=2e-5)
# Training
def train(model):
    model.train()
    train_loss = 0
    for batch in train_dataloader:
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
        optimizer.zero_grad()
        loss, logits = model(b_input_ids, 
                             token_type_ids=None, 
                             attention_mask=b_input_mask, 
                             labels=b_labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        train_loss += loss.item()
    return train_loss


# Testing
def test(model):
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch in test_dataloader:
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)
            with torch.no_grad():        
                (loss, logits) = model(b_input_ids, 
                                    token_type_ids=None, 
                                    attention_mask=b_input_mask,
                                    labels=b_labels)
            val_loss += loss.item()
    return val_loss

# Train task
max_epoch = 1
train_loss_ = []
test_loss_ = []

for epoch in range(max_epoch):
    train_ = train(model)
    test_ = test(model)
    train_loss_.append(train_)
    test_loss_.append(test_)

</code></pre>
<p>Out[]</p>
<pre><code>Expected input batch_size (13) to match target batch_size (442).
</code></pre>
<p>This is the description of my model:</p>
<pre><code>from transformers import BertForSequenceClassification, AdamW, BertConfig

model = BertForSequenceClassification.from_pretrained(
    &quot;cl-tohoku/bert-base-japanese-whole-word-masking&quot;, # æ—¥æœ¬èªžPre trainedãƒ¢ãƒ‡ãƒ«
    num_labels = 34, 
    output_attentions = False,
    output_hidden_states = False,
)
</code></pre>
<p>I have clearly stated that I want the batch size to be 13. However, during the training process pytorch is throwing a Runtime Error</p>
<p>Where is the number 442 even coming from? I have clearly stated that I want each batch to have a size of 13 rows.</p>
<p>I have already confirmed that each batch has input_id with dimensions [13,512], attention tensor with dimensions [13,512], and labels with dimensions [13,34].</p>
<p>I have tried caving in and using a batch size of 442 when initializing the DataLoader, but after a single batch iteration, it throws another <code>Pytorch Value Error Expected:  input batch size does not match target batch size</code>, this time showing:</p>
<pre><code>ValueError: Expected input batch_size (442) to match target batch_size (15028).
</code></pre>
<p>Why does the batch size keep on changing? Where is the number 15028 even coming from?</p>
<p>The following are some of the answers I have looked through, but had no luck on applying to my source code:</p>
<p><a href=""https://discuss.pytorch.org/t/valueerror-expected-input-batch-size-324-to-match-target-batch-size-4/24498"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/valueerror-expected-input-batch-size-324-to-match-target-batch-size-4/24498</a></p>
<p><a href=""https://discuss.pytorch.org/t/valueerror-expected-input-batch-size-1-to-match-target-batch-size-64/43071"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/valueerror-expected-input-batch-size-1-to-match-target-batch-size-64/43071</a></p>
<p><a href=""https://stackoverflow.com/questions/54928638/pytorch-cnn-error-expected-input-batch-size-4-to-match-target-batch-size-64"">Pytorch CNN error: Expected input batch_size (4) to match target batch_size (64)</a></p>
<p>Thanks in advance. Your support is truly appreciated :)</p>
","6714771","","6714771","","2020-10-03 03:15:58","2020-10-06 09:51:10","Batch size keeps on changin, throwing `Pytorch Value Error Expected: input batch size does not match target batch size`","<python><pytorch><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"64883224","1","","","2020-11-17 20:58:03","","0","137","<p>I am using model from haggingfaces (Arabic-ALBERT)to do text classification task, the following is a snapshot of my code:</p>
<pre><code>model_name = 'kuisailab/albert-base-arabic' 
num_labels = 2
task_name = 'classification'

model_config = AutoConfig.from_pretrained(model_name,num_labels=num_labels) ##needed for the visualizations
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Load model to defined device.
model.to(device)
print('Model loaded to `%s`'%device)

for param in model.parameters():
    param.requires_grad = False

## Add three new layers at the end of the network
model.classifier = nn.Sequential(
    nn.Linear(768, 256),
    nn.ReLU(),
    nn.Linear(256, 64),
    nn.ReLU(),
    nn.Linear(64, 2),
    nn.Softmax(dim=1)
)

model = model.to(device)


from transformers import BertForSequenceClassification, AdamW, BertConfig

criterion = nn.MSELoss().to(device)
##optimizer = optim.SGD(model.classifier.parameters(), lr=0.01)  acc=0.70
optimizer = AdamW(model.parameters(),
                  lr = 0.001  ,      ###lr = 2e-5  acc=0.75 ### learning rate 
                  eps = 1e-8
                )
</code></pre>
<p>the following function is for the text preprocessing and tokenizing:</p>
<pre><code>  def preprocess_text(text):
    parts = []

    text_len = len(text.split(' '))
    delta = 300
    max_parts = 5
    nb_cuts = int(text_len / delta)
    nb_cuts = min(nb_cuts, max_parts)
        
    for i in range(nb_cuts + 1):
        text_part = ' '.join(text.split(' ')[i * delta: (i + 1) * delta])
        parts.append(tokenizer.encode(text_part, return_tensors=&quot;pt&quot;, max_length=500).to(device))

    return parts
</code></pre>
<p>and here is the training loop code which caused the error</p>
<pre><code>    print_every = 300

total_loss = 0
all_losses = []

CUDA_LAUNCH_BLOCKING=1

model.train()

for idx, row in train_data.iterrows():
    text_parts = preprocess_text(str(row['sentence']))
    label = torch.tensor([row['label']]).long().to(device)

    optimizer.zero_grad()
overall_output = torch.zeros((1, 2)).float().to(device)
for part in text_parts:
    if len(part) &gt; 0:
        try:
            input = part.reshape(-1)[:512].reshape(1, -1)
            # print(input.shape)
            overall_output += model(input, labels=label)[1].float().to(device)
        except Exception as e:
            print(str(e))
`overall_output /= len(text_parts)`
overall_output = F.softmax(overall_output[0], dim=-1)

if label == 0:
    label = torch.tensor([1.0, 0.0]).float().to(device)
elif label == 1:
    label = torch.tensor([0.0, 1.0]).float().to(device)

# print(overall_output, label)

loss = criterion(overall_output, label)
total_loss += loss.item()

loss.backward()
optimizer.step()

if idx % print_every == 0 and idx &gt; 0:
    average_loss = total_loss / print_every
    print(&quot;{}/{}. Average loss: {}&quot;.format(idx, len(train_data), average_loss))
    all_losses.append(average_loss)
    total_loss = 0
</code></pre>
<p>any suggestion to solve  the error &quot;RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn&quot;</p>
","13201830","","6664872","","2020-11-27 23:03:47","2020-11-27 23:03:47","RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn. training pytorch","<machine-learning><nlp><classification><bert-language-model><huggingface-transformers>","0","1","1","","","CC BY-SA 4.0"
"63953597","1","63995083","","2020-09-18 10:03:06","","0","788","<p>I'm trying to use Huggingface zero-shot text classification using 12 labels with large data set (57K sentences) read from a CSV file as follows:</p>
<pre><code>csv_file = tf.keras.utils.get_file('batch.csv', filename)
df = pd.read_csv(csv_file)
classifier = pipeline('zero-shot-classification')
results = classifier(df['description'].to_list(), labels, multi_class=True)
</code></pre>
<p>This keeps crashing as python runs out of memory.
I tried to create a dataset instead as follows:</p>
<pre><code>dataset = load_dataset('csv', data_files=filename)
</code></pre>
<p>But not sure how to use it with Huggingface's classifier. What is the best way to batch process classification?</p>
<p>I eventually would like to feed it over 1M sentences for classification.</p>
","944109","","944109","","2020-09-18 10:55:39","2021-07-05 12:17:52","Using Huggingface zero-shot text classification with large data set","<python><huggingface-transformers>","2","4","1","","","CC BY-SA 4.0"
"62750798","1","","","2020-07-06 06:59:36","","3","352","<p>The idea behind this is I want to try some old school gradient ascent style visualization with Bert Model.</p>
<p>I want to know the effect of input on a specific layer's specific dimension. Thus, I took the gradient of the output of a specific layer's specific dimension wrt the first word embedding layer's output.</p>
<p>The best thing I can do here is the following:</p>
<pre><code>from transformers import BertTokenizer, BertModel
model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True,output_hidden_states=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=True)
s = 'I want to sleep'
inputs = tokenizer.encode_plus(s,return_tensors='pt', add_special_tokens=False,is_pretokenized=True)
input_ids = inputs['input_ids']
output = model(input_ids)
hidden_states = output[-2]

X = hidden_states[0] #embedding space, shape: [1,4,768] (batch_size,sentence_length,embedding dimension)
y = hidden_states[3][0][0][0] ##the 0th position and 0th dimension of output of 3rd hidden layer. Dimension should just be [1], a scalar.

torch.autograd.grad(y,X,retain_graph=True, create_graph=True) #I take the gradient of y wrt. Since y is scalar. The dimension of the gradient is just the dimension of X. 
</code></pre>
<p>This is, however, not good enough. I want the gradient wrt the actual word embedding layer. However, Transformer's embedding contains &quot;position_embedding&quot; and &quot;token_type_embedding&quot;. Here's the code for the first layer embedding:</p>
<pre><code>class BertEmbeddings(nn.Module):
    &quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.
    &quot;&quot;&quot;
    def __init__(self, config):
        super(BertEmbeddings, self).__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)

        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
        # any TensorFlow checkpoint file
        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, input_ids, token_type_ids=None, position_ids=None):
        seq_length = input_ids.size(1)
        if position_ids is None:
            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)

        words_embeddings = self.word_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)

        embeddings = words_embeddings + position_embeddings + token_type_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings
</code></pre>
<p>Ideally, I want the gradient wrt JUST â€œwords_embeddings&quot; Rather than, wrt &quot;words_embeddings + position_embeddings + token_type_embeddings&quot; and follows by layerNorm and dropout.</p>
<p>I think I can do this by modifying changing the model. Is there a way to it without changing the model?</p>
","13802302","","249341","","2020-11-16 14:05:37","2020-11-16 14:05:37","How to get gradient wrt to a specific layer's output pytorch","<python><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"67300403","1","","","2021-04-28 12:37:19","","0","95","<p>I am trying to adapt the longformer transformer TF model from huggingface into a bigger three class classification model, i have gotten the model to compile but i cannot run a test example on it. The model and attempted output is as below:</p>
<pre><code>import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GaussianNoise,LocallyConnected2D,LocallyConnected1D,Input,MaxPooling1D,Dense,Dropout,BatchNormalization,LSTM,GRU,ConvLSTM2D,Flatten,LayerNormalization,TimeDistributed,Conv1D,Reshape,Masking
from tensorflow.keras import backend as K
import pathlib 
from tensorflow.keras.callbacks import Callback
from tensorflow.keras import regularizers,callbacks
import numpy as np
from tensorflow.keras.layers import Concatenate
from transformers import TFLongformerModel, LongformerTokenizer




if __name__ == &quot;__main__&quot;:
    
    model_longformer = TFLongformerModel.from_pretrained('longformer-base-4096',output_hidden_states=True)
    print(model_longformer.summary())
    
    
    input_ids = tf.keras.Input(shape=(4096),dtype='int32')
    attention_mask = tf.keras.Input(shape=(4096), dtype='int32')
    opt=Adam()
    transformer = model_longformer([input_ids, attention_mask])   
    transformer_outputs = transformer[1] #sequence output
    print(&quot;Transformer output shape:&quot;)
    print(transformer_outputs.shape)
    #Grab the last 64 sequence entries, out of allegedly (,768). This is the bit 
    #that causes the error to mention the number '-63'
    hidden_states_size =  64 
    hiddes_states_ind = list(range(-hidden_states_size, 0, 1))
    selected_hidden_states = tf.keras.layers.concatenate(tuple([transformer_outputs[i] for i in hiddes_states_ind]))
    print(selected_hidden_states.shape)
    #array_hidden = np.asarray(selected_hiddes_states)
    #flatter_longformer_1 = Flatten(array_hidden)
    reshape_longformer_1 = Reshape((1,1,),input_shape=(49152,))(selected_hidden_states) #49152 = 64*768

    rnn_cells = [tf.keras.layers.GRUCell(64,dropout=0.5,recurrent_dropout=0.25,kernel_regularizer=regularizers.l2(0.005)),tf.keras.layers.GRUCell(64,kernel_regularizer=regularizers.l2(0.005),dropout=0,recurrent_dropout=0)]
    stacked_gru = tf.keras.layers.StackedRNNCells(rnn_cells)
    gru_layer = tf.keras.layers.RNN(stacked_gru)(reshape_longformer_1)
    bn_merge = BatchNormalization()(gru_layer)
    drop_merge = Dropout(0.1)(bn_merge)
    dense_1 = Dense(25,kernel_regularizer=regularizers.l2(0.0))(drop_merge) #0.015
    bn_dense_1 = BatchNormalization()(dense_1)
    drop_dense_1 = Dropout(0.1)(bn_dense_1)
    dense_final = Dense(3, activation = &quot;softmax&quot;)(drop_dense_1)

    model = Model(inputs=[input_ids, attention_mask], outputs=dense_final)
    model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=opt)
    print(model.summary())
    text_input = &quot;Queensland detectives are investigating the death of a man after he died in hospital yesterday.  9News understands an altercation took place between the man - who lives at a unit complex in the Brisbane suburb of Stafford - and a group of friends while they were drinking last week.  The altercation resulted in the man being stuck in the back of the head a number of times, with him then being rushed to hospital.  The man died from the injuries in hospital yesterday.&quot;



    tokenizer = LongformerTokenizer.from_pretrained(&quot;allenai/longformer-base-4096&quot;)
    encoded_input = tokenizer(text_input, return_tensors='tf',padding='max_length',max_length=4096)

    model([encoded_input['input_ids'],encoded_input['attention_mask']])
</code></pre>
<p>Which outputs:</p>
<pre><code>Model: &quot;tf_longformer_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
longformer (TFLongformerMain multiple                  148659456
=================================================================
Total params: 148,659,456
Trainable params: 148,659,456
Non-trainable params: 0
_________________________________________________________________
None
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:From C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\ops\array_ops.py:5041: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.
Instructions for updating:
The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.
Transformer output shape:
(None, 768)
(49152,)
Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 4096)]       0
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 4096)]       0
__________________________________________________________________________________________________
tf_longformer_model (TFLongform TFLongformerBaseMode 148659456   input_1[0][0]
                                                                input_2[0][0]
__________________________________________________________________________________________________
tf.__operators__.getitem (Slici (768,)               0           tf_longformer_model[0][14]
__________________________________________________________________________________________________
tf.__operators__.getitem_1 (Sli (768,)               0           tf_longformer_model[0][14]
__________________________________________________________________________________________________
EDITED OUT ANOTHER 62 SIMILAR LAYERS
__________________________________________________________________________________________________
tf.__operators__.getitem_63 (Sl (768,)               0           tf_longformer_model[0][14]
__________________________________________________________________________________________________
concatenate (Concatenate)       (49152,)             0           tf.__operators__.getitem[0][0]
                                                                tf.__operators__.getitem_1[0][0]
EDITED ANOTHER 62 SIMILAR LINES
                                                                tf.__operators__.getitem_63[0][0]
__________________________________________________________________________________________________
reshape (Reshape)               (49152, 1, 1)        0           concatenate[0][0]
__________________________________________________________________________________________________
rnn (RNN)                       (49152, 64)          37824       reshape[0][0]
__________________________________________________________________________________________________
batch_normalization (BatchNorma (49152, 64)          256         rnn[0][0]
__________________________________________________________________________________________________
dropout_49 (Dropout)            (49152, 64)          0           batch_normalization[0][0]
__________________________________________________________________________________________________
dense (Dense)                   (49152, 25)          1625        dropout_49[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (49152, 25)          100         dense[0][0]
__________________________________________________________________________________________________
dropout_50 (Dropout)            (49152, 25)          0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (49152, 3)           78          dropout_50[0][0]
==================================================================================================
Total params: 148,699,339
Trainable params: 148,699,161
Non-trainable params: 178
__________________________________________________________________________________________________
None
2021-04-29 08:53:45.368311: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at strided_slice_op.cc:108 : Invalid argument: slice index -63 of dimension 0 out of bounds.
Traceback (most recent call last):
 File &quot;c:\Automator_alpha\Just_longformer.py&quot;, line 60, in &lt;module&gt;
   model([encoded_input['input_ids'],encoded_input['attention_mask']])
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\engine\base_layer.py&quot;, line 1014, in __call__
   outputs = call_fn(inputs, *args, **kwargs)
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\engine\functional.py&quot;, line 426, in call
   return self._run_internal_graph(
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\engine\functional.py&quot;, line 562, in _run_internal_graph
   outputs = node.layer(*args, **kwargs)
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\engine\base_layer.py&quot;, line 1014, in __call__
   outputs = call_fn(inputs, *args, **kwargs)
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\layers\core.py&quot;, line 1520, in _call_wrapper
   return original_call(*new_args, **new_kwargs)
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\layers\core.py&quot;, line 1326, in _call_wrapper
   return self._call_wrapper(*args, **kwargs)
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\layers\core.py&quot;, line 1358, in _call_wrapper
   result = self.function(*args, **kwargs)
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\util\dispatch.py&quot;, line 206, in wrapper
   return target(*args, **kwargs)
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\ops\array_ops.py&quot;, line 1037, in _slice_helper
   return strided_slice(
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\util\dispatch.py&quot;, line 206, in wrapper
   return target(*args, **kwargs)
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\ops\array_ops.py&quot;, line 1210, in strided_slice
   op = gen_array_ops.strided_slice(
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\ops\gen_array_ops.py&quot;, line 10484, in strided_slice
   _ops.raise_from_not_ok_status(e, name)
 File &quot;C:\ProgramData\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 6868, in raise_from_not_ok_status
   six.raise_from(core._status_to_exception(e.code, message), None)
 File &quot;&lt;string&gt;&quot;, line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: slice index -63 of dimension 0 out of bounds. [Op:StridedSlice] name: model/tf.__operators__.getitem/strided_slice/
</code></pre>
<p>I am using 4096 for the input layers as that was the input length specified in the longformer paper. I have tried using different value, not 64, i have tried iterating through the values without specifying index(with a for statement, in which the error says cannot iterate not knowing the first dimension).</p>
<p>I am new to this and feel like i am missing something basic.</p>
","14404984","","14404984","","2021-04-29 08:57:55","2021-04-29 08:57:55","Tensorflow ""Index out of bound"" error when trying to use huggingface TF longformer transformer in a custom TF network","<tensorflow><neural-network><nlp><huggingface-transformers><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"64020998","1","","","2020-09-23 04:23:55","","1","1488","<p>Getting the following error while  trying to import the classificationmodel from simpletransformers.</p>
<hr />
<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-1-29f08e6c2d87&gt; in &lt;module&gt;()
----&gt; 1 from simpletransformers.classification import ClassificationModel, ClassificationArgs

3 frames
/usr/local/lib/python3.6/dist-packages/simpletransformers/classification/transformer_models/roberta_model.py in &lt;module&gt;()
      2 import torch.nn as nn
      3 from torch.nn import CrossEntropyLoss, MSELoss
----&gt; 4 from transformers.modeling_roberta import (
      5     ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST,
      6     BertPreTrainedModel,

ImportError: cannot import name 'BertPreTrainedModel'

---------------------------------------------------------------------------
</code></pre>
","11540649","","","","","2020-09-23 07:44:56","Getting an error(cannot import name 'BertPreTrainedModel') while importing classification model from simpletransformers","<keras><pytorch><tensorflow2.0><bert-language-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"63672169","1","63679073","","2020-08-31 13:56:09","","0","231","<p>I've been trying to find a suitable model for my project (multiclass German text classification) but got a little confused with the models offered <a href=""https://huggingface.co/models?search=german"" rel=""nofollow noreferrer"">here</a>. There are models with <code>text-classification</code> tag, but they are for binary classification. Most of the other models are for <code>[MASK]</code> word predicting. I am not sure, which one to choose and if it will work with multiple classes at all</p>
<p>Would appreciate any advice!</p>
","9510022","","","","","2020-08-31 22:39:55","HuggingFace Transformers model for German news classification","<python><nlp><text-classification><bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"64623851","1","","","2020-10-31 16:16:31","","1","478","<p>I am using BertForQuestionAnswering from hungging face transformers. I am getting tensor size problem.
I have tried setting congiguration using BertConfig. But It didn't solve the problem</p>
<p>Here is my code</p>
<pre><code>import torch
from transformers import BertForQuestionAnswering
from transformers import BertTokenizer, BertConfig
import time
#Model
config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,
      num_hidden_layers=12, num_attention_heads=12, intermediate_size=6072, 
      torchscript=True, max_position_embeddings=6144)
model_bert = BertForQuestionAnswering(config)
model_bert = model_bert.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

#Tokenizer
tokenizer_bert = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
encoding = tokenizer_bert.encode_plus(text=question,text_pair=paragraph, add_special=True)
inputs = encoding['input_ids']  #Token embeddings
sentence_embedding = encoding['token_type_ids']  #Segment embeddings
tokens = tokenizer_bert.convert_ids_to_tokens(inputs) #input tokens

start_scores, end_scores = model_bert(input_ids=torch.tensor([inputs]), 
token_type_ids=torch.tensor([sentence_embedding]))

start_index = torch.argmax(start_scores)
end_index = torch.argmax(end_scores)

answer = ' '.join(tokens[start_index:end_index+1])
</code></pre>
<p>data(question and text):</p>
<pre><code>question = '''who is Sundar Pichai'''

paragraph = ''' Google, LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It is considered one of the Big Five technology companies in the U.S. information technology industry, alongside Amazon, Facebook, Apple, and Microsoft. Google was founded in September 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock. They incorporated Google as a California privately held company on September 4, 1998, in California. Google was then reincorporated in Delaware on October 22, 2002.[12] An initial public offering (IPO) took place on August 19, 2004, and Google moved to its headquarters in Mountain View, California, nicknamed the Googleplex. In August 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet Inc. Google is Alphabet's leading subsidiary and will continue to be the umbrella company for Alphabet's Internet interests. Sundar Pichai was appointed CEO of Google, replacing Larry Page, who became the CEO of Alphabet. The company's rapid growth since incorporation has triggered a chain of products, acquisitions, and partnerships beyond Google's core search engine (Google Search). It offers services designed for work and productivity (Google Docs, Google Sheets, and Google Slides), email (Gmail), scheduling and time management (Google Calendar), cloud storage (Google Drive), instant messaging and video chat (Duo, Hangouts, Chat, and Meet), language translation (Google Translate), mapping and navigation (Google Maps, Waze, Google Earth, and Street View), podcast hosting (Google Podcasts), video sharing (YouTube), blog publishing (Blogger), note-taking (Google Keep and Google Jamboard), and photo organizing and editing (Google Photos). The company leads the development of the Android mobile operating system, the Google Chrome web browser, and Chrome OS, a lightweight operating system based on the Chrome browser. Google has moved increasingly into hardware; from 2010 to 2015, it partnered with major electronics manufacturers in the production of its Nexus devices, and it released multiple hardware products in October 2016, including the Google Pixel smartphone, Google Home smart speaker, Google Wifi mesh wireless router, and Google Daydream virtual reality headset. Google has also experimented with becoming an Internet carrier (Google Fiber, Google Fi, and Google Station)'''
</code></pre>
<p>Error:</p>
<pre><code>RuntimeError                              Traceback (most recent call 
last)
&lt;ipython-input-5-48d08888656c&gt; in &lt;module&gt;()
  6 tokens = tokenizer_bert.convert_ids_to_tokens(inputs) #input tokens
  7 
----&gt; 8 start_scores, end_scores = model_bert(input_ids=torch.tensor([inputs]),token_type_ids=torch.tensor([sentence_embedding]))
  9 
 10 

/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds)
199         token_type_embeddings = self.token_type_embeddings(token_type_ids)
200 
--&gt; 201         embeddings = inputs_embeds + position_embeddings + token_type_embeddings
202         embeddings = self.LayerNorm(embeddings)
203         embeddings = self.dropout(embeddings)

RuntimeError: The size of tensor a (546) must match the size of tensor b (512) at non-singleton dimension 1
</code></pre>
<p>I know that size of input text is greater than the default setting tensor size 512. But I don't know how to set the value manually.</p>
","11228062","","11228062","","2020-11-01 06:40:37","2020-11-01 06:40:37","RuntimeError: The size of tensor a (546) must match the size of tensor b (512) at non-singleton dimension 1","<nlp><bert-language-model><huggingface-transformers><question-answering>","0","3","","","","CC BY-SA 4.0"
"64881478","1","64884458","","2020-11-17 18:50:15","","3","1244","<p>I have a dataset with paragraphs that I need to classify into two classes. These paragraphs are usually 3-5 sentences long. The overwhelming majority of them are less than 500 words long. I would like to make use of BERT to tackle this problem.</p>
<p>I am wondering how I should use BERT to generate vector representations of these paragraphs and especially, whether it is fine to just pass the whole paragraph into BERT?</p>
<p>There have been informative discussions of related problems <a href=""https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification/63413589#63413589"">here</a> and <a href=""https://stackoverflow.com/questions/63671085/how-to-use-bert-for-long-sentences"">here</a>. These discussions focus on how to use BERT for representing whole documents. In my case the paragraphs are not that long, and indeed could be passed to BERT without exceeding its maximum length of 512. However, BERT was trained on sentences. Sentences are relatively self-contained units of meaning. I wonder if feeding multiple sentences into BERT doesn't conflict fundamentally with what the model was designed to do (although this appears to be done regularly).</p>
","13654239","","","","","2020-11-17 22:49:22","Passing multiple sentences to BERT?","<nlp><text-classification><bert-language-model><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"62765212","1","","","2020-07-06 22:04:32","","1","112","<p>My goal is to get the most important features for each class in a text classification task.
I created the model, learner and predictor like this:</p>
<pre><code>t = text.Transformer(model_name, maxlen=MAX_SEQ_LENGTH, class_names=emotions)
trn = t.preprocess_train(X_train.values, y_train.values)
val = t.preprocess_test(X_test.values, y_test.values)
model = t.get_classifier()
learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=BATCH_SIZE)
predictor = ktrain.get_predictor(learner.model, preproc=t)
</code></pre>
<p>Is there any way to get the top features like this:</p>
<p><a href=""https://i.stack.imgur.com/V6QXK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V6QXK.png"" alt=""enter image description here"" /></a></p>
<p>(created for SVM)</p>
","13880588","","7001213","","2020-07-07 08:52:02","2020-07-07 08:52:02","BERT (Huggingface Transformer) - Get important features for class","<text-classification><huggingface-transformers><bert-language-model>","0","3","","","","CC BY-SA 4.0"
"64124757","1","","","2020-09-29 17:33:31","","-2","1345","<p>I am working on text classification using Longformer Model. I took even just first 100 rows of dataframe. I am getting memory error. I am using google colab.</p>
<p>This is my model :</p>
<pre><code>model = LongformerForMultiSequenceClassification.from_pretrained('allenai/longformer-base-4096',
                                        config=config)
# Accessing the model configuration
configuration = model.config
</code></pre>
<p><img src=""https://i.stack.imgur.com/x2CSl.png"" alt=""Configuration model image "" /></p>
<p><img src=""https://i.stack.imgur.com/8F17q.png"" alt=""Custom class for Global attention image"" /></p>
<p>Training Loop</p>
<pre><code>   
for epoch in tqdm(range(1, epochs+1)):
    
    model.train()
    
    loss_train_total = 0

    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
    for batch in progress_bar:
        
        #this will empty the gradients from the previous iterations
        model.zero_grad()
        
        #take out inputs
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }       
        #insert the input into the model and get the result
        outputs = model(**inputs)
        
        #calculate loss
        loss = outputs[0]
        loss_train_total += loss.item()

        #this will calculate the gradients
        loss.backward()
        # for preventening gradient explosion
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        #this will update the weights 
        optimizer.step()
        #optimizing learning rate
        scheduler.step()
        
        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})
         
        
    torch.save(model.state_dict(), f'/content/Gdrive/My Drive/finetuned_longformer_epoch_{epoch}.model')
    #torch.save(model.state_dict(), f'checkpoint{epoch}.pth')
        
    tqdm.write(f'\nEpoch {epoch}')
    
    loss_train_avg = loss_train_total/len(dataloader_train)            
    tqdm.write(f'Training loss: {loss_train_avg}')
    
    val_loss, predictions, true_vals = evaluate(dataloader_validation)
    val_f1 = f1_score_func(predictions, true_vals)
    tqdm.write(f'Validation loss: {val_loss}')
    tqdm.write(f'F1 Score (Weighted): {val_f1}')

</code></pre>
<p>Error :</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-32-7e534d564c0a&gt; in &lt;module&gt;()
     20               }       
     21      #insert the input into the model and get the result
---&gt; 22      outputs = model(**inputs)
     23 
     24      #calculate loss

12 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in dropout(input, p, training, inplace)
    971     return (_VF.dropout_(input, p, training)
    972             if inplace
--&gt; 973             else _VF.dropout(input, p, training))
    974 
    975 

RuntimeError: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 0; 11.17 GiB total capacity; 10.23 GiB already allocated; 59.81 MiB free; 10.69 GiB reserved in total by PyTorch)
</code></pre>
<p>You can check my config file and model structure and custom class for Global Attention or My complete code is on colab is here :</p>
<p><a href=""https://colab.research.google.com/drive/19JkCht_4u6UrwcUcWNnSD2YtnsJYer0H?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/19JkCht_4u6UrwcUcWNnSD2YtnsJYer0H?usp=sharing</a></p>
<p>I ran a similar code using BERT and it works without any problem.</p>
<p><B> I am new to datascience. Please help. </B></p>
","11014647","","494631","","2020-09-29 18:59:14","2020-10-02 22:37:49","Getting Cuda Out of Memory while running Longformer Model in Google Colab. Similar code using Bert is working fine","<python><machine-learning><pytorch><huggingface-transformers><attention-model>","1","3","","","","CC BY-SA 4.0"
"64010752","1","","","2020-09-22 13:25:46","","0","180","<p>I am using a huggingface model of type <code>transformers.modeling_gpt2.GPT2LMHeadModel</code> and using beam search to predict the text.</p>
<ul>
<li>Is there any way to get the probability calculated in beam search for returned sequence.</li>
<li>Can I put a condition to return a text sequence only when it crosses some threshold probability.</li>
</ul>
<p>Below code gives the 5 texts' tokens but I need the probability of those 5 sequences.</p>
<pre><code>test_beam_outputs = model.generate(
    test_input_ids,
    max_length=len(test_text.split(' ')) + 20,
    num_beams=5,
    early_stopping=True,
    
    length_penalty=0.5,
    num_return_sequences=5,
    no_repeat_ngram_size=2
)```



</code></pre>
","3234468","","3234468","","2020-09-27 16:39:08","2020-09-27 16:39:08","Is there a way to put a probability threshold on beam search when using hugginface transformer?","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"64610841","1","64798263","","2020-10-30 15:00:15","","4","821","<p>I am trying to train an NER model using the HuggingFace transformers library on Colab cloud GPUs, pickle it and load the model on my own CPU to make predictions.</p>
<p><strong>Code</strong></p>
<p>The model is the following:</p>
<pre><code>from transformers import BertForTokenClassification

model = BertForTokenClassification.from_pretrained(
    &quot;bert-base-cased&quot;,
    num_labels=NUM_LABELS,
    output_attentions = False,
    output_hidden_states = False
)
</code></pre>
<p>I am using this snippet to save the model on Colab</p>
<pre><code>import torch

torch.save(model.state_dict(), FILENAME)
</code></pre>
<p>Then load it on my local CPU using</p>
<pre><code># Initiating an instance of the model type

model_reload = BertForTokenClassification.from_pretrained(
    &quot;bert-base-cased&quot;,
    num_labels=len(tag2idx),
    output_attentions = False,
    output_hidden_states = False
)

# Loading the model
model_reload.load_state_dict(torch.load(FILENAME, map_location='cpu'))
model_reload.eval()

</code></pre>
<p>The code snippet used to tokenize the text and make actual predictions is the same both on the Colab GPU notebook instance and my CPU notebook instance.</p>
<p><strong>Expected Behavior</strong></p>
<p>The GPU-trained model behaves correctly and classifies the following tokens perfectly:</p>
<pre><code>O       [CLS]
O       Good
O       morning
O       ,
O       my
O       name
O       is
B-per   John
I-per   Kennedy
O       and
O       I
O       am
O       working
O       at
B-org   Apple
O       in
O       the
O       headquarters
O       of
B-geo   Cupertino
O       [SEP]
</code></pre>
<p><strong>Actual Behavior</strong></p>
<p>When loading the model and use it to make predictions on my CPU, the predictions are totally wrong:</p>
<pre><code>I-eve   [CLS]
I-eve   Good
I-eve   morning
I-eve   ,
I-eve   my
I-eve   name
I-eve   is
I-geo   John
B-eve   Kennedy
I-eve   and
I-eve   I
I-eve   am
I-eve   working
I-eve   at
I-gpe   Apple
I-eve   in
I-eve   the
I-eve   headquarters
I-eve   of
B-org   Cupertino
I-eve   [SEP]
</code></pre>
<p>Does anyone have ideas why it doesn't work? Did I miss something?</p>
","13364419","","","","","2020-11-12 05:08:22","BERT-based NER model giving inconsistent prediction when deserialized","<python><pytorch><bert-language-model><huggingface-transformers>","1","2","1","","","CC BY-SA 4.0"
"65733681","1","","","2021-01-15 09:38:57","","0","208","<p>I have a fine tuned <code>distilgpt2</code> model that I want to deploy using GCP ai-platform.</p>
<p>I've followed all the documentation for deploying a custom prediction routine on GCP but when creating the model I get the error:</p>
<blockquote>
<p>Create Version failed. Bad model detected with error: Model requires more memory than allowed. Please try to decrease the model size and re-deploy.</p>
</blockquote>
<p>Here is my <code>setup.py</code> file:</p>
<pre><code>from setuptools import setup

setup(
    name=&quot;generator_package&quot;,
    version=&quot;0.2&quot;,
    include_package_data=True,
    scripts=[&quot;generator_class.py&quot;],
    install_requires=['transformers==2.8.0']
)
</code></pre>
<p>I then create a model version using:</p>
<pre><code>gcloud beta ai-platform versions create v1 --model my_model \
 --origin=gs://my_bucket/model/ \
 --python-version=3.7 \
 --runtime-version=2.3 \
 --package-uris=gs://my_bucket/packages/gpt2-0.1.tar.gz,gs://cloud-ai-pytorch/torch-1.3.1+cpu-cp37-cp37m-linux_x86_64.whl \
 --prediction-class=model_prediction.CustomModelPrediction
</code></pre>
<p>Following this answer: <a href=""https://stackoverflow.com/questions/60423140/pytorch-model-deployment-in-ai-platform"">PyTorch model deployment in AI Platform</a>, I figured out how to get pytorch installed on my custom prediction routine, but am still getting the above error. I believe it may have something to do with the <code>transformers</code> package, as it has <code>torch</code> as a dependency. Can that be causing the issue?</p>
<p>I have tried every suggested route and cant get this to work and I'm still getting the above error. I'm using the smallest gpt2 model and am well within memory.</p>
<p>Can anyone who have successfully deployed to GCP please give some insight here.</p>
<p><strong>Update:</strong></p>
<p>So to address the above concern of <code>transformers</code> also trying to install <code>torch</code> and that may be causing the problem I rebuilt the <code>.whl</code> file from source with the additional packages removed, below is the edited <code>setup.py</code> file and built using <code>python setup.py bdist_wheel</code>.</p>
<p>I then added this <code>whl</code> to the required dependencies when creating a model version in GCP and removed the <code>transformers==2.8.0</code> from my own <code>setup.py</code>. But it is still giving the same error of model requires more memory =(</p>
<pre><code>import shutil

from pathlib import Path



from setuptools import find_packages, setup





# Remove stale transformers.egg-info directory to avoid https://github.com/pypa/pip/issues/5466

stale_egg_info = Path(__file__).parent / &quot;transformers.egg-info&quot;

if stale_egg_info.exists():

    print(

        (

            &quot;Warning: {} exists.\n\n&quot;

            &quot;If you recently updated transformers to 3.0 or later, this is expected,\n&quot;

            &quot;but it may prevent transformers from installing in editable mode.\n\n&quot;

            &quot;This directory is automatically generated by Python's packaging tools.\n&quot;

            &quot;I will remove it now.\n\n&quot;

            &quot;See https://github.com/pypa/pip/issues/5466 for details.\n&quot;

        ).format(stale_egg_info)

    )

    shutil.rmtree(stale_egg_info)





extras = {}



extras[&quot;mecab&quot;] = [&quot;mecab-python3&quot;]

extras[&quot;sklearn&quot;] = [&quot;scikit-learn&quot;]

# extras[&quot;tf&quot;] = [&quot;tensorflow&quot;]

# extras[&quot;tf-cpu&quot;] = [&quot;tensorflow-cpu&quot;]

# extras[&quot;torch&quot;] = [&quot;torch&quot;]



extras[&quot;serving&quot;] = [&quot;pydantic&quot;, &quot;uvicorn&quot;, &quot;fastapi&quot;, &quot;starlette&quot;]

extras[&quot;all&quot;] = extras[&quot;serving&quot;] + [&quot;tensorflow&quot;, &quot;torch&quot;]



extras[&quot;testing&quot;] = [&quot;pytest&quot;, &quot;pytest-xdist&quot;]

extras[&quot;docs&quot;] = [&quot;recommonmark&quot;, &quot;sphinx&quot;, &quot;sphinx-markdown-tables&quot;, &quot;sphinx-rtd-theme&quot;]

extras[&quot;quality&quot;] = [

    &quot;black&quot;,

    &quot;isort&quot;,

    &quot;flake8&quot;,

]

extras[&quot;dev&quot;] = extras[&quot;testing&quot;] + extras[&quot;quality&quot;] + [&quot;mecab-python3&quot;, &quot;scikit-learn&quot;, &quot;tensorflow&quot;, &quot;torch&quot;]



setup(

    name=&quot;transformers&quot;,

    version=&quot;2.8.0&quot;,

    author=&quot;Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors&quot;,

    author_email=&quot;thomas@huggingface.co&quot;,

    description=&quot;State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch&quot;,

    long_description=open(&quot;README.md&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;).read(),

    long_description_content_type=&quot;text/markdown&quot;,

    keywords=&quot;NLP deep learning transformer pytorch tensorflow BERT GPT GPT-2 google openai CMU&quot;,

    license=&quot;Apache&quot;,

    url=&quot;https://github.com/huggingface/transformers&quot;,

    package_dir={&quot;&quot;: &quot;src&quot;},

    packages=find_packages(&quot;src&quot;),

    install_requires=[

        &quot;numpy&quot;,

        &quot;tokenizers == 0.5.2&quot;,

        # dataclasses for Python versions that don't have it

        &quot;dataclasses;python_version&lt;'3.7'&quot;,

        # accessing files from S3 directly

        &quot;boto3&quot;,

        # filesystem locks e.g. to prevent parallel downloads

        &quot;filelock&quot;,

        # for downloading models over HTTPS

        &quot;requests&quot;,

        # progress bars in model download and training scripts

        &quot;tqdm &gt;= 4.27&quot;,

        # for OpenAI GPT

        &quot;regex != 2019.12.17&quot;,

        # for XLNet

        &quot;sentencepiece&quot;,

        # for XLM

        &quot;sacremoses&quot;,

    ],

    extras_require=extras,

    scripts=[&quot;transformers-cli&quot;],

    python_requires=&quot;&gt;=3.6.0&quot;,

    classifiers=[

        &quot;Development Status :: 5 - Production/Stable&quot;,

        &quot;Intended Audience :: Developers&quot;,

        &quot;Intended Audience :: Education&quot;,

        &quot;Intended Audience :: Science/Research&quot;,

        &quot;License :: OSI Approved :: Apache Software License&quot;,

        &quot;Operating System :: OS Independent&quot;,

        &quot;Programming Language :: Python :: 3&quot;,

        &quot;Programming Language :: Python :: 3.6&quot;,

        &quot;Programming Language :: Python :: 3.7&quot;,

        &quot;Topic :: Scientific/Engineering :: Artificial Intelligence&quot;,

    ],

)
</code></pre>
","843036","","843036","","2021-01-15 12:36:08","2021-01-15 12:36:08","Cannot deploy a small transformers model for prediction serving using Google Cloud AI Platform due to ""Model requires more memory than allowed""","<google-cloud-platform><pytorch><google-cloud-ml><huggingface-transformers><transformer>","0","3","","","","CC BY-SA 4.0"
"65744442","1","","","2021-01-15 22:38:11","","7","839","<p>I'm trying to run a hugging face model, mode exactly <strong>&quot;cardiffnlp/twitter-roberta-base-sentiment&quot;</strong> on threads. But at the same time, I want just one single instance of it because it's really costly in terms of time.</p>
<p>In other words, I have multiple CSV files (several thousand) and each of them has around 20k-30k lines and I want that each line from all of them to be executed by the huggingface model, as you probably can imagine already this is the reason why I don't want to instantiate a model for each thread (where each thread would be used just to read one line and write it in the database).
The problem with my approach is that when I'm running the code is going to give me an error from huggingface model.</p>
<blockquote>
<p>RuntimeError: Already borrowed</p>
</blockquote>
<p>Could any of you help me to understand how cand I fix it?</p>
<p><em><strong>Hugging face model:</strong></em></p>
<pre><code>class EmotionDetection(object):
    def __init__(self, model_name=&quot;cardiffnlp/twitter-roberta-base-sentiment&quot;):
        self.model_name = model_name
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True,
                                                     task=&quot;sentiment-analysis&quot;, device=0)

    def get_emotion_by_label(self, label: str):
        if label == &quot;LABEL_0&quot;:
            return &quot;negative&quot;
        elif label == &quot;LABEL_1&quot;:
            return &quot;neutral&quot;
        elif label == &quot;LABEL_2&quot;:
            return &quot;positive&quot;
        else:
            print(&quot;SOMETHING IS WRONG&quot;)
            return &quot;&quot;

    def get_emotion(self, phrase):
        results = self.classifier(phrase)
        res = dict()
        for result in results:
            for emotion in result:
                res.update({self.get_emotion_by_label(emotion['label']): emotion['score']})
        return res
</code></pre>
<p><em><strong>My code for generating database:</strong></em></p>
<pre><code>class GenerateDbThread(object):
    def __init__(self, text: str, created_at: datetime.datetime, get_emotion_function, cursor, table_name):
        self.table_name = table_name

        self.text = text
        self.created_at = created_at
        emotions = get_emotion_function(self.text)

        self.pos = emotions['positive']
        self.neg = emotions['negative']
        self.neu = emotions['neutral']

        self.cursor = cursor

    def execute(self):
        query = f&quot;INSERT INTO {self.table_name}(date, positive, negative, neutral, tweet) &quot; \
                f&quot;VALUES (datetime('{str(self.created_at)}'),{self.pos},{self.neg},{self.neu}, '{self.text}')&quot;
        self.cursor.execute(query)
        self.cursor.commit()


def get_all_data_files_path(data_dir: str):
    return [f for f in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, f))]


def run(file: str, table_name: str):
    df = pd.read_csv(os.path.join('data', file), delimiter=',')
    for index, row in df.iterrows():
        text = row['tweet']
        language = row['language']
        split_data = row['created_at'].split(&quot; &quot;)
        GTB_Time = f&quot;{split_data[2]} {split_data[3]} {split_data[4]}&quot;
        created_at = datetime.datetime.strptime(row['created_at'], f&quot;%Y-%m-%d %H:%M:%S {GTB_Time}&quot;)
        if language == &quot;en&quot;:
            GenerateDbThread(text, created_at, emotion_detector.get_emotion, cursor, table_name)


def init_db(db_name, table_name):
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()
    cursor.execute(f&quot;&quot;&quot;
    CREATE TABLE IF NOT EXISTS {table_name} (
        uid INTEGER PRIMARY KEY AUTOINCREMENT,
        date DATETIME NOT NULL,
        positive REAL NOT NULL,
        negative REAL NOT NULL,
        neutral REAL NOT NULL,
        text TEXT NOT NULL
    )&quot;&quot;&quot;)
    cursor.execute(f&quot;CREATE INDEX IF NOT EXISTS ix_tweets_index ON {table_name}(uid)&quot;)
    cursor.close()


ex = ThreadPoolExecutor(max_workers=10)

files = get_all_data_files_path('data')

init_db(&quot;DB_NAME.db&quot;, &quot;TABLE_NAME&quot;)

emotion_detector = EmotionDetection()
conn = sqlite3.connect(&quot;DB_NAME.db&quot;)
cursor = conn.cursor()

pbar = tqdm(total=len(files))
futures = [ex.submit(run, file, &quot;TABLE_NAME&quot;) for file in files]
for future in futures:
    res = future.result()
    pbar.update(1)
pbar.close()
</code></pre>
","7164581","","","","","2021-01-15 22:38:11","How to use threads for huggingface transformers","<python><multithreading><threadpool><huggingface-transformers><huggingface-tokenizers>","0","3","1","","","CC BY-SA 4.0"
"67309104","1","","","2021-04-28 23:18:58","","0","44","<p>I am using PyTorch's BertForTokenClassification pretrained model to do custom word tagging (not NER or POS, but essentially the same). There are 20 different possible tags (using BIO scheme): 9 B's, 9 I's, and an O. Despite there being 19 possible tags, the feed-forward layer that is added on top of BERT has 20 tags. I have used other datasets, too, and the result is the same: there is always one more output than the number of classes. Can anyone tell me why this is?</p>
","2756199","","","","","2021-05-03 14:34:33","BertForTokenClassification Has Extra Output","<pytorch><bert-language-model><huggingface-transformers><named-entity-recognition><ner>","1","2","","","","CC BY-SA 4.0"
"63676307","1","63684430","","2020-08-31 18:31:12","","0","1491","<p>How do I run the run_language_modeling.py script from hugging face using the pretrained roberta case model to fine-tune  using my own data on the Azure databricks with a GPU cluster.</p>
<p>Using Transformer version 2.9.1 and 3.0 .
Python 3.6
Torch `1.5.0
torchvision 0.6</p>
<p>This is the script I ran below on Azure databricks</p>
<pre><code>%run '/dbfs/FileStore/tables/dev/run_language_modeling.py' \
  --output_dir='/dbfs/FileStore/tables/final_train/models/roberta_base_reduce_n' \
  --model_type=roberta \
  --model_name_or_path=roberta-base \
  --do_train \
  --num_train_epochs 5 \
  --train_data_file='/dbfs/FileStore/tables/final_train/train_data/all_data_desc_list_full.txt' \
  --mlm 
</code></pre>
<p>This is the error I get after running the above command.</p>
<pre><code>/dbfs/FileStore/tables/dev/run_language_modeling.py in &lt;module&gt;
   279 
   280 if __name__ == &quot;__main__&quot;:
--&gt; 281     main()

/dbfs/FileStore/tables/dev/run_language_modeling.py in main()
   243             else None
   244         )
--&gt; 245         trainer.train(model_path=model_path)
   246         trainer.save_model()
   247         # For convenience, we also re-save the tokenizer to the same directory,

/databricks/python/lib/python3.7/site-packages/transformers/trainer.py in train(self, model_path)
   497                     continue
   498 
--&gt; 499                 tr_loss += self._training_step(model, inputs, optimizer)
   500 
   501                 if (step + 1) % self.args.gradient_accumulation_steps == 0 or (

/databricks/python/lib/python3.7/site-packages/transformers/trainer.py in _training_step(self, model, inputs, optimizer)
   620             inputs[&quot;mems&quot;] = self._past
   621 
--&gt; 622         outputs = model(**inputs)
   623         loss = outputs[0]  # model outputs are always tuple in transformers (see doc)
   624 

/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
   548             result = self._slow_forward(*input, **kwargs)
   549         else:
--&gt; 550             result = self.forward(*input, **kwargs)
   551         for hook in self._forward_hooks.values():
   552             hook_result = hook(self, input, result)

/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)
   153             return self.module(*inputs[0], **kwargs[0])
   154         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
--&gt; 155         outputs = self.parallel_apply(replicas, inputs, kwargs)
   156         return self.gather(outputs, self.output_device)
   157 

/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py in parallel_apply(self, replicas, inputs, kwargs)
   163 
   164     def parallel_apply(self, replicas, inputs, kwargs):
--&gt; 165         return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
   166 
   167     def gather(self, outputs, output_device):

/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py in parallel_apply(modules, inputs, kwargs_tup, devices)
    83         output = results[i]
    84         if isinstance(output, ExceptionWrapper):
---&gt; 85             output.reraise()
    86         outputs.append(output)
    87     return outputs

/databricks/python/lib/python3.7/site-packages/torch/_utils.py in reraise(self)
   393             # (https://bugs.python.org/issue2651), so we work around it.
   394             msg = KeyErrorMessage(msg)
--&gt; 395         raise self.exc_type(msg)

RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py&quot;, line 60, in _worker
   output = module(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_roberta.py&quot;, line 239, in forward
   output_hidden_states=output_hidden_states,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 762, in forward
   output_hidden_states=output_hidden_states,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 439, in forward
   output_attentions,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 371, in forward
   hidden_states, attention_mask, head_mask, output_attentions=output_attentions,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 315, in forward
   hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions,
 File &quot;/databricks/python/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
   result = self.forward(*input, **kwargs)
 File &quot;/databricks/python/lib/python3.7/site-packages/transformers/modeling_bert.py&quot;, line 240, in forward
   attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 11.17 GiB total capacity; 10.68 GiB already allocated; 95.31 MiB free; 10.77 GiB reserved in total by PyTorch)```

Please how do I resolve this
</code></pre>
","10382230","","10382230","","2020-08-31 18:36:50","2020-09-01 08:55:48","Hugging face - RuntimeError: Caught RuntimeError in replica 0 on device 0 on Azure Databricks","<pytorch><databricks><azure-databricks><bert-language-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"62772819","1","","","2020-07-07 10:06:23","","3","904","<p>Huggingface Trainer keeps giving <code>Segmentation Fault</code> with this setup code.
The dataset is around 600MB, and the server has 2*32GB Nvidia V100. Can anyone help find the issue?</p>
<pre><code>from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, LineByLineTextDataset
from transformers import GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast

tokenizer = GPT2TokenizerFast.from_pretrained(&quot;./data/TOKEN&quot;)

config = GPT2Config.from_pretrained('gpt2-large')
model = GPT2LMHeadModel(config=config)
tokenizer = GPT2TokenizerFast.from_pretrained(&quot;./data/TOKEN&quot;, model_max_length=1024)

print('loading dataset...')
dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=&quot;./data/kowiki.txt&quot;,
    block_size=128,
)

training_args = TrainingArguments(
    output_dir='./m',          # output directory
    num_train_epochs=1,              # total # of training epochs
    per_device_train_batch_size=1,  # batch size per device during training - the higher the better, but may OOM
    per_device_eval_batch_size=1,   # batch size for evaluation
    logging_dir='./logs',            # directory for storing logs
    save_steps=10000,
    do_train=True
)

trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=dataset,         # training dataset
)

trainer.train()
</code></pre>
<p>Error message :</p>
<pre><code>loading dataset...
Epoch:   0%|                                              | 0/1 [00:00&lt;?, ?it/s]
Fatal Python error: Segmentation fault                | 0/99996 [00:00&lt;?, ?it/s]

Thread 0x00007f872dfff700 (most recent call first):
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 299 in wait
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 551 in wait
  File &quot;/opt/conda/lib/python3.6/site-packages/tqdm/_monitor.py&quot;, line 69 in run
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 916 in _bootstrap_inner
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 884 in _bootstrap

Thread 0x00007f8736bb5700 (most recent call first):
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 299 in wait
  File &quot;/opt/conda/lib/python3.6/queue.py&quot;, line 173 in get
  File &quot;/opt/conda/lib/python3.6/site-packages/tensorboard/summary/writer/event_file_writer.py&quot;, line 205 in run
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 916 in _bootstrap_inner
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 884 in _bootstrap

Current thread 0x00007f88273e7740 (most recent call first):
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/cuda/comm.py&quot;, line 39 in broadcast_coalesced
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py&quot;, line 21 in forward
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/replicate.py&quot;, line 71 in _broadcast_coalesced_reshape
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/replicate.py&quot;, line 88 in replicate
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 159 in replicate
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 154 in forward
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 577 in __call__
  File &quot;/opt/conda/lib/python3.6/site-packages/transformers/trainer.py&quot;, line 622 in _training_step
  File &quot;/opt/conda/lib/python3.6/site-packages/transformers/trainer.py&quot;, line 499 in train
  File &quot;trainer.py&quot;, line 34 in &lt;module&gt;
Segmentation fault (core dumped)

</code></pre>
<p>Python version is 3.7.7 with Pytorch 1.5.1+cu101
with a 'recently installed' HF transformer &amp; tokenizer(0.8.0).</p>
<p>First time here :D sorry if I didn't keep trivial stuff</p>
<p>EDIT : May be a bug - <a href=""https://github.com/huggingface/transformers/issues/5590"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/5590</a></p>
<p>EDIT 2: Also segfaults on transformer 3.0.2 and tokenizer 0.7.0, 0.8.1-rc1</p>
","13883836","","13883836","","2020-07-11 12:01:24","2020-07-11 12:01:24","HuggingFace Trainer Segmentation Fault","<python><machine-learning><huggingface-transformers><language-model>","0","5","","","","CC BY-SA 4.0"
"62830701","1","","","2020-07-10 08:55:13","","1","289","<p>What would be the best strategy to <strong>mask only specific words</strong> during the LM training?</p>
<p>My aim is to dynamically mask <em>at batch-time</em> only words of interest which I have previously collected in a list.</p>
<p>I have already had a look at the <code>mask_tokens()</code> function into the <code>DataCollatorForLanguageModeling class</code>, which is the function actually masking the tokens during each batch, but I cannot find any efficient and smart way to mask only specific words and their corresponding IDs.</p>
<p>I tried one <em>naive</em> approach consisting of matching all the IDs of each batch with a list of word's IDs to mask. However, a for-loop approach has a negative on the performance.</p>
<p>.</p>
<hr />
<p><em>Side issue about word prefixed space - Already fixed</em></p>
<p>Thanks to @amdex1 and @cronoik for helping with a side issue.
This problem arose since the tokenizer, not only splits a single word in multiple tokens, but it also adds special characters if the word does not occur at the begging of a sentence.
E.g.:</p>
<blockquote>
<p>The word &quot;Valkyria&quot;:</p>
<ul>
<li>at the <em>beginning</em> of a sentences gets split as ['V', 'alky', 'ria'] with corresponding IDs: [846, 44068, 6374].</li>
<li>while in the <em>middle</em> of a sentence as ['Ä V', 'alky', 'ria'] with corresponding IDs: [468, 44068, 6374],</li>
</ul>
</blockquote>
<p>It is solved by setting <code>add_prefix_space=True</code> in the tokenizer.</p>
","1851386","","1851386","","2020-07-12 07:35:35","2020-07-12 07:35:35","Masking only specific words with Huggingface","<python><nlp><pytorch><huggingface-transformers><huggingface-tokenizers>","0","10","","","","CC BY-SA 4.0"
"62830783","1","62830856","","2020-07-10 08:59:58","","1","238","<p>I am following the <a href=""https://huggingface.co/transformers/v2.0.0/examples.html"" rel=""nofollow noreferrer"">documentation</a> on the hugging face website, in there they say that to fine-tune GPT-2 I should use the script
<a href=""https://github.com/huggingface/transformers/blob/master/examples/run_lm_finetuning.py"" rel=""nofollow noreferrer"">run_lm_finetuning.py</a> for fine-tuning, and the script <a href=""https://github.com/huggingface/transformers/blob/master/examples/run_generation.py"" rel=""nofollow noreferrer"">run_generation.py</a>
for inference.
However, both scripts don't actually exist on GitHub anymore.</p>
<p>Does anybody know whether the documentation is outdated? or where to find those two scripts?</p>
<p>Thanks</p>
","7710572","","1243762","","2020-11-29 12:03:01","2020-11-29 12:03:01","Scripts missing for GPT-2 fine tune, and inference in Hugging-face GitHub?","<python><huggingface-transformers><language-model><gpt-2>","1","0","","","","CC BY-SA 4.0"
"65750520","1","","","2021-01-16 14:02:09","","0","51","<p>I wanted to get masked word predictions for a few bert-base models. I am converting the pytorch models to the original bert tf format using <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py"" rel=""nofollow noreferrer"">this</a> by modifying the code to load BertForPreTraining state_dict. I am unaware of the use of cls/predictions/decoder in the <a href=""https://i.stack.imgur.com/RvsP1.png"" rel=""nofollow noreferrer"">snippet</a> here, to make the masked predictions. The original BERT codebase does not have this layer, hence. Is it used, or can I safely disregard this to obtain predictions?</p>
","15014682","","","","","2021-01-16 14:02:09","Weights used for Masked LM predictions on Huggingface","<tensorflow><bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"62757772","1","","","2020-07-06 14:04:07","","0","1836","<p>I am using transformer version 3.0.0 for my project and have some questions.</p>
<p>I want to use a bert model with masked lm pretraining for protein sequences.
To get a character level tokenizer I derived from the BertTokenizer</p>
<pre><code>from transformers import BertTokenizer
class DerivedBertTok(BertTokenizer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    def tokenize(self, text):
        if isinstance(text, np.ndarray):
            assert len(text) == 1
            text = text[0]
        return [x if x in self.vocab else self.unk_token for x in text]
</code></pre>
<p>my vocab looks like this</p>
<pre><code>[PAD]
[CLS]
[SEP]
[UNK]
[MASK]
A
R
N
D
B
C
E
Q
Z
G
H
I
L
K
M
F
P
S
T
W
Y
V
</code></pre>
<p>The usage seems quite similar to what i have seen in the docs:</p>
<pre><code>d_tokenizer = DerivedBertTok(
    vocab_file=vocab_path,
    do_lower_case=False,
    do_basic_tokenize=False,
    tokenize_chinese_chars=False
)
d_tokenizer.encode_plus(np.array([&quot;AXEF&quot;])[0], 
                      max_length=20,
                      pad_to_max_length=True,
                      add_special_tokens=True,
                      truncation=True,
                      return_tensors='pt')
</code></pre>
<p>From this I was building a pytorch Dataset with a custom collate function.
all the collate function does is taking all input tensors and stacking them</p>
<pre><code>from transformers import BatchEncoding
    def collate_fn(self, batch):
        # this function will not work for higher dimension inputs
        elem = batch[0]
        elem_type = type(elem)
        if isinstance(elem, BatchEncoding):
            new_shapes = {key: (len(batch), value.shape[1]) for key, value in elem.items()}
            outs = {key: value.new_empty(new_shapes[key]) for key, value in elem.items()}
            if torch.utils.data.get_worker_info() is not None:
                [v.share_memory_() for v in outs.values()]
            return {key: torch.stack(tuple((d[key].view(-1) for d in batch)), 0, out=outs[key]) for key in elem.keys()}
        else:
            raise ValueError(f&quot;type: {elem_type} not understood&quot;)
</code></pre>
<p>Question 1:
So I was wondering if the BatchEncoding or another class is already capable of doing this (and doing it possibly better?). Or using a different Dataset/ DataLoader class altogether.</p>
<p>Question 2:
Additionally, I want to mask some of the Inputs as required for the masked LM, however I did not manage find any implementation in the transformer library. Are there any recommendations for doing this?</p>
","13878065","","","","","2021-04-07 14:18:33","Hugging face: tokenizer for masked lm question","<python-3.x><pytorch><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"68945422","1","","","2021-08-26 21:20:01","","0","69","<p>I am a bit confused about how to consume huggingface <code>transformers</code> outputs to train a simple language binary classifier model that predicts if Albert Einstein said a sentence or not.</p>
<pre><code>from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)

inputs = [&quot;Hello World&quot;, &quot;Hello There&quot;, &quot;Bye Bye&quot;, &quot;Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.&quot;]
for input in inputs:
  inputs = tokenizer(input, return_tensors=&quot;pt&quot;)
  outputs = model(**inputs)
  print(outputs[0].shape, input, len(input))
</code></pre>
<p>Output:</p>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
torch.Size([1, 4, 768]) Hello World 11
torch.Size([1, 4, 768]) Hello There 11
torch.Size([1, 4, 768]) Bye Bye 7
torch.Size([1, 23, 768]) Two things are infinite: the universe and human stupidity; and I'm not sure about the universe. 95
</code></pre>
<p>As you can see the dimensions of the output varies with the length of the input. Now assume I would like to train a binary classifier that predicts if Einstein said the input sentence or not and the input of the network will be the prediction of the BERT <code>transformer</code>.</p>
<p>How could I write a CNN model that takes a tensor <code>[1, None, 768]</code> in pytorch? It seems that the second dimension changes with the length of the input.</p>
","1031417","","1031417","","2021-08-27 11:33:11","2021-08-27 11:33:11","How to use a BERT model from transformers to feed a binary classifier CNN?","<python><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65529156","1","65563077","","2021-01-01 11:07:28","","1","1073","<p>Resuming the <code>GPT2</code> finetuning, implemented from <code>run_clm.py</code></p>
<p>Does GPT2 <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface</a>  has a parameter to resume the training from the saved checkpoint, instead training again from the beginning? Suppose the python notebook crashes while training, the checkpoints will be saved, but when I train the model again still it starts the training from the beginning.</p>
<p>Source: <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py"" rel=""nofollow noreferrer"">here</a></p>
<p>finetuning code:</p>
<pre><code>!python3 run_clm.py \
    --train_file source.txt \
    --do_train \
    --output_dir gpt-finetuned \
    --overwrite_output_dir \
    --per_device_train_batch_size 2 \
    --model_name_or_path=gpt2 \
    --save_steps 100 \
    --num_train_epochs=1 \
    --block_size=200 \
    --tokenizer_name=gpt2
</code></pre>
<p>From the above code, <code>run_clm.py</code> is a script provided by <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py"" rel=""nofollow noreferrer"">huggingface</a> to finetune gpt2 to train with the customized dataset</p>
","1793799","","","","","2021-01-18 14:07:04","Huggingface Transformer - GPT2 resume training from saved checkpoint","<python><pytorch><huggingface-transformers><language-model><gpt-2>","1","0","","","","CC BY-SA 4.0"
"68958200","1","","","2021-08-27 18:58:27","","0","40","<p>Based on <a href=""https://github.com/huggingface/tokenizers/issues/244"" rel=""nofollow noreferrer"">https://github.com/huggingface/tokenizers/issues/244</a> question I'm trying to complete my request to use <em>WordLevel</em> tokenizer with roberta transformers model. My vocabulary containts numbers as string and special tokens. I have some issue and I can localize what is wrong - but don't know how to fix it. The situation is following:</p>
<pre><code>tokenizer = RobertaTokenizerFast.from_pretrained(&quot;wordlevel&quot;, max_len=num_secs_max)
dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=&quot;./optiver.txt&quot;,
    block_size=128,
)
</code></pre>
<p>I see that <em>LineByLineTextDataset</em> splits numbers on separate digits - and it is wrong for me. I see that it is result of the tokenizer.batch_encode_plus working.
I have found the advice that I need to add <em>is_split_into_words</em> <em>=</em> <em>True</em> parameter when construct <em>RobertaTokenizerFast</em> - but I didn't have success. Please explain me how split my corpus by words not symbols...</p>
<p>Below more details about used code:</p>
<pre><code>from tokenizers.implementations import BaseTokenizer
class WordLevelBertTokenizer(BaseTokenizer):
    &quot;&quot;&quot; WordLevelBertTokenizer
    Represents a simple word level tokenization for BERT.
    &quot;&quot;&quot;

    def __init__(
        self,
        vocab_file: str,
    ):
        tokenizer = Tokenizer(WordLevel.from_file(vocab_file))
        tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()
        
        if vocab_file is not None:
            sep_token_id = tokenizer.token_to_id(str(&quot;&lt;/s&gt;&quot;))
            if sep_token_id is None:
                raise TypeError(&quot;sep_token not found in the vocabulary&quot;)
            cls_token_id = tokenizer.token_to_id(str(&quot;&lt;s&gt;&quot;))
            if cls_token_id is None:
                raise TypeError(&quot;cls_token not found in the vocabulary&quot;)

            tokenizer.post_processor = BertProcessing(
                (str(&quot;&lt;/s&gt;&quot;), sep_token_id), (str(&quot;&lt;s&gt;&quot;), cls_token_id)
            )

        parameters = {
            &quot;model&quot;: &quot;WordLevel&quot;,
            &quot;sep_token&quot;: &quot;&lt;/s&gt;&quot;,
            &quot;cls_token&quot;: &quot;&lt;s&gt;&quot;,
            &quot;pad_token&quot;: &quot;&lt;pad&gt;&quot;,
            &quot;mask_token&quot;: &quot;&lt;mask&gt;&quot;,
        }

        super().__init__(tokenizer, parameters)


from transformers import RobertaConfig

tokenizer = WordLevelBertTokenizer(&quot;./wordlevel/vocab.json&quot;)

config = RobertaConfig(
    vocab_size=tokenizer.get_vocab_size(),
    max_position_embeddings=tokenizer.get_vocab_size(),
    num_attention_heads=12,
    num_hidden_layers=6,
    type_vocab_size=1,
)

from transformers import RobertaTokenizerFast

tokenizer = RobertaTokenizerFast.from_pretrained(&quot;wordlevel&quot;, max_len=num_secs_max, add_prefix_space=True)

from transformers import RobertaForMaskedLM

model = RobertaForMaskedLM(config=config)

print(f'Num of model parameters = {model.num_parameters()}')

dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=&quot;./optiver.txt&quot;,
    block_size=128,
)
</code></pre>
<p>Also simple test for batch_encode_plus
<em>test = [['1234']]
tokenizer.batch_encode_plus(t, is_split_into_words  = True)</em>
output:
<em>{'input_ids': [[1224, 2, 3, 4, 5, 1225]], 'attention_mask': [[1, 1, 1, 1, 1, 1]]}</em>
It seems this output is not I want tokenizer splits number on separate digits.
P.S. Here is fragment of my vocab file:
{&quot;0&quot;:1, &quot;1&quot;:2, &quot;2&quot;:3, &quot;3&quot;:4, &quot;4&quot;:5, &quot;5&quot;:6, &quot;6&quot;:7, &quot;7&quot;:8, ....  &quot;1220&quot;:1221, &quot;1221&quot;:1222, &quot;&quot;:1223, &quot;<s>&quot;:1224, &quot;</s>&quot;:1225, &quot;&quot;:1226}</p>
","5592430","","5592430","","2021-08-29 21:05:55","2021-08-29 21:05:55","How to force LineByLineTextDataset split text corpus by words rather than symbols","<python><huggingface-transformers><huggingface-tokenizers>","0","2","","","","CC BY-SA 4.0"
"68997701","1","","","2021-08-31 11:17:32","","0","78","<p>I have successfully installed transformers package in my Jupyter Notebook from Anaconda administrator console using the command '<code>conda install -c conda-forge transformers</code>'.</p>
<p>However when I try to load the transformers package in my Jupyter notebook using '<code>import transformers</code>' command, I am getting an error, <code>'ValueError: got_ver is None'</code>.</p>
<p>I am not sure how I can resolve this. Appreciate any inputs.</p>
<p>Below is the complete error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-58-279c49635b32&gt; in &lt;module&gt;
----&gt; 1 import transformers

C:\ProgramData\Anaconda3\lib\site-packages\transformers\__init__.py in &lt;module&gt;
     41 
     42 # Check the dependencies satisfy the minimal versions required.
---&gt; 43 from . import dependency_versions_check
     44 from .file_utils import (
     45     _LazyModule,

C:\ProgramData\Anaconda3\lib\site-packages\transformers\dependency_versions_check.py in &lt;module&gt;
     39                 continue  # not required, check version only if installed
     40 
---&gt; 41         require_version_core(deps[pkg])
     42     else:
     43         raise ValueError(f&quot;can't find {pkg} in {deps.keys()}, check dependency_versions_table.py&quot;)

C:\ProgramData\Anaconda3\lib\site-packages\transformers\utils\versions.py in require_version_core(requirement)
    118     &quot;&quot;&quot;require_version wrapper which emits a core-specific hint on failure&quot;&quot;&quot;
    119     hint = &quot;Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git master&quot;
--&gt; 120     return require_version(requirement, hint)

C:\ProgramData\Anaconda3\lib\site-packages\transformers\utils\versions.py in require_version(requirement, hint)
    112     if want_ver is not None:
    113         for op, want_ver in wanted.items():
--&gt; 114             _compare_versions(op, got_ver, want_ver, requirement, pkg, hint)
    115 
    116 

C:\ProgramData\Anaconda3\lib\site-packages\transformers\utils\versions.py in _compare_versions(op, got_ver, want_ver, requirement, pkg, hint)
     43 def _compare_versions(op, got_ver, want_ver, requirement, pkg, hint):
     44     if got_ver is None:
---&gt; 45         raise ValueError(&quot;got_ver is None&quot;)
     46     if want_ver is None:
     47         raise ValueError(&quot;want_ver is None&quot;)

ValueError: got_ver is None
</code></pre>
","12129443","","12129443","","2021-09-02 12:50:21","2021-09-02 12:50:21","Import of transformers package throwing value_error","<python><anaconda><jupyter><huggingface-transformers>","0","4","","","","CC BY-SA 4.0"
"67309910","1","","","2021-04-29 01:31:16","","0","36","<p>I am trying to add an attention mechanism to get the weighted average of last hidden states from BERT encoder rather than just using Avg pooling. I have a sentence matrix of size [n_words * dimension] and I want to reduce it to [1*dimension].</p>
<p>I don't know where or how I should add the attention layer and how to train the layer to get the weight parameters. Please help.</p>
<p>I am using the HuggingFace model to train/fine-tune my BERT on the STS benchmark dataset.</p>
","15789305","","","","","2021-04-29 01:31:16","Adding Attention mechanism to BERT hidden states to get a single sentence vector","<bert-language-model><huggingface-transformers><transformer><attention-model>","0","0","","","","CC BY-SA 4.0"
"63689270","1","","","2020-09-01 14:02:15","","0","873","<p>When trying to convert the checkpoint of a self pre-trained tensorflow BERT model (using the <strong><a href=""https://github.com/google-research/bert/blob/master/run_pretraining.py"" rel=""nofollow noreferrer"">create-pretraining.py</a></strong> script from google) into a pytorch model using the <strong><a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py"" rel=""nofollow noreferrer"">convert_bert_original_tf_checkpoint_to_pytorch.py</a></strong> from Huggingface.</p>
<p>I always end up with the following error:
<strong>AttributeError: 'BertEmbeddings' object has no attribute 'bias'</strong></p>
<p>The init_vars names (just the first ones) look like this:</p>
<pre><code>['bert/embeddings/layer_normalization/beta', 'bert/embeddings/layer_normalization/beta/adam_m', 'bert/embeddings/layer_normalization/beta/adam_v', 'bert/embeddings/layer_normalization/gamma', 'bert/embeddings/layer_normalization/gamma/adam_m', 'bert/embeddings/layer_normalization/gamma/adam_v']
</code></pre>
<p>Code that produces the error looks like this:</p>
<pre><code>for m_name in name:                                                     
            if re.fullmatch(r&quot;[A-Za-z]+_\d+&quot;, m_name):                          
                scope_names = re.split(r&quot;_(\d+)&quot;, m_name)                       
            else:                                                               
                scope_names = [m_name]                                          
            if scope_names[0] == &quot;kernel&quot; or scope_names[0] == &quot;gamma&quot;:         
                pointer = getattr(pointer, &quot;weight&quot;)                            
            elif scope_names[0] == &quot;output_bias&quot; or scope_names[0] == &quot;beta&quot;:   
                print(scope_names)                                              
                pointer = getattr(pointer, &quot;bias&quot;)                              
            elif scope_names[0] == &quot;output_weights&quot;:                            
                pointer = getattr(pointer, &quot;weight&quot;)                            
            elif scope_names[0] == &quot;squad&quot;:                                     
                pointer = getattr(pointer, &quot;classifier&quot;)                        
            else:                                                               
                try:                                                            
                    pointer = getattr(pointer, scope_names[0])                  
                except AttributeError:                                          
                    logger.info(&quot;Skipping {}&quot;.format(&quot;/&quot;.join(name)))
</code></pre>
<p>Going through all the names and getting the right attributes from the model. When it comes to the Layer Normalization in the BertEmbeddings the script produces an error. Did anyone else encouter that error before? How did you fix this?</p>
<p>Here again the whole stacktrace:</p>
<pre><code>Traceback (most recent call last):
  File &quot;convert_bert_original_tf_checkpoint_to_pytorch.py&quot;, line 62, in &lt;module&gt;
    convert_tf_checkpoint_to_pytorch(args.tf_checkpoint_path, args.bert_config_file, args.pytorch_dump_path)
  File &quot;convert_bert_original_tf_checkpoint_to_pytorch.py&quot;, line 37, in convert_tf_checkpoint_to_pytorch
    load_tf_weights_in_bert(model, config, tf_checkpoint_path)
  File &quot;/modeling_bert.py&quot;, line 136, in load_tf_weights_in_bert
    pointer = getattr(pointer, &quot;bias&quot;)
  File &quot;module.py&quot;, line 594, in __getattr__
    type(self).__name__, name))
AttributeError: 'BertEmbeddings' object has no attribute 'bias'
</code></pre>
<p>Bert Config is the following:</p>
<pre><code>Building PyTorch model from configuration: BertConfig {
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 512,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 2048,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;bert&quot;,
  &quot;num_attention_heads&quot;: 8,
  &quot;num_hidden_layers&quot;: 8,
  &quot;pad_token_id&quot;: 0,
  &quot;type_vocab_size&quot;: 2,
  &quot;vocab_size&quot;: 30522
}
</code></pre>
","14202824","","","","","2021-06-14 17:47:23","'BertEmbeddings' object has no attribute 'bias' while converting tf checkpoint","<python><tensorflow><pytorch><bert-language-model><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"67409781","1","","","2021-05-05 22:57:59","","0","236","<p>I try to set up a german ner, pretrained with bert via the huggingface pipeline. For some texts the following code throws an error &quot;RuntimeError: The size of tensor a (921) must match the size of tensor b (512) at non-singleton dimension 1&quot; for the line &quot;ner = classifier(text)&quot;.</p>
<p>I already did some research with stackoverflow and this is the most similar problem i found:<a href=""https://stackoverflow.com/questions/64320883/the-size-of-tensor-a-707-must-match-the-size-of-tensor-b-512-at-non-singleto"">The size of tensor a (707) must match the size of tensor b (512) at non-singleton dimension 1</a></p>
<p>The solution sounds good, I just don't know where i can specify those settings while using the huggingface pipeline. What do I need to change in my code to make it work properly?</p>
<p>thanks!</p>
<pre><code>from transformers import pipeline
classifier = pipeline('ner', model=&quot;fhswf/bert_de_ner&quot;, grouped_entities=True)
text = (dic[pi].text)
ner = classifier(text)
</code></pre>
","11494405","","","","","2021-05-05 22:57:59","huggingface pipeline: bert NER task throws RuntimeError: The size of tensor a (921) must match the size of tensor b (512) at non-singleton dimension 1","<python><bert-language-model><huggingface-transformers><ner><huggingface-tokenizers>","0","1","","","","CC BY-SA 4.0"
"68988378","1","68998482","","2021-08-30 17:52:05","","0","33","<p>When using <code>TFAutoModel.from_pretrained()</code> the following error is returned</p>
<pre class=""lang-py prettyprint-override""><code>~/opt/anaconda3/envs/contracts/lib/python3.8/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols)
    110         # Prepare IPython progress bar
    111         if IProgress is None:  # #187 #451 #558 #872
--&gt; 112             raise ImportError(
    113                 &quot;IProgress not found. Please update jupyter and ipywidgets.&quot;
    114                 &quot; See https://ipywidgets.readthedocs.io/en/stable&quot;

ImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
</code></pre>
<p>I've followed these links, manually installed tqdm, IProgress, and ipywidgets. For whatever reason, this TensorFlow object method is unable to execute. Any recommendations?</p>
","13142245","","13142245","","2021-08-30 18:52:34","2021-08-31 12:12:40","Jupyter notebook progress bar issue in TensorFlow method","<python><tensorflow><jupyter-notebook><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"69046964","1","69051496","","2021-09-03 14:58:19","","1","56","<p>I am confused about using huggingface BERT models and about how to make them yield a prediction at a fixed shape, regardless of input size (i.e., input string length).</p>
<p>I tried to call the tokenizer with the parameters <code>padding=True, truncation=True, max_length = 15</code>, but the prediction output dimensions for <code>inputs = [&quot;a&quot;, &quot;a&quot;*20, &quot;a&quot;*100, &quot;abcede&quot;*20000]</code> are not fixed. What am I missing here?</p>
<pre><code>from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)

inputs = [&quot;a&quot;, &quot;a&quot;*20, &quot;a&quot;*100, &quot;abcede&quot;*20000]
for input in inputs:
  inputs = tokenizer(input, padding=True, truncation=True, max_length = 15, return_tensors=&quot;pt&quot;)
  outputs = model(**inputs)
  print(outputs.last_hidden_state.shape, input, len(input))
</code></pre>
<p>output:</p>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
torch.Size([1, 3, 768]) a 1
torch.Size([1, 12, 768]) aaaaaaaaaaaaaaaaaaaa 20
torch.Size([1, 15, 768]) aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa 100
torch.Size([1, 3, 768]) abcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcededeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeab....deabbcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcede 120000
</code></pre>
","1031417","","1031417","","2021-09-03 15:08:33","2021-09-03 23:54:48","Can BERT output be fixed in shape, irrespective of string size?","<python><pytorch><huggingface-transformers><huggingface-tokenizers>","2","0","","","","CC BY-SA 4.0"
"67420868","1","","","2021-05-06 15:06:06","","0","72","<p>I'm having some issues trying to calculate the accuracy of a custom BERT model which also uses the pretrained model from Huggingface. This is the code that I have :</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn import metrics, linear_model
import torch
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertModel
from torch import cuda
import re
import torch.nn as nn

device = 'cuda' if cuda.is_available() else 'cpu'
MAX_LEN = 200
TRAIN_BATCH_SIZE = 8  # 12, 64
VALID_BATCH_SIZE = 4
EPOCHS = 1
LEARNING_RATE = 1e-4 #3e-4, 1e-4, 5e-5, 3e-5
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')
file1 = open('test.txt', 'r')
list_com = []
list_label = []
for line in file1:
 possible_labels = 'positive|negative'
 label = re.findall(possible_labels, line)
 line = re.sub(possible_labels, ' ', line)
 line = re.sub('\n', ' ', line)
 list_com.append(line)
 list_label.append(label[0])

list_tuples = list(zip(list_com, list_label))
file1.close()
labels = ['positive', 'negative']
df = pd.DataFrame(list_tuples, columns=['review', 'sentiment'])
df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})
for i in range(0,len(df['sentiment'])):
 list_label[i] = df['sentiment'][i]
#print(df)

class CustomDataset(Dataset):
 def __init__(self, dataframe, tokenizer, max_len):
  self.tokenizer = tokenizer
  self.data = dataframe
  self.comment_text = dataframe.review
  self.targets = self.data.sentiment
  self.max_len = max_len
 def __len__(self):
  return len(self.comment_text)
 def __getitem__(self, index):
  comment_text = str(self.comment_text[index])
  comment_text = &quot; &quot;.join(comment_text.split())

  inputs = self.tokenizer.encode_plus(comment_text,None,add_special_tokens=True,max_length=self.max_len,
   pad_to_max_length=True,return_token_type_ids=False,truncation=True)
  ids = inputs['input_ids']
  mask = inputs['attention_mask']

  return {
   'ids': torch.tensor(ids, dtype=torch.long),
   'mask': torch.tensor(mask, dtype=torch.long),
   'targets': torch.tensor(self.targets[index], dtype=torch.float)
  }
train_size = 0.8
train_dataset=df.sample(frac=train_size,random_state=200)
test_dataset=df.drop(train_dataset.index).reset_index(drop=True)
train_dataset = train_dataset.reset_index(drop=True)

print(&quot;FULL Dataset: {}&quot;.format(df.shape))
print(&quot;TRAIN Dataset: {}&quot;.format(train_dataset.shape))
print(&quot;TEST Dataset: {}&quot;.format(test_dataset.shape))

training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)
testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)
train_params = {'batch_size': TRAIN_BATCH_SIZE,'shuffle': True,'num_workers': 0}
test_params = {'batch_size': VALID_BATCH_SIZE,'shuffle': True,'num_workers': 0}
training_loader = DataLoader(training_set, **train_params)
testing_loader = DataLoader(testing_set, **test_params)


class BERTClass(torch.nn.Module):
 def __init__(self):
   super(BERTClass, self).__init__()
   self.bert = BertModel.from_pretrained('bert-base-multilingual-uncased',return_dict=False,num_labels = 2)
   self.lstm = nn.LSTM(768, 256, batch_first=True, bidirectional=True)
   self.linear = nn.Linear(256*2,2)

 def forward(self, ids , mask):
  sequence_output, pooled_output = self.bert(ids, attention_mask=mask )
  lstm_output, (h, c) = self.lstm(sequence_output)  ## extract the 1st token's embeddings
  hidden = torch.cat((lstm_output[:, -1, :256], lstm_output[:, 0, 256:]), dim=-1)
  linear_output = self.linear(lstm_output[:, -1].view(-1, 256 * 2))

  return linear_output

model = BERTClass()
model.to(device)
#print(model)
def loss_fn(outputs, targets):
 return torch.nn.CrossEntropyLoss()(outputs, targets)
optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)

def train(epoch):
 model.train()
 for _, data in enumerate(training_loader, 0):
  ids = data['ids'].to(device, dtype=torch.long)
  mask = data['mask'].to(device, dtype=torch.long)
  targets = data['targets'].to(device, dtype=torch.long)
  outputs = model(ids, mask)
  optimizer.zero_grad()
  loss = loss_fn(outputs, targets)
  if _ % 1000 == 0:
   print(f'Epoch: {epoch}, Loss:  {loss.item()}')
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

for epoch in range(EPOCHS):
  train(epoch)


def validation(epoch):
 model.eval()
 fin_targets = []
 fin_outputs = []
 with torch.no_grad():
  for _, data in enumerate(testing_loader, 0):
   ids = data['ids'].to(device, dtype=torch.long)
   mask = data['mask'].to(device, dtype=torch.long)
   targets = data['targets'].to(device, dtype=torch.float)
   outputs = model(ids, mask)
   fin_targets.extend(targets.cpu().detach().numpy().tolist())
   fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())
 return fin_outputs, fin_targets

for epoch in range(EPOCHS):
    outputs, targets = validation(epoch)
    outputs = np.array(outputs) &gt;= 0.5
    accuracy = metrics.accuracy_score(targets, outputs)
    print(f&quot;Accuracy Score = {accuracy}&quot;)


torch.save(model.state_dict, 'model.pt')
print(f'Model saved!')
</code></pre>
<p>It should be a binary classification, positive(1) or negative(0), but when i try to compute the accuracy i get the error <code>ValueError: Classification metrics can't handle a mix of binary and multilabel-indicator targets</code> oh this line <code>accuracy = metrics.accuracy_score(targets, outputs)</code> .The outputs look like this:</p>
<pre><code>[[ True False]
 [False False]
 [ True False]
 [ True False]
 [ True False]
 [ True False]
 [ True False]
 [False  True]
 [ True False]
 [ True False]
 [False  True]]
</code></pre>
<p>Can someone advise what would be the fix to this? Or if there something else that can improve this? Also, I saved the model and I want to know how can I use the saved model to classify user input in another .py file?(assuming that we enter a sentence from keyboard and we want the model to classify it).</p>
","15645097","","15645097","","2021-05-06 17:03:44","2021-05-06 17:03:44","Issues calculating accuracy for custom BERT model","<python-3.x><nlp><bert-language-model><huggingface-transformers>","0","5","","","","CC BY-SA 4.0"
"68961546","1","68964358","","2021-08-28 05:15:45","","1","63","<p><code>BertTokenizer</code> can tokenize a sentence to a list of tokens, where some long words e.g. &quot;embeddings&quot; is splitted to several subwords i.e. 'em', '##bed', '##ding', and '##s'.</p>
<p><strong>Is there a way to locate the subwords?</strong> For example,</p>
<pre><code>t = BertTokenizer.from_pretrained('bert-base-uncased')

tokens = t('word embeddings', add_special_tokens=False)
location = locate_subwords(tokens)
</code></pre>
<p>I want the <code>location</code> be like <code>[0, 1, 1, 1, 1]</code> corresponding to <code>['word', 'em', '##bed', '##ding', '##s']</code>, where 0 means normal word, 1 means subword.</p>
","13491606","","","","","2021-08-28 12:28:44","Get the index of subwords produced by BertTokenizer (in transformers library)","<pytorch><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"65285054","1","65332563","","2020-12-14 07:22:16","","0","942","<p>I am trying to do a multitask multiclass sentence classification task using the pretrained BERT model from the huggingface transformers library . I have tried to use the BERTForSequenceClassification model from there but the issue I am having is that I am not able to extend it for multiple tasks . I will try to make it more informative through this example.</p>
<p>Suppose we have four different tasks and for each sentence and for each task we have labels like this as follows in the examples:</p>
<ol>
<li>A :[ 'a' , 'b' , 'c' , 'd' ]</li>
<li>B :[ 'e' , 'f' , 'g' , 'h' ]</li>
<li>C :[ 'i' , 'j' , 'k' , 'l' ]</li>
<li>D :[ 'm' , 'n' , 'o' , 'p' ]</li>
</ol>
<p>Now , if I have a sentence for this model , I want the output to give me output for all the four different tasks (A,B,C,D).</p>
<p>This is what I was doing earlier</p>
<pre><code>   model = BertForSequenceClassification.from_pretrained(
    &quot;bert-base-uncased&quot;, # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 4, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)
</code></pre>
<p>Then I tried to implement a CustomBERT model like this :</p>
<pre><code>class CustomBERTModel(nn.Module):
    def __init__(self):
          super(CustomBERTModel, self).__init__()
          self.bert = BertModelForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;)
          ### New layers:
          self.linear1 = nn.Linear(768, 256)
          self.linear2 = nn.Linear(256, num_classes) ## num_classes is the number of classes in this example

    def forward(self, ids, mask):
          sequence_output, pooled_output = self.bert(
               ids, 
               attention_mask=mask)

          # sequence_output has the following shape: (batch_size, sequence_length, 768)
          linear1_output = self.linear1(sequence_output[:,0,:].view(-1,768)) 
          linear2_output = self.linear2(linear2_output)

          return linear2_output
</code></pre>
<p>I have went through the answers to questions similar to it available earlier but none of them appeared to
answer my question . I have tried to get through all the points which I think can be helpful for the understanding of my problem and would try to clear further more in case of any descrepancies made by me in the explaination of the question . Any answers related to this will be very much helpful .</p>
","10773907","","","","","2020-12-16 23:22:46","How to add a multiclass multilabel layer on top of pretrained BERT model?","<deep-learning><pytorch><bert-language-model><huggingface-transformers><transfer-learning>","1","3","","","","CC BY-SA 4.0"
"65288668","1","","","2020-12-14 11:58:03","","0","34","<p>I am using the the TFRobertaForSequenceClassification class of the huggingface library to create a classifier. According to the documentation, the logits output should have a shape of (batch_size, num_labels). I however get (batch_size, seq_length, num_labels) and I dont understand why.</p>
<p>To reproduce this:</p>
<pre><code>from transformers import TFRobertaForSequenceClassification, RobertaConfig
import numpy as np

seq_len = 512

classifier = TFRobertaForSequenceClassification(RobertaConfig())

#create random inputs for demo
input_ids = np.random.randint(0,10000, size=(seq_len,))
attention_mask = np.random.randint(0,2, size=(seq_len,))
token_type_ids = np.random.randint(0,2, size=(seq_len,))

#make a prediction with batch_size of 1
output = classifier.predict([input_ids, attention_mask, token_type_ids])

print(output.logits.shape)
</code></pre>
<p>This outputs logits in the shape of (512,2) but I am expecting (1,2) or (batch_size, num_labels). Can anyone shed any light on why it behaves like this?</p>
","1180286","","1180286","","2020-12-14 12:51:59","2020-12-15 07:37:29","Getting wrong shape of logits from TFRobertaForSequenceClassification","<python-3.x><tensorflow><keras><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65741789","1","","","2021-01-15 18:36:48","","0","69","<p>I was looking to convert a few BertForMaskedLM models to TF1 bert ckpt format. I understand that <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py"" rel=""nofollow noreferrer"">this</a> can be used but supports BertModel only right now without the CLS layer. Any idea how I can change it to support BertForMaskedLM?</p>
","15014682","","","","","2021-01-15 18:36:48","Convert BertForMaskedLM pytorch checkpoint to original TF1 bert ckpt","<tensorflow><bert-language-model><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68948814","1","","","2021-08-27 06:18:12","","0","14","<pre><code>gpt2 = GPT2LMHeadModel.from_pretrained(â€˜gpt2â€™, cache_dir=&quot;./cache&quot;, local_files_only=True)
gpt2.trainable = False
gpt2.config.pad_token_id=50256
gen_nlp = pipeline(â€œtext-generationâ€, model=gpt2, tokenizer=tokenizer_gpt2, device=args.gpu, return_full_text=False)
contents = ds.df_train.sample(10)[â€˜contentâ€™].tolist()
results_trunk = gen_nlp(contents, max_length=64, do_sample=True, top_p=0.9, top_k=0,
repetition_penalty=1.0, num_return_sequences=4, clean_up_tokenization_spaces=True)
</code></pre>
<p>I use off-the-shelf GPT2 for open-ended generation. I find that there is a parameter</p>
<blockquote>
<p>trainable</p>
</blockquote>
<p>which needs to be set False or True before using.
Anyone know the nuance in this setting?
What is the best setting for this parameter?</p>
","6407393","","","","","2021-08-27 06:18:12","Nuance in usage of GPT2 when setting the attribute trainable","<huggingface-transformers>","0","2","1","","","CC BY-SA 4.0"
"69025750","1","69025751","","2021-09-02 07:18:15","","-1","82","<p>Is there a Step by step explanation on how to fine-tune HuggingFace BERT model?</p>
","4281353","","4281353","","2021-09-04 00:22:45","2021-09-04 00:22:45","How to fine-tune HuggingFace BERT model for Text Classification","<machine-learning><huggingface-transformers><transfer-learning>","1","0","","2021-09-05 06:17:45","","CC BY-SA 4.0"
"65304058","1","65304411","","2020-12-15 10:29:14","","1","57","<p>I am trying to create a question-answering model with the word embedding model BERT from google. I am new to this and would really want to use my own corpus for the training. At first I used an example from the <a href=""https://huggingface.co/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2"" rel=""nofollow noreferrer"">huggingface site</a> and that worked fine:</p>
<pre><code>from transformers import pipeline

qa_pipeline = pipeline(
    &quot;question-answering&quot;,
    model=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;,
    tokenizer=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;
)

qa_pipeline({
    'context': &quot;Amsterdam is de hoofdstad en de dichtstbevolkte stad van Nederland.&quot;,
    'question': &quot;Wat is de hoofdstad van Nederland?&quot;})
</code></pre>
<p>output</p>
<pre><code>&gt; {'answer': 'Amsterdam', 'end': 9, 'score': 0.825619101524353, 'start': 0}
</code></pre>
<p>So, I tried creating a .txt file to test if it was possible to interchange the sentence in the context parameter with the exact same sentence but in a .txt file.</p>
<pre><code>with open('test.txt') as f:
    lines = f.readlines()

qa_pipeline = pipeline(
    &quot;question-answering&quot;,
    model=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;,
    tokenizer=&quot;henryk/bert-base-multilingual-cased-finetuned-dutch-squad2&quot;
)

qa_pipeline({
    'context': lines,
    'question': &quot;Wat is de hoofdstad van Nederland?&quot;})
</code></pre>
<p>But this gave me the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-7-2bae0ecad43e&gt; in &lt;module&gt;()
     10 qa_pipeline({
     11     'context': lines,
---&gt; 12     'question': &quot;Wat is de hoofdstad van Nederland?&quot;})

5 frames
/usr/local/lib/python3.6/dist-packages/transformers/data/processors/squad.py in _is_whitespace(c)
     84 
     85 def _is_whitespace(c):
---&gt; 86     if c == &quot; &quot; or c == &quot;\t&quot; or c == &quot;\r&quot; or c == &quot;\n&quot; or ord(c) == 0x202F:
     87         return True
     88     return False

TypeError: ord() expected a character, but string of length 66 found
</code></pre>
<p>I was just experimenting with ways to read and use a .txt file, but I don't seem to find a different solution. I did some research on the huggingface pipeline() function and this is what was written about the question and context parameters:</p>
<p><a href=""https://i.stack.imgur.com/Yk4bM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yk4bM.png"" alt=""enter image description here"" /></a></p>
","10792857","","","","","2020-12-15 10:50:46","How to use my own corpus on word embedding model BERT","<word-embedding><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67412925","1","67416078","","2021-05-06 06:30:55","","0","119","<p>I'm trying to add a few new words to the vocabulary of a pretrained HuggingFace Transformers model. I did the following to change the vocabulary of the tokenizer and also increase the embedding size of the model:</p>
<pre><code>tokenizer.add_tokens(['word1', 'word2', 'word3', 'word4'])
model.resize_token_embeddings(len(tokenizer))
print(len(tokenizer)) # outputs len_vocabulary + 4
</code></pre>
<p>But after training the model on my corpus and saving it, I found out that the saved tokenizer vocabulary size hasn't changed. After checking again I found out that the abovementioned code does not change the vocabulary size (tokenizer.vocab_size is still the same) and only the len(tokenizer) has changed.</p>
<p>So now my question is; what is the difference between tokenizer.vocab_size and len(tokenizer)?</p>
","11810876","","11810876","","2021-05-06 06:50:24","2021-05-06 10:13:33","what is the difference between len(tokenizer) and tokenizer.vocab_size","<nlp><tokenize><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"67425742","1","","","2021-05-06 20:46:53","","1","20","<p>I am using a Huggingface EncoderDecoderModel with a Bert model as the encoder and a Bert model with LM head as the decoder to convert a phone sequence to a sentence (/huh-lOH/ -&gt; Hello).</p>
<p>Everything works perfectly (0.17 Cross entropy loss, really good results) except at the end of the generated sentence (generated with <code>model.generate(input_ids, ...)</code>) . The ending either:</p>
<ul>
<li>Is perfectly fine (My name is Dan)</li>
<li>Lacks some words (My name is) -&gt; This is the most common case</li>
<li>Adds repetition (My name is Dan is is Dan is) -&gt; quite common too</li>
</ul>
<p>Such specific bugs are usually easy to debug but I'm quite stuck. I was wondering if people around with better understanding of transformer models would have some intuition on the cause.</p>
<p>Thanks for any piece of help or advice</p>
","4046693","","","","","2021-05-06 20:46:53","Huggingface EncoderDecoder text generation repeats or deletes generated text ending","<bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"65321216","1","","","2020-12-16 10:12:49","","2","232","<p>I have uploaded a transformer roberta model in S3 bucket. Am now trying to run inference against the model using Pytorch with SageMaker Python SDK. I specified the model directory <code>s3://snet101/sent.tar.gz</code> which is a compressed file of the model (pytorch_model.bin) and all its dependencies.  Here is the code</p>
<pre><code>model = PyTorchModel(model_data=model_artifact,
                   name=name_from_base('roberta-model'),
                   role=role, 
                   entry_point='torchserve-predictor2.py',
                   source_dir='source_dir',
                   framework_version='1.4.0',
                   py_version = 'py3',
                   predictor_cls=SentimentAnalysis)
predictor = model.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')
test_data = {&quot;text&quot;: &quot;How many cows are in the farm ?&quot;}
prediction = predictor.predict(test_data)
</code></pre>
<p>I get the following error on the predict method of the predictor object:</p>
<pre><code>ModelError                                Traceback (most recent call last)
&lt;ipython-input-6-bc621eb2e056&gt; in &lt;module&gt;
----&gt; 1 prediction = predictor.predict(test_data)

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py in predict(self, data, initial_args, target_model, target_variant)
    123 
    124         request_args = self._create_request_args(data, initial_args, target_model, target_variant)
--&gt; 125         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
    126         return self._handle_response(response)
    127 

~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    355                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)
    356             # The &quot;self&quot; in this scope is referring to the BaseClient.
--&gt; 357             return self._make_api_call(operation_name, kwargs)
    358 
    359         _api_call.__name__ = str(py_operation_name)

~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    674             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)
    675             error_class = self.exceptions.from_code(error_code)
--&gt; 676             raise error_class(parsed_response, operation_name)
    677         else:
    678             return parsed_response

ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message &quot;Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.&quot;. See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/roberta-model-2020-12-16-09-42-37-479 in account 165258297056 for more information.
</code></pre>
<p>I checked the server log error</p>
<pre><code>java.lang.IllegalArgumentException: reasonPhrase contains one of the following prohibited characters: \r\n: Can't load config for '/.sagemaker/mms/models/model'. Make sure that:
'/.sagemaker/mms/models/model' is a correct model identifier listed on 'https://huggingface.co/models'
or '/.sagemaker/mms/models/model' is the correct path to a directory containing a config.json file
</code></pre>
<p>How can I fix this?</p>
","9128790","","","","","2021-09-30 08:55:03","Amazon Sagemaker ModelError when serving model","<amazon-s3><pytorch><amazon-sagemaker><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"58991927","1","","","2019-11-22 10:08:44","","2","186","<p>According to the HuggingFace Transformer's website (<a href=""https://huggingface.co/transformers/model_doc/gpt2.html#gpt2doubleheadsmodel"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/gpt2.html#gpt2doubleheadsmodel</a>), <strong>GPT2DoubleHeadsModel</strong> (NOT <strong>GPT2LMHeadModel</strong> but <strong>GPT2DoubleHeadsModel</strong>) is the GPT-2 transformer model with a language modelling and a multiple-choice classification head on top e.g. for RocStories/SWAG tasks.</p>

<p>Does this mean that we can use the <strong>GPT2DoubleHeadsModel</strong> to process both non-multiple-choice-based language modelling tasks (i.e. next word prediction) as well as the multiple-choice questions, without making any adjustment to its head? Or would I need to adjust the head of the <strong>GPT2DoubleHeadsModel</strong> if I want to do the non-multiple-choice-based next word predictions because the <strong>GPT2DoubleHeadsModel</strong> is for answering multiple-choice type questions only?</p>

<p>I am a bit confused by this because the impression that I got from reading your GPT-2 paper is that GPT-2 uses language modelling process to process every type of language task (therefore GPT-2 would only have the regular language modelling head at the top), yet the name ""<strong>GPT2DoubleHeadsModel</strong>"" seem to suggest that I need to adjust the head of this GPT-2 for different types of language tasks.</p>

<p>Thank you,</p>
","12205961","","1243762","","2020-11-29 12:06:50","2020-11-29 12:06:50","Can the HuggingFace GPT2DoubleHeadsModel be used for non-multiple-choice next token prediction?","<nlp><huggingface-transformers><transformer><gpt-2>","0","0","1","","","CC BY-SA 4.0"
"69050690","1","","","2021-09-03 21:25:16","","0","41","<p>I am trying to fine-tune a transformers tapas model for QA using codes <a href=""https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb#scrollTo=t5iU5byAICWb"" rel=""nofollow noreferrer"">here</a> and description <a href=""https://huggingface.co/transformers/model_doc/tapas.html#tapasforsequenceclassification"" rel=""nofollow noreferrer"">here</a>. I have a table with questions:</p>
<p><a href=""https://i.stack.imgur.com/EwzPH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EwzPH.png"" alt=""enter image description here"" /></a></p>
<p>and some tables with values:</p>
<p><a href=""https://i.stack.imgur.com/hZo3D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hZo3D.png"" alt=""enter image description here"" /></a></p>
<p>After creating data file, when i try to encode an item with the following code:</p>
<pre><code>encoding = tokenizer(table=table, queries=item.question, answer_coordinates=item.answer_coordinates, answer_text=item.answer_text,
                         truncation=True, padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;)
    encoding.keys()
</code></pre>
<p>I receive an error:</p>
<pre><code>    ValueError                                Traceback (most recent call last)
&lt;ipython-input-30-fe3f86cf3091&gt; in &lt;module&gt;()
      8 
      9 encoding = tokenizer(table=table, queries=item.question, answer_coordinates=item.answer_coordinates, answer_text=item.answer_text,
---&gt; 10                      truncation=True, padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;)
     11 encoding.keys()

5 frames
/usr/local/lib/python3.7/dist-packages/transformers/models/tapas/tokenization_tapas.py in _get_answer_ids(self, column_ids, row_ids, answer_coordinates)
   1764 
   1765         if missing_count:
-&gt; 1766             raise ValueError(&quot;Couldn't find all answers&quot;)
   1767         return answer_ids
   1768 

ValueError: Couldn't find all answers
</code></pre>
<p>Is there anything wrong with my tables?</p>
","10550623","","10550623","","2021-09-04 07:50:23","2021-09-04 07:50:23","Transformers Tapas: ValueError: Couldn't find all answers","<encoding><huggingface-transformers><huggingface-tokenizers>","0","3","","","","CC BY-SA 4.0"
"65668828","1","","","2021-01-11 14:34:22","","0","126","<p>First of all, I'm rather new to TensorFlow so probably I doing something wrong. I'm trying to create a standalone TensorFlow model for binary text classification. I want to create a SavedModel that can be loaded via <a href=""https://www.tensorflow.org/tfx/guide/serving"" rel=""nofollow noreferrer"">TensorFlow Serving</a> or <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html"" rel=""nofollow noreferrer"">AWS SageMaker</a>. By standalone I mean I would like it to be end-to-end: feed the text to an endpoint, get a floating number as a response.</p>
<p>I've started with the <a href=""https://www.tensorflow.org/tutorials/text/classify_text_with_bert"" rel=""nofollow noreferrer"">Classify text with BERT notebook</a> locally on my machine with <code>electra_small</code> model as a base model for fine-tuning. I've saved fine-tuned model locally, loaded it back and made sure it works. I turned my PC off and the next day I found out I can't load my model anymore. Turned out it was looking for a temporary tfhub files which were no longer there causing <code>/tmp/tfhub_modules/09bd4e665682e6f03bc72fbcff7a68bf6879910e/assets/vocab.txt; No such file or directory</code> error. I've re-downloaded these models from TF hub and model started loading and working just fine. I removed <code>/tmp/tfhub_modules/</code> directory and model stopped loading again. I've checked the directory I've saved my fine-tuned model into and it had the very same <code>vocab.txt</code> file with the same content as in TF Hub temp file, but refused to use it for some reason. I expected SavedModel to be self-contained and use its own local vocbulary file rather than hard-referencing a vocab file from external temporary dir of TF Hub.</p>
<p>Most of the tutorials I can find online are doing <code>word_ids + masks -&gt; model</code> kind of classification, making a <code>text -&gt; word_ids + masks</code> a part of a preprocessing step. I don't really like this idea as I would like to have API consumers completely oblivious to any sort of NLP-related stuff. I would like to keep it &quot;texts in, probabilities out&quot;. I would rather not write a wrapper for TensorFlow Serving API to handle the preprocessing if it is possible. I hoped to find and modify the source code for <a href=""https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2"" rel=""nofollow noreferrer"">BERT EN uncased preprocess</a> but couldn't. I.e. I can't find a definition of <a href=""https://github.com/search?q=org%3Atensorflow%20bert_pack_inputs&amp;type=Code"" rel=""nofollow noreferrer"">bert_pack_inputs</a>, a somewhat unique function name, on TensorFlow's GitHub. <a href=""https://github.com/tensorflow/tensorflow/issues/46293"" rel=""nofollow noreferrer"">My issue</a>.</p>
<p>Code example:</p>
<pre><code>import tensorflow_hub as hub
import tensorflow_text
import tensorflow as tf


def build_classifier_model():
    tfhub_handle_encoder = 'https://tfhub.dev/google/electra_small/2'
    tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2'

    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
    return tf.keras.Model(text_input, net)


model_save_path = './modelname'

# Run this one first, it will create a saved model in your working dir.

# if __name__ == '__main__':
#     classifier_model = build_classifier_model()
#     bert_raw_result = classifier_model(tf.constant(['some dummy text']))
#     print(bert_raw_result)
#     classifier_model.save(model_save_path, include_optimizer=False)


# Run this one next, this should work.

if __name__ == '__main__':
    classifier_model = tf.saved_model.load(model_save_path)
    bert_raw_result = classifier_model(tf.constant(['some dummy text']))
    print(bert_raw_result)

# Now, try to remove / rename `/tmp/tfhub_modules/` directory and run the code above again. It fails for me with exception I provided in OP.
# So there's a hard reference to `/tmp/tfhub_modules/09bd4e665682e6f03bc72fbcff7a68bf6879910e/assets/vocab.txt` file instead of the one in &quot;modelname&quot; saved model.
</code></pre>
","2782477","","2782477","","2021-01-11 19:10:51","2021-01-11 19:10:51","Fine-tuned BERT SavedModel hard-referencing vocab.txt from TF Hub temporary files","<tensorflow><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","1","","","","CC BY-SA 4.0"
"63774619","1","63777582","","2020-09-07 09:12:43","","0","666","<p>I am using a pretrained MarianMT machine translation model from <a href=""https://huggingface.co/Helsinki-NLP/opus-mt-en-de"" rel=""nofollow noreferrer"">English to German</a>. I also have a large set of high quality English-to-German sentence pairs that I would like to use to enhance the performance of the model, which is trained on the OPUS corpus, but <strong>without</strong> making the model <em>forget</em> the OPUS training data. Is there a way to do that? Thanks.</p>
","6570794","","","","","2021-04-16 11:33:17","Enhance a MarianMT pretrained model from HuggingFace with more training data","<python><huggingface-transformers>","2","1","","","","CC BY-SA 4.0"
"65683013","1","65685178","","2021-01-12 11:27:42","","0","1242","<p>I am trying to fine tune a Roberta model after adding some special tokens to its tokenizer:</p>
<pre><code>    special_tokens_dict = {'additional_special_tokens': ['[Tok1]','[Tok2]']}

    tokenizer.add_special_tokens(special_tokens_dict)
</code></pre>
<p>I get this error when i try to train the model (on cpu):</p>
<pre><code>IndexError                                Traceback (most recent call last)
&lt;ipython-input-75-d63f8d3c6c67&gt; in &lt;module&gt;()
     50         l = model(b_input_ids, 
     51                      attention_mask=b_input_mask,
---&gt; 52                     labels=b_labels)
     53         loss,logits = l
     54         total_train_loss += l[0].item()

8 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1850         # remove once script supports set_grad_enabled
   1851         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1852     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1853 
   1854 

IndexError: index out of range in self
</code></pre>
<p>p.s. If I comment <code>add_special_tokens</code> the code works.</p>
","10550623","","","","","2021-01-12 13:44:16","IndexError: index out of range in self while try to fine tune Roberta model after adding special tokens","<bert-language-model><huggingface-transformers><roberta-language-model>","1","0","","","","CC BY-SA 4.0"
"65279115","1","65875359","","2020-12-13 18:23:01","","8","9799","<p>I am trying to train a pretrained roberta model using <em>3</em> inputs, <em>3</em> input_masks and a label as tensors of my training dataset.</p>
<p>I do this using the following code:</p>
<pre><code>from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
batch_size = 32
# Create the DataLoader for our training set.
train_data = TensorDataset(train_AT, train_BT, train_CT, train_maskAT, train_maskBT, train_maskCT, labels_trainT)
train_dataloader = DataLoader(train_data, batch_size=batch_size)

# Create the Dataloader for our validation set.
validation_data = TensorDataset(val_AT, val_BT, val_CT, val_maskAT, val_maskBT, val_maskCT, labels_valT)
val_dataloader = DataLoader(validation_data, batch_size=batch_size)

# Pytorch Training
training_args = TrainingArguments(
    output_dir='C:/Users/samvd/Documents/Master/AppliedMachineLearning/FinalProject/results',          # output directory
    num_train_epochs=1,              # total # of training epochs
    per_device_train_batch_size=32,  # batch size per device during training
    per_device_eval_batch_size=32,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='C:/Users/samvd/Documents/Master/AppliedMachineLearning/FinalProject/logs',            # directory for storing logs
)

trainer = Trainer(
    model=model,                          # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                   # training arguments, defined above
    train_dataset = train_data,           # training dataset
    eval_dataset = validation_data,       # evaluation dataset
)

trainer.train()
</code></pre>
<p>However this gives me the following error:</p>
<blockquote>
<p>TypeError: vars() argument must have <strong>dict</strong> attribute</p>
</blockquote>
<p>Now I have found out that it is probably because I don't use <code>collate_fn</code> when using <code>DataLoader</code>, but I can't really find a source that helps me define this correctly so the trainer understands the different tensors I put in.</p>
<p>Can anyone point me in the right direction?</p>
","14320652","","6331369","","2020-12-13 18:34:31","2021-01-24 20:07:03","How to use 'collate_fn' with dataloaders?","<python><pytorch><huggingface-transformers><dataloader>","1","2","4","","","CC BY-SA 4.0"
"63720207","1","","","2020-09-03 08:57:20","","1","118","<p>My Input tensor Looks like :</p>
<pre><code>torch.Size([8, 23])

// where,
// 8 -&gt; batch size
// 23 -&gt; words in each of them
</code></pre>
<p>My output tensor Looks like :</p>
<pre><code>torch.Size([8, 23, 103])

// where,
// 8 -&gt; batch size
// 23 -&gt; words predictions
// 103 -&gt; vocab size.
</code></pre>
<p>I want to calculate sparse cross Entropy Loss for this task, but I canâ€™t since PyTorch only calculates the loss single element. How can I code it to work? Thanks for your help.</p>
","12836746","","12444527","","2020-09-03 09:32:55","2020-09-03 09:32:55","Sparse Cross Entropy Loss for calculating Loss in NLP problems. PyTorch","<nlp><pytorch><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"65676389","1","65680781","","2021-01-12 00:24:13","","3","582","<p>TL;DR:
My model always predicts the same labels and I don't know why. Below is my entire code for fine-tuning in the hopes that someone can point out to me where I am going wrong.</p>
<p>I am using Huggingface's TFBertForSequenceClassification for sequence classification task to predict 4 labels of sentences in German text.</p>
<p>I use the bert-base-german-cased model since I don't use only lower case text (since German is more case sensitive than English).</p>
<p>I get my input from a csv file that I construct from an annotated corpus I received. Here's a sample of that:</p>
<pre><code>0       Hier kommen wir ins Spiel Die App Cognitive At...
1       Doch wenn Athlet Lebron James jede einzelne Mu...
2       Wie kann ein Gehirn auf Hochleistung getrimmt ...
3       Wie schafft es Warren Buffett knapp 1000 WÃ¶rte...
4       Entfalte dein mentales Potenzial und werde ein...
Name: sentence_clean, Length: 3094, dtype: object
</code></pre>
<p>And those are my labels, from the same csv file:</p>
<pre><code>0       e_1
1       e_4
2       e_4
3       e_4
4       e_4
</code></pre>
<p>The distinct labels are: e_1, e_2, e_3, and e_4</p>
<p>This is the code I am using to fine tune my model:</p>
<pre><code>import pandas as pd
import numpy as np
import os
    
# read in data
# sentences_df = pd.read_csv('path/file.csv')


X = sentences_df.sentence_clean
Y = sentences_df.classId

# =============================================================================
# One hot encode labels
# =============================================================================

# integer encode labels
from numpy import array
from numpy import argmax
from sklearn.preprocessing import LabelEncoder


label_encoder = LabelEncoder()
Y_integer_encoded = label_encoder.fit_transform(list(Y))


# one hot encode labels
from sklearn.preprocessing import OneHotEncoder

onehot_encoder = OneHotEncoder(sparse=False)
Y_integer_encoded_reshaped = Y_integer_encoded.reshape(len(Y_integer_encoded), 1)
Y_one_hot_encoded = onehot_encoder.fit_transform(Y_integer_encoded_reshaped)

# train test split
from sklearn.model_selection import train_test_split


X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, Y_one_hot_encoded, test_size=0.20, random_state=42)


# =============================================================================
# Perpare datasets for finetuning
# =============================================================================
import tensorflow as tf
physical_devices = tf.config.list_physical_devices('GPU') 
tf.config.experimental.set_memory_growth(physical_devices[0], True)

from transformers import BertTokenizer, TFBertForSequenceClassification


tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased') # initialize tokenizer


# tokenize trai and test sets
max_seq_length = 128

X_train_tokens = tokenizer(list(X_train_raw),
                            truncation=True,
                            padding=True)

X_test_tokens = tokenizer(list(X_test_raw),
                            truncation=True,
                            padding=True)


# create TF datasets as input for BERT model
bert_train_ds = tf.data.Dataset.from_tensor_slices((
    dict(X_train_tokens),
    y_train
))

bert_test_ds = tf.data.Dataset.from_tensor_slices((
    dict(X_test_tokens),
    y_test
))

# =============================================================================
# setup model and finetune
# =============================================================================

# define hyperparams
num_labels = 4
learninge_rate = 2e-5
epochs = 3
batch_size = 16

# create BERT model
bert_categorical_partial = TFBertForSequenceClassification.from_pretrained('bert-base-german-cased', num_labels=num_labels)

optimizer = tf.keras.optimizers.Adam(learning_rate=learninge_rate)
bert_categorical_partial.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

history = bert_categorical_partial.fit(bert_train_ds.shuffle(100).batch(batch_size),
          epochs=epochs,
          # batch_size=batch_size,
          validation_data=bert_test_ds.shuffle(100).batch(batch_size))
</code></pre>
<p>And here is the output from fine-tuning:</p>
<pre><code>Epoch 1/3
155/155 [==============================] - 31s 198ms/step - loss: 8.3038 - accuracy: 0.2990 - val_loss: 8.7751 - val_accuracy: 0.2811
Epoch 2/3
155/155 [==============================] - 30s 196ms/step - loss: 8.2451 - accuracy: 0.2913 - val_loss: 8.9314 - val_accuracy: 0.2779
Epoch 3/3
155/155 [==============================] - 30s 196ms/step - loss: 8.3101 - accuracy: 0.2913 - val_loss: 9.0355 - val_accuracy: 0.2746
</code></pre>
<p>Lastly, I try to predict the labels of the test set and validate the results with a confusion matrix:</p>
<pre><code>X_test_tokens_new = {'input_ids': np.asarray(X_test_tokens['input_ids']),
                     'token_type_ids': np.asarray(X_test_tokens['token_type_ids']),
                     'attention_mask': np.asarray(X_test_tokens['attention_mask']),
                     }

pred_raw = bert_categorical_partial.predict(X_test_tokens_new)
pred_proba = tf.nn.softmax(pred_raw).numpy()
pred = pred_proba[0].argmax(axis = 1)
y_true = y_test.argmax(axis = 1)

cm = confusion_matrix(y_true, pred)
</code></pre>
<p>Output of print(cm):</p>
<pre><code>array([[  0,   0,   0,  41],
       [  2,   0,   0, 253],
       [  2,   0,   0, 219],
       [  6,   0,   0,  96]], dtype=int64)
</code></pre>
<p>As you can see, my accuracy is really bad, and when I look at the cm, I can see that my model pretty much just predicts one single label.
I've tried everything and ran the model multiple times, but I always get the same results.
I do know that the data I am working with isn't great and I am only training on abour 2k sentences with labels. But I have a feeling the accuracy should still be higher and, more importantly, the model shouldn't just predict one single label 98% of the time, right?</p>
<p>I posted everything I am using to run the model in the hopes someone can point me to where I am going wrong.
Thank very much in advance for your help!</p>
","8291269","","8291269","","2021-01-12 07:53:24","2021-01-12 09:01:25","Huggingface TFBertForSequenceClassification always predicts the same label","<python><tensorflow><bert-language-model><huggingface-transformers>","1","8","","","","CC BY-SA 4.0"
"67343478","1","","","2021-05-01 06:28:28","","2","58","<ul>
<li>I wanted to use the  <a href=""https://huggingface.co/ncoop57/multilingual-codesearch"" rel=""nofollow noreferrer"">multilingual-codesearch</a> model but first the code doesn't work and outputs the following error which suggest that it cannot load with only weights:</li>
</ul>
<pre><code>    from transformers import AutoTokenizer, AutoModel
      
    tokenizer = AutoTokenizer.from_pretrained(&quot;ncoop57/multilingual-codesearch&quot;)
    
    model = AutoModel.from_pretrained(&quot;ncoop57/multilingual-codesearch&quot;)
</code></pre>
<pre><code>ValueError: Unrecognized model in ncoop57/multilingual-codesearch. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: gpt_neo, big_bird, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas
</code></pre>
<ul>
<li>Then I downloaded the pytorch bin file but it only contains the weight dictionnary (state dictionnary as mentioned <a href=""https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html"" rel=""nofollow noreferrer"">here</a>), which means that if I want to use the model I have to initialize the good architecture and then load the weights.</li>
</ul>
<p>But how am I supposed to find the architecture fitting the weight of a model that complex ? I saw that some method could find back the model based on the weight dictionnary but I didn't manage to make them work (I think about <a href=""https://huggingface.co/transformers/model_doc/auto.html"" rel=""nofollow noreferrer"">enter link description here</a>).</p>
<p>How can one find back the architecture of a weight dictionnary in order to make the model work ? Is it even possible ?</p>
","10835758","","11924635","","2021-05-01 08:44:27","2021-05-01 08:44:27","How to find back the architecture of a pytorch model having only the weight dictionnary?","<pytorch><huggingface-transformers><huggingface-tokenizers><state-dict>","0","1","","","","CC BY-SA 4.0"
"65646925","1","65760008","","2021-01-09 19:46:24","","1","2497","<p>Iâ€™m trying to train BERT model from scratch using my own dataset using HuggingFace library. I would like to train the model in a way that it has the exact architecture of the original BERT model.</p>
<p>In the original paper, it stated that: <em>â€œBERT is trained on two tasks: predicting randomly masked tokens (MLM) and predicting whether two sentences follow each other (NSP). SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text.â€</em></p>
<p>Iâ€™m trying to understand how to train the model on two tasks as above. At the moment, I initialised the model as below:</p>
<pre><code>from transformers import BertForMaskedLM
model = BertForMaskedLM(config=config)
</code></pre>
<p>However, it would just be for MLM and not NSP. How can I initialize and train the model with NSP as well or maybe my original approach was fine as it is?</p>
<p>My assumptions would be either</p>
<ol>
<li><p>Initialize with <code>BertForPreTraining</code> (for both MLM and NSP), OR</p>
</li>
<li><p>After finish training with <code>BertForMaskedLM</code>,
initalize the same model and train again with
<code>BertForNextSentencePrediction</code> (but this approachâ€™s computation and
resources would cost twiceâ€¦)</p>
</li>
</ol>
<p>Iâ€™m not sure which one is the correct way. Any insights or advice would be greatly appreciated.</p>
","13747728","","13747728","","2021-01-10 08:31:04","2021-06-01 14:42:52","How to train BERT from scratch on a new domain for both MLM and NSP?","<deep-learning><nlp><bert-language-model><huggingface-transformers><transformer>","2","0","1","","","CC BY-SA 4.0"
"62830062","1","","","2020-07-10 08:18:34","","3","479","<p>I found two fine tune ways for Sequence Classification  with transformer:</p>
<p>1,BertForSequenceClassification.from_pretrained(<a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/training.html</a>):</p>
<pre><code>from transformers import BertForSequenceClassification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
model.train()
</code></pre>
<p>2,python run_glue.py(<a href=""https://github.com/huggingface/transformers/tree/master/examples/text-classification"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/text-classification</a>):</p>
<pre><code>export GLUE_DIR=/path/to/glue
export TASK_NAME=MRPC

python run_glue.py \
  --model_name_or_path bert-base-cased \
  --task_name $TASK_NAME \
  --do_train \
  --do_eval \
  --data_dir $GLUE_DIR/$TASK_NAME \
  --max_seq_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3.0 \
  --output_dir /tmp/$TASK_NAME/
</code></pre>
<p>Are there any difference between them?</p>
","7241796","","","","","2020-07-10 08:18:34","what's the difference between BertForSequenceClassification.from_pretrained and python run_glue.py?","<huggingface-transformers>","0","1","1","","","CC BY-SA 4.0"
"64782045","1","","","2020-11-11 07:16:18","","1","224","<p>I am trying to do bert based NER and trying to import,</p>
<pre><code>from simpletransformers.ner import NERModel, NERArgs
</code></pre>
<p>I have installed simpletranformers in pip and when I try to import,</p>
<pre><code>import simpletransformers.ner
</code></pre>
<p>it says, <code>ImportError: cannot import name 'BertweetTokenizer'</code>. When I try to install BertweetTokenizer, it throws</p>
<pre><code>ERROR: No matching distribution found for BertweetTokenizer
</code></pre>
<p>Not sure, what I am missing. Kindly help</p>
","7216834","","7226080","","2020-11-12 14:58:33","2020-11-12 14:58:33","Not able to import simpletransformers.ner in python as it says ImportError: cannot import name 'BertweetTokenizer'","<bert-language-model><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"69023072","1","","","2021-09-02 01:36:40","","0","17","<p>I need to build a model that can expand multiple short sentences to multiple long sentences. I was thinking to use a pre-trained Transformer model to do this just like when we want to do a paragraph or text summarization except, in this case, I switched the output and input values. I tried this using <code>t5-base</code>, ran it on Google Colab, and using really minimum data like 10 rows of data, the idea was to see whether it works or not regardless of the output. But I always got errors like below:</p>
<blockquote>
<p>RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 0;
11.17 GiB total capacity; 10.29 GiB already allocated; 237.81 MiB free; 10.49 GiB reserved in total by PyTorch)</p>
</blockquote>
<p>I interpret this error as I did something wrong or my idea did not work. Is there anyone who can suggest how to do this?</p>
<p>Please advise</p>
","4431554","","4431554","","2021-09-02 02:37:57","2021-09-02 06:03:40","Using Pre-trained Transformer Model to Expand Short Sentences to Long Sentences","<python><nlp><huggingface-transformers><pre-trained-model>","1","2","","","","CC BY-SA 4.0"
"65327781","1","","","2020-12-16 17:00:12","","0","120","<p>I fine-tuned the <code>jplu/tf-xlm-roberta-base</code> from HuggingFace on emotion classification. I am trying to save the model with the save_pretrained function but I am getting this error:</p>
<pre><code>AttributeError                            Traceback (most recent call last)

&lt;ipython-input-29-4f61a9338d09&gt; in &lt;module&gt;()
      1 
      2 
----&gt; 3 model9.save_pretrained(&quot;/content/drive/MyDrive/XLM-R@SemEval&quot;)

AttributeError: 'Functional' object has no attribute 'save_pretrained'
</code></pre>
","14838725","","675721","","2020-12-16 17:47:36","2020-12-16 17:47:36","How can I save my fine-tuned functional XLM-R model?","<tensorflow><huggingface-transformers><transformer>","0","1","","","","CC BY-SA 4.0"
"65852264","1","66360105","","2021-01-22 20:22:11","","1","125","<p>Following  <a href=""https://medium.com/swlh/using-xlnet-for-sentiment-classification-cfa948e65e85"" rel=""nofollow noreferrer"">this link</a>, I am trying to use my own data to do sentiment analysis. But I get this error:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;

&lt;ipython-input-41-5f2f35b7976e&gt; in train_epoch(model, data_loader, optimizer, device, scheduler, n_examples)
      7 
      8     for d in data_loader:
----&gt; 9         input_ids = d[&quot;input_ids&quot;].reshape(4,64).to(device)
     10         attention_mask = d[&quot;attention_mask&quot;].to(device)
     11         targets = d[&quot;targets&quot;].to(device)

RuntimeError: shape '[4, 64]' is invalid for input of size 64
</code></pre>
<p>When I try to run this code</p>
<pre><code>history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):
    print(f'Epoch {epoch + 1}/{EPOCHS}')
    print('-' * 10)

    train_acc, train_loss = train_epoch(
        model,
        train_data_loader,     
        optimizer, 
        device, 
        scheduler, 
        len(df_train)
    )

    print(f'Train loss {train_loss} Train accuracy {train_acc}')

    val_acc, val_loss = eval_model(
        model,
        val_data_loader, 
        device, 
        len(df_val)
    )

    print(f'Val loss {val_loss} Val accuracy {val_acc}')
    print()

    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['val_acc'].append(val_acc)
    history['val_loss'].append(val_loss)
</code></pre>
<p>I know this error has something to do with the shape of my data but I am not sure how to find the correct <code>reshape</code> parameters in order to make this work.</p>
","12090782","","6664872","","2021-01-29 23:05:20","2021-02-24 23:07:08","Using XLNet for sentiment analysis - setting the correct reshape parameters","<python><machine-learning><nlp><sentiment-analysis><huggingface-transformers>","2","3","","","","CC BY-SA 4.0"
"63743557","1","","","2020-09-04 14:56:10","","5","187","<p>I'm doing a finetuning of a <a href=""https://arxiv.org/abs/2004.05150"" rel=""nofollow noreferrer"">Longformer</a> on a <strong>document text binary classification task</strong> using <a href=""https://huggingface.co/transformers/main_classes/trainer.html"" rel=""nofollow noreferrer"">Huggingface Trainer class</a> and I'm monitoring the measures of some checkpoints with Tensorboard.</p>
<p>Even if the F1 score and accuracy is quite high, I have perplexities about the <strong>fluctuations of training loss</strong>.</p>
<p>I read online a reason for that can be:</p>
<ul>
<li>the <em>too high learning rate</em>, but I tried with 3 values (1e-4, 1e-5 and 1e-6) and all of them made the same effect</li>
<li>a <em>small batch size</em>. I'm using a <strong>Sagemaker notebook p2.8xlarge</strong> which has 8xK80 GPUs. The batch size per GPU I can use to avoid the <strong>CUDA out of memory error</strong> is 1. So the total batch size is <strong>8</strong>. My intuition is that a bs of 8 is too small for a dataset containing 57K examples (7K steps per epoch). Unfortunately it's the highest value I can use.</li>
</ul>
<p>Here I have reported the trend of F1, accuracy, loss and smoothed loss. The grey line is with 1e-6 of learning rate while the pink one is 1e-5.</p>
<img src=""https://i.stack.imgur.com/KaM9D.png"" width=""400"" />
<img src=""https://i.stack.imgur.com/aUCtr.png"" width=""400"" />
<img src=""https://i.stack.imgur.com/u91dT.png"" width=""400"" />
<img src=""https://i.stack.imgur.com/B0IMj.png"" width=""400"" />
<p>I reasume all the <strong>info</strong> of my training:</p>
<ul>
<li><strong>batch size</strong>: 1 x 8GPU = <strong>8</strong></li>
<li><strong>learning rate</strong>: <strong>1e-4</strong>, <strong>1e-5</strong>, <strong>1e-6</strong> (all of them tested without improvement on loss)</li>
<li><strong>model</strong>: <strong>Longformer</strong></li>
<li><strong>dataset</strong>:
<ul>
<li><em>training set</em>: <strong>57K examples</strong></li>
<li><em>dev set</em>: <strong>12K examples</strong></li>
<li><em>test set</em>: <strong>12K examples</strong></li>
</ul>
</li>
</ul>
<p>Which could be the reason? Can this be considered a problem despite the quite good F1 and accuracy results?</p>
","12569908","","12569908","","2020-09-05 09:31:45","2021-04-10 12:54:59","Fluctuating loss during training for text binary classification","<python><machine-learning><pytorch><huggingface-transformers><allennlp>","1","5","1","","","CC BY-SA 4.0"
"65674086","1","65690135","","2021-01-11 20:30:53","","2","74","<p>I have a script that relies almost entirely on SpaCy for a series of nlp tasks.
Since SpaCy only supports 3 english models by default (sm, md, lg), i would like to replace them with an external model such that i can vectorize my text and perform all the SpaCy methods i currently do in my pipeline.</p>
<p>Is it possible to replace the <code>nlp = spacy.load('en_core_web_lg')</code>
line with something else without affecting the rest of my pipeline? For example by defining 'nlp' with one of the language models supported in the transformers library?</p>
<p>For example, i use SpaCy's <code>(a).similarity(b)</code> method, and would like to retain the pipeline which includes this, except have the calculations based on the word vectors generated by a non-default language model.</p>
","14138364","","6573902","","2021-01-12 18:30:23","2021-01-12 18:48:12","is it possible to use an external vectorizer in a standard SpaCy pipeline?","<python><nlp><spacy><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67367757","1","67598571","","2021-05-03 10:55:46","","0","101","<p>I am trying to load a custom dataset that I will then use for language modeling. The dataset consists of a text file that has a whole document in each line, meaning that each line overpasses the normal 512 tokens limit of most tokenizers.</p>
<p>I would like to understand what is the process to build a text dataset that tokenizes each line, having previously split the documents in the dataset into lines of a &quot;tokenizable&quot; size, as the old <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/data/datasets/language_modeling.py"" rel=""nofollow noreferrer"">TextDataset</a> class would do, where you only had to do the following, and a tokenized dataset without text loss would be available to pass to a DataCollator:</p>
<pre><code>model_checkpoint = 'distilbert-base-uncased'

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

from transformers import TextDataset

dataset = TextDataset(
    tokenizer=tokenizer,
    file_path=&quot;path/to/text_file.txt&quot;,
    block_size=512,
)
</code></pre>
<p>Instead of this way, which is to be deprecated soon, I would like to use the <a href=""https://huggingface.co/docs/datasets/"" rel=""nofollow noreferrer"">datasets</a> library. For now, what I have is the following, which, of course, throws an error because each line is longer than the maximum block size in the tokenizer:</p>
<pre><code>import datasets
dataset = datasets.load_dataset('path/to/text_file.txt')

model_checkpoint = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;])

tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[&quot;text&quot;])
</code></pre>
<p>So what would be the &quot;standard&quot; way of creating a dataset in the way it was done before but with the datasets lib?</p>
<p>Thank you very much for the help :))</p>
","8979274","","","","","2021-05-19 07:18:14","How to build a dataset for language modeling with the datasets library as with the old TextDataset from the transformers library","<python><bert-language-model><huggingface-transformers>","1","4","","","","CC BY-SA 4.0"
"65318981","1","","","2020-12-16 07:34:44","","0","205","<p>Facing this error when I am trying to import <code>simpletransformers</code>.</p>
<pre><code>from simpletransformers.classification import ClassificationModel, ClassificationArgs
</code></pre>
<p>Error:</p>
<pre><code> cannot import name 'Unigram' from 'tokenizers.models' (/opt/conda/lib/python3.7/site-packages/tokenizers/models/__init__.py)
</code></pre>
","6315469","","6573902","","2020-12-16 07:37:01","2020-12-16 07:37:01","Importing Simple Transformer","<python><python-3.x><nlp><huggingface-transformers><huggingface-tokenizers>","0","2","","","","CC BY-SA 4.0"
"67379533","1","","","2021-05-04 05:59:46","","2","203","<p>I am using the GPT2 pre-trained model for a research project and when I load the pre-trained model with the following code,</p>
<pre><code>from transformers.models.gpt2.modeling_gpt2 import GPT2Model
gpt2 = GPT2Model.from_pretrained('gpt2')
</code></pre>
<p>I get the following warning message:</p>
<blockquote>
<p>Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</p>
</blockquote>
<p>From my understanding, it says that the weights of the above layers are not initialized from the pre-trained model. But we all know that attention layers ('attn') are so important in GPT2 and if we can not have their actual weights from the pre-trained model, then what is the point of using a pre-trained model?</p>
<p>I really appreciate it if someone could explain this to me and tell me how I can fix this.</p>
","10575373","","","","","2021-05-12 21:43:34","Why some weights of GPT2Model are not initialized?","<pytorch><huggingface-transformers><gpt-2>","1","2","","","","CC BY-SA 4.0"
"67381993","1","","","2021-05-04 09:19:36","","0","26","<p>I am trying to generate a long sequence of text using PyTorch-Transformers from a sample text. I am following <a href=""https://www.analyticsvidhya.com/blog/2019/07/pytorch-transformers-nlp-python/"" rel=""nofollow noreferrer"">this tutorial</a> for this purpose. Because the original article only predicts one word from a given text, I modified that script to generate long sequence instead of one. This is the modified part of the code</p>
<pre><code># Encode a text inputs
text = &quot;&quot;&quot;An examination can be defined as a detailed inspection or analysis
 of an object or person. For example, an engineer will examine a structure,
  like a bridge, to see if it is safe. A doctor may conduct&quot;&quot;&quot;

indexed_tokens = tokenizer.encode(text)

# Convert indexed tokens in a PyTorch tensor
tokens_tensor = torch.tensor([indexed_tokens])
seq_len = tokens_tensor.shape[1]
tokens_tensor = tokens_tensor.to('cuda')


with torch.no_grad():
    for i in range(50):
        outputs = model(tokens_tensor[:,-seq_len:])
        predictions = outputs[0]
        predicted_index = torch.argmax(predictions[0, -1, :])
        tokens_tensor = torch.cat((tokens_tensor,predicted_index.reshape(1,1)),1)


pred = tokens_tensor.detach().cpu().numpy().tolist()
predicted_text = tokenizer.decode(pred[0])
print(predicted_text)
</code></pre>
<p>Output</p>
<blockquote>
<p>An examination can be defined as a detailed inspection or analysis
of an object or person. For example, an engineer will examine a
structure,   like a bridge, to see if it is safe. A doctor may conduct
an examination of a patient's body to see if it is safe.</p>
<p>The doctor may also examine a patient's body to see if it is safe. A
doctor may conduct an examination of a patient's body to see if it is
safe.</p>
</blockquote>
<p>As you can see the generated text does not generates any unique text sequence but it generates the same sentence over and over again with minor changes.</p>
<p>How should we create long sequence using  PyTorch-Transformers?</p>
","996366","","","","","2021-05-04 14:17:47","What is the right way to generate long sequence using PyTorch-Transformers?","<deep-learning><nlp><pytorch><huggingface-transformers><transformer>","1","1","","","","CC BY-SA 4.0"
"67344784","1","","","2021-05-01 09:38:15","","0","16","<p>I wanna get the query and key from the code below. And this part is from <a href=""https://huggingface.co/transformers/_modules/transformers/models/roberta/modeling_roberta.html#RobertaModel"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/_modules/transformers/models/roberta/modeling_roberta.html#RobertaModel</a>
At the end, I wanna reproduce the figure 1 in this <a href=""https://arxiv.org/pdf/2006.04768.pdf"" rel=""nofollow noreferrer"">paper</a>
Could someone give a intuitive instruction about how they would do it?</p>
<pre class=""lang-py prettyprint-override""><code>class RobertaSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, &quot;embedding_size&quot;):
            raise ValueError(
                f&quot;The hidden size ({config.hidden_size}) is not a multiple of the number of attention &quot;
                f&quot;heads ({config.num_attention_heads})&quot;
            )

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        self.position_embedding_type = getattr(config, &quot;position_embedding_type&quot;, &quot;absolute&quot;)
        if self.position_embedding_type == &quot;relative_key&quot; or self.position_embedding_type == &quot;relative_key_query&quot;:
            self.max_position_embeddings = config.max_position_embeddings
            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)

        self.is_decoder = config.is_decoder
</code></pre>
","14692517","","3795691","","2021-05-02 07:38:46","2021-05-02 07:38:46","how to use souce code class RobertaSelfAttention(nn.Module):","<nlp><huggingface-transformers><roberta-language-model>","0","0","","","","CC BY-SA 4.0"
"67350914","1","","","2021-05-01 21:14:38","","0","23","<p>I want to fine tuning bert multilingual model on Bangla language. Can anyone give me some easy Sources like colab notebook, or tutorial for the fine tuning of multilingual model?</p>
","7088146","","","","","2021-05-01 21:14:38","How to fine tuning a bert language model for Bangla","<bert-language-model><huggingface-transformers><language-model>","0","0","","","","CC BY-SA 4.0"
"67398452","1","","","2021-05-05 09:18:55","","0","58","<p>I'm a total newbie so please excuse if my questions are super stupid.</p>
<p>I want to use this BERT model (<a href=""https://github.com/NPoe/ebert"" rel=""nofollow noreferrer"">https://github.com/NPoe/ebert</a>) and as a NLP task I would like to just do Masked Language Modeling, no extra downstream NLP task like classification or QA.</p>
<p>I found the hugging face transformers library and thought maybe I could just use the pipeline (nlp=pipeline(&quot;fill-mask&quot;, &quot;my_model&quot;)) to get the masked language modeling in the Fine-tuning.</p>
<p>My question is: Is this idea nonsense or in what other way could I get the BERT Model to do the MLM in the FineTuning/as the downstream task?</p>
","15840994","","","","","2021-05-05 09:18:55","How to use BERT model for MLM in finetuning phase","<bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"65872566","1","65943903","","2021-01-24 15:40:42","","2","311","<p>I'm trying to train <code>TFBertForNextSentencePrediction</code> on my own corpus, not from scratch, but rather taking the existing bert model with only a next sentence prediction head and further train it on a specific cuprous of text (pairs of sentences). Then I want to use the model I trained to be able to extract sentence embeddings from the last hidden state for other texts.</p>
<p>Currently the problem I encounter is that after I train the <code>keras</code> model I am not able to extract the hidden states of the last layer before the next sentence prediction head.</p>
<p>Below is the code. Here I only train it on a few sentences just to make sure the code works.
Any help will be greatly appreciated.</p>
<p>Thanks,
Ayala</p>
<pre><code>import numpy as np
import pandas as pd
import tensorflow as tf
from datetime import datetime
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.callbacks import ModelCheckpoint
from transformers import BertTokenizer, PreTrainedTokenizer, BertConfig, TFBertForNextSentencePrediction
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score


PRETRAINED_MODEL = 'bert-base-uncased'

# set paths and file names
time_stamp = str(datetime.now().year) + &quot;_&quot; + str(datetime.now().month) + &quot;_&quot; + str(datetime.now().day) + &quot;_&quot; + \
                     str(datetime.now().hour) + &quot;_&quot; + str(datetime.now().minute)
model_name = &quot;pretrained_nsp_model&quot;
model_dir_data = model_name + &quot;_&quot; + time_stamp
model_fn = model_dir_data + &quot;.h5&quot;
base_path = os.path.dirname(__file__)
input_path = os.path.join(base_path, &quot;input_data&quot;)
output_path = os.path.join(base_path, &quot;output_models&quot;)
model_path = os.path.join(output_path, model_dir_data)
if not os.path.exists(model_path):
    os.makedirs(model_path)

# set model checkpoint
checkpoint = ModelCheckpoint(os.path.join(model_path, model_fn), monitor=&quot;val_loss&quot;, verbose=1, save_best_only=True,
                             save_weights_only=True, mode=&quot;min&quot;)

# read data
max_length = 512

def get_tokenizer(pretrained_model_name):
    tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)
    return tokenizer

def tokenize_nsp_data(A, B, max_length):
    data_inputs = tokenizer(A, B, add_special_tokens=True, max_length=max_length, truncation=True,
                             pad_to_max_length=True, return_attention_mask=True,
                             return_tensors=&quot;tf&quot;)
    return data_inputs

def get_data_features(data_inputs, max_length):
    data_features = {}
    for key in data_inputs:
        data_features[key] = sequence.pad_sequences(data_inputs[key], maxlen=max_length, truncating=&quot;post&quot;,
                                                          padding=&quot;post&quot;, value=0)
    return data_features

def get_transformer_model(transformer_model_name):
    # get transformer model
    config = BertConfig(output_attentions=True)
    config.output_hidden_states = True
    config.return_dict = True
    transformer_model = TFBertForNextSentencePrediction.from_pretrained(transformer_model_name, config=config)
    return transformer_model

def get_keras_model(transformer_model):
    # get keras model
    input_ids = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype='int32')
    input_masks_ids = tf.keras.layers.Input(shape=(max_length,), name='attention_mask', dtype='int32')
    token_type_ids = tf.keras.layers.Input(shape=(max_length,), name='token_type_ids', dtype='int32')
    X = transformer_model({'input_ids': input_ids, 'attention_mask': input_masks_ids, 'token_type_ids': token_type_ids})[0]
    model = tf.keras.Model(inputs=[input_ids, input_masks_ids, token_type_ids], outputs=X)
    model.summary()
    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                  optimizer=tf.optimizers.Adam(learning_rate=0.00005), metrics=['accuracy'])
    return model

def get_metrices(true_values, pred_values):
    cm = confusion_matrix(true_values, pred_values)
    acc_score = accuracy_score(true_values, pred_values)
    f1 = f1_score(true_values, pred_values, average=&quot;binary&quot;)
    precision = precision_score(true_values, pred_values, average=&quot;binary&quot;)
    recall = recall_score(true_values, pred_values, average=&quot;binary&quot;)
    metrices = {'confusion_matrix': cm,
                'acc_score': acc_score,
                'f1': f1,
                'precision': precision,
                'recall': recall
                }
    for k, v in metrices.items():
        print(k, ':\n', v)
    return metrices

# get tokenizer
tokenizer = get_tokenizer(PRETRAINED_MODEL)

# train 
prompt = [&quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;]
next_sentence = [&quot;How are you?&quot;, &quot;Pizza&quot;, &quot;How are you?&quot;, &quot;Pizza&quot;]
train_labels = [0, 1, 0, 1]
train_labels = to_categorical(train_labels)
train_inputs = tokenize_nsp_data(prompt, next_sentence, max_length)
train_data_features = get_data_features(train_inputs, max_length)

# val
prompt = [&quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;, &quot;Hello&quot;]
next_sentence = [&quot;How are you?&quot;, &quot;Pizza&quot;, &quot;How are you?&quot;, &quot;Pizza&quot;]
val_labels = [0, 1, 0, 1]
val_labels = to_categorical(val_labels)
val_inputs = tokenize_nsp_data(prompt, next_sentence, max_length)
val_data_features = get_data_features(val_inputs, max_length)

# get transformer model
transformer_model = get_transformer_model(PRETRAINED_MODEL)

# get keras model
model = get_keras_model(transformer_model)

callback_list = []
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, min_delta=0.005, verbose=1)
callback_list.append(early_stop)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, epsilon=0.001)
callback_list.append(reduce_lr)
callback_list.append(checkpoint)

history = model.fit([train_data_features['input_ids'], train_data_features['attention_mask'],
                     train_data_features['token_type_ids']], np.array(train_labels), batch_size=2, epochs=3,
                    validation_data=([val_data_features['input_ids'], val_data_features['attention_mask'],
                                      val_data_features['token_type_ids']], np.array(val_labels)), verbose=1,
                    callbacks=callback_list)

model.layers[3].save_pretrained(model_path)  # need to save this and make sure i can get the hidden states

##  predict
# load model
transformer_model = get_transformer_model(model_path)
model = get_keras_model(transformer_model)
model.summary()
model.load_weights(os.path.join(model_path, model_fn))


# test
prompt = [&quot;Hello&quot;, &quot;Hello&quot;]
next_sentence = [&quot;How are you?&quot;, &quot;Pizza&quot;]
test_labels = [0, 1]
test_df = pd.DataFrame({'A': prompt, 'B': next_sentence, 'label': test_labels})
test_labels = to_categorical(val_labels)
test_inputs = tokenize_nsp_data(prompt, next_sentence, max_length)
test_data_features = get_data_features(test_inputs, max_length)

# predict
pred_test = model.predict([test_data_features['input_ids'], test_data_features['attention_mask'], test_data_features['token_type_ids']])
preds = tf.keras.activations.softmax(tf.convert_to_tensor(pred_test)).numpy()

true_test = test_df['label'].to_list()
pred_test = [1 if p[1] &gt; 0.5 else 0 for p in preds]
test_df['pred_val'] = pred_test

metrices = get_metrices(true_test, pred_test)


</code></pre>
<p>I am also attaching a picture from the debugging mode in which I try (with no success) to view the hidden state. <strong>The problem is I am not able to see and save the transform model I trained and view the embeddings of the last hidden state.</strong> I tried converting the <code>KerasTensor</code> to <code>numpy array</code> but without success.</p>
<p><a href=""https://i.stack.imgur.com/inP4a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/inP4a.png"" alt=""enter image description here"" /></a></p>
","4194509","","6664872","","2021-01-30 17:06:09","2021-01-30 17:06:09","Using tensorflow and TFBertForNextSentencePrediction to further train bert on a specific corpus","<python><tensorflow><keras><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"57204864","1","","","2019-07-25 14:53:00","","2","1419","<p>I am looking to finetune the huggingface's GPT-2 PyTorch model.<br/>
<a href=""https://github.com/huggingface/pytorch-transformers/tree/master"" rel=""nofollow noreferrer"">https://github.com/huggingface/pytorch-transformers/tree/master</a></p>

<p>I read the <em>'Examples'</em> section in their documentation where they have shown an example of training the old GPT model on RocStories dataset.<br/>
<a href=""https://huggingface.co/pytorch-transformers/examples.html#fine-tuning"" rel=""nofollow noreferrer"">https://huggingface.co/pytorch-transformers/examples.html#fine-tuning</a></p>

<p>I would like to know how to perform this kind of training for GPT-2. Also, my dataset doesn't have an <em>'Evaluation Set'</em>. I simply want to finetune GPT-2 on my dataset and then use it to generate new text which is 'similar' to the dataset it was finetuned on. </p>

<p>I am pretty sure I am missing something somewhere in the documentation. 
I would be glad if anyone can point me in the right direction.</p>
","2042701","","1150683","","2020-01-16 08:58:42","2020-01-16 08:58:42","Finetuning GPT-2 in huggingface's pytorch-transformers library","<python><deep-learning><nlp><pytorch><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"65279567","1","","","2020-12-13 19:11:14","","0","296","<p>I am trying to run a script example from the huggingface documentation:</p>
<pre><code>import torch

tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
model = GPT2LMHeadModel.from_pretrained('gpt2')

generated = tokenizer.encode(&quot;The Manhattan bridge&quot;)
context = torch.tensor([generated])
past = None

for i in range(100):
    print(i)
    output, past = model(context, past=past)
    token = torch.argmax(output[..., -1, :])

    generated += [token.tolist()]
    context = token.unsqueeze(0)

sequence = tokenizer.decode(generated)

print(sequence)
</code></pre>
<p>But I have an error:</p>
<pre><code>TypeError: forward() got an unexpected keyword argument 'past'
</code></pre>
<p>What should I change to use 'past'?</p>
","13375915","","4685471","","2020-12-14 00:50:20","2020-12-14 11:21:51","Using past in gpt2","<deep-learning><nlp><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65632248","1","","","2021-01-08 15:53:45","","1","42","<br>
I have computed the vectors for two same sentences using XLNet embedding-as-service (https://github.com/amansrivastava17/embedding-as-service). But the model produces different vector embeddings for both the two same sentences hence the cosine similarity is not 1 and the Euclidean distances also not 0. in case of BERT its works fine.
for example; if <br>
vec1 = en.encode(texts=['he is anger'],pooling='reduce_mean') <br>
vec2 = en.encode(texts=['he is anger'],pooling='reduce_mean') <br>
the model (XLNet) saying that these two sentences are dissimilar.<br>
thanks.
","14953381","","14953381","","2021-01-08 17:24:45","2021-01-08 17:24:45","Same sentences produces a different vector in XLNet","<bert-language-model><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"65645537","1","","","2021-01-09 17:26:33","","0","393","<p>I have a PySpark dataframe containing text and am trying to run a Huggingface model on it to get logits which I could then use downstream for classification. Here is the code I am using:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)
model = AutoModelWithLMHead.from_pretrained(&quot;gpt2&quot;)
str2tokens = udf(lambda s: tokenizer.tokenize(s, return_tensors='pt'), ArrayType(StringType()))
tokens2ids = udf(lambda s: tokenizer.convert_tokens_to_ids(s), ArrayType(IntegerType()))
vectorize_ids = udf(lambda s: list(model(tensor(s)).logits.mean(axis=0).detach().numpy()), ArrayType(DoubleType()))
df = spark.read.json(&quot;path/to/data&quot;).sample(False, 0.001)
df = df.withColumn('tokens', str2tokens('text'))
df = df.withColumn('ids', tokens2ids('tokens'))
</code></pre>
<p>Up until here, there is no error and the output is what you would expect (the text is from twitter):</p>
<pre><code>Row(text='my timeline the writer of the hangover movies and an original snl castmember ', user_id=230*****, username='NU***', tokens=['my', 'Ä timeline', 'Ä the', 'Ä writer', 'Ä of', 'Ä the', 'Ä hang', 'over', 'Ä movies', 'Ä and', 'Ä an', 'Ä original', 'Ä sn', 'l', 'Ä cast', 'member', 'Ä '], ids=[1820, 15264, 262, 6260, 286, 262, 8181, 2502, 6918, 290, 281, 2656, 3013, 75, 3350, 19522, 220])
</code></pre>
<p>Where the trouble I'm running into occurs is vectorizing the <code>ids</code>:</p>
<pre><code>df = df.withColumn('logits', vectorize_ids('ids'))
</code></pre>
<p>Error:</p>
<pre><code>    An error was encountered:
An error occurred while calling o197.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 260, ip-172-18-8-49.us-west-2.compute.internal, executor 1): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)
    at net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
    at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)
    at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)
    at net.razorvine.pickle.Unpickler.load(Unpickler.java:99)
    at net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)
    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:407)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3267)
    at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3264)
    at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
    at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)
    at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)
    at org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)
    at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)
    at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
    at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3264)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)
Caused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)
    at net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
    at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)
    at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)
    at net.razorvine.pickle.Unpickler.load(Unpickler.java:99)
    at net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)
    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more

Traceback (most recent call last):
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py&quot;, line 1256, in head
    rs = self.head(1)
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py&quot;, line 1258, in head
    return self.take(n)
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py&quot;, line 574, in take
    return self.limit(num).collect()
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py&quot;, line 535, in collect
    sock_info = self._jdf.collectToPython()
  File &quot;/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py&quot;, line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py&quot;, line 63, in deco
    return f(*a, **kw)
  File &quot;/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py&quot;, line 328, in get_return_value
    format(target_id, &quot;.&quot;, name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o197.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 260, ip-172-18-8-49.us-west-2.compute.internal, executor 1): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)
    at net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
    at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)
    at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)
    at net.razorvine.pickle.Unpickler.load(Unpickler.java:99)
    at net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)
    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:407)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3267)
    at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3264)
    at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
    at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)
    at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)
    at org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)
    at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)
    at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
    at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3264)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)
Caused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)
    at net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
    at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)
    at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)
    at net.razorvine.pickle.Unpickler.load(Unpickler.java:99)
    at net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)
    at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)
    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
</code></pre>
","1245418","","","","","2021-01-09 17:26:33","Error Doing Inference with Huggingface model in PySpark","<apache-spark><pyspark><deep-learning><pytorch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"65654202","1","","","2021-01-10 14:12:04","","0","17","<p>As part of my MSc project, I am required to transform the Q and K vectors used within scaled dot product attention. I am trying to find a method to rotate a selected K vector towards a selected Q vector gradually, as a way of changing the weight between the words represented by Q and K. From what I have found on Google, I need to compute a rotation matrix to do this, however, I have been struggling to find a simple explanation for how this can be computed and applied.</p>
<p>Could someone help me understand how this can be accomplished?</p>
<p>Thank you!</p>
","8405910","","","","","2021-01-10 14:12:04","Gradually rotate a 64-dimensional vector in the same direction as another vector?","<python><vector><linear-algebra><huggingface-transformers><attention-model>","0","0","","","","CC BY-SA 4.0"
"67355776","1","","","2021-05-02 11:22:19","","0","78","<p>I have a type of machine translation task in which I have to translate English Sentences to Hinglish Sentences. I am try to use the pre-trained Transformer-XL Model by fine tuning it on my custom dataset. Here is my code:</p>
<pre><code>import pandas as pd
import tensorflow as tf
from transformers import TransfoXLTokenizer
from transformers import TFTransfoXLModel
import numpy as np
from sklearn.model_selection import train_test_split

#Loading data
dataFrame = pd.read_csv(&quot;data.csv&quot;)
dataFrame.head(3)

#-----Output 1-----

#Splitting Dataset
X = dataFrame['English']
Y = dataFrame['Hinglish']
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)

#Tokenization
tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')

tokenizer.pad_token = tokenizer.eos_token

XTrainEncodings = tokenizer(X_train.to_list(),  max_length = 150, padding = True)
XTestEncodings = tokenizer(X_test.to_list(), max_length = 150, padding = True)
YTrainEncodings = tokenizer(Y_train.to_list(), max_length = 150, padding = True)
YTestEncodings = tokenizer(Y_test.to_list(), max_length = 150, padding = True)
print(&quot;XTrainEncodings : &quot;, XTrainEncodings)
print(&quot;YTrainEncodings : &quot;, YTrainEncodings)

#-----Output 2-----

#Converting to Tensors
X_train = tf.data.Dataset.from_tensor_slices((dict(XTrainEncodings), (dict(YTrainEncodings))))
X_test = tf.data.Dataset.from_tensor_slices((dict(XTestEncodings), (dict(YTestEncodings))))
print(X_train)

#-----Output 3-----

#Fine Tuning
model = TFTransfoXLModel.from_pretrained('transfo-xl-wt103')

optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5)
model.compile(optimizer = optimizer, loss = tf.losses.SparseCategoricalCrossentropy(), metrics = ['accuracy'])

history = model.fit(X_train.batch(1), epochs = 2, batch_size = 1, validation_data = X_test.batch(1))
</code></pre>
<p><strong>Outputs:</strong></p>
<pre><code>-----Output 1-----

    English                     Hinglish
How are you ?                Tum kaise ho ?
I am fine.                  Main theek hoon
......

-----Output 2-----
XTrainEncodings :  {'input_ids': [[4241, 0, 0, 0, 0, 0], [4827, 37, 304, 788, 0, 0],....
YTrainEncodings :  {'input_ids': [[13762, 0, 0, 0, 0], [71271, 24, 33289, 788, 0],....

-----Output 3-----
&lt;TensorSliceDataset shapes: ({input_ids: (6,)}, {input_ids: (5,)}), types: ({input_ids: tf.int32}, {input_ids: tf.int32})&gt;
</code></pre>
<p>I am getting the following error:</p>
<pre><code>ValueError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:758 train_step
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:387 update_state
        self.build(y_pred, y_true)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:318 build
        self._metrics, y_true, y_pred)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py:1163 map_structure_up_to
        **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py:1245 map_structure_with_tuple_paths_up_to
        expand_composites=expand_composites)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py:878 assert_shallow_structure
        input_length=len(input_tree), shallow_length=len(shallow_tree)))

    ValueError: The two structures don't have the same sequence length. Input structure has length 3, while shallow structure has length 2.
</code></pre>
<p>Please help me in detecting the reason and solving the error. Also I want to know whether I am following a correct way to achieve my task or is there some another better ways. As I am new to deep learning so I am not sure about that. Thanks</p>
","7024599","","7024599","","2021-05-02 12:44:57","2021-05-02 12:44:57","How we can Fine Tune HuggingFace Transformer-XL Model For Text Generation & Machine Translation?","<deep-learning><tensorflow2.0><huggingface-transformers><transformer><attention-model>","0","0","","","","CC BY-SA 4.0"
"65961373","1","","","2021-01-29 20:37:36","","0","304","<p>I have been playing around with tensorflow (CPU), and some language model'ing - and it have been a blast so far - everything working great.<br />
But after watching my old CPU slowly getting killed from all the model-training - i decided it was time to  finally get some use out of my RTX 2080. I have been following the guide from <a href=""https://github.com/jeffheaton/t81_558_deep_learning/blob/master/install/tensorflow-install-jul-2020.ipynb"" rel=""nofollow noreferrer"">washinton university</a>:.  Pretty quickly i got tensorflow-gpu running, ran it on some light grade-prediction and stuff like that.</p>
<p>But when i got to running GPT2 language model, i ran into some minor problems. I start by tokenizing the data:</p>
<pre><code>from tokenizers.models import BPE
from tokenizers import Tokenizer
from tokenizers.decoders import ByteLevel as ByteLevelDecoder
from tokenizers.normalizers import NFKC, Sequence
from tokenizers.pre_tokenizers import ByteLevel
from tokenizers.trainers import BpeTrainer

class BPE_token(object):
def __init__(self):
    self.tokenizer = Tokenizer(BPE())
    self.tokenizer.normalizer = Sequence([
        NFKC()
    ])
    self.tokenizer.pre_tokenizer = ByteLevel()
    self.tokenizer.decoder = ByteLevelDecoder()

def bpe_train(self, paths):
    trainer = BpeTrainer(vocab_size=50000, show_progress=True, inital_alphabet=ByteLevel.alphabet(),         special_tokens=[
        &quot;&lt;s&gt;&quot;,
        &quot;&lt;pad&gt;&quot;,
        &quot;&lt;/s&gt;&quot;,
        &quot;&lt;unk&gt;&quot;,
        &quot;&lt;mask&gt;&quot;
    ])
    self.tokenizer.train(trainer, paths)

def save_tokenizer(self, location, prefix=None):
    if not os.path.exists(location):
        os.makedirs(location)
    self.tokenizer.model.save(location, prefix)

# ////////// TOKENIZE DATA ////////////
from pathlib import Pa th
import os# the folder 'text' contains all the files
paths = [str(x) for x in Path(&quot;./da_corpus/&quot;).glob(&quot;**/*.txt&quot;)]
tokenizer = BPE_token()# train the tokenizer model
tokenizer.bpe_train(paths)# saving the tokenized data in our specified folder
save_path = 'tokenized_data'
tokenizer.save_tokenizer(save_path)
</code></pre>
<p>Code above works perfectly and tokenizes the data - just like with tensorflow (CPU). After having my data tokenized i start to train my model - but before it even gets start, i get the following ImportError:</p>
<pre><code>from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer # loading tokenizer from the   saved model path
ImportError: cannot import name 'TFGPT2LMHeadModel' from 'transformers' (unknown location)
</code></pre>
<p>Transformers package seems to be installed correctly in the site-packages lib, and i seem to be able to use the other transformers - but not <strong>TFGPT2LMHeadModel</strong>
I have read everything on google and <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">hugging.co</a> - tried different versions of tensorflow-gpu, transformers, tokenizers and alot of other packages -  -  sadly nothing helps.</p>
<p><strong>Packages:</strong></p>
<ul>
<li>Python, 3.7.1</li>
<li>Tensorflow 2.1.0</li>
<li>Tensorflow-gpu 2.1.0</li>
<li>Tensorflow-base 2.1.0</li>
<li>Tensorflow-estimator 2.1.0</li>
<li>Transformers 4.2.2</li>
<li>Tokenizers 0.9.4</li>
<li>cudnn 7.6.5</li>
<li>cudatoolkit 10.1.243</li>
</ul>
","9837081","","9837081","","2021-01-29 21:12:16","2021-05-14 11:20:18","TFGPT2LMHeadModel unknown location","<python><tensorflow><importerror><huggingface-transformers><huggingface-tokenizers>","2","2","","","","CC BY-SA 4.0"
"67387829","1","","","2021-05-04 15:36:16","","0","71","<p>I've been working on a project where I want to calculate the similarity between 2 sentences as input to my model (using BERT by HuggingFace Transformers library and Qoura sentence pair dataset from kaggle). I was trying to use my scoring function as torch cosine similarity but for every input I get a value around 0.98, but as far as cosine similarity output goes, it values comes between -1 to 1. I have included the required code below :</p>
<pre><code>class SentencePairSimilarityBert(nn.Module):
    def __init__(self):
        super(SentencePairSimilarityBert, self).__init__()
        self.bert1 = transformers.BertModel.from_pretrained('bert-base-uncased')
        self.bert2 = transformers.BertModel.from_pretrained('bert-base-uncased')
        self.cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)

    def forward(self, q1_input_ids, q1_attention_mask, q2_input_ids, q2_attention_mask):
        _, q1_pooled_output = self.bert1(input_ids=q1_input_ids, attention_mask=q1_attention_mask, return_dict=False)
        _, q2_pooled_output = self.bert2(input_ids=q2_input_ids, attention_mask=q2_attention_mask, return_dict=False)
        cosine_distance = self.cosine_similarity(q1_pooled_output, q2_pooled_output)
        return cosine_distance
</code></pre>
<p>I'm using loss function as Mean Squared Error loss (MSELoss) because I want to calculate how far I am from my target/real value.</p>
<p>The cosine_distance for a batch of 4 comes out to be : [0.982, 0.976, 0.974, 0.948] and targets are : [0, 0, 1, 0]</p>
<p>0 means both sentence are dissimilar and 1 means they are similar.</p>
<p><strong>I'm trying the following training approach but I don't think so I'm going correct because the cosine_distance should be between -1 to 1 and my targets are 0 and 1, so how would I map my targets to this range of cosine_distance in order to calculate the loss and back-propagate.</strong></p>
<p><code>loss_function = nn.MSELoss()</code></p>
<p>This is my train function below :</p>
<pre><code>model = model.train()

losses = []

for dictionary in data_loader:
    q1_input_ids = dictionary['q1_input_ids'].to(device)
    q1_attention_mask = dictionary['q1_attention_mask'].to(device)
    q2_input_ids = dictionary['q2_input_ids'].to(device)
    q2_attention_mask = dictionary['q2_attention_mask'].to(device)
    targets = dictionary['targets'].to(device)

results = model(
     q1_input_ids=q1_input_ids,
     q1_attention_mask=q1_attention_mask,
     q2_input_ids=q2_input_ids,
     q2_attention_mask=q2_attention_mask,
)

loss = loss_function(results, targets)
losses.append(loss.item())
loss.backward()
optimizer.step()
scheduler.step()
optimizer.zero_grad()
</code></pre>
<p>Any help would be appreciated.
Thanks in advance !</p>
","11685381","","","","","2021-05-04 15:36:16","How to add cosine similarity as score function in sentence similarity using BERT Transformers","<python><pytorch><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"66064503","1","66074573","","2021-02-05 13:51:05","","0","1196","<p>I am using <code>DistilBertTokenizer</code> tokenizer from <a href=""https://huggingface.co"" rel=""nofollow noreferrer"">HuggingFace</a>.</p>
<p>I would like to tokenize my text by simple splitting it on space:</p>
<pre><code>[&quot;Don't&quot;, &quot;you&quot;, &quot;love&quot;, &quot;ðŸ¤—&quot;, &quot;Transformers?&quot;, &quot;We&quot;, &quot;sure&quot;, &quot;do.&quot;]
</code></pre>
<p>instead of the default behavior, which is like this:</p>
<pre><code>[&quot;Do&quot;, &quot;n't&quot;, &quot;you&quot;, &quot;love&quot;, &quot;ðŸ¤—&quot;, &quot;Transformers&quot;, &quot;?&quot;, &quot;We&quot;, &quot;sure&quot;, &quot;do&quot;, &quot;.&quot;]
</code></pre>
<p>I read their documentation about <a href=""https://huggingface.co/transformers/tokenizer_summary.html"" rel=""nofollow noreferrer"">Tokenization</a> in general as well as about <a href=""https://huggingface.co/transformers/model_doc/bert.html#transformers.BertTokenizer"" rel=""nofollow noreferrer"">BERT Tokenizer</a> specifically, but could not find an answer to this simple question :(</p>
<p>I assume that it should be a parameter when loading Tokenizer, but I could not find it among the parameters list ...</p>
<p>EDIT:
Minimal code example to reproduce:</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('distilbert-base-cased')

tokens = tokenizer.tokenize(&quot;Don't you love ðŸ¤— Transformers? We sure do.&quot;)
print(&quot;Tokens: &quot;, tokens)
</code></pre>
","4776477","","4776477","","2021-02-05 14:42:54","2021-02-12 08:15:31","In HuggingFace tokenizers: how can I split a sequence simply on spaces?","<split><tokenize><huggingface-transformers><huggingface-tokenizers>","2","2","","","","CC BY-SA 4.0"
"67356666","1","67511553","","2021-05-02 12:57:50","","0","373","<p>i was decoding the tokenized tokens from <strong>bert tokenizer</strong> and it was giving <strong>[UNK]</strong> for â‚¬ symbol. but i tried by add ##â‚¬ token in vocab.txt file. but it was not reflected in prediction result was same as previous it was giving [UNK] again. please let me know to solve this problem did i need to <strong>fine tune</strong> the model for again to reflect the changes in <strong>prediction</strong>. till now i was avoiding fine tuning again because it takes more than 10 hours.
Thanks in advance</p>
","15542816","","","","","2021-07-29 22:06:44","how to add tokens in vocab.txt which decoded as [UNK] bert tokenizer","<python><nlp><bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","1","1","","","CC BY-SA 4.0"
"67357718","1","","","2021-05-02 14:52:08","","2","107","<p>I want to import pipline from transformers and I get this error. My Tensorflow version is 2.0.0 and my torch version is 1.8.1</p>
<pre><code>from transformers import pipeline
</code></pre>
<p>That's the error message: &quot;ImportError: cannot import name 'CausalLMOutputWithPastAndCrossAttentions' from 'transformers.modeling_outputs'&quot;</p>
","13508539","","","","","2021-05-02 14:52:08","ImportError: cannot import name 'CausalLMOutputWithPastAndCrossAttentions' from 'transformers.modeling_outputs'","<python><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"66207138","1","66260061","","2021-02-15 11:23:51","","0","52","<p>I am trying to train an XLNET model as the following. I want to set the hyperparameters by myself without using any pretrained models.</p>
<pre><code>from transformers import XLNetConfig, XLNetModel
from transformers import Trainer, TrainingArguments
# Initializing an XLNet configuration
configuration = XLNetConfig(use_mems_train = True)
model = XLNetModel(configuration)
train_dataset = 'sentences.txt'
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total # of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
)
trainer = Trainer(
    model=model,                         # the instantiated Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
)
trainer.train()
</code></pre>
<p>However, the following errors appear:</p>
<pre><code>C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])
  0%|          | 0/9 [00:00&lt;?, ?it/s]Traceback (most recent call last):
  File &quot;untitled1/dfgd.py&quot;, line 23, in &lt;module&gt;
    trainer.train()
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\transformers\trainer.py&quot;, line 925, in train
    for step, inputs in enumerate(epoch_iterator):
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\torch\utils\data\dataloader.py&quot;, line 435, in __next__
    data = self._next_data()
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\torch\utils\data\dataloader.py&quot;, line 475, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\torch\utils\data\_utils\fetch.py&quot;, line 47, in fetch
    return self.collate_fn(data)
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\transformers\data\data_collator.py&quot;, line 52, in default_data_collator
    features = [vars(f) for f in features]
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\transformers\data\data_collator.py&quot;, line 52, in &lt;listcomp&gt;
    features = [vars(f) for f in features]
TypeError: vars() argument must have __dict__ attribute
Exception ignored in: &lt;function tqdm.__del__ at 0x0000014A17ABE828&gt;
Traceback (most recent call last):
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\std.py&quot;, line 1039, in __del__
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\std.py&quot;, line 1223, in close
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\std.py&quot;, line 555, in _decr_instances
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\_monitor.py&quot;, line 51, in exit
  File &quot;C:\ProgramData\Anaconda3\lib\threading.py&quot;, line 522, in set
  File &quot;C:\ProgramData\Anaconda3\lib\threading.py&quot;, line 365, in notify_all
  File &quot;C:\ProgramData\Anaconda3\lib\threading.py&quot;, line 348, in notify
TypeError: 'NoneType' object is not callable
</code></pre>
<p>How should I handle these errors? How can I train my XLNET model?</p>
","14251114","","6664872","","2021-02-17 11:29:10","2021-02-18 12:33:39","Errors appear when training an XLNET model","<python><huggingface-transformers><transformer>","1","1","","","","CC BY-SA 4.0"
"67391794","1","","","2021-05-04 20:30:08","","2","316","<p>I am fine tuning a classification BERT model using <code>transformers 4.5.1</code> and <code>tensorflow 2.4.0</code>. I am using SST2 dataset (using TFRecord format).</p>
<p>I am using the simple way:</p>
<pre><code>model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased',num_labels=2)

model.summary()
Model: &quot;tf_bert_for_sequence_classification_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
bert (TFBertMainLayer)       multiple                  167356416 
_________________________________________________________________
dropout_75 (Dropout)         multiple                  0         
_________________________________________________________________
classifier (Dense)           multiple                  1538      
=================================================================
Total params: 167,357,954
Trainable params: 167,357,954
Non-trainable params: 0
</code></pre>
<p>The model is trained in the following way:</p>
<pre><code>tf.keras.backend.clear_session()

learning_rate=5e-5
epsilon=1e-8

# loss
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# metric
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

# optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)

model._name = 'tf_bert_classification'

# compile Keras model
model.compile(optimizer=optimizer,
              loss=loss,
              metrics=[metric])

model.fit(train_data,
          epochs=1,
          steps_per_epoch=5,
          validation_data=eval_data,
          validation_steps=1)
</code></pre>
<p>The data looks like that:
train_data</p>
<pre><code>&lt;BatchDataset shapes: ({input_ids: (None, 128), attention_mask: (None, 128), token_type_ids: (None, 128)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)&gt;
</code></pre>
<p>We can iterate over the data to check again the shape:</p>
<pre><code>for i in eval_data:
    print(i[0]['input_ids'].shape)
    print(i[0]['attention_mask'].shape)
    print(i[0]['token_type_ids'].shape)
    print(i[1].shape)
    break


(32, 128)
(32, 128)
(32, 128)
(32,)
</code></pre>
<p>The model can be trained with a small amount of data.
We can also do prediction:</p>
<pre><code>model.predict(eval_data)

FSequenceClassifierOutput(loss=None, logits=array([[-0.9976097 ,  0.05450067],
       [-0.99764097,  0.05462598],
       [-0.9975417 ,  0.05509752],
       ...,
       [-0.9975057 ,  0.05470059],
       [-0.99738246,  0.0544476 ],
       [-0.9978782 ,  0.05457467]], dtype=float32), hidden_states=None, attentions=None)
</code></pre>
<p>I can now save the model and reload it:</p>
<pre><code>model.save('bert_default')
reconstructed_model = tf.keras.models.load_model('bert_default')
reconstructed_model.summary()
Model: &quot;tf_bert_classification&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
bert (TFBertMainLayer)       multiple                  167356416 
_________________________________________________________________
dropout_75 (Dropout)         multiple                  0         
_________________________________________________________________
classifier (Dense)           multiple                  1538      
=================================================================
Total params: 167,357,954
Trainable params: 167,357,954
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>Now if I try to do prediction with the same data as before I get the following issue:</p>
<pre><code>reconstructed_model.predict(eval_data)

    ValueError: Could not find matching function to call loaded from the SavedModel. Got:
      Positional arguments (11 total):
        * {'input_ids': &lt;tf.Tensor 'input_ids_1:0' shape=(None, 128) dtype=int32&gt;, 'attention_mask': &lt;tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32&gt;, 'token_type_ids': &lt;tf.Tensor 'input_ids_2:0' shape=(None, 128) dtype=int32&gt;}
        * None
        * None
        * None
        * None
        * None
        * None
        * None
        * None
        * None
        * False
      Keyword arguments: {}
    
    Expected these arguments to match one of the following 2 option(s):
    
    Option 1:
      Positional arguments (11 total):
        * {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}
        * None
        * None
        * None
        * None
        * None
        * None
        * None
        * None
        * None
        * True
      Keyword arguments: {}
    
    Option 2:
      Positional arguments (11 total):
        * {'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}
        * None
        * None
        * None
        * None
        * None
        * None
        * None
        * None
        * None
        * False
      Keyword arguments: {}
</code></pre>
<p>I see some Warning when saving the model:</p>
<pre><code>WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 1055). These functions will not be directly callable after loading.
WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 1055). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: ert_default/assets
</code></pre>
<p>Is it the reason while the shape is changed from (-1, 128) to (-1, 5) ?</p>
<p>When training the model I see other warning but people as saying they should be ignire:</p>
<pre><code>WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
</code></pre>
<p>How can I fix this issue that appear only when the model is saved ? Should I set some parameters in the config ?</p>
<p>[Edit]
Same issue with <code>tensorflow 2.5.0-rc3</code>
Using Huggingface save/load is working fine</p>
<pre><code>model.save_pretrained('bert_input')
reconstructed_model = TFBertForSequenceClassification.from_pretrained('bert_input')
</code></pre>
<p>But this is not what I need for tf.serving.</p>
<p>It seems some class/function definition are not saved with the model so reloading the model will not work. Maybe there are some paramters to set to have this working ?</p>
","6430839","","6430839","","2021-05-07 20:07:55","2021-09-04 12:29:22","saved model after reloading with tf.keras.models.load_model request different input shape -> untraced functions (embeddings_layer)","<tensorflow><tf.keras><huggingface-transformers>","0","2","1","","","CC BY-SA 4.0"
"64406166","1","","","2020-10-17 18:52:54","","1","1312","<p>I am attempting to use the BertTokenizer part of the transformers package. First I install as below.</p>
<pre><code>pip install transformers
</code></pre>
<p>Which says it succeeds.</p>
<p>When I try to import parts of the package as below I get the following.</p>
<pre><code> from transformers import BertTokenizer
Traceback (most recent call last):

  File &quot;&lt;ipython-input-2-89505a24ece6&gt;&quot;, line 1, in &lt;module&gt;
    from transformers import BertTokenizer

  File &quot;C:\Users\User\anaconda3\lib\site-packages\transformers\__init__.py&quot;, line 22, in &lt;module&gt;
    from .integrations import (  # isort:skip

  File &quot;C:\Users\User\anaconda3\lib\site-packages\transformers\integrations.py&quot;, line 42, in &lt;module&gt;
    from .trainer_utils import PREFIX_CHECKPOINT_DIR, BestRun  # isort:skip

  File &quot;C:\Users\User\anaconda3\lib\site-packages\transformers\trainer_utils.py&quot;, line 10, in &lt;module&gt;
    from .tokenization_utils_base import ExplicitEnum

  File &quot;C:\Users\User\anaconda3\lib\site-packages\transformers\tokenization_utils_base.py&quot;, line 31, in &lt;module&gt;
    from tokenizers import AddedToken

  File &quot;C:\Users\User\anaconda3\lib\site-packages\tokenizers\__init__.py&quot;, line 17, in &lt;module&gt;
    from .tokenizers import Tokenizer, Encoding, AddedToken

ModuleNotFoundError: No module named 'tokenizers.tokenizers'
</code></pre>
<p>The package is detailed here so I think it should be available
<a href=""https://huggingface.co/transformers/model_doc/bert.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html</a></p>
","2355903","","","","","2021-09-03 21:00:09","Can't Import BertTokenizer","<python><huggingface-transformers>","3","0","","","","CC BY-SA 4.0"
"66073395","1","66074032","","2021-02-06 03:50:23","","1","97","<p>I'm following along with the <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering"" rel=""nofollow noreferrer"">documentation</a> for importing a pretrained model question and answer model from huggingface</p>
<pre><code>from transformers import BertTokenizer, BertForQuestionAnswering
import torch
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')
question, text = &quot;Who was Jim Henson?&quot;, &quot;Jim Henson was a nice puppet&quot;
inputs = tokenizer(question, text, return_tensors='pt')
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])
outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits
</code></pre>
<p>this returns start and end scores, but how can I get a meaningful text answer from here?</p>
","1938988","","1938988","","2021-02-08 14:21:58","2021-02-08 14:22:09","BERT Convert 'SpanAnnotation' to answers using scores from hugging face models","<python><pytorch><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65854722","1","66089859","","2021-01-23 01:00:27","","5","1862","<p>I simply tried the sample code from hugging face website: <a href=""https://huggingface.co/albert-base-v2"" rel=""noreferrer"">https://huggingface.co/albert-base-v2</a></p>
<pre><code>`from transformers import AlbertTokenizer, AlbertModel` 
`tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')`
`text = &quot;Replace me by any text you'd like.&quot;`
`encoded_input = tokenizer(text, return_tensors='pt')`
</code></pre>
<p>then I got the following error at the tokenizer step:
----&gt; 5 encoded_input = tokenizer(text, return_tensors='pt')</p>
<p>TypeError: 'NoneType' object is not callable</p>
<p>I tried the same code on my local machine, it worked no problem. The problem seems within Colab. However, I do need help to run this model on colab GPU.</p>
<p>My python version on colab is Python 3.6.9</p>
","8426105","","8426105","","2021-01-23 01:09:07","2021-03-08 15:39:47","Huggingface AlBert tokenizer NoneType error with Colab","<google-colaboratory><huggingface-transformers><huggingface-tokenizers>","2","3","","","","CC BY-SA 4.0"
"65857708","1","","","2021-01-23 09:43:16","","0","343","<p>I'm using a BERT tokenizer over a large dataset of sentences (2.3M lines, 6.53bn words):</p>
<pre><code>#creating a BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', 
                                          do_lower_case=True)

#encoding the data using our tokenizer
encoded_dict = tokenizer.batch_encode_plus(
    df[df.data_type=='train'].comment.values, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    pad_to_max_length=True, 
    max_length=256, 
    return_tensors='pt'
)
</code></pre>
<p>As-is, it runs on CPU and only on 1 core. I tried to parallelize, but that will only speed up processing by 16x with my 16 cores CPU, which will still make it run for ages if I want to tokenize the full dataset.</p>
<p>Is there any way to make it run on GPU or to speed this up some other way?</p>
<p>EDIT:
I have also tried using a fast tokenizer:</p>
<pre><code>#creating a BERT tokenizer
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', 
                                          do_lower_case=True)
</code></pre>
<p>Then passing the output to my batch_encode_plus:</p>
<pre><code>#encoding the data using our tokenizer
encoded_dict = tokenizer.batch_encode_plus(
    df[df.data_type=='train'].comment.values, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    pad_to_max_length=True, 
    max_length=256, 
    return_tensors='pt'
)
</code></pre>
<p>But the batch_encode_plus returns the following error:</p>
<blockquote>
<p>TypeError: batch_text_or_text_pairs has to be a list (got &lt;class
'numpy.ndarray'&gt;)</p>
</blockquote>
","2175173","","2175173","","2021-01-26 15:34:01","2021-01-26 15:34:01","Is there a way to use GPU instead of CPU for BERT tokenization?","<pytorch><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","7","0","","","CC BY-SA 4.0"
"57581587","1","","","2019-08-20 21:11:58","","4","508","<p>I am finetuning the bert model from <a href=""https://github.com/huggingface/pytorch-transformers"" rel=""nofollow noreferrer"">huggingface</a>. Is there a way to manually set the initial embedding of a certain word piece? e.g. having the initial embedding of the word ""dog"" equal to <code>torch.ones(768)</code>. Thanks!</p>
","1767774","","9084663","","2020-02-24 16:02:47","2020-04-07 07:08:20","huggingface pytorch-transformers: how to initialize embeddings with certain values?","<nlp><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66221102","1","","","2021-02-16 08:41:37","","2","562","<p>I'm working with word embeddings. I obtained word embeddings using 'BERT'.</p>
<p>I have a data like this</p>
<pre><code>1992 regular unleaded 172 6 MANUAL all wheel drive 4 Luxury Midsize Sedan 21 16 3105 200
</code></pre>
<p>and as a label:</p>
<pre><code>df['Make'] = df['Make'].replace(['Chrysler'],1)
</code></pre>
<p>I try to give embeddings as a LSTM inputs.</p>
<p>Using below code for BERT:</p>
<pre><code> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')                                                     
 bert_model = BertModel.from_pretrained('bert-base-uncased')
</code></pre>
<p>For tokinizing this code:</p>
<pre><code>def tokenize_text(df, max_seq):
    return [ 
        tokenizer.encode(text, add_special_tokens=True)[:max_seq] for text in df
    ]


def pad_text(tokenized_text, max_seq):
    return np.array([el + [0] * (max_seq - len(el)) for el in tokenized_text])


def tokenize_and_pad_text(df, max_seq):
    tokenized_text = tokenize_text(df, max_seq)
    padded_text = pad_text(tokenized_text, max_seq)
    return padded_text
</code></pre>
<p>and :</p>
<pre><code>train_indices = tokenize_and_pad_text(df_train, max_seq)
</code></pre>
<p>for BERT embedding matrix:</p>
<pre><code>def get_bert_embed_matrix():
    bert = transformers.BertModel.from_pretrained('bert-base-uncased')
    bert_embeddings = list(bert.children())[0]
    bert_word_embeddings = list(bert_embeddings.children())[0]
    mat = bert_word_embeddings.weight.data.numpy()
    return mat

embedding_matrix = get_bert_embed_matrix()
</code></pre>
<p>and LSTM Model:</p>
<pre><code>embedding_layer =Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], input_length=max_seq_len, trainable=True)
model = Sequential()
model.add(embedding_layer)
model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(1, activation='softmax'))

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss=&quot;categorical_crossentropy&quot;,
    metrics=[&quot;accuracy&quot;],
)

model.summary()
</code></pre>
<p>and for model fit this code:</p>
<pre><code>model.fit(train_indices, y_train, epochs=20, verbose=1)
</code></pre>
<p>I give a output like this:</p>
<pre><code>Epoch 1/20
1/1 [==============================] - 3s 3s/step - loss: 0.0000e+00 - accuracy: 0.3704
Epoch 20/20
1/1 [==============================] - 0s 484ms/step - loss: 0.0000e+00 - accuracy: 0.3704
</code></pre>
<p>I don't know what I'm missing.</p>
<p>Firstly, what can we do about it? <br/>
Secondly, how can we implement Pytorch Model?</p>
<p>Thanks a lot.</p>
","15067555","","6664872","","2021-02-20 13:21:06","2021-02-20 13:21:06","BERT Embeddings in Pytorch Embedding Layer","<pytorch><keras-layer><bert-language-model><huggingface-transformers>","0","5","","","","CC BY-SA 4.0"
"66219090","1","66219975","","2021-02-16 05:26:28","","1","407","<p>I first fine-tune the Bert model on a text classification task and but then I want to get the embeddings of the fine-tuned model in TensorFlow. Unfortunately, I can only say <code>output_hidden_states=True</code>, in the first line where I download the pre-trained Bert model but not in the second stage where I create a <code>tf.Keras.Model</code>. Here is the code for how I make and train the model:</p>
<pre><code>max_len = 55

from transformers import BertConfig, BertTokenizer, TFBertModel

def build_custome_model():
    bert_encoder = TFBertModel.from_pretrained(Base_BERT_Path)

    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_mask&quot;)
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_type_ids&quot;)
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    clf_output = embedding[:,0,:]

    net = tf.keras.layers.Dropout(0.4)(clf_output)
    output = tf.keras.layers.Dense(5, activation='softmax')(net)
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)

    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
</code></pre>
<p>Then I train the model on a dataset with 2 sentences and as score for their similarity</p>
<pre><code>#------Training with stratifiedkfold-------


k = 5
kfold = StratifiedKFold(n_splits = k, shuffle = True)

for i, (train_idx, val_idx) in enumerate(kfold.split(first_sentences, labels.score), 1):

    epoch_evaluation = {}

    train_input = create_input(np.array(first_sentences)[train_idx], np.array(second_sentences)[train_idx], tokenizer, max_len=max_seq_length)
    validation_input = create_input(np.array(first_sentences)[val_idx], np.array(second_sentences)[val_idx], tokenizer, max_len=max_seq_length)

    history = model.fit(x = train_input, y = labels.loc[train_idx, 'score'],
                        validation_data= (validation_input, labels.loc[val_idx, 'score']),
                        epochs = 5,
                        verbose = 1,
                        batch_size = 8)
</code></pre>
<p>My goal is to have a model at the end that is trained on this dataset and can output the embeddings (first layer of the hidden states (output[2][0])) whenever I give it a sentence so that I can get the mean of all the fine-tuned token embeddings of a sentence.</p>
","11810876","","11810876","","2021-02-16 08:16:05","2021-02-16 13:58:19","How to get the hidden states of a fine-tuned TFBertModel?","<python><tensorflow><deep-learning><nlp><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"64663385","1","","","2020-11-03 13:03:34","","10","4301","<p>I am trying to reload a fine-tuned DistilBertForTokenClassification model. I am using transformers 3.4.0 and pytorch version 1.6.0+cu101. After using the Trainer to train the downloaded model, I save the model with trainer.save_model() and in my trouble shooting I save in a different directory via model.save_pretrained(). I am using Google Colab and saving the model to my Google drive. After testing the model I also evaluated the model on my test getting great results, however, when I return to the notebook (or Factory restart the colab notebook) and try to reload the model, the predictions are terrible. Upon checking the directories, the config.json file is there as is the pytorch_mode.bin. Below is the full code.</p>
<pre><code>from transformers import DistilBertForTokenClassification

# load the pretrained model from huggingface
#model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(uniq_labels))
model = DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=len(uniq_labels)) 

model.to('cuda');

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir = model_dir +  'mitmovie_pt_distilbert_uncased/results',          # output directory
    #overwrite_output_dir = True,
    evaluation_strategy='epoch',
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir = model_dir +  'mitmovie_pt_distilbert_uncased/logs',            # directory for storing logs
    logging_steps=10,
    load_best_model_at_end = True
)

trainer = Trainer(
    model = model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args = training_args,                  # training arguments, defined above
    train_dataset = train_dataset,         # training dataset
    eval_dataset = test_dataset             # evaluation dataset
)

trainer.train()

trainer.evaluate()

model_dir = '/content/drive/My Drive/Colab Notebooks/models/'
trainer.save_model(model_dir + 'mitmovie_pt_distilbert_uncased/model')

# alternative saving method and folder
model.save_pretrained(model_dir + 'distilbert_testing')
</code></pre>
<p>Coming back to the notebook after restarting...</p>
<pre><code>from transformers import DistilBertForTokenClassification, DistilBertConfig, AutoModelForTokenClassification

# retreive the saved model 
model = DistilBertForTokenClassification.from_pretrained(model_dir + 'mitmovie_pt_distilbert_uncased/model', 
                                                        local_files_only=True)

model.to('cuda')
</code></pre>
<p>Model predictions are terrible now from either directory, however, the model does work and outputs the number of classes I would expect, it appears that the actual trained weights have not been saved or are somehow not getting loaded.</p>
","4283043","","3890632","","2020-11-03 16:15:04","2021-09-16 09:50:16","Saving and reload huggingface fine-tuned transformer","<python><pytorch><huggingface-transformers>","1","4","","","","CC BY-SA 4.0"
"64094098","1","","","2020-09-27 23:03:34","","1","107","<p>BertForSequenceClassification uses [CLS] token's representation to feed a linear classifier. I want to leverage another token (say [X] in the input sequence) rather than [CLS]. What's the most straightforward way to implement that in Transformers?</p>
","3052875","","","","","2021-10-01 13:18:42","How to initialize BertForSequenceClassification for different input rather than [CLS] token?","<huggingface-transformers>","1","3","","","","CC BY-SA 4.0"
"64193049","1","","","2020-10-04 09:34:37","","4","726","<pre><code>ner_model = pipeline('ner', model=model, tokenizer=tokenizer, device=0, grouped_entities=True)
</code></pre>
<p>the <em><strong>device</strong></em> indicated pipeline to use no_gpu=0(only using GPU), please show me how to use multi-gpu.</p>
","2281101","","286934","","2020-10-04 11:19:55","2020-10-04 11:19:55","How to use transformers pipeline with multi-gpu?","<python><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"64870416","1","","","2020-11-17 06:38:45","","0","45","<p>I am using Bert embedding for Key Phrase extraction from documents. I am using Bert embeddings followed by span based feature. The train data has candidate phrases that are identified using pos tagging. Below are the implementation details</p>
<pre><code>encoder = TFBertModel.from_pretrained(&quot;bert-base-uncased&quot;)


input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)
attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)
embedding = encoder(input_ids, attention_mask=attention_mask)[0]

bilstm1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40,
                                                             #kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.02,stddev=0.25),
                                                             dropout = 0.1,
                                                             return_sequences=True),
                                                             merge_mode=None)(embedding)
pos_mask = tf.keras.layers.Input(shape=(2,146),dtype='int32')
mask_start = pos_mask[0][0]
mask_end = pos_mask[0][1]

start_rep_fr = tf.gather(bilstm1[0],mask_start,axis=1)
start_rep_bk = tf.gather(bilstm1[1],mask_start,axis=1)
end_rep_fr = tf.gather(bilstm1[0],mask_end,axis=1)
end_rep_bk = tf.gather(bilstm1[0],mask_end,axis=1)


span_fe_diff_fr = start_rep_fr-end_rep_fr
span_fe_prod_fr = tf.math.multiply(start_rep_fr,end_rep_fr)
span_fe_diff_bk = start_rep_bk-end_rep_bk
span_fe_prod_bk = tf.math.multiply(start_rep_bk,end_rep_bk)


span_fe = tf.keras.layers.concatenate([start_rep_fr,
                     end_rep_fr,
                     start_rep_bk,
                     end_rep_bk,
                     span_fe_diff_fr,
                     span_fe_diff_bk,
                     span_fe_prod_fr,
                     span_fe_prod_bk
                    ],2)
bilstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10,return_sequences=True,dropout = 0.1,
                                                            #kernel_initializer=tf.keras.initializers.(mean=0.0,stddev=0.05),
                                                            ),
                                        
                                         merge_mode='ave',
                                         input_shape=(146,40*4))(span_fe)
output = tf.keras.layers.Dense(2,activation='softmax')(bilstm2)

kpe_model = tf.keras.models.Model(inputs=[input_ids,attention_mask,pos_mask], outputs=output)
kpe_model.layers[3].trainable = False

opt = tf.keras.optimizers.Adam(learning_rate=0.00005)
kpe_model.compile(optimizer=opt,
              loss=loss_function,
              metrics=[ac_metrics])

</code></pre>
<p>The output represents the probability of a candidate phrase being a keyphrase. Not sure what part is incorrect here. The model converges within 2-3 steps and forces all probabilities &lt;0.5. Any ideas on fixing this would be helpful.</p>
","11561595","","6664872","","2020-11-27 23:02:45","2020-11-27 23:02:45","Keyphrase extraction with BERT forcing all labels to zero. Seems to be something wrong with the way embeddings are used","<nlp><topic-modeling><bert-language-model><huggingface-transformers><ner>","0","0","","","","CC BY-SA 4.0"
"64564545","1","64569354","","2020-10-27 23:51:42","","1","396","<p>I want to classify a bunch of tweets and therefore I'm using the huggingface implementation of BERT. However I noticed that the deafult BertTokenizer does not use special tokens for urls.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; from transformers import BertTokenizer
&gt;&gt;&gt; tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
&gt;&gt;&gt; tokenizer.tokenize(&quot;https://stackoverflow.com/questions/ask&quot;)
['https', ':', '/', '/', 'stack', '##over', '##flow', '.', 'com', '/', 'questions', '/', 'ask']
</code></pre>
<p>This seems quite inefficent to me. What would be the best way, to encode URLs?</p>
","4334725","","","","","2020-10-28 08:51:46","BERT tokenize URLs","<python><machine-learning><bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"65337804","1","65341640","","2020-12-17 09:21:05","","0","75","<p>I'm creating a model using BertModel to identify answer span (without using BertForQA).</p>
<p>I have an indepent linear layer for determining start and end token respectively. In <strong>init</strong>():</p>
<pre><code>self.start_linear = nn.Linear(h, output_dim)

self.end_linear = nn.Linear(h, output_dim)
</code></pre>
<p>In forward(), I output a predicted start layer and predicted end layer:</p>
<pre><code> def forward(self, input_ids, attention_mask):

 outputs = self.bert(input_ids, attention_mask) # input = bert tokenizer encoding

 lhs = outputs.last_hidden_state # (batch_size, sequence_length, hidden_size)

 out = lhs[:, -1, :] # (batch_size, hidden_dim)

 st = self.start_linear(out)

 end = self.end_linear(out) 



 predict_start = self.softmax(st)

 predict_end = self.softmax(end)

 return predict_start, predict_end
</code></pre>
<p>Then in train_epoch(), I tried to backpropagate the losses separately:</p>
<pre><code>def train_epoch(model, train_loader, optimizer):

 model.train()

 total = 0

 st_loss, st_correct, st_total_loss = 0, 0, 0

 end_loss, end_correct, end_total_loss = 0, 0, 0

 for batch in train_loader:

   optimizer.zero_grad()

   input_ids = batch['input_ids'].to(device)

   attention_mask = batch['attention_mask'].to(device)

   start_idx = batch['start'].to(device)

   end_idx = batch['end'].to(device)

   start, end = model(input_ids=input_ids, attention_mask=attention_mask)


   st_loss = model.compute_loss(start, start_idx)

   end_loss = model.compute_loss(end, end_idx)

   st_total_loss += st_loss.item()

   end_total_loss += end_loss.item()

 # perform backward propagation to compute the gradients

   st_loss.backward()

   end_loss.backward()

 # update the weights

   optimizer.step() 
</code></pre>
<p>But then I got on the line of <code>end_loss.backward()</code>:</p>
<pre><code>Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
</code></pre>
<p>Am I supposed to do the backward pass separately? Or should I do it in another way? Thank you!</p>
","12935622","","","","","2020-12-17 13:27:26","calculate two losses in a model and backpropagate twice","<python><nlp><pytorch><bert-language-model><huggingface-transformers>","1","3","","","","CC BY-SA 4.0"
"66954682","1","","","2021-04-05 14:33:10","","3","2589","<p>I'm trying to get the sentiments for comments with the help of hugging face sentiment analysis pretrained model. It's returning error like <code>Token indices sequence length is longer than the specified maximum sequence length for this model (651 &gt; 512) with Hugging face sentiment classifier</code>.</p>
<p>Below I'm attaching the code please look at it</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import transformers
import pandas as pd

model = AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/Huggingface-Sentiment-Pipeline')
token = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Huggingface-Sentiment-Pipeline')

classifier = pipeline(task='sentiment-analysis', model=model, tokenizer=token)

data = pd.read_csv('/content/drive/MyDrive/DisneylandReviews.csv', encoding='latin-1')

data.head()
</code></pre>
<p>Output is</p>
<pre><code>    Review
0   If you've ever been to Disneyland anywhere you...
1   Its been a while since d last time we visit HK...
2   Thanks God it wasn t too hot or too humid wh...
3   HK Disneyland is a great compact park. Unfortu...
4   the location is not in the city, took around 1...
</code></pre>
<p>Followed by</p>
<pre><code>classifier(&quot;My name is mark&quot;)
</code></pre>
<p>Output is</p>
<pre><code>[{'label': 'POSITIVE', 'score': 0.9953688383102417}]
</code></pre>
<p>Followed by code</p>
<pre><code>basic_sentiment = [i['label'] for i in value if 'label' in i]
basic_sentiment
</code></pre>
<p>Output is</p>
<pre><code>['POSITIVE']
</code></pre>
<p>Appending the total rows to empty list</p>
<pre><code>text = []

for index, row in data.iterrows():
    text.append(row['Review'])
</code></pre>
<p>I'm trying to get the sentiment for all the rows</p>
<pre><code>sent = []

for i in range(len(data)):
    sentiment = classifier(data.iloc[i,0])
    sent.append(sentiment)
</code></pre>
<p>The error is :</p>
<pre><code>Token indices sequence length is longer than the specified maximum sequence length for this model (651 &gt; 512). Running this sequence through the model will result in indexing errors
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-19-4bb136563e7c&gt; in &lt;module&gt;()
      2 
      3 for i in range(len(data)):
----&gt; 4     sentiment = classifier(data.iloc[i,0])
      5     sent.append(sentiment)

11 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1914         # remove once script supports set_grad_enabled
   1915         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1916     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1917 
   1918 

IndexError: index out of range in self
</code></pre>
","15224778","","","","","2021-06-21 14:45:27","Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512) with Hugging face sentiment classifier","<deep-learning><nlp><sentiment-analysis><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"67340574","1","","","2021-04-30 21:19:29","","0","43","<p>I want to build a language model (i.e., next word prediction) using HuggingFace library. I want to use my own dataset corpus with transfer learning. Is there any tutorial series present or any blog written on this ? any kind of help would be greatly appreciated.</p>
<p>I have found most of the project doing same task using LSTM. but I want to know how can I build a language model using transformers.</p>
","6805178","","6805178","","2021-05-01 20:11:23","2021-05-01 20:11:23","Build Language model (i.e. next word predictor) using HuggingFace library","<nlp><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"65644994","1","","","2021-01-09 16:33:06","","0","15","<p>As part of my MSc project, I am attempting to find a method to gradually increase the dot product between the Q and K vectors within a Transformer, as a way of manually altering the self-attention weights post-training. So far, I have found an equation which allows me to interpolate between the two vectors as a way of making their direction and magnitude more or less similar, however, this does not always result in a higher dot product between the two. I have put my python implementation of this equation below:</p>
<pre><code>t = 0.5 # How 'close' we want k to be to q.

new_k = q * t + (1 - t) * k
</code></pre>
<p>From what I understand now, if I was to increase the dot product, I would need a method which makes K more similar to Q in terms of direction, whilst simultaneously increasing the magnitude of K (if I wanted to lower the dot product, I would do the opposite). Does anyone know of a way I could do this? Could I just use the interpolation equation and multiply the resulting vector by a scalar value to increase the magnitude? I feel as if there is a more elegant/suitable way of doing this.</p>
<pre><code>t = 0.5 # How 'close' we want k to be to q.

new_k = q * t + (1 - t) * k

new_k *= some_scalar_value
</code></pre>
<p>Thank you!</p>
","8405910","","","","","2021-01-09 16:33:06","Gradually increase the dot product between transformer Q and K vectors manually?","<python><vector><linear-algebra><word-embedding><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"67345611","1","","","2021-05-01 11:13:43","","2","62","<p>Hello I'm using t5 pretrained abstractive summarization how I can evaluate the summary output accuracy IN short how much percent my model are accurate</p>
","12201916","","","","","2021-06-07 11:39:18","Abstractive Text summarization using T5 pre-trained model","<huggingface-transformers><summarization>","1","1","","","","CC BY-SA 4.0"
"66204936","1","","","2021-02-15 08:42:10","","1","66","<p>I fine-tuned a bert (or roberta) model for sequence classification. Can I fine-tune the same model for a different task (QA or Sentiment Analysis)?</p>
","10550623","","","","","2021-02-15 08:42:10","How to use fine-tuned bert model in other tasks?","<bert-language-model><huggingface-transformers><roberta-language-model>","0","1","","","","CC BY-SA 4.0"
"66078771","1","","","2021-02-06 15:49:50","","0","38","<p>Using lighttag or DocAnno (just like many other tools) I created a few annotated files (for a NER like token classifiaction task) and exported them in JSON format.</p>
<p>I want to import these into a transformers trainer. Is there a guide, or a convertor for the mission? Most of the examples I've found describe how to import existing datasets (in the cloud) or conll files. And since I am kind of newbie, I do not understand how/if they are interchangable.</p>
","4226806","","","","","2021-02-06 15:49:50","How do you import annotated text into JSON file into Huggingface Transformer","<nlp><huggingface-transformers><conll>","0","1","","","","CC BY-SA 4.0"
"66956460","1","","","2021-04-05 16:40:57","","1","96","<p>I'm trying to use a GPT language model and get the weights it assigns to each word in the last state of text generation. My model is a GPT2 from the transformers library. Below is how I call the pretrained model:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(
&quot;HooshvareLab/gpt2-fa-poetry&quot;
) 

model = AutoModelForCausalLM.from_pretrained(
    &quot;HooshvareLab/gpt2-fa-poetry&quot;
)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

model = model.to(device)
</code></pre>
<p>My goal is to use this information from the last layer of this model (a matrix with the length of vocabulary after the softmax activation) and use it in combination with another model.</p>
<p>I'm trying to do this in TensorFlowPlease, but share your comments if you think there are easier and more convenient ways of doing this in PyTorch.</p>
","11810876","","11810876","","2021-04-06 11:37:47","2021-04-06 11:37:47","Huggingface GPT transformers layers output","<tensorflow><nlp><huggingface-transformers><language-model><gpt-2>","0","3","","","","CC BY-SA 4.0"
"66950157","1","66951495","","2021-04-05 08:24:24","","1","89","<p>I have a classifier on top of BERT, and I would like to see the predict probability for creating the ROC curve. How do I get the predict proba?. The predicted probas will be used to calculate the TPR FPR and threshold for ROC curve. <br>
here is the code</p>
<pre><code>class BertBinaryClassifier(nn.Module):
    def __init__(self, dropout=0.1):
        super(BertBinaryClassifier, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(768, 1)
        self.sigmoid = nn.Sigmoid()
        
    
    def forward(self, tokens, masks=None):
        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)
        dropout_output = self.dropout(pooled_output)
        linear_output = self.linear(dropout_output)
        prediction = self.sigmoid(linear_output)
        return prediction
# Config setting
BATCH_SIZE = 4
EPOCHS = 5
# Making dataloaders
train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)
train_sampler =  torch.utils.data.RandomSampler(train_dataset)
train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)
test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)
test_sampler =  torch.utils.data.SequentialSampler(test_dataset)
test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)

bert_clf = BertBinaryClassifier()
bert_clf = bert_clf.cuda()
#wandb.watch(bert_clf)
optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)

# training 
for epoch_num in range(EPOCHS):
    bert_clf.train()
    train_loss = 0
    for step_num, batch_data in enumerate(train_dataloader):
        token_ids, masks, labels = tuple(t for t in batch_data)
        token_ids, masks, labels = token_ids.to(device), masks.to(device), labels.to(device)
        preds = bert_clf(token_ids, masks)
        loss_func = nn.BCELoss()
        batch_loss = loss_func(preds, labels)
        train_loss += batch_loss.item()
        bert_clf.zero_grad()
        batch_loss.backward()
        optimizer.step()
        #wandb.log({&quot;Training loss&quot;: train_loss})
        print('Epoch: ', epoch_num + 1)
        print(&quot;\r&quot; + &quot;{0}/{1} loss: {2} &quot;.format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))

# evaluating on test
bert_clf.eval()
bert_predicted = []
all_logits = []
probs=[]
with torch.no_grad():
    test_loss = 0
    for step_num, batch_data in enumerate(test_dataloader):
        token_ids, masks, labels = tuple(t for t in batch_data)
        token_ids, masks, labels = token_ids.to(device), masks.to(device), labels.to(device)
        logits = bert_clf(token_ids, masks)
        pr=logits.ravel()
        probs+=pr
        loss_func = nn.BCELoss()
        loss = loss_func(logits, labels)
        test_loss += loss.item()
        numpy_logits = logits.cpu().detach().numpy()
        #print(numpy_logits)
        #wandb.log({&quot;Testing loss&quot;: test_loss})
        bert_predicted += list(numpy_logits[:, 0] &gt; 0.5)
        all_logits += list(numpy_logits[:, 0])
</code></pre>
<p>I am able to get the prediction score to calculate the accuracy or f1 score. But not the probability for creating ROC curve.
Thanks</p>
","14259188","","9067615","","2021-05-09 13:03:25","2021-05-09 13:03:25","Getting predict.proba from BERT classififer","<python><python-3.x><pytorch><bert-language-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"59465885","1","","","2019-12-24 08:21:35","","2","1384","<p>When using Hugginface Transformers on GLUE task, I've got the error <code>AttributeError: 'MrpcProcessor' object has no attribute 'tfds_map'</code></p>

<p>I suspect a problem of compatibility.</p>
","1209842","","1150683","","2020-01-16 08:59:28","2020-06-28 15:35:45","Huggingface Transformers - AttributeError: 'MrpcProcessor' object has no attribute 'tfds_map'","<transformer><bert-language-model><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"66927834","1","","","2021-04-03 04:51:51","","1","22","<p>I am running the NER tutorials provided by HuggingFace official repo (see <a href=""https://github.com/huggingface/transformers/blob/master/examples/token-classification/run_ner.py"" rel=""nofollow noreferrer"">here</a>). The following info is printed during the training process</p>
<pre><code>{'loss': 0.0724, 'learning_rate': 3.8425925925925924e-05, 'epoch': 0.69}        
{'loss': 0.0315, 'learning_rate': 2.6851851851851855e-05, 'epoch': 1.39}        
{'loss': 0.0198, 'learning_rate': 1.527777777777778e-05, 'epoch': 2.08}         
{'loss': 0.0147, 'learning_rate': 3.7037037037037037e-06, 'epoch': 2.78}        
{'train_runtime': 228.6028, 'train_samples_per_second': 9.449, 'epoch': 3.0} 
</code></pre>
<p>Now I would like to also print out validation metrics after each epoch. I checked out the <code>TrainingArguments()</code> <a href=""https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments"" rel=""nofollow noreferrer"">doc</a>, but it seems that the only related options are the following and <strong>none</strong> of them are related to the contents of logging</p>
<ul>
<li><code>logging_dir</code></li>
<li><code>logging_strategy</code></li>
<li><code>logging_first_step</code></li>
<li><code>logging_steps</code></li>
</ul>
","7784797","","","","","2021-04-03 04:51:51","Customized logging in HuggingFace","<huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"58417374","1","58427691","","2019-10-16 15:57:36","","8","5720","<p>I fine-tuned a pretrained BERT model in Pytorch using huggingface transformer. All the training/validation is done on a GPU in cloud.</p>
<p>At the end of the training, I save the model and tokenizer like below:</p>
<pre><code>best_model.save_pretrained('./saved_model/')
tokenizer.save_pretrained('./saved_model/')
</code></pre>
<p>This creates below files in the <code>saved_model</code> directory:</p>
<pre><code>config.json
added_token.json
special_tokens_map.json
tokenizer_config.json
vocab.txt
pytorch_model.bin
</code></pre>
<p>Now, I download the <code>saved_model</code> directory in my computer and want to load the model and tokenizer. I can load the model like below</p>
<p><code>model = torch.load('./saved_model/pytorch_model.bin',map_location=torch.device('cpu'))</code></p>
<p>But how do I load the tokenizer? I am new to pytorch and not sure because there are multiple files. Probably I am not saving the model in the right way?</p>
","1450312","","6622587","","2021-07-11 19:00:53","2021-07-11 19:01:22","How to load the saved tokenizer from pretrained model","<machine-learning><pytorch><huggingface-transformers>","1","3","1","","","CC BY-SA 4.0"
"64684506","1","64687051","","2020-11-04 17:01:51","","0","457","<p>This is very basic question, but I spend hours struggling to find the answer. I built NER using Hugginface transformers.</p>
<p>Say I have input sentence</p>
<pre class=""lang-py prettyprint-override""><code>input = &quot;Damien Hirst oil in canvas&quot;
</code></pre>
<p>I tokenize it to get</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')
tokenized = tokenizer.encode(input) #[101, 12587, 7632, 12096, 3514, 1999, 10683, 102]
</code></pre>
<p>Feed tokenized sentence to the model to get predicted tags for the tokens</p>
<pre class=""lang-py prettyprint-override""><code>['B-ARTIST' 'B-ARTIST' 'I-ARTIST' 'I-ARTIST' 'B-MEDIUM' 'I-MEDIUM'
 'I-MEDIUM' 'B-ARTIST']
</code></pre>
<p><code>prediction</code> comes as output from the model. It assigns tags to different tokens.</p>
<p>How can I recombine this data to obtain tags for words instead of tokens? So I would know that</p>
<pre><code>&quot;Damien Hirst&quot; = ARTIST
&quot;Oil in canvas&quot; = MEDIUM
</code></pre>
","11022906","","11022906","","2020-11-04 17:07:10","2020-11-04 21:27:43","Transformers get named entity prediction for words instead of tokens","<nlp><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67494009","1","","","2021-05-11 20:36:45","","0","84","<p>Huggingface transformer <a href=""https://huggingface.co/transformers/main_classes/trainer.html"" rel=""nofollow noreferrer"">Trainer</a> says &quot;your model can accept multiple label arguments (use the label_names in your TrainingArguments to indicate their name to the Trainer)&quot;. Anyone know the correct format to pass multiple labels into the trainer? It doesn't seem clear from the documentation. I would like to avoid putting all of the labels into one label column, since some have multiple labels at the same time, so there would be all the combinations of column names (there is a 1 in the row for each label that a comment applies to, so like one-hot encoding with possibly multiple 1's in a row).</p>
","","user15900546","","user15900546","2021-05-14 22:21:16","2021-05-14 22:21:16","How to pass multiple labels into Huggingface transformers Trainer","<huggingface-transformers>","0","4","","","","CC BY-SA 4.0"
"65332165","1","","","2020-12-16 22:41:32","","0","585","<p>I'm using <a href=""https://huggingface.co/"" rel=""nofollow noreferrer"">HuggingFace</a>'s Transformer's library and Iâ€™m trying to fine-tune a pre-trained NLI model (<code>ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli</code>) on a dataset of around 276.000 hypothesis-premise pairs. Iâ€™m following the instructions from the docs <a href=""https://huggingface.co/transformers/custom_datasets.html"" rel=""nofollow noreferrer"">here</a> and <a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">here</a>. I have the impression that the fine-tuning works (it does the training and saves the checkpoints), but <code>trainer.train()</code> and <code>trainer.evaluate()</code> return &quot;nan&quot; for the loss.</p>
<p><strong>What I've tried:</strong></p>
<ul>
<li>I tried using both <code>ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli</code> and <code>facebook/bart-large-mnli</code> to make sure that it's not linked to specific model, but I get the issue for both models</li>
<li>I tried following the advice in this <a href=""https://github.com/huggingface/transformers/issues/1727"" rel=""nofollow noreferrer"">related github issue</a>, but adding <code>num_labels=3</code> to the config file does not solve the issue. (I think my issue is different because the models are already fine-tuned on NLI in my case)</li>
<li>I tried many different ways of changing my input data because I suspected that there could be an issue with my input data, but I also couldn't solve it that way.</li>
<li><strong>The probable source of the issue:</strong> <em>I inspected the prediction output from the model during training and the weird thing is the prediction value always seems to be &quot;0&quot; (entailment) in 100% of cases (see printed output in the code below). This is clearly an error. I think the source for this is that the logits the model seems to return during training are <code>torch.tensor([[np.nan, np.nan, np.nan]])</code> and when you apply <code>.argmax(-1)</code> to this, you get torch.tensor(0). The big mystery for me is why the logits would become &quot;nan&quot;, because the model does not do that when I use the same input data only outside of the trainer.
=&gt; Does anyone know where this issues comes from? See my code below.</em></li>
</ul>
<p>Thanks a lot in advance for any suggestion!</p>
<h3>Here is my code:</h3>
<pre><code>### load model &amp; tokenize
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

max_length = 256
hg_model_hub_name = &quot;ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli&quot;
# also tried: hg_model_hub_name = &quot;facebook/bart-large-mnli&quot;
tokenizer = AutoTokenizer.from_pretrained(hg_model_hub_name)
model = AutoModelForSequenceClassification.from_pretrained(hg_model_hub_name)
model.config

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
print(f&quot;Device: {device}&quot;)
if device == &quot;cuda&quot;:
  model = model.half()
model.to(device)
model.train();

#... some data preprocessing

encodings_train = tokenizer(premise_train, hypothesis_train, return_tensors=&quot;pt&quot;, max_length=max_length,
                            return_token_type_ids=True, truncation=False, padding=True)
encodings_val = tokenizer(premise_val, hypothesis_val, return_tensors=&quot;pt&quot;, max_length=max_length,
                          return_token_type_ids=True, truncation=False, padding=True)
encodings_test = tokenizer(premise_test, hypothesis_test, return_tensors=&quot;pt&quot;, max_length=max_length,
                           return_token_type_ids=True, truncation=False, padding=True)


### create pytorch dataset object
class XDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx):
        item = {key: torch.as_tensor(val[idx]) for key, val in self.encodings.items()}
        #item = {key: torch.as_tensor(val[idx]).to(device) for key, val in self.encodings.items()}
        item['labels'] = torch.as_tensor(self.labels[idx])
        #item['labels'] = self.labels[idx]
        return item
    def __len__(self):
        return len(self.labels)

dataset_train = XDataset(encodings_train, label_train)
dataset_val = XDataset(encodings_val, label_val)
dataset_test = XDataset(encodings_test, label_test)

# compute metrics with trainer
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
def compute_metrics(pred):
    labels = pred.label_ids
    print(labels)
    preds = pred.predictions.argmax(-1)
    print(preds)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', pos_label=0)
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }


## training
from transformers import Trainer, TrainingArguments

# https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=1,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=8,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=100,
)

trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=dataset_train,         # training dataset
    eval_dataset=dataset_val             # evaluation dataset
)

trainer.train()
# output: TrainOutput(global_step=181, training_loss=nan)
trainer.evaluate()
# output: 
[2 2 2 0 0 2 2 2 0 2 0 0 2 2 2 2 0 2 0 2 2 2 2 0 2 0 2 0 0 2 0 0 2 0 0 0 2
 0 2 0 0 0 0 0 2 0 0 2 2 2 0 2 2 2 2 2 0 0 0 0 2 0 0 0 2 2 0 0 0 2 0 0 0 2
 2 0 2 0 0 2 2 2 0 2 2 0 0 0 0 0 0 0 2 0 0 0 0 2 0 2 2 0 2 0 0 2 2 2 2 2 2
 2 0 0 0 0 2 0 0 2 0 0 0 0 2 2 2 0 0 0 0 0 2 0 0 2 0 2 0 2 0 2 0 0 2 2 0 0
 2 2 2 2 2 2 0 0 2 2 2 2 0 2 0 0 2 2 2 0 0 2 0 2 0 2 0 0 0 0 0 0 2 0 0 2 2
 0 2 2 2 0 2 2 0 2 2 2 2 2 2 0 0 2 0 0 2 2 0 0 0 2 0 2 2 2 0 0 0 0 0 0 0 0
 2 0 2 2 2 0 2 0 0 2 0 2 2 0 0 0 0 2 2 2 0 0 0 2 2 2 2 0 2 0 2 2 2]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

{'epoch': 1.0,
 'eval_accuracy': 0.5137254901960784,
 'eval_f1': 0.6787564766839378,
 'eval_loss': nan,
 'eval_precision': 0.5137254901960784,
 'eval_recall': 1.0}
</code></pre>
<p><strong>Edit:</strong>  I've also opened a github issue with a but more detailed description of the issue here: <a href=""https://github.com/huggingface/transformers/issues/9160"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/9160</a></p>
","10667419","","10667419","","2020-12-16 23:59:04","2020-12-17 13:47:43","Loss is â€œnanâ€ when fine-tuning HuggingFace NLI model (both RoBERTa/BART)","<python><nlp><pytorch><huggingface-transformers><huggingface-tokenizers>","1","3","","","","CC BY-SA 4.0"
"64675655","1","64728238","","2020-11-04 07:21:22","","0","1207","<p>I am fine-tuning BERT on a financial news dataset.
Unfortunately BERT seems to be trapped in a local minimum. It is content with learning to always predict the same class.</p>
<ul>
<li>balancing the dataset didnt work</li>
<li>tuning parameters didnt work as well</li>
</ul>
<p>I am honestly not sure what is causing this problem. With the simpletransformers library I am getting very good results. I would really appreciate if somebody could help me. thanks a lot!</p>
<p>Full code on github:
<a href=""https://github.com/Bene939/BERT_News_Sentiment_Classifier"" rel=""nofollow noreferrer"">https://github.com/Bene939/BERT_News_Sentiment_Classifier</a></p>
<p>Code:</p>
<pre><code>from transformers import BertForSequenceClassification, AdamW, BertTokenizer, get_linear_schedule_with_warmup, Trainer, TrainingArguments
import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
import pandas as pd
from pathlib import Path
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
from torch.nn import functional as F
from collections import defaultdict
import random


#defining tokenizer, model and optimizer
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)


if torch.cuda.is_available():
  print(&quot;\nUsing: &quot;, torch.cuda.get_device_name(0))
  device = torch.device('cuda')
else:
  print(&quot;\nUsing: CPU&quot;)
  device = torch.device('cpu')
model = model.to(device)


#loading dataset
labeled_dataset = &quot;news_headlines_sentiment.csv&quot;
labeled_dataset_file = Path(labeled_dataset)
file_loaded = False
while not file_loaded:
  if labeled_dataset_file.exists():
    labeled_dataset = pd.read_csv(labeled_dataset_file)
    file_loaded = True
    print(&quot;Dataset Loaded&quot;)
  else:
    print(&quot;File not Found&quot;)
print(labeled_dataset)

#counting sentiments
negative = 0
neutral = 0
positive = 0
for idx, row in labeled_dataset.iterrows():
  if row[&quot;sentiment&quot;] == 0:
    negative += 1
  elif row[&quot;sentiment&quot;] == 1:
    neutral += 1
  else:
    positive += 1
print(&quot;Unbalanced Dataset&quot;)
print(&quot;negative: &quot;, negative)
print(&quot;neutral: &quot;, neutral)
print(&quot;positive: &quot;, positive)

#balancing dataset to 1/3 per sentiment
for idx, row in labeled_dataset.iterrows():
  if row[&quot;sentiment&quot;] == 0:
    if negative - neutral != 0:
      index_name = labeled_dataset[labeled_dataset[&quot;news&quot;] == row[&quot;news&quot;]].index
      labeled_dataset.drop(index_name, inplace=True)
      negative -= 1
  elif row[&quot;sentiment&quot;] == 2:
    if positive - neutral != 0:
      index_name = labeled_dataset[labeled_dataset[&quot;news&quot;] == row[&quot;news&quot;]].index
      labeled_dataset.drop(index_name, inplace=True)
      positive -= 1

#custom dataset class
class NewsSentimentDataset(torch.utils.data.Dataset):
  def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

  def __getitem__(self, idx):
      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
      item['labels'] = torch.tensor(self.labels[idx])
      return item

  def __len__(self):
      return len(self.labels)

#method for tokenizing dataset list
def tokenize_headlines(headlines, labels, tokenizer):

  encodings = tokenizer.batch_encode_plus(
      headlines,
      add_special_tokens = True,
      truncation = True,
      padding = 'max_length',
      return_attention_mask = True,
      return_token_type_ids = True
  )

  dataset = NewsSentimentDataset(encodings, labels)
  return dataset

#splitting dataset into training and validation set
#load news sentiment dataset
all_headlines = labeled_dataset['news'].tolist()
all_labels = labeled_dataset['sentiment'].tolist()

train_headlines, val_headlines, train_labels, val_labels = train_test_split(all_headlines, all_labels, test_size=.2)

val_dataset = tokenize_headlines(val_headlines, val_labels, tokenizer)
train_dataset = tokenize_headlines(train_headlines, val_labels, tokenizer)

#data loader
train_batch_size = 8
val_batch_size = 8

train_data_loader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle=True)
val_data_loader = DataLoader(val_dataset, batch_size = val_batch_size, sampler=SequentialSampler(val_dataset))

#optimizer and scheduler
num_epochs = 1
num_steps = len(train_data_loader) * num_epochs
optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_steps*0.06, num_training_steps=num_steps)

#training and evaluation
seed_val = 64

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

for epoch in range(num_epochs):

  print(&quot;\n###################################################&quot;)
  print(&quot;Epoch: {}/{}&quot;.format(epoch+1, num_epochs))
  print(&quot;###################################################\n&quot;)

  #training phase
 
  average_train_loss = 0
  average_train_acc = 0
  model.train() 
  for step, batch in enumerate(train_data_loader):
      
      
      input_ids = batch['input_ids'].to(device)
      attention_mask = batch['attention_mask'].to(device)
      labels = batch['labels'].to(device)
      token_type_ids = batch['token_type_ids'].to(device)


      outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids = token_type_ids)

      loss = F.cross_entropy(outputs[0], labels)
      average_train_loss += loss

      if step % 40 == 0:
        print(&quot;Training Loss: &quot;, loss)

      logits = outputs[0].detach().cpu().numpy()
      label_ids = labels.to('cpu').numpy()

      average_train_acc += sklearn.metrics.accuracy_score(label_ids, np.argmax(logits, axis=1))
      print(&quot;predictions: &quot;,np.argmax(logits, axis=1))
      print(&quot;labels:      &quot;,label_ids)
      print(&quot;#############&quot;)
      optimizer.zero_grad()
      loss.backward()
      #maximum gradient clipping
      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
      
      optimizer.step()
      scheduler.step()
      model.zero_grad()

  average_train_loss = average_train_loss / len(train_data_loader)
  average_train_acc = average_train_acc / len(train_data_loader)
  print(&quot;======Average Training Loss: {:.5f}======&quot;.format(average_train_loss))
  print(&quot;======Average Training Accuracy: {:.2f}%======&quot;.format(average_train_acc*100))

  #validation phase
  average_val_loss = 0
  average_val_acc = 0
  model.eval()
  for step,batch in enumerate(val_data_loader):
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['labels'].to(device)
    token_type_ids = batch['token_type_ids'].to(device)

    pred = []
    with torch.no_grad():
      

      outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

      loss = F.cross_entropy(outputs[0], labels)
      average_val_loss += loss

      logits = outputs[0].detach().cpu().numpy()
      label_ids = labels.to('cpu').numpy()
      print(&quot;predictions: &quot;,np.argmax(logits, axis=1))
      print(&quot;labels:      &quot;,label_ids)
      print(&quot;#############&quot;)

      average_val_acc += sklearn.metrics.accuracy_score(label_ids, np.argmax(logits, axis=1))

  average_val_loss = average_val_loss / len(val_data_loader)
  average_val_acc = average_val_acc / len(val_data_loader)

  print(&quot;======Average Validation Loss: {:.5f}======&quot;.format(average_val_loss))
  print(&quot;======Average Validation Accuracy: {:.2f}%======&quot;.format(average_val_acc*100))
###################################################
Epoch: 1/1
###################################################

Training Loss:  tensor(1.1006, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)
predictions:  [1 0 2 0 0 0 2 0]
labels:       [2 0 1 1 0 1 0 1]
#############
predictions:  [2 2 0 0 0 2 0 0]
labels:       [1 2 1 0 2 0 1 2]
#############
predictions:  [0 0 0 0 1 0 0 1]
labels:       [0 1 1 0 1 1 2 0]
#############
predictions:  [0 0 0 2 0 1 0 0]
labels:       [0 0 0 2 0 0 2 1]
#############
predictions:  [1 0 0 0 0 0 2 0]
labels:       [0 2 2 1 0 0 0 0]
#############
predictions:  [0 0 0 0 0 1 0 0]
labels:       [1 0 2 2 2 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 2 2 2 0 2 0]
#############
predictions:  [0 1 0 0 0 0 0 0]
labels:       [2 2 0 2 0 0 0 1]
#############
predictions:  [0 0 0 0 0 2 0 1]
labels:       [0 1 0 2 2 0 1 2]
#############
predictions:  [0 0 2 0 0 0 1 0]
labels:       [0 0 0 1 2 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 1 0 1 0 1 1]
#############
predictions:  [0 2 0 0 0 0 0 0]
labels:       [2 2 0 1 0 1 2 1]
#############
predictions:  [0 1 0 0 0 0 1 2]
labels:       [2 2 1 0 2 0 0 2]
#############
predictions:  [0 0 1 1 1 1 0 1]
labels:       [1 2 1 1 1 1 2 2]
#############
predictions:  [1 0 0 0 0 1 2 1]
labels:       [1 0 1 1 0 0 0 2]
#############
predictions:  [0 1 1 1 1 0 2 1]
labels:       [2 2 1 2 2 1 1 2]
#############
predictions:  [0 0 1 0 1 1 0 0]
labels:       [1 0 0 1 0 1 0 2]
#############
predictions:  [1 2 0 0 1 2 0 0]
labels:       [0 2 2 1 2 0 1 0]
#############
predictions:  [0 2 1 1 0 1 1 0]
labels:       [2 2 0 1 1 0 1 2]
#############
predictions:  [1 0 1 1 1 1 1 0]
labels:       [0 2 0 1 0 1 2 2]
#############
predictions:  [0 2 1 2 0 0 1 1]
labels:       [2 1 1 1 1 2 2 0]
#############
predictions:  [0 1 2 2 2 1 1 2]
labels:       [2 2 1 1 2 1 0 1]
#############
predictions:  [2 2 2 1 2 1 1 1]
labels:       [0 1 1 0 0 2 2 1]
#############
predictions:  [1 2 2 2 1 2 1 2]
labels:       [0 0 0 0 2 0 1 2]
#############
predictions:  [2 1 1 1 2 2 2 2]
labels:       [1 0 2 2 1 0 0 0]
#############
predictions:  [2 1 2 2 2 1 2 2]
labels:       [2 1 1 1 1 1 2 2]
#############
predictions:  [1 1 0 2 1 2 1 2]
labels:       [2 2 0 2 0 1 2 0]
#############
predictions:  [0 1 1 2 0 1 2 1]
labels:       [2 2 2 1 2 2 0 1]
#############
predictions:  [2 1 1 1 1 2 1 1]
labels:       [0 1 1 2 1 0 0 2]
#############
predictions:  [1 2 2 0 1 1 1 2]
labels:       [0 1 2 1 2 1 0 1]
#############
predictions:  [0 1 1 1 1 1 1 0]
labels:       [0 2 0 1 1 2 2 2]
#############
predictions:  [1 2 1 1 2 1 1 0]
labels:       [0 2 2 2 0 0 1 0]
#############
predictions:  [2 2 2 1 2 1 1 2]
labels:       [2 2 1 2 1 0 0 0]
#############
predictions:  [2 2 1 2 2 2 1 2]
labels:       [1 1 2 2 2 0 2 1]
#############
predictions:  [2 2 2 2 2 0 2 2]
labels:       [2 2 1 2 0 1 1 2]
#############
predictions:  [1 1 2 1 2 2 0 1]
labels:       [2 1 1 1 0 0 2 2]
#############
predictions:  [2 1 2 2 2 2 1 0]
labels:       [0 2 0 2 0 0 0 0]
#############
predictions:  [2 2 2 2 2 2 2 2]
labels:       [1 1 0 2 0 1 2 1]
#############
predictions:  [2 2 2 2 1 2 2 2]
labels:       [1 0 0 1 1 0 0 0]
#############
predictions:  [2 2 2 1 2 2 2 2]
labels:       [1 0 1 1 0 2 2 0]
#############
Training Loss:  tensor(1.1104, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)
predictions:  [2 0 1 2 1 2 2 0]
labels:       [2 2 0 0 1 0 0 2]
#############
predictions:  [0 2 2 0 2 1 1 1]
labels:       [0 0 0 1 0 0 1 0]
#############
predictions:  [0 2 2 0 1 1 1 2]
labels:       [2 1 1 1 2 2 1 0]
#############
predictions:  [2 1 1 2 2 0 2 0]
labels:       [1 2 1 2 1 0 2 1]
#############
predictions:  [0 2 2 0 0 2 1 2]
labels:       [0 0 2 2 0 0 2 0]
#############
predictions:  [0 0 1 2 2 0 2 2]
labels:       [0 0 0 0 0 0 0 0]
#############
predictions:  [1 1 2 1 2 0 1 2]
labels:       [0 0 2 0 0 0 1 1]
#############
predictions:  [0 0 2 1 0 2 0 1]
labels:       [1 1 2 1 1 0 2 0]
#############
predictions:  [0 0 0 0 1 0 0 0]
labels:       [2 2 1 1 2 1 1 1]
#############
predictions:  [0 0 0 0 1 0 0 0]
labels:       [1 1 2 2 1 1 2 0]
#############
predictions:  [0 0 0 0 0 1 1 1]
labels:       [2 0 1 1 0 1 2 2]
#############
predictions:  [0 0 1 0 0 1 2 1]
labels:       [1 2 0 2 2 0 2 1]
#############
predictions:  [1 1 1 1 0 1 0 1]
labels:       [2 0 1 0 1 0 1 2]
#############
predictions:  [1 2 2 0 0 0 1 1]
labels:       [2 0 0 2 1 2 2 2]
#############
predictions:  [1 0 2 1 0 2 2 0]
labels:       [0 0 2 1 2 1 1 1]
#############
predictions:  [0 0 0 1 1 1 1 1]
labels:       [1 2 1 0 0 0 1 0]
#############
predictions:  [1 1 1 0 1 1 0 1]
labels:       [0 2 1 2 1 2 2 0]
#############
predictions:  [2 1 0 1 1 2 0 0]
labels:       [0 1 0 0 1 2 0 2]
#############
predictions:  [0 1 1 0 0 1 0 1]
labels:       [1 0 0 2 2 1 1 2]
#############
predictions:  [1 1 1 1 1 1 1 1]
labels:       [2 0 1 0 2 0 0 2]
#############
predictions:  [1 0 0 1 0 1 0 2]
labels:       [1 0 0 1 1 2 2 1]
#############
predictions:  [1 1 1 1 1 1 0 0]
labels:       [1 1 0 2 1 0 2 0]
#############
predictions:  [1 1 2 1 0 1 0 0]
labels:       [0 2 1 2 1 1 0 2]
#############
predictions:  [1 1 0 0 1 2 1 1]
labels:       [0 2 1 0 2 2 0 1]
#############
predictions:  [0 1 1 0 0 1 0 1]
labels:       [0 0 1 2 2 0 1 2]
#############
predictions:  [1 0 2 2 2 1 1 0]
labels:       [2 2 1 0 0 1 1 2]
#############
predictions:  [1 2 2 1 1 2 1 1]
labels:       [1 0 0 1 0 0 0 0]
#############
predictions:  [0 2 0 2 2 0 2 2]
labels:       [2 0 0 0 2 1 1 2]
#############
predictions:  [0 0 1 0 1 0 2 2]
labels:       [0 0 1 0 1 0 2 0]
#############
predictions:  [0 2 0 1 1 2 2 0]
labels:       [0 2 0 2 0 2 0 0]
#############
predictions:  [2 2 2 2 2 2 2 1]
labels:       [2 2 1 1 0 0 2 2]
#############
predictions:  [2 0 0 2 2 1 1 0]
labels:       [1 0 0 1 0 2 1 2]
#############
predictions:  [2 0 0 2 0 2 2 0]
labels:       [2 2 2 2 0 1 1 1]
#############
predictions:  [0 2 2 0 2 2 0 0]
labels:       [1 0 1 2 0 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 2]
labels:       [2 1 1 0 0 0 1 2]
#############
predictions:  [2 0 2 0 2 1 0 2]
labels:       [2 1 1 2 1 1 0 0]
#############
predictions:  [1 1 2 0 2 0 2 2]
labels:       [0 2 1 2 1 2 1 0]
#############
predictions:  [2 0 1 1 0 2 0 0]
labels:       [2 1 0 1 1 0 2 0]
#############
predictions:  [2 0 0 2 0 2 1 0]
labels:       [0 0 0 0 2 1 0 1]
#############
predictions:  [1 2 1 0 0 2 0 2]
labels:       [2 0 2 1 0 0 1 1]
#############
Training Loss:  tensor(1.1162, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)
predictions:  [2 0 0 1 1 1 0 1]
labels:       [0 1 1 1 1 2 2 1]
#############
predictions:  [0 2 0 1 2 0 0 1]
labels:       [2 2 1 0 1 0 0 0]
#############
predictions:  [0 0 1 0 0 0 0 1]
labels:       [1 0 2 0 0 2 2 0]
#############
predictions:  [2 1 2 2 0 1 2 0]
labels:       [2 0 1 0 2 1 0 1]
#############
predictions:  [1 0 0 2 0 0 1 1]
labels:       [2 2 0 2 0 2 0 0]
#############
predictions:  [0 0 1 0 0 0 0 0]
labels:       [2 2 2 1 2 2 2 2]
#############
predictions:  [0 0 1 1 0 1 1 0]
labels:       [2 1 1 1 0 2 1 0]
#############
predictions:  [0 0 0 1 0 0 1 0]
labels:       [2 0 2 2 0 0 1 2]
#############
predictions:  [1 0 1 0 0 2 0 0]
labels:       [1 1 2 0 0 1 0 0]
#############
predictions:  [2 1 0 0 0 1 0 0]
labels:       [1 2 0 0 0 0 0 0]
#############
predictions:  [0 2 0 0 0 0 0 0]
labels:       [2 0 1 1 2 2 1 1]
#############
predictions:  [0 1 0 0 0 1 0 2]
labels:       [0 2 1 1 0 0 1 2]
#############
predictions:  [0 2 1 0 0 1 1 1]
labels:       [1 1 0 2 0 1 1 0]
#############
predictions:  [0 1 1 0 0 0 1 0]
labels:       [0 0 1 0 1 2 1 1]
#############
predictions:  [0 1 1 0 1 0 0 0]
labels:       [0 1 1 1 2 2 2 0]
#############
predictions:  [0 0 0 0 1 1 0 0]
labels:       [2 0 2 2 1 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 0 2 2 0 1 1]
#############
predictions:  [0 1 0 0 0 0 0 0]
labels:       [0 2 0 1 1 2 0 2]
#############
predictions:  [1 1 0 1 0 1 0 2]
labels:       [1 2 0 0 2 2 2 1]
#############
predictions:  [1 1 0 0 0 1 2 1]
labels:       [0 0 1 2 2 1 2 2]
#############
predictions:  [1 1 1 0 1 1 2 0]
labels:       [0 0 0 2 0 1 0 2]
#############
predictions:  [0 1 0 0 1 1 2 1]
labels:       [2 0 0 1 2 2 1 2]
#############
predictions:  [1 0 0 0 1 0 0 1]
labels:       [1 2 2 2 2 1 0 1]
#############
predictions:  [2 0 0 0 0 0 0 0]
labels:       [1 2 0 2 2 1 1 1]
#############
predictions:  [2 0 1 1 0 0 1 0]
labels:       [0 0 0 0 2 2 1 1]
#############
predictions:  [2 0 0 1 0 0 1 1]
labels:       [2 2 1 1 0 0 1 0]
#############
predictions:  [1 1 1 1 1 2 0 0]
labels:       [0 0 2 1 0 0 0 0]
#############
predictions:  [1 1 2 0 1 2 0 1]
labels:       [0 2 1 0 2 0 0 1]
#############
predictions:  [0 0 2 1 0 2 0 1]
labels:       [1 2 0 2 2 1 0 0]
#############
predictions:  [0 0 2 0 2 1 1 2]
labels:       [2 2 1 2 2 2 0 0]
#############
predictions:  [0 1 0 0 0 0 2 1]
labels:       [1 1 0 1 1 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 0 0 2 0 0 2]
#############
predictions:  [2 2 2 0 1 1 1 0]
labels:       [1 0 2 1 1 2 0 0]
#############
predictions:  [0 0 1 0 0 0 2 0]
labels:       [0 1 2 1 1 0 0 0]
#############
predictions:  [0 2 0 1 0 2 0 0]
labels:       [0 0 2 1 1 0 2 2]
#############
predictions:  [0 0 1 2 0 2 0 1]
labels:       [2 2 0 0 0 2 2 2]
#############
predictions:  [1 0 0 0 2 0 0 1]
labels:       [2 0 1 1 1 0 0 1]
#############
predictions:  [0 1 0 0 0 0 0 2]
labels:       [1 1 1 0 0 0 2 2]
#############
predictions:  [0 2 0 1 0 2 0 0]
labels:       [1 1 1 1 2 2 1 0]
#############
predictions:  [1 2 0 0 0 0 0 0]
labels:       [2 0 2 1 0 1 1 1]
#############
Training Loss:  tensor(1.2082, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)
predictions:  [0 2 0 0 0 0 2 0]
labels:       [1 0 2 1 2 2 1 1]
#############
predictions:  [2 0 0 0 0 0 1 0]
labels:       [1 0 0 0 0 2 1 0]
#############
predictions:  [0 0 0 0 2 1 1 1]
labels:       [0 2 2 0 1 2 1 1]
#############
predictions:  [2 1 0 1 0 0 2 0]
labels:       [1 0 2 1 0 2 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 0 0 0 0 1 0]
#############
predictions:  [0 2 1 0 0 0 1 1]
labels:       [0 2 2 2 2 1 1 0]
#############
predictions:  [0 0 0 1 1 0 0 1]
labels:       [0 1 0 1 2 2 2 2]
#############
predictions:  [0 0 0 1 1 1 1 2]
labels:       [2 2 1 2 0 1 1 1]
#############
predictions:  [0 1 2 0 0 1 0 0]
labels:       [0 2 1 0 0 1 0 0]
#############
predictions:  [1 1 1 1 0 0 0 0]
labels:       [2 1 2 1 0 2 2 1]
#############
predictions:  [0 1 2 0 0 1 1 0]
labels:       [2 0 2 1 1 1 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 0 1 1 0 0]
#############
predictions:  [0 0 0 0 0 1 2 2]
labels:       [2 2 1 1 0 2 1 2]
#############
predictions:  [0 1 0 0 1 1 0 1]
labels:       [0 1 0 2 1 0 0 1]
#############
predictions:  [0 2 2 0 0 0 0 2]
labels:       [0 0 2 1 2 2 0 1]
#############
predictions:  [2 0 0 2 2 0 2 0]
labels:       [2 1 0 2 2 0 1 0]
#############
predictions:  [0 2 2 0 2 1 1 2]
labels:       [1 1 0 0 2 1 0 0]
#############
predictions:  [1 1 2 2 0 0 1 2]
labels:       [2 0 2 0 1 1 1 1]
#############
predictions:  [0 1 1 0 0 1 1 0]
labels:       [0 2 1 0 0 2 2 0]
#############
predictions:  [2 1 0 0 0 0 1 1]
labels:       [0 2 0 2 0 0 1 1]
#############
predictions:  [1 2 0 1 2 0 0 0]
labels:       [1 0 1 1 0 2 2 2]
#############
predictions:  [0 0 0 0 2 2 1 2]
labels:       [2 2 2 1 1 1 1 0]
#############
predictions:  [1 2 0 1 0 0 2 0]
labels:       [2 2 1 1 1 0 2 0]
#############
predictions:  [2 0 0 0 0 2 1]
labels:       [0 1 1 2 2 0 2]
#############
======Average Training Loss: 1.11279======
======Average Training Accuracy: 33.77%======
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 1 1 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 0 2 1 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 2 2 1 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 1 2 0 1 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 0 0 1 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 1 2 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 1 2 0 2 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 1 2 2 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 2 2 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 0 2 0 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 1 1 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 2 2 0 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 0 1 2 2 1]
#############
predictions:  [0 0 0 1 0 0 0 0]
labels:       [0 0 1 1 0 2 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 2 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 1 2 2 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 2 1 2 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 0 2 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 0 1 0 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 2 2 2 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 1 1 0 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 1 1 2 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 2 1 2 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 1 0 2 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 1 1 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 0 1 2 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 1 1 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 1 0 0 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 0 0 0 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 1 1 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 1 2 1 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 0 1 1 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 1 0 1 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 2 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 0 2 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 1 1 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 1 2 2 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 0 0 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 0 1 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 2 1 1 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 2 2 2 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 2 2 1 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 2 2 2 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 0 0 1 0 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 0 0 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 1 2 0 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 2 0 1 2 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 2 0 0 0 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 1 0 0 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 0 1 1 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 0 0 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 2 1 1 1 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 0 2 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 2 1 0 2 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 1 2 2 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 0 0 2 1 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 2 0 2 1 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 2 0 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 0 0 1 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 2 2 0 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 1 1 1 0 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 2 2 1 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 0 2 0 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 1 1 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 1 1 1 1 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 2 1 0 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 2 1 0 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 2 0 0 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 2 0 0 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 0 2 2 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 0 1 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 1 2 0 1 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 0 0 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 1 2 0 2 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 1 0 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 0 1 0 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 0 0 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 1 2 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 2 0 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 0 1 1 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 0 1 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 1 2 0 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 1 1 1 0 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 1 2 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 0 1 1 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 0 2 1 0 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 0 0 0 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 1 2 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 0 1 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 1 0 2 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 1 2 0 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 2 2 2 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 1 2 0 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 1 1 0 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 2 2 2 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 0 0 1 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 2 1 2 1 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 0 0 0 2 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 1 1 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 1 0 2 2 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 1 1 1 2 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 0 1 0 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 2 2 0 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 1 2 2 2 1 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 0 1 0 2 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 2 1 0 2 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 2 0 2 2 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 2 0 0 1 0 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 1 0 0 0 2 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 2 0 1 2 1 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 2 2 2 2 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 0 1 2 0 2 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 1 1 1 1 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 0 0 0 1 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 1 2 0 1 2 2 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 1 1 1 2 1 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [1 0 1 1 1 0 0 2]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 2 0 0 0 0 1 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [0 0 1 1 2 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 1 1 1 0 1 0 0]
#############
predictions:  [0 0 0 0 0 0 0 0]
labels:       [2 0 2 2 2 0 0 1]
#############
predictions:  [0 0 0 0 0 0 0]
labels:       [2 2 1 1 0 0 1]
#############
======Average Validation Loss: 1.09527======
======Average Validation Accuracy: 35.53%======
</code></pre>
","13733783","","","","","2021-01-28 13:37:03","BERT always predicts same class (Fine-Tuning)","<python><machine-learning><pytorch><huggingface-transformers><simpletransformers>","1","8","","","","CC BY-SA 4.0"
"59589483","1","59590798","","2020-01-04 09:29:43","","-2","283","<p>In the <a href=""https://github.com/huggingface/transformers#installation"" rel=""nofollow noreferrer"">huggingface github</a> it is written:</p>
<blockquote>
<p>You should install ðŸ¤— Transformers in a virtual environment. If you're
unfamiliar with Python virtual environments, check out the user guide.</p>
<p>Create a virtual environment with the version of Python you're going
to use and activate it.</p>
<p>Now, if you want to use ðŸ¤— Transformers, you can install it with pip.
If you'd like to play with the examples, you must install it from
source.</p>
</blockquote>
<p>Why should it be installed in a virtual python environment?
What are the advantages of doing that rather than installing it on python as is?</p>
","2182857","","-1","","2020-06-20 09:12:55","2020-01-04 12:48:00","Why should the ðŸ¤— (huggingface) Transformers library be installed on a virtual environment?","<python><pytorch><huggingface-transformers>","1","3","","2020-01-04 13:18:07","","CC BY-SA 4.0"
"59760328","1","59766443","","2020-01-15 22:14:34","","8","4307","<p>I've read all the documentations I could find about torch.distributed.barrier(), but still having trouble understanding how it's being used in <a href=""https://github.com/huggingface/transformers/blob/master/examples/run_glue.py"" rel=""noreferrer"">this script</a> and would really appreciate some help.</p>

<p>So the <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier"" rel=""noreferrer"">official doc of torch.distributed.barrier</a> says it ""Synchronizes all processes.This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().""</p>

<p>It's used in two places in the script: </p>

<p><a href=""https://github.com/huggingface/transformers/blob/c76c3cebed3c707178d9f721349c5abd5206a57f/examples/run_glue.py#L360"" rel=""noreferrer"">First place</a></p>

<pre><code>    if args.local_rank not in [-1, 0] and not evaluate:
        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache

        ... (preprocesses the data and save the preprocessed data)

    if args.local_rank == 0 and not evaluate:
        torch.distributed.barrier() 
</code></pre>

<p><a href=""https://github.com/huggingface/transformers/blob/c76c3cebed3c707178d9f721349c5abd5206a57f/examples/run_glue.py#L609"" rel=""noreferrer"">Second place</a></p>

<pre><code>    if args.local_rank not in [-1, 0]:
        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model &amp; vocab

        ... (loads the model and the vocabulary)

    if args.local_rank == 0:
        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model &amp; vocab
</code></pre>

<p>I'm having trouble relating the comment in the code to the functionality of this function stated in the official doc. How does it make sure only the first process executes the code between the two calls of torch.distributed.barrier() and why it only checks whether the local rank is 0 before the second call? </p>

<p>Thanks in advance!</p>
","4639274","","","","","2020-01-16 09:24:29","How does torch.distributed.barrier() work","<pytorch><huggingface-transformers>","1","0","4","","","CC BY-SA 4.0"
"64727226","1","","","2020-11-07 11:34:12","","1","499","<pre><code>import torch
import transformers
tokenizer = transformers.AlbertTokenizer.from_pretrained('albert-base-v2', do_lower_case=True)
transformer = transformers.AlbertModel.from_pretrained(&quot;albert-base-v2&quot;)
</code></pre>
<p>I tried:</p>
<pre><code>transformer.num_parameters
</code></pre>
<p>But, It gives all layers with parameters :</p>
<blockquote>
<pre><code>&lt;bound method ModuleUtilsMixin.num_parameters of AlbertModel(
  (embeddings): AlbertEmbeddings(
    (word_embeddings): Embedding(30000, 128, padding_idx=0)
    (position_embeddings): Embedding(512, 128)
    (token_type_embeddings): Embedding(2, 128)
    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): AlbertTransformer(
    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)
    (albert_layer_groups): ModuleList(
      (0): AlbertLayerGroup(
        (albert_layers): ModuleList(
          (0): AlbertLayer(
            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (attention): AlbertAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (attention_dropout): Dropout(p=0, inplace=False)
              (output_dropout): Dropout(p=0, inplace=False)
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            )
            (ffn): Linear(in_features=768, out_features=3072, bias=True)
            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
  )
  (pooler): Linear(in_features=768, out_features=768, bias=True)
  (pooler_activation): Tanh()
)&gt;
</code></pre>
</blockquote>
<p>I need to access to out_features = 768 the input of the last Linear function</p>
<blockquote>
<p>((pooler): Linear(in_features=768, out_features=768, bias=True) )</p>
</blockquote>
","5821250","","6664872","","2020-11-08 01:15:22","2020-11-08 01:15:22","How to get access intermediate layers parameters in BERT pretrained models?","<python><pytorch><bert-language-model><huggingface-transformers>","0","5","","","","CC BY-SA 4.0"
"64685243","1","64714245","","2020-11-04 17:52:22","","2","2023","<p>How do i get an embedding for the whole sentence from huggingface's feature extraction pipeline?</p>
<p>I understand how to get the features for each token (below) but how do i get the overall features for the sentence as a whole?</p>
<pre><code>feature_extraction = pipeline('feature-extraction', model=&quot;distilroberta-base&quot;, tokenizer=&quot;distilroberta-base&quot;)
features = feature_extraction(&quot;i am sentence&quot;)
</code></pre>
","3472360","","4561314","","2020-11-06 03:53:37","2020-12-30 08:49:33","Getting sentence embedding from huggingface Feature Extraction Pipeline","<machine-learning><nlp><huggingface-transformers><spacy-transformers>","2","0","1","","","CC BY-SA 4.0"
"67484439","1","","","2021-05-11 09:55:57","","0","57","<p>I am looking for a way to utilise Allen NLP interpret module with HuggingFace transformers BERT model of architecture BertForSequenceClassification (trained with pytorch).</p>
<p>I've found an Allen NLP guide regarding common usage of interpret module with models that are structurized differently than my model.</p>
<p>Could someone help me and describe what could be the proper approach at converting BertForSequenceClassification model that could be loaded into <code>Predictor</code> object?</p>
<p>Guide sample found at <a href=""https://guide.allennlp.org/interpret#3"" rel=""nofollow noreferrer"">https://guide.allennlp.org/interpret#3</a> :</p>
<pre><code>from allennlp.interpret.saliency_interpreters import SimpleGradient
from allennlp.predictors import Predictor

inputs = {&quot;sentence&quot;: &quot;a very well-made, funny and entertaining picture.&quot;}
archive = (
    &quot;https://storage.googleapis.com/allennlp-public-models/&quot;
    &quot;basic_stanford_sentiment_treebank-2020.06.09.tar.gz&quot;
)
predictor = Predictor.from_path(archive)
interpreter = SimpleGradient(predictor)
interpretation = interpreter.saliency_interpret_from_json(inputs)

print(interpretation)
</code></pre>
","14661154","","14661154","","2021-05-11 10:04:31","2021-05-11 10:04:31","Loading HF transformers BERT model into Allen NLP interpret module","<huggingface-transformers><allennlp>","0","2","","","","CC BY-SA 4.0"
"60133236","1","60216893","","2020-02-09 03:35:08","","2","524","<p>I'm running a fine-tuned model of BERT and ALBERT for Questing Answering. And, I'm evaluating the performance of these models on a subset of questions from <a href=""https://rajpurkar.github.io/SQuAD-explorer/"" rel=""nofollow noreferrer"">SQuAD v2.0</a>. I use <a href=""https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/"" rel=""nofollow noreferrer"">SQuAD's official evaluation script</a> for evaluation. </p>

<p>I use Huggingface <code>transformers</code> and in the following you can find an actual code and example I'm running (might be also helpful for some folks who are trying to run fine-tuned model of ALBERT on SQuAD v2.0):</p>

<pre><code>tokenizer = AutoTokenizer.from_pretrained(""ktrapeznikov/albert-xlarge-v2-squad-v2"")
model = AutoModelForQuestionAnswering.from_pretrained(""ktrapeznikov/albert-xlarge-v2-squad-v2"")

question = ""Why aren't the examples of bouregois architecture visible today?""
text = """"""Exceptional examples of the bourgeois architecture of the later periods were not restored by the communist authorities after the war (like mentioned Kronenberg Palace and Insurance Company Rosja building) or they were rebuilt in socialist realism style (like Warsaw Philharmony edifice originally inspired by Palais Garnier in Paris). Despite that the Warsaw University of Technology building (1899\u20131902) is the most interesting of the late 19th-century architecture. Some 19th-century buildings in the Praga district (the Vistula\u2019s right bank) have been restored although many have been poorly maintained. Warsaw\u2019s municipal government authorities have decided to rebuild the Saxon Palace and the Br\u00fchl Palace, the most distinctive buildings in prewar Warsaw.""""""

input_dict = tokenizer.encode_plus(question, text, return_tensors=""pt"")
input_ids = input_dict[""input_ids""].tolist()
start_scores, end_scores = model(**input_dict)

all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]).replace('â–', '')
print(answer)
</code></pre>

<p>And the output is like the following:</p>

<pre><code>[CLS] why aren ' t the examples of bour ego is architecture visible today ? [SEP] exceptional examples of the  bourgeois architecture of the later periods were not restored by the communist authorities after the war
</code></pre>

<p>As you can see there are BERT's special tokens in the answer including <code>[CLS]</code> and <code>[SEP]</code>.</p>

<p>I understand that in cases where the answer is just <code>[CLS]</code> (having two <code>tensor(0)</code> for <code>start_scores</code> and <code>end_scores</code>) it basically means model thinks there's no answer to the question in context which makes sense. And in these cases I just simply set the answer to that question to a null string when running the evaluation script.</p>

<p><strong>But</strong> I wonder in cases like the example above, should I again assume that model could not find an answer and set the answer to empty string? or should I just leave the answer like that when I'm evaluating the model performance?</p>

<p>I'm asking this question because as far as I understand, the performance calculated using the evaluation script can change (correct me if I'm wrong) if I have such cases as answers and I may not get a realistic sense of the performance of these models. </p>
","2347063","","","","","2020-02-13 21:41:42","What does BERT's special characters appearance in SQuAD's QA answers mean?","<question-answering><bert-language-model><huggingface-transformers><squad>","1","0","","","","CC BY-SA 4.0"
"59701981","1","59703711","","2020-01-12 07:56:32","","6","13278","<p>I`m beginner.. I'm working with Bert. However, due to the security of the company network, the following code does not receive the bert model directly.</p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)
model = BertForSequenceClassification.from_pretrained(""bert-base-multilingual-cased"", num_labels=2) 
</code></pre>

<p>So I think I have to download these files and enter the location manually.
But I'm new to this, and I'm wondering if it's simple to download a format like .py from github and put it in a location.</p>

<p>I'm currently using the bert model implemented by hugging face's pytorch, and the address of the source file I found is:</p>

<p><a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers</a></p>

<p>Please let me know if the method I thought is correct, and if so, what file to get.</p>

<p>Thanks in advance for the comment.</p>
","12681165","","6866692","","2020-05-18 13:17:02","2020-05-23 07:24:00","BERT tokenizer & model download","<python><github><pytorch><huggingface-transformers><bert-language-model>","1","0","4","","","CC BY-SA 4.0"
"60121768","1","60173172","","2020-02-07 22:08:48","","0","975","<p>I am trying to run <a href=""https://huggingface.co/transformers/model_doc/gpt2.html#tfgpt2lmheadmodel"" rel=""nofollow noreferrer"">hugginface</a> gpt2-xl model. I ran code from the <a href=""https://huggingface.co/transformers/quickstart.html"" rel=""nofollow noreferrer"">quickstart</a> page that load the small gpt2 model and generate text by the following code: </p>

<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
model = GPT2LMHeadModel.from_pretrained('gpt2')

generated = tokenizer.encode(""The Manhattan bridge"")
context = torch.tensor([generated])
past = None

for i in range(100):
    print(i)
    output, past = model(context, past=past)
    token = torch.argmax(output[0, :])

    generated += [token.tolist()]
    context = token.unsqueeze(0)

sequence = tokenizer.decode(generated)

print(sequence)
</code></pre>

<p>This is running perfectly. Then I try to run <code>gpt2-xl</code> model. 
I changed <code>tokenizer</code> and <code>model</code> loading code like following:
  tokenizer = GPT2Tokenizer.from_pretrained(""gpt2-xl"")
  model = GPT2LMHeadModel.from_pretrained('gpt2-xl')</p>

<p>The <code>tokenizer</code> and <code>model</code> loaded perfectly. But I a getting error on the following line:</p>

<pre><code>output, past = model(context, past=past)
</code></pre>

<p>The error is:</p>

<pre><code>RuntimeError: index out of range: Tried to access index 204483 out of table with 50256 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:418
</code></pre>

<p>Looking at error it seems that the embedding size is not correct. So I write the following line to specifically fetch the config file of <code>gpt2-xl</code>:</p>

<pre><code>config = GPT2Config.from_pretrained(""gpt2-xl"")
</code></pre>

<p>But, here <code>vocab_size:50257</code>
So I changed explicitly the value by:</p>

<pre><code>config.vocab_size=204483
</code></pre>

<p>Then after printing the <code>config</code>, I can see that the previous line took effect in the configuration. But still, I am getting the same error. </p>
","3363813","","","","","2020-02-11 16:15:02","while running huggingface gpt2-xl model embedding index getting out of range","<python-3.x><language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60137162","1","60146856","","2020-02-09 13:33:21","","1","444","<p>I want to pre-train BERT and RoBERTa MLM using domain corpus (sentiment-related text). How long it gonna take for using 50k~100k words. Since RoBERTa is not trained on predicting the next sentence objective, one training objective less than BERT and with larger mini-batches and learning rates, I assume RoBERTa will be much faster?</p>
","9280294","","8913537","","2020-03-26 11:30:15","2020-03-26 11:30:15","Pre-training BERT/RoBERTa language model using domain text, how long it gonna take estimately? which is faster?","<language-model><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66967698","1","","","2021-04-06 11:22:24","","1","157","<p><code>torch</code> ver: <code>1.8.1+cu102</code>
<code>transformers</code> ver: <code>4.4.2</code></p>
<p>I adopt the example codes from <a href=""https://github.com/huggingface/transformers/blob/master/examples/text-generation/run_generation.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/examples/text-generation/run_generation.py</a></p>
<p>to generate text by using ctrl.
here is the head part of my codes:</p>
<pre><code>import torch
from transformers import CTRLTokenizer, CTRLLMHeadModel
tokenizer = CTRLTokenizer.from_pretrained('ctrl')
model = CTRLLMHeadModel.from_pretrained('ctrl')

encoded_prompt = tokenizer.encode(&quot;Links Hello, my dog is cute&quot;, add_special_tokens=False)
</code></pre>
<p>ERROR:</p>
<pre><code>ValueError Traceback (most recent call last) in
----&gt; 1 encoded_prompt = tokenizer.encode(&quot;Links Hello, my dog is cute&quot;, add_special_tokens=False)

~/yanan/env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in encode(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs) 2030 stride=stride, 2031 return_tensors=return_tensors,
-&gt; 2032 **kwargs, 2033 ) 2034

~/yanan/env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in encode_plus(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs) 2355 return_length=return_length, 2356 verbose=verbose,
-&gt; 2357 **kwargs, 2358 ) 2359

~/yanan/env/lib/python3.6/site-packages/transformers/tokenization_utils.py in _encode_plus(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs) 458 return_special_tokens_mask=return_special_tokens_mask, 459 return_length=return_length,
--&gt; 460 verbose=verbose, 461 ) 462

~/yanan/env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in prepare_for_model(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis,
**kwargs) 2792 padding=padding_strategy.value, 2793 pad_to_multiple_of=pad_to_multiple_of,
-&gt; 2794 return_attention_mask=return_attention_mask, 2795 ) 2796

~/yanan/env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose) 2591 else: 2592 raise ValueError(
-&gt; 2593 f&quot;type of {first_element} unknown: {type(first_element)}. &quot; 2594 f&quot;Should be one of a python, numpy, pytorch or tensorflow object.&quot; 2595 )

ValueError: type of None unknown: &lt;class 'NoneType'&gt;. Should be one of a python, numpy, pytorch or tensorflow object.
</code></pre>
","6407393","","13273054","","2021-04-07 00:02:37","2021-04-07 00:02:37","huggingface transformer CTRL model can not work for generation text","<python><pytorch><torch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"60157959","1","60315929","","2020-02-10 20:31:24","","1","349","<p>I use Ai-powered summarization from <a href=""https://github.com/huggingface/transformers/tree/master/examples/summarization"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/summarization</a> - state of the art results.</p>

<p><strong>Should i train it myself to get summary output longer than used in original huggingface github training script?</strong>
:</p>

<pre><code>python run_summarization.py \
    --documents_dir $DATA_PATH \
    --summaries_output_dir $SUMMARIES_PATH \ # optional
    --no_cuda false \
    --batch_size 4 \
    --min_length 50 \
    --max_length 200 \
    --beam_size 5 \
    --alpha 0.95 \
    --block_trigram true \
    --compute_rouge true
</code></pre>

<p>When i do inference with </p>

<pre><code>--min_length 500 \
--max_length 600 \
</code></pre>

<p>I got a good output for 200 tokens, but the rest of the text is </p>

<pre><code>. . . [unused7] [unused7] [unused7] [unused8] [unused4] [unused7] [unused7]  [unused4] [unused7] [unused8]. [unused4] [unused7] . [unused4] [unused8] [unused4] [unused8].  [unused4]  [unused4] [unused8]  [unused4] . .  [unused4] [unused6] [unused4] [unused7] [unused6] [unused4] [unused8] [unused5] [unused4] [unused7] [unused4] [unused4] [unused7]. [unused4] [unused6]. [unused4] [unused4] [unused4] [unused8]  [unused4] [unused7]  [unused4] [unused8] [unused6] [unused4]   [unused4] [unused4]. [unused4].  [unused5] [unused4] [unused8] [unused7] [unused4] [unused7] [unused9] [unused4] [unused7]  [unused4] [unused7] [unused5] [unused4]  [unused5] [unused4] [unused6]  [unused4]. .  . [unused5]. [unused4]  [unused4]   [unused4] [unused6] [unused5] [unused4] [unused4]  [unused6] [unused4] [unused6]  [unused4] [unused4] [unused5] [unused4]. [unused5]  [unused4] . [unused4]  [unused4] [unused8] [unused8] [unused4]  [unused7] [unused4] [unused8]  [unused4] [unused7]  [unused4] [unused8]  [unused4]  [unused8] [unused4] [unused6] 
</code></pre>
","1849805","","","","","2020-02-20 08:45:28","Transformers summarization with Python Pytorch - how to get longer output?","<pytorch><huggingface-transformers><pytorch-ignite>","1","0","","","","CC BY-SA 4.0"
"67771257","1","67780228","","2021-05-31 09:33:47","","2","176","<p>I am using Transformer's RobBERT (the dutch version of RoBERTa) for sequence classification - trained for sentiment analysis on the Dutch Book Reviews dataset.</p>
<p>I wanted to test how well it works on a similar dataset (also on sentiment analysis), so I made annotations for a set of text fragments and checked its accuracy. When I checked what kind of sentence are misclassified, I noticed that the output for a unique sentence depends heavily on the length of padding I give when tokenizing. See code below.</p>
<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch.nn.functional as F
import torch


model = RobertaForSequenceClassification.from_pretrained(&quot;pdelobelle/robBERT-dutch-books&quot;, num_labels=2)
tokenizer = RobertaTokenizer.from_pretrained(&quot;pdelobelle/robBERT-dutch-books&quot;, do_lower_case=True)

sent = 'De samenwerking gaat de laatste tijd beter'
max_seq_len = 64


test_token = tokenizer(sent,
                        max_length = max_seq_len,
                        padding = 'max_length',
                        truncation = True,
                        return_tensors = 'pt'
                        )

out = model(test_token['input_ids'],test_token['attention_mask'])

probs = F.softmax(out[0], dim=1).detach().numpy()
</code></pre>
<p>For the given sample text, which translates in English to &quot;The collaboration has been improving lately&quot;, there is a huge difference in output on classification depending on the max_seq_len. Namely, for <code>max_seq_len = 64</code> the output for <code>probs</code> is:</p>
<p>[[0.99149346 0.00850648]]</p>
<p>whilst for <code>max_seq_len = 9</code>, being the actual length including cls tokens:</p>
<p>[[0.00494814 0.9950519 ]]</p>
<p>Can anyone explain why this huge difference in classification is happening? I would think that the attention mask ensures that in the output there is no difference because of padding to the max sequence length.</p>
","16083559","","16083559","","2021-05-31 11:28:45","2021-05-31 21:07:06","Why does Transformer's BERT (for sequence classification) output depend heavily on maximum sequence length padding?","<sentiment-analysis><bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","0","1","","","CC BY-SA 4.0"
"67785438","1","67787888","","2021-06-01 08:35:34","","0","608","<p>I am trying to use XLNET through transformers. however i keep getting the issue &quot;AttributeError: 'NoneType' object has no attribute 'tokenize'&quot;. I am unsure of how to proceed. if anyone could point me in the right direction it would be appreciated.</p>
<pre><code>tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)

print(' Original: ', X_train[1])

# Print the tweet split into tokens.
print('Tokenized: ', tokenizer.tokenize(X_train[1]))

# Print the tweet mapped to token ids.
print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(X_train[1])))




Original:  hey angel duh sexy really thanks haha
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-67-2b1b432b3e15&gt; in &lt;module&gt;()
      2 
      3 # Print the tweet split into tokens.
----&gt; 4 print('Tokenized: ', tokenizer.tokenize(X_train[2]))
      5 
      6 # Print the tweet mapped to token ids.

AttributeError: 'NoneType' object has no attribute 'tokenize'
</code></pre>
","14392759","","","","","2021-06-01 11:20:55","AttributeError: 'NoneType' object has no attribute 'tokenize'","<python><huggingface-transformers>","1","3","","","","CC BY-SA 4.0"
"67806282","1","","","2021-06-02 13:39:25","","2","51","<p>I am beginner in the field of machine learning and would really appreciate any help. Thank you in advance.</p>
<p>I have learned XLNet and BERT Transformer models, which both solve the text classification problem. Both artificial networks work well and I donâ€™t see any problem while using them on my GPU and CPU as well directly from my python code.</p>
<p>Few weeks ago I decided to implement a simple API to access the learned models and I have implemented a simple Flask application that uses Worker and runs in both synchronous and asynchronous modes (<a href=""https://realpython.com/flask-by-example-implementing-a-redis-task-queue/"" rel=""nofollow noreferrer"">https://realpython.com/flask-by-example-implementing-a-redis-task-queue/</a>). The application works good and I am able to access the models to let them classify the input text (the text is send using POST request in JSON format), but there is one problem which I am not able to solve.</p>
<p>The API works without any problem only during the synchronous mode (both GPU and CPU versions). The problem appears when I tried to put the same code from my CPU to my GPU and run it again asynchronously.
The problem appears when I try to access modelâ€™s predictions (its result, see <code>ret</code> variable).</p>
<p>This happens for both the Transformer Pipline and for my bert model in <code>eval()</code> mode. In case when I try to access model's result the Python returned me (see the <code>exec_job</code> function)
<code>ret is None; TypeError: 'NoneType' object is not subscriptable</code> and I am not able to access <code>ret['status']</code>, because the <code>ret</code> is None.</p>
<p>When I delete the parts of code putting my model and created tensors on GPU (I just deleted <code>.to(_configuration.GPU_DEVICE_ID)</code> parts) - everything works fine (but in this case my model is located on CPU and the evaluation takes much more time).</p>
<pre><code># This code runs in __init__
self._model.to(_configuration.GPU_DEVICE_ID) 

# These lines are executed during evaluation         
inputs = torch.tensor(input_ids).to(_configuration.GPU_DEVICE_ID)
segments = torch.tensor(segments).to(_configuration.GPU_DEVICE_ID)
masks = torch.tensor(attention_masks).to(_configuration.GPU_DEVICE_ID)
</code></pre>
<p>I tied to debug my code and notice that the function <code>self._process_tokenized_text(sentence)</code> executed successfully but the code after GPU parts not executed at all and even my <code>print()</code> code does not do anything.</p>
<p>Simplified code below.</p>
<p>Asynchronous function which runs the function <code>exec_job</code></p>
<pre><code>    @app.route('/api/text/check-json-async', methods=['POST'])
    @auth.login_required
    def check_json_async():
        request_data = request.get_json()
        text=request_data['data']
        job = q.enqueue_call(
            func='aijob.exec_job', args=(text,True,), result_ttl=60
        )
        global callbackURL
        ret={&quot;text&quot;:text,&quot;callbackURL&quot;:callbackURL,&quot;jobID&quot;:job.get_id(),&quot;status&quot;:&quot;queued&quot;}
        return jsonify(ret)
</code></pre>
<p><code>exec_job</code> function which calls _aiprocess function and access the AI core</p>
<pre><code>    # The function whitch publish the main functionality of this layer
    def exec_job(text, async_parm):
        # Evaluate text
        ret = _aiprocess(text)
        # Save classificated result --&gt; THIS CODE FAILS ONLY ON GPU MODE BECAUSE the RET is NONE
        _savePredictedResult(ret['status'] == STATUS, ret)
        if async_parm is True:
            import pickle
            f = open('callbackURL.pckl', 'rb')
            callbackURL = pickle.load(f)
            f.close()
            print(ret)
            print(callbackURL)
            requests.post(callbackURL, json=ret)
        return ret
</code></pre>
<p>The AI core the <code>_aiprocess</code> function</p>
<pre><code>   def _aiprocess(self, sentence):
        if sentence is not None:
            #function working on CPU successfully returns the result
            input_ids, attention_masks, segments = self._process_tokenized_text(sentence)
            # print works as well
            print(input_ids, attention_masks, segments)

            inputs = torch.tensor(input_ids).to(_configuration.GPU_DEVICE_ID)
            segments = torch.tensor(segments).to(_configuration.GPU_DEVICE_ID)
            masks = torch.tensor(attention_masks).to(_configuration.GPU_DEVICE_ID)
            # This code do nothing and when I try to access the predictions array there is nothing but only in GPU mode
            print(inputs)
            # Tracking variables 
            predictions = []
            
            # Predict 
            with torch.no_grad():
                outputs = self._model(input_ids = inputs,token_type_ids=segments, input_mask = masks)
                for tmptuple in outputs[:1]:
                    tmp_array = torch.nn.functional.softmax(tmptuple, dim=1).cpu().numpy()
                    for result in tmp_array:
                        predictions.append(self._resultPrediction(result))                                   
            return predictions[0]
</code></pre>
<p>Do you have any idea why this is happening? Thank you so much and I am sorry for my bad English.</p>
","16105915","","16105915","","2021-06-02 13:43:54","2021-06-02 13:43:54","Problem with BERT, XLNET Transformer Pipline and GPU acceleration with Flask API","<flask><pytorch><bert-language-model><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"60176041","1","","","2020-02-11 19:26:39","","0","2692","<p>I am running into this error when using pytorch_transformers in Colab environment after installing Anaconda and activating it. I found a similar issue in <a href=""https://github.com/huggingface/transformers/issues/1705"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/1705</a> but it does not work for me. Thanks for your help!</p>

<pre><code>!pip install transformers
!pip install simpletransformers
!pip install pytorch-transformers

import os
os.environ['PYTHONPATH'] = ""/usr/local/bin/python""

import sys!source activate transformers!source activate transformers
sys.path.append('/usr/local/lib/python3.6/site-packages/')

!conda --version
!conda update -n base conda
!conda install -q -y --prefix /usr/local -c conda-forge tqdm
!conda create -n transformers tqdm
!source activate transformers

from __future__ import absolute_import, division, print_function

import glob
import logging
import os
import random
import json

import numpy as np
import torch
from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,
                              TensorDataset)
import random
from torch.utils.data.distributed import DistributedSampler
from tqdm import tqdm_notebook, trange

from pytorch_transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer,
                                  XLMConfig, XLMForSequenceClassification, XLMTokenizer, 
                                  XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer,
                                  RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)

from pytorch_transformers import AdamW, WarmupLinearSchedule

from utils import (convert_examples_to_features,
                        output_modes, processors)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

     22 from pytorch_transformers import AdamW, WarmupLinearSchedule
     23 
---&gt; 24 from utils import (convert_examples_to_features,
     25                         output_modes, processors)
     26 

ModuleNotFoundError: No module named 'utils'
</code></pre>
","5445777","","5445777","","2020-02-11 19:32:19","2020-02-11 19:32:19","from pytorch_transformers ModuleNotFoundError: No module named 'utils'","<huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"66958575","1","","","2021-04-05 19:19:17","","1","67","<p>I am trying to finetune a Transformers language model for text classification using PyTorch . However, I get the following memory error:</p>
<pre><code>CUDA out of memory. Tried to allocate 578.00 MiB (GPU 2; 10.76 GiB total capacity; 2.79 GiB already allocated; 6.55 GiB free; 3.24 GiB reserved in total by PyTorch
</code></pre>
<p>I am confused since I definitely have enough memory according to the error message.
I also tried to train the model on a different GPU with a little less memory before as well as on the CPU with much less data to check my code and both versions worked fine (but cannot be used for the full training). It also already worked for a different Transformers model when setting the batch size to 16. I also want to avoid decreasing my batch size any further since I am already at 16. Does anyone have an idea why the error occurs? Thanks in advance!</p>
","12383879","","681865","","2021-04-05 22:07:09","2021-04-05 22:07:09","PyTorch GPU out of memory although enough memory is free","<pytorch><runtime-error><out-of-memory><huggingface-transformers>","0","0","1","2021-04-05 23:49:46","","CC BY-SA 4.0"
"64702885","1","64710897","","2020-11-05 18:04:26","","-1","231","<p>I am trying to use pipeline from transformers to summarize the text. But what I can get is only truncated text from original one.
My code is:</p>
<pre><code>from transformers import pipeline
summarizer = pipeline(&quot;summarization&quot;)
summarizer(&quot;The present invention discloses a pharmaceutical composition comprising therapeutically effective amount of, or an extract consisting essentially therapeutically effective amount of at least one cannabinoid selected from the group consisting of: Cannabidiol (CBD) or a derivative thereof, Tetrahydrocannabinol (THC) or a derivative thereof, and any combination thereof, for use in the treatment of multiple myeloma (MM). The present invention further discloses methods and uses of the aforementioned composition.&quot;, 
       min_length=5, max_length=10)
</code></pre>
<p>The output is</p>
<pre><code>[{'summary_text': ' The present invention discloses a pharmaceutical'}]
</code></pre>
<p>That is just a beginning of the text for analysis. What I am doing wrong?</p>
","14176284","","","","","2020-11-06 08:09:06","How to use pipeline from transformers, summarization? Python","<python><pipeline><huggingface-transformers><transformer>","1","1","","","","CC BY-SA 4.0"
"67783283","1","","","2021-06-01 05:46:24","","0","191","<p>I was not able to understand one thing , when it says &quot;fine-tuning of BERT&quot;,  what does it actually mean:</p>
<ol>
<li>Are we retraining the entire model again with new data.</li>
<li>Or are we just training top few transformer layers with new data.</li>
<li>Or we are training the entire model but considering the pretrained weights as initial weight.</li>
<li>Or there is already few layers of ANN on top of transformer layers which is only getting trained keeping transformer weight freeze.</li>
</ol>
<p>Tried Google but I am getting confused, if someone can help me on this.</p>
<p>Thanks in advance!</p>
","5585598","","3607203","","2021-06-01 07:43:13","2021-06-01 07:43:13","What does ""fine-tuning of a BERT model"" refer to?","<nlp><bert-language-model><huggingface-transformers><transformer>","1","0","","","","CC BY-SA 4.0"
"67811619","1","","","2021-06-02 19:47:09","","0","53","<p>I am trying to use the <code>batch_encode_plus()</code> function from BERT. The problem comes when it says that this function does not exist. I go to the documentation in this <a href=""https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus"" rel=""nofollow noreferrer"">page</a> and it says the following: &quot;<em>This method is deprecated, <code>__call__</code> should be used instead.&quot;</em>. I tried to use <code>__call__</code> as a function but it does not work.</p>
<p>Then I get into this page and it seems that the function <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html"" rel=""nofollow noreferrer""><code>batch_encode_plus()</code> was replaced by <code>__call__</code></a>. But when I use the function is not working. I tried to use <code>encoded_plus()</code> but it does not give the the expected results.</p>
<p>I do not know how to use this <code>__call__</code> function, any suggestions?</p>
","15575118","","202229","","2021-06-03 21:42:00","2021-06-03 21:42:00","This method is deprecated, __call__ should be used instead, how to solve this problem in BERT?","<python><huggingface-transformers><huggingface-tokenizers>","0","2","","","","CC BY-SA 4.0"
"69019476","1","","","2021-09-01 19:19:40","","0","17","<p>I did fork the transformers repo in my github account then I did:</p>
<pre><code>git clone --recursive https://github.com/myaccount/transformers.git
cd transformers/
conda create -n hf-dev-py380 python=3.8.0
conda activate hf-dev-py380
git checkout v4.9.2-release
pip install -e â€œ.[dev]â€
conda install -c conda-forge librosa
conda install libgcc
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/miniconda3/lib/
transformers-cli env

conda install jupyter
nohup jupyter notebook --ip=0.0.0.0 --port=myport &amp;
</code></pre>
<p>In my jupyter notebook I print:</p>
<pre><code>import transformers
print(transformers)
&lt;module â€˜transformersâ€™ from â€˜/mypath/miniconda3/lib/python3.7/site-packages/transformers/init.pyâ€™&gt;
</code></pre>
<p>How do I make sure my jupyter notebook is using the transformer code I cloned in my terminal?</p>
<p>Thanks!</p>
","11693843","","","","","2021-09-01 19:19:40","How to make sure my py env used in jupyter notebook is using the transformer version/code that I cloned from github?","<python><jupyter-notebook><huggingface-transformers><virtual-environment>","0","0","","","","CC BY-SA 4.0"
"67359654","1","","","2021-05-02 18:09:34","","0","360","<p>I'm trying to implement a code for sentiment analysis( positive or negative labels) using BERT and i want to add a BiLSTM layer to see if I can increase the accuracy of the pretrained model from HuggingFace. I have the below code and a few questions :</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn import metrics
import transformers
import torch
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertModel, BertConfig
from torch import cuda
import re
import torch.nn as nn

device = 'cuda' if cuda.is_available() else 'cpu'
MAX_LEN = 200
TRAIN_BATCH_SIZE = 8
VALID_BATCH_SIZE = 4
EPOCHS = 1
LEARNING_RATE = 1e-05 #5e-5, 3e-5 or 2e-5
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class CustomDataset(Dataset):
 def __init__(self, dataframe, tokenizer, max_len):
  self.tokenizer = tokenizer
  self.data = dataframe
  self.comment_text = dataframe.review
  self.targets = self.data.sentiment
  self.max_len = max_len
 def __len__(self):
  return len(self.comment_text)
 def __getitem__(self, index):
  comment_text = str(self.comment_text[index])
  comment_text = &quot; &quot;.join(comment_text.split())

  inputs = self.tokenizer.encode_plus(comment_text,None,add_special_tokens=True,max_length=self.max_len,
   pad_to_max_length=True,return_token_type_ids=True)
  ids = inputs['input_ids']
  mask = inputs['attention_mask']
  token_type_ids = inputs[&quot;token_type_ids&quot;]

  return {
   'ids': torch.tensor(ids, dtype=torch.long),
   'mask': torch.tensor(mask, dtype=torch.long),
   'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
   'targets': torch.tensor(self.targets[index], dtype=torch.float)
  }
train_size = 0.8
train_dataset=df.sample(frac=train_size,random_state=200)
test_dataset=df.drop(train_dataset.index).reset_index(drop=True)
train_dataset = train_dataset.reset_index(drop=True)

print(&quot;FULL Dataset: {}&quot;.format(df.shape))
print(&quot;TRAIN Dataset: {}&quot;.format(train_dataset.shape))
print(&quot;TEST Dataset: {}&quot;.format(test_dataset.shape))

training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)
testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)
train_params = {'batch_size': TRAIN_BATCH_SIZE,'shuffle': True,'num_workers': 0}
test_params = {'batch_size': VALID_BATCH_SIZE,'shuffle': True,'num_workers': 0}
training_loader = DataLoader(training_set, **train_params)
testing_loader = DataLoader(testing_set, **test_params)


class BERTClass(torch.nn.Module):
 def __init__(self):
   super(BERTClass, self).__init__()
   self.bert = BertModel.from_pretrained('bert-base-uncased',return_dict=False, num_labels =2)
   self.lstm = nn.LSTM(768, 256, batch_first=True, bidirectional=True)
   self.linear = nn.Linear(256*2,2)

 def forward(self, ids , mask,token_type_ids):
  sequence_output, pooled_output = self.bert(ids, attention_mask=mask, token_type_ids = token_type_ids)
  lstm_output, (h, c) = self.lstm(sequence_output)  ## extract the 1st token's embeddings
  hidden = torch.cat((lstm_output[:, -1, :256], lstm_output[:, 0, 256:]), dim=-1)
  linear_output = self.linear(lstm_output[:, -1].view(-1, 256 * 2))

  return linear_output

model = BERTClass()
model.to(device)
print(model)
def loss_fn(outputs, targets):
 return torch.nn.BCEWithLogitsLoss()(outputs, targets)
optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)

def train(epoch):
 model.train()
 for _, data in enumerate(training_loader, 0):
  ids = data['ids'].to(device, dtype=torch.long)
  mask = data['mask'].to(device, dtype=torch.long)
  token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)
  targets = data['targets'].to(device, dtype=torch.float)
  outputs = model(ids, mask, token_type_ids)
  optimizer.zero_grad()
  loss = loss_fn(outputs, targets)
  if _ % 5000 == 0:
   print(f'Epoch: {epoch}, Loss:  {loss.item()}')
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

for epoch in range(EPOCHS):
  train(epoch)
</code></pre>
<p>So on the above code I ran into the error :  <code>Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 2]))</code> .  Checked online and tried to use <code>targets = targets.unsqueeze(2)</code> but then I get another error that I must use values from [-2,1] for unsqueeze. I also tried to modify the loss function to</p>
<pre><code>def loss_fn(outputs, targets):
 return torch.nn.BCELoss()(outputs, targets)
</code></pre>
<p>but I still receive the same error. Can someone advise if there is a solution to this problem? Or what can I do to make this work fine?  Many thanks in advance.</p>
","15645097","","15645097","","2021-05-03 08:57:55","2021-05-03 08:57:55","ValueError: Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 2]))","<python-3.x><pytorch><bert-language-model><huggingface-transformers><bilstm>","0","9","","","","CC BY-SA 4.0"
"67496616","1","67510250","","2021-05-12 02:36:11","","0","759","<p>I am getting &quot;RuntimeError: Input, output and indices must be on the current device.&quot;
when I run this line.
fill_mask(&quot;Auto Car .&quot;)</p>
<p>I am running it on Colab.
My Code:</p>
<pre><code>from transformers import BertTokenizer, BertForMaskedLM
from pathlib import Path
from tokenizers import ByteLevelBPETokenizer
from transformers import BertTokenizer, BertForMaskedLM


paths = [str(x) for x in Path(&quot;.&quot;).glob(&quot;**/*.txt&quot;)]
print(paths)

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

from transformers import BertModel, BertConfig

configuration = BertConfig()
model = BertModel(configuration)
configuration = model.config
print(configuration)

model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)

from transformers import LineByLineTextDataset
dataset = LineByLineTextDataset(
    tokenizer=bert_tokenizer,
    file_path=&quot;./kant.txt&quot;,
    block_size=128,
)

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15
)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=&quot;./KantaiBERT&quot;,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
    )

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()

from transformers import pipeline

fill_mask = pipeline(
    &quot;fill-mask&quot;,
    model=model,
    tokenizer=bert_tokenizer
)

fill_mask(&quot;Auto Car &lt;mask&gt;.&quot;)
</code></pre>
<p>The last line is giving me the error mentioned above. Please let me know what I am doing wrong or what I have to do in order to remove this error.</p>
","6805178","","6805178","","2021-05-12 17:07:58","2021-05-12 19:58:34","RuntimeError: Input, output and indices must be on the current device. (fill_mask(""Random text <mask>."")","<python><nlp><pytorch><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60120849","1","60147063","","2020-02-07 20:46:37","","2","3721","<p>I was following <a href=""https://www.aclweb.org/anthology/P19-1328/"" rel=""nofollow noreferrer"">a paper</a> on BERT-based lexical substitution (specifically trying to implement equation (2) - if someone has already implemented the whole paper that would also be great). Thus, I wanted to obtain both the last hidden layers (only thing I am unsure is the ordering of the layers in the output: last first or first first?) and the attention from a basic BERT model (bert-base-uncased). </p>

<p>However, I am a bit unsure whether the <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface/transformers library</a> actually outputs the attention (I was using torch, but am open to using TF instead) for bert-base-uncased?</p>

<p>From <a href=""https://github.com/huggingface/transformers/issues/1073"" rel=""nofollow noreferrer"">what I had read</a>, I was expected to get a tuple of (logits, hidden_states, attentions), but with the example below (runs e.g. in Google Colab), I get of length 2 instead. </p>

<p>Am I misinterpreting what I am getting or going about this the wrong way? I did the obvious test and used <code>output_attention=False</code> instead of <code>output_attention=True</code> (while <code>output_hidden_states=True</code> does indeed seem to add the hidden states, as expected) and nothing change in the output I got. That's clearly a bad sign about my understanding of the library or indicates an issue.</p>

<pre><code>import numpy as np
import torch
!pip install transformers

from transformers import (AutoModelWithLMHead, 
                          AutoTokenizer, 
                          BertConfig)

bert_tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True, output_attention=True) # Nothign changes, when I switch to output_attention=False
bert_model = AutoModelWithLMHead.from_config(config)

sequence = ""We went to an ice cream cafe and had a chocolate ice cream.""
bert_tokenized_sequence = bert_tokenizer.tokenize(sequence)

indexed_tokens = bert_tokenizer.encode(bert_tokenized_sequence, return_tensors='pt')

predictions = bert_model(indexed_tokens)

########## Now let's have a look at what the predictions look like #############
print(len(predictions)) # Length is 2, I expected 3: logits, hidden_layers, attention

print(predictions[0].shape) # torch.Size([1, 16, 30522]) - seems to be logits (shape is 1 x sequence length x vocabulary

print(len(predictions[1])) # Length is 13 - the hidden layers?! There are meant to be 12, right? Is one somehow the attention?

for k in range(len(predictions[1])):
  print(predictions[1][k].shape) # These all seem to be torch.Size([1, 16, 768]), so presumably the hidden layers?
</code></pre>

<h1>Explanation of what worked in the end inspired by accepted answer</h1>

<pre><code>import numpy as np
import torch
!pip install transformers

from transformers import BertModel, BertConfig, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True, output_attentions=True)
model = BertModel.from_pretrained('bert-base-uncased', config=config)
sequence = ""We went to an ice cream cafe and had a chocolate ice cream.""
tokenized_sequence = tokenizer.tokenize(sequence)
indexed_tokens = tokenizer.encode(tokenized_sequence, return_tensors='pt'
enter code here`outputs = model(indexed_tokens)
print( len(outputs) ) # 4 
print( outputs[0].shape ) #1, 16, 768 
print( outputs[1].shape ) # 1, 768
print( len(outputs[2]) ) # 13  = input embedding (index 0) + 12 hidden layers (indices 1 to 12)
print( outputs[2][0].shape ) # for each of these 13: 1,16,768 = input sequence, index of each input id in sequence, size of hidden layer
print( len(outputs[3]) ) # 12 (=attenion for each layer)
print( outputs[3][0].shape ) # 0 index = first layer, 1,12,16,16 = , layer, index of each input id in sequence, index of each input id in sequence
</code></pre>
","7744356","","975097","","2020-04-25 00:01:28","2020-12-16 14:41:49","Outputting attention for bert-base-uncased with huggingface/transformers (torch)","<python><attention-model><huggingface-transformers><bert-language-model>","2","2","2","","","CC BY-SA 4.0"
"67833477","1","","","2021-06-04 07:53:00","","0","125","<p>I am running inference using a Huggingface transformer model for question answering.</p>
<p>I have exposed this as an API using flask and gunicorn (with multiple workers).</p>
<p>I have noticed that when a single request is issued, more than 1 CPU cores are being utilized.</p>
<p>Hence, when more than one request is issued concurrently, the response for each is taking much longer. For example, 1 request = 1.2 seconds whereas, 2 requests = getting both responses after 2 seconds.</p>
<p>It would be ideal if both requests took 1.2 seconds to be served.</p>
<p>This issue is occurring although I have more than one workers in gunicorn.</p>
<p><a href=""https://blog.roblox.com/2020/05/scaled-bert-serve-1-billion-daily-requests-cpus/"" rel=""nofollow noreferrer"">In this article</a>, they address the issue by thread tuning in PyTorch, they offer a solution by adding this line to their code:
<code>torch.set_num_threads(1)</code></p>
<p>It seems that this is supposed to ensure that each worker uses exactly one cpu core and make sure they are not competing for the same resources.</p>
<p>I have incorporated this into my code. However, the problem still seems to persist.</p>
<p>Hence, I suspect that this is due to some configuration in gunicorn.</p>
<p>The Flask app is defined in the file QA_transformer.py and this is what I use to start gunicorn:</p>
<pre><code>gunicorn --bind 0.0.0.0:5000 QA_transformer:app --workers 3
</code></pre>
<p>Is there something that should be added or changed?
How can I make sure that each worker uses only one CPU core?
Or does the <code>torch.set_num_threads(1)</code> not work as expected for 'pipeline' in the transformers library from Huggingface?</p>
<p>PS. I am using a machine with 4 CPU cores.</p>
","14793466","","","","","2021-06-04 07:53:00","How can I make sure that each worker in gunicorn uses only one cpu core?","<python><flask><pytorch><gunicorn><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"59656096","1","59684161","","2020-01-09 01:41:04","","4","1210","<p>I am training a binary classifier that uses Bert (huggingface). The model looks like this:</p>

<pre><code>def get_model(lr=0.00001):
    inp_bert = Input(shape=(512), dtype=""int32"")
    bert = TFBertModel.from_pretrained('bert-base-multilingual-cased')(inp_bert)[0]
    doc_encodings = tf.squeeze(bert[:, 0:1, :], axis=1)
    out = Dense(1, activation=""sigmoid"")(doc_encodings)
    model = Model(inp_bert, out)
    adam = optimizers.Adam(lr=lr)
    model.compile(optimizer=adam, loss=""binary_crossentropy"", metrics=[""accuracy""])
    return model
</code></pre>

<p>After fine tuning for my classification task, I want to save the model.</p>

<pre><code>model.save(""best_model.h5"")
</code></pre>

<p>However this raises a NotImplementedError:</p>

<pre><code>---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-55-8c5545f0cd9b&gt; in &lt;module&gt;()
----&gt; 1 model.save(""best_spam.h5"")
      2 # import transformers

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)
    973     """"""
    974     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,
--&gt; 975                       signatures, options)
    976 
    977   def save_weights(self, filepath, overwrite=True, save_format=None):

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)
    110           'or using `save_weights`.')
    111     hdf5_format.save_model_to_hdf5(
--&gt; 112         model, filepath, overwrite, include_optimizer)
    113   else:
    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)
     97 
     98   try:
---&gt; 99     model_metadata = saving_utils.model_metadata(model, include_optimizer)
    100     for k, v in model_metadata.items():
    101       if isinstance(v, (dict, list, tuple)):

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in model_metadata(model, include_optimizer, require_config)
    163   except NotImplementedError as e:
    164     if require_config:
--&gt; 165       raise e
    166 
    167   metadata = dict(

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in model_metadata(model, include_optimizer, require_config)
    160   model_config = {'class_name': model.__class__.__name__}
    161   try:
--&gt; 162     model_config['config'] = model.get_config()
    163   except NotImplementedError as e:
    164     if require_config:

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in get_config(self)
    885     if not self._is_graph_network:
    886       raise NotImplementedError
--&gt; 887     return copy.deepcopy(get_network_config(self))
    888 
    889   @classmethod

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in get_network_config(network, serialize_layer_fn)
   1940           filtered_inbound_nodes.append(node_data)
   1941 
-&gt; 1942     layer_config = serialize_layer_fn(layer)
   1943     layer_config['name'] = layer.name
   1944     layer_config['inbound_nodes'] = filtered_inbound_nodes

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py in serialize_keras_object(instance)
    138   if hasattr(instance, 'get_config'):
    139     return serialize_keras_class_and_config(instance.__class__.__name__,
--&gt; 140                                             instance.get_config())
    141   if hasattr(instance, '__name__'):
    142     return instance.__name__

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in get_config(self)
    884   def get_config(self):
    885     if not self._is_graph_network:
--&gt; 886       raise NotImplementedError
    887     return copy.deepcopy(get_network_config(self))
    888 

NotImplementedError: 
</code></pre>

<p>I am aware that huggingface provides a model.save_pretrained() method for TFBertModel, but I prefer to wrap it in tf.keras.Model as I plan to add other components/features to this network. Can anyone suggest a solution to saving the current model?</p>
","9555101","","","","","2020-02-24 09:19:35","Trouble saving tf.keras model with Bert (huggingface) classifier","<python><tensorflow2.0><huggingface-transformers>","1","3","","","","CC BY-SA 4.0"
"60173639","1","60175519","","2020-02-11 16:41:17","","0","396","<p>In <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface transformer</a>, it is possible to use the pre-trained GPT2-XL language model. But I don't find, on which dataset it is trained? Is it the same trained model which OpenAI used for their <a href=""https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"" rel=""nofollow noreferrer"">paper</a> (trained on 40GB dataset called <code>webtext</code>) ?</p>
","3363813","","","","","2020-02-11 18:47:29","Size of the training data of GPT2-XL pre-trained model","<pytorch><language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67836223","1","","","2021-06-04 11:12:17","","0","47","<p>I am trying to train an MLM model using HuggingFace.</p>
<pre><code>trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train,
    eval_dataset=test
)
</code></pre>
<p>And it returns this error: <code>AttributeError: 'str' object has no attribute</code>, does anyone have any idea what have I done wrong?</p>
","14762296","","14762296","","2021-06-04 13:30:23","2021-06-04 13:30:23","HuggingFace AttributeError: 'str' object has no attribute 'to'","<huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"67848275","1","","","2021-06-05 09:43:57","","0","80","<p>First of all I have search all stack overflow I know that the error message is the same but different root cause. Here is the error message :</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-38-29d47e6260b2&gt; in &lt;module&gt;()
----&gt; 1 trainer.train( )

4 frames
/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py in forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)
    674             batch_size = inputs_embeds.shape[0]
    675         else:
--&gt; 676             raise ValueError(&quot;You have to specify either input_ids or inputs_embeds&quot;)
    677 
    678         device = input_ids.device if input_ids is not None else inputs_embeds.device
</code></pre>
<p>and below is the code that possibly causing the error</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model, Trainer, trainer_utils, TrainingArguments
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')

from datasets import Dataset
dataset = Dataset.from_text('/content/chatbot.txt')

trainArgs = TrainingArguments( output_dir = os.path.join( os.getcwd(), 'customGPT2' )
                              , overwrite_output_dir = True
                              , do_train=True
                              , do_eval=True
                              , evaluation_strategy='steps'
                              , per_device_train_batch_size=4
                              , per_device_eval_batch_size =4
                              , gradient_accumulation_steps=1
                              , eval_accumulation_steps=1
                              , weight_decay=0
                              , adam_epsilon= 1e-08
                              , max_grad_norm = 1.0
                              , num_train_epochs =3.0
                              , max_steps = -1
                              , lr_scheduler_type = trainer_utils.SchedulerType('linear')
                              , logging_dir = os.path.join( os.getcwd(), 'log' ) 
                              , logging_steps = 2000
                              , logging_strategy = 'steps'
                              , save_steps = 2000
                              , save_strategy = 'steps'
                              , seed = 66
                              , fp16 = False
                              , fp16_opt_level = 'O1')

trainer=Trainer( model, args = trainArgs,train_dataset=dataset)
</code></pre>
<p>I just follow documentation with a bit changing bit to fit my need so when I troubleshoot this. Please help me as I am quite new to training on hugging face transformer. Thanks</p>
","9680282","","","","","2021-06-05 09:43:57","ValueError: You have to specify either input_ids or inputs_embeds","<python><torch><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"67798070","1","","","2021-06-02 02:18:10","","3","183","<p>I am more or less following <a href=""http://4/1AY0e-g4pMh6JPfkexh5nvWf9lvug3sHK98_jxAnwhsYlrB3F20Jkp350PKY"" rel=""nofollow noreferrer"">this example</a> to integrate the ray tune hyperparameter library with the huggingface transformers library using my own dataset.</p>
<p>Here is my script:</p>
<pre><code>import ray
from ray import tune
from ray.tune import CLIReporter
from ray.tune.examples.pbt_transformers.utils import download_data, \
    build_compute_metrics_fn
from ray.tune.schedulers import PopulationBasedTraining
from transformers import glue_tasks_num_labels, AutoConfig, \
    AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments

def get_model():
    # tokenizer = AutoTokenizer.from_pretrained(model_name, additional_special_tokens = ['[CHARACTER]'])
    model = ElectraForSequenceClassification.from_pretrained('google/electra-small-discriminator', num_labels=2)
    model.resize_token_embeddings(len(tokenizer))
    return model

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

training_args = TrainingArguments(
    &quot;electra_hp_tune&quot;,
    report_to = &quot;wandb&quot;,
    learning_rate=2e-5,  # config
    do_train=True,
    do_eval=True,
    evaluation_strategy=&quot;epoch&quot;,
    load_best_model_at_end=True,
    num_train_epochs=2,  # config
    per_device_train_batch_size=16,  # config
    per_device_eval_batch_size=16,  # config
    warmup_steps=0,
    weight_decay=0.1,  # config
    logging_dir=&quot;./logs&quot;,
)

trainer = Trainer(
    model_init=get_model,
    args=training_args,
    train_dataset=chunked_encoded_dataset['train'],
    eval_dataset=chunked_encoded_dataset['validation'],
    compute_metrics=compute_metrics
)

tune_config = {
    &quot;per_device_train_batch_size&quot;: 32,
    &quot;per_device_eval_batch_size&quot;: 32,
    &quot;num_train_epochs&quot;: tune.choice([2, 3, 4, 5])
}

scheduler = PopulationBasedTraining(
    time_attr=&quot;training_iteration&quot;,
    metric=&quot;eval_acc&quot;,
    mode=&quot;max&quot;,
    perturbation_interval=1,
    hyperparam_mutations={
        &quot;weight_decay&quot;: tune.uniform(0.0, 0.3),
        &quot;learning_rate&quot;: tune.uniform(1e-5, 2.5e-5),
        &quot;per_device_train_batch_size&quot;: [16, 32, 64],
    })

reporter = CLIReporter(
    parameter_columns={
        &quot;weight_decay&quot;: &quot;w_decay&quot;,
        &quot;learning_rate&quot;: &quot;lr&quot;,
        &quot;per_device_train_batch_size&quot;: &quot;train_bs/gpu&quot;,
        &quot;num_train_epochs&quot;: &quot;num_epochs&quot;
    },
    metric_columns=[
        &quot;eval_f1&quot;, &quot;eval_loss&quot;, &quot;epoch&quot;, &quot;training_iteration&quot;
    ])

from ray.tune.integration.wandb import WandbLogger
trainer.hyperparameter_search(
    hp_space=lambda _: tune_config,
    backend=&quot;ray&quot;,
    n_trials=10,
    scheduler=scheduler,
    keep_checkpoints_num=1,
    checkpoint_score_attr=&quot;training_iteration&quot;,
    progress_reporter=reporter,
    name=&quot;tune_transformer_gr&quot;)
</code></pre>
<p>The last function call (to trainer.hyperparameter_search) is when the error is raised. The error message is:</p>
<blockquote>
<p>AttributeError: module 'pickle' has no attribute 'PickleBuffer'</p>
</blockquote>
<p>And here is the full stack trace:</p>
<blockquote>
<hr />
<p>AttributeError                            Traceback (most recent call
last)</p>
<p> in ()
8     checkpoint_score_attr=&quot;training_iteration&quot;,
9     progress_reporter=reporter,
---&gt; 10     name=&quot;tune_transformer_gr&quot;)</p>
<p>14 frames</p>
<p>/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in
hyperparameter_search(self, hp_space, compute_objective, n_trials,
direction, backend, hp_name, **kwargs)    1666     1667<br />
run_hp_search = run_hp_search_optuna if backend ==
HPSearchBackend.OPTUNA else run_hp_search_ray
-&gt; 1668         best_run = run_hp_search(self, n_trials, direction, **kwargs)    1669     1670         self.hp_search_backend = None</p>
<p>/usr/local/lib/python3.7/dist-packages/transformers/integrations.py in
run_hp_search_ray(trainer, n_trials, direction, **kwargs)
231
232     analysis = ray.tune.run(
--&gt; 233         ray.tune.with_parameters(_objective, local_trainer=trainer),
234         config=trainer.hp_space(None),
235         num_samples=n_trials,</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py in
with_parameters(trainable, **kwargs)
294     prefix = f&quot;{str(trainable)}_&quot;
295     for k, v in kwargs.items():
--&gt; 296         parameter_registry.put(prefix + k, v)
297
298     trainable_name = getattr(trainable, &quot;<strong>name</strong>&quot;, &quot;tune_with_parameters&quot;)</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/tune/registry.py in
put(self, k, v)
160         self.to_flush[k] = v
161         if ray.is_initialized():
--&gt; 162             self.flush()
163
164     def get(self, k):</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/tune/registry.py in
flush(self)
169     def flush(self):
170         for k, v in self.to_flush.items():
--&gt; 171             self.references[k] = ray.put(v)
172         self.to_flush.clear()
173</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py
in wrapper(*args, **kwargs)
45         if client_mode_should_convert():
46             return getattr(ray, func.<strong>name</strong>)(*args, **kwargs)
---&gt; 47         return func(*args, **kwargs)
48
49     return wrapper</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/worker.py in put(value)<br />
1512     with profiling.profile(&quot;ray.put&quot;):    1513         try:
-&gt; 1514             object_ref = worker.put_object(value)    1515         except ObjectStoreFullError:    1516             logger.info(</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/worker.py in
put_object(self, value, object_ref)
259                                         &quot;inserting with an ObjectRef&quot;)
260
--&gt; 261         serialized_value = self.get_serialization_context().serialize(value)
262         # This <em>must</em> be the first place that we construct this python
263         # ObjectRef because an entry with 0 local references is created when</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/serialization.py in
serialize(self, value)
322             return RawSerializedObject(value)
323         else:
--&gt; 324             return self._serialize_to_msgpack(value)</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/serialization.py in
_serialize_to_msgpack(self, value)
302             metadata = ray_constants.OBJECT_METADATA_TYPE_PYTHON
303             pickle5_serialized_object = <br />
--&gt; 304                 self._serialize_to_pickle5(metadata, python_objects)
305         else:
306             pickle5_serialized_object = None</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/serialization.py in
_serialize_to_pickle5(self, metadata, value)
262         except Exception as e:
263             self.get_and_clear_contained_object_refs()
--&gt; 264             raise e
265         finally:
266             self.set_out_of_band_serialization()</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/serialization.py in
_serialize_to_pickle5(self, metadata, value)
259             self.set_in_band_serialization()
260             inband = pickle.dumps(
--&gt; 261                 value, protocol=5, buffer_callback=writer.buffer_callback)
262         except Exception as e:
263             self.get_and_clear_contained_object_refs()</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/cloudpickle/cloudpickle_fast.py
in dumps(obj, protocol, buffer_callback)
71                 file, protocol=protocol, buffer_callback=buffer_callback
72             )
---&gt; 73             cp.dump(obj)
74             return file.getvalue()
75</p>
<p>/usr/local/lib/python3.7/dist-packages/ray/cloudpickle/cloudpickle_fast.py
in dump(self, obj)
578     def dump(self, obj):
579         try:
--&gt; 580             return Pickler.dump(self, obj)
581         except RuntimeError as e:
582             if &quot;recursion&quot; in e.args[0]:</p>
<p>/usr/local/lib/python3.7/dist-packages/pyarrow/io.pxi in
pyarrow.lib.Buffer.<strong>reduce_ex</strong>()</p>
<p>AttributeError: module 'pickle' has no attribute 'PickleBuffer'</p>
</blockquote>
<p>My environment set-up:</p>
<ul>
<li>Am using Google Colab</li>
<li>Platform: Linux-5.4.109+-x86_64-with-Ubuntu-18.04-bionic</li>
<li>Python version: 3.7.10</li>
<li>Transformers version: 4.6.1</li>
<li>ray version: 1.3.0</li>
</ul>
<p>What I have tried:</p>
<ul>
<li>Updating pickle</li>
<li>Installed and imported pickle5 as pickle</li>
<li>Made sure that I did not have a python file with the name of 'pickle' in my immediate directory</li>
</ul>
<p>Where is this bug coming from and how can I resolve it?</p>
","7254514","","7254514","","2021-06-02 18:01:34","2021-09-21 23:45:37","Raytune is throwing error: ""module 'pickle' has no attribute 'PickleBuffer'"" when attempting hyperparameter search","<python><pickle><huggingface-transformers><ray-tune>","1","2","1","","","CC BY-SA 4.0"
"67812969","1","","","2021-06-02 21:48:31","","0","78","<p>I am trying to load in a Tensorflow model from HuggingFace here: <a href=""https://huggingface.co/facebook/wav2vec2-base-960h"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/wav2vec2-base-960h</a></p>
<p>It seems like most development work was done in PyTorch, so when I run the default commands they ask us to use:</p>
<pre><code> tokenizer = Wav2Vec2Tokenizer.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)
 model = Wav2Vec2ForCTC.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)
</code></pre>
<p>then it creates a torch model.</p>
<p>I noticed there was a 'tf_model.h5' file in the huggingface repo. But if I load in the tf_model.h5 file and run the command tf.keras.models.load_models('tf_model.h5'), it fails with the following error:</p>
<pre><code>OSError: SavedModel file does not exist at: tf_model.h5/{saved_model.pbtxt|saved_model.pb}
</code></pre>
<p>What do I need to do to read in this as a tensorflow model?</p>
","6441908","","","","","2021-06-02 21:48:31","How to load Wav2Vec2 TF model form HuggingFace","<tensorflow><huggingface-transformers>","0","0","1","","","CC BY-SA 4.0"
"67851322","1","67861565","","2021-06-05 15:49:48","","0","392","<p>I have followed this tutorial for masked language modelling from Hugging Face using BERT, but I am unsure how to actually deploy the model.</p>
<p>Tutorial: <a href=""https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb"" rel=""nofollow noreferrer"">https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb</a></p>
<p>I have trained the model using my own dataset, which has worked fine, but I don't know how to actually use the model, as the notebook does not include an example on how to do this, sadly.</p>
<p><a href=""https://i.stack.imgur.com/dLyex.png"" rel=""nofollow noreferrer"">Example of what I want to do with my trained model</a></p>
<p>On the Hugging Face website, this is the code used in the example; hence, I want to do this exact thing but with my model:</p>
<pre><code>&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='bert-base-uncased')
&gt;&gt;&gt; unmasker(&quot;Hello I'm a [MASK] model.&quot;)

[{'sequence': &quot;[CLS] hello i'm a fashion model. [SEP]&quot;,
  'score': 0.1073106899857521,
  'token': 4827,
  'token_str': 'fashion'},
 {'sequence': &quot;[CLS] hello i'm a role model. [SEP]&quot;,
  'score': 0.08774490654468536,
  'token': 2535,
  'token_str': 'role'},
 {'sequence': &quot;[CLS] hello i'm a new model. [SEP]&quot;,
  'score': 0.05338378623127937,
  'token': 2047,
  'token_str': 'new'},
 {'sequence': &quot;[CLS] hello i'm a super model. [SEP]&quot;,
  'score': 0.04667217284440994,
  'token': 3565,
  'token_str': 'super'},
 {'sequence': &quot;[CLS] hello i'm a fine model. [SEP]&quot;,
  'score': 0.027095865458250046,
  'token': 2986,
  'token_str': 'fine'}
</code></pre>
<p>Any help on how to do this would be great.</p>
","","user14946125","","","","2021-06-06 16:53:34","How to test masked language model after training it?","<python><nlp><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67851797","1","","","2021-06-05 16:44:09","","0","29","<p>I am using the following sample code for Causal LM from HuggingFace:</p>
<pre><code>from transformers import AutoModelWithLMHead, AutoTokenizer, top_k_top_p_filtering
import torch
from torch.nn import functional as F


tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)
model = AutoModelWithLMHead.from_pretrained(&quot;gpt2&quot;)

sequence = f&quot;Hugging Face is based in DUMBO, New York City, and &quot;

input_ids = tokenizer.encode(sequence, return_tensors=&quot;pt&quot;)

# get logits of last hidden state
next_token_logits = model(input_ids)[0][:, -1, :]

# filter
filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)

# sample
probs = F.softmax(filtered_next_token_logits, dim=-1)
next_token = torch.multinomial(probs, num_samples=1)

generated = torch.cat([input_ids, next_token], dim=-1)

resulting_string = tokenizer.decode(generated.tolist()[0])
print(resulting_string)
</code></pre>
<p>The next token generated by the sample code is <code>has</code> whereas when I run the same code on my local system, the next token generated in <code>\xa0</code>. Seems like somewhere version mismatch about the vocabulary be there. Can anyone give a precise answer about this ambiguity?</p>
<p>PS: transformers version on my local system: 4.6.1</p>
","3306097","","","","","2021-06-05 19:57:50","Next_token ambiguity in Causal Language Modeling sample","<python><machine-learning><nlp><pytorch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"69084798","1","","","2021-09-07 08:29:13","","1","18","<p>When input the same image, in Google ViT model output.last_hidden_state is not equal to output.hidden_states[-1] ?
I tried in Bertï¼Œ the outputs are the same.</p>
<p>feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')</p>
<pre><code>model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')
inputs = feature_extractor(images=[image], return_tensors=&quot;pt&quot;)
outputs = model(pixel_values=inputs['pixel_values'], output_hidden_states=True)

vec1 = outputs.hidden_states[-1][0, 0, :]
vec2 = outputs.last_hidden_state[0, 0, :]
</code></pre>
<p>in my mind, vec1 should be the same as vec2. But the fact is they are not the same at all.</p>
","10904769","","","","","2021-09-07 08:29:13","In transformers of ViT model, last_hidden_state is not equal to hidden_states[-1]","<python><pytorch><huggingface-transformers><transformer>","0","0","1","","","CC BY-SA 4.0"
"60120043","1","61558319","","2020-02-07 19:40:11","","8","11166","<p>I'm trying to fine-tune a model with BERT (using <code>transformers</code> library), and I'm a bit unsure about the optimizer and scheduler.</p>

<p>First, I understand that I should use <code>transformers.AdamW</code> instead of Pytorch's version of it. Also, we should use a warmup scheduler as suggested in the paper, so the scheduler is created using <code>get_linear_scheduler_with_warmup</code> function from <code>transformers</code> package.</p>

<p>The main questions I have are:</p>

<ol>
<li><code>get_linear_scheduler_with_warmup</code> should be called with the warm up. Is it ok to use 2 for warmup out of 10 epochs? </li>
<li>When should I call <code>scheduler.step()</code>? If I do after <code>train</code>, the learning rate is zero for the first epoch. Should I call it for each batch?</li>
</ol>

<p>Am I doing something wrong with this?</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import AdamW
from transformers.optimization import get_linear_scheduler_with_warmup

N_EPOCHS = 10

model = BertGRUModel(finetune_bert=True,...)
num_training_steps = N_EPOCHS+1
num_warmup_steps = 2
warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1

optimizer = AdamW(model.parameters())
criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([class_weights[1]]))


scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=num_warmup_steps, 
    num_training_steps=num_training_steps
)

for epoch in range(N_EPOCHS):
    scheduler.step() #If I do after train, LR = 0 for the first epoch
    print(optimizer.param_groups[0][""lr""])

    train(...) # here we call optimizer.step()
    evaluate(...)
</code></pre>

<p>My model and train routine(quite similar to <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb"" rel=""noreferrer"">this notebook</a>)</p>

<pre class=""lang-py prettyprint-override""><code>class BERTGRUSentiment(nn.Module):
    def __init__(self,
                 bert,
                 hidden_dim,
                 output_dim,
                 n_layers=1, 
                 bidirectional=False,
                 finetune_bert=False,
                 dropout=0.2):

        super().__init__()

        self.bert = bert

        embedding_dim = bert.config.to_dict()['hidden_size']

        self.finetune_bert = finetune_bert

        self.rnn = nn.GRU(embedding_dim,
                          hidden_dim,
                          num_layers = n_layers,
                          bidirectional = bidirectional,
                          batch_first = True,
                          dropout = 0 if n_layers &lt; 2 else dropout)

        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)        
        self.dropout = nn.Dropout(dropout)

    def forward(self, text):    
        #text = [batch size, sent len]

        if not self.finetune_bert:
            with torch.no_grad():
                embedded = self.bert(text)[0]
        else:
            embedded = self.bert(text)[0]
        #embedded = [batch size, sent len, emb dim]
        _, hidden = self.rnn(embedded)

        #hidden = [n layers * n directions, batch size, emb dim]

        if self.rnn.bidirectional:
            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))
        else:
            hidden = self.dropout(hidden[-1,:,:])

        #hidden = [batch size, hid dim]

        output = self.out(hidden)

        #output = [batch size, out dim]

        return output


import torch
from sklearn.metrics import accuracy_score, f1_score


def train(model, iterator, optimizer, criterion, max_grad_norm=None):
    """"""
    Trains the model for one full epoch
    """"""
    epoch_loss = 0
    epoch_acc = 0

    model.train()

    for i, batch in enumerate(iterator):
        optimizer.zero_grad()
        text, lens = batch.text

        predictions = model(text)

        target = batch.target

        loss = criterion(predictions.squeeze(1), target)

        prob_predictions = torch.sigmoid(predictions)

        preds = torch.round(prob_predictions).detach().cpu()
        acc = accuracy_score(preds, target.cpu())

        loss.backward()
        # Gradient clipping
        if max_grad_norm:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator)


</code></pre>
","578696","","","","","2020-05-05 19:10:19","Optimizer and scheduler for BERT fine-tuning","<nlp><pytorch><huggingface-transformers>","2","0","1","","","CC BY-SA 4.0"
"60142937","1","60481050","","2020-02-10 01:14:21","","4","1934","<p>I wanted to test TextGeneration with CTRL using PyTorch-Transformers, before using it for fine-tuning. But it doesn't prompt anything like it does with GPT-2 and other similar language generation models. I'm very new for this and am stuck and can't figure out what's going on. </p>

<p>This is the procedure I followed in my Colab notebook,</p>

<pre><code>!pip install transformers
</code></pre>

<pre><code>!git clone https://github.com/huggingface/pytorch-transformers.git
</code></pre>

<pre><code>!python pytorch-transformers/examples/run_generation.py \
    --model_type=ctrl \
    --length=100 \
    --model_name_or_path=ctrl \
    --temperature=0.2 \
    --repetition_penalty=1.2 \
</code></pre>

<p>And this is what I get after running the script</p>

<pre><code>02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-vocab.json from cache at /root/.cache/torch/transformers/a858ad854d3847b02da3aac63555142de6a05f2a26d928bb49e881970514e186.285c96a541cf6719677cfb634929022b56b76a0c9a540186ba3d8bbdf02bca42
02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-merges.txt from cache at /root/.cache/torch/transformers/aa2c569e6648690484ade28535a8157aa415f15202e84a62e82cc36ea0c20fa9.26153bf569b71aaf15ae54be4c1b9254dbeff58ca6fc3e29468c4eed078ac142
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   loading configuration file https://storage.googleapis.com/sf-ctrl/pytorch/ctrl-config.json from cache at /root/.cache/torch/transformers/d6492ca334c2a4e079f43df30956acf935134081b2b3844dc97457be69b623d0.1ebc47eb44e70492e0c20494a084f108332d20fea7fe5ad408ef5e7a8f2baef4
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   Model config CTRLConfig {
  ""architectures"": null,
  ""attn_pdrop"": 0.1,
  ""bos_token_id"": 0,
  ""dff"": 8192,
  ""do_sample"": false,
  ""embd_pdrop"": 0.1,
  ""eos_token_ids"": 0,
  ""finetuning_task"": null,
  ""from_tf"": false,
  ""id2label"": {
    ""0"": ""LABEL_0""
  },
  ""initializer_range"": 0.02,
  ""is_decoder"": false,
  ""label2id"": {
    ""LABEL_0"": 0
  },
  ""layer_norm_epsilon"": 1e-06,
  ""length_penalty"": 1.0,
  ""max_length"": 20,
  ""model_type"": ""ctrl"",
  ""n_ctx"": 512,
  ""n_embd"": 1280,
  ""n_head"": 16,
  ""n_layer"": 48,
  ""n_positions"": 50000,
  ""num_beams"": 1,
  ""num_labels"": 1,
  ""num_return_sequences"": 1,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""output_past"": true,
  ""pad_token_id"": 0,
  ""pruned_heads"": {},
  ""repetition_penalty"": 1.0,
  ""resid_pdrop"": 0.1,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""temperature"": 1.0,
  ""top_k"": 50,
  ""top_p"": 1.0,
  ""torchscript"": false,
  ""use_bfloat16"": false,
  ""vocab_size"": 246534
}

02/10/2020 01:02:31 - INFO - transformers.modeling_utils -   loading weights file https://storage.googleapis.com/sf-ctrl/pytorch/seqlen256_v1.bin from cache at /root/.cache/torch/transformers/c146cc96724f27295a0c3ada1fbb3632074adf87e9aef8269e44c9208787f8c8.b986347cbab65fa276683efbb9c2f7ee22552277bcf6e1f1166557ed0852fdf0
tcmalloc: large alloc 1262256128 bytes == 0x38b92000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x5096b7 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9
tcmalloc: large alloc 1262256128 bytes == 0x19fdda000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50d390 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5a067e 0x50d966 0x508245
^C
</code></pre>

<p>and then terminates. Could this be because of a GPU problem?</p>

<p>Any sort of help is appreciated.</p>
","10598769","","10598769","","2020-03-02 02:21:08","2020-03-02 02:21:08","HuggingFace Transformers For Text Generation with CTRL with Google Colab's free GPU","<python><deep-learning><nlp><pytorch><huggingface-transformers>","1","4","1","","","CC BY-SA 4.0"
"60170037","1","60171807","","2020-02-11 13:32:13","","2","905","<p><a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">Hugging Face documentation describes</a> how to do a sequence classification using a Bert model:</p>

<pre><code>from transformers import BertTokenizer, BertForSequenceClassification
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)

loss, logits = outputs[:2]
</code></pre>

<p>However, there is only example for batch size 1. How to implement it when we have a list of phrases and want to use a bigger batch size?</p>
","8645424","","","","","2020-02-11 15:02:16","How to use a batch size bigger than zero in Bert Sequence Classification","<python><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67883723","1","","","2021-06-08 08:04:06","","0","72","<p>I want to use the GPT2 from huggingface transformers in tensorflow keras model definition.</p>
<pre><code>input_ids = tf.keras.layers.Input(
    shape=(max_len,), dtype=tf.int32, name=&quot;input_ids&quot;
)
attention_masks = tf.keras.layers.Input(
    shape=(max_len,), dtype=tf.int32, name=&quot;attention_masks&quot;
)
gpt2 = TFGPT2LMHeadModel.from_pretrained('gpt2')
gpt2.trainable = True

#outputs = model(inputs)
output_sequences = gpt2.generate(
    input_ids = input_ids,#inputs['input_ids'],
    attention_mask = attention_masks, #inputs['attention_mask'],
    max_length= max_len*2,
    temperature=1,
    top_k=0,
    top_p=0.9,
    repetition_penalty=1,
    do_sample=True,
    num_return_sequences=num_return_sequences
)
model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output_sequences)
</code></pre>
<p>however, <code>gpt2.generate</code> can not take input_ids and attention_masks as inputs.</p>
<p>The error:</p>
<blockquote>
<p>TypeError: Keras symbolic inputs/outputs do not implement <code>__len__</code>.
You may be trying to pass Keras symbolic inputs/outputs to a TF API
that does not register dispatching, preventing Keras from
automatically converting the API call to a lambda layer in the
Functional Model. This error will also get raised if you try asserting
a symbolic input/output directly.</p>
</blockquote>
<p>How can I use generate process of gpt2 in the model ?</p>
<p>The final goal if to calculate the loss outside, based on <code>output_sequences</code> and update the parameters of the model which contains GPT2.</p>
","6407393","","1000551","","2021-06-08 08:13:01","2021-06-08 08:13:01","How to use generation of gpt2 from huggingface transformers in tensorflow keras model?","<python><tensorflow2.0><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"60184117","1","60340477","","2020-02-12 08:32:48","","1","992","<p>I'm running roberta on huggingface <code>language_modeling.py</code>. After doing 400 steps I suddenly get a CUDA out of memory issue. Don't know how to deal with it. Can you please help? Thanks</p>
","12864870","","3607203","","2020-02-20 08:30:31","2020-02-21 14:18:40","How to check the root cause of CUDA out of memory issue in the middle of training?","<gpu><pytorch><huggingface-transformers>","2","2","1","","","CC BY-SA 4.0"
"67872803","1","67873520","","2021-06-07 13:42:11","","0","155","<p>I am trying to use the pretrained SciBERT model (<a href=""https://huggingface.co/allenai/scibert_scivocab_uncased"" rel=""nofollow noreferrer"">https://huggingface.co/allenai/scibert_scivocab_uncased</a>) from Huggingface to predict masked words in scientific/biomedical text.  This produces errors, and not sure how to move forward from this point.</p>
<p>Here is the code so far -</p>
<pre><code>!pip install transformers

from transformers import pipeline, AutoTokenizer, AutoModel
  
tokenizer = AutoTokenizer.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)

model = AutoModel.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)

unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)
unmasker(&quot;the patient is a 55 year old [MASK] admitted with pneumonia&quot;)
</code></pre>
<p>This works with BERT alone, but is not the specialized pre-trained model -</p>
<pre><code>!pip install transformers

from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')
unmasker(&quot;the patient is a 55 year old [MASK] admitted with pneumonia&quot;)
</code></pre>
<p>The errors with SciBERT are -</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)
    494         kwargs[&quot;feature_extractor&quot;] = feature_extractor
    495 
--&gt; 496     return task_class(model=model, framework=framework, task=task, **kwargs)

/usr/local/lib/python3.7/dist-packages/transformers/pipelines/fill_mask.py in __init__(self, model, tokenizer, modelcard, framework, args_parser, device, top_k, task)
     73         )
     74 
---&gt; 75         self.check_model_type(TF_MODEL_WITH_LM_HEAD_MAPPING if self.framework == &quot;tf&quot; else MODEL_FOR_MASKED_LM_MAPPING)
     76         self.top_k = top_k
     77 

/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py in check_model_type(self, supported_models)
    652                 self.task,
    653                 self.model.base_model_prefix,
--&gt; 654                 f&quot;The model '{self.model.__class__.__name__}' is not supported for {self.task}. Supported models are {supported_models}&quot;,
    655             )
    656 

PipelineException: The model 'BertModel' is not supported for fill-mask. Supported models are ['BigBirdForMaskedLM', 'Wav2Vec2ForMaskedLM', 'ConvBertForMaskedLM', 'LayoutLMForMaskedLM', 'DistilBertForMaskedLM', 'AlbertForMaskedLM', 'BartForConditionalGeneration', 'MBartForConditionalGeneration', 'CamembertForMaskedLM', 'XLMRobertaForMaskedLM', 'LongformerForMaskedLM', 'RobertaForMaskedLM', 'SqueezeBertForMaskedLM', 'BertForMaskedLM', 'MegatronBertForMaskedLM', 'MobileBertForMaskedLM', 'FlaubertWithLMHeadModel', 'XLMWithLMHeadModel', 'ElectraForMaskedLM', 'ReformerForMaskedLM', 'FunnelForMaskedLM', 'MPNetForMaskedLM', 'TapasForMaskedLM', 'DebertaForMaskedLM', 'DebertaV2ForMaskedLM', 'IBertForMaskedLM']
</code></pre>
","8858290","","9197808","","2021-06-07 14:23:54","2021-06-07 14:28:00","Huggingface SciBERT predict masked word not working","<python><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60187939","1","","","2020-02-12 12:04:50","","2","1718","<p>I trained my own BERT NER following this Medium post: <code>https://medium.com/@yingbiao/ner-with-bert-in-action-936ff275bc73</code></p>

<p>I saved my model to the disc and successfully loaded it. </p>

<pre><code>model = BertForTokenClassification.from_pretrained(bert_out_address, num_labels=len(tag2idx))
</code></pre>

<p>model.eval() works:</p>

<pre><code>model.eval()
</code></pre>

<p>I am new to BERT and the transformer lib. I would expect something similar to </p>

<pre><code>model.predict('Hello I am an example sentence') 
</code></pre>

<p>would show me recognised entities.</p>

<p>I also tried:</p>

<pre><code>input_ids = torch.tensor([tokenizer.encode(""Here is some text to encode"")])
output = model(input_ids)
</code></pre>

<p>where output gives me a big tensor and I do not know what to do with it.</p>

<p>How can I use the model now to predict the entities in an example sentence? What am I supposed to do with the output?</p>

<p>Thanks!</p>
","12885322","","12885322","","2020-02-12 15:21:44","2020-02-13 09:25:47","How do I use my trained BERT NER (named entity recognition) model to predict a new example?","<ner><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60186935","1","","","2020-02-12 11:06:10","","24","3206","<p>There is a problem we are trying to solve where we want to do a semantic search on our set of data,
i.e we have a domain-specific data (example: sentences talking about automobiles)</p>

<p>Our data is just a bunch of sentences and what we want is to give a phrase and get back the sentences which are:</p>

<ol>
<li>Similar to that phrase</li>
<li>Has a part of a sentence that is similar to the phrase</li>
<li>A sentence which is having contextually similar meanings </li>
</ol>

<p><br/></p>

<p>Let me try giving you an example suppose I search for the phrase ""Buying Experience"", I should get the sentences like:</p>

<ul>
<li>I never thought car buying could take less than 30 minutes to sign
and buy.</li>
<li><p>I found a car that i liked and the purchase process was<br>
straightforward and easy</p></li>
<li><p>I absolutely hated going car shopping, but today iâ€™m glad i did</p></li>
</ul>

<p><br/>
I want to lay emphasis on the fact that we are looking for <strong>contextual similarity</strong> and not just a brute force word search.</p>

<p>If the sentence uses different words then also it should be able to find it.</p>

<p>Things that we have already tried:</p>

<ol>
<li><p><a href=""https://www.opensemanticsearch.org/"" rel=""noreferrer"">Open Semantic Search</a> the problem we faced here is generating ontology from the data we have, or
for that sake searching for available ontology from different domains of our interest.</p></li>
<li><p>Elastic Search(BM25 + Vectors(tf-idf)), we tried this where it gave a few sentences but precision was not that great. The accuracy was bad
as well. We tried against a human-curated dataset, it was able to get around 10% of the sentences only.</p></li>
<li><p>We tried different embeddings like the once mentioned in <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""noreferrer"">sentence-transformers</a> and also went through the <a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.p"" rel=""noreferrer"">example</a> and tried evaluating against our human-curated set
and that also had very low accuracy.</p></li>
<li><p>We tried <a href=""https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604"" rel=""noreferrer"">ELMO</a>. This was better but still lower accuracy than we expected and there is a
cognitive load to decide the cosine value below which we shouldn't consider the sentences. This even applies to point 3.</p></li>
</ol>

<p>Any help will be appreciated. Thanks a lot for the help in advance</p>
","2125612","","","","","2020-08-19 19:54:53","How to build semantic search for a given domain","<python><elasticsearch><nlp><sentence-similarity><huggingface-transformers>","3","7","12","","","CC BY-SA 4.0"
"67886512","1","","","2021-06-08 12:02:53","","0","28","<pre><code>from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, TFGPT2Model, TFAutoModelForCausalLM
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.padding_side = &quot;left&quot; 
tokenizer.pad_token = tokenizer.eos_token # to avoid an error

gpt2 = TFGPT2LMHeadModel.from_pretrained('gpt2')
gpt2.trainable = True

#model = TFAutoModelForCausalLM.from_pretrained(&quot;gpt2&quot;)
#model = TFGPT2LMHeadModel.from_pretrained('gpt2')

#model.train()
# when generating, we will use the logits of right-most token to predict the next token
# so the padding should be on the left



num_return_sequences = 1
#prompts = list(x_batch_train.numpy().reshape(-1))

#token_lens = [len(tokenizer.tokenize(sent)) for sent in prompts]
#max_length = math.ceil(np.array(token_lens).max())*2
max_len = get_tokens_len(ds, 0.99) 

cce = tf.keras.losses.CategoricalCrossentropy()   
optimizer = keras.optimizers.Adam(learning_rate=0.0001)


def loss_fn(output_sequences, labels):
    syn_sents = tokenizer.batch_decode(output_sequences, clean_up_tokenization_spaces=True, skip_special_tokens=True)
    syn_sents_pure = []
    for sent, sent_syn in zip(prompts, syn_sents):
        syn_sents_pure.append(sent_syn.replace(sent, '').replace('\n',' ').strip())

    preds = model(np.array(syn_sents_pure))

    assert preds.shape[0] == len(prompts) and preds.shape[1] == num_classes

    label_oht = tf.keras.utils.to_categorical( np.array([label_idx[l] for l in labels]), num_classes = num_classes, dtype='int' ) 
    label_oht_tf = tf.convert_to_tensor(label_oht)
    assert label_oht.shape == preds.shape

    loss_value = cce(label_oht_tf, preds)#.numpy()
    return loss_value

rows = ds.df_test.sample(5)
prompts = rows['content'].tolist()
labels = rows['label'].tolist()

with tf.GradientTape() as tape:
    # Run the forward pass of the layer.
    # The operations that the layer applies
    # to its inputs are going to be recorded
    # on the GradientTape.
    #logits = model(x_batch_train, training=True)  # Logits for this minibatch

    inputs = tokenizer(prompts, padding='max_length', truncation=True, max_length=max_len, return_tensors=&quot;tf&quot;)
    output_sequences = gpt2.generate(
        input_ids = inputs['input_ids'],
        attention_mask = inputs['attention_mask'],
        max_length= max_len*2,
        temperature=1,
        top_k=0,
        top_p=0.9,
        repetition_penalty=1,
        do_sample=True,
        num_return_sequences=num_return_sequences
    )

    # Compute the loss value for this minibatch.
    #loss_value = loss_fn(y_batch_train, logits)
    loss_value = loss_fn(output_sequences, labels) # &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.062384058&gt;


# Use the gradient tape to automatically retrieve
# the gradients of the trainable variables with respect to the loss.
grads = tape.gradient(loss_value, gpt2.trainable_weights)
</code></pre>
<p>I load the pre-trained model gpt2 from <code>TFGPT2LMHeadModel</code> and I use its synthesis sentences given prompts to calculate the loss.</p>
<p>The loss seems ok, it is a tensor, such as `</p>
<blockquote>
<p>&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0446845&gt;</p>
</blockquote>
<p>`</p>
<p>But all the elements of <code>grads</code> is</p>
<blockquote>
<p>None</p>
</blockquote>
<p>Why this? Any hints ?</p>
<p>Thanks.</p>
","6407393","","","","","2021-06-08 12:02:53","grads is None when using GPT2 transformers in tensorflow","<tensorflow2.0><huggingface-transformers><gpt-2>","0","0","","","","CC BY-SA 4.0"
"67849144","1","","","2021-06-05 11:35:35","","0","55","<pre><code>import numpy as np
import torch
import torchvision
from transformers import MarianTokenizer, MarianMTModel
from typing import List
</code></pre>
<p>I'm getting</p>
<pre><code>ImportError: cannot import name 'MarianMTModel' 
</code></pre>
<p>I've already installed transformers. What can I do to solve this issue?<br />
<strong>Edit</strong> : Does this import Marian model implemented in C++?</p>
","12219510","","12219510","","2021-06-10 06:28:54","2021-06-10 06:28:54","ImportError: cannot import name 'MarianMTModel'","<python><machine-learning><huggingface-transformers>","0","4","0","","","CC BY-SA 4.0"
"67861716","1","","","2021-06-06 17:09:49","","0","43","<p>After a week of struggle, I had to come here and ask professionals. I've been trying to code my own GPT model from scratch using PyTorch and I did the part but now I'm confused about how to prepare my data for the pre-training part for example I tried using sentencePiece tokenizer to tokenize my huge &quot;Hindi&quot; language corpus but somehow it is throwing errors.</p>
<p>So can anyone guide me on how do I prepare my Hindi language corpus for pre-training my GPT, any library for tokenizing or preparing the data?</p>
<p>This is what I've tried yet:
I'm using <code>tokenizers</code> library.</p>
<pre class=""lang-py prettyprint-override""><code>from tokenizers.models import BPE
from tokenizers import Tokenizer
from tokenizers.decoders import ByteLevel as ByteLevelDecoder
from tokenizers.normalizers import NFKC, Sequence
from tokenizers.pre_tokenizers import ByteLevel
from tokenizers.trainers import BpeTrainer

class BytePairEncodingTokenizer(object):
    def __init__(self):
        self.tokenizer = Tokenizer(BPE())
        self.tokenizer.normalizer = Sequence([
            NFKC()
        ])
        self.tokenizer.pre_tokenizer = ByteLevel()
        self.tokenizer.decoder = ByteLevelDecoder()

    def bpe_train(self, paths):
        trainer = BpeTrainer(vocab_size=50_000, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[
            &quot;&lt;s&gt;&quot;,
            &quot;&lt;pad&gt;&quot;,
            &quot;&lt;/s&gt;&quot;,
            &quot;&lt;unk&gt;&quot;,
            &quot;&lt;mask&gt;&quot;
        ])
        self.tokenizer.train(trainer, paths)

    def save_tokenizer(self, location, prefix=None):
        if not os.path.exists(location):
            os.makedirs(location)
        self.tokenizer.model.save(location, prefix)

tokenizer = BytePairEncodingTokenizer()
tokenizer.bpe_train(&quot;/content/drive/MyDrive/datasets/hindi/hindi.txt&quot;)
tokenizer.save_tokenizer(&quot;/content/drive/MyDrive/datasets/hindi/&quot;, prefix=&quot;hindiTokenizer&quot;)
</code></pre>
<p>This was not working on my corpus.</p>
<p>So the main question is how to prepare my corpus for pre-training my GPT.</p>
<p>colab notebook: <a href=""https://colab.research.google.com/drive/1UdLrJ8zioMqsSylf4VGmETJiELP6FzFQ?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1UdLrJ8zioMqsSylf4VGmETJiELP6FzFQ?usp=sharing</a></p>
<p>dataset link: <a href=""https://drive.google.com/file/d/1MKA3FMDUMRqz-_AACv068zWqdHCYamvA/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1MKA3FMDUMRqz-_AACv068zWqdHCYamvA/view?usp=sharing</a></p>
","11685381","","3607203","","2021-06-07 09:35:12","2021-06-07 09:35:12","How to prepare data for training a GPT model from scratch on a new language","<nlp><pytorch><huggingface-transformers><transformer><gpt-2>","0","1","","","","CC BY-SA 4.0"
"67901540","1","","","2021-06-09 09:42:05","","0","19","<p>So I am trying to train a scibert-model and add extra features to the model which are transformed using tfidf. The approach I am taking is a lot like: <a href=""https://colab.research.google.com/drive/1eB8EMCwEE1_o5QOdC0gEejxqgkv6Q_cO?usp=sharing#scrollTo=dZGZhYgbtz--"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1eB8EMCwEE1_o5QOdC0gEejxqgkv6Q_cO?usp=sharing#scrollTo=dZGZhYgbtz--</a>.</p>
<p>The data is loaded using</p>
<pre><code>class DefinitionsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, extra_features, labels):
        self.encodings = encodings
        self.labels = labels
        self.extra_features = extra_features
    def __getitem__(self, idx):
        item = {
            k: torch.tensor(v[idx]).type(torch.LongTensor)
            for k, v in self.encodings.items()
        }
        item[&quot;labels&quot;] = torch.tensor([self.labels[idx]
                                       ]).type(torch.LongTensor)
        item[&quot;extra_features&quot;] = torch.tensor(
            [self.extra_features[idx]]).type(torch.LongTensor)
        return item
    def __len__(self):
        return len(self.labels)
</code></pre>
<p>Where <code>extra_features</code> is a TFiDF matrix.</p>
<p>When trying to train the model using the Trainer:</p>
<pre><code>training_args = TrainingArguments(
    output_dir = './results',  # output directory
    num_train_epochs = 6,  # total # of training epochs
    per_device_train_batch_size = 8,  # batch size per device during training
    per_device_eval_batch_size = 8,  # batch size for evaluation
    warmup_steps = 500,  # number of warmup steps for learning rate scheduler
    weight_decay = 0.01,  # strength of weight decay
    logging_dir = './logdirectory',  # directory for storing logs
    logging_steps = 1000,  # log &amp; save weights each X logging_steps
    evaluation_strategy = &quot;epoch&quot;,  # evaluate each `epoch`
)

# Set trainer
trainer = Trainer(
    model = model,  # the instantiated ðŸ¤— Transformers model to be trained
    args = training_args,  # training arguments, defined above
    train_dataset = train_data,  # training dataset
    eval_dataset = test_data,  # evaluation dataset
    compute_metrics = compute_metrics,  # compute metrics for evaluation
)

#Start training and evaluating:
print(&quot;Training&quot;)
trainer.train()
</code></pre>
<p>I get the error message:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-170-6c90adef35fd&gt; in &lt;module&gt;
     23 #Start training and evaluating:
     24 print(&quot;Training&quot;)
---&gt; 25 trainer.train()
     26 print(&quot;Evaluating&quot;)
     27 trainer.evaluate()

~/.local/lib/python3.8/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
   1244             self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)
   1245 
-&gt; 1246             for step, inputs in enumerate(epoch_iterator):
   1247 
   1248                 # Skip past any already trained steps if resuming training

~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py in __next__(self)
    515             if self._sampler_iter is None:
    516                 self._reset()
--&gt; 517             data = self._next_data()
    518             self._num_yielded += 1
    519             if self._dataset_kind == _DatasetKind.Iterable and \

~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py in _next_data(self)
    555     def _next_data(self):
    556         index = self._next_index()  # may raise StopIteration
--&gt; 557         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    558         if self._pin_memory:
    559             data = _utils.pin_memory.pin_memory(data)

~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]

~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py in &lt;listcomp&gt;(.0)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]

&lt;ipython-input-156-bcd0d62db01e&gt; in __getitem__(self, idx)
     12         item[&quot;labels&quot;] = torch.tensor([self.labels[idx]
     13                                        ]).type(torch.LongTensor)
---&gt; 14         item[&quot;extra_features&quot;] = torch.tensor(
     15             [self.extra_features[idx]]).type(torch.LongTensor)
     16         return item

ValueError: too many dimensions 'matrix'
</code></pre>
<p>I have tried inputting a dense and a sparse version of the matrix and tried to lower the number of features in the matrix, but I still get the error and I can't really find any others online with this error.
Any help would be appreciated!</p>
","14320652","","6664872","","2021-06-09 13:21:49","2021-06-09 13:21:49","What is wrong with this TFiDF SciBERT-input?","<python><nlp><pytorch><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"60192523","1","","","2020-02-12 16:07:03","","3","3599","<p>I have designed a model based on BERT to solve NER task. I am using <code>transformers</code> library with the <code>""dccuchile/bert-base-spanish-wwm-cased""</code> pre-trained model. The problem comes when my model detect an entity but the token is <code>'[UNK]'</code>. How could I know which is the string behind that token?</p>

<p>I know that an unknown token can't be reverted to the original one, but I would like to at least capture that values before passing the inputs to the model. </p>

<p>The code is really simple:</p>

<pre><code>    sentenceIds = tokenizer.encode(sentence,add_special_tokens = True)

    inputs = pad_sequences([sentenceIds], maxlen=256, dtype=""long"", 
                              value=0, truncating=""post"", padding=""post"")

    att_mask = torch.tensor([[int(token_id &gt; 0) for token_id in inputs[0]]]).to(device)
    inputs = torch.tensor(inputs).to(device)

    with torch.no_grad():        
        outputs = model(inputs, 
                          token_type_ids=None, 
                          attention_mask=att_mask)
</code></pre>

<p>As you see is really simple, just tokenize, padding or truncating, creating attentionMask and calling to the model.</p>

<p>I have tried using <code>regex</code>, trying to find the two tokens that are around it and things like that, but I can't solve it properly.</p>
","9683062","","9683062","","2020-02-13 09:46:45","2020-02-13 12:36:01","Get the value of '[UNK]' in BERT","<python-3.x><pytorch><bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"67783991","1","","","2021-06-01 06:53:43","","0","84","<p>I am training a multilingual bert model for a sentiment classification task. I have 2 GPUs on 1 Machine so I am using Huggingface <code>Accelerator</code> for distributed training. But when I run the code it throws a Runtime Error.</p>
<h2>Model</h2>
<pre><code>class BERTModel(nn.Module):
    def __init__(self):
        super(BERTModel, self).__init__()
        self.bert = transformers.BertModel.from_pretrained(&quot;bert-base-multilingual-uncased&quot;)
        self.bert_drop = nn.Dropout(0.3)
        self.out = nn.Linear(768 * 2, 1) # *2 since we have 2 pooling layers

    def forward(self, ids, mask, token_type_ids):
        o1, _ = self.bert(
            ids, 
            attention_mask=mask, 
            token_type_ids=token_type_ids
        )
        
        mean_pooling = torch.mean(o1, 1)
        max_pooling, _ = torch.max(o1, 1)
        cat = torch.cat((mean_pooling, max_pooling), 1)
        
        bo = self.bert_drop(cat)
        output = self.out(bo)
        return output
</code></pre>
<h2>Train Function</h2>
<pre><code>def train_fn(data_loader, model, optimizer, scheduler):
    
        
        &quot;&quot;&quot;
        Training Function for the Model
        
        parameters: data_loader - PyTorch DataLoader
                    model - The Model to be used for training
                    optimizer - The Optimizer to be used for training
                    scheduler - The Learning Rate Scheduler 
                    
        returns: None
        &quot;&quot;&quot;
        accelerator = Accelerator()

        model, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)
        model.train()

        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
            ids = d[&quot;ids&quot;]
            token_type_ids = d[&quot;token_type_ids&quot;]
            mask = d[&quot;mask&quot;]
            targets = d[&quot;targets&quot;]

            
            ids = ids.to(torch.long)
            token_type_ids = token_type_ids.to(torch.long)
            mask = mask.to(torch.long)
            targets = targets.to(torch.float)

            optimizer.zero_grad()
            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)

            loss = loss_fn(outputs, targets)
            
            if bi % 1000 == 0:
                print(f&quot;bi={bi}, loss={loss}&quot;)

            accelerator.backward(loss)
            optimizer.step()
            scheduler.step()
</code></pre>
<h2>Error</h2>
<pre><code>---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;

/opt/conda/lib/python3.6/site-packages/accelerate/notebook_launcher.py in notebook_launcher(function, args, num_processes, use_fp16, use_port)
    107             try:
    108                 print(f&quot;Launching a training on {num_processes} GPUs.&quot;)
--&gt; 109                 start_processes(launcher, nprocs=num_processes, start_method=&quot;fork&quot;)
    110             finally:
    111                 # Clean up the environment variables set.

/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)
    156 
    157     # Loop on join until it returns True or raises an exception.
--&gt; 158     while not context.join():
    159         pass
    160 

/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py in join(self, timeout)
    117         msg = &quot;\n\n-- Process %d terminated with the following error:\n&quot; % error_index
    118         msg += original_trace
--&gt; 119         raise Exception(msg)
    120 
    121 

Exception: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py&quot;, line 20, in _wrap
    fn(i, *args)
  File &quot;/opt/conda/lib/python3.6/site-packages/accelerate/utils.py&quot;, line 274, in __call__
    self.launcher(*args)
  File &quot;&lt;timed exec&gt;&quot;, line 276, in run
  File &quot;&lt;timed exec&gt;&quot;, line 74, in train_fn
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 726, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/cuda/amp/autocast_mode.py&quot;, line 135, in decorate_autocast
    return func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py&quot;, line 585, in forward
    self.reducer.prepare_for_backward([])

RuntimeError: Expected to have finished reduction in the prior iteration before 
starting a new one. This error indicates that your module has parameters that 
were not used in producing loss. You can enable unused parameter detection by (1) 
passing the keyword argument `find_unused_parameters=True` to 
`torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward`
 function outputs participate in calculating loss. If you already have done the 
above two steps, then the distributed data parallel module wasn't able to locate
 the output tensors in the return value of your module's `forward` function. 
Please include the loss function and the structure of the return value of 
`forward` of your module when reporting this issue (e.g. list, dict, iterable).
</code></pre>
","13540652","","","","","2021-06-01 06:53:43","Pytorch: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one","<python><python-3.x><nlp><pytorch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"67815850","1","","","2021-06-03 05:31:29","","0","272","<p>I trained a BERT based encoder decoder model (<code>EncoderDecoderModel</code>) named <code>ed_model</code> with HuggingFace's transformers module.</p>
<p>I used the <code>BertTokenizer</code> named as <code>input_tokenizer</code></p>
<p>I tokenized the input with:</p>
<pre><code>txt = &quot;Some wonderful sentence to encode&quot;
inputs = input_tokenizer(txt, return_tensors=&quot;pt&quot;).to(device)
print(inputs)
</code></pre>
<p>The output clearly shows that a <code>input_ids</code> is the return dict</p>
<pre><code>
{'input_ids': tensor([[ 101, 5660, 7975, 2127, 2053, 2936, 5061,  102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}
</code></pre>
<p>But when I try to predict, I get this error:
<code>ed_model.forward(**inputs)</code></p>
<blockquote>
<p>ValueError: You have to specify either input_ids or inputs_embeds</p>
</blockquote>
<p>Any ideas ?</p>
","1097347","","","","","2021-06-04 10:41:45","""You have to specify either input_ids or inputs_embeds"", but I did specify the input_ids","<bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"67828917","1","","","2021-06-03 21:34:05","","6","167","<p>I am looking for a way to store embedding generated by language model like (T5), in BigQuery of Google.</p>
<p>The embedding are in the form of Numpy array or tensor.</p>
<p>I found 3 approaches:</p>
<ol>
<li>TFRecord, write it to a TFRecord file and store to cloud storage</li>
<li>convert numpy array to string and store as a String column in a table</li>
<li>store to a column with mode as REPEAT. (Not sure in this way if the order of the embedding vector entries can be preserved)</li>
</ol>
<p>Hope anybody can give some suggestions or other approaches.</p>
<p>Many thanks</p>
","14491000","","","","","2021-06-10 07:41:44","Storing sentence embeddings in Google Cloud BigQuery","<google-cloud-platform><google-bigquery><embedding><huggingface-transformers>","2","2","","","","CC BY-SA 4.0"
"67906629","1","","","2021-06-09 14:52:38","","0","50","<p>I am creating an API Rest (using Flask) that does inference with several huggingface transformer models (using pipelines and custom fine-tuned models).</p>
<p>The problem is that all the models donâ€™t fit at GPU at the same time.</p>
<p>Is there a way of loading a model into GPU make inference with that model and move it to CPU and load next model to GPU for inference then to CPU.</p>
<p>I have tried that moving the model from &quot;cuda&quot; to and from &quot;cpu&quot; each time the the model gets a call, the problem is that it does not delete the predicted variable after running the model, it keeps memory in use and when I load other model cuda memory error is thrown!</p>
","15409228","","15409228","","2021-06-09 16:18:39","2021-06-09 16:18:39","API Rest with several models loaded using GPU but not at same time","<python><flask><gpu><flask-restful><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"67915123","1","","","2021-06-10 05:13:36","","0","28","<p>Iâ€™m trying to get feature vectors from the encoder model using pre-trained ALBERT v2 weights. i have a nvidia 1650ti gpu (4 GB) , and sufficient RAM(8GB) but for some reason Iâ€™m getting Runtime error saying -</p>
<blockquote>
<p>RuntimeError: [enforce fail at â€¦\c10\core\CPUAllocator.cpp:75] data.
DefaultCPUAllocator: not enough memory: you tried to allocate
491520000 bytes. Buy new RAM!</p>
</blockquote>
<p>Iâ€™m really new to pytorch and deep learning in general. Can anyone please tell me what is wrong?</p>
<p>My entire code -</p>
<pre><code>encoded_test_data = tokenized_test_values[â€˜input_idsâ€™]

encoded_test_masks = tokenized_test_values[â€˜attention_maskâ€™]

encoded_train_data = torch.from_numpy(encoded_train_data).to(device)

encoded_masks = torch.from_numpy(encoded_masks).to(device)

encoded_test_data = torch.from_numpy(encoded_test_data).to(device)

encoded_test_masks = torch.from_numpy(encoded_test_masks).to(device)

config = EncoderDecoderConfig.from_encoder_decoder_configs(BertConfig(),BertConfig())

EnD_model = EncoderDecoderModel.from_pretrained(â€˜albert-base-v2â€™,config=config)

feature_extractor = EnD_model.get_encoder()

feature_vector = feature_extractor.forward(input_ids=encoded_train_data,attention_mask = encoded_masks)

feature_test_vector = feature_extractor.forward(input_ids = encoded_test_data, attention_mask = encoded_test_masks)
</code></pre>
<p>Also 491520000 bytes is about 490 MB which should not be a problem.</p>
<p>I tried reducing the number of training examples and also the length of the maximum padded input . The OOM error still exists even though the required space now is 153 MB , which should easily be managable.
I also have maxed out the RAM limit of the heap of pycharm software to 2048 MB. I really dont know what to do nowâ€¦</p>
","13353188","","8893595","","2021-06-10 09:02:31","2021-06-10 09:02:31","RuntimeError on running ALBERT for obtaining encoding vectors from text","<nlp><runtime-error><huggingface-transformers><encoder-decoder>","0","3","","","","CC BY-SA 4.0"
"67956017","1","","","2021-06-13 07:51:46","","0","85","<p>I tried to add an extra dimension to the Huggingface pre-trained BERT tokenizer. The extra column represents the extra label. For example, if the original embedding of the word â€œdogâ€ was [1,1,1,1,1,1,1], then I might add a special column with index 2 to represent â€˜nounâ€™. Thus, the new embedding becomes [1,1,1,1,1,1,1,2]. Then, I will feed the new input [1,1,1,1,1,1,1,2] into the Bert model. How can I do this in Huggingface?</p>
<p>There is something called tokenizer.add_special_tokens which extends the original vocabulary with new tokens. However, I want to concatenate the embedding of the original vocabulary with the embedding of the tokenizer. For example, I want the Bert model to understand that Dog is a noun by connecting the embedding of dog to the embedding of noun. Should I even change the input word embedding of a pre-trained model? Or should I somehow enhance the attention on â€œdogâ€ and â€œnounâ€ in the middle layer?</p>
<p>Here is the example of using tokenizer.add_special_tokens</p>
<pre><code>tokenizer = GPT2Tokenizer.from_pretrained(â€˜gpt2â€™)
model = GPT2Model.from_pretrained(â€˜gpt2â€™)

special_tokens_dict = {â€˜cls_tokenâ€™: â€˜â€™}

num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
print(â€˜We have addedâ€™, num_added_toks, â€˜tokensâ€™)
model.resize_token_embeddings(len(tokenizer))

assert tokenizer.cls_token == â€˜â€™
</code></pre>
","9241131","","","","","2021-06-13 08:03:38","How to concatenate the word embedding for special tokens and words in HuggingFace?","<bert-language-model><huggingface-transformers><ner>","1","0","","","","CC BY-SA 4.0"
"67956097","1","","","2021-06-13 08:01:47","","-2","113","<p>I am working with PyTorch on a Text Classification problem with BERT. This is the PyTorch Dataset format I am using but when I try to access the inputs from the Dataset I get an error.</p>
<h2>PyTorch Dataset</h2>
<p>The Dataset Returns a Dictionary containing : <code>ids</code>, <code>mask</code>, <code>token_type_ids</code>, <code>targets</code></p>
<pre><code>class JigsawDataset:
    def __init__(self, df, train_transforms = None):
        self.comment_text = df[&quot;comment_text&quot;].values
        self.target = df[&quot;toxic&quot;].values
        self.tokenizer = config.BERT_TOKENIZER
        self.max_len = config.MAX_LEN
        self.langs = df[&quot;lang&quot;].values
        self.train_transforms = train_transforms

    def __len__(self):
        return len(self.comment_text)

    def __getitem__(self, item):
        comment_text = str(self.comment_text[item])
        comment_text = &quot; &quot;.join(comment_text.split())
        lang = self.langs[item]
        
        if self.train_transforms:
            comment_text, _ = self.train_transforms(data=(comment_text, lang))['data']

        inputs = self.tokenizer.encode_plus(
            comment_text,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            pad_to_max_length=True,
            truncation=True,
        )

        ids = inputs[&quot;input_ids&quot;]
        mask = inputs[&quot;attention_mask&quot;]
        token_type_ids = inputs[&quot;token_type_ids&quot;]

        data_loader_dict = {}
        data_loader_dict[&quot;ids&quot;] = torch.tensor(ids, dtype=torch.long)
        data_loader_dict[&quot;mask&quot;] = torch.tensor(mask, dtype=torch.long)
        data_loader_dict[&quot;token_type_ids&quot;] = torch.tensor(token_type_ids, dtype=torch.long)
        data_loader_dict[&quot;targets&quot;] = torch.tensor(self.target[item], dtype=torch.float)
        
        
        return data_loader_dict
</code></pre>
<h2>Relevant Code which Gives Error</h2>
<p>In this case I am trying to load only 1 Sample and make it to the format of the PyTorch Dataset</p>
<pre><code>df = pd.read_csv(&quot;dataset.csv&quot;)
df = df.head(1) # Trying with only 1 Sample
dataset = JigsawDataset(df)

ids = dataset[&quot;ids&quot;]    # Error occurs at this line
mask = dataset[&quot;mask&quot;]
token_type_ids = [&quot;token_type_ids&quot;]
</code></pre>
<h2>Error</h2>
<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-78-4608dd623cac&gt; in &lt;module&gt;
      3 dataset = JigsawDataset(df)
      4 
----&gt; 5 ids = dataset[&quot;ids&quot;]    # Error occurs at this line
      6 mask = dataset[&quot;mask&quot;]
      7 token_type_ids = [&quot;token_type_ids&quot;]

&lt;ipython-input-40-121d8aa71516&gt; in __getitem__(self, item)
     13 
     14     def __getitem__(self, item):
---&gt; 15         comment_text = str(self.comment_text[item])
     16         comment_text = &quot; &quot;.join(comment_text.split())
     17         lang = self.langs[item]

IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
</code></pre>
<p>How to fix this?</p>
","13540652","","","","","2021-06-13 12:37:17","Python - PyTorch: IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices","<python><pandas><numpy><pytorch><huggingface-transformers>","2","1","","","","CC BY-SA 4.0"
"67956633","1","","","2021-06-13 09:12:40","","2","121","<p><strong>I'm trying to do semantic search with Pre trained bert models and transformers. I'm using Facebook AI library Faiss.</strong></p>
<p>The code is :</p>
<pre><code>encoded_data = model.encode(df.Plot.tolist())
encoded_data = np.asarray(encoded_data.astype('float32'))
index = faiss.IndexIDMap(faiss.IndexFlatIP(768))
index.add_with_ids(encoded_data, np.array(range(0, len(encoded_data))))
faiss.write_index(index, 'movie_plot.index')
</code></pre>
<p>The error it's returning is :</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-19-c09b9ccadf2a&gt; in &lt;module&gt;
----&gt; 1 index.add_with_ids(encoded_data, np.array(range(0, len(encoded_data))))
      2 faiss.write_index(index, 'movie_plot.index')

~\t5\lib\site-packages\faiss\__init__.py in replacement_add_with_ids(self, x, ids)
    233 
    234         assert ids.shape == (n, ), 'not same nb of vectors as ids'
--&gt; 235         self.add_with_ids_c(n, swig_ptr(x), swig_ptr(ids))
    236 
    237     def replacement_assign(self, x, k, labels=None):

~\t5\lib\site-packages\faiss\swigfaiss.py in add_with_ids(self, n, x, xids)
   4950 
   4951     def add_with_ids(self, n, x, xids):
-&gt; 4952         return _swigfaiss.IndexIDMap_add_with_ids(self, n, x, xids)
   4953 
   4954     def add(self, n, x):

TypeError: in method 'IndexIDMap_add_with_ids', argument 4 of type 'faiss::IndexIDMapTemplate&lt; faiss::Index &gt;::idx_t const *'
</code></pre>
<p><strong>When i ran the same program in google colab, no error was returned. I'm running this program now in windows 10 local pc</strong></p>
<p><strong>I got the answer, we have to convert the np.array(range(0, len(encoded_data))) into int64</strong></p>
<pre><code>encoded_data = model.encode(df.Plot.tolist())
encoded_data = np.asarray(encoded_data.astype('float32'))
index = faiss.IndexIDMap(faiss.IndexFlatIP(768))
ids = np.array(range(0, len(df)))
ids = np.asarray(ids.astype('int64'))
index.add_with_ids(encoded_data, ids)
faiss.write_index(index, 'movie_plot.index')
</code></pre>
","15224778","","15224778","","2021-06-15 06:15:01","2021-06-15 06:15:01","TypeError: in method 'IndexIDMap_add_with_ids', argument 4 of type 'faiss::IndexIDMapTemplate< faiss::Index >::idx_t const *'","<nlp><bert-language-model><huggingface-transformers><sentence>","1","1","1","","","CC BY-SA 4.0"
"67957446","1","","","2021-06-13 10:56:16","","2","145","<p>First i create tokenizer as follow</p>
<pre><code>from tokenizers import Tokenizer
from tokenizers.models import BPE,WordPiece
tokenizer = Tokenizer(WordPiece(unk_token=&quot;[UNK]&quot;))

from tokenizers.trainers import BpeTrainer,WordPieceTrainer
trainer = WordPieceTrainer(vocab_size=5000,min_frequency=3,
                     special_tokens=[&quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;, &quot;[MASK]&quot;])

from tokenizers.pre_tokenizers import Whitespace,WhitespaceSplit
tokenizer.pre_tokenizer = WhitespaceSplit()
tokenizer.train(files, trainer)

from tokenizers.processors import TemplateProcessing
tokenizer.token_to_id(&quot;[SEP]&quot;),tokenizer.token_to_id(&quot;[CLS]&quot;)
tokenizer.post_processor = TemplateProcessing(
    single=&quot;[CLS] $A [SEP]&quot;,
    pair=&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;,
    special_tokens=[
        (&quot;[CLS]&quot;, tokenizer.token_to_id(&quot;[CLS]&quot;)),
        (&quot;[SEP]&quot;, tokenizer.token_to_id(&quot;[SEP]&quot;)),
    ],
)
</code></pre>
<p>Next, I want to train BERT model on these tokens. I tried as follow</p>
<pre><code>from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=True, mlm_probability=0.15)
</code></pre>
<p>But it gives me an error
<code>AttributeError: 'tokenizers.Tokenizer' object has no attribute 'mask_token'</code>
&quot;This tokenizer does not have a mask token which is necessary for masked language modeling. &quot;
Though I have <code>attention_mask</code>. Is is different than <code>mask token</code></p>
","11170350","","","","","2021-06-13 10:56:16","Train BERT model from scratch on a different language","<python><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","0","3","","","CC BY-SA 4.0"
"67869674","1","","","2021-06-07 10:05:02","","1","36","<p>I am developing an application which uses <a href=""https://huggingface.co/sshleifer/distilbart-cnn-12-6"" rel=""nofollow noreferrer"">sshleifer/distilbart-cnn-12-6</a> Hugging Face model for generating text summary. Big model size is a problem for my application so I've decided to use PyTorch's <a href=""https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html"" rel=""nofollow noreferrer"">dynamic quantization</a> to reduce it a little bit. After applying the quantization I experience a weird behavior. The model breaks  when I save and re-load it from disk. Am I doing something wrong or is it a bug in <code>transformers</code>? I can add that I didn't expect such problems with another model (<a href=""https://huggingface.co/nateraw/bert-base-uncased-emotion"" rel=""nofollow noreferrer"">nateraw/bert-base-uncased-emotion</a>).</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import torch  # version 1.8.1+cpu
&gt;&gt;&gt; from transformers import pipeline  # version 4.5.1
&gt;&gt;&gt; sample_text = &quot;&quot;&quot;
The Queen has conducted her first in-person royal duty since her husband, 
the Duke of Edinburgh, died on Friday. The monarch hosted a ceremony in which 
the Earl Peel formally stood down as Lord Chamberlain, whose office organises
royal ceremonies. During a private event held at Windsor Castle, the Queen
accepted her former royal aide's wand and office insignia. The Royal Family is
observing two weeks of mourning. The duke's funeral will take place at Windsor
on Saturday. A royal official said members of the family would continue &quot;to 
undertake engagements appropriate to the circumstances&quot;.
&quot;&quot;&quot;
&gt;&gt;&gt; p = pipeline(&quot;summarization&quot;, model=&quot;sshleifer/distilbart-cnn-12-6&quot;)
&gt;&gt;&gt; quantized_model = torch.quantization.quantize_dynamic(
    p.model,
    {torch.nn.Linear},
    dtype=torch.qint8
)
&gt;&gt;&gt; quantized_pipeline = pipeline(&quot;summarization&quot;, model=quantized_model, tokenizer=p.tokenizer)
&gt;&gt;&gt; quantized_pipeline(sample_text)
[{'summary_text': ' The Queen has been in the spotlight for the first time in the past week . During a private event at Windsor Castle, the Queen was at the centre of the royal family . The Queen is the Queen, and the Queen has a lot to do well, but the Queen is in the news this week .'}]
&gt;&gt;&gt; quantized_pipeline.save_pretrained(&quot;/tmp/my_model&quot;)
&gt;&gt;&gt; loaded_pipeline = pipeline(&quot;summarization&quot;, model=&quot;/tmp/my_model&quot;)
&gt;&gt;&gt; loaded_pipeline(sample_text)
[{'summary_text': ' high high highhighhighhigh high high High High HighHighHighHigh high high highest highest highest lowest lowest lowest highest highesthighesthighesthighest highest highest Highest Highest Highesthighesthighest lowest lowest lows lows highs highs highs lows lows lows low low lowlowlowlow low lowLowLowLow low low lowest lowest safest safest safest safe safe safesafesafesafe safe safe safest safest safer safer safer safe safe secure secure secure secured secured secured secure securesecuresecuresecure secure secureSecureSecureSecure secure secure securing securing securing secured secured securing securing secure secure Secure Secure Secure secure secure obtain obtain obtain obtained obtained obtained obtain obtain obtaining obtaining obtaining obtain obtain attain attain attain obtain obtainGetGetGet Get Get GetGetGet get get get Get'}]
</code></pre>
<p>The summary generated after quantization is not very good but it has something in common with the input text. What I'm getting after saving the model and re-loading from disk is pure garbage.</p>
<p>There's also a warning displayed when I'm loading the model from disk (no such message when instantiating <code>quatized_pipeline</code> though):</p>
<pre><code>Some weights of the model checkpoint at /tmp/my_model were not used when initializing BartForConditionalGeneration: ['model.encoder.layers.0.self_attn.k_proj.scale', 'model.encoder.layers.0.self_attn.k_proj.zero_point', 'model.encoder.layers.0.self_attn.k_proj._packed_params.dtype', 'model.encoder.layers.0.self_attn.k_proj._packed_params._packed_params', 'model.encoder.layers.0.self_attn.v_proj.scale', 'model.encoder.layers.0.self_attn.v_proj.zero_point', 'model.encoder.layers.0.self_attn.v_proj._packed_params.dtype', 'model.encoder.layers.0.self_attn.v_proj._packed_params._packed_params', 'model.encoder.layers.0.self_attn.q_proj.scale', 'model.encoder.layers.0.self_attn.q_proj.zero_point', 'model.encoder.layers.0.self_attn.q_proj._packed_params.dtype', 'model.encoder.layers.0.self_attn.q_proj._packed_params._packed_params', 'model.encoder.layers.0.self_attn.out_proj.scale', 'model.encoder.layers.0.self_attn.out_proj.zero_point', 'model.encoder.layers.0.self_attn.out_proj._packed_params.dtype', 'model.encoder.layers.0.self_attn.out_proj._packed_params._packed_params', 'model.encoder.layers.0.fc1.scale', 'model.encoder.layers.0.fc1.zero_point', 'model.encoder.layers.0.fc1._packed_params.dtype', 'model.encoder.layers.0.fc1._packed_params._packed_params', 'model.encoder.layers.0.fc2.scale', 'model.encoder.layers.0.fc2.zero_point', 'model.encoder.layers.0.fc2._packed_params.dtype', 'model.encoder.layers.0.fc2._packed_params._packed_params', ...]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
","15704881","","","","","2021-06-07 10:05:02","Summarization pipeline with quantized model breaks after saving to disk","<python><neural-network><pytorch><huggingface-transformers><summarization>","0","0","","","","CC BY-SA 4.0"
"67894649","1","67894899","","2021-06-08 21:13:48","","0","33","<p>I'm trying to import the NERDA library in order use it to engage in a Named-Entity Recognition task in Python. I initially tried importing the library in a jupyter notebook and got the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\NERDA\models.py&quot;, line 13, in &lt;module&gt;
    from .networks import NERDANetwork
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\NERDA\networks.py&quot;, line 4, in &lt;module&gt;
    from transformers import AutoConfig
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\__init__.py&quot;, line 43, in &lt;module&gt;
    from . import dependency_versions_check
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\dependency_versions_check.py&quot;, line 36, in &lt;module&gt;
    from .file_utils import is_tokenizers_available
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\file_utils.py&quot;, line 51, in &lt;module&gt;
    from huggingface_hub import HfApi, HfFolder, Repository
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\huggingface_hub\__init__.py&quot;, line 31, in &lt;module&gt;
    from .file_download import cached_download, hf_hub_url
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\huggingface_hub\file_download.py&quot;, line 37, in &lt;module&gt;
    if tuple(int(i) for i in _PY_VERSION.split(&quot;.&quot;)) &lt; (3, 8, 0):
  File &quot;C:\Users\oefel\AppData\Local\Programs\Python\Python38\lib\site-packages\huggingface_hub\file_download.py&quot;, line 37, in &lt;genexpr&gt;
    if tuple(int(i) for i in _PY_VERSION.split(&quot;.&quot;)) &lt; (3, 8, 0):
ValueError: invalid literal for int() with base 10: '6rc1'
</code></pre>
<p>I then tried globally installing using pip in gitbash and got the same error. The library appeared to install without error but when I try the following import, I get that same ValueError:</p>
<pre><code>from NERDA.models import NERDA
</code></pre>
<p>I've also tried some of the pre-cooked model imports and gotten the same ValueError.</p>
<pre><code>from NERDA.precooked import EN_ELECTRA_EN
from NERDA.precooked import EN_BERT_ML
</code></pre>
<p>I can't find anything on this error online and am hoping someone may be able to lend some insight? Thanks so much!</p>
","12170032","","","","","2021-06-08 21:38:50","ValueError with NERDA model import","<python><huggingface-transformers><ner>","1","0","","","","CC BY-SA 4.0"
"67968635","1","","","2021-06-14 10:28:47","","1","41","<p>Dear stackoverflow community,</p>
<p>I have the following question: Is it possible to disable fast tokenization in an allennlp model?</p>
<p>I am trying to use the following model in my nlp pipeline but can't use the fast tokenization as it causes issues when multithreaded. My initial thought was to simply replace the tokenizer but this seemed to have no effect. I would greatly appreciate your help on this issue. Please tell me the obviouse thing that i am missing.</p>
<pre><code>from allennlp.predictors.sentence_tagger import SentenceTaggerPredictor
predictor = SentenceTaggerPredictor.from_path(&quot;
            https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz&quot;)

# attempt (not working as expected):

from transformers import RobertaTokenizer
predictor._dataset_reader._token_indexers[&quot;tokens&quot;]._tokenizer = RobertaTokenizer.from_pretrained(&quot;roberta-base&quot;, use_fast=False)

# This still causes problems while being used when multithreaded
# and still calls transformers/tokenization_utils_fast.py

</code></pre>
","9300533","","","","","2021-06-14 10:28:47","disabling fast tokenization in allennlp models","<python><multithreading><tokenize><huggingface-transformers><allennlp>","0","0","","","","CC BY-SA 4.0"
"67945853","1","","","2021-06-12 05:20:38","","0","144","<p>I am trying to access the intermediate layers of Bert Model and the hidden states output of [CLS] and other hidden states of the last layer output. My original code was</p>
<pre><code>sequence_output, pooled_output = self.bert(
      input_ids=input_ids,
      attention_mask=attention_mask
    )
</code></pre>
<p>Where <code>sequence_output</code> get the last layer hidden states outputs, <code>pooled_output</code> get the hidden states outputs for [CLS].</p>
<p>To get the intermediate layers of the Bert Model, I should set <code>config.output_hidden_states = True</code> according to the answer
<a href=""https://stackoverflow.com/questions/61465103/how-to-get-intermediate-layers-output-of-pre-trained-bert-model-in-huggingface"">How to get intermediate layers&#39; output of pre-trained BERT model in HuggingFace Transformers library?</a></p>
<p>However, if I do so, then my code will becomes</p>
<pre><code>pooled_output = self.bert(
      input_ids=input_ids,
      attention_mask=attention_mask
    )
</code></pre>
<p>I can get the intermediate layers by <code>hidden_states = outputs[2][1:]</code>, but I do not know how to get the output for [CLS] and output hidden states. I read the documentation of hugging face and still do not understand what each dimension of <code>outputs</code> represents for.</p>
<p>Is there any more detailed documentation or explanation I can read? How do I get hidden states for [CLS], output hidden states at the last layer, hidden states for intermediate layer?</p>
<p>Thank you.</p>
","9241131","","","","","2021-06-12 05:20:38","How to get intermediate layers of Bert Model in HuggingFace?","<bert-language-model><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"67958756","1","68038142","","2021-06-13 13:25:33","","0","49","<p>I am using HuggingFace Transformers with PyTorch. My modus operandi is to download a pre-trained model and save it in a local project folder.</p>
<p>While doing so, I can see that .bin file is saved locally, which stands for the model. However, I am also downloading and saving a tokenizer, for which I cannot see any associated file.</p>
<p>So, how do I check if a tokenizer is saved locally before downloading? Secondly, apart from the usual <code>os.path.isfile(...)</code> check, is there any other better way to prioritize local copy usage from a given location before downloading?</p>
","305904","","","","","2021-06-18 15:52:15","How do I check if a tokenizer/model is already saved","<python><pytorch><huggingface-transformers>","1","3","","","","CC BY-SA 4.0"
"67972661","1","67972700","","2021-06-14 15:00:03","","-2","343","<p>I am following this tutorial here: <a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/training.html</a> - though, I am coming across an error, and I think the tutorial is missing an import, but i do not know which.</p>
<p>These are my current imports:</p>
<pre><code># Transformers installation
! pip install transformers
# To install from source instead of the last release, comment the command above and uncomment the following one.
# ! pip install git+https://github.com/huggingface/transformers.git

! pip install datasets transformers

from transformers import pipeline
</code></pre>
<p>Current code:</p>
<pre><code>from datasets import load_dataset

raw_datasets = load_dataset(&quot;imdb&quot;)
</code></pre>
<pre><code>from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
</code></pre>
<pre><code>inputs = tokenizer(sentences, padding=&quot;max_length&quot;, truncation=True)
</code></pre>
<p>The error:</p>
<pre><code>NameError                                 Traceback (most recent call last)

&lt;ipython-input-9-5a234f114e2e&gt; in &lt;module&gt;()
----&gt; 1 inputs = tokenizer(sentences, padding=&quot;max_length&quot;, truncation=True)

NameError: name 'sentences' is not defined
</code></pre>
","16098918","","","","","2021-06-14 15:16:31","Hugging Face: NameError: name 'sentences' is not defined","<python><bert-language-model><huggingface-transformers><huggingface-tokenizers><huggingface-datasets>","2","0","","","","CC BY-SA 4.0"
"67849833","1","68609735","","2021-06-05 12:56:25","","2","181","<p>I currently use a huggingface pipeline for sentiment-analysis like so:</p>
<pre><code>from transformers import pipeline
classifier = pipeline('sentiment-analysis', device=0)
</code></pre>
<p>The problem is that when I pass texts larger than 512 tokens, it just crashes saying that the input is too long.  Is there any way of passing the max_length and truncate parameters from the tokenizer directly to the pipeline?</p>
<p>My work around is to do:</p>
<p>from transformers import AutoTokenizer, AutoModelForSequenceClassification</p>
<pre><code>model_name = &quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, device=0)
</code></pre>
<p>And then when I call the tokenizer:</p>
<pre><code>pt_batch = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=&quot;pt&quot;)
</code></pre>
<p>But it would be much nicer to simply be able to call the pipeline directly like so:</p>
<pre><code>classifier(text, padding=True, truncation=True, max_length=512)
</code></pre>
","9140","","","","","2021-08-01 11:00:02","How to truncate input iin Huggingface pipeline?","<huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"67903181","1","","","2021-06-09 11:23:02","","1","145","<p>I am new to using Hugging Face and BERT, and I am trying to make an attempt at doing some masked language modelling (MLM) using my own dataset. My own dataset contains about 20,000 tweets including emojis.</p>
<p>An example of a tweet is something like this:</p>
<pre><code>@John123 I am so annoyed with you ðŸ˜¡
</code></pre>
<p>Using masked language modelling, I want to do something where I mask the emoji token by replacing them with <code>[MASK]</code>. However, I am unsure how to add the emojis to my tokenizer - is there a way to do this?</p>
<p>At the moment, when I do this, the output does not seem to predict emojis. Instead, it just does something like this below.</p>
<p>Input:</p>
<pre><code>The weather is bad today [MASK]
</code></pre>
<p>Actual output:</p>
<pre><code>The weather is bad today.
</code></pre>
<p>Anticipated output:</p>
<pre><code>The weather is bad today ðŸ˜
</code></pre>
<p>I know that I am close to achieving what I want, but I need some additional guidance on how to actually accomplish this.</p>
<p>I have referred to <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">this</a> tutorial, which uses special tokens, but not emojis. Below, is the code used in that tutorial for special tokens:</p>
<pre><code>from pathlib import Path

from tokenizers import ByteLevelBPETokenizer

paths = [str(x) for x in Path(&quot;./eo_data/&quot;).glob(&quot;**/*.txt&quot;)]

# Initialize a tokenizer
tokenizer = ByteLevelBPETokenizer()

# Customize training
tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[
    &quot;&lt;s&gt;&quot;,
    &quot;&lt;pad&gt;&quot;,
    &quot;&lt;/s&gt;&quot;,
    &quot;&lt;unk&gt;&quot;,
    &quot;&lt;mask&gt;&quot;,
])
</code></pre>
<p>I think that I need to use this piece of code, but modify it for emojis - though, I am unsure how to do that.</p>
<p>Any help would be good, thanks!</p>
","16098918","","","","","2021-06-09 11:23:02","How do I add emojis to my tokenizer in masked language modelling using BERT?","<python><machine-learning><bert-language-model><huggingface-transformers><huggingface-tokenizers>","0","9","","","","CC BY-SA 4.0"
"67979876","1","67982206","","2021-06-15 04:07:50","","1","82","<p>I am seeing someone other's BERT model, in which the vocab.txt's size is 22110, but the <code>vocab_size</code> parameter's value is 21128 in bert_config.json.</p>
<p>I understand that these two numbers must be exactly the same. Is that right?</p>
","3943868","","5652313","","2021-06-15 07:55:11","2021-06-15 07:55:11","Must the vocab size must math the vocab_size in bert_config.json exactly?","<bert-language-model><huggingface-transformers><transformer>","1","1","","","","CC BY-SA 4.0"
"67990545","1","67990786","","2021-06-15 16:57:22","","0","29","<p><strong>I'm using bert pre-trained model for question and answering. It's returning correct result but with lot of spaces between the text</strong></p>
<p>The code is below :</p>
<pre><code>def get_answer_using_bert(question, reference_text):
  
  bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

  bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

  input_ids = bert_tokenizer.encode(question, reference_text)
  input_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids)

  sep_location = input_ids.index(bert_tokenizer.sep_token_id)
  first_seg_len, second_seg_len = sep_location + 1, len(input_ids) - (sep_location + 1)
  seg_embedding = [0] * first_seg_len + [1] * second_seg_len

  model_scores = bert_model(torch.tensor([input_ids]), 
  token_type_ids=torch.tensor([seg_embedding]))
  ans_start_loc, ans_end_loc = torch.argmax(model_scores[0]), torch.argmax(model_scores[1])
  result = ' '.join(input_tokens[ans_start_loc:ans_end_loc + 1])

  result = result.replace('#', '')
  return result
</code></pre>
<p>Followed by code below :</p>
<pre><code>reference_text = 'Mukesh Dhirubhai Ambani was born on 19 April 1957 in the British Crown colony of Aden (present-day Yemen) to Dhirubhai Ambani and Kokilaben Ambani. He has a younger brother Anil Ambani and two sisters, Nina Bhadrashyam Kothari and Dipti Dattaraj Salgaonkar. Ambani lived only briefly in Yemen, because his father decided to move back to India in 1958 to start a trading business that focused on spices and textiles. The latter was originally named Vimal but later changed to Only Vimal His family lived in a modest two-bedroom apartment in Bhuleshwar, Mumbai until the 1970s. The family financial status slightly improved when they moved to India but Ambani still lived in a communal society, used public transportation, and never received an allowance. Dhirubhai later purchased a 14-floor apartment block called Sea Wind in Colaba, where, until recently, Ambani and his brother lived with their families on different floors.'
question = 'What is the name of mukesh ambani brother?'

get_answer_using_bert(question, reference_text)
</code></pre>
<p>And the output is :</p>
<pre><code>'an il am ban i'
</code></pre>
<p><strong>Can anyone help me how to fix this issue. It would be really helpful.</strong></p>
","15224778","","","","","2021-06-15 17:14:53","I'm using bert pre-trained model for question and answering. It's returning correct result but with lot of spaces between the text","<deep-learning><nlp><bert-language-model><huggingface-transformers><question-answering>","1","0","","","","CC BY-SA 4.0"
"67948945","1","67950754","","2021-06-12 12:39:11","","3","418","<p>I want to force the Huggingface transformer (BERT) to make use of CUDA.
nvidia-smi showed that all my CPU cores were maxed out during the code execution, but my GPU was at 0% utilization. Unfortunately, I'm new to the Hugginface library as well as PyTorch and don't know where to place the CUDA attributes <code>device = cuda:0</code> or <code>.to(cuda:0)</code>.</p>
<p>The code below is basically a customized part from <a href=""https://huggingface.co/oliverguhr/german-sentiment-bert"" rel=""nofollow noreferrer"">german sentiment BERT working example</a></p>
<pre><code>class SentimentModel_t(pt.nn.Module):
      def __init__(self, model_name: str = &quot;oliverguhr/german-sentiment-bert&quot;):
           DEVICE = &quot;cuda:0&quot; if pt.cuda.is_available() else &quot;cpu&quot;
           print(DEVICE)
           super(SentimentModel_t,self).__init__()

           self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(DEVICE)
           self.tokenizer = BertTokenizerFast.from_pretrained(model_name)
    
        def predict_sentiment(self, texts: List[str])-&gt; List[str]:
            texts = [self.clean_text(text) for text in texts]
            # Add special tokens takes care of adding [CLS], [SEP], &lt;s&gt;... tokens in the right way for each model.
            input_ids = self.tokenizer.batch_encode_plus(texts,padding=True, add_special_tokens=True, truncation=True, max_length=self.tokenizer.max_len_single_sentence)
            input_ids = pt.tensor(input_ids[&quot;input_ids&quot;])
    
            with pt.no_grad():
                logits = self.model(input_ids)
    
            label_ids = pt.argmax(logits[0], axis=1)
    
            labels = [self.model.config.id2label[label_id] for label_id in label_ids.tolist()]
            return labels
</code></pre>
<p>EDIT: After applying the suggestions of @KonstantinosKokos (see edited code above) I got a</p>
<pre><code>RuntimeError: Input, output and indices must be on the current device
</code></pre>
<p>pointing to</p>
<pre><code>        with pt.no_grad():
           logits = self.model(input_ids)
</code></pre>
<p>The full error code can be obtained down below:</p>
<pre><code>&lt;ipython-input-15-b843edd87a1a&gt; in predict_sentiment(self, texts)
     23 
     24         with pt.no_grad():
---&gt; 25             logits = self.model(input_ids)
     26 
     27         label_ids = pt.argmax(logits[0], axis=1)

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1364         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
   1365 
-&gt; 1366         outputs = self.bert(
   1367             input_ids,
   1368             attention_mask=attention_mask,

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)
    859         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)
    860 
--&gt; 861         embedding_output = self.embeddings(
    862             input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
    863         )

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds)
    196 
    197         if inputs_embeds is None:
--&gt; 198             inputs_embeds = self.word_embeddings(input_ids)
    199         token_type_embeddings = self.token_type_embeddings(token_type_ids)
    200 

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py in forward(self, input)
    122 
    123     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 124         return F.embedding(
    125             input, self.weight, self.padding_idx, self.max_norm,
    126             self.norm_type, self.scale_grad_by_freq, self.sparse)

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1850         # remove once script supports set_grad_enabled
   1851         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1852     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1853 
   1854 
</code></pre>
","15445597","","15445597","","2021-06-13 09:57:53","2021-06-17 08:49:02","Force BERT transformer to use CUDA","<python><pytorch><huggingface-transformers><transformer>","2","0","","","","CC BY-SA 4.0"
"67962020","1","","","2021-06-13 19:37:15","","0","21","<p>I am fine-tuning PEGASUS model using <a href=""https://gist.github.com/jiahao87/50cec29725824da7ff6dd9314b53c4b3"" rel=""nofollow noreferrer"">this script</a>.
I am currently using the SAMSum dataset and I have reached a point in which the output doesn't get better.</p>
<p>Examples:</p>
<p>The Actual Summary</p>
<blockquote>
<p>Alexis and Carter met tonight. Carter would like to meet again, but Alexis is busy.</p>
</blockquote>
<p>The Best Output Summary (based on human evaluation)</p>
<blockquote>
<p>'Carter and Alexis are up for it.'</p>
</blockquote>
<p>The Second Best Output Summary (based on human evaluation)</p>
<blockquote>
<p>['Carter and Alexis are up for it, I want to see some tomorrow. But']</p>
</blockquote>
<p>As seen above the summaries don't share the same meaning so I would like to know if changing either the weight decay or the warm-up steps would help achieve better results or not? and if so would it be better to increase or decrease the values of the values of the weight decay or the warm-up steps?</p>
<p><strong>NOTES:</strong></p>
<ol>
<li><p>I am using batch size 1 as I am using Colab pro and the maximum GPU size is 16280MB and so using a larger batch size doesn't permit using the whole dataset size and this leads to worse results. Also the current warm-up steps are 500 and I am having a total of 4000 steps in 2000 epochs with weight decay of 0.01</p>
</li>
<li><p>I have already used different combinations and sizes for the training/validation/testing. the default was 90/5/5 but I tried 90/10/0, 70/15/15, 70/30/0</p>
</li>
<li><p>The Best Output is always produced around the 500 steps and the Second Best Output is produced at 2500 steps in the combinations of 90/10/0, 70/15/15 and 70/30/0</p>
</li>
</ol>
<p>Any further tips to enhance the output would be much appreciated and thank you in advance!</p>
","15517911","","2423278","","2021-06-17 13:41:00","2021-06-17 13:41:00","What is the effect of changing the weight decay and warm-up steps in fine-tuning PEGASUS?","<tensorflow><machine-learning><huggingface-transformers><summarization><huggingface-datasets>","0","0","","","","CC BY-SA 4.0"
"67998798","1","","","2021-06-16 08:12:16","","0","37","<p>I am using Huggingface library and transformers to find whether a sentence is well-formed or not. I am using a masked language model called XLMR. I first tokenize my sentence, and then mask each word of the sentence one by one, and then process the masked sentences and find the probability that the predicted masked word is right.</p>
<pre><code>def calculate_scores(sent, model, tokenizer, device, print_pred=False, maskval=False):

k = 0
dic = {}
ls = tokenizer.batch_encode_plus(sent)
input_list = ls.input_ids
h=0
with torch.no_grad():
    for i in tqdm(range(len(input_list))):

        item = input_list[i]
        real_input = item
        attmask = [1]*len(item)
        seg = [0]*len(item)
        seglist = [seg]
        masked_list = [real_input]
        attlist = [attmask]
        for j in range(1, len(item)-1):
            input = copy.deepcopy(real_input)
            input[j] = 50264
            masked_list.append(input)
            attlist.append(attmask)
            seglist.append(seg)
        
        inid = torch.tensor(masked_list)
        segtensor = torch.tensor(seglist)
        atttensor = torch.tensor(attlist)
        inid=inid.to(device)
        segtensor=segtensor.to(device)
        output = model(inid, segtensor)

        predictions_logits = output.logits
        predictions = torch.softmax(predictions_logits, dim=2)
        ppscore = 0
        for j in range(1, len(item)-1):
            ppscore = ppscore+math.log(predictions[j, j, item[j]], 2)
        try:
            score = math.pow(2, (-1/(len(item)-2))*ppscore)
            dic[sent[i]] = score
        except:
            print(sent[i])
            dic[sent[i]] = 10000000

        # dic[sent[i]]=10000000
return dic
</code></pre>
<p>I will explain my code quickly. The function calculate_scores has <em>sent</em> as an input which is a list of sentences. I first batch encode this list of sentences. And then for each encoded sentence that I get, I generate masked sentences where only one word is masked and the rest are un-masked. Then I input these generated sentences to output and get the probability. Then I compute perplexity.</p>
<p>But the way I'm using this is not a very good way of utilizing GPU. I want to process multiple sentences at once but at the same time, I also need to find the perplexity scores for each sentence. How would I go about doing this?</p>
","9222360","","","","","2021-06-16 08:12:16","How to efficient batch-process in huggingface?","<python-3.x><machine-learning><pytorch><gpu><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"67924216","1","68366022","","2021-06-10 15:29:36","","1","64","<h2>Background</h2>
<p>I'm working with a finetuned <a href=""https://huggingface.co/transformers/model_doc/mbart.html?highlight=mbart"" rel=""nofollow noreferrer"">Mbart50</a> model that I need sped up for inferencing because using the HuggingFace model as-is is fairly slow with my current hardware. I wanted to use <a href=""https://pytorch.org/docs/stable/jit.html"" rel=""nofollow noreferrer"">TorchScript</a> because I couldn't get <a href=""https://huggingface.co/transformers/serialization.html?highlight=onnx"" rel=""nofollow noreferrer"">onnx</a> to export this particular model as it seems it will be supported at a later time (I would be glad to be wrong otherwise).</p>
<h2>Convert Transformer to a Pytorch trace:</h2>
<pre class=""lang-py prettyprint-override""><code>import torch
&quot;&quot;&quot; Model data  &quot;&quot;&quot;
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model = MBartForConditionalGeneration.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;, torchscript= True)

tokenizer = MBart50TokenizerFast.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;)
tokenizer.src_lang = 'en_XX'

dummy = &quot;To celebrate World Oceans Day, we're swimming through a shoal of jack fish just off the coast of Baja, California, in Cabo Pulmo National Park. This Mexican marine park in the Sea of Cortez is home to the northernmost and oldest coral reef on the west coast of North America, estimated to be about 20,000 years old. Jacks are clearly plentiful here, but divers and snorkelers in Cabo Pulmo can also come across many other species of fish and marine mammals, including several varieties of sharks, whales, dolphins, tortoises, and manta rays.&quot;

model.config.forced_bos_token_id=250006
myTokenBatch = tokenizer(dummy, max_length=192, padding='max_length', truncation = True, return_tensors=&quot;pt&quot;)

torch.jit.save(torch.jit.trace(model, [myTokenBatch.input_ids,myTokenBatch.attention_mask]), &quot;././traced-model/mbart-many.pt&quot;)
</code></pre>
<h2>Inference Step:</h2>
<pre class=""lang-py prettyprint-override""><code>

import torch
 &quot;&quot;&quot; Model data  &quot;&quot;&quot;
from transformers import MBart50TokenizerFast

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
tokenizer = MBart50TokenizerFast.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;)
 
model = torch.jit.load('././traced-model/mbart-many.pt')
MAX_LENGTH =  192

tokenizer = MBart50TokenizerFast.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;)
 
model.to(device)
model.eval()

tokenizer.src_lang = 'en_XX'

dummy = &quot;To celebrate World Oceans Day, we're swimming through a shoal of jack fish just off the coast of Baja, California, in Cabo Pulmo National Park. This Mexican marine park in the Sea of Cortez is home to the northernmost and oldest coral reef on the west coast of North America, estimated to be about 20,000 years old. Jacks are clearly plentiful here, but divers and snorkelers in Cabo Pulmo can also come across many other species of fish and marine mammals, including several varieties of sharks, whales, dolphins, tortoises, and manta rays.&quot;

myTokenBatch = tokenizer(dummy, max_length=192, padding='max_length', truncation = True, return_tensors=&quot;pt&quot;)

encode, pool , norm  = model(myTokenBatch.input_ids,myTokenBatch.attention_mask)


</code></pre>
<h2>Expected Encoding Output:</h2>
<p>These are tokens that can be decoded to words with MBart50TokenizerFast.</p>
<pre><code>
tensor([[250004,    717, 176016,   6661,  55609,      7,  10013,      4,    642,
             25,    107, 192298,   8305,     10,  15756,    289,    111, 121477,
          67155,   1660,   5773,     70, 184085,    111, 118191,      4,  39897,
              4,     23, 143740,  21694,    432,   9907,   5227,      5,   3293,
         181815, 122084,   9201,     23,     70,  27414,    111,  48892,    169,
             83,   5368,     47,     70, 144477,   9022,    840,     18,    136,
          10332,    525, 184518,    456,   4240,     98,     70,  65272, 184085,
            111,  23924,  21629,      4,  25902,   3674,     47,    186,   1672,
              6,  91578,   5369,  10332,      5,  21763,      7,    621, 123019,
          32328,    118,   7844,   3688,      4,   1284,  41767,    136, 120379,
           2590,   1314,     23, 143740,  21694,    432,    831,   2843,   1380,
          36880,   5941,   3789, 114149,    111,  67155,    136, 122084,  21968,
           8080,      4,  26719,  40368,    285,  68794,    111,  54524,   1224,
              4,    148,  50742,      7,      4,  13111,  19379,   1779,      4,
          43807, 125216,      7,      4,    136,    332,    102,  62656,      7,
              5,      2,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1]])
</code></pre>
<h2>Actual Output:</h2>
<p>I don't know what this is... <code>print(encode)</code></p>
<pre><code>
(tensor([[[[-9.3383e-02, -2.0395e-01,  4.8226e-03,  ...,  1.8068e+00,
            1.1528e-01,  7.0406e-02],
          [-4.4630e-02, -2.2453e-01,  9.5264e-02,  ...,  1.6921e+00,
            1.4607e-01,  4.8238e-02],
          [-7.8206e-01,  1.2699e-01,  1.6467e+00,  ..., -1.7057e+00,
            8.7768e-01,  8.2230e-01],
          ...,
 [-1.2145e-02, -2.1855e-03, -6.0966e-03,  ...,  2.9296e-02,
            2.2141e-03,  3.2074e-02],
          [-1.4671e-02, -2.8995e-03, -5.8610e-03,  ...,  2.8525e-02,
            2.4620e-03,  3.1593e-02],
          [-1.5877e-02, -3.5165e-03, -4.8743e-03,  ...,  2.8930e-02,
            2.9877e-03,  3.3892e-02]]]], grad_fn=&lt;CopyBackwards&gt;))

</code></pre>
","13568346","","13568346","","2021-06-11 20:46:26","2021-07-13 16:14:08","Why would a Torchscript trace return different looking encoded_inputs compared to the original Transformer model?","<pytorch><huggingface-transformers><transformer><machine-translation><torchscript>","1","2","","","","CC BY-SA 4.0"
"67949960","1","","","2021-06-12 14:49:17","","1","219","<p>Using HuggingFace to train a transformer model to predict a target variable (e.g., movie ratings). I'm new to Python and this is likely a simple question, but I canâ€™t figure out how to save a trained classifier model (via Colab) and then reload so to make target variable predictions on new data. As an example, I trained a model to predict imbd ratings with an example from the HuggingFace resources, shown below. Iâ€™ve tried a number of ways (save_model, save_pretrained) and either am struggling to save it at all or when loaded, canâ€™t figure out what to call to get predictions. Any help would be incredibly appreciated on the steps that involve saving, loading, then creating new predicted scores based on the model on test data.</p>
<pre><code>#example mainly from here: https://huggingface.co/transformers/training.html
!pip install transformers
!pip install datasets

from datasets import load_dataset
raw_datasets = load_dataset(&quot;imdb&quot;)

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)

def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], max_length = 128, padding=&quot;max_length&quot;, truncation=True) 

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

#choosing small datasets for example#
small_train_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets[&quot;test&quot;].shuffle(seed=42).select(range(500))

### TRAINING classification ###
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=2)

from transformers import TrainingArguments
from transformers import Trainer

training_args = TrainingArguments(&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;, num_train_epochs=2, weight_decay=.0001, learning_rate=0.00001, per_device_train_batch_size=32) 

trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset)
trainer.train()

y_test_predicted_original = model_loaded.predict(small_eval_dataset)

#### Saving ###
from google.colab import drive
drive.mount('/content/gdrive')
%cd /content/gdrive/My\ Drive/FOLDER

trainer.save_pretrained (&quot;Trained model&quot;) #assumed this would save but did not
model.save_pretrained (&quot;Trained model&quot;) #did save

### Loading Model and Creating Predicted Scores ###

#perhaps this....#
from transformers import BertConfig, BertModel
conf = BertConfig.from_pretrained(&quot;Trained model&quot;, num_labels=2)
model_loaded = AutoModelForSequenceClassification.from_pretrained(&quot;Trained model&quot;, config=conf)

#or...#
model_loaded = AutoModelForSequenceClassification.from_pretrained(&quot;Trained model&quot;, local_files_only=True)
model_loaded 

#with ultimate goal of getting predicted scores (not sure what to call here)...
y_test_predicted_loaded = model_loaded.predict(small_eval_dataset)
</code></pre>
","5132954","","","","","2021-06-12 14:49:17","HuggingFace Saving-Loading Model (Colab) to Make Predictions","<python><google-colaboratory><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68005302","1","","","2021-06-16 15:02:57","","0","36","<p>Iâ€™ve fine tuned some models from Hugging Face for the QA task using the <a href=""http://sag.art.uniroma2.it/demo-software/squadit/"" rel=""nofollow noreferrer"">SQuAD-it</a> dataset. Itâ€™s an italian version of SQuAD v1.1, thus it use the same <a href=""https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py"" rel=""nofollow noreferrer"">evaluation script</a>. Anyway, Iâ€™m new in coding and I really donâ€™t know how to prepare my data to be fed into the evaluation script. I have a test.json file and fine-tuned models. The last step Iâ€™ve made is this:</p>
<pre><code>from transformers import TrainingArguments, Trainer


training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=16,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
    label_names = [&quot;start_positions&quot;, &quot;end_positions&quot;]
)

trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()
</code></pre>
<p>The evaluation script is this:</p>
<pre><code>&quot;&quot;&quot; Official evaluation script for v1.1 of the SQuAD dataset. &quot;&quot;&quot;
from __future__ import print_function
from collections import Counter
import string
import re
import argparse
import json
import sys


def normalize_answer(s):
    &quot;&quot;&quot;Lower text and remove punctuation, articles and extra whitespace.&quot;&quot;&quot;
    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)

    def white_space_fix(text):
        return ' '.join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))


def f1_score(prediction, ground_truth):
    prediction_tokens = normalize_answer(prediction).split()
    ground_truth_tokens = normalize_answer(ground_truth).split()
    common = Counter(prediction_tokens) &amp; Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0:
        return 0
    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1


def exact_match_score(prediction, ground_truth):
    return (normalize_answer(prediction) == normalize_answer(ground_truth))


def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):
    scores_for_ground_truths = []
    for ground_truth in ground_truths:
        score = metric_fn(prediction, ground_truth)
        scores_for_ground_truths.append(score)
    return max(scores_for_ground_truths)


def evaluate(dataset, predictions):
    f1 = exact_match = total = 0
    for article in dataset:
        for paragraph in article['paragraphs']:
            for qa in paragraph['qas']:
                total += 1
                if qa['id'] not in predictions:
                    message = 'Unanswered question ' + qa['id'] + \
                              ' will receive score 0.'
                    print(message, file=sys.stderr)
                    continue
                ground_truths = list(map(lambda x: x['text'], qa['answers']))
                prediction = predictions[qa['id']]
                exact_match += metric_max_over_ground_truths(
                    exact_match_score, prediction, ground_truths)
                f1 += metric_max_over_ground_truths(
                    f1_score, prediction, ground_truths)

    exact_match = 100.0 * exact_match / total
    f1 = 100.0 * f1 / total

    return {'exact_match': exact_match, 'f1': f1}


if __name__ == '__main__':
    expected_version = '1.1'
    parser = argparse.ArgumentParser(
        description='Evaluation for SQuAD ' + expected_version)
    parser.add_argument('dataset_file', help='Dataset file')
    parser.add_argument('prediction_file', help='Prediction File')
    args = parser.parse_args()
    with open(args.dataset_file) as dataset_file:
        dataset_json = json.load(dataset_file)
        if (dataset_json['version'] != expected_version):
            print('Evaluation expects v-' + expected_version +
                  ', but got dataset with v-' + dataset_json['version'],
                  file=sys.stderr)
        dataset = dataset_json['data']
    with open(args.prediction_file) as prediction_file:
        predictions = json.load(prediction_file)
    print(json.dumps(evaluate(dataset, predictions)))
</code></pre>
<p>Thus I think I have to change the â€˜dataset_fileâ€™ in the script with a dataset from my test.json and to change â€˜prediction_fileâ€™ with predictions I made with my models. How do I build this last? And must it be another dataset?</p>
","15261008","","6045800","","2021-07-17 21:30:42","2021-07-17 21:30:42","How to evaluate a Question Answering fine-tuned model","<python><pytorch><evaluation><huggingface-transformers><question-answering>","0","0","","","","CC BY-SA 4.0"
"60209265","1","","","2020-02-13 13:42:54","","3","1426","<p>I'm trying to do a simple text classification project with Transformers, I want to use the pipeline feature added in the V2.3, but there is little to no documentation.</p>

<pre><code>data = pd.read_csv(""data.csv"")
FLAUBERT_NAME = ""flaubert-base-cased""

encoder = LabelEncoder()
target = encoder.fit_transform(data[""category""])
y = target
X = data[""text""]

model = FlaubertForSequenceClassification.from_pretrained(FLAUBERT_NAME)
tokenizer = FlaubertTokenizer.from_pretrained(FLAUBERT_NAME)
pipe = TextClassificationPipeline(model, tokenizer, device=-1)  # device=-1 -&gt; Use only CPU

print(""Test #1: pipe('Bonjour le monde')="", pipe(['Bonjour le monde']))
</code></pre>

<hr>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/PLHT09191/Documents/work/dev/Classif_Annonces/src/classif_annonce.py"", line 33, in &lt;module&gt;
    model = FlaubertForSequenceClassification.from_pretrained(FLAUBERT_NAME)
  File ""C:\Users\Myself\Documents\work\dev\Classif_Annonces\venv\lib\site-packages\transformers-2.4.1-py3.5.egg\transformers\modeling_utils.py"", line 463, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File ""C:\Users\Myself\Documents\work\dev\Classif_Annonces\venv\lib\site-packages\transformers-2.4.1-py3.5.egg\transformers\modeling_flaubert.py"", line 343, in __init__
    super(FlaubertForSequenceClassification, self).__init__(config)
  File ""C:\Users\Myself\Documents\work\dev\Classif_Annonces\venv\lib\site-packages\transformers-2.4.1-py3.5.egg\transformers\modeling_xlm.py"", line 733, in __init__
    self.transformer = XLMModel(config)
  File ""C:\Users\Myself\Documents\work\dev\Classif_Annonces\venv\lib\site-packages\transformers-2.4.1-py3.5.egg\transformers\modeling_xlm.py"", line 382, in __init__
    self.ffns.append(TransformerFFN(self.dim, self.hidden_dim, self.dim, config=config))
  File ""C:\Users\Myself\Documents\work\dev\Classif_Annonces\venv\lib\site-packages\transformers-2.4.1-py3.5.egg\transformers\modeling_xlm.py"", line 203, in __init__
    self.lin2 = nn.Linear(dim_hidden, out_dim)
  File ""C:\Users\Myself\Documents\work\dev\Classif_Annonces\venv\lib\site-packages\torch\nn\modules\linear.py"", line 72, in __init__
    self.weight = Parameter(torch.Tensor(out_features, in_features))
RuntimeError: [enforce fail at ..\c10\core\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9437184 bytes. Buy new RAM!


Process finished with exit code 1
</code></pre>

<p>How can I use my pipeline with my <code>X</code> and <code>y</code> data?</p>
","8947333","","","","","2020-02-13 13:42:54","How to use the HuggingFace transformers pipelines?","<python><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"68006956","1","68013944","","2021-06-16 16:47:11","","0","188","<p>I am trying to fine tune Wav2Vec2 model for medical vocabulary. When I try to run the following code on my VS Code Jupyter notebook, I am getting an error, but when I run the same thing on Google Colab, it works fine.</p>
<pre><code>from transformers import Wav2Vec2ForCTC
 
model = Wav2Vec2ForCTC.from_pretrained(
    &quot;facebook/wav2vec2-base&quot;, 
    gradient_checkpointing=True, 
    ctc_loss_reduction=&quot;mean&quot;, 
    pad_token_id=processor.tokenizer.pad_token_id,
)
</code></pre>
<p>And here is the error that I getting on my VS Code</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-59-926a81051d7b&gt; in &lt;module&gt;
      1 from transformers import Wav2Vec2ForCTC
      2 
----&gt; 3 model = Wav2Vec2ForCTC.from_pretrained(
      4     &quot;facebook/wav2vec2-base&quot;,
      5     gradient_checkpointing=True,

AttributeError: type object 'Wav2Vec2ForCTC' has no attribute 'from_pretrained'
</code></pre>
","14327913","","","","","2021-06-17 06:17:50","AttributeError: type object 'Wav2Vec2ForCTC' has no attribute 'from_pretrained'","<python><visual-studio-code><jupyter-notebook><huggingface-transformers><huggingface-tokenizers>","1","4","","","","CC BY-SA 4.0"
"68007097","1","","","2021-06-16 16:57:17","","1","60","<p>Following is my Cell Code:</p>
<pre><code>from transformers import TrainingArguments

training_args = TrainingArguments(
  output_dir=&quot;wav2vec2-medical&quot;,
  group_by_length=True,
  per_device_train_batch_size=32,
  evaluation_strategy=&quot;steps&quot;,
  num_train_epochs=30,
  fp16=True,
  save_steps=500,
  eval_steps=500,
  logging_steps=500,
  learning_rate=1e-4,
  weight_decay=0.005,
  warmup_steps=1000,
  save_total_limit=2,
)
</code></pre>
<p>And here is the error that I am getting. I am not sure about what to make out of it.</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-26-f9014a6221db&gt; in &lt;module&gt;
      1 from transformers import TrainingArguments
      2 
----&gt; 3 training_args = TrainingArguments(
      4   # output_dir=&quot;/content/gdrive/MyDrive/wav2vec2-base-timit-demo&quot;,
      5   output_dir=&quot;./wav2vec2-medical&quot;,

~/Library/Python/3.8/lib/python/site-packages/transformers/training_args.py in __init__(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, logging_dir, logging_strategy, logging_first_step, logging_steps, save_strategy, save_steps, save_total_limit, no_cuda, seed, fp16, fp16_opt_level, fp16_backend, fp16_full_eval, local_rank, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, deepspeed, label_smoothing_factor, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, mp_parameters)

~/Library/Python/3.8/lib/python/site-packages/transformers/training_args.py in __post_init__(self)
    609 
    610         if is_torch_available() and self.device.type != &quot;cuda&quot; and (self.fp16 or self.fp16_full_eval):
--&gt; 611             raise ValueError(
    612                 &quot;Mixed precision training with AMP or APEX (`--fp16`) and FP16 evaluation can only be used on CUDA devices.&quot;
    613             )

ValueError: Mixed precision training with AMP or APEX (`--fp16`) and FP16 evaluation can only be used on CUDA devices.
</code></pre>
<p>I tried to run it on Jupyter notebook on local device and also on Google Colab but still I got the same error</p>
","16245372","","681865","","2021-06-17 23:33:56","2021-08-25 09:03:21","Getting a Mixed Precison Cuda Error while running a cell trying to fine tuning a Wav2Wav medical vocabulary module","<python-3.x><speech-recognition><huggingface-transformers><transformer>","0","0","","","","CC BY-SA 4.0"
"68007754","1","","","2021-06-16 17:49:44","","0","94","<p>Following this stackoverflow question:
<a href=""https://stackoverflow.com/questions/60120849/outputting-attention-for-bert-base-uncased-with-huggingface-transformers-torch"">Outputting attention for bert-base-uncased with huggingface/transformers (torch)</a></p>
<p>I am leveraging Hugging-face's Bart model for summarising text. In particular, I am going to follow their suggestion of using 'Bart for conditional generation'</p>
<p>I am trying to identify, analyse and tweak the encoder attention layer for articles depending on different topics.</p>
<p>The code I use to set up the model is as follows:</p>
<pre><code>from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig
import torch

config = BartConfig.from_pretrained(model_name, output_hidden_states=True, output_attention=True)
model = BartForConditionalGeneration.from_pretrained(model_name, config=config)
tokenizer = BartTokenizer.from_pretrained(model_name)
tokenized_sequence = tokenizer.tokenize(article)
indexed_tokens = tokenizer.encode(tokenized_sequence, return_tensors='pt')
outputs = model(indexed_tokens, return_dict=True)
</code></pre>
<p>Now, I am trying to understand the different outputs of the model</p>
<p>The keys of the outputs are listed below but do not include the attention layer?</p>
<pre><code>logits: [1, 599, 50264] -&gt; [index, Unknown?, vocab size]
past_key_values: tuple, len=12 -&gt; 12 hidden layers
decoder_hidden_states: tuple, len=13 -&gt; input bedding + 12 hidden layers
encoder_last_hidden_state: [1, 599, 1024] -&gt; [index, unknown, layers]
encoder_hidden_states: tuple, len=13 -&gt; input bedding + 12 hidden layers
</code></pre>
<p>Am I missing where the attention layer is stored?</p>
<p>In a more general sense, is it a sensible idea to take the attention layer, isolate neurons which seem to have a 1-1 mapping for select topics e.g. 'Cars' always activates neuron 507 with 0.6 and try to force the encoder attention layer going forward for text to better summarise articles with the topic 'cars'?</p>
","10645756","","","","","2021-06-16 17:49:44","Outputting attention for BartForConditionalGeneration pre-trained on facebook/bart-large-cnn with huggingface/transformers (torch)","<python><nlp><bert-language-model><huggingface-transformers><attention-model>","0","1","","","","CC BY-SA 4.0"
"68019253","1","","","2021-06-17 12:21:31","","0","38","<p>I got this issue when training a BERT model using HuggingFace library on Colab with TPU runtime . I've setup the TPU correctly and check if it is working well .</p>
<p>The Training parameters for BERT model are as follows:</p>
<pre><code>from transformers import TFTrainer, TFTrainingArguments

training_args = TFTrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=5,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=16,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
    evaluation_strategy=&quot;epoch&quot;
)
</code></pre>
<p>The training of model itself is below :</p>
<pre><code>with training_args.strategy.scope():
    model = BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;, num_labels=2)

trainer = TFTrainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=test_dataset,           # evaluation dataset
    compute_metrics=compute_metrics             
)

trainer.train()
</code></pre>
<p>However as soon as I execute above trainer it throws this error:</p>
<pre><code>WARNING:tensorflow:TPU system grpc://10.78.251.74:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.
WARNING:tensorflow:TPU system grpc://10.78.251.74:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.
INFO:tensorflow:Initializing the TPU system: grpc://10.78.251.74:8470
INFO:tensorflow:Initializing the TPU system: grpc://10.78.251.74:8470
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
&lt;ipython-input-33-8e05c1767b42&gt; in &lt;module&gt;()
     10 )
     11 
---&gt; 12 trainer.train()

4 frames
/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)

UnimplementedError: File system scheme '[local]' not implemented (file: './logs')
    Encountered when executing an operation using EagerExecutor. This error cancels all future operations and poisons their output tensors.
</code></pre>
","13680779","","","","","2021-06-21 18:16:08","File system scheme '[local]' not implemented (file: './logs')","<tensorflow><nlp><bert-language-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"68032190","1","","","2021-06-18 08:51:50","","0","46","<p>I get an error</p>
<pre><code>RuntimeError: Expected 3-dimensional input for 3-dimensional weight [512, 1, 10], but got 5-dimensional input of size [1, 1, 1, 240000, 2] instead
</code></pre>
<p>while feeding the <code>Wav2Vec2Processor</code> and <code>HubertForCTC</code> with a <code>wav</code> audio file:</p>
<pre><code>processor = Wav2Vec2Processor.from_pretrained(&quot;facebook/hubert-xlarge-ls960-ft&quot;
    , cache_dir=os.getenv(&quot;cache_dir&quot;, &quot;../../models&quot;))
model = HubertForCTC.from_pretrained(&quot;facebook/hubert-xlarge-ls960-ft&quot;
    , cache_dir=os.getenv(&quot;cache_dir&quot;, &quot;../../models&quot;))
for idx, audio in enumerate(train_loader):
    input_values = processor(audio, sampling_rate=sampling_rate, return_tensors=&quot;pt&quot;).input_values  # Batch size 1
    logits = model(input_values).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.decode(predicted_ids[0])
    print(transcription)
</code></pre>
<p>where the audio input comes from this function</p>
<pre><code>def read_audio(self, audio_path):
        try:
            import soundfile as sf
            y, _ = sf.read(audio_path)
            return y # [1, 960000]
        except Exception as err:
            try:
                import librosa
                y, _ = librosa.load(audio_path, sr=self.sr)
                return y
            except Exception as err:
                pass
                return None
</code></pre>
<p>The shape of the audio is like (with <code>soundfile</code>):</p>
<pre><code>0 torch.Size([1, 960000])
1 torch.Size([1, 240000, 2])
</code></pre>
<p>and <code>librosa</code>:</p>
<pre><code>0 torch.Size([1, 240000])
1 torch.Size([1, 960000])
</code></pre>
<p>Testing code is <a href=""https://github.com/loretoparisi/hf-experiments/blob/master/src/hubert/run.py"" rel=""nofollow noreferrer"">here</a>.</p>
","758836","","","","","2021-06-18 08:51:50","HuBERT: RuntimeError: Expected 3-dimensional input for 3-dimensional weight but got 5-dimensional input","<audio><huggingface-transformers><hubert>","0","0","1","","","CC BY-SA 4.0"
"67941739","1","","","2021-06-11 18:11:36","","1","65","<p>When using Hugging Face for sentiment analysis, it looks like you get the sentiment for a sentence as a whole. However if you have a sentence that compares 2 subject and one surpasses the other, it would look at the sentence as a whole. But the sentiment of one subject would be more positive while the other would be more negative.</p>
<p>Example:</p>
<blockquote>
<p>'People enjoy Hagen Dazs over Breyers'</p>
</blockquote>
<p>That's good for Hagen Dazs, but bad for Breyers. Hugging Face returns &quot;Positive&quot;.</p>
<blockquote>
<p>'You should buy Hagen Dazs instead of Breyers'</p>
</blockquote>
<p>Hugging Face returns &quot;Negative&quot; here which would apply to Breyers but not Hagen Dazs.</p>
<pre><code># Example script using the above text
from transformers import pipeline
classifier = pipeline(&quot;sentiment-analysis&quot;)
classifier('People enjoy Hagen Dazs over Breyers')

&gt;&gt; [{'label': 'POSITIVE', 'score': 0.9575461745262146}]

classifier('You should buy Hagen Dazs instead of Breyers')

&gt;&gt; [{'label': 'NEGATIVE', 'score': 0.9988893270492554}]
</code></pre>
<p>Is is possible to specify the topic to get the sentiment for in a sentence?</p>
","2980008","","","","","2021-06-11 18:11:36","How to use Hugging Face sentiment analysis for a specified subject in a sentence?","<python><nlp><sentiment-analysis><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"68021561","1","","","2021-06-17 14:40:55","","0","45","<p>I am looking to build a pipeline that applies the hugging-face BART model step-by-step. Once I have built the pipeline, I will be looking to substitute the encoder attention heads with a pre-trained / pre-defined encoder attention head.</p>
<p>The pipeline I will be looking to implement is as follows:</p>
<ol>
<li>Tokenize input</li>
<li>Run the tokenized input through the encoder with an adjusted attention layer</li>
<li>Run the output through the decoder</li>
<li>change output of decoder into text summary</li>
</ol>
<p>At the moment my code looks like the below with comments where I am stuck</p>
<pre><code>from transformers import AutoTokenizer, AutoModel, BartConfig, EncoderDecoderModel

article = &quot;&quot;&quot;Text to be summarised.&quot;&quot;&quot;

model_name = &quot;facebook/bart-large-cnn&quot;

# Values of dictionaries are tensors
attention_heads = {&quot;Cars&quot;: cars_encoder_attention_layer, 
                   &quot;Countries&quot;: countries_encoder_attention_layer
                  }

model_name = &quot;facebook/bart-large-cnn&quot;
config = BartConfig.from_pretrained(model_name, output_hidden_states=True, output_attention=True)
tokenizer = AutoTokenizer.from_pretrained(model_name)
inputs = tokenizer(article, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
model = AutoModel.from_pretrained(model_name)
model.config.output_attentions = True
outputs = model(**inputs)

# Overwrite the encoder attentions with the desired attention heads
outputs.encoder_attentions = attention_heads [&quot;Cars&quot;]

# Here I would take the overwritten encoder and insert into the decoder to generate the summary with the adjusted attention heads
</code></pre>
","10645756","","","","","2021-06-17 14:40:55","Run hugging-face BART model decoder with a BART encoder which has had the attention heads overwritten","<python><bert-language-model><huggingface-transformers><encoder-decoder><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"68058647","1","","","2021-06-20 17:57:01","","1","78","<p>How is it possible to initialize BERT with random weights? I want to compare the performance of multilingual vs monolingual vs randomly initialized BERT in a masked language modeling task. While in the former cases it is very straightforward:</p>
<pre><code>from transformers import BertTokenizer, BertForMaskedLM

tokenizer_multi = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model_multi = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')
model_multi.eval()

tokenizer_mono = BertTokenizer.from_pretrained('bert-base-cased')
model_mono = BertForMaskedLM.from_pretrained('bert-base-cased')
model_mono.eval()
</code></pre>
<p>I don't know how to load random weights.</p>
<p>Thanks in advance!</p>
","12607108","","","","","2021-06-20 17:57:01","Initialize HuggingFace Bert with random weights","<bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"60187634","1","","","2020-02-12 11:46:54","","1","85","<p>I was reading the code of GPT2 language model. The transformation of hidden states to the probability distribution over the vocabulary has done in the following line:</p>

<pre><code>lm_logits = self.lm_head(hidden_states)
</code></pre>

<p>Here,</p>

<pre><code>self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
</code></pre>

<p>However, 
In the <a href=""https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf"" rel=""nofollow noreferrer"">original paper</a>, they suggested multiplying hidden states with the token embedding matrix whereas <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface</a>  implementation used another matrix.</p>

<p>Is there any advantage of this? Am I missing something?  </p>
","3363813","","931175","","2020-03-10 01:08:30","2020-03-10 01:08:30","GPT-2 language model: multiplying decoder-transformer output with token embedding or another weight matrix","<python><nlp><pytorch><language-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"68015912","1","","","2021-06-17 08:45:19","","0","31","<p>I am looking to approach topic-aware modelling from a different angle. I am looking to provide the model with a topic e.g. 'Car' and the model to prioritise its summary based on this keyword.</p>
<p>For absolute clarity, if I was to input the same article (A body of text describing cars and their country of origin) and I inserted the topic 'Car' and 'Country' on separate runs, I would expect two different summaries, each focusing on either 'Car' or 'Country' respectively.</p>
<p>I am looking to implement my solution in python and my starting point is to implement the BART model from hugging-face's API. Now I am looking at the transformer code to see whether one of the below implementations is possible</p>
<ol>
<li>Insert a keyword parameter that the model can use to prioritise words with the closer context or proximity to the word -&gt; Most ideal solution but least likely with current progress made in this space</li>
<li>Output the attention layer and map topics to certain attention layer structures e.g. if neurons 614, 700 both stay static with 'Car' across articles, I can train the model with attention layers with this structure in future runs</li>
<li>Train the model multiple times on different topics</li>
</ol>
<p>The code I have currently implemented focuses on options 2 and 3 and is outlined below</p>
<p>All ideas are welcome for discussion including other potential models I can implement.</p>
<pre><code>import os, torch
from transformers import AutoTokenizer, AutoModel, BartConfig
article = &quot;&quot;&quot;Article of text describing cars and their country of origin&quot;&quot;&quot;
model_name = &quot;facebook/bart-large-cnn&quot;
config = BartConfig.from_pretrained(model_name, output_hidden_states=True, output_attention=True)
tokenizer = AutoTokenizer.from_pretrained(model_name)
inputs = tokenizer(article, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
model = AutoModel.from_pretrained(model_name)
model.config.output_attentions = True
outputs = model(**inputs)
print(len(outputs.encoder_attentions))
attentions_container.append(outputs.encoder_attentions)
</code></pre>
","10645756","","","","","2021-06-17 08:45:19","Add topic keyword to BART model (hugging face)","<python><bert-language-model><huggingface-transformers><topic-modeling><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"68040716","1","","","2021-06-18 19:48:50","","0","77","<p>Im working on a Multiclass Classification (Bengali Language Sentiment Analysis) on a pretrained Huggingface (BertForMaskedLM) model.</p>
<p>When the error occured I knew I have to change the label(output) size to match the input. But do not know how. Im adding the code snippents below.</p>
<pre><code>MAX_LEN = 200
BATCH_SIZE = 16
</code></pre>
<p><strong>The pretrained models used:</strong></p>
<pre><code>from transformers import BertForMaskedLM, BertTokenizer, pipeline
model = BertForMaskedLM.from_pretrained(&quot;sagorsarker/bangla-bert-base&quot;)
tokenizer = BertTokenizer.from_pretrained(&quot;sagorsarker/bangla-bert-base&quot;)
</code></pre>
<p><strong>Code to make the pytorch dataset</strong>:</p>
<pre><code>class GPReviewDataset(Dataset):

def __init__(self, reviews, targets, tokenizer, max_len):
self.reviews = reviews
self.targets = targets
self.tokenizer = tokenizer
self.max_len = max_len

def __len__(self):
 return len(self.reviews)

def __getitem__(self, item):
 review = str(self.reviews[item])
 target = self.targets[item]

encoding = self.tokenizer.encode_plus(
  review,
  add_special_tokens=True,
  max_length=self.max_len,
  truncation = True,
  return_token_type_ids=False,
  padding='max_length',
  return_attention_mask=True,
  return_tensors='pt',
)

return {
  'review_text': review,
  'input_ids': encoding['input_ids'].flatten(),
  'attention_mask': encoding['attention_mask'].flatten(),
  'targets': torch.tensor(target, dtype=torch.long)
}
</code></pre>
<p>The input dimentions are:</p>
<pre><code>print(data['input_ids'].shape)
print(data['attention_mask'].shape)
print(data['targets'].shape)
</code></pre>
<p>Which Outputs:</p>
<pre><code>torch.Size([16, 200])
torch.Size([16, 200])
torch.Size([16])
</code></pre>
<p><strong>Training Class</strong></p>
<pre><code>def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):

  model = model.train()    # tells your model that we are training
  losses = []
  correct_predictions = 0

  for d in data_loader:
    input_ids = d[&quot;input_ids&quot;].to(device)
    attention_mask = d[&quot;attention_mask&quot;].to(device)
    targets = d[&quot;targets&quot;].to(device)

    loss, logits = model(
      input_ids=input_ids,
      attention_mask=attention_mask,
      labels = targets
    )
    
    #logits = classification scores befroe softmax
    #loss = classification loss
    
    logits = logits.view(-1, 28*28).detach().cpu().numpy()
    label_ids = targets.to('cpu').numpy()

    preds = np.argmax(logits, axis=1).flatten()   #returns indices of maximum logit
    targ = label_ids.flatten()

    correct_predictions += np.sum(preds == targ)

    losses.append(loss.item())
    loss.backward()   # performs backpropagation(computes derivates of loss w.r.t to parameters)
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  #clipping gradients so they dont explode
    optimizer.step()       #After gradients are computed by loss.backward() this makes the optimizer iterate over all parameters it is supposed to update and use internally #stored grad to update their values
    scheduler.step()    # this will make sure learning rate changes. If we dont provide this learning rate stays at initial value
    optimizer.zero_grad()     # clears old gradients from last step

  return correct_predictions / n_examples, np.mean(losses)
</code></pre>
<p><strong>Where the training Starts (<em>Where the error triggers</em>)</strong>:</p>
<pre><code>%%time
# standard block
# used accuracy as metric here
history = defaultdict(list)

best_acc = 0

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(model, train_data_loader, optimizer, device, scheduler, len(df_train))

  print(f'Train loss {train_loss} Accuracy {train_acc}')

  val_acc, val_loss = eval_model(model, valid_data_loader, device, len(df_valid))

  print(f'Val   loss {val_loss} Accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc &gt; best_acc:
    torch.save(model.state_dict(), 'best_model_state_a5.bin')
    best_acc = val_acc
</code></pre>
<p><strong>The error</strong>:</p>
<pre><code>Epoch 1/5
----------
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-41-fb5a4d77ce37&gt; in &lt;module&gt;()
----&gt; 1 get_ipython().run_cell_magic('time', '', &quot;# standard block\n# used accuracy as metric here\nhistory = defaultdict(list)\n\nbest_acc = 0\n\nfor epoch in range(EPOCHS):\n\n  print(f'Epoch {epoch + 1}/{EPOCHS}')\n  print('-' * 10)\n\n  train_acc, train_loss = train_epoch(model, train_data_loader, optimizer, device, scheduler, len(df_train))\n\n  print(f'Train loss {train_loss} Accuracy {train_acc}')\n\n  val_acc, val_loss = eval_model(model, valid_data_loader, device, len(df_valid))\n\n  print(f'Val   loss {val_loss} Accuracy {val_acc}')\n  print()\n\n  history['train_acc'].append(train_acc)\n  history['train_loss'].append(train_loss)\n  history['val_acc'].append(val_acc)\n  history['val_loss'].append(val_loss)\n\n  if val_acc &gt; best_acc:\n    torch.save(model.state_dict(), 'best_model_state_a5.bin')\n    best_acc = val_acc\n\n# We are storing state of best model indicated by highest validation accuracy&quot;)

8 frames
/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell)
   2115             magic_arg_s = self.var_expand(line, stack_depth)
   2116             with self.builtin_trap:
-&gt; 2117                 result = fn(magic_arg_s, cell)
   2118             return result
   2119 

&lt;decorator-gen-53&gt; in time(self, line, cell, local_ns)

/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py in &lt;lambda&gt;(f, *a, **k)
    186     # but it's overkill for just that one bit of state.
    187     def magic_deco(arg):
--&gt; 188         call = lambda f, *a, **k: f(*a, **k)
    189 
    190         if callable(arg):

/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py in time(self, line, cell, local_ns)
   1191         else:
   1192             st = clock2()
-&gt; 1193             exec(code, glob, local_ns)
   1194             end = clock2()
   1195             out = None

&lt;timed exec&gt; in &lt;module&gt;()

&lt;ipython-input-39-948eefef2f8d&gt; in train_epoch(model, data_loader, optimizer, device, scheduler, n_examples)
     13       input_ids=input_ids,
     14       attention_mask=attention_mask,
---&gt; 15       labels = targets
     16     )
     17 

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)
   1327         if labels is not None:
   1328             loss_fct = CrossEntropyLoss()  # -100 index = padding token
-&gt; 1329             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
   1330 
   1331         if not return_dict:

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py in forward(self, input, target)
   1119     def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
   1120         return F.cross_entropy(input, target, weight=self.weight,
-&gt; 1121                                ignore_index=self.ignore_index, reduction=self.reduction)
   1122 
   1123 

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)
   2822     if size_average is not None or reduce is not None:
   2823         reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 2824     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2825 
   2826 

ValueError: Expected input batch_size (3200) to match target batch_size (16).
</code></pre>
","16263386","","","","","2021-06-18 19:48:50","HuggingFace BertForMaskedLM: Expected input batch_size (3200) to match target batch_size (16)","<deep-learning><nlp><bert-language-model><huggingface-transformers><multiclass-classification>","0","0","","","","CC BY-SA 4.0"
"68074898","1","","","2021-06-21 21:23:47","","0","43","<p>I'm trying to fine-tune BERT / transformer model that takes tabular data as an input.</p>
<p>Each row has many text inputs and some numeric features.</p>
<p>I want to have one neural network on one side. Then a transformer on the other side. The transformer will take the text inputs. The neural network will take the numeric inputs. The outputs will be concatenated and a final dense 1 sigmoid layer will produce a prediction.</p>
<pre class=""lang-py prettyprint-override""><code>def bert_model():
    num_columns = 10
    inputs = []
    numeric_inputs = layers.Input(shape = (window_size, num_columns))
    categorical_inputs = layers.Input(shape = (window_size, num_columns))
    
    inputs.append(numeric_inputs)
    inputs.append(categorical_inputs)
    
    input_ids = layers.Input(shape=(512,), name='input_token', dtype='int32')
    input_masks_ids = layers.Input(shape=(512,), name='masked_token', dtype='int32')
    unpooledoutputs = tfbert_model(input_ids, input_masks_ids)
    bert_layer = layers.Dropout(0.1)(unpooledoutputs.pooler_output)
    inputs.append(input_ids)
    inputs.append(input_masks_ids)
    # textual data
    for i in range(1, window_size):
        input_ids = layers.Input(shape=(512,), name=f'input_token_{i}', dtype='int32')
        input_masks_ids = layers.Input(shape=(512,), name=f'masked_token_{i}', dtype='int32')
        inputs.append(input_ids)
        inputs.append(input_masks_ids)
    
        unpooledoutputs = tfbert_model(input_ids, input_masks_ids)
        unpooledoutputs = layers.Dropout(0.1)(unpooledoutputs.pooler_output)
        bert_layer = layers.Concatenate()([bert_layer, \
                                                 unpooledoutputs])
    ... rest of the concatenation with other model

</code></pre>
<p>Is this the correct way to do this? I would like to use the single bert model to be fine-tuned over all of the inputs. I do want want to have to use like 50 BERT models, one for every text input, but to just have one.</p>
","2573069","","2573069","","2021-06-21 23:51:19","2021-06-21 23:51:19","Fine-tune BERT HuggingFace with many text inputs","<tensorflow><keras><time-series><classification><huggingface-transformers>","0","4","","","","CC BY-SA 4.0"
"68072148","1","","","2021-06-21 17:18:21","","0","128","<p>I am trying to use the pretrained SciBERT model (<a href=""https://huggingface.co/allenai/scibert_scivocab_uncased"" rel=""nofollow noreferrer"">https://huggingface.co/allenai/scibert_scivocab_uncased</a>) from Huggingface to evaluate masked words in scientific/biomedical text for bias using CrowS-Pairs (<a href=""https://github.com/nyu-mll/crows-pairs/"" rel=""nofollow noreferrer"">https://github.com/nyu-mll/crows-pairs/</a>).  The CrowS-Pairs code works great with the built in models like BERT.</p>
<p>I modified the code of metric.py with the goal of allowing an option of using the SciBERT model -</p>
<pre><code>import os
import csv
import json
import math
import torch
import argparse
import difflib
import logging
import numpy as np
import pandas as pd

from transformers import BertTokenizer, BertForMaskedLM
from transformers import AlbertTokenizer, AlbertForMaskedLM
from transformers import RobertaTokenizer, RobertaForMaskedLM
from transformers import AutoTokenizer, AutoModelForMaskedLM
</code></pre>
<p>and get the following error</p>
<pre><code>2021-06-21 17:11:38.626413: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File &quot;metric.py&quot;, line 15, in &lt;module&gt;
    from transformers import AutoTokenizer, AutoModelForMaskedLM
ImportError: cannot import name 'AutoModelForMaskedLM' from 'transformers' (/usr/local/lib/python3.7/dist-packages/transformers/__init__.py)
</code></pre>
<p>Later in the Python file, the AutoTokenizer and AutoModelForMaskedLM are defined as</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;)
model = AutoModelForMaskedLM.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;) 
</code></pre>
<p>Libraries</p>
<pre><code>huggingface-hub-0.0.8
sacremoses-0.0.45
tokenizers-0.10.3
transformers-4.7.0 
</code></pre>
<p>The error occurs with and without GPU support.</p>
","8858290","","8858290","","2021-06-23 02:40:50","2021-10-01 13:27:58","HuggingFace SciBert AutoModelForMaskedLM cannot be imported","<python><python-3.x><bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"68085588","1","","","2021-06-22 14:38:54","","1","119","<p>I am having difficulties understanding the <code>tokenizer.pad</code> method from the huggingface transformers library. In order to optimize training, I am performing tokenization in the Dataset such that no complicated operations are performed during data loading. My dataset looks like:</p>
<pre class=""lang-py prettyprint-override""><code>class DatasetTokenized(Dataset):

    def __init__(self, data: pd.DataFrame, text_column: str,
                 label_columns: List[str], tokenizer_name: str):
        super(DatasetTokenized, self).__init__()

        self.data = data
        self.text_column = text_column
        self.label_columns = label_columns
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)
        self.tokenized_data = self.tokenize_data(data)

    def __len__(self) -&gt; int:
        return len(self.tokenized_data)

    def __getitem__(self, index: int) -&gt; Dict:
        return self.tokenized_data[index]

    def tokenize_data(self, data: pd.DataFrame):
        tokenized_data = []

        print('Tokenizing data:')
        for _, row in tqdm(data.iterrows(), total=len(data)):

            text = row[self.text_column]
            labels = row[self.label_columns]
            encoding = self.tokenizer(text,
                                      add_special_tokens=True,
                                      max_length=512,
                                      padding=False,
                                      truncation=True,
                                      return_attention_mask=True,
                                      return_tensors='pt')

            tokenized_data.append({
                'text': text,
                'encoding': encoding,
                'labels': torch.FloatTensor(labels)
            })

        return tokenized_data
</code></pre>
<p>and my collator looks like:</p>
<pre class=""lang-py prettyprint-override""><code>class BertCollatorTokenized:

    def __init__(self, tokenizer_name: str):
        super(BertCollatorTokenized, self).__init__()

        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)

    def __call__(self, batch: List[Any]):
        text, encodings, labels = zip(
            *[[sample['text'], sample['encoding'], sample['labels']]
              for sample in batch])

        encodings = list(encodings)
        encodings = self.tokenizer.pad(encodings,
                                       max_length=512,
                                       padding='longest',
                                       return_tensors='pt')

        return {
            'text': text,
            'input_ids': encodings['input_ids'],
            'attention_mask': encodings['attention_mask'],
            'labels': torch.FloatTensor(labels)
        }
</code></pre>
<p>I have confirmed that <code>encodings</code> is a list of <code>BatchEncoding</code> as required by <code>tokenizer.pad</code>. However, I am getting the following error:</p>
<blockquote>
<p>ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.</p>
</blockquote>
<p>Which is a bit confusing as that should be the whole point of using <code>tokenizer.pad</code>. Anyone has an idea of what's going on?</p>
","3687776","","3687776","","2021-06-22 15:22:27","2021-06-22 15:22:27","Manually padding a list of BatchEncodings using huggingface's tokenizer","<pytorch><huggingface-transformers><huggingface-tokenizers>","0","3","","","","CC BY-SA 4.0"
"68043091","1","","","2021-06-19 02:23:27","","0","27","<p>I am trying to use my fine-tuned DistilBERT model to extract the pooled output of my dataset. AKA I am trying to extract the hidden state of the last layer of the '[CLS]' token.</p>
<p>I wrote this function to accomplish this task:</p>
<pre><code>def getPooledOutputs(model, encoded_dataset, batch_size = 128):
  pooled_outputs = []
  print(&quot;total number of iters &quot;, len(encoded_dataset['input_ids'])//batch_size + 1)
  
  with torch.no_grad():
    for i in range(len(encoded_dataset['input_ids'])//batch_size + 1):
      print(i)
      up_to = i*batch_size + batch_size
      if len(encoded_dataset['input_ids']) &lt; up_to:
        up_to = len(encoded_dataset['input_ids'])
      input_ids = th.LongTensor(encoded_dataset['input_ids'][i*batch_size:up_to])
      attention_mask = th.LongTensor(encoded_dataset['attention_mask'][i*batch_size:up_to])

      start = time.time()
      embeddings = model.distilbert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)[0][:,0] # Pooled output
      end = time.time()
      print(&quot;time for inference took:&quot;, end - start)
      pooled_outputs.extend(embeddings)

  return pooled_outputs
</code></pre>
<p>And I am calling it like so:</p>
<pre><code>train_set_embeddings = getPooledOutputs(model, chunked_encoded_dataset['train'])
</code></pre>
<p>This seems to work well and give me the results I want except by the time the code has finished executing, the program has consumed almost all 25 GB of RAM!! When the total amount of RAM used by the output of the function (train_set_embeddings) is some 22MB.</p>
<p>The RAM tends to spike towards the end of the loop in which it will jump from a couple GB to 18 GB.</p>
<p>Here is a screenshot of the RAM usage which you can see spikes towards the end of the run:</p>
<p><a href=""https://i.stack.imgur.com/vlKaX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vlKaX.png"" alt=""enter image description here"" /></a></p>
<p>Here is a screenshot of the runtime log in case it might help:
<a href=""https://i.stack.imgur.com/JsghE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JsghE.png"" alt=""enter image description here"" /></a></p>
","7254514","","","","","2021-06-19 02:23:27","Extracting pooled output from Huggingface Transformer model consumes too much memory","<memory-management><pytorch><ram><bert-language-model><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68091545","1","","","2021-06-22 22:54:51","","0","23","<p>I have been trying to do some token classification using huggingface transformers. I'm seeing instances where the tokenizer returns overlapping tokens. Sometimes (but not always) this will result in the model giving me an entity such that the (start, end) correspond to where the overlap starts and ends, but it lists the entity word as the empty string.</p>
<p>Here is a simple example to illustrate where it returns overlapping tokens:</p>
<pre><code>&gt;&gt;&gt; import transformers

&gt;&gt;&gt; tokenizer = transformers.AutoTokenizer.from_pretrained ('xlm-roberta-large-finetuned-conll03-english')

&gt;&gt;&gt; text = &quot;No clue.&quot;
&gt;&gt;&gt; tokens = tokenizer(text)

&gt;&gt;&gt; tokens[0].offsets
[(0, 0), (0, 2), (3, 4), (3, 6), (6, 7), (7, 8), (0, 0)]

&gt;&gt;&gt; [text[start:end] for (start,end) in tokens[0].offsets[1:-1]]
['No', 'c', 'clu', 'e', '.']
</code></pre>
<p>The examples where the model returns the overlapping character as a named entity are quite a bit longer. I can include them if needed, but shouldn't the tokenizer always return a non-overlapping set of tokens?</p>
","11529254","","","","","2021-06-22 22:54:51","Transformers tokenizer returns overlapping tokens. Is that a bug or am I doing something wrong?","<huggingface-transformers><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"68098813","1","","","2021-06-23 11:18:07","","1","640","<p>I am newbie to Machine Learning in general. I am currently trying to follow a tutorial on sentiment analysis using BERT and Transformers <a href=""https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"" rel=""nofollow noreferrer"">https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/</a></p>
<p>However when I train the model it has appeared that the model is overfitting
<a href=""https://i.stack.imgur.com/GSOTs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GSOTs.png"" alt=""Train/val loss &amp; accuracy"" /></a></p>
<p>I do not know how to fix this. I have tried lowering amount of epochs, increasing batch size , shuffling my data (which is ordered) and increasing the validation split. So far nothing has worked. I have even tried changing different learning rate but the one I am using now is the smallest.</p>
<p>Below is my code:</p>
<pre><code>PRE_TRAINED_MODEL_NAME = 'TurkuNLP/bert-base-finnish-cased-v1'
tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

MAX_LEN = 40

#Make a PyTorch dataset
class FIDataset(Dataset):

  def __init__(self, texts, targets, tokenizer, max_len):

    self.texts = texts

    self.targets = targets

    self.tokenizer = tokenizer

    self.max_len = max_len

  def __len__(self):

    return len(self.texts)

  def __getitem__(self, item):

    text = str(self.texts[item])

    target = self.targets[item]

    encoding = self.tokenizer.encode_plus(

      text,

      add_special_tokens=True,

      max_length=self.max_len,

      return_token_type_ids=False,

      pad_to_max_length=True,

      return_attention_mask=True,

      return_tensors='pt',

    )

    return {

      'text': text,

      'input_ids': encoding['input_ids'].flatten(),

      'attention_mask': encoding['attention_mask'].flatten(),

      'targets': torch.tensor(target, dtype=torch.long)

    }

#split test and train
df_train, df_test = train_test_split(

  df,

  test_size=0.1,

  random_state=RANDOM_SEED

)

df_val, df_test = train_test_split(

  df_test,

  test_size=0.5,

  random_state=RANDOM_SEED

)


#data loader function
def create_data_loader(df, tokenizer, max_len, batch_size):

  ds = FIDataset(

    texts=df.content.to_numpy(),

    targets=df.sentiment.to_numpy(),

    tokenizer=tokenizer,

    max_len=max_len

  )

  return DataLoader(

    ds,

    batch_size=batch_size,

    num_workers=4

  )

BATCH_SIZE = 32

#Load data into train, test, val
train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)

val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)

test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)

#Bert model loading
bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)

# Sentiment Classifier based on Bert model just loaded
class SentimentClassifier(nn.Module):

  def __init__(self, n_classes):

    super(SentimentClassifier, self).__init__()

    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)

    self.drop = nn.Dropout(p=0.1)

    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)

  def forward(self, input_ids, attention_mask):
    returned = self.bert(
        
        input_ids=input_ids,
        attention_mask=attention_mask
    )
    pooled_output = returned[&quot;pooler_output&quot;]
    output = self.drop(pooled_output)
    
    return self.out(output)

#Create a Classifier instance and move to GPU
model = SentimentClassifier(3)

model = model.to(device)

#Optimize with AdamW
EPOCHS = 6

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)

total_steps = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(

  optimizer,

  num_warmup_steps=0,

  num_training_steps=total_steps

)

loss_fn = nn.CrossEntropyLoss().to(device)

#Train each Epoch function
def train_epoch(

  model,

  data_loader,

  loss_fn,

  optimizer,

  device,

  scheduler,

  n_examples

):

  model = model.train()

  losses = []

  correct_predictions = 0

  for d in data_loader:

    input_ids = d[&quot;input_ids&quot;].to(device)

    attention_mask = d[&quot;attention_mask&quot;].to(device)

    targets = d[&quot;targets&quot;].to(device)

    outputs = model(

      input_ids=input_ids,

      attention_mask=attention_mask

    )

    _, preds = torch.max(outputs, dim=1)

    loss = loss_fn(outputs, targets)

    correct_predictions += torch.sum(preds == targets)

    losses.append(loss.item())

    loss.backward()

    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    optimizer.step()

    scheduler.step()

    optimizer.zero_grad()

  return correct_predictions.double() / n_examples, np.mean(losses)

import torch

history = defaultdict(list)

best_accuracy = 0

if __name__ == '__main__':    
    for epoch in range(EPOCHS):

      print(f'Epoch {epoch + 1}/{EPOCHS}')

      print('-' * 10)

      train_acc, train_loss = train_epoch(

        model,

        train_data_loader,

        loss_fn,

        optimizer,

        device,

        scheduler,

        len(df_train)

      )

      print(f'Train loss {train_loss} accuracy {train_acc}')

      val_acc, val_loss = eval_model(

        model,

        val_data_loader,

        loss_fn,

        device,

        len(df_val)

      )

      print(f'Val   loss {val_loss} accuracy {val_acc}')

      print()

      history['train_acc'].append(train_acc)

      history['train_loss'].append(train_loss)

      history['val_acc'].append(val_acc)

      history['val_loss'].append(val_loss)

      if val_acc &gt; best_accuracy:

        torch.save(model.state_dict(), 'best_model_state.bin')

        best_accuracy = val_acc


</code></pre>
","13719914","","","","","2021-06-27 04:24:23","Overfitting when fine-tuning BERT sentiment analysis","<python><sentiment-analysis><bert-language-model><huggingface-transformers><overfitting-underfitting>","1","0","","","","CC BY-SA 4.0"
"67994639","1","","","2021-06-15 23:38:12","","2","38","<p>I'm trying to use hugging face's BERT-base-uncased model to train on emoji prediction on tweets, and it seems that after the first epoch, the model immediately starts to overfit. I have tried the following:</p>
<ol>
<li>Increasing the training data (I increased this from 1x to 10x with no effect)</li>
<li>Changing the learning rate (no differences there)</li>
<li>Using different models from hugging face (the results were the same again)</li>
<li>Changing the batch size (went from 32, 72, 128, 256, 512, 1024)</li>
<li>Creating a model from scratch, but I ran into issues and decided to post here first to see if I was missing anything obvious.</li>
</ol>
<p>At this point, I'm concerned that the individual tweets don't give enough information for the model to make a good guess, but wouldn't it be random in that case, rather than overfitting?</p>
<p>Also, training time seems to be ~4.5 hours on Colab's free GPUs, is there any way to speed that up? I tried their TPU, but it doesn't seem to be recognized.</p>
<p>This is what the data looks like</p>
<p><a href=""https://i.stack.imgur.com/mmaQO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mmaQO.png"" alt=""dataset screenshot"" /></a></p>
<p>And this is my code below:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import json
import re
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.model_selection import train_test_split
import torch
from transformers import TrainingArguments, Trainer
from transformers import EarlyStoppingCallback
from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score
import numpy as np

# opening up the data and removing all symbols
df = pd.read_json('/content/drive/MyDrive/computed_results.json.bz2')
df['text_no_emoji'] = df['text_no_emoji'].apply(lambda text: re.sub(r'[^\w\s]', '', text))


# loading the tokenizer and the model from huggingface
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;, num_labels=5).to('cuda')

# test train split
train, test = train_test_split(df[['text_no_emoji', 'emoji_codes']].sample(frac=1), test_size=0.2)

# defining a dataset class that generates the encoder and labels on the fly to minimize memory usage
class Dataset(torch.utils.data.Dataset):    
    def __init__(self, input, labels=None):
        self.input = input
        self.labels = labels

    def __getitem__(self, pos):
        encoded = tokenizer(self.input[pos], truncation=True, max_length=15, padding='max_length')
        label = self.labels[pos]
        ret = {key: torch.tensor(val) for key, val in encoded.items()}

        ret['labels'] = torch.tensor(label)
        return ret

    def __len__(self):
        return len(self.labels)

# training and validation datasets are defined here
train_dataset = Dataset(train['text_no_emoji'].tolist(), train['emoji_codes'].tolist())
val_dataset = Dataset(train['text_no_emoji'].tolist(), test['emoji_codes'].tolist())

# defining the training arguments
args = TrainingArguments(
    output_dir=&quot;output&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    logging_steps = 10,
    per_device_train_batch_size=1024,
    per_device_eval_batch_size=1024,
    num_train_epochs=5,
    save_steps=3000,
    seed=0,
    load_best_model_at_end=True,
    weight_decay=0.2,
)

# defining the model trainer
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Training the model
trainer.train()
</code></pre>
<p>Results: After this, the training generally stops pretty fast due to the early stopper</p>
<p>The dataset can be found <a href=""https://drive.google.com/file/d/1N5ld3vzoKcBznOlAV7Rn09pwJVgkz-D3/view?usp=drivesdk"" rel=""nofollow noreferrer"">here</a> (39 Mb compressed)</p>
<p><a href=""https://i.stack.imgur.com/Ueuhm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ueuhm.png"" alt=""Results from 3 epochs"" /></a></p>
","5799075","","5799075","","2021-06-16 08:40:22","2021-06-16 08:40:22","Model overfits after first epoch","<pytorch><huggingface-transformers><huggingface-tokenizers>","0","9","","","","CC BY-SA 4.0"
"68014189","1","","","2021-06-17 06:40:35","","0","76","<p>This is my code</p>
<pre><code>from transformers import Trainer

trainer = Trainer(
    model=model,
    data_collator=data_collator,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=dataset_prepared[&quot;train&quot;],
    eval_dataset=dataset_prepared[&quot;test&quot;],
    tokenizer=processor.feature_extractor,
)
trainer.train()
</code></pre>
<p>And the error that I am getting is as follows:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-29-3435b262f1ae&gt; in &lt;module&gt;()
----&gt; 1 trainer.train()

5 frames
/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
   1051                         tr_loss += self.training_step(model, inputs)
   1052                 else:
-&gt; 1053                     tr_loss += self.training_step(model, inputs)
   1054                 self._total_flos += float(self.floating_point_ops(inputs))
   1055 

/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in training_step(self, model, inputs)
   1439         if self.use_amp:
   1440             with autocast():
-&gt; 1441                 loss = self.compute_loss(model, inputs)
   1442         else:
   1443             loss = self.compute_loss(model, inputs)

/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in compute_loss(self, model, inputs, return_outputs)
   1473         else:
   1474             labels = None
-&gt; 1475         outputs = model(**inputs)
   1476         # Save past state if it exists
   1477         # TODO: this needs to be fixed and made cleaner later.

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    887             result = self._slow_forward(*input, **kwargs)
    888         else:
--&gt; 889             result = self.forward(*input, **kwargs)
    890         for hook in itertools.chain(
    891                 _global_forward_hooks.values(),

/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py in forward(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)
   1081                     blank=self.config.pad_token_id,
   1082                     reduction=self.config.ctc_loss_reduction,
-&gt; 1083                     zero_infinity=self.config.ctc_zero_infinity,
   1084                 )
   1085 

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in ctc_loss(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)
   2306     &quot;&quot;&quot;
   2307     return torch.ctc_loss(
-&gt; 2308         log_probs, targets, input_lengths, target_lengths, blank, _Reduction.get_enum(reduction), zero_infinity
   2309     )
   2310 

RuntimeError: blank must be in label range
</code></pre>
<p>I am not quite sure, about what to make out of this. Here is the <a href=""https://colab.research.google.com/drive/1Ij_SabSpbs_EnY-yIP58aePArg1QcT3c?usp=sharing#scrollTo=Pp--Sx5EF2hp"" rel=""nofollow noreferrer"">link</a> to the Google Colab which I am using, it contains all of the code that I have been using to train the dataset, I don't know what is causing this error and how to fix it</p>
","14327913","","","","","2021-06-17 06:40:35","RuntimeError: blank must be in label range while fine tuning Wav2Vec2 model","<python><pytorch><torch><training-data><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"68114414","1","","","2021-06-24 10:55:12","","1","61","<p>I am trying to download the &quot;librispeech_asr&quot; dataset which totals 29GB, but due to limited space in google colab, I'm not able to download/load the dataset i.e. the notebook crashes.</p>
<p>So I did some research and found the <code>split</code> argument that we can pass in the <code>load_dataset</code> function to download a part of dataset, but it is still downloading the whole 30GB dataset on notebook. The argument <code>split</code> is not working...</p>
<pre><code>from datasets import load_dataset

dataset = load_dataset(&quot;librispeech_asr&quot;, 'clean', split=['train.360[:50%]', 'validation'])
</code></pre>
<p>I was trying to load only 50% of 'train.360' data but I'm unable to do so.</p>
<p>What is the correct method and what I am doing wrong?</p>
","11685381","","","","","2021-06-24 10:55:12","How to load a percentage of data from huggingface load_dataset","<python><nlp><speech-recognition><huggingface-transformers><huggingface-datasets>","0","1","1","","","CC BY-SA 4.0"
"68073045","1","","","2021-06-21 18:31:19","","0","44","<p>I am using transformers library to run a transformer model (<code>roberta-large-mnli</code>):</p>
<pre><code>def model(n_classes):
    input_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=&quot;input_ids&quot;)
    attention_mask = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=&quot;attention_mask&quot;)
    token_type_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=&quot;token_type_ids&quot;)
    inputs_ = {'input_ids':input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}

    net = TFRobertaModel.from_pretrained('roberta-large-mnli')(inputs_)['pooler_output']
    
    classes_ = tf.keras.layers.Dense(n_classes, activation='softmax')(net)
    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=classes_)
    model.summary()

    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
</code></pre>
<p>And this is how I start the training process:</p>
<pre><code>model.fit(x=training, steps_per_epoch=compute_steps(train['text']), epochs=EPOCHS,
                  validation_data=validation, validation_steps=compute_steps(dev['text']),
                  validation_freq=1, verbose=1, callbacks=callbacks())
</code></pre>
<p><code>training</code> is a tensorflow dataset (<code>tensorflow.python.data.ops.dataset_ops.PrefetchDataset</code>).</p>
<p>When I run this code, I get the following error:</p>
<pre><code>__________________________________________________________________________________________________                                                    Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
attention_mask (InputLayer)     [(None, 256)]        0                                                                                      
__________________________________________________________________________________________________
input_ids (InputLayer)          [(None, 256)]        0                                                                                     
__________________________________________________________________________________________________
token_type_ids (InputLayer)     [(None, 256)]        0
__________________________________________________________________________________________________
tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 355359744   attention_mask[0][0]                                                  
                                                                 input_ids[0][0]                                                                       10:21 21-Jun-21
                                                                 token_type_ids[0][0]                                                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 3)            3075        tf_roberta_model[0][1]                                       
==================================================================================================
Total params: 355,362,819                                                                                                                       
Trainable params: 355,362,819                               
Non-trainable params: 0                                                                                                                          
__________________________________________________________________________________________________
Texts contains 99 rows                                                                                                            
Texts contains 95 rows                  
Epoch 1/100                                                                                                                     
Traceback (most recent call last):     
  File &quot;n_shots.py&quot;, line 110, in &lt;module&gt;                                                                                      
    validation_freq=1, verbose=1, callbacks=callbacks())
  File &quot;/home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 108, in _method_wrapper
    return method(self, *args, **kwargs)     
  File &quot;/home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 1098, in fit  
    tmp_logs = train_function(iterator)                                 
  File &quot;/home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py&quot;, line 780, in __call__
    result = self._call(*args, **kwds)                                                                
  File &quot;/home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py&quot;, line 823, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)                                                  
  File &quot;/home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py&quot;, line 697, in _initialize
    *args, **kwds))
  File &quot;/home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/eager/function.py&quot;, line 2855, in _get_concrete_function_
internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File &quot;/home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/eager/function.py&quot;, line 3213, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File &quot;/home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/eager/function.py&quot;, line 3075, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File &quot;/home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 986, in func_graph_from_py
_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File &quot;/home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py&quot;, line 600, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File &quot;/home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 973, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AssertionError: in user code:

    /home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    /home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **
        outputs = model.train_step(data)
    /home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:747 train_step
        y_pred = self(x, training=True)
    /home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py:386 call
        inputs, training=training, mask=mask)
    /home/USER_NAME/venv/tf_23/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py:517 _run_internal_graph
        assert x_id in tensor_dict, 'Could not compute output ' + str(x)

    AssertionError: Could not compute output Tensor(&quot;dense/Softmax:0&quot;, shape=(None, 3), dtype=float32)  
</code></pre>
<p>If I change the line:</p>
<pre><code>net = TFRobertaModel.from_pretrained('roberta-large-mnli')(inputs_)['pooler_output']
</code></pre>
<p>to</p>
<pre><code>net = TFBertModel.from_pretrained('bert-base-uncased').bert(inputs_)['pooler_output']
</code></pre>
<p>everything works well.</p>
","3261292","","","","","2021-06-21 18:31:19","Error: AssertionError: Could not compute output Tensor(""dense/Softmax:0"", shape=(None, 3), dtype=float32)","<python><tensorflow><bert-language-model><huggingface-transformers><roberta-language-model>","0","0","","","","CC BY-SA 4.0"
"68086929","1","","","2021-06-22 15:56:14","","0","108","<p>I keep getting a <code>CUDA out of memory</code> error when trying to fine-tune a Hugging Face pretrained XML Roberta model. So, the first thing I want to find out is the size of the pretrained model.</p>
<pre><code>model = XLMRobertaForCausalLM.from_pretrained('xlm-roberta-base', config=config)
device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)

model.to(device)
</code></pre>
<p>I have tried to get the size of the model with</p>
<pre><code>sys.getsizeof(model)
</code></pre>
<p>and, unsurprisingly, I get an incorrect result.  I get 56 as a result, which is the size of the python object.</p>
<p>But then, I tried <code>model. element_size()</code>, and I get the error</p>
<pre><code>ModuleAttributeError: 'XLMRobertaForCausalLM' object has no attribute 'element_size'
</code></pre>
<p>I have searched in the Hugging Face documentation, but I have not found how to do it. Does anyone here know how to do it?</p>
","9390089","","","","","2021-06-28 17:31:36","How to get the size of a Hugging Face pretrained model?","<python><deep-learning><nlp><pytorch><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"68090115","1","","","2021-06-22 20:03:33","","1","21","<p>For kaggle competition I have decided to use transformers and 'distilbert-base-cased' for prediction of the text readability targets. Below is
example of dataset:</p>
<pre><code>                  excerpt                                      target
1705    The commutator is peculiar, consisting of only...   -3.676268
</code></pre>
<p>To check my model I trained it on one sample and expected to receive good prediction.
But I was surprised to find that the network converges to the result very slowly. Starting to understand, I also found that the back propogation gradient practically does not decrease:</p>
<p>*</p>
<pre><code>**tensor(12.2468, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(0): 3.499540571304336
tensor(11.5582, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(1): 3.399740736557078
tensor(11.3492, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(2): 3.3688636378441
tensor(10.6344, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(3): 3.261047659723675
tensor(9.8555, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(4): 3.139343303844696
tensor(9.0270, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(5): 3.0044979708165265
tensor(8.5099, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(6): 2.9171786994167936
tensor(7.6673, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(7): 2.7689972054580028
tensor(6.4712, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(8): 2.543863493615884
tensor(5.7926, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(9): 2.406785682401822
tensor(4.8025, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(10): 2.1914508563447015
tensor(3.9520, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(11): 1.9879615391746877
tensor(3.2887, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(12): 1.8134857654081245
tensor(2.3853, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(13): 1.5444405405115664
tensor(1.8604, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(14): 1.3639743212746638
tensor(1.5982, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(15): 1.2642145185690903
tensor(0.9373, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(16): 0.968127973883182
tensor(1.1406, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(17): 1.0679721723388667
tensor(0.7393, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(18): 0.859804150326125
tensor(0.7153, grad_fn=&lt;MseLossBackward&gt;)
Training RMSE Epoch(19): 0.8457331486894163**
</code></pre>
<ul>
<li></li>
</ul>
<p>Using other alternative network I receive the good results - for me it is proof that with data all are Okey. It seems I'm doing something
wrong - but cannot understand what. Could you look on my code and point out on my mistake.</p>
<p>Below is squeeze of my solution.</p>
<pre><code>def regression_calculate_rmse(big_val, targets):
    delta = 0
    for val1, val2 in zip(big_val.cpu().numpy(), targets.cpu().numpy()):
        delta += (val2 - val1)*(val2 - val1)

    return delta


class Triage(Dataset):
    def __init__(self, dataframe, tokenizer, max_len, isSubmit = False):
        self.len = len(dataframe)
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.isSubmit = isSubmit
        
    def __getitem__(self, index):
        title = str(self.data.excerpt[index])
        title = &quot; &quot;.join(title.split())
        inputs = self.tokenizer.encode_plus(
            title,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            pad_to_max_length=True,
            return_token_type_ids=True,
            truncation=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']

        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'targets': torch.tensor(self.data.target[index], dtype=torch.float)
            } 
    
    def __len__(self):
        return self.len


class DistillBERTClass(torch.nn.Module):
    def __init__(self):
        super(DistillBERTClass, self).__init__()
        self.l1 = AutoModel.from_pretrained(model_name,output_hidden_states=True)
        self.pre_regressor = torch.nn.Linear(768, 768)
        self.dropout = torch.nn.Dropout(0.1)
        self.regressor = torch.nn.Linear(768, 1)

    def forward(self, input_ids, attention_mask):
        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)
        hidden_state = output_1[0]
        pooler = hidden_state[:, 0]
        pooler = self.pre_regressor(pooler)
        pooler = torch.nn.ReLU()(pooler)
        pooler = self.dropout(pooler)
        output = self.regressor(pooler)
        return output


def train(epoch, lr_scheduler = None):
    nb_tr_examples = 0
    rmse = 0
    model.train()
    for _,data in enumerate(training_loader, 0):
        ids = data['ids'].to(device, dtype = torch.long)
        mask = data['mask'].to(device, dtype = torch.long)
        targets = data['targets'].to(device, dtype = torch.float)
        
        outputs = model(input_ids = ids, attention_mask = mask)

        loss = loss_function(outputs.view(-1), targets.view(-1))
        big_val, big_idx = torch.max(outputs.data, dim=1)
        rmse += regression_calculate_rmse(big_val, targets)
            
        nb_tr_examples+=targets.size(0)
        print(loss)
        
        optimizer.zero_grad()
        loss.backward()
        # # When using GPU
        optimizer.step()

    epoch_rmse = math.sqrt(rmse/nb_tr_examples)
    print(f&quot;Training RMSE Epoch({epoch}): {epoch_rmse}&quot;)

    return 


train_params = {'batch_size': 1,
                'shuffle': True,
                'num_workers': 0
                }

model_name = 'distilbert-base-cased'
tokenizer = AutoTokenizer.from_pretrained(model_name)

LEARNING_RATE = 2e-05
EPOCHS = 20

model = DistillBERTClass()


results:

model.to(device)

optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)

for epoch in range(EPOCHS):
    train(epoch)
</code></pre>
","5592430","","","","","2021-06-22 20:03:33","Torch neural network converges very slowly","<python><neural-network><torch><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"69089843","1","","","2021-09-07 14:14:41","","1","30","<p>I use roberta-base tokenizer <code>tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base',add_prefix_space=True)</code> trained on english data to tokenize bengali just to see how it behaves . When I try to to encode a bengali character <code>tokenizer.encode('à¦¬à¦¾')</code> , I get <code>[0, 1437, 35861, 11582, 35861, 4726, 2]</code> which means that it finds some tokens in it vocabulary which match bengali characters even though train on english. On further exploration I find these are all special characters <code>['&lt;s&gt;', 'Ä ', 'Ã Â¦', 'Â¬', 'Ã Â¦', 'Â¾', '&lt;/s&gt;']</code> . My question is why does it happen, isn't it supposed to output unknown tokens when applied on a new language ? Any help greatly appreciated</p>
","3728793","","","","","2021-09-07 14:14:41","Why BPE encoding trained on English and applied on Bengali doesnot return unknown tokens?","<huggingface-transformers><huggingface-tokenizers><roberta-language-model>","0","1","","","","CC BY-SA 4.0"
"69103528","1","","","2021-09-08 12:55:12","","1","23","<p>This is a question from ML newbee :-)</p>
<p>I am building AWS StepFunction with Serverless framework and one of the steps is intended to deploy a Sagemaker endpoint with HuggingFace deep learning container (DLC).</p>
<p>The problem is that I could not make Lambda to work with SageMaker (to build estimator).</p>
<p>One of the solutions I have is to manually launch the endpoint using the SageMaker studio, but I really want to have everything within the code.</p>
<p>Here is what I am trying to do to get Sagemaker working</p>
<pre class=""lang-py prettyprint-override""><code>def installPack(package):
    import subprocess
    import sys
    subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, package])

installPack('sagemaker')
from sagemaker.huggingface import HuggingFaceModel
import sagemaker 

role = sagemaker.get_execution_role()

# Hub Model configuration. https://huggingface.co/models
hub = {
        'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models
        'HF_TASK':'question-answering' # NLP task you want to use for predictions
        }

# create Hugging Face Model Class
huggingface_model = sagemaker.HuggingFaceModel(
            env=hub,
            role=role, # iam role with permissions to create an Endpoint
            transformers_version=&quot;4.6&quot;, # transformers version used
            pytorch_version=&quot;1.7&quot;, # pytorch version used
            py_version=&quot;py36&quot;, # python version of the DLC

..........

</code></pre>
<p>The error I'm getting is</p>
<pre><code>WARNING: The directory '/home/sbx_user1051/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.
</code></pre>
<p><em>(... then there are many lines of log like Collecting pyparsing&gt;=2.0.2
Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)...</em></p>
<pre><code>Downloading pox-0.3.0-py2.py3-none-any.whl (30 kB)
Collecting multiprocess&gt;=0.70.12
Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)
Using legacy 'setup.py install' for sagemaker, since package 'wheel' is not installed.
Using legacy 'setup.py install' for protobuf3-to-dict, since package 'wheel' is not installed.
Installing collected packages: dill, zipp, pytz, pyparsing, protobuf, ppft, pox, numpy, multiprocess, smdebug-rulesconfig, protobuf3-to-dict, pathos, pandas, packaging, importlib-metadata, google-pasta, attrs, sagemaker
ERROR: Could not install packages due to an OSError: [Errno 30] Read-only file system: '/home/sbx_user1051'
</code></pre>
","6564605","","6564605","","2021-09-08 13:38:05","2021-09-14 11:17:50","How to make inference with Huggingface deep learning container from Lambda using Serverless framework","<python><aws-lambda><serverless-framework><amazon-sagemaker><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"68068606","1","","","2021-06-21 13:14:06","","0","16","<p>I wonder if there is an easy way to use tokenizer with 3D list as I am following this tutorial: <a href=""https://huggingface.co/transformers/preprocessing.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/preprocessing.html</a></p>
<p>Actually in my case I have a 3D list like this:</p>
<pre><code>example_list = [[['drinking water systems']], [['irrigation systems'], ['irrigation networks'], ['irrigation ditches']]]
</code></pre>
<p>I have tried this but it did not work well it just work for one element I think:</p>
<pre><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
for i in ex_list:
  encoded_inputs = tokenizer(i[0], padding=True, truncation='longest_first', return_tensors=&quot;pt&quot;, max_length=512)
</code></pre>
<p>Is there an easy way to do this?</p>
<p>Expected result should be the same dimension as the original list just replace the string with the token ids.</p>
<pre><code>tensor([[[token_ids]],
 [[token_ids], [token_ids], [token_ids]]])
</code></pre>
","14471688","","","","","2021-06-21 13:14:06","How to use tokenizer for 3D list with transformers python?","<python><list><huggingface-transformers><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"69096660","1","","","2021-09-08 03:31:15","","0","27","<p>We would like to run hugging face transformer models in a firewalled environment. So, when installed transformers using conda (pointing to org jfrog repo) and tried running, it tries to download model files to cache folder in runtime which is failed.</p>
<p>However, my org will not allow downloading files via internet and place them manually in cache to run them offline. Only allowed way is via Conda install, does anyone aware if these hugging face transformer models are available in conda repo? can help provide conda package name of this?
Any pointers will be helpful.</p>
<p>Please note we already referred to this <a href=""https://huggingface.co/transformers/installation.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/installation.html</a>, but not useful in this context because we are forbidden to download files via internet.</p>
","15169517","","","","","2021-09-08 03:31:15","Hugging Face Models Installation Firewalled Environment or Using Conda","<python><conda><huggingface-transformers><machine-translation>","0","3","","","","CC BY-SA 4.0"
"69127607","1","","","2021-09-10 05:23:31","","1","64","<p>For some reason, I need to do further (2nd-stage) pre-training on Huggingface Bert model, and I find my training outcome is very bad.</p>
<p>After debugging for hours, surprisingly, I find even training one single batch after loading the base model, will cause the model to predict a very bad choice when I ask it to unmask some test sentences. I boil down my code to the minimal reproducible version here:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AdamW, BertTokenizer
from transformers import BertForPreTraining

MSK_CODE = 103
CE_IGN_IDX = -100 # CrossEntropyLoss ignore index value

def sanity_check(tokenizer, inputs):
    print(tokenizer.decode(inputs['input_ids'][0]))
    print(tokenizer.convert_ids_to_tokens(
        inputs[&quot;labels&quot;][0]
    ))
    print('Label:', inputs[&quot;next_sentence_label&quot;][0])

def test(tokenizer, model, topk=3):
    test_data = &quot;She needs to [MASK] that [MASK] has only ten minutes.&quot;
    print('\n \033[92m', test_data, '\033[0m')
    test_inputs = tokenizer([test_data],
                       padding=True, truncation=True, return_tensors=&quot;pt&quot;)
    def classifier_hook(module, inputs, outputs):
        unmask_scores, seq_rel_scores = outputs
        token_ids = test_inputs['input_ids'][0]
        masked_idx = (
            token_ids == torch.tensor([MSK_CODE])
        )
        scores = unmask_scores[0][masked_idx]
        cands = torch.argsort(scores, dim=1, descending=True)
        for i, mask_cands in enumerate(cands):
            top_cands = mask_cands[:topk].detach().cpu()
            print(f'MASK[{i}] top candidates:', end=&quot; &quot;)
            print(tokenizer.convert_ids_to_tokens(top_cands))
    classifier = model.cls
    hook = classifier.register_forward_hook(classifier_hook)
    model.eval()
    model(**test_inputs)
    hook.remove()
    print()

# load model
model = BertForPreTraining.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)

# first test
test(tokenizer, model)

# our single-iteration inputs
#   [CLS]  1   2   3   4   5   6   [SEP]  8      9    10  11    12  [SEP]
pair = [['the man went to the store', 'penguins are flightless birds']]
relation_label = 1

# construct inputs
inputs = tokenizer(pair, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
inputs[&quot;next_sentence_label&quot;] = torch.tensor([relation_label])
mask_labels = torch.full(inputs[&quot;input_ids&quot;].shape, fill_value=CE_IGN_IDX)
inputs[&quot;labels&quot;] = mask_labels

# mask two words
inputs[&quot;input_ids&quot;][0][4] = MSK_CODE
inputs[&quot;input_ids&quot;][0][9] = MSK_CODE
mask_labels[0][4] = tokenizer.convert_tokens_to_ids('to')
mask_labels[0][9] = tokenizer.convert_tokens_to_ids('are')

# train for one single iteration
sanity_check(tokenizer, inputs)
model.train()
optimizer.zero_grad()
outputs = model(**inputs)
loss = outputs.loss
loss.backward(loss)
optimizer.step()

# second test
test(tokenizer, model)

</code></pre>
<p>output:</p>
<pre class=""lang-sh prettyprint-override""><code>Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

  She needs to [MASK] that [MASK] has only ten minutes. 
MASK[0] top candidates: ['know', 'understand', 'remember']
MASK[1] top candidates: ['she', 'he', 'it']

[CLS] the man went [MASK] the store [SEP] penguins [MASK] flightless birds [SEP]
['[UNK]', '[UNK]', '[UNK]', '[UNK]', 'to', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'are', '[UNK]', '[UNK]', '[UNK]', '[UNK]']
Label: tensor(1)

  She needs to [MASK] that [MASK] has only ten minutes. 
MASK[0] top candidates: ['are', 'know', 'be']
MASK[1] top candidates: ['are', 'is', 'she']
</code></pre>
<p>Basically, I use <code>She needs to [MASK] that [MASK] has only ten minutes.</code> as a test sentence to test the unmasking.
As you may see, at the beginning when I tested the base model, it works perfectly.
However, after I feed the pre-train model with a single pair of training batch:</p>
<p><code>[CLS] the man went [MASK] the store [SEP] penguins [MASK] flightless birds [SEP]</code></p>
<p>The updated model no longer makes sense, it unmasks <code>She needs to [MASK] that [MASK] has only ten minutes.</code> into <code>She needs to [are] that [are] has only ten minutes. </code></p>
<p>I can think of two possibilities why this happens...</p>
<ol>
<li>Bert model is extremely sensitive to training batch size, a small batch causes unacceptable bias.</li>
<li>there is a bug in the code?</li>
</ol>
<p>So, any idea?</p>
","2736576","","","","","2021-09-10 05:23:31","One single-batch training on Huggingface Bert model ""ruins"" the model","<pytorch><huggingface-transformers><bert-language-model>","0","4","1","","","CC BY-SA 4.0"
"69128109","1","","","2021-09-10 06:29:35","","0","16","<p>I want to  use TFAlbertForMaskedLM to train a Albert,i use the same code  when i successfully use TFBertForSequenceClassification to finetune Bert:</p>
<pre><code>learning_rate = 2e-10

flag=0

if flag==0:
    model = TFAlbertForMaskedLM.from_pretrained('../input/alber-model')
else:
    model = TFAlbertForMaskedLM.from_pretrained('./output_new')

optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08, clipnorm=1)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

bert_history = model.fit(x=X_train.values(), y=y_train,epochs=2,batch_size=256)
</code></pre>
<p>but there is a error about size of X_train and y_train, but the official doc don't have the information about the right size.where can I find the tips for the size of  X_train and y_train?</p>
","8300481","","8300481","","2021-09-10 06:34:53","2021-09-10 06:34:53","how to use TFAlbertForMaskedLM to train?","<tensorflow><huggingface-transformers><bert-language-model><transformer>","0","1","","","","CC BY-SA 4.0"
"69157517","1","","","2021-09-13 05:07:43","","0","12","<p>I'm using a python package called <code>transformers</code>. This package is complicated and has lots of classes that inherit from each other.</p>
<p>I'm using an instance (lets call it <code>model</code>) of one particular class (let's call it <code>Model</code>). There is a class method <code>model.generate()</code> that is useful, but
I would like to change one line of this method <code>generate()</code> (specifically, to remove its decorator).</p>
<p>I have tried defining a function <code>generate1()</code> and copy-pasting the code of <code>model.generate()</code> into <code>generate1()</code>, and then executing <code>model.generate1 = generate1()</code>. Unfortunately I run into some problems</p>
<ul>
<li>the error <code>generate1() missing 1 required positional argument: 'self'</code>. I could remove <code>self</code> from the function definition, but there is many lines like <code>if self.a.b.c == True</code> inside the function, so it's not easy to change all of those lines too.</li>
<li>there are also many missing imports. These are defined in the body of the classes of <code>transformers</code>, but not in my function <code>generate1()</code></li>
</ul>
<p>The easiest way seems to be changing the source code of the package directly, but if I update the package I might lose the changes, so a bit reluctant to do this.</p>
<p>Is there an easier way?</p>
","5381490","","","","","2021-09-13 05:07:43","How can I easily change a class method from a package in Python","<python><huggingface-transformers>","0","1","","2021-09-13 05:10:36","","CC BY-SA 4.0"
"68058905","1","","","2021-06-20 18:27:50","","0","30","<p>I am trying to use the huggingface implementation of the vision transformer to get the feature vector of the last but one dense layer</p>
","11899766","","","","","2021-06-21 20:16:05","How to get the output of the last but one layer of the Vision transformer using the hugging face implementation?","<huggingface-transformers><transformer>","1","1","","","","CC BY-SA 4.0"
"68058974","1","","","2021-06-20 18:35:50","","0","33","<p>I want to do batch inference on MarianMT model. Here's the code:</p>
<pre><code>from transformers import MarianTokenizer
tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')
src_texts = [ &quot;I am a small frog.&quot;, &quot;Tom asked his teacher for advice.&quot;]
tgt_texts = [&quot;Ich bin ein kleiner Frosch.&quot;, &quot;Tom bat seinen Lehrer um Rat.&quot;]  # optional
inputs = tokenizer(src_texts, return_tensors=&quot;pt&quot;, padding=True)
with tokenizer.as_target_tokenizer():
    labels = tokenizer(tgt_texts, return_tensors=&quot;pt&quot;, padding=True)
inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]
outputs = model(**inputs) 
</code></pre>
<p>How do I do batch inference?</p>
","12219510","","","","","2021-06-20 18:35:50","How to do batch inferenece for Hugging face models?","<machine-learning><deep-learning><nlp><huggingface-transformers><machine-translation>","0","0","","","","CC BY-SA 4.0"
"60220842","1","60224015","","2020-02-14 06:02:09","","2","816","<p>I am using Huggingface's <code>transformers</code> library and want to perform NER using BERT. I tried to find an explicit example of how to properly format the data for NER using BERT. It is not entirely clear to me from the paper and the comments I've found.</p>

<p>Let's say we have a following sentence and labels:</p>

<pre><code>sent = ""John Johanson lives in Ramat Gan.""
labels = ['B-PER', 'I-PER', 'O', 'O', 'B-LOC', 'I-LOC']
</code></pre>

<p>Would data that we input to the model be something like this:</p>

<pre><code>sent = ['[CLS]', 'john', 'johan',  '##son', 'lives',  'in', 'ramat', 'gan', '.', '[SEP]']
labels = ['O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O']
attention_mask = [0, 1, 1, 1, 1, 1, 1, 1, 1, 0]
sentence_id = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</code></pre>

<p>?</p>

<p>Thank you!</p>
","7757104","","7757104","","2020-02-14 17:26:25","2021-08-27 12:27:00","How should properly formatted data for NER in BERT look like?","<python><nlp><format><bert-language-model><huggingface-transformers>","1","1","2","","","CC BY-SA 4.0"
"60232485","1","60263925","","2020-02-14 19:34:41","","0","244","<p>I have:</p>

<pre><code>from transformers import XLNetTokenizer, XLNetForQuestionAnswering
import torch

tokenizer =  XLNetTokenizer.from_pretrained('xlnet-base-cased')
model = XLNetForQuestionAnswering.from_pretrained('xlnet-base-cased')

input_ids = torch.tensor(tokenizer.encode(""What is my name?"", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])
outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)
loss = outputs[0]

print(outputs)
print(loss)
</code></pre>

<p>As per the docs. This does something giving:</p>

<pre><code>(tensor(2.3008, grad_fn=&lt;DivBackward0&gt;),)
tensor(2.3008, grad_fn=&lt;DivBackward0&gt;)
</code></pre>

<p>However, I want an actual answer if possible?</p>
","239879","","","","","2020-02-17 13:50:02","How can I implement basic question answering with hugging-face?","<python><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"69159507","1","69200389","","2021-09-13 08:26:35","","1","90","<p>I would like to load the BERT's weights (or whatever transformer) into a <a href=""https://huggingface.co/transformers/model_doc/dpr.html#transformers.DPRQuestionEncoder"" rel=""nofollow noreferrer"">DPRQuestionEncoder</a> architecture, such that I can use the HuggingFace <em>save_pretrained</em> method and plug the saved model into the <a href=""https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag"" rel=""nofollow noreferrer"">RAG architecture to do end-to-end fine-tuning</a>.</p>
<pre><code>from transformers import DPRQuestionEncoder
model = DPRQuestionEncoder.from_pretrained('bert-base-uncased')
</code></pre>
<p>But I got the following error</p>
<pre><code>You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.

NotImplementedErrorTraceback (most recent call last)
&lt;ipython-input-27-1f1b990b906b&gt; in &lt;module&gt;
----&gt; 1 model = DPRQuestionEncoder.from_pretrained(model_name)
      2 # https://github.com/huggingface/transformers/blob/41cd52a768a222a13da0c6aaae877a92fc6c783c/src/transformers/models/dpr/modeling_dpr.py#L520

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1211                     )
   1212 
-&gt; 1213             model, missing_keys, unexpected_keys, error_msgs = cls._load_state_dict_into_model(
   1214                 model, state_dict, pretrained_model_name_or_path, _fast_init=_fast_init
   1215             )

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in _load_state_dict_into_model(cls, model, state_dict, pretrained_model_name_or_path, _fast_init)
   1286             )
   1287             for module in unintialized_modules:
-&gt; 1288                 model._init_weights(module)
   1289 
   1290         # copy state_dict so _load_from_state_dict can modify it

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in _init_weights(self, module)
    515         Initialize the weights. This method should be overridden by derived class.
    516         &quot;&quot;&quot;
--&gt; 517         raise NotImplementedError(f&quot;Make sure `_init_weigths` is implemented for {self.__class__}&quot;)
    518 
    519     def tie_weights(self):

NotImplementedError: Make sure `_init_weigths` is implemented for &lt;class 'transformers.models.dpr.modeling_dpr.DPRQuestionEncoder'&gt;
</code></pre>
<p>I am using the last version of Transformers.</p>
","13418745","","","","","2021-09-15 22:24:52","Load a model as DPRQuestionEncoder in HuggingFace","<python><nlp><huggingface-transformers><bert-language-model><transformer>","1","2","","","","CC BY-SA 4.0"
"68038820","1","","","2021-06-18 16:47:17","","0","449","<p>I uesd TFBertModel and Tensorflow model to combined and training with Hugging Face transformers.I want to save the best model of val_accuracy of each epoch.I used 'tensorflow checkpoint', but I got the error.How do I save the best model of each epoch with transformers bert in tensorflow?</p>
<pre><code>from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.initializers import TruncatedNormal
from tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy,BinaryAccuracy
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.utils import plot_model
from transformers import AutoTokenizer,TFBertModel
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')
bert = TFBertModel.from_pretrained('bert-large-uncased')
max_len = max_length
input_ids = Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_ids&quot;)
input_mask = Input(shape=(max_len,), dtype=tf.int32, name=&quot;attention_mask&quot;)
# embeddings = dbert_model(input_ids,attention_mask = input_mask)[0]


embeddings = bert(input_ids,attention_mask = input_mask)[1] #(0 is the last hidden states,1 means pooler_output)
# out = tf.keras.layers.GlobalMaxPool1D()(embeddings)
out = tf.keras.layers.Dropout(0.1)(embeddings)

out = Dense(128, activation='relu')(out)
out = tf.keras.layers.Dropout(0.1)(out)
out = Dense(32,activation = 'relu')(out)

y = Dense(1,activation = 'sigmoid')(out)
    
model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)
model.layers[2].trainable = True
#model.save_weights('path/savefile')
</code></pre>
","13785227","","13785227","","2021-06-19 05:09:33","2021-06-19 05:09:33","How to save the best model of each epoch with transformers bert in tensorflow","<python><tensorflow><nlp><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"68039639","1","","","2021-06-18 18:00:16","","0","10","<p>I have sequence classification problem, I want my RoBERTa-base-sequence classier to predict weather the answer for a particular question is valid or Invalid. I am already able to do this for answers which are of 1 or 2 tokens by training this <a href=""https://huggingface.co/iarfmoose/bert-base-cased-qa-evaluator"" rel=""nofollow noreferrer"">https://huggingface.co/iarfmoose/bert-base-cased-qa-evaluator</a> transformers model on Hotpot QA and Squad.
But the problem arises when we want to classify answers which are long (i.e. answers with 3 or more tokens). The RoBerta-sequence classifier is unable to learn for longer sequences.
Is there a way to train it on longer sequence answers?</p>
","13034631","","","","","2021-06-18 18:00:16","Sequence classifer not working on long sentences","<python><nlp><artificial-intelligence><huggingface-transformers><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"68087139","1","","","2021-06-22 16:09:46","","2","82","<p>When saving a version in Kaggle, I get <strong>StdinNotImplementedError: getpass was called, but this frontend does not support input requests</strong> whenever I use the Transformers.Trainer class. The general code I use:</p>
<pre><code>from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(params)
trainer = Trainer(params)
trainer.train()
</code></pre>
<p>And the specific cell I am running now:</p>
<pre><code>from transformers import Trainer, TrainingArguments,EarlyStoppingCallback

early_stopping = EarlyStoppingCallback()

training_args = TrainingArguments(
    output_dir=OUT_FINETUNED_MODEL_PATH,          
    num_train_epochs=20,              
    per_device_train_batch_size=16,  
    per_device_eval_batch_size=16,   
    warmup_steps=0,                
    weight_decay=0.01,               
    logging_dir='./logs',            
    logging_steps=100,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model=&quot;eval_loss&quot;,
    greater_is_better=False
    
)

trainer = Trainer(
    model=model,                         
    args=training_args,                  
    train_dataset=train_dataset,         
    eval_dataset=val_dataset,             
    callbacks=[early_stopping]

)

trainer.train()
</code></pre>
<p>When trainer.train() is called, I get the error below, which I do not get if I train with native PyTorch. I understood that the error arises since I am asked to input a password, but no password is asked when using native PyTorch code, nor when using the same code with trainer.train() on Google Colab.
Any solution would be ok, like:</p>
<ol>
<li>Avoid being asked the password.</li>
<li>Enable input requests when saving a notebook on Kaggle. After that, if I understood correctly, I would need to go to <a href=""https://wandb.ai/authorize"" rel=""nofollow noreferrer"">https://wandb.ai/authorize</a> (after having created an account) and copy the generated key to console. However, I do not understand why wandb should be necessary since I never explicitly used it so far.</li>
</ol>
<pre><code>wandb: You can find your API key in your browser here: https://wandb.ai/authorize
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py&quot;, line 741, in init
    wi.setup(kwargs)
  File &quot;/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py&quot;, line 155, in setup
    wandb_login._login(anonymous=anonymous, force=force, _disable_warning=True)
  File &quot;/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py&quot;, line 210, in _login
    wlogin.prompt_api_key()
  File &quot;/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py&quot;, line 144, in prompt_api_key
    no_create=self._settings.force,
  File &quot;/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/apikey.py&quot;, line 135, in prompt_api_key
    key = input_callback(api_ask).strip()
  File &quot;/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py&quot;, line 825, in getpass
    &quot;getpass was called, but this frontend does not support input requests.&quot;
IPython.core.error.StdinNotImplementedError: getpass was called, but this frontend does not support input requests.
wandb: ERROR Abnormal program exit
---------------------------------------------------------------------------
StdinNotImplementedError                  Traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)
    740         wi = _WandbInit()
--&gt; 741         wi.setup(kwargs)
    742         except_exit = wi.settings._except_exit

/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py in setup(self, kwargs)
    154         if not settings._offline and not settings._noop:
--&gt; 155             wandb_login._login(anonymous=anonymous, force=force, _disable_warning=True)
    156 

/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py in _login(anonymous, key, relogin, host, force, _backend, _silent, _disable_warning)
    209     if not key:
--&gt; 210         wlogin.prompt_api_key()
    211 

/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py in prompt_api_key(self)
    143             no_offline=self._settings.force,
--&gt; 144             no_create=self._settings.force,
    145         )

/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/apikey.py in prompt_api_key(settings, api, input_callback, browser_callback, no_offline, no_create, local)
    134             )
--&gt; 135             key = input_callback(api_ask).strip()
    136         write_key(settings, key, api=api)

/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py in getpass(self, prompt, stream)
    824             raise StdinNotImplementedError(
--&gt; 825                 &quot;getpass was called, but this frontend does not support input requests.&quot;
    826             )

StdinNotImplementedError: getpass was called, but this frontend does not support input requests.

The above exception was the direct cause of the following exception:

Exception                                 Traceback (most recent call last)
&lt;ipython-input-82-4d1046ab80b8&gt; in &lt;module&gt;
     42     )
     43 
---&gt; 44     trainer.train()

/opt/conda/lib/python3.7/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
   1067         model.zero_grad()
   1068 
-&gt; 1069         self.control = self.callback_handler.on_train_begin(self.args, self.state, self.control)
   1070 
   1071         # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.

/opt/conda/lib/python3.7/site-packages/transformers/trainer_callback.py in on_train_begin(self, args, state, control)
    338     def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):
    339         control.should_training_stop = False
--&gt; 340         return self.call_event(&quot;on_train_begin&quot;, args, state, control)
    341 
    342     def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):

/opt/conda/lib/python3.7/site-packages/transformers/trainer_callback.py in call_event(self, event, args, state, control, **kwargs)
    386                 train_dataloader=self.train_dataloader,
    387                 eval_dataloader=self.eval_dataloader,
--&gt; 388                 **kwargs,
    389             )
    390             # A Callback can skip the return of `control` if it doesn't change it.

/opt/conda/lib/python3.7/site-packages/transformers/integrations.py in on_train_begin(self, args, state, control, model, **kwargs)
    627             self._wandb.finish()
    628         if not self._initialized:
--&gt; 629             self.setup(args, state, model, **kwargs)
    630 
    631     def on_train_end(self, args, state, control, model=None, tokenizer=None, **kwargs):

/opt/conda/lib/python3.7/site-packages/transformers/integrations.py in setup(self, args, state, model, **kwargs)
    604                     project=os.getenv(&quot;WANDB_PROJECT&quot;, &quot;huggingface&quot;),
    605                     name=run_name,
--&gt; 606                     **init_args,
    607                 )
    608             # add config parameters (run may have been created manually)

/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)
    779             if except_exit:
    780                 os._exit(-1)
--&gt; 781             six.raise_from(Exception(&quot;problem&quot;), error_seen)
    782     return run

/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

Exception: problem
</code></pre>
","16291114","","","","","2021-07-21 13:06:38","trainer.train() in Kaggle: StdinNotImplementedError: getpass was called, but this frontend does not support input requests","<huggingface-transformers><kaggle><getpass><wandb>","1","0","","","","CC BY-SA 4.0"
"68116355","1","","","2021-06-24 13:04:01","","0","25","<p>I have a PyTorch/Transformers model for NER. When I use it for inference, it crashes with too long sentences (the famous CUDA out of memory).</p>
<p>Is there a way to break sentences in the same fashion as the <a href=""https://github.com/explosion/spacy-transformers/blob/master/spacy_transformers/span_getters.py"" rel=""nofollow noreferrer"">spacy span_getters</a>?</p>
<p>Is there a transformers' argument that can be set for that?</p>
<p>Thanks.</p>
","11348232","","","","","2021-06-24 13:04:01","How to feed long sentences to transformer model","<nlp><pytorch><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"69194640","1","","","2021-09-15 14:01:14","","1","59","<p>I just trained a BERT model on a Dataset composed by products and labels (departments) for an e-commerce website. It's a multiclass problem. I used BertForSequenceClassification to predict the department for each product. I split it in train and evaluation, I used dataloader from pytorch, and I've got a good score with no overfit.</p>
<p>Now I want to try it on a new Dataset to check how it works on unseen data. But I can't achieve to load the model and apply on the new Dataset. I get the following error:</p>
<pre><code>RuntimeError: Error(s) in loading state_dict for BertForSequenceClassification:
    size mismatch for classifier.weight: copying a param with shape torch.Size([59, 1024]) from checkpoint, the shape in current model is torch.Size([105, 1024]).
    size mismatch for classifier.bias: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([105]).
</code></pre>
<p>I see that the problem probably is a mismatch from labels size between both Datasets. I've searched a bit and I've found a recommendation to use <code>ignore_mismatched_sizes=True</code> as and argument for <code>pretrained</code>. But I keep receiving the same error.</p>
<p>Here is part of my code when trying to predict on unseen data:</p>
<pre><code>from transformers import BertForSequenceClassification

# Just right before the actual usage select your hardware
device = torch.device('cuda') # or cpu
model = model.to(device)      # send your model to your hardware



model = BertForSequenceClassification.from_pretrained(&quot;neuralmind/bert-large-portuguese-cased&quot;,
                                                      num_labels=len(label_dict),
                                                      output_attentions=False,
                                                      output_hidden_states=False,
                                                      ignore_mismatched_sizes=True)

model.to(device)

model.load_state_dict(torch.load('finetuned_BERT_epoch_2_full-Copy1.model', map_location=torch.device('cuda')))

_, predictions, true_vals = evaluate(dataloader_validation)
accuracy_per_class(predictions, true_vals)
</code></pre>
<p>Could someone help me how could I deal with it? I don't know what more can I do!</p>
<p>Any help I'm very grateful!</p>
","15673147","","15673147","","2021-09-15 14:12:35","2021-09-15 16:48:43","Mismatched size on BertForSequenceClassification from Transformers and multiclass problem","<python><deep-learning><pytorch><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"69138037","1","69138038","","2021-09-10 21:11:05","","1","29","<p>I would like to load a custom dataset from csv using <code>huggingfaces-transformers</code></p>
","10746597","","","","","2021-09-11 09:58:33","How to load custom dataset from CSV in Huggingfaces","<huggingface-transformers><huggingface-datasets>","1","0","","","","CC BY-SA 4.0"
"69195950","1","69336070","","2021-09-15 15:28:40","","2","79","<p>I'm trying to build the model illustrated in this picture:
<a href=""https://i.stack.imgur.com/4eiAK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4eiAK.png"" alt=""enter image description here"" /></a></p>
<p>I obtained a pre-trained BERT and respective tokenizer from HuggingFace's <code>transformers</code> in the following way:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, TFBertModel
model_name = &quot;dbmdz/bert-base-italian-xxl-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
bert = TFBertModel.from_pretrained(model_name)
</code></pre>
<p>The model will be fed a sequence of italian tweets and will need to determine if they are ironic or not.</p>
<p>I'm having problems building the initial part of the model, which takes the inputs and feeds them to the tokenizer in order to get a representation I can feed to BERT.</p>
<p>I can do it outside of the model-building context:</p>
<pre><code>my_phrase = &quot;Ciao, come va?&quot;
# an equivalent version is tokenizer(my_phrase, other parameters)
bert_input = tokenizer.encode(my_phrase, add_special_tokens=True, return_tensors='tf', max_length=110, padding='max_length', truncation=True) 
attention_mask = bert_input &gt; 0
outputs = bert(bert_input, attention_mask)['pooler_output']
</code></pre>
<p>but I'm having troubles building a model that does this. Here is the code for building such a model (the problem is in the first 4 lines ):</p>
<pre><code>def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = tokenizer(text_input, return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  
  X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(net)
  X = tf.keras.layers.Concatenate(axis=-1)([X, input_layer])
  X = tf.keras.layers.MaxPooling1D(20)(X)
  X = tf.keras.layers.SpatialDropout1D(0.4)(X)
  X = tf.keras.layers.Flatten()(X)
  X = tf.keras.layers.Dense(128, activation=&quot;relu&quot;)(X)
  X = tf.keras.layers.Dropout(0.25)(X)
  X = tf.keras.layers.Dense(2, activation='softmax')(X)

  model = tf.keras.Model(inputs=text_input, outputs = X) 
  
  return model
</code></pre>
<p>And when I call the function for creating this model I get this error:</p>
<blockquote>
<p>text input must of type <code>str</code> (single example), <code>List[str]</code> (batch or single pretokenized example) or <code>List[List[str]]</code> (batch of pretokenized examples).</p>
</blockquote>
<p>One thing I thought was that maybe I had to use the <code>tokenizer.batch_encode_plus</code> function which works with lists of strings:</p>
<pre class=""lang-py prettyprint-override""><code>class BertPreprocessingLayer(tf.keras.layers.Layer):
  def __init__(self, tokenizer, maxlength):
    super().__init__()
    self._tokenizer = tokenizer
    self._maxlength = maxlength
  
  def call(self, inputs):
    print(type(inputs))
    print(inputs)
    tokenized = tokenizer.batch_encode_plus(inputs, add_special_tokens=True, return_tensors='tf', max_length=self._maxlength, padding='max_length', truncation=True)
    return tokenized

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = BertPreprocessingLayer(tokenizer, 100)(text_input)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  # ... same as above
</code></pre>
<p>but I get this error:</p>
<blockquote>
<p>batch_text_or_text_pairs has to be a list (got &lt;class 'keras.engine.keras_tensor.KerasTensor'&gt;)</p>
</blockquote>
<p>and beside the fact I haven't found a way to convert that tensor to a list with a quick google search, it seems weird that I have to go in and out of tensorflow in this way.</p>
<p>I've also looked up on the huggingface's <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""nofollow noreferrer"">documentation</a> but there is only a single usage example, with a single phrase, and what they do is analogous at my &quot;out of model-building context&quot; example.</p>
<p>EDIT:</p>
<p>I also tried with <code>Lambda</code>s in this way:</p>
<pre><code>tf.executing_eagerly()

def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  return tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='text')
  
  encoder_inputs = tf.keras.layers.Lambda(tokenize_tensor, name='tokenize')(text_input)
  ...
  
  outputs = bert(encoder_inputs)
</code></pre>
<p>but I get the following error:</p>
<blockquote>
<p>'Tensor' object has no attribute 'numpy'</p>
</blockquote>
<p>EDIT 2:</p>
<p>I also tried the approach suggested by @mdaoust of wrapping everything in a <code>tf.py_function</code> and got this error.</p>
<pre class=""lang-py prettyprint-override""><code>def py_func_tokenize_tensor(tensor):
  return tf.py_function(tokenize_tensor, [tensor], Tout=[tf.int32, tf.int32, tf.int32])
</code></pre>
<blockquote>
<p>eager_py_func() missing 1 required positional argument: 'Tout'</p>
</blockquote>
<p>Then I defined Tout as the type of the value returned by the tokenizer:</p>
<p><code>transformers.tokenization_utils_base.BatchEncoding</code></p>
<p>and got the following error:</p>
<blockquote>
<p>Expected DataType for argument 'Tout' not &lt;class
'transformers.tokenization_utils_base.BatchEncoding'&gt;</p>
</blockquote>
<p>Finally I unpacked the value in the BatchEncoding in the following way:</p>
<pre class=""lang-py prettyprint-override""><code>def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  dictionary = tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)
  #unpacking
  input_ids = dictionary['input_ids']
  tok_type = dictionary['token_type_ids']
  attention_mask = dictionary['attention_mask']
  return input_ids, tok_type, attention_mask
</code></pre>
<p>And get an error in the line below:</p>
<pre class=""lang-py prettyprint-override""><code>...
outputs = bert(encoder_inputs)
</code></pre>
<blockquote>
<p>ValueError: Cannot take the length of shape with unknown rank.</p>
</blockquote>
","11579184","","11579184","","2021-09-26 11:26:13","2021-09-26 14:51:54","Problem with inputs when building a model with TFBertModel and AutoTokenizer from HuggingFace's transformers","<tensorflow><keras><huggingface-transformers><bert-language-model><huggingface-tokenizers>","4","0","1","","","CC BY-SA 4.0"
"69196995","1","69215166","","2021-09-15 16:47:04","","2","71","<p>I am working on using a transformer. Pipeline to get BERT embeddings to my input. using this without a pipeline i am able to get constant outputs but not with pipeline since I was not able to pass arguments to it.</p>
<p>How can I pass transformer-related arguments for my Pipeline?</p>
<pre class=""lang-py prettyprint-override""><code># These are BERT and tokenizer definitions
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

inputs = ['hello world']

# Normally I would do something like this to initialize the tokenizer and get the result with constant output
tokens = tokenizer(inputs,padding='max_length', truncation=True, max_length = 500, return_tensors=&quot;pt&quot;)
model(**tokens)[0].detach().numpy().shape


# using the pipeline 
pipeline(&quot;feature-extraction&quot;, model=model, tokenizer=tokenizer, device=0)

# or other option
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;,padding='max_length', truncation=True, max_length = 500, return_tensors=&quot;pt&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

nlp=pipeline(&quot;feature-extraction&quot;, model=model, tokenizer=tokenizer, device=0)

# to call the pipeline
nlp(&quot;hello world&quot;)
</code></pre>
<p>I have tried several ways like the options listed above but was not able to get results with constant output size. one can achieve constant output size by setting the tokenizer arguments but have no idea how to give arguments for the pipeline.</p>
<p>any idea?</p>
","4409660","","3607203","","2021-09-16 10:54:48","2021-09-16 21:06:50","Using Hugging-face transformer with arguments in pipeline","<pytorch><huggingface-transformers><bert-language-model><transformer><huggingface-tokenizers>","1","2","","","","CC BY-SA 4.0"
"69211207","1","","","2021-09-16 15:26:35","","0","51","<p>I am trying to train a model using tf2 &amp; huggingface-transformers, and start a tensorflow serving then.</p>
<p>tensorflow==2.3.1</p>
<p>transformers==4.2.1</p>
<p>My model define as:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import Model 
from tensorflow.keras.layers import *
from transformers import TFAutoModel

input_ids = Input(shape=(3000), name='INPUT_input_ids', dtype=tf.int32)
input_mask = Input(shape=(3000), name='INPUT_input_mask', dtype=tf.int32)
segment_ids = Input(shape=(3000), name='INPUT_segment_ids', dtype=tf.int32)
passage_mask = Input(shape=(10), name='INPUT_passage_mask', dtype=tf.int32)
input_ids_reshape = K.reshape(input_ids,(-1, 300))
input_mask_reshape = K.reshape(input_mask,(-1, 300))
segment_ids_reshape = K.reshape(segment_ids,(-1, 300))
transformer = TFAutoModel.from_pretrained('hfl/chinese-roberta-wwm-ext', from_pt=False)
transformer_output = transformer([input_ids_reshape, input_mask_reshape, segment_ids_reshape])[0]
......
model = Model(
    inputs  = [input_ids, input_mask, segment_ids, passage_mask], 
    outputs = [start_prob, end_prob]
)
</code></pre>
<p>I try to save model in this way:</p>
<pre><code>model.save(path)
</code></pre>
<p>but I got error</p>
<pre><code>/lib/python3.6/site-packages/transformers/modeling_tf_utils.py in input_processing(func, config, input_ids, **kwargs)
    364                     output[tensor_name] = input
    365                 else:
--&gt; 366                     output[parameter_names[i]] = input
    367             elif isinstance(input, allowed_types) or input is None:
    368                 output[parameter_names[i]] = input

IndexError: list index out of range
</code></pre>
<p>model.predict() and model.save_weights() is working.
I can get a .h5 file with model.save_weights(). But it can not use in tensorflow serving.
When I delete</p>
<pre><code>transformer = TFAutoModel.from_pretrained('hfl/chinese-roberta-wwm-ext', from_pt=False)
transformer_output = transformer([input_ids, input_mask, segment_ids])[0]
</code></pre>
<p>this two line, the model can save with model.save() and it can start a tensorflow serving.</p>
<p>How to use model.save() with huggingface-transformers?
OR How to write model with huggingface-transformers?
I just want to use transformers as a keras layer in my model.</p>
<p>I try to change .h5 file to .pb file in this way:</p>
<pre><code>def save_model_to_serving(model, export_version, export_path):
    signature = tf.compat.v1.saved_model.predict_signature_def(
        inputs={t.name: t for t in model.inputs}, 
        outputs={t.name: t for t in model.output}
    )
    export_path = os.path.join(tf.compat.as_bytes(export_path), tf.compat.as_bytes(str(export_version)))
    builder = tf.compat.v1.saved_model.Builder(export_path)
    legacy_init_op = tf.group(tf.compat.v1.tables_initializer(), name='legacy_init_op')
    builder.add_meta_graph_and_variables(
        sess=tf.compat.v1.keras.backend.get_session(),
        tags=[tf.compat.v1.saved_model.tag_constants.SERVING],
        signature_def_map = {'predict':signature},
        strip_default_attrs=True,
        legacy_init_op=legacy_init_op)
    builder.save()
</code></pre>
<p>It can save to .pb file successfully, but when I start a tensorflow serving and post, I got error as follow:</p>
<pre><code>'error': 'Tensor INPUT_input_ids:0, specified in either feed_devices or fetch_devices was not found in the Graph'
</code></pre>
<p>It seems that the default graph and my model's graph is not same.</p>
","7056451","","7056451","","2021-09-16 16:27:25","2021-09-16 16:27:25","How to save model using model.save() to .pb file with tf2 & huggingface-transformers?","<python><keras><tensorflow-serving><huggingface-transformers><tensorflow2.x>","0","1","1","","","CC BY-SA 4.0"
"68105165","1","","","2021-06-23 18:13:04","","0","56","<p>According to theory, BERT takes word embeddings and position embeddings as input. My goal is to feed the BERT model with word embeddings from a different model like word2vec or Glove.
Is there a way to feed static word embedding to BERT to get contextualized Word embedding from BERT finally?
Please let me know about any relevant links</p>
","15638074","","","","","2021-06-23 18:13:04","Input Embeddings to BERT","<word2vec><bert-language-model><huggingface-transformers><transformer>","0","2","","","","CC BY-SA 4.0"
"69154195","1","","","2021-09-12 18:32:53","","0","31","<p>I'm using HuggingFace to create a fine-tuned GPTNEO model.</p>
<p>Here is my code</p>
<pre><code>import datasets
from transformers import GPT2Tokenizer, GPTNeoForCausalLM
from datasets import load_dataset
from transformers import Trainer, TrainingArguments
import pickle
dataset = load_dataset('text',data_files={'train':['youtube_3/script.txt','youtube_3/endgame.txt','youtube_3/infinitywar.txt']})
tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')
model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B')

print(dataset)
pickle.dump(dataset,open('youtube_3/avengers.pickle','wb'))
def tokenize(bruh):
    return tokenizer(bruh['text'])
tk_dataset = dataset.map(tokenize)
ttk_dataset = tk_dataset['train']
training_args = TrainingArguments('test_trainer')
trainer = Trainer(
    model=model, args=training_args, train_dataset=ttk_dataset
)
trainer.train()
model.save_pretrained('youtube_3/avengers')

</code></pre>
<p>But whenever I do this I get this error</p>
<pre><code>ValueError: expected sequence of length 19 at dim 1 (got 5)

</code></pre>
<p>Since all the pytorch is handled in HuggingFace itself I don't know what to do.</p>
<p>Any help would be appreciated.</p>
<p>FULL TRACEBACK</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/*************/Documents/vsc/youtube_3/sadge_ai.py&quot;, line 20, in &lt;module&gt;
    trainer.train()
  File &quot;/Users/*************/miniforge3/envs/python386/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1258, in train
    for step, inputs in enumerate(epoch_iterator):
  File &quot;/Users/*************/miniforge3/envs/python386/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 521, in __next__
    data = self._next_data()
  File &quot;/Users/*************/miniforge3/envs/python386/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 561, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File &quot;/Users/*************/miniforge3/envs/python386/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 47, in fetch
    return self.collate_fn(data)
  File &quot;/Users/*************/miniforge3/envs/python386/lib/python3.8/site-packages/transformers/data/data_collator.py&quot;, line 66, in default_data_collator
    return torch_default_data_collator(features)
  File &quot;/Users/*************/miniforge3/envs/python386/lib/python3.8/site-packages/transformers/data/data_collator.py&quot;, line 107, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
ValueError: expected sequence of length 19 at dim 1 (got 5)
  0%|        

</code></pre>
","15201553","","15201553","","2021-09-12 18:41:28","2021-09-12 18:41:28","ValueError: expected sequence of length 19 at dim 1 (got 5) when using Hugging Face","<python><huggingface-transformers>","0","3","","","","CC BY-SA 4.0"
"69271347","1","","","2021-09-21 15:07:56","","0","44","<p>I am trying to run this example script: <a href=""https://github.com/hetpandya/paraphrase-datasets-pretrained-models/blob/main/examples/t5_paraphrase_model_training_example.ipynb"" rel=""nofollow noreferrer"">https://github.com/hetpandya/paraphrase-datasets-pretrained-models/blob/main/examples/t5_paraphrase_model_training_example.ipynb</a> with the german part of the dataset and the mt5 model instead of t5. While training, I get this following warning:</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:3365: FutureWarning: 
`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare
your targets.
</code></pre>
<p>Here is a short example:</p>
<pre><code>model_inputs = tokenizer(src_texts, ...)
with tokenizer.as_target_tokenizer():
    labels = tokenizer(tgt_texts, ...)
model_inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]

See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
For a more complete example, see the implementation of `prepare_seq2seq_batch`.

  warnings.warn(formatted_warning, FutureWarning)
Using Adafactor for T5
Epoch 1 of 1: 100%
1/1 [4:37:28&lt;00:00, 16648.54s/it]
Epochs 0/1. Running Loss: nan: 100%
9347/9347 [4:37:23&lt;00:00, 1.38s/it]
/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  &quot;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&quot;, UserWarning)
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
(584, nan)
</code></pre>
<p>Does this break the model? I was able to save it and generate some input which unfortunately looks like this:</p>
<pre><code>Generating outputs: 100%
1/1 [00:00&lt;00:00, 1.60it/s]
Decoding outputs: 100%
5/5 [00:01&lt;00:00, 1.65s/it]
[['&lt;extra_id_0&gt;.',
  '&lt;extra_id_0&gt;.',
  '&lt;extra_id_0&gt;',
  '&lt;extra_id_0&gt;) &lt;extra_id_36&gt; ein.',
  '&lt;extra_id_0&gt; waren']]
</code></pre>
<p>I only trained for one epoch. Maybe this is way to short and therefore I get these weird outputs?</p>
","15550823","","4685471","","2021-09-21 15:16:36","2021-09-21 15:16:36","WARNING:root:NaN or Inf found in input tensor when fine-tuning mt5 model","<python><tensorflow><machine-learning><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"69189231","1","","","2021-09-15 08:02:15","","0","51","<p>I am trying to run <code>layoutxlm</code> code , <code>https://github.com/microsoft/unilm/tree/master/layoutxlm</code></p>
<p>A latest multi-lingual NLP model. Installation is done and no issues in that, but while running code i am seeing this error .</p>
<p><code>Auto_class_factory</code> module is present inside <code>transformers</code> but its not able to import.</p>
<p>Paths information is below.</p>
<p>1)The <code>auto-factory</code> is present in <code>layoutlmft</code> virtual enviroment <code> C:\Users\Dell\anaconda3\envs\layoutlmft\Lib\sitepackages\transformers\models\auto</code></p>
<ol start=""2"">
<li>Error I am getting :</li>
</ol>
<pre><code>ImportError: **cannot import name auto_class_factory from transformers.models.auto.modeling_auto

(C:\Users\Dell\anaconda3\envs\layoutlmft\lib\site-packages\transformers\models\auto\modeling_auto.py)
</code></pre>
","8425675","","3306097","","2021-09-16 07:46:44","2021-09-16 07:46:44","ImportError: cannot import name 'auto_class_factory' from 'transformers.models.auto.modeling_auto'","<nlp><importerror><huggingface-transformers><spacy-transformers><simpletransformers>","0","1","","","","CC BY-SA 4.0"
"69276018","1","","","2021-09-21 22:03:32","","1","30","<p>The tutorial url:
<a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a></p>
<p>The pt-en tokenizer model code:</p>
<pre><code>examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)
train_examples, val_examples = examples['train'], examples['validation']

# 2. Get BertTokenizer
model_name = &quot;ted_hrlr_translate_pt_en_converter&quot;
tf.keras.utils.get_file(
    f&quot;{model_name}.zip&quot;,
    f&quot;https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip&quot;,
    cache_dir='.', cache_subdir='', extract=True
)

tokenizers = tf.saved_model.load(model_name)
en_tokenizer_items = [item for item in dir(tokenizers.en) if not item.startswith('_')]
print('En tokenizer methods: ', en_tokenizer_items)

# 3. Tokenizer examples
def tokenize_pairs(pt, en):
    pt = tokenizers.pt.tokenize(pt)
    
    # Convert from ragged to dense, padding with zeros.
    pt = pt.to_tensor()

    en = tokenizers.en.tokenize(en)
    # Convert from ragged to dense, padding with zeros.
    en = en.to_tensor()
    return pt, en


# 4. Make batches
BUFFER_SIZE = 20000
BATCH_SIZE = 64
def make_batches(ds):
  return (
      ds
      .cache()
      .shuffle(BUFFER_SIZE)
      .batch(BATCH_SIZE)
      .map(tokenize_pairs, num_parallel_calls=tf.data.experimental.AUTOTUNE)
      .prefetch(tf.data.experimental.AUTOTUNE))

train_batches = make_batches(train_examples)
val_batches = make_batches(val_examples)
</code></pre>
<p>In this line of code below, the tokenize takes a tensor of string as input:</p>
<pre><code>pt = tokenizers.pt.tokenize(pt)
</code></pre>
<p>A transorformer pretrained tokenizer usually takes a string as input rather than a tensor here. If I want to switch the tokenizers from portugues to Chinese, how can I adapt the transformers tokenizer to work with the 'make_batches' and 'tokenize_pairs' functions?</p>
<p>I simply import the transformer tokenizers but it didn't work:</p>
<pre><code>from transformers import BertTokenizer
tokenizer_en = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
tokenizer_zh = BertTokenizer.from_pretrained(&quot;bert-base-chinese&quot;)

def tokenize_pairs(zh, en):
    zh = tokenizer_zh.tokenize(zh)
    # Convert from ragged to dense, padding with zeros.
    zh = zh.to_tensor()
    en = tokenizer_en.tokenize(en)
    # Convert from ragged to dense, padding with zeros.
    en = en.to_tensor()
    return zh, en
</code></pre>
<p>This line below reports an error:</p>
<pre><code>zh = tokenizer_zh.tokenize(zh)



/Users/cong/transformer/data_zh.py:44 tokenize_pairs  *
        zh = tokenizer_zh.tokenize(zh)
    /Users/cong/.venv/tf2/lib/python3.8/site-packages/transformers/tokenization_utils.py:336 split_on_tokens  *
        if not text.strip():
    /Users/cong/.venv/tf2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:401 __getattr__
        self.__getattribute__(name)

    AttributeError: 'Tensor' object has no attribute 'strip'
</code></pre>
","3943868","","","","","2021-09-21 22:03:32","How to adapt transfomer pretrained tokenizers to work with this translation tutorial?","<tensorflow><deep-learning><nlp><huggingface-transformers><transformer>","0","0","","","","CC BY-SA 4.0"
"69239492","1","","","2021-09-19 00:41:02","","-1","24","<p>I am trying to build a pre-trained model using dialog-gpt2 <em>(Grossmend/rudialogpt3_medium_based_on_gpt2)</em> and a custom dataset. Whenever i try to run the main function I get this error:</p>
<pre><code>TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]

/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_fast.py in _batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)
    406             batch_text_or_text_pairs,
    407             add_special_tokens=add_special_tokens,
--&gt; 408             is_pretokenized=is_split_into_words,
    409         )
    410 
</code></pre>
<p>Several things need mentioning. Firstly, dataset is in encoding cp1251, which i used when reading csv file. Secondly, the code runs in google-colab, i haven't tried it on local machine.</p>
","14184516","","","","","2021-09-19 00:41:02","TextEncodeInput must be Union","<python><huggingface-transformers><huggingface-tokenizers><gpt-2><huggingface-datasets>","0","0","","","","CC BY-SA 4.0"
"69239925","1","69240001","","2021-09-19 02:52:22","","1","34","<p>I want to run this code for question answering using hugging face transformers.</p>
<pre><code>import torch
from transformers import BertForQuestionAnswering
from transformers import BertTokenizer

#Model
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

#Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

question = '''Why was the student group called &quot;the Methodists?&quot;'''

paragraph = ''' The movement which would become The United Methodist Church began in the mid-18th century within the Church of England.
            A small group of students, including John Wesley, Charles Wesley and George Whitefield, met on the Oxford University campus.
            They focused on Bible study, methodical study of scripture and living a holy life.
            Other students mocked them, saying they were the &quot;Holy Club&quot; and &quot;the Methodists&quot;, being methodical and exceptionally detailed in their Bible study, opinions and disciplined lifestyle.
            Eventually, the so-called Methodists started individual societies or classes for members of the Church of England who wanted to live a more religious life. '''
            
encoding = tokenizer.encode_plus(text=question,text_pair=paragraph)

inputs = encoding['input_ids']  #Token embeddings
sentence_embedding = encoding['token_type_ids']  #Segment embeddings
tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens

start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))

start_index = torch.argmax(start_scores)
</code></pre>
<p>but I get this error at the last line:</p>
<pre><code>Exception has occurred: TypeError
argmax(): argument 'input' (position 1) must be Tensor, not str
  File &quot;D:\bert\QuestionAnswering.py&quot;, line 33, in &lt;module&gt;
    start_index = torch.argmax(start_scores)
</code></pre>
<p>I don't know what's wrong. can anyone help me?</p>
","8853313","","","","","2021-09-19 03:38:34","TypeError in torch.argmax() when want to find the tokens with the highest `start` score","<python><pytorch><torch><huggingface-transformers><bert-language-model>","2","0","","","","CC BY-SA 4.0"
"69311785","1","","","2021-09-24 08:22:32","","-3","25","<p>I was trying to build a project for generating story by taking input as genre of story and starting text of the story, but I can't find a way to add two inputs in hugging face transformer pipeline in text generation. Pls help.</p>
","13875389","","","","","2021-09-24 08:22:32","Giving alternate text for text generation in transformer pipeline","<python><huggingface-transformers>","0","1","","2021-09-26 20:39:17","","CC BY-SA 4.0"
"68113075","1","68513871","","2021-06-24 09:26:22","","0","154","<p>I am encountering a strange issue in the <code>batch_encode_plus</code> method of the tokenizers. I have recently switched from transformer version 3.3.0 to 4.5.1. (I am creating my databunch for NER).</p>
<p>I have 2 sentences whom I need to encode, and I have a case where the sentences are already tokenized, but since both the sentences differs in length so I need to <code>pad [PAD]</code> the shorter sentence in order to have my batch of uniform lengths.</p>
<p>Here is the code below of I did with 3.3.0 version of transformers</p>
<pre><code>from transformers import AutoTokenizer

pretrained_model_name = 'distilbert-base-cased'
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, add_prefix_space=True)

sentences = [&quot;He is an uninvited guest.&quot;, &quot;The host of the party didn't sent him the invite.&quot;]

# here we have the complete sentences
encodings = tokenizer.batch_encode_plus(sentences, max_length=20, padding=True)
batch_token_ids, attention_masks = encodings[&quot;input_ids&quot;], encodings[&quot;attention_mask&quot;]
print(batch_token_ids[0])
print(tokenizer.convert_ids_to_tokens(batch_token_ids[0]))

# And the output
# [101, 1124, 1110, 1126, 8362, 1394, 5086, 1906, 3648, 119, 102, 0, 0, 0, 0]
# ['[CLS]', 'He', 'is', 'an', 'un', '##in', '##vi', '##ted', 'guest', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']

# here we have the already tokenized sentences
encodings = tokenizer.batch_encode_plus(batch_token_ids, max_length=20, padding=True, truncation=True, is_split_into_words=True, add_special_tokens=False, return_tensors=&quot;pt&quot;)

batch_token_ids, attention_masks = encodings[&quot;input_ids&quot;], encodings[&quot;attention_mask&quot;]
print(batch_token_ids[0])
print(tokenizer.convert_ids_to_tokens(batch_token_ids[0])) 

# And the output 
tensor([ 101, 1124, 1110, 1126, 8362, 1394, 5086, 1906, 3648,  119,  102, 0, 0, 0, 0])
['[CLS]', 'He', 'is', 'an', 'un', '##in', '##vi', '##ted', 'guest', '.', '[SEP]', '[PAD]', [PAD]', '[PAD]', '[PAD]']
</code></pre>
<p>But if I try to mimic the same behavior in transformer version 4.5.1, I get different output</p>
<pre><code>from transformers import AutoTokenizer
    
pretrained_model_name = 'distilbert-base-cased'
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, add_prefix_space=True)

sentences = [&quot;He is an uninvited guest.&quot;, &quot;The host of the party didn't sent him the invite.&quot;]

# here we have the complete sentences
encodings = tokenizer.batch_encode_plus(sentences, max_length=20, padding=True)
batch_token_ids, attention_masks = encodings[&quot;input_ids&quot;], encodings[&quot;attention_mask&quot;]
print(batch_token_ids[0])
print(tokenizer.convert_ids_to_tokens(batch_token_ids[0]))

# And the output
#[101, 1124, 1110, 1126, 8362, 1394, 5086, 1906, 3648, 119, 102, 0, 0, 0, 0]
#['[CLS]', 'He', 'is', 'an', 'un', '##in', '##vi', '##ted', 'guest', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']

# here we have the already tokenized sentences, Note we cannot pass the batch_token_ids 
# to the batch_encode_plus method in the newer version, so need to convert them to token first
tokens1 = tokenizer.tokenize(sentences[0], add_special_tokens=True)
tokens2 = tokenizer.tokenize(sentences[1], add_special_tokens=True)

encodings = tokenizer.batch_encode_plus([tokens1, tokens2], max_length=20, padding=True, truncation=True, is_split_into_words=True, add_special_tokens=False, return_tensors=&quot;pt&quot;)

batch_token_ids, attention_masks = encodings[&quot;input_ids&quot;], encodings[&quot;attention_mask&quot;]
print(batch_token_ids[0])
print(tokenizer.convert_ids_to_tokens(batch_token_ids[0]))

# And the output (not the desired one)
tensor([  101,  1124,  1110,  1126,  8362,   108,   108,  1107,   108,   108,
          191,  1182,   108,   108, 21359,  1181,  3648,   119,   102])
['[CLS]', 'He', 'is', 'an', 'un', '#', '#', 'in', '#', '#', 'v', '##i', '#', '#', 'te', '##d', 'guest', '.', '[SEP]']
</code></pre>
<p>Not sure how to handle this, or what I am doing wrong here.</p>
","1364646","","","","","2021-07-24 21:15:49","Problem with batch_encode_plus method of tokenizer","<python><pytorch><huggingface-transformers><huggingface-tokenizers><huggingface-datasets>","2","0","","","","CC BY-SA 4.0"
"68120730","1","","","2021-06-24 18:04:39","","0","189","<p>I am trying to fine-tune a BERT model on a dataset of sentences that has two different real-valued attributes for each sentence. For each one, there is a Valence score and an Arousal score, with real values between 0 and 1. I need to add a classification layer on top of BERTbase, and that layer must have two outputs, and take them both into account for calculating the loss and performing the backpropagation.</p>
<p>Since I am inexperienced, it was suggested to be that I use the Trainer classes / API for this task. I am using BertForSequenceClassification, and I was told that by setting the num_labels parameter to 2, giving the model an input with two feature columns, and overriding the compute_loss method of the Trainer class to force the loss to be computed with both the outputs.</p>
<p>The thing is, when I read the BERTForSequenceClassification documentation, it says that &quot;If config.num_labels == 1 a regression loss is computed (Mean-Square loss), If config.num_labels &gt; 1 a classification loss is computed (Cross-Entropy).&quot;. On the other hand, the Trainer documentation has the compute_loss function, which I override. From inspecting the code, I think that by overriding compute_loss and defining it as desired, the &quot;num_labels&quot; parameter on BertForSequenceClassification actually becomes irrelevant and the loss is entirely computed on the redefined method.</p>
<p>I am still afraid that something might be wrong in this adaptation for a 2-output regression model, either with the loss, with the gradient backpropagation or somewhere else. I will leave my code below, and I am incredibly grateful for any help or feedback in using HuggingFace's Trainer for a 2-output regression BERT model.</p>
<pre><code>model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', num_labels=2)
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case = True)

class MyDataset(Dataset):
    def __init__(self, filename,maxlen):
        nas = ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '', '#NA', 'NaN', '-NaN', '']
        data = pd.read_csv(filename,na_values=nas,keep_default_na=False)
        self.maxlen = maxlen
        self.texts = data[&quot;Word/Sentence&quot;].tolist()
        self.labels1 = data['Valence'].tolist()
        self.labels2 = data['Arousal'].tolist()
    def __getitem__(self, idx):
        item = { }
        aux = tokenizer(self.texts[idx], max_length=self.maxlen, truncation=True, padding=False)
        item['input_ids'] = torch.tensor(aux['input_ids'])
        item['attention_mask'] = torch.tensor(aux['attention_mask'])
        item['labels'] = torch.tensor( [ self.labels1[idx], self.labels2[idx] ] )
        return item
    def __len__(self):
        return len(self.texts)

class MyTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop(&quot;labels&quot;)
        outputs = model(**inputs)
        predictions = outputs[0]
        predictions = torch.sigmoid(predictions)
        loss = torch.nn.MSELoss()
        loss = loss(predictions.view(-1), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

train_dataset = MyDataset(&quot;fold1.csv&quot;,250)
val_dataset = MyDataset(&quot;fold2.csv&quot;,250)
data_collator = DataCollatorWithPadding(tokenizer, padding = &quot;longest&quot;, max_length = 250)

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=4,              # total number of training epochs
    per_device_train_batch_size=16,   # batch size per device during training
    per_device_eval_batch_size=16,    # batch size for evaluation
    warmup_steps=50,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
    save_steps=150,
    group_by_length = True,
    evaluation_strategy = &quot;epoch&quot;
)

trainer = MyTrainer(
    model=model,                         # the instantiated :hugging_face: Transformers model to be traine
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,            # evaluation dataset
    data_collator=data_collator,           
)
</code></pre>
","7294202","","","","","2021-06-24 18:04:39","HuggingFace Tranfsormers BERTForSequenceClassification with Trainer: How to do multi-output regression?","<python><pytorch><bert-language-model><huggingface-transformers><transformer>","0","7","","","","CC BY-SA 4.0"
"69098628","1","","","2021-09-08 07:22:01","","2","41","<p>I am training a sparse <code>multi-label text classification</code> problem using <code>Hugging Face</code> models which is one part of <code>SMART REPLY System</code>. The task which I am doing is mentioned below:</p>
<p>I classify <code>Customer Utterances</code> as input to the model and classify to which <code>Agent Response</code> clusters it belongs to. I have <code>60</code> clusters and <code>Customer Utterances</code> can map to one or more clusters.</p>
<p><strong>Input to Model</strong></p>
<pre><code>Input                             Output

My account is blocked             [0,0,0,1,1,0....0,0,0,0,0]
</code></pre>
<p>The Output is Encoding Vector for Cluster labels. In the above example the customer query maps into  <code>cluster 4</code> and <code>cluster 5</code> of agent responses.</p>
<p><strong>Problem:</strong></p>
<p>The model always predict the cluster numbers which are very frequent. It doesn't take the rare clusters.</p>
<p>Only few 1's are present at a time in output labels and the rest are 0.</p>
<p><strong>Code:</strong></p>
<pre><code>#Dividing the params into those which needs to be updated and rest
param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [
    {
        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
        'weight_decay_rate': 0.01
    },
    {
        'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
        'weight_decay_rate': 0.0
    }
]

optimizer = BertAdam(optimizer_grouped_parameters, lr =0.05, warmup = .1)
</code></pre>
<p><strong>Model Training</strong></p>
<pre><code>#Empty the GPU memory as it might be memory and CPU intensive while training
torch.cuda.empty_cache()
#Number of times the whole dataset will run through the network and model is fine-tuned
epochs = 10
epoch_count = 1
#Iterate over number of epochs
for _ in trange(epochs, desc = &quot;Epoch&quot;):
    #Switch model to train phase where it will update gradients
    model.train()
    #Initaite train and validation loss, number of rows passed and number of batches passed
    tr_loss = 0
    nb_tr_examples, nb_tr_steps = 0, 0
    val_loss = 0
    nb_val_examples, nb_val_steps = 0, 0
   
    #Iterate over batches within the same epoch
    for batch in tqdm(train_dataloader):
        #Shift the batch to GPU for computation
        #pdb.set_trace()
        batch = tuple(t.to(device) for t in batch)
        #Load the input ids and masks from the batch
        b_input_ids, b_input_mask, b_labels = batch
        #Initiate gradients to 0 as they tend to add up
        optimizer.zero_grad()
        #Forward pass the input data
        logits = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)
        #We will be using the Binary Cross entropy loss with added sigmoid function after that in BCEWithLogitsLoss
        loss_func = BCEWithLogitsLoss()
        #Calculate the loss between multilabel predicted outputs and actuals
        loss = loss_func(logits, b_labels.type_as(logits))
        
        #Backpropogate the loss and calculate the gradients
        loss.backward()
        #Update the weights with the calculated gradients
        optimizer.step()
        #Add the loss of the batch to the final loss, number of rows and batches
        tr_loss += loss.item()
        nb_tr_examples += b_input_ids.size(0)
        nb_tr_steps += 1
    #Print the current training loss 
    print(&quot;Train Loss: {}&quot;.format(tr_loss/nb_tr_examples))
    
    # Save the trained model after each epoch.
#     pickle.dump(model, open(&quot;conv_bert_model_&quot;+str(epoch_count)+&quot;.pkl&quot;, &quot;wb&quot;))
    epoch_count=epoch_count+1
</code></pre>
<p>I am using this loss function currently:</p>
<pre><code>loss_func = BCEWithLogitsLoss()
#Calculate the loss between multilabel predicted outputs and actuals
loss = loss_func(logits, b_labels.type_as(logits))
</code></pre>
<p>Is there any way to improve models output.( Recall and Precision) by using different loss function?</p>
<p>How we tackle the cluster imbalance problem in Hugging face models in case of MULTI LABLES classification .</p>
","9907733","","6664872","","2021-09-17 21:09:39","2021-09-17 21:09:39","Which loss function to use for training sparse multi-label text classification problem and class skewness/imbalance","<pytorch><loss-function><huggingface-transformers><multilabel-classification><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"69126923","1","","","2021-09-10 03:30:56","","0","55","<p>i find a answer of training model from scratch in this question:
<a href=""https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp"">How to train BERT from scratch on a new domain for both MLM and NSP?</a></p>
<p>one answer use Trainer and TrainingArguments like this:</p>
<pre><code>from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir= &quot;/path/to/output/dir/for/training/arguments&quot;
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_gpu_train_batch_size= 16,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()
trainer.save_model(&quot;path/to/your/model&quot;)
</code></pre>
<p>but huggingface official doc <a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">Fine-tuning a pretrained model
</a> also use Trainer and TrainingArguments in the same way to finetune .
so when I use Trainer and TrainingArguments to train model, Do I train model from scratch or just finetune?</p>
","8300481","","","","","2021-09-10 03:30:56","how to train a bert model from scratch with huggingface?","<huggingface-transformers><bert-language-model><transformer><fine-tune>","0","4","","","","CC BY-SA 4.0"
"69315295","1","","","2021-09-24 12:47:21","","-1","16","<p>I am going to fine-tune the BERT model using HuggingFace Transformers. When I was trying to run this line:</p>
<pre><code>bert = TFAutoModel.from_pretrained('bert-base-cased')
</code></pre>
<p>I got this error :</p>
<pre><code>RecursionError                            Traceback (most recent call last)
/tmp/ipykernel_1850345/304243349.py in &lt;module&gt;
      5 from transformers import AutoModel
      6 #bert = AutoModel.from_pretrained('bert-base-cased')
----&gt; 7 bert = TFAutoModel.from_pretrained('bert-base-uncased')
      8 
      9 # we can view the model using the summary method

~/anaconda3/envs/tf2/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    385         )
    386 
--&gt; 387 
    388 def insert_head_doc(docstring, head_doc=&quot;&quot;):
    389     if len(head_doc) &gt; 0:

~/anaconda3/envs/tf2/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py in _get_model_class(config, model_mapping)
    334         return supported_models
    335 
--&gt; 336     name_to_model = {model.__name__: model for model in supported_models}
    337     architectures = getattr(config, &quot;architectures&quot;, [])
    338     for arch in architectures:

~/anaconda3/envs/tf2/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py in __getitem__(self, key)

~/anaconda3/envs/tf2/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py in _load_attr_from_module(self, model_type, attr)

~/anaconda3/envs/tf2/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py in getattribute_from_module(module, attr)

... last 1 frames repeated, from the frame below ...

~/anaconda3/envs/tf2/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py in getattribute_from_module(module, attr)

RecursionError: maximum recursion depth exceeded in comparison
</code></pre>
<p>I increased the maximum recursion amount in sys, but the error still is.
How can I solve this?</p>
","312140","","312140","","2021-09-25 18:28:04","2021-09-25 18:28:04","Transformers RecursionError: Maximum recursion depth exceeded in comparison","<nlp><huggingface-transformers><bert-language-model>","0","5","","","","CC BY-SA 4.0"
"69304395","1","","","2021-09-23 17:18:54","","0","33","<p>My am trying to understand the differences between Pipeline based approach and Direct Model Based approach in Huggingface Transformers. Working on Question and Answering based NLP problem with <em>distilbert-base-cased-distilled-squad</em> pretrained model.</p>
<p>I understand that <strong>Pipeline</strong> offers more easy approach to output the inference on a task, And <strong>direct model based</strong> offers provides more flexibility and control over the parameters that is provided. My Question is,</p>
<ol>
<li><strong>What parameters</strong> specifically provides direct model based approach better control over the pipeline based approach.</li>
<li><strong>Where would it be right</strong> to use each approach?</li>
</ol>
<p>Planning on using the Pipeline based approach and would like to know the bottlenecks of the approach.</p>
","13912530","","1937197","","2021-09-23 18:39:52","2021-09-23 18:39:52","What is the difference between using HuggingFace ""pipeline"" and ""direct model""?","<nlp><pytorch><huggingface-transformers><question-answering><squad>","0","1","","2021-09-24 16:06:10","","CC BY-SA 4.0"
"69319644","1","","","2021-09-24 18:27:28","","1","43","<p>I am trying to use a text generation model using the transformers library. I am specifically following a tutorial from <a href=""https://huggingface.co/blog/how-to-generate"" rel=""nofollow noreferrer"">HuggingFace</a>, but I am running into a memory issue.</p>
<p><strong>The Problem:</strong></p>
<p>The issue arises when downloading the model:</p>
<pre><code>model = TFGPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;, pad_token_id=tokenizer.eos_token_id)
</code></pre>
<p>Which results in the following error:</p>
<pre><code>ResourceExhaustedError: failed to allocate memory [Op:AddV2]
</code></pre>
<p><strong>What I tried:</strong></p>
<p>I ran this using two databricks clusters (One more CPU, another more GPU). To make sure the GPU was available, i used the following command:</p>
<pre><code>tf.test.is_gpu_available()
</code></pre>
<p>Output:</p>
<pre><code>True
</code></pre>
<p>To check the NVIDIA-SMI, I ran the following:</p>
<pre><code>!nvidia-smi
</code></pre>
<p>Output:</p>
<pre><code>Fri Sep 24 18:11:37 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   39C    P0   172W / 300W |  16140MiB / 16160MiB |     73%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
</code></pre>
<p>In addition to checking the memory, I also tried a few other 'lighter' models instead of &quot;gpt2&quot; such as &quot;distilgpt2&quot;.</p>
<p><strong>What I am asking:</strong></p>
<p>I understand that it is running out of memory. What I do not know is what is why or what to do next.</p>
<p>I do not know enough about memory, GPUs, and CPUs as they relate to deep learning models. Any guidance on what I can do to fix this, or what I can check next would be appreciated!</p>
<p>Thank you!</p>
","10516272","","","","","2021-09-25 10:54:31","Transformers: ResourceExhaustedError: failed to allocate memory [Op:AddV2]","<python-3.x><tensorflow><gpu><out-of-memory><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"69208280","1","","","2021-09-16 12:17:24","","0","23","<p>I am working on Abstract Summarization and choose Google-Pegasus-xsum model for it. It is working find on my local setup with Flask server. But when I tried it to serve it on gunicorn server, it never loads the model and goes into infinite loading and server never starts.</p>
<p>Here is my code:</p>
<pre><code>from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration
def load_model():
    print(&quot;Loading Model&quot;)
    model = TFPegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')
    tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')
</code></pre>
<p>on starting the server, it calls the load_model but the model never loads, it print loading model, and then again after sometime server restarts and never goes after loading model.</p>
<p>Can anyone suggest what is the issue?</p>
","11152653","","","","","2021-09-16 12:17:24","abstract summarization using transformers model not working with Gunicorn server","<python><tensorflow><flask><gunicorn><huggingface-transformers>","0","5","","","","CC BY-SA 4.0"
"69208695","1","","","2021-09-16 12:45:11","","0","35","<p>I am trying to load simple transformers NER model from my local path where I downloaded the Bert base cased model, I am not able to figure out why it is giving tokenizer error.
<img src=""https://i.stack.imgur.com/hkfJa.png"" alt=""Here is my line of code and problem please help me to find the solution"" /></p>
","16927928","","6664872","","2021-09-16 16:26:31","2021-09-16 16:26:31","OSError: Can't load tokenizer for 'path/bert-base-cased'","<python><huggingface-transformers><bert-language-model>","0","3","","","","CC BY-SA 4.0"
"69223520","1","69299182","","2021-09-17 12:34:47","","1","62","<p>Let's say that I want to include <code>distilbert</code> <a href=""https://huggingface.co/distilbert-base-uncased"" rel=""nofollow noreferrer"">https://huggingface.co/distilbert-base-uncased</a> from Hugging Face into spaCy 3.0 pipeline. I think that this is possible and I found some code on how to convert this model for spaCy 2.0 but it doesn't work in v3.0. What I really want is to load this model using something like this</p>
<pre><code>nlp = spacy.load('path_to_distilbert')
</code></pre>
<p>Is it even possible and could you please provide the exact steps to do that.</p>
","8895744","","","","","2021-09-23 11:31:33","How to use Hugging Face transfomers with spaCy 3.0","<spacy><huggingface-transformers><spacy-3><spacy-transformers>","2","0","1","","","CC BY-SA 4.0"
"69237163","1","","","2021-09-18 17:55:54","","-1","28","<p>I use the run_squad.py script to fine-tune QA models like BERT and RoBERTa. However, I don't have much data, so I want to use cross-validation.
Is there any way to use cross-validation with the script?</p>
<p>I really appreciate any help you can provide.</p>
","16090334","","","","","2021-09-18 17:55:54","Cross validation to QA models","<cross-validation><huggingface-transformers><bert-language-model><question-answering><roberta-language-model>","0","2","","","","CC BY-SA 4.0"
"69296379","1","","","2021-09-23 08:17:59","","0","61","<p>I would like to finetune <code>facebook/mbart-large-cc25</code> on my data using pre-training tasks, in particular Masked Language Modeling (MLM).</p>
<p>How can I do that in HuggingFace?</p>
<p>Edit: rewrote the question for the sake of clarity</p>
","13418745","","13418745","","2021-09-23 09:50:22","2021-09-23 09:50:22","Finetune mBART on pre-train tasks using HuggingFace","<python><nlp><huggingface-transformers><pre-trained-model><fine-tune>","1","2","","","","CC BY-SA 4.0"
"69324085","1","","","2021-09-25 07:37:05","","0","24","<p>I'm trying to run the fine-tuning example from: <a href=""https://huggingface.co/transformers/custom_datasets.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/custom_datasets.html</a>.</p>
<p>I'm using Pycharm,  and decreased the batch size to 1 in <code>training_args</code>:</p>
<pre><code>per_device_train_batch_size=1,  # batch size per device during training
per_device_eval_batch_size=1,   # batch size for evaluation
</code></pre>
<p>However, I'm getting the following error message:</p>
<pre><code>line 585, in train self.args.eval_steps &gt; 0
TypeError: '&gt;' not supported between instances of 'NoneType' and 'int' 
</code></pre>
<p><img src=""https://i.stack.imgur.com/FZ8gt.png"" alt=""error"" /></p>
","16424098","","2423278","","2021-09-25 07:59:54","2021-09-25 07:59:54","Tensorflow: TypeError: '>' not supported between instances of 'NoneType' and 'int'","<tensorflow><machine-learning><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"69300098","1","","","2021-09-23 12:33:43","","1","20","<p>According to the documentation of Huggingface's <code>transformers</code> library, <a href=""https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.beam_search"" rel=""nofollow noreferrer""><code>beam_search()</code></a> and <a href=""https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.group_beam_search"" rel=""nofollow noreferrer""><code>group_beam_search()</code></a> are two methods to generate outputs from Encoder-Decoder models.<br />
Both take the exact same input arguments, including batched sequence tensors, and generate outputs via beam search.</p>
<p>The question is therefore: what is the difference between the two methods?</p>
","3607203","","3607203","","2021-09-24 09:21:37","2021-09-24 09:21:37","What is the difference between Huggingface's `beam_search()`and `group_beam_search()`","<huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"69328541","1","","","2021-09-25 17:30:22","","-2","63","<p>Earlier I posted <a href=""https://stackoverflow.com/questions/69286889/transformers-and-bert-downloading-to-your-local-machine#69287116"">this</a> and the answer suggested to check the version of packages. They are as below</p>
<pre><code>torch version - 1.4.0
transformer version - 4.0.0
python version is 3.6.8
</code></pre>
<p>is the package version the problem why I am getting the error <code>OSErrr: unable to load weights from pytorch checkpoint file for bert-base-uncased2/ at bert-base-uncased/pytorch_model.bin If you tried to load a pytroch model from a TF 2 checkpoint, please set from_tf=True</code></p>
<p>I get that error when I run <code>model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)</code></p>
<p>I cannot update these packages due to my workplace restrictions and would like to know if there is any work around</p>
<p>#update1</p>
<p>full error as below</p>
<pre><code>    ---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/usr/local/anaconda/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    950             try:
--&gt; 951                 state_dict = torch.load(resolved_archive_file, map_location=&quot;cpu&quot;)
    952             except Exception:

/usr/local/anaconda/lib/python3.6/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)
    528                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
--&gt; 529         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
    530 

/usr/local/anaconda/lib/python3.6/site-packages/torch/serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)
    708         assert key in deserialized_objects
--&gt; 709         deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)
    710         if offset is not None:

RuntimeError: unexpected EOF, expected 1735661 more bytes. The file might be corrupted.

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
&lt;ipython-input-14-88bcb0dd5d54&gt; in &lt;module&gt;
      5 
      6 tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased2/&quot;)
----&gt; 7 model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased2/&quot;)
      8 

/usr/local/anaconda/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    952             except Exception:
    953                 raise OSError(
--&gt; 954                     f&quot;Unable to load weights from pytorch checkpoint file for '{pretrained_model_name_or_path}' &quot;
    955                     f&quot;at '{resolved_archive_file}'&quot;
    956                     &quot;If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. &quot;

OSError: Unable to load weights from pytorch checkpoint file for 'bert-base-uncased2/' at 'bert-base-uncased2/pytorch_model.bin'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. 
</code></pre>
<p>files that are in the above folder <code>bert-base-uncased2</code>:</p>
<p><a href=""https://i.stack.imgur.com/lx3du.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lx3du.png"" alt=""enter image description here"" /></a></p>
","2543622","","2543622","","2021-09-26 21:48:03","2021-09-26 21:48:03","transformers and BERT downloading to your local machine - package version","<python><pytorch><huggingface-transformers><bert-language-model>","0","7","","","","CC BY-SA 4.0"
"69187967","1","","","2021-09-15 06:20:15","","2","39","<p>I want to further improve the inference time from BERT.
Here is the code below:</p>
<pre class=""lang-py prettyprint-override""><code>for sentence in list(data_dict.values()):
    tokens = {'input_ids': [], 'attention_mask': []}
    new_tokens = tokenizer.encode_plus(sentence, max_length=512,
                                        truncation=True, padding='max_length',
                                        return_tensors='pt',
                                        return_attention_mask=True)
    tokens['input_ids'].append(new_tokens['input_ids'][0])
    tokens['attention_mask'].append(new_tokens['attention_mask'][0])

    # reformat list of tensors into single tensor
    tokens['input_ids'] = torch.stack(tokens['input_ids'])
    tokens['attention_mask'] = torch.stack(tokens['attention_mask'])

    outputs = model(**tokens)
    embeddings = outputs[0]
</code></pre>
<p>Is there a way to provide batches (like in training) instead of the whole dataset?</p>
","16915654","","3607203","","2021-09-15 12:27:54","2021-09-22 09:47:57","Reduce inference time for BERT","<pytorch><huggingface-transformers><pytorch-dataloader>","1","3","","","","CC BY-SA 4.0"
"69274391","1","69285968","","2021-09-21 19:12:41","","3","45","<p>I'm writing a inference script for already trained NER model, but I have trouble with converting encoded tokens (their ids) into original words.</p>
<pre class=""lang-py prettyprint-override""><code># example input
df = pd.DataFrame({'_id': [1], 'body': ['Amazon and Tesla are currently the best picks out there!']})

# calling method that handles inference:
ner_model = NER()
ner_model.recognize_from_df(df, 'body')

# here is only part of larger NER class that handles the inference:
def recognize_from_df(self, df: pd.DataFrame, input_col: str):
    predictions = []
    df = df[['_id', input_col]].copy()
    dataset = Dataset.from_pandas(df)
    # tokenization, padding, truncation:
    encoded_dataset = dataset.map(lambda examples: self.bert_tokenizer(examples[input_col], 
                                      padding='max_length', truncation=True, max_length=512), batched=True)
    encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'], device=device)
    dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=32)
    encoded_dataset_ids = encoded_dataset['_id']

    for batch in dataloader:
        output = self.model(**batch)
        # decoding predictions and tokens
        for i in range(batch['input_ids'].shape[0]):
            tags = [self.unique_labels[label_id] for label_id in output[i]]
            tokens = [t for t in self.bert_tokenizer.convert_ids_to_tokens(batch['input_ids'][i]) if t != '[PAD]']
        ...
</code></pre>
<p>The results are close to what I need:</p>
<pre class=""lang-py prettyprint-override""><code># tokens:
['[CLS]', 'am', '##az', '##on', 'and', 'te', '##sla', 'are', 'currently', 'the', 'best', 'picks', 'out', 'there', ...]
# tags:
['X', 'B-COMPANY', 'X', 'X', 'O', 'B-COMPANY', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...]
</code></pre>
<p><strong>How to combine <code>'am', '##az', '##on'</code> and <code>'B-COMPANY', 'X', 'X'</code> into one token/tag?</strong> I know that there is a method called <code>convert_tokens_to_string</code> in Tokenizer, but it returns just one big string, which is hard to map to tag.</p>
<p>Regards</p>
","10874791","","","","","2021-09-22 14:13:09","How to convert tokenized words back to the original ones after inference?","<python><pytorch><huggingface-transformers><huggingface-tokenizers><huggingface-datasets>","1","1","","","","CC BY-SA 4.0"
"69332269","1","","","2021-09-26 06:06:21","","1","32","<p>I need to make a for loop for running text summarization models as they have a maximum input limit for text summarization using <code>huggingface transformers</code>.</p>
<p>To execute the for loop and get its range, I need to pass tokenized input to the model and prevent it from tokenizing again inside the pipeline.</p>
<p>here is the code snippet:</p>
<pre class=""lang-py prettyprint-override""><code>summarizer = transformers.pipeline(&quot;summarization&quot;, model = 't5-small', tokenizer = 't5-small')

tokenized_text = summarizer.tokenizer(text)

</code></pre>
<p>I need to iterate over this tokenized_text.</p>
<p>If I pass slices of <code>tokenizer_text</code> into <code>summarizer</code> , it will get tokenizer again. My aim is to prevent that from happening the second time.</p>
","13357292","","","","","2021-09-26 06:06:21","How to use custom Tokenizer in Hugging Face pretrained model for text summarization?","<machine-learning><nlp><huggingface-transformers><summarization><huggingface-tokenizers>","0","4","","","","CC BY-SA 4.0"
"69087044","1","","","2021-09-07 11:02:19","","1","69","<p>I am fine tuning a BERT model for a multiclass classification task. My problem is that I don't know how to add &quot;early stopping&quot; to those Trainer instances. Any ideas?</p>
","10604508","","6117017","","2021-09-07 11:03:12","2021-09-08 08:12:43","Early stopping in Bert Trainer instances","<python><neural-network><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"69249187","1","69254313","","2021-09-20 04:44:25","","2","51","<p>Doing things on Google Colab.</p>
<ul>
<li>transformers: 4.10.2</li>
<li>pytorch-lightning: 1.2.7</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.utils.data import DataLoader
from transformers import BertJapaneseTokenizer, BertForSequenceClassification
import pytorch_lightning as pl

dataset_for_loader = [
    {'data':torch.tensor([0,1]), 'labels':torch.tensor(0)},
    {'data':torch.tensor([2,3]), 'labels':torch.tensor(1)},
    {'data':torch.tensor([4,5]), 'labels':torch.tensor(2)},
    {'data':torch.tensor([6,7]), 'labels':torch.tensor(3)},
]
loader = DataLoader(dataset_for_loader, batch_size=2)

for idx, batch in enumerate(loader):
    print(f'# batch {idx}')
    print(batch)

category_list = [
    'dokujo-tsushin',
    'it-life-hack',
    'kaden-channel',
    'livedoor-homme',
    'movie-enter',
    'peachy',
    'smax',
    'sports-watch',
    'topic-news'
]

tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)

max_length = 128
dataset_for_loader = []
for label, category in enumerate(tqdm(category_list)):
    # file ./text has lots of articles, categorized by category
    # and they are just plain texts, whose content begins from forth line
    for file in glob.glob(f'./text/{category}/{category}*'):
        lines = open(file).read().splitlines()
        text = '\n'.join(lines[3:])
        encoding = tokenizer(
            text,
            max_length=max_length, 
            padding='max_length',
            truncation=True
        )
        encoding['labels'] = label
        encoding = { k: torch.tensor(v) for k, v in encoding.items() }
        dataset_for_loader.append(encoding)

SEED=lambda:0.0

# random.shuffle(dataset_for_loader) # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«
random.shuffle(dataset_for_loader,SEED)
n = len(dataset_for_loader)
n_train = int(0.6*n)
n_val = int(0.2*n)
dataset_train = dataset_for_loader[:n_train]
dataset_val = dataset_for_loader[n_train:n_train+n_val]
dataset_test = dataset_for_loader[n_train+n_val:]

dataloader_train = DataLoader(
    dataset_train, batch_size=32, shuffle=True    
) 
dataloader_val = DataLoader(dataset_val, batch_size=256)
dataloader_test = DataLoader(dataset_test, batch_size=256)

class BertForSequenceClassification_pl(pl.LightningModule):
    def __init__(self, model_name, num_labels, lr):
        super().__init__()
        self.save_hyperparameters()
        self.bert_sc = BertForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels
        )

    def training_step(self, batch, batch_idx):
        output = self.bert_sc(**batch)
        loss = output.loss
        self.log('train_loss', loss)
        return loss

    def validation_step(self, batch, batch_idx):
        output = self.bert_sc(**batch)
        val_loss = output.loss
        self.log('val_loss', val_loss)

    def test_step(self, batch, batch_idx):
        labels = batch.pop('labels')
        output = self.bert_sc(**batch)
        labels_predicted = output.logits.argmax(-1)
        num_correct = ( labels_predicted == labels ).sum().item()
        accuracy = num_correct/labels.size(0)
        self.log('accuracy', accuracy)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)

checkpoint = pl.callbacks.ModelCheckpoint(
    monitor='val_loss',
    mode='min',
    save_top_k=1,
    save_weights_only=True,
    dirpath='model/',
)
trainer = pl.Trainer(
    gpus=1,
    max_epochs=10,
    callbacks = [checkpoint]
)

model = BertForSequenceClassification_pl(
    MODEL_NAME, num_labels=9, lr=1e-5
)

### (a) ###

# I think this is where I am doing fine-tuning
trainer.fit(model, dataloader_train, dataloader_val)

# this is to score after fine-tuning
test = trainer.test(test_dataloaders=dataloader_test)
print(f'Accuracy: {test[0][&quot;accuracy&quot;]:.2f}')
</code></pre>
<p>But I am not really sure how to do a test before fine-tuning, in order to compare two models before and after fine-tuning, in order to show how effective fine-tuning is.</p>
<p>Inserting the following two lines to <code>### (a) ###</code>:</p>
<pre class=""lang-py prettyprint-override""><code>test = trainer.test(test_dataloaders=dataloader_test)
print(f'Accuracy: {test[0][&quot;accuracy&quot;]:.2f}')
</code></pre>
<p>I got this result:</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-13-c8b2c67f2d5c&gt; in &lt;module&gt;()
      9 
     10 # 6-19
---&gt; 11 test = trainer.test(test_dataloaders=dataloader_test)
     12 print(f'Accuracy: {test[0][&quot;accuracy&quot;]:.2f}')
     13 

/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in test(self, model, test_dataloaders, ckpt_path, verbose, datamodule)
    896         self.verbose_test = verbose
    897 
--&gt; 898         self._set_running_stage(RunningStage.TESTING, model or self.lightning_module)
    899 
    900         # If you supply a datamodule you can't supply train_dataloader or val_dataloaders

/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _set_running_stage(self, stage, model_ref)
    563         the trainer and the model
    564         &quot;&quot;&quot;
--&gt; 565         model_ref.running_stage = stage
    566         self._running_stage = stage
    567 

AttributeError: 'NoneType' object has no attribute 'running_stage'
</code></pre>
<p>I noticed that <a href=""https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#fit"" rel=""nofollow noreferrer""><code>Trainer.fit()</code> can take <code>None</code> as arguments other than <code>model</code></a>, so I tried this:</p>
<pre class=""lang-py prettyprint-override""><code>trainer.fit(model)
test=trainer.test(test_dataloaders=dataloader_test)
print(f'Accuracy: {test[0][&quot;accuracy&quot;]:.2f}')
</code></pre>
<p>The result:</p>
<pre class=""lang-py prettyprint-override""><code>MisconfigurationException: No `train_dataloader()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.
</code></pre>
<p>Thanks.</p>
","15283025","","15283025","","2021-09-20 05:21:17","2021-09-20 13:25:30","How to test a model before fine-tuning in Pytorch Lightning?","<huggingface-transformers><pytorch-lightning>","1","0","","","","CC BY-SA 4.0"
"69293446","1","","","2021-09-23 02:51:52","","0","28","<p>I use hugging face's pretrained model, bert, to help me get the meaning of sentence pooling(which means tokenize the sentence and get the average vector of all embedding words). My codes are as follows. I want to get the word which pooling vector refers to.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertModel, BertTokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
# Load the model
model = BertModel.from_pretrained(model_name)
# input sentence
input_text = &quot;Here is some text to encode&quot;
# from tokenizer to token_id
input_ids = tokenizer.encode(input_text, add_special_tokens=True)
# input_ids: [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]
input_ids = torch.tensor([input_ids])
# get the tensors
with torch.no_grad():
    last_hidden_states = model(input_ids)[0] # Models outputs are now tuples
# sentence pooling
last_hidden_states = last_hidden_states.mean(1)
print(last_hidden_states)
# last_hidden_states.shape = [1,768]
</code></pre>
<p>After this, I want to get the word of this encode vector([1,768]).<br />
Theoretically, I should use this <code>embedding vecter @ embedding_matrix(size is[ dictionary_dimention ,embedding_dimention])</code><br />
And then use the result of above matrix to be the index of the dictionary.<br />
How could I get the embedding_matrix in embedding layers of hugging face, Please.</p>
","16849546","","16849546","","2021-09-23 02:58:07","2021-09-23 02:58:07","How to get the word of a embedding vector from the pretrained model of hugging face?","<python><deep-learning><nlp><huggingface-transformers><huggingface-tokenizers>","0","1","","","","CC BY-SA 4.0"
"69364068","1","","","2021-09-28 14:57:06","","0","32","<p>I created a <code>tf.keras model</code> that has <strong>BERT</strong> and I want to train and save it for further use.
Loading this model is a big issue cause I keep getting error: <code>ValueError: The two structures don't have the same nested structure.</code></p>
<p>I simplified the model a lot, to see where is the problem exactly. The code is pretty simple:</p>
<pre><code>bert = TFBertModel.from_pretrained(&quot;bert-base-german-cased&quot;)

model_name = &quot;Model&quot;
txt12_input_ids = tf.keras.layers.Input(shape=(max_length,),  name='txt12_input_ids', dtype='int32')
txt12_mask      = tf.keras.layers.Input(shape=(max_length,),  name='txt12_mask', dtype='int32')
txt12_outputs = bert(txt12_input_ids, txt12_mask).pooler_output

model_K = tf.keras.Model(inputs=(txt12_input_ids,  txt12_mask), outputs=txt12_outputs, name=model_name)
model_K.compile(optimizer=Adam(1e-5), loss=&quot;binary_crossentropy&quot;, metrics=&quot;accuracy&quot;)


model_K.save(dir_path+'Prob')
model_2 = tf.keras.models.load_model(dir_path+'Prob')
</code></pre>
<p><em>Some notes before you start replying:</em></p>
<ol>
<li><p>I did specified <code>dtype</code>.</p>
</li>
<li><p>No, I don't want to save just weights.</p>
</li>
<li><p>I tried to use <code>tf.keras.models.save_model(model_K, dir_path+'Prob')</code> instead and it gives the same error.</p>
</li>
</ol>
<p>And the last thing, I work with <code>tf version: 2.6.0</code>. Does anyone knows how to solve it?</p>
<p>Full error message:</p>
<pre><code>ValueError: The two structures don't have the same nested structure.

First structure: type=tuple str=(({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}, None, None, None, None, None, None, None, None, False), {})

Second structure: type=tuple str=((TensorSpec(shape=(None, 120), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 120), dtype=tf.int32, name='attention_mask'), None, None, None, None, None, None, None, False), {})

More specifically: Substructure &quot;type=dict str={'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}&quot; is a sequence, while substructure &quot;type=TensorSpec str=TensorSpec(shape=(None, 120), dtype=tf.int32, name='input_ids')&quot; is not
Entire first structure:
(({'input_ids': .}, ., ., ., ., ., ., ., ., .), {})
Entire second structure:
((., ., ., ., ., ., ., ., ., .), {})
</code></pre>
","6714820","","6714820","","2021-09-29 14:37:29","2021-09-29 14:37:29","Loading tf.keras model, ValueError: The two structures don't have the same nested structure","<python><tensorflow><keras><huggingface-transformers><bert-language-model>","0","0","","","","CC BY-SA 4.0"
"69353325","1","","","2021-09-27 21:14:15","","3","31","<p>I'm doing sentiment analysis of Spanish tweets.</p>
<p>After reviewing some of the recent literature, I've seen that there's been a most recent effort to train a RoBERTa model exclusively on Spanish text (<code>roberta-base-bne</code>). It seems to perform better than the current state-of-the-art model for Spanish language modeling so far, <a href=""https://huggingface.co/finiteautomata/beto-sentiment-analysis#beto-sentiment-analysis"" rel=""nofollow noreferrer"">BETO</a>.</p>
<p>The RoBERTa model has been trained for a variety of tasks, which do not include text classification.
I want to take this <a href=""https://huggingface.co/BSC-TeMU/roberta-base-bne"" rel=""nofollow noreferrer"">RoBERTa model</a> and fine-tune it for text classification, more specifically, sentiment analysis.</p>
<p>I've done all the preprocessing and created the dataset objects, and want to natively train the model.</p>
<p><strong>Code</strong> <br></p>
<pre><code># Training with native TensorFlow 

from transformers import TFRobertaForSequenceClassification

model = TFRobertaForSequenceClassification.from_pretrained(&quot;BSC-TeMU/roberta-base-bne&quot;)

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)
</code></pre>
<p><strong>Question</strong><br>
My questions is regarding the <code>TFRobertaForSequenceClassification</code>: <br>
Is it correct to use this, since it's not specified in the <a href=""https://huggingface.co/BSC-TeMU/roberta-base-bne"" rel=""nofollow noreferrer"">model card</a>? Instead of the <code>AutoModelForMaskedLM </code> specified in the model card. <br></p>
<p>Do we, by simply applying <code>TFRobertaForSequenceClassification</code>, imply that it will automatically apply the trained (and pretrained) knowledge to the new task, namely text classification?</p>
","14513812","","14513812","","2021-09-28 20:32:22","2021-09-28 20:32:22","Fine-tuning a pretrained Spanish RoBERTa model for a different task, sentiment analysis","<python><tensorflow><sentiment-analysis><text-classification><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"69367112","1","","","2021-09-28 18:53:44","","0","19","<p>I am trying to execute the below command in Azure synapse Notebook.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForMaskedLM 
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModelForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<p>However, I am getting the below error.</p>
<pre><code>ImportError: tokenizers&gt;=0.10.1,&lt;0.11 is required for a normal functioning of this module, but found tokenizers==0.9.2.
Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git master
Traceback (most recent call last):

  File &quot;/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/transformers/__init__.py&quot;, line 43, in &lt;module&gt;
    from . import dependency_versions_check

  File &quot;/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/transformers/dependency_versions_check.py&quot;, line 41, in &lt;module&gt;
    require_version_core(deps[pkg])

  File &quot;/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/transformers/utils/versions.py&quot;, line 120, in require_version_core
    return require_version(requirement, hint)

  File &quot;/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/transformers/utils/versions.py&quot;, line 114, in require_version
    _compare_versions(op, got_ver, want_ver, requirement, pkg, hint)

  File &quot;/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/transformers/utils/versions.py&quot;, line 50, in _compare_versions
    f&quot;{requirement} is required for a normal functioning of this module, but found {pkg}=={got_ver}.{hint}&quot;

ImportError: tokenizers&gt;=0.10.1,&lt;0.11 is required for a normal functioning of this module, but found tokenizers==0.9.2.
Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git master
</code></pre>
<p>The above error is there even after I upgraded the transformer and tokenizer version, and validated it by below code:</p>
<pre><code>import pkg_resources
for d in pkg_resources.working_set:
    print(d)
</code></pre>
<p>Sample output of the above code:</p>
<pre><code>transformers 4.10.3
tokenizers 0.10.3
</code></pre>
<p>Any thoughts or suggestions? Thanks in advance.</p>
","16743924","","16743924","","2021-10-01 17:53:09","2021-10-01 17:53:09","Error Azure Synapse Notebook: ImportError: tokenizers>=0.10.1,<0.11 is required for a normal functioning of this module, but found tokenizers==0.9.2","<python><azure><huggingface-transformers><azure-synapse>","1","0","","","","CC BY-SA 4.0"
"69120220","1","","","2021-09-09 14:41:10","","0","64","<p>I am training a T5 model using simpletranformers and getting a lot of messages:</p>
<p><code>Warning: NaN or Inf found in input tensor.</code></p>
<p>How can I get to the root cause of this?</p>
","2202762","","","","","2021-09-09 14:41:10","Warning: NaN or Inf found in input tensor","<huggingface-transformers><simpletransformers>","0","1","","","","CC BY-SA 4.0"
"69147788","1","","","2021-09-12 01:24:49","","0","76","<p>I'm training an NLP model at work (e-commerce SEO) applying a <code>BERT</code> variation for portuguese language (<code>BERTimbau</code>) through <code>Transformers</code> by Hugging Face.</p>
<p>I didn't used the <code>Trainer</code> from Transformers API. I used <code>PyTorch</code> to set all parameters through <code>DataLoader.utils</code> and <code>adamW</code>. I trained my model using <code>run_glue.py</code>.</p>
<p><strong>I'm training with a VM on GCP using Jupyterlab</strong>. I know that I can use Weights &amp; Biases both for PyTorch and Transformers. But I don't know exactly how to set it using <code>run_glue.py</code>. It's my first time using Weights &amp; Biases.</p>
<p><em>After preprocessing and splitting train and test through Sklearn, my code is as it follows</em>:</p>
<pre><code>from transformers import BertTokenizer
import torch
#import torchvision
from torch.utils.data import Dataset, TensorDataset
import collections.abc as container_abcs

# To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.
# Constructs a BERT tokenizer. Based on WordPiece. 
# The tokenization must be performed by the tokenizer included with BERT
tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', 
                                          do_lower_case=True)

# Tokenize all of the sentences and map the tokens to thier word IDs. To convert all the titles from text into encoded form.
# We will use padding and truncation because the training routine expects all tensors within a batch to have the same dimensions.
encoded_data_train = tokenizer.batch_encode_plus(
    df[df.data_type=='train'].text.values, 
    add_special_tokens=True,                            # Add '[CLS]' and '[SEP]'. Sequences encoded with special tokens relative to their model
    return_attention_mask=True,                         # Return mask according to specific tokenizer defined by max_length
    pad_to_max_length=True,                             # Pad &amp; truncate all sentences. Pad all titles to certain maximum length                  
    max_length=128,                                     # Do not need to set max_length=256
    return_tensors='pt'                                 # Set to use PyTorch tensors
)

encoded_data_val = tokenizer.batch_encode_plus(
    df[df.data_type=='val'].text.values, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    pad_to_max_length=True, 
    max_length=128, 
    return_tensors='pt'
)

# Split the data into input_ids, attention_masks and labels. 
# Converting the input data to the tensor , which can be feeded to the model
input_ids_train = encoded_data_train['input_ids']                    # Add the encoded sentence to the list.
attention_masks_train = encoded_data_train['attention_mask']         # And its attention mask (simply differentiates padding from non-padding).
labels_train = torch.tensor(df[df.data_type=='train'].label.values)

input_ids_val = encoded_data_val['input_ids']
attention_masks_val = encoded_data_val['attention_mask']
labels_val = torch.tensor(df[df.data_type=='val'].label.values)

# Create training data and validation data
dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)
dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)
</code></pre>
<pre><code>from transformers import BertForSequenceClassification
model = BertForSequenceClassification.from_pretrained(&quot;neuralmind/bert-large-portuguese-cased&quot;,  # Select your pretrained Model
                                                      num_labels=len(label_dict),                # Labels tp predict
                                                      output_attentions=False,                   # Whether the model returns attentions weights. We donâ€™t really care about output_attentions. 
                                                      output_hidden_states=False)                # Whether the model returns all hidden-states. We also donâ€™t need output_hidden_states
</code></pre>
<pre><code>from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

batch_size = 32                                                              # Set your batch size according to your GPU memory      

dataloader_train = DataLoader(dataset_train                                  # Use DataLoader to Optimize your model
                              ,sampler=RandomSampler(dataset_train)          # Random Sampler from your dataset
                              ,batch_size=batch_size)                        # If your batch_size is too high you will get a warning when you run the model 
                              #,num_workers=4                                # Number of cores
                              #,pin_memory=True)                             # Use GPU to send your batch 

dataloader_validation = DataLoader(dataset_val 
                                   ,sampler=SequentialSampler(dataset_val)   # For validation the order doesn't matter. Sequential Sampler consumes less GPU.
                                   ,batch_size=batch_size) 
                                   #,num_workers=4
                                   #,pin_memory=True)
from transformers import AdamW, get_linear_schedule_with_warmup
# hyperparameters
# To construct an optimizer, we have to give it an iterable containing the parameters to optimize. 
# Then, we can specify optimizer-specific options such as the learning rate, epsilon, etc.
optimizer = AdamW(model.parameters(),  # AdamW is a class from the huggingface library (as opposed to pytorch) 
                  lr=2e-5,             # args.learning_rate - default is 5e-5
                  eps=1e-8)            # args.adam_epsilon  - default is 1e-8


# Number of training epochs. The BERT authors recommend between 2 and 4. 
epochs = 2

# Create the learning rate scheduler that decreases linearly from the initial learning rate set in the optimizer to 0, 
# after a warmup period during which it increases linearly from 0 to the initial learning rate set in the optimizer.
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps=0,                               # Default value in run_glue.py                           
                                            num_training_steps=len(dataloader_train)*epochs)  # Total number of training steps is [number of batches] x [number of epochs]. 
                                                                                              # Note that this is not the same as the number of training samples).
</code></pre>
<pre><code>from sklearn.metrics import f1_score

def f1_score_func(preds, labels):
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return f1_score(labels_flat, preds_flat, average='weighted')

def accuracy_per_class(preds, labels):
    label_dict_inverse = {v: k for k, v in label_dict.items()}
    
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    for label in np.unique(labels_flat):
        y_preds = preds_flat[labels_flat==label]
        y_true = labels_flat[labels_flat==label]
        print(f'Class: {label_dict_inverse[label]}')
        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\n')

</code></pre>
<p>And here follows <code>run_glue.py</code>:</p>
<pre><code>import random
from tqdm import tqdm
import torch
import numpy as np
# from tqdm.notebook import trange, tqdm

'''
This training code is based on the 'run_glue.py' script here:
https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128
'''

# Just right before the actual usage select your hardware
device = torch.device('cuda') # or cpu
model = model.to(device)      # send your model to your hardware

# Set the seed value all over the place to make this reproducible.
seed_val = 17
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

# We'll store a number of quantities such as training and validation loss, validation and timings.
def evaluate(dataloader_val):
    '''
    Put the model in evaluation mode--the dropout layers behave differently
    during evaluation.
    '''
    
    model.eval()
    
    loss_val_total = 0 # Tracking variables 

    predictions, true_vals = [], []
    
    for batch in dataloader_val:
        '''
         Unpack this training batch from our dataloader.         
         As we unpack the batch, we'll also copy each tensor to the GPU using the 
         `to` method.
        
         `batch` contains three pytorch tensors:
           [0]: input ids 
           [1]: attention masks
           [2]: labels 
        '''
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }
        
        '''
        Tell pytorch not to bother with constructing the compute graph during
        the forward pass, since this is only needed for backprop (training).
        '''
        with torch.no_grad():        
            outputs = model(**inputs)
            
        '''
        Perform a forward pass (evaluate the model on this training batch).
        The documentation for this `model` function is here: 
        https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification
        This will return the loss (rather than the model output) 
        because we have provided the `labels`.
        It returns different numbers of parameters depending on what arguments
        arge given and what flags are set. For our useage here, it returns
        the loss (because we provided labels) and the &quot;logits&quot;--the model
        outputs prior to activation.
        '''
        loss = outputs[0]
        logits = outputs[1]
        loss_val_total += loss.item() # Accumulate the validation loss.

                
        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)
    
    loss_val_avg = loss_val_total/len(dataloader_val) # Calculate the average loss over all of the batches.

    
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
            
    return loss_val_avg, predictions, true_vals

    # ========================================
    #               Training
    # ========================================

# For each epoch...
for epoch in tqdm(range(1, epochs+1)):    
    '''
    Put the model into training mode. Don't be mislead--the call to 
    `train` just changes the *mode*, it doesn't *perform* the training.
    `dropout` and `batchnorm` layers behave differently during training
    vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)
    '''
    
    model.train()        # Put the model into training mode.

    
    loss_train_total = 0 # Reset the total loss for this epoch.

    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
    for batch in progress_bar:

        model.zero_grad()        
        '''
        Always clear any previously calculated gradients before performing a
        backward pass. PyTorch doesn't do this automatically because 
        accumulating the gradients is &quot;convenient while training RNNs&quot;. 
        (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)
        '''
        
        batch = tuple(b.to(device) for b in batch)
        '''
        Unpack this training batch from our dataloader. 
        
         As we unpack the batch, we'll also copy each tensor to the GPU using the 
         `to` method.
        
         `batch` contains three pytorch tensors:
           [0]: input ids 
           [1]: attention masks
           [2]: labels
         '''
        
        inputs = {'input_ids':      batch[0], #.to(device)
                  'attention_mask': batch[1], #.to(device)
                  'labels':         batch[2], #.to(device)
                 }       

        outputs = model(**inputs)
        

        loss = outputs[0] # The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.
        loss_train_total += loss.item()         # Accumulate the training loss over all of the batches so that we can
                                                # calculate the average loss at the end. `loss` is a Tensor containing a
                                                # single value; the `.item()` function just returns the Python value 
                                                # from the tensor.
        loss.backward() # Perform a backward pass to calculate the gradients.

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    # Clip the norm of the gradients to 1.0.
                                                                   # This is to help prevent the &quot;exploding gradients&quot; problem.
                                                                   # modified based on their gradients, the learning rate, etc.
                
        optimizer.step()        # Update parameters and take a step using the computed gradient.
                                # The optimizer dictates the &quot;update rule&quot;--how the parameters are
                                # modified based on their gradients, the learning rate, etc.
                
        scheduler.step()        # Update the learning rate.

        
        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})
         
        
    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model') # Save Model
        
    tqdm.write(f'\nEpoch {epoch}') # Show running epoch
    
    loss_train_avg = loss_train_total/len(dataloader_train) # Calculate the average loss over all of the batches.
           
    tqdm.write(f'Training loss: {loss_train_avg}') # Show loss average
    
        # ========================================
        #               Validation
        # ========================================
        # After the completion of each training epoch, measure our performance on
        # our validation set.
    
    # Record all statistics from this epoch.
    val_loss, predictions, true_vals = evaluate(dataloader_validation)
    val_f1 = f1_score_func(predictions, true_vals)
    tqdm.write(f'Validation loss: {val_loss}')
    tqdm.write(f'F1 Score (Weighted): {val_f1}')
</code></pre>
","15673147","","15673147","","2021-09-13 14:46:38","2021-09-13 14:46:38","Weights & Biases with Transformers and PyTorch?","<python><google-cloud-platform><pytorch><monitoring><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"69266293","1","69275576","","2021-09-21 09:10:09","","1","46","<p>I am trying to get the embeddings from pre-trained wav2vec2 models (e.g., from jonatasgrosman/wav2vec2-large-xlsr-53-german) using my own dataset.</p>
<p>My aim is to use these features for a downstream task (not specifically speech recognition). Namely, since the dataset is relatively small, I would train an SVM with these embeddings for the final classification.</p>
<p>So far I have tried this:</p>
<pre><code>model_name = &quot;facebook/wav2vec2-large-xlsr-53-german&quot;
feature_extractor = Wav2Vec2Processor.from_pretrained(model_name)
model = Wav2Vec2Model.from_pretrained(model_name)

input_values = feature_extractor(train_dataset[:10][&quot;speech&quot;], return_tensors=&quot;pt&quot;, padding=True, 
                                 feature_size=1, sampling_rate=16000 ).input_values 
</code></pre>
<p>Then, I am not sure whether the embeddings here correspond to the sequence of last_hidden_states:</p>
<pre><code>hidden_states = model(input_values).last_hidden_state
</code></pre>
<p>or to the sequence of features of the last conv layer of the model:</p>
<pre><code>features_last_cnn_layer = model(input_values).extract_features
</code></pre>
<p>Also, is this the correct way to extract features from a pre-trained model?</p>
<p>How one can get embeddings from a specific layer?</p>
<p>PD: Posting here as the HuggingFace's forum seems to be less active.</p>
","3885769","","14719844","","2021-09-21 09:56:50","2021-09-21 21:07:24","Getting embeddings from wav2vec2 models in HuggingFace","<python><huggingface-transformers><pre-trained-model>","1","0","","","","CC BY-SA 4.0"
"69392642","1","","","2021-09-30 12:56:54","","0","13","<p>I want to use 50GB and 1TB datasets for training a model from HuggingFace. I have build my <a href=""https://huggingface.co/docs/datasets/loading_datasets.html"" rel=""nofollow noreferrer"">data_loader</a> that works fine on smaller version of the dataset. However, for complete dataset it is taking too long in caching. We have enough compute power but one person can reserve a resource on server for <strong>at max 6 hours</strong>. Therefore, <a href=""https://huggingface.co/docs/datasets/loading_datasets.html"" rel=""nofollow noreferrer"">data_loader</a> could cache upto 60-70% of 50G dataset in the allowed time slot.</p>
<p>Is there any way to:</p>
<ol>
<li>load data from local directory without caching?</li>
<li>Resume caching process after the time-out (6 hours) occurs.</li>
</ol>
<p>I tried to increase the number of <code>num_proc</code>, <code>dataloader_num_workers</code> but none showed any benefit.</p>
<p>Note that I can not have time slot more than 6 hours. My data_loader works fine on local system, caches data in 24 hours.</p>
","3122926","","","","","2021-09-30 12:56:54","HuggingFace: Load large dataset without cache (from local dir)","<pytorch><torch><huggingface-transformers><huggingface-datasets>","0","0","","","","CC BY-SA 4.0"
"69406937","1","69417899","","2021-10-01 13:44:52","","0","16","<p>I'm trying to use BERT models to do text classification. As the text is about scientific texts, I intend to use the <strong>SicBERT</strong> pre-trained model: <a href=""https://github.com/allenai/scibert"" rel=""nofollow noreferrer"">https://github.com/allenai/scibert</a></p>
<p>I have faced several limitations which I want to know if there is any solutions for them:</p>
<ol>
<li><p>When I want to do tokenization and batching, it only allows me to use <code>max_length</code> of &lt;=512. Is there any way to use more tokens. Doen't this limitation of 512 mean that I am actually not using all the text information during training? Any solution to use all the text?</p>
</li>
<li><p>I have tried to use this pretrained library with other models such as DeBERTa or RoBERTa. But it doesn't let me. I has only worked with BERT. Is there anyway I can do that?</p>
</li>
<li><p>I know this is a general question, but any suggestion that I can improve my fine tuning (from data to hyper parameter, etc)? Currently, I'm getting ~75% accuracy. Thanks</p>
</li>
</ol>
<p><strong>Codes:</strong></p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')

encoded_data_train = tokenizer.batch_encode_plus(
    df_train.text.values, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    padding=True,
    max_length=256
)

input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(df_train.label.values)

dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)

dataloader_train = DataLoader(dataset_train, 
                              sampler=RandomSampler(dataset_train), 
                              batch_size=batch_size)

model = BertForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased',
                                                      num_labels=len(labels),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

epochs = 1

optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)

scheduler = get_linear_schedule_with_warmup(optimizer,
num_training_steps=len(dataloader_train)*epochs)
</code></pre>
","5514178","","5514178","","2021-10-02 15:11:14","2021-10-02 15:11:14","How to use SciBERT in the best manner?","<nlp><pytorch><text-classification><huggingface-transformers><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"69334124","1","","","2021-09-26 10:44:57","","0","18","<p>I reproduce the training code from DataParallel to DistributedDataParallel, It does not release bugs in training, but it does not print any log or running.
Could show me what is wrong with my code?</p>
<p>This is my code and the distribute data parallel reference from <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#basic-use-case"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#basic-use-case</a></p>
<pre><code>&quot;&quot;&quot;
    Usages:
        CUDA_VISIBLE_DEVICES=0 python train.py --train_config default_config --work_dir runs/train/layoutlmv2-base-uncased_50e/

&quot;&quot;&quot;

import argparse
from torch import distributed as dist
from transformers import AutoModelForQuestionAnswering
import torch.nn as nn
from utils import create_logger, get_gpu_memory_map, load_feature_from_file, setup, cleanup
from config import TRAIN_FEATURE_PATH, VAL_FEATURE_PATH, MODEL_CHECKPOINT, TRAINING_CONFIGs
import numpy as np
import torch
import os
os.environ[&quot;TOKENIZERS_PARALLELISM&quot;] = &quot;false&quot;
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.multiprocessing as mp


def train(rank, model, train_data, val_data, world_size,
        epochs, optimizer, lr, save_freq,
        eval_freq, work_dir):


    device = rank
    print(&quot;Running DDP with model parallel example on cuda:{} device&quot;.format(rank))
    # logger.info(&quot;Running DDP with model parallel example on cuda:{} device&quot;.format(rank))

    setup(rank, world_size)
    GPU_usage_before = get_gpu_memory_map()
    model = model.to(rank)
    model = DDP(model, device_ids=[rank], find_unused_parameters=True)
    gpus_usage = np.sum(get_gpu_memory_map() - GPU_usage_before)
    print(&quot;GPUs usages for model: {} Mb&quot;.format(gpus_usage))
    # logger.info(&quot;GPUs usages for model: {} Mb&quot;.format(gpus_usage))
    
    optimizer = optimizer(model.parameters(), lr=lr)

    model.train()

    min_valid_loss = np.inf
    idx = 1

    for epoch in range(1, epochs):

        print(&quot;Epoch {}/{}&quot;.format(epoch, epochs))
        
        # logger.info(&quot;Epoch {}/{}&quot;.format(epoch, epochs))
        
        train_loss = 0.0
        for _, train_batch in enumerate(train_data):

            input_ids         = train_batch[&quot;input_ids&quot;].to(device)
            attention_mask    = train_batch[&quot;attention_mask&quot;].to(device)
            token_type_ids    = train_batch[&quot;token_type_ids&quot;].to(device)
            bbox              = train_batch[&quot;bbox&quot;].to(device)
            image             = train_batch[&quot;image&quot;].to(device)
            start_positions   = train_batch[&quot;start_positions&quot;].to(device)
            end_positions     = train_batch[&quot;end_positions&quot;].to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,
                            bbox=bbox, image=image, start_positions=start_positions, end_positions=end_positions)
    
            loss = outputs.loss
            
            loss.backward()
            
            optimizer.step()

            train_loss += loss.item()


            # Evaluate current model on entire validation dataset after each `eval_freq` iterations
            if idx % eval_freq == 1:
                val_loss = 0.0
                model.eval()
                for _, val_batch in enumerate(val_data):
                    
                    # val_batch               = val_batch.to(device)
                    input_ids               = val_batch[&quot;input_ids&quot;].to(device)
                    attention_mask          = val_batch[&quot;attention_mask&quot;].to(device)
                    token_type_ids          = val_batch[&quot;token_type_ids&quot;].to(device)
                    bbox                    = val_batch[&quot;bbox&quot;].to(device)
                    image                   = val_batch[&quot;image&quot;].to(device)
                    start_positions         = val_batch[&quot;start_positions&quot;].to(device)
                    end_positions           = val_batch[&quot;end_positions&quot;].to(device)
                    
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,
                                bbox=bbox, image=image, start_positions=start_positions, end_positions=end_positions)
                    loss = outputs.loss
                    # Calculate Loss
                    val_loss += loss.item()
                
                print(&quot;Iterations: {:&lt;6} - epoch: {:&lt;3} - train_loss: {:&lt;6} - val_loss: {:&lt;6}&quot;.format(idx, epoch, train_loss/eval_freq, val_loss/len(val_data)))
                print(&quot;Iterations: {:&lt;6} - epoch: {:&lt;3} - train_loss: {:&lt;6} - val_loss: {:&lt;6}&quot;.format(idx, epoch, train_loss/eval_freq, val_loss/len(val_data)))

                # logger.info(&quot;Iterations: {:&lt;6} - epoch: {:&lt;3} - train_loss: {:&lt;6} - val_loss: {:&lt;6}&quot;.format(idx, epoch, train_loss/eval_freq, val_loss/len(val_data)))
                # loss_log.info(&quot;Iterations: {:&lt;6} - epoch: {:&lt;3} - train_loss: {:&lt;6} - val_loss: {:&lt;6}&quot;.format(idx, epoch, train_loss/eval_freq, val_loss/len(val_data)))

                    
                if min_valid_loss &gt; val_loss/len(val_data):
                    print(&quot;Found best model !! Validation loss descreased from {} to {}&quot;.format(min_valid_loss, val_loss/len(val_data)))
                    # logger.info(&quot;Found best model !! Validation loss descreased from {} to {}&quot;.format(min_valid_loss, val_loss/len(val_data)))
                    torch.save(model.state_dict(), os.path.join(work_dir, 'best'+'.pth'))
                    min_valid_loss = val_loss/len(val_data)

                # Save model each save_freq iteration
                if idx % save_freq == 1:
                    print(&quot;Saving model to {}&quot;.format(os.path.join(work_dir, str(idx).zfill(5)+'.pth')))
                    # logger.info(&quot;Saving model to {}&quot;.format(os.path.join(work_dir, str(idx).zfill(5)+'.pth')))
                    torch.save(model.state_dict(), os.path.join(work_dir, str(idx).zfill(5)+'.pth'))
                    dist.barrier()

                # Reset training loss
                train_loss = 0.0
                
            idx += 1


    # logger.info(&quot;Done !&quot;)
    # logger.info(&quot;The minimum on validation {}&quot;.format(min_valid_loss))
    print(&quot;DONE !&quot;)
    print(&quot;The minimum on validation {}&quot;.format(min_valid_loss))

    cleanup()

    return model


def main(args):

    gpu_ids = [i for i in range(torch.cuda.device_count())]
    torch.cuda.set_device(gpu_ids[0])
    
    if not os.path.exists(args['work_dir']):
        os.mkdir(args['work_dir'])
    # Create logger
    loss_log = create_logger(os.path.join(args[&quot;work_dir&quot;], 'loss.log'))
    logger = create_logger(os.path.join(args['work_dir'], 'log.log'))

    logger.info('Loading training configuration ...')
    config = TRAINING_CONFIGs[args['train_config']]
    optimizer, momentum, lr, epochs, batch_size,\
         eval_freq, save_freq, num_workers = config['optimizer'], config['momentum'], \
        config['lr'], config['epochs'], \
        config['batch_size'], config['eval_freq'], config['save_freq'], config['num_workers']
    logger.info(&quot;Configuration: {}&quot;.format(config))

    # Check whether feature path file existing or not
    if not os.path.exists(TRAIN_FEATURE_PATH):
        logger.error(&quot;Invalid training feature path&quot;)
        exit(0)
    if not os.path.exists(VAL_FEATURE_PATH):
        logger.error(&quot;Invalid validation feature path&quot;)
        exit(0)

    # Load data into program 
    logger.info(&quot;Loading training dataset from {} ...&quot;.format(TRAIN_FEATURE_PATH))
    train_dataloader = load_feature_from_file(path=TRAIN_FEATURE_PATH, 
                                            batch_size=batch_size, num_workers=num_workers)

    logger.info(&quot;Loading validation dataset from {} ...&quot;.format(VAL_FEATURE_PATH))
    val_dataloader = load_feature_from_file(path=VAL_FEATURE_PATH, 
                                            batch_size=batch_size, num_workers=num_workers)

    logger.info(&quot;Training size: {} - Validation size: {}&quot;.format(
        len(train_dataloader.dataset), len(val_dataloader.dataset)))

    logger.info(&quot;Loading pre-training model from {} checkpoint&quot;.format(MODEL_CHECKPOINT))
    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT)
    
    # Fine-tuning model
    # trained_model = train(model=model, train_data=train_dataloader, val_data=val_dataloader,
    #                   epochs=epochs, optimizer=optimizer, lr=lr, loss_log=loss_log, save_freq=save_freq,
    #                     work_dir=args['work_dir'], logger=logger, eval_freq=eval_freq, gpu_ids=gpu_ids)

    mp.spawn(train,
             args=(model, train_dataloader, val_dataloader, len(gpu_ids), 
                   epochs, optimizer, lr, save_freq,
                   eval_freq, args['work_dir']),
             nprocs=len(gpu_ids),
             join=True)


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Fine tuning pre-training model on DocVQA data')

    parser.add_argument('--work_dir', default='runs/train/train_1/',
        help='The directory store model checkpoint and log file',
    )

    parser.add_argument('--train_config', default='default_config', 
        help='The training configurations: learning rate, batch size, epochs, optimizer, ...'
    )

    args = vars(parser.parse_args())

    main(args)
</code></pre>
<p><strong>The output in terminal like below.</strong></p>
<pre class=""lang-sh prettyprint-override""><code>(transformer_env)root@ae94a4e6c92d:/mlcv/WorkingSpace/NCKH/tiennv/vqa_thesis/docvqa/libs/layoutlmv2# CUDA_VISIBLE_DEVICES=1,2 python train.py --work_dir ./runs/train/test_multi-gpus --train_config default_config
2021-09-26 10:11:49,801 - INFO - Loading training configuration ...
2021-09-26 10:11:49,802 - INFO - Configuration: {'optimizer': &lt;class 'torch.optim.adam.Adam'&gt;, 'lr': 0.0001, 'epochs': 2, 'batch_size': 2, 'momentum': 0.9, 'eval_freq': 1, 'save_freq': 1, 'num_workers': 4}
2021-09-26 10:11:49,803 - INFO - Loading training dataset from /mlcv/Databases/DocVQA_2020-21/task_1/extracted_features/layoutlmv2/train ...
2021-09-26 10:11:49,953 - INFO - Loading validation dataset from /mlcv/Databases/DocVQA_2020-21/task_1/extracted_features/layoutlmv2/val ...
2021-09-26 10:11:49,977 - INFO - Training size: 39456 - Validation size: 5344
2021-09-26 10:11:49,978 - INFO - Loading pre-training model from microsoft/layoutlmv2-base-uncased checkpoint
Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2ForQuestionAnswering: ['layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked']
- This IS expected if you are initializing LayoutLMv2ForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LayoutLMv2ForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LayoutLMv2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias', 'layoutlmv2.visual_segment_embedding']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running DDP with model parallel example on cuda:0 device
Running DDP with model parallel example on cuda:1 device
GPUs usages for model: 6721 Mb
Epoch 1/2
GPUs usages for model: 6721 Mb
Epoch 1/2
</code></pre>
<p>It still prints anything ...
Please help me ...</p>
","17007416","","","","","2021-09-26 10:44:57","How to train AutoModelForQuestionAnswering model with Distribute Data Parallel?","<pytorch><multiprocessing><huggingface-transformers><transformer>","0","0","","","","CC BY-SA 4.0"
"69375368","1","","","2021-09-29 11:18:53","","0","15","<p>I'm fine-tuning a <a href=""https://huggingface.co/BSC-TeMU/roberta-base-bne"" rel=""nofollow noreferrer"">Spanish RoBERTa model</a> for a new task, namely text classification, to be more precise, sentiment analysis, and want to know how I can evaluate the models performance after fine-tuning for the test or validation data.</p>
<h3>Code <br></h3>
<p>The preprocessing looks like this:</p>
<pre><code>tass_train = pd.read_csv('https://raw.githubusercontent.com/lucamarcelo/Vaccine-Tweets-Sentiment-Analysis/main/es_train.tsv', sep='\t', header=None, usecols=[1,2])
tass_test = pd.read_csv('https://raw.githubusercontent.com/lucamarcelo/Vaccine-Tweets-Sentiment-Analysis/main/es_val.tsv', sep='\t', header=None, usecols=[1,2])

from sklearn.model_selection import train_test_split

# Create validation data from the training data
train_texts, val_texts, train_labels, val_labels = train_test_split(list(tass_train[1]), list(tass_train[2]), test_size=0.1)

# Creating lists for the test text and test labels
test_texts = list(tass_test[1])
test_labels = list(tass_test[2])

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;BSC-TeMU/roberta-base-bne&quot;)

# Create the preprocessed texts 
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

# Convert the labels to ints

## Create a dictionary for the mapping
d = {'NEU':0, 'N':1, 'P':2}

## Map the values in the dictionary to the three lists of labels 
train_labels = list(pd.Series(train_labels).map(d).astype(int))
val_labels = list(pd.Series(val_labels).map(d).astype(int))
test_labels = list(pd.Series(test_labels).map(d).astype(int))

# Create the tensorflow datasets from our encodings
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels))

val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels))

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels))
</code></pre>
<p>My fine-tuning looks like this:</p>
<pre><code># Training with native TensorFlow 
from transformers import TFAutoModelForSequenceClassification

## Model Definition
model = TFAutoModelForSequenceClassification.from_pretrained(&quot;BSC-TeMU/roberta-base-bne&quot;, from_pt=True, num_labels=3)

## Model Compilation
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.metrics.SparseCategoricalAccuracy()
model.compile(optimizer=optimizer, 
              loss=loss,
              metrics=metric) 

## Fitting the data
history = model.fit(train_dataset.shuffle(1000).batch(64), epochs=5, batch_size=64)
</code></pre>
<p>I've used the <code>.fit()</code> part from a <a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">HuggingFace Tutorial</a>, but what I've found strange is that there's no validation/test dataset in there.</p>
<h3>Output</h3>
<p>The following output I get from the training, is therefore, I guess, the performance on the training set.</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  &quot;Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 &quot;
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']
- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/5
16/16 [==============================] - 37s 1s/step - loss: 1.0225 - sparse_categorical_accuracy: 0.4857
Epoch 2/5
16/16 [==============================] - 18s 1s/step - loss: 0.6771 - sparse_categorical_accuracy: 0.7177
Epoch 3/5
16/16 [==============================] - 18s 1s/step - loss: 0.3543 - sparse_categorical_accuracy: 0.8786
Epoch 4/5
16/16 [==============================] - 18s 1s/step - loss: 0.1371 - sparse_categorical_accuracy: 0.9625
Epoch 5/5
16/16 [==============================] - 18s 1s/step - loss: 0.0445 - sparse_categorical_accuracy: 0.9921
</code></pre>
<h3>Question</h3>
<p>How can I evaluate the performance of the model on the test/validation data?</p>
<h4>Elaboration</h4>
<p>I'm guessing I have to specify that when I call <code>.fit()</code>, so what I did is:</p>
<pre><code>history = model.fit(train_dataset, validation_data=val_dataset, epochs=5, batch_size=64)
</code></pre>
<p>But this results in an error:
<code> ValueError: Shape mismatch: The shape of labels (received (1,)) should equal the shape of logits except for the last dimension (received (49, 3)).</code></p>
<p>I've also tried:</p>
<pre><code>results = model.evaluate(test_dataset)
    
print('Test Loss: {}'.format(results[0]))
print('Test Accuracy: {}'.format(results[1]))
</code></pre>
<p>This won't work either, and I get the following error:
<code> ValueError: Shape mismatch: The shape of labels (received (1,)) should equal the shape of logits except for the last dimension (received (53, 3)).</code> -- Very similar to the one above.</p>
<p>Unfortunately, the <a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">HuggingFace documentation</a> doesn't mention this issue, as far as I've seen.</p>
","14513812","","","","","2021-09-29 11:18:53","How can I evaluate a Huggingface model after fine-tuning?","<tensorflow><keras><nlp><text-classification><huggingface-transformers>","0","0","","2021-09-29 15:05:52","","CC BY-SA 4.0"
"69403349","1","","","2021-10-01 09:09:07","","1","16","<p>I used Amazon SageMaker to train a HuggingFace model. At the end of the training script provided to the estimator, I saved the model into the correct path (<code>SM_MODEL_DIR</code>):</p>
<pre><code>if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser()
    parser.add_argument(&quot;--model-dir&quot;, type=str, default=os.environ[&quot;SM_MODEL_DIR&quot;])
    
    ...
    
    trainer.model.save_pretrained(args.model_dir)
</code></pre>
<p>After the model was trained, I deployed it using the <code>deploy</code> method of the HuggingFace estimator. Once the endpoint was successfully created, I tried inference with the returned predictor:</p>
<pre><code>response = self.predictor.predict(
    {&quot;inputs&quot;: &quot;I want to know where is my order&quot;}
)
</code></pre>
<p>And I received the following client error:</p>
<pre><code>{'code': 400, 'type': 'InternalServerException', 'message': &quot;Can't load tokenizer for '/.sagemaker/mms/models/model'. Make sure that:\n\n- '/.sagemaker/mms/models/model' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/.sagemaker/mms/models/model' is the correct path to a directory containing relevant tokenizer files\n\n&quot;}
</code></pre>
<p>Why cannot the tokenizer be loaded?</p>
","17041637","","","","","2021-10-01 09:22:28","SageMaker endpoint can't load huggingface tokenizer","<python><amazon-web-services><amazon-sagemaker><huggingface-transformers><huggingface-tokenizers>","0","0","1","","","CC BY-SA 4.0"
"69403613","1","","","2021-10-01 09:30:21","","1","8","<p>I am using GPT-Neo model from <code>transformers</code> to generate text. Because the prompt I use starts with <code>'{'</code>, so I would like to stop the sentence once the paring <code>'}'</code> is generated.
I found that there is a <code>StoppingCriteria</code> method in the source code but without further instructions on how to use it. Does anyone have found a way to early-stop the model generation? Thanks!</p>
","2632462","","","","","2021-10-01 09:30:21","How to early-stop autoregressive model with a list of stop words?","<huggingface-transformers><autoregressive-models><gpt-2>","0","1","","","","CC BY-SA 4.0"
"69417148","1","","","2021-10-02 13:01:45","","0","14","<p>Based on <a href=""https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling"" rel=""nofollow noreferrer"">examples</a>, I am trying to train a tokenizer and a model for T5 for Persian.
I use Google Colab pro,
when I tried to run the following code:</p>
<pre><code>import datasets

from t5_tokenizer_model import SentencePieceUnigramTokenizer


vocab_size = 32_000
input_sentence_size = None # change to 100_000 works

# Initialize a dataset
dataset = datasets.load_dataset(&quot;oscar&quot;, name=&quot;unshuffled_deduplicated_fa&quot;, split=&quot;train&quot;)

tokenizer = SentencePieceUnigramTokenizer(unk_token=&quot;&lt;unk&gt;&quot;, eos_token=&quot;&lt;/s&gt;&quot;, pad_token=&quot;&lt;pad&gt;&quot;)

print(&quot;len dataset:&quot;, len(dataset))

# Build an iterator over this dataset
def batch_iterator(input_sentence_size=None):
    if input_sentence_size is None:
        input_sentence_size = len(dataset)
    batch_length = 100
    for i in range(0, input_sentence_size, batch_length):
        yield dataset[i: i + batch_length][&quot;text&quot;]


# Train tokenizer
tokenizer.train_from_iterator(
    iterator=batch_iterator(input_sentence_size=input_sentence_size),
    vocab_size=vocab_size,
    show_progress=True,
)

# Save files to disk
tokenizer.save(&quot;/content/drive/MyDrive/Pouramini/tokenizer.json&quot;)
</code></pre>
<p>It get stuck in <code>train_from_iterator</code> because the size of dataset is large (<code>input_sentence_size</code> is around 8M sentences)
How can I divide the dataset and run the code on each block and then merge them to a tokenizer output?</p>
","2651073","","","","","2021-10-02 13:01:45","How to train a tokenizer on a big dataset?","<python><huggingface-transformers><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"69373160","1","","","2021-09-29 08:44:22","","0","9","<p>How to build a Open domain question answering(ODQA/KBQA) model using Deeppavlov of Huggingface</p>
<p>I want to build a custom model for this.</p>
","16609985","","","","","2021-09-29 09:33:06","How to build a Open domain question answering model(ODQA/KBQA) using Deeppavlov of Huggingface","<huggingface-transformers><question-answering><deeppavlov>","1","0","","","","CC BY-SA 4.0"
"69373796","1","","","2021-09-29 09:32:48","","0","32","<p>I'm training/fine-tuning a <a href=""https://huggingface.co/BSC-TeMU/roberta-base-bne"" rel=""nofollow noreferrer"">Spanish RoBERTa model</a> that has recently been pre-trained for a variety of NLP tasks except for text classification.</p>
<p>Since the baseline model seems to be promising, I want to fine-tune it for a different task: text classification, more precisely, sentiment analysis of Spanish Tweets. <br>
I have a nice selection of Spanish, labelled tweets that I can use for fine-tuning.</p>
<p>The preprocessing has been without any problems. However, when I train the model, it won't improve, i.e. the accuracy won't go up.</p>
<p><strong>Code:</strong><br>
I'll leave out the preprocessing part because I don't think there seems to be an issue.</p>
<pre><code># Training with native TensorFlow 
from transformers import TFAutoModelForSequenceClassification

## Model Definition
model = TFAutoModelForSequenceClassification.from_pretrained(&quot;BSC-TeMU/roberta-base-bne&quot;, from_pt=True, num_labels=3)

## Model Compilation
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)
metric = tf.metrics.SparseCategoricalAccuracy()
model.compile(optimizer=optimizer, 
              loss=loss,
              metrics=metric) 

## Fitting the data 
history = model.fit(train_dataset.shuffle(1000).batch(64), epochs=3, batch_size=64)
</code></pre>
<p><strong>Output:</strong><br>
For this, with <code>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)</code> I get:</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  &quot;Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 &quot;
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']
- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/5
16/16 [==============================] - 34s 1s/step - loss: 1.0986 - sparse_categorical_accuracy: 0.2734
Epoch 2/5
16/16 [==============================] - 17s 1s/step - loss: 1.0986 - sparse_categorical_accuracy: 0.2734
Epoch 3/5
16/16 [==============================] - 17s 1s/step - loss: 1.0986 - sparse_categorical_accuracy: 0.2734
Epoch 4/5
16/16 [==============================] - 17s 1s/step - loss: 1.0986 - sparse_categorical_accuracy: 0.2734
Epoch 5/5
16/16 [==============================] - 17s 1s/step - loss: 1.0986 - sparse_categorical_accuracy: 0.2734
</code></pre>
<p>When I use <code>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)</code>, however, I get:</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  &quot;Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 &quot;
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']
- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/5
16/16 [==============================] - 35s 1s/step - loss: 1.0184 - sparse_categorical_accuracy: 0.4778
Epoch 2/5
16/16 [==============================] - 18s 1s/step - loss: 0.6688 - sparse_categorical_accuracy: 0.7394
Epoch 3/5
16/16 [==============================] - 18s 1s/step - loss: 0.3270 - sparse_categorical_accuracy: 0.8845
Epoch 4/5
16/16 [==============================] - 18s 1s/step - loss: 0.1200 - sparse_categorical_accuracy: 0.9654
Epoch 5/5
16/16 [==============================] - 18s 1s/step - loss: 0.0500 - sparse_categorical_accuracy: 0.9872
</code></pre>
<p><strong>Question:</strong><br>
Which one should I use, i.e. which one is the correct one and why? <br>
Also, I'm wondering whether I'm using the correct metric and loss.</p>
","14513812","","","","","2021-09-29 09:32:48","from_logits=True vs. from_logits=False in RoBERTa fine-tuning for new task","<python><tensorflow><keras><text-classification><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"69374258","1","","","2021-09-29 10:03:15","","2","50","<p>I have tried different approaches to <strong>sentence similarity</strong>, namely:</p>
<ul>
<li><p><strong>spaCy models</strong>: <a href=""https://spacy.io/models/en#en_core_web_md"" rel=""nofollow noreferrer""><code>en_core_web_md</code></a> and <a href=""https://spacy.io/models/en#en_core_web_lg"" rel=""nofollow noreferrer""><code>en_core_web_lg</code></a>.</p>
</li>
<li><p><strong>Transformers</strong>: using the packages <a href=""https://pypi.org/project/sentence-similarity/"" rel=""nofollow noreferrer""><code>sentence-similarity</code></a> and <a href=""https://www.sbert.net/index.html"" rel=""nofollow noreferrer""><code>sentence-transformers</code></a>, I've tried models such as <a href=""https://huggingface.co/distilbert-base-uncased"" rel=""nofollow noreferrer""><code>distilbert-base-uncased</code></a>, <a href=""https://huggingface.co/bert-base-uncased"" rel=""nofollow noreferrer""><code>bert-base-uncased</code></a> or <a href=""https://huggingface.co/sentence-transformers/all-mpnet-base-v2"" rel=""nofollow noreferrer""><code>sentence-transformers/all-mpnet-base-v2</code></a>.</p>
</li>
<li><p><strong>Universal Sentence Encoding</strong>: using the package <a href=""https://github.com/MartinoMensio/spacy-universal-sentence-encoder"" rel=""nofollow noreferrer""><code>spacy-universal-sentence-encoder</code></a>, with the models <a href=""https://tfhub.dev/google/universal-sentence-encoder"" rel=""nofollow noreferrer""><code>en_use_md</code></a> and <a href=""https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-large"" rel=""nofollow noreferrer""><code>en_use_cmlm_lg</code></a>.</p>
</li>
</ul>
<p>However, while these models generally correctly detect similarity for equivalent sentences, they all fail when inputting negated sentences. E.g., these <strong>opposite sentences</strong>:</p>
<ul>
<li>&quot;I like rainy days because they make me feel relaxed.&quot;</li>
<li>&quot;I don't like rainy days because they don't make me feel relaxed.&quot;</li>
</ul>
<p>return a <strong>similarity of 0.931</strong> with the model <code>en_use_md</code>.</p>
<p>However, sentences that could be considered <strong>very similar</strong>:</p>
<ul>
<li>&quot;I like rainy days because they make me feel relaxed.&quot;</li>
<li>&quot;I enjoy rainy days because they make me feel calm.&quot;</li>
</ul>
<p>return a <strong>smaller similarity: 0.914</strong>.</p>
<p><strong>My question is</strong>: Is there any way around this? Are there any other models/approaches that take into account the affirmative/negative nature of sentences when calculating similarity?</p>
","14683209","","","","","2021-09-30 08:25:22","Sentence similarity models not capturing opposite sentences","<python><nlp><spacy><huggingface-transformers><sentence-similarity>","2","1","1","","","CC BY-SA 4.0"
"69374271","1","69374378","","2021-09-29 10:04:24","","1","35","<p>I've trained/fine-tuned a <a href=""https://huggingface.co/BSC-TeMU/roberta-base-bne"" rel=""nofollow noreferrer"">Spanish RoBERTa</a> model that has recently been pre-trained for a variety of NLP tasks except for text classification.</p>
<p>Since the baseline model seems to be promising, I want to fine-tune it for a different task: text classification, more precisely, sentiment analysis of Spanish Tweets and use it to predict labels on scraped tweets I have.</p>
<p>The preprocessing and the training seem to work correctly. However, I don't know how I can use this mode afterwards for prediction.</p>
<p>I'll leave out the preprocessing part because I don't think there seems to be an issue.</p>
<h3>Code:</h3>
<pre><code># Training with native TensorFlow 
from transformers import TFAutoModelForSequenceClassification

## Model Definition
model = TFAutoModelForSequenceClassification.from_pretrained(&quot;BSC-TeMU/roberta-base-bne&quot;, from_pt=True, num_labels=3)

## Model Compilation
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.metrics.SparseCategoricalAccuracy()
model.compile(optimizer=optimizer, 
              loss=loss,
              metrics=metric) 

## Fitting the data 
history = model.fit(train_dataset.shuffle(1000).batch(64), epochs=3, batch_size=64)
</code></pre>
<h3>Output:</h3>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  &quot;Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 &quot;
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']
- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/5
16/16 [==============================] - 35s 1s/step - loss: 1.0455 - sparse_categorical_accuracy: 0.4452
Epoch 2/5
16/16 [==============================] - 18s 1s/step - loss: 0.6923 - sparse_categorical_accuracy: 0.7206
Epoch 3/5
16/16 [==============================] - 18s 1s/step - loss: 0.3533 - sparse_categorical_accuracy: 0.8885
Epoch 4/5
16/16 [==============================] - 18s 1s/step - loss: 0.1871 - sparse_categorical_accuracy: 0.9477
Epoch 5/5
16/16 [==============================] - 18s 1s/step - loss: 0.1031 - sparse_categorical_accuracy: 0.9714
</code></pre>
<h3>Question:</h3>
<p>How can I use the model after fine-tuning for text classification/sentiment analysis? (I want to create a predicted label for each tweet I scraped.)
<br>What would be a good way of approaching this?</p>
<p>I've tried to save the model, but I don't know where I can find it and use then:</p>
<pre><code># Save the model
model.save_pretrained('Twitter_Roberta_Model')
</code></pre>
<p>I've also tried to just add it to a HuggingFace pipeline like the following. But I'm not sure if this works correctly.</p>
<pre><code>classifier = pipeline('sentiment-analysis', 
model=model, 
tokenizer=AutoTokenizer.from_pretrained(&quot;BSC-TeMU/roberta-base-bne&quot;))
</code></pre>
","14513812","","14513812","","2021-09-29 16:43:04","2021-09-29 16:43:04","How to use a language model for prediction after fine-tuning?","<tensorflow><keras><nlp><huggingface-transformers><transfer-learning>","1","0","","","","CC BY-SA 4.0"
"67515765","1","","","2021-05-13 07:55:29","","0","81","<p>I'm trying out the QnA model (DistilBertForQuestionAnswering -'distilbert-base-uncased') by using HuggingFace pipeline. I saved the model in a local location using 'save_pretrained'. Then I reloaded the model later using 'from_pretrained'. However, the model performs differently when loaded from the local location. There seems to be something amiss in the way I am loading the saved models.</p>
<pre><code>    from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering
    path_d  = '/content/drive/MyDrive/Pipeline/distilbert'
    #  1 - save model 
    model_n = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')
    model_n.save_pretrained(path_d)
    #2 - save tokenizer 
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    tokenizer.save_pretrained(path_d) 
    
    #load the saved model in another file. 
    model_d = DistilBertForQuestionAnswering.from_pretrained(path_d)       
    #load tokenizer 
    tokenizer_d = DistilBertTokenizer.from_pretrained(path_d)
    
    context = 'The earliest reports of a coronavirus infection in animals occurred in the late 1920s, when an acute respiratory infection of domesticated chickens emerged in North America. Arthur
Schalk and M.C. Hawn in 1931 made the first detailed report which described a new respiratory infection of chickens in North Dakota. The infection of new-born chicks was characterized by
gasping and listlessness with high mortality rates of 40â€“90%. Leland David Bushnell and Carl Alfred Brandly isolated the virus that caused the infection in 1933. The virus was then known
as infectious bronchitis virus (IBV). Charles D. Hudson and Fred Robert Beaudette cultivated the virus for the first time in 1937. The specimen came to be known as the Beaudette strain.
In the late 1940s, two more animal coronaviruses, JHM that causes brain disease (murine encephalitis) and mouse hepatitis virus (MHV) that causes hepatitis in mice were discovered. It
was not realized at the time that these three different viruses were related.'
   # from https://en.wikipedia.org/wiki/Coronavirus 
    question = 'What is MHV?'
    
    from transformers import pipeline
    # using saved model 
    sec_layer = pipeline('question-answering', model= model_d, tokenizer = tokenizer_d )
    output = sec_layer(question = question, context  =context )
    print(output)  
    # {'score': 5.989291094010696e-05, 'start': 662, 'end': 685, 'answer': 'the first time in 1937.'}
    
    #  here the model is loaded again and the answer is different ,and is the correct answer
    
    sec_layer = pipeline('question-answering') 
    out = sec_layer(question = question, context  =context ,model= model_d, tokenizer = tokenizer_d )
    print(out) 
    #{'score': 0.837104082107544, 'start': 847, 'end': 868, 'answer': 'mouse hepatitis virus'}
</code></pre>
<p>Can someone please guide me here to understand what I have done wrong in saving and reloading the model?</p>
","11918296","","","","","2021-05-13 07:55:29","Huggingface Pipeline for Question And Answering","<huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"69340856","1","","","2021-09-27 03:23:53","","0","30","<p>I know I can train word embedding in Tensorflow or Gensim, then I can retrieve top N most similar words for a target word. Given that transformer is now the main stream model for text representation, I want to know whether there is a better way to compute word similarity than Word Embedding. In genesim, I can do:</p>
<pre><code>sims = model.wv.most_similar('computer', topn=10)
</code></pre>
<p>For example, if I use sentence transformer to compute:</p>
<p><a href=""https://huggingface.co/sentence-transformers/LaBSE"" rel=""nofollow noreferrer"">https://huggingface.co/sentence-transformers/LaBSE</a></p>
<pre><code>from sentence_transformers import SentenceTransformer
sentences = [&quot;This is an example sentence&quot;, &quot;Each sentence is converted&quot;]

model = SentenceTransformer('sentence-transformers/LaBSE')
embeddings = model.encode(sentences)
print(embeddings)
</code></pre>
<p>Then use this embedding to compute similarity, would that work for word similarity, if I treat any word as a 'sentence'? Or I use Bert embedding model:</p>
<p><a href=""https://huggingface.co/transformers/model_doc/bert.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html</a></p>
<p>I feed a word like 'computer' as input to get an embedding, then compute its topN similarity. Does this make sense? or it won't work better than embedding trained without involving transformer?</p>
","3943868","","","","","2021-09-27 03:23:53","How to calculate word similarity based on transformer?","<huggingface-transformers><bert-language-model><word-embedding><transformer>","0","4","","","","CC BY-SA 4.0"
"67546911","1","","","2021-05-15 12:50:32","","3","2642","<p>I am creating an entity extraction model in PyTorch using <code>bert-base-uncased</code> but when I try to run the model I get this error:</p>
<h2>Error:</h2>
<pre><code>Some weights of the model checkpoint at D:\Transformers\bert-entity-extraction\input\bert-base-uncased_L-12_H-768_A-12 were not used when initializing BertModel:    
['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight',   'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias',  
 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight',  
 'cls.predictions.bias']  
    - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
<p>I have downloaded the bert model from <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">here</a> and the additional files from <a href=""https://huggingface.co/bert-base-uncased/tree/main"" rel=""nofollow noreferrer"">here</a></p>
<h2>Code</h2>
<p>Following is the code for my model:</p>
<pre><code>import config
import torch
import transformers
import torch.nn as nn

def loss_fn(output, target, mask, num_labels):

    lfn = nn.CrossEntropyLoss()
    active_loss = mask.view(-1) == 1
    active_logits = output.view(-1, num_labels)
    active_labels = torch.where(
        active_loss,
        target.view(-1),
        torch.tensor(lfn.ignore_index).type_as(target)
    )
    loss = lfn(active_logits, active_labels)
    return loss

class EntityModel(nn.Module):
    def __init__(self, num_tag, num_pos):
        super(EntityModel, self).__init__()

        self.num_tag = num_tag
        self.num_pos = num_pos
        self.bert = transformers.BertModel.from_pretrained(config.BASE_MODEL_PATH)
        self.bert_drop_1 = nn.Dropout(p = 0.3)
        self.bert_drop_2 = nn.Dropout(p = 0.3)
        self.out_tag = nn.Linear(768, self.num_tag)
        self.out_pos = nn.Linear(768, self.num_pos)

    def forward(self, ids, mask, token_type_ids, target_pos, target_tag):
        o1, _ = self.bert(ids, 
                          attention_mask = mask,
                          token_type_ids = token_type_ids)

        bo_tag = self.bert_drop_1(o1)
        bo_pos = self.bert_drop_2(o1)

        tag = self.out_tag(bo_tag)
        pos = self.out_pos(bo_pos)

        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)
        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)

        loss = (loss_tag + loss_pos) / 2

        return tag, pos, loss 

print(&quot;model.py run success!&quot;)
</code></pre>
","13540652","","","","","2021-08-21 03:47:05","Python: BERT Error - Some weights of the model checkpoint at were not used when initializing BertModel","<python><nlp><pytorch><bert-language-model><huggingface-transformers>","2","5","","","","CC BY-SA 4.0"
"67511285","1","","","2021-05-12 21:38:05","","1","167","<p>I'm using a BERT model for Extractive QA task with the <code>transformers</code> class library <code>BertForQuestionAnswering</code>.  Extractive Question Answering is the task of answering a question for a given context text and outputting the start and end indexes of where the answer matches in the context. <a href=""https://github.com/loretoparisi/hf-experiments/blob/master/src/bert/run.py"" rel=""nofollow noreferrer"">My code</a> is the following:</p>
<pre><code>model = BertForQuestionAnswering.from_pretrained('bert-base-uncased',
    cache_dir=os.getenv(&quot;cache_dir&quot;, &quot;../../models&quot;))
question = &quot;What is the capital of Italy?&quot;
text = &quot;The capital of Italy is Rome.&quot;
inputs = tokenizer.encode_plus(question, text, return_tensors='pt')
start, end = model(**inputs)
start_max = torch.argmax(F.softmax(start, dim = -1))
end_max = torch.argmax(F.softmax(end, dim = -1)) + 1 ## add one ##because of python list indexing
answer = tokenizer.decode(inputs[&quot;input_ids&quot;][0][start_max : end_max])
print(answer)
</code></pre>
<p>I get this error</p>
<pre><code>start_max = torch.argmax(F.softmax(start, dim = -1))
  File &quot;/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py&quot;, line 1583, in softmax
    ret = input.softmax(dim)
AttributeError: 'str' object has no attribute 'softmax'
</code></pre>
<p>I have also tried this approach, slightly different</p>
<pre><code>encoding = tokenizer.encode_plus(text=question,text_pair=text, add_special=True)
inputs = encoding['input_ids']  #Token embeddings
sentence_embedding = encoding['token_type_ids']  #Segment embeddings
tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens
start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))
start_index = torch.argmax(start_scores)
end_index = torch.argmax(end_scores)
answer = ' '.join(tokens[start_index:end_index+1])
</code></pre>
<p>but the error is likely the same:</p>
<pre><code>    start_index = torch.argmax(start_scores)
TypeError: argmax(): argument 'input' (position 1) must be Tensor, not str
</code></pre>
<p>I assume due to the unpack of the output as</p>
<pre><code>start, end = model(**inputs)
</code></pre>
<p>If so, how to correct unpack this model's outputs?</p>
","758836","","758836","","2021-05-12 22:08:22","2021-05-12 22:08:22","Using transformers class BertForQuestionAnswering for Extractive Question Answering","<python><bert-language-model><huggingface-transformers>","0","2","1","","","CC BY-SA 4.0"
"67567587","1","67567885","","2021-05-17 10:03:29","","-2","218","<p>I am working on the <code>bert-base-mutilingual-uncased</code> model but when I try to set the <code>TOKENIZER</code> in the <code>config</code> it throws an <code>OSError</code>.</p>
<h3>Model Config</h3>
<pre><code>class config: 
    DEVICE = &quot;cuda:0&quot;
    MAX_LEN = 256
    TRAIN_BATCH_SIZE = 8
    VALID_BATCH_SIZE = 4
    EPOCHS = 1

    BERT_PATH = {&quot;bert-base-multilingual-uncased&quot;: &quot;workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased&quot;}
    MODEL_PATH = &quot;workspace/data/jigsaw-multilingual/model.bin&quot;

    TOKENIZER = transformers.BertTokenizer.from_pretrained(
            BERT_PATH[&quot;bert-base-multilingual-uncased&quot;], 
            do_lower_case=True)
</code></pre>
<h3>Error</h3>
<pre><code>    ---------------------------------------------------------------------------
    OSError                                   Traceback (most recent call last)
    &lt;ipython-input-33-83880b6b788e&gt; in &lt;module&gt;
    ----&gt; 1 class config:
          2 #     def __init__(self):
          3 
          4         DEVICE = &quot;cuda:0&quot;
          5         MAX_LEN = 256
    
    &lt;ipython-input-33-83880b6b788e&gt; in config()
         11         TOKENIZER = transformers.BertTokenizer.from_pretrained(
         12             BERT_PATH[&quot;bert-base-multilingual-uncased&quot;],
    ---&gt; 13             do_lower_case=True)
    
    /opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, *inputs, **kwargs)
       1138 
       1139         &quot;&quot;&quot;
    -&gt; 1140         return cls._from_pretrained(*inputs, **kwargs)
       1141 
       1142     @classmethod
    
    /opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
       1244                     &quot;, &quot;.join(s3_models),
       1245                     pretrained_model_name_or_path,
    -&gt; 1246                     list(cls.vocab_files_names.values()),
       1247                 )
       1248             )
    
    OSError: Model name 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased' was not  
 found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking,   
bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc,   
bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1,     
wietsedv/bert-base-dutch-cased). 

We assumed 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased' was a path, a model   identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such  
 vocabulary files at this path or url.
</code></pre>
<p>As I can interpret the error, it says that the <code>vocab.txt</code> file was not found at the given location but actually its present.</p>
<p>Following are the files available in the <code>bert-base-multilingual-uncased</code> folder:</p>
<ul>
<li><code>config.json</code></li>
<li><code>pytorch_model.bin</code></li>
<li><code>vocab.txt</code></li>
</ul>
<p>I am new to working with <code>bert</code>, so I am not sure if there is a different way to define the tokenizer.</p>
","13540652","","","","","2021-05-17 10:24:27","Python: BERT Tokenizer cannot be loaded","<python><nlp><pytorch><bert-language-model><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"67513831","1","","","2021-05-13 04:17:30","","1","110","<p>I am working on molecule data with representation called SMILES. an example molecule string looks like <code>Cc1ccccc1N1C(=O)NC(=O)C(=Cc2cc(Br)c(N3CCOCC3)o2)C1=O</code>.</p>
<p>Now, I want a custom <code>Tokenizer</code> which can be used with Huggingface transformer APIs. I also donot want to use the existing tokenizer models like <code>BPE</code> etc. I want the SMILES string parsed through regex to give individual characters as tokens as follows:</p>
<pre><code>import re

SMI_REGEX_PATTERN = r&quot;&quot;&quot;(\[[^\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|
#|-|\+|\\|\/|:|~|@|\?|&gt;&gt;?|\*|\$|\%[0-9]{2}|[0-9])&quot;&quot;&quot;

regex = re.compile(SMI_REGEX_PATTERN)

molecule = 'Cc1ccccc1N1C(=O)NC(=O)C(=Cc2cc(Br)c(N3CCOCC3)o2)C1=O'
tokens = regex.findall(molecule)
</code></pre>
<p>It is fairly simple to do the above, but I need a tokenizer which works with, let's say <code>BERT</code> API of Huggingface. Also, I donot want to use lowercase conversion, but still use BERT.</p>
","1349516","","","","","2021-05-13 04:17:30","Custom huggingface Tokenizer with custom model","<python><nlp><huggingface-transformers><huggingface-tokenizers>","0","2","","","","CC BY-SA 4.0"
"67578328","1","","","2021-05-17 23:57:35","","0","79","<p>I am working on multiple-choice QA. I am using the official notebook of huggingface/transformers which is implemented for SWAG dataset.</p>
<p>I want to use it for other multiple-choice datasets. Therefore, I add some modifications related to dataset. all code is given in <a href=""https://colab.research.google.com/drive/1K2nKN7mP3IlQdNngdWMTMAsGaiHVlXrP"" rel=""nofollow noreferrer"">notebook</a>.</p>
<p>SWAG dataset contains following columns including 'label'.</p>
<pre><code> train: Dataset({
        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],
        num_rows: 73546
    })
</code></pre>
<p>The dataset that I want to use has the following columns including 'answerKey' for target.</p>
<pre><code>train: Dataset({
        features: ['id', 'question_stem', 'choices', 'answerKey'],
        num_rows: 4957
    })
</code></pre>
<p>The error is given in dataloader which is</p>
<pre><code>@dataclass
class DataCollatorForMultipleChoice:
    &quot;&quot;&quot;
    Data collator that will dynamically pad the inputs for multiple choice received.
    &quot;&quot;&quot;

    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = True
    max_length: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None

    def __call__(self, features):
        print(features[0].keys())
        label_name = &quot;label&quot; if &quot;label&quot; in features[0].keys() else &quot;labels&quot;
        labels = [feature.pop(label_name) for feature in features]
        batch_size = len(features)
        num_choices = len(features[0][&quot;input_ids&quot;])
        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]
        flattened_features = sum(flattened_features, [])
        
        batch = self.tokenizer.pad(
            flattened_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors=&quot;pt&quot;,
        )
        
        # Un-flatten
        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}
        # Add back labels
        batch[&quot;labels&quot;] = torch.tensor(labels, dtype=torch.int64)
        return batch
</code></pre>
<p>It is given the error in the following line:</p>
<pre><code>label_name = &quot;label&quot; if &quot;label&quot; in features[0].keys() else &quot;labels&quot;
        labels = [feature.pop(label_name) for feature in features]
</code></pre>
<p>the error is obtained in trainer.train()</p>
<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-64-3435b262f1ae&gt; in &lt;module&gt;()
----&gt; 1 trainer.train()

5 frames
&lt;ipython-input-60-d1262e974b03&gt; in &lt;listcomp&gt;(.0)
     18         print(features[0].keys())
     19         label_name = &quot;label&quot; if &quot;label&quot; in features[0].keys() else &quot;labels&quot;
---&gt; 20         labels = [feature.pop(label_name) for feature in features]
     21         batch_size = len(features)
     22         num_choices = len(features[0][&quot;input_ids&quot;])

KeyError: 'labels'
</code></pre>
<p>I don't know what causes the error. I think it is related to target keys. But I could not solve it. Any ideas?</p>
<p>Thanks,</p>
","14301445","","","","","2021-05-17 23:57:35","DataCollatorForMultipleChoice gives KeyError: 'labels' in trainer.train","<pytorch><huggingface-transformers><huggingface-tokenizers><pytorch-dataloader>","0","2","","","","CC BY-SA 4.0"
"67517130","1","","","2021-05-13 09:47:38","","0","138","<p>I want to add a regression layer following the last layer of pretrained BERT model.
So it should be something like:</p>
<pre><code>output = bert_model([input_ids,attention_masks])
output = output[1]
output = tf.keras.layers.Dense(100,activation='relu')(output)
</code></pre>
<p>But I don't find how I can freeze the BERT model and train only the regression layer.
Is it possible?</p>
","6057371","","","","","2021-05-18 09:52:07","HuggingFace transformer how to freeze base tranformer after adding additional keras layer","<keras><deep-learning><nlp><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67580591","1","","","2021-05-18 05:48:59","","1","148","<p>After updating the tokenizer, when I run this line:</p>
<p>dataset = LineByLineTextDataset(tokenizer=bert_tokenizer, file_path=&quot;./some_file.txt&quot;,
block_size=128,)</p>
<p>it keeps on loading forever.</p>
<p>Here is the complete code:</p>
<pre><code>from transformers import BertTokenizer, BertForMaskedLM

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)

new_tokens = []
text = open(&quot;parsed_data.txt&quot;, &quot;r&quot;)
for line in text:
        for word in line.split():
           new_tokens.append(word)   

print(len(bert_tokenizer))  # 30522
bert_tokenizer.add_tokens(new_tokens)
model.resize_token_embeddings(len(bert_tokenizer))
print(type(new_tokens))
print(len(new_tokens))      # 53966
print(len(bert_tokenizer))  # 36369

from transformers import LineByLineTextDataset
dataset = LineByLineTextDataset(
    tokenizer=bert_tokenizer,
    file_path=&quot;./parsed_data.txt&quot;,
    block_size=128,
)
</code></pre>
<p>parsed_data.txt file contains simple text.</p>
<p>same issue was posted by someone before.
Link: github.com/huggingface/transformers/issues/5944</p>
","6805178","","6664872","","2021-05-25 22:38:29","2021-05-25 22:38:29","After updating the tokenizer, LineByLineTextDataset keep on loading forever","<python><nlp><bert-language-model><huggingface-transformers>","0","8","","","","CC BY-SA 4.0"
"67595060","1","","","2021-05-18 23:14:33","","0","223","<p>When I try the huggingface models and it gives the following error message:</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)
inputs = tokenizer(&quot;Hello world!&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
</code></pre>
<p>And the error message:</p>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
<p>My purpose is to find a pretrained model to create embedding vectors for my text, so that it can be used in downstream text. I don't want to create my own pretrained models to generate the embedding vector. In this case, can I ignore those warning messages, or I need to continue to train on my own data? In another post I learn that &quot;Most of the official models don't have pretrained output layers. The weights are randomly initialized. You need to train them for your task.&quot;  My understanding is that I don't need to train if I just want to get generic embedding vector for my text based on the public models, like Huggingface. Is that right?</p>
<p>I am new to transformer and please comment.</p>
","3943868","","","","","2021-05-20 11:35:21","Do I need to train on my own data in using bert model as an embedding vector?","<bert-language-model><huggingface-transformers><transformer>","1","0","","","","CC BY-SA 4.0"
"67511800","1","67511877","","2021-05-12 22:35:14","","2","199","<p>I am getting this error &quot;PipelineException: No mask_token ([MASK]) found on the input&quot;
when I run this line.
fill_mask(&quot;Auto Car .&quot;)</p>
<p>I am running it on Colab.
My Code:</p>
<pre><code>from transformers import BertTokenizer, BertForMaskedLM
from pathlib import Path
from tokenizers import ByteLevelBPETokenizer
from transformers import BertTokenizer, BertForMaskedLM


paths = [str(x) for x in Path(&quot;.&quot;).glob(&quot;**/*.txt&quot;)]
print(paths)

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

from transformers import BertModel, BertConfig

configuration = BertConfig()
model = BertModel(configuration)
configuration = model.config
print(configuration)

model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)

from transformers import LineByLineTextDataset
dataset = LineByLineTextDataset(
    tokenizer=bert_tokenizer,
    file_path=&quot;./kant.txt&quot;,
    block_size=128,
)

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15
)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=&quot;./KantaiBERT&quot;,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
    )

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()

from transformers import pipeline

fill_mask = pipeline(
    &quot;fill-mask&quot;,
    model=model,
    tokenizer=bert_tokenizer,
    device=0,
)

fill_mask(&quot;Auto Car &lt;mask&gt;.&quot;).     # This line is giving me the error...
</code></pre>
<p>The last line is giving me the error mentioned above. Please let me know what I am doing wrong or what I have to do in order to remove this error.</p>
<p>Complete error: &quot;f&quot;No mask_token ({self.tokenizer.mask_token}) found on the input&quot;,&quot;</p>
","6805178","","","","","2021-07-27 09:09:28","PipelineException: No mask_token ([MASK]) found on the input","<python><nlp><pytorch><bert-language-model><huggingface-transformers>","2","1","","","","CC BY-SA 4.0"
"59851553","1","","","2020-01-22 01:47:14","","4","1099","<p>I'm trying to fine-tune a Huggingface transformers BERT model on TPU. It works in Colab but fails when I switch to a paid TPU on GCP. Jupyter notebook code is as follows:</p>

<pre><code>[1] model = transformers.TFBertModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
# works
[2] cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
    tpu='[My TPU]',
    zone='us-central1-a',
    project='[My Project]'
)
tf.config.experimental_connect_to_cluster(cluster_resolver)
tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)
#Also works. Got a bunch of startup messages from the TPU - all good.

[3] with tpu_strategy.scope():
    model = TFBertModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
#Generates the error below (long). Same line works in Colab.
</code></pre>

<p>Here's the error message:</p>

<pre><code>NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-14-2cfc1a238903&gt; in &lt;module&gt;
      1 with tpu_strategy.scope():
----&gt; 2     model = TFBertModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

~/.local/lib/python3.5/site-packages/transformers/modeling_tf_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    309             return load_pytorch_checkpoint_in_tf2_model(model, resolved_archive_file, allow_missing_keys=True)
    310 
--&gt; 311         ret = model(model.dummy_inputs, training=False)  # build the network with dummy inputs
    312 
    313         assert os.path.isfile(resolved_archive_file), ""Error retrieving file {}"".format(resolved_archive_file)

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--&gt; 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

~/.local/lib/python3.5/site-packages/transformers/modeling_tf_bert.py in call(self, inputs, **kwargs)
    688 
    689     def call(self, inputs, **kwargs):
--&gt; 690         outputs = self.bert(inputs, **kwargs)
    691         return outputs
    692 

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--&gt; 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

~/.local/lib/python3.5/site-packages/transformers/modeling_tf_bert.py in call(self, inputs, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, training)
    548 
    549         embedding_output = self.embeddings([input_ids, position_ids, token_type_ids, inputs_embeds], training=training)
--&gt; 550         encoder_outputs = self.encoder([embedding_output, extended_attention_mask, head_mask], training=training)
    551 
    552         sequence_output = encoder_outputs[0]

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--&gt; 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

~/.local/lib/python3.5/site-packages/transformers/modeling_tf_bert.py in call(self, inputs, training)
    365                 all_hidden_states = all_hidden_states + (hidden_states,)
    366 
--&gt; 367             layer_outputs = layer_module([hidden_states, attention_mask, head_mask[i]], training=training)
    368             hidden_states = layer_outputs[0]
    369 

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--&gt; 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

~/.local/lib/python3.5/site-packages/transformers/modeling_tf_bert.py in call(self, inputs, training)
    341         hidden_states, attention_mask, head_mask = inputs
    342 
--&gt; 343         attention_outputs = self.attention([hidden_states, attention_mask, head_mask], training=training)
    344         attention_output = attention_outputs[0]
    345         intermediate_output = self.intermediate(attention_output)

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--&gt; 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

~/.local/lib/python3.5/site-packages/transformers/modeling_tf_bert.py in call(self, inputs, training)
    290         input_tensor, attention_mask, head_mask = inputs
    291 
--&gt; 292         self_outputs = self.self_attention([input_tensor, attention_mask, head_mask], training=training)
    293         attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)
    294         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--&gt; 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

~/.local/lib/python3.5/site-packages/transformers/modeling_tf_bert.py in call(self, inputs, training)
    222 
    223         batch_size = shape_list(hidden_states)[0]
--&gt; 224         mixed_query_layer = self.query(hidden_states)
    225         mixed_key_layer = self.key(hidden_states)
    226         mixed_value_layer = self.value(hidden_states)

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--&gt; 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/layers/core.py in call(self, inputs)
   1142         outputs = gen_math_ops.mat_mul(inputs, self.kernel)
   1143     if self.use_bias:
-&gt; 1144       outputs = nn.bias_add(outputs, self.bias)
   1145     if self.activation is not None:
   1146       return self.activation(outputs)  # pylint: disable=not-callable

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_ops.py in bias_add(value, bias, data_format, name)
   2756     else:
   2757       return gen_nn_ops.bias_add(
-&gt; 2758           value, bias, data_format=data_format, name=name)
   2759 
   2760 

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py in bias_add(value, bias, data_format, name)
    675       try:
    676         return bias_add_eager_fallback(
--&gt; 677             value, bias, data_format=data_format, name=name, ctx=_ctx)
    678       except _core._SymbolicException:
    679         pass  # Add nodes to the TensorFlow graph.

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py in bias_add_eager_fallback(value, bias, data_format, name, ctx)
    703     data_format = ""NHWC""
    704   data_format = _execute.make_str(data_format, ""data_format"")
--&gt; 705   _attr_T, _inputs_T = _execute.args_to_matching_eager([value, bias], ctx)
    706   (value, bias) = _inputs_T
    707   _inputs_flat = [value, bias]

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/execute.py in args_to_matching_eager(l, ctx, default_dtype)
    265         dtype = ret[-1].dtype
    266   else:
--&gt; 267     ret = [ops.convert_to_tensor(t, dtype, ctx=ctx) for t in l]
    268 
    269   # TODO(slebedev): consider removing this as it leaks a Keras concept.

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/execute.py in &lt;listcomp&gt;(.0)
    265         dtype = ret[-1].dtype
    266   else:
--&gt; 267     ret = [ops.convert_to_tensor(t, dtype, ctx=ctx) for t in l]
    268 
    269   # TODO(slebedev): consider removing this as it leaks a Keras concept.

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1312 
   1313     if ret is None:
-&gt; 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1315 
   1316     if ret is NotImplemented:

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/values.py in _tensor_conversion_mirrored(var, dtype, name, as_ref)
   1174 # allowing instances of the class to be used as tensors.
   1175 def _tensor_conversion_mirrored(var, dtype=None, name=None, as_ref=False):
-&gt; 1176   return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access
   1177 
   1178 

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/values.py in _dense_var_to_tensor(self, dtype, name, as_ref)
    908     if _enclosing_tpu_context() is None:
    909       return super(TPUVariableMixin, self)._dense_var_to_tensor(
--&gt; 910           dtype=dtype, name=name, as_ref=as_ref)
    911     # pylint: enable=protected-access
    912     elif dtype is not None and dtype != self.dtype:

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/values.py in _dense_var_to_tensor(self, dtype, name, as_ref)
   1164     assert not as_ref
   1165     return ops.convert_to_tensor(
-&gt; 1166         self.get(), dtype=dtype, name=name, as_ref=as_ref)
   1167 
   1168   def _clone_with_new_values(self, new_values):

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/values.py in get(self, device)
    835   def get(self, device=None):
    836     if (_enclosing_tpu_context() is None) or (device is not None):
--&gt; 837       return super(TPUVariableMixin, self).get(device=device)
    838     else:
    839       raise NotImplementedError(

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/values.py in get(self, device)
    320         device = distribute_lib.get_update_device()
    321         if device is None:
--&gt; 322           return self._get_cross_replica()
    323     device = device_util.canonicalize(device)
    324     return self._device_map.select_for_device(self._values, device)

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/values.py in _get_cross_replica(self)
   1136     replica_id = self._device_map.replica_for_device(device)
   1137     if replica_id is None:
-&gt; 1138       return array_ops.identity(self.primary)
   1139     return array_ops.identity(self._values[replica_id])
   1140 

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--&gt; 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/array_ops.py in identity(input, name)
    265     # variables. Variables have correct handle data when graph building.
    266     input = ops.convert_to_tensor(input)
--&gt; 267   ret = gen_array_ops.identity(input, name=name)
    268   # Propagate handle data for happier shape inference for resource variables.
    269   if hasattr(input, ""_handle_data""):

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/gen_array_ops.py in identity(input, name)
   3824         pass  # Add nodes to the TensorFlow graph.
   3825     except _core._NotOkStatusException as e:
-&gt; 3826       _ops.raise_from_not_ok_status(e, name)
   3827   # Add nodes to the TensorFlow graph.
   3828   _, _, _op, _outputs = _op_def_library._apply_op_helper(

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6604   message = e.message + ("" name: "" + name if name is not None else """")
   6605   # pylint: disable=protected-access
-&gt; 6606   six.raise_from(core._status_to_exception(e.code, message), None)
   6607   # pylint: enable=protected-access
   6608 

/usr/local/lib/python3.5/dist-packages/six.py in raise_from(value, from_value)

NotFoundError: '_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-aa2fcfb7-w-0. One possible root cause is the client and server binaries are not built with the same version. Please make sure the operation or function is registered in the binary running in this process. [Op:Identity]
</code></pre>

<p>I posted this on the Huggingface github (<a href=""https://github.com/huggingface/transformers/issues/2572"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/2572</a>) and they suggest the TPU server version may not match the TPU client version, but a) I don't know how to check for that nor b) what to do about it. Suggestions appreciated.</p>
","9676571","","975097","","2020-04-25 00:00:31","2020-04-25 00:00:31","Huggingface Bert TPU fine-tuning works on Colab but not in GCP","<google-cloud-platform><google-colaboratory><huggingface-transformers><google-cloud-tpu><bert-language-model>","0","1","1","","","CC BY-SA 4.0"
"59955402","1","59965083","","2020-01-28 19:02:15","","1","258","<p>I'm looking into using a pretrained BERT ('bert-base-uncased') model to extract contextualised word-level encodings from a bunch sentences.</p>

<p>Wordpiece tokenisation breaks down some of the words in my input into subword units. Possibly a trivial question, but I was wondering what would be the most sensible way to combine output encodings for subword tokens into word-level encodings.</p>

<p>Is averaging subword encodings a reasonable way to go? If not, is there any better alternative?</p>
","12801373","","","","","2020-01-29 10:27:28","getting word-level encodings from sub-word tokens encodings","<nlp><tokenize><bert-language-model><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"59962851","1","","","2020-01-29 08:10:59","","1","684","<p>I wish to fine tune Huggingface's GPT-2 transformer model on my own text data. I want to do this on a Google Colab notebook. However, I have two problems. The first is that it doesn't seem to work. </p>

<p>I install the various bits and pieces via the Colab:</p>

<pre><code>!git clone https://github.com/huggingface/transformers
%cd transformers
!pip install .
!pip install -r ./examples/requirements.txt
</code></pre>

<p><a href=""https://huggingface.co/transformers/examples.html"" rel=""nofollow noreferrer"">Following the example</a>, I upload the suggested WikiText sample data to the for training and run the suggested CLI commands in the notebook.</p>

<pre><code>!export TRAIN_FILE=wiki.train.raw
!export TEST_FILE=wiki.test.raw

!python run_lm_finetuning.py \
    --output_dir=output \
    --model_type=gpt2 \
    --model_name_or_path=gpt2 \
    --do_train \
    --train_data_file=$TRAIN_FILE \
    --do_eval \
    --eval_data_file=$TEST_FILE
</code></pre>

<p>This chugs along for a bit, but then I get an assertion error:</p>

<pre><code>Traceback (most recent call last):
  File ""run_lm_finetuning.py"", line 790, in &lt;module&gt;
    main()
  File ""run_lm_finetuning.py"", line 735, in main
    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)
  File ""run_lm_finetuning.py"", line 149, in load_and_cache_examples
    return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)
  File ""run_lm_finetuning.py"", line 88, in __init__
    assert os.path.isfile(file_path)
AssertionError
</code></pre>

<p>I think this is to do with my training data? Note that both files in the same folder as the lm_finetuning.py script, so I'm not sure what the os.path issue might be.</p>

<pre><code>benchmarks.py     run_generation.py   summarization
contrib       run_glue.py         test_examples.py
distillation      run_lm_finetuning.py    tests_samples
hans          run_multiple_choice.py  utils_multiple_choice.py
mm-imdb       run_ner.py          utils_ner.py
pplm          run_squad.py        wiki.test.raw
README.md     run_tf_glue.py      wiki.test.tokens
requirements.txt  run_tf_ner.py       wiki.train.raw
run_bertology.py  run_xnli.py         wiki.train.tokens
</code></pre>

<p>My second problem is that, even if fine tuning did work, I don't know how to duplicate the results with my own text data. I can't open the WikiText raw files, so I have no idea what format they're in. Are the ordinary plain text? Are they tokenized in some way? If anyone can enlighten me on this it would would be really appreciated!</p>
","6289601","","","","","2020-01-30 18:49:29","Trouble fine tuning Huggingface GPT-2 on Colab -- Assertion error","<python-3.x><nlp><artificial-intelligence><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67561900","1","","","2021-05-16 21:59:03","","0","75","<p>I am trying to use transformers on commonsense_qa dataset. I want to use data as</p>
<blockquote>
<p>question, option1
question, option2, etc.
for each option I will pass question also for every input. For input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, I do the following.
max_length = 128</p>
</blockquote>
<pre><code>def convert_to_commonsense_qa_features(example_batch):
    num_examples = len(example_batch[&quot;question&quot;])
    num_choices = len(example_batch[&quot;choices&quot;][0][&quot;text&quot;])
    features, features2 = {}, {}
    labels2id = {char: i for i, char in enumerate(&quot;ABCDE&quot;)}
    for example_i in range(num_examples):
        choices_inputs = tokenizer.batch_encode_plus(
            list(zip(
                [example_batch[&quot;question&quot;][example_i]] * num_choices,
                example_batch[&quot;choices&quot;][example_i][&quot;text&quot;],
            )),
            max_length=max_length, pad_to_max_length=True,
        )
        for k, v in choices_inputs.items():
            if k not in features:
                features[k] = []
            features[k].append(v)
        choices_outputs = tokenizer.batch_encode_plus(
                      str(labels2id[example_batch[&quot;answerKey&quot;][example_i]]),
                      max_length=max_length, pad_to_max_length=True,
        )
        for k, v in choices_outputs.items():
            if k not in features2:
                features2[k] = []
            features2[k].append(v)
    labels2id = {char: i for i, char in enumerate(&quot;ABCDE&quot;)}

    features[&quot;decoder_input_ids&quot;] = features2[&quot;input_ids&quot;]
    features[&quot;decoder_attention_mask&quot;] = features2[&quot;attention_mask&quot;]
    # Dummy answers for test
    &quot;&quot;&quot;
    if example_batch[&quot;answerKey&quot;][0]:
        features[&quot;labels&quot;] = [labels2id[ans] for ans in example_batch[&quot;answerKey&quot;]]
    else:
        features[&quot;labels&quot;] = [0] * num_examples
    &quot;&quot;&quot; 
    features[&quot;labels&quot;] = features2[&quot;input_ids&quot;]  
    #print(features)
    return features

convert_func_dict = {
    &quot;commonsense_qa&quot;: convert_to_commonsense_qa_features,
}


columns_dict = {
    &quot;commonsense_qa&quot;: ['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'],
}

features_dict = {}
for task_name, dataset in dataset_dict.items():
    features_dict[task_name] = {}
    for phase, phase_dataset in dataset.items():
        print(phase, &quot; &quot;,phase_dataset)
        if phase!=&quot;test&quot;:
          features_dict[task_name][phase] = phase_dataset.map(
              convert_func_dict[task_name],
              batched=True,
              load_from_cache_file=False,
          )
          print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))
          features_dict[task_name][phase].set_format(
              type=&quot;torch&quot;, 
              columns=columns_dict[task_name],
          )
          print(task_name, phase, len(phase_dataset), len(features_dict[task_name][phase]))
</code></pre>
<p>and dataloader is</p>
<pre><code>class NLPDataCollator(DataCollator):
    &quot;&quot;&quot;
    Extending the existing DataCollator to work with NLP dataset batches
    &quot;&quot;&quot;
    def collate_batch(self, features: List[Union[InputDataClass, Dict]]) -&gt; Dict[str, torch.Tensor]:
        first = features[0]
        if isinstance(first, dict):
          # NLP data sets current works presents features as lists of dictionary
          # (one per example), so we  will adapt the collate_batch logic for that
          if &quot;labels&quot; in first and first[&quot;labels&quot;] is not None:
              if first[&quot;labels&quot;].dtype == torch.int64:
                  labels = torch.tensor([f[&quot;labels&quot;] for f in features], dtype=torch.long)
              else:
                  labels = torch.tensor([f[&quot;labels&quot;] for f in features], dtype=torch.float)
              batch = {&quot;labels&quot;: labels}
          for k, v in first.items():
              if k != &quot;labels&quot; and v is not None and not isinstance(v, str):
                  batch[k] = torch.stack([f[k] for f in features])
          return batch
        else:
          # otherwise, revert to using the default collate_batch
          return DefaultDataCollator().collate_batch(features)
</code></pre>
<p>trainer is as</p>
<pre><code>trainer = MultitaskTrainer(
    model=model,
    args=transformers.TrainingArguments(
        output_dir=&quot;./models/&quot;,
        overwrite_output_dir=True,
        learning_rate=1e-5,
        do_train=True,
        do_eval=True,
        num_train_epochs=3,
        # Adjust batch size if this doesn't fit on the Colab GPU
        per_device_train_batch_size=8,  
        save_steps=3000,
    ),
    data_collator=NLPDataCollator(),
    train_dataset=train_dataset,
    eval_dataset = valid_dataset,

)
trainer.train()
</code></pre>
<p>I get the following error:</p>
<pre><code>ValueError                                Traceback (most recent call last)

&lt;ipython-input-49-68181c1063c6&gt; in &lt;module&gt;()
     26 
     27 )
---&gt; 28 trainer.train()

8 frames

&lt;ipython-input-39-4359c3725689&gt; in collate_batch(self, features)
     20           if &quot;labels&quot; in first and first[&quot;labels&quot;] is not None:
     21               if first[&quot;labels&quot;].dtype == torch.int64:
---&gt; 22                   labels = torch.tensor([f[&quot;labels&quot;] for f in features], dtype=torch.long)
     23               else:
     24                   labels = torch.tensor([f[&quot;labels&quot;] for f in features], dtype=torch.float)

ValueError: only one element tensors can be converted to Python scalars
</code></pre>
","14301445","","","","","2021-05-16 21:59:03","in collate_batch(self, features)->ValueError: only one element tensors can be converted to Python scalars","<pytorch><huggingface-transformers><pytorch-dataloader>","0","0","","","","CC BY-SA 4.0"
"67570890","1","","","2021-05-17 13:42:03","","0","718","<p>I am trying to train a <code>bert-base-multilingual-uncased</code> model for a task. I have all the required files present in my dataset including the <code>config.json</code> bert file but when I run the model it gives an error.</p>
<h3>Config</h3>
<pre><code>class config:
    DEVICE = &quot;cuda:2&quot;
    MAX_LEN = 256
    TRAIN_BATCH_SIZE = 8
    VALID_BATCH_SIZE = 4
    EPOCHS = 1
    BERT_PATH = &quot;workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased&quot;
    MODEL_PATH = &quot;workspace/data/jigsaw-multilingual/model.bin&quot;
    TOKENIZER = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)
</code></pre>
<h3>Model</h3>
<pre><code>class BERTBaseUncased(nn.Module):
    def __init__(self):
        super(BERTBaseUncased, self).__init__()
        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH)
        self.bert_drop = nn.Dropout(0.3)
        self.out = nn.Linear(768 * 2, 1) # *2 since we have 2 pooling layers

    def forward(self, ids, mask, token_type_ids):
        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)
        
        mean_pooling = torch.mean(o1, 1)
        max_pooling, _ = torch.max(o1, 1)
        cat = torch.cat((mean_pooling, max_pooling), 1)
        
        bo = self.bert_drop(cat)
        output = self.out(bo)
        return output
</code></pre>
<h3>Error</h3>
<pre><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    241             if resolved_config_file is None:
--&gt; 242                 raise EnvironmentError
    243             config_dict = cls._dict_from_json_file(resolved_config_file)

OSError: 

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
&lt;ipython-input-64-9f2999c88020&gt; in &lt;module&gt;
     79 
     80 if __name__ == &quot;__main__&quot;:
---&gt; 81     run()

&lt;ipython-input-64-9f2999c88020&gt; in run()
     38 
     39     device = torch.device(config.DEVICE)
---&gt; 40     model = BERTBaseUncased()
     41     model.to(device)
     42 

&lt;ipython-input-60-8e1508eac60a&gt; in __init__(self)
      2     def __init__(self):
      3         super(BERTBaseUncased, self).__init__()
----&gt; 4         self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH)
      5         self.bert_drop = nn.Dropout(0.3)
      6         self.out = nn.Linear(768 * 2, 1) # *2 since we have 2 pooling layers

/opt/conda/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    601                 proxies=proxies,
    602                 local_files_only=local_files_only,
--&gt; 603                 **kwargs,
    604             )
    605         else:

/opt/conda/lib/python3.6/site-packages/transformers/configuration_utils.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    198 
    199         &quot;&quot;&quot;
--&gt; 200         config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
    201         return cls.from_dict(config_dict, **kwargs)
    202 

/opt/conda/lib/python3.6/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    249                 f&quot;- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\n\n&quot;
    250             )
--&gt; 251             raise EnvironmentError(msg)
    252 
    253         except json.JSONDecodeError:

OSError: Can't load config for 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased'. Make sure that:

- 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'workspace/data/jigsaw-multilingual/input/bert-base-multilingual-uncased' is the correct path to a directory containing a config.json file
</code></pre>
<p>These are the files present in my <code>bert</code> dataset:<br />
-&gt; <code>config.json</code><br />
-&gt;<code>pytorch_model.bin</code><br />
-&gt; <code>vocab.txt</code></p>
<p>How to fix this issue?</p>
","13540652","","","","","2021-05-17 13:42:03","Python: OSError can't load config for bert","<python><nlp><pytorch><bert-language-model><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"67590284","1","","","2021-05-18 16:26:54","","0","152","<p>I tried to fine-tune BERT for a classification downstream task.</p>
<p>Now I loaded the model again and I run into the following warning:</p>
<p>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']</p>
<ul>
<li>This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).</li>
<li>This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</li>
</ul>
<p>[Screen Shot][1]
[1]: https://i.stack.imgur.com/YJZVc.png</p>
<p>I already deleted and reinstalled transformers==4.6.0 but nothing helped.
I thought maybe through the parameter &quot;force_download=True&quot; it might get the original weights back but nothing helped.</p>
<p>Shall I continue and ignore the warning? Is there a way to delete the model checkpoints such when the model is downloaded the weights are fixed again?</p>
<p>Thanks in advance!</p>
<p>Best,
Alex</p>
","14706526","","","","","2021-05-20 10:48:21","Delete and Reinitialize pertained BERT weights / parameters","<nlp><pytorch><bert-language-model><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"67614781","1","","","2021-05-20 06:04:21","","0","36","<p>code:</p>
<pre><code>model = BertWSD.from_pretrained(model_dir)
tokenizer = BertTokenizer.from_pretrained(model_dir)
# add new special token
if '[TGT]' not in tokenizer.additional_special_tokens:
    tokenizer.add_special_tokens({'additional_special_tokens': ['[TGT]']})
    assert '[TGT]' in tokenizer.additional_special_tokens
    model.resize_token_embeddings(len(tokenizer))
    
model.to(DEVICE)
model.eval()
</code></pre>
<p>For some reason I keep getting a strange error whenever I run the code:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    625             try:
--&gt; 626                 state_dict = torch.load(resolved_archive_file, map_location=&quot;cpu&quot;)
    627             except Exception:

3 frames
RuntimeError: unexpected EOF, expected 5298350 more bytes. The file might be corrupted.

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    627             except Exception:
    628                 raise OSError(
--&gt; 629                     &quot;Unable to load weights from pytorch checkpoint file. &quot;
    630                     &quot;If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. &quot;
    631                 )

OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. 
</code></pre>
<p>Does anyone know why I'm getting this error?</p>
","15978417","","","","","2021-05-20 06:04:21","While I am running the below code I'm getting error :","<python><pytorch><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"67625349","1","","","2021-05-20 17:31:59","","2","395","<p>I am following this tutorial to learn about the trainer API.
<a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/training.html</a></p>
<p>I copied the code as below:</p>
<pre><code>from datasets import load_dataset

import numpy as np
from datasets import load_metric

metric = load_metric(&quot;accuracy&quot;)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

print('Download dataset ...')
raw_datasets = load_dataset(&quot;imdb&quot;)
from transformers import AutoTokenizer

print('Tokenize text ...')
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

print('Prepare data ...')
small_train_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42).select(range(500))
small_eval_dataset = tokenized_datasets[&quot;test&quot;].shuffle(seed=42).select(range(500))
full_train_dataset = tokenized_datasets[&quot;train&quot;]
full_eval_dataset = tokenized_datasets[&quot;test&quot;]

print('Define model ...')
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=2)

print('Define trainer ...')
from transformers import TrainingArguments, Trainer
training_args = TrainingArguments(&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

print('Fine-tune train ...')
trainer.evaluate()
</code></pre>
<p>However, it doesn't report anything about training metrics, but the following message:</p>
<pre><code>Download dataset ...
Reusing dataset imdb (/Users/congminmin/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)
Tokenize text ...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:06&lt;00:00,  4.01ba/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:06&lt;00:00,  3.99ba/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:13&lt;00:00,  3.73ba/s]
Prepare data ...
Define model ...
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Define trainer ...
Fine-tune train ...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [08:35&lt;00:00,  8.19s/it]

Process finished with exit code 0
</code></pre>
<p>Isn't the tutorial updated? should I make some configuration changes to report the metrics?</p>
","3943868","","","","","2021-05-26 20:30:57","Why doesn't trainer report evaluation metrics while training in the tutorial?","<python><huggingface-transformers><transformer>","2","0","","","","CC BY-SA 4.0"
"67542767","1","","","2021-05-15 02:24:19","","0","79","<pre><code>GLUE_TASKS = [&quot;cola&quot;, &quot;mnli&quot;, &quot;mnli-mm&quot;, &quot;mrpc&quot;, &quot;qnli&quot;, &quot;qqp&quot;, &quot;rte&quot;, &quot;sst2&quot;, &quot;stsb&quot;, &quot;wnli&quot;]

#useful in preprocessing, this sets what each task does. 
task_to_keys = {
    &quot;cola&quot;: (&quot;sentence&quot;, None),
    &quot;mnli&quot;: (&quot;premise&quot;, &quot;hypothesis&quot;),
    &quot;mnli-mm&quot;: (&quot;premise&quot;, &quot;hypothesis&quot;),
    &quot;mrpc&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;),
    &quot;qnli&quot;: (&quot;question&quot;, &quot;sentence&quot;),
    &quot;qqp&quot;: (&quot;question1&quot;, &quot;question2&quot;),
    &quot;rte&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;),
    &quot;sst2&quot;: (&quot;sentence&quot;, None),
    &quot;stsb&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;),
    &quot;wnli&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;),
}

def set_module_grad_status(module, flag=False):
    if isinstance(module, list):
        for m in module:
            set_module_grad_status(m, flag)
    else:
        for p in module.parameters():
            p.requires_grad = flag

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    if task != &quot;stsb&quot;:
        predictions = np.argmax(predictions, axis=1)
    else:
        predictions = predictions[:, 0]
    return metric.compute(predictions=predictions, references=labels)

#setting some model parameters
task = &quot;qnli&quot;
sentence1_key, sentence2_key = task_to_keys[task]
# model_checkpoint = &quot;test-glue/basemodel&quot;

num_labels = 3 if task.startswith(&quot;mnli&quot;) else 1 if task==&quot;stsb&quot; else 2

metric_name = &quot;pearson&quot; if task == &quot;stsb&quot; else &quot;matthews_correlation&quot; if task == &quot;cola&quot; else &quot;accuracy&quot;
validation_key = &quot;validation_mismatched&quot; if task == &quot;mnli-mm&quot; else &quot;validation_matched&quot; if task == &quot;mnli&quot; else &quot;validation&quot;

model_checkpoint = &quot;google/mobilebert-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)
model = model.to(device)

args = TrainingArguments(
    &quot;test-glue&quot;,
    evaluation_strategy = &quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=epochs,
    weight_decay=0.01,
    save_steps=0
    # load_best_model_at_end=True,
    # metric_for_best_model=metric_name,
)

#load dataset
actual_task = &quot;mnli&quot; if task == &quot;mnli-mm&quot; else task #some error checking.. 
dataset = load_dataset(&quot;glue&quot;, actual_task)
metric = load_metric('glue', actual_task)

# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
def preprocess_function(examples):
    if sentence2_key is None:
        return tokenizer(examples[sentence1_key], truncation=True)
    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)
encoded_dataset = dataset.map(preprocess_function, batched=True)


trainer = Trainer(
    model,
    args,
    train_dataset=encoded_dataset[&quot;train&quot;],
    eval_dataset=encoded_dataset[validation_key],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()

</code></pre>
<p>I not quite sure why I am getting the error &quot;RuntimeError: The size of tensor a (549) must match the size of tensor b (512) at non-singleton dimension 1&quot; at the like &quot;trainer.train()&quot;. Like isn't the each sample supposed to be consistently size of 512. This only happens with MobileBERT model and not on DistilBERT model. The code that I am using is mostly based on  the huggingface colab tutorial <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=7k8ge1L1IrJk"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=7k8ge1L1IrJk</a></p>
","13735618","","","","","2021-05-15 02:24:19","RuntimeError: The size of tensor a (549) must match the size of tensor b (512) at non-singleton dimension 1","<python><pytorch><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"59978959","1","","","2020-01-30 04:10:51","","12","2463","<p>I am trying to do binary text classification on custom data (which is in csv format) using different transformer architectures that Hugging Face 'Transformers' library offers. I am using this <a href=""https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html"" rel=""noreferrer"">Tensorflow blog post</a> as reference. </p>

<p>I am loading the custom dataset into 'tf.data.Dataset' format using the following code:</p>

<pre><code>def get_dataset(file_path, **kwargs):
   dataset = tf.data.experimental.make_csv_dataset(
     file_path,
     batch_size=5, # Artificially small to make examples easier to show.
     na_value="""",
     num_epochs=1,
     ignore_errors=True, 
     **kwargs)
   return dataset 
</code></pre>

<p>After this when I tried using the <a href=""https://huggingface.co/transformers/main_classes/processors.html?highlight=glue_convert_examples_to_features#transformers.data.processors.glue.glue_convert_examples_to_features"" rel=""noreferrer"">'glue_convert_examples_to_features'</a> method to tokenize as below:</p>

<pre><code>train_dataset = glue_convert_examples_to_features(
                           examples = train_data,
                           tokenizer = tokenizer, 
                           task = None,
                           label_list = ['0', '1'],
                           max_length = 128
                           )
</code></pre>

<p>which throws an error ""UnboundLocalError: local variable 'processor' referenced before assignment"" at: </p>

<pre><code> if is_tf_dataset:
    example = processor.get_example_from_tensor_dict(example)
    example = processor.tfds_map(example)
</code></pre>

<p>In all the examples, I see that they are using the tasks like 'mrpc' etc which are pre-defined and have a glue_processor to handle. Error raises at the 'line 85' in <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/data/processors/glue.py"" rel=""noreferrer"">source code</a>.</p>

<p>Can anyone help with solving this issue using with 'custom data' ?</p>
","11444268","","","","","2020-08-20 08:33:03","How to use Hugging Face Transformers library in Tensorflow for text classification on custom data?","<python><tensorflow><text-classification><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"67638118","1","","","2021-05-21 13:48:26","","2","251","<p>I'm building a Named Entity Recognition (NER) model using the Hugging Face implementation of emilyalsentzer/Bio_ClinicalBERT.  Up to today, I've had no issues with the model.  I'm hopeful that someone can help me understand why it's currently not working as expected.</p>
<p>Question 1 - today, trying to train using:</p>
<pre><code>MODEL_NAME = 'emilyalsentzer/Bio_ClinicalBERT'
model = text.sequence_tagger('bilstm-bert', preproc, bert_model=MODEL_NAME)
</code></pre>
<p>results in this error:
404 Client Error: Not Found for url: <a href=""https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/tf_model.h5"" rel=""nofollow noreferrer"">https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/tf_model.h5</a></p>
<p>Does Hugging Face offer any kind of health check to ascertain the status of their models?</p>
<p>Question 2 - working with files (model.h5, model.json, and preproc.sav) I'd saved from earlier training iterations, I'm getting the same 404 error shown above.  I don't understand wherein these files the call to Hugging Face is occurring.  It doesn't seem to be in the .json, and the .h5 and .sav file formats are hard to inspect.  Read more about what these files are:
<a href=""https://medium.com/analytics-vidhya/how-to-deploy-your-neural-network-model-using-ktrain-ae255b134c77"" rel=""nofollow noreferrer"">https://medium.com/analytics-vidhya/how-to-deploy-your-neural-network-model-using-ktrain-ae255b134c77</a></p>
<p>Back in February, I'd used these exact model.h5, model.json, and preproc.sav files to run the NER app using Streamlit, no problem.  Not sure if this is temporary issue with Bio_ClinicalBERT or if I need to retool my original approach due to potentially permanent problems with this transformer model.</p>
","11610317","","","","","2021-07-28 17:57:08","Hugging Face model Bio_ClinicalBERT producing 404 error","<python><nlp><huggingface-transformers>","2","0","1","","","CC BY-SA 4.0"
"69343395","1","69352027","","2021-09-27 08:25:53","","0","45","<p>I'm trying to load model from Hugging Face and I downloaded h5 model from here: <a href=""https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/tree/main</a></p>
<pre><code>from flask import Flask, jsonify, request  # import objects from the Flask model
from keras.models import load_model
from transformers import AutoTokenizer, AutoModelForSequenceClassification,TextClassificationPipeline

model = load_model('./tf_model.h5') # trying to load model here
</code></pre>
<p>And the error shows up:</p>
<pre><code>File &quot;C:\D\Learning\Flask\flask-pp-rest\main.py&quot;, line 11, in &lt;module&gt;
    model = load_model('./tf_model.h5') File &quot;C:\Users\ndrez\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\saving\save.py&quot;,
line 200, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, File
&quot;C:\Users\ndrez\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\saving\hdf5_format.py&quot;,
line 176, in load_model_from_hdf5
    raise ValueError('No model found in config file.') ValueError: **No model found in config file.**
</code></pre>
<p>Does anyone know how to solve this? If you know please help me out. I will monitor this question and try to implement your solution's answer.</p>
","6318410","","982257","","2021-09-27 12:46:45","2021-09-27 19:04:41","Hugging Face H5 load model error : No model found in config file","<python><tensorflow><keras><huggingface-transformers>","2","1","","","","CC BY-SA 4.0"
"69357881","1","","","2021-09-28 07:59:38","","0","22","<pre><code>from transformers import CTRLTokenizer, TFCTRLLMHeadModel
tokenizer_ctrl = CTRLTokenizer.from_pretrained('ctrl', cache_dir='./cache', local_files_only=True)
model_ctrl = TFCTRLLMHeadModel.from_pretrained('ctrl', cache_dir='./cache', local_files_only=True)
print(tokenizer_ctrl)
gen_nlp  = pipeline(&quot;text-generation&quot;, model=model_ctrl, tokenizer=tokenizer_ctrl, device=1, return_full_text=False)
</code></pre>
<p>Hello, my codes can load the transformer model, for example, CTRL here, into the gpu memory.
How to remove it from GPU after usage, to free more gpu memory?</p>
<p>show I use <code>torch.cuda.empty_cache()</code> ?</p>
<p>Thanks.</p>
","6407393","","","","","2021-09-28 10:00:47","How to remove the model of transformers in GPU memory","<pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"59961023","1","","","2020-01-29 05:36:17","","2","125","<p>I am reviewing huggingface's version of Albert. </p>

<p>However, I cannot find any code or comment about SOP.</p>

<p>I can find NSP(Next Sentence Prediction) implementation from modeling_from <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py"" rel=""nofollow noreferrer"">src/transformers/modeling_bert.py</a>.</p>

<pre><code>if masked_lm_labels is not None and next_sentence_label is not None:
    loss_fct = CrossEntropyLoss()
    masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))
    next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))
    total_loss = masked_lm_loss + next_sentence_loss
    outputs = (total_loss,) + outputs
</code></pre>

<p>Is SOP inherited from here with SOP-style labeling? or Is there anything I am missing?</p>
","2909599","","","","","2021-09-22 13:35:31","is SOP(sentence order prediction) implemented?","<huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"60001698","1","","","2020-01-31 10:02:31","","6","4343","<p>I wanted to employ the <code>examples/run_lm_finetuning.py</code> from the <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">Huggingface Transformers repository</a> on a pretrained Bert model. However, from following the documentation it is not evident how a corpus file should be structured (apart from referencing the Wiki-2 dataset). I've tried</p>

<ul>
<li>One document per line (multiple sentences)</li>
<li>One sentence per line. Documents are separated by a blank line (this I found in some older pytorch-transformers documentation)</li>
</ul>

<p>By looking at the code of <code>examples/run_lm_finetuning.py</code> it is not directly evident how sequence pairs for the Next Sentence Prediction objective are formed. Would the <code>--line-by-line</code> option help here? I'd be grateful, if someone could give me some hints how a text corpus file should look like.</p>

<p>Many thanks and cheers,</p>

<p>nminds</p>
","12818675","","975097","","2020-04-24 23:58:50","2020-07-17 07:48:15","How exactly should the input file be formatted for the language model finetuning (BERT through Huggingface Transformers)?","<python><pytorch><huggingface-transformers><bert-language-model>","1","0","2","","","CC BY-SA 4.0"
"67668402","1","","","2021-05-24 07:48:26","","2","106","<p>I am testing this piece of code:</p>
<pre><code>from transformers import BertTokenizer, BertModel, BertForMaskedLM

tokenizer = BertTokenizer.from_pretrained(&quot;hfl/chinese-roberta-wwm-ext&quot;)
model = BertForMaskedLM.from_pretrained(&quot;hfl/chinese-roberta-wwm-ext&quot;)

from transformers import pipeline

def check_model(model, tokenizer):
    fill_mask = pipeline(
        &quot;fill-mask&quot;,
        model=model,
        tokenizer=tokenizer
    )
    print('Fill blank: ')
    fill_mask(&quot;æˆ‘å–œæ¬¢ {nlp.tokenizer.mask_token}.&quot;)

    print('Fill blank: ')
    fill_mask(&quot;è¿™ä¸ªå“ç‰Œçš„é¢è†œ {nlp.tokenizer.mask_token}.&quot;)

print('Check model ...')
check_model(model, tokenizer)
</code></pre>
<p>But it prints out this error message:</p>
<pre><code>raceback (most recent call last):
  File &quot;/Users/congminmin/nlp/embedding/transformer/bert_roberta_wwm_test.py&quot;, line 21, in &lt;module&gt;
    check_model(model, tokenizer)
  File &quot;/Users/congminmin/nlp/embedding/transformer/bert_roberta_wwm_test.py&quot;, line 15, in check_model
    fill_mask(&quot;æˆ‘å–œæ¬¢ {nlp.tokenizer.mask_token}.&quot;)
  File &quot;/Users/congminmin/.venv/wbkg/lib/python3.7/site-packages/transformers/pipelines/fill_mask.py&quot;, line 162, in __call__
    self.ensure_exactly_one_mask_token(masked_index.numpy())
  File &quot;/Users/congminmin/.venv/wbkg/lib/python3.7/site-packages/transformers/pipelines/fill_mask.py&quot;, line 90, in ensure_exactly_one_mask_token
    f&quot;No mask_token ({self.tokenizer.mask_token}) found on the input&quot;,
transformers.pipelines.base.PipelineException: No mask_token ([MASK]) found on the input
</code></pre>
","3943868","","","","","2021-05-26 16:07:08","Why doesn't BertForMaskedLM generate right masked tokens?","<nlp><bert-language-model><huggingface-transformers><transformer>","1","0","","","","CC BY-SA 4.0"
"67691530","1","","","2021-05-25 15:55:42","","1","183","<p>I am trying to fine tune the T5 transformer for summarization but I am receiving a key error message:</p>
<pre><code>KeyError: 'Indexing with integers (to access backend Encoding for a given batch index) is not available when using Python based tokenizers'
</code></pre>
<p>The code I am using is basically this:</p>
<pre><code>model_name = '...'
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)
model.to(device)

(...)

df_dataset = df_dataset[['summary','document']]
df_dataset.document = 'summarize: ' + df_dataset.document

X = list(df_dataset['document'])
y = list(df_dataset['summary'])
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)
y_train_tokenized = tokenizer(y_train, padding=True, truncation=True, max_length=512)
X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)
y_val_tokenized = tokenizer(y_val, padding=True, truncation=True, max_length=512)

# Create torch dataset
class Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels) 

training_set = Dataset(X_train_tokenized, y_train_tokenized)
validation_set = Dataset(X_val_tokenized, y_val_tokenized)

# Define Trainer
args = TrainingArguments(
    output_dir=&quot;output&quot;,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=500,
    per_device_train_batch_size=TRAIN_BATCH_SIZE,
    per_device_eval_batch_size=VALID_BATCH_SIZE,
    num_train_epochs=TRAIN_EPOCHS,
    save_steps=3000,
    load_best_model_at_end = True,    
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=training_set,
    eval_dataset=validation_set,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)

trainer.train()
</code></pre>
<p>And the full error:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-29-f31e4c5cde21&gt; in &lt;module&gt;
      1 # Train pre-trained model
----&gt; 2 trainer.train()

c:\programdata\anaconda3\envs\summa\lib\site-packages\transformers\trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
   1099             self.control = self.callback_handler.on_epoch_begin(self.args, self.state, self.control)
   1100 
-&gt; 1101             for step, inputs in enumerate(epoch_iterator):
   1102 
   1103                 # Skip past any already trained steps if resuming training

c:\programdata\anaconda3\envs\summa\lib\site-packages\torch\utils\data\dataloader.py in __next__(self)
    515             if self._sampler_iter is None:
    516                 self._reset()
--&gt; 517             data = self._next_data()
    518             self._num_yielded += 1
    519             if self._dataset_kind == _DatasetKind.Iterable and \

c:\programdata\anaconda3\envs\summa\lib\site-packages\torch\utils\data\dataloader.py in _next_data(self)
    555     def _next_data(self):
    556         index = self._next_index()  # may raise StopIteration
--&gt; 557         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    558         if self._pin_memory:
    559             data = _utils.pin_memory.pin_memory(data)

c:\programdata\anaconda3\envs\summa\lib\site-packages\torch\utils\data\_utils\fetch.py in fetch(self, possibly_batched_index)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]

c:\programdata\anaconda3\envs\summa\lib\site-packages\torch\utils\data\_utils\fetch.py in &lt;listcomp&gt;(.0)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]

&lt;ipython-input-24-67979e648b75&gt; in __getitem__(self, idx)
      7     def __getitem__(self, idx):
      8         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
----&gt; 9         item['labels'] = torch.tensor(self.labels[idx])
     10         return item
     11 

c:\programdata\anaconda3\envs\summa\lib\site-packages\transformers\tokenization_utils_base.py in __getitem__(self, item)
    232             return self._encodings[item]
    233         else:
--&gt; 234             raise KeyError(
    235                 &quot;Indexing with integers (to access backend Encoding for a given batch index) &quot;
    236                 &quot;is not available when using Python based tokenizers&quot;

KeyError: 'Indexing with integers (to access backend Encoding for a given batch index) is not available when using Python based tokenizers'
</code></pre>
<p>And if change the line:</p>
<pre><code>tokenizer = T5Tokenizer.from_pretrained(model_name)
</code></pre>
<p>To:</p>
<pre><code>tokenizer = T5TokenizerFast.from_pretrained(model_name)
</code></pre>
<p>the error changes to:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-28-f31e4c5cde21&gt; in &lt;module&gt;
      1 # Train pre-trained model
----&gt; 2 trainer.train()

c:\programdata\anaconda3\envs\summa\lib\site-packages\transformers\trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
   1099             self.control = self.callback_handler.on_epoch_begin(self.args, self.state, self.control)
   1100 
-&gt; 1101             for step, inputs in enumerate(epoch_iterator):
   1102 
   1103                 # Skip past any already trained steps if resuming training

c:\programdata\anaconda3\envs\summa\lib\site-packages\torch\utils\data\dataloader.py in __next__(self)
    515             if self._sampler_iter is None:
    516                 self._reset()
--&gt; 517             data = self._next_data()
    518             self._num_yielded += 1
    519             if self._dataset_kind == _DatasetKind.Iterable and \

c:\programdata\anaconda3\envs\summa\lib\site-packages\torch\utils\data\dataloader.py in _next_data(self)
    555     def _next_data(self):
    556         index = self._next_index()  # may raise StopIteration
--&gt; 557         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    558         if self._pin_memory:
    559             data = _utils.pin_memory.pin_memory(data)

c:\programdata\anaconda3\envs\summa\lib\site-packages\torch\utils\data\_utils\fetch.py in fetch(self, possibly_batched_index)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]

c:\programdata\anaconda3\envs\summa\lib\site-packages\torch\utils\data\_utils\fetch.py in &lt;listcomp&gt;(.0)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]

&lt;ipython-input-23-67979e648b75&gt; in __getitem__(self, idx)
      7     def __getitem__(self, idx):
      8         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
----&gt; 9         item['labels'] = torch.tensor(self.labels[idx])
     10         return item
     11 

RuntimeError: Could not infer dtype of tokenizers.Encoding
</code></pre>
<p>Any idea of what is wrong?</p>
","9942391","","9942391","","2021-05-25 18:58:34","2021-07-20 19:03:40","Key Error while fine tunning T5 for summarization with HuggingFace","<python><huggingface-transformers><huggingface-tokenizers>","1","4","","","","CC BY-SA 4.0"
"67699354","1","67712401","","2021-05-26 06:07:27","","1","78","<p>I am testing Bert base and Bert distilled model in Huggingface with 4 scenarios of speeds, batch_size = 1:</p>
<pre><code>1) bert-base-uncased: 154ms per request
2) bert-base-uncased with quantifization: 94ms per request
3) distilbert-base-uncased: 86ms per request
4) distilbert-base-uncased with quantifization: 69ms per request
</code></pre>
<p>I am using the IMDB text as experimental data and set the max_length=512, so it's quite long. The cpu on Ubuntu 18.04 info is below:</p>
<pre><code>cat /proc/cpuinfo  | grep 'name'| uniq
model name  : Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz
</code></pre>
<p>The machine has 3 GPU available for use:</p>
<pre><code>Tesla V100-SXM2
</code></pre>
<p>It seems quite slow for realtime application. Are those speeds normal for bert base model?</p>
<p>The testing code is below:</p>
<pre><code>import pandas as pd
import torch.quantization

from transformers import AutoTokenizer, AutoModel, DistilBertTokenizer, DistilBertModel

def get_embedding(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors=&quot;pt&quot;, max_length=512, truncation=True)
    outputs = model(**inputs)
    output_tensors = outputs[0][0]
    output_numpy = output_tensors.detach().numpy()
    embedding = output_numpy.tolist()[0]

def process_text(model, tokenizer, text_lines):
    for index, line in enumerate(text_lines):
        embedding = get_embedding(model, tokenizer, line)
        if index % 100 == 0:
            print('Current index: {}'.format(index))

import time
from datetime import timedelta
if __name__ == &quot;__main__&quot;:

    df = pd.read_csv('../data/train.csv', sep='\t')
    df = df.head(1000)
    text_lines = df['review']
    text_line_count = len(text_lines)
    print('Text size: {}'.format(text_line_count))

    start = time.time()

    tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
    model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)
    process_text(model, tokenizer, text_lines)

    end = time.time()
    print('Total time spent with bert base: {}'.format(str(timedelta(seconds=end - start))))

    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
    process_text(model, tokenizer, text_lines)

    end2 = time.time()
    print('Total time spent with bert base quantization: {}'.format(str(timedelta(seconds=end2 - end))))

    tokenizer = DistilBertTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;)
    model = DistilBertModel.from_pretrained(&quot;distilbert-base-uncased&quot;)
    process_text(model, tokenizer, text_lines)

    end3 = time.time()
    print('Total time spent with distilbert: {}'.format(str(timedelta(seconds=end3 - end2))))

    model = DistilBertModel.from_pretrained(&quot;distilbert-base-uncased&quot;)
    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
    process_text(model, tokenizer, text_lines)

    end4 = time.time()
    print('Total time spent with distilbert quantization: {}'.format(str(timedelta(seconds=end4 - end3))))
</code></pre>
<p>EDIT: based on suggestion I changed to the following:</p>
<pre><code>inputs = tokenizer(text_batch, padding=True, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
</code></pre>
<p>Where text_batch is a list of text as input.</p>
","3943868","","3943868","","2021-05-26 23:43:02","2021-05-26 23:43:02","Are these normal speed of Bert Pretrained Model Inference in PyTorch","<bert-language-model><huggingface-transformers><transformer><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"67644405","1","","","2021-05-21 22:19:32","","2","72","<p>Let's say I am using multiple GPUs (0,1,2,3) on one machine and later someone else also needs to use GPUs on this machine. Is there a way for me to reduce the number of gpu usage (i.e. only use 0 and 1) from my training without terminating the training and start over again? I don't want to waste the training I already did.</p>
<p>This sounds like a common need in a team. Is that possible?</p>
","3943868","","","","","2021-05-26 20:42:34","Can I reduce number of GPUs without terminating the training?","<tensorflow><pytorch><huggingface-transformers>","1","1","1","","","CC BY-SA 4.0"
"67689219","1","67693078","","2021-05-25 13:41:01","","1","201","<p>I have a pre-trained model which I load like so:</p>
<pre><code>from transformers import BertForSequenceClassification, AdamW, BertConfig, BertModel
model = BertForSequenceClassification.from_pretrained(
    &quot;bert-base-uncased&quot;, # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)
</code></pre>
<p>I want to create a new model with the same architecture, and random initial weights, <em>except</em> for the embedding layer:</p>
<pre><code>==== Embedding Layer ====

bert.embeddings.word_embeddings.weight                  (30522, 768)
bert.embeddings.position_embeddings.weight                (512, 768)
bert.embeddings.token_type_embeddings.weight                (2, 768)
bert.embeddings.LayerNorm.weight                              (768,)
bert.embeddings.LayerNorm.bias                                (768,)
</code></pre>
<p>It seems I can do this to create a new model with the same architecture, but then <em>all</em> the weights are random:</p>
<pre><code>configuration   = model.config
untrained_model = BertForSequenceClassification(configuration)
</code></pre>
<p>So how do I copy over <code>model</code>'s embedding layer weights to the new <code>untrained_model</code>?</p>
","2554824","","","","","2021-05-25 17:44:37","Copy one layer's weights from one Huggingface BERT model to another","<python><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67714372","1","","","2021-05-27 00:52:49","","2","130","<p>I've been working on a sentence transformation task that involves paraphrase identification as a critical step: if we are confident enough that the state of the program (a sentence repeatedly modified) has become a paraphrase of a target sentence, stop transforming. The overall goal is actually to study potential reasoning in predictive models that can generate language prior to a target sentence. The approach is just one specific way of reaching that goal. Nevertheless, I've become interested in the paraphrase identification task itself, as it's received some boost from language models recently.</p>
<p>The problem I run into is when I manipulate sentences from examples or datasets. For example, in this <a href=""https://huggingface.co/transformers/v2.6.0/usage.html#sequence-classification"" rel=""nofollow noreferrer"">HuggingFace example</a>, if I negate either sequence or change the subject to Bloomberg, I still get a majority &quot;is paraphrase&quot; prediction. I started going through many examples in the <a href=""https://github.com/wasiahmad/paraphrase_identification/blob/master/dataset/msr-paraphrase-corpus/msr_paraphrase_train.txt"" rel=""nofollow noreferrer"">MSRPC</a> training set and negating one sentence in a positive example or making one sentence in a negative example a paraphrase of the other, especially when doing so would be a few word edit. I found to my surprise that various language models, like <code>bert-base-cased-finetuned-mrpc</code> and  <code>textattack/roberta-base-MRPC</code>, don't change their confidences much on these sorts of changes. It's surprising as these models claim an f1 score of <a href=""https://huggingface.co/textattack/facebook-bart-large-MRPC/blob/main/eval_results_mrpc.txt"" rel=""nofollow noreferrer"">0.918</a>+. The dataset is clearly missing a focus on negative examples and small perturbative examples.</p>
<p>My question is, are there datasets, techniques, or models that deal well when given small edits? I know that this is an extremely generic question, much more than is typically asked on StackOverflow, but my concern is in finding practical tools. If there is a theoretical technique, then it might not be suitable as I'm in the category of &quot;available tools define your approach&quot; rather than vice-versa. So I hope that the community would have a recommendation on this.</p>
","16042594","","","","","2021-07-10 00:11:29","Are transformer-based language models overfitting on the paraphrase identification task? What tools overcome this?","<nlp><huggingface-transformers><msrpc>","2","0","","","","CC BY-SA 4.0"
"67721587","1","","","2021-05-27 11:53:16","","0","74","<pre><code>from os import listdir
from os.path import isfile, join
from datasets import load_dataset
from transformers import BertTokenizer

test_files = [join('./test/', f) for f in listdir('./test') if isfile(join('./test', f))]

dataset = load_dataset('json', data_files={&quot;test&quot;: test_files}, cache_dir=&quot;./.cache_dir&quot;)
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)

def encode(batch):
  return tokenizer.encode_plus(batch[&quot;abstract&quot;], max_length=32, add_special_tokens=True, pad_to_max_length=True,
                        return_attention_mask=True, return_token_type_ids=False, return_tensors=&quot;pt&quot;)

dataset.set_transform(encode)
</code></pre>
<p>When I run this code, I have</p>
<pre><code>ValueError: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.
</code></pre>
<p>Instead of having a list of strings, I have a list of lists of strings. Here is the content of <code>batch[&quot;article&quot;]</code>:</p>
<pre><code>[['eleven politicians from 7 parties made comments in letter to a newspaper .', &quot;said dpp alison saunders had ` damaged public confidence ' in justice .&quot;, 'ms saunders ruled lord janner unfit to stand trial over child abuse claims .', 'the cps has pursued at least 19 suspected paedophiles with dementia .'], ['an increasing number of surveys claim to reveal what makes us happiest .', 'but are these generic lists really of any use to us ?', 'janet street-porter makes her own list - of things making her unhappy !'], [&quot;author of ` into the wild ' spoke to five rape victims in missoula , montana .&quot;, &quot;` missoula : rape and the justice system in a college town ' was released april 21 .&quot;, &quot;three of five victims profiled in the book sat down with abc 's nightline wednesday night .&quot;, 'kelsey belnap , allison huguet and hillary mclaughlin said they had been raped by university of montana football players .', &quot;huguet and mclaughlin 's attacker , beau donaldson , pleaded guilty to rape in 2012 and was sentenced to 10 years .&quot;, 'belnap claimed four players gang-raped her in 2010 , but prosecutors never charged them citing lack of probable cause .', 'mr krakauer wrote book after realizing close friend was a rape victim .'], ['tesco announced a record annual loss of Â£ 6.38 billion yesterday .', 'drop in sales , one-off costs and pensions blamed for financial loss .', 'supermarket giant now under pressure to close 200 stores nationwide .', 'here , retail industry veterans , plus mail writers , identify what went wrong .'], ...,  ['snp leader said alex salmond did not field questions over his family .', &quot;said she was not ` moaning ' but also attacked criticism of women 's looks .&quot;, 'she made the remarks in latest programme profiling the main party leaders .', 'ms sturgeon also revealed her tv habits and recent image makeover .', 'she said she relaxed by eating steak and chips on a saturday night .']]
</code></pre>
<p>How could I fix this issue?</p>
","16037218","","16037218","","2021-05-27 12:52:35","2021-05-27 12:52:35","ValueError: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers","<huggingface-transformers><huggingface-datasets>","0","3","","","","CC BY-SA 4.0"
"67721779","1","","","2021-05-27 12:06:55","","1","76","<pre><code>from os import listdir
from os.path import isfile, join
from datasets import load_dataset
from transformers import BertTokenizer

test_files = [join('./test/', f) for f in listdir('./test') if isfile(join('./test', f))]

dataset = load_dataset('json', data_files={&quot;test&quot;: test_files}, cache_dir=&quot;./.cache_dir&quot;)
</code></pre>
<p>After running the code, here output of <code>dataset[&quot;test&quot;][&quot;abstract&quot;]</code>:</p>
<pre><code>[['eleven politicians from 7 parties made comments in letter to a newspaper .',
  &quot;said dpp alison saunders had ` damaged public confidence ' in justice .&quot;,
  'ms saunders ruled lord janner unfit to stand trial over child abuse claims .',
  'the cps has pursued at least 19 suspected paedophiles with dementia .'],
 ['an increasing number of surveys claim to reveal what makes us happiest .',
  'but are these generic lists really of any use to us ?',
  'janet street-porter makes her own list - of things making her unhappy !'],
 [&quot;author of ` into the wild ' spoke to five rape victims in missoula , montana .&quot;,
  &quot;` missoula : rape and the justice system in a college town ' was released april 21 .&quot;,
  &quot;three of five victims profiled in the book sat down with abc 's nightline wednesday night .&quot;,
  'kelsey belnap , allison huguet and hillary mclaughlin said they had been raped by university of montana football '
  'players .',
  &quot;huguet and mclaughlin 's attacker , beau donaldson , pleaded guilty to rape in 2012 and was sentenced to 10 years .&quot;,
  'belnap claimed four players gang-raped her in 2010 , but prosecutors never charged them citing lack of probable '
  'cause .',
  'mr krakauer wrote book after realizing close friend was a rape victim .'],
 ['tesco announced a record annual loss of Â£ 6.38 billion yesterday .',
  'drop in sales , one-off costs and pensions blamed for financial loss .',
  'supermarket giant now under pressure to close 200 stores nationwide .',
  'here , retail industry veterans , plus mail writers , identify what went wrong .'],
 ...,
 ['snp leader said alex salmond did not field questions over his family .',
  &quot;said she was not ` moaning ' but also attacked criticism of women 's looks .&quot;,
  'she made the remarks in latest programme profiling the main party leaders .',
  'ms sturgeon also revealed her tv habits and recent image makeover .',
  'she said she relaxed by eating steak and chips on a saturday night .']]
</code></pre>
<p><a href=""https://i.stack.imgur.com/JglWq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JglWq.png"" alt=""enter image description here"" /></a></p>
<p>I would like that each sentence to have this structure of tokenizing. How can I do such thing using huggingface? In fact, I think I have to flatten each list of the above list to get a list of strings and then tokenize each string.</p>
","16037218","","16037218","","2021-05-28 14:06:59","2021-05-28 14:06:59","Tokenizing sentences a special way","<python><huggingface-transformers><huggingface-datasets>","0","1","1","","","CC BY-SA 4.0"
"67645399","1","","","2021-05-22 01:44:12","","0","51","<p>What does <code>do_sample</code> parameter of the <a href=""https://huggingface.co/transformers/main_classes/model.html?highlight=do_sample#transformers.generation_utils.GenerationMixin.generate"" rel=""nofollow noreferrer"">generate</a> method of the Hugging face model do?</p>
<blockquote>
<p>Generates sequences for models with a language modeling head. The method currently supports greedy decoding, multinomial sampling, beam-search decoding, and beam-search multinomial sampling.</p>
</blockquote>
<pre><code>do_sample (bool, optional, defaults to False) â€“ Whether or not to use sampling; 
use greedy decoding otherwise.
</code></pre>
<p>When the Beam search length is 1, it can be called <strong>greedy</strong>. Does <code>do_sample=False</code> mean the same?</p>
","4281353","","","","","2021-05-22 01:44:12","Huggingeface model generator method do_sample parameter","<huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"67722746","1","","","2021-05-27 13:02:55","","0","83","<p>I am very new to BERT. Just trying to understand a few concepts of finetuning with custom data for QuestionAndAnswering task and am using the example from this <a href=""https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb"" rel=""nofollow noreferrer"">notebook</a> from HuggingFace. I have a small custom dataset   with the following columns with train and validation sizes as (767, 5) (8, 5) repectively.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column</th>
<th>Dtype</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td>object</td>
</tr>
<tr>
<td>text</td>
<td>object</td>
</tr>
<tr>
<td>answer_start</td>
<td>int64</td>
</tr>
<tr>
<td>question</td>
<td>object</td>
</tr>
<tr>
<td>context</td>
<td>object</td>
</tr>
</tbody>
</table>
</div>
<p>The validation_features.features are as follows</p>
<pre><code>{'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),
 'example_id': Value(dtype='int64', id=None),
 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),
 'offset_mapping': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None)}
</code></pre>
<p>I am getting an error at the following line for the  evaluation (trying to understand the metrics used).</p>
<pre><code>raw_predictions = trainer.predict(validation_features)

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py in convert_to_tensors(self, tensor_type, prepend_batch_axis)
    698                 if not is_tensor(value):
--&gt; 699                     tensor = as_tensor(value)
    700 

RuntimeError: Could not infer dtype of NoneType

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
9 frames
/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py in convert_to_tensors(self, tensor_type, prepend_batch_axis)
    714                     )
    715                 raise ValueError(
--&gt; 716                     &quot;Unable to create tensor, you should probably activate truncation and/or padding &quot;
    717                     &quot;with 'padding=True' 'truncation=True' to have batched tensors with the same length.&quot;
    718                 )

ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.
</code></pre>
<p>I want to understand what is the error here and how to solve. Please help.</p>
<p>Note: The training completed without errors. I have saved and tested the model - it provides answers.</p>
","11918296","","11918296","","2021-05-27 13:08:30","2021-05-27 13:08:30","Error on Finetuning a HuggingFace Distilbert model for QuestionAndAnswer","<huggingface-transformers><question-answering>","0","0","","","","CC BY-SA 4.0"
"67738117","1","","","2021-05-28 11:37:48","","0","128","<p>I am facing issue while loading the model using torch which was trained using GPU, I am trying to load that model using CPU. however I am successfully able to load the model but while predicting the results I am getting error. However if I use GPU machine I am able to predict the output but not on the CPU:</p>
<p>My code:</p>
<pre><code>****To save the model I am using :****
PATH = &quot;model.pt&quot;
torch.save(model, PATH)

**To Load the Model**


 import torch
    PATH = &quot;model.pt&quot;
    device = torch.device('cpu')
    loaded_model=torch.load(PATH, map_location=device)
</code></pre>
<p>I am able to successfully load the model. but while predicting I am getting runtime error</p>
<pre><code>**Predicting the loaded model using CPU**
predicted_title = loaded_model.predict([abstract])
</code></pre>
<blockquote>
<p>Runtime Error: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver</p>
</blockquote>
<p>I am sorry if the error might turn out very simple but I am not able to rectify this.</p>
","11259950","","4420967","","2021-05-28 14:42:40","2021-06-02 20:50:21","Runtime Error: Found no NVIDIA driver on your system","<gpu><cpu><torch><huggingface-transformers><seq2seq>","1","1","","","","CC BY-SA 4.0"
"67638951","1","","","2021-05-21 14:34:48","","0","14","<p>I'm very new to nlp topic and it's my first project. I'm trying to follow this code only for RoBERTa: <a href=""https://github.com/JMSaindon/NlpSquad/blob/master/Bert_fine_tune.ipynb"" rel=""nofollow noreferrer"">https://github.com/JMSaindon/NlpSquad/blob/master/Bert_fine_tune.ipynb</a>, but have a problem with the evaluation part, getting such an error:
<a href=""https://i.stack.imgur.com/Ztrld.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ztrld.png"" alt=""enter image description here"" /></a></p>
<p>So, as I understand the problem is in this code part before training:
<a href=""https://i.stack.imgur.com/dfPuS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dfPuS.png"" alt=""enter image description here"" /></a></p>
<p>And using defined function here:</p>
<pre><code> output = [to_list(output[i]) for output in outputs]
</code></pre>
<p>How to fix it?</p>
","12486445","","","","","2021-05-21 14:34:48","NLP - Error using detach during evaluation","<python><error-handling><nlp><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"67639478","1","67640143","","2021-05-21 15:08:34","","1","17","<p>is calling tokenizer on a batch significantly faster than on calling it on each item in a batch? e.g.</p>
<pre class=""lang-py prettyprint-override""><code>encodings = tokenizer(sentences)
# vs
encodings = [tokenizer(x) for x in sentences]
</code></pre>
","2160809","","","","","2021-05-21 15:49:19","Is there a significant speed improvement when using transformers tokenizer over batch compared to per item?","<pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67669242","1","","","2021-05-24 08:57:10","","0","38","<p>I am looking for a Hugging Face model that allows to perform <strong>Question Answering without providing the context</strong> for the answer itself.</p>
<p>For instance, let's assume that I have the following question:</p>
<blockquote>
<p>&quot;Who was the fifth King of Rome?&quot;</p>
</blockquote>
<p>I would like the model to exploit its <em>own knowledge</em> (i.e. the one that it created in the training phase) to give the answer, without relying on a given context.</p>
<p>So, given as input the raw question I would like the model to output several possibile answers to that question.</p>
<p>I understand that, as stated in the <a href=""https://huggingface.co/transformers/task_summary.html#extractive-question-answering"" rel=""nofollow noreferrer"">Hugging Face original website</a>, the provided models can perform the &quot;<strong>Extractive Question Answering</strong>&quot; task which, by definition, needs a context from which to extract the answer to the question.</p>
<p>Is there a way to get rid of the context and use the pre-trained model to perform a &quot;<strong>Non-Extractive Question Answering</strong>&quot; with the provided models?</p>
","16014729","","16014729","","2021-05-24 20:40:26","2021-05-24 20:40:26","Is there a way to perform Question Answering without providing a context for the answer?","<python><huggingface-transformers><question-answering>","0","2","2","","","CC BY-SA 4.0"
"67740498","1","67741467","","2021-05-28 14:12:50","","1","286","<p>I have trained an electra model from scratch using <a href=""https://github.com/google-research/electra"" rel=""nofollow noreferrer"">google implementation code</a>.</p>
<pre class=""lang-sh prettyprint-override""><code>python run_pretraining.py --data-dir gc://bucket-electra/dataset/ --model-name greek_electra --hparams hparams.json
</code></pre>
<p>with this json hyperparams:</p>
<pre class=""lang-json prettyprint-override""><code>{
&quot;embedding_size&quot;: 768,
&quot;max_seq_length&quot;: 512,
&quot;train_batch_size&quot;: 128,
&quot;vocab_size&quot;: 100000,
&quot;model_size&quot;: &quot;base&quot;,
&quot;num_train_steps&quot;: 1500000
}
</code></pre>
<p>After having trained the model, I used the <a href=""https://github.com/huggingface/transformers/blob/d5d7d886128732091e92afff7fcb3e094c71a7ec/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py"" rel=""nofollow noreferrer"">convert_electra_original_tf_checkpoint_to_pytorch.py</a> script from transformers library to convert the checkpoint.</p>
<pre class=""lang-sh prettyprint-override""><code>python convert_electra_original_tf_checkpoint_to_pytorch.py --tf_checkpoint_path output/models/transformer/greek_electra --config_file resources/hparams.json --pytorch_dump_path output/models/transformer/discriminator  --discriminator_or_generator &quot;discriminator&quot;
</code></pre>
<p>Now I am trying to load the model:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import ElectraForPreTraining

model = ElectraForPreTraining.from_pretrained('discriminator')
</code></pre>
<p>but I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;~/.local/lib/python3.9/site-packages/transformers/configuration_utils.py&quot;, line 427, in get_config_dict
    config_dict = cls._dict_from_json_file(resolved_config_file)
  File &quot;~/.local/lib/python3.9/site-packages/transformers/configuration_utils.py&quot;, line 510, in _dict_from_json_file
    text = reader.read()
  File &quot;/usr/lib/python3.9/codecs.py&quot;, line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte
</code></pre>
<p>Any ideas what's causing this &amp; how to solve it?</p>
","11096524","","","","","2021-05-28 15:14:27","Huggingface Electra - Load model trained with google implementation error: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte","<python><tensorflow><pytorch><bert-language-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"67633551","1","","","2021-05-21 08:44:47","","1","84","<p>Loading a <a href=""https://huggingface.co/transformers/main_classes/model.html#pretrainedmodel"" rel=""nofollow noreferrer"">huggingface pretrained transformer model</a> seemingly requires you to have the model saved locally (as described <a href=""https://stackoverflow.com/a/64007213/1571593"">here</a>), such that you simply pass a local path to your model and config:</p>
<pre class=""lang-py prettyprint-override""><code>model = PreTrainedModel.from_pretrained('path/to/model', local_files_only=True)
</code></pre>
<p>Can this be achieved when the model is stored on S3?</p>
","1571593","","","","","2021-08-13 15:23:41","Reading a pretrained huggingface transformer directly from S3","<amazon-s3><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"60048900","1","","","2020-02-04 00:16:58","","2","64","<p>I have sentences that belong to a paragraph. Each sentence has a label.
[s1,s2,s3,â€¦], [l1,l2,l3,â€¦]
I understand that I have to encode each sentence using an encoder, and then use sequence labeling. Could you guide me on how I could do that, combining them?</p>
","10396375","","","","","2020-02-04 09:18:47","Sequence labeling for sentences and not tokens","<deep-learning><nlp><pytorch><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"60062624","1","","","2020-02-04 17:28:36","","3","222","<p>Sorry for my naive question but I am trying to save my keras model () in which I use TFBertModel() function as an hidden layer. To do that I use the save() function provided by the tf.keras package.</p>

<p>But I got this error:</p>

<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------

NotImplementedError                       Traceback (most recent call last)

&lt;ipython-input-13-3b315f7219da&gt; in &lt;module&gt;()
----&gt; 1 model.save('model_weights.h5')

8 frames

/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/network.py in get_config(self)
    915   def get_config(self):
    916     if not self._is_graph_network:
--&gt; 917       raise NotImplementedError
    918     return copy.deepcopy(get_network_config(self))
    919 

NotImplementedError: 
</code></pre>

<p>The error can be reproduce from my colab : <a href=""https://colab.research.google.com/drive/18HYwffkXCylPqeA-8raL82vfwOjb-aLP"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/18HYwffkXCylPqeA-8raL82vfwOjb-aLP</a></p>

<p>And another question is how should I call this model for prediction ?</p>

<p>Thx for your help!</p>
","3223514","","3223514","","2020-02-06 14:53:10","2020-02-24 22:38:08","Save model wrapped in Keras","<python-3.x><tensorflow2.0><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"67740759","1","67780432","","2021-05-28 14:27:04","","2","249","<p>I am interested in using pre-trained models from Huggingface for named entity recognition (NER) tasks without further training or testing of the model.</p>
<p>On the <a href=""https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT"" rel=""nofollow noreferrer"">model page of HuggingFace</a>, the only information for reusing the model are as follow:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
</code></pre>
<p>I tried the following code, but I am getting a tensor output instead of class labels for each named entity.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

text = &quot;my text for named entity recognition here.&quot;

input_ids = torch.tensor(tokenizer.encode(text, padding=True, truncation=True,max_length=50, add_special_tokens = True)).unsqueeze(0)

with torch.no_grad():
  output = model(input_ids, output_attentions=True)
</code></pre>
<p>Any suggestions on how to apply the model on a text for NER?</p>
","9271275","","9271275","","2021-05-30 18:33:15","2021-05-31 21:32:45","How to apply a pretrained transformer model from huggingface?","<huggingface-transformers><ner><transformer>","1","0","","","","CC BY-SA 4.0"
"67531678","1","","","2021-05-14 08:58:46","","1","148","<p>I am quite new to the BERT language model. I am currently using the Huggingface transformer libraryand i'm  encountering an error when encoding the inputs. The goal of the model is to classify fake news.</p>
<p>First I downloaded the dataset which I turned into a pandas dataframe containing 3 columns. Index, tweet, label. The pretrained auto tokenizer from bert large uncased is used to encode the input.</p>
<pre><code>TOKENIZER = AutoTokenizer.from_pretrained(&quot;bert-large-uncased&quot;)
</code></pre>
<p>The following function is used:</p>
<pre><code>def bert_encode(data,maximum_len) :
input_ids = []
attention_masks = []


for i in range(len(data.tweet)):
    encoded = TOKENIZER.encode_plus(data.tweet[i],
                                    add_special_tokens=True,
                                    max_length=maximum_len,
                                    pad_to_max_length=True,
                                    return_attention_mask=True,
                                    truncation=True)
  
    input_ids.append(encoded['input_ids'])
    attention_masks.append(encoded['attention_mask'])
    
return np.array(input_ids),np.array(attention_masks)
</code></pre>
<p>The function is applied to the the data to get the train input id and the attention masks:</p>
<pre><code>train_input_ids,train_attention_masks = bert_encode(train,600)
test_input_ids,test_attention_masks = bert_encode(test,600)
</code></pre>
<p>However, calling the function gives me the following error: KeyError: 3
Provided beolow is the exact error message.</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2897             try:
-&gt; 2898                 return self._engine.get_loc(casted_key)
   2899             except KeyError as err:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

KeyError: 3

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
4 frames
/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2898                 return self._engine.get_loc(casted_key)
   2899             except KeyError as err:
-&gt; 2900                 raise KeyError(key) from err
   2901 
   2902         if tolerance is not None:

KeyError: 3
</code></pre>
<p>Any insight on how to debug are welcome.</p>
","15924213","","6664872","","2021-05-26 17:19:36","2021-05-26 17:19:36","BERT transformer KeyError: 3","<python><bert-language-model><huggingface-transformers><keyerror><transformer>","0","1","","","","CC BY-SA 4.0"
"65983144","1","","","2021-01-31 19:08:14","","1","701","<p>I am using Google Colab and trying to use <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">transformers</a>. first, I installed <code>trasnformers</code> using <code>pip</code>, and it installed successfully but I was still unable to import the following functions</p>
<pre class=""lang-py prettyprint-override""><code>from transformers.trainer_utils import get_last_checkpoint,is_main_process
</code></pre>
<p>Next I tried to install <code>Transformers</code> from source in a virtual environment. I installed it successfully, but was still getting the same error as shown below.</p>
<pre class=""lang-none prettyprint-override""><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-53-d42272f1d6ac&gt; in &lt;module&gt;()
----&gt; 1 from transformers.trainer_utils import get_last_checkpoint,is_main_process

ModuleNotFoundError: No module named 'transformers.trainer_utils'
</code></pre>
","4508678","","14469685","","2021-02-01 06:29:47","2021-02-08 08:45:49","Huggingface transformers error: ""from transformers.trainer_utils import get_last_checkpoint,is_main_process""","<tensorflow><google-colaboratory><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"67745058","1","","","2021-05-28 20:21:34","","3","46","<p>I have used <a href=""https://github.com/google-research/electra"" rel=""nofollow noreferrer"">google's implementation of electra</a> to train a model from scratch.
For the pretraining I have followed <a href=""https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379"" rel=""nofollow noreferrer"">this tutorial</a> -- with modifications of course, since it uses google's implementation for BERT. After having finished the training &amp; also having converted the electra discriminator checkpoint to huggingface format using <a href=""https://github.com/huggingface/transformers/blob/d5d7d886128732091e92afff7fcb3e094c71a7ec/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py"" rel=""nofollow noreferrer"">this script</a>, I am trying to load the model in order to get the embeddings for some sentences.</p>
<p>First, I create the ElectraConfig, and then I create a tokenizer using the vocab.txt I have created (based on the aforementioned tutorial)</p>
<pre class=""lang-py prettyprint-override""><code>config = ElectraConfig(vocab_size=100000,
                       embedding_size=768,
                       hidden_size=768,
                       num_hidden_layers=12,
                       num_attention_heads=4,
                       intermediate_size=3072,
                       hidden_act=&quot;gelu&quot;,
                       hidden_dropout_prob=0.1,
                       attention_probs_dropout_prob=0.1,
                       max_position_embeddings=512,
                       position_embedding_type=&quot;absolute&quot;)

tokenizer = ElectraTokenizer.from_pretrained(path_to_vocab)
</code></pre>
<p>My problem is that the tokenizer encodes all tokens in all sentences as unknown.
Do I need to convert the vocab.txt file? Is the creation of the vocab.txt file described in the tutorial incompatible with huggingface?</p>
","11096524","","6664872","","2021-05-31 21:15:46","2021-05-31 21:15:46","Huggingface tokenizer always encodes as unknown - Conversion of vocab.txt?","<python><pytorch><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"65987683","1","65991030","","2021-02-01 05:42:01","","2","233","<p>Below is the code to configure <a href=""https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments"" rel=""nofollow noreferrer"">TrainingArguments</a> consumed from the <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">HuggingFace transformers</a> library to finetune the <a href=""https://huggingface.co/transformers/model_doc/gpt2.html"" rel=""nofollow noreferrer"">GPT2</a> language model.</p>
<pre><code>training_args = TrainingArguments(
        output_dir=&quot;./gpt2-language-model&quot;, #The output directory
        num_train_epochs=100, # number of training epochs
        per_device_train_batch_size=8, # batch size for training #32, 10
        per_device_eval_batch_size=8,  # batch size for evaluation #64, 10
        save_steps=100, # after # steps model is saved
        warmup_steps=500,# number of warmup steps for learning rate scheduler
        prediction_loss_only=True,
        metric_for_best_model = &quot;eval_loss&quot;,
        load_best_model_at_end = True,
        evaluation_strategy=&quot;epoch&quot;,
        learning_rate=0.00004, # learning rate
    )

early_stop_callback = EarlyStoppingCallback(early_stopping_patience  = 3)
    
trainer = Trainer(
        model=gpt2_model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        callbacks = [early_stop_callback],
 )
</code></pre>
<p>The <strong>number of epochs</strong> as <strong>100</strong> and <strong>learning_rate</strong> as <strong>0.00004</strong> and also the <strong>early_stopping</strong> is configured with the patience value as <strong>3</strong>.</p>
<p>The model ran for <strong>5/100</strong> epochs and noticed that the difference in loss_value is negligible. The latest checkpoint is saved as <code>checkpoint-latest</code>.</p>
<p>Now Can I modify the <code>learning_rate</code> may be to <code>0.01</code> from <code>0.00004</code> and resume the training from the latest saved checkpoint - <code>checkpoint-latest</code>? Doing that will be efficient?</p>
<p>Or to train with the new <code>learning_rate</code> value should I start the <strong>training</strong> from the beginning?</p>
","1793799","","1793799","","2021-02-01 05:48:22","2021-02-01 10:30:40","Modifying the Learning Rate in the middle of the Model Training in Deep Learning","<deep-learning><pytorch><huggingface-transformers><language-model><gpt-2>","2","1","","","","CC BY-SA 4.0"
"67745420","1","","","2021-05-28 21:00:44","","1","770","<p>When I try to install BigBirdTokenizer I get the
Following error</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-12-c81eaf9abd32&gt; in &lt;module&gt;()
----&gt; 1 from summarizer import Summarizer
      2 f = open(&quot;macineLearning&quot;,&quot;r&quot;)
      3 full_text = f.read()
      4 model = Summarizer()
      5 result = model(full_text, min_length=60, max_length = 500 , ratio = 0.4)

4 frames
/usr/local/lib/python3.7/dist-packages/summarizer/__init__.py in &lt;module&gt;()
----&gt; 1 from summarizer.model_processors import Summarizer, TransformerSummarizer

/usr/local/lib/python3.7/dist-packages/summarizer/model_processors.py in &lt;module&gt;()
      2 
      3 import numpy as np
----&gt; 4 from transformers import *
      5 
      6 from summarizer.bert_parent import BertParent

/usr/local/lib/python3.7/dist-packages/transformers/__init__.py in __getattr__(self, name)

/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py in __getattr__(self, name)

/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py in __getattr__(self, name)

AttributeError: module transformers.models.big_bird has no attribute BigBirdTokenizer
</code></pre>
<p>I am doing this [error on first line of code below]</p>
<pre><code>from summarizer import Summarizer
f = open(&quot;macineLearning&quot;,&quot;r&quot;)
full_text = f.read()
model = Summarizer()
result = model(full_text, min_length=60, max_length = 500 , ratio = 0.4)
summarized_text = ''.join(result)
print (summarized_text)
</code></pre>
<p>I've also installed:</p>
<pre><code>!pip install gensim
!pip install git+https://github.com/boudinfl/pke.git
!python -m spacy download en
!pip install bert-extractive-summarizer --upgrade --force-reinstall
!pip install spacy==2.1.3 --upgrade --force-reinstall
!pip install -U nltk
!pip install -U pywsd
import nltk
nltk.download('stopwords')
nltk.download('popular')
!pip install sentencepiece
!pip install BigBirdTokenizer
!pip install sentence-transformers==0.2.5.1
!pip install transformers==2.6.0
</code></pre>
<p>But the BigBirdTokenizer always seems to fail. I'm quite new to NLP but would like to understand why the issue occurs.</p>
","11805611","","472495","","2021-05-29 13:00:38","2021-06-20 14:06:58","How to install BigBirdTokenizer for NLP?","<python><nlp><huggingface-transformers><huggingface-tokenizers>","1","2","","","","CC BY-SA 4.0"
"65353104","1","","","2020-12-18 07:18:18","","0","142","<p>I want to use &quot;grouped_entities&quot; in the huggingface pipeline for ner task. However having issues doing that.</p>
<p>I do look the following link on git but this did not help:
<a href=""https://github.com/huggingface/transformers/pull/4987"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/pull/4987</a></p>
","1771338","","1771338","","2020-12-18 07:27:20","2020-12-18 11:16:55","I want to use ""grouped_entities"" in the huggingface pipeline for ner task, how to do that?","<huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"65396968","1","65397572","","2020-12-21 17:04:06","","2","1664","<p>Short TL;DR:
I am using BERT for a sequence classification task and don't understand the output I get.</p>
<p>This is my first post, so please bear with me:
I am using bert for a sequence classification task with 3 labels. To do this, I am using huggingface transformers with tensorflow, more specifically the TFBertForSequenceClassification class with the bert-base-german-cased model (yes, using german sentences).</p>
<p>I am by no means an expert in NLP, which is why I pretty much followed this approch here: <a href=""https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333"" rel=""nofollow noreferrer"">https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333</a> (with some tweaks of course)</p>
<p>Everything seems to be working fine, but the output I receive from my model is what throws me off.
Here's just some of the output along the way for context.</p>
<p>The main difference I have to the example from the article is the number of labels. I have 3 while the article only featured 2.</p>
<p>I use a LabelEncoder from sklearn.preprocessing to process my labels</p>
<pre><code>label_encoder = LabelEncoder()
Y_integer_encoded = label_encoder.fit_transform(Y)
</code></pre>
<p>*Y here is a list of labels as strings, so something like this</p>
<pre><code>['e_3', 'e_1', 'e_2',]
</code></pre>
<p>then turns into this:</p>
<pre><code>array([0, 1, 2], dtype=int64)
</code></pre>
<p>I then use the BertTokenizer to process my text and create the input datasets (training and testing).
These are the shapes of those:</p>
<pre><code> &lt;TensorSliceDataset shapes: ({input_ids: (99,), token_type_ids: (99,), attention_mask: (99,)}, ()), types: ({input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}, tf.int32)&gt;
</code></pre>
<p>I then train the model as per Huggingface docs.</p>
<p>The last epoch while training the model looks like this:</p>
<pre><code>Epoch 3/3
108/108 [==============================] - 24s 223ms/step - loss: 25.8196 - accuracy: 0.7963 - val_loss: 24.5137 - val_accuracy: 0.7243
</code></pre>
<p>Then I run model.predict on an example sentence and get this output (yes I tokenized the sentence accordingly just like the other article does). The output looks like this:</p>
<pre><code>array([ 3.1293588, -5.280143 ,  2.4700692], dtype=float32)
</code></pre>
<p>And lastly that's the softmax function I apply in the end and it's output:</p>
<pre><code>tf_prediction = tf.nn.softmax(tf_output, axis=0).numpy()[0]

output: 0.6590041
</code></pre>
<p>So here's my question:
I don't quite understand that output. With an accuracy of ~70% (validation accuracy), my model should be okay in predicting the labels. Yet only the logits from the direct output don't mean much to me tbh and the output after the softmax function seems to be on a linear scale, as if it came from a sigmoid function. How do I interpret this and translate it to the label I am trying to predict?</p>
<p>And also: shouldn't I feed one hot encoded labels into my bert model for it to work? I always thought Bert needs that but it seems like it doesn't.</p>
<p>Thanks a lot in advance!</p>
","8291269","","5561472","","2021-01-18 08:27:14","2021-01-18 08:27:14","How do I interpret my BERT output from Huggingface Transformers for Sequence Classification and tensorflow?","<python><tensorflow><bert-language-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"60087613","1","","","2020-02-06 04:10:53","","7","970","<p>In the last few layers of sequence classification by <a href=""https://github.com/huggingface/transformers/blob/33d3072e1c54bcd235447b98c6dea1b4cb71234c/src/transformers/modeling_distilbert.py#L634"" rel=""noreferrer"">HuggingFace</a>, they took the first hidden state of the sequence length of the transformer output to be used for classification. </p>

<pre><code>hidden_state = distilbert_output[0]  # (bs, seq_len, dim) &lt;-- transformer output
pooled_output = hidden_state[:, 0]  # (bs, dim)           &lt;-- first hidden state
pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)
pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)
pooled_output = self.dropout(pooled_output)  # (bs, dim)
logits = self.classifier(pooled_output)  # (bs, dim)
</code></pre>

<p>Is there any benefit to taking the first hidden state over the last, average, or even the use of a Flatten layer instead?</p>
","11151288","","11151288","","2020-02-07 02:42:09","2021-08-22 09:41:23","why take the first hidden state for sequence classification (DistilBertForSequenceClassification) by HuggingFace","<time-series><sequence><tensorflow2.0><text-classification><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67743498","1","","","2021-05-28 17:49:08","","2","201","<p>I'm training a <a href=""https://huggingface.co/transformers/task_summary.html#named-entity-recognition"" rel=""nofollow noreferrer"">token classification</a> (AKA named entity recognition) model with the <a href=""https://huggingface.co/transformers/index.html"" rel=""nofollow noreferrer"">HuggingFace Transformers</a> library, with a customized data loader.</p>
<p>Like most NER datasets (I'd imagine?) there's a pretty significant <strong>class imbalance</strong>: A large majority of tokens are <code>other</code> - i.e. <strong>not</strong> an entity - and of course there's a little variation between the different entity classes themselves.</p>
<p>As we might expect, my &quot;accuracy&quot; metrics are getting distorted quite a lot by this: It's no great achievement to get 80% token classification accuracy if 90% of your tokens are <code>other</code>... A trivial model could have done better!</p>
<p>I can calculate some additional and more insightful evaluation metrics - but it got me wondering... Can/should we somehow incorporate these weights into the training loss? How would this be done using a typical <code>*ForTokenClassification</code> model e.g. <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertfortokenclassification"" rel=""nofollow noreferrer"">BERTForTokenClassification</a>?</p>
","13352657","","","","","2021-05-28 18:47:32","How can/should we weight classes in HuggingFace token classification (entity recognition)?","<pytorch><huggingface-transformers><transformer>","1","0","","","","CC BY-SA 4.0"
"65383059","1","65385914","","2020-12-20 18:01:12","","1","195","<p>I have been using transformers fine up until today. However, when I imported the package today, I received this error message:</p>
<pre><code>In Transformers v4.0.0, the default path to cache downloaded models changed from '~/.cache/torch/transformers' to '~/.cache/huggingface/transformers'. Since you don't seem to have overridden and '~/.cache/torch/transformers' is a directory that exists, we're moving it to '~/.cache/huggingface/transformers' to avoid redownloading models you have already in the cache. You should only see this message once.

Error: Destination path '/home/user/.cache/huggingface/transformers/transformers' already exists
</code></pre>
<p>I have tried to install and uninstall the package but still unable to make it work.</p>
<p>Any suggestions to fix this would be really appreciated.</p>
","13747728","","","","","2020-12-21 11:23:05","Unable to import Hugging Face transformers","<python><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"65396650","1","65464360","","2020-12-21 16:41:47","","1","93","<p>I trained a DistilBERT model with DistilBertForTokenClassification on ConLL data fro predicting NER. Training seem to have completed with no problems but I have 2 problems during evaluation phase.</p>
<ol>
<li><p>I'm getting negative loss value</p>
</li>
<li><p>During training, I used shuffle=True for DataLoader. But during evaluation, when I do shuffle=True for DataLoader, I get very poor metric results(f_1, accuracy, recall etc). But if I do shuffle = False or use a Sampler instead of shuffling I get pretty good metric results. I'm wondering if there is anything wrong with my code.</p>
</li>
</ol>
<p>Here is the evaluation code:</p>
<hr />
<pre><code>print('Prediction started on test data')
model.eval()

eval_loss = 0
predictions , true_labels = [], []

for batch in val_loader:
  b_input_ids = batch['input_ids'].to(device)
  b_input_mask = batch['attention_mask'].to(device)
  b_labels = batch['labels'].to(device)

  with torch.no_grad():
      outputs = model(b_input_ids, 
                      attention_mask=b_input_mask)

  logits = outputs[0]
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.detach().cpu().numpy()
  
  predictions.append(logits)
  true_labels.append(label_ids)

  eval_loss += outputs[0].mean().item()


print('Prediction completed')
eval_loss = eval_loss / len(val_loader)
print(&quot;Validation loss: {}&quot;.format(eval_loss))
</code></pre>
<p>out:</p>
<pre><code>Prediction started on test data
Prediction completed
Validation loss: -0.2584906197858579
</code></pre>
<p>I believe I'm calculating the loss wrong here. Is it possible to get negative loss values with BERT?</p>
<p>For DataLoader, if I use the code snippet below, I have no problems with the metric results.</p>
<pre><code>val_sampler = SequentialSampler(val_dataset)
val_loader = DataLoader(val_dataset, sampler=val_sampler, batch_size=128)
</code></pre>
<p>Bu if I do this one I get very poor metric results</p>
<pre><code>val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)
</code></pre>
<p>Is it normal that I'm getting vastly different results with shuffle=True vs shuffle=False ?</p>
<p>code for the metric calculation:</p>
<pre><code>metric = load_metric(&quot;seqeval&quot;)
results = metric.compute(predictions=true_predictions, references=true_labels)
results
</code></pre>
<p>out:</p>
<pre><code>{'LOCATION': {'f1': 0.9588207767898924,
  'number': 2134,
  'precision': 0.9574766355140187,
  'recall': 0.9601686972820993},
 'MISC': {'f1': 0.8658965344048217,
  'number': 995,
  'precision': 0.8654618473895582,
  'recall': 0.8663316582914573},
 'ORGANIZATION': {'f1': 0.9066332916145182,
  'number': 1971,
  'precision': 0.8947628458498024,
  'recall': 0.9188229325215627},
 'PERSON': {'f1': 0.9632426988922457,
  'number': 2015,
  'precision': 0.9775166070516096,
  'recall': 0.9493796526054591},
 'overall_accuracy': 0.988255561629313,
 'overall_f1': 0.9324058459808882,
 'overall_precision': 0.9322748349023465,
 'overall_recall': 0.932536893886156}
</code></pre>
<p>The above metrics are printed when I use Sampler or shuffle=False. If I use shuffle=True, I get:</p>
<pre><code>{'LOCATION': {'f1': 0.03902284263959391,
  'number': 2134,
  'precision': 0.029496402877697843,
  'recall': 0.057638238050609185},
 'MISC': {'f1': 0.010318142734307824,
  'number': 995,
  'precision': 0.009015777610818933,
  'recall': 0.012060301507537688},
 'ORGANIZATION': {'f1': 0.027420984269014285,
  'number': 1971,
  'precision': 0.019160951996772892,
  'recall': 0.04819888381532217},
 'PERSON': {'f1': 0.02119907254057635,
  'number': 2015,
  'precision': 0.01590852597564007,
  'recall': 0.03176178660049628},
 'overall_accuracy': 0.5651741788003777,
 'overall_f1': 0.02722600361161272,
 'overall_precision': 0.020301063389034663,
 'overall_recall': 0.041321152494729445}
</code></pre>
<p>UPDATE: I modified loss code for evaluation. There seems to be no problem with this code. You can see the new code below:</p>
<pre><code>print('Prediction started on test data')
model.eval()

eval_loss = 0
predictions , true_labels = [], []

for batch in val_loader:

  b_labels = batch['labels'].to(device)

  batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}
  
  with torch.no_grad():
      outputs = model(**batch)

      loss, logits = outputs[0:2]
      logits = logits.detach().cpu().numpy()
      label_ids = b_labels.detach().cpu().numpy()
  
      predictions.append(logits)
      true_labels.append(label_ids)

      eval_loss += loss


print('Prediction completed')
eval_loss = eval_loss / len(val_loader)
print(&quot;Validation loss: {}&quot;.format(eval_loss))
</code></pre>
<p>Though I still haven't got an asnwer to the DataLoader question.
Also I jsut realised when I do <code>print(model.eval())</code> I still get dropouts from the model in evaluation mode.</p>
","6093015","","6093015","","2020-12-21 18:53:32","2020-12-27 09:22:19","BERT DataLoader: Difference between shuffle=True vs Sampler?","<python><pytorch><bert-language-model><huggingface-transformers>","2","3","","","","CC BY-SA 4.0"
"65400112","1","","","2020-12-21 21:20:38","","2","393","<p>I want to include a pre-trained XLNet (or possibly another state of the art transformer) in a model to fine-tune it.</p>
<p>However, it doesn't work when I include it with keras layers.</p>
<pre><code>import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModel

inputs = tf.keras.Input(shape=2000, dtype='int32')
x = inputs
xlnetPretrainedModel = TFAutoModel.from_pretrained(&quot;xlnet-base-cased&quot;)
x = xlnetPretrainedModel(x)
x = tf.keras.layers.GlobalAveragePooling1D()(x)
x = tf.keras.layers.Dense(32, activation='relu')(x)
x = tf.keras.layers.Dense(32, activation=None)(x)
model = tf.keras.Model(inputs=inputs, outputs=x)
model.compile(optimizer='adam',
                      loss='mean_squared_error')
model.summary()
</code></pre>
<p>The bug is</p>
<pre><code>AttributeError: 'NoneType' object has no attribute 'shape'
</code></pre>
<p>at the line</p>
<pre><code>x = xlnetPretrainedModel(x)
</code></pre>
<p>So when the model is used on the input layer.</p>
<p>The XLNet model works if used on a numpy array, but then I wouldn't be able to train it.</p>
<p>The full error message is:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-23-d543506f9697&gt; in &lt;module&gt;
      5 x = inputs
      6 xlnetPretrainedModel = TFAutoModel.from_pretrained(&quot;xlnet-base-cased&quot;)
----&gt; 7 x = xlnetPretrainedModel(x)
      8 x = tf.keras.layers.GlobalAveragePooling1D()(x)
      9 x = tf.keras.layers.Dense(32, activation='relu')(x)

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    771                     not base_layer_utils.is_in_eager_or_tf_function()):
    772                   with auto_control_deps.AutomaticControlDependencies() as acd:
--&gt; 773                     outputs = call_fn(cast_inputs, *args, **kwargs)
    774                     # Wrap Tensors in `outputs` in `tf.identity` to avoid
    775                     # circular dependencies.

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    235       except Exception as e:  # pylint:disable=broad-except
    236         if hasattr(e, 'ag_error_metadata'):
--&gt; 237           raise e.ag_error_metadata.to_exception(e)
    238         else:
    239           raise

AttributeError: in converted code:

    /opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_xlnet.py:810 call  *
        outputs = self.transformer(inputs, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:805 __call__
        inputs, outputs, args, kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2014 _set_connectivity_metadata_
        input_tensors=inputs, output_tensors=outputs, arguments=arguments)
    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2044 _add_inbound_node
        arguments=arguments)
    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/node.py:110 __init__
        self.output_shapes = nest.map_structure(backend.int_shape, output_tensors)
    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:568 map_structure
        structure[0], [func(*x) for x in entries],
    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:568 &lt;listcomp&gt;
        structure[0], [func(*x) for x in entries],
    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:1172 int_shape
        shape = x.shape

    AttributeError: 'NoneType' object has no attribute 'shape'
</code></pre>
<p>or after trying a solution presented here <a href=""https://github.com/huggingface/transformers/issues/1350"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/1350</a> by decoring the call by a tf.function</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-16-c852fba5aa15&gt; in &lt;module&gt;
      8 xlnetPretrainedModel = TFAutoModel.from_pretrained(&quot;xlnet-base-cased&quot;)
      9 xlnetPretrainedModel.call = tf.function(xlnetPretrainedModel.transformer.call)
---&gt; 10 x = xlnetPretrainedModel(x)
     11 
     12 x = tf.keras.layers.GlobalAveragePooling1D()(x)

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    803               kwargs.pop('mask')
    804             inputs, outputs = self._set_connectivity_metadata_(
--&gt; 805                 inputs, outputs, args, kwargs)
    806           self._handle_activity_regularization(inputs, outputs)
    807           self._set_mask_metadata(inputs, outputs, input_masks)

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in _set_connectivity_metadata_(self, inputs, outputs, args, kwargs)
   2012     # This updates the layer history of the output tensor(s).
   2013     self._add_inbound_node(
-&gt; 2014         input_tensors=inputs, output_tensors=outputs, arguments=arguments)
   2015     return inputs, outputs
   2016 

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in _add_inbound_node(self, input_tensors, output_tensors, arguments)
   2042         input_tensors=input_tensors,
   2043         output_tensors=output_tensors,
-&gt; 2044         arguments=arguments)
   2045 
   2046     # Update tensor history metadata.

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/node.py in __init__(self, outbound_layer, inbound_layers, node_indices, tensor_indices, input_tensors, output_tensors, arguments)
    108     self.input_shapes = nest.map_structure(backend.int_shape, input_tensors)
    109     # Nested structure of shape tuples, shapes of output_tensors.
--&gt; 110     self.output_shapes = nest.map_structure(backend.int_shape, output_tensors)
    111 
    112     # Optional keyword arguments to layer's `call`.

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py in map_structure(func, *structure, **kwargs)
    566 
    567   return pack_sequence_as(
--&gt; 568       structure[0], [func(*x) for x in entries],
    569       expand_composites=expand_composites)
    570 

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py in &lt;listcomp&gt;(.0)
    566 
    567   return pack_sequence_as(
--&gt; 568       structure[0], [func(*x) for x in entries],
    569       expand_composites=expand_composites)
    570 

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in int_shape(x)
   1170   &quot;&quot;&quot;
   1171   try:
-&gt; 1172     shape = x.shape
   1173     if not isinstance(shape, tuple):
   1174       shape = tuple(shape.as_list())

AttributeError: 'NoneType' object has no attribute 'shape'
</code></pre>
<p>Please, can anyone help me fix this error?</p>
","14867995","","","","","2021-02-14 13:53:45","Train a model using XLNet transformers from huggingface package","<python><tensorflow><keras><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"67595500","1","67599169","","2021-05-19 00:34:01","","0","2004","<pre><code>https://huggingface.co/models
</code></pre>
<p>For example, I want to download 'bert-base-uncased', but cann't find a 'Download' link. Please help. Or is it not downloadable?</p>
","3943868","","","","","2021-09-30 09:58:16","How to download model from huggingface?","<huggingface-transformers><transformer>","3","0","","","","CC BY-SA 4.0"
"67635055","1","67665158","","2021-05-21 10:24:36","","0","40","<p>I am writing the code to train a <code>bert</code> model on my dataset. By when I run the code it throws an error in the average pool layer. I am unable to understand what causes this error.</p>
<h2>Model</h2>
<pre><code>class BERTBaseUncased(nn.Module):
    def __init__(self, bert_path):
        super(BERTBaseUncased, self).__init__()
        self.bert_path = bert_path
        self.bert = transformers.BertModel.from_pretrained(self.bert_path)
        self.bert_drop = nn.Dropout(0.3)
        self.out = nn.Linear(768 * 2, 1)

    def forward(
            self,
            ids,
            mask,
            token_type_ids
    ):
        o1, _ = self.bert(
            ids,
            attention_mask=mask,
            token_type_ids=token_type_ids)
        
        apool = torch.mean(o1, 1)
        mpool, _ = torch.max(o1, 1)
        cat = torch.cat((apool, mpool), 1)

        bo = self.bert_drop(cat)
        p2 = self.out(bo)
        return p2
</code></pre>
<h2>Error</h2>
<pre><code>Exception in device=TPU:0: mean() received an invalid combination of arguments - got (str, int), but expected one of:
 * (Tensor input, *, torch.dtype dtype)
 * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)
 * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)

Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py&quot;, line 228, in _start_fn
    fn(gindex, *args)
  File &quot;&lt;ipython-input-12-94e926c1f4df&gt;&quot;, line 4, in _mp_fn
    a = _run()
  File &quot;&lt;ipython-input-5-ef9fa564682f&gt;&quot;, line 146, in _run
    train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)
  File &quot;&lt;ipython-input-5-ef9fa564682f&gt;&quot;, line 22, in train_loop_fn
    token_type_ids=token_type_ids
  File &quot;/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 577, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;&lt;ipython-input-11-9196e0d23668&gt;&quot;, line 73, in forward
    apool = torch.mean(o1, 1)
TypeError: mean() received an invalid combination of arguments - got (str, int), but expected one of:
 * (Tensor input, *, torch.dtype dtype)
 * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)
 * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)
</code></pre>
<p>I am trying to run this on a Kaggle TPU. How to fix this?</p>
","13540652","","","","","2021-05-23 23:12:13","Python: BERT Model Pooling Error - mean() received an invalid combination of arguments - got (str, int)","<python><nlp><pytorch><kaggle><huggingface-transformers>","1","0","1","","","CC BY-SA 4.0"
"67694920","1","","","2021-05-25 20:04:26","","1","64","<p>Use case, I am training model 1 (using train1) with a specific loss function that involves tensor A. I am accumulating the loss and then want to perform an update. Next I am training a second model 2 (train2) in which I want to calculate the gradients wrt A using the loss calculated in train2. Thus I am adding loss 1 to loss2.</p>
<pre class=""lang-py prettyprint-override""><code>#reproduce error
from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel
import torch
import torch.nn.functional as F
model1 = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints
model2 = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints


optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)
A=torch.rand(1, requires_grad=True)
optimizer3 = torch.optim.SGD([A], lr=0.1)

en_input=torch.tensor([[1,2], [3,4]])
en_masks=torch.tensor([[0,0], [0,0]])
de_output=torch.tensor([[3,1], [4,2]])
de_masks=torch.tensor([[0,0], [0,0]])
lm_labels=torch.tensor([[5,7], [6,8]])

torch.autograd.set_detect_anomaly(True)

def train1():
  acc=torch.zeros(1)
  for i in range(2):
    optimizer1.zero_grad()
    out = model1(input_ids=en_input, attention_mask=en_masks, decoder_input_ids=de_output, 
                        decoder_attention_mask=de_masks, labels=lm_labels.clone())
          

    prediction_scores = out[1]
    predictions = F.log_softmax(prediction_scores, dim=2)
    p=((predictions.sum() - de_output.sum())*A).sum()
    p=torch.unsqueeze(p, dim=0)
    acc = torch.cat((p,acc)) # accumulating the loss 

  loss=acc.sum()
  A.retain_grad()
  loss.backward(retain_graph=True) 
  optimizer1.step() 
  return loss


def train2(loss1):
  for i in range (2):
     optimizer3.zero_grad()
     output = model2(input_ids=en_input, attention_mask=en_masks, 
                               decoder_input_ids=de_output, 
                      decoder_attention_mask=de_masks, labels=lm_labels.clone())
        
     prediction_scores_ = output[1]
     predictions_= F.log_softmax(prediction_scores_, dim=2)
     loss2=((predictions_.sum() - de_output.sum())).sum()+loss1 # want to calculate gradients wrt A
     A.retain_grad()
     loss2.backward(inputs=[A], retain_graph=True) 
     optimizer3.step() #update A based on calculated gradients

loss1=train1()
train2(loss1)

</code></pre>
<p>If this is the right method, I am not understanding whats wrong in my code? If its not right, I would appreciate if someone pointed me in the right direction</p>
<p>Error trace</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:147: UserWarning: Error detected in MulBackward0. Traceback of forward call that caused the error:
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py&quot;, line 16, in &lt;module&gt;
    app.launch_new_instance()
  File &quot;/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py&quot;, line 845, in launch_instance
    app.start()
  File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py&quot;, line 499, in start
    self.io_loop.start()
  File &quot;/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py&quot;, line 132, in start
    self.asyncio_loop.run_forever()
  File &quot;/usr/lib/python3.7/asyncio/base_events.py&quot;, line 541, in run_forever
    self._run_once()
  File &quot;/usr/lib/python3.7/asyncio/base_events.py&quot;, line 1786, in _run_once
    handle._run()
  File &quot;/usr/lib/python3.7/asyncio/events.py&quot;, line 88, in _run
    self._context.run(self._callback, *self._args)
  File &quot;/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py&quot;, line 122, in _handle_events
    handler_func(fileobj, events)
  File &quot;/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py&quot;, line 300, in null_wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 451, in _handle_events
    self._handle_recv()
  File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 480, in _handle_recv
    self._run_callback(callback, msg)
  File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 434, in _run_callback
    callback(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py&quot;, line 300, in null_wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 233, in dispatch_shell
    handler(stream, idents, msg)
  File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 399, in execute_request
    user_expressions, allow_stdin)
  File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py&quot;, line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py&quot;, line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2718, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2822, in run_ast_nodes
    if self.run_code(code, result):
  File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File &quot;&lt;ipython-input-66-c603f915c713&gt;&quot;, line 78, in &lt;module&gt;
    loss1=train1()
  File &quot;&lt;ipython-input-66-c603f915c713&gt;&quot;, line 25, in train1
    p=((predictions.sum() - de_output.sum())*A).sum()
 (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-66-c603f915c713&gt; in &lt;module&gt;()
     77 for i in range(2):
     78   loss1=train1()
---&gt; 79   train2(loss1)

2 frames
&lt;ipython-input-66-c603f915c713&gt; in train2(loss1)
     69     print(A.grad)
     70     #loss2.grad(inputs=A,outputs=A, only_inputs=True)
---&gt; 71     loss2.backward(inputs=[A],retain_graph=True) #calculates gradients # retain_graph=True #list(dec.parameters())
     72     print(A.grad)
     73     # torch.nn.utils.clip_grad_norm_(model1.parameters(), 1.0)

/usr/local/lib/python3.7/dist-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)
    243                 create_graph=create_graph,
    244                 inputs=inputs)
--&gt; 245         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
    246 
    247     def register_hook(self, hook):

/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    145     Variable._execution_engine.run_backward(
    146         tensors, grad_tensors_, retain_graph, create_graph, inputs,
--&gt; 147         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
    148 
    149 

RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]] is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
</code></pre>
","13174257","","6664872","","2021-05-26 15:40:52","2021-05-26 15:40:52","RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation. Can't understand why?","<pytorch><huggingface-transformers>","0","4","","","","CC BY-SA 4.0"
"65990855","1","","","2021-02-01 10:17:10","","0","70","<p>I am currently using <strong>TFlongformerForSequenceClassification</strong> from the <strong>transformers</strong> library, and I am having this error :</p>
<pre><code>  File &quot;prosenet.py&quot;, line 116, in &lt;module&gt;
    trainer.train()
  File &quot;/home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/transformers/trainer_tf.py&quot;, line 549, in train    self.distributed_training_steps(batch)
  File &quot;/home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 828, in __call__
    result = self._call(*args, **kwds)
  File &quot;/home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 871, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File &quot;/home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 726, in _initialize
    *args, **kwds))
  File &quot;/home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 2969, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File &quot;/home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 3361, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File &quot;/home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 3206, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File &quot;/home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 990, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File &quot;/home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 634, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File &quot;/home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 3887, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File &quot;/home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 977, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/transformers/trainer_tf.py:671 distributed_training_steps  *
        self.args.strategy.run(self.apply_gradients, inputs)
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/transformers/trainer_tf.py:643 apply_gradients  *
        self.training_step(reduced_features, reduced_labels, nb_instances_in_global_batch)
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/transformers/trainer_tf.py:616 training_step  *
        per_example_loss, _ = self.run_model(features, labels, True)
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/transformers/trainer_tf.py:718 run_model  *
        outputs = self.model(features, labels=labels, training=training)[:2]
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/transformers/models/longformer/modeling_tf_longformer.py:2377 call  *
        inputs[&quot;global_attention_mask&quot;] = tf.tensor_scatter_nd_update(
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **
        return target(*args, **kwargs)
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5512 tensor_scatter_nd_update
        tensor=tensor, indices=indices, updates=updates, name=name)
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:11247 tensor_scatter_update
        updates=updates, name=name)
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:592 _create_op_internal
        compute_device)
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3536 _create_op_internal
        op_def=op_def)
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:2016 __init__
        control_input_ops, op_def)
    /home/pfrod/anaconda3/envs/env_minus/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1856 _create_c_op
        raise ValueError(str(e))

    ValueError: Dimensions [2,3) of input[shape=[8,1,512]] = [512] must match dimensions [1,1) of updates[shape=[8]] = []: Shapes must be equal rank, but are 1 and 0 for '{{node while/tf_longformer_for_sequence_classification/TensorScatterUpdate}} = TensorScatterUpdate[T=DT_INT32, Tindices=DT_INT32](while/tf_longformer_for_sequence_classification/zeros_like, while/tf_longformer_for_sequence_classification/TensorScatterUpdate/indices, while/tf_longformer_for_sequence_classification/TensorScatterUpdate/updates)' with input shapes: [8,1,512], [8,2], [8].
</code></pre>
<p><strong>when running the following code :</strong></p>
<pre><code>from transformers import TFLongformerForSequenceClassification, LongformerTokenizer, TFTrainer, TFTrainingArguments

import tensorflow as tf
from tensorflow.data import Dataset
from pathlib import Path
from tqdm import tqdm
from sklearn.model_selection import train_test_split
GPU = tf.config.list_physical_devices('GPU')[0]
tf.config.experimental.set_virtual_device_configuration(GPU, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8192//2)])

tokenizer = LongformerTokenizer.from_pretrained('../storage/tokenizer', max_length = 2048)
model = TFLongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',
                                                               gradient_checkpointing=True,
                                                               attention_window = 512)

PATH = Path(&quot;../storage/treated_articles&quot;)

iterd = PATH.iterdir()
dat = []
labels = []
for label in iterd:
    for article in tqdm(label.iterdir()):
        dat.append(str(article))
        labels.append(str(label)[-17 :] == '/RELEVANT_TREATED')
#print(dat[:100])
files_train, files_test, y_train, y_test = train_test_split(dat, labels, test_size = 0.33, shuffle = True)

x_t = {'input_ids' : [None]*len(files_train), 'attention_mask' : [None]*len(files_train)}
for i, file in enumerate(files_train) : 
    tok = tokenizer.encode_plus(open(file, 'r').read().replace('\n\n','. ').replace('..', '.').replace('\n', ''), padding = 'max_length', truncation = True, max_length = 512, return_tensors = 'tf')
    x_t['input_ids'][i] = tok['input_ids']
    x_t['attention_mask'][i] = tok['attention_mask']

x_te = {'input_ids' : [None]*len(files_test), 'attention_mask' : [None]*len(files_test)}
for i, file in enumerate(files_test) : 
    tok = tokenizer.encode_plus(open(file, 'r').read().replace('\n\n','. ').replace('..', '.').replace('\n', ''), 
padding = 'max_length', 
truncation = True, 
max_length = 512, 
return_tensors = 'tf')
    x_te['input_ids'][i] = tok['input_ids']
    x_te['attention_mask'][i] = tok['attention_mask']
data_x_train = Dataset.from_tensor_slices(x_t)
data_y_train = Dataset.from_tensor_slices([int(y) for y in y_train])
data_train = Dataset.zip((data_x_train, data_y_train))
data_x_test = Dataset.from_tensor_slices(x_te)
data_y_test = Dataset.from_tensor_slices([int(y) for y in y_test])
data_test = Dataset.zip((data_x_test, data_y_test))

training_args = TFTrainingArguments(
    output_dir = '../results/interpretable_longformer',
    num_train_epochs = 8,
    per_device_train_batch_size = 8,
    gradient_accumulation_steps = 8,    
    per_device_eval_batch_size= 8,
    evaluation_strategy = &quot;epoch&quot;,
    disable_tqdm = False, 
    #load_best_model_at_end=True,
    warmup_steps=150,
    weight_decay=0.01,
    logging_steps = 4,
    fp16 = True,
    logging_dir='../results/logging_interpretable_longformer',
    #dataloader_num_workers = 0,
    run_name = 'longformer-classification-updated-rtx3090_paper_replication_2_warm', 
)

trainer = TFTrainer(model=model, args=training_args,
                               train_dataset=data_train, eval_dataset=data_test)

trainer.train()
</code></pre>
<p>I have been struggling with this error for almost a week now, I have also tried with this dataset processing :</p>
<pre><code>from transformers import BertTokenizer, glue_convert_examples_to_features
import tensorflow as tf
import tensorflow_datasets as tfds
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
data = tfds.load('glue/mrpc')
train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=2048, task='mrpc')
data_train = train_dataset.shuffle(100).batch(8)
test_dataset = glue_convert_examples_to_features(data['test'], tokenizer, max_length=2048, task='mrpc')
data_test = train_dataset.shuffle(100).batch(8)
</code></pre>
<p><strong>which is directly available here :</strong>
<a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/training.html</a>
Both the former and latter elements of the dataset have the same format when I iterate on it :</p>
<pre><code>({'input_ids':[1, 3, 189, ...], 'attention_mask' : [1, 1, 1, ..., 0, 0, ...]}, 1)
</code></pre>
<p>However, even this processing doesn't seem to work, be It with the trainer code above, or this piece of code :</p>
<pre><code>model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5), loss = 'binary_crossentropy')
model.fit(data_train, epochs=3)
</code></pre>
<p>Can someone help me ? Thanks in advance.</p>
","15121781","","15121781","","2021-02-01 13:51:26","2021-02-01 13:51:26","tf.tensor_scatter_nd_update Value Error using HuggingFace's TFlongformerForSequenceClassification","<python><tensorflow><nlp><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"66005027","1","","","2021-02-02 07:05:27","","2","797","<p>I need to transfer a <code>pytorch_model.bin</code> of a pretrained deeppavlov ruBERT model but I have a file size limit. So I split it into chunks using python, transferred and reassembled in the correct order. However, the size of the file increased, and when I tried to load the resulting file using <code>BertModel.from_pretrained(pytorch_model.bin)</code> I received an error:</p>
<pre><code>During handling of the above exception, another exception occurred:
OSError: Unable to load weights from pytorch checkpoint &lt;...&gt;
</code></pre>
<p>So my question is: is it actually possible to split the file like that? I could possibly have a mistake in the way I split and reassemble the file. However, this could also be some version mismatch.</p>
<p>My python code to get chunks:</p>
<pre><code>chunk_size = 40000000
file_num = 1
with open(&quot;pytorch_model.bin&quot;, &quot;rb&quot;) as f:
    chunk = f.read(chunk_size)
    while chunk:
        with open(&quot;chunk_&quot; + str(file_num), &quot;wb&quot;) as chunk_file:
            chunk_file.write(chunk)
        file_num += 1
        chunk = f.read(chunk_size)

</code></pre>
<p>Code to reassemble one file:</p>
<pre><code>chunks = !ls | grep chunk_
chunks = sorted(chunks, key=lambda x: int(x.split(&quot;_&quot;)[-1]))

for chunk in chunks:
    with open(chunk, &quot;rb&quot;) as f:
        contents = f.read()
    if chunk == chunks[0]:
        write_mode = &quot;wb&quot;
    else:
        write_mode = &quot;ab&quot;
    with open(&quot;pytorch_model.bin&quot;, write_mode) as f:
        f.write(contents)
</code></pre>
<p>python 3.7.0, torch 1.5.1, transformers 4.2.2. I have no way to move files bigger than 40 MB.</p>
<p>TIA for your help!</p>
","9183762","","","","","2021-08-06 18:15:46","Unable to load weights from pytorch checkpoint after splitting pytorch_model.bin into chunks","<python><serialization><pytorch><huggingface-transformers><bin>","2","0","","","","CC BY-SA 4.0"
"65408757","1","65419118","","2020-12-22 12:28:05","","3","284","<p><strong>Context</strong></p>
<p>I am using MarianMT von Huggingface via Python in order to translate text from a source to a target language.</p>
<p><strong>Expected behaviour</strong></p>
<p>I enter a sequence into the MarianMT model and get this sequence translated back. For this, I use a corresponding language model and a tokeniser. All the sentences I enter also come back. The sentences are treated as a sequence.</p>
<p><strong>Current behaviour</strong></p>
<p>Depending on the language model, the model does not translate everything, but only returns parts. In this example, the last sentence is missing:</p>
<p><strong>Original (German):</strong> Ein Nilpferd lief im Dschungel rum und musste aufs WC. Da traf es einen Kakadu und fragte nach dem Weg. Der sagte wenn du Kaka musst, dann pass mal ganz kurz auf. Ich sag dir wo du hingehen musst, ich kenn mich hier gut aus.</p>
<p><strong>Result (English):</strong>  A hippopotamus ran around in the jungle and had to go to the toilet. There was a cockatoo and asked for the way. He said if you have to Kaka, then watch out for a minute. <strong>I'll tell you where you have to go, I know my way around here.</strong></p>
<p><strong>Result (Dutch):</strong>  Een nijlpaard liep rond in de jungle en moest naar het toilet... en een kaketoe vroeg naar de weg... die zei dat als je Kaka moest, ik even moest oppassen.</p>
<p><strong>Current Code</strong></p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM


def translate_text(input, source, target):

    # Prepare output
    output = &quot;&quot;

    model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-&quot; + source + &quot;-&quot; + target)
    tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-&quot; + source + &quot;-&quot; + target)

    inputs = tokenizer.encode(input[:512], return_tensors=&quot;pt&quot;, padding='longest')
    outputs = model.generate(inputs, max_length=4000, num_beams=4, early_stopping=True)

    for t in [tokenizer.convert_ids_to_tokens(s) for s in outputs.tolist()[0]]:
        output = output + t.replace(&quot;â–&quot;, &quot; &quot;).replace(&quot;&lt;/s&gt;&quot;, &quot;&quot;)

    output.replace(&quot;&lt;pad&gt;&quot;, &quot;&quot;)

    return output


print(translate_text(&quot;Ein Nilpferd lief im Dschungel rum und musste aufs WC. Da traf es einen Kakadu und fragte nach dem Weg. Der sagte wenn du Kaka musst, dann pass mal ganz kurz auf. Ich sag dir wo du hingehen musst, ich kenn mich hier gut aus.&quot;, &quot;de&quot;, &quot;nl&quot;))
print(translate_text(&quot;Ein Nilpferd lief im Dschungel rum und musste aufs WC. Da traf es einen Kakadu und fragte nach dem Weg. Der sagte wenn du Kaka musst, dann pass mal ganz kurz auf. Ich sag dir wo du hingehen musst, ich kenn mich hier gut aus.&quot;, &quot;de&quot;, &quot;en&quot;))
</code></pre>
<p><strong>Help needed</strong></p>
<p>What do I miss? Why are some sequence parts missing?</p>
","4875926","","4875926","","2020-12-22 12:50:18","2020-12-23 04:39:45","Huggingface MarianMT translators lose content, depending on the model","<huggingface-transformers><machine-translation><huggingface-tokenizers>","1","6","2","","","CC BY-SA 4.0"
"65422011","1","","","2020-12-23 09:36:01","","0","42","<p>My code is as follows:</p>
<pre><code>import tensorflow as tf
from transformers import RobertaConfig, TFRobertaMainLayer

# 1. Create a class to be able to use fit()

class Transformer(tf.keras.Model):
  def __init__(self):
    super(Transformer, self).__init__()

    config = RobertaConfig(
        vocab_size=100,
        hidden_size=128,
        intermediate_size=128, 
        max_position_embeddings=514,
        num_attention_heads=8,
        num_hidden_layers=6,
        type_vocab_size=1,
    )  
    self.encoder = TFRobertaMainLayer(config)

  def call(self, inp, training=False):
    return self.encoder(inp)[0]

model = Transformer()

# 2. Calculating loss manually for dummy input

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
x = tf.constant([[1, 0]])
y_true = tf.constant([[1, 0]])
y_pred = model((x, x))
loss = loss_fn(y_true, y_pred)
print(loss) #                              printing 4.8093767

# 3. Run fit()

model.compile(loss=loss_fn)
model.fit((x, x), y_true) #                printing 4.7854
</code></pre>
<p>The losses are different:</p>
<blockquote>
<p>tf.Tensor(4.8093767, shape=(), dtype=float32) 1/1
[==============================] - 0s 0s/step - loss: 4.7854</p>
</blockquote>
","5561472","","5561472","","2020-12-23 11:10:27","2020-12-23 11:10:27","Why loss printed by fit() differs from loss from custom loop for huggingface RoBERTa?","<python><tensorflow><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"67706707","1","","","2021-05-26 14:11:35","","1","57","<p>I have followed <a href=""https://huggingface.co/transformers/custom_datasets.html#tok-ner"" rel=""nofollow noreferrer"">this tutorial</a> on a custom dataset of mine, and I have now a trained model.</p>
<p>I would like to use <code>seqeval</code>'s <code>classification_report</code> function so that I have a nice display of the performances of my model.</p>
<p>On which data should I call <code>classification_report</code>?</p>
","11348232","","","","","2021-05-26 14:11:35","How to use seqeval classification_report after having performed NER with HuggingFace transformers?","<nlp><huggingface-transformers><named-entity-recognition>","0","0","","","","CC BY-SA 4.0"
"60068129","1","","","2020-02-05 02:16:04","","2","1481","<p>Referring to the <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html"" rel=""nofollow noreferrer"">documentation</a> of the awesome Transformers library from Huggingface, I came across the <code>add_tokens</code> functions.</p>

<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])
model.resize_token_embeddings(len(tokenizer))
</code></pre>

<p>I tried the above by adding previously absent words in the default vocabulary. However, keeping all else constant, I noticed a decrease in accuracy of the fine tuned classifier making use of this updated <code>tokenizer</code>. I was able to replicate similar behavior even when just 10% of the previously absent words were added.</p>

<p>My questions</p>

<ol>
<li>Am I missing something?</li>
<li>Instead of whole words, is the <code>add_tokens</code> function expecting masked tokens, for example :  <code>'##ah'</code>, <code>'##red'</code>, <code>'##ik'</code>, <code>'##si</code>', etc.? If yes, is there a procedure to generate such masked tokens?</li>
</ol>

<p>Any help would be appreciated.</p>

<p>Thanks in advance.</p>
","799188","","","","","2020-02-05 10:32:28","Transformers PreTrainedTokenizer add_tokens Functionality","<pytorch><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"65433047","1","","","2020-12-24 01:51:54","","0","76","<p>Some Chinese Text has some English words, for example: &quot;Applesæ˜¯è‹¹æžœçš„å¤æ•°å½¢å¼ã€‚&quot;. I have questions about how to tokenize the text:</p>
<ol>
<li>why Chinese Bert Case sensitive, but I can't find even 'A' in vocab.txt</li>
<li>Because English words in Chinese vocab.txt is few, should I use wordpiece tokenizer as default, like &quot;['apple', '##s', 'æ˜¯', 'è‹¹', ...]&quot;or split to char to tokenize, like &quot;['a', 'p', 'p', 'l', 'e', 's', 'æ˜¯', 'è‹¹', ...]&quot;?</li>
</ol>
","14434387","","","","","2020-12-24 01:51:54","Why Bert-chinese use do_lower_case=False?","<bert-language-model><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"66096703","1","66116061","","2021-02-08 06:16:32","","1","1215","<p>I'm dealing with a huge text dataset for content classification. I've implemented the distilbert model and distilberttokenizer.from_pretrained() tokenizer..
This tokenizer is taking incredibly long to tokenizer my text data roughly 7 mins for just 14k records and that's because it runs on my CPU.</p>
<p>Is there any way to force the tokenizer to run on my GPU.</p>
","13590837","","","","","2021-02-09 09:20:08","Running huggingface Bert tokenizer on GPU","<deep-learning><nlp><huggingface-transformers><huggingface-tokenizers>","1","1","1","","","CC BY-SA 4.0"
"66232938","1","","","2021-02-16 22:14:25","","2","709","<p>I have a sentence and I need to return the text corresponding to N BERT tokens to the left and right of a specific word.</p>
<pre><code>from transformers import BertTokenizer
tz = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
sentence = &quot;The Natural Science Museum of Madrid shows the RECONSTRUCTION of a dinosaur&quot;

tokens = tz.tokenize(sentence)
print(tokens)

&gt;&gt;['The', 'Natural', 'Science', 'Museum', 'of', 'Madrid', 'shows', 'the', 'R', '##EC', '##ON', '##ST', '##R', '##UC', '##TI', '##ON', 'of', 'a', 'dinosaur']
</code></pre>
<p>What I want is to get the text corresponding to 4 tokens to the left and to the right of the token Madrid. So i want the tokens: ['Natural', 'Science', 'Museum', 'of', 'Madrid', 'shows', 'the', 'R', '##EC'] and then transform them into the original text. In this case it would be 'Natural Science Museum of Madrid shows the REC'.</p>
<p>Is there a way to do this?</p>
","7973042","","6664872","","2021-02-20 13:19:33","2021-02-20 13:35:29","How to untokenize BERT tokens?","<python><tokenize><bert-language-model><huggingface-transformers><huggingface-tokenizers>","2","0","","","","CC BY-SA 4.0"
"65445371","1","","","2020-12-25 04:52:21","","1","379","<p>I'm having a project for ner, and i want to use pipline component of spacy for ner with word vector generated from a pre-trained model in the transformer. I using spacy-transformer of spacy and follow their guild but it not work.<br />
I'm using spacy-2.3.5, transformer-0.6.2, python-2.3.5 and trying to run it in colab.<br />
Link of spacy-transformer GitHub: <a href=""https://github.com/explosion/spacy-transformers/tree/v0.6.x"" rel=""nofollow noreferrer"">Link to git hub</a><br />
Link to the model in the transformer: <a href=""https://huggingface.co/vinai/phobert-base"" rel=""nofollow noreferrer"">Link to vinai/phobert-base</a><br />
Name of model in tranform: <code>vinai/phobert-base</code><br />
I have a question: Whether we can use any pre-train model in the transformer via spacy-transformer or just some kind of model?</p>
<p>In their guild, before loading pre-trained model in spacy we need to initialize it. <a href=""https://pypi.org/project/spacy-transformers/"" rel=""nofollow noreferrer"">here their guild</a></p>
<pre><code>! export CUDA_PATH=&quot;/opt/nvidia/cuda&quot;
! pip install -U spacy[cuda101]
! pip install spacy-transformers
! git clone -b v0.6.x https://github.com/explosion/spacy-transformers
! python /content/spacy-transformers/examples/init_model.py -n &quot;vinai/phobert-base&quot; \
-l vi vi_vinai_phobert_base
</code></pre>
<p>I received a log:</p>
<pre><code>2020-12-25 03:46:18.163785: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
â„¹ Creating model for 'vinai/phobert-base' (vi)
Downloading: 100% 557/557 [00:00&lt;00:00, 689kB/s]
â ¼ Setting up the pipeline...
Traceback (most recent call last):
  File &quot;/content/spacy-transformers/examples/init_model.py&quot;, line 32, in &lt;module&gt;
    plac.call(main)
  File &quot;/usr/local/lib/python3.6/dist-packages/plac_core.py&quot;, line 367, in call
    cmd, result = parser.consume(arglist)
  File &quot;/usr/local/lib/python3.6/dist-packages/plac_core.py&quot;, line 232, in consume
    return cmd, self.func(*(args + varargs + extraopts), **kwargs)
  File &quot;/content/spacy-transformers/examples/init_model.py&quot;, line 19, in main
    nlp.add_pipe(TransformersWordPiecer.from_pretrained(nlp.vocab, name))
  File &quot;/usr/local/lib/python3.6/dist-packages/spacy_transformers/pipeline/wordpiecer.py&quot;, line 26, in from_pretrained
    model = get_tokenizer(trf_name).from_pretrained(trf_name)
  File &quot;/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py&quot;, line 393, in from_pretrained
    return cls._from_pretrained(*inputs, **kwargs)
  File &quot;/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py&quot;, line 496, in _from_pretrained
    list(cls.vocab_files_names.values()),
OSError: Model name 'vinai/phobert-base' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'vinai/phobert-base' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
</code></pre>
<p>Anyone can help me out or give a give me advice? Thank you so much !</p>
","11783168","","","","","2020-12-25 04:52:21","Can not initializing models from the huggingface models repo in spacy","<python><google-colaboratory><spacy><huggingface-transformers><ner>","0","0","","","","CC BY-SA 4.0"
"65445651","1","","","2020-12-25 05:54:18","","2","1902","<p>I am trying to explore <a href=""https://huggingface.co/transformers/model_doc/t5.html#"" rel=""nofollow noreferrer"">T5</a></p>
<p>this is the code</p>
<pre><code>!pip install transformers
from transformers import T5Tokenizer, T5ForConditionalGeneration
qa_input = &quot;&quot;&quot;question: What is the capital of Syria? context: The name &quot;Syria&quot; historically referred to a wider region,
 broadly synonymous with the Levant, and known in Arabic as al-Sham. The modern state encompasses the sites of several ancient 
 kingdoms and empires, including the Eblan civilization of the 3rd millennium BC. Aleppo and the capital city Damascus are 
 among the oldest continuously inhabited cities in the world.&quot;&quot;&quot;
tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')
input_ids = tokenizer.encode(qa_input, return_tensors=&quot;pt&quot;)  # Batch size 1
outputs = model.generate(input_ids)
output_str = tokenizer.decode(outputs.reshape(-1))

</code></pre>
<p>I got this error:</p>
<pre><code>---------------------------------------------------------------------------

ImportError                               Traceback (most recent call last)

&lt;ipython-input-2-8d24c6a196e4&gt; in &lt;module&gt;()
      5  kingdoms and empires, including the Eblan civilization of the 3rd millennium BC. Aleppo and the capital city Damascus are
      6  among the oldest continuously inhabited cities in the world.&quot;&quot;&quot;
----&gt; 7 tokenizer = T5Tokenizer.from_pretrained('t5-small')
      8 model = T5ForConditionalGeneration.from_pretrained('t5-small')
      9 input_ids = tokenizer.encode(qa_input, return_tensors=&quot;pt&quot;)  # Batch size 1

1 frames

/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py in requires_sentencepiece(obj)
    521     name = obj.__name__ if hasattr(obj, &quot;__name__&quot;) else obj.__class__.__name__
    522     if not is_sentencepiece_available():
--&gt; 523         raise ImportError(SENTENCEPIECE_IMPORT_ERROR.format(name))
    524 
    525 

ImportError: 
T5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment.


--------------------------------------------------------------------------
</code></pre>
<p>after that I install sentencepiece library as was suggested like this:</p>
<pre><code>!pip install transformers
!pip install sentencepiece

from transformers import T5Tokenizer, T5ForConditionalGeneration
qa_input = &quot;&quot;&quot;question: What is the capital of Syria? context: The name &quot;Syria&quot; historically referred to a wider region,
 broadly synonymous with the Levant, and known in Arabic as al-Sham. The modern state encompasses the sites of several ancient 
 kingdoms and empires, including the Eblan civilization of the 3rd millennium BC. Aleppo and the capital city Damascus are 
 among the oldest continuously inhabited cities in the world.&quot;&quot;&quot;
tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')
input_ids = tokenizer.encode(qa_input, return_tensors=&quot;pt&quot;)  # Batch size 1
outputs = model.generate(input_ids)
output_str = tokenizer.decode(outputs.reshape(-1))

</code></pre>
<p>but I got another issue:</p>
<blockquote>
<p>Some weights of the model checkpoint at t5-small were not used when
initializing T5ForConditionalGeneration:
['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']</p>
<ul>
<li>This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another
architecture (e.g. initializing a BertForSequenceClassification model
from a BertForPreTraining model).</li>
<li>This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you
expect to be exactly identical (initializing a
BertForSequenceClassification model from a
BertForSequenceClassification model).</li>
</ul>
</blockquote>
<p>so I did not understand what is going on, any explanation?</p>
","14494113","","14494113","","2020-12-25 06:57:14","2021-04-17 13:19:14","T5Tokenizer requires the SentencePiece library but it was not found in your environment","<huggingface-transformers><google-colaboratory>","1","6","","","","CC BY-SA 4.0"
"66255377","1","","","2021-02-18 07:24:24","","0","117","<p>I am trying to convert the bert-base-cased model from pytorch to ONNX in Google Colab, so I ran this command</p>
<pre><code>!python3 transformers/src/transformers/convert_graph_to_onnx.py --framework pt --model bert-base-cased bert-base-cased.onnx
</code></pre>
<p>but I ran into this error</p>
<pre><code>Traceback (most recent call last):
  File &quot;transformers/src/transformers/convert_graph_to_onnx.py&quot;, line 22, in &lt;module&gt;
    from .file_utils import ModelOutput, is_tf_available, is_torch_available
ModuleNotFoundError: No module named '__main__.file_utils'; '__main__' is not a package
</code></pre>
<p>Many people have run this command before and have not had any issues, any ideas as to why it is happening to me?</p>
","14882176","","","","","2021-02-18 07:24:24","ModuleNotFoundError: No module named '__main__.file_utils'; '__main__' is not a package in Hugginface's Transformers Library","<python><nlp><pytorch><huggingface-transformers><onnx>","0","1","","","","CC BY-SA 4.0"
"65419656","1","65421607","","2020-12-23 05:55:15","","1","109","<p>I am looking to use <code>bert-english-uncased-finetuned-pos</code> transformer, mentioned here</p>
<p><a href=""https://huggingface.co/vblagoje/bert-english-uncased-finetuned-pos?text=My+name+is+Clara+and+I+live+in+Berkeley%2C+California"" rel=""nofollow noreferrer"">https://huggingface.co/vblagoje/bert-english-uncased-finetuned-pos?text=My+name+is+Clara+and+I+live+in+Berkeley%2C+California</a>.</p>
<p>I am querying the transformer this way...</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained(&quot;vblagoje/bert-english-uncased-finetuned-pos&quot;)

model = AutoModelForTokenClassification.from_pretrained(&quot;vblagoje/bert-english-uncased-finetuned-pos&quot;)

text = &quot;My name is Clara and I live in Berkeley, California.&quot;
input_ids = tokenizer.encode(text + '&lt;/s&gt;', return_tensors='pt')
outputs = model(input_ids)
</code></pre>
<p>But the <code>outputs</code> is coming something like this</p>
<blockquote>
<p>(tensor([[[-1.8196e+00, -1.9783e+00, -1.7416e+00,  1.2082e+00,
-7.0337e-02,
-7.0322e-03,  3.4300e-01, -9.6914e-01, -1.3546e+00,  7.7266e-03,
3.7128e+00, -3.4061e-01,  4.8385e+00, -1.2548e+00, -5.1845e-01,
7.0140e-01,  1.0394e+00],<br />
[-1.2702e+00, -1.5518e+00, -1.1553e+00, -4.4077e-01, -9.8661e-01,
-3.2680e-01, -6.5338e-01, -3.9779e-01, -7.5383e-01, -1.2677e+00,
9.6353e+00,  1.9938e-01, -1.0282e+00, -7.5071e-01, -1.0307e+00,
-8.0589e-01,  4.2073e-01],<br />
[-9.6988e-01, -5.0090e-01, -1.3858e+00, -1.0554e+00, -1.4040e+00,
-7.5977e-01, -7.4156e-01,  8.0594e+00, -5.1854e-01, -1.9098e+00,
-1.6362e-02,  1.0594e+00, -8.4962e-01, -1.7415e+00, -1.0628e+00,
-1.7485e-01, -1.1490e+00],<br />
[-1.4368e+00, -1.6313e-01, -1.3202e+00,  8.7465e+00, -1.3782e+00,
-9.8889e-01, -1.1371e+00, -1.0917e+00, -9.8495e-01, -9.3237e-01,
-9.6111e-01, -4.1658e-01, -7.3133e-01, -9.6004e-01, -9.5337e-01,
3.1836e+00, -8.3462e-01],<br />
[-7.9476e-01, -7.9640e-01, -9.0027e-01, -6.9506e-01, -8.9706e-01,
-6.9383e-01, -3.1590e-01,  1.2390e+00, -1.0443e+00, -9.9977e-01,
-8.8189e-01,  8.7941e+00, -9.9445e-01, -1.2076e+00, -1.1424e+00,
-9.7801e-01,  5.6683e-01],<br />
[-8.2837e-01, -5.5060e-01, -2.1352e-01, -8.8721e-01,  9.5536e+00,
1.0478e+00, -5.6208e-01, -7.1037e-01, -7.0248e-01,  1.1298e-01</p>
<p>...</p>
<p>-7.3788e-01,  4.3640e-03,  1.6994e+00,  1.1528e-01, -1.0983e+00,
-8.9202e-01, -1.2869e+00,  4.9141e+00, -6.2096e-01,  4.8374e+00,
3.2384e-01,  4.6213e-01],<br />
[-1.3622e+00,  2.0772e+00, -1.6680e+00, -8.8679e-01, -8.6959e-01,
-1.7468e+00, -1.1424e+00,  1.6996e+00,  3.5800e-01, -4.3927e-01,
-3.6129e-01, -4.2220e-01, -1.7912e+00,  8.0154e-01,  7.4594e-01,
-1.0620e+00,  3.8152e+00],<br />
[-1.2889e+00, -2.9379e-01, -1.6543e+00, -4.3326e-01, -2.4919e-01,
-4.0112e-01, -4.4255e-01,  2.2697e-01, -4.6042e-01, -3.7862e-03,
-6.3061e-01, -1.3280e+00,  8.5533e+00, -4.6881e-01,  2.3882e+00,
2.4533e-01, -1.4095e-01],<br />
[-9.5640e-01, -5.7213e-01, -1.0245e+00, -5.3566e-01, -1.5287e-01,
-6.6977e-01, -5.3392e-01, -3.1967e-02, -7.3077e-01, -3.1048e-01,
-7.2973e-01, -3.1701e-01,  1.0196e+01, -5.2346e-01,  4.0820e-01,
-2.1350e-01,  1.0340e+00]]], grad_fn=),)</p>
</blockquote>
<p>But as per the documentation, I am expecting output to be in a JSON format...</p>
<blockquote>
<p>[   {
&quot;entity_group&quot;: &quot;PRON&quot;,
&quot;score&quot;: 0.9994694590568542,
&quot;word&quot;: &quot;my&quot;   },   {
&quot;entity_group&quot;: &quot;NOUN&quot;,
&quot;score&quot;: 0.997125506401062,
&quot;word&quot;: &quot;name&quot;   },   {
&quot;entity_group&quot;: &quot;AUX&quot;,
&quot;score&quot;: 0.9938186407089233,
&quot;word&quot;: &quot;is&quot;   },   {
&quot;entity_group&quot;: &quot;PROPN&quot;,
&quot;score&quot;: 0.9983252882957458,
&quot;word&quot;: &quot;clara&quot;   },   {
&quot;entity_group&quot;: &quot;CCONJ&quot;,
&quot;score&quot;: 0.9991229772567749,
&quot;word&quot;: &quot;and&quot;   },   {
&quot;entity_group&quot;: &quot;PRON&quot;,
&quot;score&quot;: 0.9994894862174988,
&quot;word&quot;: &quot;i&quot;   },   {
&quot;entity_group&quot;: &quot;VERB&quot;,
&quot;score&quot;: 0.9983153939247131,
&quot;word&quot;: &quot;live&quot;   },   {
&quot;entity_group&quot;: &quot;ADP&quot;,
&quot;score&quot;: 0.999370276927948,
&quot;word&quot;: &quot;in&quot;   },   {
&quot;entity_group&quot;: &quot;PROPN&quot;,
&quot;score&quot;: 0.9987357258796692,
&quot;word&quot;: &quot;berkeley&quot;   },   {
&quot;entity_group&quot;: &quot;PUNCT&quot;,
&quot;score&quot;: 0.9996636509895325,
&quot;word&quot;: &quot;,&quot;   },   {
&quot;entity_group&quot;: &quot;PROPN&quot;,
&quot;score&quot;: 0.9985638856887817,
&quot;word&quot;: &quot;california&quot;   },   {
&quot;entity_group&quot;: &quot;PUNCT&quot;,
&quot;score&quot;: 0.9996631145477295,
&quot;word&quot;: &quot;.&quot;   } ]</p>
</blockquote>
<p>What am I doing wrong? How can I parse the current output to the desired JSON output?</p>
","2515660","","6664872","","2020-12-23 09:07:32","2020-12-23 09:15:43","Parsing the Hugging Face Transformer Output","<huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"66249631","1","66250147","","2021-02-17 20:41:16","","0","249","<p>I have around 70 categories (it can be 20 or 30 also) and I want to be able to parallelize the process using ray but I get an error:</p>
<pre><code>import pandas as pd
import swifter
import json
import ray
from transformers import pipeline

classifier = pipeline(&quot;zero-shot-classification&quot;)

labels = [&quot;vegetables&quot;, &quot;potato&quot;, &quot;bell pepper&quot;, &quot;tomato&quot;, &quot;onion&quot;, &quot;carrot&quot;, &quot;broccoli&quot;,
          &quot;lettuce&quot;, &quot;cucumber&quot;, &quot;celery&quot;, &quot;corn&quot;, &quot;garlic&quot;, &quot;mashrooms&quot;, &quot;cabbage&quot;, &quot;spinach&quot;,
          &quot;beans&quot;, &quot;cauliflower&quot;, &quot;asparagus&quot;, &quot;fruits&quot;, &quot;bananas&quot;, &quot;apples&quot;, &quot;strawberries&quot;,
          &quot;grapes&quot;, &quot;oranges&quot;, &quot;lemons&quot;, &quot;avocados&quot;, &quot;peaches&quot;, &quot;blueberries&quot;, &quot;pineapple&quot;,
          &quot;cherries&quot;, &quot;pears&quot;, &quot;mangoe&quot;, &quot;berries&quot;, &quot;red meat&quot;, &quot;beef&quot;, &quot;pork&quot;, &quot;mutton&quot;,
          &quot;veal&quot;, &quot;lamb&quot;, &quot;venison&quot;, &quot;goat&quot;, &quot;mince&quot;, &quot;white meat&quot;, &quot;chicken&quot;, &quot;turkey&quot;,
          &quot;duck&quot;, &quot;goose&quot;, &quot;pheasant&quot;, &quot;rabbit&quot;, &quot;Processed meat&quot;, &quot;sausages&quot;, &quot;bacon&quot;,
          &quot;ham&quot;, &quot;hot dogs&quot;, &quot;frankfurters&quot;, &quot;tinned meat&quot;, &quot;salami&quot;, &quot;pÃ¢tÃ©s&quot;, &quot;beef jerky&quot;,
          &quot;chorizo&quot;, &quot;pepperoni&quot;, &quot;corned beef&quot;, &quot;fish&quot;, &quot;catfish&quot;, &quot;cod&quot;, &quot;pangasius&quot;, &quot;pollock&quot;,
          &quot;tilapia&quot;, &quot;tuna&quot;, &quot;salmon&quot;, &quot;seafood&quot;, &quot;shrimp&quot;, &quot;squid&quot;, &quot;mussels&quot;, &quot;scallop&quot;,
          &quot;octopus&quot;, &quot;grains&quot;, &quot;rice&quot;, &quot;wheat&quot;, &quot;bulgur&quot;, &quot;corn&quot;, &quot;oat&quot;, &quot;quinoa&quot;, &quot;buckwheat&quot;,
          &quot;meals&quot;, &quot;salad&quot;, &quot;soup&quot;, &quot;steak&quot;, &quot;pizza&quot;, &quot;pie&quot;, &quot;burger&quot;, &quot;backery&quot;, &quot;bread&quot;, &quot;souce&quot;,
          &quot;pasta&quot;, &quot;sandwich&quot;, &quot;waffles&quot;, &quot;barbecue&quot;, &quot;roll&quot;, &quot;wings&quot;, &quot;ribs&quot;, &quot;cookies&quot;]


ray.init()
@ray.remote
def get_meal_category(seq, labels, n=3):
    res_dict = classifier(seq, labels)
    return list(zip([seq for i in range(n)], res_dict[&quot;labels&quot;][0:n], res_dict[&quot;scores&quot;][0:n]))

res_list = ray.get([get_meal_category.remote(merged_df[&quot;title&quot;][i], labels) for i in range(10)])
</code></pre>
<p>Where merged_df is a big dataframe with meal names in it's labels column like:</p>
<pre><code>['Cappuccino',
 'Stove Top Stuffing Mix For Turkey (Kraft)',
 'Stove Top Stuffing Mix For Turkey (Kraft)',
 'Roasted Dark Turkey Meat',
 'Roasted Dark Turkey Meat',
 'Roasted Dark Turkey Meat',
 'Cappuccino',
 'Low Fat 2% Small Curd Cottage Cheese (Daisy)',
 'Rice Cereal (Gerber)',
 'Oranges']
</code></pre>
<p>Please advise how to avoid ray's error and parallelize the classification.</p>
<p>The error:</p>
<pre><code>2021-02-17 16:54:51,689 WARNING worker.py:1107 -- Warning: The remote function __main__.get_meal_category has size 1630925709 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.
---------------------------------------------------------------------------
ConnectionResetError                      Traceback (most recent call last)
~/.local/lib/python3.8/site-packages/redis/connection.py in send_packed_command(self, command, check_health)
    705             for item in command:
--&gt; 706                 sendall(self._sock, item)
    707         except socket.timeout:

~/.local/lib/python3.8/site-packages/redis/_compat.py in sendall(sock, *args, **kwargs)
      8 def sendall(sock, *args, **kwargs):
----&gt; 9     return sock.sendall(*args, **kwargs)
     10 

ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
&lt;ipython-input-9-1a5345832fba&gt; in &lt;module&gt;
----&gt; 1 res_list = ray.get([get_meal_category.remote(merged_df[&quot;title&quot;][i], labels) for i in range(10)])

&lt;ipython-input-9-1a5345832fba&gt; in &lt;listcomp&gt;(.0)
----&gt; 1 res_list = ray.get([get_meal_category.remote(merged_df[&quot;title&quot;][i], labels) for i in range(10)])

~/.local/lib/python3.8/site-packages/ray/remote_function.py in _remote_proxy(*args, **kwargs)
     99         @wraps(function)
    100         def _remote_proxy(*args, **kwargs):
--&gt; 101             return self._remote(args=args, kwargs=kwargs)
    102 
    103         self.remote = _remote_proxy

~/.local/lib/python3.8/site-packages/ray/remote_function.py in _remote(self, args, kwargs, num_returns, num_cpus, num_gpus, memory, object_store_memory, accelerator_type, resources, max_retries, placement_group, placement_group_bundle_index, placement_group_capture_child_tasks, override_environment_variables, name)
    205 
    206             self._last_export_session_and_job = worker.current_session_and_job
--&gt; 207             worker.function_actor_manager.export(self)
    208 
    209         kwargs = {} if kwargs is None else kwargs

~/.local/lib/python3.8/site-packages/ray/function_manager.py in export(self, remote_function)
    142         key = (b&quot;RemoteFunction:&quot; + self._worker.current_job_id.binary() + b&quot;:&quot;
    143                + remote_function._function_descriptor.function_id.binary())
--&gt; 144         self._worker.redis_client.hset(
    145             key,
    146             mapping={

~/.local/lib/python3.8/site-packages/redis/client.py in hset(self, name, key, value, mapping)
   3048                 items.extend(pair)
   3049 
-&gt; 3050         return self.execute_command('HSET', name, *items)
   3051 
   3052     def hsetnx(self, name, key, value):

~/.local/lib/python3.8/site-packages/redis/client.py in execute_command(self, *args, **options)
    898         conn = self.connection or pool.get_connection(command_name, **options)
    899         try:
--&gt; 900             conn.send_command(*args)
    901             return self.parse_response(conn, command_name, **options)
    902         except (ConnectionError, TimeoutError) as e:

~/.local/lib/python3.8/site-packages/redis/connection.py in send_command(self, *args, **kwargs)
    723     def send_command(self, *args, **kwargs):
    724         &quot;Pack and send a command to the Redis server&quot;
--&gt; 725         self.send_packed_command(self.pack_command(*args),
    726                                  check_health=kwargs.get('check_health', True))
    727 

~/.local/lib/python3.8/site-packages/redis/connection.py in send_packed_command(self, command, check_health)
    715                 errno = e.args[0]
    716                 errmsg = e.args[1]
--&gt; 717             raise ConnectionError(&quot;Error %s while writing to socket. %s.&quot; %
    718                                   (errno, errmsg))
    719         except BaseException:

ConnectionError: Error 104 while writing to socket. Connection reset by peer.
</code></pre>
","1030099","","","","","2021-02-18 01:31:08","How to parallelize classification with Zero Shot Classification by Huggingface?","<python-3.x><redis><classification><huggingface-transformers><ray>","1","3","","","","CC BY-SA 4.0"
"66266001","1","","","2021-02-18 18:29:54","","1","204","<p>I have implemented bert model for a regression task. trained the model and predict, however, model predicts 0 for each instance.</p>
<p>In a nutshell, task was predicting the positivity/negativity of a scientific article between -1 and 1.</p>
<p>Obviously the problem turns out from train (model does not learn) because validation loss does not decrease as expected. I will share the code of my model and look forward to hearing any suggestions. Please, let me know if you need to see other parts of the code.</p>
<p>Thanks!</p>
<pre><code>class CustomSciBERTModel(nn.Module):
    def __init__(self):
      super(CustomSciBERTModel, self).__init__()
      self.scibert = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')
      self.num_labels = 1
      self.linear_layer = nn.Linear(768, 1)

    def forward(self, ids, mask,labels):
      output = self.scibert(
            input_ids=ids, 
            attention_mask=mask)
      


      logits = self.linear_layer(output[1]) 
     

      loss = None
      # while training return loss and while validation/ testing resturn logits
      if labels is not None:
        loss_fct = nn.MSELoss()
        #loss_fct = nn.CrossEntropyLoss() *************
        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
        return loss
      else:
        return logits
</code></pre>
","12302635","","6664872","","2021-02-20 12:59:09","2021-02-20 12:59:09","BERT- pythorch- regression task - predicting same score for each instance","<python><nlp><pytorch><bert-language-model><huggingface-transformers>","0","2","0","","","CC BY-SA 4.0"
"66266033","1","","","2021-02-18 18:32:02","","1","39","<p>I am trying to run Huggingface Zero Shot Classification on 200 strings that I want to match with 20 categories.</p>
<p>It takes almost 10GB RAM but utilises only one CPU. Here is what I have done so far:</p>
<pre><code>from transformers import pipeline

classifier = pipeline(&quot;zero-shot-classification&quot;)

labels = [&quot;bread&quot;, &quot;pasta&quot;, &quot;cereal&quot;, &quot;rice&quot;, &quot;grains&quot;, &quot;biscuits&quot;, 
              &quot;crackers&quot;, &quot;popcorn&quot;, &quot;vegetables&quot;, &quot;fruits&quot;, &quot;meat&quot;, 
              &quot;poultry&quot;, &quot;fish&quot;, &quot;eggs&quot;, &quot;legumes&quot;, &quot;nuts&quot;, &quot;seeds&quot;, 
              &quot;salad&quot;, &quot;soup&quot;, &quot;dessert&quot;, &quot;sweets&quot;, &quot;beverages&quot;, &quot;souce&quot;]
unique_titles = ['Cappuccino',
             'Stove Top Stuffing Mix For Turkey (Kraft)',
             'Roasted Dark Turkey Meat',
             'Low Fat 2% Small Curd Cottage Cheese (Daisy)',
             'Rice Cereal (Gerber)',
             'Oranges',
             &quot;Cheese Spinach Pie (Heba's Health Foods)&quot;,
             'Turkey Sausage Egg &amp; Cheese Croissant (Wawa)',
             'Bananas',
             'Light Nonfat Yogurt (Great Value)',
             'Blueberries',
             'Strawberries',
             'Macaroni',
             'Chicken Breast',
             'Mushrooms',
             'Jamoca Almond Fudge Ice Cream (Baskin-Robbins)',
             'Boston Cream Donut (Wawa)',
             &quot;Plain Bagel (Dunkin' Donuts)&quot;,
             'Fried Egg',
             'Bison &amp; Beef Hot Dogs (Great Range)',
             'Mushrooms Pieces &amp; Stems (Great Value)',
             'Cookie',
             'Penne',
             'Beef Sausage',
             'Egg Omelet or Scrambled Egg with Mushrooms',
             'Apples',
             'Peach',
             'Ramen Noodles with Chicken Flavor (35% Less Sodium) (Maruchan)',
             'Greek Nonfat Yogurt - Plain (Friendly Farms)',
             'French or Vienna Roll',
             'Sea Salt Carmel Frozen Yogurt (Kemps)',
             'Gluten Free Thin &amp; Crispy Signature Pepperoni Pizza (Freschetta)',
             'Red Tomatoes',
             'Sliced Ham  (Regular, Approx. 11% Fat)',
             &quot;Low Moisture Part-skim Mozzarella Light String Cheese (Trader Joe's)&quot;,
             'Low Moisture-Part Skim Mozzarella String Cheese (Crystal Farms)',
             'Red Table Wine',
             'Sly Redâ€™s Seeded Ancient Grain Bread (Simply Nature)',
             'Rose Wine',
             'Bison Meat (Lean Only)',
             'White Rice',
             'Baked or Broiled Salmon',
             'Hazelnuts Filberts',
             'Dark Chocolate &amp; Nuts (Choceur)',
             'Deli Select Pastrami (Hillshire Farm)',
             'Ciabatta Sandwich Rolls (Specially Selected)',
             'Deli Sliced Mozzarella (Happy Farms)',
             'Enlive (Ensure)',
             'Greek Yogurt (Kirkland Signature)',
             'Salmon',
             'High Protein Shake - Chocolate (Premier Nutrition)',
             'Tangerine',
             'Pastrami (Columbus Salame)',
             'Olive Tapenade Hummus (Little Salad Bar)',
             'Multigrain Tortilla Chips (Simply Nature)',
             'Fruit Smoothie Drink',
             'Sliced Mushrooms (Kroger)',
             'Mozzarella Shredded Cheese (Sargento)',
             'Cooked Egg White',
             'Spinach (Chopped or Leaf, Frozen)',
             'Turkey Bacon',
             'Non Dairy Chocolate Protein Shake (14 oz) (Muscle Milk)',
             &quot;Great Northern Beans (Dakota's Pride)&quot;,
             'Tomato Sauce (Happy Harvest)',
             'Pickled Green Tomato',
             'Grain Free Cookie Bites Chocolate Chip (Simply Nature)',
             'Jumbo Bison Hot Dog (Body By Bison)',
             'Spinach',
             'Chocolate Low-Fat Protein Shake (Muscle Milk)',
             'Organic Yellow Corn Tortilla Chips (Simply Nature)',
             'Rich Chocolate Complete Protein Shake (Pure Protein)',
             'Ramen Noodle Soup - Beef Flavor (Maruchan)',
             'White Corn Tortilla Chips (Delish)',
             'Vegan Veggie Burger (Earth Grown)',
             'Apple',
             'Peaches',
             'B12 Gummies (Sundown Naturals)',
             'String Cheese Mozzarella Cheese Snacks (Sargento)',
             'Natural Mozzarella String Cheese (Kraft)',
             'Vanilla Milkshake (Pure Protein)',
             'Hard-Boiled Egg',
             'Chicken Breast (Skin Not Eaten)',
             '0% Plain Greek Yogurt (8 oz) (Chobani)',
             'Grapes (Red or Green, European Type Varieties Such As Thompson Seedless)',
             &quot;Tzaziki (Heba's Health Foods)&quot;,
             &quot;Ham, Egg White &amp; Cheese Wake-Up Wrap (Dunkin' Donuts)&quot;,
             'Whole Wheat Waffles (Essential Everyday)',
             &quot;Large Grade A Eggs (Eggland's Best)&quot;,
             'Turkey Bacon (Appleton Farms)',
             'Strawberry Milkshake (Pure Protein)',
             'Feta Cheese',
             'Egg',
             'Portuguese Rolls (Stop &amp; Shop)',
             'Chicken Bacon Ranch Casserole',
             'Carrot Muffin',
             &quot;Chunky Honey Roasted Ham with Potatoes Soup (Campbell's)&quot;,
             'Biscotti Cookie (Italian Sugar Cookie)',
             'Chewy Granola Bars - Chocolate Chip (Quaker)',
             'Maple &amp; Brown Sugar Instant Oatmeal (Great Value)',
             'Walnuts',
             'Taco Salad',
             'Potato French Fries',
             &quot;Boneless Chicken Wings (Chester's)&quot;,
             'Avocado (Calavo)',
             'Bacon',
             'Chicken Enchilada Soup (Signature Cafe)',
             'Shredded Hash Browns (Great Value)',
             'Soft White Roll',
             'Chicken &amp; Apple Sausage (Aidells)',
             'Bell Peppers',
             'Mayonnaise',
             'Classics Milk Chocolate Hot Cocoa Mix (Swiss Miss)',
             &quot;Seasoned Steak Bites (Schwan's)&quot;,
             'Potatoes (Flesh, with Salt, Boiled)',
             'Lettuce Salad with Assorted Vegetables',
             'Everything Bagel (Publix)',
             'Oven Roasted Deli Style Chicken Breast (Kroger)',
             'Waffle Potato Fries (Small) (Chick-fil-A)',
             'Chicken Nuggets (8 Count) (Chick-fil-A)',
             'Pasta with Meat Sauce',
             'Garlic Bread',
             &quot;Kettle Cooked Mesquite BBQ Chips (Package) (Lay's)&quot;,
             'Low-Fat Vegetarian Black Bean Soup (Bowl) (Panera Bread)',
             'Sour Dough Bread',
             'Snickerdoodle Cookie (Betty Crocker)',
             'Steak Sandwich on Roll',
             'Cauliflower Bites (Don Lee Farms)',
             'Black Bean Soup',
             'All Natural Creamy Almond Butter (Maranatha)',
             'Texas Toast (Wonder)',
             &quot;Thin Crust Pizza (Nancy's Pizza)&quot;,
             'Saltine Crackers (Great Value)',
             'Chocolate Fudge Candies (with Nuts)',
             'Baguette',
             'Beef Stew with Potatoes and Vegetables in Gravy',
             'Chocolate Chip Ice Cream (Breyers)',
             'Pecan Nuts',
             'Hint of Lime Tortilla Chips (Tostitos)',
             &quot;Chicken Enchilada Soup (Cup) (Chili's)&quot;,
             'Whole Wheat Bread',
             'pulled pork sandwich, regular',
             'Asian Chicken Salad (Large) (Urban Plates)',
             'Grilled Steak (Urban Plates)',
             'Tortilla Chips - Island Lime (Margaritaville)',
             'Oreo Thins',
             'Egg, Sausage, Cheese &amp; Potato Breakfast Burrito (El Monterey)',
             'Apple Juice',
             'Frozen Blueberries',
             'Almond Milk Vanilla (Silk)',
             'Cashew Halves &amp; Pieces (Great Value)',
             &quot;Cocoa Special Dark (Hershey's)&quot;,
             'Pork Barbecue Sandwich or Sloppy Joe on Bun',
             'Sourdough Sliced Bread (San Luis Sourdough)',
             'Lettuce',
             'Organic Country-Style Potatoes (Cascadian Farm)',
             'Frozen Tropical Fruit Mix (Great Value)',
             'Pure Almond Milk - Original (Silk)',
             'Cashew Butter',
             'Organic Blue Agave Nectar (Wholesome Sweeteners)',
             'Stuffed Bell Peppers (Kirkland Signature)',
             'Taco Flavor Tortilla Chips (Doritos)',
             'Skinless Chicken Breast',
             'Young Green Onions',
             'Finely Shredded Mexican Style Four-cheese Blend Cheese (Market Pantry)',
             'Chocolate Covered Caramel with Nuts',
             'Garlic Naan Bread (Archer Farms)',
             'Red Baby Potatoes (Safeway)',
             'Grilled Bruschetta Chicken (Dream Dinners)',
             'Hashed Brown Potatoes',
             'Egg Roll in a Bowl',
             'Toasted Bagel',
             'Irish Soda Bread (Jewel)',
             'Chicken Barbecue Sandwich',
             'Low Carb Pancakes',
             'Garlic Naan (Tandoor Chef)',
             'Large Hamburger',
             'Garlic Fries (Gordon Biersch)',
             'Pork Egg Roll in A Bowl',
             'Chicken Nachos (Queso) (Personal) (Taco Cabana)',
             '100% Whole Wheat Bread',
             'Burrito with Chicken',
             &quot;Tortilla Chips Hint of Lime (Clancy's)&quot;,
             'Baked Potato (Peel Eaten)',
             'Steak',
             &quot;21 Whole Grains &amp; Seeds Bread (Dave's Killer Bread)&quot;,
             'Clif Kid Organic Z Bar - Chocolate Chip (Clif Bar)',
             &quot;Steak Street Taco (Rubio's Fresh Mexican Grill)&quot;,
             'Hamburger (Single Patty with Condiments)',
             'Ice Cream',
             'Tortilla Corn Chips',
             'Organic Brown Eggs (Large) (Organic Valley)',
             'Whole Wheat Bread (Commercial)',
             'Energy Granola (Clif Bar)',
             '33% Milk Chocolate (Chocolove)',
             'Baked Restructured Chips White Potato',
             'Shrimp',
             'Zucchini',
             'Cream Cheese',
             'Butter',
             'Toasted Garlic Bread']
        
classifier_dicts = classifier(unique_titles[0:100], labels, multi_class=True)
</code></pre>
<p>Please advise how can I parallelize the process to make it run faster?</p>
","1030099","","","","","2021-02-18 18:32:02","How to utilise all CPU's / use workers to predict categories faster with Huggingface Zero Shot Classification?","<python-3.x><parallel-processing><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"65499783","1","66196277","","2020-12-29 22:47:37","","0","149","<p>I have a situation where I want to apply a translation model to each and every row in one of data frame columns.</p>
<p>The translation code that I am using :</p>
<pre><code>from transformers import FSMTForConditionalGeneration, FSMTTokenizer
mname = &quot;allenai/wmt19-de-en-6-6-big&quot;
tokenizer = FSMTTokenizer.from_pretrained(mname)
model = FSMTForConditionalGeneration.from_pretrained(mname)
#Loop here for all rows in the German_Text column

input_ids = tokenizer.encode(input, return_tensors=&quot;pt&quot;)
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(decoded)
</code></pre>
<p>I want to apply this model to the following column and create a new translated column post this:</p>
<pre><code>German_Text                     English_Text
Wie geht es dir heute
mir geht es gut
</code></pre>
<p>The column English text will consist of the translated text from the model above and hence I would like to apply that model to each row in the German_text column to create corresponding translations in the English_Text column</p>
","12226377","","13302","","2021-02-28 18:02:19","2021-02-28 18:02:19","Apply transformer model to each row in a pandas column","<python-3.x><pandas><loops><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"65696968","1","","","2021-01-13 06:51:24","","1","539","<p>When i tried to get word embeddings of a sentence using bio_clinical bert, for a sentence of 8 words i am getting 11 token ids(+start and end) because &quot;embeddings&quot; is an out of vocabulary word/token, that is being split into <code>em</code>, <code>bed</code> ,<code>ding</code>, <code>s</code>.</p>
<p>I would like to know if there is any aggregation strategies available that make sense apart from doing a mean of these vectors.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
# download and load model
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

sentences = ['This framework generates embeddings for each input sentence']


#Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')


#Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)

print(encoded_input['input_ids'].shape)
</code></pre>
<p>Output:
<code>torch.Size([1, 13])</code></p>
<pre class=""lang-py prettyprint-override""><code>for token in encoded_input['input_ids'][0]:
      print(tokenizer.decode([token]))
</code></pre>
<p>Output:</p>
<pre><code>[CLS]
this
framework
generates
em
##bed
##ding
##s
for
each
input
sentence
[SEP]
</code></pre>
","10656943","","3607203","","2021-01-13 10:13:57","2021-01-13 10:19:36","How to i get word embeddings for out of vocabulary words using a transformer model?","<nlp><huggingface-transformers><transformer><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"65974247","1","","","2021-01-30 23:33:47","","1","89","<p>I'm currently trying to learn python - and at the same time learning machine learning with GPT-2 language modeling - i have had some problems, and i got over most of them, and finally got something decent running.</p>
<p><strong>But...</strong> as most of you probably know, training your model takes alot of CPU/GPU power &amp; time - time i can spare, but the problem is that i cant have it running non-stop on my home computer (yes i know i can rent a GPU @ google) - since i want be able to do anything else while training my model.</p>
<p>So i have the following questions:</p>
<ul>
<li>Can i somehow stop and restart my models training? i read something about checkpoints, but their is so much outdated info on this topic - so i havent been able to figure it out.</li>
<li>Can i incrementally feed my model fx. 10% of my dataset, let it finish - and then next week feed it another 10% and so on? if so how?</li>
<li>Bonus question... is it better to aim for many epochs with a lower data set? or a larger dataset and more epochs? what is a good amount of epochs?</li>
</ul>
<p><strong>Packages:</strong></p>
<ul>
<li>Python, 3.7.9</li>
<li>Tensorflow-gpu 2.3.0</li>
<li>Tensorflow-estimator 2.3.0</li>
<li>Transformers 4.2.2</li>
<li>Tokenizers 0.9.4</li>
<li>cudatoolkit 10.1</li>
</ul>
<p><strong>Code - Tokenizer</strong></p>
<pre><code>from tokenizers.models import BPE
from tokenizers import Tokenizer
from tokenizers.decoders import ByteLevel as ByteLevelDecoder
from tokenizers.normalizers import NFKC, Sequence
from tokenizers.pre_tokenizers import ByteLevel
from tokenizers.trainers import BpeTrainer

class BPE_token(object):
def __init__(self):
    self.tokenizer = Tokenizer(BPE())
    self.tokenizer.normalizer = Sequence([
        NFKC()
    ])
    self.tokenizer.pre_tokenizer = ByteLevel()
    self.tokenizer.decoder = ByteLevelDecoder()

def bpe_train(self, paths):
    trainer = BpeTrainer(vocab_size=50000, show_progress=True, inital_alphabet=ByteLevel.alphabet(),         special_tokens=[
        &quot;&lt;s&gt;&quot;,
        &quot;&lt;pad&gt;&quot;,
        &quot;&lt;/s&gt;&quot;,
        &quot;&lt;unk&gt;&quot;,
        &quot;&lt;mask&gt;&quot;
    ])
    self.tokenizer.train(trainer, paths)

def save_tokenizer(self, location, prefix=None):
    if not os.path.exists(location):
        os.makedirs(location)
    self.tokenizer.model.save(location, prefix)

# ////////// TOKENIZE DATA ////////////
from pathlib import Pa th
import os# the folder 'text' contains all the files
paths = [str(x) for x in Path(&quot;./da_corpus/&quot;).glob(&quot;**/*.txt&quot;)]
tokenizer = BPE_token()# train the tokenizer model
tokenizer.bpe_train(paths)# saving the tokenized data in our specified folder
save_path = 'tokenized_data'
tokenizer.save_tokenizer(save_path)
</code></pre>
<p><strong>Code -- Model Trainer</strong></p>
<pre><code>save_path = 'tokenized_data'
tokenizer = GPT2Tokenizer.from_pretrained(save_path)
paths = [str(x) for x in Path(&quot;./da_corpus/&quot;).glob(&quot;**/*.txt&quot;)]
# tokenizer = Tokenizer.from_file(&quot;./tokenized_data/tokenizer-wiki.json&quot;)
tokenizer.add_special_tokens({
  &quot;eos_token&quot;: &quot;&lt;/s&gt;&quot;,
  &quot;bos_token&quot;: &quot;&lt;s&gt;&quot;,
  &quot;unk_token&quot;: &quot;&lt;unk&gt;&quot;,
  &quot;pad_token&quot;: &quot;&lt;pad&gt;&quot;,
  &quot;mask_token&quot;: &quot;&lt;mask&gt;&quot;
})# creating the configurations from which the model can be made
config = GPT2Config(
  vocab_size=tokenizer.vocab_size,
  bos_token_id=tokenizer.bos_token_id,
  eos_token_id=tokenizer.eos_token_id
)# creating the model
model = TFGPT2LMHeadModel(config)

single_string = ''
for filename in paths:
    with open(filename, &quot;r&quot;, encoding='utf-8') as f:
        x = f.read()
    single_string += x + tokenizer.eos_token
string_tokenized = tokenizer.encode(single_string)
# print(string_tokenized)



examples = []
block_size = 100
BATCH_SIZE = 12
BUFFER_SIZE = 2000
for i in range(0, len(string_tokenized) - block_size + 1, block_size):
    examples.append(string_tokenized[i:i + block_size])
    inputs, labels = [], []


for ex in examples:
    inputs.append(ex[:-1])
    labels.append(ex[1:])

dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

# defining our optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')# compiling the model
model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])
num_epoch = 20
history = model.fit(dataset, epochs=num_epoch)


output_dir = './model_bn_custom/'

if not os.path.exists(output_dir):
    os.mkdir(output_dir)


model_to_save = model.module if hasattr(model, 'module') else model
output_model_file = os.path.join(output_dir, WEIGHTS_NAME)
output_config_file = os.path.join(output_dir, CONFIG_NAME)

# save model and model configs
model.save_pretrained(output_dir)
model_to_save.config.to_json_file(output_config_file)

# save tokenizer
tokenizer.save_pretrained(output_dir)
</code></pre>
","9837081","","","","","2021-01-30 23:33:47","Incrementally training || pause&resume training, GPT2 language model'ing","<python><tensorflow><tensorflow2.0><huggingface-transformers><gpt-2>","0","0","","","","CC BY-SA 4.0"
"65387101","1","","","2020-12-21 03:06:36","","0","819","<p>When I run the demo.py</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
    
tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;)
model = AutoModel.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;, return_dict=True)
# print(model)
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)
print(count_parameters(model))
inputs = tokenizer(&quot;å²å¯†æ–¯å…ˆç”Ÿä¸åœ¨ï¼Œä»–åŽ»çœ‹ç”µå½±äº†ã€‚Mr Smith is not in. He ________ ________to the cinema&quot;, return_tensors=&quot;pt&quot;)
print(inputs)
outputs = model(**inputs)
print(outputs)
</code></pre>
<p>the code show</p>
<pre><code>{'input_ids': tensor([[  101,  2759,  3417,  4332,  2431,  5600,  2080,  3031, 10064,  2196,
      2724,  5765,  5614,  3756,  2146,  1882, 12916, 11673, 10124, 10472,
     10106,   119, 10357,   168,   168,   168,   168,   168,   168,   168,
       168,   168,   168,   168,   168,   168,   168,   168,   168, 10114,
     10105, 18458,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
</code></pre>
<p><strong>Using bos_token, but it is not set yet.
Using eos_token, but it is not set yet.</strong>
why bos_token is printed?</p>
","4792964","","","","","2020-12-21 09:40:09","what 's the meaning of ""Using bos_token, but it is not set yet.""","<multilingual><huggingface-transformers><huggingface-tokenizers><distilbert>","1","0","","","","CC BY-SA 4.0"
"65440010","1","65443822","","2020-12-24 15:03:02","","1","274","<p>I tried to add new words to the <code>Bert tokenizer vocab</code>. I see that the length of the vocab is increasing, however I can't find the newly added word in the vocab.</p>
<pre><code>tokenizer.add_tokens(['covid', 'wuhan'])

v = tokenizer.get_vocab()

print(len(v))
'covid' in tokenizer.vocab
</code></pre>
<p>Output:</p>
<pre><code>30524

False
</code></pre>
","14569416","","6248542","","2020-12-24 15:23:06","2020-12-24 22:47:26","Unable to find the word that I added to the Huggingface Bert tokenizer vocabulary","<bert-language-model><huggingface-transformers><nltokenizer>","1","0","","","","CC BY-SA 4.0"
"65708902","1","","","2021-01-13 19:54:35","","2","323","<p>I'm trying to run the model given in the hyperparameter optimization <a href=""https://simpletransformers.ai/docs/tips-and-tricks/#6-putting-it-all-together"" rel=""nofollow noreferrer"">example</a> from the simple transformers documentation, but while searching for hyperparameters after a certain number of iterations, a CUDA out of memory error occurs. Also during the search for hyperparameters GPU Memory Allocated is constantly increased.</p>
<p>Here is a graph of memory allocation:</p>
<p><a href=""https://i.stack.imgur.com/JgXiC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JgXiC.png"" alt=""enter image description here"" /></a></p>
<p>Here's my code. Code I'm launching on Google Collab. How I can fix this error?</p>
<pre class=""lang-py prettyprint-override""><code>import logging

import pandas as pd
import sklearn

import wandb
from simpletransformers.classification import (
    ClassificationArgs,
    ClassificationModel,
)

sweep_config = {
    &quot;method&quot;: &quot;bayes&quot;,  # grid, random
    &quot;metric&quot;: {&quot;name&quot;: &quot;train_loss&quot;, &quot;goal&quot;: &quot;minimize&quot;},
    &quot;parameters&quot;: {
        &quot;num_train_epochs&quot;: {&quot;values&quot;: [2, 3, 5]},
        &quot;learning_rate&quot;: {&quot;min&quot;: 5e-5, &quot;max&quot;: 4e-4},
    },
}

sweep_id = wandb.sweep(sweep_config, project=&quot;Simple Sweep&quot;)

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger(&quot;transformers&quot;)
transformers_logger.setLevel(logging.WARNING)

# Preparing train data
train_data = [
    [&quot;Aragorn was the heir of Isildur&quot;, &quot;true&quot;],
    [&quot;Frodo was the heir of Isildur&quot;, &quot;false&quot;],
]
train_df = pd.DataFrame(train_data)
train_df.columns = [&quot;text&quot;, &quot;labels&quot;]

# Preparing eval data
eval_data = [
    [&quot;Theoden was the king of Rohan&quot;, &quot;true&quot;],
    [&quot;Merry was the king of Rohan&quot;, &quot;false&quot;],
]
eval_df = pd.DataFrame(eval_data)
eval_df.columns = [&quot;text&quot;, &quot;labels&quot;]

model_args = ClassificationArgs()
model_args.reprocess_input_data = True
model_args.overwrite_output_dir = True
model_args.evaluate_during_training = True
model_args.manual_seed = 4
model_args.use_multiprocessing = True
model_args.train_batch_size = 16
model_args.eval_batch_size = 8
model_args.labels_list = [&quot;true&quot;, &quot;false&quot;]
model_args.wandb_project = &quot;Simple Sweep&quot;

def train():
    # Initialize a new wandb run
    wandb.init()

    # Create a TransformerModel
    model = ClassificationModel(
        &quot;roberta&quot;,
        &quot;roberta-base&quot;,
        use_cuda=True,
        args=model_args,
        sweep_config=wandb.config,
    )

    # Train the model
    model.train_model(train_df, eval_df=eval_df)

    # Evaluate the model
    model.eval_model(eval_df)

    # Sync wandb
    wandb.join()


wandb.agent(sweep_id, train)

</code></pre>
","14088946","","681865","","2021-01-13 20:59:30","2021-01-13 20:59:30","wandb - RuntimeError: CUDA out of memory","<python><pytorch><huggingface-transformers><simpletransformers>","0","7","","","","CC BY-SA 4.0"
"66109084","1","66117248","","2021-02-08 20:44:13","","4","1158","<p>I am trying to convert the Pegasus newsroom in HuggingFace's transformers model to the ONNX format. I followed <a href=""https://colab.research.google.com/github/huggingface/transformers/blob/master/notebooks/04-onnx-export.ipynb#scrollTo=foYlXrSksR_v"" rel=""nofollow noreferrer"">this</a> guide published by Huggingface. After installing the prereqs, I ran this code:</p>
<pre><code>!rm -rf onnx/
from pathlib import Path
from transformers.convert_graph_to_onnx import convert

convert(framework=&quot;pt&quot;, model=&quot;google/pegasus-newsroom&quot;, output=Path(&quot;onnx/google/pegasus-newsroom.onnx&quot;), opset=11)

</code></pre>
<p>and got these errors:</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-9-3b37ed1ceda5&gt; in &lt;module&gt;()
      3 from transformers.convert_graph_to_onnx import convert
      4 
----&gt; 5 convert(framework=&quot;pt&quot;, model=&quot;google/pegasus-newsroom&quot;, output=Path(&quot;onnx/google/pegasus-newsroom.onnx&quot;), opset=11)
      6 
      7 

6 frames
/usr/local/lib/python3.6/dist-packages/transformers/models/pegasus/modeling_pegasus.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, encoder_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)
    938             input_shape = inputs_embeds.size()[:-1]
    939         else:
--&gt; 940             raise ValueError(&quot;You have to specify either decoder_input_ids or decoder_inputs_embeds&quot;)
    941 
    942         # past_key_values_length

ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

</code></pre>
<p>I have never seen this error before. Any ideas?</p>
","14882176","","13273054","","2021-02-14 18:53:25","2021-03-18 10:14:33","how to convert HuggingFace's Seq2seq models to onnx format","<python><tensorflow><pytorch><huggingface-transformers><onnx>","1","0","1","","","CC BY-SA 4.0"
"66276186","1","66278433","","2021-02-19 10:53:15","","0","448","<p>The GPT2 finetuned model is uploaded in <a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">huggingface-models</a> for the inferencing</p>
<p>Below error is observed during the inference,</p>
<p><strong>Can't load tokenizer using from_pretrained, please update its configuration: Can't load tokenizer for 'bala1802/model_1_test'. Make sure that: - 'bala1802/model_1_test' is a correct model identifier listed on 'https://huggingface.co/models' - or 'bala1802/model_1_test' is the correct path to a directory containing relevant tokenizer files</strong></p>
<p>Below is the configuration - config.json file for the Finetuned huggingface model,</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;gpt2&quot;,
  &quot;activation_function&quot;: &quot;gelu_new&quot;,
  &quot;architectures&quot;: [
    &quot;GPT2LMHeadModel&quot;
  ],
  &quot;attn_pdrop&quot;: 0.1,
  &quot;bos_token_id&quot;: 50256,
  &quot;embd_pdrop&quot;: 0.1,
  &quot;eos_token_id&quot;: 50256,
  &quot;gradient_checkpointing&quot;: false,
  &quot;initializer_range&quot;: 0.02,
  &quot;layer_norm_epsilon&quot;: 1e-05,
  &quot;model_type&quot;: &quot;gpt2&quot;,
  &quot;n_ctx&quot;: 1024,
  &quot;n_embd&quot;: 768,
  &quot;n_head&quot;: 12,
  &quot;n_inner&quot;: null,
  &quot;n_layer&quot;: 12,
  &quot;n_positions&quot;: 1024,
  &quot;resid_pdrop&quot;: 0.1,
  &quot;summary_activation&quot;: null,
  &quot;summary_first_dropout&quot;: 0.1,
  &quot;summary_proj_to_labels&quot;: true,
  &quot;summary_type&quot;: &quot;cls_index&quot;,
  &quot;summary_use_proj&quot;: true,
  &quot;task_specific_params&quot;: {
    &quot;text-generation&quot;: {
      &quot;do_sample&quot;: true,
      &quot;max_length&quot;: 50
    }
  },
  &quot;transformers_version&quot;: &quot;4.3.2&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 50257
}
</code></pre>
<p>Should I configure the GPT2 Tokenizer just like the <code>&quot;model_type&quot;: &quot;gpt2&quot;</code> in the config.json file</p>
","1793799","","","","","2021-02-19 13:25:37","HuggingFace - GPT2 Tokenizer configuration in config.json","<pytorch><huggingface-transformers><language-model><huggingface-tokenizers><gpt-2>","1","0","","","","CC BY-SA 4.0"
"65501470","1","","","2020-12-30 02:57:24","","2","401","<p>The task is to detect whether a blood label is present in a text sequence using BERT for sequence classification pre-trained model.</p>
<pre class=""lang-py prettyprint-override""><code>class BloodDataset(Dataset):
    &quot;&quot;&quot;MIMIC Blood dataset.&quot;&quot;&quot;

    def __init__(self, arff_file):
        &quot;&quot;&quot;
        Args:
            arff_file (string): Path to the arff file with annotations.
        &quot;&quot;&quot;
        self.indices, self.contents, self.labels = read_arff(arff_file)
        self.labels = torch.as_tensor(self.labels)
        self.inputs = encode(self.contents)
        self.input_ids = (self.inputs['input_ids'])
        self.attention_mask = (self.inputs['attention_mask'])

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        if idx in self.indices:
            sample_index = self.indices.index(idx)
            sample = {'index': idx,
                      'content': self.contents[sample_index],
                      'label': self.labels[sample_index],
                      'input_ids': self.input_ids[sample_index],
                      'attention_mask': self.attention_mask[sample_index]
                      }
            return sample
        else:
            return &quot;Sample not found!&quot;
</code></pre>
<p>Tutorial from Huggingface proposes a trainer solution:</p>
<pre class=""lang-py prettyprint-override""><code>    model = BertForSequenceClassification.from_pretrained(model_type)
    training_args = TrainingArguments(
        output_dir='./results',          # output directory
        logging_dir='./logs',            # directory for storing logs
    )
    trainer = Trainer(
        # the instantiated ðŸ¤— Transformers model to be trained
        model=model,
        args=training_args,
        train_dataset=train_dataset,         # training dataset
        eval_dataset=test_dataset            # evaluation dataset
    )
    return trainer
</code></pre>
<p><a href=""https://huggingface.co/transformers/training.html#trainer"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/training.html#trainer</a></p>
<pre class=""lang-py prettyprint-override""><code>train_dataset = preprocess.BloodDataset(&quot;test_blood.arff&quot;)
trainer = train.run(train_dataset, train_dataset)
trainer.train()
</code></pre>
<p>yields error:</p>
<pre class=""lang-sh prettyprint-override""><code>/.local/lib/python3.6/site-packages/transformers/data/data_collator.py&quot;, line 38, in &lt;listcomp&gt;
    features = [vars(f) for f in features]
TypeError: vars() argument must have __dict__ attribute
</code></pre>
<p>What's a proper dataset input for the trainer?</p>
","4843407","","3607203","","2021-01-02 21:18:10","2021-01-08 05:20:59","How to prepare the training dataset for a trainer of BERT in pytorch?","<pytorch><bert-language-model><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"66283812","1","","","2021-02-19 19:23:54","","0","77","<p>My output labels are one-hot encoded in the following format: Positive, Negative, Mixed, Neutral with 1s and 0s e.g. <code>[1 0 0 0]</code> representing a text as being Positive</p>
<p>I have preloaded the model as follows:</p>
<pre><code>transformer_name = &quot;bert-base-uncased&quot;
pre_trained_model = TFBertForSequenceClassification.from_pretrained(transformer_name)
tokenizer = BertTokenizer.from_pretrained(transformer_name)
</code></pre>
<p>The model summary gives me the following:</p>
<p><a href=""https://i.stack.imgur.com/H1WCD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H1WCD.png"" alt=""enter image description here"" /></a></p>
<p>From there, I use the call method to create layers from this as follows:</p>
<pre><code>inputs = tf.keras.Input(shape=(512,), dtype='int32') # 512 is input shape to transformer model
print(inputs)

pre_trained_model.call(inputs)

layer_position = 2 # choose last layer position in the model
count = 0

for layer in pre_trained_model.layers:
    print(layer.output)
    
    count = count + 1
    if count == layer_position:
        last_output = layer.output

print('last output is ', last_output)

last output is  Tensor(&quot;dropout_37/Identity:0&quot;, shape=(None, 768), dtype=float32)
</code></pre>
<p>I then want to feed this last layer I got into my own custom layer as such:</p>
<pre><code>transformer_inputs = layers.GlobalAveragePooling1D()(last_output)
...
</code></pre>
<p>But get the following error:</p>
<pre><code>Input 0 of layer global_average_pooling1d is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 768]
</code></pre>
<p>I understand the issue here is <code>global_average_pooling1d</code>requires a 3dim shape as is only getting 2. Why is the output later shape from the transformer model only in 2dim - what solution can I use to fix this?</p>
<pre><code>  [1]: https://i.stack.imgur.com/Gqe9n.png
</code></pre>
","2809834","","","","","2021-02-19 19:23:54","Transfer Learning with BERT transformers","<python><bert-language-model><huggingface-transformers><transfer-learning>","0","0","","","","CC BY-SA 4.0"
"65447378","1","","","2020-12-25 10:45:01","","1","326","<p>Iâ€™m trying to fine-tune a pre-trainted BERT model (huggingface transformers) by inserting a CNN layer.
In this model, the outputs of all transformer encoders are used, not only the output of the latest
transformer encoder.
So that the output vectors of each transformer encoder are concatenated, and a matrix is produced:</p>
<p>The convolutional operation is performed with a window of size (3, hidden size of BERT which is 768 in BERT_base model) and the maximum value is generated for each transformer encoder by
applying max pooling on the convolution output.</p>
<p>By concatenating these values, a vector is generated which is given as input to a fully connected network. By applying softmax on the input, the classification operation is performed.</p>
<p><a href=""https://i.stack.imgur.com/ZAKCH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZAKCH.jpg"" alt=""enter image description here"" /></a></p>
<p>My problem is that I canâ€™t seem to find the right arguments to perform the convolution and the maxpooling on that matrix.</p>
<p>With batch size = 32, there are 13 layers of Transformer encoders, each one get as an input [64, 768] of encoded tokenized text and outputs an encoding of the same dimensions. (64 is the max-length in tokenization)</p>
<p>I want to perform convolution on each transformerâ€™s output matrix ([64,768]) separately, then perform maxpooling on that convolutionâ€™s output. So, I should get a max-value per each transformer, and these max values are inserted into the neural network.</p>
<p>My code is:</p>
<pre><code>class BERT_Arch(nn.Module):

    def __init__(self, bert):
        super(BERT_Arch, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.conv = nn.Conv2d(in_channels=13, out_channels=13, kernel_size= (3, 768), padding=True) 
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(kernel_size=768, stride=1)
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(9118464, 3)
        self.flat = nn.Flatten()
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, sent_id, mask):
        _, _, all_layers = self.bert(sent_id, attention_mask=mask, output_hidden_states=True)
        # all_layers  = [32, 13, 64, 768]
        x = torch.cat(all_layers, 0) # x= [416, 64, 768]
        x = self.conv(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.flat(x)
        x = self.fc(x)
        return self.softmax(x)
</code></pre>
<p>I keep getting an error saying that the convolution method expected a certain dimensions as input but got a different one.</p>
<pre><code>&lt;generator object BERT_Arch.forward.&lt;locals&gt;.&lt;genexpr&gt; at 0x7fbeffc2d200&gt;
torch.Size([416, 64, 768])
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-12-3a2c2cd7c02d&gt; in &lt;module&gt;()
    362 
    363         # train model
--&gt; 364         train_loss, _ = train()
    365 
    366         # evaluate model

5 frames
&lt;ipython-input-12-3a2c2cd7c02d&gt; in train()
    148 
    149         # get model predictions for the current batch
--&gt; 150         preds = model(sent_id, mask)
    151 
    152         # compute the loss between actual and predicted values

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

&lt;ipython-input-12-3a2c2cd7c02d&gt; in forward(self, sent_id, mask)
     42         x = torch.cat(all_layers, 0) # torch.Size([13, 32, 64, 768])
     43         print(x.shape)
---&gt; 44         x = self.conv(x)
     45         x = self.relu(x)
     46         x = self.pool(x)

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py in forward(self, input)
    421 
    422     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 423         return self._conv_forward(input, self.weight)
    424 
    425 class Conv3d(_ConvNd):

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight)
    418                             _pair(0), self.dilation, self.groups)
    419         return F.conv2d(input, weight, self.bias, self.stride,
--&gt; 420                         self.padding, self.dilation, self.groups)
    421 
    422     def forward(self, input: Tensor) -&gt; Tensor:

RuntimeError: Expected 4-dimensional input for 4-dimensional weight [13, 13, 3, 768], but got 3-dimensional input of size [416, 64, 768] instead
</code></pre>
<p>I tried different values for the convolution method arguments, I still got a similar error.
And sometimes an error saying that the maxpooling output size is too small:</p>
<pre><code>Given input size: (64x62x1). Calculated output size: (64x31x0). Output size is too small
</code></pre>
<p>and sometimes this error (after changing the arguments of the cnn layer):</p>
<pre><code>RuntimeError: Given groups=1, weight of size [32, 32, 3, 3], expected input[13, 4, 64, 768] to have 32 channels, but got 4 channels instead
</code></pre>
<p>or</p>
<pre><code>Expected input batch_size (X) to match target batch_size (Y)
</code></pre>
<p>How can I do this?
I would be grateful for any help on how to do this CNN layer correctly.</p>
","10303779","","","","","2020-12-25 10:58:46","BERT Based CNN - Convolution and Maxpooling","<deep-learning><neural-network><pytorch><conv-neural-network><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"65881820","1","65882542","","2021-01-25 09:13:28","","1","2402","<p>I am getting the following error :</p>
<p><code>AssertionError: text input must of type str (single example), List[str] (batch or single pretokenized example) or List[List[str]] (batch of pretokenized examples).</code>, when I run <code>classifier(encoded)</code>. My text type is <code>str</code> so I am not sure what I am doing wrong. Any help is very appreciated.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoTokenizer, BertTokenizer, BertModel, BertForMaskedLM, AutoModelForSequenceClassification, pipeline

# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows
import logging
logging.basicConfig(level=logging.INFO)

# Load pre-trained model tokenizer (vocabulary)
# used the cased instead of uncased to account for cases like BAD.
tokenizer = BertTokenizer.from_pretrained('bert-base-cased') 


# alternative? what is the difference between these two tokenizers? 
#tokenizer = AutoTokenizer.from_pretrained(&quot;textattack/bert-base-uncased-SST-2&quot;)

model = AutoModelForSequenceClassification.from_pretrained(&quot;textattack/bert-base-uncased-SST-2&quot;)


# feed the model and the tokenizer into the pipeline
classifier = pipeline('sentiment-analysis', model=model, tokenizer= tokenizer)


#---------------sample raw input passage--------

text = &quot;Who was Jim Henson ? Jim Henson was a puppeteer. He is simply awful.&quot;
# tokenized_text = tokenizer.tokenize(text)

#----------Tokenization and Padding---------
# Encode the sentences to get tokenized and add padding stuff
encoded = tokenizer.encode_plus(
    text=text,  # the sentences to be encoded
    add_special_tokens=True,  # Add [CLS] and [SEP] !!!
    max_length = 64,  # maximum length of a sentence  (TODO Figure the longest passage length)
    pad_to_max_length=True,  # Add [PAD]s
    return_attention_mask = True,  # Generate the attention mask
    truncation=True,  #explicitly truncate examples to max length
    return_tensors = 'pt',  # ask the function to return PyTorch tensors
)

#-------------------------------------------
# view the IDs
for key, value in encoded.items():
    print(f&quot;{key}: {value.numpy().tolist()}&quot;)
    
#-------------------------------------------


classifier(encoded)

</code></pre>
","5669747","","7013263","","2021-01-25 09:51:29","2021-01-25 10:02:04","HuggingFace Bert Sentiment analysis","<python><bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"66286485","1","","","2021-02-19 23:53:35","","0","165","<p>I'm using the transformers FeatureExtractionPipeline like this:</p>
<pre><code>from transformers import pipeline, LongformerTokenizer, LongformerModel

tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')

model = LongformerModel.from_pretrained(&quot;allenai/longformer-base-4096&quot;)

nlu_feature_pipeline = pipeline(task=&quot;feature-extraction&quot;, model=model, tokenizer=tokenizer)
</code></pre>
<p>However, it seems like the pipeline doesn't use truncation to ensure that no sequence is longer 4096, resulting in this:</p>
<blockquote>
<p>Token indices sequence length is longer than the specified maximum sequence length for this model (8912 &gt; 4096). Running this sequence through the model will result in indexing errors</p>
</blockquote>
<p>Is there any way I can enable the truncation in the pipeline? Or is it somehow possible to maybe tokenize beforehand and then feed it into the pipeline?</p>
","7667887","","","","","2021-02-19 23:53:35","Enabling truncation in transformers feature extraction pipeline","<huggingface-transformers><huggingface-tokenizers>","0","1","","","","CC BY-SA 4.0"
"66287735","1","66380309","","2021-02-20 03:32:21","","0","1240","<p>Based on <a href=""https://github.com/huggingface/transformers/tree/master/examples/language-modeling"" rel=""nofollow noreferrer"">HuggingFace script</a> to train a transformers model from scratch.
I run:</p>
<pre><code>python3 run_mlm.py \
--dataset_name wikipedia \
--tokenizer_name roberta-base \
--model_type roberta \
--dataset_config_name 20200501.en \
--do_train \
--do_eval \
--learning_rate 1e-5 \
--num_train_epochs 5 \
--save_steps 5000 \
--warmup_steps=10000 \ 
--seed 666 \
--gradient_accumulation_steps=4 \ 
--output_dir models/mlm_wikipedia_scratch/ \
--per_gpu_train_batch_size 8
</code></pre>
<p>I don't understand why I can't see my python3 process on GPU running <code>nvidia-smi</code>
Here a screen:
<a href=""https://i.stack.imgur.com/9JkYX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9JkYX.png"" alt=""top | nvidia-smi | training_script "" /></a></p>
","7607899","","7607899","","2021-02-24 18:23:20","2021-02-26 04:50:27","HuggingFace Training using GPU","<python><python-3.x><nlp><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"65431837","1","65431838","","2020-12-23 22:44:30","","9","4913","<p>I'm following the transformer's pretrained model <a href=""https://huggingface.co/joeddav/xlm-roberta-large-xnli?text=%0A&amp;candidate_labels=&amp;multi_class=true"" rel=""noreferrer"">xlm-roberta-large-xnli</a> example</p>
<pre><code>from transformers import pipeline
classifier = pipeline(&quot;zero-shot-classification&quot;,
                      model=&quot;joeddav/xlm-roberta-large-xnli&quot;)
</code></pre>
<p>and I get the following error</p>
<pre><code>ValueError: Couldn't instantiate the backend tokenizer from one of: (1) a `tokenizers` library serialization file, (2) a slow tokenizer instance to convert or (3) an equivalent slow tokenizer class to instantiate and convert. You need to have sentencepiece installed to convert a slow tokenizer to a fast one.
</code></pre>
<p>I'm using Transformers version <code>'4.1.1'</code></p>
","12483346","","355230","","2020-12-23 22:58:14","2021-08-24 04:37:00","Transformers v4.x: Convert slow tokenizer to fast tokenizer","<python><nlp><huggingface-transformers><huggingface-tokenizers>","3","0","1","","","CC BY-SA 4.0"
"57891759","1","","","2019-09-11 14:51:08","","1","608","<p>I am following the quick start guide <a href=""https://huggingface.co/pytorch-transformers/quickstart.html"" rel=""nofollow noreferrer"">here</a>. The problem is they have provided code for GPU machine and I am running the code on CPU based Ubuntu machine. I have commented the lines that put everything in CUDA. The code is now showing error and I don't know how to resolve it.
The question is ""How can I make this work?""</p>

<p>I have checked <a href=""https://stackoverflow.com/questions/50495053/if-im-not-specifying-to-use-cpu-gpu-which-one-is-my-script-using"">this</a> answer and this is not what I'm looking for.</p>

<p>The full code is <a href=""https://huggingface.co/pytorch-transformers/quickstart.html#bert-example"" rel=""nofollow noreferrer"">here</a></p>

1. Using BertModel to encode inputs in hidden-states:

<pre><code>#Load pre-trained model (weights)
model = BertModel.from_pretrained('bert-base-uncased')

#Set the model in evaluation mode to desactivate the DropOut modules
# This is IMPORTANT to have reproducible results during evaluation!
model.eval()

#***I have commented these 3 lines*** 

# If you have a GPU, put everything on cuda
#tokens_tensor = tokens_tensor.to('cuda')
#segments_tensors = segments_tensors.to('cuda')
#model.to('cuda')

#Rest all is untouched
# *** -----------------***---------------***

# Predict hidden states features for each layer
with torch.no_grad():
    # See the models docstrings for the detail of the inputs
    outputs = model(tokens_tensor, token_type_ids=segments_tensors)
 # PyTorch-Transformers models always output tuples.
 # See the models docstrings for the detail of all the outputs
 # In our case, the first element is the hidden state of the last layer of the Bert model
    encoded_layers = outputs[0]
# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)
assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)
</code></pre>

<p>Error for 1:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-40-a86e9643e7f3&gt; in &lt;module&gt;
     11 
     12 # We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)
---&gt; 13 assert tuple(encoded_layers).shape == (1, len(indexed_tokens), model.config.hidden_size)

AttributeError: 'tuple' object has no attribute 'shape'
</code></pre>

<h1>2. Using BertForMaskedLM to predict a masked token:</h1>

<pre><code># Load pre-trained model (weights)
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval()

#***---------------Commented--------------------------
# If you have a GPU, put everything on cuda
#tokens_tensor = tokens_tensor.to('cuda')
#segments_tensors = segments_tensors.to('cuda')
#model.to('cuda')

#***---------------------------------------------

# Predict all tokens
with torch.no_grad():
    outputs = model(tokens_tensor, token_type_ids=segments_tensors)
    predictions = outputs[0]

# confirm we were able to predict 'henson'
predicted_index = torch.argmax(predictions[0, masked_index]).item()
predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
assert predicted_token == 'henson'
</code></pre>

<p>Error for 2:</p>

<pre><code>---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-42-9b965490d278&gt; in &lt;module&gt;
     17 predicted_index = torch.argmax(predictions[0, masked_index]).item()
     18 predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
---&gt; 19 assert predicted_token == 'henson'

AssertionError:
</code></pre>
","7198441","","1150683","","2020-01-16 10:24:52","2020-01-16 10:24:52","Code example in Hugging Face Pytorch-Transformers quickstart documentation","<python><nlp><pytorch><huggingface-transformers>","0","5","","","","CC BY-SA 4.0"
"66250745","1","","","2021-02-17 22:19:20","","-1","67","<p>I used pytorch ran a ML model and used flask created an app. Everything works fine on my local machine as well as on server. Newly learnt docker image and wish to pack everything in. I have mac and installed docker desktop. With numerous attempts and configurations the container based on the image i created stops after finish running the py code.</p>
<p>Code for setting up the dockerfile</p>
<pre><code>FROM python:3.7.9-stretch
RUN apt-get update
WORKDIR  /usr/src/App
COPY requirements.txt . 
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
RUN ls -la .
CMD [&quot;python3&quot;, &quot;app.py&quot;, &quot;--host=0.0.0.0&quot;, &quot;--port=8000&quot;]
</code></pre>
<p>code for building the image:</p>
<pre><code>docker build -t app:v1 . 
</code></pre>
<p>code for running the container:</p>
<pre><code>docker run -p 8000:8000 app:v1
</code></pre>
<p>in my py file, i also specified:</p>
<pre><code>if __name__ == &quot;__main__&quot;:
app.run(host='0.0.0.0',port=8000, debug=True)
</code></pre>
<p>After the code finish running, the container exits.</p>
<p>I looked at the log file, there's seemingly some error, but I don't think that causes anything as it also says after the error that if i'm using gpu then i can ignore it (i am not using gpu)</p>
<blockquote>
<p><a href=""https://i.stack.imgur.com/ZORC9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZORC9.png"" alt=""Blockquote"" /></a></p>
</blockquote>
","8426105","","8426105","","2021-02-17 22:38:24","2021-02-18 18:10:56","pytorch based Flask app image exits after running","<docker><tensorflow><dockerfile><huggingface-transformers>","2","0","","","","CC BY-SA 4.0"
"66321085","1","","","2021-02-22 18:05:57","","1","1316","<p>Iâ€™d like to train a transformer encoder (e.g. BERT) on time-series data for a task that can be modeled as classification. Let met briefly describe the data Iâ€™m using before talking about the issue Iâ€™m facing.</p>
<p>Iâ€™m working with 90 seconds windows, and I have access to 100 values for each second (i.e. 90 vectors of size 100). My goal is to predict a binary label (0 or 1) <strong>for each second</strong> (i.e. produce a final vector of 0s ans 1s of length 90).</p>
<p>My first idea was to model this as a multi-label classification problem, where I would use BERT to produce a vector of size 90 filled with numbers between 0 and 1 and regress using nn.BCELoss and the groundtruth label (y_true looks like [0,0,0,1,1,1,0,0,1,1,1,0...,0]). A simple analogy would be to consider each second as a <em>word</em>, and the 100 values I have access to as the corresponding <em>word embedding</em>. The goal is then to train BERT (from scratch) on these sequences of 100-dim embedding (all sequence lengths are the same: 90).</p>
<p>The problem: when dealing with textual inputs, we simply add the CLS and SEP tokens to the input sequences, and let the tokenizer and the model do the rest of the job. When training directly on embeddings, what should we do to account for CLS and SEP tokens?</p>
<p>One idea I had was to add a 100-dim embedding at position 0 standing for the CLS token, as well as a 100-dim embedding on position 90+1=91 standing for the SEP token. But I donâ€™t know what embeddings I should use for these two tokens. And Iâ€™m not sure thatâ€™s a good solution either.</p>
<p>Any ideas?</p>
<p>(I tried asking this question on Huggingface forums but didn't get any response.)</p>
","12041431","","4685471","","2021-02-22 22:07:25","2021-02-24 02:53:17","BERT for time series classification","<python><deep-learning><time-series><bert-language-model><huggingface-transformers>","1","1","","","","CC BY-SA 4.0"
"66372741","1","66375940","","2021-02-25 16:41:42","","2","861","<p>I am in a situation where I am working with huggingface transformers and have got some insights into it. I am working with the facebook/bart-large-cnn model to perform text summarisation for my project and I am using the following code as of now to do some tests:</p>
<pre><code>text = &quot;&quot;&quot;
Justin Timberlake and Jessica Biel, welcome to parenthood. 
The celebrity couple announced the arrival of their son, Silas Randall Timberlake, in 
statements to People.&quot;&quot;&quot;

from transformers import pipeline
smr_bart = pipeline(task=&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;)
smbart = smr_bart(text, max_length=150)
print(smbart[0]['summary_text'])
</code></pre>
<p>The small peice of code is actually giving me a very good summary of the text. But my ask is that how can I apply the same pre trained model on top of my dataframe column. My dataframe looks like this:</p>
<pre><code>ID        Lang          Text
1         EN            some long text here...
2         EN            some long text here...
3         EN            some long text here...
</code></pre>
<p>.... and so on for 50K rows</p>
<p>Now I want to apply the pre trained model to the col Text to generate a new column df['summary'] from it and the resultant dataframe should look like:</p>
<pre><code>ID        Lang         Text                              Summary
1         EN            some long text here...           Text summary goes here...
2         EN            some long text here...           Text summary goes here...
3         EN            some long text here...           Text summary goes here...
</code></pre>
<p>How can I achieve this? Any help would be much appreciated.</p>
","12226377","","","","","2021-02-25 20:23:13","Applying pre trained facebook/bart-large-cnn for text summarization in python","<python-3.x><nlp><huggingface-transformers><summarization><huggingface-tokenizers>","1","1","","","","CC BY-SA 4.0"
"66315926","1","66333421","","2021-02-22 12:41:49","","0","613","<p>I'm trying to localize all the [UNK] tokens of BERT tokenizer on my text. Once I have the position of the UNK token, I need to identify what word it belongs to. For that, I tried to get the position of the word using words_ids() or token_to_words() methods (the result is the same, I think) which give me the id word of this token.</p>
<p>The problem is, for a large text, there are many ways to split the text by words, and the ways I tried don't match with the position I get from token_to_words method. How I can split my text in the same way Bert tokenizer do?</p>
<p>I saw BERT use WordPiece for tokenize in sub-words, but nothing for complete words.</p>
<p>I'm at this point:</p>
<pre><code>  tokenized_text = tokenizer.tokenize(texto) # Tokens
  encoding_text = tokenizer(texto) # Esto es de tipo batchEncoding, como una instancia del tokenizer
  tpos = [i for i, element in enumerate(tokenized_text) if element == &quot;[UNK]&quot;]  # Posicion en la lista de tokens

  word_list = texto.split(&quot; &quot;)
  for x in tpos:
    wpos = encoding_text.token_to_word(x) # Posicion en la lista de palabras
    print(&quot;La palabra:  &quot;, word_list[wpos], &quot;    contiene un token desconocido: &quot;, tokenizer.tokenize(word_list[wpos]))
</code></pre>
<p>but it fails because the index &quot;wpos&quot; doesn't fit properly with my word_list.</p>
","15259559","","15259559","","2021-02-23 17:25:33","2021-02-23 17:25:33","Split a sentence by words just as BERT Tokenizer would do?","<python><nlp><tokenize><bert-language-model><huggingface-transformers>","1","5","1","","","CC BY-SA 4.0"
"66344921","1","","","2021-02-24 04:55:47","","0","144","<p>I am using the DialoGPT model from Microsoft to build a chatbot. To fine-tune the model on my dataset, I am using this blog <a href=""https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30"" rel=""nofollow noreferrer"">https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30</a> whose reference is given on the DialoGPT GitHub page.
The fine-tuning done in this blog is for a fixed number of steps. Instead of keeping a fixed number of steps, I want my conversation to end only when my user says &quot;bye&quot; or a similar word. If the user says &quot;bye&quot; in say 6 or 10 steps, the fine-tuned model is working fine. However, if the conversation goes long, the model starts emitting special characters. Attached is the screenshot of the same.</p>
<p>Does anybody have any idea what causes a language model to emit special characters instead of words? I saw similar comments on this blog by others too but no solution.</p>
<p>Thank you in advance.<br />
<a href=""https://i.stack.imgur.com/Cns2i.png"" rel=""nofollow noreferrer"">Screenshot of the conversation</a></p>
","13129980","","","","","2021-06-26 10:36:38","DialoGPT model generating special symbols as chatbot reply","<pytorch><chatbot><huggingface-transformers><transfer-learning>","0","0","0","","","CC BY-SA 4.0"
"66391650","1","","","2021-02-26 19:22:02","","0","46","<p>I have a code snippet working perfectly on short sentences (&lt;1000 tokens). But I wonder how to turn this snippet to support longer sentences. I was thinking about batch processing, however it is not a classification task, and there are no labels for the sentences. Thus most online posts using torchtext with don't work for my case.</p>
<p>Is there any efficient way to calculate overall perplexity for document which has very long sentences?</p>
<p>Here is my code snippet:</p>
<pre><code>encodings = tokenizer(input_text, return_tensors=&quot;pt&quot;,
                      return_attention_mask=True,
                      add_special_tokens=True,
                      truncation=True,
                      max_length=1024)
input_ids = encodings[&quot;input_ids&quot;]
att_mask = encodings[&quot;attention_mask&quot;]
input_ids = input_ids.to(DEVICE)
att_mask = att_mask.to(DEVICE)
outputs = model(input_ids, attention_mask=att_mask, labels=input_ids)
perp = math.exp(outputs[0].item())
</code></pre>
<p>where <code>input_text</code> is the document with longer sentences, <code>tokenizer</code> and <code>model</code> are instances from huggingface's transformer packages, i.e., <code>BertTokenizer</code>, <code>BertModel</code>.</p>
<p>Thanks!</p>
","4826046","","","","","2021-02-26 19:22:02","How to calculate perplexity on very long documents in pytorch?","<python-3.x><nlp><pytorch><huggingface-transformers>","0","0","1","","","CC BY-SA 4.0"
"66302371","1","","","2021-02-21 12:37:35","","2","217","<p>I have followed the basic example as given below, from: <a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/training.html</a></p>
<pre><code>from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments

model = TFBertForSequenceClassification.from_pretrained(&quot;bert-large-uncased&quot;)

training_args = TFTrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total # of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
)

trainer = TFTrainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=tfds_train_dataset,    # tensorflow_datasets training dataset
    eval_dataset=tfds_test_dataset       # tensorflow_datasets evaluation dataset
)
trainer.train()
</code></pre>
<p>But there seems to be no way to specify the loss function for the classifier. For-ex if I finetune on a binary classification problem, I would use</p>
<pre><code>tf.keras.losses.BinaryCrossentropy(from_logits=True)
</code></pre>
<p>else I would use</p>
<pre><code>tf.keras.losses.CategoricalCrossentropy(from_logits=True)
</code></pre>
<p>My set up is as follows:</p>
<pre><code>transformers==4.3.2
tensorflow==2.3.1
python==3.6.12
</code></pre>
","10944913","","","","","2021-05-23 08:30:09","How to specify the loss function when finetuning a model using the Huggingface TFTrainer Class?","<python-3.x><tensorflow><nlp><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"66324875","1","","","2021-02-22 23:23:44","","0","33","<p>I have a tensor of values and a corresponding tensor of indices. I want to sum all the values where the index is not a special token (tokenizers).</p>
<p>For instance:</p>
<pre><code>
values=tensor([1.0000, 0.1574, 0.1507, 0.2520, 0.2456, 0.2365, 0.2330, 0.2294, 0.2321,
         0.2339], grad_fn=&lt;MaxBackward0&gt;),
indices=tensor([32099,    12,    24,     3,     3,     3,     3,     3,     3,     3])
</code></pre>
<p>I want sum(0.1574, 0.1507)
What is the optimal way to achieve this?</p>
","6392022","","7802200","","2021-02-28 17:27:47","2021-02-28 17:27:47","Adding the tensor values based on indices","<python-3.x><pytorch><huggingface-transformers><huggingface-tokenizers>","0","0","","","","CC BY-SA 4.0"
"66377625","1","","","2021-02-25 22:46:30","","0","492","<p>I have this code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, AutoModel

model = 'bert-base-uncased' 
tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModel.from_pretrained(model)
Sentence_vectorList = []
for sent in x_train:

  input_sentence = torch.tensor(tokenizer.encode(sent)).unsqueeze(0)
  out = model(input_sentence)
  embeddings_of_last_layer = out[0]
  cls_embeddings = embeddings_of_last_layer[0]

  cls_layer = cls_embeddings.detach().numpy()

  sent_emd = np.average(cls_layer,axis=0)
</code></pre>
<p>The task is to take the sentence vectors and detach them in [n x 768] then I save them as sent2vec. This process is taking a lot of time. Is there a more efficient way to do it?</p>
","15286493","","5652313","","2021-02-26 07:58:25","2021-02-26 07:58:25","Speed up sentence processing by BERT in Transformers","<python><nlp><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66417579","1","","","2021-03-01 06:30:07","","0","252","<p>I am trying to load and train again a HuggingFace BERT model. But, I keep getting <code>'BertForPreTraining' object has no attribute 'train_model'</code> error.</p>
<p>The model and the other files are downloaded from here: <a href=""https://huggingface.co/bert-base-uncased/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/bert-base-uncased/tree/main</a></p>
<p>I downloaded and renamed the model file to <code>pytorch_model.bin</code></p>
<p>Here is the simple code to load and train the model.</p>
<pre><code>    BERT_PATH = './input/pretrained_models/'
    config = BertConfig.from_pretrained(BERT_PATH + 'config.json')
    model = BertForPreTraining.from_pretrained(BERT_PATH, config=config)
    model.train_model(train_df)
</code></pre>
","9815697","","","","","2021-03-01 06:30:07","'BertForPreTraining' object has no attribute 'train_model'","<pytorch><bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"66561880","1","","","2021-03-10 09:30:45","","0","221","<p>I am using the <a href=""https://github.com/pair-code/lit"" rel=""nofollow noreferrer"">Language Interpretability Toolkit</a> (LIT) to load and analyze a BERT model that I pre-trained on an NER task.</p>
<p>However, when I'm starting the LIT script with the path to my pre-trained model passed to it, it fails to initialize the weights and tells me:</p>
<pre><code>    modeling_utils.py:648] loading weights file bert_remote/examples/token-classification/Data/Models/results_21_03_04_cleaned_annotations/04.03._8_16_5e-5_cleaned_annotations/04-03-2021 (15.22.23)/pytorch_model.bin
    modeling_utils.py:739] Weights of BertForTokenClassification not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
    modeling_utils.py:745] Weights from pretrained model not used in BertForTokenClassification: ['bert.embeddings.position_ids']

</code></pre>
<p>It then simply uses the <code>bert-base-german-cased</code> version of BERT, which of course doesn't have my custom labels and thus fails to predict anything. I think it might have to do with PyTorch, but I can't find the error.</p>
<p>If relevant, here is how I load my dataset into CoNLL 2003 format (modification of the dataloader scripts found <a href=""https://github.com/PAIR-code/lit/tree/main/lit_nlp/examples/datasets"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>    def __init__(self):

        # Read ConLL Test Files

        self._examples = []

        data_path = &quot;lit_remote/lit_nlp/examples/datasets/NER_Data&quot;
        with open(os.path.join(data_path, &quot;test.txt&quot;), &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
            lines = f.readlines()

        for line in lines[:2000]:
            if line != &quot;\n&quot;:
                token, label = line.split(&quot; &quot;)
                self._examples.append({
                    'token': token,
                    'label': label,
                })
            else:
                self._examples.append({
                    'token': &quot;\n&quot;,
                    'label': &quot;O&quot;
                })

    def spec(self):
        return {
            'token': lit_types.Tokens(),
            'label': lit_types.SequenceTags(align=&quot;token&quot;),
        }
</code></pre>
<p>And this is how I initialize the model and start the LIT server (modification of the <code>simple_pytorch_demo.py</code> script found <a href=""https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/simple_pytorch_demo.py"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>    def __init__(self, model_name_or_path):
        self.tokenizer = transformers.AutoTokenizer.from_pretrained(
            model_name_or_path)
        model_config = transformers.AutoConfig.from_pretrained(
            model_name_or_path,
            num_labels=15,  # FIXME CHANGE
            output_hidden_states=True,
            output_attentions=True,
        )
        # This is a just a regular PyTorch model.
        self.model = _from_pretrained(
            transformers.AutoModelForTokenClassification,
            model_name_or_path,
            config=model_config)
        self.model.eval()

## Some omitted snippets here

    def input_spec(self) -&gt; lit_types.Spec:
        return {
            &quot;token&quot;: lit_types.Tokens(),
            &quot;label&quot;: lit_types.SequenceTags(align=&quot;token&quot;)
        }

    def output_spec(self) -&gt; lit_types.Spec:
        return {
            &quot;tokens&quot;: lit_types.Tokens(),
            &quot;probas&quot;: lit_types.MulticlassPreds(parent=&quot;label&quot;, vocab=self.LABELS),
            &quot;cls_emb&quot;: lit_types.Embeddings()
</code></pre>
","13440007","","13440007","","2021-03-10 09:56:21","2021-03-10 09:56:21","Weights of pre-trained BERT model not initialized","<tensorflow><nlp><pytorch><bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"66579324","1","66619742","","2021-03-11 09:03:59","","2","198","<p>I am trying to train a seq2seq model. I ran the example code in Colab:</p>
<pre><code>!git clone https://github.com/huggingface/transformers
!git clone https://github.com/huggingface/datasets
!pip install transformers
!pip install datasets
</code></pre>
<pre><code>!python transformers/examples/seq2seq/run_seq2seq.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --task summarization \
    --dataset_name xsum \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate \
    --max_train_samples 500 \
    --max_val_samples 500
</code></pre>
<p>and got this error</p>
<pre><code>I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File &quot;transformers/examples/seq2seq/run_seq2seq.py&quot;, line 47, in &lt;module&gt;
    from transformers.file_utils import is_offline_mode
ImportError: cannot import name 'is_offline_mode' from 'transformers.file_utils' (/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py)
</code></pre>
<p>Any ideas?</p>
","14882176","","","","","2021-03-13 23:54:54","Error running run_seq2seq.py Transformers training script","<python><tensorflow><machine-learning><nlp><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"65475908","1","","","2020-12-28 10:07:53","","0","207","<p>I need a suggestion of using an existing bert model which is pre-trained for sentence classification.</p>
<p>the existing model accepts text in form of: â€œClC1=CC=CC(Cl)=C1C=O.ClC1=CC=CC(Cl)â€ which is a chemical reaction for example. Now to enhance the model where I can use continuous values such as time and temperature as features to it to retrain the model on my dataset.</p>
<p>my idea is to use an ensemble approach, where I take the last layer output values of the existing bert model, create another model to just accept the continuous variables and concat them in an ensemble approach similar to this link: Combining Trained Models in PyTorch</p>
<p>Any alternate approaches or suggestions? or links to use as a resource for this implementation</p>
<p>I use pytorch and huggingface transformers library</p>
<p>Thanks
Rahul Raj Devaraja</p>
","7468634","","13302","","2020-12-28 11:30:56","2020-12-28 11:30:56","Pretraining bert model with additional numerical features","<nlp><pytorch><bert-language-model><huggingface-transformers><transfer-learning>","0","0","","","","CC BY-SA 4.0"
"65484081","1","65490127","","2020-12-28 21:05:27","","0","413","<p>I have a situation where I am trying to using the pre-trained hugging-face models to translate a pandas column of text from Dutch to English. My input is simple:</p>
<pre><code>Dutch_text             
Hallo, het gaat goed
Hallo, ik ben niet in orde
Stackoverflow is nuttig
</code></pre>
<p>I am using the below code to translate the above column and I want to store my result into a new column ENG_Text. So the output will look like this:</p>
<pre><code>ENG_Text             
Hello, I am good
Hi, I'm not okay
Stackoverflow is helpful
</code></pre>
<p>The code that I am using is as follows:</p>
<pre><code>#https://huggingface.co/Helsinki-NLP for other pretrained models 
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-nl-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-nl-en&quot;)
input_1 = df['Dutch_text']
input_ids = tokenizer(&quot;translate English to Dutch: &quot;+input_1, return_tensors=&quot;pt&quot;).input_ids # Batch size 1
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(decoded)
</code></pre>
<p>Any help would be appreciated!</p>
","12226377","","","","","2020-12-29 09:37:06","Translating using pre-trained hugging face transformers not working","<python-3.x><nlp><translation><huggingface-transformers><huggingface-tokenizers>","1","1","","","","CC BY-SA 4.0"
"66581492","1","66583001","","2021-03-11 11:17:32","","-1","87","<p>I'm taking a pre-trained pegasus model through Huggingface transformers, (specifically, <code>google/pegasus-cnn_dailymail</code>, and I'm using Huggingface transformers through Pytorch) and I want to finetune it on my own data. This is however quite a large dataset and I've run into the problem of running out of VRAM halfway through training, which because of the size of the dataset can be a few days after training even started, which makes a trial-and-error approach very inefficient.</p>
<p>I'm wondering how I can make sure ahead of time that it doesn't run out of memory. I would think that the memory usage of the model is in some way proportional to the size of the input, so I've passed <code>truncation=True</code>, <code>padding=True</code>, <code>max_length=1024</code> to my tokenizer, which if my understanding is correct should make all the outputs of the tokenizer of the same size per line. Considering that the batch size is also a constant, I would think that the amount of VRAM in use should be stable. So I should just be able to cut up the dataset into managable parts, just looking at the ram/vram use of the first run, and infer that it will run smoothly from start to finish.</p>
<p>However, the opposite seems to be true. I've been observing the amount of VRAM used at any time and it can vary wildly, from ~12GB at one time to suddenly requiring more than 24GB and crashing (because I don't have more than 24GB).</p>
<p>So, how do I make sure that the amount of vram in use will stay within reasonable bounds for the full duration of the training process, and avoid it crashing due to a lack of vram when I'm already days into the training process?</p>
","1816721","","","","","2021-03-11 12:55:18","How do I prevent a lack of VRAM halfway through training a Huggingface Transformers (Pegasus) model?","<pytorch><huggingface-transformers><huggingface-tokenizers>","1","0","","","","CC BY-SA 4.0"
"66238009","1","","","2021-02-17 08:12:20","","1","68","<p>I am converting the T5 model from pytorch to ONNX using <a href=""https://github.com/onnx/models/blob/master/text/machine_comprehension/t5/dependencies/T5-export.py"" rel=""nofollow noreferrer"">this</a> script, but I am running into an Import module error. I don't think this a structural problem as this script has been used before by other people with no issues. Any ideas as to why I might be getting this error?</p>
<p>Here is the full error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:GitHub\models\text\machine_comprehension\t5\dependencies\T5-export.py&quot;, line 2, in &lt;module&gt;
    from .models import CombinedDecoder, SimplifiedT5Encoder
ImportError: attempted relative import with no known parent package
</code></pre>
","14882176","","14882176","","2021-02-17 17:31:06","2021-02-17 17:31:06","ImportError: attempted relative import with no known parent package in ONNX Library","<python><nlp><huggingface-transformers><onnx>","0","2","","","","CC BY-SA 4.0"
"65464004","1","65467742","","2020-12-27 08:25:57","","0","52","<p>I trained a BERT model using huggingface for tensorflow on my localhost. Running predictions on my localhost works fine.</p>
<p>I then implemented a solution so I can call my model from a GCP VM instance (Ubuntu 16.04) via flask. The process seems to work as I can successfully make the calls to my app on the VM.</p>
<p>However, the prediction I receive from the VM differs from the one I receive on my localhost (which is the expected output), yet I use identical code. I use a model for Sequence Classification, and when trying to get the probabilities for both labels on my localhost I get: <code>array([0.67829543, 0.32170454], dtype=float32)</code> while the VM returns <code>array([1, 1], dtype=float32)</code>.
This snippet is what I use to predict the model just for reference:</p>
<pre><code>predict_input = tokenizer.encode(sentence,
                                  truncation=True,
                                  padding=True,
                                  return_tensors=&quot;tf&quot;
                                  )
tf_output = model.predict(predict_input)[0]
    
tf_prediction = tf.nn.softmax(tf_output, axis=0).numpy()
</code></pre>
<p>On my localhost I trained the model using tf with GPU support, the VM of course only has two vCPUs. When loading tensorflow on the VM I get the following warnings:</p>
<pre><code>2020-12-27 07:57:55.533847: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-12-27 07:57:55.533896: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2020-12-27 07:57:56.792914: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-12-27 07:57:56.792966: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-12-27 07:57:56.793002: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bertvm-1): /proc/driver/nvidia/version does not exist
2020-12-27 07:57:56.793316: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 A
VX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-27 07:57:56.801469: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000129999 Hz
2020-12-27 07:57:56.801693: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x64b8fe0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-12-27 07:57:56.801805: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
</code></pre>
<p>I'm not sure if that is the root of the error or if it's because I trained the model using tf for GPU and am predicting on an instance that runs tf for CPU but that doesn't seem to make too much sense to me.
The warnings only seem to pertain to CUDA 'issues' which I believe is related to GPU support.</p>
<p>Any idea or tips as to what could be the cause of the different predictions?
Thanks for your help in advance!</p>
<p>EDIT:
It seems that the model returns the same logits on both, the VM and the localhost. When I then apply <code>tf.nn.softmax(tf_output, axis=0).numpy()</code> I get different results.
tf_output being <code>[1.9530067 1.2070574]</code> on both instances while the above function returns <code>[0.67829543 0.32170454]</code> on localhost and <code>[[1. 1.]]</code> on the VM (both formatted here as a string) as mentioned above.</p>
","8291269","","400617","","2020-12-27 14:18:19","2020-12-27 17:20:36","Running a flask app that uses tensorflow on an Ubuntu 16.04 instance on GCP, model runs but predictions are different than on local host","<tensorflow><google-cloud-platform><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66602656","1","66603865","","2021-03-12 15:21:51","","3","729","<p>I am following the instruction (<a href=""https://github.com/huggingface/transfer-learning-conv-ai"" rel=""nofollow noreferrer"">https://github.com/huggingface/transfer-learning-conv-ai</a>) to install conv-ai from huggingface, but I got stuck on the docker build step: <code>docker build -t convai .</code></p>
<p>I am using Mac 10.15, python 3.8, increased Docker memory to 4G.</p>
<p>I have tried the following ways to solve the issue:</p>
<ol>
<li>add <code>numpy</code> in <code>requirements.txt</code></li>
<li>add <code>RUN pip3 install --upgrade setuptools</code> in Dockerfile</li>
<li>add <code>--upgrade</code> to <code>RUN pip3 install -r /tmp/requirements.txt</code> in Dockerfile</li>
<li>add <code>RUN pip3 install numpy</code> before <code>RUN pip3 install -r /tmp/requirements.txt</code> in Dockerfile</li>
<li>add <code>RUN apt-get install python3-numpy</code> before <code>RUN pip3 install -r /tmp/requirements.txt</code> in Dockerfile</li>
<li>using python 3.6.13 because of this <a href=""https://github.com/huggingface/transfer-learning-conv-ai/issues/97"" rel=""nofollow noreferrer"">post</a>, but it has exact same error.</li>
<li>I am currently working on debugging inside the container by entering right before the <code>RUN pip3 install requirements.txt</code></li>
</ol>
<p>Can anyone help me on this? Thank you!!</p>
<p>The error:</p>
<pre><code> =&gt; [6/9] COPY . ./                                                                                                          0.0s
 =&gt; [7/9] COPY requirements.txt /tmp/requirements.txt                                                                        0.0s
 =&gt; ERROR [8/9] RUN pip3 install -r /tmp/requirements.txt                                                                   98.2s
------
 &gt; [8/9] RUN pip3 install -r /tmp/requirements.txt:
#12 1.111 Collecting torch (from -r /tmp/requirements.txt (line 1))
#12 1.754   Downloading https://files.pythonhosted.org/packages/46/99/8b658e5095b9fb02e38ccb7ecc931eb1a03b5160d77148aecf68f8a7eeda/torch-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (735.5MB)
#12 81.11 Collecting pytorch-ignite (from -r /tmp/requirements.txt (line 2))
#12 81.76   Downloading https://files.pythonhosted.org/packages/f8/d3/640f70d69393b415e6a29b27c735047ad86267921ad62682d1d756556d48/pytorch_ignite-0.4.4-py3-none-any.whl (200kB)
#12 81.82 Collecting transformers==2.5.1 (from -r /tmp/requirements.txt (line 3))
#12 82.17   Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)
#12 82.29 Collecting tensorboardX==1.8 (from -r /tmp/requirements.txt (line 4))
#12 82.50   Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)
#12 82.57 Collecting tensorflow (from -r /tmp/requirements.txt (line 5))
#12 83.12   Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)
#12 95.24 Collecting spacy (from -r /tmp/requirements.txt (line 6))
#12 95.81   Downloading https://files.pythonhosted.org/packages/65/01/fd65769520d4b146d92920170fd00e01e826cda39a366bde82a87ca249db/spacy-3.0.5.tar.gz (7.0MB)
#12 97.41     Complete output from command python setup.py egg_info:
#12 97.41     Traceback (most recent call last):
#12 97.41       File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
#12 97.41       File &quot;/tmp/pip-build-cc3a804w/spacy/setup.py&quot;, line 5, in &lt;module&gt;
#12 97.41         import numpy
#12 97.41     ModuleNotFoundError: No module named 'numpy'
#12 97.41     
#12 97.41     ----------------------------------------
#12 98.11 Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-cc3a804w/spacy/
</code></pre>
<p>@HÃ¥ken Lid The error I got if I <code>RUN pip3 install numpy</code> right before <code>RUN pip3 install -r tmp/requirements</code>:</p>
<pre><code> =&gt; [ 8/10] RUN pip3 install numpy                                                                                          10.1s
 =&gt; ERROR [ 9/10] RUN pip3 install -r /tmp/requirements.txt                                                                112.4s
------                                                                                                                            
 &gt; [ 9/10] RUN pip3 install -r /tmp/requirements.txt:                                                                             
#13 1.067 Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r /tmp/requirements.txt (line 1)) 
#13 1.074 Collecting torch (from -r /tmp/requirements.txt (line 2))                                                               
#13 1.656   Downloading https://files.pythonhosted.org/packages/46/99/8b658e5095b9fb02e38ccb7ecc931eb1a03b5160d77148aecf68f8a7eeda/torch-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (735.5MB)                                                                           
#13 96.46 Collecting pytorch-ignite (from -r /tmp/requirements.txt (line 3))
#13 97.02   Downloading https://files.pythonhosted.org/packages/f8/d3/640f70d69393b415e6a29b27c735047ad86267921ad62682d1d756556d48/pytorch_ignite-0.4.4-py3-none-any.whl (200kB)
#13 97.07 Collecting transformers==2.5.1 (from -r /tmp/requirements.txt (line 4))
#13 97.32   Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)
#13 97.43 Collecting tensorboardX==1.8 (from -r /tmp/requirements.txt (line 5))
#13 97.70   Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)
#13 97.76 Collecting tensorflow (from -r /tmp/requirements.txt (line 6))
#13 98.27   Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)
#13 109.6 Collecting spacy (from -r /tmp/requirements.txt (line 7))
#13 110.0   Downloading https://files.pythonhosted.org/packages/65/01/fd65769520d4b146d92920170fd00e01e826cda39a366bde82a87ca249db/spacy-3.0.5.tar.gz (7.0MB)
#13 111.6     Complete output from command python setup.py egg_info:
#13 111.6     Traceback (most recent call last):
#13 111.6       File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
#13 111.6       File &quot;/tmp/pip-build-t6n57csv/spacy/setup.py&quot;, line 10, in &lt;module&gt;
#13 111.6         from Cython.Build import cythonize
#13 111.6     ModuleNotFoundError: No module named 'Cython'
#13 111.6     
#13 111.6     ----------------------------------------
#13 112.3 Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-t6n57csv/spacy/
------
executor failed running [/bin/sh -c pip3 install -r /tmp/requirements.txt]: exit code: 1
</code></pre>
<p>requirements.txt:</p>
<pre><code>torch
pytorch-ignite
transformers==2.5.1
tensorboardX==1.8
tensorflow  # for tensorboardX
spacy
</code></pre>
<p>Dockerfile:</p>
<pre><code>FROM ubuntu:18.04

MAINTAINER Loreto Parisi loretoparisi@gmail.com

########################################  BASE SYSTEM
# set noninteractive installation
ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update &amp;&amp; apt-get install -y apt-utils
RUN apt-get install -y --no-install-recommends \
    build-essential \
    pkg-config \
    tzdata \
    curl

######################################## PYTHON3
RUN apt-get install -y \
    python3 \
    python3-pip

# set local timezone
RUN ln -fs /usr/share/zoneinfo/America/New_York /etc/localtime &amp;&amp; \
    dpkg-reconfigure --frontend noninteractive tzdata

# transfer-learning-conv-ai
ENV PYTHONPATH /usr/local/lib/python3.6 
COPY . ./
COPY requirements.txt /tmp/requirements.txt
RUN pip3 install -r /tmp/requirements.txt

# model zoo
RUN mkdir models &amp;&amp; \
    curl https://s3.amazonaws.com/models.huggingface.co/transfer-learning-chatbot/finetuned_chatbot_gpt.tar.gz &gt; models/finetuned_chatbot_gpt.tar.gz &amp;&amp; \
    cd models/ &amp;&amp; \
    tar -xvzf finetuned_chatbot_gpt.tar.gz &amp;&amp; \
    rm finetuned_chatbot_gpt.tar.gz
    
CMD [&quot;bash&quot;]
</code></pre>
<p>Steps I ran so far:</p>
<pre><code>git clone https://github.com/huggingface/transfer-learning-conv-ai
cd transfer-learning-conv-ai
pip install -r requirements.txt
python -m spacy download en
docker build -t convai .
</code></pre>
","11210924","","11210924","","2021-03-12 16:28:56","2021-05-27 16:31:43","No module named 'numpy' during docker build","<python><docker><numpy><spacy><huggingface-transformers>","2","7","1","","","CC BY-SA 4.0"
"66625945","1","","","2021-03-14 14:56:26","","1","95","<p>I'm trying to pass the all of the huggingface's ...ForMaskedLM to the FitBert model for fill-in-the-blank task and see which pretrained yields the best result on the data I've prepared. But in the Reformer module I have this error says that I need to do 'config.is_decoder=False' but I don't really get what this means (This is my first time using huggingface). I tried to pass a ReformerConfig(is_decoder=False) to the model but still get the same error. How can I fix this?</p>
<p>My code:</p>
<pre><code>pretrained_weights = ['google/reformer-crime-and-punishment', 
                      'google/reformer-enwik8']
configurations = ReformerConfig(is_decoder=False)
for weight in pretrained_weights:
    print(weight)
    model = ReformerForMaskedLM(configurations).from_pretrained(weight)
    tokenizer = ReformerTokenizer.from_pretrained(weight)
    fb = FitBert(model=model, tokenizer=tokenizer)
    predicts = []
    for _, row in df.iterrows():
        predicts.append(fb.rank(row['question'], options=[row['1'], row['2'], row['3'], row['4']])[0])
    print(weight,':', np.sum(df.anwser==predicts) / df.shape[0])
</code></pre>
<p>Error:</p>
<pre><code>AssertionError                            Traceback (most recent call last)
&lt;ipython-input-5-a6016e0015ba&gt; in &lt;module&gt;()
      4 for weight in pretrained_weights:
      5     print(weight)
----&gt; 6     model = ReformerForMaskedLM(configurations).from_pretrained(weight)
      7     tokenizer = ReformerTokenizer.from_pretrained(weight)
      8     fb = FitBert(model=model, tokenizer=tokenizer)
/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1032 
   1033         # Instantiate model.
-&gt; 1034         model = cls(config, *model_args, **model_kwargs)
   1035 
   1036         if state_dict is None and not from_tf:

/usr/local/lib/python3.7/dist-packages/transformers/models/reformer/modeling_reformer.py in __init__(self, config)
   2304         assert (
   2305             not config.is_decoder
-&gt; 2306         ), &quot;If you want to use `ReformerForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.&quot;
   2307         self.reformer = ReformerModel(config)
   2308         self.lm_head = ReformerOnlyLMHead(config)

AssertionError: If you want to use `ReformerForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
</code></pre>
","13845609","","13845609","","2021-03-15 08:36:05","2021-03-15 10:37:54","huggingface's ReformerForMaskedLM configuration issue","<nlp><bert-language-model><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66405793","1","","","2021-02-28 04:19:25","","0","43","<p>I'm trying to use Helsenki translation model (en-&gt;de and de-&gt;en) for back-translation implementation. However, when I tested it on VM for a short paragraph, it took me around 23 seconds to translate. I'm not sure if it's supposed to take that long or I'm doing something wrong here. Given a big dataset, how can I translate all of them. THank you!</p>
","14096307","","","","","2021-02-28 04:19:25","Slow inference for HelsinkiNLP translation model","<nlp><pytorch><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"66590981","1","","","2021-03-11 21:43:47","","4","7939","<p>I am working on a machine learning project on Google Colab, it seems recently there is an issue when trying to import packages from transformers. The error message says:</p>
<blockquote>
<p>ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler' (/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py)</p>
</blockquote>
<p>The code is simple as follow:</p>
<pre><code>!pip install transformers==3.5.1

from transformers import BertTokenizer
</code></pre>
<p>So far I've tried to install different versions of the transformers, and import some other packages, but it seems importing any package with:</p>
<pre><code>from transformers import *Package
</code></pre>
<p>is not working, and will result in the same error. I wonder if anyone is running into the same issue as well?
<a href=""https://i.stack.imgur.com/paBmu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/paBmu.png"" alt=""Screenshot of the error"" /></a></p>
","15108267","","","","","2021-08-19 13:05:33","Transformer: Error importing packages. ""ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler'""","<python><nlp><google-colaboratory><bert-language-model><huggingface-transformers>","3","0","1","","","CC BY-SA 4.0"
"66633813","1","66666648","","2021-03-15 07:22:13","","3","140","<p>I'm trying to figure out how sequence to sequence loss is calculated. I am using the huggingface transformers library in this case, but this might actually be relevant to other DL libraries.</p>
<p>So to get the required data we can do:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import EncoderDecoderModel, BertTokenizer
import torch
import torch.nn.functional as F
torch.manual_seed(42)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
MAX_LEN = 128
tokenize = lambda x: tokenizer(x, max_length=MAX_LEN, truncation=True, padding=True, return_tensors=&quot;pt&quot;)

model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints
input_seq = [&quot;Hello, my dog is cute&quot;, &quot;my cat cute&quot;]
output_seq = [&quot;Yes it is&quot;, &quot;ok&quot;]
input_tokens = tokenize(input_seq)
output_tokens = tokenize(output_seq)

outputs = model(
    input_ids=input_tokens[&quot;input_ids&quot;], 
    attention_mask=input_tokens[&quot;attention_mask&quot;],
    decoder_input_ids=output_tokens[&quot;input_ids&quot;], 
    decoder_attention_mask=output_tokens[&quot;attention_mask&quot;],
    labels=output_tokens[&quot;input_ids&quot;], 
    return_dict=True)

idx = output_tokens[&quot;input_ids&quot;]
logits = F.log_softmax(outputs[&quot;logits&quot;], dim=-1)
mask = output_tokens[&quot;attention_mask&quot;]
</code></pre>
<h2>Edit 1</h2>
<p>Thanks to @cronoik I was able to replicate the loss calculated by huggingface as being:</p>
<pre class=""lang-py prettyprint-override""><code>output_logits = logits[:,:-1,:]
output_mask = mask[:,:-1]
label_tokens = output_tokens[&quot;input_ids&quot;][:, 1:].unsqueeze(-1)
select_logits = torch.gather(output_logits, -1, label_tokens).squeeze()
huggingface_loss = -select_logits.mean()
</code></pre>
<p>However, since the last two tokens of the second input is just padding, shouldn't we calculate the loss to be:</p>
<pre class=""lang-py prettyprint-override""><code>seq_loss = (select_logits * output_mask).sum(dim=-1, keepdims=True) / output_mask.sum(dim=-1, keepdims=True)
seq_loss = -seq_loss.mean()
</code></pre>
<p>^This takes into account the length of the sequence of each row of outputs, and the padding by masking it out. Think this is especially useful when we have batches of varying length outputs.</p>
","2530674","","2530674","","2021-03-16 00:28:55","2021-03-17 03:35:49","Sequence to Sequence Loss","<deep-learning><pytorch><huggingface-transformers>","1","1","1","","","CC BY-SA 4.0"
"66589879","1","","","2021-03-11 20:13:20","","0","45","<p>I am using Raytune with HuggingFace for hyperparameter tuning, following is my code snippet:</p>
<pre><code>from ray.tune.schedulers import PopulationBasedTraining
from ray.tune import uniform
from random import randint
scheduler = PopulationBasedTraining(
    mode = &quot;max&quot;,
    metric='mean_accuracy',
    perturbation_interval=2,
    hyperparam_mutations={
        &quot;weight_decay&quot;: lambda: uniform(0.0, 0.3),
        &quot;learning_rate&quot;: lambda: uniform(1e-5, 5e-5),
        &quot;per_gpu_train_batch_size&quot;: [16, 32, 64],
        &quot;num_train_epochs&quot;: [2,3,4],
        &quot;warmup_steps&quot;:lambda: randint(0, 500)
    }
)

best_trial = trainer.hyperparameter_search(
    direction=&quot;maximize&quot;,
    backend=&quot;ray&quot;,
    n_trials=4,
    keep_checkpoints_num=1,
    scheduler=scheduler)
</code></pre>
<p>But, I dont understand it gives me error as:</p>
<pre><code>  [TuneError: ('Trials did not complete', \[_inner_53895_00000, _inner_53895_00001, _inner_53895_00002, _inner_53895_00003\])][1]
</code></pre>
<p>Output: [1]: <a href=""https://i.stack.imgur.com/1zmM7.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/1zmM7.png</a></p>
","15370087","","","","","2021-03-11 20:13:20","ray.exceptions.RayTaskError(TuneError) HuggingFace+RayTune","<python><machine-learning><huggingface-transformers><ray-tune>","0","0","","","","CC BY-SA 4.0"
"66625389","1","66626813","","2021-03-14 14:00:28","","0","416","<p>I am trying to use Huggingface to transform stuff from English to Hindi.
This is the code snippet</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-hi&quot;)

model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-hi&quot;)
text = &quot;Hello my friends! How are you doing today?&quot;
tokenized_text = tokenizer.prepare_seq2seq_batch([text])

# Perform translation and decode the output
translation = model.generate(**tokenized_text)
translated_text = tokenizer.batch_decode(translation, skip_special_tokens=True)[0]

# Print translated text
print(translated_text)
</code></pre>
<p>I am getting this error while trying to call the method generate on 'model'.</p>
<blockquote>
<p>AttributeError: 'list' object has no attribute 'size'.</p>
</blockquote>
<p>I am running on transformer version 4.3.3.</p>
","6786996","","4420967","","2021-03-14 16:18:07","2021-03-14 16:21:50","AttributeError: 'list' object has no attribute 'size' Hugging-Face transformers","<python-3.x><nlp><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66632085","1","","","2021-03-15 03:41:15","","0","188","<p>I am trying to build a model from tweets for sentiment analysis. I am using the HuggingFace Feature Extraction Pipeline to extract features for feeding into another model. However, when I try to extract features from a list of text with a pipeline, my CoLab instance (The default GPU runtime) crashes due to lack of RAM.</p>
<p>For reference, here's some snippets from my code. I'm using the <a href=""https://huggingface.co/transformers/main_classes/pipelines.html#transformers.FeatureExtractionPipeline"" rel=""nofollow noreferrer"">Feature Extraction Pipeline</a> from HuggingFace as is.</p>
<pre><code>#Feature extraction pipeline
import numpy as np
from transformers import AutoTokenizer, AutoModel, pipeline


model = AutoModel.from_pretrained('distilbert-base-uncased')
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', add_special_tokens = 'true', padding = 'longest')
nlp = pipeline('feature-extraction', model=model, tokenizer=tokenizer, device = 0)
</code></pre>
<pre><code>#Generator function
def get_tweets(file_name):
    with open(file_name, &quot;r&quot;, encoding=&quot;utf&quot;) as tweet_records:
        for tweet_record in csv.reader(tweet_records):
            tweet_label = tweet_record[3]
            tweet_record = tweet_record[2]#remove first two features
            #tweet_record = #preprocessing step/s here

            tweet_record = nlp(tweet_record) #get extracted features. 
            #nlp(features) is used to extract features using the pipeline
            yield tweet_record, tweet_label
</code></pre>
<pre><code>#Testing
filename = '/content/drive/My Drive/Twitter Files/combined-0-1 (2).csv'
store_list = list(iter(get_tweets(filename)))
next(iter_tweet)  # Skipping the column names

</code></pre>
<p>This still crashes due to lacking RAM, even when a generator has been implemented. There are 236K rows within the CSV. Any idea how to fix this?</p>
","15397202","","15397202","","2021-03-16 11:05:16","2021-03-16 11:05:16","Generator for extracting features from text using HuggingFace","<python><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"66555455","1","","","2021-03-09 22:04:43","","3","476","<p>I'm trying to fine-tune a pre-trained BERT model from Huggingface using Tensorflow. Everything runs smoothly and the model builds and trains without error. But when I try to save the model it stops with the error &quot;IndexError: list index out of range&quot;. I'm using Google Colab with TPU.</p>
<p>Any help would be much appreciated!</p>
<p>Code:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import activations, optimizers, losses
from transformers import TFBertModel

def create_model(max_sequence, model_name, num_labels):
    bert_model = TFBertModel.from_pretrained(model_name)
    input_ids = tf.keras.layers.Input(shape=(max_sequence,), dtype=tf.int32, name='input_ids')
    attention_mask = tf.keras.layers.Input((max_sequence,), dtype=tf.int32, name='attention_mask')
    output = bert_model([input_ids, attention_mask])[0]
    output = output[:, 0, :]
    output = tf.keras.layers.Dense(num_labels, activation='sigmoid')(output)
    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)
    return model

with strategy.scope():
  model = create_model(20, 'bert-base-uncased', 1)
  opt = optimizers.Adam(learning_rate=3e-5)
  loss = 'binary_crossentropy'
  model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])

model.fit(tfdataset_train, batch_size=32, epochs=2)
SAVE_PATH = 'path/to/save/location'
model.save(SAVE_PATH)
</code></pre>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-22-255116b49022&gt; in &lt;module&gt;()
      1 SAVE_PATH = 'path/to/save/location'
----&gt; 2 model.save(SAVE_PATH,save_format='tf')

50 frames
/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py in input_processing(func, config, input_ids, **kwargs)
    372                     output[tensor_name] = input
    373                 else:
--&gt; 374                     output[parameter_names[i]] = input
    375             elif isinstance(input, allowed_types) or input is None:
    376                 output[parameter_names[i]] = input

IndexError: list index out of range
</code></pre>
<p>Model plotted with shapes:
<a href=""https://i.stack.imgur.com/jZ5Hj.png"" rel=""nofollow noreferrer"">Tensorflow Model</a></p>
","15364164","","","","","2021-08-04 09:14:08","List index out of range when saving finetuned Tensorflow model","<python><tensorflow><google-colaboratory><bert-language-model><huggingface-transformers>","3","1","","","","CC BY-SA 4.0"
"66585016","1","","","2021-03-11 14:56:10","","0","245","<p>Recently I've stumbled upon an issue that I'm unable to resolve myself.</p>
<p>I've been reproducing examples for using text classification scripts:
<a href=""https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py</a>
<a href=""https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_tf_glue.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_tf_glue.py</a></p>
<p>Both PyTorch and Tensorflow version of them.</p>
<p>I've conducted few tests and one of its results are unclear to me.
These tests were including mostly switching between both ML framework scripts (PyTorch/Tensorflow) and using different models with respective datasets.</p>
<p>I was trying to fine-tune base language models like <code>bert-base-uncased</code> (uploaded via <a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">https://huggingface.co/models</a>) with my custom CSV dataset to perform well at sequence classification task.</p>
<p>Most of these tests were entirely succesful, and I was able to verify models inference and predictions.</p>
<p>The issue began when I've tried to use different model with PyTorch framework script.<br></p>
<p>Maybe it is worth to add that same model worked flawlessly on Tensorflow fine-tuning script. Errors occur only when I'm fine-tuning with PyTorch script.</p>
<p>This model's config is as below:</p>
<pre><code>{
  &quot;architectures&quot;: [
    &quot;BertForPreTraining&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;output_past&quot;: true,
  &quot;pad_token_id&quot;: 0,
  &quot;type_vocab_size&quot;: 2,
  &quot;vocab_size&quot;: 60000
}

</code></pre>
<p>I kept receiving errors like:<br></p>
<pre><code>RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling cublasCreate(handle)
</code></pre>
<pre><code>  File &quot;/home/konrad/miniconda3/envs/test_1_3/lib/python3.8/site-packages/torch/nn/functional.py&quot;, line 2387, in nll_loss
    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
RuntimeError: cuda runtime error (710) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1614378083779/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:115

</code></pre>
<p>Which eventually led me to investigate the sizes of input and target tensors.</p>
<p>I've found out that input tensor size was not inheriting the count of labels.</p>
<p>It was:</p>
<pre><code>torch.Size([16, 2])
</code></pre>
<p>while it should be: (77 unique labels in dataset)</p>
<pre><code>torch.Size([16, 77])
</code></pre>
<p>I'm pretty sure it should be like that, because other successful test's input tensor sizes were correct.</p>
<p>So my question emerged.<br>
What kind of methods/parameters one may use/set to ensure that correct input tensor size will be adjusted accordingly to the number of unique labels in my dataset?</p>
<p>May the BERT model transformers architecture be relevant?
Because when I was using models of architecture like BertModel or BertForMaskedLM there were no issues like that, everything was going smoothly.</p>
<p>That experimental model that caused errors was of different architecture - BertForPretraining.</p>
<p>The same questions I'd like to ask You.
I would appreciate any tips and further insights that could help me find a solution.</p>
<p>EDIT: Added code below.</p>
<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python
# coding=utf-8
# Copyright 2020 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
&quot;&quot;&quot; Finetuning the library models for sequence classification on GLUE.&quot;&quot;&quot;
# You can also adapt this script on your own text classification task. Pointers for this are left as comments.

import logging
import os
import random
import sys
from dataclasses import dataclass, field
from typing import Optional

import numpy as np
from datasets import load_dataset, load_metric

import transformers
from transformers import (
    AutoConfig,
    BertForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    EvalPrediction,
    HfArgumentParser,
    PretrainedConfig,
    Trainer,
    TrainingArguments,
    default_data_collator,
    set_seed,
)
from transformers.trainer_utils import get_last_checkpoint, is_main_process




task_to_keys = {
    &quot;cola&quot;: (&quot;sentence&quot;, None),
    &quot;mnli&quot;: (&quot;premise&quot;, &quot;hypothesis&quot;),
    &quot;mrpc&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;),
    &quot;qnli&quot;: (&quot;question&quot;, &quot;sentence&quot;),
    &quot;qqp&quot;: (&quot;question1&quot;, &quot;question2&quot;),
    &quot;rte&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;),
    &quot;sst2&quot;: (&quot;sentence&quot;, None),
    &quot;stsb&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;),
    &quot;wnli&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;),
}

logger = logging.getLogger(__name__)


@dataclass
class DataTrainingArguments:
    &quot;&quot;&quot;
    Arguments pertaining to what data we are going to input our model for training and eval.
    Using `HfArgumentParser` we can turn this class
    into argparse arguments to be able to specify them on
    the command line.
    &quot;&quot;&quot;

    task_name: Optional[str] = field(
        default=None,
        metadata={&quot;help&quot;: &quot;The name of the task to train on: &quot; + &quot;, &quot;.join(task_to_keys.keys())},
    )
    max_seq_length: int = field(
        default=128,
        metadata={
            &quot;help&quot;: &quot;The maximum total input sequence length after tokenization. Sequences longer &quot;
            &quot;than this will be truncated, sequences shorter will be padded.&quot;
        },
    )
    overwrite_cache: bool = field(
        default=False, metadata={&quot;help&quot;: &quot;Overwrite the cached preprocessed datasets or not.&quot;}
    )
    pad_to_max_length: bool = field(
        default=True,
        metadata={
            &quot;help&quot;: &quot;Whether to pad all samples to `max_seq_length`. &quot;
            &quot;If False, will pad the samples dynamically when batching to the maximum length in the batch.&quot;
        },
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            &quot;help&quot;: &quot;For debugging purposes or quicker training, truncate the number of training examples to this &quot;
            &quot;value if set.&quot;
        },
    )
    max_val_samples: Optional[int] = field(
        default=None,
        metadata={
            &quot;help&quot;: &quot;For debugging purposes or quicker training, truncate the number of validation examples to this &quot;
            &quot;value if set.&quot;
        },
    )
    max_test_samples: Optional[int] = field(
        default=None,
        metadata={
            &quot;help&quot;: &quot;For debugging purposes or quicker training, truncate the number of test examples to this &quot;
            &quot;value if set.&quot;
        },
    )
    train_file: Optional[str] = field(
        default=None, metadata={&quot;help&quot;: &quot;A csv or a json file containing the training data.&quot;}
    )
    validation_file: Optional[str] = field(
        default=None, metadata={&quot;help&quot;: &quot;A csv or a json file containing the validation data.&quot;}
    )
    test_file: Optional[str] = field(default=None, metadata={&quot;help&quot;: &quot;A csv or a json file containing the test data.&quot;})

    def __post_init__(self):
        if self.task_name is not None:
            self.task_name = self.task_name.lower()
            if self.task_name not in task_to_keys.keys():
                raise ValueError(&quot;Unknown task, you should pick one in &quot; + &quot;,&quot;.join(task_to_keys.keys()))
        elif self.train_file is None or self.validation_file is None:
            raise ValueError(&quot;Need either a GLUE task or a training/validation file.&quot;)
        else:
            train_extension = self.train_file.split(&quot;.&quot;)[-1]
            assert train_extension in [&quot;csv&quot;, &quot;json&quot;], &quot;`train_file` should be a csv or a json file.&quot;
            validation_extension = self.validation_file.split(&quot;.&quot;)[-1]
            assert (
                validation_extension == train_extension
            ), &quot;`validation_file` should have the same extension (csv or json) as `train_file`.&quot;


@dataclass
class ModelArguments:
    &quot;&quot;&quot;
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    &quot;&quot;&quot;

    model_name_or_path: str = field(
        metadata={&quot;help&quot;: &quot;Path to pretrained model or model identifier from huggingface.co/models&quot;}
    )
    config_name: Optional[str] = field(
        default=None, metadata={&quot;help&quot;: &quot;Pretrained config name or path if not the same as model_name&quot;}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={&quot;help&quot;: &quot;Pretrained tokenizer name or path if not the same as model_name&quot;}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={&quot;help&quot;: &quot;Where do you want to store the pretrained models downloaded from huggingface.co&quot;},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={&quot;help&quot;: &quot;Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.&quot;},
    )
    model_revision: str = field(
        default=&quot;main&quot;,
        metadata={&quot;help&quot;: &quot;The specific model version to use (can be a branch name, tag name or commit id).&quot;},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            &quot;help&quot;: &quot;Will use the token generated when running `transformers-cli login` (necessary to use this script &quot;
            &quot;with private models).&quot;
        },
    )


def main():
    # See all possible arguments in src/transformers/training_args.py
    # or by passing the --help flag to this script.
    # We now keep distinct sets of args, for a cleaner separation of concerns.

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(&quot;.json&quot;):
        # If we pass only one argument to the script and it's the path to a json file,
        # let's parse it to get our arguments.
        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # Detecting last checkpoint.
    last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) &gt; 0:
            raise ValueError(
                f&quot;Output directory ({training_args.output_dir}) already exists and is not empty. &quot;
                &quot;Use --overwrite_output_dir to overcome.&quot;
            )
        elif last_checkpoint is not None:
            logger.info(
                f&quot;Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change &quot;
                &quot;the `--output_dir` or add `--overwrite_output_dir` to train from scratch.&quot;
            )

    # Setup logging
    logging.basicConfig(
        format=&quot;%(asctime)s - %(levelname)s - %(name)s -   %(message)s&quot;,
        datefmt=&quot;%m/%d/%Y %H:%M:%S&quot;,
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)

    # Log on each process the small summary:
    logger.warning(
        f&quot;Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}&quot;
        + f&quot;distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}&quot;
    )
    # Set the verbosity to info of the Transformers logger (on main process only):
    if is_main_process(training_args.local_rank):
        transformers.utils.logging.set_verbosity_info()
        transformers.utils.logging.enable_default_handler()
        transformers.utils.logging.enable_explicit_format()
    logger.info(f&quot;Training/evaluation parameters {training_args}&quot;)

    # Set seed before initializing model.
    set_seed(training_args.seed)

    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)
    # or specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).
    #
    # For CSV/JSON files, this script will use as labels the column called 'label' and as pair of sentences the
    # sentences in columns called 'sentence1' and 'sentence2' if such column exists or the first two columns not named
    # label if at least two columns are provided.
    #
    # If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this
    # single column. You can easily tweak this behavior (see below)
    #
    # In distributed training, the load_dataset function guarantee that only one local process can concurrently
    # download the dataset.
    if data_args.task_name is not None:
        # Downloading and loading a dataset from the hub.
        datasets = load_dataset(&quot;glue&quot;, data_args.task_name)
    else:
        # Loading a dataset from your local files.
        # CSV/JSON training and evaluation files are needed.
        data_files = {&quot;train&quot;: data_args.train_file, &quot;validation&quot;: data_args.validation_file}

        # Get the test dataset: you can provide your own CSV/JSON test file (see below)
        # when you use `do_predict` without specifying a GLUE benchmark task.
        if training_args.do_predict:
            if data_args.test_file is not None:
                train_extension = data_args.train_file.split(&quot;.&quot;)[-1]
                test_extension = data_args.test_file.split(&quot;.&quot;)[-1]
                assert (
                    test_extension == train_extension
                ), &quot;`test_file` should have the same extension (csv or json) as `train_file`.&quot;
                data_files[&quot;test&quot;] = data_args.test_file
            else:
                raise ValueError(&quot;Need either a GLUE task or a test file for `do_predict`.&quot;)

        for key in data_files.keys():
            logger.info(f&quot;load a local file for {key}: {data_files[key]}&quot;)

        if data_args.train_file.endswith(&quot;.csv&quot;):
            # Loading a dataset from local csv files
            datasets = load_dataset(&quot;csv&quot;, data_files=data_files)
        else:
            # Loading a dataset from local json files
            datasets = load_dataset(&quot;json&quot;, data_files=data_files)
    # See more about loading any type of standard or custom dataset at
    # https://huggingface.co/docs/datasets/loading_datasets.html.

    # Labels
    if data_args.task_name is not None:
        is_regression = data_args.task_name == &quot;stsb&quot;
        if not is_regression:
            label_list = datasets[&quot;train&quot;].features[&quot;label&quot;].names
            num_labels = len(label_list)
        else:
            num_labels = 1
    else:
        # Trying to have good defaults here, don't hesitate to tweak to your needs.
        is_regression = datasets[&quot;train&quot;].features[&quot;label&quot;].dtype in [&quot;float32&quot;, &quot;float64&quot;]
        if is_regression:
            num_labels = 1
        else:
            # A useful fast method:
            # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique
            label_list = datasets[&quot;train&quot;].unique(&quot;label&quot;)
            label_list.sort()  # Let's sort it for determinism
            num_labels = len(label_list)

    print(&quot;\n\n&quot;)
    print(&quot;label_list&quot;, label_list[:10])
    print(&quot;\n&quot;, len(label_list))

    ############################################################
    # Crucial addition
    label2id = {label: i for i, label in enumerate(label_list)}


    # output sorted labels to json file
    import json

    sorted_labels_dict = {k: v for k, v in sorted(label2id.items(), key=lambda item: item[1])}
    with open(&quot;sorted_labels.json&quot;, &quot;w&quot;) as outfile:
        json.dump(sorted_labels_dict, outfile)

    ############################################################

    # Load pretrained model and tokenizer
    #
    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently
    # download model &amp; vocab.
    config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        num_labels=num_labels,

        ############################################################
        # Crucial addition
        label2id=label2id,
        id2label={id: label for label, id in label2id.items()},
        ############################################################

        finetuning_task=data_args.task_name,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    config.num_labels = 77
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
        cache_dir=model_args.cache_dir,
        use_fast=model_args.use_fast_tokenizer,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    model = BertForSequenceClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(&quot;.ckpt&quot; in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    print(&quot;\n\n\n\n&quot;, model.parameters, &quot;\n\n\n\n&quot;)


    # Preprocessing the datasets
    if data_args.task_name is not None:
        sentence1_key, sentence2_key = task_to_keys[data_args.task_name]
    else:
        # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.
        non_label_column_names = [name for name in datasets[&quot;train&quot;].column_names if name != &quot;label&quot;]
        if &quot;sentence1&quot; in non_label_column_names and &quot;sentence2&quot; in non_label_column_names:
            sentence1_key, sentence2_key = &quot;sentence1&quot;, &quot;sentence2&quot;
        else:
            if len(non_label_column_names) &gt;= 2:
                sentence1_key, sentence2_key = non_label_column_names[:2]
            else:
                sentence1_key, sentence2_key = non_label_column_names[0], None


    # Padding strategy
    if data_args.pad_to_max_length:
        padding = &quot;max_length&quot;
    else:
        # We will pad later, dynamically at batch creation, to the max sequence length in each batch
        padding = False

    # Some models have set the order of the labels to use, so let's make sure we do use it.
    label_to_id = None
    if (
        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id
        and data_args.task_name is not None
        and not is_regression
    ):
        # Some have all caps in their config, some don't.
        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}



        if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):
            label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}
        else:
            logger.warn(
                &quot;Your model seems to have been trained with labels, but they don't match the dataset: &quot;,
                f&quot;model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.&quot;
                &quot;\nIgnoring the model labels as a result.&quot;,
            )
    elif data_args.task_name is None and not is_regression:
        label_to_id = {v: i for i, v in enumerate(label_list)}


    if data_args.max_seq_length &gt; tokenizer.model_max_length:
        logger.warn(
            f&quot;The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the&quot;
            f&quot;model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.&quot;
        )
    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

    def preprocess_function(examples):
        # Tokenize the texts
        args = (
            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])
        )
        result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)

        # Map labels to IDs (not necessary for GLUE tasks)
        if label_to_id is not None and &quot;label&quot; in examples:
            result[&quot;label&quot;] = [(label_to_id[l] if l != -1 else -1) for l in examples[&quot;label&quot;]]
        return result


    if training_args.do_train:
        if &quot;train&quot; not in datasets:
            raise ValueError(&quot;--do_train requires a train dataset&quot;)
        train_dataset = datasets[&quot;train&quot;]
        if data_args.max_train_samples is not None:
            train_dataset = train_dataset.select(range(data_args.max_train_samples))
        train_dataset = train_dataset.map(
            preprocess_function,
            batched=True,
            load_from_cache_file=not data_args.overwrite_cache,
        )

    if training_args.do_eval:
        if &quot;validation&quot; not in datasets and &quot;validation_matched&quot; not in datasets:
            raise ValueError(&quot;--do_eval requires a validation dataset&quot;)
        eval_dataset = datasets[&quot;validation_matched&quot; if data_args.task_name == &quot;mnli&quot; else &quot;validation&quot;]
        if data_args.max_val_samples is not None:
            eval_dataset = eval_dataset.select(range(data_args.max_val_samples))
        eval_dataset = eval_dataset.map(
            preprocess_function,
            batched=True,
            load_from_cache_file=not data_args.overwrite_cache,
        )

    if training_args.do_predict or data_args.task_name is not None or data_args.test_file is not None:
        if &quot;test&quot; not in datasets and &quot;test_matched&quot; not in datasets:
            raise ValueError(&quot;--do_predict requires a test dataset&quot;)
        test_dataset = datasets[&quot;test_matched&quot; if data_args.task_name == &quot;mnli&quot; else &quot;test&quot;]
        if data_args.max_test_samples is not None:
            test_dataset = test_dataset.select(range(data_args.max_test_samples))
        test_dataset = test_dataset.map(
            preprocess_function,
            batched=True,
            load_from_cache_file=not data_args.overwrite_cache,
        )

    # Log a few random samples from the training set:
    for index in random.sample(range(len(train_dataset)), 3):
        logger.info(f&quot;Sample {index} of the training set: {train_dataset[index]}.&quot;)

    # Get the metric function
    if data_args.task_name is not None:
        metric = load_metric(&quot;glue&quot;, data_args.task_name)
    # TODO: When datasets metrics include regular accuracy, make an else here and remove special branch from
    # compute_metrics

    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a
    # predictions and label_ids field) and has to return a dictionary string to float.
    def compute_metrics(p: EvalPrediction):
        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
        preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)
        if data_args.task_name is not None:
            result = metric.compute(predictions=preds, references=p.label_ids)
            if len(result) &gt; 1:
                result[&quot;combined_score&quot;] = np.mean(list(result.values())).item()
            return result
        elif is_regression:
            return {&quot;mse&quot;: ((preds - p.label_ids) ** 2).mean().item()}
        else:
            return {&quot;accuracy&quot;: (preds == p.label_ids).astype(np.float32).mean().item()}

    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.
    if data_args.pad_to_max_length:
        data_collator = default_data_collator
    elif training_args.fp16:
        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)
    else:
        data_collator = None

    # Initialize our Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        compute_metrics=compute_metrics,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    config.num_labels = 77


    print(&quot;\n\n\n AutoConfig.from_pretrained(model_args.model_name_or_path).num_label\n&quot;, AutoConfig.from_pretrained(model_args.model_name_or_path).num_labels)

    if training_args.do_train:
        if last_checkpoint is not None:
            model_path = last_checkpoint
        elif os.path.isdir(model_args.model_name_or_path):
            model_path = model_args.model_name_or_path
        else:
            model_path = None

        config.num_labels = 77
        train_result = trainer.train(model_path=model_path)
        metrics = train_result.metrics

        trainer.save_model()  # Saves the tokenizer too for easy upload

        output_train_file = os.path.join(training_args.output_dir, &quot;train_results.txt&quot;)
        if trainer.is_world_process_zero():
            with open(output_train_file, &quot;w&quot;) as writer:
                logger.info(&quot;***** Train results *****&quot;)
                for key, value in sorted(metrics.items()):
                    logger.info(f&quot;  {key} = {value}&quot;)
                    writer.write(f&quot;{key} = {value}\n&quot;)

            # Need to save the state, since Trainer.save_model saves only the tokenizer with the model
            trainer.state.save_to_json(os.path.join(training_args.output_dir, &quot;trainer_state.json&quot;))


    eval_results = {}
    if training_args.do_eval:
        logger.info(&quot;*** Evaluate ***&quot;)

        # Loop to handle MNLI double evaluation (matched, mis-matched)
        tasks = [data_args.task_name]
        eval_datasets = [eval_dataset]
        if data_args.task_name == &quot;mnli&quot;:
            tasks.append(&quot;mnli-mm&quot;)
            eval_datasets.append(datasets[&quot;validation_mismatched&quot;])

        for eval_dataset, task in zip(eval_datasets, tasks):
            eval_result = trainer.evaluate(eval_dataset=eval_dataset)

            output_eval_file = os.path.join(training_args.output_dir, f&quot;eval_results_{task}.txt&quot;)
            if trainer.is_world_process_zero():
                with open(output_eval_file, &quot;w&quot;) as writer:
                    logger.info(f&quot;***** Eval results {task} *****&quot;)
                    for key, value in sorted(eval_result.items()):
                        logger.info(f&quot;  {key} = {value}&quot;)
                        writer.write(f&quot;{key} = {value}\n&quot;)

            eval_results.update(eval_result)


    if training_args.do_predict:
        logger.info(&quot;*** Test ***&quot;)

        # Loop to handle MNLI double evaluation (matched, mis-matched)
        tasks = [data_args.task_name]
        test_datasets = [test_dataset]
        if data_args.task_name == &quot;mnli&quot;:
            tasks.append(&quot;mnli-mm&quot;)
            test_datasets.append(datasets[&quot;test_mismatched&quot;])

        for test_dataset, task in zip(test_datasets, tasks):
            # Removing the `label` columns because it contains -1 and Trainer won't like that.
            test_dataset.remove_columns_(&quot;label&quot;)
            predictions = trainer.predict(test_dataset=test_dataset).predictions
            predictions = np.squeeze(predictions) if is_regression else np.argmax(predictions, axis=1)

            output_test_file = os.path.join(training_args.output_dir, f&quot;test_results_{task}.txt&quot;)
            if trainer.is_world_process_zero():
                with open(output_test_file, &quot;w&quot;) as writer:
                    logger.info(f&quot;***** Test results {task} *****&quot;)
                    writer.write(&quot;index\tprediction\n&quot;)
                    for index, item in enumerate(predictions):
                        if is_regression:
                            writer.write(f&quot;{index}\t{item:3.3f}\n&quot;)
                        else:
                            item = label_list[item]
                            writer.write(f&quot;{index}\t{item}\n&quot;)


def _mp_fn(index):
    # For xla_spawn (TPUs)
    main()


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
","14661154","","14661154","","2021-03-15 08:16:00","2021-05-27 12:28:34","Input tensor size doesnt inherit training dataset labels count","<pytorch><tensorflow2.0><huggingface-transformers>","1","10","","","","CC BY-SA 4.0"
"66603536","1","","","2021-03-12 16:20:29","","0","218","<p>Recently, I have started working on using docker images. I want to deploy PyTorch based text classification model which requires GPU to run on. When the docker image is called upon, then it's not able to detect GPU in the VM. Hence, my code is failing by throwing <code>no Cuda device found error</code>.</p>
<p>This is my base image <code>FROM gcr.io/deeplearning-platform-release/pytorch-gpu.1-0</code>. I don't know what all steps to follow to install Nvidia drivers in the docker. Please help me out.</p>
","6315469","","","","","2021-03-12 16:23:12","GPU in docker container for deep learning task","<python><docker><pytorch><huggingface-transformers>","1","0","","","","CC BY-SA 4.0"
"66639722","1","","","2021-03-15 14:19:50","","0","178","<p>I am trying to summarize the input text using Bart's pretrained summarization pipeline. However, I am noticing that the generated summary is exactly the same as the text that I am feeding the model to summarize upon. I also tried fine-tuning the model on the text-summary pairs(human-generated summaries), but for new input texts as well, the same input texts are being generated as the output.</p>
<p>I intend my summary to be a gist of the given input text. What methods can I use to resolve this? And, are there any other models which might perform better at generating summaries?</p>
","15400884","","","","","2021-04-29 12:52:47","Why does HuggingFace's Bart Summarizer replicate the given input text?","<python><deep-learning><nlp><huggingface-transformers><summarization>","1","1","","","","CC BY-SA 4.0"
"66478495","1","","","2021-03-04 15:50:42","","0","86","<p>I've downloaded bert pretrained model 'bert-base-cased. I'm unable to load the model with help of BertTokenizer. I'm trying for bert tokenizer. In the bert-pretrained-model folder I have config.json and pytorch_model.bin.</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained(r'C:\Downloads\bert-pretrained-model')
</code></pre>
<p>I'm facing error like</p>
<pre><code>OSError                                   Traceback (most recent call last)
&lt;ipython-input-17-bd4c0051c48e&gt; in &lt;module&gt;
----&gt; 1 tokenizer = BertTokenizer.from_pretrained(r'\Downloads\bert-pretrained-model')

~\sentiment_analysis\lib\site-packages\transformers\tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1775                 f&quot;- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\n\n&quot;
   1776             )
-&gt; 1777             raise EnvironmentError(msg)
   1778 
   1779         for file_id, file_path in vocab_files.items():

OSError: Can't load tokenizer for 'C:\Downloads\bert-pretrained-model'. Make sure that:

- 'C:\Downloads\bert-pretrained-model' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'C:\Downloads\bert-pretrained-model' is the correct path to a directory containing relevant tokenizer files
</code></pre>
<p><strong>When I'm trying load with BertModel, it's loading. But when i'm trying with BertTokenizer it's not loading.</strong></p>
","15224778","","","","","2021-03-05 15:25:10","I've downloaded bert pretrained model 'bert-base-cased'. I'm unable to load the model with help of BertTokenizer","<nlp><pytorch><bert-language-model><huggingface-transformers><huggingface-tokenizers>","1","7","","","","CC BY-SA 4.0"
"66516359","1","","","2021-03-07 12:09:25","","4","106","<p><strong>The goal is to run <code>python -m spacy train</code> with FP16 mixed precision</strong> to enable the use of large transformers (<code>roberta-large</code>, <code>albert-large</code>, etc.) in limited VRAM (RTX 2080ti 11 GB).</p>
<p>The new Spacy3 <a href=""https://spacy.io/usage/projects#project-yml"" rel=""nofollow noreferrer"">project.yml approach</a> to training directly uses <a href=""https://huggingface.co/models?filter=pytorch"" rel=""nofollow noreferrer"">Huggingface-transformers models</a> loaded via <a href=""https://github.com/explosion/spacy-transformers"" rel=""nofollow noreferrer"">Spacy-transformers v1.0</a>. Huggingface models can be run with mixed precision just by adding the <code>--fp16</code> flag (<a href=""https://huggingface.co/transformers/examples.html#distributed-training-and-mixed-precision"" rel=""nofollow noreferrer"">as described here</a>).</p>
<p>The spacy config was generated using <code>python -m spacy init config --lang en --pipeline ner --optimize efficiency --gpu -F default.cfg</code>, and checked to be complete by <code>python -m spacy init fill-config default.cfg config.cfg --diff</code>. Yet no FP16 / mixed-precision is to be found.</p>
<h3>To reproduce</h3>
<p>Use the <a href=""https://github.com/explosion/projects/tree/v3/pipelines/ner_wikiner"" rel=""nofollow noreferrer"">spaCy Project: Named Entity Recognition (WikiNER)</a> with changed <code>init-config</code> in <code>project.yml</code> to use a GPU and a transformer (<code>roberta-base</code> by default):</p>
<pre class=""lang-yaml prettyprint-override""><code>commands:
  -
    name: init-config
    help: &quot;Generate a transformer English NER config&quot;
    script:
      - &quot;python -m spacy init config --lang en --pipeline ner --gpu -F --optimize efficiency -C configs/${vars.config}.cfg&quot;
</code></pre>
<h3>What was tested</h3>
<ul>
<li>Added <code>--fp16</code> to <code>python -m spacy project run</code></li>
<li>Added <code>--fp16</code> to <code>python -m spacy train</code></li>
<li>Added <code>fp16 = true</code> to <code>default.cfg</code> in various sections (<code>[components.transformer], [components.transformer.model], [training], [initialize]</code>)</li>
</ul>
<p>The logic was <code>transformers</code> are run in FP16 as such:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import TrainingArguments
TrainingArguments(..., fp16=True, ...)
</code></pre>
<h3>SW stack specifics</h3>
<pre class=""lang-sh prettyprint-override""><code> - spacy              3.0.3
 - spacy-transformers 1.0.1
 - transformers       4.2.2
 - torch              1.6.0+cu101
</code></pre>
","6253183","","6253183","","2021-03-12 17:35:14","2021-03-12 17:35:14","How to train Spacy3 project with FP16 mixed precision","<nlp><pytorch><spacy><huggingface-transformers><spacy-3>","0","0","1","","","CC BY-SA 4.0"
"66442648","1","","","2021-03-02 15:52:08","","3","380","<p>I am using Huggingface to further train a BERT model. I saved the model using two methods: step (1) Saving the entire model using this code: <code>model.save_pretrained(save_location)</code>, and step (2) save the state_dict of the model using this code: <code>torch.save(model.state_dict(),'model.pth')</code>
However, when I try to load this pretrained BERT model using the following code <code>bert_mask_lm = BertForMaskedLM.from_pretrained('save_location')</code> for step (1) and <code>torch.load('model.pth')</code> for step (2), I am getting this following error in both steps:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/torch/serialization.py in _check_seekable(f)
    307     try:
--&gt; 308         f.seek(f.tell())
    309         return True

AttributeError: 'torch._C.PyTorchFileReader' object has no attribute 'seek'

During handling of the above exception, another exception occurred:
</code></pre>
<p>Detailed stacktrace of step (1) is as follows:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/torch/serialization.py in _check_seekable(f)
    307     try:
--&gt; 308         f.seek(f.tell())
    309         return True

AttributeError: 'torch._C.PyTorchFileReader' object has no attribute 'seek'

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1037             try:
-&gt; 1038                 state_dict = torch.load(resolved_archive_file, map_location=&quot;cpu&quot;)
   1039             except Exception:

~/anaconda3/lib/python3.6/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)
    593                     return torch.jit.load(opened_file)
--&gt; 594                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
    595         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)

~/anaconda3/lib/python3.6/site-packages/moxing/framework/file/file_io_patch.py in _load(f, map_location, pickle_module, **pickle_load_args)
    199 
--&gt; 200     _check_seekable(f)
    201     f_should_read_directly = _should_read_directly(f)

~/anaconda3/lib/python3.6/site-packages/torch/serialization.py in _check_seekable(f)
    310     except (io.UnsupportedOperation, AttributeError) as e:
--&gt; 311         raise_err_msg([&quot;seek&quot;, &quot;tell&quot;], e)
    312     return False

~/anaconda3/lib/python3.6/site-packages/torch/serialization.py in raise_err_msg(patterns, e)
    303                                 + &quot; try to load from it instead.&quot;)
--&gt; 304                 raise type(e)(msg)
    305         raise e

AttributeError: 'torch._C.PyTorchFileReader' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
~/work/algo-FineTuningBert3/FineTuningBert3.py in &lt;module&gt;()
      1 #Model load checking
----&gt; 2 loadded_model = BertForMaskedLM.from_pretrained('/cache/raw_model/')

~/anaconda3/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1039             except Exception:
   1040                 raise OSError(
-&gt; 1041                     f&quot;Unable to load weights from pytorch checkpoint file for '{pretrained_model_name_or_path}' &quot;
   1042                     f&quot;at '{resolved_archive_file}'&quot;
   1043                     &quot;If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. &quot;

OSError: Unable to load weights from pytorch checkpoint file for '/cache/raw_model/' at '/cache/raw_model/pytorch_model.bin'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. 
</code></pre>
<p>I am using the latest torch (1.7.1) and transformers (4.3.3) packages. I do not clearly understand what causes this error and how to solve this issue.</p>
","852775","","852775","","2021-03-02 16:09:23","2021-09-06 07:41:36","Loading pretrained BERT model issue","<python><pytorch><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"59182421","1","","","2019-12-04 18:15:51","","4","587","<p>I am trying to run the last example from the <a href=""https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/"" rel=""nofollow noreferrer"">page</a>. I have cloned the repository in the directory <code>C:/Users/nn/Desktop/BERT/transformers-master</code>. I am on windows machine and using spyder IDE. Why i do get below error and how could i resolve it? How do i input the initial part of the poem?</p>

<pre><code>import os

os.chdir('C:/Users/nn/Desktop/BERT/transformers-master/examples')
os.listdir()# It shows run_generation.py file

python run_generation.py \
    --model_type=gpt2 \
    --length=100 \
    --model_name_or_path=gpt2 \

python run_generation.py \
    --model_type=gpt2 \
    --length=100 \
    --model_name_or_path=gpt2 \
  File ""&lt;ipython-input-10-501d266b0e64&gt;"", line 1
    python run_generation.py \
                        ^
SyntaxError: invalid syntax
</code></pre>

<p>I went to command prompt and tried below</p>

<pre><code>cd C:/Users/nn/Desktop/BERT/transformers-master/examples
python3 run_generation.py \--model_type=gpt2 \--length=100 \--model_name_or_path=gpt2 \--promt=""Hello world""
</code></pre>

<p>nothing happens :(</p>

<p>when i try the same with python command i get an error as below :(</p>

<pre><code>python run_generation.py \--model_type=gpt2 \--length=100 \--model_name_or_path=gpt2 \--promt=""Hello world""
2019-12-04 11:23:36.345648: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2019-12-04 11:23:36.352875: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
usage: run_generation.py [-h] --model_type MODEL_TYPE --model_name_or_path
                         MODEL_NAME_OR_PATH [--prompt PROMPT]
                         [--padding_text PADDING_TEXT] [--xlm_lang XLM_LANG]
                         [--length LENGTH] [--num_samples NUM_SAMPLES]
                         [--temperature TEMPERATURE]
                         [--repetition_penalty REPETITION_PENALTY]
                         [--top_k TOP_K] [--top_p TOP_P] [--no_cuda]
                         [--seed SEED] [--stop_token STOP_TOKEN]
run_generation.py: error: the following arguments are required: --model_type, --model_name_or_path
</code></pre>

<p><strong><em>####update 2 ------------------</em></strong></p>

<p>I followed suggestions in the comments and it worked. It seems that the code downloads 3 files. </p>

<ol>
<li>Can i copy those files manually so that I dont have to rely on downloading them every time in a temp folder? </li>
<li>Where should i store those files? which folder location? would it be <code>C:\Users\nnn\Desktop\BERT\transformers-master\examples</code> - same as <code>run_generation.py</code> file?</li>
</ol>

<p><code>abc</code></p>

<pre><code>C:\Users\nnn\Desktop\BERT\transformers-master\examples&gt;python run_generation.py --model_type=gpt2 --length=100 --model_name_or_path=gpt2 --prompt=""My job is""

2019-12-12 11:11:57.740810: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2019-12-12 11:11:57.748330: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
12/12/2019 11:12:01 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to C:\Users\nnn\AppData\Local\Temp\tmpt_29gyqi
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1042301/1042301 [00:00&lt;00:00, 2275416.04B/s]
12/12/2019 11:12:02 - INFO - transformers.file_utils -   copying C:\Users\nnn\AppData\Local\Temp\tmpt_29gyqi to cache at C:\Users\nnn\.cache\torch\transformers\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
12/12/2019 11:12:02 - INFO - transformers.file_utils -   creating metadata file for C:\Users\nnn\.cache\torch\transformers\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
12/12/2019 11:12:02 - INFO - transformers.file_utils -   removing temp file C:\Users\nnn\AppData\Local\Temp\tmpt_29gyqi
12/12/2019 11:12:03 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to C:\Users\nnn\AppData\Local\Temp\tmpj1_y4sn8
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456318/456318 [00:00&lt;00:00, 1456594.78B/s]
12/12/2019 11:12:03 - INFO - transformers.file_utils -   copying C:\Users\nnn\AppData\Local\Temp\tmpj1_y4sn8 to cache at C:\Users\nnn\.cache\torch\transformers\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
12/12/2019 11:12:03 - INFO - transformers.file_utils -   creating metadata file for C:\Users\nnn\.cache\torch\transformers\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
12/12/2019 11:12:03 - INFO - transformers.file_utils -   removing temp file C:\Users\nnn\AppData\Local\Temp\tmpj1_y4sn8
12/12/2019 11:12:03 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at C:\Users\nnn\.cache\torch\transformers\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
12/12/2019 11:12:03 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at C:\Users\nnn\.cache\torch\transformers\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
12/12/2019 11:12:04 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache or force_download set to True, downloading to C:\Users\nnn\AppData\Local\Temp\tmpyxywrts1
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00&lt;00:00, 17738.31B/s]
12/12/2019 11:12:04 - INFO - transformers.file_utils -   copying C:\Users\nnn\AppData\Local\Temp\tmpyxywrts1 to cache at C:\Users\nnn\.cache\torch\transformers\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80
12/12/2019 11:12:04 - INFO - transformers.file_utils -   creating metadata file for C:\Users\nnn\.cache\torch\transformers\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80
12/12/2019 11:12:04 - INFO - transformers.file_utils -   removing temp file C:\Users\nnn\AppData\Local\Temp\tmpyxywrts1
12/12/2019 11:12:04 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at C:\Users\nnn\.cache\torch\transformers\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80
12/12/2019 11:12:04 - INFO - transformers.configuration_utils -   Model config {
  ""attn_pdrop"": 0.1,
  ""embd_pdrop"": 0.1,
  ""finetuning_task"": null,
  ""initializer_range"": 0.02,
  ""layer_norm_epsilon"": 1e-05,
  ""n_ctx"": 1024,
  ""n_embd"": 768,
  ""n_head"": 12,
  ""n_layer"": 12,
  ""n_positions"": 1024,
  ""num_labels"": 1,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""output_past"": true,
  ""pruned_heads"": {},
  ""resid_pdrop"": 0.1,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""torchscript"": false,
  ""use_bfloat16"": false,
  ""vocab_size"": 50257
}

12/12/2019 11:12:04 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to C:\Users\nnn\AppData\Local\Temp\tmpn8i9o_tm
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548118077/548118077 [01:12&lt;00:00, 7544610.26B/s]
12/12/2019 11:13:18 - INFO - transformers.file_utils -   copying C:\Users\nnn\AppData\Local\Temp\tmpn8i9o_tm to cache at C:\Users\nnn\.cache\torch\transformers\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1
12/12/2019 11:13:24 - INFO - transformers.file_utils -   creating metadata file for C:\Users\nnn\.cache\torch\transformers\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1
12/12/2019 11:13:24 - INFO - transformers.file_utils -   removing temp file C:\Users\nnn\AppData\Local\Temp\tmpn8i9o_tm
12/12/2019 11:13:24 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at C:\Users\nnn\.cache\torch\transformers\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1
12/12/2019 11:13:32 - INFO - __main__ -   Namespace(device=device(type='cpu'), length=100, model_name_or_path='gpt2', model_type='gpt2', n_gpu=0, no_cuda=False, num_samples=1, padding_text='', prompt='My job is', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, top_k=0, top_p=0.9, xlm_lang='')
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23&lt;00:00,  2.49it/s]
 to know when it will change, it's up to you.""

National Communications Director Alex Brynner said the Trump administration needs to help then-Secretary of State Rex Tillerson learn from him.

""The Cabinet, like any other government job, has to be attentive to the needs of an individual that might challenge his or her position,"" Brynner said. ""This is especially true in times of renewed volatility.""

Brynner said Tillerson has not ""failed at vetting
</code></pre>
","2543622","","1150683","","2020-01-16 10:26:31","2020-01-16 10:26:31","windows spyder invalid syntax error while running py file","<windows><pytorch><huggingface-transformers>","1","4","","","","CC BY-SA 4.0"
"66434969","1","","","2021-03-02 07:25:10","","0","129","<p>I am currently using huggingface package to train my layoutlm model. However, I am experiencing overfitting for a token classification task. My dataset contains only 400 documents. I know it is very small dataset but I don't have any other chance to collect more data.</p>
<p>My results are in the table below. I have tried <code>weight_decay=0.1</code> which is a high number in my opinion and also tried early stopping based on f1 score and loss seperately, but they didn't work.
<a href=""https://i.stack.imgur.com/gPCHw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gPCHw.png"" alt=""enter image description here"" /></a></p>
<p>Which regularisation techniques should I try extra? Do you have any solution to overfitting to a small dataset with bert-like models?</p>
","11758585","","","","","2021-03-02 07:25:10","Overfitting on a small dataset while finetuning Layoutlm. Which regularization techniques are suggested?","<python><pytorch><huggingface-transformers><named-entity-recognition><named-entity-extraction>","0","2","","","","CC BY-SA 4.0"
"66501492","1","","","2021-03-06 01:22:11","","4","425","<p>I have a model trained in sagemaker (custom training job), and saved by my training script with the keras <code>model.save()</code> method that produces a <code>variables</code> directory with the weights and index, and a <code>.pb</code> file. The model is a <code>TFBertForSequenceClassification</code> from huggingface's <code>transformer</code> library, and according to their documentation, this model subclasses from a keras model. When I try to load the model with <code>keras.models.load_model()</code> however, I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py&quot;, line 187, in load_model
    return saved_model_load.load(filepath, compile, options)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py&quot;, line 121, in load
    path, options=options, loader_cls=KerasObjectLoader)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py&quot;, line 633, in load_internal
    ckpt_options)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py&quot;, line 194, in __init__
    super(KerasObjectLoader, self).__init__(*args, **kwargs)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py&quot;, line 130, in __init__
    self._load_all()
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py&quot;, line 215, in _load_all
    self._layer_nodes = self._load_layers()
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py&quot;, line 315, in _load_layers
    layers[node_id] = self._load_layer(proto.user_object, node_id)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py&quot;, line 341, in _load_layer
    obj, setter = self._revive_from_config(proto.identifier, metadata, node_id)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py&quot;, line 368, in _revive_from_config
    obj, self._proto.nodes[node_id], node_id)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py&quot;, line 298, in _add_children_recreated_from_config
    obj_child, child_proto, child_id)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py&quot;, line 298, in _add_children_recreated_from_config
    obj_child, child_proto, child_id)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py&quot;, line 250, in _add_children_recreated_from_config
    metadata = json_utils.decode(proto.user_object.metadata)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/json_utils.py&quot;, line 60, in decode
    return json.loads(json_string, object_hook=_decode_helper)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/json/__init__.py&quot;, line 361, in loads
    return cls(**kw).decode(s)
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/json/decoder.py&quot;, line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File &quot;/home/tyarosevich/anaconda3/envs/fresh_env/lib/python3.7/json/decoder.py&quot;, line 355, in raw_decode
    raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
</code></pre>
<p>I'm stumped. The transformer library's own <code>save_pretrained()</code> method saves layer info in a <code>.json</code> file, but I don't see why the keras model saves would know/care about this (and I don't think that's what the issue is anyway). Any help?</p>
","6910488","","","","","2021-08-31 01:00:12","Can't load TF transformer model with keras.models.load_model()","<python><tensorflow><keras><huggingface-transformers>","0","2","","","","CC BY-SA 4.0"
"66636192","1","","","2021-03-15 10:27:05","","0","23","<p>I would like to get a better understanding on how one may achieve more control over classification abilities of BERT based language model fine-tuned for multi-label text classification task (or any other LM architecture in general).</p>
<p>My basic use case is to be able to increase or decrease the relevance of certain words included in the input to the already trained and fine-tuned model.</p>
<p>For instance, let's say there were too few training data samples for certain, quite specific label during fine-tuning process. Then I'd like to kind of boost classification of inputs to that label under condition of certain words being included in the input.</p>
<p>I guess there are some ways of tweaking within its attention mechanism, but still I've got no idea what and where should be done.</p>
","14661154","","14661154","","2021-03-15 11:27:35","2021-03-15 11:27:35","What are the methods to gain more control over BERT outputs? Increasing/decreasing relevance of certain words in multi-label text classification task","<nlp><text-classification><bert-language-model><huggingface-transformers>","0","1","","","","CC BY-SA 4.0"
"67446187","1","","","2021-05-08 09:49:39","","0","238","<p>I have the below code for a binary classification and it works fine but i would like to modify the nn.Sequential parameters and add an BiLSTM layer. I have the below code:</p>
<pre><code>class BertClassifier(nn.Module):
 def __init__(self, freeze_bert=False):
  super(BertClassifier, self).__init__()
  # Specify hidden size of BERT, hidden size of our classifier, and number of labels
  D_in, H, D_out = 768, 50, 2

  # Instantiate BERT model
  self.bert = BertModel.from_pretrained('bert-base-multilingual-uncased')
  # Instantiate an one-layer feed-forward classifier
  self.classifier = nn.Sequential(nn.Linear(D_in, H),nn.ReLU(),nn.Linear(H, D_out))

  # Freeze the BERT model
  if freeze_bert:
   for param in self.bert.parameters():
    param.requires_grad = False

 def forward(self, input_ids, attention_mask):
  # Feed input to BERT
  outputs = self.bert(input_ids=input_ids,attention_mask=attention_mask)
  # Extract the last hidden state of the token `[CLS]` for classification task
  last_hidden_state_cls = outputs[0][:, 0, :]
  # Feed input to classifier to compute logits
  logits = self.classifier(last_hidden_state_cls)

  return logits
</code></pre>
<p>I have tried to modify the sequential like this <code>self.classifier = nn.Sequential(nn.LSTM(D_in, H, batch_first=True, bidirectional=True),nn.ReLU(),nn.Linear(H, D_out))</code> but then it throws the error <code>RuntimeError: input must have 3 dimensions, got 2</code> on line <code>logits = self.classifier(last_hidden_state_cls)</code>. I found that I can use nn.ModuleDict instead of nn.Sequential and i made the below :</p>
<pre><code>  self.classifier = nn.ModuleDict({
   'lstm': nn.LSTM(input_size=D_in, hidden_size=H,batch_first=True, bidirectional=True ),
   'linear': nn.Linear(in_features=H,out_features=D_out)})
</code></pre>
<p>But now I'm having issues computing the forward function with this. Can someone advice how i can properly modify the forward function?</p>
<p>Update: I also installed CUDA and now when I run the code it returns the error <code>CUDA out of memory. Tried to allocate 16.00 MiB</code>  and I tried to lower the batch size but that doesn't fix the problem. I also tried the below but didn't resolved either. Any advice, please?</p>
<pre><code>import torch, gc
gc.collect()
torch.cuda.empty_cache()
</code></pre>
<p>Update with the code:</p>
<pre><code>    MAX_LEN = 64
    # For fine-tuning BERT, the authors recommend a batch size of 16 or 32.
    batch_size = 32
    VALID_BATCH_SIZE = 4
    
    file1 = open('MH.txt', 'r')
    list_com = []
    list_label = []
    for line in file1:
     possible_labels = 'positive|negative'
     label = re.findall(possible_labels, line)
     line = re.sub(possible_labels, ' ', line)
     line = re.sub('\n', ' ', line)
     list_com.append(line)
     list_label.append(label[0])
    
    list_tuples = list(zip(list_com, list_label))
    file1.close()
    labels = ['positive', 'negative']
    df = pd.DataFrame(list_tuples, columns=['text', 'label'])
    df['label'] = df['label'].map({'positive': 1, 'negative': 0})
    for i in range(0,len(df['label'])):
     list_label[i] = df['label'][i]
    #print(df)
    #print(df['label'].value_counts())
    X = df.text.values
    y = df.label.values
    X_train, X_val, y_train, y_val =\
        train_test_split(X, y, test_size=0.1, random_state=2020)
    
    def text_preprocessing(text):
     # Remove '@name'
     text = re.sub(r'(@.*?)[\s]', ' ', text)
     # Replace '&amp;amp;' with '&amp;'
     text = re.sub(r'&amp;amp;', '&amp;', text)
     # Remove trailing whitespace
     text = re.sub(r'\s+', ' ', text).strip()
     return text
    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)
    
    # Create a function to tokenize a set of texts
    def preprocessing_for_bert(data):
     input_ids = []
     attention_masks = []
    
     for sent in data:
      encoded_sent = tokenizer.encode_plus(
       text=text_preprocessing(sent),  # Preprocess sentence
       add_special_tokens=True,  # Add `[CLS]` and `[SEP]`
       max_length=MAX_LEN,  # Max length to truncate/pad
       pad_to_max_length=True,  # Pad sentence to max length
       # return_tensors='pt',           # Return PyTorch tensor
       return_attention_mask=True  # Return attention mask
      )
    
      # Add the outputs to the lists
      input_ids.append(encoded_sent.get('input_ids'))
      attention_masks.append(encoded_sent.get('attention_mask'))
    
     # Convert lists to tensors
     input_ids = torch.tensor(input_ids)
     attention_masks = torch.tensor(attention_masks)
    
     return input_ids, attention_masks
    
    train_inputs, train_masks = preprocessing_for_bert(X_train)
    val_inputs, val_masks = preprocessing_for_bert(X_val)
    
    # Convert other data types to torch.Tensor
    train_labels = torch.tensor(y_train)
    val_labels = torch.tensor(y_val)
    
    # Create the DataLoader for our training set
    train_data = TensorDataset(train_inputs, train_masks, train_labels)
    train_sampler = RandomSampler(train_data)
    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)
    # Create the DataLoader for our validation set
    val_data = TensorDataset(val_inputs, val_masks, val_labels)
    val_sampler = SequentialSampler(val_data)
    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)
    
    # Create the BertClassfier class
    class BertClassifier(nn.Module):
     &quot;&quot;&quot;Bert Model for Classification Tasks.&quot;&quot;&quot;
     def __init__(self, freeze_bert=False):
      &quot;&quot;&quot;
      @param    bert: a BertModel object
      @param    classifier: a torch.nn.Module classifier
      @param    freeze_bert (bool): Set `False` to fine-tune the BERT model
      &quot;&quot;&quot;
      super(BertClassifier, self).__init__()
      # Specify hidden size of BERT, hidden size of our classifier, and number of labels
      D_in, H, D_out = 768, 50, 2
    
      # Instantiate BERT model
      self.bert = BertModel.from_pretrained('bert-base-multilingual-uncased')
      # Instantiate an one-layer feed-forward classifier
      self.classifier = nn.ModuleDict({
       'lstm': nn.LSTM(input_size=D_in, hidden_size=H, batch_first=True, bidirectional=True),
       'linear': nn.Linear(in_features=H, out_features=D_out)})
    
      # Freeze the BERT model
      if freeze_bert:
       for param in self.bert.parameters():
        param.requires_grad = False
    
     def forward(self, input_ids, attention_mask):
      outputs = self.bert(input_ids=input_ids,attention_mask=attention_mask)
      sequence_output = outputs[0]
      sequence_output, _ = self.lstm(sequence_output)
      linear_output = self.linear(sequence_output[:, -1])

  return linear_output
    
    def initialize_model(epochs=4):
        # Instantiate Bert Classifier
        bert_classifier = BertClassifier(freeze_bert=False)
        print(bert_classifier)
        # Tell PyTorch to run the model on GPU
        bert_classifier.to(device)
        # Create the optimizer
        optimizer = AdamW(bert_classifier.parameters(), lr=5e-5)
        # Total number of training steps
        total_steps = len(train_dataloader) * epochs
        # Set up the learning rate scheduler
        scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)
        return bert_classifier, optimizer, scheduler
    
    # Specify loss function
    loss_fn = nn.CrossEntropyLoss()
    
    def set_seed(seed_value=42):
     &quot;&quot;&quot;Set seed for reproducibility.&quot;&quot;&quot;
     random.seed(seed_value)
     np.random.seed(seed_value)
     torch.manual_seed(seed_value)
     torch.cuda.manual_seed_all(seed_value)
    
    
    def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):
     &quot;&quot;&quot;Train the BertClassifier model.&quot;&quot;&quot;
     # Start training loop
     print(&quot;Start training...\n&quot;)
     for epoch_i in range(epochs):
      # Print the header of the result table
      print(f&quot;{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}&quot;)
      print(&quot;-&quot; * 70)
      # Measure the elapsed time of each epoch
      t0_epoch, t0_batch = time.time(), time.time()
      # Reset tracking variables at the beginning of each epoch
      total_loss, batch_loss, batch_counts = 0, 0, 0
      # Put the model into the training mode
      model.train()
      # For each batch of training data...
      for step, batch in enumerate(train_dataloader):
       batch_counts += 1
       # Load batch to GPU
       b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)
       # Zero out any previously calculated gradients
       model.zero_grad()
       # Perform a forward pass. This will return logits.
       logits = model(b_input_ids, b_attn_mask)
       # Compute loss and accumulate the loss values
       loss = loss_fn(logits, b_labels)
       batch_loss += loss.item()
       total_loss += loss.item()
       # Perform a backward pass to calculate gradients
       loss.backward()
       # Clip the norm of the gradients to 1.0 to prevent &quot;exploding gradients&quot;
       torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
       # Update parameters and the learning rate
       optimizer.step()
       scheduler.step()
       # Print the loss values and time elapsed for every 20 batches
       if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):
        # Calculate time elapsed for 20 batches
        time_elapsed = time.time() - t0_batch
        # Print training results
        print(
         f&quot;{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}&quot;)
        # Reset batch tracking variables
        batch_loss, batch_counts = 0, 0
        t0_batch = time.time()
      # Calculate the average loss over the entire training data
      avg_train_loss = total_loss / len(train_dataloader)
    
      print(&quot;-&quot; * 70)
      #Evaluation
      if evaluation == True:
       # After the completion of each training epoch, measure the model's performance
       # on our validation set.
       val_loss, val_accuracy = evaluate(model, val_dataloader)
    
       # Print performance over the entire training data
       time_elapsed = time.time() - t0_epoch
    
       print(
        f&quot;{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}&quot;)
       print(&quot;-&quot; * 70)
      print(&quot;\n&quot;)
    
     print(&quot;Training complete!&quot;)
    
    
    def evaluate(model, val_dataloader):
     &quot;&quot;&quot;After the completion of each training epoch, measure the model's performance
     on our validation set.
     &quot;&quot;&quot;
     # Put the model into the evaluation mode. The dropout layers are disabled during
     # the test time.
     model.eval()
    
     # Tracking variables
     val_accuracy = []
     val_loss = []
    
     # For each batch in our validation set...
     for batch in val_dataloader:
      # Load batch to GPU
      b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)
    
      # Compute logits
      with torch.no_grad():
       logits = model(b_input_ids, b_attn_mask)
    
      # Compute loss
      loss = loss_fn(logits, b_labels)
      val_loss.append(loss.item())
    
      # Get the predictions
      preds = torch.argmax(logits, dim=1).flatten()
    
      # Calculate the accuracy rate
      accuracy = (preds == b_labels).cpu().numpy().mean() * 100
      val_accuracy.append(accuracy)
    
     # Compute the average accuracy and loss over the validation set.
     val_loss = np.mean(val_loss)
     val_accuracy = np.mean(val_accuracy)
    
     return val_loss, val_accuracy
    
    def accuracy(probs, y_true):
     &quot;&quot;&quot;
     - Print AUC and accuracy on the test set
     @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)
     @params    y_true (np.array): an array of the true values with shape (len(y_true),)
    
    
     fpr, tpr, threshold = roc_curve(y_true, preds)
     roc_auc = auc(fpr, tpr)
     print(f'AUC: {roc_auc:.4f}')
    &quot;&quot;&quot;
     preds = probs[:, 1]
     # Get accuracy over the test set
     y_pred = np.where(preds &gt;= 0.5, 1, 0)
     accuracy = accuracy_score(y_true, y_pred)
     print(f'Accuracy: {accuracy * 100:.2f}%')
    
    def bert_predict(model, test_dataloader):
     &quot;&quot;&quot;Perform a forward pass on the trained BERT model to predict probabilities on the test set.&quot;&quot;&quot;
     # Put the model into the evaluation mode. The dropout layers are disabled during the test time.
     model.eval()
     all_logits = []
     # For each batch in our test set...
     for batch in test_dataloader:
      # Load batch to GPU
      b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]
    
      # Compute logits
      with torch.no_grad():
       logits = model(b_input_ids, b_attn_mask)
      all_logits.append(logits)
    
     # Concatenate logits from each batch
     all_logits = torch.cat(all_logits, dim=0)
     # Apply softmax to calculate probabilities
     probs = F.softmax(all_logits, dim=1).cpu().numpy()
    
     return probs
    
    set_seed(42)    # Set seed for reproducibility
    bert_classifier, optimizer, scheduler = initialize_model(epochs=3)
    # start training
    train(bert_classifier, train_dataloader, val_dataloader, epochs=3, evaluation=True)
    # Compute predicted probabilities on the test set
    probs = bert_predict(bert_classifier, val_dataloader)
    # Evaluate the Bert classifier
    accuracy(probs, y_val)
</code></pre>
","15645097","","15645097","","2021-05-24 06:29:50","2021-05-24 06:29:50","How to add BiLSTM on top of BERT from Huggingface + CUDA out of memory. Tried to allocate 16.00 MiB","<python><lstm><bert-language-model><huggingface-transformers>","0","15","","","","CC BY-SA 4.0"
"66488170","1","","","2021-03-05 07:06:38","","0","49","<p>I'm facing BrokenPipeError when I'm trying to run sentiment analysis with hugging face. It's returning [Error No] 32 Broken Pipe.</p>
<p>Link with total code '<a href=""https://colab.research.google.com/drive/1wBXKa-gkbSPPk-o7XdwixcGk7gSHRMas?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1wBXKa-gkbSPPk-o7XdwixcGk7gSHRMas?usp=sharing</a>'</p>
<p>The code is</p>
<pre><code>def create_data_loader(df, tokenizer, max_len, batch_size):
  ds = GPReviewDataset(
    reviews=df.content.to_numpy(),
    targets=df.sentiment.to_numpy(),
    tokenizer=tokenizer,
    max_len=max_len
  )
  return DataLoader(
    ds,
    batch_size=batch_size,
    num_workers=4
  )
</code></pre>
<p>Followed by below code</p>
<pre><code>BATCH_SIZE = 16
train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)
</code></pre>
<p>Followed by</p>
<pre><code>data = next(iter(train_data_loader))
data.keys()
</code></pre>
<p>I'm facing error with this '<code>data = next(iter(train_data_loader))</code>' code</p>
<p>Error is <code>BrokenPipeError: [Errno 32] Broken pipe</code></p>
","15224778","","15224778","","2021-03-05 13:05:42","2021-03-05 13:05:42","I'm facing BrokenPipeError when I'm trying to run sentiment analysis with hugging face","<python><deep-learning><huggingface-transformers><broken-pipe><huggingface-tokenizers>","0","4","","","","CC BY-SA 4.0"
"66520993","1","","","2021-03-07 20:18:49","","0","46","<p>I'm trying to build a model on the imdb dataset for a binary classification task on texts. I have built the model but unable to meaningfully make predictions with it.</p>
<p>Here's my workflow:</p>
<p><strong>Get the data</strong></p>
<pre><code>from pathlib import Path

def read_imdb_split(split_dir):
    split_dir = Path(split_dir)
    texts = []
    labels = []
    for label_dir in [&quot;pos&quot;, &quot;neg&quot;]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir == &quot;neg&quot; else 1)

    return texts, labels

train_texts, train_labels = tuple(map(lambda x:x[:50], read_imdb_split('aclImdb/train')))
test_texts, test_labels = tuple(map(lambda x:x[:50], read_imdb_split('aclImdb/test')))
</code></pre>
<p><strong>Split up the data</strong></p>
<pre><code>from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)
</code></pre>
<p><strong>Tokenize the text</strong></p>
<pre><code>
from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncase')
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)
</code></pre>
<p><strong>Convert tensors into tf datasets</strong></p>
<pre><code>import tensorflow as tf

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))

</code></pre>
<p><strong>Fit</strong></p>
<pre><code>import tensorflow as tf
from transformers import TFDistilBertForSequenceClassification

model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)
</code></pre>
<p><em><strong>Now, I am trying to evaluate this model against some custom data. How do I accomplish this?</strong></em></p>
<hr />
<p>My attempt:</p>
<p>To start off, I tried to evaluate the model against the already-compiled test dataset.</p>
<pre><code>x = test_dataset.shuffle(3)
result = model.predict(x)
print(result)
</code></pre>
<p>However, this gave me some weird results. When I do <code>len(result[0])</code> it says <code>25600</code> -- which is odd, because the length of the test_dataset itself is just <code>50</code></p>
","7212809","","7212809","","2021-03-07 20:58:51","2021-03-07 20:58:51","Tensorflow + transformers: How to test a model after fitting it?","<python><tensorflow><keras><huggingface-transformers>","0","1","1","","","CC BY-SA 4.0"
"66535377","1","","","2021-03-08 18:41:55","","0","70","<p>Most of the code below is taken from <a href=""https://huggingface.co/transformers/custom_datasets.html"" rel=""nofollow noreferrer"">this huggingface doc page, for tensorflow code selections</a>. What confuses me is that after fine-tuning a pretrained model on a few new sentences and running <code>predict</code> on two test-set sentences, I get <code>predict()</code> output that is 16x2 array.</p>
<p>x2 makes sense as I have two classes (0,1), but why length 16 when I passed a test-set of 2 (not 16) sequences, to a 'SequenceClassification' model? How do I get the predicted classes for the two test-set sequences? (ps I have no problem converting from logits to predicted probabilities, just confused about the shape of the output).</p>
<p>Reproducible code example below. Also feel free to step through code in google colab environment <a href=""https://colab.research.google.com/drive/1pZIRhxlSwuFdPYH2ZuvfL87E58qzGDyy?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>from transformers import DistilBertTokenizerFast
from transformers import TFDistilBertForSequenceClassification
import tensorflow as tf

# set up arbitrary example data
train_txt = ['this sentence is about dinosaurs', 'this also mentions dinosaurs', 'this does not']
test_txt  = ['the land before time was cool', 'alligators are basically dinosaurs']
train_labels = [1,1,0]
test_labels = [1,1]

# convert sentence lists to Distilbert Encodings and then TF Datasets
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer([str(s) for s in train_txt], truncation=True, padding=True)
test_encodings = tokenizer([str(s) for s in test_txt], truncation=True, padding=True)

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))

# Fine-tune pretrained Distilbert Classifier on our data
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(3), epochs=3, batch_size=3)

# Generate test-set predictions
test_preds = model.predict(test_dataset)
</code></pre>
<p><code>test_preds</code> output:</p>
<pre><code>&gt;test_preds
TFSequenceClassifierOutput([('logits', array([[ 0.1527334 ,  0.17010647],
                                    [ 0.10007463,  0.15664947],
                                    [-0.10294056,  0.18813357],
                                    [-0.05231615,  0.1587314 ],
                                    [-0.11520502,  0.16303074],
                                    [ 0.00855697,  0.13974288],
                                    [-0.17962483,  0.12381783],
                                    [ 0.05765227,  0.04970012],
                                    [ 0.1527334 ,  0.17010647],
                                    [-0.12754977,  0.11164709],
                                    [-0.00847345,  0.12885672],
                                    [-0.01731028,  0.13520113],
                                    [-0.08433925,  0.16828224],
                                    [-0.20086896,  0.08963215],
                                    [ 0.05765227,  0.04970012],
                                    [ 0.02467203,  0.15794128]], dtype=float32))])
</code></pre>
","1870832","","","","","2021-03-08 18:41:55","Confused about predict() output from Huggingface Transformers Sequence Classification","<python><tensorflow><keras><nlp><huggingface-transformers>","0","0","","","","CC BY-SA 4.0"
"66596142","1","66597745","","2021-03-12 07:53:12","","3","714","<p>I want to use Bert only for embedding and use the Bert output as an input for a classification net that I will build from scratch.</p>
<p>I am not sure if I want to do finetuning for the model.</p>
<p>I think the relevant classes are BertModel or BertForPreTraining.</p>
<p><a href=""https://dejanbatanjac.github.io/bert-word-predicting/"" rel=""nofollow noreferrer"">BertForPreTraining</a>  head contains two &quot;actions&quot;:
self.predictions is MLM (Masked Language Modeling) head is what gives BERT the power to fix the grammar errors, and self.seq_relationship is NSP (Next Sentence Prediction); usually refereed as the classification head.</p>
<pre><code>class BertPreTrainingHeads(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.predictions = BertLMPredictionHead(config)
        self.seq_relationship = nn.Linear(config.hidden_size, 2)
</code></pre>
<p>I think the NSP isn't relevant for my task so I can &quot;override&quot; it.
what does the MLM do and is it relevant for my goal or should I use the BertModel?</p>
","7169209","","4685471","","2021-03-12 10:36:33","2021-03-12 10:36:33","BertModel or BertForPreTraining","<deep-learning><nlp><bert-language-model><huggingface-transformers><transformer>","1","0","","","","CC BY-SA 4.0"
"66644432","1","67126834","","2021-03-15 19:25:19","","3","426","<p>I am trying to use the huggingface transformers library in a hosted Jupyter notebook platform called Deepnote. I want to download a model through the pipeline class but unfortunately deepnote does not support IPyWidgets. Is there a way to disable IPywidgets when using transformers? Specifically the below command.</p>
<pre><code>
classifier = pipeline(&quot;zero-shot-classification&quot;)
</code></pre>
<p>And the error I receive.</p>
<pre><code>ImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
</code></pre>
<p>Note: Installing IPyWidgets is not an option</p>
","7981821","","","","","2021-04-16 14:13:11","Use huggingface transformers without IPyWidgets","<python><jupyter-notebook><huggingface-transformers><ipywidgets><deepnote>","1","0","1","","","CC BY-SA 4.0"
"66524542","1","66524690","","2021-03-08 05:03:19","","2","803","<p><strong>AttributeError: 'str' object has no attribute 'shape' while encoding tensor using BertModel with PyTorch (Hugging Face). Below is the code</strong></p>
<pre><code>bert_model = BertModel.from_pretrained(r'downloads\bert-pretrained-model')
input_ids
</code></pre>
<p>Output is:</p>
<pre><code>tensor([[  101,   156, 13329,  ...,     0,     0,     0],
        [  101,   156, 13329,  ...,     0,     0,     0],
        [  101,  1302,  1251,  ...,     0,     0,     0],
        ...,
        [  101, 25456,  1200,  ...,     0,     0,     0],
        [  101,   143,  9664,  ...,     0,     0,     0],
        [  101,  2586,  7340,  ...,     0,     0,     0]])
</code></pre>
<p>Followed by code below</p>
<pre><code>last_hidden_state, pooled_output = bert_model(
  input_ids=encoding['input_ids'],
  attention_mask=encoding['attention_mask']
)
</code></pre>
<p>Followed by code below</p>
<pre><code>last_hidden_state.shape
</code></pre>
<p>Output is</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-70-9628339f425d&gt; in &lt;module&gt;
----&gt; 1 last_hidden_state.shape

AttributeError: 'str' object has no attribute 'shape'
</code></pre>
<p>Complete Code link is '<a href=""https://colab.research.google.com/drive/1FY4WtqCi2CQ9RjHj4slZwtdMhwaWv2-2?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1FY4WtqCi2CQ9RjHj4slZwtdMhwaWv2-2?usp=sharing</a>'</p>
","15224778","","2956066","","2021-03-08 05:48:10","2021-03-08 05:48:10","AttributeError: 'str' object has no attribute 'shape' while encoding tensor using BertModel with PyTorch (Hugging Face)","<python><string><pytorch><attributeerror><huggingface-transformers>","1","2","","","","CC BY-SA 4.0"
"66510340","1","","","2021-03-06 20:27:35","","1","27","<p>I am currently writing my masters' thesis on transformers in NLP. I have been reading a lot and have been wondering about one fact for a while. In transformers, we have self-attention and attention heads. Say I have word embeddings of 512 dimensions and 8 heads, then every head will deal with 64 dimensions of every input word to calculate self-attention.</p>
<p>Here is a picture from a textbook that might illustrate what I mean:</p>
<p><a href=""https://i.stack.imgur.com/DWPHK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DWPHK.png"" alt=""enter image description here"" /></a></p>
<p>Here is my question now. Could we say that every attention-head has a sort of &quot;competence&quot; for a part-meaning of the words? Say the first 64 dimensions of a word embeddings always deal with the words' emotionality; would the first head then be the emotionality-head? And what would that mean for interpretability and the learning in the network?</p>
<p>This is my first question here; I hope to have expressed myself clear enough.</p>
<p>Thank you for any answers!</p>
","15043138","","6664872","","2021-03-14 00:17:23","2021-03-14 00:17:23","Attention heads ""competence"" in NLP transformer networks","<nlp><huggingface-transformers><attention-model>","0","1","","","","CC BY-SA 4.0"