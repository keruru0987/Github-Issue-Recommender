Id,PostTypeId,AcceptedAnswerId,ParentId,CreationDate,DeletionDate,Score,ViewCount,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditorDisplayName,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate,CommunityOwnedDate,ContentLicense
"68862752","1","","","2021-08-20 13:07:18","","0","25","<p>I am new to the AllenNLP library. I am using the Pretrained Bidaf-elmo model for a reading comprehension task. My code looks like -</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>from allennlp.predictors.predictor import Predictor
import allennlp_models.rc
from allennlp_models import pretrained
from allennlp.training.util import evaluate
import allennlp.data.data_loaders.simple_data_loader

archive_file_path = ""https://storage.googleapis.com/allennlp-public-models/bidaf-model-2020.03.19.tar.gz""
input_path = ""C:\\Users\\SHRIPRIYA\\sample_dataset.json""

data_load = simple_data_loader(input_path)
evaluate(model=archive_file_path, data_loader = data_load, output_file=output, predictions_output_file=pred_output_file, cuda_device=0)</code></pre>
</div>
</div>
</p>
<p>The line <code>simple_data_loader()</code> throws an error - <code>name 'simple_data_loader' is not defined</code>. I know this is a syntax error but I could not find any examples to load a JSON file using a Data Loader function from AllenNLP and evaluate it using a pre-trained model.</p>
<p>About my data:</p>
<ol>
<li>total sample passages = 10,000</li>
<li>total questions = 1000</li>
</ol>
<p>Each sample passage needs to be subjected to all the 1000 questions. My sample JSON input looks like -</p>
<pre><code>{
  &quot;passage&quot;: &quot;Venus is named after the Roman goddess of love and beauty. Venus is the second planet from the sun. Is the brightest object in the sky besides our Sun and the Moon. Venus has no moons. It is also known as the morning star because at sunrise it appears in the east. It is also known as the evening star as it appears at sunset when it is in the west. It cannot be seen in the middle of the night. Venus and Earth are close together in space and similar in size, which is the reason Venus is called Earth's sister planet. Venus has more volcanoes than any other planet. It is the hottest planet in the solar system, even hotter than Mercury, which is closer to the Sun. The temperature on the surface of Venus is about 460° Celsius. The atmosphere on Venus is composed of carbon dioxide. The surface is heated by radiation from the sun, but the heat cannot escape through the clouds and layer of carbon dioxide. (This is a “greenhouse effect”).&quot;,
  &quot;questions&quot;: [
    &quot;How many moons does Venus have?&quot;,
    &quot;Venus was named after which Roman goddess?&quot;,    
    &quot;At what position does Venus lie from Sun?&quot;,
    &quot;What is the temperature of Venus surface?&quot;,
    &quot;Why is Venus called Earth’s sister planet?&quot;,
    &quot;What is the atmosphere of Venus composed of?&quot;
  ]}
</code></pre>
<p>If there's any faster alternative to evaluate multiple questions against multiple passages, please let me know.</p>
<p>Thanks!</p>
","16713798","","","","","2021-08-23 22:38:26","How to write evaluate function for a pretrained model and specify a dataloader?","<machine-learning><allennlp>","1","0","","","","CC BY-SA 4.0"
"69090025","1","","","2021-09-07 14:26:58","","0","46","<p>I have been reading through the AllenNLP guide and documentation and was hoping to train an SRL Bert model on French.</p>
<p>On the SRL demo page you have the command for training a SRL Bert model as seen below:</p>
<pre><code>allennlp train \
        https://raw.githubusercontent.com/allenai/allennlp-models/main/training_config/structured_prediction/bert_base_srl.jsonnet \
        -s /path/to/output
</code></pre>
<p>Looking into that jsonnet file AllenNLP points out that they use the CONLL formatted Ontonotes 5.0 data. Since, as AllenNLP mentions, this data is not publicly available I went searching for what the format of this data looked like. Which lead me <a href=""https://github.com/ontonotes/conll-formatted-ontonotes-5.0/blob/master/conll-formatted-ontonotes-5.0/data/train/data/english/annotations/bc/cctv/00/cctv_0001.gold_skel"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Not fully understanding the format at that link I found this <a href=""https://github.com/allenai/allennlp/blob/e5a74abccb969efe33001e20b8c749780d8f657e/allennlp/data/dataset_readers/dataset_utils/ontonotes.py#L83"" rel=""nofollow noreferrer"">description</a> in AllenNLP's code for their Ontonotes class which was extremely helpful.</p>
<p>In light of all the details above I have a couple questions:</p>
<ol>
<li><p>When setting the environment variables <code>SRL_TRAIN_DATA_PATH</code> and <code>SRL_VALIDATION_DATA_PATH</code> that are used in the jsonnet file does the directory structure need to look exactly like the structure described in the Ontonotes class code (seen below) or what is the bare minimum if I will only have one file for training?</p>
<pre><code>└── train
       └── data
           └── english
               └── annotations
                   ├── bc
                   ├── bn
                   ├── mz
                   ├── nw
                   ├── pt
                   ├── tc
                   └── wb
</code></pre>
</li>
<li><p>My second question, using whatever directory structure is necessary, will I be able to train a French model if I create a file just like the <a href=""https://github.com/ontonotes/conll-formatted-ontonotes-5.0/blob/master/conll-formatted-ontonotes-5.0/data/train/data/english/annotations/bc/cctv/00/cctv_0001.gold_skel"" rel=""nofollow noreferrer"">CONLL one</a> but all the words would be in French?</p>
</li>
<li><p>Third and finally if I can train a SRL Bert model using a CONLL file in the appropriate format are all of the columns in the CONLL file necessary to have data in. For example, Column 11 is the named entities, is it necessary to have named entities for training or can that column just be blank (i.e. nothing but hyphens). If it is the case that not all columns need data, which columns need to have data for training and which can be empty?</p>
</li>
</ol>
<p>I know it's a fair amount of questions so thank you in advance.</p>
","2759574","","","","","2021-09-18 00:36:56","How to train AllenNLP SRL on non-English languages?","<allennlp>","1","0","","","","CC BY-SA 4.0"
"68752423","1","","","2021-08-12 06:09:26","","0","14","<p>I've adapted the allennlp <a href=""https://github.com/allenai/allennlp-models/blob/4eb7c27d7fad00ac886ffdefc6c152909fa28f23/allennlp_models/coref/models/coref.py#L538"" rel=""nofollow noreferrer"">coref model</a> into my project. One thing I wasn't sure of was how to ensure that the label tensors are either a cluster index if the input span represents a valid mention, or -1 (otherwise the model doesn't correctly mask out the invalid spans).</p>
<p>I created a dataset reader that assigns a label field for each span. I used a dummy label for spans that aren't valid mentions (i.e. &quot;O&quot;). The issue is the token indexer assigns an arbitrary index to &quot;O&quot;. It just happens that it's normally zero, it's certainly never -1. I worked around this by adding code in the coref module to mask out the dummy labels.</p>
<pre><code>    self.invalid_span_index = self.vocab.get_token_index(invalid_span_label, label_namespace)

    if span_labels is not None:
        valid_span_labels_mask = span_labels != self.invalid_span_index 
        span_labels = util.replace_masked_values(
            span_labels, valid_span_labels_mask, -1
        )
</code></pre>
<p>This works ok, but it should really be done by the token indexer or earlier, so I guess my question is how do you ensure that the token indexer produces specific indexes for certain special labels? Particularly in this case where the model has a requirement that the index is -1</p>
<p>Perhaps I should use <code>skip_indexing=True</code> in the dataloader and index the clusters myself. The issue I have with this approach is I'm using a <code>MultiTaskDataLoader</code>. It should work for now as I'm using <code>HomogeneousRoundRobinScheduler</code> so the model shouldn't see spans from more than one document at a time so at this stage at least I can probably get away with not using a vocabulary. But it doesn't feel like the proper way to do this?</p>
","2981639","","","","","2021-08-12 06:09:26","How to specify -1 label index for allennlp coref model","<allennlp>","0","8","","","","CC BY-SA 4.0"
"69175524","1","","","2021-09-14 09:45:34","","1","25","<p>I've been looking to train my own ELMo model for the past week and came across these two implementations <a href=""https://github.com/allenai/bilm-tf"" rel=""nofollow noreferrer"">allenai/bilm-tf</a> &amp; <a href=""https://github.com/allenai/allennlp"" rel=""nofollow noreferrer"">allenai/allennlp</a>. I've been facing a few roadblocks for a few techniques I've tried and would like to clarify my findings, so that I can get a clearer direction.</p>
<p>As my project revolves around healthcare, I would like to train the embeddings from scratch for better results. The dataset I am working on is MIMIC-III and the entire dataset is stored in one .csv, unlike 1 Billion Word Language Model Benchmark (data used in tutorials) where files are stored in separate .txt files.</p>
<p>I was following this &quot;Using ELMo as a PyTorch Module to train a new model&quot; <a href=""https://github.com/allenai/allennlp/blob/v0.8.3/tutorials/how_to/elmo.md#using-elmo-as-a-pytorch-module-to-train-a-new-model"" rel=""nofollow noreferrer"">tutorial</a> but I figured out that one of the requirements is a .hdf5 weights_file.</p>
<p><strong>(Question)</strong> Does this mean that I will have to train a bilm model first to get .hdf5 weights to input? Can I train an ELMo model from scratch using allennlp.modules.elmo.Elmo? Is there any other way where I can train a model this way with an empty .hdf5 as I was able to run this successfully with tutorial data.</p>
<p><strong>(Question)</strong> What will be the best method for me to train my embeddings? (PS: some methods I've tried are documented below). In my case where I will probably need a custom DatasetReader, rather than converting the csv to txt files, wasting memory.</p>
<hr />
<p>Here, let me go into the details of other methods I have tried so far. Serves as a backstory to the main question of what may be the best technique. Please let me know if you know of any other methods to train my own ELMo model, or if one of the following methods are preferred over the others.</p>
<p>I've tried training a model using the <code>allennlp train ...</code> command by following <a href=""https://towardsdatascience.com/pytorch-elmo-844d2391a0b2#b484"" rel=""nofollow noreferrer"">this tutorial</a>. However, I was unable to run with tutorial data due to the following error which I am still unable to solve.</p>
<pre><code>allennlp.common.checks.ConfigurationError: Experiment specified GPU device 1 but there are only 1 devices  available.
</code></pre>
<p>Secondly, this is a technique that I found but have not tried. Similar to the technique above it uses the <code>allennlp train ...</code> command but instead I use <a href=""https://github.com/allenai/allennlp-template-config-files"" rel=""nofollow noreferrer"">allenai/allennlp-template-config-files</a> as a template and modify the Model and DatasetReader.</p>
<p>Lastly, I tried using the TensorFlow implementation <a href=""https://github.com/allenai/bilm-tf"" rel=""nofollow noreferrer"">allenai/bilm-tf</a> following tutorials like <a href=""https://appliedmachinelearning.blog/2019/11/30/training-elmo-from-scratch-on-custom-data-set-for-generating-embeddings-tensorflow/"" rel=""nofollow noreferrer"">this</a>. However, I would like to avoid this method as TF1 is quite outdated. Besides receiving tons of warnings, I faced an error for CUDA as well.</p>
<pre><code>2021-09-14 17:31:36.222624: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 18.45M (19346432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
</code></pre>
","8769769","","4685471","","2021-09-14 10:01:53","2021-09-14 10:01:53","Can I train an ELMo model from scratch using allennlp.modules.elmo.Elmo?","<python><machine-learning><deep-learning><allennlp><elmo>","0","0","","","","CC BY-SA 4.0"
"69365546","1","69437913","","2021-09-28 16:42:10","","0","29","<h1>Background</h1>
<p>I am working on a project where I need to do coreference resolution on a lot of text. In doing so I've dipped my toe into the NLP world and found AllenNLP's coref model.</p>
<p>In general I have a script where I use pandas to load in a dataset of &quot;articles&quot; to be resolved and pass those articles to the <code>predictor.from_path()</code> object to be resolved. Because of the large number of articles that I want to resolve, I'm running this on a remote cluster(though I don't believe that is the source of this problem as this problem also occurs when I run the script locally). That is, my script looks something like this:</p>
<pre><code>from allennlp.predictors.predictor import Predictor
import allennlp_models.tagging
import pandas as pd

print(&quot;HERE TEST&quot;)

def predictorFunc(article):
     predictor = predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz&quot;)
     resolved_object = predictor(document=article)

     ### Some other interrogation of the predicted clusters ###
     return resolved_object['document']

df = pd.read_csv('articles.csv')
### Some pandas magic ###

resolved_text = predictorFunc(article_pre_resolved)
</code></pre>
<h1>The Problem</h1>
<p>When I execute the script the following message is printed to my .log file <em>before anything else</em> (for example the <code>print(&quot;HERE TEST&quot;)</code> that I included) -- even before the <code>predictor</code> object itself is called:</p>
<pre><code>Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>I understand that this message itself is to be expected as I'm using a pre-trained model, but when this message appears it completely locks up the .log file (nothing else gets printed until the script ends and everything gets printed at once). This has been deeply problematic for me as it makes it almost impossible to debug other parts of my script in a meaningful way. (It will also make tracking the final script's progress on a large dataset very difficult... :'( ) Also, I would very much like to know why the predictor object appears to be loading even before it gets called.
Though I can't tell for sure, I also think that whatever is causing this is also causing runaway memory use (even for toy examples of just a single 'article' (a couple hundred words as a string)).</p>
<p>Has anyone else had this problem/know why this happens? Thanks very much in advance!</p>
","17027207","","","","","2021-10-04 14:41:29","AllenNLP predictor object loading before it is called -- rest of the script hangs","<python><slurm><allennlp><coreference-resolution>","1","2","","","","CC BY-SA 4.0"
"69416568","1","","","2021-10-02 11:41:42","","0","15","<p>I'm trying to implement a model similar to <a href=""https://aclanthology.org/N19-1289.pdf"" rel=""nofollow noreferrer"">https://aclanthology.org/N19-1289.pdf</a> using <code>allennlp</code></p>
<p>This consists of two parallel input modules, an embedding module that embeds the inputs and a parallel module that encodes the labels. MSE loss is used to encourage both to produce the same encoding.</p>
<p>Then the output from the label embedded is passed through an output module which recreates the original labels. I have this working, however, I believe that I'm not implementing validation correctly. For validation, the output from the input embedded should be passed through the decoder, not the output from the label encoder.</p>
<p>I'm not sure how to implement this in <code>allennlp</code> though, I need to detect if the model is being trained or validated in the forward method despite both receiving the same arguments (i.e. both x and y are provided).</p>
<p>My current code is</p>
<pre><code>    embedded = self._embedder(text)
    if labels is not None:
        encoded = self._encoder(labels)
        decoded = self._decoder(encoded)

        # compute loss / accuracy
        encoder_loss = MSE(embedded, encoded)
        reconstruction_loss = CDL(labels, decoded)
    else:
        decoded = self._decoder(embedded)
</code></pre>
<p>But what I want to do is</p>
<pre><code>    embedded = self._embedder(text)
    if labels is not None:
        encoded = self._encoder(labels)
        if training:
            decoded = self._decoder(encoded)
        else:
            decoded = self._decoder(embedded)

        # compute loss / accuracy
        encoder_loss = MSE(embedded, encoded)
        reconstruction_loss = CDL(labels, decoded)
    else:
        decoded = self._decoder(embedded)
</code></pre>
<p>How do I do this? How do I ensure that when the model is validated, but labels are supplied that the model doesn't pass the validation labels to the encoder (i.e. if this happens the validation isn't testing how well the embedded replicates the encoder)?</p>
","2981639","","2981639","","2021-10-02 22:31:04","2021-10-03 00:04:16","Correct validation of allennlp auto-encoder","<machine-learning><autoencoder><allennlp>","1","0","","","","CC BY-SA 4.0"
"60499791","1","60510837","","2020-03-03 03:46:55","","1","281","<p>Using <code>NLTK</code> Unigram Tagger, I am training sentences in <code>Brown Corpus</code></p>

<p>I try different <code>categories</code> and I get about the same value. The value is around <code>0.9328</code>... for each <code>categories</code> such as <code>fiction</code>, <code>romance</code> or <code>humor</code></p>

<pre><code>from nltk.corpus import brown


# Fiction    
brown_tagged_sents = brown.tagged_sents(categories='fiction')
brown_sents = brown.sents(categories='fiction')
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
unigram_tagger.evaluate(brown_tagged_sents)
&gt;&gt;&gt; 0.9415956079897209

# Romance
brown_tagged_sents = brown.tagged_sents(categories='romance')
brown_sents = brown.sents(categories='romance')
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
unigram_tagger.evaluate(brown_tagged_sents)
&gt;&gt;&gt; 0.9348490474422324
</code></pre>

<p>Why is it that the case? is it because they are from the same <code>corpus</code>? or are their <code>part-of-speech</code> tagging is the same?</p>
","9161607","","9161607","","2020-03-03 04:06:14","2020-03-03 15:45:19","Unigram tagging in NLTK","<nlp><nltk><stanford-nlp><allennlp>","1","2","","","","CC BY-SA 4.0"
"50043826","1","","","2018-04-26 13:08:52","","0","1536","<p>I am getting this error while importing allennlp,</p>

<pre><code>from allennlp.common.util import sanitize
ModuleNotFoundError: No module named 'allennlp.common'
(venv-kbs) administrator@NLR:~/aman/Project$ python
Python 3.6.3 (default, Oct  6 2017, 00:00:00)
[GCC 4.8.4] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import allennlp
&gt;&gt;&gt; from allennlp.common.util import sanitize
Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ModuleNotFoundError: No module named 'allennlp.common'
</code></pre>

<p>My allennlp version is 0.2.1 and even trying to update to 0.4.1 it gives the same error. </p>

<p>TIA</p>
","8922846","","7483494","","2019-08-22 19:04:21","2019-08-22 19:04:21","ModuleNotFoundError: No module named 'allennlp.common'","<python-3.x><nlp><allennlp>","1","0","1","","","CC BY-SA 3.0"
"69597860","1","","","2021-10-16 17:10:51","","0","15","<p>Does anyone know how to change the tokenizer in <a href=""https://demo.allennlp.org/coreference-resolution"" rel=""nofollow noreferrer"">AllenNLP's coreference resolution</a>? By default, it uses SpaCy and I would like to use a white space tokenizer so as to tokenize only words, not punctuation.</p>
<p>This is what I have tried so far but it does not seem to work:</p>
<pre><code>review = &quot;&quot;&quot;Judging from previous posts this used to be a good place, but not any longer.
        We, there were four of us, arrived at noon - the place was empty - 
        and the staff acted like we were imposing on them and they were very rude. 
        They never brought us complimentary noodles, ignored repeated requests for sugar, and threw our dishes on the table.
        The food was lousy - too sweet or too salty and the portions tiny.
        After all that, they complained to me about the small tip.
        Avoid this place!&quot;&quot;&quot;

from allennlp.data.tokenizers.whitespace_tokenizer import WhitespaceTokenizer
from allennlp.predictors.predictor import Predictor

predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz&quot;)
predictor._tokenizer = WhitespaceTokenizer()

pred = predictor.predict(document=review)

# expected output: 'Judging', 'from', 'previous', 'posts', 'this', 'used', 'to', 'be', 'a', 'good', 'place,', 'but', 'not', 'any', 'longer.'
print(pred['document'])
</code></pre>
<p>I found the documentation on tokenizers <a href=""http://docs.allennlp.org/main/api/data/tokenizers/whitespace_tokenizer/"" rel=""nofollow noreferrer"">here</a>, but I don't know if it is possible to use them on other models like on coreference resolution.</p>
","17169225","","","","","2021-10-16 17:10:51","Is there a way to change the tokenizer in AllenNLP's coreference resolution model?","<nlp><tokenize><allennlp><coreference-resolution>","0","1","","","","CC BY-SA 4.0"
"69178847","1","69196743","","2021-09-14 13:37:00","","1","41","<p>The goal is to train <a href=""https://paperswithcode.com/lib/allennlp/srl-bert"" rel=""nofollow noreferrer"">BERT SRL</a> on another data set. According to <a href=""https://raw.githubusercontent.com/allenai/allennlp-models/v2.1.0/training_config/structured_prediction/bert_base_srl.jsonnet"" rel=""nofollow noreferrer"">configuration</a>, it requires <code>conll-formatted-ontonotes-5.0</code>.</p>
<p>Natively, my data comes in a CoNLL format and I converted it to the conll-formatted-ontonotes-5.0 format of the <a href=""https://github.com/ontonotes/conll-formatted-ontonotes-5.0"" rel=""nofollow noreferrer"">GitHub edition of OntoNotes v.5.0</a>. Reading the data works and training <em>seems</em> to work, except that precision remains at 0. I suspect that either the encoding of SRL arguments (BOI or phrasal?) or the column structure (other OntoNotes editions in CoNLL format differ here) differ from the expected input. Alternatively, the error may arise because if the role labels are hard-wired in the code. I followed the reference data in using the long form (<code>ARGM-TMP</code>), but you often see the short form (<code>AM-TMP</code>) in other data.</p>
<p>The question is <em>which</em> dataset and format is expected here. I guess it's one of the CoNLL/Skel formats for OntoNotes 5.0 with a restored WORD column, <em>but</em></p>
<ul>
<li><p>The CoNLL edition doesn't seem to be shipped with the <a href=""https://catalog.ldc.upenn.edu/LDC2013T19"" rel=""nofollow noreferrer"">LDC edition of OntoNotes</a></p>
</li>
<li><p>It does not seem to be the format of the &quot;conll-formatted-ontonotes-5.0&quot; edition of OntoNotes v.5.0 on <a href=""https://github.com/ontonotes/conll-formatted-ontonotes-5.0"" rel=""nofollow noreferrer"">GitHub</a> provided by the OntoNotes creators.</p>
</li>
<li><p>There is at least one other CoNLL/Skel edition of OntoNotes 5.0 data as part of <a href=""https://github.com/propbank/propbank-release/blob/master/data/ontonotes/nw/wsj/00/wsj_0001.gold_skel"" rel=""nofollow noreferrer"">PropBank</a>. This differs from the other one in leaving out 3 columns and in the encoding of predicates. (For parts of my data, this is the native format.)</p>
</li>
<li><p>The <a href=""https://docs.allennlp.org/models/main/models/structured_prediction/dataset_readers/srl/"" rel=""nofollow noreferrer"">SrlReader</a> documentation mentions BIO (IOBES) encoding. This has been used in other CoNLL editions of PropBank data, indeed, but <em>not</em> in the above-mentioned OntoNotes corpora. Other such formats are the CoNLL-2008 and CoNLL-2009 formats, for example, and different variants.</p>
</li>
</ul>
<p>Before I start reverse-engineering the SrlReader, does anyone have a data snippet at hand so that I can prepare my data accordingly?</p>
<p><a href=""https://github.com/ontonotes/conll-formatted-ontonotes-5.0"" rel=""nofollow noreferrer""><code>conll-formatted-ontonotes-5.0</code></a> version of my data (sample from EWT corpus):</p>
<pre><code>google/ewt/answers/00/20070404104007AAY1Chs_ans.xml 0   0   where   WRB (TOP(S(SBARQ(WHADVP*)   -   -   -   -   *   (ARGM-LOC*) *   *   -
google/ewt/answers/00/20070404104007AAY1Chs_ans.xml 0   1   can MD  (SQ*    -   -   -   -   *   (ARGM-MOD*) *   *   -
google/ewt/answers/00/20070404104007AAY1Chs_ans.xml 0   2   I   PRP (NP*)   -   -   -   -   *   (ARG0*) *   *   -
google/ewt/answers/00/20070404104007AAY1Chs_ans.xml 0   3   get VB  (VP*    get 01  -   -   *   (V*)    *   *   -
google/ewt/answers/00/20070404104007AAY1Chs_ans.xml 0   4   morcillas   NNS (NP*)   -   -   -   -   *   (ARG1*) *   *   -
</code></pre>
","6427741","","6427741","","2021-09-14 13:52:54","2021-09-15 16:27:04","AllenNLP BERT SRL input format (""OntoNotes v. 5.0 formatted"")","<allennlp><conll><srl>","1","2","","","","CC BY-SA 4.0"
"69599858","1","","","2021-10-16 22:03:32","","0","17","<p>I'm working on a project where I need to fine-tune <a href=""https://github.com/allenai/allennlp-models/blob/main/allennlp_models/modelcards/pair-classification-roberta-snli.json"" rel=""nofollow noreferrer"">pair-classification-roberta-snli</a> model offered by AllenNLP. I have prepared my custom dataset in the snli format but couldn't manage to find a way to retrain the model. Currently, I am following <a href=""https://github.com/dh1105/Sentence-Entailment"" rel=""nofollow noreferrer"">this approach</a> to train bert-base model for textual entailment. But how to fine-tune AllenNLP's pair-classification-roberta-snli model?</p>
","17170686","","15993687","","2021-10-18 03:20:05","2021-10-18 03:20:05","How to fine-tune Allennlp's RoBERTa text entailment model on custom data?","<nlp><bert-language-model><allennlp><roberta-language-model>","0","1","","","","CC BY-SA 4.0"
"68962384","1","","","2021-08-28 07:50:17","","0","31","<p>I'm new to AllenNlp.
Please help me know how to load a model from Huggingface into predictor.</p>
<p>I'm trying to let AllenNlp load HuggingFace Transformers model.</p>
<p>When I try the following sample code, I face the error after the download:</p>
<pre><code>from allennlp.predictors.predictor import Predictor


predictor = Predictor.from_path(&quot;hf://bert-large-uncased-whole-word-masking-finetuned-squad&quot;)
</code></pre>
<pre><code>2021-08-28 15:48:44,371 - INFO - allennlp.models.archival - loading archive file hf://bert-large-uncased-whole-word-masking-finetuned-squad from cache at /home/user/.allennlp/cache/bert-large-uncased-whole-word-masking-finetuned-squad.242d9dbb66bb5033025196d5678907307f8fb098
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/tmp/ipykernel_20988/3906614977.py in &lt;module&gt;
      2 
      3 
----&gt; 4 predictor = Predictor.from_path(&quot;hf://bert-large-uncased-whole-word-masking-finetuned-squad&quot;)

~/data/anaconda3/envs/py37/lib/python3.7/site-packages/allennlp/predictors/predictor.py in from_path(cls, archive_path, predictor_name, cuda_device, dataset_reader_to_load, frozen, import_plugins, overrides, **kwargs)
    364             plugins.import_plugins()
    365         return Predictor.from_archive(
--&gt; 366             load_archive(archive_path, cuda_device=cuda_device, overrides=overrides),
    367             predictor_name,
    368             dataset_reader_to_load=dataset_reader_to_load,

~/data/anaconda3/envs/py37/lib/python3.7/site-packages/allennlp/models/archival.py in load_archive(archive_file, cuda_device, overrides, weights_file)
    223         # Instantiate model and dataset readers. Use a duplicate of the config, as it will get consumed.
    224         dataset_reader, validation_dataset_reader = _load_dataset_readers(
--&gt; 225             config.duplicate(), serialization_dir
    226         )
    227         model = _load_model(config.duplicate(), weights_path, serialization_dir, cuda_device)

~/data/anaconda3/envs/py37/lib/python3.7/site-packages/allennlp/models/archival.py in _load_dataset_readers(config, serialization_dir)
    255     # to the default dataset_reader used for both training and validation.
    256     validation_dataset_reader_params = config.get(
--&gt; 257         &quot;validation_dataset_reader&quot;, dataset_reader_params.duplicate()
    258     )
    259 

AttributeError: 'NoneType' object has no attribute 'duplicate'
</code></pre>
","12528853","","10871073","","2021-08-30 03:08:57","2021-08-30 03:08:57","AttributeError: 'NoneType' object has no attribute 'duplicate' when calling Predictor.from_path","<allennlp>","0","0","","","","CC BY-SA 4.0"
"68972008","1","","","2021-08-29 10:32:08","","0","11","<p>My intuition is that <code>TokenEmbedder</code> generates the token (words or subwords) representations, and its output would be used for <code>Seq2VecEncoder</code>.
However, <code>[CLS]</code> is used for classification during the training (e.g., Next Sentence Prediction ).
Therefore, it would be reasonable for me to exclude the first representation for <code>[CLS]</code> or, at least, adding an option in <code>PretrainedTransformerEmbedder</code> to do this.</p>
<p>This is just my intuitive thought. Hope to get more ideas and criticisms, especially the effect of including <code>[CLS]</code> representation?</p>
","11561452","","","","","2021-08-29 10:32:08","Why AllenNLP `PretrainedTransformerEmbedder` outputs all `last_hidden_state` including the first `[CLS]`?","<bert-language-model><allennlp>","0","0","","","","CC BY-SA 4.0"
"69637920","1","","","2021-10-19 21:37:14","","0","9","<pre><code>    archive = load_archive(
    &quot;elmo-constituency-parser-2018.03.14.tar.gz&quot;
)
predictor = Predictor.from_archive(archive, 'constituency-parser')

predictor.predict_json({&quot;sentence&quot;: &quot;This is a sentence to be predicted!&quot;})
</code></pre>
<p>Loading the elmo-constituency-parser is thorwing this error:
allennlp.common.checks.ConfigurationError: ptb_trees not in acceptable choices for</p>
<blockquote>
<p>dataset_reader.type: ['babi', 'conll2003', 'interleaving', 'multitask', 'sequence_tagging', 'sharded', 'text_classification_json', 'multitask_shim']. You should either use the --include-package flag to make sure the correct module is loaded, or use a fully qualified class name in your config file like {&quot;model&quot;: &quot;my_module.models.MyModel&quot;} to have it imported automatically.</p>
</blockquote>
<p>Seems the load_archive func returnd a model name &quot;ptb_trees&quot; and a name containd &quot;.&quot; was required, such as {&quot;model&quot;: &quot;my_module.models.MyModel&quot;}</p>
<p>Any suggestion? Thanks!</p>
","17196034","","","","","2021-10-19 21:37:14","Predictor.from_archive failed","<allennlp>","0","1","","","","CC BY-SA 4.0"
"69670615","1","","","2021-10-22 02:01:12","","0","32","<p>I am playing with the old v0.9.0 and the latest version here <a href=""https://guide.allennlp.org/representing-text-as-features#6"" rel=""nofollow noreferrer"">https://guide.allennlp.org/representing-text-as-features#6</a>.</p>
<p>I find that the output of the command:</p>
<pre><code>allennlp elmo input_file.txt output_file.hdf5 --all
</code></pre>
<p>in v0.9.0 is different from that of ElmoTokenEmbedder of the latest version. (For sure with the same model weights and options)</p>
<p>Specifically, in ElmoTokenEmbedder, there exists some parameters that are not in the old allennlp elmo command. For example</p>
<pre><code>do_layer_norm , dropout
</code></pre>
<p>My question is how should I set up the parameters in ElmoTokenEmbedder so that I can get the same output as the old allennlp elmo?</p>
<p>Here are the github pages that might be helpful:
<code>allennlp elmo</code> command:   <a href=""https://github.com/allenai/allennlp/commit/08874e95026e1364dbe116752a7c0664af052965#diff-3f976d2f3e0e2993f1ee72b1c5ce446e12d8e01e4a07382f84970be1a502f9d5L97"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp/commit/08874e95026e1364dbe116752a7c0664af052965#diff-3f976d2f3e0e2993f1ee72b1c5ce446e12d8e01e4a07382f84970be1a502f9d5L97</a></p>
<p>ElmoTokenEmbedder:
<a href=""https://github.com/allenai/allennlp/blob/main/allennlp/modules/token_embedders/elmo_token_embedder.py"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp/blob/main/allennlp/modules/token_embedders/elmo_token_embedder.py</a></p>
","15760525","","5083558","","2021-10-22 14:49:59","2021-10-22 14:49:59","Difference between allennlp elmo command and ElmoTokenEmbedder class","<nlp><allennlp><elmo>","0","0","","","","CC BY-SA 4.0"
"52920419","1","52920505","","2018-10-21 22:20:08","","1","501","<p>Which versions of  the python packages <code>sanic-cors</code>, <code>sanic</code>, and <code>sanic-plugins-framework</code> should I use when installing <code>allennlp==0.3.0</code> with Python 3.6 or 3.7?</p>

<p>When I run:</p>

<pre><code>conda create --name selector-py36 python=3.6 
source activate selector-py36
pip install allennlp==0.3.0
</code></pre>

<p>I get the error message:</p>

<pre><code>sanic-plugins-framework 0.6.3.dev20180717 has requirement sanic&gt;=0.7.0, but you'll have sanic 0.6.0 which is incompatible.
sanic-cors 0.9.6 has requirement sanic&gt;=0.8.1, but you'll have sanic 0.6.0 which is incompatible.
</code></pre>

<p>which later on prevents my program relying on allennlp from running (<code>RuntimeError: You cannot use Sanic-CORS with sanic &lt; v0.7.0</code>):</p>

<pre><code>(selector-py36) dernon@ilcompn0:/mnt/ilcompnnco/selector$ 
      python -m allennlp.run train data/allen_config_test.json --serialization-dir scratch
Traceback (most recent call last):
  File ""/mnt/ilcompnnco/anaconda3/envs/sensei-selector-py36b/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/mnt/ilcompnnco/anaconda3/envs/sensei-selector-py36b/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/mnt/ilcompnnco/anaconda3/envs/sensei-selector-py36b/lib/python3.6/site-packages/allennlp/run.py"", line 10, in &lt;module&gt;
    from allennlp.commands import main  # pylint: disable=wrong-import-position
  File ""/mnt/ilcompnnco/anaconda3/envs/sensei-selector-py36b/lib/python3.6/site-packages/allennlp/commands/__init__.py"", line 6, in &lt;module&gt;
    from allennlp.commands.serve import Serve
  File ""/mnt/ilcompnnco/anaconda3/envs/sensei-selector-py36b/lib/python3.6/site-packages/allennlp/commands/serve.py"", line 25, in &lt;module&gt;
    from allennlp.service import server_sanic
  File ""/mnt/ilcompnnco/anaconda3/envs/sensei-selector-py36b/lib/python3.6/site-packages/allennlp/service/server_sanic.py"", line 19, in &lt;module&gt;
    from sanic_cors import CORS
  File ""/mnt/ilcompnnco/anaconda3/envs/sensei-selector-py36b/lib/python3.6/site-packages/sanic_cors/__init__.py"", line 11, in &lt;module&gt;
    from .decorator import cross_origin
  File ""/mnt/ilcompnnco/anaconda3/envs/sensei-selector-py36b/lib/python3.6/site-packages/sanic_cors/decorator.py"", line 15, in &lt;module&gt;
    from .extension import cors
  File ""/mnt/ilcompnnco/anaconda3/envs/sensei-selector-py36b/lib/python3.6/site-packages/sanic_cors/extension.py"", line 326, in &lt;module&gt;
    instance = cors = CORS()
  File ""/mnt/ilcompnnco/anaconda3/envs/sensei-selector-py36b/lib/python3.6/site-packages/sanic_cors/extension.py"", line 139, in __init__
    raise RuntimeError(""You cannot use Sanic-CORS with sanic &lt; v0.7.0"")
RuntimeError: You cannot use Sanic-CORS with sanic &lt; v0.7.0
</code></pre>
","395857","","395857","","2018-10-21 22:27:57","2018-11-03 22:38:49","Which versions of sanic-cors, sanic, and sanic-plugins-framework should I use when installing allennlp==0.3.0?","<python><pip><sanic><allennlp>","1","0","","","","CC BY-SA 4.0"
"65013129","1","65261292","","2020-11-25 21:44:25","","1","154","<p>Simple sentences involving the verb, &quot;is&quot; return no results for semantic role labeling, either via the demo page or by using AllenNLP in Python3.8 with the latest November Bert base model.</p>
<p>For example, &quot;I am here.&quot; returns nothing.</p>
<p>In short:</p>
<ul>
<li>Instances of simple &quot;A is B&quot; sentences don't return any results.</li>
<li>I believe there should be some sort of output, as other SRL engines do return results.</li>
<li>The same goes for &quot;I am.&quot; The expected result is an ARG1 for &quot;I&quot; and a predicate of &quot;am.&quot;</li>
</ul>
<p>This used to work with an earlier version:</p>
<pre><code>allennlp==1.0.0
allennlp-models==1.0.0
</code></pre>
<h2>Related issues or possible duplicates</h2>
<ul>
<li>None</li>
</ul>
<h2>Environment</h2>

<p>OS: macOS 10.15.7 (Catalina)</p>

<p>Python version: 3.8.6 (via home-brew)</p>

<pre><code>allennlp==1.2.2
allennlp-models==1.2.2
attrs==20.3.0
blis==0.4.1
boto3==1.16.24
botocore==1.19.24
catalogue==1.0.0
certifi==2020.11.8
chardet==3.0.4
click==7.1.2
conllu==4.2.1
cymem==2.0.4
dataclasses==0.6
filelock==3.0.12
ftfy==5.8
future==0.18.2
h5py==3.1.0
idna==2.10
importlib-metadata==3.1.0
iniconfig==1.1.1
jmespath==0.10.0
joblib==0.17.0
jsonnet==0.17.0
jsonpickle==1.4.1
murmurhash==1.0.4
nltk==3.5
numpy==1.19.4
overrides==3.1.0
packaging==20.4
plac==1.1.3
pluggy==0.13.1
preshed==3.0.4
protobuf==3.14.0
py==1.9.0
py-rouge==1.1
pyparsing==2.4.7
pytest==6.1.2
python-dateutil==2.8.1
regex==2020.11.13
requests==2.25.0
s3transfer==0.3.3
sacremoses==0.0.43
scikit-learn==0.23.2
scipy==1.5.4
sentencepiece==0.1.91
six==1.15.0
spacy==2.3.2
srsly==1.0.4
tensorboardX==2.1
thinc==7.4.1
threadpoolctl==2.1.0
tokenizers==0.9.3
toml==0.10.2
torch==1.7.0
tqdm==4.53.0
transformers==3.5.1
typing-extensions==3.7.4.3
urllib3==1.26.2
wasabi==0.8.0
wcwidth==0.2.5
word2number==1.1
zipp==3.4.0
</code></pre>
</p>

<h2>Steps to reproduce</h2>
<p><a href=""https://demo.allennlp.org/semantic-role-labeling"" rel=""nofollow noreferrer"">Visit the demo website for SRL</a></p>

<b>Example:</b>
<p>Enter almost any variation of:
&quot;I am here.&quot;
&quot;We are people.&quot;
&quot;I am.&quot;</p>
<p>

<pre><code>#  https://demo.allennlp.org/semantic-role-labeling/MjU3NDk3NA==

# or
from allennlp.predictors.predictor import Predictor

allen_predictor_srl = Predictor.from_path(
    &quot;./models/bert-base-srl-2020.11.19.tar.gz&quot;
)

output = allen_predictor_srl.predict(sentence=&quot;I am here.&quot;)
print(output)

# observe nothing
</code></pre>
</p>

<p>EDIT:
I tried installing the exact library versions I used to have (i.e. AllenNLP 1.0), but the issue persists. I frankly have no idea what is going on.</p>
<p>EDIT 2:</p>
<p>Install script:</p>
<pre><code>python3 -m venv env
source ./env/bin/activate


pip3 install --upgrade pip

pip3 install -U --no-cache-dir

pip3 install -U allennlp allennlp-models --no-cache-dir

python3 -m spacy download en_core_web_lg    --no-cache-dir
python3 -m spacy download en_core_web_sm    --no-cache-dir
python3 -m spacy download en_vectors_web_lg --no-cache-dir
python3 -m spacy download de_core_news_md   --no-cache-dir
</code></pre>
","7361580","","7361580","","2020-11-25 22:40:56","2020-12-12 03:21:23","AllenNLP fails on ""to be"" sentences. Something Broke along the Way","<python><allennlp>","1","11","","","","CC BY-SA 4.0"
"54000564","1","54030515","","2019-01-02 02:29:13","","2","5854","<p>I am trying to learn how to use Elmo embeddings via this tutorial:</p>

<p><a href=""https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md</a></p>

<p>I am specifically trying to use the interactive mode as described like this:</p>

<pre><code>$ ipython
&gt; from allennlp.commands.elmo import ElmoEmbedder
&gt; elmo = ElmoEmbedder()
&gt; tokens = [""I"", ""ate"", ""an"", ""apple"", ""for"", ""breakfast""]
&gt; vectors = elmo.embed_sentence(tokens)

&gt; assert(len(vectors) == 3) # one for each layer in the ELMo output
&gt; assert(len(vectors[0]) == len(tokens)) # the vector elements 
correspond with the input tokens

&gt; import scipy
&gt; vectors2 = elmo.embed_sentence([""I"", ""ate"", ""a"", ""carrot"", ""for"", 
""breakfast""])
&gt; scipy.spatial.distance.cosine(vectors[2][3], vectors2[2][3]) # cosine 
distance between ""apple"" and ""carrot"" in the last layer
0.18020617961883545
</code></pre>

<p>My overall question is how do I make sure to use the pre-trained elmo model on the original 5.5B set (described here: <a href=""https://allennlp.org/elmo"" rel=""nofollow noreferrer"">https://allennlp.org/elmo</a>)?</p>

<p>I don't quite understand why we have to call ""assert"" or why we use the [2][3] indexing on the vector output. </p>

<p>My ultimate purpose is to average the all the word embeddings in order to get a sentence embedding, so I want to make sure I do it right! </p>

<p>Thanks for your patience as I am pretty new in all this.</p>
","3325586","","7483494","","2019-08-22 19:25:30","2019-08-22 19:25:30","How to use Elmo word embedding with the original pre-trained model (5.5B) in interactive mode","<python><machine-learning><nlp><artificial-intelligence><allennlp>","1","0","0","","","CC BY-SA 4.0"
"69658880","1","","","2021-10-21 08:56:42","","0","16","<p>I am trying to implement one research paper. While running this code (<a href=""https://github.com/abubakar-ucr/LEONA/blob/master/model/end2end_model.py"" rel=""nofollow noreferrer"">https://github.com/abubakar-ucr/LEONA/blob/master/model/end2end_model.py</a>), I keept getting this error. The error occurs at line number 36.</p>
<p>Traceback (most recent call last):
File &quot;end2end_model.py&quot;, line 36, in 
from allennlp.data.dataset_readers.dataset_reader import AllennlpDataset
ImportError: cannot import name 'AllennlpDataset' from 'allennlp.data.dataset_readers.dataset_reader'</p>
","17209422","","17209422","","2021-10-22 09:10:28","2021-10-22 09:10:28","cannot import name 'AllennlpDataset' from 'allennlp.data.dataset_readers.dataset_reader'","<python-3.x><nlp><chatbot><opennlp><allennlp>","0","4","","","","CC BY-SA 4.0"
"63643262","1","63717195","","2020-08-29 03:55:42","","0","276","<p>i am trying to use flair nlp framework ,</p>
<p><a href=""https://github.com/flairNLP/flair"" rel=""nofollow noreferrer"">https://github.com/flairNLP/flair</a></p>
<p>but getting error</p>
<pre class=""lang-py prettyprint-override""><code># load the NER tagger
tagger = SequenceTagger.load('ner')
</code></pre>
<p>tried on local , remote and other machine . all failing as its not able to download a model from s3 .google colab also gives same error .</p>
<p>all give error message as:</p>
<pre><code>OSError: HEAD request failed for url https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/NER-conll03-english/en-ner-conll03-v0.4.pt with status code 301.
</code></pre>
<p>.</p>
","5284326","","235019","","2020-08-29 19:49:20","2020-09-03 05:28:37","flair import failing on mac ubuntu and google colab","<opennlp><allennlp><flair>","1","0","","","","CC BY-SA 4.0"
"62980509","1","","","2020-07-19 12:49:23","","0","485","<p>I want to do Named Entity Recognition on scientific articles.</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_cased')
model=TFBertForTokenClassification.from_pretrained(&quot;allenai/scibert_scivocab_uncased&quot;,from_pt=True,config=config)
</code></pre>
<p>But it gives following warning.</p>
<blockquote>
<p>Some weights of the PyTorch model were not used when initializing the
TF 2.0 model TFBertForTokenClassification: ['classifier.weight',
'classifier.bias'] - This IS expected if you are initializing
TFBertForTokenClassification from a TF 2.0 model trained on another
task or with another architecture (e.g. initializing a
BertForSequenceClassification model from a TFBertForPretraining
model). - This IS NOT expected if you are initializing
TFBertForTokenClassification from a TF 2.0 model that you expect to be
exactly identical (initializing a BertForSequenceClassification model
from a TFBertForSequenceClassification model). Some weights or buffers
of the PyTorch model TFBertForTokenClassification were not initialized
from the TF 2.0 model and are newly initialized:
['cls.predictions.transform.dense.weight',
'cls.predictions.decoder.bias',
'cls.predictions.transform.LayerNorm.weight',
'cls.predictions.transform.LayerNorm.bias',
'cls.predictions.decoder.weight', 'cls.seq_relationship.weight',
'cls.predictions.bias', 'cls.predictions.transform.dense.bias',
'cls.seq_relationship.bias'] You should probably TRAIN this model on a
down-stream task to be able to use it for predictions and inference.</p>
</blockquote>
<p>And when I try to fit the model, it gives follwing warning.</p>
<blockquote>
<p>WARNING:tensorflow:Gradients do not exist for variables
['tf_bert_for_token_classification/bert/pooler/dense/kernel:0',
'tf_bert_for_token_classification/bert/pooler/dense/bias:0'] when
minimizing the loss.</p>
</blockquote>
<p>Does anyone know why it is giving these warnings?</p>
","13958024","","6664872","","2020-07-19 13:20:07","2020-07-19 13:20:07","Why TFBertForTokenClassification.from_pretrained(""allenai/scibert_scivocab_uncased"",from_pt=True,config=config) gives weights warning?","<pytorch><tensorflow2.0><huggingface-transformers><bert-language-model><allennlp>","0","3","","","","CC BY-SA 4.0"
"56244953","1","","","2019-05-21 19:07:58","","3","783","<p>I have a homework which requires me to build an algorithm that can guess a missing word from a sentence. For example, when the input sentence is : "" I took my **** for a walk this morning"" , I want the output to guess the missing word(dog). My assignment requires me to train my own model from scratch. I built my corpus which has about 500.000 sentences. I cleaned the corpus. It is all lower-case and every sentence is seperated with a new line (\n) character. I also have vocabulary.txt file which lists all the unique words in descending order in frequency. The vocabulary file starts with the first 3 line 'S', '/S' and 'UNK' (these 3 tokens are surrounded with &lt;> in vocabulary.txt but using &lt;> in this website hides the characters between them for some reason) . I also have a small set of sentences with one missing word in every sentence which is denoted with [MASK], one sentence per line.</p>

<p>I followed the instructions in the <a href=""https://github.com/allenai/bilm-tf"" rel=""nofollow noreferrer"">https://github.com/allenai/bilm-tf</a> ,which provides steps to train your own model from scratch using Elmo.</p>

<p>After gathering the data.txt and vocabulary file, I used the</p>

<pre><code>python bin/train_elmo.py --train_prefix= &lt;path to training folder&gt; --vocab_file &lt;path to vocab file&gt; --save_dir &lt;path where models will be checkpointed&gt;`
</code></pre>

<p>and trained my corpus with tensorflow and CUDA enabled gpu.</p>

<p>After the training is finished, I used the following command:</p>

<pre><code>python bin/dump_weights.py --save_dir /output_path/to/checkpoint --outfile/output_path/to/weights.hdf5
</code></pre>

<p>Which gave me the weights.hdf5 and options.json files. The only warning I received while training my model is : </p>

<pre><code>WARNING : Error encountered when serializing lstm_output_embeddings.Type is unsupported, or the types of the items don't match field type in CollectionDef. 'list' object has no attribute 'name'
</code></pre>

<p>which was mentioned in the AllenAI repo as harmless. So it is safe to assume that the model training phase is finished correctly. My problem is, I have no idea what to do after this point. In this stackOverflow link <a href=""https://stackoverflow.com/questions/54978443/predicting-missing-words-in-a-sentence-natural-language-processing-model"">Predicting Missing Words in a sentence - Natural Language Processing Model</a>, the answer states that the following code can be used to predict the missing word:</p>

<pre><code>import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel,BertForMaskedLM

# OPTIONAL: if you want to have more information on what's happening,activate the logger as follows
import logging
logging.basicConfig(level=logging.INFO)

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = '[CLS] I want to [MASK] the car because it is cheap . [SEP]'
tokenized_text = tokenizer.tokenize(text)
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

# Create the segments tensors.
segments_ids = [0] * len(tokenized_text)

# Convert inputs to PyTorch tensors
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])

# Load pre-trained model (weights)
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval()

# Predict all tokens
with torch.no_grad():
predictions = model(tokens_tensor, segments_tensors)

predicted_index = torch.argmax(predictions[0, masked_index]).item()
predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]

print(predicted_token)
</code></pre>

<p>Unfortunately, the code above is for Bert models. My assignment requires me to use Elmo models. I tried to find a library similiar to <strong>pytorch_pretrained_bert</strong> for Elmo but I couldn't find anything. What can I do to predict the masked words using my Elmo model?</p>

<p>Thanks.</p>
","7795205","","7795205","","2019-05-21 19:23:28","2019-05-21 19:23:28","Using Elmo models to predict the masked word in a sentence","<python><machine-learning><nlp><allennlp><elmo>","0","2","","","","CC BY-SA 4.0"
"69341381","1","","","2021-09-27 04:53:59","","0","31","<p>I am using OPENIE6 (<a href=""https://github.com/dair-iitd/openie6"" rel=""nofollow noreferrer"">https://github.com/dair-iitd/openie6</a>) with the following input:-</p>
<p><strong>President Trump met the leaders of India and China.</strong></p>
<p>But I am getting only one triplet:-</p>
<pre><code>ARG1 = President trump
V = met
ARG2 = the leaders of India and China.
</code></pre>
<p>Instead, as mentioned in the documentation and demos, there should be two triplets:-</p>
<pre><code>ARG1 = President trump
V = met,
ARG2 = the leaders of India.

ARG1 = President trump
V = met
ARG2 = the leaders of China.
</code></pre>
<p>Can anyone help, what is the exact issue?</p>
","7434281","","7434281","","2021-09-30 04:38:37","2021-09-30 04:38:37","Conjunction issue in OPENIE 6","<stanford-nlp><transformer><allennlp><triples><knowledge-graph>","0","0","","","","CC BY-SA 4.0"
"63195266","1","65643391","","2020-07-31 16:00:47","","1","124","<p>I want to build a Q&amp;A bot with allennlp. I already found the code for it and an English model - I want to use the bot with german. I already searched for it, but I only found <a href=""https://github.com/t-systems-on-site-services-gmbh/german-elmo-model"" rel=""nofollow noreferrer"">this</a> model which is unfortunately not compatible with allennlp. Do you know about any compatible german elmo models, or is there a more common alternative to allennlp which has more compatible models?</p>
<p>This is the code I found:</p>
<pre><code>from allennlp.predictors.predictor import Predictor
import allennlp_models.rc

predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/bidaf-elmo-model-2020.03.19.tar.gz&quot;)

result = predictor.predict(
  passage=&quot;Steve Jobs died in 2011. He was born in 1955. He has multiple children.&quot;,
  question=&quot;When did Steve Jobs die?&quot;
)
print(result['best_span_str'])
</code></pre>
","13693395","","","","","2021-01-09 13:48:28","Allennlp german elmo model","<python><elmo><allennlp>","1","0","1","","","CC BY-SA 4.0"
"60981545","1","60982213","","2020-04-01 22:51:00","","1","669","<p>I'm building a Flask test predictor using AllenNLP. </p>

<p>I'm passing 'passage' and 'question' from a .json file to the predictor. </p>

<p>However, when I pass the json file using curl, it doesn't return a response.  Is there a special return in Flask to get it?</p>

<p>Code looks like:</p>

<pre><code> from allennlp.predictors.predictor import Predictor as AllenNLPPredictor


from flask import Flask
from flask import request
app = Flask(__name__)

@app.route(""/"", methods=['GET','POST'])
def hello():
    return ""&lt;h1&gt;Test app!&lt;/h1&gt;""


class PythonPredictor:
    def __init__(self, config):
        self.predictor = AllenNLPPredictor.from_path(
            ""https://storage.googleapis.com/allennlp-public-models/bidaf-elmo-model-2018.11.30-charpad.tar.gz""
        )

    def predict(self, payload):
        if request.method == ""POST"":
            prediction = self.predictor.predict(
                passage=payload[""passage""], question=payload[""question""]
            )
            return prediction[""best_span_str""]
</code></pre>

<p>Curl command looks like:
curl <a href=""http://127.0.0.1:5000"" rel=""nofollow noreferrer"">http://127.0.0.1:5000</a> -X POST -H ""Content-Type: application/json"" -d @sample.json</p>
","921561","","921561","","2020-04-01 23:26:02","2020-04-02 00:00:11","How do you get a response from a Flask app with curl?","<api><flask><allennlp>","1","2","","","","CC BY-SA 4.0"
"55254845","1","","","2019-03-20 06:39:04","","2","748","<p>How can I train the <a href=""https://demo.allennlp.org/semantic-role-labeling/NjU2MjA3"" rel=""nofollow noreferrer"">semantic role labeling model in AllenNLP</a>?</p>

<p>I am aware of the <a href=""https://allenai.github.io/allennlp-docs/api/allennlp.training.trainer.html"" rel=""nofollow noreferrer""><code>allennlp.training.trainer</code></a> function but I don't know how to use it to train the semantic role labeling model.</p>

<p>Let's assume that the training samples are BIO tagged, e.g.:</p>

<pre><code>Remove B_O
the B_ARG1
fish I_ARG1
in B_LOC
the I_LOC 
background I_LOC 
</code></pre>
","395857","","395857","","2019-03-20 06:49:24","2019-04-10 02:59:54","How can I train the semantic role labeling model in AllenNLP?","<python><nlp><allennlp>","1","0","3","","","CC BY-SA 4.0"
"69527860","1","","","2021-10-11 14:26:53","","-1","15","<p>I have trained a T5 model on a specific dataset for the purpose of keyword extraction. I wish to use Allen NLP Interpret to know various saliency mappings for the inputs given to my model. Where do I make changes such that I can use the package.</p>
","10680920","","10680920","","2021-10-17 07:01:50","2021-10-17 07:01:50","How to use AllenNLP interpret on finetuned t5 model","<huggingface-transformers><allennlp>","1","1","","","","CC BY-SA 4.0"
"69656942","1","","","2021-10-21 06:31:25","","0","12","<p>I am trying to load a local copy of the <code>coref-spanbert</code> model using <code>Predictor.from_path</code> but it starts downloading the model again into cache/huggingface. Can anyone help me to fix this.</p>
<pre><code>&gt;&gt;&gt; from allennlp.predictors import Predictor
&gt;&gt;&gt; coref_model = Predictor.from_path('coref-spanbert-large-2021.03.10.tar.gz')
Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 414/414 [00:00&lt;00:00, 436kB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 213k/213k [00:00&lt;00:00, 239kB/s]
Downloading:  34%|███████████████████████████████████████████████████
</code></pre>
","4154185","","","","","2021-10-21 06:31:25","Predictor.from_path('coref-spanbert-large-2021.03.10.tar.gz') downloads model into cache though I provide a local copy of the model","<huggingface-transformers><allennlp><coref-spanbert>","0","1","","","","CC BY-SA 4.0"
"65147947","1","","","2020-12-04 17:22:46","","1","374","<p>I would like to extract clean triples in tuple form of (subject,relation,object) from the Allen NLP Open IE predictor model.</p>
<p>Currently, I see that the steps are as follows</p>
<pre><code>OIE_output = predictor_OIE.predict(sentence=sent)
for verb in OIE_output['verbs']:
    srl_output = predictor_OIE.make_srl_string(words,verb['tags'])
</code></pre>
<p>srl_output, when printed, gives me tags within sentences such as:</p>
<blockquote>
<p>[ARG0: Raytheon Technologies Corporation researches] , develops , and [V: manufactures] [ARG1: advanced technology products] [ARGM-LOC: in the aerospace and defense industry] , [C-ARG1: including aircraft engines , avionics , aerostructures , cybersecurity] [ARGM-ADV: , missiles , air defense systems , and drones] .</p>
</blockquote>
<p>The problem is, I cannot find any code to convert an example sentence like this, to multiple tuples of the form (subject,relation,object). Particularly, the special types of tags such as ARGM-LOC and ARGM-ADV make things more difficult.</p>
<p>I have already looked at the outdated Github repository <a href=""https://github.com/gabrielStanovsky/supervised_oie_wrapper"" rel=""nofollow noreferrer"">https://github.com/gabrielStanovsky/supervised_oie_wrapper</a> (which does not quite make these types of tuples), and the closest function to this I could find in AllenNLP was the <code>make_srl_string</code> method, but the output still needs to be postprocessed to create tuples.</p>
<p>Is there any code that creates tuples from the AllenNLP Open IE model? If not, could you add this as a feature?</p>
<p>Detailed explanation in github issue <a href=""https://github.com/allenai/allennlp/issues/4857"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp/issues/4857</a></p>
","14764822","","14764822","","2020-12-09 20:38:12","2020-12-12 03:12:56","Extracting Postprocessed Triples from AllenNLP Open IE model","<python><triples><allennlp>","1","1","","","","CC BY-SA 4.0"
"65984950","1","66165479","","2021-01-31 22:33:19","","1","88","<p>I would like to compute the f1-score for a classifier trained with allen-nlp. I used the working code from a allen-nlp guide, which computed accuracy, not F1, so I tried to adjust the metric in the code.</p>
<p>According to the documentation, <a href=""https://docs.allennlp.org/v2.0.0/api/training/metrics/categorical_accuracy/"" rel=""nofollow noreferrer"">CategoricalAccuracy</a> and <a href=""https://docs.allennlp.org/v2.0.0/api/training/metrics/fbeta_multi_label_measure/"" rel=""nofollow noreferrer"">FBetaMultiLabelMeasure</a> take the same inputs. (predictions: <code>torch.Tensor</code> of shape <code>[batch_size, ..., num_classes]</code>, gold_labels: <code>torch.Tensor</code> of shape <code>[batch_size, ...]</code>)</p>
<p>But for some reason the input that worked perfectly well for the accuracy results in a RuntimeError when given to the f1-multi-label metric.</p>
<p>I condensed the problem to the following code snippet:</p>
<pre><code>&gt;&gt;&gt; from allennlp.training.metrics import CategoricalAccuracy, FBetaMultiLabelMeasure
&gt;&gt;&gt; import torch
&gt;&gt;&gt; labels = torch.LongTensor([0, 0, 2, 1, 0])
&gt;&gt;&gt; logits = torch.FloatTensor([[ 0.0063, -0.0118,  0.1857], [ 0.0013, -0.0217,  0.0356], [-0.0028, -0.0512,  0.0253], [-0.0460, -0.0347,  0.0400], [-0.0418,  0.0254,  0.1001]])
&gt;&gt;&gt; labels.shape
torch.Size([5])
&gt;&gt;&gt; logits.shape
torch.Size([5, 3])
&gt;&gt;&gt; ca = CategoricalAccuracy()
&gt;&gt;&gt; f1 = FBetaMultiLabelMeasure()
&gt;&gt;&gt; ca(logits, labels)
&gt;&gt;&gt; f1(logits, labels)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;.../lib/python3.8/site-packages/allennlp/training/metrics/fbeta_multi_label_measure.py&quot;, line 130, in __call__
true_positives = (gold_labels * threshold_predictions).bool() &amp; mask &amp; pred_mask
RuntimeError: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 1
</code></pre>
<p>Why is this error happening? What am I missing here?</p>
","5800213","","5800213","","2021-02-03 14:35:15","2021-02-12 02:01:25","AllenNLP 2.0: Can't get FBetaMultiLabelMeasure to run","<pytorch><allennlp>","1","0","","","","CC BY-SA 4.0"
"53974956","1","","","2018-12-30 02:55:28","","0","249","<p>AllenNLP has a predictor function that takes in a JSON file then output a JSON file. From the documentation one can @overrides predictor.load_line and write in a function to take in, say, a text file. </p>

<p>How would you write this function? And how to implement the function (ie import it as a module)?</p>

<p>AllenNLP load_line API: <a href=""https://allenai.github.io/allennlp-docs/api/allennlp.predictors.html?highlight=sentencetaggerpredictor#allennlp.predictors.predictor.Predictor"" rel=""nofollow noreferrer"">https://allenai.github.io/allennlp-docs/api/allennlp.predictors.html?highlight=sentencetaggerpredictor#allennlp.predictors.predictor.Predictor</a></p>

<p>I am following the tutorial here:
<a href=""https://github.com/allenai/allennlp/blob/master/tutorials/tagger/README.md"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp/blob/master/tutorials/tagger/README.md</a></p>
","6703017","","6703017","","2018-12-30 03:21:46","2019-01-08 03:29:55","How to write an @overrides allennlp predictor.load_line?","<python><natural-language-processing><allennlp>","1","0","","","","CC BY-SA 4.0"
"64513991","1","","","2020-10-24 13:44:52","","1","69","<p>i have a forward function in allenNlp given by :</p>
<pre><code>  def forward(self, input_tokens, output_tokens):
    '''
    This is the main process of the Model where the actual computation happens. 
    Each Instance is fed to the forward method. 
    It takes dicts of tensors as input, with same keys as the fields in your Instance (input_tokens, output_tokens)
    It outputs the results of predicted tokens and the evaluation metrics as a dictionary. 
    '''

    mask = get_text_field_mask(input_tokens)
    embeddings = self.embedder(input_tokens)
    rnn_hidden = self.rnn(embeddings, mask)
    out_logits = self.hidden2out(rnn_hidden)
    loss = sequence_cross_entropy_with_logits(out_logits, output_tokens['tokens'], mask)

    return {'loss': loss}
</code></pre>
<p>the out_logits variable contains probabilities of tokens, how to dispaly these tokens.
the outlogits gives :</p>
<pre><code> array([[ 0.02416356,  0.0195566 , -0.03279119,  0.057118  ,  0.05091334,
    -0.01906729, -0.05311333,  0.04695245,  0.06872341,  0.05173637,
    -0.03523348, -0.00537474, -0.03946163, -0.05817827, -0.04316377,
    -0.06042208,  0.01190596,  0.00574979,  0.01183304,  0.02330608,
     0.04587644,  0.02319966,  0.0020873 ,  0.03781978, -0.03975108,
    -0.0131919 ,  0.00393738,  0.04785313,  0.00159995,  0.05751844,
     0.05420169, -0.01404533, -0.02716331, -0.03871592,  0.00949999,
    -0.02924301,  0.03504215,  0.00397302, -0.0305252 , -0.00228448,
     0.04034173,  0.01458408],
   [ 0.02050283,  0.0204745 , -0.03081856,  0.06295916,  0.04601778,
    -0.0167818 , -0.05653084,  0.05017883,  0.07212739,  0.06197165,
    -0.03590995, -0.01142827, -0.03807197, -0.05942211, -0.0375165 ,
    -0.06769539,  0.01200251,  0.01012686,  0.01514241,  0.01875677,
     0.04499928,  0.02748671,  0.0012517 ,  0.04062563, -0.04049949,
    -0.01986902,  0.00630998,  0.05092276,  0.00276728,  0.05341531,
     0.05047017, -0.01111878, -0.03038253, -0.04320357,  0.01768938,
    -0.03470382,  0.03567442,  0.00776757, -0.02703476, -0.00392571,
     0.04700187,  0.01671317]] dtype=float32)}
</code></pre>
<p>i want to convert the last array to token ?</p>
","13873517","","13873517","","2020-10-25 10:00:18","2020-10-27 08:41:44","how to convert outlogits to tokens?","<nlp><pytorch><allennlp>","1","0","","","","CC BY-SA 4.0"
"49588352","1","","","2018-03-31 13:35:40","","2","2964","<p>When trying to run a python notebook on google colab, mentioned in <a href=""https://medium.com/@markn_67491/run-allennlp-models-on-free-gpus-using-googles-colab-notebooks-4db9359970c1"" rel=""nofollow noreferrer"">https://medium.com/@markn_67491/run-allennlp-models-on-free-gpus-using-googles-colab-notebooks-4db9359970c1</a> available from: <a href=""https://drive.google.com/file/d/1JH6dz8GJbwh9GhPoZQKwR-EipeR5JBrV/view"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1JH6dz8GJbwh9GhPoZQKwR-EipeR5JBrV/view</a></p>

<p>after installing pytorch and AllenNlp I run</p>

<pre><code>from allennlp.models.archival import load_archive
from allennlp.service.predictors import Predictor
import pprint

pretty_print = pprint.PrettyPrinter(indent=4)
</code></pre>

<p>which crashes on the second line due to ...</p>

<p>AttributeError: module 'torch.optim.lr_scheduler' has no attribute 'CosineAnnealingLR'</p>

<p>note: <a href=""https://github.com/pytorch/pytorch/issues/3214"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/issues/3214</a> suggest upgrading to version 3.0 but even with
pip3 install <a href=""http://download.pytorch.org/whl/cu80/torch-0.3.1-cp36-cp36m-linux_x86_64.whl"" rel=""nofollow noreferrer"">http://download.pytorch.org/whl/cu80/torch-0.3.1-cp36-cp36m-linux_x86_64.whl</a> 
i.e. version 3.1 the issues persists. Any suggestions</p>
","5113941","","7483494","","2019-11-14 17:06:03","2019-11-14 17:06:03","pytorch AttributeError: module 'torch.optim.lr_scheduler' has no attribute 'CosineAnnealingLR'","<python><nlp><pytorch><allennlp>","0","1","","","","CC BY-SA 3.0"
"69098781","1","","","2021-09-08 07:33:28","","0","15","<p>I tried to save allennlp model by using :</p>
<pre><code>with open(&quot;D:/allennlp2/modelallen.th&quot;, 'wb') as f:
    torch.save(model.state_dict(), f)

vocab.save_to_files(&quot;D:/allennlp2/vocabulary&quot;)
</code></pre>
<p>and then I tried to load it with :</p>
<pre><code>vocab2 = Vocabulary.from_files(&quot;D:/allennlp2/vocabulary&quot;)

model2 = build_model(vocab2)
with open(&quot;D:/allennlp2/modelallen.th&quot;, 'rb') as f:
    models = model2.load_state_dict(torch.load(f))
</code></pre>
<p>The build_model has the argument to pass vocab.
It didn't load the model, instead gave me the output of :
</p>
<p>Does anyone know how to load model without config using allennlp?</p>
","16748781","","","","","2021-09-08 07:33:28","Load model Allennlp","<python><load><allennlp>","0","0","","","","CC BY-SA 4.0"
"66140980","1","","","2021-02-10 16:33:17","","0","482","<p>Within my Conda environment with Python 3.6.9, I've installed AllenNLP 9.2.0. I tried to install AllenNLP Server following the instruction from <a href=""https://github.com/allenai/allennlp-server"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp-server</a> by running <code>pip install --editable .</code></p>
<p>However, <a href=""https://i.stack.imgur.com/rLHrT.png"" rel=""nofollow noreferrer"">the installation procedure never finished as the compatibility checks with several modules</a>, e.g. <code>pip is looking at multiple versions of tqdm to determine which version is compatible with other requirements. This could take a while. Collecting tqdm&gt;=4.19</code></p>
<p>Does anybody know what happens here? Should I add more restrictions to steup.py in AllenNLP server? However, there is any code included in such file.</p>
<p>Thanks a lot for your help.</p>
","15184816","","","","","2021-02-12 01:11:14","AllenNLP Server: pip is looking at multiple versions of each package","<allennlp>","1","1","","","","CC BY-SA 4.0"
"57277214","1","57362125","","2019-07-30 17:37:26","","3","821","<p>I'm trying to replicate (or come close) to the results obtained by the <a href=""https://www.aclweb.org/anthology/D17-1018"" rel=""nofollow noreferrer"">End-to-end Neural Coreference Resolution</a> paper on the <a href=""http://conll.cemantix.org/2012/introduction.html"" rel=""nofollow noreferrer"">CoNLL-2012 shared task</a>. I intend to do some enhancements on top of this, so I decided to use <a href=""http://conll.cemantix.org/2012/introduction.html"" rel=""nofollow noreferrer"">AllenNLP's <code>CoreferenceResolver</code></a>. This is how I'm initialising &amp; training the model:</p>

<pre class=""lang-py prettyprint-override""><code>import torch
from allennlp.common import Params
from allennlp.data import Vocabulary
from allennlp.data.dataset_readers import ConllCorefReader
from allennlp.data.dataset_readers.dataset_utils import Ontonotes
from allennlp.data.iterators import BasicIterator, MultiprocessIterator
from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenCharactersIndexer
from allennlp.models import CoreferenceResolver
from allennlp.modules import Embedding, FeedForward
from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper
from allennlp.modules.seq2vec_encoders import CnnEncoder
from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder
from allennlp.modules.token_embedders import TokenCharactersEncoder
from allennlp.training import Trainer
from allennlp.training.learning_rate_schedulers import LearningRateScheduler
from torch.nn import LSTM, ReLU
from torch.optim import Adam


def read_data(directory_path):
    data = []
    for file_path in Ontonotes().dataset_path_iterator(directory_path):
        data += dataset_reader.read(file_path)
    return data


INPUT_FILE_PATH_TEMPLATE = ""data/CoNLL-2012/v4/data/%s""
dataset_reader = ConllCorefReader(10, {""tokens"": SingleIdTokenIndexer(),
                                       ""token_characters"": TokenCharactersIndexer()})
training_data = read_data(INPUT_FILE_PATH_TEMPLATE % ""train"")
validation_data = read_data(INPUT_FILE_PATH_TEMPLATE % ""development"")

vocabulary = Vocabulary.from_instances(training_data + validation_data)
model = CoreferenceResolver(vocab=vocabulary,
                            text_field_embedder=BasicTextFieldEmbedder({""tokens"": Embedding.from_params(vocabulary, Params({""embedding_dim"": embeddings_dimension, ""pretrained_file"": ""glove.840B.300d.txt""})),
                                                                        ""token_characters"": TokenCharactersEncoder(embedding=Embedding(num_embeddings=vocabulary.get_vocab_size(""token_characters""), embedding_dim=8, vocab_namespace=""token_characters""),
                                                                                                                   encoder=CnnEncoder(embedding_dim=8, num_filters=50, ngram_filter_sizes=(3, 4, 5), output_dim=100))}),
                            context_layer=PytorchSeq2SeqWrapper(LSTM(input_size=400, hidden_size=200, num_layers=1, dropout=0.2, bidirectional=True, batch_first=True)),
                            mention_feedforward=FeedForward(input_dim=1220, num_layers=2, hidden_dims=[150, 150], activations=[ReLU(), ReLU()], dropout=[0.2, 0.2]),
                            antecedent_feedforward=FeedForward(input_dim=3680, num_layers=2, hidden_dims=[150, 150], activations=[ReLU(), ReLU()], dropout=[0.2, 0.2]),
                            feature_size=20,
                            max_span_width=10,
                            spans_per_word=0.4,
                            max_antecedents=250,
                            lexical_dropout=0.5)

if torch.cuda.is_available():
    cuda_device = 0
    model = model.cuda(cuda_device)
else:
    cuda_device = -1

iterator = BasicIterator(batch_size=1)
iterator.index_with(vocabulary)
optimiser = Adam(model.parameters(), weight_decay=0.1)
Trainer(model=model,
        train_dataset=training_data,
        validation_dataset=validation_data,
        optimizer=optimiser,
        learning_rate_scheduler=LearningRateScheduler.from_params(optimiser, Params({""type"": ""step"", ""step_size"": 100})),
        iterator=iterator,
        num_epochs=150,
        patience=1,
        cuda_device=cuda_device).train()
</code></pre>

<p>After reading the data I've trained the model but ran out of GPU memory: <code>RuntimeError: CUDA out of memory. Tried to allocate 4.43 GiB (GPU 0; 11.17 GiB total capacity; 3.96 GiB already allocated; 3.40 GiB free; 3.47 GiB cached)</code>. Therefore, I attempted to make use of multiple GPUs to train this model. I'm making use of Tesla K80s (which have 12GiB memory).</p>

<p>I've tried making use of AllenNLP's <a href=""https://allenai.github.io/allennlp-docs/api/allennlp.data.iterators.html#multiprocess-iterator"" rel=""nofollow noreferrer""><code>MultiprocessIterator</code></a>, by itialising the <code>iterator</code> as <code>MultiprocessIterator(BasicIterator(batch_size=1), num_workers=torch.cuda.device_count())</code>. However, only 1 GPU is being used (by monitoring the memory usage through the <code>nvidia-smi</code> command) &amp; got the error below. I also tried fiddling with its parameters (increasing <code>num_workers</code> or decreasing <code>output_queue_size</code>) &amp; the <code>ulimit</code> (as mentioned by <a href=""https://github.com/pytorch/pytorch/issues/973#issuecomment-345088750"" rel=""nofollow noreferrer"">this PyTorch issue</a>) to no avail.</p>

<pre><code>Process Process-3:
Traceback (most recent call last):
  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
Traceback (most recent call last):
  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/user/.local/lib/python3.6/site-packages/allennlp/data/iterators/multiprocess_iterator.py"", line 32, in _create_tensor_dicts
    output_queue.put(tensor_dict)
  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/user/.local/lib/python3.6/site-packages/allennlp/data/iterators/multiprocess_iterator.py"", line 32, in _create_tensor_dicts
    output_queue.put(tensor_dict)
  File ""&lt;string&gt;"", line 2, in put
  File ""&lt;string&gt;"", line 2, in put
  File ""/usr/lib/python3.6/multiprocessing/managers.py"", line 772, in _callmethod
    raise convert_to_error(kind, result)
  File ""/usr/lib/python3.6/multiprocessing/managers.py"", line 772, in _callmethod
    raise convert_to_error(kind, result)
multiprocessing.managers.RemoteError: 
---------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/python3.6/multiprocessing/managers.py"", line 228, in serve_client
    request = recv()
  File ""/usr/lib/python3.6/multiprocessing/connection.py"", line 251, in recv
    return _ForkingPickler.loads(buf.getbuffer())
  File ""/home/user/.local/lib/python3.6/site-packages/torch/multiprocessing/reductions.py"", line 276, in rebuild_storage_fd
    fd = df.detach()
  File ""/usr/lib/python3.6/multiprocessing/resource_sharer.py"", line 58, in detach
    return reduction.recv_handle(conn)
  File ""/usr/lib/python3.6/multiprocessing/reduction.py"", line 182, in recv_handle
    return recvfds(s, 1)[0]
  File ""/usr/lib/python3.6/multiprocessing/reduction.py"", line 161, in recvfds
    len(ancdata))
RuntimeError: received 0 items of ancdata
---------------------------------------------------------------------------
</code></pre>

<p>I also tried achieving this through <a href=""https://pytorch.org/docs/stable/nn.html#dataparallel"" rel=""nofollow noreferrer"">PyTorch's DataParallel</a>, by wrapping the model's <code>context_layer</code>, <code>mention_feedforward</code>, <code>antecedent_feedforward</code> with a custom <code>DataParallelWrapper</code> (to provide compatibility with the AllenNLP-assumed class functions). Still, only 1 GPU is used &amp; it eventually runs out of memory as before.</p>

<pre><code>class DataParallelWrapper(DataParallel):
    def __init__(self, module):
        super().__init__(module)

    def get_output_dim(self):
        return self.module.get_output_dim()

    def get_input_dim(self):
        return self.module.get_input_dim()

    def forward(self, *inputs):
        return self.module.forward(inputs)
</code></pre>
","11858455","","11858455","","2019-08-05 15:43:04","2019-08-05 15:55:30","Multi-GPU training of AllenNLP coreference resolution","<python><pytorch><allennlp>","1","0","","","","CC BY-SA 4.0"
"65853924","1","","","2021-01-22 23:03:49","","0","157","<p>So the issue is that, for using autotuning (like optuna) with AllenNLP, the suggested practice is to use, in jsonnet scripts, references to environment variables, and then to set up a study to modify those parameters.</p>
<p>That works fine, when the values are integers or floating points.  For integers, you use <code>std.parseInt(std.extVar(varname))</code>, for floating point numbers, you use <code>std.parseJson(std.extVar(varname))</code>.</p>
<p>But if I want to change, say the optimization technique in my tests between &quot;adam&quot;, &quot;sparseadam&quot;, &quot;adamax&quot;, adamw&quot;, etc. or change the type of RNN I am using, there does not appear to be an easy way to do that.</p>
<p>It would seem that you should be able to do <code>std.extVar(varname)</code> in that case without wrapping it inside a <code>parseJson()</code> or <code>parseInt()</code>, but that returns an error. Has anybody else had that problem and how did you  get around it?</p>
<p>Just to add to this, I am trying this with three different string parameters. Here is the jsonnet for the first one, &quot;bert_vocab&quot;:</p>
<pre><code>local bert_vocab=std.extvar('bert_vocab');
</code></pre>
<p>Error message:</p>
<pre><code>    486         ext_vars = {**_environment_variables(), **ext_vars}
    487 
--&gt; 488         file_dict = json.loads(evaluate_file(params_file, ext_vars=ext_vars))
    489 
    490         if isinstance(params_overrides, dict):

RuntimeError: RUNTIME ERROR: field does not exist: extvar
    /bigdisk/lax/cox/jupyter/bert_config.jsonnet:28:18-28   thunk &lt;bert_vocab&gt;
    /bigdisk/lax/cox/jupyter/bert_config.jsonnet:61:22-32   object &lt;anonymous&gt;
    /bigdisk/lax/cox/jupyter/bert_config.jsonnet:(59:16)-(63:12)    object &lt;anonymous&gt;
    /bigdisk/lax/cox/jupyter/bert_config.jsonnet:(58:21)-(64:10)    object &lt;anonymous&gt;
    /bigdisk/lax/cox/jupyter/bert_config.jsonnet:(56:19)-(65:8) object &lt;anonymous&gt;
    During manifestation    
</code></pre>
<p>I also tried various &quot;string escaping functions&quot; like here (but none of the string escaping functions work either:</p>
<pre><code>local bert_vocab=std.escapeStringBash(std.extvar(&quot;bert_vocab&quot;));
</code></pre>
<p>I can do the following to verify that the os environment variable is set:</p>
<p><code>os.environ['bert_vocab']</code> returns <code>'bert-base-uncased'</code></p>
","13599235","","8080648","","2021-01-29 19:33:06","2021-01-29 19:33:06","As far as I can tell, there is no way to parameterize character strings in an AllenNLP config file --- only ints or floats","<allennlp><jsonnet><optuna>","1","3","","","","CC BY-SA 4.0"
"66342675","1","","","2021-02-23 23:44:55","","1","22","<p>Having trouble figuring out how to use ParlAI DECODE task for contradiction detection.  Was also hoping to fine tune an AllenNLP model with the decode dataset.</p>
","3779515","","","","","2021-02-23 23:44:55","How to use ParlAI DECODE to detect contradictions and how to fine tune AllenNLP textual entailment model using ParlAI DECODE?","<allennlp><parlai>","0","0","","","","CC BY-SA 4.0"
"67326772","1","","","2021-04-30 00:39:18","","0","158","<p>I am using Windows 10 and pip installed the latest allennlp branch. When I successfully install the package, I encountered the following:</p>
<pre><code>$ allennlp test-install

Traceback (most recent call last):
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;C:\Users\604194\AppData\Local\Continuum\anaconda3\envs\domains\Scripts\allennlp.exe\__main__.py&quot;, line 4, in &lt;module&gt;
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\run.py&quot;, line 15, in &lt;module&gt;
    from allennlp.commands import main  # pylint: disable=wrong-import-position
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\commands\__init__.py&quot;, line 8, in &lt;module&gt;
    from allennlp.commands.configure import Configure
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\commands\configure.py&quot;, line 27, in &lt;module&gt;
    from allennlp.service.config_explorer import make_app
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\service\config_explorer.py&quot;, line 24, in &lt;module&gt;
    from allennlp.common.configuration import configure, choices
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\common\__init__.py&quot;, line 1, in &lt;module&gt;
    from allennlp.common.from_params import FromParams
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\common\from_params.py&quot;, line 48, in &lt;module&gt;
    from allennlp.common.params import Params
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\common\params.py&quot;, line 173, in &lt;module&gt;
    class Params(MutableMapping):
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\allennlp\common\params.py&quot;, line 236, in Params
    def pop(self, key: str, default: Any = DEFAULT) -&gt; Any:
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\overrides\overrides.py&quot;, line 67, in overrides
    return _overrides(method, check_signature, check_at_runtime)
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\overrides\overrides.py&quot;, line 93, in _overrides
    _validate_method(method, super_class, check_signature)
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\overrides\overrides.py&quot;, line 114, in _validate_method
    ensure_signature_is_compatible(super_method, method, is_static)
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\overrides\signature.py&quot;, line 87, in ensure_signature_is_compatible
    super_sig, sub_sig, super_type_hints, sub_type_hints, is_static, method_name
  File &quot;c:\users\604194\appdata\local\continuum\anaconda3\envs\domains\lib\site-packages\overrides\signature.py&quot;, line 213, in ensure_all_positional_args_defined_in_sub
    f&quot;`{method_name}: {sub_param.name} must be a supertype of `{super_param.annotation}` but is `{sub_param.annotation}`&quot;
TypeError: `Params.pop: key must be a supertype of `&lt;class 'inspect._empty'&gt;` but is `&lt;class 'str'&gt;`
</code></pre>
<p>Any advice on how to resolve this issue? Thanks!</p>
<p>EDIT: Edited my question for more information if needed. Thanks!</p>
","7779709","","7779709","","2021-04-30 00:55:06","2021-04-30 00:55:06","Encountering type error handling when running allennlp test-install","<huggingface-transformers><allennlp>","0","4","","","","CC BY-SA 4.0"
"69417267","1","","","2021-10-02 13:16:45","","1","40","<p>I'm attempting to create an adversarially debiased bert masked language model using 'AdversarialBiasMitigator' alongside the AllenNLP pretrained MLM (from here: <a href=""https://storage.googleapis.com/allennlp-public-models/bert-masked-lm-2020-10-07.tar.gz"" rel=""nofollow noreferrer"">https://storage.googleapis.com/allennlp-public-models/bert-masked-lm-2020-10-07.tar.gz</a>).
The training data I am using is a variation of the WinoBias dataset, edited to work for masked language modelling. The format of this data is a pandas df, with the first column containing the sentences (which already contains [CLS], [SEP], and [MASK] tokens), and the second column containing the target (which is a gendered pronoun).
I have edited the masked_language_model_reader.py to correctly read in my pandas df, and I have edited the adversarial_bias_mitigator config file. The remaining files (adversarial_bias_mitigator.py and masked_language_model.py) I have kept the same, so I think the source of the error must be either in the config or the mlm dataset reader I have created.</p>
<p>The main changes I have made in the dataset reader are changing the tokenizer to PretrainedTransformerTokenizer, and editing the _read() method to the following:</p>
<pre><code>
    @overrides
    def _read(self, file_path: str):
            import pandas as pd
            data= pd.read_csv(file_path)
            targets = data.iloc[:,0].tolist()
            sentences = data.iloc[:,1].tolist()
            zipped = zip(sentences, targets)
            for t, s in zipped:
                    sentence = s
                    tokens = self._tokenizer.tokenize(sentence) 
                    target = str(t)
                    t = Token(&quot;[MASK]&quot;)
                    yield self.text_to_instance(sentence, tokens, [target])

</code></pre>
<p>The rest I have kept virtually the same as the original masked_language_model_reader.py (<a href=""https://github.com/allenai/allennlp-models/blob/aed4876f04a73c7effddf41b3164e1fb6fb6c275/allennlp_models/lm/masked_language_model_reader.py"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp-models/blob/aed4876f04a73c7effddf41b3164e1fb6fb6c275/allennlp_models/lm/masked_language_model_reader.py</a>). I know the above isn't very pythonic but it is the simplest way I could think of, and my dataset isn't that large (only 1000 sentences) so I don't think it is a problem of computing time.</p>
<p>When running all the relevant files in the CLI, the below error appears:</p>
<blockquote>
<p>2021-10-02 10:52:20,351 - INFO -
allennlp.training.gradient_descent_trainer - Training 0it [00:00,
?it/s] loading instances: 0it [00:00, ?it/s] loading instances: 162it
[00:00, 1616.98it/s] loading instances: 324it [00:00, 1545.78it/s]
loading instances: 479it [00:00, 1524.23it/s] loading instances: 681it
[00:00, 1713.15it/s] loading instances: 1049it [00:00, 1764.63it/s]
0it [00:00, ?it/s] 2021-10-02 10:52:20,959 - CRITICAL - root -
Uncaught exception Traceback (most recent call last):   File
&quot;/usr/local/bin/allennlp&quot;, line 8, in 
sys.exit(run())   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/<strong>main</strong>.py&quot;, line
46, in run
main(prog=&quot;allennlp&quot;)   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/commands/<strong>init</strong>.py&quot;,
line 122, in main
args.func(args)   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py&quot;,
line 121, in train_model_from_args
file_friendly_logging=args.file_friendly_logging,   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py&quot;,
line 187, in train_model_from_file
return_model=return_model,   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py&quot;,
line 260, in train_model
file_friendly_logging=file_friendly_logging,   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py&quot;,
line 504, in _train_worker
metrics = train_loop.run()   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py&quot;,
line 577, in run
return self.trainer.train()   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py&quot;,
line 750, in train
metrics, epoch = self._try_train()   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py&quot;,
line 773, in _try_train
train_metrics = self._train_epoch(epoch)   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py&quot;,
line 490, in _train_epoch
batch_outputs = self.batch_outputs(batch, for_training=True)   File
&quot;/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py&quot;,
line 383, in batch_outputs
output_dict = self._pytorch_model(**batch)   File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;,
line 1071, in _call_impl
result = forward_call(*input, **kwargs)   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/fairness/adversarial_bias_mitigator.py&quot;,
line 121, in forward
predictor_output_dict = self.predictor.forward(*args, **kwargs)   File
&quot;/usr/local/lib/python3.7/dist-packages/allennlp_models/lm/models/masked_language_model.py&quot;, line 110, in forward
embeddings = self._text_field_embedder(tokens)   File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;,
line 1071, in _call_impl
result = forward_call(*input, **kwargs)   File &quot;/usr/local/lib/python3.7/dist-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py&quot;,
line 103, in forward
token_vectors = embedder(**tensors, **forward_params_values)   File
&quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;,
line 1071, in _call_impl
result = forward_call(*input, **kwargs) TypeError: forward() got an unexpected keyword argument 'tokens'</p>
</blockquote>
<p>I can't seem to work out what the problem is. I can't understand why being passed 'tokens' would be a  problem? I'm wondering if it the way I am reading in the data and if it isn't correctly being formatted into an instance, but again, I can't seem to see an obvious problem with my method in comparison to the original script as well.
To try to fix the problem I have also added into the config:</p>
<pre><code>    &quot;token_indexers&quot;: {
              &quot;bert&quot;: {
                &quot;type&quot;: &quot;single_id&quot;
              }
</code></pre>
<p>as well as:</p>
<pre><code>    &quot;sorting_keys&quot;:[&quot;tokens&quot;]
</code></pre>
<p>I'm not sure if either of these things are related or helping/worsening the problem!</p>
<p>Thanks for any help.</p>
","17058458","","17058458","","2021-10-04 11:28:09","2021-10-08 19:07:03","Error when training AllenNLP adversarial bias mitigator using a pretrained masked language model","<python><json><bert-language-model><allennlp><mlmodel>","1","0","","","","CC BY-SA 4.0"
"69466924","1","","","2021-10-06 13:43:08","","0","25","<p>I am using &quot;allennlp.predictors.predictor&quot; to extract the information of dependency parsing for sentences. May I know how many numbers of &quot;tags&quot; and &quot;predicted_dependencies&quot; are in the dependency parsing? I can not find the exactly number of &quot;tags&quot; and &quot;predicted_dependencies&quot;. Is there any reference?</p>
","17089930","","10871073","","2021-10-11 00:43:07","2021-10-11 00:43:07","How many of ""tags"" and ""predicted_dependencies"" in the dependency parsing (allennlp.predictors.predictor)?","<allennlp><dependency-parsing>","0","2","","","","CC BY-SA 4.0"
"63430717","1","","","2020-08-15 20:53:04","","2","138","<p>I have below code and I want to save this exact model on the disk and load that in the code. So that I can prevent resource exhaustion.</p>
<pre><code>import scispacy
import spacy
from spacy import displacy
from scispacy.abbreviation import AbbreviationDetector
from scispacy.umls_linking import UmlsEntityLinker
nlp = spacy.load('en_core_sci_sm-0.2.5/en_core_sci_sm/en_core_sci_sm-0.2.5')

linker = UmlsEntityLinker(resolve_abbreviations=True)

nlp.add_pipe(linker)
</code></pre>
","12279163","","","","","2020-08-15 20:53:04","How to save a spacy model with UMLSEntityLinker on disk and load that as well?","<python><spacy><named-entity-recognition><allennlp>","0","0","","","","CC BY-SA 4.0"
"56128100","1","","","2019-05-14 10:17:17","","2","367","<p>I am using allennlp for NER tagging currently. </p>

<p>Code:</p>

<pre><code>from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path(""...path to model..."")
sentence = ""Top Gun was inspired by a newspaper article.""
result = predictor.predict(sentence)
lang = {}
for word, tag in zip(result[""words""], result[""tags""]):
  if tag != ""O"":
    lang[word] = tag
</code></pre>

<p>Are there any parsers which could merge the output below so that it returns ""Top Gun"" and tag ""WORK_OF_ART"" ?</p>

<pre><code>{'Top': 'B-WORK_OF_ART', 'Gun': 'L-WORK_OF_ART'}
</code></pre>
","11497702","","11497702","","2019-05-14 10:36:54","2021-01-20 04:36:59","How to merge multiword NER tags?","<python-3.x><named-entity-recognition><natural-language-processing><allennlp>","2","1","","","","CC BY-SA 4.0"
"67342447","1","","","2021-05-01 02:43:44","","0","82","<p>Currently training models using AllenNLP 1.2 and the <code>commands</code> api:</p>
<p><code>allennlp train -f --include-package custom-exp /usr/training_config/mock_model_config.jsonnet -s test-mock-out</code></p>
<p>I'm trying to execute a forward pass on a test dataset after training is completed. I know how to add an <code>epoch_callback</code>, but am not sure about the syntax for the <code>end_callback</code>.</p>
<p>In my <code>config.json</code>, I have the following:</p>
<pre><code>{
...
&quot;trainer&quot;: {
...
&quot;epoch_callbacks&quot;: [{&quot;type&quot;: 'log_metrics_to_wandb',},]
}
...
}
</code></pre>
<p>I've tried:</p>
<pre><code>              &quot;end_callback&quot;: [{&quot;type&quot;: 'my_custom_function',},]
</code></pre>
<p>but got an illegal argument error. Also, I am not sure how I would accurately specify the exact custom function and communicate it to the trainer.</p>
","6395770","","","","","2021-10-14 15:12:34","How to add training end callback to AllenNLP config file?","<machine-learning><pytorch><transfer-learning><allennlp>","2","0","","","","CC BY-SA 4.0"
"67980985","1","","","2021-06-15 06:21:27","","0","16","<p><strong>Which is best pretrained model now in AllenNlp to use for Semantic search engine?</strong></p>
<p>I have one pretrained model, but it's built in 2017. I think by this time we will have so many new pretrained models.</p>
<pre><code>predictor = Predictor.from_path(&quot;https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz&quot;)
</code></pre>
<p>The above is the pretrained model i saw.</p>
<p><strong>Can anyone suggest the latest and best AllenNlp pretrained model for semantic search engine?</strong></p>
","15224778","","","","","2021-06-15 06:21:27","Which is best pretrained model now in AllenNlp to use for Semantic search engine?","<nlp><pytorch><bert-language-model><allennlp>","0","0","","","","CC BY-SA 4.0"
"65851398","1","","","2021-01-22 19:14:22","","1","79","<p>Is it already possible to install AllenNLP on Raspbian? I can install AllenNLP on Windows 10 Python 64 bit, but not yet on a RPi4.</p>
<p>I've already tried to install AllenNLP via Miniconda, pip and docker but none worked out for me.</p>
<p>Reference readme of their github: (<a href=""https://github.com/allenai/allennlp#installation"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp#installation</a>)</p>
<p>I'm kinda new in this world so if anyone would have a working allennlp package with these specifications:</p>
<ul>
<li>Raspbian 10 Buster</li>
<li>aarch64</li>
<li>Python 3.7</li>
</ul>
<p>and specify the steps, it would be awesome!</p>
<p>Is there any wheel package already available?</p>
<p>I really want to use a Semantice Role Labeling model of AllenNLP (<a href=""https://demo.allennlp.org/semantic-role-labeling/semantic-role-labeling"" rel=""nofollow noreferrer"">https://demo.allennlp.org/semantic-role-labeling/semantic-role-labeling</a>) but when i try pip3 install allennlp==1.0.0 allennlp-models==1.0.0 i get this:</p>
<p><a href=""https://i.stack.imgur.com/cR4nK.png"" rel=""nofollow noreferrer"">image of pip install</a></p>
<p>Am I doing something wrong? I already have pytorch installed on version 1.6.0 (output of script.py)</p>
<p>Any help would be awesome!</p>
<p>Have a good day</p>
<p>Lennert</p>
","15061737","","4017881","","2021-01-30 17:29:58","2021-01-30 17:29:58","Is it possible to install python package AllenNLP on RPi4?","<python><raspberry-pi><raspbian><allennlp>","1","1","","","","CC BY-SA 4.0"
"67779179","1","","","2021-05-31 19:13:20","","0","46","<p>I'm using the allennlp library and I'm trying to get the following lines to work offline. The computer where I will be deploying has no access to the internet:</p>
<pre><code>ee = allennlp.commands.elmo.ElmoEmbedder(
        options_file=options_file, 
        weight_file=weight_file, 
        cuda_device=cuda_device
    )
</code></pre>
<p>Where options_file and weight_file are:<br />
<code>options_file = &quot;https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json&quot;</code>,<br />
<code>weight_file = &quot;https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5&quot;</code> and<br />
<code>cuda_device = 0</code> or <code>1</code>.</p>
<p>How can I save those two files to a folder so that they work offline? Thanks!</p>
<p>By the way. I have 2 json files and 2 other files on my <code>~/.allennlp/cache</code> folder that were created when I ran the command.</p>
","1267849","","3607203","","2021-06-01 07:44:45","2021-06-01 07:44:45","How to fetch elmo embeddings offline?","<nlp><named-entity-recognition><allennlp><flair>","0","1","","","","CC BY-SA 4.0"
"67983392","1","","","2021-06-15 09:12:54","","0","63","<p><code>allennlp.common.checks.ConfigurationError:</code> <code>from_params</code> was passed a <code>params</code> object that was not a <code>Params</code>. This probably indicates malformed parameters in a configuration file, where something that should have been a dictionary was actually a list, or something else. This happened when constructing an object of type <code>&lt;class 'allennlp.nn.initializers.InitializerApplicator'&gt;</code>.</p>
","16231539","","13440669","","2021-06-15 09:27:43","2021-06-15 09:27:43","allennlp.common.checks.ConfigurationError","<allennlp>","0","2","","","","CC BY-SA 4.0"
"67990773","1","","","2021-06-15 17:13:53","","0","25","<p>does anyone has an idea what is the input text size limit that can be passed to the
predict(passage, question) method of the AllenNLP Predictors.</p>
<p>I have tried with passage of 30-40 sentences, which is working fine. But eventually it is not working for me when I am passing some significant amount of text around 5K statement.</p>
","1731856","","","","","2021-06-18 16:29:17","Size of input allowed in AllenNLP - Predict when using a Predictor","<pytorch><prediction><allennlp>","1","0","","","","CC BY-SA 4.0"
"66520279","1","66609269","","2021-03-07 19:03:30","","0","97","<p>The official document of AllenNLP suggests specifying &quot;validation_data_path&quot; in the configuration file, but what if one wants to construct a dataset from a single source and then randomly split it into train and validation datasets with a given ratio?</p>
<p>Does AllenNLP support this? I would greatly appreciate your comments.</p>
","2598787","","","","","2021-03-13 01:26:41","How to write a configuration file to tell the AllenNLP trainer to randomly split dataset into train and dev","<allennlp>","1","0","","","","CC BY-SA 4.0"
"67950416","1","","","2021-06-12 15:40:12","","0","48","<p>I was trying to load a installed semantic role labeling model locally. I put the model.tar file in the same location as the python script. When loading the model from the link,</p>
<pre><code>Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz&quot;)
</code></pre>
<p>the code works fine.</p>
<p>However, If I load the installed model from local <code>Predictor.from_path(&quot;model.tar.gz&quot;)</code></p>
<p>Then the error message will pop up.</p>
<pre><code>raise FileNotFoundError(f&quot;file {url_or_filename} not found&quot;)
FileNotFoundError: file model.tar.gz not found
</code></pre>
<p>When print the location of the current path use the code <code>print(os.getcwd())</code> , it prints the correct location.</p>
<p>What did I do wrong?
Thank you</p>
","9241131","","","","","2021-06-12 15:40:12","Allennlp is not able to load model.tar.gz locally, FileNotFoundError","<pytorch><allennlp>","0","1","","","","CC BY-SA 4.0"
"67991110","1","","","2021-06-15 17:40:33","","0","26","<p>What is the max passage limit or hardware limit to use transformer-qa model for reading comprehension in allennlp:</p>
<p><strong>Predictor.from_path('https://storage.googleapis.com/allennlp-public-models/transformer-qa-2020-10-03.tar.gz').predict(passage=passage, question=question)</strong></p>
<p>I'm getting &quot;DefaultCPUAllocator: not enough memory: you tried to allocate 23437770752 bytes. Buy new RAM!&quot; error</p>
","11056287","","","","","2021-06-30 23:52:13","Passage limit for Reading comprehension by using Transformer QA pretrained model in allennlp","<deep-learning><nlp><transformer><question-answering><allennlp>","1","0","","","","CC BY-SA 4.0"
"59771715","1","","","2020-01-16 14:23:46","","1","108","<p>We have a production scenario with users invoking expensive NLP functions running for short periods of time (say 30s). Because of the high load and intermittent usage, we're looking into Lambda function deployment. However - our packages are big. </p>

<p>I'm trying to fit AllenNLP in a lambda function, which in turn depends on pytorch, scipy, spacy and numpy and a few other libs. </p>

<h2>What I've tried</h2>

<p>Following recommendations made <a href=""https://medium.com/@angelatao0123/serving-pytorch-nlp-models-on-aws-lambda-f735190ec16c"" rel=""nofollow noreferrer"">here</a> and the example <a href=""https://github.com/ryfeus/lambda-packs"" rel=""nofollow noreferrer"">here</a>, tests and additional files are removed. I also use a non-cuda version of Pytorch which gets its' size down. I can package an AllenNLP deployment down to about 512mb. Currently, this is still too big for AWS Lambda. </p>

<h2>Possible fixes?</h2>

<p>I'm wondering if anyone of has experience with one of the following potential pathways:</p>

<ol>
<li><p>Cutting PyTorch out of AllenNLP. Without Pytorch, we're in reach of getting it to 250mb. We only need to load archived models in production, but that does seem to use some of the PyTorch infrastructure. Maybe there are alternatives?</p></li>
<li><p>Invoking PyTorch in (a fork of) AllenNLP as a second lambda function. </p></li>
<li><p>Using S3 to deliver some of the dependencies: SIMlinking some of the larger <code>.so</code> files and serving them from an S3 bucket might help. This does create an additional problem: the Semnatic Role Labelling we're using from AllenNLP also requires some language models of around 500mb, for which the ephemeral storage could be used - but maybe these can be streamed directly into RAM from S3?</p></li>
</ol>

<p>Maybe i'm missing an easy solution. Any direction or experiences would be much appreciated!</p>
","7967438","","","","","2020-01-16 15:36:45","How to circumvent AWS package and ephemeral limits for large packages + large models","<python><aws-lambda><pytorch><allennlp>","1","0","","","","CC BY-SA 4.0"
"68166242","1","68295217","","2021-06-28 15:40:08","","0","22","<p>I have learned to use AMP and GA tricks on training model from <a href=""https://medium.com/ai2-blog/tutorial-training-on-larger-batches-with-less-memory-in-allennlp-1cd2047d92ad"" rel=""nofollow noreferrer"">https://medium.com/ai2-blog/tutorial-training-on-larger-batches-with-less-memory-in-allennlp-1cd2047d92ad</a>,
But it seems not supported in the 2.4.0 ?</p>
<p>File &quot;/root/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/training/util.py&quot;, line 217, in create_serialization_dir
f&quot;Value for '{key}' in training configuration does not match that the value in &quot;</p>
","9594332","","","","","2021-07-08 03:23:10","how to use amp training in Allennlp 2.4.0?","<allennlp>","1","4","","","","CC BY-SA 4.0"
"67327156","1","","","2021-04-30 01:42:22","","0","145","<p>I'm using python 3.7 and pytorch in google colab.
I installed</p>
<pre><code>Pip install alennlp==2.4.0
pip install allennlp-models
</code></pre>
<p>In google colab, but when I run this code:
allennlp train experiments/oie_labeler_crf.json -s ~/Desktop/PHD/NLP/large-scale-oie/results/classic_train_crf --include-package large_scale_oie</p>
<p>I get this error:</p>
<pre><code>File &quot;/content/large-scale-oie/large_scale_oie/dataset_readers/oie_reader.py&quot;, line 12, in
from allennlp.data.dataset_readers.dataset_utils import Ontonotes, OntonotesSentence
ImportError: cannot import name 'Ontonotes' from 'allennlp.data.dataset_readers.dataset_utils' (/usr/local/lib/python3.7/dist-packages/allennlp/data/dataset_readers/dataset_utils/init.py)
</code></pre>
<p>Please help me! What do I do?</p>
","5028133","","10860403","","2021-04-30 08:37:52","2021-10-12 00:39:23","ImportError: cannot import name 'Ontonotes' from 'allennlp.data.dataset_readers.dataset_utils'","<python-3.x><pytorch><allennlp>","1","2","","","","CC BY-SA 4.0"
"67345651","1","","","2021-05-01 11:18:51","","0","18","<p>I'm using pytorch in google colab. I have python 3.7 and allennlp 2.4.0 and allen-models.
when I run my code, I get this error:</p>
<p>Cannot register srl_bert as Model; name already in use for SrlBert</p>
<p>what do I do?</p>
","5028133","","","","","2021-05-01 11:18:51","Cannot register srl_bert as Model; name already in use for SrlBert","<python-3.x><pytorch><allennlp>","0","0","","","","CC BY-SA 4.0"
"67767604","1","","","2021-05-31 03:31:30","","0","52","<p>I want to use ElmoEmbedder from Elmo.</p>
<p>Requirements:
Python 3.6 - lower versions of Python do not work
AllenNLP 0.5.1 - to compute the ELMo representations
Keras 2.2.0 - For the creation of BiLSTM-CNN-CRF architecture</p>
<p>When i install allennlp 0.5.0, i got error like this.</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement torch==0.4.0 (from allennlp) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 1.7.0, 1.7.1, 1.8.0, 1.8.1) ERROR: No matching distribution found for torch==0.4.0
</code></pre>
<p>So i install the torch 0.4.0 but it gave me another error like this</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement torch==0.4.0 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 1.7.0, 1.7.1, 1.8.0, 1.8.1) ERROR: No matching distribution found for torch==0.4.0
</code></pre>
<p>So, how can i install allennlp 0.5.0?</p>
","15092867","","","","","2021-05-31 03:31:30","Can't install allennlp 0.5.0 in Colab","<python><allennlp><elmo>","0","3","","","","CC BY-SA 4.0"
"67798417","1","67826955","","2021-06-02 03:08:16","","0","16","<p>I'm migrating my allennlp model from classes to config and there's one last construct I'm having problems with.</p>
<p>I'm using a feedforward projection layer in my LSTM CRF decoder, i.e.</p>
<pre><code>vocab_size = vocab.get_vocab_size(&quot;tokens&quot;)

feedforward = FeedForward(
    input_dim=encoder.get_output_dim(), 
    num_layers=2,
    hidden_dims=[text_field_embedder.get_output_dim(), vocab_size],
    activations=[Activation.by_name(Activation.by_name(&quot;relu&quot;),Activation.by_name(&quot;linear&quot;)(),],
    dropout=[0.15,0.15],
    )

model = CrfTagger(
    vocab=vocab, 
    text_field_embedder=text_field_embedder,
    encoder=encoder,
    feedforward=feedforward,
    )
</code></pre>
<p>The issue I'm running into is how to express the last hidden dim size (vocab_size) in json, since it's dependent on the runtime value of <code>vocab.get_vocab_size(&quot;tokens&quot;)</code>?</p>
<p>It seems that I need to either construct FeedForward inside CrfTagger (so I have access to vocab at runtime) or create my own FeedForward derived class.</p>
<p>I'm wondering if there's a cleaner way, is there a way I can register a constructor for FeedForward (essentially a factory function)?</p>
","2981639","","","","","2021-06-03 18:30:44","Specifiying feedforward size based on vocab size in crf_tagger using allennlp config","<allennlp>","1","0","","","","CC BY-SA 4.0"
"66799479","1","","","2021-03-25 12:27:13","","2","69","<p>i'm using Scispacy (which is awesome!) but when I type 'tau' into the app found here <a href=""https://scispacy.apps.allenai.org/"" rel=""nofollow noreferrer"">https://scispacy.apps.allenai.org/</a>
the UMLS entity gives me the canonical name of 'MAPT gene' which is what I want.
But when I do the exact same thing in my python code based on the app code (see here <a href=""https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42"" rel=""nofollow noreferrer"">https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42</a>)
the first canonical name on the list is 'uridine triacetate' (the second is 'MAPT gene')</p>
<p>in the app code there is the call 'if show_only_top:break' so I assume somehow their app implementation orders the linked entities differently.</p>
<p>if someone can explain the difference in ordering and how to fix that would be great thanks!!</p>
","14014871","","","","","2021-04-05 20:34:33","what controls the order of UMLS linked entities from scispacy if the scores are all 1","<spacy><allennlp><entity-linking>","1","0","","","","CC BY-SA 4.0"
"67360264","1","67392050","","2021-05-02 19:14:02","","-1","56","<p>Is there a parameter that I can set in the config file (maybe for the trainer?) that would save the model (archive) after each epoch or after a specific number of steps?
I'm using seq2seq dataloader and &quot;composed_seq2seq&quot; as my model. This is how my trainer looks like currently:</p>
<pre><code>&quot;trainer&quot;: {
&quot;num_epochs&quot;: 50,
&quot;cuda_device&quot;: 0,
&quot;optimizer&quot;: {
  &quot;type&quot;: &quot;adamw&quot;,
  &quot;lr&quot;: 0.0005
}
</code></pre>
","15787679","","15787679","","2021-05-02 19:21:56","2021-05-06 23:03:09","Save model after each epoch - AllenNLP","<allennlp>","1","0","","","","CC BY-SA 4.0"
"68022957","1","","","2021-06-17 16:10:24","","0","78","<p>I am new to allenNLP library.
In order to set args for <code>dataset_reader</code>, I want to set config for evaluation, like train (<a href=""https://github.com/allenai/allennlp-template-config-files/blob/master/training_config/my_model_trained_on_my_dataset.jsonnet"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp-template-config-files/blob/master/training_config/my_model_trained_on_my_dataset.jsonnet</a>)</p>
<p>But I am not sure if there is a config file template for evaluation, like train, and the config file below works (where <code>train_data_path</code> and <code>trainer</code> parts are deleted.)</p>
<pre><code>{
    &quot;dataset_reader&quot; : {
        // This name needs to match the name that you used to register your dataset reader, with
        // the call to `@DatasetReader.register()`.
        &quot;type&quot;: &quot;classification-tsv&quot;,
        // These other parameters exactly match the constructor parameters of your dataset reader class.
        &quot;token_indexers&quot;: {
            &quot;tokens&quot;: {
                &quot;type&quot;: &quot;single_id&quot;
            }
        }
    },
    &quot;validation_data_path&quot;: &quot;/path/to/your/validation/data/here.tsv&quot;,
    &quot;model&quot;: {
        // This name needs to match the name that you used to register your model, with
        // the call to `@Model.register()`.
        &quot;type&quot;: &quot;simple_classifier&quot;,
        // These other parameters exactly match the constructor parameters of your model class.
        &quot;embedder&quot;: {
            &quot;token_embedders&quot;: {
                &quot;tokens&quot;: {
                    &quot;type&quot;: &quot;embedding&quot;,
                    &quot;embedding_dim&quot;: 10
                }
            }
        },
        &quot;encoder&quot;: {
            &quot;type&quot;: &quot;bag_of_embeddings&quot;,
            &quot;embedding_dim&quot;: 10
        }
    },
    &quot;data_loader&quot;: {
        // See http://docs.allennlp.org/master/api/data/dataloader/ for more info on acceptable
        // parameters here.
        &quot;batch_size&quot;: 8,
        &quot;shuffle&quot;: true
    },
}
</code></pre>
<p>Thanks in advance.</p>
","10616385","","","","","2021-06-30 23:48:59","AllenNLP) Is there a way to set config for evaluation, epeicially for reader?","<allennlp>","1","1","","","","CC BY-SA 4.0"
"59856012","1","","","2020-01-22 09:05:36","","0","126","<p>I'm searching for the latest tar file of coref model of allennlp. where can i find it. </p>

<p>Because in demo page of allennlp that output is different and when i tried using the example code the output is different.
The version in the example was 2018-05-02.</p>
","12759711","","","","","2020-02-21 14:29:09","Allennlp coref model latest version","<allennlp>","1","0","","","","CC BY-SA 4.0"
"68219209","1","","","2021-07-02 03:19:01","","0","46","<p>When using a huggingface pre-traind model,i passed a tokennizer and indexer for my textfied in Datasetreader, also i want use the same tokennizer and indexer in my model. Which way is an appropriate way in allennlp ? (using config file ?)
Here is my code, i think this is a bad sloution. Give me some suggestions please.</p>
<blockquote>
<p>`In my Dataset Reader::</p>
</blockquote>
<pre><code>    self._tokenizer = PretrainedTransformerTokenizer(&quot;microsoft/DialoGPT-small&quot;,tokenizer_kwargs={'cls_token': '[CLS]',
                                                                                    'sep_token': '[SEP]',
                                                                                                  'bos_token':'[BOS]'})
    self._tokenindexer = {&quot;tokens&quot;: PretrainedTransformerIndexer(&quot;microsoft/DialoGPT-small&quot;,
                                                                  tokenizer_kwargs={'cls_token': '[CLS]',
                                                                                    'sep_token': '[SEP]',
                                                                                    'bos_token':'[BOS]'})}
</code></pre>
<blockquote>
<p>In my Model:</p>
</blockquote>
<pre><code>
self.tokenizer = GPT2Tokenizer.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)

num_added_tokens = self.tokenizer.add_special_tokens({'bos_token':'[BOS]','sep_token': '[SEP]','cls_token':'[CLS]'})

self.emb_dim = len(self.tokenizer) 

self.embeded_layer = self.encoder.resize_token_embeddings(self.emb_dim)
</code></pre>
<p>I have create two tokenizers for datasetreader and model, and both the tokenizers have the common vocabulary and special tokens. but when i add the three special token in the same order, the special token will have a different index. so i switched the order in Model`s codes to achieve the same indexs.(stupid but effective)
Is there exists a way to pass the tokennizer or vocab from DatasetReader to Model?
Which way is an appropriate way in allennlp  to slove this problem ?</p>
","9594332","","9594332","","2021-07-05 01:31:51","2021-07-05 01:31:51","Use pre-trained model vocabulary in an appropriate way with allennlp","<vocabulary><allennlp>","0","3","","","","CC BY-SA 4.0"
"67174175","1","67174714","","2021-04-20 07:17:12","","1","49","<p>I implemented <a href=""https://demo.allennlp.org/open-information-extraction"" rel=""nofollow noreferrer"">allennlp's OIE</a>, which extracts subject, predicate, object information (in the form of ARG0, V, ARG1 etc) embedded in nested strings. However, I need to make sure that each output is linked to the given <code>ID</code> of the original sentence.</p>
<p>I produced the following pandas dataframe, where <code>OIE output</code> contains the raw output of the allennlp algorithm.</p>
<p><strong>Current output:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">sentence</th>
<th style=""text-align: center;"">ID</th>
<th style=""text-align: right;"">OIE output</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">'The girl went to the cinema'</td>
<td style=""text-align: center;"">'abcd'</td>
<td style=""text-align: right;"">{'verbs':[{'verb': 'went', 'description':'[ARG0: The girl] [V: went] [ARG1:to the cinema]'}]}</td>
</tr>
<tr>
<td style=""text-align: left;"">'He is right and he is an engineer'</td>
<td style=""text-align: center;"">'efgh'</td>
<td style=""text-align: right;"">{'verbs':[{'verb': 'is', 'description':'[ARG0: He] [V: is] [ARG1:right]'}, {'verb': 'is', 'description':'[ARG0: He] [V: is] [ARG1:an engineer]'}]}</td>
</tr>
</tbody>
</table>
</div>
<p>My code to get the above table:</p>
<pre><code>oie_l = []

for sent in sentences:
  oie_pred = predictor_oie.predict(sentence=sent) #allennlp oie predictor
  for d in oie_pred['verbs']: #get to the nested info
    d.pop('tags') #remove unnecessary info
  oie_l.append(oie_pred)

df['OIE out'] = oie_l #add new column to df

</code></pre>
<p><strong>Desired output:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">sentence</th>
<th style=""text-align: center;"">ID</th>
<th style=""text-align: right;"">OIE Triples</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">'The girl went to the cinema'</td>
<td style=""text-align: center;"">'abcd'</td>
<td style=""text-align: right;"">'[ARG0: The girl] [V: went] [ARG1:to the cinema]'</td>
</tr>
<tr>
<td style=""text-align: left;"">'He is right and he is an engineer'</td>
<td style=""text-align: center;"">'efgh'</td>
<td style=""text-align: right;"">'[ARG0: He] [V: is] [ARG1:right]'</td>
</tr>
<tr>
<td style=""text-align: left;"">'He is right and he is an engineer'</td>
<td style=""text-align: center;"">'efgh'</td>
<td style=""text-align: right;"">'[ARG0: He] [V: is] [ARG1:an engineer]'</td>
</tr>
</tbody>
</table>
</div>
<p>Approach idea:</p>
<p>To get to the desired output of 'OIE Triples' , I was considering transforming the initial 'OIE output' into a string and then using regular expression to extract the ARGs. However, I am not sure if this is the best solution, as the 'ARGs' can vary. Another approach, would be to iterate to the nested values of  <code>description:</code> , replace what is currently in the OIE output in the form of a list and then implement <code>df.explode()</code> method to expand it, so that the right sentence and id columns are linked to the triple after 'exploding'.</p>
<p>Any advice is appreciated.</p>
","12368010","","12368010","","2021-04-22 07:38:48","2021-04-22 07:38:48","Get an item value from a nested dictionary inside the rows of a pandas df and get rid off the rest","<python><pandas><triples><allennlp>","1","0","","","","CC BY-SA 4.0"
"67474412","1","","","2021-05-10 16:41:55","","0","84","<p>I'm trying to finetune a pretrained multilingual BERT model on dependency parsing using the Udify library. This library used allennlp==0.9.0, while I need to use allennlp=1.3.0 and I'm trying update the code for that. After making some changes in configuration file, I'm having an issue with input in the forward() method of the model. Namely, <code>tokens</code> are passed which in my case is a dictionary that looks like this:</p>
<pre><code>tokens = {
            &quot;bert&quot;: {
                &quot;bert&quot;: tensor(...),
                &quot;bert-offsets&quot;: tensor(...),
                &quot;bert-type-ids&quot;: tensor(...),
                &quot;mask&quot;: tensor(...)
            },
            &quot;tokens&quot;: {
                &quot;tokens&quot;: tensor(...)
            }
        }
</code></pre>
<p>The error happens in <a href=""https://github.com/blazejdolicki/udify/blob/2d9f24182c904791cacd922aa23fb450bb671d70/udify/models/udify_model.py#L146"" rel=""nofollow noreferrer"">this method</a>:</p>
<pre><code>Traceback (most recent call last):
  File &quot;train.py&quot;, line 74, in &lt;module&gt;
    train_model(train_params, serialization_dir, recover=bool(args.resume))
  File &quot;/home/lcur0308/.conda/envs/atcs-project/lib/python3.8/site-packages/allennlp/commands/train.py&quot;, line 236, in train_model
    model = _train_worker(
  File &quot;/home/lcur0308/.conda/envs/atcs-project/lib/python3.8/site-packages/allennlp/commands/train.py&quot;, line 466, in _train_worker
    metrics = train_loop.run()
  File &quot;/home/lcur0308/.conda/envs/atcs-project/lib/python3.8/site-packages/allennlp/commands/train.py&quot;, line 528, in run
    return self.trainer.train()
  File &quot;/home/lcur0308/.conda/envs/atcs-project/lib/python3.8/site-packages/allennlp/training/trainer.py&quot;, line 966, in train
    return self._try_train()
  File &quot;/home/lcur0308/.conda/envs/atcs-project/lib/python3.8/site-packages/allennlp/training/trainer.py&quot;, line 1001, in _try_train
    train_metrics = self._train_epoch(epoch)
  File &quot;/home/lcur0308/.conda/envs/atcs-project/lib/python3.8/site-packages/allennlp/training/trainer.py&quot;, line 716, in _train_epoch
    batch_outputs = self.batch_outputs(batch, for_training=True)
  File &quot;/home/lcur0308/.conda/envs/atcs-project/lib/python3.8/site-packages/allennlp/training/trainer.py&quot;, line 604, in batch_outputs
    output_dict = self._pytorch_model(**batch)
  File &quot;/home/lcur0308/.conda/envs/atcs-project/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/lcur0308/atcs-project/multilingual-interference/udify/udify/models/udify_model.py&quot;, line 120, in forward
    self._apply_token_dropout(tokens)
  File &quot;/home/lcur0308/atcs-project/multilingual-interference/udify/udify/models/udify_model.py&quot;, line 197, in _apply_token_dropout
    tokens[&quot;tokens&quot;] = self.token_dropout(
  File &quot;/home/lcur0308/atcs-project/multilingual-interference/udify/udify/models/udify_model.py&quot;, line 243, in token_dropout
    device = tokens.device
AttributeError: 'dict' object has no attribute 'device'
</code></pre>
<p>I believe that <code>self.token_dropout()</code> expects to receive a tensor (<code>tokens[&quot;tokens&quot;] = tensor(...)</code>) while instead it receives a dictionary with a tensor (<code>tokens[&quot;tokens&quot;] = {&quot;tokens&quot;:tensor(...)}</code>). However, I don't know how to fix that. Of course, I could hack a workaround like passing tokens[&quot;tokens&quot;][&quot;tokens&quot;] instead of tokens[&quot;tokens&quot;], but I have a hunch that my error is a side effect of some underlying error in configuration and a quick hack will not entirely solve the problem.</p>
<p>My current configuration after some changes to make it compatible with 1.3.0 looks like this (my apologies for attaching such a long block, but I don't know which part is relevant):</p>
<pre><code>{
  &quot;dataset_reader&quot;: {
    &quot;lazy&quot;: false,
    &quot;token_indexers&quot;: {
      &quot;tokens&quot;: {
        &quot;type&quot;: &quot;single_id&quot;,
        &quot;lowercase_tokens&quot;: true
      },
      &quot;bert&quot;: {
        &quot;type&quot;: &quot;udify-bert-pretrained&quot;,
        &quot;pretrained_model&quot;: &quot;config/archive/bert-base-multilingual-cased/vocab.txt&quot;,
        &quot;do_lowercase&quot;: false,
        &quot;use_starting_offsets&quot;: true
      }
    }
  },
  &quot;train_data_path&quot;: &quot;data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-train.conllu&quot;,
  &quot;validation_data_path&quot;: &quot;data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-dev.conllu&quot;,
  &quot;test_data_path&quot;: &quot;data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-test.conllu&quot;,
  &quot;vocabulary&quot;: {
    &quot;type&quot;: &quot;from_files&quot;,
    &quot;directory&quot;: &quot;data/concat-exp-mix/vocab/concat-exp-mix/vocabulary/&quot;
  },
  &quot;model&quot;: {
    &quot;word_dropout&quot;: 0.2,
    &quot;mix_embedding&quot;: 12,
    &quot;layer_dropout&quot;: 0.1,
    &quot;tasks&quot;: [&quot;deps&quot;],
    &quot;pretrained_model&quot;: &quot;bert-base-multilingual-cased&quot;,
    &quot;text_field_embedder&quot;: {
      &quot;type&quot;: &quot;udify_embedder&quot;,
      &quot;dropout&quot;: 0.5,
      &quot;allow_unmatched_keys&quot;: true,
      &quot;embedder_to_indexer_map&quot;: {
        &quot;bert&quot;: [&quot;bert&quot;, &quot;bert-offsets&quot;]
      },
      &quot;token_embedders&quot;: {
        &quot;bert&quot;: {
          &quot;type&quot;: &quot;udify-bert-pretrained&quot;,
          &quot;pretrained_model&quot;: &quot;bert-base-multilingual-cased&quot;,
          &quot;requires_grad&quot;: true,
          &quot;dropout&quot;: 0.15,
          &quot;layer_dropout&quot;: 0.1,
          &quot;combine_layers&quot;: &quot;all&quot;
        }
      }
    },
    &quot;encoder&quot;: {
      &quot;type&quot;: &quot;pass_through&quot;,
      &quot;input_dim&quot;: 768
    },
    &quot;decoders&quot;: {
      &quot;upos&quot;: {
        &quot;encoder&quot;: {
          &quot;type&quot;: &quot;pass_through&quot;,
          &quot;input_dim&quot;: 768
        }
      },
      &quot;feats&quot;: {
        &quot;encoder&quot;: {
          &quot;type&quot;: &quot;pass_through&quot;,
          &quot;input_dim&quot;: 768
        },
        &quot;adaptive&quot;: true
      },
      &quot;lemmas&quot;: {
        &quot;encoder&quot;: {
          &quot;type&quot;: &quot;pass_through&quot;,
          &quot;input_dim&quot;: 768
        },
        &quot;adaptive&quot;: true
      },
      &quot;deps&quot;: {
        &quot;tag_representation_dim&quot;: 256,
        &quot;arc_representation_dim&quot;: 768,
        &quot;encoder&quot;: {
          &quot;type&quot;: &quot;pass_through&quot;,
          &quot;input_dim&quot;: 768
        }
      }
    }
  },
  &quot;data_loader&quot;: {
    &quot;batch_sampler&quot;:{
      &quot;batch_size&quot;: 16
    }

  },
  &quot;trainer&quot;: {
    &quot;num_epochs&quot;: 5,
    &quot;patience&quot;: 40,
    &quot;optimizer&quot;: {
      &quot;type&quot;: &quot;adamw&quot;,
      &quot;betas&quot;: [0.9, 0.99],
      &quot;weight_decay&quot;: 0.01,
      &quot;lr&quot;: 1e-3,
      &quot;parameter_groups&quot;: [
        [[&quot;^text_field_embedder.*.bert_model.embeddings&quot;,
          &quot;^text_field_embedder.*.bert_model.encoder&quot;], {}],
        [[&quot;^text_field_embedder.*._scalar_mix&quot;,
          &quot;^text_field_embedder.*.pooler&quot;,
          &quot;^scalar_mix&quot;,
          &quot;^decoders&quot;,
          &quot;^shared_encoder&quot;], {}]
      ]
    },
    &quot;learning_rate_scheduler&quot;: {
      &quot;type&quot;: &quot;ulmfit_sqrt&quot;,
      &quot;model_size&quot;: 1,
      &quot;warmup_steps&quot;: 392,
      &quot;start_step&quot;: 392,
      &quot;factor&quot;: 5.0,
      &quot;gradual_unfreezing&quot;: true,
      &quot;discriminative_fine_tuning&quot;: true,
      &quot;decay_factor&quot;: 0.04
    }
  },
  &quot;udify_replace&quot;: [
    &quot;dataset_reader.token_indexers&quot;,
    &quot;model.text_field_embedder&quot;,
    &quot;model.encoder&quot;,
    &quot;model.decoders.xpos&quot;,
    &quot;model.decoders.deps.encoder&quot;,
    &quot;model.decoders.upos.encoder&quot;,
    &quot;model.decoders.feats.encoder&quot;,
    &quot;model.decoders.lemmas.encoder&quot;,
    &quot;trainer.learning_rate_scheduler&quot;,
    &quot;trainer.optimizer&quot;
  ]
}

</code></pre>
<p>The corresponding configuration that worked for allennlp 0.9.0 is the following:</p>
<pre><code>{
  &quot;dataset_reader&quot;: {
    &quot;lazy&quot;: false,
    &quot;token_indexers&quot;: {
      &quot;tokens&quot;: {
        &quot;type&quot;: &quot;single_id&quot;,
        &quot;lowercase_tokens&quot;: true
      },
      &quot;bert&quot;: {
        &quot;type&quot;: &quot;udify-bert-pretrained&quot;,
        &quot;pretrained_model&quot;: &quot;config/archive/bert-base-multilingual-cased/vocab.txt&quot;,
        &quot;do_lowercase&quot;: false,
        &quot;use_starting_offsets&quot;: true
      }
    }
  },
  &quot;train_data_path&quot;: &quot;data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-train.conllu&quot;,
  &quot;validation_data_path&quot;: &quot;data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-dev.conllu&quot;,
  &quot;test_data_path&quot;: &quot;data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-test.conllu&quot;,
  &quot;vocabulary&quot;: {
    &quot;directory_path&quot;: &quot;data/vocab/english_only_expmix4/vocabulary&quot;
  },
  &quot;model&quot;: {
    &quot;word_dropout&quot;: 0.2,
    &quot;mix_embedding&quot;: 12,
    &quot;layer_dropout&quot;: 0.1,
    &quot;tasks&quot;: [&quot;deps&quot;],
    &quot;text_field_embedder&quot;: {
      &quot;type&quot;: &quot;udify_embedder&quot;,
      &quot;dropout&quot;: 0.5,
      &quot;allow_unmatched_keys&quot;: true,
      &quot;embedder_to_indexer_map&quot;: {
        &quot;bert&quot;: [&quot;bert&quot;, &quot;bert-offsets&quot;]
      },
      &quot;token_embedders&quot;: {
        &quot;bert&quot;: {
          &quot;type&quot;: &quot;udify-bert-pretrained&quot;,
          &quot;pretrained_model&quot;: &quot;bert-base-multilingual-cased&quot;,
          &quot;requires_grad&quot;: true,
          &quot;dropout&quot;: 0.15,
          &quot;layer_dropout&quot;: 0.1,
          &quot;combine_layers&quot;: &quot;all&quot;
        }
      }
    },
    &quot;encoder&quot;: {
      &quot;type&quot;: &quot;pass_through&quot;,
      &quot;input_dim&quot;: 768
    },
    &quot;decoders&quot;: {
      &quot;upos&quot;: {
        &quot;encoder&quot;: {
          &quot;type&quot;: &quot;pass_through&quot;,
          &quot;input_dim&quot;: 768
        }
      },
      &quot;feats&quot;: {
        &quot;encoder&quot;: {
          &quot;type&quot;: &quot;pass_through&quot;,
          &quot;input_dim&quot;: 768
        },
        &quot;adaptive&quot;: true
      },
      &quot;lemmas&quot;: {
        &quot;encoder&quot;: {
          &quot;type&quot;: &quot;pass_through&quot;,
          &quot;input_dim&quot;: 768
        },
        &quot;adaptive&quot;: true
      },
      &quot;deps&quot;: {
        &quot;tag_representation_dim&quot;: 256,
        &quot;arc_representation_dim&quot;: 768,
        &quot;encoder&quot;: {
          &quot;type&quot;: &quot;pass_through&quot;,
          &quot;input_dim&quot;: 768
        }
      }
    }
  },
  &quot;iterator&quot;: {
    &quot;batch_size&quot;: 16
  },
  &quot;trainer&quot;: {
    &quot;num_epochs&quot;: 5,
    &quot;patience&quot;: 40,
    &quot;num_serialized_models_to_keep&quot;: 1,
    &quot;should_log_learning_rate&quot;: true,
    &quot;summary_interval&quot;: 100,
    &quot;optimizer&quot;: {
      &quot;type&quot;: &quot;bert_adam&quot;,
      &quot;b1&quot;: 0.9,
      &quot;b2&quot;: 0.99,
      &quot;weight_decay&quot;: 0.01,
      &quot;lr&quot;: 1e-3,
      &quot;parameter_groups&quot;: [
        [[&quot;^text_field_embedder.*.bert_model.embeddings&quot;,
          &quot;^text_field_embedder.*.bert_model.encoder&quot;], {}],
        [[&quot;^text_field_embedder.*._scalar_mix&quot;,
          &quot;^text_field_embedder.*.pooler&quot;,
          &quot;^scalar_mix&quot;,
          &quot;^decoders&quot;,
          &quot;^shared_encoder&quot;], {}]
      ]
    },
    &quot;learning_rate_scheduler&quot;: {
      &quot;type&quot;: &quot;ulmfit_sqrt&quot;,
      &quot;model_size&quot;: 1,
      &quot;warmup_steps&quot;: 392,
      &quot;start_step&quot;: 392,
      &quot;factor&quot;: 5.0,
      &quot;gradual_unfreezing&quot;: true,
      &quot;discriminative_fine_tuning&quot;: true,
      &quot;decay_factor&quot;: 0.04
    }
  },
  &quot;udify_replace&quot;: [
    &quot;dataset_reader.token_indexers&quot;,
    &quot;model.text_field_embedder&quot;,
    &quot;model.encoder&quot;,
    &quot;model.decoders.xpos&quot;,
    &quot;model.decoders.deps.encoder&quot;,
    &quot;model.decoders.upos.encoder&quot;,
    &quot;model.decoders.feats.encoder&quot;,
    &quot;model.decoders.lemmas.encoder&quot;,
    &quot;trainer.learning_rate_scheduler&quot;,
    &quot;trainer.optimizer&quot;
  ]
}
</code></pre>
","9215579","","","","","2021-05-10 16:41:55","Updating model configuration from allennlp 0.9.0 to 1.3.0","<python><allennlp>","0","1","1","","","CC BY-SA 4.0"
"66810883","1","66815715","","2021-03-26 04:01:31","","-1","80","<p>I'm trying to use a Huggingface pretrained model &quot;GPT2dialog&quot; as a encoder for sentences,But the textindexer confused me.
In detail ,I can run a unittest for dataset_reader with a pretrained indexer normally,when use the train command to train the model caused a Bug:</p>
<pre><code>File &quot;/home/lee/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/lazy.py&quot;, line 54, in constructor_to_use
    return constructor.from_params(Params({}), **kwargs)  # type: ignore[union-attr]
  File &quot;/home/lee/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/from_params.py&quot;, line 604, in from_params
    **extras,
  File &quot;/home/lee/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/common/from_params.py&quot;, line 634, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File &quot;/home/lee/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/data/vocabulary.py&quot;, line 310, in from_instances
    instance.count_vocab_items(namespace_token_counts)
  File &quot;/home/lee/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/data/instance.py&quot;, line 60, in count_vocab_items
    field.count_vocab_items(counter)
  File &quot;/home/lee/anaconda3/envs/allennlp/lib/python3.6/site-packages/allennlp/data/fields/text_field.py&quot;, line 78, in count_vocab_items
    for indexer in self.token_indexers.values():
AttributeError: 'PretrainedTransformerIndexer' object has no attribute 'values'
</code></pre>
<p>Here is my dataset_reader code.</p>
<pre><code>class MultiWozDatasetReader(DatasetReader):
 def __init__(self,
              lazy:bool = False,
              tokenizer: Tokenizer = None,
              tokenindexer:Dict[str, TokenIndexer] = None
              ) -&gt; None:
     super().__init__(lazy)
     self._tokenizer = tokenizer or WhitespaceTokenizer()
     self._tokenindexer = PretrainedTransformerIndexer(&quot;microsoft/DialoGPT-small&quot;)

 @overrides
 def read(self, file_path: str):
     logger.warn(&quot;call read&quot;)
     with open(file_path, 'r') as data_file:
         dialogs = json.load(data_file)
         for dialog in dialogs:
             dialogue = dialog[&quot;dialogue&quot;]
             for turn_num in range(len(dialogue)):
                 dia_single_turn = dialogue[turn_num]
                 sys_utt = dia_single_turn[&quot;system_transcript&quot;]
                 user_utt = dia_single_turn[&quot;transcript&quot;]
                 state_category = dia_single_turn[&quot;state_category&quot;]
                 span_info = dia_single_turn[&quot;span&quot;]

                 yield self.text_to_instance(sys_utt, user_utt, state_category, span_info)
 @overrides
 def text_to_instance(self, sys_utt, user_utt, state_catgory, span_info):

     tokenized_sys_utt = self._tokenizer.tokenize(sys_utt)
     tokenized_user_utt = self._tokenizer.tokenize(user_utt)
     tokenized_span_info = self._tokenizer.tokenize(span_info)

     tokenized_classifier_input = self._tokenizer.tokenize(&quot;[CLS] &quot;+ sys_utt + &quot; [SEP] &quot;+ user_utt)

     sys_utt_field = TextField(tokenized_sys_utt, self._tokenindexer)
     user_utt_field = TextField(tokenized_user_utt, self._tokenindexer)
     classifier_filed = TextField(tokenized_classifier_input, self._tokenindexer)
     span_field = TextField(tokenized_span_info, self._tokenindexer)
     fields = {&quot;sys_utt&quot;: sys_utt_field,&quot;user_utt&quot;:user_utt_field,&quot;classifier_input&quot;:classifier_filed,&quot;span&quot;:span_field}
     fields['label']=LabelField(state_catgory)
     return Instance(fields)
</code></pre>
<p>I am searching for a long time on net. But no use. Please help or try to give some ideas how to achieve this.</p>
","9594332","","9594332","","2021-03-26 06:25:19","2021-03-26 11:07:02","How to use a pre-trained language model correctly?","<python><nlp><allennlp>","1","2","","","","CC BY-SA 4.0"
"68011338","1","68038643","","2021-06-17 00:03:26","","0","68","<p>I'm training an allennlp crf_tagger. I'm using a predictor which is based on the
<a href=""https://github.com/allenai/allennlp/blob/39c40fe38cd2fd36b3465b0b3c031f54ec824160/allennlp/predictors/sentence_tagger.py#L26"" rel=""nofollow noreferrer"">SentenceTaggerPredictor</a>. The issue is the tokenizer argument - in the case of the SentenceTaggerPredictor there's a language argument.</p>
<p>Since SentenceTaggerPredictor has language=&quot;en_core_web_sm&quot; as a defauly argument, when I do</p>
<pre><code>Predictor.from_path(&quot;model.tar.gz&quot;, &quot;sentence_tagger&quot;)
</code></pre>
<p>The tokenizer is created using the default language. But what happens if the training data was tokenized using a different language. How do I specify the arguments for the predictor in the model <code>config.json</code> such that <code>Predictor.from_path</code> will be constructed using a non-default language?</p>
","2981639","","","","","2021-06-18 16:32:31","Including allennlp predictor arguments in config.json","<prediction><allennlp>","1","0","","","","CC BY-SA 4.0"
"67182864","1","","","2021-04-20 16:31:40","","0","32","<p>I'm not sure what's going on in <a href=""https://github.com/allenai/allennlp/blob/master/scripts/compile_coref_data.sh"" rel=""nofollow noreferrer"">this</a> script which is meant to download and compile some data. The download works for me but then I get an error &quot;file not found&quot;. Should I have downloaded some other data beforehand?</p>
<p>The data is not really important to me. However, if anyone happens to know what's going on, I'm curious. Thanks in advance!</p>
<p>Last couple of logging messages:</p>
<pre><code>...
reference-coreference-scorers/v8.01/test/DataFiles/TC-N-6.response
reference-coreference-scorers/v8.01/test/DataFiles/TC-N.key
reference-coreference-scorers/v8.01/test/test.pl
reference-coreference-scorers/v8.01/test/TestCases.README
----------------------------------------------------------------------------------------------------

could not find the gold parse [./data/files/data/arabic/annotations/nw/ann/02/ann_0283.parse] in the ontonotes distributi

----------------------------------------------------------------------------------------------------
cat: 'conll-2012/v0/data/development/data/english/annotations/*/*/*/*.v0_gold_conll': No such file or directory
cat: 'conll-2012/v0/data/train/data/english/annotations/*/*/*/*.v0_gold_conll': No such file or directory
cat: 'conll-2012/v0/data/test/data/english/annotations/*/*/*/*.v0_gold_conll': No such file or directory
</code></pre>
","12452939","","134204","","2021-04-21 08:07:35","2021-04-22 23:24:45","file not found compile_coref_data.sh","<bash><allennlp>","1","1","","","","CC BY-SA 4.0"
"64978703","1","64979824","","2020-11-24 00:48:08","","1","114","<p>I am trying to install allennlp via pip on the latest version of macOS Catalina. The Python version is 3.9.0. The pip version is 20.2.4.</p>
<p>I was just able to install only a couple of weeks ago, but now I receive the following error when I run the command:</p>
<p><code>pip3 install allennlp allennlp-models</code></p>
<pre><code>ERROR: No matching distribution found for torch&lt;1.8.0,&gt;=1.6.0 (from allennlp)
</code></pre>
<p>This is really bad because a time-sensitive project I am working on right now completely depends on allennlp working. I needed to reinstall due to some git issues, and my virtual env was reset. What can I do to resolve this issue?</p>
<p>Thanks.</p>
<p>EDIT: Somehow I ended up uninstalling 3.9 in homebrew and now I'm stuck at jsonnet. It errors out. I saw some posts about this issue, but none of the recommendations work.</p>
<p>EDIT2: I tried reinstalling brew's python 3.9. jsonnet standalone works again, but then I have the pytorch issue again.</p>
<p>I'm stuck in a loop. Neither the system macOS 3.8 python nor the latest 3.9 home-brew python works. I'd rather get the home-brew python working, but there's no valid version of torch available for that one apparently.</p>
","7361580","","7361580","","2020-11-24 02:19:09","2020-11-24 03:25:19","AllenNLP Torch Version Unavailable","<pip><allennlp>","1","0","","","","CC BY-SA 4.0"
"68682786","1","","","2021-08-06 13:53:51","","1","56","<p>The BERT-based SRL model that Shi and Lin develop (which is currently the the backend of the AllenNLP SRL model) has more consistent advantages over Ouichi et al.'s (2018) ensemble model when using BERT-large, instead of BERT-base. For example, the Shi and Lin model gets better F1 than Ouichi on CoNLL 05 only when using BERT-Large.</p>
<p>So, is it possible to use the AllenNLP SRL model with BERT-large rather than BERT-base?</p>
","2554824","","","","","2021-08-06 13:53:51","Is it possible to use the AllenNLP Semantic Role Labeler with BERT-Large instead of BERT-base?","<bert-language-model><allennlp>","0","8","","","","CC BY-SA 4.0"
"64864557","1","","","2020-11-16 19:33:51","","5","949","<p>I have finetuned the SciBERT model on the SciIE dataset. The repository uses AllenNLP to finetune the model. The training is executed as follows:</p>
<pre><code>python -m allennlp.run train $CONFIG_FILE  --include-package scibert -s &quot;$@&quot; 
</code></pre>
<p>After a successful training I have a model.tar.gz file as an output that contains weights.th, config.json, and vocabulary folder. I have tried to load it in the allenlp predictor:</p>
<pre><code>from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path(&quot;model.tar.gz&quot;)
</code></pre>
<p>But I get the following error:</p>
<blockquote>
<p>ConfigurationError: bert-pretrained not in acceptable choices for
dataset_reader.token_indexers.bert.type: ['single_id', 'characters',
'elmo_characters', 'spacy', 'pretrained_transformer',
'pretrained_transformer_mismatched']. You should either use the
--include-package flag to make sure the correct module is loaded, or use a fully qualified class name in your config file like {&quot;model&quot;:
&quot;my_module.models.MyModel&quot;} to have it imported automatically.</p>
</blockquote>
<p>I have never worked with allenNLP, so I am quite lost about what to do.</p>
<p>For reference, this is the part of the config that describer token indexers</p>
<pre><code>&quot;token_indexers&quot;: {
            &quot;bert&quot;: {
                &quot;type&quot;: &quot;bert-pretrained&quot;,
                &quot;do_lowercase&quot;: &quot;false&quot;,
                &quot;pretrained_model&quot;: &quot;/home/tomaz/neo4j/scibert/model/vocab.txt&quot;,
                &quot;use_starting_offsets&quot;: true
            }
        }
</code></pre>
<p>I am using allenlp version</p>
<p>Name: allennlp
Version: 1.2.1</p>
<p>Edit:</p>
<p>I think I have made a lot of progress, I have to use the same version that was used to train the model and I can import the modules like so:</p>
<pre><code>from allennlp.predictors.predictor import Predictor
from scibert.models.bert_crf_tagger import *
from scibert.models.bert_text_classifier import *
from scibert.models.dummy_seq2seq import *
from scibert.dataset_readers.classification_dataset_reader import *

predictor = Predictor.from_path(&quot;scibert_ner/model.tar.gz&quot;)
dataset_reader=&quot;classification_dataset_reader&quot;)
predictor.predict(
  sentence=&quot;Did Uriah honestly think he could beat The Legend of Zelda in under three hours?&quot;
)
</code></pre>
<p>Now I get an error:</p>
<blockquote>
<p>No default predictor for model type bert_crf_tagger.\nPlease specify a
predictor explicitly</p>
</blockquote>
<p>I know that I can use the <code>predictor_name</code> to specify a predictor explicitly, but I haven't got the faintest idea which name to pick that would work</p>
","6692895","","6692895","","2020-11-19 12:00:01","2020-12-10 03:06:08","How to load a finetuned sciBERT model in AllenNLP?","<python><allennlp>","1","9","1","","","CC BY-SA 4.0"
"66809952","1","66815600","","2021-03-26 01:48:55","","0","51","<p>I'm trying to run the <a href=""https://github.com/allenai/allennlp-models/tree/main/allennlp_models/pair_classification"" rel=""nofollow noreferrer"">pair classification model in AllenNLP repo</a> via config file.</p>
<p>It is appreciated that the repo gives sample of datareader and model file. But without a corresponding config file, I cannot really run the model.</p>
<p>The <a href=""https://guide.allennlp.org/training-and-prediction#2"" rel=""nofollow noreferrer"">example config file of simpleclassification model</a> does't describe enough detailed parameters for pair classification config.</p>
<p>How should write the config file for pair classification? Is there a generic method or systematic tutorial to do so? Any help is appreciated. THX</p>
","10737202","","","","","2021-03-26 11:00:07","How to write the config file for pair classification model in AllenNLP git repo?","<python><nlp><allennlp>","1","0","","","","CC BY-SA 4.0"
"62884591","1","","","2020-07-13 21:31:44","","1","3169","<p>I installed the allennlp package by using <code>pip install allennlp</code>. But when I tried to import it from Jupyter Notebook using this command <code>from allennlp.commands.elmo import ElmoEmbedder</code>, it gave me an error <code>ModuleNotFoundError: No module named 'allennlp.commands.elmo'</code>. I am wondering how should I fix this, I have the Python 3.7 installed and Spacy installed as well but for some reasons, I just cannot import the <code>ElmoEmbedder</code></p>
","13868963","","","","","2020-07-15 12:49:39","ModuleNotFoundError: No module named 'allennlp.commands.elmo'","<python><macos><allennlp>","1","2","","","","CC BY-SA 4.0"
"67968635","1","","","2021-06-14 10:28:47","","1","43","<p>Dear stackoverflow community,</p>
<p>I have the following question: Is it possible to disable fast tokenization in an allennlp model?</p>
<p>I am trying to use the following model in my nlp pipeline but can't use the fast tokenization as it causes issues when multithreaded. My initial thought was to simply replace the tokenizer but this seemed to have no effect. I would greatly appreciate your help on this issue. Please tell me the obviouse thing that i am missing.</p>
<pre><code>from allennlp.predictors.sentence_tagger import SentenceTaggerPredictor
predictor = SentenceTaggerPredictor.from_path(&quot;
            https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz&quot;)

# attempt (not working as expected):

from transformers import RobertaTokenizer
predictor._dataset_reader._token_indexers[&quot;tokens&quot;]._tokenizer = RobertaTokenizer.from_pretrained(&quot;roberta-base&quot;, use_fast=False)

# This still causes problems while being used when multithreaded
# and still calls transformers/tokenization_utils_fast.py

</code></pre>
","9300533","","","","","2021-06-14 10:28:47","disabling fast tokenization in allennlp models","<python><multithreading><tokenize><huggingface-transformers><allennlp>","0","0","","","","CC BY-SA 4.0"
"67378820","1","67439313","","2021-05-04 04:29:28","","0","80","<p>Currently I'm trying to implement lazy loading with allennlp, but can't.
My code is as the followings.</p>
<pre class=""lang-py prettyprint-override""><code>def biencoder_training():
    params = BiEncoderExperiemntParams()
    config = params.opts
    reader = SmallJaWikiReader(config=config)

    # Loading Datasets
    train, dev, test = reader.read('train'), reader.read('dev'), reader.read('test')
    vocab = build_vocab(train)
    vocab.extend_from_instances(dev)

    # TODO: avoid memory consumption and lazy loading
    train, dev, test = list(reader.read('train')), list(reader.read('dev')), list(reader.read('test'))

    train_loader, dev_loader, test_loader = build_data_loaders(config, train, dev, test)
    train_loader.index_with(vocab)
    dev_loader.index_with(vocab)

    embedder = emb_returner()
    mention_encoder, entity_encoder = Pooler_for_mention(word_embedder=embedder), \
                                      Pooler_for_cano_and_def(word_embedder=embedder)

    model = Biencoder(mention_encoder, entity_encoder, vocab)

    trainer = build_trainer(lr=config.lr,
                            num_epochs=config.num_epochs,
                            model=model,
                            train_loader=train_loader,
                            dev_loader=dev_loader)
    trainer.train()

    return model
</code></pre>
<p>When I commented-out <code> train, dev, test = list(reader.read('train')), list(reader.read('dev')), list(reader.read('test'))</code>, iterator doesn't work and training is conducted with 0 sample.</p>
<pre><code>Building the vocabulary
100it [00:00, 442.15it/s]01, 133.57it/s]
building vocab: 100it [00:01, 95.84it/s]
100it [00:00, 413.40it/s]
100it [00:00, 138.38it/s]
You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
0it [00:00, ?it/s]
0it [00:00, ?it/s]
</code></pre>
<p>I'd like to know if there is any solution for avoid this.
Thanks.</p>
<hr />
<p><strong>Supplement, added at fifth, May.</strong></p>
<p>Currently I am trying to avoid putting all of each sample data on top of memory before training the model.</p>
<p>So I have implemented the _read method as a generator. My understanding is that by calling this method and wrapping it with SimpleDataLoader, I can actually pass the data to the model.</p>
<p>In the DatasetReader, the code for the _read method looks like this. It is my understanding that this is intended to be a generator that avoids memory consumption.</p>
<pre class=""lang-py prettyprint-override""><code>    @overrides
    def _read(self, train_dev_test_flag: str) -&gt; Iterator[Instance]:
        '''
        :param train_dev_test_flag: 'train', 'dev', 'test'
        :return: list of instances
        '''
        if train_dev_test_flag == 'train':
            dataset = self._train_loader()
            random.shuffle(dataset)
        elif train_dev_test_flag == 'dev':
            dataset = self._dev_loader()
        elif train_dev_test_flag == 'test':
            dataset = self._test_loader()
        else:
            raise NotImplementedError(
                &quot;{} is not a valid flag. Choose from train, dev and test&quot;.format(train_dev_test_flag))

        if self.config.debug:
            dataset = dataset[:self.config.debug_data_num]

        for data in tqdm(enumerate(dataset)):
            data = self._one_line_parser(data=data, train_dev_test_flag=train_dev_test_flag)
            yield self.text_to_instance(data)
</code></pre>
<p>Also, <code>build_data_loaders</code> actually looks like this.</p>
<pre class=""lang-py prettyprint-override""><code>def build_data_loaders(config,
    train_data: List[Instance],
    dev_data: List[Instance],
    test_data: List[Instance]) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:

    train_loader = SimpleDataLoader(train_data, config.batch_size_for_train, shuffle=False)
    dev_loader = SimpleDataLoader(dev_data, config.batch_size_for_eval, shuffle=False)
    test_loader = SimpleDataLoader(test_data, config.batch_size_for_eval, shuffle=False)

    return train_loader, dev_loader, test_loader
</code></pre>
<p>But, by somewhat reason I don't know, this code doesn't work.</p>
<pre><code>def biencoder_training():
    params = BiEncoderExperiemntParams()
    config = params.opts
    reader = SmallJaWikiReader(config=config)

    # Loading Datasets
    train, dev, test = reader.read('train'), reader.read('dev'), reader.read('test')
    vocab = build_vocab(train)
    vocab.extend_from_instances(dev)

    train_loader, dev_loader, test_loader = build_data_loaders(config, train, dev, test)
    train_loader.index_with(vocab)
    dev_loader.index_with(vocab)

    embedder = emb_returner()
    mention_encoder, entity_encoder = Pooler_for_mention(word_embedder=embedder), \
                                      Pooler_for_cano_and_def(word_embedder=embedder)

    model = Biencoder(mention_encoder, entity_encoder, vocab)

    trainer = build_trainer(lr=config.lr,
                            num_epochs=config.num_epochs,
                            model=model,
                            train_loader=train_loader,
                            dev_loader=dev_loader)
    trainer.train()

    return model
</code></pre>
<p>In this code, the SimpleDataLoader will wrap the generator type as it is. I would like to do the lazy loading that allennlp did in the 0.9 version.</p>
<p>But this code iterates training over 0 instances, so currently I have added</p>
<p><code>train, dev, test = list(reader.read('train')), list(reader.read('dev')), list(reader.read('test'))</code></p>
<p>before</p>
<p><code>train_loader, dev_loader, test_loader = build_data_loaders(config, train, dev, test)</code>.</p>
<p>And it works. But this means that I can't train or evaluate the model until I have all the instances in memory. Rather, I want each batch to be called into memory only when it is time to train.</p>
","11036609","","11036609","","2021-05-05 05:13:00","2021-05-07 17:24:38","Can't do lazy loading with allennlp","<nlp><pytorch><bert-language-model><allennlp>","1","0","","","","CC BY-SA 4.0"
"56113646","1","56116091","","2019-05-13 13:41:48","","2","564","<p>Please first search our GitHub repository for similar questions.  If you don't find a similar example you can use the following template:</p>

<p><strong>System (please complete the following information):</strong>
 - OS: Ubunti 18.04
 - Python version: 3.6.7
 - AllenNLP version: v0.8.3
 - PyTorch version: 1.1.0</p>

<p><strong>Question</strong>
When I Try to predict string using SimpleSeq2SeqPredictor, It always show that</p>

<pre class=""lang-sh prettyprint-override""><code>Traceback (most recent call last):
  File ""predict.py"", line 96, in &lt;module&gt;
    p = predictor.predict(i)
  File ""venv/lib/python3.6/site-packages/allennlp/predictors/seq2seq.py"", line 17, in predict
    return self.predict_json({""source"" : source})
  File ""/venv/lib/python3.6/site-packages/allennlp/predictors/predictor.py"", line 56, in predict_json
    return self.predict_instance(instance)
  File ""/venv/lib/python3.6/site-packages/allennlp/predictors/predictor.py"", line 93, in predict_instance
    outputs = self._model.forward_on_instance(instance)
  File ""/venv/lib/python3.6/site-packages/allennlp/models/model.py"", line 124, in forward_on_instance
    return self.forward_on_instances([instance])[0]
  File ""/venv/lib/python3.6/site-packages/allennlp/models/model.py"", line 153, in forward_on_instances
    outputs = self.decode(self(**model_input))
  File ""/venv/lib/python3.6/site-packages/allennlp/models/encoder_decoders/simple_seq2seq.py"", line 247, in decode
    predicted_indices = output_dict[""predictions""]
KeyError: 'predictions'
</code></pre>

<p>I try to do a translate system, but I am newbie, most of code come from
<a href=""https://github.com/mhagiwara/realworldnlp/blob/master/examples/mt/mt.py"" rel=""nofollow noreferrer"">https://github.com/mhagiwara/realworldnlp/blob/master/examples/mt/mt.py</a>
<a href=""http://www.realworldnlpbook.com/blog/building-seq2seq-machine-translation-models-using-allennlp.html"" rel=""nofollow noreferrer"">http://www.realworldnlpbook.com/blog/building-seq2seq-machine-translation-models-using-allennlp.html</a></p>

<p>this is my training code</p>

<pre class=""lang-py prettyprint-override""><code>EN_EMBEDDING_DIM = 256
ZH_EMBEDDING_DIM = 256
HIDDEN_DIM = 256
CUDA_DEVICE = 0
prefix = 'small'

reader = Seq2SeqDatasetReader(
    source_tokenizer=WordTokenizer(),
    target_tokenizer=CharacterTokenizer(),
    source_token_indexers={'tokens': SingleIdTokenIndexer()},
    target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')},
    lazy = True)
train_dataset = reader.read(f'./{prefix}-data/train.tsv')
validation_dataset = reader.read(f'./{prefix}-data/val.tsv')

vocab = Vocabulary.from_instances(train_dataset,
                                    min_count={'tokens': 3, 'target_tokens': 3})

en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),
                            embedding_dim=EN_EMBEDDING_DIM)
# encoder = PytorchSeq2SeqWrapper(
#     torch.nn.LSTM(EN_EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))
encoder = StackedSelfAttentionEncoder(input_dim=EN_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, projection_dim=128, feedforward_hidden_dim=128, num_layers=1, num_attention_heads=8)

source_embedder = BasicTextFieldEmbedder({""tokens"": en_embedding})

# attention = LinearAttention(HIDDEN_DIM, HIDDEN_DIM, activation=Activation.by_name('tanh')())
# attention = BilinearAttention(HIDDEN_DIM, HIDDEN_DIM)
attention = DotProductAttention()

max_decoding_steps = 20   # TODO: make this variable
model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,
                        target_embedding_dim=ZH_EMBEDDING_DIM,
                        target_namespace='target_tokens',
                        attention=attention,
                        beam_size=8,
                        use_bleu=True)
optimizer = optim.Adam(model.parameters())
iterator = BucketIterator(batch_size=32, sorting_keys=[(""source_tokens"", ""num_tokens"")])

iterator.index_with(vocab)
if torch.cuda.is_available():
    cuda_device = 0
    model = model.cuda(cuda_device)
else:
    cuda_device = -1
trainer = Trainer(model=model,
                    optimizer=optimizer,
                    iterator=iterator,
                    train_dataset=train_dataset,
                    validation_dataset=validation_dataset,
                    num_epochs=50,
                    serialization_dir=f'ck/{prefix}/',
                    cuda_device=cuda_device)

# for i in range(50):
    # print('Epoch: {}'.format(i))
trainer.train()

predictor = SimpleSeq2SeqPredictor(model, reader)

for instance in itertools.islice(validation_dataset, 10):
    print('SOURCE:', instance.fields['source_tokens'].tokens)
    print('GOLD:', instance.fields['target_tokens'].tokens)
    print('PRED:', predictor.predict_instance(instance)['predicted_tokens'])

# Here's how to save the model.
with open(f""ck/{prefix}/manually_save_model.th"", 'wb') as f:
    torch.save(model.state_dict(), f)
vocab.save_to_files(f""ck/{prefix}/vocabulary"")
</code></pre>

<p>and this is my predict code</p>

<pre class=""lang-py prettyprint-override""><code>EN_EMBEDDING_DIM = 256
ZH_EMBEDDING_DIM = 256
HIDDEN_DIM = 256
CUDA_DEVICE = 0
prefix = 'big'

reader = Seq2SeqDatasetReader(
    source_tokenizer=WordTokenizer(),
    target_tokenizer=CharacterTokenizer(),
    source_token_indexers={'tokens': SingleIdTokenIndexer()},
    target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')},
    lazy = True)
# train_dataset = reader.read(f'./{prefix}-data/train.tsv')
# validation_dataset = reader.read(f'./{prefix}-data/val.tsv')

# vocab = Vocabulary.from_instances(train_dataset,
#                                     min_count={'tokens': 3, 'target_tokens': 3})
vocab = Vocabulary.from_files(""ck/small/vocabulary"")

en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),
                            embedding_dim=EN_EMBEDDING_DIM)
# encoder = PytorchSeq2SeqWrapper(
#     torch.nn.LSTM(EN_EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))
encoder = StackedSelfAttentionEncoder(input_dim=EN_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, projection_dim=128, feedforward_hidden_dim=128, num_layers=1, num_attention_heads=8)

source_embedder = BasicTextFieldEmbedder({""tokens"": en_embedding})

# attention = LinearAttention(HIDDEN_DIM, HIDDEN_DIM, activation=Activation.by_name('tanh')())
# attention = BilinearAttention(HIDDEN_DIM, HIDDEN_DIM)
attention = DotProductAttention()

max_decoding_steps = 20   # TODO: make this variable
model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,
                        target_embedding_dim=ZH_EMBEDDING_DIM,
                        target_namespace='target_tokens',
                        attention=attention,
                        beam_size=8,
                        use_bleu=True)

# And here's how to reload the model.
with open(""./ck/small/best.th"", 'rb') as f:
    model.load_state_dict(torch.load(f))

predictor = Seq2SeqPredictor(model, dataset_reader=reader)
# print(predictor.predict(""The dog ate the apple""))


test = [
    'Surely ,he has no power over those who believe and put their trust in their Lord ;',
    'And assuredly We have destroyed the generations before you when they did wrong ,while their apostles came unto them with the evidences ,and they were not such as to believe . In this wise We requite the sinning people .',
    'And warn your tribe ( O Muhammad SAW ) of near kindred .',
    'And to the Noble Messengers whom We have mentioned to you before ,and to the Noble Messengers We have not mentioned to you ; and Allah really did speak to Moosa .',
    'It is He who gave you hearing ,sight ,and hearts ,but only few of you give thanks .',
    'spreading in them much corruption ?',
    'That will envelop the people . This will be a painful punishment .',
    'When you received it with your tongues and spoke with your mouths what you had no knowledge of ,and you deemed it an easy matter while with Allah it was grievous .',
    'of which you are disregardful .',
    'Whoever disbelieves ,then the calamity of his disbelief is only on him ; and those who do good deeds ,are preparing for themselves .'
]




for i in test:
    p = predictor.predict(i) # &lt;------------------- ERROR !!!!!!!
    print(p) 
</code></pre>

<p>Am I do something wrong ?</p>
","4754280","","","","","2019-05-13 16:06:31","KeyError: 'predictions' When use SimpleSeq2SeqPredictor to predict string","<python><pytorch><allennlp>","1","0","1","","","CC BY-SA 4.0"
"68591645","1","","","2021-07-30 13:20:04","","0","22","<p>Windows machine, Ubuntu partition, Anaconda 4.10.1, Python 3.6.13, AllenNLP 2.2.0</p>
<p>Trying to use AllenNLP as part of Document retrieval task, but allennlp_models won't load properly. Using the final code block in <a href=""https://medium.com/geekculture/how-to-use-allennlps-pretrained-ner-model-in-2021-b60335904fc3"" rel=""nofollow noreferrer"">this blog post</a> as an example, I get the following errorss:</p>
<pre><code>Plugin allennlp_models could not be loaded: No module named 'allennlp.modules.transformer.t5'

...

allennlp.common.checks.ConfigurationError: crf_tagger is not a registered name for Model. You probably need to use the --include-package flag to load your custom code. Alternatively, you can specify your choices using fully-qualified paths, e.g. {&quot;model&quot;: &quot;my_module.models.MyModel&quot;} in which case they will be automatically imported correctly.
</code></pre>
<p>Tried:</p>
<ul>
<li>pip, conda, git clone installs for allennlp and allennlp_models</li>
<li>steps in blog post above</li>
<li>--include-package is supposed to solve the Config error, but will this even work if _models isn't loaded properly? Doubtful.</li>
<li>import allennlp-models.[specific-models]</li>
</ul>
<p>Is this a problem in a particular release of allennlp that I can avoid? Loading consistently fails on the t5 transformer- can I just remove this and hope for the best?? Many thanks, etc etc</p>
","12243745","","","","","2021-07-30 13:20:04","Plugin allennlp_models could not be loaded: No module named 'allennlp.modules.transformer.t5'","<python><allennlp>","0","4","","","","CC BY-SA 4.0"
"65386745","1","","","2020-12-21 02:10:12","","3","438","<p>I am trying to run the AllenNLP demo <a href=""https://demo.allennlp.org/semantic-role-labeling"" rel=""nofollow noreferrer"">https://demo.allennlp.org/semantic-role-labeling</a>. When I run the command line or the Python version via Juptyer I get the error mentioned below.</p>
<p>I have installed the required libraries?
pip install allennlp==1.0.0 allennlp-models==1.0.0</p>
<p>Running this gives me an error:</p>
<pre><code>from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.11.19.tar.gz&quot;)
predictor.predict(
  sentence=&quot;Did Uriah honestly think he could beat the game in under three hours?&quot;
)
</code></pre>
<p><strong>Update</strong>: If I use an older gz download I do not get the error
I grabbed this one from their public models site bert-base-srl-2020.09.03.tar.gz</p>
<pre><code>from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.09.03.tar.gz&quot;)

text = &quot;Did Uriah honestly think he could beat the game in under three hours?&quot;
predictor.predict(sentence=text)
</code></pre>
<p>Error:</p>
<pre><code>RuntimeError: Error(s) in loading state_dict for SrlBert:
    Unexpected key(s) in state_dict: &quot;bert_model.embeddings.position_ids&quot;.
</code></pre>
<p>OS &amp; versions:</p>
<pre><code>Python: 3.8.6
pip: 20.2.1
Mac OS: Catalina 10.15.7
</code></pre>
<p>Are there some dependencies I am maybe missing for BERT? I haven't had any issues with the other AllenNLP examples.</p>
<p><strong>I added an answer but they deleted it. This is how you resolve this:</strong></p>
<p>After posting on github, found out from the AllenNLP folks that it is a version issue. I needed to be using allennlp=1.3.0 and the latest model. Now it works as expected.</p>
<pre><code>This should be fixed in the latest allennlp 1.3 release. Also, the latest archive file is structured-prediction-srl-bert.2020.12.15.tar.gz.
</code></pre>
<p><a href=""https://github.com/allenai/allennlp/issues/4876"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp/issues/4876</a></p>
","192204","","192204","","2021-02-22 13:11:27","2021-02-22 13:11:27","AllenNLP Semantic Role Label - SRLBert Error","<python><bert-language-model><allennlp>","0","0","","","","CC BY-SA 4.0"
"67337066","1","","","2021-04-30 16:03:46","","1","213","<p>I'm using python 3.7 in google colab. I install <code>allennlp=2.4.0</code> and <code>allennlp-models</code>.</p>
<p>When I run my code, I get this error:</p>
<pre><code>from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter
</code></pre>
<blockquote>
<p>ModuleNotFoundError: No module named
'allennlp.data.tokenizers.word_splitter'</p>
</blockquote>
","5028133","","6117017","","2021-04-30 16:05:35","2021-04-30 16:05:35","No module named 'allennlp.data.tokenizers.word_splitter'","<python-3.x><pytorch><allennlp>","0","3","","","","CC BY-SA 4.0"
"68011769","1","68060646","","2021-06-17 01:28:10","","0","94","<p>Is there a code to perform coreference resolution in AllenNLP in Python?</p>
<p>The existing code samples on the internet don't seem to work unfortunately.</p>
","9372256","","","","","2021-06-20 22:51:47","Is there a code to perform coreference resolution in AllenNLP in Python?","<python><machine-learning><nlp><allennlp><coreference-resolution>","1","2","","","","CC BY-SA 4.0"
"66551801","1","","","2021-03-09 17:22:58","","0","50","<p><strong>System</strong></p>
<p>OS: Ubuntu
Python version: 3.6.7
AllenNLP version: 1.0.0</p>
<p><strong>QUESTION</strong>: With the following command I am able to train the model but I can't tell if the weights are being updated for fine-tuning or its just training a new model?</p>
<p>I am trying to fine tune a reading comprehension model on new data (that is in SQuAD format) using the following command:</p>
<p><code>allennlp train fine_tune_config.json -s fine-tune-test/</code> and the following config:</p>
<pre><code>{
    &quot;dataset_reader&quot;: {
      &quot;type&quot;: &quot;squad&quot;,
      &quot;token_indexers&quot;: {
        &quot;tokens&quot;: {
          &quot;type&quot;: &quot;single_id&quot;,
          &quot;lowercase_tokens&quot;: true
        },
        &quot;elmo&quot;: {
          &quot;type&quot;: &quot;elmo_characters&quot;
        },
        &quot;token_characters&quot;: {
          &quot;type&quot;: &quot;characters&quot;,
          &quot;character_tokenizer&quot;: {
            &quot;byte_encoding&quot;: &quot;utf-8&quot;,
            &quot;start_tokens&quot;: [259],
            &quot;end_tokens&quot;: [260]
          },
          &quot;min_padding_length&quot;: 5
        }
      }
    },
    &quot;train_data_path&quot;: &quot;train-v2.0_trimed.json&quot;,
    &quot;validation_data_path&quot;: &quot;validation_data.json&quot;,
    &quot;model&quot;: {
      &quot;type&quot;: &quot;bidaf&quot;,
      &quot;text_field_embedder&quot;: {
          &quot;token_embedders&quot;: {
              &quot;tokens&quot;: {
                  &quot;type&quot;: &quot;embedding&quot;,
                  &quot;pretrained_file&quot;: &quot;glove.6B.100d.txt&quot;,
                  &quot;embedding_dim&quot;: 100,
                  &quot;trainable&quot;: true
              },
              &quot;elmo&quot;:{
                  &quot;type&quot;: &quot;elmo_token_embedder&quot;,
                //   &quot;options_file&quot;: &quot;https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json&quot;,
                &quot;options_file&quot;: &quot;options_file.json&quot;,
                //   &quot;weight_file&quot;: &quot;https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5&quot;,
                &quot;weight_file&quot;: &quot;elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5&quot;,
                  &quot;do_layer_norm&quot;: false,
                  &quot;dropout&quot;: 0.0
              },
              &quot;token_characters&quot;: {
                  &quot;type&quot;: &quot;character_encoding&quot;,
                  &quot;embedding&quot;: {
                  &quot;num_embeddings&quot;: 262,
                  &quot;embedding_dim&quot;: 16
                  },
                  &quot;encoder&quot;: {
                  &quot;type&quot;: &quot;cnn&quot;,
                  &quot;embedding_dim&quot;: 16,
                  &quot;num_filters&quot;: 100,
                  &quot;ngram_filter_sizes&quot;: [5]
                  },
                  &quot;dropout&quot;: 0.2
              }
          }
      },
      &quot;num_highway_layers&quot;: 2,
      &quot;phrase_layer&quot;: {
        &quot;type&quot;: &quot;lstm&quot;,
        &quot;bidirectional&quot;: true,
        &quot;input_size&quot;: 1224,
        &quot;hidden_size&quot;: 100,
        &quot;num_layers&quot;: 1
      },
      &quot;matrix_attention&quot;: {
        &quot;type&quot;: &quot;linear&quot;,
        &quot;combination&quot;: &quot;x,y,x*y&quot;,
        &quot;tensor_1_dim&quot;: 200,
        &quot;tensor_2_dim&quot;: 200
      },
      &quot;modeling_layer&quot;: {
        &quot;type&quot;: &quot;lstm&quot;,
        &quot;bidirectional&quot;: true,
        &quot;input_size&quot;: 800,
        &quot;hidden_size&quot;: 100,
        &quot;num_layers&quot;: 2,
        &quot;dropout&quot;: 0.2
      },
      &quot;span_end_encoder&quot;: {
        &quot;type&quot;: &quot;lstm&quot;,
        &quot;bidirectional&quot;: true,
        &quot;input_size&quot;: 1400,
        &quot;hidden_size&quot;: 100,
        &quot;num_layers&quot;: 1
      },
      &quot;dropout&quot;: 0.2
    },
    &quot;data_loader&quot;: {
      &quot;batch_sampler&quot;: {
        &quot;type&quot;: &quot;bucket&quot;,
        &quot;batch_size&quot;: 40
      }
    },
    &quot;trainer&quot;: {
      &quot;num_epochs&quot;: 20,
      &quot;grad_norm&quot;: 5.0,
      &quot;patience&quot;: 10,
      &quot;validation_metric&quot;: &quot;+em&quot;,
      &quot;learning_rate_scheduler&quot;: {
        &quot;type&quot;: &quot;reduce_on_plateau&quot;,
        &quot;factor&quot;: 0.5,
        &quot;mode&quot;: &quot;max&quot;,
        &quot;patience&quot;: 2
      },
      &quot;optimizer&quot;: {
        &quot;type&quot;: &quot;adam&quot;,
        &quot;betas&quot;: [0.9, 0.9]
      }
    }
  } 
'''

</code></pre>
","5170278","","","","","2021-03-13 01:21:17","Fine-tune a reading comprehension model","<allennlp>","1","0","","","","CC BY-SA 4.0"
"66577876","1","66609234","","2021-03-11 07:08:50","","0","107","<p>I needed to build pytorch from source in order to use the latest cuda toolkit - hence I did that. My environment is a conda enviornment.</p>
<p>The package I am trying to install is <strong>allennlp</strong>.</p>
<p><code>Pip list</code> is not showing <strong>torch</strong> installed, conda is showing it is.
Tried building allennlp from source through:</p>
<pre><code>conda skeleton pypi &lt;package&gt;
conda build &lt;package&gt;
</code></pre>
<p><strong>conda build</strong> is also causing problems due to incompability with pip packages etc...</p>
<p>I am just wondering if there is a simpler way to do this?</p>
<p><strong>For example:</strong> Tell pip that torch is already installed so it stops uninstalling the current torch (why can't it just pick it up as installed).
Appreciate the help!</p>
","2537747","","","","","2021-03-13 01:19:39","Pip is uninstalling pytorch built from source when i use 'pip install <some other package that needs pytorch>'. Is there a way?","<python><pip><pytorch><conda><allennlp>","1","2","","","","CC BY-SA 4.0"
"62801889","1","62812313","","2020-07-08 19:01:27","","3","148","<p>I'm new to Allennlp, and this is my first time trying it out. I already installed all the required libraries <code>!pip install allennlp !pip install --pre allennlp-models</code> and my code should be fine too, but I still get this error message saying: <code>ConfigurationError: key &quot;matrix_attention&quot; is required at location &quot;model.&quot;</code>
Here's my code:</p>
<pre><code>import sys

from allennlp.predictors.predictor import Predictor
import allennlp_models

predictor = Predictor.from_path(
&quot;https://storage.googleapis.com/allennlp-public-models/bidaf-elmo-model-2018.11.30-charpad.tar.gz&quot;
)

prediction = predictor.predict(
passage=&quot;Life expectancy in Asia is at 68 years&quot;, question=&quot;What is the life expectancy in Asia&quot;
)
print(prediction[&quot;best_span_str&quot;])
</code></pre>
<p>Do you have any idea how to fix this error? I'm using macOS Catalina and Python 3.6. I really don't know what to do now, so I really need your help. Thanks in advance!</p>
","13693395","","","","","2020-07-09 09:58:45","Allennlp ConfigurationError: key ""matrix_attention"" is required at location ""model.""","<nlp><allennlp>","1","0","","","","CC BY-SA 4.0"
"66092443","1","","","2021-02-07 19:53:03","","0","141","<p>I am using <strong>AllenNLP</strong> to train a hierarchical <strong>attention network model</strong>. My training dataset consists of a list of <strong>JSON objects</strong> (eg, each object in the list is a <strong>JSON object</strong> with keys := [&quot;text&quot;, &quot;label&quot;]. The value associated with the text key is a list of lists, eg:</p>
<pre><code>[{&quot;text&quot;:[[&quot;i&quot;, &quot;feel&quot;, &quot;sad&quot;], [&quot;not&quot;, &quot;sure&quot;, &quot;i&quot;, &quot;guess&quot;, &quot;the&quot;, &quot;weather&quot;]], &quot;label&quot;:0} ... {&quot;text&quot;:[[str]], &quot;label&quot;:int}] 
</code></pre>
<p>My DatasetReader class looks like:</p>
<pre><code>@DatasetReader.register(&quot;my_reader&quot;)
class TranscriptDataReader(DatasetReader):
    def __init__(self,
                 token_indexers: Optional[Dict[str, TokenIndexer]] = None,
                 lazy: bool = True) -&gt; None:
        super().__init__(lazy)
        self._token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}

    def _read(self, file_path: str) -&gt; Iterator[Instance]:
        with open(file_path, 'r') as f:
            data = json.loads(f.read())
            for _,data_json in enumerate(data):
                sent_list = []
                for segment in data_json[&quot;text&quot;]:
                    sent_list.append(self.get_text_field(segment))
                yield self.create_instance(sent_list, str(data_json[&quot;label&quot;]))

    def get_text_field(self, segment):
        return TextField([Token(token.lower()) for token in segment],self._token_indexers)


    def create_instance(self, sent_list, label):
        label_field = LabelField(label, skip_indexing=False)
        fields = {'tokens': ListField(sent_list), 'label': label_field}
        return Instance(fields)

</code></pre>
<p>and in my config file, I have:</p>
<pre><code>{
  dataset_reader: {
    type: 'my_reader',
  },

  train_data_path: 'data/train.json',
  validation_data_path: 'data/dev.json',

 data_loader: {
    batch_sampler: {
      type: 'bucket',
      batch_size: 10
    }
 },

</code></pre>
<p>I have tried (alternatively) setting the <code>lazy</code> param for the dataset reader to <code>True</code> and <code>False</code>.</p>
<ul>
<li>When set to <code>True</code>, the model is able to train, however, I observe that only one train and one dev instance actually get loaded, when my dataset contains ~100.</li>
<li>When set to <code>False</code>, I've modified the <code>yield</code> line in <code>_read</code> to be <code>return</code>; however, this causes a type error in the base vocabulary class. I've also tried keeping the <code>yield</code> as is when set to <code>False</code>; in this case, no instances get loaded at all, and since the set of instances is empty, the vocabulary does not get instantiated, and the embedding class throws an error.</li>
</ul>
<p>Would appreciate pointers, and/or tips for debugging.</p>
","3416089","","3416089","","2021-02-07 20:27:56","2021-02-16 06:01:39","AllenNLP DatasetReader: only loads a single instance, instead of iterating over all instances in the training dataset","<python><nlp><pytorch><allennlp>","2","1","","","","CC BY-SA 4.0"
"68384647","1","","","2021-07-14 20:23:38","","0","70","<p>I use <code>.jsonnet</code> file as a template for <a href=""https://github.com/allenai/allennlp"" rel=""nofollow noreferrer"">AllenNLP</a> model config</p>
<p>I search through hyperparameter space with <a href=""https://optuna.org/"" rel=""nofollow noreferrer"">Optuna</a> package and evaluate that template on each step with suggested hyperparameters as <code>extVar</code>s. Then I train a model and save it to an archive.</p>
<p>The problem is, apart from hyperparameters, some parts of my <code>.jsonnet</code> config include file paths relative to a <code>extVar</code> variable(a dir inside my package).  Those file paths get rendered, too and in the end I have absolute paths in my <code>model.tar.gz</code>, which is wrong, as they may even not exist on a machine loading that archive</p>
<p>.jsonnet:</p>
<pre><code>{
...
  &quot;train_data_path&quot;: std.extVar(&quot;TRAIN_DATA_PATH&quot;),
  &quot;validation_data_path&quot;: std.extVar(&quot;VALID_DATA_PATH&quot;),
...
}
</code></pre>
<p>rendered.json :</p>
<pre class=""lang-json prettyprint-override""><code>{
...
  &quot;train_data_path&quot;: &quot;/home/user/datasets/train.json&quot;,
  &quot;validation_data_path&quot;: &quot;/home/user/datasets/valid.json&quot;,
...
}
</code></pre>
<p>So I would like to save the original path expression instead and supply an environment variable on loading, but I haven`t found a way to serialize a .jsonnet file in python, only .json</p>
","3659405","","","","","2021-07-23 04:35:10","Can a .jsonnet file be constructed in pure python","<python><allennlp><jsonnet><optuna>","1","1","","","","CC BY-SA 4.0"
"64315396","1","64323043","","2020-10-12 09:44:54","","0","33","<p>I wish disable serializing all model/state weights in standard AllenNLP model training via the use of <code>jsonnet</code> config files.</p>
<p>The reason for this is that I am running automatic hyperparameter optimization using Optuna.
Testing dozens of models fills up a drive pretty quickly.
I already have disabled the checkpointer by setting <code>num_serialized_models_to_keep</code> to <code>0</code>:</p>
<pre><code>trainer +: {
    checkpointer +: {
        num_serialized_models_to_keep: 0,
    },
</code></pre>
<p>I do not wish to set <code>serialization_dir</code> to <code>None</code> as I still want the default behaviour regarding logging of intermediate metrics, etc. I only want to <strong>disable the default model state, training state, and best model weights writing</strong>.</p>
<p>Besides the option I set above, are there any default trainer or checkpointer options to disable all serialization of model weights? I checked the API docs and webpage but could not find any.</p>
<p>If I need to define the functionality for such an option myself, which base function(s) from the AllenNLP should I override in my Model subclass?</p>
<p>Alternatively, is their any utility for cleaning up intermediate model state when training is concluded?</p>
<p><strong>EDIT:</strong> <a href=""https://stackoverflow.com/a/64323043/2425270"">@petew's answer</a> shows the solution for a custom checkpointer, but I am not clear on how to make this code findable to <code>allennlp train</code> for my use-case.</p>
<p>I wish to make the custom_checkpointer callable from a config file as below:</p>
<pre><code>trainer +: {
    checkpointer +: {
        type: empty,
    },
</code></pre>
<p>What would be best practice to load the checkpointer when calling <code>allennlp train --include-package &lt;$my_package&gt;</code>?</p>
<p>I have my_package with submodules in subdirectories such as <code>my_package/models</code>s and <code>my_package/training</code>.
I would like to place the custom checkpointer code in <code>my_package/training/custom_checkpointer.py</code>
My main model is located in <code>my_package/models/main_model.py</code>.
Do I have to edit or import any code/functions in my main_model class to use the custom checkpointer?</p>
","2425270","","2425270","","2020-10-13 11:26:50","2020-10-13 11:26:50","How to disable model/weight serialization fully with AllenNLP settings?","<allennlp>","1","0","","","","CC BY-SA 4.0"
"67484439","1","","","2021-05-11 09:55:57","","0","59","<p>I am looking for a way to utilise Allen NLP interpret module with HuggingFace transformers BERT model of architecture BertForSequenceClassification (trained with pytorch).</p>
<p>I've found an Allen NLP guide regarding common usage of interpret module with models that are structurized differently than my model.</p>
<p>Could someone help me and describe what could be the proper approach at converting BertForSequenceClassification model that could be loaded into <code>Predictor</code> object?</p>
<p>Guide sample found at <a href=""https://guide.allennlp.org/interpret#3"" rel=""nofollow noreferrer"">https://guide.allennlp.org/interpret#3</a> :</p>
<pre><code>from allennlp.interpret.saliency_interpreters import SimpleGradient
from allennlp.predictors import Predictor

inputs = {&quot;sentence&quot;: &quot;a very well-made, funny and entertaining picture.&quot;}
archive = (
    &quot;https://storage.googleapis.com/allennlp-public-models/&quot;
    &quot;basic_stanford_sentiment_treebank-2020.06.09.tar.gz&quot;
)
predictor = Predictor.from_path(archive)
interpreter = SimpleGradient(predictor)
interpretation = interpreter.saliency_interpret_from_json(inputs)

print(interpretation)
</code></pre>
","14661154","","14661154","","2021-05-11 10:04:31","2021-05-11 10:04:31","Loading HF transformers BERT model into Allen NLP interpret module","<huggingface-transformers><allennlp>","0","2","","","","CC BY-SA 4.0"
"67498622","1","","","2021-05-12 06:52:35","","0","10","<p>I want to create a new train command for allennlp. Is it possible to run it with &quot;allennlp my_own_command&quot; ?</p>
","5081054","","","","","2021-05-14 16:31:08","Can you write your own customized allennlp command?","<allennlp>","1","0","","","","CC BY-SA 4.0"
"68349445","1","","","2021-07-12 14:49:12","","1","28","<p>I use AWS SageMaker to run a training with AllenNLP. In order to track the loss and metrics I need to have them printed on the log INFO level during training (or at least after each epoch). However, when I run the training all loss and metric information is printed to the console without using the logger:</p>
<pre><code>2021-07-12 16:39:25,799 - INFO - allennlp.training.trainer - Epoch 2/24
2021-07-12 16:39:25,803 - INFO - allennlp.training.trainer - Worker 0 memory usage: 1.5G
2021-07-12 16:39:25,806 - INFO - allennlp.training.trainer - Training
accuracy: 0.1116, batch_loss: 0.4598, loss: 0.4742 ||: 100%|##########| 8/8 [00:13&lt;00:00,  1.64s/it]
2021-07-12 16:39:40,229 - INFO - allennlp.training.trainer - Validating
accuracy: 0.2000, batch_loss: 0.4377, loss: 0.4215 ||: 100%|##########| 2/2 [00:03&lt;00:00,  1.87s/it]
</code></pre>
<p>So far, I could not find anything in the issues or on StackOverflow. As I said, having the loss and metrics logged on INFO level once per epoch would be totally fine.</p>
<p>Also <a href=""https://github.com/allenai/allennlp-guide/issues/135"" rel=""nofollow noreferrer"">in this example</a> it seems like the loss and metrics are logged the way I would like to have it.</p>
","3414626","","","","","2021-07-12 14:49:12","Log loss and metrics on INFO level","<pytorch><tensorboard><allennlp>","0","3","","","","CC BY-SA 4.0"
"65370772","1","65381775","","2020-12-19 14:21:49","","0","137","<p>I am trying to parse the JSON object that gets returned by the allennlp predictor. I was able to find a helpful function to find all of the children values, but what I really want to do with the dependencies, is given an entity &quot;man&quot; can I get the associated attributes from the JSON object.</p>
<p>Example sentence: &quot;When I was walking to the park yesterday, I saw a man wearing a blue shirt.&quot;</p>
<p>The dependency tree has wearing, blue, shirt, etc. that is associated with the entity. How can I get the associated JSON block back for man in that structure? I am not sure how I can modify my helper function or develop another one to get that block out of the JSON output. Any help or suggestions would be greatly appreciated.</p>
<p>AllenNLP Code:</p>
<pre><code>text = &quot;When I was walking to the park yesterday, I saw a man wearing a blue shirt.&quot;
from allennlp.predictors.predictor import Predictor
import allennlp_models.structured_prediction
predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz&quot;)
tree = predictor.predict(sentence=text)

tree = tree['hierplane_tree']
tree
</code></pre>
<p>Helper Function that will let me get the children values:</p>
<pre><code>&quot;&quot;&quot;Extract nested values from a JSON tree.&quot;&quot;&quot;


def json_extract(obj, key):
    &quot;&quot;&quot;Recursively fetch values from nested JSON.&quot;&quot;&quot;
    arr = []

    def extract(obj, arr, key):
        &quot;&quot;&quot;Recursively search for values of key in JSON tree.&quot;&quot;&quot;
        if isinstance(obj, dict):
            for k, v in obj.items():
                if isinstance(v, (dict, list)):
                    extract(v, arr, key)
                elif k == key:
                    arr.append(v)
        elif isinstance(obj, list):
            for item in obj:
                extract(item, arr, key)
        return arr

    values = extract(obj, arr, key)
    return values
</code></pre>
<p>The function can give me the values:</p>
<pre><code># Find every instance of `name` in a Python dictionary.
children = json_extract(tree, 'word')
print(children)

['walking', 'When', 'I', 'was', 'to', 'park', 'the', 'yesterday', ',', 'saw', 'I', 'man', 'a', 'wearing', 'shirt', 'a', 'blue', '.']
</code></pre>
<p>JSON Extract (What I want to try to get when I provide &quot;man&quot;:</p>
<pre><code>{'word': 'man',
        'nodeType': 'dep',
        'attributes': ['NOUN'],
        'link': 'dep',
        'spans': [{'start': 51, 'end': 55}],
        'children': [{'word': 'a',
          'nodeType': 'det',
          'attributes': ['DET'],
          'link': 'det',
          'spans': [{'start': 49, 'end': 51}]},
         {'word': 'wearing',
          'nodeType': 'dep',
          'attributes': ['VERB'],
          'link': 'dep',
          'spans': [{'start': 55, 'end': 63}],
          'children': [{'word': 'shirt',
            'nodeType': 'dep',
            'attributes': ['NOUN'],
            'link': 'dep',
            'spans': [{'start': 70, 'end': 76}],
            'children': [{'word': 'a',
              'nodeType': 'dep',
              'attributes': ['DET'],
              'link': 'dep',
              'spans': [{'start': 63, 'end': 65}]},
             {'word': 'blue',
              'nodeType': 'dep',
              'attributes': ['ADJ'],
              'link': 'dep',
              'spans': [{'start': 65, 'end': 70}]}]}]}]}]}]}
</code></pre>
<p>JSON Output:</p>
<pre><code>{'text': 'When I was walking to the park yesterday , I saw a man wearing a blue shirt .',
 'root': {'word': 'walking',
  'nodeType': 'root',
  'attributes': ['VERB'],
  'link': 'root',
  'spans': [{'start': 11, 'end': 19}],
  'children': [{'word': 'When',
    'nodeType': 'dep',
    'attributes': ['ADV'],
    'link': 'dep',
    'spans': [{'start': 0, 'end': 5}]},
   {'word': 'I',
    'nodeType': 'nsubj',
    'attributes': ['PRON'],
    'link': 'nsubj',
    'spans': [{'start': 5, 'end': 7}]},
   {'word': 'was',
    'nodeType': 'aux',
    'attributes': ['AUX'],
    'link': 'aux',
    'spans': [{'start': 7, 'end': 11}]},
   {'word': 'to',
    'nodeType': 'prep',
    'attributes': ['ADP'],
    'link': 'prep',
    'spans': [{'start': 19, 'end': 22}],
    'children': [{'word': 'park',
      'nodeType': 'pobj',
      'attributes': ['NOUN'],
      'link': 'pobj',
      'spans': [{'start': 26, 'end': 31}],
      'children': [{'word': 'the',
        'nodeType': 'det',
        'attributes': ['DET'],
        'link': 'det',
        'spans': [{'start': 22, 'end': 26}]}]}]},
   {'word': 'yesterday',
    'nodeType': 'tmod',
    'attributes': ['NOUN'],
    'link': 'tmod',
    'spans': [{'start': 31, 'end': 41}]},
   {'word': ',',
    'nodeType': 'dep',
    'attributes': ['PUNCT'],
    'link': 'dep',
    'spans': [{'start': 41, 'end': 43}],
    'children': [{'word': 'saw',
      'nodeType': 'dep',
      'attributes': ['VERB'],
      'link': 'dep',
      'spans': [{'start': 45, 'end': 49}],
      'children': [{'word': 'I',
        'nodeType': 'nsubj',
        'attributes': ['PRON'],
        'link': 'nsubj',
        'spans': [{'start': 43, 'end': 45}]},
       {'word': 'man',
        'nodeType': 'dep',
        'attributes': ['NOUN'],
        'link': 'dep',
        'spans': [{'start': 51, 'end': 55}],
        'children': [{'word': 'a',
          'nodeType': 'det',
          'attributes': ['DET'],
          'link': 'det',
          'spans': [{'start': 49, 'end': 51}]},
         {'word': 'wearing',
          'nodeType': 'dep',
          'attributes': ['VERB'],
          'link': 'dep',
          'spans': [{'start': 55, 'end': 63}],
          'children': [{'word': 'shirt',
            'nodeType': 'dep',
            'attributes': ['NOUN'],
            'link': 'dep',
            'spans': [{'start': 70, 'end': 76}],
            'children': [{'word': 'a',
              'nodeType': 'dep',
              'attributes': ['DET'],
              'link': 'dep',
              'spans': [{'start': 63, 'end': 65}]},
             {'word': 'blue',
              'nodeType': 'dep',
              'attributes': ['ADJ'],
              'link': 'dep',
              'spans': [{'start': 65, 'end': 70}]}]}]}]}]}]},
   {'word': '.',
    'nodeType': 'punct',
    'attributes': ['PUNCT'],
    'link': 'punct',
    'spans': [{'start': 76, 'end': 78}]}]},
 'nodeTypeToStyle': {'root': ['color5', 'strong'],
  'dep': ['color5', 'strong'],
  'nsubj': ['color1'],
  'nsubjpass': ['color1'],
  'csubj': ['color1'],
  'csubjpass': ['color1'],
  'pobj': ['color2'],
  'dobj': ['color2'],
  'iobj': ['color2'],
  'mark': ['color2'],
  'pcomp': ['color2'],
  'xcomp': ['color2'],
  'ccomp': ['color2'],
  'acomp': ['color2'],
  'aux': ['color3'],
  'cop': ['color3'],
  'det': ['color3'],
  'conj': ['color3'],
  'cc': ['color3'],
  'prep': ['color3'],
  'number': ['color3'],
  'possesive': ['color3'],
  'poss': ['color3'],
  'discourse': ['color3'],
  'expletive': ['color3'],
  'prt': ['color3'],
  'advcl': ['color3'],
  'mod': ['color4'],
  'amod': ['color4'],
  'tmod': ['color4'],
  'quantmod': ['color4'],
  'npadvmod': ['color4'],
  'infmod': ['color4'],
  'advmod': ['color4'],
  'appos': ['color4'],
  'nn': ['color4'],
  'neg': ['color0'],
  'punct': ['color0']},
 'linkToPosition': {'nsubj': 'left',
  'nsubjpass': 'left',
  'csubj': 'left',
  'csubjpass': 'left',
  'pobj': 'right',
  'dobj': 'right',
  'iobj': 'right',
  'pcomp': 'right',
  'xcomp': 'right',
  'ccomp': 'right',
  'acomp': 'right'}}
</code></pre>
","192204","","192204","","2020-12-21 01:08:26","2020-12-21 01:08:26","Parsing nested dictionary (allen nlp hierplane_tree)","<python><json><allennlp>","1","0","","","","CC BY-SA 4.0"
"52956944","1","","","2018-10-23 19:58:07","","2","256","<p>How can I use JupyterLab with allennlp==0.3.0?</p>

<p>When I go to jupyterlab through my browser, the python kernel dies:</p>

<pre><code>notebook_1    |     from prompt_toolkit.shortcuts import create_prompt_application, create_eventloop, create_prompt_layout, create_output
notebook_1    | ImportError: cannot import name 'create_prompt_application'
notebook_1    | [I 18:47:49.552 LabApp] KernelRestarter: restarting kernel (3/5), new random ports
notebook_1    | Traceback (most recent call last):
notebook_1    |   File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
notebook_1    |     ""__main__"", mod_spec)
notebook_1    |   File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/runpy.py"", line 85, in _run_code
notebook_1    |     exec(code, run_globals)
notebook_1    |   File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py"", line 15, in &lt;module&gt;
notebook_1    |     from ipykernel import kernelapp as app
notebook_1    |   File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/ipykernel/__init__.py"", line 2, in &lt;module&gt;
notebook_1    |     from .connect import *
notebook_1    |   File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/ipykernel/connect.py"", line 13, in &lt;module&gt;
notebook_1    |     from IPython.core.profiledir import ProfileDir
notebook_1    |   File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/IPython/__init__.py"", line 55, in &lt;module&gt;
notebook_1    |     from .terminal.embed import embed
notebook_1    |   File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/IPython/terminal/embed.py"", line 16, in &lt;module&gt;
notebook_1    |     from IPython.terminal.interactiveshell import TerminalInteractiveShell
notebook_1    |   File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py"", line 22, in &lt;module&gt;
notebook_1    |     from prompt_toolkit.shortcuts import create_prompt_application, create_eventloop, create_prompt_layout, create_output
notebook_1    | ImportError: cannot import name 'create_prompt_application'
notebook_1    | [W 18:47:55.574 LabApp] KernelRestarter: restart failed
</code></pre>

<p>Installing an older version of create_prompt_application didn't help (it's causing other issues).</p>
","395857","","","","","2018-10-24 17:13:45","How can I use JupyterLab with allennlp==0.3.0?","<python><jupyter-lab><allennlp>","1","0","2","","","CC BY-SA 4.0"
"66091092","1","66124271","","2021-02-07 17:41:41","","2","211","<br>
While studying AllenNLP framework (version 2.0.1), I tried to implement the example code from https://guide.allennlp.org/training-and-prediction#1.<br>
While reading the data from a Parquet file I got:<br>
<pre><code>TypeError: unsupported operand type(s) for +: 'generator' and 'generator'
</code></pre>
<p>for the next line:</p>
<pre><code>vocab = build_vocab(train_data + dev_data)
</code></pre>
<p>I suspect the return value should be AllennlpDataset but maybe I got it mixed up.
What did I do wrong?</p>
<p>Full code:</p>
<pre><code>train_path = &lt;some_path&gt;
test_path = &lt;some_other_path&gt;

class ClassificationJobReader(DatasetReader):
    def __init__(self,
                 lazy: bool = False,
                 tokenizer: Tokenizer = None,
                 token_indexers: Dict[str, TokenIndexer] = None,
                 max_tokens: int = None):
        super().__init__(lazy)
        self.tokenizer = tokenizer or WhitespaceTokenizer()
        self.token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}
        self.max_tokens = max_tokens

    def _read(self, file_path: str) -&gt; Iterable[Instance]:
      df = pd.read_parquet(data_path)
      for idx in df.index:
        text = row['title'][idx] + ' ' + row['description'][idx]
        print(f'text : {text}')
        label = row['class_id'][idx]
        print(f'label : {label}')
        tokens = self.tokenizer.tokenize(text)
        if self.max_tokens:
            tokens = tokens[:self.max_tokens]
        text_field = TextField(tokens, self.token_indexers)
        label_field = LabelField(label)
        fields = {'text': text_field, 'label': label_field}
        yield Instance(fields)

def build_dataset_reader() -&gt; DatasetReader:
    return ClassificationJobReader()

def read_data(reader: DatasetReader) -&gt; Tuple[Iterable[Instance], Iterable[Instance]]:
    print(&quot;Reading data&quot;)
    training_data = reader.read(train_path)
    validation_data = reader.read(test_path)
    return training_data, validation_data

def build_vocab(instances: Iterable[Instance]) -&gt; Vocabulary:
    print(&quot;Building the vocabulary&quot;)
    return Vocabulary.from_instances(instances)

dataset_reader = build_dataset_reader()
train_data, dev_data = read_data(dataset_reader)
vocab = build_vocab(train_data + dev_data)
</code></pre>
<p>Thanks for your help</p>
","1966765","","1966765","","2021-02-07 17:46:55","2021-02-09 19:15:42","AllenNLP DatasetReader.read returns generator instead of AllennlpDataset","<pytorch><allennlp>","1","1","","","","CC BY-SA 4.0"
"65726442","1","","","2021-01-14 20:29:35","","0","46","<p>I tried to use allennlp.predictors.Predictor.get_gradients to get the gradients of an instance.</p>
<p>allennlp==0.8.5</p>
<p>The main code:</p>
<pre><code>for instance in targeted_dev_data: 
    #model is locally trained on GPU
    #instance is from allennlp.data.dataset_readers.reader 
    predictor = Predictor(model,  
                    StanfordSentimentTreeBankDatasetReader(granularity=&quot;2-class&quot;,
                                                    token_indexers={&quot;tokens&quot;: single_id_indexer},
                                                    use_subtrees=True))
    
    input_gradients = predictor.get_gradients([instance])
</code></pre>
<p>But I got the following error:</p>
<pre><code>File &quot;sst_sememe.py&quot;, line 436, in main
    input_gradients = predictor.get_gradients([instance])
File &quot;/home/rui.ye/universal-triggers-master/myenv/lib/python3.7/site-packages/allennlp/predictors/predictor.py&quot;, line 110, in get_gradients
    outputs = self._model.decode(self._model.forward(**dataset.as_tensor_dict()))
File &quot;sst_sememe.py&quot;, line 59, in forward
    embeddings = self.word_embeddings(tokens)
File &quot;/home/rui.ye/universal-triggers-master/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 493, in __call__
    result = self.forward(*input, **kwargs)
File &quot;/home/rui.ye/universal-triggers-master/myenv/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py&quot;, line 131, in forward
    token_vectors = embedder(*tensors, **forward_params_values)
File &quot;/home/rui.ye/universal-triggers-master/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 493, in __call__
    result = self.forward(*input, **kwargs)
File &quot;/home/rui.ye/universal-triggers-master/myenv/lib/python3.7/site-packages/allennlp/modules/token_embedders/embedding.py&quot;, line 144, in forward
    sparse=self.sparse)
File &quot;/home/rui.ye/universal-triggers-master/myenv/lib/python3.7/site-packages/torch/nn/functional.py&quot;, line 1506, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'
</code></pre>
<p>However, I can pass the Predictor.predict_instance test with the following code</p>
<pre><code>for instance in targeted_dev_data: 
    #model is locally trained on GPU
    #instance is from allennlp.data.dataset_readers.reader 
    predictor = Predictor(model,  
                    StanfordSentimentTreeBankDatasetReader(granularity=&quot;2-class&quot;,
                                                    token_indexers={&quot;tokens&quot;: single_id_indexer},
                                                    use_subtrees=True))
    
    input_preds = predictor.predict_instance(instance)

</code></pre>
","15008236","","15008236","","2021-01-15 10:41:17","2021-01-15 10:44:01","allennlp.predictors.predictor.get_gradients got error RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'","<gradient-descent><allennlp>","1","3","","","","CC BY-SA 4.0"
"65393947","1","65458823","","2020-12-21 13:49:58","","0","230","<p>for the AllenNLP Semantic Role Labeling implementation, how do the Argument annotations get applied like what is shown in the demo? When I run the code from my local implementation I see the verbs and description, but not the annotations? I must be missing an additional step or logic that needs to be applied to support that part of the output.</p>
<p>What is showing up in the Demo App</p>
<pre><code>Sentence: At noon, a man exited the park on the N side. 

exited: [ARGM-TMP: At noon] , [ARG0: a man] [V: exited] [ARG1: the park] [ARGM-DIR: on the N side] .
</code></pre>
<p>My Code:</p>
<pre><code>from allennlp.predictors.predictor import Predictor

download = &quot;https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.09.03.tar.gz&quot;
# predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.11.19.tar.gz&quot;)
predictor = Predictor.from_path(download)

text = &quot;At noon, a man exited the park on the N side. &quot;

tree = predictor.predict(sentence=text)
value = predictor.dump_line(tree)
value
</code></pre>
<p>The output from my code for the same sentence:</p>
<pre><code>'{&quot;verbs&quot;: [{&quot;verb&quot;: &quot;exited&quot;, &quot;description&quot;: &quot;At noon , a man exited the park on the N side .&quot;, &quot;tags&quot;: [&quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;, &quot;O&quot;]}], &quot;words&quot;: [&quot;At&quot;, &quot;noon&quot;, &quot;,&quot;, &quot;a&quot;, &quot;man&quot;, &quot;exited&quot;, &quot;the&quot;, &quot;park&quot;, &quot;on&quot;, &quot;the&quot;, &quot;N&quot;, &quot;side&quot;, &quot;.&quot;]}
</code></pre>
","192204","","","","","2020-12-26 17:17:07","AllenNLP Semantic Role Labeler - Argument Annotations","<python><allennlp>","1","0","1","","","CC BY-SA 4.0"
"65490270","1","","","2020-12-29 09:50:04","","0","159","<p>Where is the training config file? The link on this page is dead <a href=""https://demo.allennlp.org/semantic-role-labeling"" rel=""nofollow noreferrer"">https://demo.allennlp.org/semantic-role-labeling</a></p>
<blockquote>
<p>Training
The SRL model was evaluated on the CoNLL 2012 dataset. Unfortunately we cannot release this data due to licensing restrictions by the LDC. You can put together evaluation data yourself by following the CoNLL 2012 instructions for working with the data. Once you have compiled the dataset, you can use the configuration file at <a href=""https://github.com/allenai/allennlp/blob/master/training_config/semantic_role_labeler.jsonnet"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp/blob/master/training_config/semantic_role_labeler.jsonnet</a> to train.</p>
</blockquote>
","14293598","","","","","2021-01-08 18:36:58","How to train semantic role labeling with allennlp?","<python><nlp><allennlp>","1","0","","","","CC BY-SA 4.0"
"54227872","1","","","2019-01-17 01:46:59","","2","1828","<p>I am trying to load an AllenNLP model weights. I could not find any documentation on how to save/load a whole model, so playing with weights only.</p>

<pre class=""lang-py prettyprint-override""><code>from allennlp.nn import util
model_state = torch.load(filename_model, map_location=util.device_mapping(-1))
model.load_state_dict(model_state)
</code></pre>

<p>I modified my input corpus a bit and I am guessing because of this I am getting corpus-size mismatch:</p>

<pre class=""lang-py prettyprint-override""><code>RuntimeError: Error(s) in loading state_dict for BasicTextFieldEmbedder:

    size mismatch for token_embedder_tokens.weight: 
    copying a param with shape torch.Size([2117, 16]) from checkpoint, 
    the shape in current model is torch.Size([2129, 16]).
</code></pre>

<p>Seemingly there is no official way to save model with corpus vocabulary. Any hacks around it?</p>
","1716733","","7483494","","2019-03-17 09:12:03","2019-10-27 22:04:47","Saving/Loading models in AllenNLP package","<pytorch><allennlp>","1","0","","","","CC BY-SA 4.0"
"63359246","1","","","2020-08-11 13:31:27","","1","560","<p>I use <a href=""https://github.com/allenai/allennlp/blob/master/allennlp/modules/conditional_random_field.py"" rel=""nofollow noreferrer"">this</a> python script from AllenAI to integrate CRF module (Conditional random fields) layer in my ML-system ...</p>
<p>and execute the following example to test the CRF module.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from allennlp.modules import ConditionalRandomField

num_tags = 2

model = ConditionalRandomField(num_tags)
    
seq_length = 3  # maximum sequence length in a batch
batch_size = 2  # number of samples in the batch
emissions = torch.randn(seq_length, batch_size, num_tags)
tags = torch.tensor([[0.0, 1.0], [1.0, 1.0], [0.0, 1.0]], dtype=torch.long)  # (seq_length, batch_size)
model(emissions, tags)
</code></pre>
<p>Upon execution, I get the following error.</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-3-415939853a93&gt; in &lt;module&gt;
  7 emissions = torch.randn(seq_length, batch_size, num_tags)
  8 tags = torch.tensor([[0.0, 1.0], [1.0, 1.0], [0.0, 1.0]], dtype=torch.long)  # 
(seq_length, batch_size)
----&gt; 9 model(emissions, tags)

~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py in 
__call__(self, *input, **kwargs)
545                     result = (result,)
546                 input = result
--&gt; 547         if torch._C._get_tracing_state():
548             result = self._slow_forward(*input, **kwargs)
549         else:

&lt;ipython-input-2-2d984bd97cf1&gt; in forward(self, inputs, tags, mask)
329             mask = mask.to(torch.bool)
330 
--&gt; 331         log_denominator = self._input_likelihood(inputs, mask)
332         log_numerator = self._joint_likelihood(inputs, tags, mask)
333 

&lt;ipython-input-2-2d984bd97cf1&gt; in _input_likelihood(self, logits, mask)
249             # In valid positions (mask == True) we want to take the logsumexp over the 
current_tag dimension
250             # of `inner`. Otherwise (mask == False) we want to retain the previous 
alpha.
--&gt; 251             alpha = util.logsumexp(inner, 1) * mask[i].view(batch_size, 1) + alpha * 
(
252                 ~mask[i]
253             ).view(batch_size, 1)

RuntimeError: expected device cpu and dtype Float but got device cpu and dtype Bool
</code></pre>
<p>I do understand that this is a problem of PyTorch version incompatibility with the functions used in allennlp package, but I am not sure what versions of torch and torchvision it expects.
These are the versions of PyTorch installations on my machine.</p>
<pre><code>torch                            1.5.1               
torchvision                      0.4.2
allennlp                         1.0.0               
</code></pre>
<p>Any help or hint will be appreciated.</p>
","9965155","","13562","","2020-10-12 05:48:18","2020-10-12 05:48:18","RuntimeError: expected device cpu and dtype Float but got device cpu and dtype Bool","<pytorch><allennlp>","0","3","","","","CC BY-SA 4.0"
"54947258","1","56443910","","2019-03-01 15:04:37","","3","766","<p>I am trying my hand at ELMo by simply using it as part of a larger PyTorch model. A basic example is given <a href=""https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md#using-elmo-as-a-pytorch-module-to-train-a-new-model"" rel=""nofollow noreferrer"">here</a>.</p>

<blockquote>
  <p>This is a torch.nn.Module subclass that computes any number of ELMo
  representations and introduces trainable scalar weights for each. For
  example, this code snippet computes two layers of representations (as
  in the SNLI and SQuAD models from our paper):</p>
</blockquote>

<pre><code>from allennlp.modules.elmo import Elmo, batch_to_ids

options_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json""
weight_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5""

# Compute two different representation for each token.
# Each representation is a linear weighted combination for the
# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))
elmo = Elmo(options_file, weight_file, 2, dropout=0)

# use batch_to_ids to convert sentences to character ids
sentences = [['First', 'sentence', '.'], ['Another', '.']]
character_ids = batch_to_ids(sentences)

embeddings = elmo(character_ids)

# embeddings['elmo_representations'] is length two list of tensors.
# Each element contains one layer of ELMo representations with shape
# (2, 3, 1024).
#   2    - the batch size
#   3    - the sequence length of the batch
#   1024 - the length of each ELMo vector
</code></pre>

<p>My question concerns the 'representations'. Can you compare them to normal word2vec output layers? You can choose how <em>many</em> ELMo will give back (increasing an n-th dimension), but what is the difference between these generated representations and what is their typical use? </p>

<p>To give you an idea, for the above code, <code>embeddings['elmo_representations']</code> returns a list of two items (the two representation layers) but they are identical.</p>

<p>In short, how can one define the 'representations' in ELMo? </p>
","1150683","","1150683","","2019-06-01 16:03:42","2019-06-04 12:28:45","Understanding ELMo's number of presentations","<python><nlp><pytorch><allennlp><elmo>","1","0","","","","CC BY-SA 4.0"
"65964536","1","","","2021-01-30 04:12:49","","1","28","<p>is it canceled? then how can I add two dataset for build a vocab?
in allennlp1.0, I can do</p>
<pre><code>reader = StanfordSentimentTreeBankDatasetReader()
train_dataset = reader.read('train.txt')
dev_dataset = reader.read('dev.txt')

for inst in train_dataset + dev_dataset:
    print(inst)
    break
</code></pre>
<p>and then build a vocab together, but why allennlp 2.0 canceled it?
sorry for being a fresh in allennlp</p>
","15109967","","","","","2021-02-05 17:42:24","why the new version of allennlp's datasetloader doesn't have the __add__ method?","<allennlp>","1","0","","","","CC BY-SA 4.0"
"50183857","1","","","2018-05-04 22:30:38","","1","104","<p>I have the following version of python installed.</p>

<pre><code>$ which python
/Library/Frameworks/Python.framework/Versions/2.7/bin/python
$ which python3
/Library/Frameworks/Python.framework/Versions/3.6/bin/python3
$ which python2
/Library/Frameworks/Python.framework/Versions/2.7/bin/python2
</code></pre>

<p>allennlp is installed here.</p>

<pre><code>$ which allennlp
/Library/Frameworks/Python.framework/Versions/3.6/bin/allennlp
</code></pre>

<p>But when I run allennlp. It obviously used python version 2. I don't want to manually edit allennlp or make a new symlink of python pointing to python3.</p>

<pre><code>$ echo ""The cryptocurrency space is now figuring out to have the highest search on Google globally ."" &gt; sentences.txt
$ echo ""Bitcoin alone has a sixty percent share of global search ."" &gt;&gt; sentences.txt
$ allennlp elmo sentences.txt elmo_layers.hdf5
/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python: No module named allennlp
</code></pre>

<p>Does anybody know what is the best way to deal with this case? Thanks.</p>
","1424739","","7483494","","2019-08-22 19:03:51","2019-08-22 19:03:51","How to use the correct version of python (OSX)?","<python><macos><allennlp>","0","7","","","","CC BY-SA 4.0"
"57239001","1","","","2019-07-28 08:01:20","","1","766","<p>File""/home/yangzhiwei/workshop/scibert/scibert/models/bert_text_classifier.py"", line 15, in  from text_classifier import TextClassifier File ""/home/yangzhiwei/workshop/scibert/scibert/models/text_classifier.py"", line 15, in  class TextClassifier(Model):File ""/home/yangzhiwei/workshop/scibert/src/allennlp/allennlp/common/registrable.py"", line 49, in add_subclass_to_registry raise configurationError(message)
allennlp.common.checks.ConfigurationError: 'Cannot register text_classifier as Model; name already in use for TextClassifier'</p>
","8562196","","","","","2019-07-28 08:01:20","How to fix 'allennlp.common.checks.ConfigurationError' in running scibert projec?","<python><allennlp>","0","2","","","","CC BY-SA 4.0"
"66364658","1","","","2021-02-25 08:11:06","","0","142","<p>when trying to run AllenNLP train command, I ran into this Error.</p>
<blockquote>
<p>2021-02-25 02:57:40,043 - INFO - allennlp.common.plugins - Plugin allennlp_models available</p>
<blockquote>
<p>Traceback (most recent call last):</p>
<blockquote>
<p>File &quot;/home/zliu60/miniconda/envs/sftp-env/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
&quot;<strong>main</strong>&quot;, mod_spec)
File &quot;/home/zliu60/miniconda/envs/sftp-env/lib/python3.7/runpy.py&quot;, line 85, in _run_code
exec(code, run_globals)
File &quot;/home/zliu60/miniconda/envs/sftp-env/lib/python3.7/site-packages/allennlp/<strong>main</strong>.py&quot;, line 38, in 
run()
File &quot;/home/zliu60/miniconda/envs/sftp-env/lib/python3.7/site-packages/allennlp/<strong>main</strong>.py&quot;, line 34, in run
main(prog=&quot;allennlp&quot;)
File &quot;/home/zliu60/miniconda/envs/sftp-env/lib/python3.7/site-packages/allennlp/commands/<strong>init</strong>.py&quot;, line 117, in main
import_module_and_submodules(package_name)
File &quot;/home/zliu60/miniconda/envs/sftp-env/lib/python3.7/site-packages/allennlp/common/util.py&quot;, line 351, in import_module_and_submodules
if path_string and module_finder.path != path_string:
AttributeError: 'zipimport.zipimporter' object has no attribute 'path'</p>
</blockquote>
</blockquote>
</blockquote>
<p>It seems like this is an allennlp related issue, but I don't know which part in my code (whether config file or modules) or the environment this is related to. Could someone please help me.</p>
","15281211","","","","","2021-02-25 08:11:06","Allennlp: AttributeError: 'zipimport.zipimporter' object has no attribute 'path'","<python-module><allennlp>","0","1","","","","CC BY-SA 4.0"
"61889477","1","","","2020-05-19 11:02:52","","2","191","<p>AllenNLP Interpret and Textattack are supposed to ""attack"" models to figure out why they generate their output. I have mostly used spaCy to train my models and would like to try either of the frameworks and see if they give me a better understanding of my models. But it seems like they're not compatible with spaCy models (or maybe I'm doing something wrong). For Textattack I tried following this example:
<a href=""https://textattack.readthedocs.io/en/latest/quickstart/overview.html"" rel=""nofollow noreferrer"">https://textattack.readthedocs.io/en/latest/quickstart/overview.html</a>
but swapping the model with a spaCy model. That didn't work well, because inside the class TokenizedText there is</p>

<pre><code>ids = tokenizer.encode(text)
</code></pre>

<p>which throws an error, because spaCy's Tokenizer object doesn't have a method called <strong>encode()</strong>. I noticed that there were multiple subclasses of the Textattack's Tokenizer and a SpacyTokenizer among them. If that's the compatible version of Tokenizer why isn't it automatically detected and called instead? I tried swapping them up, but I got confused by some of the parameters SpacyTokenizer requires:</p>

<pre><code>def __init__(self, word2id, oov_id, pad_id, max_seq_length=128)
</code></pre>

<p><strong>word2id</strong> is a word-id pairing, but what kind of ids? Is it for all words in the vocab or just the tokens of this particular sentence?
<strong>oov_id</strong> is even more confusing, because ""oov"" stands for ""out-of-variable"", not ""out-of-vocabulary"" as is the case in spaCy. Moreover in spaCy it's a boolean value, not an id.
<strong>pad_id</strong> is not explained at all and I have no idea what it is.</p>

<p>So it seems like there is some connection between Textattack and spaCy, but I can't figure out how to put it together into a working example.</p>

<p>When it comes to AllenNLP Interpret I tried using the hotflip attack, but the very first thing that happens is this error message:</p>

<pre><code>for i in self.vocab._index_to_token[self.namespace]:
AttributeError: 'spacy.vocab.Vocab' object has no attribute '_index_to_token'
</code></pre>

<p>so it doesn't seem that this framework is suited for spaCy either, because it expects the <strong>_index_to_token</strong>, but spaCy's Vocab doesn't have that.</p>

<p>Can someone help me out?</p>
","13572790","","","","","2020-05-19 18:02:51","Using spaCy models with AllenNLP Interpret or Textattack","<python><nlp><spacy><allennlp>","1","0","","","","CC BY-SA 4.0"
"53909303","1","","","2018-12-24 04:56:23","","-1","69","<p>I have created an API in Django. It is supposed to take a request and pass the argument to allenNLP files to gather a computed response. I want to know how to run my django app in allenNLP environment and I want all the source code of allenNLP to be in a folder in my django project. Is it possible and how can I do it?</p>
","10433447","","10433447","","2018-12-24 05:06:02","2018-12-24 05:11:41","How to run django app in allen NLP environment?","<django><python-3.x><allennlp>","1","0","","","","CC BY-SA 4.0"
"66941283","1","","","2021-04-04 12:28:15","","0","24","<p>I installed AllenNLP via a terminal and anaconda as well, but for some reason it won't import.
Haven't found anything that is related on the web, the typical issues are about particular libraries inside AllenNLP.
Any idea? Thanks to the helpers</p>
","9888098","","","","","2021-04-04 12:28:15","Cannot import AllenNLP in Jupyter","<python><jupyter><allennlp>","0","0","","","","CC BY-SA 4.0"
"54034242","1","","","2019-01-04 06:49:00","","4","100","<p>I need an older version of torch (0.5.0) for a certain library (allennlp==0.7.2). But I already have an existing torch version (1.0.0) as required by flair and other libraries. How do I cater to different requirements of the same package of multiple libraries in the <em>same</em> python environment without having to manually specify the version every time I import that package?</p>
","10866089","","","","","2019-01-04 06:54:40","Maintaining packages with different versions in one python environment","<python><version><pytorch><allennlp>","0","1","1","2019-01-04 09:33:13","","CC BY-SA 4.0"
"64356111","1","","","2020-10-14 14:58:59","","0","24","<p>i'm reading a tutorial about character based neural networks using AllenNlp framework, the goal is building a model which can complete a sentence. there is a step of instances building after that i want to train my model. i have the code below, i could not understand the role of forward function, anyone can help ? could someone provide an example</p>
<pre><code>class RNNLanguageModel(Model):
def __init__(self,
             embedder: TextFieldEmbedder,
             hidden_size: int,
             max_len: int,
             vocab: Vocabulary) -&gt; None:
    super().__init__(vocab)

    self.embedder = embedder

    # initialize a Seq2Seq encoder, LSTM
    self.rnn = PytorchSeq2SeqWrapper(
        torch.nn.LSTM(EMBEDDING_SIZE, HIDDEN_SIZE, batch_first=True))

    self.hidden2out = torch.nn.Linear(in_features=self.rnn.get_output_dim(), out_features=vocab.get_vocab_size('tokens'))
    self.hidden_size = hidden_size
    self.max_len = max_len

def forward(self, input_tokens, output_tokens):
    '''
    This is the main process of the Model where the actual computation happens. 
    Each Instance is fed to the forward method. 
    It takes dicts of tensors as input, with same keys as the fields in your Instance (input_tokens, output_tokens)
    It outputs the results of predicted tokens and the evaluation metrics as a dictionary. 
    '''

    mask = get_text_field_mask(input_tokens)
    embeddings = self.embedder(input_tokens)
    rnn_hidden = self.rnn(embeddings, mask)
    out_logits = self.hidden2out(rnn_hidden)
    loss = sequence_cross_entropy_with_logits(out_logits, output_tokens['tokens'], mask)

    return {'loss': loss}
</code></pre>
","13873517","","","","","2020-10-14 16:21:13","what is the role of RNNLanguageModel's forward method?","<python-3.x><nlp><recurrent-neural-network><allennlp>","1","0","","","","CC BY-SA 4.0"
"53350905","1","","","2018-11-17 11:44:28","","16","16604","<p>I want to make a cross validation in my project based on Pytorch.
And I didn't find any method that pytorch provided to delete the current model and empty the memory of GPU. Could you tell that how can I do it?</p>
","10666819","","681865","","2018-11-17 11:47:27","2018-11-18 16:16:13","pytorch delete model from gpu","<gpu><pytorch><allennlp>","1","1","","","","CC BY-SA 4.0"
"65457108","1","","","2020-12-26 13:56:58","","0","71","<p>I want to use Semantic Role Labeling with custom tokenizer. Specifically, I'd like to merge some tokens after the spacy tokenizer.
With spacy, I can do this with things like <code>add_pipe(my_component, before=&quot;parser&quot;)</code>. How can I add such custom component to the tokenization process in Semantic Role Labeling?</p>
","14293598","","","","","2020-12-26 13:56:58","AllenNLP: How to add custom components to pipeline for predictor?","<python><nlp><allennlp>","0","1","","","","CC BY-SA 4.0"
"66156046","1","66392769","","2021-02-11 13:51:19","","0","434","<p>I trained a multitask model using allennlp 2.0 and now want to predict on new examples using the <code>allennlp predict</code> command.</p>
<p><strong>Problem/Error:</strong>
I am using the following command: <code>allennlp predict results/model.tar.gz new_instances.jsonl --include-package mtl_sd --predictor mtlsd_predictor --use-dataset-reader --dataset-reader-choice validation</code></p>
<p>This gives me the following error:</p>
<pre><code>Traceback (most recent call last):
File &quot;.../mtl_sd_venv/bin/allennlp&quot;, line 10, in &lt;module&gt;
sys.exit(run())
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/__main__.py&quot;, line 34, in run
main(prog=&quot;allennlp&quot;)
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/commands/__init__.py&quot;, line 119, in main
args.func(args)
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/commands/predict.py&quot;, line 220, in _predict
manager.run()
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/commands/predict.py&quot;, line 186, in run
for batch in lazy_groups_of(self._get_instance_data(), self._batch_size):
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/common/util.py&quot;, line 139, in lazy_groups_of
s = list(islice(iterator, group_size))
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/commands/predict.py&quot;, line 180, in _get_instance_data
yield from self._dataset_reader.read(self._input_file)
File &quot;.../mtl_sd_venv/lib/python3.7/site-packages/allennlp/data/dataset_readers/multitask.py&quot;, line 31, in read
raise RuntimeError(&quot;This class is not designed to be called like this&quot;)
RuntimeError: This class is not designed to be called like this
</code></pre>
<p><strong>As far as I understand, that's what's going on:</strong></p>
<p>This <a href=""https://github.com/allenai/allennlp/blob/d2ae540d489336ba05f15479d3c55530b0bd6949/allennlp/data/dataset_readers/multitask.py#L30"" rel=""nofollow noreferrer"">RuntimeError is raised by the MultiTaskDatasetReader</a> because the <code>read()</code>-method of the MultiTaskDatasetReader should not be called  itself. The <code>read()</code>-method should only be called for specific DatasetReaders in <code>MultiTaskDatasetReader.readers</code>.</p>
<p>The read()-method of the MultiTaskDatasetReader is called because in the jsonnet-config I have specified the DatasetsReaders as follows:</p>
<pre class=""lang-json prettyprint-override""><code>&quot;dataset_reader&quot;: {
    &quot;type&quot;: &quot;multitask&quot;,
    &quot;readers&quot;: {
        &quot;SemEval2016&quot;: {
            &quot;type&quot;: &quot;SemEval2016&quot;,
            &quot;max_sequence_length&quot;: 509,
            &quot;token_indexers&quot;: {
                &quot;bert&quot;: {
                    &quot;type&quot;: &quot;pretrained_transformer&quot;,
                    &quot;model_name&quot;: &quot;bert-base-cased&quot;
                }
            },
            &quot;tokenizer&quot;: {
                &quot;type&quot;: &quot;pretrained_transformer&quot;,
                &quot;model_name&quot;: &quot;bert-base-cased&quot;
            }
        }, ...
    }
}
</code></pre>
<p>Usually the <code>type</code> of dataset_reader indicates the dataset-reader class to be instanciated for prediction. But in this case the <code>type</code> just points MultiTaskDatasetReader, which has no <code>read()</code>-method implemented and contains multiple DatasetReaders.</p>
<p>As far as I understand, when using <code>allennlp predict</code> I need to specify somehow which of the multiple DatasetReaders should be used.</p>
<p><strong>The questions is:</strong></p>
<p>How can I specify which specific DatasetReader (of the multiple DatasetReaders in <code>MultiTaskDatasetReader.readers</code>) should be used when executing <code>allennlp predict</code>? Or more generally: How can I get <code>allennlp predict</code> to run with a MultiTaskDatasetReader?</p>
<p><strong>Additional code, for the sake of completeness:</strong>
The predictor:</p>
<pre class=""lang-py prettyprint-override""><code>@Predictor.register('mtlsd_predictor')
class MTLSDPredictor(Predictor):

    def predict(self, sentence: str) -&gt; JsonDict:
        return self.predict_json({'sentence': sentence})

    @overrides
    def _json_to_instance(self, json_dict: JsonDict) -&gt; Instance:
        target = json_dict['text1']
        claim = json_dict['text2']
        return self._dataset_reader.text_to_instance(target, claim)
</code></pre>
","5800213","","","","","2021-02-26 21:03:45","AllenNLP 2.0: Using `allennlp predict` with MultiTaskDatasetReader leads to RuntimeError","<python><nlp><allennlp>","1","11","","","","CC BY-SA 4.0"
"66874397","1","66893612","","2021-03-30 15:58:32","","1","50","<p>I have trained a (AllenNLP) multi-task model. I would like to keep the encoder/backbone weights and continue training with new heads on new datasets. How can I do that with AllenNLP?</p>
<p>I have two basic ideas for how to do that:</p>
<ol>
<li><p>I followed <a href=""https://guide.allennlp.org/building-your-model#3"" rel=""nofollow noreferrer"">this AllenNLP tutorial</a> to load the trained model and then instead of just making predictions I wanted to change the configuration and the model-heads to continue training on the new datasets...but I am kinda lost in how to do that.</p>
</li>
<li><p>I guess it should be possible to (a) save the state-dict of the previously trained encoder in a file and then (b) point to those weights in the configuration file for the new model (instead of pointing to &quot;bert-base-cased&quot;-weights for example). But looking at the <a href=""https://github.com/allenai/allennlp/blob/main/allennlp/modules/token_embedders/pretrained_transformer_embedder.py"" rel=""nofollow noreferrer"">PretrainedTransformerEmbedder-class</a> I don't see how I could pass my own model-weights to that class.</p>
</li>
</ol>
<p>As an additional question: Is it also possible to save the weights of the heads separately and initialize new heads with those weights?</p>
<p>Any help is appreciated :)</p>
","5800213","","","","","2021-03-31 18:48:34","AllenNLP Multi-Task Model: Keep encoder weights for new heads","<pytorch><allennlp>","1","0","","","","CC BY-SA 4.0"
"65452775","1","","","2020-12-26 00:16:14","","0","256","<p>I am trying out Allen NLP pre-trained models for Q&amp;A.</p>
<p>The online demo is here : <a href=""https://demo.allennlp.org/reading-comprehension"" rel=""nofollow noreferrer"">https://demo.allennlp.org/reading-comprehension</a></p>
<p>I have created a python script to try out various models.</p>
<ul>
<li><a href=""https://gist.github.com/sujee/a22f6fa11065791314d5bfede434b516#file-qa-allen-nlp-py"" rel=""nofollow noreferrer"">python script</a></li>
<li><a href=""https://gist.github.com/sujee/a22f6fa11065791314d5bfede434b516#file-output"" rel=""nofollow noreferrer"">script output</a></li>
</ul>
<p>Here is the benchmark summary on my laptop</p>
<ul>
<li>Macbook Pro (2017)</li>
<li>2.9 Ghz Intel i7 quad-core</li>
<li>16 G memory</li>
</ul>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Benchmark</th>
<th>transformer-qa</th>
<th>bidaf-model</th>
<th>bidaf-elmo-model</th>
</tr>
</thead>
<tbody>
<tr>
<td>loading time</td>
<td>31.6 seconds</td>
<td>1.6 seconds</td>
<td>13.8 seconds</td>
</tr>
<tr>
<td>questions</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Who stars in The Matrix?</td>
<td>794  ms</td>
<td>62  ms</td>
<td>1,798 ms</td>
</tr>
<tr>
<td>where does polar bear live</td>
<td>2,211 ms</td>
<td>96 ms</td>
<td>7,125 ms</td>
</tr>
<tr>
<td>how much does a polar bear weigh</td>
<td>2,435 ms</td>
<td>98 ms</td>
<td>7,082 ms</td>
</tr>
<tr>
<td>what is lightning</td>
<td>1,361  ms</td>
<td>69 ms</td>
<td>3,173 ms</td>
</tr>
<tr>
<td>How many lightning bolts strike earth</td>
<td>1,019  ms</td>
<td>47 ms</td>
<td>2,885 ms</td>
</tr>
</tbody>
</table>
</div>
<p>Looking at the <a href=""https://gist.github.com/sujee/a22f6fa11065791314d5bfede434b516#file-output"" rel=""nofollow noreferrer"">output</a>  I can see all 3 models are providing good answers.  I like the <code>transformer-qa</code> model but it takes a while (in the order of seconds) to predict.</p>
<p>Is there a way to speed up prediction times?</p>
<p>thanks!</p>
","87241","","","","","2021-01-08 18:21:15","How to improve Allen NLP question answering performance","<python><nlp><question-answering><allennlp>","1","0","2","","","CC BY-SA 4.0"
"66394232","1","67992175","","2021-02-26 23:55:47","","2","55","<p>Running the training command from <a href=""https://demo.allennlp.org/open-information-extraction"" rel=""nofollow noreferrer"">open information extraction demo</a> fails with the following error message:</p>
<pre><code>OSError: HEAD request failed for url https://raw.githubusercontent.com/allenai/allennlp-models/main/training_config/structured-prediction/srl.jsonnet with status code 404
</code></pre>
<p>Does this mean the file is removed? Where can the replacement file be found?</p>
<p>Looking for a replacement file I found <a href=""https://github.com/allenai/allennlp-models/blob/main/allennlp_models/modelcards/structured-prediction-srl.json"" rel=""nofollow noreferrer"">this file</a>  from allenai repo but it seems to have required fields such as &quot;dataset_reader&quot; missing.</p>
","15293354","","","","","2021-06-15 19:11:35","AllenNLP: HEAD request failed for url with status code 404 - open information extraction","<allennlp>","1","2","","","","CC BY-SA 4.0"
"68646712","1","","","2021-08-04 06:58:42","","0","15","<p>Allen NLP 0.9.0 (and maybe later) had a pruner module - intended to score and prune spans (<a href=""https://docs.allennlp.org/v0.9.0/api/allennlp.modules.pruner.html"" rel=""nofollow noreferrer"">https://docs.allennlp.org/v0.9.0/api/allennlp.modules.pruner.html</a>). This seems to have gone missing, I cannot find it in the latest release, or the allennlp-models repo. Is it still part of the library - if not I have the original code so it's no big deal but if it was removed what was the reasoning?</p>
","2981639","","2981639","","2021-08-04 22:22:02","2021-08-04 22:22:02","Where has the AllenNLP Pruner gone?","<allennlp>","0","3","","","","CC BY-SA 4.0"
"64268152","1","64272856","","2020-10-08 17:56:33","","3","192","<p>I am trying to understand how to build configuration file for our experiements</p>
<p>let us take this example from the AllenNLP documentation</p>
<p><a href=""https://guide.allennlp.org/training-and-prediction#2"" rel=""nofollow noreferrer"">training and prediction</a></p>
<p>in particular this snippet
<a href=""https://i.stack.imgur.com/0Hs53.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Hs53.png"" alt="""" /></a></p>
<p>From where we got &quot;token_embedders&quot;?  shouldn't be &quot;basic&quot; as documentation <a href=""https://github.com/allenai/allennlp/blob/master/allennlp/modules/text_field_embedders/basic_text_field_embedder.py"" rel=""nofollow noreferrer"">here?</a></p>
<p>or do I miss something?</p>
","14494113","","14494113","","2020-10-28 18:16:51","2020-10-28 18:16:51","configuration file allenNLP","<allennlp>","1","0","","","","CC BY-SA 4.0"
"65806905","1","","","2021-01-20 09:42:14","","1","194","<p>I wish to use Allen NLP Interpret for integrated visualization and Saliency mapping.on custom transformer model, can you please tell me how to do that?</p>
","9651632","","6664872","","2021-01-20 16:41:24","2021-01-29 08:42:54","How to use Allen NLP interpret on custom models","<heatmap><allennlp>","1","0","","","","CC BY-SA 4.0"
"65154160","1","65261237","","2020-12-05 05:43:29","","1","40","<p>I am using the sentiment classifier in python according to this <a href=""https://demo.allennlp.org/sentiment-analysis/"" rel=""nofollow noreferrer"">demo</a>.</p>
<p>Is it possible to give pre-tokenized text as input to the predictor? I would like to be able to use my own custom tokenizer.</p>
","4360157","","","","","2020-12-12 03:10:57","Giving pretokenized input to sentiment classifier","<allennlp>","1","0","","","","CC BY-SA 4.0"
"66317439","1","66394227","","2021-02-22 14:18:28","","0","58","<p>When running the AllenNLP <code>train</code> or <code>evaluate</code> CLI commands, is there a configuration option (in the json/jsonnet file) to write <strong>test set</strong> evaluation metrics to Tensorboard?</p>
<p>If not, how can I do it in my own script?</p>
<p>Thanks in advance for your time and help. Best regards</p>
","7803975","","","","","2021-02-26 23:55:07","AllenNLP - configuration to write test set metrics to Tensorboard","<allennlp>","1","0","","","","CC BY-SA 4.0"
"64661565","1","","","2020-11-03 11:01:15","","2","155","<p>I am trying <code>AllenNLP reading comprehension</code> with the <code>Transformer QA Model</code> to get the answer to question <code>&quot;Who is CEO of ABB?&quot;</code> from the passage <code>&quot;ABB opened its first dedicated global healthcare research center for robotics in October 2019.&quot;</code>.</p>
<p>As expected, the UI demo shows <code>no answer returned</code>. The API response in network tab also shows that. In the json response, <code>best_span_str</code> is empty, but <code>best_span_scores</code> is <code>9.9</code>. <a href=""https://i.stack.imgur.com/RS3qJ.png"" rel=""nofollow noreferrer"">Screenshot of demo UI and API response in network tab.</a></p>
<p>When I execute the similar code via python library, I get a different result.</p>
<pre><code>from allennlp.predictors.predictor import Predictor
import pandas

def allen_nlp_demo_1():
  import allennlp_models.structured_prediction
  import allennlp_models.rc
  predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/transformer-qa-2020-05-26.tar.gz&quot;)
  data = predictor.predict(
    passage=&quot;ABB opened its first dedicated global healthcare research center for robotics in October 2019.&quot;,
    question= &quot;Who is CEO of ABB?&quot;
  )
  print(data)

if __name__ == '__main__':
  allen_nlp_demo_1()
</code></pre>
<p>provides following json output</p>
<pre><code>{
  &quot;span_start_logits&quot;: [...],
  &quot;best_span&quot;: [
    7,
    15
  ],
  &quot;best_span_scores&quot;: -10.418445587158203,
  &quot;loss&quot;: 0,
  &quot;best_span_str&quot;: &quot;healthcare research center for robotics in October 2019&quot;,
  &quot;context_tokens&quot;: [...],
  &quot;id&quot;: &quot;1&quot;,
  &quot;answers&quot;: []
}
</code></pre>
<p>Here I see <code>best_span_str</code> coming up, and <code>best_span_scores</code> as <code>-10.418445587158203</code>.  <a href=""https://i.stack.imgur.com/tJoSy.png"" rel=""nofollow noreferrer"">Attaching python code and output snippet.</a></p>
<p>Why is this difference in output in the UI demo vs library? Also, what is the range of <code>best_span_scores</code> and where can I decide a threshold to discard false results?</p>
","1061408","","","","","2020-11-06 19:32:23","AllenNLP Reading Comprehension results are different in UI Demo and Python Library","<python><nlp><allennlp>","1","0","","","","CC BY-SA 4.0"
"62431557","1","62431620","","2020-06-17 14:35:56","","0","85","<p>I am trying to read a JSON file (BioRelEx dataset: <a href=""https://github.com/YerevaNN/BioRelEx/releases/tag/1.0alpha7"" rel=""nofollow noreferrer"">https://github.com/YerevaNN/BioRelEx/releases/tag/1.0alpha7</a>) in Python. The JSON file is a list of objects, one per sentence.
This is how I try to do it: </p>

<pre><code> def _read(self, file_path):
        with open(cached_path(file_path), ""r"") as data_file:
            for line in data_file.readlines():
                if not line:
                    continue
                 items = json.loads(lines)
                 text = items[""text""]
                 label = items.get(""label"")
</code></pre>

<p>My code is failing on <code>items = json.loads(line)</code>. It looks like the data is not formatted as the code expects it to be, but how can I change it? </p>

<p>Thanks in advance for your time!</p>

<p>Best, </p>

<p>Julia</p>
","13763248","","","","","2020-06-17 14:47:01","Read JSON file correctly","<python><json><allennlp>","2","1","","","","CC BY-SA 4.0"
"66408517","1","","","2021-02-28 11:21:20","","0","70","<p>I tried implementing an Active Learning procedure in AllenNLP v2.0.1 . However, with the current <code>GradientDescentTrainer</code> implementation, I am unable to continue training on a new batch of <code>Instance</code>.</p>
<p>The model (also trained using AllenNLP) has finished training for the predefined number of epochs on an initial training dataset. I restore the model using the <code>Model.from_archive</code> method and a <code>Trainer</code> is instantiated for it using the <code>Trainer.from_params</code> static constructor.</p>
<p>Thereafter, when I attempt to continue training on a new batch of <code>Instance</code> by calling <code>trainer.train()</code>, it skips training because of the following code snippet in the <code>_try_train</code> method,</p>
<pre class=""lang-py prettyprint-override""><code>for epoch in range(epoch_counter, self._num_epochs)
</code></pre>
<p>This is because the <code>epoch_counter</code> is restored to 5, which is from training on the initial training data previously. This is the relevant code snippet for it,</p>
<pre class=""lang-py prettyprint-override""><code>def _try_train(self) -&gt; Tuple[Dict[str, Any], int]:
    try:
        epoch_counter = self._restore_checkpoint()
</code></pre>
<p><code>self._num_epochs</code> is also 5 which I assume is the number of epochs defined in my .jsonnet training configuration file.</p>
<p>Simply, my requirement is to load an AllenNLP model that has already been trained and to continue training it on a new batch of Instances (single Instance actually which I would load using a <code>SimpleDataLoader</code>)</p>
<p>I have also attached the configuration for the Trainer below. The model I am using is a custom wrapper around the BasicClassifier, solely for the purpose of logging additional metrics.</p>
<p>Thanks in advance.</p>
<pre class=""lang-json prettyprint-override""><code>&quot;trainer&quot;: {
  &quot;num_epochs&quot;: 5,
  &quot;patience&quot;: 1, // for early stopping
  &quot;grad_norm&quot;: 5.0,
  &quot;validation_metric&quot;: &quot;+accuracy&quot;,
  &quot;optimizer&quot;: {
    &quot;type&quot;: &quot;adam&quot;,
    &quot;lr&quot;: 0.001
  },
  &quot;callbacks&quot;: [
    {
      &quot;type&quot;: &quot;tensorboard&quot;
    }
  ]
}
</code></pre>
","7803975","","7803975","","2021-02-28 11:57:41","2021-03-05 18:07:56","Active Learning in AllenNLP v2.0.1","<allennlp>","1","0","","","","CC BY-SA 4.0"
"65245446","1","","","2020-12-11 03:36:36","","1","56","<p>if i use <code>encoder = PretrainedTransformerEmbedder(model_name, sub_module=&quot;encoder&quot;)</code> as the encoder to pass to Bart(encoder=encoder), it reports error because it doesn't implement <code>get_input_dim()</code>, if i pass <code>encoder = PretrainedTransformerEmbedder(model_name, sub_module=&quot;encoder&quot;), encoder = encoder.encoder</code> as the input encoder like metioned <a href=""https://github.com/allenai/allennlp/issues/4604"" rel=""nofollow noreferrer"">here</a>, it reports error because the <code>PretrainedTransformerEmbedder(model_name, sub_module=&quot;encoder&quot;)</code> doesn't has an attribute <code>encoder</code>.
So how can i use the full bart model(including token_embed, position_embed) for seq2seq task in allennlp?</p>
","14805202","","","","","2020-12-12 02:56:22","How to use Bart with PretrainedTransformerEmbedder?","<allennlp>","1","1","","","","CC BY-SA 4.0"
"64808660","1","","","2020-11-12 17:19:04","","2","92","<p>I'm pretty new to AllenNLP and I'm currently using its pre-trained question answering model. I wonder if it has a passage length limit to ensure its performance? I know BERT will have a maximum length of 512 and will truncate longer passages.</p>
<p>I have tried longer passages on AllenNLP and it seems working but I just want to confirm. Thank you.</p>
","10305261","","","","","2020-11-12 20:36:38","Passage Length Limit for AllenNLP Question Answering","<maxlength><question-answering><allennlp>","1","0","","","","CC BY-SA 4.0"
"56141193","1","","","2019-05-15 03:00:08","","4","1344","<p>I am trying to install allennlp on my mac. I have tried installing macOS headers which solved the missing headers problem but now i am experiencing new problems. </p>

<p>The error when i run <code>pip install allennlp</code>:</p>

<pre><code>Running setup.py bdist_wheel for jsonnet ... error
  Complete output from command /anaconda3/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/qf/jkn4v43j08xgst0r9yxyl0dc0000gn/T/pip-install-i4nyb384/jsonnet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /private/var/folders/qf/jkn4v43j08xgst0r9yxyl0dc0000gn/T/pip-wheel-eof7cc6k --python-tag cp37:
  running bdist_wheel
  running build
  running build_ext
  x86_64-apple-darwin13.4.0-clang++ -c -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -std=c++14 -fmessage-length=0 core/desugarer.cpp -o core/desugarer.o
  In file included from core/desugarer.cpp:17:
  In file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/cassert:21:
  In file included from /Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/usr/include/assert.h:44:
  /Library/Developer/CommandLineTools/usr/include/c++/v1/stdlib.h:111:82: error: use of undeclared identifier 'labs'; did you mean 'abs'?
  inline _LIBCPP_INLINE_VISIBILITY long      abs(     long __x) _NOEXCEPT {return  labs(__x);}
                                                                                   ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/stdlib.h:111:44: note: 'abs' declared here
  inline _LIBCPP_INLINE_VISIBILITY long      abs(     long __x) _NOEXCEPT {return  labs(__x);}
                                             ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/stdlib.h:113:81: error: use of undeclared identifier 'llabs'
  inline _LIBCPP_INLINE_VISIBILITY long long abs(long long __x) _NOEXCEPT {return llabs(__x);}
                                                                                  ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/stdlib.h:116:35: error: unknown type name 'ldiv_t'
  inline _LIBCPP_INLINE_VISIBILITY  ldiv_t div(     long __x,      long __y) _NOEXCEPT {return  ldiv(__x, __y);}
                                    ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/stdlib.h:116:95: error: use of undeclared identifier 'ldiv'; did you mean 'div'?
  inline _LIBCPP_INLINE_VISIBILITY  ldiv_t div(     long __x,      long __y) _NOEXCEPT {return  ldiv(__x, __y);}
                                                                                                ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/stdlib.h:116:42: note: 'div' declared here
  inline _LIBCPP_INLINE_VISIBILITY  ldiv_t div(     long __x,      long __y) _NOEXCEPT {return  ldiv(__x, __y);}
                                           ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/stdlib.h:118:34: error: unknown type name 'lldiv_t'
  inline _LIBCPP_INLINE_VISIBILITY lldiv_t div(long long __x, long long __y) _NOEXCEPT {return lldiv(__x, __y);}
                                   ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/stdlib.h:118:94: error: use of undeclared identifier 'lldiv'
  inline _LIBCPP_INLINE_VISIBILITY lldiv_t div(long long __x, long long __y) _NOEXCEPT {return lldiv(__x, __y);}
                                                                                               ^
  In file included from core/desugarer.cpp:19:
  In file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/algorithm:642:
  In file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/cstring:61:
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:74:64: error: use of undeclared identifier 'strchr'
  char* __libcpp_strchr(const char* __s, int __c) {return (char*)strchr(__s, __c);}
                                                                 ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:81:75: error: use of undeclared identifier 'strpbrk'
  char* __libcpp_strpbrk(const char* __s1, const char* __s2) {return (char*)strpbrk(__s1, __s2);}
                                                                            ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:88:65: error: use of undeclared identifier 'strrchr'; did you mean 'strchr'?
  char* __libcpp_strrchr(const char* __s, int __c) {return (char*)strrchr(__s, __c);}
                                                                  ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:76:13: note: 'strchr' declared here
  const char* strchr(const char* __s, int __c) {return __libcpp_strchr(__s, __c);}
              ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:95:76: error: use of undeclared identifier 'memchr'
  void* __libcpp_memchr(const void* __s, int __c, size_t __n) {return (void*)memchr(__s, __c, __n);}
                                                                             ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:102:74: error: use of undeclared identifier 'strstr'; did you mean 'strchr'?
  char* __libcpp_strstr(const char* __s1, const char* __s2) {return (char*)strstr(__s1, __s2);}
                                                                           ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:78:13: note: 'strchr' declared here
        char* strchr(      char* __s, int __c) {return __libcpp_strchr(__s, __c);}
              ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:102:74: error: no matching function for call to 'strchr'
  char* __libcpp_strstr(const char* __s1, const char* __s2) {return (char*)strstr(__s1, __s2);}
                                                                           ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:78:13: note: candidate disabled: &lt;no message provided&gt;
        char* strchr(      char* __s, int __c) {return __libcpp_strchr(__s, __c);}
              ^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:102:81: error: cannot initialize a parameter of type 'char *' with an lvalue of type 'const char *'
  char* __libcpp_strstr(const char* __s1, const char* __s2) {return (char*)strstr(__s1, __s2);}
                                                                                  ^~~~
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:78:32: note: passing argument to parameter '__s' here
        char* strchr(      char* __s, int __c) {return __libcpp_strchr(__s, __c);}
                                 ^
  In file included from core/desugarer.cpp:19:
  In file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/algorithm:642:
  /Library/Developer/CommandLineTools/usr/include/c++/v1/cstring:70:9: error: no member named 'memcpy' in the global namespace; did you mean 'memchr'?
  using ::memcpy;
        ~~^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:97:13: note: 'memchr' declared here
  const void* memchr(const void* __s, int __c, size_t __n) {return __libcpp_memchr(__s, __c, __n);}
              ^
  In file included from core/desugarer.cpp:19:
  In file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/algorithm:642:
  /Library/Developer/CommandLineTools/usr/include/c++/v1/cstring:71:9: error: no member named 'memmove' in the global namespace
  using ::memmove;
        ~~^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/cstring:72:9: error: no member named 'strcpy' in the global namespace; did you mean 'strchr'?
  using ::strcpy;
        ~~^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:76:13: note: 'strchr' declared here
  const char* strchr(const char* __s, int __c) {return __libcpp_strchr(__s, __c);}
              ^
  In file included from core/desugarer.cpp:19:
  In file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/algorithm:642:
  /Library/Developer/CommandLineTools/usr/include/c++/v1/cstring:73:9: error: no member named 'strncpy' in the global namespace
  using ::strncpy;
        ~~^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/cstring:74:9: error: no member named 'strcat' in the global namespace; did you mean 'strchr'?
  using ::strcat;
        ~~^
  /Library/Developer/CommandLineTools/usr/include/c++/v1/string.h:76:13: note: 'strchr' declared here
  const char* strchr(const char* __s, int __c) {return __libcpp_strchr(__s, __c);}
              ^
  In file included from core/desugarer.cpp:19:
  In file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/algorithm:642:
  /Library/Developer/CommandLineTools/usr/include/c++/v1/cstring:75:9: error: no member named 'strncat' in the global namespace
  using ::strncat;
        ~~^
  fatal error: too many errors emitted, stopping now [-ferror-limit=]
  20 errors generated.
  make: *** [Makefile:118: core/desugarer.o] Error 1
  Traceback (most recent call last):
    File ""&lt;string&gt;"", line 1, in &lt;module&gt;
    File ""/private/var/folders/qf/jkn4v43j08xgst0r9yxyl0dc0000gn/T/pip-install-i4nyb384/jsonnet/setup.py"", line 75, in &lt;module&gt;
      test_suite=""python._jsonnet_test"",
    File ""/anaconda3/lib/python3.7/site-packages/setuptools/__init__.py"", line 143, in setup
      return distutils.core.setup(**attrs)
    File ""/anaconda3/lib/python3.7/distutils/core.py"", line 148, in setup
      dist.run_commands()
    File ""/anaconda3/lib/python3.7/distutils/dist.py"", line 966, in run_commands
      self.run_command(cmd)
    File ""/anaconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/anaconda3/lib/python3.7/site-packages/wheel/bdist_wheel.py"", line 188, in run
      self.run_command('build')
    File ""/anaconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/anaconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/anaconda3/lib/python3.7/distutils/command/build.py"", line 135, in run
      self.run_command(cmd_name)
    File ""/anaconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/anaconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/private/var/folders/qf/jkn4v43j08xgst0r9yxyl0dc0000gn/T/pip-install-i4nyb384/jsonnet/setup.py"", line 54, in run
      raise Exception('Could not build %s' % (', '.join(LIB_OBJECTS)))
  Exception: Could not build core/desugarer.o, core/formatter.o, core/libjsonnet.o, core/lexer.o, core/parser.o, core/pass.o, core/static_analysis.o, core/string_utils.o, core/vm.o, third_party/md5/md5.o

  ----------------------------------------
  Failed building wheel for jsonnet
  Running setup.py clean for jsonnet
Failed to build jsonnet
</code></pre>

<p>My compiler and gcc:</p>

<pre><code>(base) Sakets-MacBook-Pro:usr saketkhandelwal$ gcc -v
Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 10.0.1 (clang-1001.0.46.4)
Target: x86_64-apple-darwin18.5.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
(base) Sakets-MacBook-Pro:usr saketkhandelwal$ clang --version
clang version 4.0.1 (tags/RELEASE_401/final)
Target: x86_64-apple-darwin18.5.0
Thread model: posix
InstalledDir: /anaconda3/bin
</code></pre>

<p>How do i fix this, i have tried reinstalling command line tools and package headers but still no luck. </p>
","11497702","","","","","2019-07-26 07:33:30","Cant install allennlp with pip on mac","<python-3.x><macos><clang++><command-line-tool><allennlp>","1","0","","","","CC BY-SA 4.0"
"65267452","1","","","2020-12-12 16:54:34","","0","268","<p>I am trying to use allennlp predictor for biaffine parser. Here is the code:-</p>
<pre><code>from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path(&quot;https://s3-us-west-2.amazonaws.com/allennlp/models/biaffine-dependency-parser-ptb-2018.08.23.tar.gz&quot;)
</code></pre>
<p>However, I get this error:-</p>
<pre><code>ConfigurationError                        Traceback (most recent call last)
&lt;ipython-input-3-74e2bcfcbf43&gt; in &lt;module&gt;()
      1 from allennlp.predictors.predictor import Predictor
----&gt; 2 predictor = Predictor.from_path(&quot;https://s3-us-west-2.amazonaws.com/allennlp/models/biaffine-dependency-parser-ptb-2018.08.23.tar.gz&quot;)

4 frames
/usr/local/lib/python3.6/dist-packages/allennlp/common/params.py in pop_choice(self, key, choices, default_to_first_choice, allow_class_names)
    350                 &quot;&quot;&quot;{&quot;model&quot;: &quot;my_module.models.MyModel&quot;} to have it imported automatically.&quot;&quot;&quot;
    351             )
--&gt; 352             raise ConfigurationError(message)
    353         return value
    354 

ConfigurationError: universal_dependencies not in acceptable choices for dataset_reader.type: ['conll2003', 'interleaving', 'sequence_tagging', 'sharded', 'babi', 'text_classification_json']. You should either use the --include-package flag to make sure the correct module is loaded, or use a fully qualified class name in your config file like {&quot;model&quot;: &quot;my_module.models.MyModel&quot;} to have it imported automatically.
</code></pre>
","10228144","","","","","2020-12-21 17:10:40","Unable to use Allennlp biaffine parser model","<nlp><pytorch><dependency-parsing><allennlp>","1","3","1","","","CC BY-SA 4.0"
"65536066","1","65634820","","2021-01-02 04:42:57","","0","106","<p>On allennlp textual entailment <a href=""https://demo.allennlp.org/textual-entailment"" rel=""nofollow noreferrer"">demo website</a>, the hypothesis and premise in examples always only consist of one sentence. Does allennlp textual entailment model work when hypothesis and premise both include multiple sentences? Is it theoretically practical? Or could I train the model on my own labeled dataset to make it work on paragraph texts?</p>
<p>For example:</p>
<ul>
<li>Premise: &quot;Whenever Jack is asked whether he prefers mom or dad, he doesn't know how to respond. To be honest, he has no idea why he has to make a choice. &quot;</li>
<li>Hypothesis: &quot;Whom do you love more, mom or dad? Some adults like to use this question to tease kids. For Jack, he doesn't like this question.&quot;</li>
</ul>
<p>I read the paper <a href=""https://www.semanticscholar.org/paper/A-Decomposable-Attention-Model-for-Natural-Language-Parikh-T%C3%A4ckstr%C3%B6m/2cd8e8f510c89c7c18268e8ad51c061e459ad321"" rel=""nofollow noreferrer"">decomposable attention model (Parikh et al, 2017)</a>. This paper doesn't discuss such a scenario. The idea behind the paper is text alignment. So intuitively, I think it should also be reasonable to work on paragraph texts. But I'm not very confident about it.</p>
<p>I sincerely appreciate it if anyone can help with it.</p>
","10737202","","","","","2021-01-08 18:51:17","Does allennlp textual entailment model work when hypothesis and premise both involve multiple sentences?","<python><nlp><allennlp>","1","0","","","","CC BY-SA 4.0"
"66913956","1","","","2021-04-02 03:08:13","","0","62","<p>I`m trying to realize a seq2seq generation style experiment, but i cannot find any instructions for implement a 'Decoder'.
I'm using the Newest Allennlp V2.1. So,I will appreciate it if you could give me some advice!</p>
","9594332","","","","","2021-04-02 11:37:07","How to use AllenNLP to implement a Decoder in seq2seq generation task?","<nlp><pytorch><seq2seq><allennlp>","1","0","","","","CC BY-SA 4.0"
"65735518","1","65745965","","2021-01-15 11:44:59","","0","129","<p>I am pretty new to allennlp and I am struggling with building a model that does not seem to fit perfectly in the standard way of building model in allennlp.</p>
<p>I want to build a pipeline model using NLP. The pipeline consists basically of two models, let's call them A and B. First A is trained and based on the prediction of the full train A, B trained afterwards.</p>
<p>What I have seen is that people define two separate models, train both using the command line interface <code>allennlp train ...</code> in a shell script that looks like</p>
<pre><code># set a bunch of environment variables
...
allennlp train -s $OUTPUT_BASE_PATH_A --include-package MyModel --force $CONFIG_MODEL_A

# prepare environment variables for model b
...
allennlp train -s $OUTPUT_BASE_PATH_B --include-package MyModel --force $CONFIG_MODEL_B
</code></pre>
<p>I have two concerns about that:</p>
<ol>
<li>This code is hard to debug</li>
<li>It's not very flexible. When I want to do a forward pass of the fully trained model I have write another script that bash script that does that.</li>
</ol>
<p>Any ideas on how to do that in a better way?</p>
<p>I thought about using a python script instead of a shell script and invoke <code>allennlp.commands.main(..)</code> directly. Doing so at least you have a joint python module you can run using a debugger.</p>
","3411517","","","","","2021-01-16 02:36:19","Building a Pipline Model using allennlp","<allennlp>","1","0","","","","CC BY-SA 4.0"
"67558874","1","67715821","","2021-05-16 16:06:41","","0","43","<p>I am trying to retrieve embeddings for words based on the pretrained ELMo model available on tensorflow hub. The code I am using is modified from here: <a href=""https://www.geeksforgeeks.org/overview-of-word-embedding-using-embeddings-from-language-models-elmo/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/overview-of-word-embedding-using-embeddings-from-language-models-elmo/</a></p>
<p>The sentence that I am inputting is
<br />
bod =&quot; is coming up in and every project is expected to do a video due on we look forward to discussing this with you at our meeting this this time they have laid out the selection criteria for the video award s go for the top spot this time &quot;</p>
<p>and these are the keywords I want embeddings for:
<br />
words=[&quot;do&quot;, &quot;a&quot;, &quot;video&quot;]</p>
<pre><code>embeddings = elmo([bod],
signature=&quot;default&quot;,
as_dict=True)[&quot;elmo&quot;]
init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)
</code></pre>
<p>this sentence is 236 characters in length.
this is the picture showing that
<a href=""https://i.stack.imgur.com/qlvQN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qlvQN.png"" alt=""lenbod"" /></a></p>
<p>but when I put this sentence into the ELMo model, the tensor that is returned is only contains a string of length 48
<a href=""https://i.stack.imgur.com/w90QH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w90QH.png"" alt=""tensor dim"" /></a></p>
<p>and this becomes a problem when i try to extract embeddings for keywords that are outside the 48 length limit because the indices of the keywords are shown to be outside this length:
<a href=""https://i.stack.imgur.com/kruxD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kruxD.png"" alt=""kywordlen"" /></a></p>
<p>this is the code I used to get the indices for the words in 'bod'(as shown above)</p>
<pre><code>num_list=[]
for item in words:
  print(item)
  index = bod.index(item)
  num_list.append(index)
num_list
</code></pre>
<p>But i keep running into this error:
<a href=""https://i.stack.imgur.com/1Qc2V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1Qc2V.png"" alt=""error"" /></a></p>
<p>I tried looking for ELMo documentation to explain why this is happening but I have not found anything related to this problem of pruned input.</p>
<p>Any advice is much appreciated!</p>
<p>Thank You</p>
","12948573","","","","","2021-05-27 04:47:21","NLP ELMo model pruning input","<neural-network><nlp><word-embedding><allennlp><elmo>","1","0","","","","CC BY-SA 4.0"
"65110398","1","","","2020-12-02 14:48:09","","0","99","<p>I'm trying to load the decomposable attention model proposed in this paper <a href=""https://www.semanticscholar.org/paper/A-Decomposable-Attention-Model-for-Natural-Languag-Parikh-T%C3%A4ckstr%C3%B6m/07a9478e87a8304fc3267fa16e83e9f3bbd98b27"" rel=""nofollow noreferrer"">The decomposable attention model (Parikh et al, 2017) combined with ELMo embeddings trained on SNLI.</a>, and used the code suggested as the <a href=""https://demo.allennlp.org/textual-entailment/"" rel=""nofollow noreferrer"">demo website</a> described:</p>
<pre class=""lang-py prettyprint-override""><code>predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/decomposable-attention-elmo-2020.04.09.tar.gz&quot;, &quot;textual_entailment&quot;)
predictor.predict(
  hypothesis=&quot;Two women are sitting on a blanket near some rocks talking about politics.&quot;,
  premise=&quot;Two women are wandering along the shore drinking iced tea.&quot;
)
</code></pre>
<p>I found this from log:</p>
<pre><code>Did not use initialization regex that was passed: .*token_embedder_tokens\._projection.*weight
</code></pre>
<p>and the prediction was also different from what I got on the demo website (which I intended to see). Did I miss anything here?</p>
<p>Also, I tried the two other versions of the pretrained model, <code>decomposable-attention-elmo-2018.02.19.tar.gz</code> and <code>decomposable-attention-elmo-2020.02.10.tar.gz</code>. Neither of them works and I got this error:</p>
<pre><code>ConfigurationError: key &quot;token_embedders&quot; is required at location &quot;model.text_field_embedder.&quot;
</code></pre>
<p>What do I need to do to get the exact output as presented in the demo website?</p>
","12705269","","","","","2020-12-12 03:17:20","Problem loading ""decomposable-attention-elmo"" with `Predictor.from_path`","<allennlp>","1","0","","","","CC BY-SA 4.0"
"66844202","1","66978557","","2021-03-28 17:26:31","","0","144","<p>I'm having trouble fine-tuning the decomposable-attention-elmo model. I have been able to download the model: <code>wget https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz</code>. I'm trying to load the model and then fine-tune it on my data using the AllenNLP train command line command.</p>
<p>I also created a custom dataset Reader which is similar to the <code>SNLIDatasetReader</code> and it seems to be working well.</p>
<p>I created a <code>.jsonnet</code> file, similar to what is <a href=""https://github.com/allenai/allennlp-models/blob/c62a3479c45a1fe948d17f1d3ac1d024b04550c3/training_config/pair_classification/decomposable_attention_elmo.jsonnet"" rel=""nofollow noreferrer"">here</a>, but I'm having trouble getting it to work.</p>
<p>When I use this version:</p>
<pre><code>// Configuraiton for a textual entailment model based on:
//  Parikh, Ankur P. et al. “A Decomposable Attention Model for Natural Language Inference.” EMNLP (2016).
{
  &quot;dataset_reader&quot;: {
    &quot;type&quot;: &quot;custom_reader&quot;,
    &quot;token_indexers&quot;: {
      &quot;elmo&quot;: {
        &quot;type&quot;: &quot;elmo_characters&quot;
      }
    },
    &quot;tokenizer&quot;: {
      &quot;end_tokens&quot;: [&quot;@@NULL@@&quot;]
    }
  },
  &quot;train_data_path&quot;: &quot;examples_train_&quot;,
  &quot;validation_data_path&quot;: &quot;examples_val_&quot;,
  &quot;model&quot;: {
    &quot;type&quot;: &quot;from_archive&quot;,
    &quot;archive_file&quot;: &quot;decomposable-attention-elmo-2018.02.19.tar.gz&quot;,
    &quot;text_field_embedder&quot;: {
      &quot;token_embedders&quot;: {
        &quot;elmo&quot;: {
            &quot;type&quot;: &quot;elmo_token_embedder&quot;,
            &quot;do_layer_norm&quot;: false,
            &quot;dropout&quot;: 0.2
        }
      }
    },
   },
  &quot;data_loader&quot;: {
    &quot;batch_sampler&quot;: {
      &quot;type&quot;: &quot;bucket&quot;,
      &quot;batch_size&quot;: 64
    }
  },
  &quot;trainer&quot;: {
    &quot;num_epochs&quot;: 140,
    &quot;patience&quot;: 20,
    &quot;grad_clipping&quot;: 5.0,
    &quot;validation_metric&quot;: &quot;+accuracy&quot;,
    &quot;optimizer&quot;: {
      &quot;type&quot;: &quot;adagrad&quot;
    }
  }
}
</code></pre>
<p>I get an error:</p>
<pre><code> File &quot;lib/python3.6/site-packages/allennlp/common/params.py&quot;, line 423, in assert_empty
    &quot;Extra parameters passed to {}: {}&quot;.format(class_name, self.params)
allennlp.common.checks.ConfigurationError: Extra parameters passed to Model: {'text_field_embedder': {'token_embedders': {'elmo': {'do_layer_norm': False, 'dropout': 0.2, 'type': 'elmo_token_embedder'}}}}
</code></pre>
<p>Then, when I take that <code>text_field_embedder</code> portion out, and use this version:</p>
<pre><code>// Configuraiton for a textual entailment model based on:
//  Parikh, Ankur P. et al. “A Decomposable Attention Model for Natural Language Inference.” EMNLP (2016).
{
  &quot;dataset_reader&quot;: {
    &quot;type&quot;: &quot;fake_news&quot;,
    &quot;token_indexers&quot;: {
      &quot;elmo&quot;: {
        &quot;type&quot;: &quot;elmo_characters&quot;
      }
    },
    &quot;tokenizer&quot;: {
      &quot;end_tokens&quot;: [&quot;@@NULL@@&quot;]
    }
  },
  &quot;train_data_path&quot;: &quot;examples_train_&quot;,
  &quot;validation_data_path&quot;: &quot;examples_val_&quot;,
  &quot;model&quot;: {
    &quot;type&quot;: &quot;from_archive&quot;,
    &quot;archive_file&quot;: &quot;decomposable-attention-elmo-2018.02.19.tar.gz&quot;,
   },
  &quot;data_loader&quot;: {
    &quot;batch_sampler&quot;: {
      &quot;type&quot;: &quot;bucket&quot;,
      &quot;batch_size&quot;: 64
    }
  },
  &quot;trainer&quot;: {
    &quot;num_epochs&quot;: 140,
    &quot;patience&quot;: 20,
    &quot;grad_clipping&quot;: 5.0,
    &quot;validation_metric&quot;: &quot;+accuracy&quot;,
    &quot;optimizer&quot;: {
      &quot;type&quot;: &quot;adagrad&quot;
    }
  }
}
</code></pre>
<p>I get an error:</p>
<pre><code>    raise ConfigurationError(msg)
allennlp.common.checks.ConfigurationError: key &quot;token_embedders&quot; is required at location &quot;model.text_field_embedder.&quot;
</code></pre>
<p>The two errors seem contradictory and I'm not sure how to proceed with this fine-tuning.</p>
","4400904","","","","","2021-04-07 01:51:30","Trouble Finetuning Decomposable Attention Model in AllenNLP","<python><allennlp>","1","0","","","","CC BY-SA 4.0"
"66886969","1","","","2021-03-31 11:28:20","","0","87","<p>I need to connect AllenNLP model to Microsoft Web Chat or Microsoft Bot Framework. Is it possible?</p>
<p>is AllenNLP provides any APIs endpoints for connecting?</p>
<p>How do I create models and where that models are store in AllenNLP?</p>
<p>If anyone has a good amount of knowledge, please let me know your feedback on the above questions.</p>
<p>Thanks in Advance.</p>
","15470275","","","","","2021-04-02 01:18:44","How to connect AllenNLP model to Microsoft Web Chat or Microsoft Bot Framework","<api><botframework><web-chat><allennlp>","1","0","","","","CC BY-SA 4.0"
"65446142","1","","","2020-12-25 07:35:09","","0","105","<p>I use allennlp frame for nlp learning. When using single gpu, it works. But when I change it to multi gpus, it will get stuck at the beginning.</p>
<p>The configuration works fine with single gpu.</p>
<p><strong>environment</strong></p>
<pre><code>using anaconda
ubuntu 16.04

pytorch==1.7.1
allennlp==1.3.0
nvcc -V v10.2.89
driver version: 440.33.01
cuda version: 10.2
</code></pre>
<p>And I use 1080ti * 2 and AMD Ryzen 5 1600</p>
<p>The program generate 3 logs.<code>out.log</code>, <code>out_worker0.log</code>, <code>out_worker1.log</code>.</p>
<p>list them below</p>
<pre><code># out.log

2020-12-25 14:54:22,558 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2020-12-25 14:54:22,558 - INFO - allennlp.common.params - dataset_reader.type = my_simple_reader
2020-12-25 14:54:22,559 - INFO - allennlp.common.params - dataset_reader.lazy = False
2020-12-25 14:54:22,559 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2020-12-25 14:54:22,559 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2020-12-25 14:54:22,559 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2020-12-25 14:54:22,559 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2020-12-25 14:54:22,559 - INFO - allennlp.common.params - train_data_path = data/train.txt
2020-12-25 14:54:22,559 - INFO - allennlp.training.util - Reading training data from data/train.txt
2020-12-25 14:54:22,561 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2020-12-25 14:54:23,212 - INFO - allennlp.common.params - vocabulary.type = from_instances
2020-12-25 14:54:23,213 - INFO - allennlp.common.params - vocabulary.min_count = None
2020-12-25 14:54:23,213 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None
2020-12-25 14:54:23,213 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')
2020-12-25 14:54:23,213 - INFO - allennlp.common.params - vocabulary.pretrained_files = None
2020-12-25 14:54:23,213 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False
2020-12-25 14:54:23,213 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None
2020-12-25 14:54:23,213 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None
2020-12-25 14:54:23,213 - INFO - allennlp.common.params - vocabulary.padding_token = @@PADDING@@
2020-12-25 14:54:23,213 - INFO - allennlp.common.params - vocabulary.oov_token = @@UNKNOWN@@
2020-12-25 14:54:23,213 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2020-12-25 14:54:23,214 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2020-12-25 14:54:23,214 - INFO - allennlp.training.util - writing the vocabulary to tmp/debugger/vocabulary.
2020-12-25 14:54:23,214 - INFO - allennlp.training.util - done creating vocab
2020-12-25 14:54:23,214 - INFO - root - Switching to distributed training mode since multiple GPUs are configured | Master is at: 127.0.0.1:37039 | Rank of this node: 0 | Number of workers in this node: 2 | Number of nodes: 1 | World size: 2

</code></pre>
<pre><code># out_worker0.log

0 | 2020-12-25 14:54:24,863 - INFO - allennlp.common.params - random_seed = 13370
0 | 2020-12-25 14:54:24,863 - INFO - allennlp.common.params - numpy_seed = 1337
0 | 2020-12-25 14:54:24,863 - INFO - allennlp.common.params - pytorch_seed = 133
0 | 2020-12-25 14:54:24,864 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
</code></pre>
<pre><code># out_worker1.log

1 | 2020-12-25 14:54:24,826 - INFO - allennlp.common.params - random_seed = 13370
1 | 2020-12-25 14:54:24,826 - INFO - allennlp.common.params - numpy_seed = 1337
1 | 2020-12-25 14:54:24,826 - INFO - allennlp.common.params - pytorch_seed = 133
1 | 2020-12-25 14:54:24,827 - INFO - allennlp.common.checks - Pytorch version: 1.7.1

</code></pre>
<p>It got stuck for more than 10 mins. So I ctrl-c to interrupt it. The message as below:</p>
<pre><code>^CTraceback (most recent call last):
  File &quot;/home/axx/anaconda3/envs/allen-test/bin/allennlp&quot;, line 8, in &lt;module&gt;
    sys.exit(run())
  File &quot;/home/axx/anaconda3/envs/allen-test/lib/python3.6/site-packages/allennlp/__main__.py&quot;, line 34, in run
    main(prog=&quot;allennlp&quot;)
  File &quot;/home/axx/anaconda3/envs/allen-test/lib/python3.6/site-packages/allennlp/commands/__init__.py&quot;, line 118, in main
    args.func(args)
  File &quot;/home/axx/anaconda3/envs/allen-test/lib/python3.6/site-packages/allennlp/commands/train.py&quot;, line 119, in train_model_from_args
    file_friendly_logging=args.file_friendly_logging,
  File &quot;/home/axx/anaconda3/envs/allen-test/lib/python3.6/site-packages/allennlp/commands/train.py&quot;, line 178, in train_model_from_file
    file_friendly_logging=file_friendly_logging,
  File &quot;/home/axx/anaconda3/envs/allen-test/lib/python3.6/site-packages/allennlp/commands/train.py&quot;, line 323, in train_model
    nprocs=num_procs,
  File &quot;/home/axx/anaconda3/envs/allen-test/lib/python3.6/site-packages/torch/multiprocessing/spawn.py&quot;, line 199, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File &quot;/home/axx/anaconda3/envs/allen-test/lib/python3.6/site-packages/torch/multiprocessing/spawn.py&quot;, line 157, in start_processes
    while not context.join():
  File &quot;/home/axx/anaconda3/envs/allen-test/lib/python3.6/site-packages/torch/multiprocessing/spawn.py&quot;, line 77, in join
    timeout=timeout,
  File &quot;/home/axx/anaconda3/envs/allen-test/lib/python3.6/multiprocessing/connection.py&quot;, line 911, in wait
    ready = selector.select(timeout)
  File &quot;/home/axx/anaconda3/envs/allen-test/lib/python3.6/selectors.py&quot;, line 376, in select
    fd_event_list = self._poll.poll(timeout)
KeyboardInterrupt
^CError in atexit._run_exitfuncs:
Traceback (most recent call last):
  File &quot;/home/axx/anaconda3/envs/allen-test/lib/python3.6/multiprocessing/popen_fork.py&quot;, line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
</code></pre>
","5564226","","","","","2020-12-25 07:35:09","distributed training got stuck at the beginning","<allennlp>","0","0","","","","CC BY-SA 4.0"
"65157530","1","","","2020-12-05 13:42:14","","1","110","<p>I have been working on a github project from here:
<a href=""https://github.com/jiacheng-xu/DiscoBERT"" rel=""nofollow noreferrer"">https://github.com/jiacheng-xu/DiscoBERT</a>
According to the authors, it requires Allennlp 0.9.0. I created a virtual environment using pip and tried installing 0.9, but it gave an error. So, I tried Allennlp 1.2 which installed fine,
but I am getting errors in the two following lines:</p>
<pre><code>from allennlp.commands.fine_tune import fine_tune_model_from_file_paths
from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertModel, PretrainedBertEmbedder
</code></pre>
<p>It seems, in the 1.2 version at least, there is no fine_tune and bert_token_embedder module. Is there a quick way to get past these errors?</p>
","3430220","","","","","2020-12-12 03:07:58","No Fine-Tune/ Bert_token_embedder module inside Allennlp commands or token_embedders","<allennlp>","1","0","","","","CC BY-SA 4.0"
"67516665","1","","","2021-05-13 09:09:07","","0","208","<p>A want to resolve coreferences without Internet using AllenNLP and coref-spanbert-large model.
I try to do it in the way that is describing here <a href=""https://demo.allennlp.org/coreference-resolution"" rel=""nofollow noreferrer"">https://demo.allennlp.org/coreference-resolution</a></p>
<p>My code:</p>
<pre><code>from allennlp.predictors.predictor import Predictor
import allennlp_models.tagging

predictor = Predictor.from_path(r&quot;C:\Users\aap\Desktop\coref-spanbert-large-2021.03.10.tar.gz&quot;)
example = 'Paul Allen was born on January 21, 1953, in Seattle, Washington, to Kenneth Sam Allen and Edna Faye Allen.Allen attended Lakeside School, a private school in Seattle, where he befriended Bill Gates, two years younger, with whom he shared an enthusiasm for computers.'
pred = predictor.predict(document=example)
coref_res = predictor.coref_resolved(example)
print(pred)
print(coref_res)
</code></pre>
<p>When I have an access to internet the code works correctly.
But when I don't have an access to internet I get the following errors:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:/Users/aap/Desktop/CoreNLP/Coref_AllenNLP.py&quot;, line 14, in &lt;module&gt;
    predictor = Predictor.from_path(r&quot;C:\Users\aap\Desktop\coref-spanbert-large-2021.03.10.tar.gz&quot;)
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\predictors\predictor.py&quot;, line 361, in from_path
    load_archive(archive_path, cuda_device=cuda_device, overrides=overrides),
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\models\archival.py&quot;, line 206, in load_archive
    config.duplicate(), serialization_dir
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\models\archival.py&quot;, line 232, in _load_dataset_readers
    dataset_reader_params, serialization_dir=serialization_dir
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\common\from_params.py&quot;, line 604, in from_params
    **extras,
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\common\from_params.py&quot;, line 632, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\common\from_params.py&quot;, line 200, in create_kwargs
    cls.__name__, param_name, annotation, param.default, params, **extras
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\common\from_params.py&quot;, line 307, in pop_and_construct_arg
    return construct_arg(class_name, name, popped_params, annotation, default, **extras)
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\common\from_params.py&quot;, line 391, in construct_arg
    **extras,
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\common\from_params.py&quot;, line 341, in construct_arg
    return annotation.from_params(params=popped_params, **subextras)
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\common\from_params.py&quot;, line 604, in from_params
    **extras,
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\common\from_params.py&quot;, line 634, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\data\token_indexers\pretrained_transformer_mismatched_indexer.py&quot;, line 63, in __init__
    **kwargs,
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\data\token_indexers\pretrained_transformer_indexer.py&quot;, line 58, in __init__
    model_name, tokenizer_kwargs=tokenizer_kwargs
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\data\tokenizers\pretrained_transformer_tokenizer.py&quot;, line 71, in __init__
    model_name, add_special_tokens=False, **tokenizer_kwargs
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\allennlp\common\cached_transformers.py&quot;, line 110, in get_tokenizer
    **kwargs,
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\transformers\models\auto\tokenization_auto.py&quot;, line 362, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\transformers\models\auto\configuration_auto.py&quot;, line 368, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\transformers\configuration_utils.py&quot;, line 424, in get_config_dict
    use_auth_token=use_auth_token,
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\transformers\file_utils.py&quot;, line 1087, in cached_path
    local_files_only=local_files_only,
  File &quot;C:\Users\aap\Desktop\CoreNLP\corenlp\lib\site-packages\transformers\file_utils.py&quot;, line 1268, in get_from_cache
    &quot;Connection error, and we cannot find the requested files in the cached path.&quot;
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

Process finished with exit code 1
</code></pre>
<p>Please, say me, what do I need to do my code works without Internet?</p>
","15914766","","","","","2021-05-14 16:58:14","How to resolve coreferences without Internet using AllenNLP and coref-spanbert-large?","<allennlp><coreference-resolution>","1","0","","","","CC BY-SA 4.0"
"63743557","1","","","2020-09-04 14:56:10","","5","203","<p>I'm doing a finetuning of a <a href=""https://arxiv.org/abs/2004.05150"" rel=""nofollow noreferrer"">Longformer</a> on a <strong>document text binary classification task</strong> using <a href=""https://huggingface.co/transformers/main_classes/trainer.html"" rel=""nofollow noreferrer"">Huggingface Trainer class</a> and I'm monitoring the measures of some checkpoints with Tensorboard.</p>
<p>Even if the F1 score and accuracy is quite high, I have perplexities about the <strong>fluctuations of training loss</strong>.</p>
<p>I read online a reason for that can be:</p>
<ul>
<li>the <em>too high learning rate</em>, but I tried with 3 values (1e-4, 1e-5 and 1e-6) and all of them made the same effect</li>
<li>a <em>small batch size</em>. I'm using a <strong>Sagemaker notebook p2.8xlarge</strong> which has 8xK80 GPUs. The batch size per GPU I can use to avoid the <strong>CUDA out of memory error</strong> is 1. So the total batch size is <strong>8</strong>. My intuition is that a bs of 8 is too small for a dataset containing 57K examples (7K steps per epoch). Unfortunately it's the highest value I can use.</li>
</ul>
<p>Here I have reported the trend of F1, accuracy, loss and smoothed loss. The grey line is with 1e-6 of learning rate while the pink one is 1e-5.</p>
<img src=""https://i.stack.imgur.com/KaM9D.png"" width=""400"" />
<img src=""https://i.stack.imgur.com/aUCtr.png"" width=""400"" />
<img src=""https://i.stack.imgur.com/u91dT.png"" width=""400"" />
<img src=""https://i.stack.imgur.com/B0IMj.png"" width=""400"" />
<p>I reasume all the <strong>info</strong> of my training:</p>
<ul>
<li><strong>batch size</strong>: 1 x 8GPU = <strong>8</strong></li>
<li><strong>learning rate</strong>: <strong>1e-4</strong>, <strong>1e-5</strong>, <strong>1e-6</strong> (all of them tested without improvement on loss)</li>
<li><strong>model</strong>: <strong>Longformer</strong></li>
<li><strong>dataset</strong>:
<ul>
<li><em>training set</em>: <strong>57K examples</strong></li>
<li><em>dev set</em>: <strong>12K examples</strong></li>
<li><em>test set</em>: <strong>12K examples</strong></li>
</ul>
</li>
</ul>
<p>Which could be the reason? Can this be considered a problem despite the quite good F1 and accuracy results?</p>
","12569908","","12569908","","2020-09-05 09:31:45","2021-04-10 12:54:59","Fluctuating loss during training for text binary classification","<python><machine-learning><pytorch><huggingface-transformers><allennlp>","1","5","1","","","CC BY-SA 4.0"
"66303474","1","","","2021-02-21 14:44:37","","0","69","<p>I got error (&quot;AssertionError: Found no field that needed padding; we are surprised you got this error, please open an issue on github&quot; ).</p>
<p><a href=""https://i.stack.imgur.com/MsIqW.png"" rel=""nofollow noreferrer"">&quot;AssertionError: Found no field that needed padding; we are surprised you got this error, please open an issue on github&quot;</a></p>
<p>I don't know why I got this error.</p>
<p>my config file is below.</p>
<pre><code>&quot;&quot;&quot;
{
    &quot;dataset_reader&quot;: {
        &quot;type&quot;: &quot;tbmse_drop&quot;,
        &quot;answer_field_generators&quot;: {
            &quot;arithmetic_answer&quot;: {
                &quot;type&quot;: &quot;arithmetic_answer_generator&quot;,
                &quot;special_numbers&quot;: [
                    100,
                    1
                ]
            },
            &quot;count_answer&quot;: {
                &quot;type&quot;: &quot;count_answer_generator&quot;
            },
            &quot;passage_span_answer&quot;: {
                &quot;type&quot;: &quot;span_answer_generator&quot;,
                &quot;text_type&quot;: &quot;passage&quot;
            },
            &quot;question_span_answer&quot;: {
                &quot;type&quot;: &quot;span_answer_generator&quot;,
                &quot;text_type&quot;: &quot;question&quot;
            },
            &quot;tagged_answer&quot;: {
                &quot;type&quot;: &quot;tagged_answer_generator&quot;,
                &quot;ignore_question&quot;: false,
                &quot;labels&quot;: {
                    &quot;I&quot;: 1,
                    &quot;O&quot;: 0
                }
            }
        },
        &quot;answer_generator_names_per_type&quot;: {
            &quot;date&quot;: [
                &quot;arithmetic_answer&quot;,
                &quot;passage_span_answer&quot;,
                &quot;question_span_answer&quot;,
                &quot;tagged_answer&quot;
            ],
            &quot;multiple_span&quot;: [
                &quot;tagged_answer&quot;
            ],
            &quot;number&quot;: [
                &quot;arithmetic_answer&quot;,
                &quot;count_answer&quot;,
                &quot;passage_span_answer&quot;,
                &quot;question_span_answer&quot;,
                &quot;tagged_answer&quot;
            ],
            &quot;single_span&quot;: [
                &quot;tagged_answer&quot;,
                &quot;passage_span_answer&quot;,
                &quot;question_span_answer&quot;
            ]
        },
        &quot;is_training&quot;: true,
        &quot;old_reader_behavior&quot;: true,
        &quot;pickle&quot;: {
            &quot;action&quot;: &quot;load&quot;,
            &quot;file_name&quot;: &quot;all_heads_IO_roberta-large&quot;,
            &quot;path&quot;: &quot;../pickle/drop&quot;
        },
        &quot;tokenizer&quot;: {
            &quot;type&quot;: &quot;huggingface_transformers&quot;,
            &quot;pretrained_model&quot;: &quot;roberta-large&quot;
        }
    },
    &quot;model&quot;: {
        &quot;type&quot;: &quot;multi_head&quot;,
        &quot;dataset_name&quot;: &quot;drop&quot;,
        &quot;head_predictor&quot;: {
            &quot;activations&quot;: [
                &quot;relu&quot;,
                &quot;linear&quot;
            ],
            &quot;dropout&quot;: [
                0.1,
                0
            ],
            &quot;hidden_dims&quot;: [
                1024,
                5
            ],
            &quot;input_dim&quot;: 2048,
            &quot;num_layers&quot;: 2
        },
        &quot;heads&quot;: {
            &quot;arithmetic&quot;: {
                &quot;type&quot;: &quot;arithmetic_head&quot;,
                &quot;output_layer&quot;: {
                    &quot;activations&quot;: [
                        &quot;relu&quot;,
                        &quot;linear&quot;
                    ],
                    &quot;dropout&quot;: [
                        0.1,
                        0
                    ],
                    &quot;hidden_dims&quot;: [
                        1024,
                        3
                    ],
                    &quot;input_dim&quot;: 2048,
                    &quot;num_layers&quot;: 2
                },
                &quot;special_embedding_dim&quot;: 1024,
                &quot;special_numbers&quot;: [
                    100,
                    1
                ],
                &quot;training_style&quot;: &quot;soft_em&quot;
            },
            &quot;count&quot;: {
                &quot;type&quot;: &quot;count_head&quot;,
                &quot;max_count&quot;: 10,
                &quot;output_layer&quot;: {
                    &quot;activations&quot;: [
                        &quot;relu&quot;,
                        &quot;linear&quot;
                    ],
                    &quot;dropout&quot;: [
                        0.1,
                        0
                    ],
                    &quot;hidden_dims&quot;: [
                        1024,
                        11
                    ],
                    &quot;input_dim&quot;: 1024,
                    &quot;num_layers&quot;: 2
                }
            },
            &quot;multi_span&quot;: {
                &quot;type&quot;: &quot;multi_span_head&quot;,
                &quot;decoding_style&quot;: &quot;at_least_one&quot;,
                &quot;ignore_question&quot;: false,
                &quot;labels&quot;: {
                    &quot;I&quot;: 1,
                    &quot;O&quot;: 0
                },
                &quot;output_layer&quot;: {
                    &quot;activations&quot;: [
                        &quot;relu&quot;,
                        &quot;linear&quot;
                    ],
                    &quot;dropout&quot;: [
                        0.1,
                        0
                    ],
                    &quot;hidden_dims&quot;: [
                        1024,
                        2
                    ],
                    &quot;input_dim&quot;: 1024,
                    &quot;num_layers&quot;: 2
                },
                &quot;prediction_method&quot;: &quot;viterbi&quot;,
                &quot;training_style&quot;: &quot;soft_em&quot;
            },
            &quot;passage_span&quot;: {
                &quot;type&quot;: &quot;passage_span_head&quot;,
                &quot;end_output_layer&quot;: {
                    &quot;activations&quot;: &quot;linear&quot;,
                    &quot;hidden_dims&quot;: 1,
                    &quot;input_dim&quot;: 1024,
                    &quot;num_layers&quot;: 1
                },
                &quot;start_output_layer&quot;: {
                    &quot;activations&quot;: &quot;linear&quot;,
                    &quot;hidden_dims&quot;: 1,
                    &quot;input_dim&quot;: 1024,
                    &quot;num_layers&quot;: 1
                },
                &quot;training_style&quot;: &quot;soft_em&quot;
            },
            &quot;question_span&quot;: {
                &quot;type&quot;: &quot;question_span_head&quot;,
                &quot;end_output_layer&quot;: {
                    &quot;activations&quot;: [
                        &quot;relu&quot;,
                        &quot;linear&quot;
                    ],
                    &quot;dropout&quot;: [
                        0.1,
                        0
                    ],
                    &quot;hidden_dims&quot;: [
                        1024,
                        1
                    ],
                    &quot;input_dim&quot;: 2048,
                    &quot;num_layers&quot;: 2
                },
                &quot;training_style&quot;: &quot;soft_em&quot;
            }
        },
        &quot;passage_summary_vector_module&quot;: {
            &quot;activations&quot;: &quot;linear&quot;,
            &quot;hidden_dims&quot;: 1,
            &quot;input_dim&quot;: 1024,
            &quot;num_layers&quot;: 1
        },
        &quot;pretrained_model&quot;: &quot;roberta-large&quot;,
        &quot;question_summary_vector_module&quot;: {
            &quot;activations&quot;: &quot;linear&quot;,
            &quot;hidden_dims&quot;: 1,
            &quot;input_dim&quot;: 1024,
            &quot;num_layers&quot;: 1
        }
    },
    &quot;train_data_path&quot;: &quot;drop_data/drop_dataset_train.json&quot;,
    &quot;validation_data_path&quot;: &quot;drop_data/drop_dataset_dev.json&quot;,
    &quot;trainer&quot;: {
        &quot;num_epochs&quot;: 15,
        &quot;optimizer&quot;: {
            &quot;type&quot;: &quot;adam&quot;,
            &quot;lr&quot;: 5e-06
        },
        &quot;patience&quot;: 10,
        &quot;validation_metric&quot;: &quot;+f1&quot;
    },
    &quot;data_loader&quot;: {
        &quot;batch_sampler&quot;: {
            &quot;type&quot;: &quot;bucket&quot;,
            &quot;batch_size&quot;: 1
        }
    },
    &quot;distributed&quot;: {
        &quot;cuda_devices&quot;: [
            0,
            1
        ]
    },
    &quot;validation_dataset_reader&quot;: {
        &quot;type&quot;: &quot;tbmse_drop&quot;,
        &quot;answer_field_generators&quot;: {
            &quot;arithmetic_answer&quot;: {
                &quot;type&quot;: &quot;arithmetic_answer_generator&quot;,
                &quot;special_numbers&quot;: [
                    100,
                    1
                ]
            },
            &quot;count_answer&quot;: {
                &quot;type&quot;: &quot;count_answer_generator&quot;
            },
            &quot;passage_span_answer&quot;: {
                &quot;type&quot;: &quot;span_answer_generator&quot;,
                &quot;text_type&quot;: &quot;passage&quot;
            },
            &quot;question_span_answer&quot;: {
                &quot;type&quot;: &quot;span_answer_generator&quot;,
                &quot;text_type&quot;: &quot;question&quot;
            },
            &quot;tagged_answer&quot;: {
                &quot;type&quot;: &quot;tagged_answer_generator&quot;,
                &quot;ignore_question&quot;: false,
                &quot;labels&quot;: {
                    &quot;I&quot;: 1,
                    &quot;O&quot;: 0
                }
            }
        },
        &quot;answer_generator_names_per_type&quot;: {
            &quot;date&quot;: [
                &quot;arithmetic_answer&quot;,
                &quot;passage_span_answer&quot;,
                &quot;question_span_answer&quot;,
                &quot;tagged_answer&quot;
            ],
            &quot;multiple_span&quot;: [
                &quot;tagged_answer&quot;
            ],
            &quot;number&quot;: [
                &quot;arithmetic_answer&quot;,
                &quot;count_answer&quot;,
                &quot;passage_span_answer&quot;,
                &quot;question_span_answer&quot;,
                &quot;tagged_answer&quot;
            ],
            &quot;single_span&quot;: [
                &quot;tagged_answer&quot;,
                &quot;passage_span_answer&quot;,
                &quot;question_span_answer&quot;
            ]
        },
        &quot;is_training&quot;: false,
        &quot;old_reader_behavior&quot;: true,
        &quot;pickle&quot;: {
            &quot;action&quot;: &quot;load&quot;,
            &quot;file_name&quot;: &quot;all_heads_IO_roberta-large&quot;,
            &quot;path&quot;: &quot;../pickle/drop&quot;
        },
        &quot;tokenizer&quot;: {
            &quot;type&quot;: &quot;huggingface_transformers&quot;,
            &quot;pretrained_model&quot;: &quot;roberta-large&quot;
        }
    }
}
&quot;&quot;&quot;
</code></pre>
<p>I used allennlp 2.0.1 &amp; 2.0.2 (The same error occurred in both versions.)</p>
","15254045","","6370655","","2021-02-21 18:07:08","2021-03-13 01:32:13","""AssertionError: Found no field that needed padding; we are surprised you got this error, please open an issue on github"" error occured","<python-3.x><allennlp>","1","1","","","","CC BY-SA 4.0"
"66079667","1","","","2021-02-06 17:17:15","","0","70","<p>Thank you for sharing your fantastic tool with us. Very excellent job.</p>
<p>Just a question, why I got different constituency parsing result between online task demo and local python library? I think both of them are based on <a href=""https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz"" rel=""nofollow noreferrer"">this</a> model?</p>
<p>For example, input the same sentence,</p>
<blockquote>
<p>They quickly ran to the place which is sound came from.</p>
</blockquote>
<p>(from a student's composition).</p>
<p>The online demo gave the result:
<code> (S (NP (PRP They)) (ADVP (RB quickly)) (VBD ran) (PP (IN to) (NP (NP (DT the) (NN place)) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (NP ***(NN sound)***)))))) (VP (VBD came) (PP (IN from))) (. .))</code></p>
<p>but the result of python library version:
<code>(S (NP (PRP They)) (ADVP (RB quickly)) (VBD ran) (PP (IN to) (NP (NP (DT the) (NN place)) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (NP ***(JJ sound)***)))))) (VP (VBD came) (PP (IN from))) (. .))</code></p>
<p>It seems the online demo gave a better result.</p>
","15159214","","8080648","","2021-02-06 21:10:18","2021-02-12 01:50:42","AllenNLP: Different Constituency Parsing Using Taskdemo and Python Library","<parsing><allennlp>","1","0","","","","CC BY-SA 4.0"
"51666481","1","","","2018-08-03 06:26:01","","1","235","<p>I am new to allennlp, I use sentencepiece for subword tokenization in my pipeline. </p>

<p>SentencePiece needs a training step to generate a subword model, which can then be used for tokenization.</p>

<p>Is an implementation of <code>Vocabulary</code>  class the right way to do it. Little confused whether it is the right place, given there are TokenIndexers for character tokenization etc.</p>
","3342890","","7483494","","2019-08-22 19:02:56","2019-08-22 19:02:56","Where to add SentencePiece tokenization in AllenNlp pipeline?","<python><nlp><allennlp>","0","2","","","","CC BY-SA 4.0"
"63173417","1","63173540","","2020-07-30 12:42:28","","-3","260","<p><a href=""https://pypi.org/project/nlg/"" rel=""nofollow noreferrer"">Here's the Site</a></p>
<p>From here i just try to ran the sample code provided on site, but am getting this error</p>
<p><a href=""https://i.stack.imgur.com/uf5qP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uf5qP.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>TypeError                                 Traceback (most recent call last)
 in 
----&gt; 1 text = nlp(&quot;The virginica species has the least average sepal_width.&quot;)</p>
<p>TypeError: 'NLP' object is not callable</p>
<p>I have installed all packages, but still what might have cause this issue?</p>
","9674126","","9674126","","2020-07-30 12:51:25","2020-08-30 23:02:30","TypeError: 'NLP' object is not callable","<python><nlp><nlg><allennlp><simplenlg>","2","2","","2020-07-30 20:45:06","","CC BY-SA 4.0"
"67597326","1","","","2021-05-19 05:16:56","","0","67","<p>would you please tell me why when i use the following script:</p>
<pre><code>from allennlp.common import JsonDict
</code></pre>
<p>i get the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-7-58adc5745662&gt; in &lt;module&gt;()
----&gt; 1 from allennlp.common import JsonDict

10 frames
/usr/local/lib/python3.7/dist-packages/overrides/signature.py in ensure_all_kwargs_defined_in_sub(super_sig, sub_sig, super_type_hints, sub_type_hints, check_first_parameter, method_name)
    134             name, True, sub_has_var_kwargs, sub_sig, super_param
    135         ):
--&gt; 136             raise TypeError(f&quot;{method_name}: `{name}` is not present.&quot;)
    137         elif name in sub_sig.parameters and super_param.kind != Parameter.VAR_KEYWORD:
    138             sub_index = list(sub_sig.parameters.keys()).index(name)

TypeError: FileLock.acquire: `poll_intervall` is not present.
</code></pre>
","15967983","","7985538","","2021-05-19 05:25:16","2021-05-19 05:25:16","allennlp.common.JsonDict, TypeError: FileLock.acquire: `poll_intervall` is not present","<python><allennlp>","0","2","","","","CC BY-SA 4.0"
"65840129","1","65849352","","2021-01-22 06:11:37","","-1","130","<p>I'd like to train the <a href=""https://demo.allennlp.org/textual-entailment"" rel=""nofollow noreferrer"">decomposable attention + ELMo; SNLI</a> model on the demo with my own dataset. I'm new to nlp. After going through the <a href=""https://guide.allennlp.org/"" rel=""nofollow noreferrer"">guide</a>, I still have no idea of how to start off with my own training set consisting of plain text premise, hypothesis, and label. The data format is displayed below.</p>
<p>Based on the training command on demo, I found its training set is <code>https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_train.jsonl</code>. How can I generate such a training set with my own data?</p>
<p>FYI.
my dataset is like:</p>
<pre><code>{ &quot;premise&quot;:&quot;sentences&quot;, &quot;hypothesis&quot;:&quot;sentences&quot;, &quot;label&quot;:&quot;x&quot;}
{ &quot;premise&quot;:&quot;sentences&quot;, &quot;hypothesis&quot;:&quot;sentences&quot;, &quot;label&quot;:&quot;y&quot;}
...
</code></pre>
<p>The entry in <code>snli_1.0_train.jsonl</code> is like:</p>
<pre><code>{&quot;annotator_labels&quot;: [&quot;neutral&quot;], &quot;captionID&quot;: &quot;3416050480.jpg#4&quot;, &quot;gold_label&quot;: &quot;neutral&quot;, &quot;pairID&quot;: &quot;3416050480.jpg#4r1n&quot;, &quot;sentence1&quot;: &quot;A person on a horse jumps over a broken down airplane.&quot;, &quot;sentence1_binary_parse&quot;: &quot;( ( ( A person ) ( on ( a horse ) ) ) ( ( jumps ( over ( a ( broken ( down airplane ) ) ) ) ) . ) )&quot;, &quot;sentence1_parse&quot;: &quot;(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN on) (NP (DT a) (NN horse)))) (VP (VBZ jumps) (PP (IN over) (NP (DT a) (JJ broken) (JJ down) (NN airplane)))) (. .)))&quot;, &quot;sentence2&quot;: &quot;A person is training his horse for a competition.&quot;, &quot;sentence2_binary_parse&quot;: &quot;( ( A person ) ( ( is ( ( training ( his horse ) ) ( for ( a competition ) ) ) ) . ) )&quot;, &quot;sentence2_parse&quot;: &quot;(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) (VP (VBG training) (NP (PRP$ his) (NN horse)) (PP (IN for) (NP (DT a) (NN competition))))) (. .)))&quot;}
</code></pre>
<p>I really appreciate it if anyone can help. Thanks.</p>
","10737202","","","","","2021-01-22 16:49:08","How to train a textual entailment model with my own training set?","<python><nlp><allennlp>","1","0","","","","CC BY-SA 4.0"
"67122708","1","","","2021-04-16 09:31:53","","0","240","<p>I'm trying to train AllenNLPs coreference model on a 16GB GPU, using this config file: <a href=""https://github.com/allenai/allennlp-models/blob/main/training_config/coref/coref_spanbert_large.jsonnet"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp-models/blob/main/training_config/coref/coref_spanbert_large.jsonnet</a></p>
<p>I created train, test, and dev files using this script: <a href=""https://github.com/allenai/allennlp/blob/master/scripts/compile_coref_data.sh"" rel=""nofollow noreferrer"">https://github.com/allenai/allennlp/blob/master/scripts/compile_coref_data.sh</a></p>
<p>I got CUDA out of memory almost instantly, so I tried changing &quot;spans_per_word&quot; and &quot;max_antecedents&quot; to lower values. With spans_per_words set to 0.1 instead of 0.4, I could run a bit longer but not nearly a full epoch. Is a 16GB GPU not enough? Or are there other parameters I could try changing?</p>
<blockquote>
<p>Traceback (most recent call last):
File &quot;/home/ubuntu/anaconda3/envs/allennlp/bin/allennlp&quot;, line 8, in 
sys.exit(run())
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/<strong>main</strong>.py&quot;, line 34, in run
main(prog=&quot;allennlp&quot;)
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/commands/<strong>init</strong>.py&quot;, line 119, in main
args.func(args)
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/commands/train.py&quot;, line 119, in train_model_from_args
file_friendly_logging=args.file_friendly_logging,
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/commands/train.py&quot;, line 178, in train_model_from_file
file_friendly_logging=file_friendly_logging,
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/commands/train.py&quot;, line 242, in train_model
file_friendly_logging=file_friendly_logging,
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/commands/train.py&quot;, line 466, in _train_worker
metrics = train_loop.run()
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/commands/train.py&quot;, line 528, in run
return self.trainer.train()
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/training/trainer.py&quot;, line 740, in train
metrics, epoch = self._try_train()
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/training/trainer.py&quot;, line 772, in _try_train
train_metrics = self._train_epoch(epoch)
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/training/trainer.py&quot;, line 523, in _train_epoch
loss.backward()
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/torch/tensor.py&quot;, line 245, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
File &quot;/home/ubuntu/anaconda3/envs/allennlp/lib/python3.7/site-packages/torch/autograd/<strong>init</strong>.py&quot;, line 147, in backward
allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 1.33 GiB (GPU 0; 14.76 GiB total capacity; 11.69 GiB already allocated; 639.75 MiB free; 13.09 GiB reserved in total by PyTorch)</p>
</blockquote>
","14169622","","","","","2021-04-22 21:46:18","Trying to train AllenNLP coreference resolution model on ontonotes: gets CUDA out of memory","<bert-language-model><allennlp><coreference-resolution>","1","1","","","","CC BY-SA 4.0"
"64076072","1","","","2020-09-26 09:38:18","","1","50","<p>I'm getting a recurrent problem on my Jupyter Notebook. It runs a NLP model training with no issues, but after invoking elasticsearch to visualize some data on a dedicated UI, with these command</p>
<pre><code>pl_trained.explore(valid_ds, explain=True)
</code></pre>
<p>andthese output:</p>
<pre><code>INFO:biome.text.ui.ui:Running biome UI on http://0.0.0.0:8080 with elasticsearch backend http://localhost:9200
</code></pre>
<p>The notebook sends the message that the kernel has died and it will restart automatically. Elasticsearch is being runned with a docker instance, and in the localhost 9200 everything seems to be right:</p>
<pre><code>    {
  &quot;name&quot; : &quot;f72a33c77fca&quot;,
  &quot;cluster_name&quot; : &quot;docker-cluster&quot;,
  &quot;cluster_uuid&quot; : &quot;Qac32YLsRe2Kbuqx72-aOQ&quot;,
  &quot;version&quot; : {
    &quot;number&quot; : &quot;7.7.1&quot;,
    &quot;build_flavor&quot; : &quot;default&quot;,
    &quot;build_type&quot; : &quot;docker&quot;,
    &quot;build_hash&quot; : &quot;ad56dce891c901a492bb1ee393f12dfff473a423&quot;,
    &quot;build_date&quot; : &quot;2020-05-28T16:30:01.040088Z&quot;,
    &quot;build_snapshot&quot; : false,
    &quot;lucene_version&quot; : &quot;8.5.1&quot;,
    &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;,
    &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot;
  },
  &quot;tagline&quot; : &quot;You Know, for Search&quot;
}
</code></pre>
<p>But localhost 8080 cannot be reached. Any ideas? I'm kinda new to docker, so I don't know if the problem is there or in other part of the code. The package I'm using, biome text, runs over AllenNLP, which is built on top of Pytorch.</p>
","14014925","","","","","2020-09-26 09:38:18","Jupyter Notebook kernel dies when it tries to invoke elasticsearch","<python><docker><elasticsearch><jupyter-notebook><allennlp>","0","0","","","","CC BY-SA 4.0"
"58846317","1","","","2019-11-13 22:03:22","","0","212","<p>Here is a step-by-step code to debug the error I'm getting: </p>

<pre><code>from nltk.tree import ParentedTree

teststr = 'a) Any service and handling fee imposed on customers by Used Motor Vehicle Dealers subject to these Rules: \r\n        1) shall be charged uniformly to all retail customers; \r\n        2) may not be presented as mandatory in writing, electronically, verbally, via American Sign Language, or via other media as mandatory; nor presented as mandatory or mandated by any entity, other than the Arkansas Used Motor Vehicle Dealer who or dealership which is legally permitted to invoice, charge and collect the service and handling fee established by these Rules; \r\n        3) must follow the procedures for disclosure set out by these Rules.'

#Using ALLENNLP's parser
from allennlp.predictors.predictor import Predictor
conspredictor = Predictor.from_path(""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz"")


treestr = conspredictor.predict(sentence=teststr)['trees']

ptree = ParentedTree.fromstring(treestr)
</code></pre>

<p>Here is the error I'm receiving with the traceback: </p>

<pre><code>&lt;ipython-input-391-f600cbe3ff5e&gt; in &lt;module&gt;

         10 treestr = conspredictor.predict(sentence=teststr)['trees']
         11 
    ---&gt; 12 ptree = ParentedTree.fromstring(treestr)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\tree.py in fromstring(cls, s, brackets, read_node, read_leaf, node_pattern, leaf_pattern, remove_empty_top_bracketing)

        616             if token[0] == open_b:
        617                 if len(stack) == 1 and len(stack[0][1]) &gt; 0:
    --&gt; 618                     cls._parse_error(s, match, 'end-of-string')
        619                 label = token[1:].lstrip()
        620                 if read_node is not None: label = read_node(label)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\tree.py in _parse_error(cls, s, match, expecting)

        677             offset = 13
        678         msg += '\n%s""%s""\n%s^' % (' '*16, s, ' '*(17+offset))
    --&gt; 679         raise ValueError(msg)
        680 
        681     #////////////////////////////////////////////////////////////

ValueError: ParentedTree.read(): expected 'end-of-string' but got '(:'
            at index 273.
                ""...es))))))) (: :) (S (...""
                              ^
</code></pre>
","12369418","","2214597","","2019-11-14 02:31:42","2019-11-14 02:31:42","ValueError: ParentedTree.read(): expected 'end-of-string' but got '(:'","<python><nlp><nltk><allennlp>","0","2","","","","CC BY-SA 4.0"
"55459643","1","","","2019-04-01 16:27:26","","2","1442","<p>I am trying to run allennlp package's prediction using a model which I have trained over my training data. However, at the beginning of the process, this exception comes up and halts the program. </p>

<blockquote>
  <p>raise source.error('bad escape %s' % escape, len(escape))
  sre_constants.error: bad escape \p at position 257</p>
</blockquote>

<p>Also, when I open an allennlp environment, I'm getting another error: </p>

<blockquote>
  <p>2019-04-01 12:20:43,879 - INFO - allennlp.common.params -
  dataset_reader.lazy = False Segmentation fault (core dumped)</p>
</blockquote>

<p>I'm doing a sequence-to-sequence task where the training data is a bunch of 0 and 1's (src and target) using the following command:</p>

<pre><code>CUDA_VISBILE_DEVICES=1 python -m allennlp.run train  
                       allennlp_config/simple_tagger.json 
                       --serialization-dir models/last
</code></pre>

<p>And in prediction:</p>

<pre><code>python -m allennlp.run predict models/new1/model.tar.gz 
                               data/test.src.txt 
                               --output bu-out.txt 
                               --cuda-device 0 
                               --batch-size 5
</code></pre>

<p>To be specific, I try re-producing the results in this repo: <a href=""https://github.com/sebastianGehrmann/bottom-up-summary"" rel=""nofollow noreferrer"">https://github.com/sebastianGehrmann/bottom-up-summary</a></p>
","5112804","","","","","2019-04-01 16:27:26","Exception: bad escape \p at position x","<python><neural-network><allennlp>","0","0","","","","CC BY-SA 4.0"
"66242860","1","66243368","","2021-02-17 13:27:23","","0","36","<p>Trying to use text classifier model shared by <a href=""https://github.com/allenai/scibert/blob/master/scibert/models/text_classifier.py"" rel=""nofollow noreferrer"">https://github.com/allenai/scibert/blob/master/scibert/models/text_classifier.py</a></p>
<p>Everything used to work and suddenly I keep getting this error: Cannot register text_classifier as Model; name already in use for TextClassifier</p>
<p>What might be the reason? any suggestion?</p>
<pre><code>    from typing import Dict, Optional, List, Any
    
    import torch
    import torch.nn.functional as F
    from allennlp.data import Vocabulary
    from allennlp.models.model import Model
    from allennlp.modules import FeedForward, TextFieldEmbedder, Seq2SeqEncoder
    from allennlp.nn import InitializerApplicator, RegularizerApplicator
    from allennlp.nn import util
    from allennlp.training.metrics import CategoricalAccuracy, F1Measure
    from overrides import overrides
    
    
    @Model.register(&quot;text_classifier&quot;)
    class TextClassifier(Model):
        &quot;&quot;&quot;
        Implements a basic text classifier:
        1) Embed tokens using `text_field_embedder`
        2) Seq2SeqEncoder, e.g. BiLSTM
        3) Append the first and last encoder states
        4) Final feedforward layer
        Optimized with CrossEntropyLoss.  Evaluated with CategoricalAccuracy &amp; F1.
        &quot;&quot;&quot;
    def __init__(self, vocab: Vocabulary,
                 text_field_embedder: TextFieldEmbedder,
                 text_encoder: Seq2SeqEncoder,
                 classifier_feedforward: FeedForward,
                 verbose_metrics: False,
                 initializer: InitializerApplicator = InitializerApplicator(),
                 regularizer: Optional[RegularizerApplicator] = None,
                 ) -&gt; None:
        super(TextClassifier, self).__init__(vocab, regularizer)

        self.text_field_embedder = text_field_embedder
        self.num_classes = self.vocab.get_vocab_size(&quot;labels&quot;)
        self.text_encoder = text_encoder
        self.classifier_feedforward = classifier_feedforward
        self.prediction_layer = torch.nn.Linear(self.classifier_feedforward.get_output_dim()  , self.num_classes)

        self.label_accuracy = CategoricalAccuracy()
        self.label_f1_metrics = {}

        self.verbose_metrics = verbose_metrics

        for i in range(self.num_classes):
            self.label_f1_metrics[vocab.get_token_from_index(index=i, namespace=&quot;labels&quot;)] = F1Measure(positive_label=i)
        self.loss = torch.nn.CrossEntropyLoss()

        self.pool = lambda text, mask: util.get_final_encoder_states(text, mask, bidirectional=True)

        initializer(self)

    @overrides
    def forward(self,
                text: Dict[str, torch.LongTensor],
                label: torch.IntTensor = None,
                metadata:  List[Dict[str, Any]] = None) -&gt; Dict[str, torch.Tensor]:
        &quot;&quot;&quot;
        Parameters
        ----------
        text : Dict[str, torch.LongTensor]
            From a ``TextField``
        label : torch.IntTensor, optional (default = None)
            From a ``LabelField``
        metadata : ``List[Dict[str, Any]]``, optional, (default = None)
            Metadata containing the original tokenization of the premise and
            hypothesis with 'premise_tokens' and 'hypothesis_tokens' keys respectively.
        Returns
        -------
        An output dictionary consisting of:
        label_logits : torch.FloatTensor
            A tensor of shape ``(batch_size, num_labels)`` representing unnormalised log probabilities of the label.
        label_probs : torch.FloatTensor
            A tensor of shape ``(batch_size, num_labels)`` representing probabilities of the label.
        loss : torch.FloatTensor, optional
            A scalar loss to be optimised.
        &quot;&quot;&quot;
        embedded_text = self.text_field_embedder(text)

        mask = util.get_text_field_mask(text)
        encoded_text = self.text_encoder(embedded_text, mask)
        pooled = self.pool(encoded_text, mask)
        ff_hidden = self.classifier_feedforward(pooled)
        logits = self.prediction_layer(ff_hidden)
        class_probs = F.softmax(logits, dim=1)

        output_dict = {&quot;logits&quot;: logits}
        if label is not None:
            loss = self.loss(logits, label)
            output_dict[&quot;loss&quot;] = loss

            # compute F1 per label
            for i in range(self.num_classes):
                metric = self.label_f1_metrics[self.vocab.get_token_from_index(index=i, namespace=&quot;labels&quot;)]
                metric(class_probs, label)
            self.label_accuracy(logits, label)
        return output_dict

   #@overrides
    def decode(self, output_dict: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:
        class_probabilities = F.softmax(output_dict['logits'], dim=-1)
        output_dict['class_probs'] = class_probabilities
        return output_dict

    def get_metrics(self, reset: bool = False) -&gt; Dict[str, float]:
        metric_dict = {}

        sum_f1 = 0.0
        for name, metric in self.label_f1_metrics.items():
            metric_val = metric.get_metric(reset)
            if self.verbose_metrics:
                metric_dict[name + '_P'] = metric_val[0]
                metric_dict[name + '_R'] = metric_val[1]
                metric_dict[name + '_F1'] = metric_val[2]
            sum_f1 += metric_val[2]

        names = list(self.label_f1_metrics.keys())
        total_len = len(names)
        average_f1 = sum_f1 / total_len
        metric_dict['average_F1'] = average_f1
        metric_dict['accuracy'] = self.label_accuracy.get_metric(reset)
        return metric_dict
</code></pre>
","12302635","","","","","2021-02-17 13:55:55","Cannot register text_classifier as Model; name already in use for TextClassifier","<python><bert-language-model><allennlp>","1","0","","","","CC BY-SA 4.0"
"65807492","1","65849429","","2021-01-20 10:18:39","","0","27","<p>I´m using AllenNLP for a combined classifier (one of its task is NER tagging), and while designing some tests, I've come across this doubt: how do I access the values of the different fields of my AllenNLP instance? I want to compared them with the values introduced, to make sure nothing got lost in the preprocessing.</p>
<p>I managed to get to the text field of the instance using <code>instance.__getitem__(&quot;text&quot;)</code>, but I dont know how to get the value from there, or if there is anything quicker directly from the Instance class.</p>
<p>What I'm trying to do is something like <code>assert instance.getValueFromField(&quot;text) == training_dataset[&quot;text&quot;][0]</code></p>
","14014925","","","","","2021-01-22 16:55:06","Access field values of an instance in AllenNLP","<python><machine-learning><nlp><allennlp>","1","0","","","","CC BY-SA 4.0"
"66903506","1","","","2021-04-01 11:14:23","","0","35","<p>How can i generate AllenNLP model and integrate it with Microsoft Botframework(Web-Chat)? My bot code is developed in C#.</p>
","11840119","","","","","2021-04-01 11:14:23","Integrate AllenNLP with microsoft botframework","<model><botframework><integration><allennlp>","0","1","","","","CC BY-SA 4.0"
"58659751","1","","","2019-11-01 13:08:06","","0","131","<p>I am trying to use Allennlp library to perform NER. The library works perfectly fine with conll2003 and other databases which only have entities and tokens (I had to update _read function for the same).
But the function returns ""ValueError: not enough values to unpack (expected 2, got 1)"" if I try to use my own dataset. I have compared the formatting, special characters, spacing, and even file names but couldn't find any issue.
This is the sample from the dataset which worked,</p>

<pre><code>O   show
O   me
O   films
O   with
B-ACTOR drew
I-ACTOR barrymore
O   from
O   the
B-YEAR  1980s

O   what
O   movies
O   starred
O   both
B-ACTOR al
I-ACTOR pacino
</code></pre>

<p>This is the sample from my dataset which is not working,</p>

<pre><code>O   dated
O   as
O   of
B-STARTDATE February
I-STARTDATE 9
I-STARTDATE ,
L-STARTDATE 2017
O   by
O   and
O   between
O   Allenware
O   Ltd
</code></pre>

<p>I am not able to identify the issue, please help.</p>

<p><strong>Update</strong></p>

<p>adding stderr.log as requested.</p>

<pre><code>0it [00:00, ?it/s]
1it [00:00, 556.72it/s]

0it [00:00, ?it/s]
Traceback (most recent call last):
  File ""/allennlp/bin/allennlp"", line 8, in &lt;module&gt;
    sys.exit(run())
  File ""/allennlp/lib/python3.6/site-packages/allennlp/run.py"", line 18, in run
    main(prog=""allennlp"")
  File ""/allennlp/lib/python3.6/site-packages/allennlp/commands/__init__.py"", line 102, in main
    args.func(args)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/commands/train.py"", line 124, in train_model_from_args
    args.cache_prefix)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/commands/train.py"", line 168, in train_model_from_file
    cache_directory, cache_prefix)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/commands/train.py"", line 226, in train_model
    cache_prefix)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/training/trainer_pieces.py"", line 42, in from_params
    all_datasets = training_util.datasets_from_params(params, cache_directory, cache_prefix)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/training/util.py"", line 185, in datasets_from_params
    validation_data = validation_and_test_dataset_reader.read(validation_data_path)
  File ""/allennlp/lib/python3.6/site-packages/allennlp/data/dataset_readers/dataset_reader.py"", line 134, in read
    instances = [instance for instance in Tqdm.tqdm(instances)]
  File ""/allennlp/lib/python3.6/site-packages/allennlp/data/dataset_readers/dataset_reader.py"", line 134, in &lt;listcomp&gt;
    instances = [instance for instance in Tqdm.tqdm(instances)]
  File ""/allennlp/lib/python3.6/site-packages/tqdm/std.py"", line 1081, in __iter__
    for obj in iterable:
  File ""/allennlp/lib/python3.6/site-packages/allennlp/data/dataset_readers/conll2003.py"", line 119, in _read
    ner_tags,tokens_ = fields
ValueError: not enough values to unpack (expected 2, got 1)
0it [00:00, ?it/s]
</code></pre>

<p>Adding _read and text_to_instance functions</p>

<pre><code>@overrides
    def _read(self, file_path: str) -&gt; Iterable[Instance]:
        # if `file_path` is a URL, redirect to the cache
        file_path = cached_path(file_path)

        with open(file_path, ""r"") as data_file:
            logger.info(""Reading instances from lines in file at: %s"", file_path)

            # Group into alternative divider / sentence chunks.
            for is_divider, lines in itertools.groupby(data_file, _is_divider):
                # Ignore the divider chunks, so that `lines` corresponds to the words
                # of a single sentence.
                if not is_divider:
                    fields = [line.strip().split() for line in lines]
                    # unzipping trick returns tuples, but our Fields need lists
                    fields = [list(field) for field in zip(*fields)]
                    ner_tags,tokens_ = fields
                    # TextField requires ``Token`` objects
                    tokens = [Token(token) for token in tokens_]

                    yield self.text_to_instance(tokens,ner_tags)

    def text_to_instance(  # type: ignore
        self,
        tokens: List[Token],
        ner_tags: List[str] = None,
    ) -&gt; Instance:
        """"""
        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.
        """"""

        sequence = TextField(tokens, self._token_indexers)
        instance_fields: Dict[str, Field] = {""tokens"": sequence}
        instance_fields[""metadata""] = MetadataField({""words"": [x.text for x in tokens]})
        coded_ner=ner_tags
        if 'ner' in self.feature_labels:
            if coded_ner is None:
                raise ConfigurationError(""Dataset reader was specified to use NER tags as ""
                                         "" features. Pass them to text_to_instance."")
            instance_fields['ner_tags'] = SequenceLabelField(coded_ner, sequence, ""ner_tags"")
        if self.tag_label == 'ner' and coded_ner is not None:
            instance_fields['tags'] = SequenceLabelField(coded_ner, sequence,self.label_namespace)
        return Instance(instance_fields)
</code></pre>
","4353762","","4353762","","2019-11-01 13:38:48","2019-11-01 13:38:48","""No enough value to unpack"" while loading the dataset- Allennlp _read","<python><machine-learning><nlp><allennlp>","0","5","","","","CC BY-SA 4.0"
"67306126","1","","","2021-04-28 18:42:34","","0","112","<p>Currently training models using AllenNLP 1.2:</p>
<pre><code>allennlp train -f --include-package custom-exp /usr/training_config/mock_model_config.jsonnet -s test-mock-out
</code></pre>
<p>The config is very standard:</p>
<pre><code>&quot;dataset_reader&quot; : {

        &quot;reader&quot;: &quot;params&quot;

    },
    &quot;data_loader&quot;: {
        &quot;batch_size&quot;: 3,
        &quot;num_workers&quot;: 1,
    },
    &quot;trainer&quot;: {
        &quot;trainer_params&quot;: &quot;various&quot;
    },


    &quot;vocabulary&quot;: {
        &quot;type&quot;: &quot;from_files&quot;,
        &quot;directory&quot;: vocab_folder,
        &quot;oov_token&quot;: &quot;[UNK]&quot;,
        &quot;padding_token&quot;: &quot;[PAD]&quot;,
    },

    &quot;model&quot;: {
        &quot;various params&quot;: ...
    }
</code></pre>
<p>and serializing them to  the <code>test-mock-out</code> directory (also have <code>model.tar.gz</code>).</p>
<p>Using the <code>allennlp train</code> command, is it possible to continue training? The documentation states <code>Model.from_archive</code> should be used, but it's unclear how the config should be adapted to use it.</p>
<p><a href=""http://docs.allennlp.org/v1.2.0/api/commands/train/"" rel=""nofollow noreferrer"">http://docs.allennlp.org/v1.2.0/api/commands/train/</a></p>
","6395770","","","","","2021-05-01 02:26:39","How to continue training serialized AllenNLP model using `allennlp train`?","<machine-learning><pytorch><transfer-learning><allennlp>","1","4","","","","CC BY-SA 4.0"
"66715946","1","","","2021-03-19 21:42:50","","0","29","<p>I am trying to get the saliency score for sentiment analysis task. Every time I run the code I get different saliency scores. Should this be the case? I am attaching my code for more reference.</p>
<pre><code>from allennlp.predictors.predictor import Predictor
import nltk
from allennlp.interpret.saliency_interpreters import SmoothGradient


data = &quot;purchase costume year old grandson halloween arrive one week earlier expect happy grandson absolutely love glad order larger size size barely fit material durable well make think wear many time play since halloween happy purchase worth dollars spend&quot;

words = nltk.word_tokenize(data)

predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/stanford-sentiment-treebank-roberta.2021-03-11.tar.gz&quot;)
predicted = predictor.predict(words)
saliency_scores = SmoothGradient(predictor).saliency_interpret_from_json({'sentence':words})
</code></pre>
<p>Every time I print saliency scores for the same data the values keep changing. Also the tokens that the model generates are distorted, for example halloween breaks into hall, ow and een. How can I fix this? Any help would be appreciated.</p>
","15436048","","","","","2021-04-05 20:08:23","Saliency score keeps changing allennlp","<nlp><sentiment-analysis><allennlp>","1","1","","","","CC BY-SA 4.0"
"66783059","1","66842020","","2021-03-24 14:21:12","","1","89","<p>I have created a simple Question Answering application with AllenNlp. It used to run smoothly with no warnings, but now, for every token in the passage, this spacy warning is printed in the console:</p>
<p>[WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'X'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.</p>
<p>The system versions are:
allenlp==2.1.0
spacy==3.0.5
Can anyone please help?</p>
<pre><code>
class PythonPredictor:
    def __init__(self):
        self.predictor = Predictor.from_path(
            &quot;https://storage.googleapis.com/allennlp-public-models/bidaf-elmo-model-2020.03.19.tar.gz&quot;)

    def predict(self, passage, question):
        prediction = self.predictor.predict(
            passage=passage, question=question
        )
        return prediction[&quot;best_span_str&quot;]
</code></pre>
","9464950","","9464950","","2021-03-28 13:35:29","2021-03-28 13:36:30","AllenNlp QA Application produces Spacy warning: [WARNING] [W108] for every word in the document","<python><spacy><allennlp>","2","0","","","","CC BY-SA 4.0"
"67306841","1","67309083","","2021-04-28 19:37:22","","0","79","<p>I would like to use bert for tokenization and also indexing for a seq2seq model and this is how my config file looks like so far:</p>
<pre><code>{
&quot;dataset_reader&quot;: {
    &quot;type&quot;: &quot;seq2seq&quot;,
    &quot;end_symbol&quot;: &quot;[SEP]&quot;,
    &quot;quoting&quot;: 3,
    &quot;source_token_indexers&quot;: {
        &quot;tokens&quot;: {
            &quot;type&quot;: &quot;pretrained_transformer&quot;,
            &quot;model_name&quot;: &quot;bert-base-german-cased&quot;
        }
    },
    &quot;source_tokenizer&quot;: {
        &quot;type&quot;: &quot;pretrained_transformer&quot;,
        &quot;model_name&quot;: &quot;bert-base-german-cased&quot;
    },
    &quot;start_symbol&quot;: &quot;[CLS]&quot;,
    &quot;target_token_indexers&quot;: {
        &quot;tokens&quot;: {
            &quot;namespace&quot;: &quot;tokens&quot;
        }
    },
    &quot;target_tokenizer&quot;: {
        &quot;type&quot;: &quot;pretrained_transformer&quot;,
        &quot;add_special_tokens&quot;: true,
        &quot;model_name&quot;: &quot;bert-base-german-cased&quot;
    }
},
</code></pre>
<p>and later when I load the model and use <code>predictor.predict_json</code> to predict sentences, the output looks like this.</p>
<blockquote>
<p>'predicted_tokens': ['[CLS]', 'Die', 'meisten', 'Universitäts',
'##abs', '##ch', '##lüsse', 'sind', 'nicht', 'p', '##raxis',
'##orient', '##iert', 'und', 'bereit', '##en', 'die', 'Studenten',
'nicht', 'auf', 'die', 'wirklich', '##e', 'Welt', 'vor', '.', '[SEP]',
'[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]',
'[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']</p>
</blockquote>
<p>I have 2 questions:</p>
<ol>
<li>is this a normal output (considering all the '[SEP]' tokens in the end)? or am I doing something wrong in the config file?</li>
<li>is there any function that would convert these tokens to a human-readable sentence?</li>
</ol>
<p>Thanks</p>
","15787679","","15787679","","2021-04-28 19:42:26","2021-04-29 17:28:40","AllenNLP - dataset_reader config for transformers","<allennlp>","1","0","","","","CC BY-SA 4.0"
"67450113","1","","","2021-05-08 17:01:49","","0","17","<p>Does GradientDescentTrainer update model at the end of each epoch ? or does it update once after a forward pass over a batch?</p>
","5081054","","","","","2021-05-14 16:45:24","How often does allennlp's GradientDescentTrainer update a model?","<allennlp>","1","0","","","","CC BY-SA 4.0"
"51860628","1","51866815","","2018-08-15 14:26:16","","5","6912","<p>I was trying to install a library (<code>allennlp</code>) via <code>pip3</code>. But it complained about the PyTorch version. While <code>allennlp</code> requires <code>torch=0.4.0</code> I have <code>torch=0.4.1</code>:</p>

<pre><code>...
Collecting torch==0.4.0 (from allennlp)
  Could not find a version that satisfies the requirement torch==0.4.0 (from allennlp) (from versions: 0.1.2, 0.1.2.post1, 0.4.1)
No matching distribution found for torch==0.4.0 (from allennlp)
</code></pre>

<p><em>Also manually install:</em></p>

<pre><code>pip3 install torch==0.4.0
</code></pre>

<p><em>Doesn't work either:</em></p>

<pre><code>  Could not find a version that satisfies the requirement torch==0.4.0 (from versions: 0.1.2, 0.1.2.post1, 0.4.1)
No matching distribution found for torch==0.4.0
</code></pre>

<p>Same for other versions.</p>

<p>Python is version <code>Python 3.7.0</code> installed via <code>brew</code> on Mac OS.</p>

<p>I remember that some time ago I was able to switch between version <code>0.4.0</code> and <code>0.3.1</code> by using <code>pip3 install torch==0.X.X</code>.</p>

<p>How do I solve this?</p>
","7483494","","7483494","","2018-08-15 21:27:04","2019-03-07 02:21:40","pip - Installing specific package version does not work","<python-3.x><pip><homebrew><pytorch><allennlp>","1","0","1","","","CC BY-SA 4.0"
"67207658","1","","","2021-04-22 06:10:34","","0","53","<p>I have created a virtual ubuntu machine and installed ALLENNLP,
In that and tried example from <a href=""https://demo.allennlp.org/sentiment-analysis/roberta-sentiment-analysis"" rel=""nofollow noreferrer"">ALLENNLP demo website</a></p>
<p>I have executed below code</p>
<pre><code>from allennlp.predictors.predictor import Predictor
import allennlp_models.tagging

predictor = Predictor.from_path(&quot;myLocalPath/stanford-sentiment-treebank-roberta.2021-03-11.tar.gz&quot;)
predictor.predict(&quot;a very well-made, funny and entertaining picture.&quot;)
</code></pre>
<p>which gave me below error.</p>
<pre><code>&gt;&gt;&gt; predictor.predict(&quot;a very well-made, funny and entertaining picture.&quot;)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/.local/lib/python3.8/site-packages/allennlp/predictors/text_classifier.py&quot;, line 24, in predict
    return self.predict_json({&quot;sentence&quot;: sentence})
  File &quot;/home/.local/lib/python3.8/site-packages/allennlp/predictors/predictor.py&quot;, line 54, in predict_json
    instance = self._json_to_instance(inputs)
  File &quot;/home/.local/lib/python3.8/site-packages/allennlp/predictors/text_classifier.py&quot;, line 40, in _json_to_instance
    return self._dataset_reader.text_to_instance(sentence)
  File &quot;/home/.local/lib/python3.8/site-packages/allennlp_models/classification/dataset_readers/stanford_sentiment_tree_bank.py&quot;, line 114, in text_to_instance
    assert isinstance(
AssertionError
</code></pre>
<p>But when I executed below code</p>
<pre><code>from allennlp.predictors.predictor import Predictor
import allennlp_models.tagging

predictor = Predictor.from_path(&quot;myLocalPath/sst-roberta-large-2020.06.08.tar.gz&quot;)
predictor.predict(&quot;a very well-made, funny and entertaining picture.&quot;)
</code></pre>
<p>It worked.</p>
<p>Only difference between above two code is version of <strong>roberta-large</strong> train data</p>
<p>I have installed latest version of ALLENNLP in my virtual machine
I don't have NVIDIA graphic card in my virtual machine could this be a reason?
But then how come other version is working?</p>
<p>Please help</p>
","1797678","","1797678","","2021-04-26 05:38:21","2021-04-28 06:52:00","Assertion Error while using stanford-sentiment-treebank-roberta.2021-03-11.tar.gz in ALLENNLP","<allennlp>","1","8","","","","CC BY-SA 4.0"
"64182371","1","","","2020-10-03 08:47:39","","0","30","<p>In the official <a href=""https://github.com/allenai/allennlp-guide-examples/blob/master/quick_start/my_text_classifier/models/simple_classifier.py"" rel=""nofollow noreferrer"">example</a>, both metrics and loss function are hard coded. I am wondering if we can pass those in the config jsonnet, so I can reuse my model in different datasets with different metrics.</p>
","7883622","","","","","2020-10-09 01:50:08","pass loss function and metrics in config","<allennlp>","1","1","","","","CC BY-SA 4.0"
"64215072","1","","","2020-10-05 19:34:44","","2","62","<p>I noticed that after I pulled version 1.0.0, I was no longer able to import allennlp.service. I see the docs are still updated only as far as 0.9.0 as well so I'm not sure what the status of the service module is.</p>
","14087373","","","","","2020-10-09 19:38:33","Is allennlp.service being deprecated?","<allennlp>","1","1","","","","CC BY-SA 4.0"
"67418530","1","","","2021-05-06 12:51:32","","0","13","<p>We're seeing strange behavior with Reading Comprehension. Given the following people (names redacted) and their roles and contact numbers:</p>
<p><a href=""https://i.stack.imgur.com/ouaII.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ouaII.png"" alt=""enter image description here"" /></a></p>
<p>Asking RC for PersonAfname's number results in 4444, which is correct. It also correctly returns 3333 for PersonBfname's number. But when we ask for PersonCfname's number, it returns 4444 instead of 5555. What's up?</p>
<p><a href=""https://demo.allennlp.org/reading-comprehension/bidaf-elmo/s/what-is-perscfnames-number/Z8Z6J4A9D9"" rel=""nofollow noreferrer"">https://demo.allennlp.org/reading-comprehension/bidaf-elmo/s/what-is-perscfnames-number/Z8Z6J4A9D9</a></p>
","2246766","","","","","2021-05-07 17:17:46","Inconsistent results with AllenNLP Reading Comprehension","<allennlp>","1","2","","","","CC BY-SA 4.0"
"64542462","1","","","2020-10-26 17:56:04","","1","61","<p>I use the seq2seq model and it can compute BLEU score (a NMT score) every epoch. However, I cannot set BLEU score as validation metric so it cannot early stop in training. I read the source code, but there are no hints as to what kind of string could be added to the validation metrics except for &quot;+loss&quot;. Please save me, thanks!</p>
","7656639","","","","","2020-10-26 19:36:21","Is there any other validation metrics for trainer?","<allennlp>","1","1","","","","CC BY-SA 4.0"
"57925256","1","57925788","","2019-09-13 14:10:26","","0","42","<p>Good day, I am a student that is interested in NLP. I have come across the demo on  <a href=""https://demo.allennlp.org/sentiment-analysis"" rel=""nofollow noreferrer"">AllenNLP's homepage</a>, which stated that:</p>

<blockquote>
  <p>The model is a simple LSTM using GloVe embeddings that is trained on the binary classification setting of the Stanford Sentiment Treebank. It achieves about 87% accuracy on the test set.</p>
</blockquote>

<p>Is there any reference to the sample code or any tutorial that I can follow to replicate this result, so that I can learn more about this subject? I am trying to obtain a Regression Output (Instead of classification). </p>

<p>I hope that someone can point me in the right direction.. Any help is much appreciated. Thank you!</p>
","7263895","","","","","2019-09-13 14:44:21","Replicating Semantic Analysis Model in Demo","<pytorch><semantic-analysis><allennlp>","1","0","","","","CC BY-SA 4.0"
"66781711","1","","","2021-03-24 13:07:04","","-1","78","<p>I am beginner in understanding Allennlp framework.
I tried the code given in medium post <a href=""https://medium.com/analytics-vidhya/fine-tuning-bert-with-allennlp-7459119b736c"" rel=""nofollow noreferrer"">https://medium.com/analytics-vidhya/fine-tuning-bert-with-allennlp-7459119b736c</a>.</p>
<p>But, got an ImportError: cannot import name 'SnliReader' from 'allennlp.data.dataset_readers'</p>
<p>Does any one know why this error is showing?</p>
","13221609","","","","","2021-03-26 10:56:21","ImportError: cannot import name 'SnliReader' from 'allennlp.data.dataset_readers'","<importerror><allennlp>","1","0","","","","CC BY-SA 4.0"
"65224656","1","","","2020-12-09 20:47:48","","1","34","<p>I'm using AllenNLP for SRL, but the output doesn't provide role information / frame info / lemmas for the verb. All of that would be helpful.</p>
<p>I see that this information seems to exist here: <a href=""https://github.com/allenai/allennlp-models/blob/ef5d4229f62f3ea8f44345d43b6a7fd1ab2d09fa/allennlp_models/common/ontonotes.py#L255"" rel=""nofollow noreferrer"">on the models github</a>, but I'm not sure if it's accessible for use with the latest/best SRL models that I believe use BERT.</p>
<p>Would it be possible to get the sense, frame, and lemma information alongside the SRL parse, using the state-of-the-art? If so, how is it done?</p>
","7361580","","","","","2020-12-09 20:47:48","AllenNLP SRL: Is it possible to get sense info for a lemma?","<nlp><allennlp>","0","0","1","","","CC BY-SA 4.0"
"64181843","1","64214308","","2020-10-03 07:38:57","","0","95","<p><a href=""https://github.com/allenai/allennlp/blob/17c3b84b3233d118796e0d492fd558f00f336fb6/allennlp/training/trainer.py#L499"" rel=""nofollow noreferrer"">in train_epoch function</a></p>
<p>we have three kinds of losses</p>
<ol>
<li>loss</li>
<li>batch_loss</li>
<li>train_loss</li>
</ol>
<p>as I understand loss is a tensor, batch loss is the value of the tensor , train_loss is the accumulative value of the batch_loss this is ok for me.</p>
<p>my question is why AllenNLP considered the batch_loss in for batch and did not calculate the cumulative loss for batch_group?</p>
<p>Also I did not understand the need for batch_group inside epoch, and batch inside batch_group</p>
<p>this is my understanding
we have epoch inside it we have batch_group inside batch_group we have batch the batch_loss is calculated for batch not for batch_group why?</p>
","14494113","","","","","2020-10-05 23:34:32","Calculate loss in train epoch function","<allennlp>","1","1","0","","","CC BY-SA 4.0"
"64179016","1","","","2020-10-02 22:39:07","","0","70","<p>Sometimes we may want some minor different behaviors during validation (i.e, validation on dev set during training) and final evaluation. I am not sure whether there is a simple way to inform our model whether it's called by a <code>train</code> command or a <code>evaluate</code> command. Now what I can do is just to use <code>self.training</code> to distinguish them from training, but both validation and evaluation have <code>self.training == False</code>, which makes it not possible for me to distinguish them. I mean if it's my own pytorch framework, that's extremely easy to do, since I can write my own train and evaluate methods, but in AllenNLP they are both part of the framework code that I don't really want to modify. Is there an easy way to do it?</p>
","5511906","","","","","2020-10-09 01:37:53","Is there a way to differentiate validation and evaluation in AllenNLP?","<allennlp>","1","1","","","","CC BY-SA 4.0"
"67301681","1","67308927","","2021-04-28 13:57:15","","-1","58","<p>Is there any current support for other languages? For example, Spanish.
If not, is it planned?
Or else, what would be the way to add it?</p>
","14675984","","7508700","","2021-05-02 17:34:37","2021-05-02 17:34:37","AllenNLP - Support for different languages","<nlp><allennlp>","1","2","","2021-05-02 18:09:55","","CC BY-SA 4.0"
"67418277","1","","","2021-05-06 12:36:25","","0","22","<p>Last October (2020) we ran some data through the Semantic Role function. With the sentence &quot;John would like a comprehensive map of Ohio.&quot; the word comprehensive was tagged as a modifier. Now, however, it seems like none of the adjectives in our data get tagged as modifiers.
<a href=""https://i.stack.imgur.com/WZtWD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WZtWD.png"" alt=""SR Results on May 6th 2021"" /></a>
<a href=""https://demo.allennlp.org/semantic-role-labeling/s/john-would-like-a-ohio/F8U2O6P5I3"" rel=""nofollow noreferrer"">https://demo.allennlp.org/semantic-role-labeling/s/john-would-like-a-ohio/F8U2O6P5I3</a></p>
<p>What's changed? Will the previous SemRole capability be restored? Do we have to resort to Dependency Parsing?</p>
","2246766","","","","","2021-05-06 22:38:00","Semantic Role no longer tagging adjectives as Modifiers","<allennlp>","1","0","","","","CC BY-SA 4.0"
"67203417","1","","","2021-04-21 20:59:54","","0","180","<p>I would like to use AllenNLP Interpret (code + demo) with a PyTorch classification model trained with HuggingFace (electra base discriminator). Yet, it is not obvious to me, how I can convert my model, and use it in a local allen-nlp demo server.</p>
<p>How should I proceed ?</p>
<p>Thanks in advance</p>
","15727016","","","","","2021-04-22 23:23:30","Using AllenNLP Interpret with a HuggingFace model","<nlp><huggingface-transformers><allennlp>","1","0","","","","CC BY-SA 4.0"
"66655634","1","66712363","","2021-03-16 12:54:16","","0","29","<p>I am using allennlp 2.1 and I would like to pass class weights to the pytorch-cross-entropy loss function that I use.</p>
<pre class=""lang-py prettyprint-override""><code>@Head.register('model_head_two_layers')
class ModelHeadTwoLayers(Head):

    default_predictor = 'head_predictor'

    def __init__(self, vocab: Vocabulary, input_dim: int, output_dim: int, dropout: float = 0.0,
                 class_weights: Union[List[float], None] = None):
        super().__init__(vocab=vocab)
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.layers = torch.nn.Sequential(
            torch.nn.Dropout(dropout),
            torch.nn.Linear(self.input_dim, self.input_dim),
            torch.nn.ReLU(inplace=True),
            torch.nn.Linear(self.input_dim, output_dim)
        )
        self.metrics = {
            'accuracy': CategoricalAccuracy(),
            'f1_macro': FBetaMeasure(average='macro')
        }
        if class_weights:
            self.class_weights = torch.FloatTensor(class_weights)
            self.cross_ent = torch.nn.CrossEntropyLoss(weight=self.class_weights)
        else:
            self.cross_ent = torch.nn.CrossEntropyLoss()
</code></pre>
<p>In the configuration file I pass the class weights as follows:</p>
<pre class=""lang-json prettyprint-override""><code>&quot;heads&quot;: {
            &quot;task_name&quot;: {
                &quot;type&quot;: &quot;model_head_two_layers&quot;,
                &quot;input_dim&quot;: embedding_dim,
                &quot;output_dim&quot;: 4,
                &quot;dropout&quot;: dropout,
                &quot;class_weights&quot;: [0.25, 0.90, 0.91, 0.94]
            }
        }
</code></pre>
<p>In order for the class weights to be in the correct order I need to know which index of the output tensor corresponds to which class. The only way to find that out, that I know of until now, is to first train a model without class weights and then go into the vocabulary directory of the model and check in which order the class names are written into the labels-file.</p>
<p>While that seems to work...is there an easier way to get that mapping without having to train a model first?</p>
","5800213","","","","","2021-03-19 16:51:25","AllenNLP: How to know which index of the output-tensor corresponds to which class","<allennlp>","1","0","","","","CC BY-SA 4.0"