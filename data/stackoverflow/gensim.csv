Id,PostTypeId,AcceptedAnswerId,ParentId,CreationDate,DeletionDate,Score,ViewCount,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditorDisplayName,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate,CommunityOwnedDate,ContentLicense
"60445602","1","","","2020-02-28 05:11:29","","1","199","<p>I have a use case where I want to find top n nearest words from a given set of words, to a vector. </p>

<p>Its like <code>similar_by_vector</code> where I want to restrict my vocab to a given set of words.</p>

<pre><code>similar_by_vector(vector, topn, vocab=[x,y,z...])
</code></pre>

<p>I want to create a low latency api using this, where vocab can be different for each request.
Any suggestions to how I can achieve this optimally?</p>
","10907314","","","","","2020-02-28 05:11:29","How to get top n similar words from given list of words to a vector using gensim?","<java><python><gensim><word2vec><cosine-similarity>","0","4","","","","CC BY-SA 4.0"
"60459403","1","","","2020-02-28 21:22:00","","0","174","<p>My coworker and I have the exact same code, using the same libraries, but yet his code works and mine doesn't. We've gotten stuck trying to figure out what is wrong. Any help would be greatly appreciated. The code and error are below. </p>

<p><strong>Code:</strong></p>

<pre><code>import os
os.environ.update({'MALLET_HOME':r'C:...\\mallet-2.0.8/'})
mallet_path = 'C:...\\mallet-2.0.8\\bin\\mallet' 
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)
</code></pre>

<p><strong>Output and Error:</strong></p>

<pre><code>---------------------------------------------------------------------------
CalledProcessError                        Traceback (most recent call last)
&lt;ipython-input-79-6122457c60e1&gt; in &lt;module&gt;
----&gt; 1 ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py in __init__(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)
    129         self.random_seed = random_seed
    130         if corpus is not None:
--&gt; 131             self.train(corpus)
    132 
    133     def finferencer(self):

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py in train(self, corpus)
    270 
    271         """"""
--&gt; 272         self.convert_input(corpus, infer=False)
    273         cmd = self.mallet_path + ' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\
    274             '--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py in convert_input(self, corpus, infer, serialize_corpus)
    259             cmd = cmd % (self.fcorpustxt(), self.fcorpusmallet())
    260         logger.info(""converting temporary corpus to MALLET format with %s"", cmd)
--&gt; 261         check_output(args=cmd, shell=True)
    262 
    263     def train(self, corpus):

C:\ProgramData\Anaconda3\lib\site-packages\gensim\utils.py in check_output(stdout, *popenargs, **kwargs)
   1916             error = subprocess.CalledProcessError(retcode, cmd)
   1917             error.output = output
-&gt; 1918             raise error
   1919         return output
   1920     except KeyboardInterrupt:

CalledProcessError: Command 'C:\mallet-2.0.8\bin\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\CST~1.JEO\AppData\Local\Temp\84f7e0_corpus.txt --output C:\Users\CST~1.JEO\AppData\Local\Temp\84f7e0_corpus.mallet' returned non-zero exit status 1.
</code></pre>
","12981710","","","","","2020-02-28 21:22:00","Return nonzero for LdaMallet","<gensim><lda><mallet>","0","3","","","","CC BY-SA 4.0"
"60464982","1","","","2020-02-29 12:05:54","","0","116","<pre><code>from gensim.test.utils import datapath
from gensim import utils

class MyCorpus(object):
    """"""An interator that yields sentences (lists of str).""""""

    def __iter__(self):
        corpus_path = datapath('lee_background.cor')
        i = 1
        print(str(i))
        for line in open(corpus_path):
            # assume there's one document per line, tokens separated by whitespace

            yield utils.simple_preprocess(line)

import gensim.models

sentences = MyCorpus()
model = gensim.models.Word2Vec(sentences=sentences, iter=1)
</code></pre>

<p>This is the genism's documentation code from <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html</a>.</p>

<p>I have 2 questions regarding the iter parameter:</p>

<p>1) when it is set to 1, why is the print(str(i)) executed twice?</p>

<p>2) when the ""iter=10"", the 'simple_preprocess' is executed 11 times. If my own customized 'preprocess' is very heavy, is this going to be very slow? How to avoid this preprocessing repetitions in using genism word2vec?</p>
","2079445","","","","","2020-02-29 19:24:06","How to understand the gensim's iter parameter and its implication on preprocessing?","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"45647333","1","","","2017-08-12 06:41:01","","0","134","<p>I trained my word vector like i normally would. I cleaned the text before hand where each line is a sentence with tokens separated by a space</p>

<pre><code>class Sentences:
    def __init__(self):
        pass

    def __iter__(self):
        i = 0
        with codecs.open('./data/cleaned_corpus.txt', 'r', 'utf-8') as file:
            for line in file:
                i += 1
                if i % 5000 == 0:
                    print('processed ' + str(i))

                yield line.split()


w2v = Word2Vec(Sentences(), size=100, min_count=10)
w2v.wv.save('model')
</code></pre>

<p>The issue is that some of the vectors return arrays with values that are numpy infs </p>

<pre><code>array([-inf,  inf, -inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,
    inf, -inf, -inf,  inf,  inf, -inf, -inf,  inf,  inf,  inf, -inf,
    inf,  inf, -inf, -inf, -inf,  inf, -inf,  inf, -inf,  inf,  inf,
   -inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,
   -inf, -inf,  inf,  inf,  inf, -inf, -inf, -inf, -inf, -inf, -inf,
   -inf, -inf, -inf, -inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf,
    inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf, -inf,  inf,
   -inf, -inf,  inf,  inf,  inf,  inf, -inf,  inf, -inf, -inf,  inf,
   -inf, -inf,  inf,  inf,  inf, -inf, -inf,  inf, -inf, -inf, -inf,
    inf], dtype=float32) 
</code></pre>

<p>I've already retrained the vector 3 times, I don't know what's causing this</p>
","6227091","","","","","2017-08-12 06:41:01","Some word vectors return values of infinite or -infinite","<python><numpy><gensim><word2vec>","0","3","","","","CC BY-SA 3.0"
"68758090","1","68761098","","2021-08-12 13:07:28","","0","33","<p>I'm trying to follow this page to create a wiki corpus, but I'm using Jupiter notebook <a href=""https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html"" rel=""nofollow noreferrer"">https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html</a></p>
<p>this is my code:</p>
<pre><code>import sys
from gensim.test.utils import datapath
from gensim.corpora import WikiCorpus

path_to_wiki_dump = datapath(&quot;enwiki-latest-pages-articles.xml.bz2&quot;)

wiki = WikiCorpus(path_to_wiki_dump)

output = open('wiki_en.txt', 'w',  encoding='utf-8')

i = 0
for text in wiki.get_texts():
    output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
    i = i + 1
    if (i % 10000 == 0):
        print('Processed ' + str(i) + ' articles')
output.close()
print('Processing complete!')

</code></pre>
<p>The Error I got was</p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/enwiki-latest-pages-articles.xml.bz2'

</code></pre>
<p>All the files are in one place so I'm not sure what's wrong</p>
","7045034","","","","","2021-08-12 16:26:50","Creating Corpus from wiki dump file using Jupyter notebook","<python><jupyter-notebook><gensim><wiki><corpus>","1","0","","","","CC BY-SA 4.0"
"68864019","1","","","2021-08-20 14:41:05","","0","37","<p>I'm trying to classify a dataset of files. In this dataset I have a colunm of texts and a column of labels. I want to build a model, based on wikipedia corpus, but I'm a little lost in the middle.
What I did so far...</p>
<p>I preprocessed my Text column (removing stopwords, whitespaces, deaccent, etc) and saved on a new csv file. Then I tagged using gensim lib:</p>
<pre><code>def apply_preprocessing(fname, tokens_only=False):
    with smart_open.open(fname) as f:
        for i, line in enumerate(f):
            tokens = gensim.utils.simple_preprocess(line)
            if tokens_only:
                yield tokens
            else:
                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])


preprocessed_dataset = '/content/drive/MyDrive/dataset/preprocessed_dataset.csv'
preprocessed_dataset_train = list(apply_preprocessing(preprocessed_dataset))
</code></pre>
<p>This gives me an array of arrays of words contained in my Text column for each documment I have in my preprocessed_dataset.</p>
<p>I know that doing this loop I get each array of words:</p>
<pre><code>for doc_id in range(len(preprocessed_dataset_train)):
    preprocessed_dataset_train[doc_id].words
</code></pre>
<p>My goal is to give those words and &quot;say&quot;: Based on this wikipedia pretrained embeddings (<a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">https://wikipedia2vec.github.io/wikipedia2vec/pretrained/</a>), how similiar are one doc to another based on what you learn with this wikipedia corpus?</p>
<p>How do I use this pretrained wikipedia? This is already a file of words vectors, right? If so, How can I use it to analyse my preprocessed_dataset_train?</p>
<p>What's the next step should I do/understand to get to my goal?
I'm sorry for so many questions, when I think I'm understanding the road, I'm lost again and again.</p>
","2465455","","","","","2021-08-20 14:41:05","How can I train a model with gensim lib and wikipedia2vec txt?","<python><gensim><word2vec><doc2vec>","0","4","","","","CC BY-SA 4.0"
"68773068","1","","","2021-08-13 13:19:39","","0","126","<p>I am embedding a graph with Node2Vec library but I am getting this as error :
<strong>TypeError: <strong>init</strong>() got an unexpected keyword argument 'size'</strong> to the following code block -</p>
<pre><code>model = node2vec.fit(window=10, min_count=1, batch_words=4)
</code></pre>
<p>Any idea why this is happening?</p>
","14049331","","11339311","","2021-09-17 09:51:09","2021-09-29 13:02:44","TypeError: __init__() got an unexpected keyword argument 'size' in Node2Vec","<python-3.x><graph><gensim><word2vec>","1","1","1","","","CC BY-SA 4.0"
"60515074","1","","","2020-03-03 20:33:16","","0","126","<p>Is there a way to save a gensim LDA model to ONNX format?  We need to be able to train using Python/gensim and then operationalize it into an Onnx model to publish and use.</p>
","2740927","","","","","2020-03-03 20:43:46","Saving gensim LDA model to ONNX","<gensim><lda><onnx>","1","0","","","","CC BY-SA 4.0"
"60524589","1","60532139","","2020-03-04 11:02:45","","0","137","<p>Just curiosity, but I was debugging gensim's FastText code for replicating the implementation of Out-of-Vocabulary (OOV) words, and I'm not being able to accomplish it.
So, the process i'm following is training a tiny model with a toy corpus, and then comparing the resulting vectors of a word in the vocabulary. That means if the whole process is OK, the output arrays should be the same.</p>

<p>Here is the code I've used for the test:</p>

<pre><code>from gensim.models import FastText
import numpy as np
# Default gensim's function for hashing ngrams
from gensim.models._utils_any2vec import ft_hash_bytes

# Toy corpus
sentences = [['hello', 'test', 'hello', 'greeting'],
             ['hey', 'hello', 'another', 'test']]

# Instatiate FastText gensim's class
ft = FastText(sg=1, size=5, min_count=1, \
window=2, hs=0, negative=20, \
seed=0, workers=1, bucket=100, \
min_n=3, max_n=4)

# Build vocab
ft.build_vocab(sentences)

# Fit model weights (vectors_ngram)
ft.train(sentences=sentences, total_examples=ft.corpus_count, epochs=5)

# Save model
ft.save('./ft.model')
del ft

# Load model
ft = FastText.load('./ft.model')

# Generate ngrams for test-word given min_n=3 and max_n=4
encoded_ngrams = [b""&lt;he"", b""&lt;hel"", b""hel"", b""hell"", b""ell"", b""ello"", b""llo"", b""llo&gt;"", b""lo&gt;""]
# Hash ngrams to its corresponding index, just as Gensim does
ngram_hashes = [ft_hash_bytes(n) % 100 for n in encoded_ngrams]
word_vec = np.zeros(5, dtype=np.float32)
for nh in ngram_hashes:
    word_vec += ft.wv.vectors_ngrams[nh]

# Compare both arrays
print(np.isclose(ft.wv['hello'], word_vec))

</code></pre>

<p>The output of this script is False for every dimension of the compared arrays.</p>

<p>It would be nice if someone could point me out if i'm missing something or doing something wrong. Thanks in advance!</p>
","5319955","","","","","2020-03-04 18:02:19","Cannot reproduce pre-trained word vectors from its vector_ngrams","<python-3.x><gensim><fasttext><oov>","1","0","","","","CC BY-SA 4.0"
"68872427","1","68877111","","2021-08-21 11:03:35","","0","27","<p>I am using <code>Gensim</code> to train a <code>Word2Vec</code> embedding on different corpora, pertaining to different years, to compare the embedding vectors.
My question is: if I repeat the documents of a specific year twice and documents of another year just once, do the resulting embeddings give more weight to the repeated documents?
I have in my mind to make a corpus that gives more weight to recent documents and less weight to documents from far past.
I simply train the model on my <code>Line Sentence</code> corpus file.</p>
<pre><code>Word2Vec(corpus_file=corpus, vector_size=100, window=5, min_count=5, workers=4)
</code></pre>
","4168794","","4168794","","2021-08-23 09:15:57","2021-08-23 09:15:57","the repetitions in Gensim Word2Vec training corpus","<python><gensim><word2vec><word-embedding><corpus>","1","0","","","","CC BY-SA 4.0"
"60588557","1","60588998","","2020-03-08 14:35:36","","0","123","<p>I am using 'Gensim' to generate summary of different rows I have. Here is what the original dataframe looks like:</p>

<pre><code>df.head()

                                   Example Content
0   Not happy they have just reduced rates for Und...
1   One of the worst banks. I had a very bad exper...
2   Some one in lloyds has signed a form in My nam...
3   Card blocked due to ordering a takeaway from m...
4   There are plenty of better banks than Lloyds.\...
</code></pre>

<p>I am able to apply summarization to every row using gensim. Problem is, I want every rows summary to appear against its original, and this is not happening. Here is what my code looks like:</p>

<pre><code>a = []

for i in df['Example Content']:

    i= i + str("". This is second sentence. This is third"")             # this is to add two more sentences so that gensim summarizes it. These sentence add no value to summary.
    a = summarize(i, ratio=0.4, split = True)

df['Summary'] = a
</code></pre>

<p>And here is the ouput to the above code:</p>

<pre><code>                                     Example Content                                 Summary
0   Not happy they have just reduced rates for Und...       Today I got a new phone and switched my sim an...
1   One of the worst banks. I had a very bad exper...       Today I got a new phone and switched my sim an...
2   Some one in lloyds has signed a form in My nam...       Today I got a new phone and switched my sim an...
3   Card blocked due to ordering a takeaway from m...       Today I got a new phone and switched my sim an...
4   There are plenty of better banks than Lloyds.\...       Today I got a new phone and switched my sim an...
</code></pre>

<p>Below shown are all the individual summaries, generated by gensim, of each row:</p>

<pre><code>The 2nd address was a shopping centre and they didnt even give me the name of the business.
I wasn't to know as I through Gallarias Novas was the shop name but that was just the place.
They said that they had issued a new card that I hadn't received and even though they new I was abroad using my card they stopped it anyway.
When my new card did arrive after getting home I now know the reason was that they were making me have a con tactless card whcih I did nto request.

 Today I got a new phone and switched my sim and set up my banking apps inc Halifax and Lloyd√ïs.
Halifax worked fine, usual 4 digit code and confirmation call came through and all set up in mins.
</code></pre>

<p>How should i grab individual summaries corresponding to the original content and place them in the dataframe? </p>
","12988629","","","","","2020-03-08 15:25:22","Gensim row wise dataframe summary","<python><pandas><for-loop><gensim><summarization>","1","0","","","","CC BY-SA 4.0"
"60491035","1","60498682","","2020-03-02 14:35:01","","0","557","<p>When attempting to load a word2vec model trained by Gensim on a Windows machine, I receive the following error:</p>

<p><code>AttributeError: Can't get attribute 'EpochProgress' on &lt;module '__main__'&gt;</code></p>

<p>I've successfully trained numerous models with Gensim in the past on this system. The only variation being this time I split the <code>model.build_vocab()</code> and <code>model.train()</code> phases, adding in saves &amp; time hacks for each epoch. I also used a different iterator for the vocab build and the training phrases, but on the same dataset with the same tokenization pipeline.</p>

<p>Here is how I did the epoch progress tracking/saving:</p>

<pre class=""lang-py prettyprint-override""><code>class EpochProgress(CallbackAny2Vec):
    '''saves the model after each epoch'''

    def __init__(self, path_prefix):
        self.path_prefix = path_prefix
        self.epoch = 0
        self.start_time = time.time()

    def on_epoch_begin(self, model):
        print(""epoch #{} started"".format(self.epoch))

    def on_epoch_end(self, model):
        print(""epoch #{} completed"".format(self.epoch))
        passed = (time.time() - self.start_time)/60/60 # elapsed time since start in HOURS
        print(""{} hours have passed"".format(str(passed)))
        output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))
        model.save(output_path)
        print(""model saved at: {}"".format(output_path))
        self.epoch +=1
</code></pre>

<p><code>epoch_progress = EpochProgress('E:/jade_prism/embeddings/phrase-embed-over- time/mega_WOS_word2vec/w2v_models/in_progress/')</code></p>

<p>I then load the baseline model with the vocab build and set a few parameters:</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.Word2Vec.load(baseline_models_directory+chosen_name)
model.window = window
model.size = size
model.workers = workers 
model.callbacks = [epoch_progress]
</code></pre>

<p>Then I do the training like this:</p>

<p><code>model.train(corpus, total_examples=model.corpus_count, epochs=epochs)</code></p>

<p>And finally, save the end product like this: </p>

<p><code>model.save('E:/w2v_models/trained/{}'.format(new_model_filename))</code></p>

<p>Training appeared to work properly, and model saved as expected- unfortunately now I can't load it.</p>

<p>Here is the full Debug readout:</p>

<pre><code>&gt; AttributeError                            Traceback (most recent call
&gt; last)
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\word2vec.py
&gt; in load(cls, *args, **kwargs)    1329         try:
&gt; -&gt; 1330             model = super(Word2Vec, cls).load(*args, **kwargs)    1331 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\base_any2vec.py
&gt; in load(cls, *args, **kwargs)    1243         """"""
&gt; -&gt; 1244         model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)    1245         if not hasattr(model,
&gt; 'ns_exponent'):
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\base_any2vec.py
&gt; in load(cls, fname_or_handle, **kwargs)
&gt;     602         """"""
&gt; --&gt; 603         return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
&gt;     604 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\utils.py in
&gt; load(cls, fname, mmap)
&gt;     425 
&gt; --&gt; 426         obj = unpickle(fname)
&gt;     427         obj._load_specials(fname, mmap, compress, subname)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\utils.py in
&gt; unpickle(fname)    1383         if sys.version_info &gt; (3, 0):
&gt; -&gt; 1384             return _pickle.load(f, encoding='latin1')    1385         else:
&gt; 
&gt; AttributeError: Can't get attribute 'EpochProgress' on &lt;module
&gt; '__main__'&gt;
&gt; 
&gt; During handling of the above exception, another exception occurred:
&gt; 
&gt; AttributeError                            Traceback (most recent call
&gt; last) &lt;ipython-input-4-0206f9f8f3ad&gt; in &lt;module&gt;
&gt;       3 
&gt;       4 # Load the model based onthe model name
&gt; ----&gt; 5 model = gensim.models.Word2Vec.load(model_name)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\word2vec.py
&gt; in load(cls, *args, **kwargs)    1339             logger.info('Model
&gt; saved using code from earlier Gensim Version. Re-loading old model in
&gt; a compatible way.')    1340             from
&gt; gensim.models.deprecated.word2vec import load_old_word2vec
&gt; -&gt; 1341             return load_old_word2vec(*args, **kwargs)    1342     1343 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\word2vec.py
&gt; in load_old_word2vec(*args, **kwargs)
&gt;     170 
&gt;     171 def load_old_word2vec(*args, **kwargs):
&gt; --&gt; 172     old_model = Word2Vec.load(*args, **kwargs)
&gt;     173     vector_size = getattr(old_model, 'vector_size', old_model.layer1_size)
&gt;     174     params = {
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\word2vec.py
&gt; in load(cls, *args, **kwargs)    1639     @classmethod    1640     def
&gt; load(cls, *args, **kwargs):
&gt; -&gt; 1641         model = super(Word2Vec, cls).load(*args, **kwargs)    1642         # update older models    1643         if hasattr(model,
&gt; 'table'):
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\old_saveload.py
&gt; in load(cls, fname, mmap)
&gt;      85         compress, subname = SaveLoad._adapt_by_suffix(fname)
&gt;      86 
&gt; ---&gt; 87         obj = unpickle(fname)
&gt;      88         obj._load_specials(fname, mmap, compress, subname)
&gt;      89         logger.info(""loaded %s"", fname)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\old_saveload.py
&gt; in unpickle(fname)
&gt;     377             b'gensim.models.wrappers.fasttext', b'gensim.models.deprecated.fasttext_wrapper')
&gt;     378         if sys.version_info &gt; (3, 0):
&gt; --&gt; 379             return _pickle.loads(file_bytes, encoding='latin1')
&gt;     380         else:
&gt;     381             return _pickle.loads(file_bytes)
&gt; 
&gt; AttributeError: Can't get attribute 'EpochProgress' on module '__main__'\&gt;
</code></pre>
","12993895","","130288","","2020-03-03 00:54:42","2020-03-03 01:02:42","Unable to Load Model Trained in Gensim- pickle-related error","<python-3.x><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"68824788","1","","","2021-08-17 22:39:10","","0","20","<p>Can we train OneVsRestClassifier with the output of FastText as shown below:</p>
<p><strong>GENSIM Library with FastText</strong></p>
<pre><code>fasttext_out=model_ted.wv.most_similar(&quot;The Lemon Drop Kid , a New York City swindler, is illegally touting horses at a Florida racetrack. After several successful hustles, the Kid comes across a beautiful, but gullible, woman intending to bet a lot of money. The Kid convinces her to switch her bet, employing a prefabricated con. Unfortunately for the Kid, the woman belongs to notorious gangster Moose Moran , as does the money.&quot;)

print(fasttext_out) 
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>[('Foreign legion', 0.9770169258117676), ('Kafkaesque', 0.9751489162445068), ('Reboot', 0.9710761308670044), ('Space opera', 0.9696193337440491), ('Outlaw', 0.9682430028915405), ('Libraries and librarians', 0.9682008028030396), ('Parkour in popular culture', 0.9671787619590759), ('Movies About Gladiators', 0.963977575302124), ('Baseball', 0.9581758379936218), ('Cyberpunk', 0.9565480351448059)]
</code></pre>
<p><strong>Using Logistic Regression, which is a binary classification algorithm and use it with the One-vs-Rest heuristic to perform multi-class classification</strong></p>
<pre><code>lr = LogisticRegression()
clf = OneVsRestClassifier(lr)

// xtrain_tfidf, ytrain are the outputs of TF-IDF and I want to give outputs of FastText
clf.fit(xtrain_tfidf, ytrain) 
</code></pre>
<p>Apologies about this question and I am not sure if it is possible or not but I am just curious If I can train OneVsRestClassifier with the output of FastText since I can train OneVsRestClassifier with the output of TF-IDF but I am unable to find anything similar to my question on the internet.</p>
<p>Can someone please confirm if it is possible. Thank you.</p>
","12217286","","","","","2021-08-18 01:29:42","How can we train OneVsRestClassifier with the output of FastText","<python><machine-learning><nlp><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"60613532","1","","","2020-03-10 08:03:07","","3","5377","<p>Here, best_model_lda is an sklearn based LDA model and we are trying to find a coherence score for this model..</p>

<pre><code>coherence_model_lda = CoherenceModel(model = best_lda_model,texts=data_vectorized, dictionary=dictionary,coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\n Coherence Score :',coherence_lda)
</code></pre>

<p>Output : This error pops up because i'm trying to find the coherence score of an  sklearn LDA topic model, is there a way around it. Also , what metric is the sklearn LDA using to group these words together ?</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py in _get_topics_from_model(model, topn)
   490                 matutils.argsort(topic, topn=topn, reverse=True) for topic in
--&gt; 491                 model.get_topics()
   492             ]

AttributeError: 'LatentDirichletAllocation' object has no attribute 'get_topics'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-106-ce8558d82330&gt; in &lt;module&gt;
----&gt; 1 coherence_model_lda = CoherenceModel(model = best_lda_model,texts=data_vectorized, dictionary=dictionary,coherence='c_v')
     2 coherence_lda = coherence_model_lda.get_coherence()
     3 print('\n Coherence Score :',coherence_lda)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py in __init__(self, model, topics, texts, corpus, dictionary, window_size, keyed_vectors, coherence, topn, processes)
   210         self._accumulator = None
   211         self._topics = None
--&gt; 212         self.topics = topics
   213 
   214         self.processes = processes if processes &gt;= 1 else max(1, mp.cpu_count() - 1)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py in topics(self, topics)
   433                     self.model)
   434         elif self.model is not None:
--&gt; 435             new_topics = self._get_topics()
   436             logger.debug(""Setting topics to those of the model: %s"", self.model)
   437         else:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py in _get_topics(self)
   467     def _get_topics(self):
   468         """"""Internal helper function to return topics from a trained topic model.""""""
--&gt; 469         return self._get_topics_from_model(self.model, self.topn)
   470 
   471     @staticmethod

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py in _get_topics_from_model(model, topn)
   493         except AttributeError:
   494             raise ValueError(
--&gt; 495                 ""This topic model is not currently supported. Supported topic models""
   496                 "" should implement the `get_topics` method."")
   497 

ValueError: This topic model is not currently supported. Supported topic models should implement the `get_topics` method.```
</code></pre>
","8099146","","","","","2020-06-02 12:46:44","How do I calculate the coherence score of an sklearn LDA model?","<scikit-learn><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"60451614","1","60459949","","2020-02-28 12:21:32","","0","496","<p>I see a code which uses Wikicorpus on an Arabic Wikipedia dump, and I know that the process will take a long time to execute, I also searched around about the warning that I get when executing it which says: </p>

<blockquote>
  <p>(UserWarning: detected Windows; aliasing chunkize to chunkize_serial<br>
  warnings.warn(""detected Windows; aliasing chunkize to
  chunkize_serial""))</p>
</blockquote>

<p>and answers said that it's ok, nothing serious, it's just a warning. 
But after waiting about 3 days without any response! I start wondering whether is it truly work on the Arabic dump file, or I have to do certain kind of pre-processing before passing the Arabic dump file to the Wikicorpus object?
the data size is about 989.6 MB.
and I surround the WikiCorpus code line with two print commands, to know when it started and when it finished executing, like this:</p>

<pre><code>print('start WikiCorpus')
wiki = WikiCorpus(self.in_f)
print('finish WikiCorpus')
</code></pre>

<p>where the self.in_f is the Arabic Wikipedia dump like this: (/the path where the file located/arwiki-20200201-pages-articles.xml.bz2), but never reached the second print command during the runtime.</p>
","8330729","","","","","2020-02-28 22:16:33","Does WikiCorpus from gensim library works on Arabic Wikipedia dump?","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"68792982","1","68796406","","2021-08-15 15:34:29","","0","24","<p>Assuming the source text is:</p>
<blockquote>
<p>The quick brown fox jumps over the lazy dog</p>
</blockquote>
<p>And the window size is 2, like the following picture:</p>
<p><a href=""https://i.stack.imgur.com/vJQ42.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vJQ42.png"" alt=""window size is 2"" /></a></p>
<p>So we have a lots of training samples, and the format of training samples is (input, label).</p>
<p>For example:</p>
<blockquote>
<p>(the,quick), (the,brown), (quick,the), (quick,brown), (quick,fox).....</p>
</blockquote>
<p>Does that mean the input (<code>quick</code>) has three output(<code>the</code>, <code>brown</code>, <code>fox</code>)?</p>
","13874745","","","","","2021-08-16 00:52:14","Does the input of Skip gram model have multiple labels?","<python><tensorflow><nlp><nltk><gensim>","1","0","","","","CC BY-SA 4.0"
"60584750","1","","","2020-03-08 05:30:34","","0","256","<p>I trained a Word2Vec model using Gensim, and I have two sets of words:</p>

<pre><code>S1 = {'','','' ...}
S2 = {'','','' ...}
</code></pre>

<p>for each word w1 in S1, I want to find top 5 words that are most similar to w1. I am currently doing this way:</p>

<pre><code>model = w2v_model
 word_similarities = {}
 for w1 in S1:
    similarities = {}
    for w2 in S2:
       if w1 in model.wv and w2 in model.wv:
           similarity = model.similarity(w1, w2)
           similarities[w2] = similarity
    word_similarties[w1] = similarities
</code></pre>

<p>Then for each word in word_similarities, I can get the top N from its dict values. If S1 and S2 are large, this becomes very slow.</p>

<p>Is there a quicker way to compute large pairs of words in Word2Vec, either in genism or tensorflow?</p>
","3943868","","","","","2020-03-10 21:47:57","How to speed up word2vec similarity calculation?","<tensorflow><gensim>","1","0","","","","CC BY-SA 4.0"
"60584927","1","","","2020-03-08 06:08:08","","0","57","<p>I wanna calculate recall for query""terminator_2"" or any movie on google pre-trained model (GoogleNews-vectors-negative300.bin).but i don't know about the relevant movie.is  there any solution for this problem or not?</p>
","12507321","","12507321","","2020-03-11 15:56:54","2020-03-11 15:56:54","how to calculate recall when you don't know about relevant results","<gensim><precision-recall><pre-trained-model>","0","15","","","","CC BY-SA 4.0"
"68954937","1","","","2021-08-27 14:15:16","","0","14","<p>Provided topics from a topic model, I want to calculate the coherence score for one topic each, instead of a set of topics at once.</p>
<p>I have the following code:</p>
<pre><code>from gensim.models.coherencemodel import CoherenceModel
import gensim.corpora as corpora

def calculate_coherence_value(data, topics, coherence = 'c_v'):
    id2word = corpora.Dictionary(data)
    corpus = [id2word.doc2bow(text) for text in data]
    return CoherenceModel(topics=topics,texts = data, corpus=corpus, dictionary=id2word, coherence= coherence, topn=len(topics)).get_coherence()

coherence_list = [calculate_coherence_value(data, topic) for topic in topics]
</code></pre>
<p>Initially, I had the following variables:</p>
<ul>
<li><code>topics</code> a list of lists of strings.</li>
<li><code>data</code> a list of lists of strings.</li>
</ul>
<p>However, this gives the error:</p>
<pre><code>KeyError: 'p'
</code></pre>
<p>Coherencemodel expects a list of lists with strings. And since <code>topic</code> is a list with strings, it loops through each string.</p>
<p>So instead, I cast topics into another list, so that it is:</p>
<ul>
<li><code>topics</code> a list of lists with a list with strings.</li>
</ul>
<p>When I now run the code, I don't get errors, but my output for the first 5 topics looks as follows:</p>
<pre><code>[1.0, 1.0, 1.0, 1.0, 1.0]
</code></pre>
<p>This does not seem right to me as I expect values between 0 and 1. What do I need to change to get the right coherence scores?</p>
","7714681","","7714681","","2021-08-27 14:45:37","2021-08-27 14:45:37","Calculate coherence for one topic only","<python-3.x><nlp><gensim><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"60454355","1","64484122","","2020-02-28 15:05:28","","1","463","<p>I have trained my model using Gensim. I draw a 2D plot using PCA but it is not clear too much. I wanna change it to 3D  with capable of zooming .my result is so dense.</p>

<pre><code>from sklearn.decomposition import PCA
from matplotlib import pyplot
X=model[model.wv.vocab]
pca=PCA(n_components=2)
result=pca.fit_transform(X)
pyplot.scatter(result[:,0],result[:,1])
word=list(model.wv.most_similar('eden_lake'))
for i, word in enumerate(words):
  pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
pyplot.show()
</code></pre>

<p>And the result:
<a href=""https://i.stack.imgur.com/eUFwI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eUFwI.jpg"" alt=""enter image description here""></a></p>

<p>it possible to do that?</p>
","12507321","","12507321","","2020-02-28 19:18:35","2020-12-10 18:43:07","Draw 3D Plot for Gensim model","<python-3.x><pca><gensim>","2","1","","","","CC BY-SA 4.0"
"68833707","1","68837475","","2021-08-18 13:49:23","","0","21","<p>My issue is the following.</p>
<p>In my code I'm modifying the .wv[word] before training but after .build_vocab(), which is fairly straight forward. Just instead of the vectors in there add mine for every word.</p>
<pre><code>for elem in setIntersection:
    if len(word_space[elem]) != 300:
        print('here', elem) #cast it to the fire
        sys.exit()
    w2vObjectRI.wv[elem] = np.asarray(word_space[elem], dtype=np.float32)
</code></pre>
<p>Where setIntersection is just a set of common words between gensim word2vec and RandomIndexing trained. Same size of 300 in both.</p>
<p>Now I want to also modify the hidden-to-output layer weights, which I was told that they are in .trainables.syn1neg[i], but here is my issue this matrix is not word addressable, is just a normal matrix with out names. How could I know which letter I will be modifying in this matrix? Also I see that they are initialised with 0s, I was just thinking if these weights are not reset before training? More clearly if I change those weights and then call train will it use the ones I provided? Thanks.</p>
<pre><code>for i in range(len(setIntersection)):
if len(word_space[setIntersection[i]]) != 300:
    print('here', setIntersection[i]) #cast it to the fire
    sys.exit()
w2vObjectRI.trainables.syn1neg[i] = np.asarray(word_space[setIntersection[i]], dtype=np.float32)
</code></pre>
<p>Cheers,</p>
<p>Pedro.</p>
","1423656","","","","","2021-08-18 18:15:59","Modifying .trainables.syn1neg[i] with previously trained vectors in Gensim word2vec","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"68963356","1","","","2021-08-28 10:14:49","","0","15","<p>I'm trying to generate bigrams and trigrams for an LDA topic modeling. My code is given below;</p>
<pre><code>bigram = gensim.models.Phrases(data.normalized, min_count=5, threshold=5, 
                               connector_words=gensim.models.phrases.ENGLISH_CONNECTOR_WORDS)
bigram_mod = gensim.models.phrases.Phraser(bigram)

trigram = gensim.models.Phrases(bigram_mod[data.normalized], min_count=5, threshold=2)
trigram_mod = gensim.models.phrases.Phraser(trigram)
</code></pre>
<p>For some reason, at least for one sample from the corpus, executing <code>trigram[sample]</code> gives me a different result than executing <code>trigram_mod[sample]</code>. My understanding is <code>trigram_mod</code> is the frozen phraser for <code>trigram</code>, and inherits the same parameters.</p>
<p>Why is the result different?</p>
","4602603","","","","","2021-08-28 10:14:49","Phrases gives different output from Phraser","<gensim>","0","5","","","","CC BY-SA 4.0"
"68963361","1","68967833","","2021-08-28 10:15:35","","0","24","<p>I have two corpora that are from the same field, but with a temporal shift, say one decade. I want to train Word2vec models on them, and then investigate the different factors affecting the semantic shift.</p>
<p>I wonder how should I initialize the second model with the first model's embeddings to avoid as much as possible the effect of variance in co-occurrence estimates.</p>
","4168794","","","","","2021-08-28 20:15:50","Using weight from a Gensim Word2Vec model as a starting point of another model","<python><gensim><word2vec><word-embedding><fine-tune>","1","0","","","","CC BY-SA 4.0"
"60602768","1","60612209","","2020-03-09 14:36:19","","1","926","<p>This is the code for creating the model :</p>

<pre><code>import gensim
NUM_TOPICS = 4
ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics = 
NUM_TOPICS,id2word=dictionary,passes=100)
ldamodel.save('model5.gensim')
topics = ldamodel.print_topics(num_words=4)
print(topics)
</code></pre>

<p>This is the code for GridSearchCV :</p>

<pre><code>search_params = {'n_components': [4, 6, 8, 10, 20], 'learning_decay': [.5, .7, .9]}


# Init Grid Search Class
model = GridSearchCV(ldamodel, param_grid=search_params)

# Do the Grid Search
model.fit(data_vectorized)
</code></pre>

<p>This is the output : </p>

<pre><code>*---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-108-1a35c49ac19e&gt; in &lt;module&gt;
      9 
     10 # Do the Grid Search
---&gt; 11 model.fit(data_vectorized)
~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\model_selection\_search.py in fit(self, X, y, groups, **fit_params)
    627 
    628         scorers, self.multimetric_ = _check_multimetric_scoring(
--&gt; 629             self.estimator, scoring=self.scoring)
    630 
    631         if self.multimetric_:
~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py in _check_multimetric_scoring(estimator, scoring)
    471     if callable(scoring) or scoring is None or isinstance(scoring,
    472                                                           str):
--&gt; 473         scorers = {""score"": check_scoring(estimator, scoring=scoring)}
    474         return scorers, False
    475     else:
~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py in check_scoring(estimator, scoring, allow_none)
    399     if not hasattr(estimator, 'fit'):
    400         raise TypeError(""estimator should be an estimator implementing ""
--&gt; 401                         ""'fit' method, %r was passed"" % estimator)
    402     if isinstance(scoring, str):
    403         return get_scorer(scoring)
TypeError: estimator should be an estimator implementing 'fit' method, &lt;gensim.models.ldamodel.LdaModel object at 0x000002121E55D3C8&gt; was passed*
</code></pre>
","8099146","","6573902","","2020-03-10 05:42:35","2020-03-10 05:53:58","Scikit-Learn GridSearchCV failing on on a gensim LDA model","<python><scikit-learn><gensim><lda><gridsearchcv>","1","0","","","","CC BY-SA 4.0"
"68971791","1","68975633","","2021-08-29 09:58:03","","0","22","<p>I have a dataset off millions of arrays like follows:</p>
<pre><code>  sentences=[
    [
     'query_foo bar',
     'split_query_foo',
     'split_query_bar',
     'sku_qwre',
     'brand_A B C',
     'split_brand_A',
     'split_brand_B',
     'split_brand_C',
     'color_black',
     'category_C1',
     'product_group_clothing',
     'silhouette_t_shirt_top',
  ],
  [...]
  ]
</code></pre>
<p>where you find a query, a sku that was acquired by the user doing the query and a few attributes of the SKU. My idea was to do a very basic model based on word2vec where I could find similar things together.</p>
<p>In a simple way, if I search for <code>t-shirt</code> on the model I would expect to have t-shirt SKUs near the query.</p>
<p>I try to use gensim (I'm new to this library) with different attributes to build a model:</p>
<pre><code>from gensim.models.callbacks import CallbackAny2Vec

class callback(CallbackAny2Vec):
    '''Callback to print loss after each epoch.'''

    def __init__(self):
        self.epoch = 0
        self.loss_to_be_subed = 0

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        loss_now = loss - self.loss_to_be_subed
        self.loss_to_be_subed = loss
        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))
        self.epoch += 1

model = Word2Vec(
  sentences=sentences, 
  vector_size=100, 
  window=1000, 
  min_count=2, 
  workers=-1,
  epochs=10,
#   negative=5,
  compute_loss=True,
  callbacks=[callback()]
)
</code></pre>
<p>I got this output:</p>
<pre><code>Loss after epoch 0: 0.0
Loss after epoch 1: 0.0
Loss after epoch 2: 0.0
Loss after epoch 3: 0.0
Loss after epoch 4: 0.0
Loss after epoch 5: 0.0
Loss after epoch 6: 0.0
Loss after epoch 7: 0.0
Loss after epoch 8: 0.0
Loss after epoch 9: 0.0
</code></pre>
<p>All losses of 0!!!
I start to get very suspicious at this point.</p>
<p>Note: Each element of <code>sentences</code> are independent, I hop the library don't try to mix different terms in different arrays.</p>
<p>For trying to test the model, I tried a very frequent query like <code>model.wv.most_similar('query_t-shirt', topn=100)</code> and the results are completely absurd.</p>
<p>Is my idea crazy or am I using incorrectly the library?</p>
","1356713","","","","","2021-08-29 18:01:04","Using Gensin Word2Vec to improve search","<python><nlp><gensim><information-retrieval>","1","0","","","","CC BY-SA 4.0"
"60627294","1","","","2020-03-10 23:29:42","","0","790","<p>I am using Python version 3.5 in a virtual environment and when trying to import the below command i am getting ""ImportError: cannot import name 'Type'""</p>

<p>from gensim.models.phrases import Phraser</p>

<p>I have uninstalled all other packages and just installed gensim and still it fails. Any suggestions would be of great help</p>

<p>----> 1 from gensim.models.phrases import Phraser
      2 from gensim.models.word2vec import Word2Vec
      3 import pickle
      4 from botocore.client import Config</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/gensim/<strong>init</strong>.py in 
      3 """"""
      4 
----> 5 from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
      6 import logging
      7 </p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/gensim/parsing/<strong>init</strong>.py in 
      2 
      3 from .porter import PorterStemmer  # noqa:F401
----> 4 from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
      5                             strip_tags, strip_short, strip_numeric,
      6                             strip_non_alphanum, strip_multiple_whitespaces,</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/gensim/parsing/preprocessing.py in 
     40 import glob
     41 
---> 42 from gensim import utils
     43 from gensim.parsing.porter import PorterStemmer
     44 </p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/gensim/utils.py in 
     38 import numpy as np
     39 import numbers
---> 40 import scipy.sparse
     41 
     42 from six import iterkeys, iteritems, itervalues, u, string_types, unichr</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/<strong>init</strong>.py in 
    154     # This makes ""from scipy import fft"" return scipy.fft, not np.fft
    155     del fft
--> 156     from . import fft</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/fft/<strong>init</strong>.py in 
     74 from <strong>future</strong> import division, print_function, absolute_import
     75 
---> 76 from ._basic import (
     77     fft, ifft, fft2, ifft2, fftn, ifftn,
     78     rfft, irfft, rfft2, irfft2, rfftn, irfftn,</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/fft/_basic.py in 
----> 1 from scipy._lib.uarray import generate_multimethod, Dispatchable
      2 import numpy as np
      3 
      4 
      5 def _x_replacer(args, kwargs, dispatchables):</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/_lib/uarray.py in 
     25     from uarray import _Function
     26 else:
---> 27     from ._uarray import *
     28     from ._uarray import _Function
     29 </p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/_lib/_uarray/<strong>init</strong>.py in 
    112 """"""
    113 
--> 114 from ._backend import *
    115 
    116 <strong>version</strong> = '0.5.1+5.ga864a57.scipy'</p>

<p>/simcloud-packages/venv/lib/python3.5/site-packages/scipy/_lib/_uarray/_backend.py in 
----> 1 from typing import (
      2     Callable,
      3     Iterable,enter code here
      4     Dict,
      5     Tuple,</p>

<p>ImportError: cannot import name 'Type'</p>
","13008461","","","","","2020-03-11 03:19:04","Importing Gensim gensim.models.phrases import phraser fails with ""ImportError: cannot import name 'Type'""","<gensim>","1","0","","","","CC BY-SA 4.0"
"69064948","1","69066880","","2021-09-05 15:54:22","","0","147","<p>I got gensim to work in Google Collab by following this process:</p>
<pre><code>!pip install gensim
from gensim.summarization import summarize
</code></pre>
<p>Then I was able to call <code>summarize(some_text)</code></p>
<p>Now I'm trying to run the same thing in VS code:</p>
<p>I've installed gensim:
<code>pip3 install gensim</code></p>
<p>but when I run</p>
<pre><code>from gensim.summarization import summarize
</code></pre>
<p>I get the error</p>
<pre><code>Import &quot;gensim.summarization&quot; could not be resolvedPylancereportMissingImports
</code></pre>
<p>I've also tried <code>from gensim.summarization.summarizer import summarize</code> with same error. Regardless I haven't been able to call the function <code>summarize(some_text)</code> outside of Google Collab.</p>
","11805611","","472495","","2021-09-06 20:34:37","2021-09-06 20:34:37","How to import gensim summarize","<python><visual-studio-code><nlp><gensim>","2","1","","","","CC BY-SA 4.0"
"60599396","1","","","2020-03-09 11:14:47","","0","496","<p>I saved an LDAWallet model:</p>

<p>First I did the train : </p>

<pre><code> mallet_path = 'mallet-2.0.8/bin/mallet'
 ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, id2word=id2word, 
 num_topics=14)
</code></pre>

<p>And then I saved the model using the save method:</p>

<pre><code>ldamallet.save('lda_v0.model')
</code></pre>

<p>I forgot the set the prefix to a certain file when I trained the mode, as a consequence I lost all the temporary files created by gensim when training (doctopics etc...).
And I think that because of that, when I load the model and want to predict topics  :</p>

<pre><code>model_lda = gensim.models.ldamodel.LdaModel.load('lda_v0.model')
###stuff
###stuff
###stuff
model_lda[input]
</code></pre>

<p>I get an error :</p>

<p><strong>[Errno 2] No such file or directory: '/var/folders/_f/ttl3hvqn75g4rb5cdg02qg1c0000gn/T/2e13a7_doctopics.txt.infer'</strong></p>

<p>I tried unsuccessfully to reproduce the same model with the data (and setting the prefix so that I don't lose the temporary files).
I'm wondering if it is possible to use the method print_topics (I forgot to say that loading the model is working and I can get all the topics and their words) and for each topics , retrieve the weight of the words related to the topics and compute the probability but I don't know how the lda model predict the topic for each document, so I'm not sure if my idea can work.</p>

<p>Do you have any idea how to fix this issue ?
I only want to predict for a document the probabibity of each topic.</p>

<p>Thank you</p>
","12675523","","","","","2020-04-01 16:47:47","Python Mallet LDA Errno 2 No such file or directory","<nlp><gensim><lda><mallet>","1","3","","","","CC BY-SA 4.0"
"60629671","1","60678662","","2020-03-11 05:19:59","","0","103","<p>I have a .csv term-document matrix, and I wanna perform some latent dirichlet allocation using gensim in python. However, I'm not particularly familiar with Python <em>or</em> LDA.</p>

<p>I posted in the gensim...forum? I dunno if that's what it's called. The guy that wrote the package responded and had this to say:</p>

<blockquote>
  <p>how big is your term-document CSV matrix?</p>
  
  <p>If it's small enough = fits in RAM, you could: </p>
  
  <p>1) use numpy.loadtxt()
  to load your CSV into an in-memory matrix </p>
  
  <p>2) convert the matrix to a corpus with gensim.matutils.Dense2Corpus() . Check out its documents_columns flag, it lets you switch between document-term and term-document transposition easily. </p>
  
  <p>3) use that corpus to train your LDA model.</p>
</blockquote>

<p>So that leads me to believe that the answer to <a href=""https://stackoverflow.com/questions/27220927/passing-term-document-matrix-to-gensim-lda-model"">this question</a> isn't correct. </p>

<p>It seems like a dictionary is a necessary input to a LDA model; is this not correct? Here's what I have that I think successfully sticks the .csv into a corpus. </p>

<pre><code>file = np.genfromtxt(fname=fPathName, dtype=""int"", delimiter="","", skip_header=True, missing_values="""", filling_values=0)


corpus = gensim.matutils.Dense2Corpus(file, documents_columns=False)
</code></pre>

<p>Any help would be appreciated.</p>

<p>Edit: turns out that a Gensim dictionary and a Python dictionary are not exactly the same things.</p>
","7717976","","68587","","2020-03-14 00:37:59","2020-03-14 00:37:59","Trying to make use of a library to conduct some topic modeling, but it's not going well","<python><gensim><lda><corpus>","1","0","","","","CC BY-SA 4.0"
"60669506","1","","","2020-03-13 11:17:40","","0","86","<p>Now I am trying to use <code>gensim Phrases</code> in order to learn the phrase/special meaning base on my own corpus.</p>

<p>Suppose I have the corpus related to the car brand, by removing the <strong>punctuation</strong> and <strong>stopwords</strong>, <strong>tokenizing the sentence</strong>, eg:</p>

<pre><code>sent1 = 'aston martin is a car brand'
sent2 = 'audi is a car brand'
sent3 = 'bmw is a car brand'
...
</code></pre>

<p>In this way, I would like to use <code>gensim Phrases</code> to learn so that output looks like:</p>

<pre><code>from gensim.models import Phrases
sents = [sent1, sent2, sent3, ...]
sents_stream = [sent.split() for sent in sents]
bigram = Phrases(sents_stream)

for sent in sents:
    print(bigram [sent])

# Ouput should be like:
['aston_martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
...
</code></pre>

<p>However, if a lot of sentences that have a lot of punctuation:</p>

<pre><code>sent1 = 'aston martin is a car brand'
sent2 = 'audi is a car brand'
sent3 = 'bmw is a car brand'
sent4 = 'jaguar, aston martin, mini cooper are british car brand'
sent5 = 'In all brand, I love jaguar, aston martin and mini cooper'
...

</code></pre>

<p>Then the output looks like:</p>

<pre><code>from gensim.models import Phrases
sents = [sent1, sent2, sent3, sent4, sent5, ...]
sents_stream = [sent.split() for sent in sents]
bigram = Phrases(sents_stream)

for sent in sents:
    print(bigram [sent])

# Ouput should be like:
['aston', 'martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
['jaguar', 'aston', 'martin_mini', 'cooper', 'british', 'car', 'brand']
['all', 'brand', 'love', 'jaguar', 'aston', 'martin_mini', 'cooper']
...
</code></pre>

<p>In this case, how should I handle the sentence with lot of punctuation to prevent <code>martin_mini</code> case and make the output looks like:</p>

<pre><code>['aston', 'martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
['jaguar', 'aston_martin', 'mini_cooper', 'british', 'car', 'brand'] # Change
['all', 'brand', 'love', 'jaguar', 'aston_martin', 'mini_cooper'] # Change
...
</code></pre>

<p>Thanks so much for helping!</p>
","10445333","","","","","2020-03-14 01:56:47","Gensim phrase handling sentence with a lot of punctuation","<python><nlp><gensim><phrase>","1","0","","","","CC BY-SA 4.0"
"60638629","1","60639078","","2020-03-11 14:42:06","","0","189","<p>I have a data frame like this</p>

<pre><code>import pandas as pd
from gensim.corpora import Dictionary

tmp = pd.DataFrame({""word"":  [1, 0, 0, 0, 0, 0],
                    ""house"": [0, 1, 0, 0, 0, 0],
                    ""tree"":  [0, 0, 1, 0, 0, 1], # occurred twice
                    ""car"":   [0, 0, 0, 1, 0, 0],
                    ""food"":  [0, 0, 0, 0, 1, 0],
                    ""train"": [0, 0, 0, 0, 0, 1]})
mydict = gensim.corpora.Dictionary()
</code></pre>

<p>from this, I want to create a <code>gensim</code> corpus.</p>

<p>I have tried <code>mycorp = [mydict.doc2bow(col, allow_update=True) for col in tmp.columns]</code> but the resulting corpus seems to not have been properly created:</p>

<blockquote>
  <p>TypeError: doc2bow expects an array of unicode tokens on input, not a single string</p>
</blockquote>

<p>Can someone help me with this? I would like the resulting dictionary to represent the fact that word ""tree"" occurred twice in this data frame (i.e. the sum of the column).</p>
","8839068","","8839068","","2020-03-11 15:15:50","2020-03-11 15:15:50","gensim corpus from sparse matrix","<python><python-3.x><gensim>","1","2","","","","CC BY-SA 4.0"
"69023485","1","69023726","","2021-09-02 02:42:48","","0","44","<p>Trying to use <code>gensim's fasttext</code>, testing the sample code from <code>gensim</code> with a small change of replacing the arguement to <code>corpus_iterable</code></p>
<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html</a></p>
<p><code>gensim_version == 4.0.1</code></p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import common_texts  # some example sentences

print(common_texts[0])
['human', 'interface', 'computer']
print(len(common_texts))
9
model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
model.build_vocab(corpus_iterable=common_texts)
model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=10)
</code></pre>
<p>It works, but is there any way to <code>get the vocab</code> for the model. For example, in <code>Tensorflow Tokenizer</code> there is a <code>word_index</code> which will return <code>all the words</code>. Is there something similar here?</p>
","5927701","","","","","2021-09-02 07:19:41","Gensim fast text get vocab or word index","<machine-learning><nlp><gensim><word-embedding><fasttext>","1","0","","","","CC BY-SA 4.0"
"69111745","1","69124935","","2021-09-09 03:04:01","","0","20","<p>It seems that the <a href=""https://github.com/RaRe-Technologies/gensim"" rel=""nofollow noreferrer"">Gensim's</a> implementation in FastText leads to a smaller model size than <a href=""https://github.com/facebookresearch/fastText"" rel=""nofollow noreferrer"">Facebook's</a> native implementation. With a corpus of 1 million words, the fasttext native model is is 6GB, while the gensim fasttext model size is only 68MB.</p>
<p>Is there any information stored in Facebook's implementation not present in Gensim's implementation?</p>
","5705174","","","","","2021-09-09 21:32:07","Why is gensim FastText model smaller in size than the native Fasttext model by Facebook?","<python><machine-learning><nlp><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"69127120","1","69129076","","2021-09-10 04:02:38","","1","50","
<h4>Problem description</h4>
<p>It seems that the <code>get_latest_training_loss</code> function in <code>fasttext</code> returns only 0. Both gensim <strong>4.1.0</strong> and <strong>4.0.0</strong> do not work.</p>
<pre><code>from gensim.models.callbacks import CallbackAny2Vec
from pprint import pprint as print
from gensim.models.fasttext import FastText
from gensim.test.utils import datapath

class callback(CallbackAny2Vec):
    '''Callback to print loss after each epoch.'''

    def __init__(self):
        self.epoch = 0

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        print('Loss after epoch {}: {}'.format(self.epoch, loss))
        self.epoch += 1

# Set file names for train and test data
corpus_file = datapath('lee_background.cor')

model = FastText(vector_size=100, callbacks=[callback()])

# build the vocabulary
model.build_vocab(corpus_file=corpus_file)

# train the model
model.train(
    corpus_file=corpus_file, epochs=model.epochs,
    total_examples=model.corpus_count, total_words=model.corpus_total_words,
    callbacks=model.callbacks, compute_loss=True,
)

print(model)
</code></pre>
<pre><code>'Loss after epoch 0: 0.0'
'Loss after epoch 1: 0.0'
'Loss after epoch 2: 0.0'
'Loss after epoch 3: 0.0'
'Loss after epoch 4: 0.0'
</code></pre>
<p><strong>If currently FastText does not support <code>get_latest_training_loss</code>, the documentation here needs to be removed:</strong></p>
<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.get_latest_training_loss"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.get_latest_training_loss</a></p>
<h4>Versions</h4>
<p>I have tried this in three different environments and neither of them works.</p>
<p><strong>First environment:</strong></p>
<pre><code>[GCC 9.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
Linux-3.10.0-1160.36.2.el7.x86_64-x86_64-with-glibc2.17
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:39:48)
[GCC 9.3.0]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.21.2
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
gensim 4.1.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
<p><strong>Second environment:</strong></p>
<pre><code>Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
macOS-10.16-x86_64-i386-64bit
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.20.3
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
gensim 4.1.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
<p><strong>Third environment:</strong></p>
<pre><code>Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
macOS-10.16-x86_64-i386-64bit
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.20.3
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
/Users/jinhuawang/miniconda3/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package &lt;https://pypi.org/project/python-Levenshtein/&gt; is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  warnings.warn(msg)
gensim 4.0.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
","5705174","","","","","2021-09-10 08:02:09","Gensim fasttext cannot get latest training loss","<python><nlp><gensim><word2vec><fasttext>","1","0","","","","CC BY-SA 4.0"
"44613624","1","","","2017-06-18 09:32:35","","0","1463","<p>after train the model, I use  infer_vector() to get the vector successfully.
but after I save the model and load again, error appears as follows:</p>

<pre><code>print ""infer:"", model.infer_vector(sents[0]).tolist()
File ""/Users/zhangweimin/anaconda/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 752, in infer_vector
    doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
File ""gensim/models/doc2vec_inner.pyx"", line 426, in gensim.models.doc2vec_inner.train_document_dm (./gensim/models/doc2vec_inner.c:5401)
TypeError: object of type 'NoneType' has no len()
</code></pre>

<p>the whole code is:`    </p>

<pre><code>model = train_d2v(labeled_docs, model_file, word_file, 3)

# OK
print ""before infer:"", model.infer_vector(sents[0]).tolist()

model = Doc2Vec.load(model_file)

print ""sents[0]:"", sents[0]
print ""type:"", type(model)
print ""infer:"", model.infer_vector(sents[0]).tolist() #ERROR`
</code></pre>
","5973013","","4909087","","2017-06-18 09:34:38","2017-06-20 20:09:07","How to save gensim doc2vec model","<python><gensim><doc2vec>","1","6","","","","CC BY-SA 3.0"
"69157848","1","69159563","","2021-09-13 05:50:31","","1","24","<p>I read several posts(<a href=""https://towardsdatascience.com/topic-modeling-quora-questions-with-lda-nmf-aff8dce5e1dd"" rel=""nofollow noreferrer"">here</a> and <a href=""http://www.cse.chalmers.se/%7Erichajo/dit862/L13/LDA%20with%20gensim%20(small%20example).html"" rel=""nofollow noreferrer"">here</a>) online about LDA topic modeling. All of them only use uni-grams. I would like to know why bi-grams and tri-grams are not used for LDA topic modeling?</p>
","10783663","","10783663","","2021-09-13 21:11:32","2021-09-13 21:11:32","Should bi-gram and tri-gram be used in LDA topic modeling?","<nlp><gensim><topic-modeling><n-gram>","1","1","","2021-09-13 18:50:07","","CC BY-SA 4.0"
"49021389","1","49026846","","2018-02-28 03:14:02","","6","824","<p>I used gensim fit a doc2vec model, with tagged document (length>10) as training data. The target is to get doc vectors of all training docs, but only 10 vectors can be found in model.docvecs.</p>

<p>The example of training data (length>10)</p>

<pre><code>docs = ['This is a sentence', 'This is another sentence', ....]
</code></pre>

<p>with some pre-treatment</p>

<pre><code>doc_=[d.strip().split("" "") for d in doc]
doc_tagged = []
for i in range(len(doc_)):
  tagd = TaggedDocument(doc_[i],str(i))
  doc_tagged.append(tagd)
</code></pre>

<p>tagged docs</p>

<pre><code>TaggedDocument(words=array(['a', 'b', 'c', ..., ],
  dtype='&lt;U32'), tags='117')
</code></pre>

<p>fit a doc2vec model</p>

<pre><code>model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)
model.build_vocab(doc_tagged)
model.train(doc_tagged, total_examples= model.corpus_count, epochs= model.iter)
</code></pre>

<p>then i get the final model</p>

<pre><code>len(model.docvecs)
</code></pre>

<p>the result is 10...</p>

<p>I tried other datasets (length>100, 1000) and got same result of <code>len(model.docvecs)</code>.
So, my question is:
How to use model.docvecs to get full vectors? (without using <code>model.infer_vector</code>)
Is <code>model.docvecs</code> designed to provide all training docvecs?</p>
","4313992","","712995","","2018-02-28 13:39:00","2018-02-28 13:39:00","Doc2vec: Only 10 docvecs in gensim doc2vec model?","<machine-learning><nlp><word2vec><gensim><doc2vec>","1","0","1","","","CC BY-SA 3.0"
"60935894","1","","","2020-03-30 17:16:39","","0","213","<p>In general, I have the following code:</p>

<pre class=""lang-py prettyprint-override""><code>import gensim
import gensim.downloader as api

model = api.load('word2vec-google-news-300')
...
matrix = model.similarity_matrix(dictionary, tfidf=None, threshold=0.0,
    exponent=2.0, nonzero_limit=100)
...
</code></pre>

<p>I am using the following dictionary for the variable dictioanry:</p>

<pre><code>4
0   apple   2
1   eaten   1
10  google  1
2   half    1
3   iphone  2
4   is  2
7   lost    2
11  missing 1
5   model   1
8   my  2
9   phone   2
12  pixel   1
6   plus    1
13  somewhere   1
</code></pre>

<p>So when I compute the similarity between 'phone' and 'iphone', I get a nonzero value.</p>

<pre class=""lang-py prettyprint-override""><code>sim_match.model.similarity('phone','iphone')
&gt;&gt; 0.33964303
</code></pre>

<p>But at the index 3,9 of matrix (the similarity of 'phone' and 'iphone' in the similarity matrix), I get a zero entry.</p>

<pre class=""lang-py prettyprint-override""><code>matrix[3,9]
&gt;&gt; 0.0
</code></pre>

<p>As a matter of fact, almost all entries are zero.</p>

<pre class=""lang-py prettyprint-override""><code>print(matrix)
&gt;&gt;  (0, 0)        1.0
&gt;&gt;  (1, 1)        1.0
&gt;&gt;  (2, 2)        1.0
&gt;&gt;  (3, 3)        1.0
&gt;&gt;  (4, 4)        1.0
&gt;&gt;  (5, 5)        1.0
&gt;&gt;  (6, 6)        1.0
&gt;&gt;  (7, 7)        1.0
&gt;&gt;  (11, 7)       0.21569395
&gt;&gt;  (8, 8)        1.0
&gt;&gt;  (9, 9)        1.0
&gt;&gt;  (10, 10)      1.0
&gt;&gt;  (11, 11)      1.0
&gt;&gt;  (7, 11)       0.21569395
&gt;&gt;  (12, 12)      1.0
&gt;&gt;  (13, 13)      1.0
</code></pre>

<p>Only the words 'lost' and 'missing' have a nonzero similarity and aren't equivalent.</p>

<p>I do know that the matrix is a numpy sparse matrix. Maybe its the parameters I'm passing, but I am confused as to why most of the elements in the matrix are zero. The parameters I passed, based on the documentation, seem that they shouldn't be causing so many entries to be 0. I do note that the nonzero_limit is 100, but shouldn't there be a hundred nonzero entries?</p>

<p>Is there anyway I could fix or get around this?</p>
","11620572","","","","","2020-03-30 17:16:39","Why would the similarity matrix of a gensim Word2VecKeyedVectors be very sparse compared to to the model itself?","<python><gensim>","0","14","1","","","CC BY-SA 4.0"
"60644150","1","","","2020-03-11 20:40:14","","1","154","<p>I am trying to train a word2vec model with the <a href=""http://www.english-corpora.org/coha/"" rel=""nofollow noreferrer"">COHA corpus</a> by using pre-computed bigram counts co-occurrence counts that the corpus' author makes available <a href=""https://www.ngrams.info/download_coha.asp"" rel=""nofollow noreferrer"">here</a>.</p>

<p>How can I achieve that using <a href=""https://radimrehurek.com/gensim/index.html"" rel=""nofollow noreferrer"">gensim</a>?</p>
","9582881","","","","","2020-03-12 20:06:59","How can you train a word2vec in gensim from a list of co-occurrence (bigram) counts?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"69073499","1","69083010","","2021-09-06 11:05:05","","1","30","<p>I am trying to find out how I could train a word2vec model in a federated way.</p>
<p>The data would be split into multiple parts, e.g. 4 &quot;institutions&quot;, and I would like to train the word2vec model on the data from each institution separately. They key restraint here is that the data from the institutions can not be moved to another location, so it can never be trained in a centralized way.</p>
<p>I know that it is possible to train the word2vec model iteratively, such that the data from the first institution is read and used to train &amp; update the word2vec model, but I wonder if its possible to do it simultaneously on all four institutions and then, for example, to merge all four word2vec models into one model.</p>
<p>Any ideas or suggestions are appreciated</p>
","12078469","","","","","2021-09-07 05:53:42","Can a gensim word2vec model be trained in a federated way?","<python><gensim><word2vec><text-processing><federated>","1","1","1","","","CC BY-SA 4.0"
"69143491","1","","","2021-09-11 13:31:40","","2","40","<p>I'm trying to import gensim on Jupyter Lab, but it gives me an error as below.
I've tried updating or downgrading related libraries (numpy, cython, gensim) but still no luck. Has anyone had the same issue but solved it?</p>
<pre><code>/opt/conda/lib/python3.9/site-packages/gensim/__init__.py in &lt;module&gt;
      3 &quot;&quot;&quot;
      4 
----&gt; 5 from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
      6 import logging
      7 

/opt/conda/lib/python3.9/site-packages/gensim/corpora/__init__.py in &lt;module&gt;
      4 
      5 # bring corpus classes directly into package namespace, to save some typing
----&gt; 6 from .indexedcorpus import IndexedCorpus  # noqa:F401 must appear before the other classes
      7 
      8 from .mmcorpus import MmCorpus  # noqa:F401

/opt/conda/lib/python3.9/site-packages/gensim/corpora/indexedcorpus.py in &lt;module&gt;
     13 import numpy
     14 
---&gt; 15 from gensim import interfaces, utils
     16 
     17 logger = logging.getLogger(__name__)

/opt/conda/lib/python3.9/site-packages/gensim/interfaces.py in &lt;module&gt;
     19 import logging
     20 
---&gt; 21 from gensim import utils, matutils
     22 from six.moves import range
     23 

/opt/conda/lib/python3.9/site-packages/gensim/matutils.py in &lt;module&gt;
   1102 try:
   1103     # try to load fast, cythonized code if possible
-&gt; 1104     from gensim._matutils import logsumexp, mean_absolute_difference, dirichlet_expectation
   1105 
   1106 except ImportError:

/opt/conda/lib/python3.9/site-packages/gensim/_matutils.cpython-39-x86_64-linux-gnu.so in init gensim._matutils()

AttributeError: type object 'gensim._matutils.array' has no attribute '__reduce_cython__'
</code></pre>
","16531479","","16531479","","2021-09-11 13:42:48","2021-09-11 13:42:48","'gensim._matutils.array' has no attribute '__reduce_cython__'","<python><cython><gensim>","0","0","","","","CC BY-SA 4.0"
"60938299","1","60940902","","2020-03-30 19:42:38","","0","500","<p>Does <code>sample= 0</code> in Gensim word2vec mean that no downsampling is being used during my training? The documentation says just that </p>

<blockquote>
  <p>""useful range is (0, 1e-5)""</p>
</blockquote>

<p>However putting the threshold to 0 would cause P(wi) to be equal to 1, meaning that no word would be discarded, am I understanding it right or not? </p>

<p>I'm working on a relatively small dataset of 7597 Facebook posts (18945 words) and my embeddings perform far better using <code>sample= 0</code>rather than anything else within the recommended range. Is there any particular reason? Text size? </p>
","11712834","","","","","2020-03-30 23:00:28","Gensim word2vec downsampling sample=0","<python><math><gensim><word-embedding><subsampling>","1","0","","","","CC BY-SA 4.0"
"60587089","1","","","2020-03-08 11:40:00","","0","95","<p>I am going through the <code>gensim</code> LDA implementation and it says it needs a corpus and a dictionary of the corpus?</p>
<p><a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a></p>
<p>What is the reason for this?</p>
","12195215","","6573902","","2021-01-25 14:57:01","2021-01-25 14:57:01","Why does the LDA gensim implemention need the corpus and a dictionary?","<python><nlp><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"60672361","1","60673963","","2020-03-13 14:34:59","","0","1809","<p>I have used gensims word embeddings to find vectors of each word. Then I used K-means to find clusters of word. There are close to <code>10,000</code> tokens/words and I want to plot them.</p>

<p>I want to plot the result in the following way:</p>

<ul>
<li>Annotate points with name of <code>words</code></li>
<li>Different color for clusters</li>
</ul>

<p>Here is what I have done. </p>

<pre><code>tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=500)#, random_state=13)


def tsne_plot(data):
    ""Creates and TSNE model and plots it""

    data=data.sample(n = 500).reset_index()
    word=data[""word""]
    cluster=data[""clusters""]
    data=data.drop([""clusters"",""word""],axis=1)

    X = tsne.fit_transform(data)

    plt.figure(figsize=(48, 48)) 
    for i in range(len(X)):
        plt.scatter(X[:,0][i],X[:,1][i],c=cluster[i])
        plt.annotate(word[i],
                     xy=(X[:,0][i],X[:,1][i]),
                     xytext=(3, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.show()

tsne_plot(data)
</code></pre>

<p>Though it's annotating the <code>words</code> but failing to color different groups/clusters?</p>

<p>Anyother other approach which annoates with word anmes and colors different clusters?</p>
","9907733","","","","","2020-03-14 12:46:30","How to plot the output of k-means clustering of word embedding using python?","<python-3.x><matplotlib><gensim>","1","0","","","","CC BY-SA 4.0"
"66057115","1","66057464","","2021-02-05 03:20:37","","0","31","<p>I searched gensim.matutils.Dense2Corpus documentation but I do not find what does True/False value for documents_columns do.
Ex: gensim.matutils.Dense2Corpus(input, documents_columns=True)</p>
","9352430","","","","","2021-02-05 04:10:08","What does documents_columns paramter in Sparse2Corpus do?","<python><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"60682634","1","60686815","","2020-03-14 12:18:28","","1","502","<p>I am trying to load a trained fasttext model using gensim. The model has been trained on some data. Earlier, I have used <code>model.save()</code> with a extension of <code>.bin</code> to use it later. After the training process and saving the model using <code>model.save</code> in <code>.bin</code> format, generates 3 files respectively. They are:</p>

<p>1) .bin  </p>

<p>2) bin.trainable vectors_ngrams_lockf</p>

<p>3) bin.wv.vectors_ngrams </p>

<p>Now I am unable to load the trained binary file (.bin).  </p>

<p>But I don't understand why I am getting a error named:</p>

<blockquote>
  <p>raise NotImplementedError(""Supervised fastText models are not supported"")
  NotImplementedError: Supervised fastText models are not supported</p>
</blockquote>

<p>After going through many blogs, peoples have suggested that <code>gensim</code> does not supports supervised training. It's fine. My question is how can I be able to load the trained binary model. Shall I need to train the model differently.</p>

<p>Any help is appreciated.</p>

<p>What I have tried after the training process: </p>

<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from gensim.models import FastText, fasttext
model = FastText.load_fasttext_format('m1.bin')
print(model)
</code></pre>
","3966705","","3966705","","2020-03-14 13:48:18","2020-03-14 20:00:51","Issues while loading a trained fasttext model using gensim","<python><python-3.x><gensim><word-embedding><fasttext>","1","0","","","","CC BY-SA 4.0"
"34831551","1","34849797","","2016-01-16 20:05:51","","13","6660","<p>In <a href=""https://stackoverflow.com/questions/15067734/lda-model-generates-different-topics-everytime-i-train-on-the-same-corpus"">LDA model generates different topics everytime i train on the same corpus</a> , by setting the <code>np.random.seed(0)</code>, the LDA model will always be initialized and trained in exactly the same way. </p>

<p><strong>Is it the same for the Word2Vec models from <code>gensim</code>? By setting the random seed to a constant, would the different run on the same dataset produce the same model?</strong></p>

<p>But strangely, it's already giving me the same vector at different instances. </p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
&gt;&gt;&gt; exit()
alvas@ubi:~$ python
Python 2.7.11 (default, Dec 15 2015, 16:46:19) 
[GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; word0 = sentences[0][0]
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
</code></pre>

<p><strong>Is it true then that the default random seed is fixed?</strong> If so, what is the default random seed number? Or is it because I'm testing on a small dataset? </p>

<p>If it's true that the the random seed is fixed and different runs on the same data returns the same vectors, a link to a canonical code or documentation would be much appreciated.  </p>
","610569","","-1","","2017-05-23 12:18:32","2021-07-06 23:33:27","Ensure the gensim generate the same Word2Vec model for different runs on the same data","<python><random><gensim><word2vec><word-embedding>","5","0","5","","","CC BY-SA 3.0"
"69051015","1","","","2021-09-03 22:13:14","","0","15","<p>I want to know whether MatrixSimilarity and cossim return the same result when we want to calculate document similarity given a trained LDA model. I get confused when searching questions about these topics where someone used MatrixSimilarity and the other used cossim. Are there any sources that I can read to know more about this ? Thank you!</p>
","13615736","","","","","2021-09-04 00:30:46","similarities.MatrixSimilarity vs matutils.cossim in Python Gensim","<gensim>","0","0","","","","CC BY-SA 4.0"
"69161906","1","","","2021-09-13 11:32:31","","0","24","<p>I would like to identify compound phrases in one corpus (e.g. <code>(w_1, w_2) in Corpus 1</code>) which not only appear significantly more often than their constituents (e.g. <code>(w_1),(w_2) in Corpus 1</code>) within the corpus but also more than they do in a second corpus <code>(e.g. (w_1, w_2) in Corpus 2)</code>. Consider the following informal example. I have the two corpora each consisting of a set of documents:</p>
<ol>
<li><code>[['i', 'live', 'in', 'new', 'york'], ['new', 'york', 'is', 'busy'], ...]</code></li>
<li><code>[['los', 'angeles', 'is', 'sunny'], ['los', 'angeles', 'has', 'bad', 'traffic'], ...]</code>.</li>
</ol>
<p>In this case, I would like <code>new_york</code> to be detected as a compound phrase. However, when corpus 2 is replaced by</p>
<ol start=""2"">
<li><code>[['i', 'go', 'to', 'new', york'], ['i', 'like', 'new', 'york'], ...]</code>,</li>
</ol>
<p>I would like <code>new_york</code> to be relatively disregarded.</p>
<p>I could just use a ratio between n-gram scores between corresponding phrases in corpora, but I don't see how to scale to general n. Normally, phrase detection for n-grams with <code>n&gt;2</code> is done by recursing on <code>n</code> and gradually inserting compound phrases into the documents by thresholding a score function. This insures that at step <code>n</code>, if you want to score the <code>n</code>-gram <code>(w_1, ..., w_n)</code>, then you can always normalize by the constituent <code>m</code>-grams for <code>m&lt;n</code>. But with a different corpus, these are not guaranteed to appear.</p>
<p>A reference to the literature or a relevant hack will be appreciated.</p>
","3703379","","","","","2021-09-13 11:32:31","Identifying phrases which contrast two corpora","<nlp><gensim><n-gram><linguistics><phrase>","0","2","","","","CC BY-SA 4.0"
"51265111","1","58775904","","2018-07-10 12:23:27","","1","344","<p>I'm attempting to compare a tagged document consisting of a list of words to individual tags from a list of tags.</p>

<p>My code is as follows:</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
from gensim import similarities,corpora,models
import Load

documents = Load.get_doc('docs')

data = Doc2Vec.load('vectorised.model')

print('Data Loading finished')

tags = [['word1'],['word2'],['word3'],['word4'],['word5']]

tag_vectors = []

data.n_similarity(tags[0],documents[1])
</code></pre>

<p>The issue i'm having is running:</p>

<pre><code>data.n_similarity(tags[0],documents[1])
</code></pre>

<p>feeds back KeyError: ""word 'otherword' not in vocabulary</p>

<p>I want to get the similarity between the taggeddocument and the tag itself,
so my question is what do I need to change in my code so it checks correctly and gives back a similarity value?</p>

<p>n.b. I've replaced the actual words here with placeholders</p>
","8893928","","","","","2019-11-09 02:30:16","Gensim n_similarity word not in vocabulary","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"63637245","1","63638897","","2020-08-28 15:59:06","","1","1147","<p>I am new to deep learning and I am trying to play with a pretrained word embedding model from a <a href=""https://www.cse.iitb.ac.in/%7Epb/papers/sltu-ccurl20-il-we.pdf"" rel=""nofollow noreferrer"">paper</a>. I  downloaded the following files:</p>
<p>1)sa-d300-m2-fasttext.model</p>
<p>2)sa-d300-m2-fasttext.model.trainables.syn1neg.npy</p>
<p>3)sa-d300-m2-fasttext.model.trainables.vectors_ngrams_lockf.npy</p>
<p>4)sa-d300-m2-fasttext.model.wv.vectors.npy</p>
<p>5)sa-d300-m2-fasttext.model.wv.vectors_ngrams.npy</p>
<p>6)sa-d300-m2-fasttext.model.wv.vectors_vocab.npy</p>
<p>If in  case these details are needed
sa - sanskrit
d300 - embedding dimension
fastText - fastText</p>
<p>I dont have a prior experience with gensim, how can load the model into gensim or into tensorflow.</p>
<p>I tried</p>
<pre><code>from gensim.models.wrappers import FastText
FastText.load_fasttext_format('/content/sa/300/fasttext/sa-d300-m2-fasttext.model.wv.vectors_ngrams.npy')
</code></pre>
<blockquote>
<p>FileNotFoundError: [Errno 2] No such file or directory: '/content/sa/300/fasttext/sa-d300-m2-fasttext.model.wv.vectors_ngrams.npy.bin'</p>
</blockquote>
","10531776","","130288","","2020-08-28 18:06:55","2020-08-29 03:33:16","How to load pre-trained fastText model in gensim with .npy extension","<gensim><pre-trained-model><fasttext>","1","0","","","","CC BY-SA 4.0"
"57107945","1","","","2019-07-19 07:53:23","","1","240","<p>When trying to load  <code>GoogleNews-vectors-negative300.bin</code> with pytorch <code>Vector</code> <a href=""https://torchtext.readthedocs.io/en/latest/vocab.html#vectors"" rel=""nofollow noreferrer"">struct</a> I am getting </p>

<blockquote>
  <p>ValueError: could not convert string to float: b'\x00\x00\x94:\x00\x00k\xba\x00\x00\x</p>
</blockquote>

<p>I have tried this <a href=""https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings"">post</a> (@robodasha) but without success. My goal is to build a vocabulary with the loaded embedding using <code>build_vocab</code> Any suggestions? </p>
","2551431","","2551431","","2019-10-04 16:25:34","2019-10-04 16:26:24","Building dictionary with GoogleNews-vectors-negative300.bin returns ValueError: could not convert string to float","<python><pytorch><gensim>","1","0","","","","CC BY-SA 4.0"
"69234978","1","69334450","","2021-09-18 13:11:18","","4","109","<p>Following <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim word2vec embedding tutorial</a>, I have trained a simple word2vec model:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models import Word2Vec
model = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)
model.save(&quot;/content/word2vec.model&quot;)
</code></pre>
<p>I would like to visualize it <a href=""https://projector.tensorflow.org/"" rel=""nofollow noreferrer"">using the Embedding Projector in TensorBoard</a>. <a href=""https://radimrehurek.com/gensim/scripts/word2vec2tensor.html"" rel=""nofollow noreferrer"">There is another straightforward tutorial in gensim documentation</a>. I did the following in Colab:</p>
<pre><code>!python3 -m gensim.scripts.word2vec2tensor -i /content/word2vec.model -o /content/my_model

Traceback (most recent call last):
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/scripts/word2vec2tensor.py&quot;, line 94, in &lt;module&gt;
    word2vec2tensor(args.input, args.output, args.binary)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/scripts/word2vec2tensor.py&quot;, line 68, in word2vec2tensor
    model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=binary)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py&quot;, line 1438, in load_word2vec_format
    limit=limit, datatype=datatype)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py&quot;, line 172, in _load_word2vec_format
    header = utils.to_unicode(fin.readline(), encoding=encoding)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/utils.py&quot;, line 355, in any2unicode
    return unicode(text, encoding, errors=errors)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
<p>Please note that I did check first this <a href=""https://stackoverflow.com/questions/50492676/visualize-gensim-word2vec-embeddings-in-tensorboard-projector"">exact same question from 2018</a> - but the accepted answer no longer works as both in gensim and tensorflow have been updated so I considered it was worth asking again in Q4 2021.</p>
","7762646","","7762646","","2021-09-19 21:40:26","2021-09-27 18:40:39","How to visualize Gensim Word2vec Embeddings in Tensorboard Projector","<python><tensorflow><gensim><word2vec><tensorboard>","1","2","","","","CC BY-SA 4.0"
"69258013","1","69258490","","2021-09-20 16:37:46","","0","19","<p>I have used <strong>Gensim</strong> library to find the similarity between a sentence against a collection of paragraphs, a dataset of texts. I have used Cosine similarity, Soft cosine similarity and Mover measures separately. Gensim returns a list of items including <em><strong>docid</strong></em> and <em><strong>similarity score</strong></em>. For Cosine similarity and Soft cosine similarity, I guess the similarity score is the cosine between the vectors. Am I right?</p>
<p>In Gensim documents, they wrote it is the semantic relatedness, and no extra explanation. I have search a lot, but did not find any answer. Any help please</p>
","16959157","","","","","2021-09-20 17:18:54","What does Similarity Score mean in gensim?","<python><text><gensim><sentence-similarity>","1","0","","","","CC BY-SA 4.0"
"69247049","1","69248527","","2021-09-19 21:01:40","","0","32","<p>I am training word embeddings using <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim Word2Vec</a> model with a multi-million sentence corpus that is made of 3 million unique tokens with <code>max_vocab_size = 32_000</code>.</p>
<p>Even though I set <code>min_count = 1</code>, model creates a vocabulary of far less than 32_000. When I use a subset of the corpus, vocabulary size increases!</p>
<p>In order to troubleshoot, I set up an experiment where I control the size of vocabulary with different sized subcorpus. The size of the vocabulary flactuates!</p>
<p>You can re-produce with the code below:</p>
<pre><code>import string
import numpy as np
from gensim.models import Word2Vec

letters = list(string.ascii_lowercase)

# creating toy sentences
sentences = []
number_of_sentences = 100_000

for _ in range(number_of_sentences):
    number_of_tokens = np.random.randint(1, 15, 1)[0]
    sentence = []
    for i in range(number_of_tokens):
        token = &quot;&quot;
        len_of_token = np.random.randint(1, 5, 1)[0]
        for j in range(len_of_token):
            token += np.random.choice(letters)
        sentence.append(token)
    sentences.append(sentence)

# Sanity check to ensure that input data is a list of list of strings(tokens)
for _ in range(4):
    print(np.random.choice(sentences))

# collecting some statistics about tokens
flattened = []
for sublist in sentences:
    for item in sublist:
        flattened.append(item)
        
unique_tokens = {}
for token in flattened:
    if token not in unique_tokens:
        unique_tokens[token] = len(unique_tokens)

print('Number of tokens:', f'{len(flattened):,}')
print('Number of unique tokens:', f'{len(unique_tokens):,}')


# gensim model
vocab_size = 32_000
min_count = 1
collected_data = []
for num_sentence in range(5_000, number_of_sentences + 5_000, 5_000):
    model = Word2Vec(min_count=min_count, max_vocab_size= vocab_size)
    model.build_vocab(sentences[:num_sentence])

    collected_data.append((num_sentence, len(model.wv.key_to_index)))

for duo in collected_data:
    print('Vocab size of', duo[1], 'for', duo[0], 'number of sentences!')
</code></pre>
<p>Output:</p>
<pre><code>['cpi', 'bog', 'df', 'tgi', 'xck', 'kkh', 'ktw', 'ay']
['z', 'h', 'w', 'jek', 'w', 'dqm', 'wfb', 'agq', 'egrg']
['kgwb', 'lahf', 'kzx', 'd', 'qdok', 'xka', 'hbiz', 'bjo', 'fvk', 'j', 'hx']
['old', 'c', 'ik', 'n', 'e', 'n', 'o', 'r', 'ehx', 'dlud', 'd']

Number of tokens: 748,383
Number of unique tokens: 171,485

Vocab size of 16929 for 5000 number of sentences!
Vocab size of 30314 for 10000 number of sentences!
Vocab size of 19017 for 15000 number of sentences!
Vocab size of 31394 for 20000 number of sentences!
Vocab size of 19564 for 25000 number of sentences!
Vocab size of 31831 for 30000 number of sentences!
Vocab size of 19543 for 35000 number of sentences!
Vocab size of 31744 for 40000 number of sentences!
Vocab size of 19536 for 45000 number of sentences!
Vocab size of 31642 for 50000 number of sentences!
Vocab size of 18806 for 55000 number of sentences!
Vocab size of 31255 for 60000 number of sentences!
Vocab size of 18497 for 65000 number of sentences!
Vocab size of 31166 for 70000 number of sentences!
Vocab size of 18142 for 75000 number of sentences!
Vocab size of 30886 for 80000 number of sentences!
Vocab size of 17693 for 85000 number of sentences!
Vocab size of 30390 for 90000 number of sentences!
Vocab size of 17007 for 95000 number of sentences!
Vocab size of 30196 for 100000 number of sentences!
</code></pre>
<p>I tried increasing <code>min_count</code> but it did not help this flactuation of vocabulary size. What am I missing?</p>
","4505301","","130288","","2021-09-20 02:31:14","2021-09-20 02:31:14","gensim word2vec vocabulary size fluctuates up & down as corpus grows despite `max_vocab_size` setting","<python><nlp><gensim><word2vec><word-embedding>","1","2","","","","CC BY-SA 4.0"
"69257594","1","69258370","","2021-09-20 16:04:01","","2","27","<p>What <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#training-hyperparameters"" rel=""nofollow noreferrer"">parameters</a> when training a gensim fasttext model have the biggest effect on the resulting models' size in memory?</p>
<p>gojomos answer to <a href=""https://stackoverflow.com/questions/58407649/fasttext-bin-file-cannot-fit-in-memory-even-though-i-have-enough-ram"">this question</a> mentions ways to reduce a model's size during training, apart from reducing embedding dimensionality.</p>
<p>There seem a few parameters that might have an effect: thresholds for including words in the vocabulary especially. Do the other parameters also influence model size, for example ngram range, and which parameters have the largest effect?</p>
<p>I hope this is not too lazy of a question :-)</p>
","4726173","","","","","2021-09-20 17:08:29","How to reduce RAM consumption of gensim fasttext model through training parameters?","<python><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"69261114","1","","","2021-09-20 21:37:26","","0","21","<p>Background:</p>
<p>I'm processing text (dataset with 1000 documents - applying Doc2Vec using Gensim lib), at the end I have a 300 dimension matrix for each doc.</p>
<p>So I did a cluster using K-means based on this model.</p>
<p>input:</p>
<pre><code>from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

kmeans_model = KMeans(n_clusters=5, init='k-means++', max_iter=100) 

X = kmeans_model.fit(model.docvecs.vectors_docs)

labels = kmeans_model.labels_.tolist()

l = kmeans_model.fit_predict(model.docvecs.vectors_docs)

pca = PCA(n_components=2).fit(model.docvecs.vectors_docs)

datapoint = pca.transform(model.docvecs.vectors_docs)

plt.figure(figsize=(12,12))

label1 = ['red', 'pink', 'lightgreen', 'lightblue', 'cyan']
color = [label1[i] for i in labels]

plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)

centroids = kmeans_model.cluster_centers_
centroidpoint = pca.transform(centroids)

plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='black')

plt.show()
</code></pre>
<p>output:</p>
<p><a href=""https://i.stack.imgur.com/z8evq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z8evq.png"" alt=""enter image description here"" /></a></p>
<p>My dataset has a label column, where each document has a label (class 0, 1, 2, 3 or 4).
I would like to check if this clustering is equal to those labels. I mean, I want to check if a document labeled as class 1 is grouped by k-means with others from the same class, for example.
So I was thinking of using a different symbol for each class, like, in this plot all documents are represented as dots, can I do the plot with a different symbol for each class? Is that a good way to check this? (visually)</p>
<p>Also, how could I check the accuracy of this? To see how much of those 1000 documents were clustered with the others that have the same value in my dataset (column: df['Classes'])</p>
","2465455","","","","","2021-09-20 23:39:17","How to identify labels in a plot of a kmeans cluster?","<python><matplotlib><cluster-analysis><k-means><gensim>","0","0","","2021-09-20 23:39:01","","CC BY-SA 4.0"
"69269058","1","","","2021-09-21 12:40:11","","1","27","<p>I am currently training an LDA model in gensim and would like to know if the model is converging or not. Unfortunately I get no logging output. The relevant code looks like this:</p>
<pre><code>from gensim.models.ldamodel import LdaModel
import logging
logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)
logging.root.level = logging.INFO
model = LdaModel(corpus=corpus, id2word=id2word, num_topics=6, random_state=0, chunksize=100, alpha='auto', per_word_topics=True, iterations = 100, passes = 2, eval_every=1)
</code></pre>
<p>When I use this: <code>logging.info(&quot;hello world&quot;)</code>
this works fine however in my jupyter notebook. If anybody had an idea how I could get the logging output from gensim, I'd be really gratefull.</p>
","16965842","","","","","2021-09-21 12:40:11","Gensim LDA logging not displaying","<python><logging><jupyter-notebook><gensim>","0","2","","","","CC BY-SA 4.0"
"69360816","1","69361463","","2021-09-28 11:22:10","","0","43","<p>I would like to remove a list of stopwords, namely the ones in</p>
<pre><code>from gensim.parsing.preprocessing import STOPWORDS
print(STOPWORDS)
</code></pre>
<p>In gensim, this should be pretty straightforward with <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/parsing/preprocessing.py"" rel=""nofollow noreferrer""><code>remove_stopwords </code>function</a>.</p>
<p>My code to read the text and remove the stopwords is the following:</p>
<pre><code>def read_text(text_path):
  text = []
  with open(text_path) as file:
    lines = file.readlines()
    for index, line in enumerate(lines):
      text.append(simple_preprocess(remove_stopwords(line)))
  return text

text = read_text('/content/text.txt')
text =  [x for x in text if x]
text[:3]
</code></pre>
<p>This is the output I get that contains words such as &quot;we&quot; or &quot;however&quot; which should have been removed from the <a href=""https://drive.google.com/file/d/1JKIPRb9y6XK7dnVybfwm5Yoz_lBV7mLP/view?usp=sharing"" rel=""nofollow noreferrer"">original text</a> though for instance &quot;the&quot; has been correctly removed from the first setence. I am very confused... what am I missing here?</p>
<pre><code>[['clinical', 'guidelines', 'management', 'ibd'],
 ['polygenetic',
  'risk',
  'scores',
  'add',
  'predictive',
  'power',
  'clinical',
  'models',
  'response',
  'anti',
  'tnfŒ±',
  'therapy',
  'inflammatory',
  'bowel',
  'disease'],
 ['anti',
  'tumour',
  'necrosis',
  'factor',
  'alpha',
  'tnfŒ±',
  'therapy',
  'widely',
  'management',
  'crohn',
  'disease',
  'cd',
  'ulcerative',
  'colitis',
  'uc',
  'however',
  'patients',
  'respond',
  'induction',
  'therapy',
  'patients',
  'lose',
  'response',
  'time',
  'to',
  'aid',
  'patient',
  'stratification',
  'polygenetic',
  'risk',
  'scores',
  'identified',
  'predictors',
  'response',
  'anti',
  'tnfŒ±',
  'therapy',
  'we',
  'aimed',
  'replicate',
  'association',
  'polygenetic',
  'risk',
  'scores',
  'response',
  'anti',
  'tnfŒ±',
  'therapy',
  'independent',
  'cohort',
  'patients',
  'establish',
  'clinical',
  'validity']]
</code></pre>
<p><strong>Text</strong> (complete file available <a href=""https://drive.google.com/file/d/1JKIPRb9y6XK7dnVybfwm5Yoz_lBV7mLP/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>)</p>
<p>Clinical Guidelines for the Management of IBD.</p>
<p>Polygenetic risk scores do not add predictive power to clinical models for response to anti-TNFŒ± therapy in inflammatory bowel disease.
Anti-tumour necrosis factor alpha (TNFŒ±) therapy is widely used in the management of Crohn's disease (CD) and ulcerative colitis (UC). However, up to a third of patients do not respond to induction therapy and another third of patients lose response over time. To aid patient stratification, polygenetic risk scores have been identified as predictors of response to anti-TNFŒ± therapy. We aimed to replicate the association between polygenetic risk scores and response to anti-TNFŒ± therapy in an independent cohort of patients, to establish its clinical validity.</p>
","7762646","","7762646","","2021-09-29 09:44:48","2021-09-29 09:44:48","How to preprocess a text to remove stopwords?","<python><nlp><gensim><word2vec><stop-words>","1","1","","","","CC BY-SA 4.0"
"69020452","1","","","2021-09-01 20:53:46","","1","38","<p>I am trying to write a function that loads/opens pre-trained gensim word2vec models stored in a directory.</p>
<p>I am able to load these models in one-by-one via</p>
<pre><code>gensim.models.Word2Vec.load('file_path')
</code></pre>
<pre><code>import gensim
import os 
input_dir = os.fsencode('User/Directory/') 

def iterated_gensim_load(embed_list):
    for file in os.listdir(embed_list):
        file_path = os.path.join(embed_list, file)
        model = gensim.models.Word2Vec.load(file_path)
</code></pre>
<p>In running the function:</p>
<pre><code>iterated_gensim_load(input_dir)
</code></pre>
<p>I get the following TypeError</p>
<pre><code>  model = gensim.models.Word2Vec.load(file_path)

  File &quot;/opt/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py&quot;, line 1930, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)

  File &quot;/opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py&quot;, line 483, in load
    compress, subname = SaveLoad._adapt_by_suffix(fname)

  File &quot;/opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py&quot;, line 572, in _adapt_by_suffix
    compress, suffix = (True, 'npz') if fname.endswith('.gz') or fname.endswith('.bz2') else (False, 'npy')

TypeError: endswith first arg must be bytes or a tuple of bytes, not str
</code></pre>
<p>UPDATE:</p>
<p>Following advice, I no longer get the above TypeError when I directly read in the file path as a string:</p>
<pre><code>input_dir = 'User/Directory/'
</code></pre>
<p>Running the above function results in, however, produces a new error:</p>
<pre><code>    model = gensim.models.Word2Vec.load(file_path)

  File &quot;/opt/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py&quot;, line 1930, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)

  File &quot;/opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py&quot;, line 485, in load
    obj = unpickle(fname)

  File &quot;/opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py&quot;, line 1460, in unpickle
    return _pickle.load(f, encoding='latin1')  # needed because loading from S3 doesn't support readline()

UnpicklingError: invalid load key, '\x00'.
</code></pre>
<p>However, each individual file in the directory can be successfully read in via, so I don't think they are corrupted/incompatible:</p>
<pre><code>model = gensim.models.Word2Vec.load('file_path')
</code></pre>
","16808326","","16808326","","2021-09-02 17:54:32","2021-09-02 17:54:32","Iteratively open gensim word2vec models","<python><gensim><word2vec>","0","6","","","","CC BY-SA 4.0"
"60680146","1","","","2020-03-14 05:56:44","","1","175","<p>I am trying to use Gensim LDA modelling to topic model of dataset of food recipes. I wish to have topics based the key ingredients in the recipe. But the recipe text contains more words that are generic English and are not ingredient names. Hence my topic outcome is not as good as expected. I am trying to understand the impact of word frequency in the LDA topic outcome. Thanks.</p>
","3910430","","","","","2020-09-06 15:54:08","What is the impact of word frequency on Gensim LDA Topic modelling","<python-3.x><gensim><lda><topic-modeling><word-frequency>","1","0","","","","CC BY-SA 4.0"
"69361669","1","69380892","","2021-09-28 12:21:04","","0","51","<p>I have a corpus of 250k Dutch news articles 2010-2020 to which I've applied word2vec models to uncover relationships between sets of neutral words and dimensions (e.g. good-bad). Since my aim is also to analyze the prevalence of certain topics over time, I was thinking of using doc2vec instead so as to simultaneously learn word and document embeddings. The 'prevalence' of topics in a document could then be calculated as the cosine similarities between doc vectors and word embeddings (or combinations of word vectors). In this way, I can calculate the annual topical prevalence in the corpus and see whether there's any changes over time. An example of such an approach can be found <a href=""https://export.arxiv.org/pdf/1707.03490"" rel=""nofollow noreferrer"">here</a>.</p>
<p>My issue is that the avg. yearly cosine similarities yield really strange results. As an example, the cosine similarities between document vectors and a mixture of keywords related to covid-19/coronavirus show a decrease in topical prevalence since 2016 (which obviously cannot be the case).</p>
<p>My question is whether the approach that I'm following is actually valid. Or that maybe there's something that I'm missing. A 250k documents and 100k + vocabulary should be sufficient enough?</p>
<p>Below is the code that I've written:</p>
<pre><code># Doc2Vec model 
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_docs)]
d2vmodel = Doc2Vec(docs, min_count = 5, vector_size = 200, window = 10, dm = 1)
docvecs = d2vmodel.docvecs
wordvecs = d2vmodel.wv
    
# normalize vector 
from numpy.linalg import norm
def nrm(x):
  return x/norm(x)

# topical prevalence per doc
def topicalprevalence(topic, docvecs, wordvecs):
  proj_lst = []
  for i in range(0, len(docvecs)):
    topic_lst = []
    for j in topic: 
      cossim =  nrm(docvecs[i]) @ nrm(wordvecs[j])
      topic_lst.append(cossim)
    topic_avg = sum(topic_lst) / len(topic_lst)
    proj_lst.append(topic_avg)
  topicsyrs = { 
      'topic': proj_lst,
      'year': df['datetime'].dt.year
  }
  return pd.DataFrame(topicsyrs)

# avg topic prevalence per year
def avgtopicyear(topic, docvecs, wordvecs):
  docs = topicalprevalence(topic, docvecs, wordvecs)
  return pd.DataFrame(docs.groupby(&quot;year&quot;)[&quot;topic&quot;].mean())

# run 
covid = ['corona', 'coronapandemie', 'coronacrisis', 'covid', 'pandemie']
covid_scores = topicalprevalence(covid, docvecs, wordvecs)
</code></pre>
","8639799","","8639799","","2021-09-28 12:36:34","2021-09-30 07:11:49","cosine similarity doc vectors and word vectors for topical prevalence using doc2vec","<python><gensim><word2vec><doc2vec>","2","0","","","","CC BY-SA 4.0"
"69211864","1","","","2021-09-16 16:15:34","","0","26","<p>I'm trying to load and use some pre-trained fasttext embeddings (that were trained by me and stored in .kv). In the same directory I have stored the <code>&quot;vectors_1920_fullsample.kv.vectors_vocab.npy&quot;</code> file. When Does someone know what is going on?</p>
<p>This doesn't give any error:</p>
<pre><code>import matplotlib
matplotlib.use('Agg')
import numpy as np
from scipy.spatial.distance import cosine
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer(&quot;english&quot;)
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import os
import joblib
from gensim.models import Word2Vec
import random
from gensim.models import KeyedVectors
import pandas as pd


model = KeyedVectors.load(wd_model + '/vectors_1920_fullsample.kv', mmap='r')

words = ['immigrant','immigrants','migrant','migrants','foreign','foreigner','foreigners','alien','aliens','expatriate','expatriates','emigrant','emigrants','nonnative','nonnatives','stranger','strangers']
</code></pre>
<p>But then when I do this I get the error below:</p>
<pre><code>words = pd.DataFrame([np.array(model[word]) for word in words])
</code></pre>
<p>Error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;listcomp&gt;
  File &quot;/cluster/apps/nss/gcc-6.3.0/python/3.7.4/x86_64/lib64/python3.7/site-packages/gensim/models/keyedvectors.py&quot;, line 353, in __getitem__
    return self.get_vector(entities)
  File &quot;/cluster/apps/nss/gcc-6.3.0/python/3.7.4/x86_64/lib64/python3.7/site-packages/gensim/models/keyedvectors.py&quot;, line 471, in get_vector
    return self.word_vec(word)
  File &quot;/cluster/apps/nss/gcc-6.3.0/python/3.7.4/x86_64/lib64/python3.7/site-packages/gensim/models/keyedvectors.py&quot;, line 2124, in word_vec
    if word in self.vocab:
AttributeError: 'FastTextKeyedVectors' object has no attribute 'vocab'
</code></pre>
","15739947","","15739947","","2021-10-08 16:32:13","2021-10-08 16:32:13","AttributeError: 'FastTextKeyedVectors' object has no attribute 'vocab'","<nlp><gensim><fasttext>","0","5","","","","CC BY-SA 4.0"
"60702160","1","","","2020-03-16 07:55:02","","0","1090","<pre><code>data1=[tokens.doc2bow(text) for text in texts]
ldamodel=gensim.models.ldamodel.LdaModel(corpus=data1,id2word=tokens,num_topics=10,random_state=100,update_every=1,chunksize=10,passes=10,alpha='auto',per_word_topics=True)
print(*ldamodel.print_topics(),sep=""\n"")
lda=ldamodel[data1]
l=[ldamodel.get_document_topics(item) for item in data1]
print(l)
</code></pre>

<p>While executing <code>get_document_topics()</code>, it is giving an output of hundreds of lines (as shown in picture). I don't know what does it mean. I actually want the probabilities of topics. Which method should I use to get the topic probabilities?</p>

<p><a href=""https://i.stack.imgur.com/uvDlR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uvDlR.png"" alt=""output of the get_document_topics()""></a></p>
","13016432","","6083378","","2020-03-20 04:15:46","2020-03-20 04:15:46","how to get topic probability from the ldamodel by using gensim?","<python><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"69445437","1","69453906","","2021-10-05 05:49:03","","0","22","<p>I am now trying to use word2vec by estimating skipgram embeddings via NCE (noise contrastive estimation) rather than conventional negative sampling method, as a recent paper did (<a href=""https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24421?casa_token=uCHp2XQZVV8AAAAA%3Ac7ETNVxnpqe7u9nhLzX7pIDjw5Fuq560ihU3K5tYVDcgQEOJGgXEakRudGwEQaomXnQPVRulw8gF9XeO"" rel=""nofollow noreferrer"">https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24421?casa_token=uCHp2XQZVV8AAAAA%3Ac7ETNVxnpqe7u9nhLzX7pIDjw5Fuq560ihU3K5tYVDcgQEOJGgXEakRudGwEQaomXnQPVRulw8gF9XeO</a>). The paper has a replication GitHub repository (<a href=""https://github.com/sandeepsoni/semantic-progressiveness"" rel=""nofollow noreferrer"">https://github.com/sandeepsoni/semantic-progressiveness</a>), and it mainly relied on gensim for implementing word2vec, but the repository is not well organized and in a mess, so I have no clue about how the authors implemented NCE estimation via gensim's word2vec.</p>
<p>The authors just used gensim's word2vec as a default status without including any options, so my question is what is the default estimation method for gensim's word2vec under Skip-gram embeddings. NCE? According to your manual,  it just says there is an option for negative sampling, and if set to 0, then no negative sampling is used. But then what estimation method is used?
negative (int, optional) ‚Äì If &gt; 0, negative sampling will be used, the int for negative specifies how many ‚Äúnoise words‚Äù should be drawn (usually between 5-20). If set to 0, no negative sampling is used.</p>
<p>Thanks you in advance, and look forward to hearing from you soon!</p>
","7897485","","6681858","","2021-10-05 08:44:09","2021-10-05 16:09:53","Default estimation method of Gensim's Word2vec Skip-gram?","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"69309530","1","","","2021-09-24 04:22:21","","-1","61","<p>I try to convert pandas dataframe (Doc-Term Matrix) into sparse matrix format and than to gensim corpus BOW format</p>
<p><img src=""https://i.stack.imgur.com/WQLLX.png"" alt=""I try to convert pandas dataframe (Doc-Term Matrix) into sparse matrix format and than to gensim corpus BOW format"" /></p>
<h1>This code for pict 1</h1>
<pre class=""lang-py prettyprint-override""><code>&quot;change pandas dataframe(dtm) -&gt;scisparsematrix-&gt;gensim corpus&quot;
jml_sparse_matrix = scipy.sparse.csr_matrix(uu_to_w2v_data)
corpus_uu_model = matutils.Sparse2Corpus(jml_sparse_matrix)
for line in corpus_uu_model:
    print(line)  
</code></pre>
<p>Im using dictionary from counvectorizer object to get dictionary of id and word</p>
<p><img src=""https://i.stack.imgur.com/xVN2w.png"" alt=""Im using dictionary from counvectorizer object to get dictionary of id and word"" /></p>
<h1>This code for pict 2</h1>
<pre class=""lang-py prettyprint-override""><code>&quot;This for getting dictionary from countvectorizer object&quot;
#dikarenakan gensim membutuhkan id spesifik dari kata2 yang ada di dalam korpus, maka kita akan import vectorizer object yang ada sebelumnya.
cv = pickle.load(open(&quot;D:/DB_Skripsi/DB_PICKLE/cv_uu_stop.pkl&quot;, &quot;rb&quot;))
#cv.vocabulary_.items()
id2kata = dict((v,k) for k, v in cv.vocabulary_.items())
id2kata
</code></pre>
<p>Im using Mmcorpus.Serialize() to mapping id -&gt; word, but i dont get the result that i want to mapping id -&gt; word in MM corpus format</p>
<p><img src=""https://i.stack.imgur.com/Mzkzm.png"" alt=""Im using Mmcorpus.Serialize() to mapping id -&gt; word, but i dont get the result that i want to mapping id -&gt; word in MM corpus format"" /></p>
<h1>This code for pict 3</h1>
<pre class=""lang-py prettyprint-override""><code>&quot;In this code im trying to serialize gensim bow format with dictionary that contain word-&gt; id&quot;
# combine and serializing corpus_uu_model &amp; id2kata 
from gensim.corpora import MmCorpus
from gensim.test.utils import get_tmpfile
name_mm_uu_model = get_tmpfile(&quot;D:/DB_Skripsi/DB_PICKLE/mm_uu_model.mm&quot;) # menunjukkan dimana mm akan disimpan
MmCorpus.serialize(name_mm_uu_model,corpus_uu_model,id2word=id2kata)

# Training word2vec CBOW model
import gensim
from gensim.corpora import MmCorpus
from gensim.test.utils import datapath

#from gensim.models import Word2Vec
# Training CBOW Model
sentences_to_train_cbow = MmCorpus(datapath('D:/DB_Skripsi/DB_PICKLE/mm_uu_model.mm'))
#for line in sentences_to_train_cbow :
#    print(line)
sentences_to_train_cbow
uu_w2v_cbow = gensim.models.Word2Vec(sentences=sentences_to_train_cbow,vector_size=100,window=5,min_count=1,workers=4,sg=0,epochs=10)
len(uu_w2v_cbow.wv)
</code></pre>
","9819161","","8479387","","2021-10-07 07:22:10","2021-10-07 07:22:10","Is there anyways to mapping id -> word in market matrix corpus(gensim)?","<scikit-learn><scipy><text-mining><gensim><corpus>","0","9","1","","","CC BY-SA 4.0"
"60704127","1","","","2020-03-16 10:21:34","","0","177","<p>I have a dataset of 1.2mil single sentence descriptions (5-50 words) and I want to cluster these into n clusters. For vector conversion, I want to use doc2vec to get 1.2mil equal size vectors. However, I'm not sure what should be the size parameter. I've read, it should be between 100-300 however since each document, in this case, has fewer tokens (words) should the vector be small?</p>
","7071062","","9403827","","2020-03-16 10:44:19","2020-03-16 16:47:29","tuning size parameter for doc2vec","<python><cluster-analysis><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"60727025","1","60749186","","2020-03-17 17:05:46","","1","1617","<p>Some similar questions have been asked regarding this topic, but I am not really satisfied with the replies so far; please excuse me for that first.</p>

<p>I'm using the function <code>Word2Vec</code> from the python library <code>gensim</code>.</p>

<p>My problem is that I <strong>can't run my model on every word of my corpus as long as I set the parameter <code>min_count</code> greater than one</strong>. Some would say it's logic cause I choose to ignore the words appearing only once. But the function is behaving weird cause it gives an <strong>error saying <em>word 'blabla' is not in the vocabulary</em></strong>, whereas this is exactly what I want ( I want this word to be out of the vocabulary).</p>

<p>I can imagine this is not very clear, then find below a reproducible example:</p>

<pre><code>import gensim
from gensim.models import Word2Vec

# My corpus
corpus=[[""paris"",""not"",""great"",""city""],
       [""praha"",""better"",""great"",""than"",""paris""],
       [""praha"",""not"",""country""]]

# Load a pre-trained model - The orignal one based on google news 
model_google = gensim.models.KeyedVectors.load_word2vec_format(r'GoogleNews-vectors-negative300.bin', binary=True)

# Initializing our model and upgrading it with Google's 
my_model = Word2Vec(size=300, min_count=2)#with min_count=1, everything works fine
my_model.build_vocab(corpus)
total_examples = my_model.corpus_count
my_model.build_vocab([list(model_google.vocab.keys())], update=True)
my_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, lockf=1.0)
my_model.train(corpus, total_examples=total_examples, epochs=my_model.iter)

# Show examples
print(my_model['paris'][0:10])#works cause 'paris' is present twice
print(my_model['country'][0:10])#does not work cause 'country' appears only once
</code></pre>

<p>You can find Google's model <a href=""https://github.com/mmihaltz/word2vec-GoogleNews-vectors"" rel=""nofollow noreferrer"">there</a> for example, but feel free to use any model or just do without, this is not the point of my post.</p>

<p>As notified in the commentaries of the code: running the model on 'paris' works but not on 'country'. And of course, if I set the parameter <code>min_count</code> to 1, everything works fine.</p>

<p>I hope it is clear enough.</p>

<p>Thanks.</p>
","7667219","","","","","2020-03-19 00:10:24","[Word2Vec][gensim] Handling missing words in vocabulary with the parameter min_count","<python><nlp><gensim><word2vec><word-embedding>","1","4","","","","CC BY-SA 4.0"
"68789973","1","","","2021-08-15 08:40:28","","0","31","<p>Python 3.9.6</p>
<p>I wrote the code to create word- embeddings for my domain (medicine books). My data consists of 45,000 normal length sentences (31 519 unique words, 591 347 all words). When I create / learn a model:</p>
<pre><code>from gensim.models.word2vec import Word2Vec
model = Word2Vec(sentences,
                 min_count   = 5,
                 vector_size = 200, 
                 workers     = multiprocessing.cpu_count(),
                 window      = 6
                 )  
model.save(full_path)
</code></pre>
<p>,it's trained about 1- 2 seconds, and the size of the saved model is about 15MB.</p>
<p>How can I check the correctness of the creation my word- embeddings?</p>
","14653118","","14653118","","2021-08-15 09:14:29","2021-08-16 01:08:12","Why word2vec create word- embeddings so fast?","<python><gensim><word-embedding>","1","0","","","","CC BY-SA 4.0"
"68834211","1","68837183","","2021-08-18 14:21:57","","1","43","<p>I came upon the realization that there exists the original implementation of FastText <a href=""https://fasttext.cc/"" rel=""nofollow noreferrer"">here</a> by which you can use <code>fasttext.train_unsupervised</code> in order to generate word vectors (see <a href=""https://fasttext.cc/docs/en/unsupervised-tutorial.html"" rel=""nofollow noreferrer"">this link</a> as an example). However, turns out that gensim also supports fasttext and its API is similar to that of word2vec. <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py"" rel=""nofollow noreferrer"">See example here</a>.</p>
<p>I am wondering if there is a difference between the 2 implementations? The documentation was not clear <strong>but do they both mimic the paper <a href=""https://arxiv.org/pdf/1607.04606.pdf"" rel=""nofollow noreferrer"">Enriching Word Vectors with Subword Information</a>? And if yes then why would one use gensim's fasttext over fasttext ?</strong></p>
","11212687","","11212687","","2021-08-18 15:28:26","2021-08-20 17:42:11","Difference between Gensim's FastText and Facebook's FastText","<gensim><fasttext>","2","0","","","","CC BY-SA 4.0"
"58540089","1","","","2019-10-24 11:12:40","","0","74","<p>I failed to use the callbacks to save model according to the [official documents]:<a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/callbacks.html</a></p>

<p>AttributeError: Can't pickle local object 'train_model..shf'</p>

<pre class=""lang-py prettyprint-override""><code>    from random import shuffle
    from gensim.models.callbacks import CallbackAny2Vec
    from gensim.test.utils import get_tmpfile

    class shf(CallbackAny2Vec):
        def __init__(self, x, path_prefix):
            self.epoch = 0
            self.x = x
            self.path_prefix = path_prefix
        def on_epoch_begin(self, model):
            shuffle(self.x)

        def on_epoch_end(self, model):
            print(""epoch:%s""%self.epoch)
            if self.epoch % 10 == 0:
                output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))
                model.save(output_path)
            self.epoch += 1
    model_dm = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5, workers=3)
    model_dm.build_vocab(x_train + x_test)

    fun = shf(x_train, ""\models"")
    model_dm.train(x_train, total_examples=model_dm.corpus_count, epochs=100, callbacks=[fun])
</code></pre>
","10370979","","","","","2019-10-24 16:27:42","Pickle error when I save a doc2vec model, AttributeError","<python-3.x><gensim>","1","0","","","","CC BY-SA 4.0"
"69492791","1","","","2021-10-08 08:29:05","","0","16","<p>I‚Äôm trying to understand what considerations are taken into account when Gensim ldamodel set the probabilities of topics for a document.</p>
<p>The main consideration I can think of is the sum of probabilities of the document words within the topic but my results shows almost no correlation between the document probability and the sum of words probabilities.</p>
<p>I couldn‚Äôt find explanations anywhere, Can anyone help?</p>
<p>For example:</p>
<p>Given below document and topics #0/#4 I'd expect topic #4 to get a higher probability but it doesn‚Äôt</p>
<pre><code>Document:
['result', 'throw', 'tremendous', 'act', 'accompany', 'lot', '**positive**', '**quality**', 'commitment', 'soldier', 'courage', 'loyalty']

Topics:
(0, '0.115*&quot;woman&quot; + 0.088*&quot;mother&quot; + 0.060*&quot;succeed&quot; + 0.055*&quot;right&quot; + 0.049*&quot;place&quot; + 0.034*&quot;year&quot; + 0.032*&quot;different&quot; + 0.027*&quot;work&quot; + 0.026*&quot;ability&quot; + 0.025*&quot;success&quot;')

(4, '0.069*&quot;**positive**&quot; + 0.064*&quot;**quality**&quot; + 0.047*&quot;choose&quot; + 0.034*&quot;try&quot; + 0.032*&quot;world&quot; + 0.029*&quot;eye&quot; + 0.027*&quot;trait&quot; + 0.026*&quot;goal&quot; + 0.025*&quot;desire&quot; + 0.025*&quot;imaginary&quot;')


model.get_document_topics(corpus[doc index], minimum_probability=0.00001):

[**(0, 0.31286123)**, (1, 0.005129181), (2, 0.08206084), (3, 0.005129181), **(4, 0.23428485)**, (5, 0.0051291874)‚Ä¶]
</code></pre>
","17104769","","4813913","","2021-10-13 07:05:17","2021-10-13 07:05:17","How does Gensim LdaModel determine probability for a document","<gensim><lda><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"69263687","1","","","2021-09-21 05:15:09","","1","26","<p>I am using Fasttext (from Gensim). I have two issues I don't know how to solve:</p>
<ol>
<li>I would like to set a threshold for the vocabulary to the 100,000 most frequent words. 2. I would like to ensure that a list of words (from a text file) are part of the vocabulary as well. Say this list of words is in a text file called <code>list.txt</code>.</li>
</ol>
<p>How would I do this?</p>
<p>Here is my model:</p>
<pre><code>from gensim.models import FastText

class paragraph_generator(object):
    def __init__(self,test=True,itersize=10000,year=None,state=None):
        self.test=test
        self.itersize=itersize
        self.sql = f&quot;&quot;&quot;
        SELECT
            text_id,
            lccn_sn,
            date,
            ed,
            chroniclingamerica_meta.statefp,
            chroniclingamerica_meta.countyfp,
            text_ocr
        FROM
            chroniclingamerica natural join chroniclingamerica_meta
        WHERE date_part('year',date) BETWEEN 1870 AND 1920 
        AND  seq = 1 &quot;&quot;&quot;
        if self.test:
            self.sql = self.sql+' limit 10000'   # limit 1000 means it only goes through 1000 lines of the database 
        else:
            pass
        print(self.sql)
    def __iter__(self):
        con, cur = database_connection.connect(cursor_type='server')
        cur.itersize = self.itersize
        cur.execute(self.sql)
        for p in cur.fetchall():
            tokens = p[-1].translate(str.maketrans('', '', punct)).replace('\n',' ').lower().split(' ')
            yield tokens
        con.close()


model = FastText(vector_size=256, window=8, min_count=10, epochs=1, workers=workers)
vocab = model.build_vocab(paragraph_generator(test=False, itersize=10000, year=None, state=None))
model.train(paragraph_generator(test=False, itersize=10000, year=None, state=None),
            total_examples=model.corpus_count, epochs=1)
</code></pre>
<p>I'm thinking of a mix between the parameters total_words and sorted_vocab, but I would not know how to do this.</p>
<p>Many thanks in advance for your answers!</p>
","15739947","","","","","2021-09-21 18:29:14","In gensim Fasttext (or Word2vec), I would like to set a threshold for the vocabulary to the 100,000 most frequent words. How do I do this?","<gensim><word2vec><fasttext>","2","0","","","","CC BY-SA 4.0"
"69274178","1","69274284","","2021-09-21 18:53:09","","0","50","<p>I currently have a text file with around a million sentences, each on a new line.
I am trying to build a solution where I can take a new sentence outside of this text file and have the program return the most similar sentence present in the file.</p>
<p>I have found some solutions which return the pair of sentences with the highest similarity INSIDE the existing dataset.For example <a href=""https://stackoverflow.com/questions/63718559/finding-most-similar-sentences-among-all-in-python"">this</a> one. But that is not what I am going for. I want to be able to compare a new sentence with all of those in the text file.</p>
<p>Also, I am not sure if I should be focusing on semantic similarity or cosine similarity.</p>
","13315656","","","","","2021-09-21 19:03:03","Find most similar sentence in a large dataset of sentences","<python><scikit-learn><nlp><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"69472330","1","","","2021-10-06 20:29:31","","0","13","<p>I have downloaded the wikipedia glove vectors using the gensim API. I want to save it locally so I don't have to call the API everytime to download it. How can I do this? I have looked but I am not sure if this is the right way to save them.</p>
<pre><code>import gensim.downloader as api

vectors = api.load('glove-wiki-gigaword-50')
vectors.save('vectors.bin')
</code></pre>
","4343563","","","","","2021-10-06 20:40:56","How to locally save gensim glove pretrained vectors?","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"69514666","1","","","2021-10-10 11:17:09","","0","17","<p>I am currently trying to train my own word2vec model with my own training data and I am utterly confused about the training data preprocessing.</p>
<p>I ran a short script over my text which lemmatizes and also lower-cases the words in the text such that in the end my training data from a sentence (in German) like:</p>
<p><code>&quot;Er hat heute zwei Birnen gegessen.&quot;</code></p>
<p>the following comes out:</p>
<p><code>[er, haben, heute, zwei, birne, essen] </code></p>
<p>translated in English:</p>
<p><code>He ate two pears today. </code></p>
<p>results in:</p>
<p><code>[he, eat, two, pear, today] </code></p>
<p>Now the problem is: I haven't seen anyone do this to their training data. The words are kept in uppercase and also not lemmatized and I absolutely don't get how this works. Especially for German there are so many inflections of verbs. Should I just leave them that way? I don't understand how it works not doing the lemmatization since gensim doesn't even know which language it is trained on right?</p>
<p>So in short: Should I do lemmatization and/or lowercasing or just leave every word as it is?</p>
<p>Thanks a lot!</p>
","7933158","","","","","2021-10-12 08:33:47","Gensim Word2Vec Training Data","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"69492078","1","","","2021-10-08 07:27:53","","0","16","<p>I am using pyLDAvis along with gensim.models.LdaMulticore for topic modeling. I have totally 10 topics. When I visualize the results using pyLDAvis, there is a bar called lambda with this explanation: &quot;Slide to adjust relevance metric&quot;. I am interested to extract the list of words for each topic separately for lambda = 0.1. I cannot find a way to adjust lambda in the document for extracting keywords.</p>
<p>I am using these lines:</p>
<pre><code>if 1 == 1:
    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, lambda_step=0.1)
LDAvis_prepared.topic_info
</code></pre>
<p>And these are the results:</p>
<pre><code>   Term     Freq        Total       Category logprob loglift
321 ra      2336.000000 2336.000000 Default 30.0000 30.0000
146 may     1741.000000 1741.000000 Default 29.0000 29.0000
66  doctor  1310.000000 1310.000000 Default 28.0000 28.0000
</code></pre>
<p>First of all these results are not related to what I observe with lambda of 0.1 in visualization. Secondly I cannot see the results separated by the topics.</p>
","4697714","","","","","2021-10-08 07:27:53","How to get list of words for each topic for a specific relevance metric value (lambda) in pyLDAvis?","<nlp><gensim><lda><topic-modeling><pyldavis>","0","0","","","","CC BY-SA 4.0"
"69513719","1","","","2021-10-10 08:53:48","","0","19","<p>I am a novice in using python. I am trying to get the topic numbers of a text when realizing the Hierarchical Dirichlet Process by gensim.models.HdpModel.</p>
<p>I try to use the method of #hdpmodel.show_topics# or #hdpmodel.print_topics# to get the number of topics that are calculated by the gensim.models.HdpModel. Yet, I do not get my expected topic numbers. The outputs are confusing for me.</p>
<p>My question is that how can I get topic numbers of a text using this library, i.e., gensim.models.HdpModel?</p>
<p>My code is as followings:</p>
<pre><code>`from gensim.models import HdpModel
text_test = ['Ordered 8 complete radios and one didn't work and the other had a bad battery. After replacing the battery the remaining seven have worked well.','Great walkie talkies, they were purchased for use on a cruise for our group and held up well, battery life lasted the 4 days of our cruise and good/clear transmissions with little issues on distance.']
text_doc = [[text for text in doc.split()] for doc in text_test]
dictionary_doc = corpora.Dictionary(text_doc)
corpus_doc = [dictionary_doc.doc2bow(text) for text in text_doc]
hdpmodel_doc = HdpModel(corpus=corpus_doc, id2word=dictionary_doc)
hdptopics_doc = hdpmodel_doc.show_topics(formatted=False)`
</code></pre>
","17118723","","","","","2021-10-10 08:53:48","Cannot get topic numbers when realizing the Hierarchical Dirichlet Process by gensim.models.HdpModel","<python><gensim><topic-modeling><hdp>","0","0","","","","CC BY-SA 4.0"
"60786940","1","","","2020-03-21 10:56:43","","1","670","<p>I have an runtime error: </p>

<pre><code>RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.
  0%|          | 0/29 [00:48&lt;?, ?it/s]
</code></pre>

<p>When I try run this code: </p>

<pre><code>def topic_model_coherence_generator (corpus, texts, dictionary, start_topic_count=2, end_topic_count=10, step=1, cpus=1):
    models=[]
    coherence_scores = []
    for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):
        lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, alpha='auto', eta='auto',
                                   random_state=42, iterations=500, num_topics=topic_nums, passes=20, eval_every=None)

        cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,
                                                      texts=norm_corpus_bigrams, dictionary=dictionary,
                                                      coherence='c_v')

        coherence_score= cv_coherence_model_lda.get_coherence()
        coherence_scores.append(coherence_score)
        models.append(lda_model)
    return models, coherence_scores

lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_corpus,
                                                               texts=norm_corpus_bigrams,
                                                               dictionary= dictionary,
                                                               start_topic_count=2,
                                                               end_topic_count=30,
                                                               step=1, cpus=16)
</code></pre>

<p>That I want is obtain the optimal number of topics of my corpus for obtain then the topics and interpreting topic model results. 
I'm biologist so I don't know how can I fix it. 
Thanks for your help</p>
","12978491","","","","","2020-03-22 05:56:29","Runtime Error when running a LDA model of gensim, how can I fix it?","<model><runtime-error><runtime><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"60723228","1","60725006","","2020-03-17 13:17:19","","-1","52","<p>I have a database with supermarket product items(it contains name, descriptions, price, stock, etc). </p>

<p>I want to make a price comparison between those supermarkets, but, for that i need to know if supermarket A and B refers to the same product. </p>

<p>For example I found out that supermarket <strong>A</strong> has a product called <code>Leche Evaporada GLORIA Azul Paquete 6un Lata 400g</code> and supermarket <strong>B</strong> has a product named <code>Leche Evaporada Gloria Azul Pack 6 Unid x 400 g</code> and those refers to the same product.</p>

<p>I pointed out that I will need to have semantic comparison for those cases. I'm new in this problems so I don't really know what is the best solution to not underestimate the problem or overkill it.</p>

<p>What I'm doing right now with not so great results:</p>

<ol>
<li>I'm only using product names.</li>
<li>Remove stop words from those product names.</li>
<li>Convert the sentence in an array of words.</li>
<li>Get frequency for every word.</li>
<li>If a word has frequency &lt;= 1, then delete it.</li>
<li>With that words I create a dictionary(bag of words) that i will use to map an array of words(a sentence converted) to a feature vector.</li>
<li>Then I ""train"" a TFIDF model with all feature vectors.</li>
<li>Make comparisons(with no great results).</li>
</ol>

<p>I'm using python as LP and <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow noreferrer"">gensim</a> to create models, dictionaries(bag of word) and to make comparisons.</p>

<p>EDIT:
Another examples:</p>

<pre><code>Leche Fresca UHT GLORIA Entera Bolsa 946ml == Leche Entera UHT Gloria Bolsa 946 ml
Yogurt Griego Gloria con Miel y Granola Vaso 115 g == Yogurt Griego GLORIA Batido con Miel Vaso 115g
Leche sin Lactosa GLORIA Mocaccino Botella 330ml == Shake Mocaccino UHT Gloria Frasco 330 ml.
</code></pre>
","8852408","","8852408","","2020-03-17 15:44:00","2020-03-17 15:44:00","What kind of model/technique should I use to compare supermarket product names","<python><machine-learning><nlp><artificial-intelligence><gensim>","1","4","","2020-03-19 17:27:18","","CC BY-SA 4.0"
"69556739","1","","","2021-10-13 13:46:41","","1","18","<p>There are many ways to augment text data, and many articles about this area. I would like to understand if there is a text augmentation technique which is likely to work well, when the text input is vectorized using a Doc2Vec model.</p>
<p>My intuition here would be that using synonym replacement may work well, to create a smoother decision boundary in downstream classifiers.</p>
<p>The pipeline is as follows: <code>text</code> -&gt; <code>d2v</code> -&gt; <code>clf</code></p>
<p>EDIT: based on comments.
I am not looking to augment prior to d2v training. I am using my trained model for a downstream task which looks like this:</p>
<p>text -&gt; d2v -&gt; binary classifier</p>
<p>Lets say I am predicting sentiment, but I have only a few positive samples:</p>
<p>&quot;the dog is happy&quot;</p>
<p>So I am making more samples by augmenting the positive:</p>
<p>&quot;the dog is ecstatic&quot;</p>
<p>Now I have two samples which I pass to d2v to vectorize and use downstream rather than one.</p>
<p>My question is therefore what sort of augmentation works, because for example since d2v in some modes does not care much about word order, doing things like &quot;sentence swap&quot; could be useless.</p>
","4896449","","130288","","2021-10-15 17:30:49","2021-10-15 17:30:49","Text augmentation effective when using Doc2Vec","<nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"69603325","1","69609173","","2021-10-17 10:20:40","","0","61","<p>I am trying to run word2vec (Skipgram) to a set of walks for training a network embedding model, in my graph I have 169343 nodes, i.e; word in the context of Word2vec, and for each node I run a random walk with length 80. Therefore, I have (169343,80) walks, i.e; sentences in Word2vec. after running SkipGram for 3 epochs I only get 28015 vectors instead of 169343. and here is the code for my Network Embedding.</p>
<pre class=""lang-py prettyprint-override""><code>def run_skipgram(walk_path):

    walks = np.load(walk_path).tolist()

    skipgram = Word2Vec(sentences=walks, vector_size=128, negative=5, window=8, sg=1, workers=6, epochs=3)
    
    keys = list(map(int, skipgram.wv.index_to_key))
    keys.sort()

    vectors = [skipgram.wv[key] for key in keys]

    return np.array(vectors)
</code></pre>
","14464984","","","","","2021-10-18 23:08:08","Word2Vec for network embedding ignores words (nodes) in corpus (walks)","<python><machine-learning><graph><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"69478499","1","","","2021-10-07 09:15:29","","-1","30","<p>I have installed <code>gensim</code> for <code>python version 3.6</code> . When I run my code I have seen this error that I cant handle. I have tried to update my conda list and upgrade some modules but it didnt work.</p>
<pre><code>C:\ProgramData\Anaconda3\lib\site-packages\sklearn\ensemble\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.
  from numpy.core.umath_tests import inner1d
Traceback (most recent call last):
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\fasttext.py&quot;, line 264, in &lt;module&gt;
    from gensim.models.fasttext_corpusfile import train_epoch_sg, train_epoch_cbow
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Program Files\JetBrains\PyCharm 2017.2.3\helpers\pydev\pydevd.py&quot;, line 1599, in &lt;module&gt;
    globals = debugger.run(setup['file'], None, None, is_module)
  File &quot;C:\Program Files\JetBrains\PyCharm 2017.2.3\helpers\pydev\pydevd.py&quot;, line 1026, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File &quot;C:\Program Files\JetBrains\PyCharm 2017.2.3\helpers\pydev\_pydev_imps\_pydev_execfile.py&quot;, line 18, in execfile
    exec(compile(contents+&quot;\n&quot;, file, 'exec'), glob, loc)
  File &quot;E:/Phd-project-PronounResolution/Hybrid-Model/hybrid_system_test.py&quot;, line 11, in &lt;module&gt;
    from gensim.models import KeyedVectors
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\gensim\__init__.py&quot;, line 11, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:F401
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\__init__.py&quot;, line 22, in &lt;module&gt;
    from .fasttext import FastText  # noqa:F401
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\fasttext.py&quot;, line 266, in &lt;module&gt;
    raise utils.NO_CYTHON
RuntimeError: Compiled extensions are unavailable. If you've installed from a package, ask the package maintainer to include compiled extensions. If you're building Gensim from source yourself, install Cython and a C compiler, and then run `python setup.py build_ext --inplace` to retry.
</code></pre>
","2938710","","","","","2021-10-07 09:15:29","Problem with gensim module in python. Import error and Runtime Error","<python><anaconda><cython><gensim>","0","2","","","","CC BY-SA 4.0"
"60778921","1","60781718","","2020-03-20 17:31:17","","0","155","<p>While trying to load chines fasttext model(cc.zh.300.bin) with gensim, I stucked with following error</p>

<blockquote>
  <p>UnicodeDecodeError:'utf-8' codec can't decode byte 0xba in position 0:
  invalid start byte</p>
</blockquote>

<p>Anyone can help me, please? Detailed error below :</p>

<p><a href=""https://i.stack.imgur.com/73L4H.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/73L4H.png"" alt=""enter image description here""></a></p>
","13083712","","1367159","","2020-03-20 18:52:26","2020-03-20 21:15:33","How can I load chinese fasttext model with gensim?","<gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"60852962","1","60856812","","2020-03-25 16:22:38","","3","2295","<p>I'm training word2vec from scratch on 34 GB pre-processed MS_MARCO corpus(of 22 GB). (Preprocessed corpus is sentnecepiece tokenized and so its size is more) I'm training my word2vec model using following code : </p>

<pre><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec

class Corpus():
    """"""Iterate over sentences from the corpus.""""""
    def __init__(self):
        self.files = [
            ""sp_cor1.txt"",
            ""sp_cor2.txt"",
            ""sp_cor3.txt"",
            ""sp_cor4.txt"",
            ""sp_cor5.txt"",
            ""sp_cor6.txt"",
            ""sp_cor7.txt"",
            ""sp_cor8.txt""
        ]

    def __iter__(self):
        for fname in self.files:
            for line in open(fname):
                words = line.split()
                yield words

sentences = Corpus()

model = Word2Vec(sentences, size=300, window=5, min_count=1, workers=8, sg=1, hs=1, negative=10)
model.save(""word2vec.model"")

</code></pre>

<p>My model is running now for about more than 30 hours now. This is doubtful since on my i5 laptop with 8 cores, I'm using all the 8 cores at 100% for every moment of time. Plus, my program seems to have read more than 100 GB of data from the disk now. I don't know if there is anything wrong here, but the main reason after my doubt on the training is because of this 100 GB of read from the disk. The whole corpus is of 34 GB, then why my code has read 100 GB of data from the disk? Does anyone know how much time should it take to train word2vec on 34 GB of text, with 8 cores of i5 CPU running all in parallel? Thank you. For more information, I'm also attaching the photo of my process from system monitor. </p>

<p><a href=""https://i.stack.imgur.com/QrJRM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QrJRM.png"" alt=""enter image description here""></a></p>

<p>I want to know why my model has read 112 GB from memory, even when my corpus is of 34 GB in total? Will my training ever get finished? Also I'm bit worried about health of my laptop, since it is running constantly at its peak capacity since last 30 hours. It is really hot now. 
Should I add any additional parameter in <code>Word2Vec</code> for quicker training without much performance loss?</p>
","12814257","","7256554","","2021-03-29 17:08:13","2021-03-29 17:08:13","Training time of gensim word2vec","<python><nlp><gensim><word2vec>","1","0","2","","","CC BY-SA 4.0"
"69412142","1","69427847","","2021-10-01 21:28:40","","0","28","<p>I'm trying to learn from an example which uses an older version of gensim. In particular, I have a section of code like:</p>
<pre><code>word_vectors = Word2Vec(vector_size=word_vector_dim, min_count=1)
word_vectors.build_vocab(corpus_iterable)
word_vectors.intersect_word2vec_format(pretrained_dir + 'GoogleNews-vectors-negative300.bin.gz', binary=True)
</code></pre>
<p>My understanding is that this fills the word vector vocabulary with pre-trained word vectors when available. When the words in my vocabulary are not in the pretrained vectors, they are initialized to random values. However, the method <code>intersect_word2vec_format</code> doesn't exist in the latest version of gensim. What is the cleanest way to replicate this process in gensim 4.0.0?</p>
","1961582","","","","","2021-10-03 18:42:06","Process to intersect with pre-trained word vectors with gensim 4.0.0","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"69563610","1","","","2021-10-13 23:57:58","","0","15","<p>I am experimenting with the beautiful GenSim package and downloaded the glove_42B_300D dataset.</p>
<p>I was a bit surprised to see that the result of</p>
<pre><code>model.most_similar('apple')
</code></pre>
<p>gives</p>
<pre><code>iphone
ipad
apples
blackberry
ipod
macbook
mac
android
google
microsoft
</code></pre>
<p>I mean, how about &quot;apple&quot;, the fruit ?</p>
<p>For my purposes I need this meaning, not Apple the company.</p>
<p>Any solution you can think of ? Thanks.</p>
","48778","","","","","2021-10-14 08:56:36","GenSim : can I find a more ""traditional"" model?","<python><model><nlp><dataset><gensim>","1","0","","","","CC BY-SA 4.0"
"69583960","1","69588518","","2021-10-15 11:21:42","","2","38","<p>I want to train fasttext on my own corpus. However, I have a small question before continuing. Do I need each sentences as a different item in corpus or can I have many sentences as one item?</p>
<p>For example, I have this DataFrame:</p>
<pre><code> text                                               |     summary
 ------------------------------------------------------------------
 this is sentence one this is sentence two continue | one two other
 other similar sentences some other                 | word word sent
</code></pre>
<p>Basically, the column <code>text</code> is an article so it has many sentences. Because of the preprocessing, I no longer have full stop <code>.</code>. So the question is can I do something like this directly or do I need to split each sentences.</p>
<pre><code>docs = df['text']
vectorizer = TfidfVectorizer()
vectorizer.fit_transform(docs)
</code></pre>
<p>From the tutorials I read, I need list of words for each sentences but what if I have list of words from an article? What are the differences? Is this the right way of training fasttext in your own corpus?</p>
<p>Thank you!</p>
","9417884","","","","","2021-10-15 17:29:12","Training fasttext word embedding on your own corpus","<python><tensorflow><gensim><word-embedding><fasttext>","1","0","","","","CC BY-SA 4.0"
"60850956","1","60852969","","2020-03-25 14:31:31","","0","23","<p>I'm trying to evaluate a home-made topic model. For this, I'm using the list of topics (represented by keywords), and want to use a <code>gensim.models.coherencemodel.CoherenceModel</code>, and call it on a corpus, which is a list of strings (each one being a document).
The <code>CoherenceModel</code> requires a <code>Dictionary</code>, but I don't understand what this corresponds to, and how I can get it.
I'm using the <code>TfidfVectorizer</code> from <code>sklearn</code> to vectorize the text, and <code>glove</code> embeddings from <code>gensim</code> to compute similarities within my model. </p>
","5173924","","","","","2020-03-25 16:23:17","Topic Coherence with Dictionary from Glove (gensim)","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"69654710","1","69655812","","2021-10-21 01:14:12","","0","28","<p>I am a bit confused as how to tokenize the data correctly in <code>gensim</code>.
I have a text file  <code>myfile.txt</code> that contains the following text</p>
<pre><code>&quot;&quot;&quot; 
this is a very long string with a title


and some white space. Multiple sentences, too. This is nuts!
Yay! :):):) 
&quot;&quot;&quot;
</code></pre>
<p>I load this file in <code>gensim</code> using <code>LineReader('myfile.txt')</code> to train a <code>word2vec</code> model (of course my data is much bigger than the example above)</p>
<p>But is this text tokenized propertly? I am asking this because <code>LineReader</code> seems to be very specific :</p>
<blockquote>
<p>The format of files (either text, or compressed text files) in the
path is one sentence = one line, with words already preprocessed and
separated by whitespace.
see <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence</a></p>
</blockquote>
<p>I am confused. Am I doing things right? How should I tokenize my text for <code>LineReader</code>?</p>
<p>Thanks!</p>
","1609428","","1609428","","2021-10-21 03:19:10","2021-10-21 04:15:27","tokenizing the data properly in gensim","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"60873334","1","60878185","","2020-03-26 17:54:33","","0","166","<p>I've trained a model:</p>

<pre><code>from gensim.models import Word2Vec    

model = Word2Vec(master_sent_list,
                     min_count=5,   
                     size=300,      
                     workers=5,    
                     window=5,      
                     iter=30)  
</code></pre>

<p>Saved it according to <a href=""https://stackoverflow.com/questions/50466643/in-spacy-how-to-use-your-own-word2vec-model-created-in-gensim"">this</a> post:</p>

<pre><code>model.wv.save_word2vec_format(""../moj_word2vec.txt"")
!gzip ../moj_word2vec.txt
!python -m spacy init-model en ../moj_word2vec.model --vectors-loc ../moj_word2vec.txt.gz
</code></pre>

<p>Everything looks fine:</p>

<pre><code>‚úî Successfully created model
22470it [00:02, 8397.55it/s]j_word2vec.txt.gz
‚úî Loaded vectors from ../moj_word2vec.txt.gz
‚úî Sucessfully compiled vocab
22835 entries, 22470 vectors
</code></pre>

<p>I then load the model <strong>under a different name</strong>:</p>

<pre><code>nlp = spacy.load('../moj_word2vec.model/')
</code></pre>

<p>Something goes wrong however, because I can't use common commands on <code>nlp</code>; that I can on <code>model</code>.</p>

<p>For example, these work:</p>

<pre><code>model.wv.most_similar('police')
model.vector_size
</code></pre>

<p>But these don't:</p>

<pre><code>nlp.wv.most_similar('police')
AttributeError: 'English' object has no attribute 'wv'

nlp.most_similar('police')
AttributeError: 'English' object has no attribute 'most_similar'

nlp.vector_size
AttributeError: 'English' object has no attribute 'vector_size'
</code></pre>

<p>So something seems to have broken in the loading, or perhaps the saving, could someone help please?</p>
","8746300","","","","","2020-03-27 00:23:15","Having trouble loading custom trained word vectors created in Gensim, into Spacy","<python-3.x><spacy><gensim>","1","0","","","","CC BY-SA 4.0"
"60903484","1","","","2020-03-28 16:25:37","","0","92","<pre><code>def generate_w2vModel(decTokenFlawPath, w2vModelPath):
    print(""training..."")
    model = Word2Vec(sentences= DirofCorpus(decTokenFlawPath), size=30, alpha=0.01, window=5, min_count=0, max_vocab_size=None, sample=0.001, seed=1, workers=1, min_alpha=0.0001, sg=1, hs=0, negative=10, iter=5)
    model.save(w2vModelPath)

def evaluate_w2vModel(w2vModelPath):
    print(""\nevaluating..."")
    model = Word2Vec.load(w2vModelPath)
    for sign in ['(', '+', '-', '*', 'main']:
        print(sign, "":"")
        print(model.most_similar_cosmul(positive=[sign], topn=10))

def main():
    dec_tokenFlaw_path = ['./data/cdg_ddg/corpus/']
    w2v_model_path = ""./w2v_model/wordmodel3"" 
    generate_w2vModel(dec_tokenFlaw_path, w2v_model_path)
    evaluate_w2vModel(w2v_model_path)
    print(""success!"")
</code></pre>

<p>this the python that i was running. This file is used to train word2vec model. The inputs are corpus files, and the output is the word2vec model. i got the following error :</p>

<pre><code>Traceback (most recent call last):
  File ""create_w2vmodel.py"", line 67, in &lt;module&gt;
    main()
  File ""create_w2vmodel.py"", line 62, in main
    generate_w2vModel(dec_tokenFlaw_path, w2v_model_path)
  File ""create_w2vmodel.py"", line 50, in generate_w2vModel
    model.save(w2vModelPath)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-3.4.0-py2.7-linux-x86_64.egg/gensim/models/word2vec.py"", line 930, in save
    super(Word2Vec, self).save(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-3.4.0-py2.7-linux-x86_64.egg/gensim/models/base_any2vec.py"", line 281, in save
    super(BaseAny2VecModel, self).save(fname_or_handle, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-3.4.0-py2.7-linux-x86_64.egg/gensim/utils.py"", line 691, in save
    self._smart_save(fname_or_handle, separately, sep_limit, ignore, pickle_protocol=pickle_protocol)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-3.4.0-py2.7-linux-x86_64.egg/gensim/utils.py"", line 550, in _smart_save
    pickle(self, fname, protocol=pickle_protocol)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-3.4.0-py2.7-linux-x86_64.egg/gensim/utils.py"", line 1311, in pickle
    with smart_open(fname, 'wb') as fout:  # 'b' for binary, needed on Windows
  File ""build/bdist.linux-x86_64/egg/smart_open/smart_open_lib.py"", line 89, in smart_open
  File ""build/bdist.linux-x86_64/egg/smart_open/smart_open_lib.py"", line 301, in file_smart_open
IOError: [Errno 21] Is a directory: './w2v_model/wordmodel3'
</code></pre>

<p>please help me to change this particular error. i think there is no folder like this, but i had already created the w2v_model/wordmodel3 in my folder. I had tried it in many ways. I will provide smart_open_lib.py program file below :</p>

<pre><code>def file_smart_open(fname, mode='rb'):
    """"""
    Stream from/to local filesystem, transparently (de)compressing gzip and bz2
    files if necessary.

    """"""
    _, ext = os.path.splitext(fname)

    if ext == '.bz2':
        PY2 = sys.version_info[0] == 2
        if PY2:
            from bz2file import BZ2File
        else:
            from bz2 import BZ2File
        return make_closing(BZ2File)(fname, mode)

    if ext == '.gz':
        from gzip import GzipFile
        return make_closing(GzipFile)(fname, mode)

    return open(fname, mode)
</code></pre>

<p>this is the trace back code which they tell. kindly request to help me fr change this error!!!</p>
","11267673","","","","","2020-03-28 21:44:51","IOError : [Error no: 21] is a directory : './w2v-model/wordmodel3'","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"69640267","1","69640870","","2021-10-20 04:18:25","","0","18","<p>When I attempt to run FastText using gensim in Python, the best I can get is a result that gives me the most similar but each result is a single character.  (I'm on a windows machine, which I've heard affects the result.)</p>
<p>I have all of my data stored in <em>either</em> a csv file in which I've already tokenized each sentence or in the original txt file I started with.  When I try to use the csv file, I end up with the single character result.</p>
<p>Here's the code I'm using to process my csv file (I'm looking at analyzing how sports articles discuss white vs. nonwhite NFL quarterbacks differently, this is the code for my NonWhite results csv file):</p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import get_tmpfile, datapath
import os

embedding_size = 200
window_size = 10
min_word = 5
down_sampling = 1e-2

if os.path.isfile(modelpath):
    model1 = FastText.load(modelpath)
else:
    class NWIter():
        def __iter__(self):
            path = datapath(csvpath)
            with utils.open(path, 'r') as fin:
                for line in fin:
                    yield line

    model1 = FastText(vector_size=embedding_size, window=window_size, min_count=min_word,sample=down_sampling,workers=4)
    model1.build_vocab(corpus_iterable=NWIter())
    exs1=model1.corpus_count
    model1.train(corpus_iterable=NWIter(), total_examples=exs1, epochs=50)  
    model1.save(modelpath)

</code></pre>
<p>The cleaned CSV data looked like this, with each row representing a sentence that had been cleaned (stopwords removed, tokenized, and lemmatized).
<a href=""https://i.stack.imgur.com/5HlPv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5HlPv.png"" alt=""Cleaned CSV File"" /></a></p>
<p>When that didn't work, I attempted to bring in the raw text but got lots of UTF-8 encoding errors with unrecognizable characters.  I attempted to work around this issue, finally getting to a point where it tried to read in the raw text file - only for the single character returns to come back.</p>
<p>So it seems the issue persists regardless of if I use my csv file or if I use the txt file.  So I'd prefer to stick with the csv as I've already processed the information; how can I bring that data in without Python (or gensim) seeing the individual characters as the unit of analysis?</p>
<p>Edit:
Here are the results I get when I run:</p>
<pre><code>print('NonWhite: ',model1.wv.most_similar('smart', topn=10))
</code></pre>
<p>NonWhite:  [('d', 0.36853086948394775), ('q', 0.326141357421875), ('s', 0.3181183338165283), ('M', 0.27458563446998596), ('g', 0.2703150510787964), ('o', 0.215525820851326), ('x', 0.2153075635433197), ('j', 0.21472081542015076), ('f', 0.20139966905117035), ('a', 0.18369245529174805)]</p>
","4403105","","","","","2021-10-20 05:45:21","Preparing large txt file for gensim FastText unsupervised model","<python-3.x><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"60913104","1","","","2020-03-29 10:49:34","","1","143","<p>I have trained two word2vec models suing gensim on separate corpora.
Both corpora are in English.
In one corpus there are some different words which are not present in the other. </p>

<p>I want to map the common words of one model to the other.
Then I want to get the vectors of the unknown words (after the mapping). </p>

<p>I tried the following code:</p>

<pre><code>from gensim.models import KeyedVectors
from gensim.models import translation_matrix

w2v_bin_path_old    = 'model_from_corpus1_30d.bin'
w2v_bin_path_new    = 'model_from_corpus2_30d.bin'
wv_old              = KeyedVectors.load_word2vec_format(w2v_bin_path_old, binary=True)
wv_new              = KeyedVectors.load_word2vec_format(w2v_bin_path_new, binary=True)

common_tokens       = set(wv_old.vocab.keys()).intersection(set(wv_new.vocab.keys()))
common_tokens       = [(tok, tok) for tok in common_tokens]

transmat            = translation_matrix.TranslationMatrix(wv_new, wv_old, common_tokens)
transmat.train(common_tokens)

transmat.translate('whatever')
</code></pre>

<p>However, this code returns only the top 5 words from the destination w2v model. 
I only want to get the vector of the proposed token. 
Is there a way to do it? </p>

<p>Thank you in advance!</p>
","4034203","","","","","2020-03-30 16:48:24","How do i get a vector from gensim's translation_matrix","<mapping><translation><gensim><word2vec>","1","3","","","","CC BY-SA 4.0"
"34754547","1","","","2016-01-12 21:58:53","","3","483","<p>I am using <code>gensim</code>, but when I try to save to a <code>s3</code> location with <code>Mmcorpus.serialize</code> it sends an error:</p>

<pre><code>corpora.MmCorpus.serialize('s3://my_bucket/corpus.mm', corpus)                                                                                           
2016-01-12 15:55:41,957 : INFO : storing corpus in Matrix Market format to s3://my_bucket/corpus.mm
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-33-513a98b2dfd4&gt; in &lt;module&gt;()
----&gt; 1 corpora.MmCorpus.serialize('s3://my_bucket/corpus.mm', corpus)

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/gensim/corpora/indexedcorpus.py in serialize(serializer, fname, corpus, id2word, index_fname, progress_cnt, labels, metadata)
     92                 offsets = serializer.save_corpus(fname, corpus, id2word, labels=labels, metadata=metadata)
     93             else:
---&gt; 94                 offsets = serializer.save_corpus(fname, corpus, id2word, metadata=metadata)
     95 
     96         if offsets is None:

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/gensim/corpora/mmcorpus.py in save_corpus(fname, corpus, id2word, progress_cnt, metadata)
     47         logger.info(""storing corpus in Matrix Market format to %s"" % fname)
     48         num_terms = len(id2word) if id2word is not None else None
---&gt; 49         return matutils.MmWriter.write_corpus(fname, corpus, num_terms=num_terms, index=True, progress_cnt=progress_cnt, metadata=metadata)
     50 
     51 # endclass MmCorpus

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/gensim/matutils.py in write_corpus(fname, corpus, progress_cnt, index, num_terms, metadata)
    484         is allowed to be larger than the available RAM.
    485         """"""
--&gt; 486         mw = MmWriter(fname)
    487 
    488         # write empty headers to the file (with enough space to be overwritten later)

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/gensim/matutils.py in __init__(self, fname)
    434         if fname.endswith("".gz"") or fname.endswith('.bz2'):
    435             raise NotImplementedError(""compressed output not supported with MmWriter"")
--&gt; 436         self.fout = utils.smart_open(self.fname, 'wb+') # open for both reading and writing
    437         self.headers_written = False
    438 

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/smart_open/smart_open_lib.py in smart_open(uri, mode, **kw)
    132                 return S3OpenWrite(key, **kw)
    133             else:
--&gt; 134                 raise NotImplementedError(""file mode %s not supported for %r scheme"", mode, parsed_uri.scheme)
    135 
    136         elif parsed_uri.scheme in (""hdfs"", ):

NotImplementedError: ('file mode %s not supported for %r scheme', 'wb+', 's3')
</code></pre>

<p><em>NOTE</em>: <code>s3://my_bucket</code> exists (with another name), and <code>corpus</code> is the same from the tutorial of <code>gensim</code>.</p>

<p>Which is the correct way of do it? I want to achive the following: store the corpus (or a model, like LDA) in S3 and getting it from S3 and run it again.</p>
","754176","","754176","","2016-01-15 06:07:39","2018-10-12 09:32:29","Gensim saving corpus to S3","<amazon-s3><gensim>","0","0","","","","CC BY-SA 3.0"
"54318701","1","54319915","","2019-01-23 01:15:31","","1","481","<p>I'd like to create a big gensim dictionary for french language to try getting better results in topic detection, similarities between texts and other things like that.
So I've planned to use a wikipedia dump and process it the following way:</p>

<ol>
<li>Extract each article from frwiki-YYYYMMDD-pages-articles.xml.bz2 (Done)</li>
<li>Tokenize each article (basically converting the text to lowercases, removing stop words and non-word characters) (Done)</li>
<li>Train a Phrases model on the articles to detect collocation.</li>
<li>Stem the resulting tokens in each article.</li>
<li>Feed the dictionary with the new corpus (one stemmed-collocated-tokenized article per line)</li>
</ol>

<p>Because of the very large size of the corpus, I don't store anything in memory and access the corpus via smart_open but it appears gensim Phrases model is consuming too much RAM to complete the third step.</p>

<p>Here is my sample code:</p>

<pre><code>corpus = smart_open(corpusFile, ""r"")
phrases = gensim.models.Phrases()
with smart_open(phrasesFile, ""wb"") as phrases_file:
    chunks_size = 10000
    texts, i = [], 0
    for text in corpus:
        texts.append(text.split())
        i += 1
        if i % chunks_size == 0:
            phrases.add_vocab(texts)
            texts = []
    phrases.save(phrases_file)
corpus.close()
</code></pre>

<p>Is there a way to complete the operation without freezing my computer or will I have to train the Phrases model only on a subset of my corpus?</p>
","1608467","","","","","2019-01-23 05:03:32","How to train a Phrases model from a huge corpus of articles (wikipedia)?","<python><nltk><gensim><collocation>","1","0","","","","CC BY-SA 4.0"
"69470347","1","","","2021-10-06 17:41:28","","1","30","<p>I'm looking for suggestions on how to approach a document classification problem.  I will explain by means of example:</p>
<p><strong>Problem statement</strong></p>
<p>I have a collection of papers published by a university.  I have another collection published by another university.  And so on, for many universities.</p>
<p>When a new paper comes in, I'd like to determine which university probably published it.</p>
<p><strong>My current approach</strong></p>
<ol>
<li>For each university, build a dictionary of all terms from all papers with frequencies.  Preprocess into terms and build a gensim <code>Dictionary</code> per university.</li>
<li>Build a &quot;master&quot; dictionary by merging all of the university-specific dictionaries.  Iterate and perform <code>master_dictionary.merge_with(university_dictionary)</code></li>
<li>Treat each university dictionary as a document in a new corpus. Turn it into a BoW representation, and build a model from that.  TfIdf/LSI/LDA.</li>
<li>Perform a similarity match of the new paper (as BoW) against the model.  Find the university dictionary document that matches closest.</li>
</ol>
<p><strong>My question</strong></p>
<p>How is a problem like this tackled? (And is there a name for this?)</p>
<ul>
<li>I'm currently comparing a new document with a summarized document, one per university.</li>
<li>I could compare a new document with every document across all universities.  Then get the universities for the top matching documents using metadata.</li>
<li>I've run across the Author-Topic Model but haven't looked into it, but that seems like this may be a good fit.</li>
</ul>
<p>Any other ideas?</p>
","104523","","104523","","2021-10-11 17:22:32","2021-10-11 17:22:32","NLP: Find the most similar Corpus (not Document)","<nlp><data-science><gensim><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"60729841","1","","","2020-03-17 20:35:14","","0","38","<p>This is my input (sample*)</p>

<pre><code>data = [""['human', 'interface', 'computer']"",
 ""['survey', 'user', 'computer', 'system', 'response', 'time']"",
 ""['eps', 'user', 'interface', 'system']"",
 ""['system', 'human', 'system', 'eps']"",
 ""['user', 'response', 'time']"",
 ""['trees']""]
</code></pre>

<p>And I have tried implementing</p>

<p>dictionary = corpora.Dictionary(text_data)</p>

<p>But this is the error I get,</p>

<p>TypeError: doc2bow expects an array of unicode tokens on input, not a single string</p>

<p>Please help if you see why that wouldn't work</p>
","13078688","","","","","2020-03-17 20:42:37","Python Gensim Dictionary","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"63021096","1","63021097","","2020-07-21 18:42:48","","0","104","<p>Given a list of document words e.g. <code>[['cow','boy','hat','mat],['village','boy','water','cow']....]</code>, gensim can be used to get bi-grams as follows:</p>
<pre><code>bigrams = gensim.models.Phrases(data_words, min_count=1,threshold=1) 
bigram_model = gensim.models.phrases.Phraser(bigrams)
</code></pre>
<p>I was wondering as to how to get the score of each bi-gram detected in the bigram_model?</p>
","4474531","","","","","2020-07-21 18:48:26","How to get the score of filtered bi-grams in gensim?","<python><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"54323245","1","","","2019-01-23 08:49:50","","2","457","<p>I'm trying to load a saved word2vec model. The file on loading throws error: cannot reshape array of size 974897576 into shape (6828798,300)</p>

<p>I've created a word2vec model by initialising 6828798 words to random vectors, by feeding a list of list of my vocabulary words <code>[['a', 'b', 'c']]</code> to gensim and running the model.</p>

<p>new_titles_grp is a pandas df, with synon_title (str) and averages (np.array.shape=(300,)) columns</p>

<pre><code>sentences = [new_titles_grp['synon_title'].tolist()]
model_gensim = word2vec.Word2Vec(sentences, min_count=1, size=300)
</code></pre>

<p>Then, i'm replacing all the vectors with my pre-computed vector values, (all are (300,) dimensions) with</p>

<pre><code>for ind, row in new_titles_grp.iterrows():
    model_gensim.wv.syn0[model_gensim.wv.vocab[row['synon_title']].index] = row['averages']

model_gensim.init_sims(replace=True) #doing this to normalize the values
</code></pre>

<p>The model works well when called in the same notebook</p>

<pre><code>model_gensim.most_similar(get_job_title('Data Science'), topn=20)
Out[26]: 
[('Senior Data Scientist', 0.9982969760894775),
 ('DATA SCIENTIST', 0.9951386451721191),
 ('Data Scientist Analyst', 0.9940630197525024),
 ('Associate Data Scientist', 0.9907373189926147),
 ('Lead Data Scientist', 0.9906764030456543),
 ('Data Scientist Machine Learning Engineer', 0.9891946315765381),
 ('Data Scientist Specialist', 0.9885110855102539),
 ('Data Analyst Scientist', 0.988399863243103),
 ('Data Scientist III', 0.9873133301734924),
 ('Senior Data Scientist Machine Learning', 0.9868470430374146),
 ('Research Data Scientist', 0.98624187707901),
 ('Data Scientist Engineer', 0.9855831861495972),
 ('Data Scientist Intern', 0.9855802059173584),
 ('Machine Learning Specialist', 0.9842516779899597),
 ('Principal Data Scientist', 0.9835132360458374),
 ('Senior Manager Data Scientist', 0.9829919338226318),
 ('Machine Learning Engineer', 0.9829450249671936),
 ('Data Scientist IV', 0.9827083349227905),
 ('Data Analytics Engineer', 0.9825129508972168),
 ('Senior Data Scientist Consultant', 0.9823513031005859)]
</code></pre>

<p>On loading it in different notebook is throwing this error, I thought the numpy array dimensions are not same, which is not the case, all are 300 (0-Dimension) vectors. Please help!!</p>

<p>EDIT:
I'm saving it using <code>model_gensim.save(SAVE_PATH+'/jd_w2v.bin')</code></p>

<p>Also, using </p>

<p><code>model_gensim.wv.save_word2vec_format('/dbfs/FileStore/tables/talent_lemma_collection/w2v_models_v_1/jd_w2v.bin', binary=True)</code></p>

<p>I'm able to save them (apparently)</p>

<p>Loading it using</p>

<pre><code>model_titles = gensim.models.Word2Vec.load(jd_titles_path)
</code></pre>

<p>Error: </p>

<pre><code>ValueError: cannot reshape array of size 974897576 into shape (6828798,300)
</code></pre>

<p>and</p>

<pre><code>model_titles = KeyedVectors.load_word2vec_format(datapath(jd_titles_path), binary=True, unicode_errors='ignore')
</code></pre>

<p>Error:</p>

<pre><code>ValueError: string size must be a multiple of element size
</code></pre>
","1831784","","1831784","","2019-01-24 05:54:24","2019-01-24 05:54:24","Unable to load a saved gensim model, cannot reshape array of size 974897576 into shape (6828798,300)","<python-3.x><numpy><gensim><word2vec>","0","1","1","","","CC BY-SA 4.0"
"60785538","1","60796333","","2020-03-21 07:42:28","","0","59","<p>I have calculated distances between two sentences using wmdistance() funtion of gensim with pre-trained model</p>
<p>Now, I want to similarity between them and tried with  n_similarity() funnction, but keyerror occured</p>
<p>keyerror : word not in vacabulary</p>
<p>This shows  screenshoot of error example
<a href=""https://i.stack.imgur.com/r1h9F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r1h9F.png"" alt=""screenshoot of error example"" /></a></p>
<p>Anyone have got idea on this, please?</p>
","13083712","","-1","","2020-06-20 09:12:55","2020-03-22 05:51:19","In gensim with pretrained model, wmdistance is working well, but n_similarity is not","<gensim>","1","0","","","","CC BY-SA 4.0"
"60840809","1","","","2020-03-24 23:18:27","","0","712","<p>When I saved my LdaModel <code>lda_model.save('model')</code>, it saved 4 files:</p>

<ol>
<li><code>model</code> </li>
<li><code>model.expElogbeta.npy</code> </li>
<li><code>model.id2word</code> </li>
<li><code>model.state</code></li>
</ol>

<p>I want to use <code>pyLDAvis.gensim</code> to visualize the topics, which seems to need the model, corpus and dictionary. I was able to load the model and dictionary with:</p>

<pre><code>lda_model = LdaModel.load('model')
dict = corpora.Dictionary.load('model.id2word')
</code></pre>

<p>Is it possible to load the corpus? How?</p>
","1344369","","","","","2021-05-31 12:06:10","Gensim: How to load corpus from saved lda model?","<gensim><lda><corpus>","2","0","","","","CC BY-SA 4.0"
"60960326","1","","","2020-03-31 21:38:12","","3","318","<p>In order to fine-tune <code>word2vec</code> embeddings in <code>gensim</code>, the following piece of code worked with previous versions:</p>

<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors- 
negative300.bin.gz', binary=True)
</code></pre>

<p>However, I get the error message that <code>Word2Vec.load_word2vec</code> is depracated : 
<code>DeprecationWarning: Deprecated. Use gensim.models.KeyedVectors.load_word2vec_format instead.</code>
When I use </p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews- 
vectors-negative300.bin.gz', binary=True)
</code></pre>

<p>and then try to fine tune the model with train method as below:</p>

<pre><code>model.train((corpus, total_examples=len(corpus2),epochs=10) )
</code></pre>

<p>I get the following error:</p>

<p>""AttributeError: 'Word2VecKeyedVectors' object has no attribute 'train'""</p>

<p>Is there still any solution to load the existing <code>Googlenews W2V</code> into <code>gensim</code> and fine-tune it with additional corpus?</p>

<p>In response to user:10473854: ignoring warning does not work as the module is already depracated. Also, running Word2Vec with the path for downloaded embedding will make Word2Vec fails. Check this:</p>

<pre><code>model = Word2Vec('GoogleNews-vectorsnegative300.bin.gz')
model.wv.vocab

{'/': &lt;gensim.models.keyedvectors.Vocab at 0x7ff6101c3940&gt;,
'a': &lt;gensim.models.keyedvectors.Vocab at 0x7ff6101c39e8&gt;,
'e': &lt;gensim.models.keyedvectors.Vocab at 0x7ff6101c3278&gt;}
</code></pre>
","3089485","","3089485","","2020-04-01 06:39:45","2020-04-08 20:54:32","Is there still any solution to load the existing Googlenews W2v into gensim and finetune it with additional corpus?","<python><nlp><gensim><word2vec><embedding>","1","1","1","","","CC BY-SA 4.0"
"63025899","1","","","2020-07-22 02:30:55","","4","435","<p>I am currently using Gensim LDA for topic modeling.</p>
<p>While Tuning hyper-parameters I found out that the model always gives negative log-perplexity</p>
<p>Is it normal for model to behave like this?? (is it even possible?)</p>
<p>if it is, is smaller perplexity better than bigger one? (-100 is better than -20??)</p>
","12870106","","","","","2021-10-19 12:49:20","Gensim lda gives negative log-perplexity value - is it normal and how can i interpret it?","<gensim><lda><perplexity>","0","0","","","","CC BY-SA 4.0"
"63082446","1","","","2020-07-24 22:55:27","","1","55","<p>I am trying to extract topics using Gensim library, LDA model from Persona-Chat dataset. After pre-processing part, I try to find the best number of topic in order to get the best keywords regarding to the topics.</p>
<p>So, for that I ran the code for different number of topics such as; 50-100-150-200 and 250.</p>
<p>The Coherence Score increases whenever number of topics are increased and at the same time, Perplexity Score decreases as well. So, from these information I understood I had to use 250, because as far as I know, Perplexity should be small and Coherence should be high. Below, you can see the scores of each number of topics (K)</p>
<ul>
<li>K is 50, Coherence Score is 0.5674564036353904 and Perplexity is
-30.41004925196529</li>
<li>K is 100, Coherence Score is 0.7057459566697354 and Perplexity is
-152.65336624931487</li>
<li>K is 150, Coherence Score is 0.8342139447628544 and Perplexity is
-298.6829528808594</li>
<li>K is 200, Coherence Score is 0.8342139447628546 and Perplexity is
-398.1058654785156</li>
<li>K is 250, Coherence Score is 0.8342139447628545 and Perplexity is
-497.6555480957031</li>
</ul>
<p>However, whenever I used 250 number of topics, my keywords have 0 weight and they are repeated in every topic.</p>
<ul>
<li>(0, '0.000*&quot;white&quot; + 0.000*&quot;chalk&quot; + 0.000*&quot;princeton&quot; +
0.000*&quot;gymnast&quot; + 0.000*&quot;vanilla&quot; + 0.000*&quot;rain&quot; + 0.000*&quot;cow&quot; + 0.000*&quot;ds&quot; + 0.000*&quot;employed&quot; + 0.000*&quot;dirty&quot;')</li>
<li>(1, '0.000*&quot;white&quot; + 0.000*&quot;chalk&quot; + 0.000*&quot;princeton&quot; +
0.000*&quot;gymnast&quot; + 0.000*&quot;vanilla&quot; + 0.000*&quot;rain&quot; + 0.000*&quot;cow&quot; + 0.000*&quot;ds&quot; + 0.000*&quot;employed&quot; + 0.000*&quot;dirty&quot;')</li>
<li>(2, '0.000*&quot;white&quot; + 0.000*&quot;chalk&quot; + 0.000*&quot;princeton&quot; +
0.000*&quot;gymnast&quot; + 0.000*&quot;vanilla&quot; + 0.000*&quot;rain&quot; + 0.000*&quot;cow&quot; + 0.000*&quot;ds&quot; + 0.000*&quot;employed&quot; + 0.000*&quot;dirty&quot;')</li>
</ul>
<p>I made some research and I found out that if keywords are repeated then maybe my number of topics are too much. So I decreased my number of topics from 250 to 50 but I want to know, is there any ways to fix this?</p>
<p>Since, the Perplexity and Coherence Scores shows that LDA model would work better with 250 number of  topics.</p>
<p>Below you can find the code as well.</p>
<pre><code>personality_dictionary = Dictionary(df['persona_sentences'])
corpus_personality = [personality_dictionary.doc2bow(text) for text in df['persona_sentences']]
lda = LdaModel(corpus=corpus_personality, num_topics=50, id2word=personality_dictionary)
lda_personality_topics = lda.print_topics(num_words=10, num_topics = -1)
for topic in lda_personality_topics:
    print(topic)
</code></pre>
<p>Best regards,</p>
","4180623","","4180623","","2020-07-27 00:24:48","2020-07-27 00:24:48","Topic Extraction Using Gensim","<python><nlp><gensim><lda><topic-modeling>","0","1","","","","CC BY-SA 4.0"
"69640025","1","69640966","","2021-10-20 03:36:19","","0","26","<p>I use FastText to generate the word embedding. I download the pre-trained model from <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a>
The model has 300 dimensions but I want 100 dimensions so I use reduce model command but I got an error</p>
<p><code>import gensim</code><br />
<code>model = gensim.models.fasttext.FastText.load_fasttext_format('cc.th.300.bin')</code><br />
<code>gensim.models.fasttext.utils.reduce_model(model, 100)</code><br />
I got <code>AttributeError: module 'gensim.utils' has no attribute 'reduce_model'</code><br /></p>
<p>Heres are the code from FastText docs</p>
<p><code>import fasttext</code><br />
<code>import fasttext.util</code><br />
<code>ft = fasttext.load_model('cc.en.300.bin')</code><br />
<code>fasttext.util.reduce_model(ft, 100)</code></p>
<p>How to fix this error, I cannot find any docs for the new command.</p>
<p>Thank you</p>
","10611232","","","","","2021-10-20 15:51:33","FastText: AttributeError: type object 'FastText' has no attribute 'reduce_model'","<python><nlp><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"60894446","1","","","2020-03-27 21:37:59","","0","289","<p>Is it possible to access a fasttext model (gensim) using multithreading?<br>
Currently, I'm trying to load a model once (due to size and loading time), so it stays in memory and access its similarity functions multiple thousands times in a row. I want to do that in parallel and my current approach uses a wrapper class that loads the model and is then passed to the workers. But it looks like it does not return any results.    </p>

<p>The wrapper class. Initiated once.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.fasttext import load_facebook_model

class FastTextLocalModel:
    def __init__(self):
        self.model_name = ""cc.de.300.bin""
        self.model_path = path.join(""data"", ""models"", self.model_name)
        self.fast_text = None

    def load_model(self):
        self.fast_text = load_facebook_model(self.model_path)

    def similarity(self, word1: str = None, word2: str = None):
        return self.fast_text.wv.similarity(word1, word2)
</code></pre>

<p>And the Processor class makes use of the <code>FastTextLocalModel</code> methods above:</p>

<pre class=""lang-py prettyprint-override""><code>fast_text_instance = FastTextLocalModel()
fast_text_instance.load_model()

with concurrent.futures.ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:
        docs = corpus.get_documents()  # docs is iterable
        processor = ProcessorClass(model=fast_text_instance)
        executor.map(processor.process, docs)
</code></pre>

<p>Using <code>max_workers=1</code> seems to work.<br>
I have to mention that I have no expertise in python multithreading.</p>
","2026738","","","","","2020-03-28 21:51:18","Use fasttext model (gensim) with threading","<python><multithreading><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"63096909","1","63097133","","2020-07-26 06:36:54","","0","94","<p>I am trying to implement word2vec on a problem. I will briefly explain my problem statement:</p>
<p>I am dealing with clinical data. I want to predict the top N diseases given a set of symptoms.</p>
<pre><code>Patient1: ['fever', 'loss of appetite', 'cold', '#flu#']
Patient2: ['hair loss', 'blood pressure', '#thyroid']
Patient3: ['hair loss', 'blood pressure', '#flu]
..
..
Patient30000: ['vomiting', 'nausea', '#diarrohea']
</code></pre>
<p>Note:
1.words with #prefix are diagnosis and the rest are symptoms</p>
<ol start=""2"">
<li>My corpus doesn't have any sentences or paragraphs. It just contains symptom names and diagnosis for a patient</li>
</ol>
<p>Applying word2vec on this corpus, I am able to generate the top 10 diagnosis given a set of input symptoms. Now, I want to understand how that output is generated. I know it's cosine similarity by adding the input vectors but I am unable to validate this output. Or understand how to improve this.  Really want to understand what exactly is going on in the background which leads to these output.</p>
<p>Can anyone help me answer these questions or highlight what are the drawbacks/advantages of this approach</p>
","13982723","","13982723","","2020-07-29 08:24:56","2020-07-29 08:24:56","How to interpret output from gensim's Word2vec most similar method and understand how it's coming up with the output values","<python><nlp><gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"4111979","1","4300184","","2010-11-06 04:59:22","","2","461","<p>I want to use Latent Semantic Analysis for a small app I'm building, but I don't want to build up the matrices myself. (Partly because the documents I have wouldn't make a very good training collection, because they're kinda short and heterogeneous, and partly because I just got a new computer and I'm finding it a bitch to install the linear algebra and such libraries I would need.)</p>

<p>Are there any ""default""/pre-built LSA implementations available? For example, things I'm looking for include:</p>

<ul>
<li>Default U,S,V matrices (i.e., if D is a term-document matrix from some training set, then D = U S V^T is the singular value decomposition), so that given any query vector q, I can use these matrices to compute the LSA projection of q myself.</li>
<li>Some black-box LSA algorithm that, given a query vector q, returns the LSA projection of q.</li>
</ul>
","231588","","342473","","2012-03-20 11:29:39","2012-03-20 11:29:39","""pre-built"" matrices for latent semantic analysis","<nlp><machine-learning><latent-semantic-indexing><gensim>","1","2","1","","","CC BY-SA 2.5"
"54385850","1","","","2019-01-27 07:05:34","","1","6068","<p>I've imported all the packages I need</p>

<pre><code>from gensim import corpora
from gensim import models
from gensim.models import LdaModel
from gensim.models import TfidfModel
from gensim.models import CoherenceModel
</code></pre>

<p>and then I need to run the LdaMallet model so I import them like this</p>

<pre><code>from gensim.models.wrappers import LdaMallet
</code></pre>

<p>when run the code below, I've got some <code>Namerror</code>:</p>

<pre><code>mallet_path = 'mallet-2.0.8/bin/mallet' # update this path

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path,corpus=corpus, num_topics=20, id2word=dictionary)
</code></pre>

<p>Error occurred:</p>

<pre><code>---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-22-1c656d4f8c21&gt; in &lt;module&gt;()
      1 mallet_path = 'mallet-2.0.8/bin/mallet' # update this path
      2 
----&gt; 3 ldamallet = gensim.models.wrappers.LdaMallet(mallet_path,corpus=corpus, num_topics=20, id2word=dictionary)

NameError: name 'gensim' is not defined
</code></pre>

<p>I thought I've imported all the things that I need, and the lda model ran well before I tried to use mallet. So what's the problem?</p>
","8836671","","2057919","","2019-01-27 07:16:56","2019-01-27 07:16:56","NameError: name 'gensim' is not defined","<python><gensim><lda><mallet>","2","4","","","","CC BY-SA 4.0"
"54395916","1","","","2019-01-28 05:18:12","","2","352","<p>I'm using the gensim python library to work on small corpora (around 1500 press articles each time). Let say I'm interested in creating clusters of articles relating the same news.</p>

<p>So for each corpus of articles I've tokenized, detected collocations, stemmed and then fed a little dictionary (around 20k tokens) I've passed though a TFIDF model.</p>

<p>Finally I've used the TFIDF corpus to build a LSI model of the corpus and with the help of the document similarity functions of gensim I was able to get very good results.</p>

<p>But I was curious and made some coherence checking of the LSI with:</p>

<pre><code>lsi_topics = [[word for word, prob in topic] for topicid, topic in 
lsi.show_topics(formatted=False)]
lsi_coherence = CoherenceModel(topics=lsi_topics[:10], texts=corpus, dictionary=dictionary, window_size=10).get_coherence()
logger.info(""lsi coherence: %.3f"" % lsi_coherence)
</code></pre>

<p>And I always get values around 0.45 which could seem pretty weak.</p>

<p>So I was wondering how to interpret this coherence value? And does this value make sense when you only need similarity of documents in the index to the index itself (so the queries are a full document from the corpus)?</p>

<p>Edit: I tried different things for text preprocessing such as splitting each document in real sentences before feeding the Phrases class, generating bigrams, trigrams or removing accents or not and in some cases I was able to get a coherence value around 0.55 so at least I guess it can help finding the most efficient way to process raw datas...</p>
","1608467","","1608467","","2019-01-30 03:41:31","2019-01-30 03:41:31","What is a ""good"" value for LSI topic coherence?","<python><gensim><topic-modeling><latent-semantic-indexing>","0","0","1","","","CC BY-SA 4.0"
"63079854","1","63079938","","2020-07-24 18:55:28","","0","805","<p>I have extracted about 40MB of the English wikipedia into plain text. I would to use it to build a word2vec model with gensim. To do this I need to split it into sentences first. How can I do this?  I tried:</p>
<pre><code>from __future__ import unicode_literals, print_function
import spacy
from spacy.lang.en import English 
nlp = spacy.load('en_core_web_sm')
nlp.max_length = 47084146
ftest = open(&quot;test_02&quot;, &quot;r&quot;)
raw_test = ftest.read().replace(&quot;\n&quot;, &quot; &quot;)
sentences = [i for i in nlp(raw_test).sents] 

f = open(&quot;sentences.txt&quot;, &quot;w&quot;)

for sent in sentences:
    f.write(str(sent)+&quot;\n&quot;)
f.write(&quot;\n&quot;)
f.close()
</code></pre>
<p>But this fails with: <code>MemoryError: Unable to allocate 34.8 GiB for an array with shape (9112793, 8, 64, 2) and data type float32</code></p>
<p>I have no idea why it wants to use so much RAM!</p>
<p>How can I do this?</p>
<hr />
<pre><code>Traceback (most recent call last):
  File &quot;../../processwiki.py&quot;, line 8, in &lt;module&gt;
    sentences = [i for i in nlp(raw_test).sents] 
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/spacy/language.py&quot;, line 449, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))
  File &quot;nn_parser.pyx&quot;, line 233, in spacy.syntax.nn_parser.Parser.__call__
  File &quot;nn_parser.pyx&quot;, line 274, in spacy.syntax.nn_parser.Parser.predict
  File &quot;nn_parser.pyx&quot;, line 287, in spacy.syntax.nn_parser.Parser.greedy_parse
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/thinc/neural/_classes/model.py&quot;, line 167, in __call__
    return self.predict(x)
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/thinc/neural/_classes/model.py&quot;, line 131, in predict
    y, _ = self.begin_update(X, drop=None)
  File &quot;_parser_model.pyx&quot;, line 243, in spacy.syntax._parser_model.ParserModel.begin_update
  File &quot;_parser_model.pyx&quot;, line 300, in spacy.syntax._parser_model.ParserStepModel.__init__
  File &quot;_parser_model.pyx&quot;, line 425, in spacy.syntax._parser_model.precompute_hiddens.__init__
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/spacy/_ml.py&quot;, line 183, in begin_update
    Yf = self._add_padding(Yf)
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/spacy/_ml.py&quot;, line 214, in _add_padding
    Yf_padded = self.ops.xp.vstack((self.pad, Yf))
  File &quot;&lt;__array_function__ internals&gt;&quot;, line 6, in vstack
  File &quot;/mnt/storage/software/languages/anaconda/Anaconda3-2020.02-tflow-2.2.0/lib/python3.7/site-packages/numpy/core/shape_base.py&quot;, line 283, in vstack
    return _nx.concatenate(arrs, 0)
  File &quot;&lt;__array_function__ internals&gt;&quot;, line 6, in concatenate
MemoryError: Unable to allocate 34.8 GiB for an array with shape (9112793, 8, 64, 2) and data type float32
</code></pre>
","12784384","","12784384","","2020-07-24 19:08:11","2020-07-24 20:35:24","How to split a text file into sentences for word2vec/gensim","<python><spacy><gensim>","2","6","1","","","CC BY-SA 4.0"
"54587140","1","","","2019-02-08 06:35:50","","0","348","<p>I'm working on an NLP problem and my goal is to be able to pass my data into sklearn's algos after having used Word2Vec via Python's Gensim Library. The underlying problem I am trying to solve is binary classification of a series of tweets. To do so I am modifying the code in <a href=""https://github.com/halidebey/PyCon2018/blob/master/analysis.py"" rel=""nofollow noreferrer"">this</a> git repo.</p>

<p>Here is part of the code relating to tokenization:</p>

<pre><code>from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
input_file[""tokens""] = input_file[""text""].apply(tokenizer.tokenize)
all_words = [word for tokens in input_file[""tokens""] for word in tokens]
sentence_lengths = [len(tokens) for tokens in input_file[""tokens""]]
vocabulary = sorted(set(all_words))
</code></pre>

<p>Now here is the part where I use Gensim's sklearn-api to try to vectorize my tweets:</p>

<pre><code>from sklearn.model_selection import train_test_split
from gensim.test.utils import common_texts
from gensim.sklearn_api import W2VTransformer
text = input_file[""text""].tolist()
labels = input_file[""label""].tolist()
X_train, X_test, y_train, y_test = train_test_split(text, labels, test_size=0.2,random_state=40)
model = W2VTransformer(size=10, min_count=1, seed=1)
X_train_w2v = model.fit(common_texts).transform(X_train)
</code></pre>

<p>This results in the following error:</p>

<pre><code>KeyError: ""word 'Great seeing you again, don't be a stranger!' not in vocabulary""
</code></pre>

<p>It seems that part of the issue is that Gensim is expecting to be fed one word at a time and instead it is getting entire tweets.</p>

<p>X_train is of type list, here are the first three elements of the list:</p>

<pre><code>[""Great seeing you again, don't be a stranger!"",
 ""Beautiful day here in sunny Prague. Not a cloud in the sky"",
 "" pfft! i wish I had a laptop like that""]
</code></pre>

<p><strong>Update</strong></p>

<p>In order to remedy this, I have tried the following:</p>

<pre><code>X_train_list = []
for sentence in X_train:
word_list = sentence.split(' ')
while("""" in word_list): 
    word_list.remove("""") 
X_train_list.append(word_list)
model = W2VTransformer(size=10, min_count=1, seed=1)
X_train_tfidf = model.fit(common_texts).transform(X_train_list)
</code></pre>

<p>This produces the following error:</p>

<pre><code>KeyError: ""word 'here' not in vocabulary""
</code></pre>

<p>To be honest, this one blows my mind! How a common word like 'here' is not in the vocabulary is beyond me. Also wondering if tweets with stray letters will throwing errors, I imagine the weird jumbles of letters that often pass for words will cause similar issues.</p>
","2653567","","2653567","","2019-02-08 08:26:08","2019-02-08 08:26:08","Parsing a List of Tweets in Order to Utlize Gensim Word2Vec","<python><nlp><gensim><word2vec><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"63095512","1","63096412","","2020-07-26 02:30:44","","0","605","<p>I am trying to assess a doc2vec model based on the code from <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#assessing-the-model"" rel=""nofollow noreferrer"">here</a>. Basically, I want to know the percentual of  inferred documents are found to be most similar to itself. This is my current code an:</p>
<pre><code>    for doc_id, doc in enumerate(cur.execute('SELECT Text FROM Patents')):
        docs += 1
        doc = clean_text(doc)
        inferred_vector = model.infer_vector(doc)
        sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
        rank = [docid for docid, sim in sims].index(doc_id)
        ranks.append(rank) 

    counter = collections.Counter(ranks)
    accuracy = counter[0] / docs
</code></pre>
<p>This code works perfectly with smaller datasets. However, since I have a huge file with millions of documents, this code becomes too slow, it would take months to compute. I profiled my code and most of the time is consumed by the following line: <code>sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))</code>.</p>
<p>If I am not mistaken, this is having to measure each document to every other document. I think computation time might be massively reduced if I change this to <code>topn=1</code> instead since the only thing I want to know is if the most similar document is itself or not. Doing this will basically take each doc (i.e., <code>inferred_vector</code>), measure its most similar document (i.e., <code>topn=1</code>), and then I just see if it is itself or not. How could I implement this? Any help or idea is welcome.</p>
","11086598","","","","","2020-07-26 18:44:23","Assessing doc2vec accuracy","<gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"45943832","1","45944661","","2017-08-29 16:13:47","","1","848","<p>I am trying to train a Doc2Vec model using gensim with 114M unique documents/labels and vocab size of around 3M unique words. I have 115GB Ram linux machine on Azure.
When I run build_vocab, the iterator parses all files and then throws memory error as listed below.</p>

<pre><code>    Traceback (most recent call last):
  File ""doc_2_vec.py"", line 63, in &lt;module&gt;
    model.build_vocab(sentences.to_array())
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 579, in build_vocab
    self.finalize_vocab(update=update)  # build tables &amp; arrays
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 752, in finalize_vocab
    self.reset_weights()
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 662, in reset_weights
    self.docvecs.reset_weights(self)
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 390, in reset_weights
    self.doctag_syn0 = empty((length, model.vector_size), dtype=REAL)
MemoryError
</code></pre>

<p>My code-</p>

<pre><code>import parquet
import json
import collections
import multiprocessing


# gensim modules
from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec

class LabeledLineSentence(object):
    def __init__(self, sources):
        self.sources = sources   
        flipped = {}

    def __iter__(self):
        for src in self.sources:
            with open(src) as fo:
               for row in parquet.DictReader(fo, columns=['Id','tokens']):
                    yield LabeledSentence(utils.to_unicode(row['tokens']).split('\x01'), [row['Id']])

## list of files to be open ##
sources =  glob.glob(""/data/meghana_home/data/*"")
sentences = LabeledLineSentence(sources)

#pre = Doc2Vec(min_count=0)
#pre.scan_vocab(sentences)
""""""
for num in range(0, 20):
    print('min_count: {}, size of vocab: '.format(num), pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/700)
    print(""done"")
""""""

NUM_WORKERS = multiprocessing.cpu_count()
NUM_VECTORS = 300
model = Doc2Vec(alpha=0.025, min_alpha=0.0001,min_count=15, window=3, size=NUM_VECTORS, sample=1e-4, negative=10, workers=NUM_WORKERS) 
model.build_vocab(sentences)
print(""built vocab......."")
model.train(sentences,total_examples=model.corpus_count, epochs=10)
</code></pre>

<p>Memory usage as per top is-</p>

<p><a href=""https://i.stack.imgur.com/xarz0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xarz0.png"" alt=""enter image description here""></a></p>

<p>Can someone please tell me how much is the expected memory? What is better option- Adding swap space and slow the process or add more memory so that cost of cluster might eventually be equivalent.
What vectors gensim stores in memory? Any flag that i am missing for memory efficient usage.</p>
","4883595","","4883595","","2017-08-30 05:23:15","2017-08-30 05:23:15","Gensim Doc2vec finalize_vocab Memory Error","<python><nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"45720880","1","","","2017-08-16 18:46:24","","0","165","<p>I have a gensim LSI model that I want to persist in MongoDB. Specifically, I want to persist the following:</p>

<ul>
<li>Gensim dictionary (id &lt;-> word)</li>
<li>Gensim corpus</li>
<li>LSI model</li>
<li>MatrixSimilarity index</li>
</ul>

<p>I know that all of these objects can be saved to file, but I can't figure out how I can save these data to a MongoDB (specifically, using pyMongo). From what I've gathered, I don't need anything like GridFS, because the files are relatively small. But in any case, I want to save them all in a single document, and not separately.</p>

<p>Thanks :)</p>
","2986584","","","","","2017-08-16 18:46:24","Persisting gensim LSI model in MongoDB","<mongodb><python-3.x><pymongo><gensim>","0","4","","","","CC BY-SA 3.0"
"54580260","1","54581599","","2019-02-07 18:48:10","","1","3616","<p>I am unsure how I should use the most_similar method of gensim's Word2Vec. Let's say you want to test the tried-and-true example of: <em>man stands to king as woman stands to X</em>; find X. I thought that is what you could do with this method, but from the results I am getting I don't think that is true.</p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar"" rel=""nofollow noreferrer"">The documentation</a> reads:</p>

<blockquote>
  <p>Find the top-N most similar words. Positive words contribute
  positively towards the similarity, negative words negatively.</p>
  
  <p>This method computes cosine similarity between a simple mean of the
  projection weight vectors of the given words and the vectors for each
  word in the model. The method corresponds to the word-analogy and
  distance scripts in the original word2vec implementation.</p>
</blockquote>

<p>I assume, then, that <code>most_similar</code> takes the positive examples and negative examples, and tries to find points in the vector space that are as close as possible to the positive vectors and as far away as possible from the negative ones. Is that correct?</p>

<p>Additionally, is there a method that allows us to map the relation between two points to another point and get the result (cf. the man-king woman-X example)?</p>
","1150683","","","","","2021-01-08 19:09:45","Understanding gensim word2vec's most_similar","<python><python-3.x><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"54684552","1","","","2019-02-14 06:42:33","","2","539","<p>After training an LDA model on gensim LDA model i converted the model to a with the gensim mallet via the <code>malletmodel2ldamodel</code> function provided with the wrapper. Before and after the conversion the topic word distributions are quite different. The mallet version returns very rare topic word distribution after conversion.</p>

<pre><code>ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=13, id2word=dictionary)
model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)
model.save('ldamallet.gensim')

dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')
corpus = pickle.load(open('corpus.pkl', 'rb'))
lda_mallet = gensim.models.wrappers.LdaMallet.load('ldamallet.gensim')
import pyLDAvis.gensim
lda_display = pyLDAvis.gensim.prepare(lda_mallet, corpus, dictionary, sort_topics=False)
pyLDAvis.display(lda_display)
</code></pre>

<p><a href=""https://i.stack.imgur.com/jOPUf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jOPUf.png"" alt=""Mallet Implementation after using &lt;code&gt;malletmodel2ldamodel&lt;/code&gt; ""></a></p>

<p>Here is the output from gensim original implementation:</p>

<p><a href=""https://i.stack.imgur.com/t2zpV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t2zpV.png"" alt=""Here is the output from gensim original implementation""></a></p>

<p>I can see there was a bug around this issue which has been fixed with the previous versions of gensim. I am using gensim=3.7.1</p>
","865389","","","","","2020-03-18 18:26:39","Issue with topic word distributions after malletmodel2ldamodel in gensim","<gensim><lda><topic-modeling><mallet>","1","0","","","","CC BY-SA 4.0"
"69564296","1","69567581","","2021-10-14 02:08:10","","0","29","<p>I am using <code>gensim</code> to train a <code>word2vec</code> model. The problem is that my data is very large (about 10 million documents) so my session is crashing when I try to estimate the model.</p>
<p>Note that I am able to load all the data at once in the RAM in a Pandas dataframe <code>df</code>, which looks like:</p>
<pre><code>text               id
long long text      1
another long one    2
...                 ...
</code></pre>
<p>My simple approach is to do the following:</p>
<pre><code>tokens = df['text'].str.split(r'[\s]+')
model = Word2Vec(tokens, min_count = 50)
</code></pre>
<p>However, my session crashed when it tries to create the tokens all at once. Is there a better way to proceed in <code>gensim</code>? Like feeding the data line by line?</p>
<p>Thanks!</p>
","1609428","","","","","2021-10-14 08:38:33","training a Word2Vec model with a lot of data","<python><pandas><gensim>","1","8","","","","CC BY-SA 4.0"
"60792362","1","","","2020-03-21 19:29:03","","0","44","<p>I need a suggestion in unsupervised training of Doc2Vec for the 2 options I have. The scenario is I have N documents each of size greater than 3000 tokens. So now for training which alternative is better:</p>

<ol>
<li>Training with whole document as such.</li>
<li>Breaking the documents into chunks of 1000 tokens and then training it.</li>
</ol>
","10900497","","","","","2020-03-22 05:49:29","Doc2Vec Unsupervised training","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"69579095","1","","","2021-10-15 01:46:19","","1","20","<p>I see that pyLDAvis visualize each word's saliency under each topic.</p>
<p><a href=""https://i.stack.imgur.com/honDL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/honDL.png"" alt=""enter image description here"" /></a></p>
<p>But do we have a way to extract each word's saliency under each topic? Or how to calculate each word's saliency directly using Gensim LDA?</p>
<p>So finally, I want to get a pandas dataframe such that one row represents one word, each column represents each topic and its value represents the word's saliency under the corresponding topic.</p>
<p>Many thanks in advance.</p>
","16415909","","","","","2021-10-15 17:40:41","Extract Word Saliency from Gensim LDA or pyLDAvis","<gensim><lda><topic-modeling><pyldavis>","1","0","","","","CC BY-SA 4.0"
"63101674","1","","","2020-07-26 14:45:05","","1","111","<p>I am using word2vec through gensim currently.  You can set the context size easily (that sets the number of words to the left and right of a center word to consider).  Sometimes it's better to consider the words to the left separately to the words to the the right. This would give two embeddings per word.</p>
<p>Can this be done in gensim or in any other python compatible tool?</p>
","12784384","","","","","2020-07-26 18:54:52","Can you make word2vec use left context and right context separately?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"54684338","1","","","2019-02-14 06:23:03","","0","456","<p>I'm running tfidf model in python.</p>

<pre><code>texts=[**tokenized words**]
dictionary = corpora.Dictionary(texts)
corpus = list(map(dictionary.doc2bow,texts))
test_model = models.TfidfModel(corpus)
corpus_tfidf = test_model[corpus]  
</code></pre>

<p>And it returns the output which gives some patterns of values to the exact same word.
For example, I chose the word ""AAA"".</p>

<pre><code>         key          score
0       ""AAA""       1
2323    ""AAA""       0.896502
4086    ""AAA""       0.844922 
</code></pre>

<p>Why do they have every different value even though they are exact same.</p>
","9191983","","6278334","","2019-02-14 09:22:21","2020-12-23 21:44:55","tfidf oucomes are different for the exact same word","<python><pandas><gensim><tf-idf><corpus>","1","0","1","","","CC BY-SA 4.0"
"46173339","1","","","2017-09-12 09:55:39","","0","2300","<p>I have problem installing gensim module. I installed numpy and scipy dependent modules successfully but was getting error while installing gensim.
I tried solutions given in <a href=""https://stackoverflow.com/questions/35991403/python-pip-install-gives-command-python-setup-py-egg-info-failed-with-error-c"">Python pip install gives &quot;Command &quot;python setup.py egg_info&quot; failed with error code 1&quot;</a>
but none of them worked.</p>

<p>Here is the error:</p>

<pre><code>    &gt;pip install --target=""D:\python\packages"" gensim
Collecting gensim
  Using cached gensim-2.3.0-cp36-cp36m-win32.whl
Collecting scipy&gt;=0.18.1 (from gensim)
  Using cached scipy-0.19.1.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""C:\Users\abcde\AppData\Local\Temp\pip-build-hu8lzsjz\scipy\setup.py"", line 416, in &lt;module&gt;
        setup_package()
      File ""C:\Users\abcde\AppData\Local\Temp\pip-build-hu8lzsjz\scipy\setup.py"", line 412, in setup_package
        setup(**metadata)
      File ""c:\program files (x86)\python36-32\lib\distutils\core.py"", line 108, in setup
        _setup_distribution = dist = klass(attrs)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\dist.py"", line 315, in __init__
        self.fetch_build_eggs(attrs['setup_requires'])
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\dist.py"", line 361, in fetch_build_eggs
        replace_conflicting=True,
      File ""c:\program files (x86)\python36-32\lib\site-packages\pkg_resources\__init__.py"", line 850, in resolve
        dist = best[req.key] = env.best_match(req, ws, installer)
      File ""c:\program files (x86)\python36-32\lib\site-packages\pkg_resources\__init__.py"", line 1122, in best_match
        return self.obtain(req, installer)
      File ""c:\program files (x86)\python36-32\lib\site-packages\pkg_resources\__init__.py"", line 1134, in obtain
        return installer(requirement)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\dist.py"", line 429, in fetch_build_egg
        return cmd.easy_install(req)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\command\easy_install.py"", line 653, in easy_install
        not self.always_copy, self.local_index
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 636, in fetch_distribution
        dist = find(requirement)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 617, in find
        dist.download_location = self.download(dist.location, tmpdir)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 566, in download
        found = self._download_url(scheme.group(1), spec, tmpdir)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 805, in _download_url
        return self._attempt_download(url, filename)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 811, in _attempt_download
        headers = self._download_to(url, filename)
      File ""c:\program files (x86)\python36-32\lib\site-packages\setuptools\package_index.py"", line 726, in _download_to
        block = fp.read(bs)
      File ""c:\program files (x86)\python36-32\lib\http\client.py"", line 449, in read
        n = self.readinto(b)
      File ""c:\program files (x86)\python36-32\lib\http\client.py"", line 493, in readinto
        n = self.fp.readinto(b)
      File ""c:\program files (x86)\python36-32\lib\socket.py"", line 586, in readinto
        return self._sock.recv_into(b)
      File ""c:\program files (x86)\python36-32\lib\ssl.py"", line 1002, in recv_into
        return self.read(nbytes, buffer)
      File ""c:\program files (x86)\python36-32\lib\ssl.py"", line 865, in read
        return self._sslobj.read(len, buffer)
      File ""c:\program files (x86)\python36-32\lib\ssl.py"", line 625, in read
        v = self._sslobj.read(len, buffer)
    ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\abcde\AppData\Local\Temp\pip-build-hu8lzsjz\scipy\
</code></pre>
","8086502","","","","","2018-03-19 11:32:54","Unable to install gensim in python","<python><gensim>","2","2","","","","CC BY-SA 3.0"
"61062237","1","61065363","","2020-04-06 14:35:23","","0","48","<p>I need to train my own model with word2vec and fasttext. By readind different sourcs I found different information. 
So I did the model and trained it like this:</p>

<pre><code>model = FastText(all_words, size=300, min_count= 3,sg=1)
model = Word2Vec(all_words, min_count=3, sg = 1, size = 300 )
</code></pre>

<p>So I read that that should be enough to creat and train the model. But then I saw, that some people do it seperatly:</p>

<pre><code>model = FastText(size=4, window=3, min_count=1)  # instantiate
model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)  # train
</code></pre>

<p>Now I am confused and dont know if what I did is correct. Can sombody help me to make it clear? 
Thank you</p>
","6157005","","","","","2020-04-06 17:21:02","Gensim train word2vec and Fasttext","<python><machine-learning><gensim>","1","0","","","","CC BY-SA 4.0"
"36909819","1","","","2016-04-28 08:55:36","","0","684","<p>I am using sklearn.cluster ""KMean"" with python. I am trying to cluster data and plot graph for vectors(size =109977)
Here is my code:</p>

<pre><code>model = gensim.models.Word2Vec.load(""../wordvectors_300dimeansion.model"")

vectors = model.syn0 #size 109977

n_clusters_kmeans = 20 # more for visualization 100 better for clustering
#kmeans = KMeans(init='k-means++', n_clusters=n_clusters_kmeans, n_init=10)
min_kmeans = MiniBatchKMeans(init='k-means++',    n_clusters=n_clusters_kmeans, n_init=10)

min_kmeans.fit(vectors)

X_reduced = TruncatedSVD(n_components=50,   random_state=0).fit_transform(vectors)
X_embedded = TSNE(n_components=2, perplexity=40, verbose=2).fit_transform(X_reduced)


target = min_kmeans.labels_
fig = plt.figure(figsize=(10, 10))
ax = plt.axes(frameon=False)
plt.setp(ax, xticks=(), yticks=())
plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.9, wspace=0.0, hspace=0.0)
plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=target, marker=""x"")
plt.show()
</code></pre>

<p>kMean computing pairwise distance means to create a matrix with pairwise distances of 109977X109977</p>

<p>And I think due to this I got ""out of memory"" error.</p>

<p>pl. suggest me some scalable k-means clustering algorithm for large dataset. </p>

<p>Is there any tool available for this purpose? which I can import in my program directly as I did for sklearn.cluster ""KMean"" and ""MiniBatchKMean"", so that I can directly import it in my program.</p>

<p>Thanks.</p>
","5368173","","","","","2016-04-28 08:55:36","k-means and MiniBatchKMean in python is out of memory","<python><memory-management><cluster-analysis><k-means><gensim>","0","4","","","","CC BY-SA 3.0"
"63100943","1","","","2020-07-26 13:42:36","","1","142","<p>I am currently trying to use dynamic topic modeling on some news crawled from the web.
Unfortunately, I receive a warning in the logs:</p>
<p>INFO : using serial LDA version on this node</p>
<pre><code>path/to/gensim/models/ldaseqmodel.py:1472: RuntimeWarning: invalid value encountered in double_scalars converged = np.fabs((lhood_old - lhood) / (lhood_old * total))
</code></pre>
<p>After using google to find out more about this issue, I learned that this numpy error is often produced by NaNs or null values. So with regards to dynamic topic modeling this probably refers to an empty document? but I dont have any empty documents in my dataframe</p>
","10577402","","10035361","","2020-07-27 07:29:09","2020-12-21 08:36:16","ldaseqmodel runtimewarning Invalid value in double_scalars","<nlp><gensim><lda>","1","0","1","","","CC BY-SA 4.0"
"46168600","1","","","2017-09-12 05:33:26","","37","82330","<p>I trying to import gensim.</p>

<p>I have the following code</p>

<pre><code>import gensim
model = gensim.models.Word2Vec.load_word2vec_format('./model/GoogleNews-  
vectors-negative300.bin', binary=True)  
</code></pre>

<p>I got the following error.</p>

<pre><code>ImportError                               Traceback (most recent call  
last)
&lt;ipython-input-5-50007be813d4&gt; in &lt;module&gt;()
----&gt; 1 import gensim
  2 model = gensim.models.Word2Vec.load_word2vec_format('./model  
/GoogleNews-vectors-negative300.bin', binary=True)

ImportError: No module named 'gensim'
</code></pre>

<p>I installed gensim in python. I use genssim for word2vec.</p>
","8587926","","","","","2021-07-26 12:31:36","gensim error : no module named gensim","<python><linux><gensim><word2vec>","13","2","4","","","CC BY-SA 3.0"
"52584376","1","52585160","","2018-10-01 03:56:34","","0","45","<p>I am able to build the model using the built-in lee_background corpus. But when I try to compare using most_similar method, I get an error.</p>

<pre><code>lee_train_file = '/opt/conda/lib/python3.6/site-packages/gensim/test/test_data/lee_background.cor'

train_corpus=list()
with open(lee_train_file) as f:
    for i, line in enumerate(f):
        train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]))

model = gensim.models.doc2vec.Doc2Vec(vector_size=48, min_count=2, epochs=40)
model.build_vocab(train_corpus)
model.wv.vocab['penalty'].count
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)

line=""""""
dummy text here...
""""""

inferred_vector=model.infer_vector(gensim.utils.simple_preprocess(line) )

model.docvecs.most_similar(inferred_vector, topn=3)
</code></pre>

<p>I tried this with list(inferred_vector) but still getting an error.</p>

<blockquote>
  <p>TypeError: 'numpy.float32' object is not iterable</p>
</blockquote>

<p>I am trying to compare the dummy text with the corpus and find if the entry already exist in the data file.</p>

<hr>

<p>Update:
Instead of list(inferred_vector) I need to use [inferred_vector]. This has solved my problem. But ever-time I run this code, I get different similar documents. How is this possible?</p>

<pre><code>line=""""""
The national executive of the strife-torn Democrats last night appointed little-known West Australian senator Brian Greig 
as interim leader--a shock move likely to provoke further conflict between the party's senators and its organisation. 
In a move to reassert control over the party's seven senators, the national executive last night rejected Aden Ridgeway's 
bid to become interim leader, in favour of Senator John, a supporter of deposed leader Natasha Stott Despoja and an outspoken 
gay rights activist.
""""""

inferred_vector=model.infer_vector(gensim.utils.simple_preprocess(line))

model.docvecs.most_similar([inferred_vector], topn=5)
</code></pre>

<p>Sometimes I get this list and the list keeps changing everytime I run the code even if there is no change in the model.</p>

<pre><code>[(151, 0.5980586409568787),
 (74, 0.5736572742462158),
 (106, 0.5714541077613831),
 (249, 0.5695925951004028),
 (209, 0.5642371773719788)]

[(249, 0.5727256536483765),
 (151, 0.5725511312484741),
 (74, 0.5711895823478699),
 (106, 0.5583171248435974),
 (292, 0.5491517782211304)]
</code></pre>

<p>As a matter of fact, the first line in training corpus is 99% similar to this line because only 1 word is changed. Surprisingly the document_id 1 is nowhere in the top 5 list.</p>
","139150","","139150","","2018-10-01 04:36:58","2018-10-01 05:42:01","compare documents using most similar method","<nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"63299446","1","","","2020-08-07 09:58:07","","0","81","<p>Following is the code :-</p>
<pre><code>modelDoc = Doc2Vec(size=300, window=5, dm=0, dbow_words=1, hs=0, negative=10, alpha=0.05, min_count=20,
                       workers=cores, sample=1e-5, seed=0, iter=10)
    modelDoc.build_vocab(finalSent)
  
    modelDoc.save(save_model)
</code></pre>
<p>my version :
gensim==3.8.1
numpy==1.16.2</p>
<p>after saving the model
only vocab_model file is generated</p>
<p><code>vocab_model.docvecs.doctag_syn0.npy</code> is not generated.
what is the use of this file and does it is necessary to generate this file.</p>
","12004939","","","","","2020-08-07 18:18:11","vocab_model.docvecs.doctag_syn0.npy not generated after saving doc2vec model","<python><numpy><gensim>","1","0","","","","CC BY-SA 4.0"
"63524493","1","63526267","","2020-08-21 14:11:48","","1","73","<p>Using the Gensim package, I have trained a word2vec model on the corpus that I am working with as follows:</p>
<pre><code>word2vec = Word2Vec(all_words, min_count = 3, size = 512, sg = 1)
</code></pre>
<p>Using Numpy, I have initialized a random array with the same dimensions:</p>
<pre><code>vector = (rand(512)-0.5) *20
</code></pre>
<p>Now, I would like to find the words from the word2vec that are most similar to the random vector that I initialized.</p>
<p>For words in the word2vec, you can run:</p>
<pre><code>word2vec.most_similar('word')
</code></pre>
<p>And the output is a list with most similar words and their according distance.</p>
<p>I would like to get a similar output for my initialized array.</p>
<p>However, when I run:</p>
<pre><code>word2vec.most_similar(vector)
</code></pre>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-297-3815cf183d05&gt;&quot;, line 1, in &lt;module&gt;
    word2vec.most_similar(vector)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\utils.py&quot;, line 1461, in new_func1
    return func(*args, **kwargs)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\base_any2vec.py&quot;, line 1383, in most_similar
    return self.wv.most_similar(positive, negative, topn, restrict_vocab, indexer)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 549, in most_similar
    for word, weight in positive + negative:

TypeError: cannot unpack non-iterable numpy.float64 object
</code></pre>
<p>What can I do to overcome this error and find the most similar words to my arrays?</p>
<p>I've checked <a href=""https://stackoverflow.com/questions/54273077/cannot-unpack-non-iterable-numpy-float64-object-python3-opencv"">this</a> and <a href=""https://stackoverflow.com/questions/59357940/typeerror-cannot-unpack-non-iterable-numpy-float64-object"">this</a> page. However, it is unclear to me how I could solve my problem with these suggestions.</p>
","7714681","","130288","","2020-08-21 16:04:19","2020-08-21 16:04:19","Find most similar words to randomy initialized array","<python><numpy><typeerror><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"61253758","1","61326789","","2020-04-16 15:13:45","","1","383","<p>I have a Doc2Vec model created with Gensim and want to use <code>scikit-learn</code> DBSCAN to look for clustering of sentences within the model.</p>

<p>I'm struggling to work out how to best transform the model vectors to work with DBSCAN and plot clusters and am not finding many directly applicable examples on the web.</p>

<p>Here is what I have so far:</p>

<pre class=""lang-python prettyprint-override""><code>import gensim
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

fnIn = 'NLPModels/doc2VecModel_vector_size{0}_epochs{1}.bin'

def doCluster(vector_size, epochs):
    model = gensim.models.doc2vec.Doc2Vec.load(fnIn.format(vector_size, epochs))

    Y = model.docvecs.index2entity # tags

    X = [] # Document vectors
    for tag in Y:
        X.append(model.docvecs[tag])

    db = DBSCAN(eps=.1, min_samples=5, metric='cosine').fit_predict(X)
    labels = set(db)
    print(labels)


doCluster(100, 10)
</code></pre>

<p>Output: <code>{0, 1, -1}</code></p>

<p>Which I believe to be two clusters (0 and 1) and outliers (-1).</p>

<p>Am I going about this in the right way?</p>

<p>How would I plot this on a chart to visualise the clusters?</p>

<p>Thanks.</p>
","7412939","","6573902","","2020-04-20 15:53:14","2020-04-20 15:58:59","Plotting DBSCAN Clustering of Doc2Vec model","<python><machine-learning><scikit-learn><gensim><dbscan>","1","0","","","","CC BY-SA 4.0"
"46047506","1","46059380","","2017-09-05 05:23:46","","0","2838","<p>I have applied Doc2vec to convert documents into vectors.After that, I used the vectors in clustering and figured out the 5 nearest/most similar document to the centroid of each cluster. Now I need to find the most dominant or important terms of these documents so that I can figure out the characteristics of each cluster. 
My question is is there any way to figure out the most dominat or simlar terms/word of a document in Doc2vec . I am using python's gensim package for the Doc2vec implementaton </p>
","2591096","","","","","2018-05-23 20:53:55","How to find most similar terms/words of a document in doc2vec?","<python><cluster-analysis><gensim><word2vec><doc2vec>","2","1","","2017-09-06 23:04:01","","CC BY-SA 3.0"
"63752033","1","63756573","","2020-09-05 08:40:41","","0","212","<p>I want to train a previous-trained word2vec model in a increased way that is update the word's weights if the word has been seen in the previous training process and create and update the weights of the new words that has not been seen in the previous training process. For example:</p>
<pre><code>from gensim.models import Word2Vec
# old corpus
corpus = [[&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;2&quot;, &quot;3&quot;, &quot;1&quot;]]
# first train on old corpus
model = Word2Vec(sentences=corpus, size=2, min_count=0, window=2)
# checkout the embedding weights for word &quot;1&quot;
print(model[&quot;1&quot;])

# here comes a new corpus with new word &quot;4&quot; and &quot;5&quot;
newCorpus = [[&quot;4&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;1&quot;, &quot;5&quot;, &quot;2&quot;]]

# update the previous trained model
model.build_vocab(newCorpus, update=True)
model.train(newCorpus, total_examples=model.corpus_count, epochs=1)

# check if new word has embedding weights:
print(model[&quot;4&quot;])  # yes

# check if previous word's embedding weights are updated
print(model[&quot;1&quot;])  # output the same as before
</code></pre>
<p>It seems that the previous word's embedding is not updated even though the previous word's context has benn changed in the new corpus. Could someone tell me how to make the previous embedding weights updated?</p>
","6670282","","6670282","","2020-09-07 06:26:29","2020-09-07 06:26:29","Gensim Word2vec model is not updating the previous word's embedding weights during increased training","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"35117491","1","","","2016-01-31 18:17:52","","11","8155","<p>I am using pre-trained Google news dataset for getting word vectors by using Gensim library in python</p>

<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>After loading the model I am converting training reviews sentence words into vectors</p>

<pre><code>#reading all sentences from training file
with open('restaurantSentences', 'r') as infile:
x_train = infile.readlines()
#cleaning sentences
x_train = [review_to_wordlist(review,remove_stopwords=True) for review in x_train]
train_vecs = np.concatenate([buildWordVector(z, n_dim) for z in x_train])
</code></pre>

<p>During word2Vec process i get a lot of errors for the words in my corpus, that are not in the model. Problem is how can i retrain already pre-trained model (e.g GoogleNews-vectors-negative300.bin'), in order to get word vectors for those missing words.</p>

<p>Following is what I have tried:
Trained a new model from training sentences that I had</p>

<pre><code># Set values for various parameters
num_features = 300    # Word vector dimensionality                      
min_word_count = 10   # Minimum word count                        
num_workers = 4       # Number of threads to run in parallel
context = 10          # Context window    size                                                                                    
downsampling = 1e-3   # Downsample setting for frequent words

sentences = gensim.models.word2vec.LineSentence(""restaurantSentences"")
# Initialize and train the model (this will take some time)
print ""Training model...""
model = gensim.models.Word2Vec(sentences, workers=num_workers,size=num_features, min_count = min_word_count, 
                      window = context, sample = downsampling)


model.build_vocab(sentences)
model.train(sentences)
model.n_similarity([""food""], [""rice""])
</code></pre>

<p>It worked! but the problem is that I have a really small dataset and less resources to train a large model.</p>

<p>Second way that I am looking at is to extend the already trained model such as GoogleNews-vectors-negative300.bin.</p>

<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
sentences = gensim.models.word2vec.LineSentence(""restaurantSentences"")
model.train(sentences)
</code></pre>

<p>Is it possible and is that a good way to use, please help me out</p>
","2235817","","4583366","","2016-03-14 13:10:58","2016-10-24 11:03:12","Is it possible to re-train a word2vec model (e.g. GoogleNews-vectors-negative300.bin) from a corpus of sentences in python?","<python><nlp><gensim><word2vec>","3","1","7","","","CC BY-SA 3.0"
"46168239","1","46183131","","2017-09-12 05:00:47","","0","828","<p>I'm trying to solve a problem of sentence comparison using naive approach of summing up word vectors and comparing the results. My goal is to match people by interest, so the dataset consists of names and short sentences describing their hobbies. The batches are fairly small, few hundreds of people, so i wanted to give it a try before digging into doc2vec.</p>

<p>I prepare the data by cleaning it completely, removing stop words, tokenizing and lemmatizing. I use pre-trained model for word vectors which returns adequate results when finding similarities for some test words. Also tried summing up the sentence words to find similarities in the original model - the matches do make sense. The similarities would be around general sense of the phrase.</p>

<p>For sentence matching I'm trying the following: create an empty model</p>

<pre><code>b = gs.models.Word2Vec(min_count=1, size=300, sample=0, hs=0)
</code></pre>

<p>Build vocab out of names (or person id's), no training</p>

<pre><code>#first create vocab with an empty vector
test = [['test']]
b.build_vocab(test)
b.wv.syn0[b.wv.vocab['test'].index] = b.wv.syn0[b.wv.vocab['test'].index]*0

#populate vocab from an array
b.build_vocab([personIds], update=True)
</code></pre>

<p>Summ each sentence's word vectors and store the results into the model for a corresponding id</p>

<pre><code>#sentences are pulled from pandas dataset df. 'a' is a pre-trained model i use to get vectors for each word

def summ(phrase, start_model):
    '''
    vector addition function
    '''
    #starting with a vector of 0's
    sum_vec = start_model.word_vec(""cat_NOUN"")*0
    for word in phrase:
        sum_vec += start_model.word_vec(word)
    return sum_vec

for i, row in df.iterrows():
    try:
        personId = row[""ID""]
        summVec = summ(df.iloc[i,1],a)
        #updating syn0 for each name/id in vocabulary
        b.wv.syn0[b.wv.vocab[personId].index] = summVec
    except:
        pass
</code></pre>

<p>I understand that i shouldn't be expecting much accuracy here, but the t-SNE print doesn't show any clustering whatsoever. Finding similarities method also fails to find matches (&lt;0.2 similarity coefficient basically for everything). [<img src=""https://i.stack.imgur.com/w81nX.png"" alt=""]plot of the entire model[1]""></p>

<p>Wondering if anyone has an idea of where did i go wrong? Is my approach valid at all?</p>
","4106227","","","","","2017-09-12 18:22:55","Sentence matching with gensim word2vec: manually populated model doesn't work","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"61396498","1","","","2020-04-23 20:34:51","","1","989","<p>I installed gensim module using (pip install gensim) and it installed successfully</p>

<pre><code>Successfully installed boto-2.49.0 boto3-1.12.45 botocore-1.15.45 docutils-0.15.2 gensim-3.8.2 jmespath-0.9.5 s3transfer-0.3.3 smart-open-1.11.1
</code></pre>

<p>but while importing it on my jupyter notebook, its showing:</p>

<pre><code>unable to import 'smart_open.gcs', disabling that module
</code></pre>
","","user11226067","","","","2020-05-13 06:37:34","Unable to import gensim module","<python-3.x><jupyter-notebook><gensim><word2vec><topic-modeling>","2","5","","","","CC BY-SA 4.0"
"44169631","1","","","2017-05-24 22:42:20","","0","1137","<p>I am trying to use gensim's summarizer and keywords to extract important keywords and summarizing contents. However, I am getting the following error:</p>

<pre><code>from gensim.summarization import summarize 
</code></pre>

<p>Traceback:</p>

<pre><code> ImportError                               Traceback (most recent call last)
&lt;ipython-input-12-70743b938b65&gt; in &lt;module&gt;()
----&gt; 1 from gensim.summarization import summarize

ImportError: No module named summarization
</code></pre>

<p>I checked the version which is gensim 0.10.0. I am using Anaconda distribution and installed gensim using </p>

<pre><code>conda install gensim
</code></pre>

<p>Any help would greatly help. </p>

<p>Thanks </p>
","2307804","","","","","2017-05-26 16:51:40","Getting import error when using gensim.summeraization","<python-2.7><gensim>","1","0","","","","CC BY-SA 3.0"
"61569868","1","","","2020-05-03 05:06:15","","3","1376","<p>gensim's <code>wv.most_similar</code> returns phonologically close words (similar sounds) instead of semantically similar ones. Is this normal? Why might this happen? </p>

<p>Here's the documentation on <code>most_similar</code>: <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar</a></p>

<pre><code>In [144]: len(vectors.vocab)
Out[144]: 32966

... 

In [140]: vectors.most_similar('fight')
Out[140]:
[('Night', 0.9940935373306274),
 ('knight', 0.9928507804870605),
 ('fright', 0.9925899505615234),
 ('light', 0.9919329285621643),
 ('bright', 0.9914385080337524),
 ('plight', 0.9912853240966797),
 ('Eight', 0.9912533760070801),
 ('sight', 0.9908033013343811),
 ('playwright', 0.9905624985694885),
 ('slight', 0.990411102771759)]

In [141]: vectors.most_similar('care')
Out[141]:
[('spare', 0.9710584878921509),
 ('scare', 0.9626247882843018),
 ('share', 0.9594929218292236),
 ('prepare', 0.9584596157073975),
 ('aware', 0.9551078081130981),
 ('negare', 0.9550014138221741),
 ('glassware', 0.9507938027381897),
 ('Welfare', 0.9489598274230957),
 ('warfare', 0.9487678408622742),
 ('square', 0.9473209381103516)]
</code></pre>

<p>The training data contains academic papers and this was my training script: </p>

<pre><code>from gensim.models.fasttext import FastText as FT_gensim
import gensim.models.keyedvectors as word2vec

dim_size = 300
epochs = 10
model = FT_gensim(size=dim_size, window=3, min_count=1)
model.build_vocab(sentences=corpus_reader, progress_per=1000)
model.train(sentences=corpus_reader, total_examples=total_examples, epochs=epochs)

# saving vectors to disk
path = ""/home/ubuntu/volume/my_vectors.vectors""
model.wv.save_word2vec_format(path, binary=True)

# loading vectors 
vectors = word2vec.KeyedVectors.load_word2vec_format(path)
</code></pre>
","7554627","","7554627","","2020-05-07 15:35:25","2020-10-12 21:27:14","Gensim's `model.wv.most_similar` returns phonologically similar words","<python><data-science><gensim><embedding><word-embedding>","3","0","","","","CC BY-SA 4.0"
"52979374","1","54315313","","2018-10-24 23:42:34","","3","741","<p>I have just over 100k word embeddings which I created using gensim, originally each containing 200 dimensions. I've been trying to visualize them within tensorboard's projector but I have only failed so far.
My problem is that tensorboard seems to freeze while computing PCA. At first, I left the page open for 16 hours, imagining that it was just too much to be calculated, but nothing happened. At this point, I started to try and test different scenarios just in case all I needed was more time and I was trying to rush things. The following is a list of my testing so far, all of which failed at the same spot, computing PCA:</p>

<ul>
<li>I plotted only 10 points of 200 dimensions;</li>
<li>I retrained my gensim model so that I could reduce its dimensionality to 100;</li>
<li>Then I reduced it to 10;</li>
<li>Then to 2;</li>
<li>Then I tried plotting only 2 points, i.e. 2 two dimensional points;</li>
</ul>

<p>I am using Tensorflow 1.11;
You can find my last saved tensor flow session <a href=""https://drive.google.com/open?id=10Cnzc2RH9nDFUYrf51r6d8d0jsl0-H6Y"" rel=""nofollow noreferrer"">here</a>, would you mind trying it out?</p>

<p>I am still a beginner, therefore I used a couple tutorial to get me started; I used <a href=""https://github.com/sudharsan13296/visualise-word2vec/blob/master/Word2vec%20Embeddings.ipynb"" rel=""nofollow noreferrer"">Sud Harsan</a> work so far.</p>

<p>Any help is much appreciated. Thanks.</p>

<hr>

<p>Updates:</p>

<p>A) I've found someone else <a href=""https://stackoverflow.com/questions/44054907/tensorfboard-embeddings-hangs-with-computing-pca"">dealing with the same problem</a>; I tried the solution provided, but it didn't change anything. </p>

<p>B) I thought it could have something to do with my installation, therefore I tried uninstalling tensorflow and installing it back; no luck. I then proceeded to create a new environment dedicated to tensorflow and that also didn't work. </p>

<p>C) Assuming there was something wrong with my code, I ran <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">tensorflow's basic embedding tutorial</a> to check if I could open its projector's results. And guess what?! I still can't go past ""Calculating PCA""</p>

<p>Now, I did visit <a href=""https://projector.tensorflow.org"" rel=""nofollow noreferrer"">the online projector example</a> and that loads perfectly. </p>

<p>Again, Any help would be more than appreciated. Thanks!</p>
","8383446","","8383446","","2018-11-02 20:03:05","2019-01-22 19:38:28","Tensorboard projector will compute PCA endlessly","<tensorflow><pca><gensim><tensorboard>","2","0","","","","CC BY-SA 4.0"
"44026369","1","","","2017-05-17 13:31:21","","0","248","<p>I wanted to parallelize the word2vec execution with gensim. Well, let me describe what are the steps I have done:</p>

<ol>
<li>Have installed c compiler using minGW and set the Env variable path</li>
<li>Then I downloaded the .tar.gz file of gensim-2.1.0 and installed gensim through CMD with command ""python setup.py install""</li>
<li>Gensim got installed without any error</li>
<li>My Python version, Gensim FAST_VERSION result is as below:</li>
</ol>

<p><a href=""https://i.stack.imgur.com/5nk70.jpg"" rel=""nofollow noreferrer"">Python, Gensim, FAST_VERSION screenshot</a></p>

<p>N.B: I am using the versions: Windows 7, Python version 2.7.13, gensim version 2.1.0, scipy version 0.18.1</p>
","8025686","","8025686","","2017-05-17 19:38:52","2017-05-24 11:27:10","Couldn't parallelize my execution of word2vec using gensim","<python><cython><gensim>","1","2","","","","CC BY-SA 3.0"
"35276944","1","","","2016-02-08 18:43:09","","3","2833","<p>I am building an RNN model in Keras for sentences with word embeddings from gensim. I am initializing the embedding layer with GloVe vectors. Since this is a sequential model and sentences have variable lengths, vectors are zero-padded. e.g.</p>

<pre><code>[0, 0, 0, 6, 2, 4]
</code></pre>

<p>Let's say the GloVe vectors have dimensions <code>[NUM_VOCAB, EMBEDDING_SIZE]</code>. The zero index is masked (ignored) so to get the proper indexing of words, do we add an extra column to the GloVe matrix so the dimensions are: <code>[NUM_VOCAB+1, EMBEDDING_SIZE]</code>?</p>

<p>Seems like there is an unnecessary vector that the model will estimate unless there is a more elegant way.</p>

<pre><code>glove = Word2Vec.load_word2vec_format(filename)
embedding_matrix = np.vstack([np.zeros(EMBEDDING_SIZE), glove.syn0])

model = Sequential()

# -- this uses Glove as inits
model.add(Embedding(NUM_VOCAB, EMBEDDING_SIZE, input_length=maxlen, mask_zero=True,
                           weights=[embedding_matrix]))

# -- sequence layer
model.add(LSTM(32, return_sequences=False, init='orthogonal'))
model.add(Activation('tanh'))

...
</code></pre>

<p>Thanks</p>
","2802468","","","","","2016-03-12 18:49:05","Index of Embedding layer with zero padding in Keras","<deep-learning><gensim><keras>","0","4","","","","CC BY-SA 3.0"
"44345576","1","44350483","","2017-06-03 15:29:31","","1","1443","<p>I have calculated document similarities using Doc2Vec.docvecs.similarity() in gensim. Now, I would either expect the cosine similarities to lie in the range [0.0, 1.0] if gensim used the absolute value of the cosine as the similarity metric, or roughly half of them to be negative if it does not.</p>

<p>However, what I am seeing is that <em>some</em> similarities are negative, but they are very rare ‚Äì less than 1% of pairwise similarities in my set of 30000 documents.</p>

<p>Why are almost all of the similarities positive?</p>
","1073784","","","","","2017-06-05 14:51:37","Why are almost all cosine similarities positive between word or document vectors in gensim doc2vec?","<python><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 3.0"
"61807128","1","","","2020-05-14 20:48:25","","0","15","<p>I'm actually injecting 77 document in a gensim mode by reading them from a database with a first script and i save the document on file system.</p>

<p>I then load an other doc to check the similarity with a vector</p>

<pre><code>def read_corpus_bdd(cursor, tokens_only=False):
    for i, (url_id, url_label, contenu) in enumerate(cursor):
        tokens = gensim.utils.simple_preprocess(contenu)
        if tokens_only:
            yield tokens
        else:
            # For training data, add tags
            # yield gensim.models.doc2vec.TaggedDocument(tokens, dataLine[0])
            yield gensim.models.doc2vec.TaggedDocument(tokens, [int(str(url_id))])
            print (int(str(url_id)))
targetContentCorpus = list(read_corpus_bdd(cursor))

# Param of trainer corpus
model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=40)

# Build a vocabulary
model.build_vocab(targetContentCorpus)

###############################################################################

model.train(targetContentCorpus, total_examples=model.corpus_count, epochs=model.epochs)

##generate file model name for save
from datetime import date
pathModelSave=os.getenv(""MODEL_BASE_SAVE"") +'/projet_'+ str(projetId)
</code></pre>

<p>When i infer the vector :</p>

<pre><code>inferred_vector = model.infer_vector(test_corpus[0])
sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))

len(sims) #output 335
</code></pre>

<p>So I don't understand where this 335 come from and also why </p>

<pre><code>sims[0][0]
</code></pre>

<p>return other id than the tagged one in the yield section
    enter code here</p>
","1154190","","","","","2020-05-14 20:48:25","Anormal number of sims document in gensim","<gensim>","0","5","","","","CC BY-SA 4.0"
"54917218","1","54917394","","2019-02-28 01:53:58","","0","1821","<p>I have a small python pipeline. One class cleans and lemmatizes the data.
It returns a List of Lists of Strings (i.e., <code>List[List[str]]</code>). I then pass the list on to another class which passes the data to gensim dictionary</p>

<p>The following code, however, throws this exception:</p>

<pre><code>dictionary = corpora.Dictionary(self.bowlist)
AttributeError: 'list' object has no attribute 'bowlist'
</code></pre>

<p>Code:</p>

<pre><code>from typing import List
import re
from gensim import corpora

class ListOfListsToGensimCorpora:
    def __init__(self, bow_list: List[List[str]]):
        self.bowlist = bow_list

    def perform(self):
        dictionary = corpora.Dictionary(self.bowlist)
        print(dictionary)
</code></pre>

<p>I am new to Python but I have checked through debug and other methods, bowlist is a List[List[str]].</p>
","5082504","","","","","2019-02-28 02:44:23","python gensim: AttributeError: 'list' object has no attribute","<python><gensim>","1","5","","","","CC BY-SA 4.0"
"61087427","1","61094830","","2020-04-07 18:58:56","","0","1465","<p>I tried to save word2vec vector as text, but it didnt work out, I got an error, that I dont really understand, what duplicates appear here and what is this ""wv"", that is proposed. Maybe somone can explain is to me. Thank you in advance </p>

<pre><code>model = Word2Vec(all_words, min_count=3, sg = 1, size = 300 )
model.save_word2vec_format('test_w2v.txt', binary=False)
</code></pre>

<pre><code>WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
Word2Vec(vocab=20, size=300, alpha=0.025)
Traceback (most recent call last):
  File ""/word2vec.py"", line 26, in &lt;module&gt;
    model.save_word2vec_format('test_w2v.txt', binary=False)
  File ""/word2vec.py"", line 1307, in save_word2vec_format
    raise DeprecationWarning(""Deprecated. Use model.wv.save_word2vec_format instead."")
DeprecationWarning: Deprecated. Use model.wv.save_word2vec_format instead.

</code></pre>
","6157005","","","","","2020-04-08 06:49:44","saving word2vec in text format","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"52914701","1","52915642","","2018-10-21 11:14:26","","0","1078","<p>I'm try to get started with the gensim library. My goal is pretty simple. I want to use the keywords extraction provided by gensim on a german text. Unfortunately, i'm failing hard.</p>

<p>Gensim comes with a keywords extraction build in, it is build on TextRank. While the results look good on english text, it seems not to work on german. I simple installed gensim via pypi and used it out of the box. Well such AI Products are usually driven by a model. My guess is that gensim comes with a english model. A word2vec model for german is available on a <a href=""https://github.com/devmount/GermanWordEmbeddings"" rel=""nofollow noreferrer"">github page</a>.</p>

<p>But here i'm stuck, i can't find a way how the summarization module of gensim, which provides the <a href=""https://radimrehurek.com/gensim/summarization/keywords.html"" rel=""nofollow noreferrer"">keywords function</a> i'm looking for, can work with a external model.</p>

<p>So the basic question is, how do i load the german model and get keywords from german text?</p>

<p>Thanks</p>
","5945600","","","","","2018-10-21 13:10:32","Gensim Keywords, how to load a german model?","<nlp><keyword><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"61853117","1","","","2020-05-17 14:07:44","","0","57","<p>I have trained a word2vec model using gensim. In the models matrix some values' floating point looks like this: ""-7.18556e-05""</p>

<p>I need to use the values on the matrix as a string. Is there a way to remove those ""e-05"",""e-04"" etc.?</p>

<pre><code>import nltk
from gensim.models import Word2Vec
from nltk.corpus import stopwords

text = ""My text is here""
sentences = nltk.sent_tokenize(text)
for i in range(len(sentences)):
    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]

model = Word2Vec(sentences, min_count=1)

words = model.wv.vocab

for word in words:
    matrix = model.wv[words.keys()]
</code></pre>
","13411541","","","","","2020-05-17 21:48:51","Gensim Word2Vec model floating point","<python><nlp><nltk><gensim>","1","3","","","","CC BY-SA 4.0"
"46297740","1","46300268","","2017-09-19 10:10:57","","2","4083","<p>I have a DataFrame in which the index are words and I have 100 columns with float number such that for each word I have its embedding as a 100d vector. I would like to convert my DataFrame object into a <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">gensim model object</a> so that I can use its methods; specially <code>gensim.models.keyedvectors.most_similar()</code> so that I can search for similar words within my subset.</p>

<p>Which is the preferred way of doing that?</p>

<p>Thanks</p>
","1309231","","","","","2017-09-19 12:17:40","How to turn embeddings loaded in a Pandas DataFrame into a Gensim model?","<python><pandas><gensim>","1","0","3","","","CC BY-SA 3.0"
"52397065","1","52397122","","2018-09-19 02:14:15","","0","66","<p>I would like to load a file for only it's extension name in gensim. </p>

<p>A normal code would be this:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec.load(""news.bin"")
</code></pre>

<p>But I would like it to auto open any file with "".bin"".</p>

<p>Example:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec.load(***I would like to change this part to only load any .bin***)
</code></pre>

<p>.bin files:</p>

<p>It can be ""news.bin"", ""file.bin"" or ""guess.bin"". As long as it load only the extension. Thank you.</p>
","10207014","","","","","2018-09-19 02:29:22","load a file with only its extension name","<python><gensim><word2vec>","2","2","","","","CC BY-SA 4.0"
"61448908","1","61449223","","2020-04-26 22:25:59","","2","449","<p>I have chat interaction [Utterances] between Customer and Advisor and would want to know if the advisor interactions contains particular sentences or similar sentences in the below list:</p>

<p>Example sentences i am looking for in the Advisor interactions  </p>

<pre><code>[""I would be more than happy to help you with this"",
""I would be happy to look over the account to see how I can help get this sorted out for you"",
""I‚Äôd be more than happy to look into this for you!"",
""Oh, I see, let me assist you with this concern."",
""I am more than happy to do everything I can to resolve this matter for you."",
""I would be happy to look over the account to see how I can help get this sorted out for you."",
""I am happy to have a look.""]


I have a dataset which contains the list of interaction_id and Utterances(Sample below)

```Example Chat interaction between Advisor and CLient : 
Client : Hello I would like to place an order for replacement battery
Agent: Hi Welcome to Battery service department. I would be happy to help you with your battery replacement Order.
</code></pre>

<p>How do get/Extract the sentences with similar intent or meaning.
I am newbie to NLP and i believe I have a sentences classification/Extraction problem in hand and would like to know is there any way i can achieve what i need</p>

<p>Basically I am trying to achieve the below:  </p>

<pre><code>ID    Utt                                               Help_Stmt_Present

IRJST   Hi Welcome to Battery service department. 
        I would be happy to help you with your battery
        replacement Order.                                     Yes 


</code></pre>
","13008461","","13008461","","2020-04-26 22:33:57","2020-04-26 22:58:23","How to extract sentences which has similar meaning/intent compared against a example list of sentences","<python-3.x><nlp><gensim><doc2vec><sentence-similarity>","1","0","2","","","CC BY-SA 4.0"
"26251674","1","26252270","","2014-10-08 07:45:48","","5","4164","<p>I am trying to install gensim python library. However I am facing some dependencies errors. I ve isntalled schipy and numpy throught canopy. Next step to use pip install gensim in order to get gensim package. However I am getting error messages. I have installed python 2.7.4. I ve got visual studio 2010 installed on my machine.</p>

<p><img src=""https://i.stack.imgur.com/2dPne.jpg"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/7R2g3.jpg"" alt=""enter image description here""></p>
","1194864","","1194864","","2014-10-08 07:56:49","2014-10-08 08:22:32","Installing gensim in windows 7","<python><gensim>","1","2","2","","","CC BY-SA 3.0"
"53195906","1","","","2018-11-07 18:49:45","","11","10766","<p>I'm working on project using Word2vec and gensim,</p>
<pre><code>model = gensim.models.Word2Vec(
    documents = 'userDataFile.txt',
    size=150,
    window=10,
    min_count=2,
    workers=10)
model = gensim.model.Word2Vec.load(&quot;word2vec.model&quot;)
model.train(documents, total_examples=len(documents), epochs=10)
model.save(&quot;word2vec.model&quot;)
</code></pre>
<p>this is the part code that I have at the moment, and I'm getting this error below</p>
<blockquote>
<pre><code>Traceback (most recent call last):
File &quot;C:\Users\User\Desktop\InstaSubProject\templates\HashtagData.py&quot;, line

37, in &lt;module&gt;
workers=10)
TypeError: __init__() got an unexpected keyword argument 'documents'
</code></pre>
</blockquote>
<p><code>UserDataFile.txt</code> is the file that I stored output result data that I got from web scraping.</p>
<p>I'm not really sure what I need to fix here.</p>
<p>Thank you in advance !</p>
","10548370","","2318649","","2021-02-12 10:33:35","2021-06-15 04:12:05","Getting ""__init__() got an unexpected keyword argument 'document'"" this error in python I'm working with Word2Vec and gensim","<python><gensim><word2vec>","4","1","3","","","CC BY-SA 4.0"
"37335842","1","37502976","","2016-05-19 23:49:10","","3","7598","<p>I trained a gensim.models.doc2vec.Doc2Vec model<br>
d2v_model = Doc2Vec(sentences, size=100, window=8, min_count=5, workers=4)
and I can get document vectors by
docvec = d2v_model.docvecs[0]</p>

<p>How can I get word vectors from trained model ?</p>
","3723683","","","","","2018-08-14 03:07:06","How to get word vectors from a gensim Doc2Vec?","<gensim><word2vec><doc2vec>","2","0","3","","","CC BY-SA 3.0"
"54911712","1","","","2019-02-27 17:56:52","","0","124","<p>This may seem like an odd question but I'm new to this so thought I'd ask anyway.</p>

<p>I want to use this Google News model over various different files on my laptop. This means I will be running this line over and over again in different Jupyter notebooks: </p>

<p>model=word2vec.KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin"",binary=True)</p>

<p>Does this eat 1) Storage (I've noticed my storage filling up exponentially for no reason) 
2) Less memory than it would otherwise if I close the previous notebook before running the next.</p>

<p>My storage has gone down by 50GB in one day and the only thing I have done on this computer is run the Google News model (I didn't do most_similar()). Restarting and closing notebooks hasn't helped and there aren't any big files on the laptop. Any ideas?</p>

<p>Thanks. </p>
","8322222","","8322222","","2019-02-28 11:37:21","2019-02-28 11:37:21","Does the Google News Word2Vec model take up storage every time you run it?","<python><nlp><gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"55016629","1","55018001","","2019-03-06 06:07:27","","0","1270","<p>I downloaded pretrained word vector file (.bin) from facebook (<a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a>)
However, when I tried to use this model it happens to make error.</p>

<pre><code>from gensim.models import FastText
fasttext_model = FastText.load_fasttext_format('cc.ko.300.bin', encoding='utf8')

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte
</code></pre>

<p>But weird thing is that it operates well when I use old version bin file (<a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/pretrained-vectors.html</a>)</p>

<p>What is wrong with these files?? And how can I fix it??</p>

<p>And I must use bin file because I need all n-grams to prevent OOV. So, solutions like 'use .vec file' couldn't be any help.</p>

<p>Thank you so much :)</p>
","7574561","","","","","2019-08-10 18:12:36","Facebook fasttext bin model UnicodeDecodeError","<python><facebook><utf-8><gensim><fasttext>","3","0","0","","","CC BY-SA 4.0"
"61343809","1","","","2020-04-21 12:41:06","","0","37","<p>I have a list of 10 sentences from a text file.
I want to use an existing topics model to get the topics of every sentence.</p>

<p>In all the tutorials I found - they trained the topic model on their corpus. I want to use one that was trained in existing corpus and just apply it on my sentences.</p>

<p>Is this possible?</p>
","6057371","","","","","2020-04-21 17:10:35","get topic of sentence from pre-trained model","<nlp><gensim><lda><topic-modeling>","1","1","","","","CC BY-SA 4.0"
"43868822","1","","","2017-05-09 11:39:10","","0","360","<p>In order to use the Latent semantic indexation method from gensim, I want to begin with a small ""classique"" example like :</p>

<pre><code>import logging, gensim, bz2
id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')
mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')
lsi = gensim.models.lsimodel.LsiModel(corpus=mm, id2word=id2word, num_topics=400)
etc..
</code></pre>

<p>My question is : How to get the corpus iterator 'wiki_en_tfidf.mm' ? Must I download it from somewhere ? I have searched on the Internet but I did not find anything. Help please ? </p>
","6485635","","","","","2017-05-09 13:48:09","Latent Semantic Indexation with gensim","<gensim><wikidata><latent-semantic-indexing><bz2><latent-semantic-analysis>","1","0","","","","CC BY-SA 3.0"
"61949436","1","","","2020-05-22 06:57:42","","1","158","<p>I'm trying to use <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"" rel=""nofollow noreferrer"">Gensim implementation of LDA</a> which suggests using automatic learning of hyperparameters <code>alpha</code> and <code>eta</code>:</p>

<blockquote>
  <p>We set <code>alpha = 'auto'</code> and <code>eta = 'auto'</code>. Again this is somewhat technical, but essentially we are automatically learning two parameters in the model that we usually would have to specify explicitly.</p>
</blockquote>

<p>However, after seeing <a href=""https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"" rel=""nofollow noreferrer"">this article</a> about LDA hyperparameter tuning, I can see that it is also possible to tune these parameters as black-box: train the model with different fixed values of parameters, and then select the best one:</p>

<blockquote>
  <p>Let‚Äôs call the function, and iterate it over the range of topics, alpha, and beta parameter values</p>
</blockquote>

<p>Is there any essential difference between these two methods? Is there any special case when the second method is better than the first one?</p>
","2207445","","","","","2020-05-22 06:57:42","What's the difference between automatic and manual LDA hyperparameter tuning?","<gensim><lda><topic-modeling><hyperparameters>","0","0","","","","CC BY-SA 4.0"
"61235170","1","61239081","","2020-04-15 17:43:48","","0","55","<p>I am trying to figure out solution for requirement where in I am required to map long text to unigrams or bigrams. 
For example
""Ability to motivate and manage team. You should be able to track the progress of the team and intervene to improve the progress"". This long text should be mapped to ""Team management"". Basically I am trying to figure out communication/analytical skills from the long text seen in document like Job descriptions. I am struggling to figure out a solution for this. I do not want to hard code as the long text keep changing. Thanks for any help.</p>
","3910430","","","","","2020-04-15 21:39:32","How to map detailed text to a unigram or a bigram","<python-3.x><nlp><cluster-analysis><gensim><topic-modeling>","1","3","","","","CC BY-SA 4.0"
"35121779","1","35376693","","2016-02-01 01:33:53","","0","358","<p>I was going through this website yesterday (<a href=""http://rutumulkar.com/blog/2015/word2vec/"" rel=""nofollow"">http://rutumulkar.com/blog/2015/word2vec/</a>)  and the author made use of the file <code>text8-queen</code>. In his script I noticed that she did not specify the location of the file and I was wondering how was he able to run it? I am unable to run it? Is there a way to run this file? Thank you.</p>

<p>The script is as follows:</p>

<pre><code>import gensim.models
import time
time1 = time.time()

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)


modelbase = gensim.models.Word2Vec()
sentences2 = gensim.models.word2vec.Sentences(""text8-queen"")
modelbase.build_vocab(sentences2)
modelbase.train(sentences2)
modelbase.save_word2vec_format(""wordvectors/model-text8-queen-only"")
modelbase.accuracy(""questions-words.txt"")

model = gensim.models.Word2Vec()
sentences = gensim.models.word2vec.Sentences(""text8-rest"")
model.build_vocab(sentences)
model.train(sentences)
model.save_word2vec_format(""model-text8-rest"")
model.accuracy(""questions-words.txt"")

sentences2 = gensim.models.word2vec.Sentences(""text8-queen"")
model.update_vocab(sentences2)
model.train(sentences2)
model.save_word2vec_format(""wordvectors/model-text8-queen"")
model.accuracy(""questions-words.txt"")

model1 = gensim.models.Word2Vec()
sentences = gensim.models.word2vec.Sentences(""text8-all"")
model1.build_vocab(sentences)
model1.train(sentences)
model1.save_word2vec_format(""wordvectors/model-text8-all"")
model1.accuracy(""questions-words.txt"")
print (""total time: %s"" % (time.time() - time1))
</code></pre>

<p>My question is in the in the line:  </p>

<pre><code>sentences = gensim.models.word2vec.Sentences(""text8-rest"")
</code></pre>

<p>how did the author call <code>text8-rest</code> and <code>text8-queen</code>? where should I put these text file (<code>text8-rest</code>, <code>text8-queen</code>) ? Do I have to specify the location of the text file or is python able to detect it?</p>
","5391187","","1019952","","2016-05-17 17:25:49","2016-05-17 17:25:49","How can I run this gensim code? Do I need some text files?","<python><gensim>","1","3","","","","CC BY-SA 3.0"
"52707075","1","52708247","","2018-10-08 17:07:49","","0","84","<p>I am training a doc2vec model with multiple tags, so it includes the typical doc ""ID"" tag and then it also contains a label tag ""Category 1."" I'm trying to graph the results such that I get the doc distribution in a 2d (using LargeVis) but am able to color different tags. My problem is that the vectors the model returns exceed the number of training observations by 5 making difficult to align the original tags with the vectors: </p>

<pre><code>In[1]: data.shape 
Out[1]: (17717,5)
</code></pre>

<p>Training the model on 100 parameters  </p>

<pre><code>In[2]: model.docvecs.doctag_syn0.shape
Out[2]: (17722,100) 
</code></pre>

<p>I have no idea whether the 5 additional observations shift the order of the vectors or whether they're just appended to the end. I want to avoid using string tags for the doc IDs because I am preparing this code to use on a much larger dataset.
I found an explanation in a google group <a href=""https://groups.google.com/forum/#!topic/gensim/OdvQkwuADl0"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/gensim/OdvQkwuADl0</a>
which explained that using multiple tags per doc can result in this type of output. However, I haven't been able to find a way to avoid or correct it in any forum or documentation. </p>
","1920550","","","","","2018-10-08 18:37:04","Cannot align graph because multiple tag doc2vec returning more items in doctag_syn0 than there are in the training data","<python><machine-learning><nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"44060759","1","","","2017-05-19 03:08:01","","1","2626","<p>I used the gensim LDAModel for topic extraction for customer reviews as follows:</p>

<pre><code>dictionary = corpora.Dictionary(clean_reviews)
dictionary.filter_extremes(keep_n=11000) #change filters
dictionary.compactify()
dictionary_path = ""dictionary.dict""
corpora.Dictionary.save(dictionary, dictionary_path)

# convert tokenized documents to vectors

corpus = [dictionary.doc2bow(doc) for doc in clean_reviews]
vocab = lda.datasets.load_reuters_vocab()  

# Training lda using number of topics set = 10 (which can be changed)

lda = gensim.models.LdaModel(corpus, id2word = dictionary,
                        num_topics = 20,
                        passes = 20,
                        random_state=1,
                        alpha = ""auto"")
</code></pre>

<p>This returns unigrams in topics like:</p>

<pre><code>topic1 -delivery,parcel,location

topic2 -app, login, access
</code></pre>

<p>But I am looking for ngrams. I came across sklearn's LatentDirichletAllocation which uses Tfidf vectorizer as follows:</p>

<pre><code>vectorizer = TfidfVectorizer(analyzer='word', ngram_range=[2,5], stop_words='english', min_df=2)    
X = vectorizer.fit_transform(new_review_list)
clf = decomposition.LatentDirichletAllocation(n_topics=20, random_state=3, doc_topic_prior = .1).fit(X)
</code></pre>

<p>where we can specify range for ngrams in the vectorizer. Is it possible to do so in the gensim LDA Model as well.</p>

<p>Sorry, I'm very new to using all these models, so don't know much about them.</p>
","7782626","","3374996","","2017-05-19 05:05:27","2018-10-30 16:08:39","How to implement Latent Dirichlet Allocation to give bigrams/trigrams in topics instead of unigrams","<python><scikit-learn><nlp><gensim><lda>","1","3","3","","","CC BY-SA 3.0"
"61041080","1","61047823","","2020-04-05 10:19:12","","0","122","<p>Doubt - 1</p>

<p>I am training Doc2Vec with 150000 documents. Since these documents are from legal domain they are really hard to clean and get it ready for further training. Hence I decided to remove all the periods from a document. Having said that, I am confused on how the parameter of <code>Window_size</code> in doc2vec recognize the sentences now. There are two views presented in the question :<a href=""https://stackoverflow.com/questions/42242521/doc2vec-differentiate-sentence-and-document"">Doc2Vec: Differentiate Sentence and Document</a> </p>

<ol>
<li>The algorithm just works on chunks of text, without any idea of what a sentence/paragraph/document etc might be.</li>
<li>It's even common for the tokenization to retain punctuation, such as the periods between sentences, as standalone tokens.</li>
</ol>

<p>Therefore I am in confusion if my adopted approach of eliminating the punctuation (periods) is right. Kindly provide me with some supportive answers.</p>

<p>Doubt-2</p>

<p>The documents that I scrapped range from 500 - 5500 tokens hence my approach to have a pretty even sized documents for training doc2vec and even to reduce the vocabulary is :
Consider a document of size greater than 1500 tokens in this case I make use of First 50 to 400 tokens + 600 to 1000 tokens + last 250 tokens. The motivation for this kind of approach is from a paper related to Classification of documents using BERT where the sequence of 512 tokens were generated like this.</p>

<p>So I want to know if this idea is somewhat good to proceed or it's not recommended to do this?</p>

<p><strong>Update</strong> - I just saw the common_text corpus used by gensim in the tutorial link <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a> and found that the documents in that corpus are simply tokens of words and do not contain any punctuation.
 eg: </p>

<p><code>from gensim.test.utils import common_texts, common_dictionary, common_corpus</code></p>

<p><code>print(common_texts[0:10])</code></p>

<p>Output:</p>

<p><code>[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]</code></p>

<p>Same has been followed in the tutorial <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html</a>.
So is my approach of removing periods in the document valid, if so then how will the window parameter work because in the documentation it is defined as follows:
window (int, optional) ‚Äì The maximum distance between the current and predicted word within a sentence.</p>
","10900497","","10900497","","2020-04-05 17:55:09","2020-04-05 18:55:12","significance of periods in sentences while training documents with Doc2Vec","<python><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"61198009","1","","","2020-04-13 22:47:13","","0","448","<p>For reference, I already looked at the following questions: </p>

<ol>
<li><a href=""https://stackoverflow.com/questions/31742630/gensim-lda-for-text-classification"">Gensim LDA for text classification</a></li>
<li><a href=""https://stackoverflow.com/questions/60420718/python-gensim-lda-model-show-topics-funciton"">Python Gensim LDA Model show_topics funciton</a></li>
</ol>

<p>I am looking to have my LDA model trained from Gensim classify a sentence under one of the topics that the model creates. 
Something long the lines of </p>

<pre><code>lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=7, passes=20)
lda.print_topics()
for line in document: # where each line in the document is its own sentence for simplicity
    print('Sentence: ', line)
    topic = lda.parse(line) # where the classification would occur
    print('Topic: ', topic)
</code></pre>

<p>I know gensim does not have a <code>parse</code> function, but how would one go about accomplishing this? Here is the documentation that I've been following but I haven't gotten anywhere with it: </p>

<p><a href=""https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py</a></p>

<p>Thanks in advance. </p>

<p>edit: More documentation-  <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a></p>
","5701535","","6573902","","2021-01-02 10:28:23","2021-01-02 10:28:23","Classify Text with Gensim LDA Model","<python><python-3.x><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"27104847","1","","","2014-11-24 12:26:04","","1","548","<p><code>Scikit-learn</code> is a machine learning library that requires these dependencies, <a href=""http://scikit-learn.org/stable/install.html"" rel=""nofollow"">http://scikit-learn.org/stable/install.html</a>: </p>

<pre><code>sudo apt-get install build-essential python-dev python-setuptools \
                     python-numpy python-scipy \
                     libatlas-dev libatlas3gf-base
</code></pre>

<p>before a user can:</p>

<pre><code>pip install -U scikit-learn
</code></pre>

<p>And <code>gensim</code> requires no additional dependencies, <a href=""http://radimrehurek.com/gensim/install.html"" rel=""nofollow"">http://radimrehurek.com/gensim/install.html</a> and simply do: </p>

<pre><code>pip install --upgrade gensim
</code></pre>

<p>I have my <code>setup.py</code> as such without <code>scikit</code>, its dependencies and <code>gensim</code> from <a href=""https://github.com/alvations/pywsd/blob/master/setup.py"" rel=""nofollow"">https://github.com/alvations/pywsd/blob/master/setup.py</a>:</p>

<pre><code>from distutils.core import setup

setup(
    name='pywsd',
    version='0.1',
    packages=['pywsd',],
    long_description='Python Implementations of Word Sense Disambiguation (WSD) technologies',
)
</code></pre>

<p>How do I add <code>scikit-learn</code> and <code>gensim</code> into my <code>setup.py</code>?</p>
","610569","","","","","2014-11-24 12:26:04","How to add scikit-learn and gensim into my libraries' setup.py?","<python><installation><scikit-learn><gensim>","0","9","","","","CC BY-SA 3.0"
"43712401","1","43712604","","2017-04-30 23:25:09","","-1","113","<p>I have those 2 functions which differs in only 1 line, so to avoid code duplication, I want to create a base class with a general form of those functions then inherit it for each class.</p>

<p>function 1:</p>

<pre><code>def top_similar_traces(self, stack_trace, top=10):
        words_to_test = StackTraceProcessor.preprocess(stack_trace)
        words_to_test_clean = [w for w in np.unique(words_to_test).tolist() if w in model]

        # Cos-similarity
        all_distances = np.array(1.0 - np.dot(model.wv.syn0norm, model.wv.syn0norm[
            [model.wv.vocab[word].index for word in words_to_test_clean]].transpose()), dtype=np.double)

        for i, (doc_id, rwmd_distance) in enumerate(distances):

            doc_words_clean = [w for w in self.corpus[doc_id] if w in model]
            wmd = self.wmdistance(model, words_to_test_clean, doc_words_clean, all_distances)

        return sorted(similarities, key=lambda v: v[1])[:top]
</code></pre>

<p>function 2:</p>

<pre><code>def top_similar_traces(self, stack_trace, top=10):
        words_to_test = StackTraceProcessor.preprocess(stack_trace)
        words_to_test_clean = [w for w in np.unique(words_to_test).tolist() if w in model]

        # Cos-similarity
        all_distances = np.array(1.0 - np.dot(model.wv.syn0norm, model.wv.syn0norm[
            [model.wv.vocab[word].index for word in words_to_test_clean]].transpose()), dtype=np.double)

        for i, (doc_id, rwmd_distance) in enumerate(distances):

            doc_words_clean = [w for w in self.corpus[doc_id].words if w in model]
            wmd = self.wmdistance(model, words_to_test_clean, doc_words_clean, all_distances)

        return sorted(similarities, key=lambda v: v[1])[:top]
</code></pre>

<p>You can see the only difference is at</p>

<pre><code>        doc_words_clean = [w for w in self.corpus[doc_id].words if w in model]
        doc_words_clean = [w for w in self.corpus[doc_id] if w in model]
</code></pre>
","3134867","","3134867","","2018-02-17 06:20:13","2018-02-17 06:20:13","How to refactor repeated code","<python><gensim>","3","2","","","","CC BY-SA 3.0"
"44432464","1","","","2017-06-08 09:55:47","","1","112","<p>Code:-</p>

<pre><code>import sys
from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
model = Doc2Vec.load('sentiment140.d2v')

if len(sys.argv) &lt; 4:
    print (""Please input train_pos_count, train_neg_count and classifier!"")
    sys.exit()

train_pos_count = int(sys.argv[1])
train_neg_count = int(sys.argv[2])
test_pos_count = 144
test_neg_count = 144

print (train_pos_count)
print (train_neg_count)

vec_dim = 100

print (""Build training data set..."")
train_arrays = numpy.zeros((train_pos_count + train_neg_count, vec_dim))
train_labels = numpy.zeros(train_pos_count + train_neg_count)

for i in range(train_pos_count):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    train_arrays[i] = model.docvecs[prefix_train_pos]
    train_labels[i] = 1

for i in range(train_neg_count):
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    train_arrays[train_pos_count + i] = model.docvecs[prefix_train_neg]
    train_labels[train_pos_count + i] = 0


print (""Build testing data set..."")
test_arrays = numpy.zeros((test_pos_count + test_neg_count, vec_dim))
test_labels = numpy.zeros(test_pos_count + test_neg_count)

for i in range(test_pos_count):
    prefix_test_pos = 'TEST_POS_' + str(i)
    test_arrays[i] = model.docvecs[prefix_test_pos]
    test_labels[i] = 1

for i in range(test_neg_count):
    prefix_test_neg = 'TEST_NEG_' + str(i)
    test_arrays[test_pos_count + i] = model.docvecs[prefix_test_neg]
    test_labels[test_pos_count + i] = 0


print (""Begin classification..."")
classifier = None
if sys.argv[3] == '-lr':
    print (""Logistic Regressions is used..."")
    classifier = LogisticRegression()
elif sys.argv[3] == '-svm':
    print (""Support Vector Machine is used..."")
    classifier = SVC()
elif sys.argv[3] == '-knn':
    print (""K-Nearest Neighbors is used..."")
    classifier = KNeighborsClassifier(n_neighbors=10)
elif sys.argv[3] == '-rf':
    print (""Random Forest is used..."")
    classifier = RandomForestClassifier()

classifier.fit(train_arrays, train_labels)

print (""Accuracy:"", classifier.score(test_arrays, test_labels))
</code></pre>

<p>Getting the below error:-
2017-06-08 15:24:18,013 : INFO : loading Doc2Vec object from C:/Users/Desktop/sentiment140.d2v
2017-06-08 15:24:21,556 : INFO : loading wv recursively from C:/Users/Desktop/sentiment140.d2v.wv.* with mmap=None
2017-06-08 15:24:21,556 : INFO : setting ignored attribute syn0norm to None
2017-06-08 15:24:21,571 : INFO : loading docvecs recursively from C:/Users/Desktop/sentiment140.d2v.docvecs.* with mmap=None
2017-06-08 15:24:21,571 : INFO : setting ignored attribute cum_table to None
2017-06-08 15:24:21,571 : INFO : loaded C:/Users/Desktop/sentiment140.d2v
Please input train_pos_count, train_neg_count and classifier!
C:\Users\AppData\Local\Continuum\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
  warn(""To exit: use 'exit', 'quit', or Ctrl-D."", stacklevel=1)
An exception has occurred, use %tb to see the full traceback.</p>

<p>SystemExit</p>
","8101003","","","","","2017-06-08 09:55:47","Getting system exit error while modelling test data","<python><gensim><doc2vec>","0","2","","","","CC BY-SA 3.0"
"62007088","1","62010520","","2020-05-25 16:54:27","","2","197","<p>My question is <strong>how I should interpret my situation?</strong></p>

<p>I trained a Doc2Vec model following this tutorial <a href=""https://blog.griddynamics.com/customer2vec-representation-learning-and-automl-for-customer-analytics-and-personalization/"" rel=""nofollow noreferrer"">https://blog.griddynamics.com/customer2vec-representation-learning-and-automl-for-customer-analytics-and-personalization/</a>. </p>

<p>For some reason, <code>doc_model.docvecs.doctags</code> returns <code>{}</code>. But <code>doc_model.docvecs.vectors_docs</code> seems to return a proper value.</p>

<p>Why the doc2vec object doesn't return any doctags but vectors_docs?</p>

<p>Thank you for any comments and answers in advance.</p>

<p>This is the code I used to train a Doc2Vec model.</p>

<pre><code>from gensim.models.doc2vec import LabeledSentence, TaggedDocument, Doc2Vec
import timeit
import gensim

embeddings_dim = 200    # dimensionality of user representation

filename = f'models/customer2vec.{embeddings_dim}d.model'
if TRAIN_USER_MODEL:

    class TaggedDocumentIterator(object):
        def __init__(self, df):
           self.df = df
        def __iter__(self):
            for row in self.df.itertuples():
                yield TaggedDocument(words=dict(row._asdict())['all_orders'].split(),tags=[dict(row._asdict())['user_id']])

    it = TaggedDocumentIterator(combined_orders_by_user_id)

    doc_model = gensim.models.Doc2Vec(vector_size=embeddings_dim, 
                                      window=5, 
                                      min_count=10, 
                                      workers=mp.cpu_count()-1,
                                      alpha=0.055, 
                                      min_alpha=0.055,
                                      epochs=20)   # use fixed learning rate

    train_corpus = list(it)

    doc_model.build_vocab(train_corpus)

    for epoch in tqdm(range(10)):
        doc_model.alpha -= 0.005                    # decrease the learning rate
        doc_model.min_alpha = doc_model.alpha       # fix the learning rate, no decay
        doc_model.train(train_corpus, total_examples=doc_model.corpus_count, epochs=doc_model.iter)
        print('Iteration:', epoch)

    doc_model.save(filename)
    print(f'Model saved to [{filename}]')

else:
    doc_model = Doc2Vec.load(filename)
    print(f'Model loaded from [{filename}]')
</code></pre>

<p><code>doc_model.docvecs.vectors_docs</code> returns<a href=""https://i.stack.imgur.com/bN6r4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bN6r4.jpg"" alt=""enter image description here""></a></p>
","3480376","","","","","2020-05-25 20:45:35","Why does a Gensim Doc2vec object return empty doctags?","<gensim><doc2vec>","1","0","1","","","CC BY-SA 4.0"
"52861807","1","","","2018-10-17 18:55:52","","1","482","<p>I am trying to make a model with the Gensim library. I am using python 3 and Spyder. I also want to incorporate the wiki corpus. The code is shown below:</p>

<pre><code>enter code hereimport os
import sys
import bz2
import logging
import multiprocessing
import gensim

SCRIPT_PATH = os.path.dirname(os.path.realpath(__file__))
DATA_PATH   = os.path.join(SCRIPT_PATH, 'data/')
MODEL_PATH  = os.path.join(SCRIPT_PATH, 'model/')

DICTIONARY_FILEPATH = os.path.join(DATA_PATH, 'wiki-english_wordids.txt.bz2')
WIKI_DUMP_FILEPATH = os.path.join(DATA_PATH, 'enwiki-latest-pages- 
articles.xml.bz2')

if __name__ == '__main__':

# Check if the required files have been downloaded
if not WIKI_DUMP_FILEPATH:
    print('Wikipedia articles dump could not be found..')
    print('Please see README.md for instructions!')
    sys.exit()


# Get number of available cpus
cores = multiprocessing.cpu_count()

if not os.path.exists(MODEL_PATH):
    os.makedirs(MODEL_PATH)

# Initialize logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

if not os.path.isfile(DICTIONARY_FILEPATH):
    logging.info('Dictionary has not been created yet..')
    logging.info('Creating dictionary (takes about 9h)..')

    # Construct corpus
    wiki = gensim.corpora.WikiCorpus(WIKI_DUMP_FILEPATH)

    # Remove words occuring less than 20 times, and words occuring in more
    # than 10% of the documents. (keep_n is the vocabulary size)
    wiki.dictionary.filter_extremes(no_below=20, no_above=0.1, keep_n=100000)

    # Save dictionary to file
    wiki.dictionary.save_as_text(DICTIONARY_FILEPATH)
    del wiki

# Load dictionary from file
dictionary = gensim.corpora.Dictionary.load_from_text(DICTIONARY_FILEPATH)

# Construct corpus using dictionary
wiki = gensim.corpora.WikiCorpus(WIKI_DUMP_FILEPATH, dictionary=dictionary)

class SentencesIterator:
    def __init__(self, wiki):
        self.wiki = wiki

    def __iter__(self):
        for sentence in self.wiki.get_texts():
            yield list(map(lambda x: x.decode('utf-8'), sentence))

# Initialize simple sentence iterator required for the Word2Vec model
sentences = SentencesIterator(wiki)

logging.info('Training word2vec model..')
model = gensim.models.Word2Vec(sentences=sentences, size=300, min_count=1, window=5, workers=cores)

# Save model
logging.info('Saving model..')
model.save(os.path.join(MODEL_PATH, 'word2vec.model'))
logging.info('Done training word2vec model!')
</code></pre>

<p>But I am getting the following error:</p>

<pre><code>File ""C:/Users/elli/.spyder-py3/temp.py"", line 60, in &lt;lambda&gt;
yield list(map(lambda x: x.decode('utf-8'), sentence))

AttributeError: 'str' object has no attribute 'decode'
</code></pre>

<p>This code was from github from this link:
<a href=""https://github.com/LasseRegin/gensim-word2vec-model/blob/master/train.py"" rel=""nofollow noreferrer"">https://github.com/LasseRegin/gensim-word2vec-model/blob/master/train.py</a>.</p>

<p>I suspect this should be something simple to sort. Could you please advise?</p>
","5683645","","5683645","","2018-10-17 19:02:29","2018-11-01 18:41:05","Gensim Doc2Vec Exception AttributeError: 'str' object has no attribute 'decode'","<python><python-3.x><gensim>","1","0","","","","CC BY-SA 4.0"
"62045818","1","","","2020-05-27 14:39:55","","0","76","<p>I am doing a natural language process project, when I try to use gensim'API to get LSI similarity matrix, every time I run my code, the LSImodel gives me a different similarity matrix. they are not totally different, but slightly different, like last time one of the number is -0.42562, but it change to -0.42116 next time I run my code. It makes my rest analysis change totally.</p>

<pre><code>Lsi = gensim.models.LsiModel
lsimodel = Lsi(corpus_tfidf, id2word=dictionary, num_topics=20)
lsi_similarity = similarities.MatrixSimilarity(lsimodel[corpus_tfidf])
</code></pre>

<p>I have checked that my input corpus_tfidf and dictionary is the same every time. why would this happen? is there some solution for it?</p>
","12413780","","","","","2020-05-27 14:39:55","why Lsimodel from Gensim show different output while taking the same input?","<python><nlp><gensim><lsa>","0","3","","","","CC BY-SA 4.0"
"44143441","1","44146551","","2017-05-23 19:26:36","","2","352","<p>I am using the <a href=""https://github.com/RaRe-Technologies/w2v_server_googlenews"" rel=""nofollow noreferrer"">w2v_server_googlenews</a> code from the word2vec HTTP server running at <a href=""https://rare-technologies.com/word2vec-tutorial/#bonus_app"" rel=""nofollow noreferrer"">https://rare-technologies.com/word2vec-tutorial/#bonus_app</a>. I changed the loaded file to a file of vectors trained with the original C version of word2vec. I load the file with </p>

<pre><code>gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)
</code></pre>

<p>and it seems to load without problems. But when I test the HTTP service with, let's say </p>

<pre><code>curl 'http://127.0.0.1/most_similar?positive%5B%5D=woman&amp;positive%5B%5D=king&amp;negative%5B%5D=man' 
</code></pre>

<p>I got an empty result with only the execution time.</p>

<pre><code>{""taken"": 0.0003361701965332031, ""similars"": [], ""success"": 1}
</code></pre>

<p>I put a <code>traceback.print_exc()</code> on the except part of the related method, which is in this case <code>def most_similar(self, *args, **kwargs):</code> and I got: </p>

<pre><code>Traceback (most recent call last):
  File ""./w2v_server.py"", line 114, in most_similar
    topn=5)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py"", line 304, in most_similar
    self.init_sims()
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py"", line 817, in init_sims
    self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)
AttributeError: 'KeyedVectors' object has no attribute 'syn0'
</code></pre>

<p>Any idea on why this might happens? </p>

<p>Note: I use python 2.7 and I installed gensim using pip, which gave me gensim 2.1.0.</p>
","1264899","","","","","2017-05-23 23:21:57","Code for gensim Word2vec as an HTTP service 'KeyedVectors' Attribute error","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"61182206","1","","","2020-04-13 05:49:23","","4","3830","<p>I have written this code</p>

<pre><code>import gensim
from gensim import corpora
</code></pre>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-29-a29f5eabd8f4&gt; in &lt;module&gt;
     85 # use the following command in anaconda prompt with the admionistrator privileges to install gensim
     86 # conda install -c conda-forge gensim
---&gt; 87 import gensim
     88 from gensim import corpora
     89 

~\Anaconda3\lib\site-packages\gensim\__init__.py in &lt;module&gt;
      3 """"""
      4 
----&gt; 5 from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
      6 import logging
      7 

~\Anaconda3\lib\site-packages\gensim\parsing\__init__.py in &lt;module&gt;
      2 
      3 from .porter import PorterStemmer  # noqa:F401
----&gt; 4 from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
      5                             strip_tags, strip_short, strip_numeric,
      6                             strip_non_alphanum, strip_multiple_whitespaces,

~\Anaconda3\lib\site-packages\gensim\parsing\preprocessing.py in &lt;module&gt;
     40 import glob
     41 
---&gt; 42 from gensim import utils
     43 from gensim.parsing.porter import PorterStemmer
     44 

~\Anaconda3\lib\site-packages\gensim\utils.py in &lt;module&gt;
     43 from six.moves import range
       44 
---&gt; 45 from smart_open import open
     46 
     47 from multiprocessing import cpu_count

~\Anaconda3\lib\site-packages\smart_open\__init__.py in &lt;module&gt;
     26 from smart_open import version
     27 
---&gt; 28 from .smart_open_lib import open, parse_uri, smart_open, register_compressor
     29 from .s3 import iter_bucket as s3_iter_bucket
     30 

~\Anaconda3\lib\site-packages\smart_open\smart_open_lib.py in &lt;module&gt;
     35 
     36 from smart_open import compression
---&gt; 37 from smart_open import doctools
     38 from smart_open import transport
     39 from smart_open import utils

~\Anaconda3\lib\site-packages\smart_open\doctools.py in &lt;module&gt;
     19 
     20 from . import compression
---&gt; 21 from . import transport
     22 
     23 PLACEHOLDER = '    smart_open/doctools.py magic goes here'

~\Anaconda3\lib\site-packages\smart_open\transport.py in &lt;module&gt;
     20 NO_SCHEME = ''
     21 
---&gt; 22 _REGISTRY = {NO_SCHEME: smart_open.local_file}
     23 
     24 

AttributeError: module 'smart_open' has no attribute 'local_file'
</code></pre>

<p>I have received the error : module 'smart_open' has no attribute 'local_file'
how can I solve it?</p>
","12664580","","130288","","2020-04-13 18:36:47","2021-01-03 10:06:55","module 'smart_open' has no attribute 'local_file'","<gensim>","5","1","","","","CC BY-SA 4.0"
"34898059","1","","","2016-01-20 10:55:02","","0","238","<p>I have upgraded gensim from 0.12.2 to 0.12.3 and I am facing an issue while inferencing in doc2vec. This is the code for performing inference:</p>

<pre><code>doc = query.lower().split()
inf_vec = formmodel.infer_vector(doc)
similarF = formmodel.docvecs.most_similar([inf_vec])
</code></pre>

<p>When doc2vec model training and inference were done using version 0.12.3 this code gives results as shown below:</p>

<pre><code>[(644539, 0.55715829133987427), (647249, 0.55713766813278198),...]
</code></pre>

<p>When doc2vec model training and inference were done using version 0.12.2 the same code gave results like so:</p>

<pre><code>[(docId1, 0.55715829133987427), (docId2, 0.55713766813278198),....]
</code></pre>

<p>How do I get document labels instead of numbers in 0.12.3?</p>
","995327","","995327","","2016-01-20 11:05:00","2016-01-28 21:42:29","Doc2vec inference in gensim 0.12.3","<gensim>","1","0","","","","CC BY-SA 3.0"
"43850721","1","","","2017-05-08 14:49:35","","1","103","<p>I have a file that I thought was a Gensim Dictionary file, but apparently it is not one.  I created it by merging dictionaries: <code>baseddict.merge_with(addddict)</code>. </p>

<p>However, now when I try to call doc2bow on it like this:</p>

<pre><code>class MyCorpus(object):    
    def __init__(self,arrayxx,dictionaryx):
        self.arr = arrayxx
        self.diction = dictionaryx

    def __iter__(self):
        for each in self.arr:
            yield self.diction.doc2bow(each)
</code></pre>

<p>I get an error:</p>

<blockquote>
  <p>File ""/home/rnczf01/Desktop/Files/Patent_sim/newpython/parse_xml_patentsall_embedded_pftaps_allfiles.py"",
  line 58, in <strong>iter</strong>
      yield self.diction.doc2bow(each) AttributeError: 'VocabTransform' object has no attribute 'doc2bow'</p>
</blockquote>
","7303036","","","","","2017-05-08 14:49:35","Calling a merged dictionary in Gensim","<nlp><gensim>","0","0","","","","CC BY-SA 3.0"
"61509408","1","61509516","","2020-04-29 18:48:56","","0","69","<p>I am currently getting a segmentation fault when I am loading a model with gensim. In order to create the model and save it, I do:</p>

<pre><code>glove_file = 'QGModels/embeddings/glove.6B.300d.txt'
tmp_file = 'QGModels/embeddings/word2vec-glove.6B.300d.txt'
glove2word2vec(glove_file, tmp_file)
model = KeyedVectors.load_word2vec_format(tmp_file)
model.save('QGModels/embeddings/model.model')
</code></pre>

<p>However the problem starts when I load the model and use the most_similar method using:</p>

<pre><code>model = KeyedVectors.load('QGModels/embeddings/model.model')
closestWords = model.most_similar(positive=[answer], topn=count)
</code></pre>

<p>And then get a segmentation fault:</p>

<blockquote>
  <p>Process finished with exit code 137 (interrupted by signal 9: SIGKILL)</p>
</blockquote>

<p>Any and all help is appreciated! Thank You.</p>
","11989409","","","","","2020-04-29 18:54:56","Segmentation Fault with Gensim","<python><segmentation-fault><gensim>","1","0","","","","CC BY-SA 4.0"
"61218334","1","","","2020-04-14 22:30:06","","1","1036","<p>I'm trying to process text using gensim (specifically gensim.corpora.dictionary), but I keep getting a ""<code>ModuleNotFoundError: No module named 'gensim.corpora'; 'gensim' is not a package</code>"" error. The sample code is below. I used <code>pip install gensim</code> in my command prompt in install <code>gensim</code>. I verified that numPy and sciPy were installed and up-to-date. I checked the file path of gensim and confirmed that gensim is installed on the machine. There is also a corpora folder in gensim with no obvious issues. I'm running Python 3.6.8 through the command prompt. I can call other modules, like Pandas,nltk, and NumPy, that are in the same folder location as gensim so I'm not sure why I am getting issues when I try to import gensim. I don't know how to fix this issue. Has anyone come across this issue before? I will be grateful for any help on this. Thanks.</p>

<pre><code>from gensim.corpora.dictionary import Dictionary 
from nltk.tokenize import word_tokenize

my_documents=[  'The movie was about a spaceship and aliens',
                'I really liked the movie!',
                'Awesome action scenes, but boring characters.',
                'The movie was awful! I hate alien films.',
                'Space is cool! I liked the movie.',
                'More space films, please!']

tokenized_docs=[word_tokenize(doc.lower()) for doc in my_documents]
dic= corpora.Dictionary(tokenized_docs)
print(dic.token2id)
corpus=[dic.doc2bow(doc) for doc in tokenized_docs]
print(corpus)
</code></pre>

<p>The output generated after running <code>dir /s /b ""python""</code> and <code>dir /s /b ""pip""</code> in the command prompt can be found below. </p>

<p>C:\Users\Owner>dir /S /b ""python""</p>

<p>C:\Users\Owner.vscode\extensions\ms-python.python-2020.3.71659\pythonFiles\lib\python</p>

<p>C:\Users\Owner.vscode\extensions\ms-python.python-2020.3.71659\pythonFiles\lib\python\parso\python</p>

<p>C:\Users\Owner\AppData\Local\Programs\Python</p>

<p>C:\Users\Owner\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\python</p>

<p>C:\Users\Owner\AppData\Local\Programs\Microsoft VS Code_\resources\app\extensions\python</p>

<p>C:\Users\Owner\AppData\Roaming\Python</p>

<p>C:\Users\Owner>dir /S /b ""pip""</p>

<p>C:\Users\Owner\AppData\Local\pip</p>

<p>C:\Users\Owner\AppData\Local\Programs\Python\Python36\Lib\site-packages\pip</p>

<p>C:\Users\Owner\AppData\Local\Programs\Python\Python37-32\Lib\site-packages\pip</p>
","13192924","","13192924","","2020-04-15 05:24:28","2020-04-16 07:16:10","ModuleNotFoundError: No module named 'gensim.corpora'; 'gensim' is not a package","<python-3.x><gensim>","1","10","","","","CC BY-SA 4.0"
"61608275","1","","","2020-05-05 07:52:35","","0","209","<p>I am training text data using gensim doc2vec model on google colab repository GPU runtime, and want to save trained model in test.d2v file. following is code snippet</p>

<pre><code>T = [TaggedDocument(doc, [i]) for i, doc in enumerate(data['info'])]
model = Doc2Vec(T,alpha=.025, min_alpha=.025, min_count=1)
model.save('test.d2v')
</code></pre>

<p>Following error is generated in colab notebook.</p>

<p>** /usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: <a href=""https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function</a>
  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL</p>
","5084246","","","","","2020-05-05 13:39:05","save gensim doc2vec trained model on google colab","<nlp><google-colaboratory><gensim><sentiment-analysis><doc2vec>","1","1","","","","CC BY-SA 4.0"
"53015716","1","","","2018-10-26 20:13:05","","0","1036","<p>I am pretty new at topic modeling and Gensim. So, I am still trying to understand many of concepts. I am trying to run gensim's LDA model on my corpus that contains around 25,446,114 tweets. I created a streaming corpus and id2word dictionary using gensim. I am using num_topics = 100, chunk size = 85000 (loading 85000 tweets at a time)</p>

<p>I am using 
Gensim : 3.5.0
Numpy: 1.15.3</p>

<p>Here is the link to corpus and id2word dictionary: <a href=""https://drive.google.com/drive/folders/1FrJ8gJbiDqp3VC5syOjRVcQPcESdYOYa?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1FrJ8gJbiDqp3VC5syOjRVcQPcESdYOYa?usp=sharing</a></p>

<p>I don't know what I am doing wrong or how to solve this. The topic diff first hits inf and then nan , and I start getting same topic. 
Please help !!</p>

<p>Here is the code: </p>

<pre><code>import pprint
import logging
import gensim
logging.basicConfig(filename='gensim.log',
                    format=""%(asctime)s:%(levelname)s:%(message)s"",
                    level=logging.INFO)
corpus = gensim.corpora.MmCorpus('disasterTweets.mm')
id2word = gensim.corpora.Dictionary.load('disasterTweets.dict')
id2word.filter_tokens(bad_ids=[id2word.token2id['eofeofeof']])
print('eofeofeof' in id2word.token2id)

lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       chunksize=85000,
                                       num_topics=100)
pprint.pprint(lda_model.print_topics())
</code></pre>

<p>Here are the errors I am receiving: </p>

<pre><code>/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log
 diff = np.log(self.expElogbeta)
/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py:690: RuntimeWarning: overflow encountered in add
  sstats[:, ids] += np.outer(expElogthetad.T, cts / phinorm)
/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py:700: RuntimeWarning: invalid value encountered in multiply
  sstats *= self.expElogbeta
/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py:690: RuntimeWarning: overflow encountered in add
  sstats[:, ids] += np.outer(expElogthetad.T, cts / phinorm)
/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py:700: RuntimeWarning: invalid value encountered in multiply
  sstats *= self.expElogbeta
Process ForkPoolWorker-30:
Traceback (most recent call last):
  File ""/home/linuxbrew/.linuxbrew/Cellar/python/3.7.0/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap
    self.run()
  File ""/home/linuxbrew/.linuxbrew/Cellar/python/3.7.0/lib/python3.7/multiprocessing/process.py"", line 99, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/linuxbrew/.linuxbrew/Cellar/python/3.7.0/lib/python3.7/multiprocessing/pool.py"", line 105, in worker
    initializer(*initargs)
  File ""/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamulticore.py"", line 333, in worker_e_step
    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?
  File ""/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py"", line 725, in do_estep
    gamma, sstats = self.inference(chunk, collect_sstats=True)
  File ""/home/ec2-user/env/lib/python3.7/site-packages/gensim/models/ldamodel.py"", line 662, in inference
    expElogbetad = self.expElogbeta[:, ids]
IndexError: index 287500 is out of bounds for axis 1 with size 287500
</code></pre>
","10128614","","","","","2018-10-28 23:48:08","Gensim LDA model topic diff resulting in nan","<python><python-3.x><numpy><gensim><lda>","1","0","0","","","CC BY-SA 4.0"
"44000997","1","","","2017-05-16 12:06:17","","7","1534","<p>I am trying to run gensim WMD similarity faster. Typically, this is what is in the docs:
Example corpus:</p>

<pre><code>    my_corpus = [""Human machine interface for lab abc computer applications"",
&gt;&gt;&gt;              ""A survey of user opinion of computer system response time"",
&gt;&gt;&gt;              ""The EPS user interface management system"",
&gt;&gt;&gt;              ""System and human system engineering testing of EPS"",
&gt;&gt;&gt;              ""Relation of user perceived response time to error measurement"",
&gt;&gt;&gt;              ""The generation of random binary unordered trees"",
&gt;&gt;&gt;              ""The intersection graph of paths in trees"",
&gt;&gt;&gt;              ""Graph minors IV Widths of trees and well quasi ordering"",
&gt;&gt;&gt;              ""Graph minors A survey""]

my_query = 'Human and artificial intelligence software programs'
my_tokenized_query =['human','artificial','intelligence','software','programs']

model = a trained word2Vec model on about 100,000 documents similar to my_corpus.
model = Word2Vec.load(word2vec_model)
</code></pre>

<hr>

<pre><code>from gensim import Word2Vec
from gensim.similarities import WmdSimilarity

def init_instance(my_corpus,model,num_best):
    instance = WmdSimilarity(my_corpus, model,num_best = 1)
    return instance
instance[my_tokenized_query]
</code></pre>

<p>the best matched document is <code>""Human machine interface for lab abc computer applications""</code> which is great. </p>

<p>However the function <code>instance</code> above takes an extremely long time. So I thought of breaking up the corpus into <code>N</code> parts and then doing <code>WMD</code> on each with <code>num_best = 1</code>, then at the end of it, the part with the max score will be the most similar. </p>

<pre><code>    from multiprocessing import Process, Queue ,Manager

    def main( my_query,global_jobs,process_tmp):
        process_query = gensim.utils.simple_preprocess(my_query)

        def worker(num,process_query,return_dict):  
            instance=init_instance\
(my_corpus[num*chunk+1:num*chunk+chunk], model,1)
            x = instance[process_query][0][0]
            y = instance[process_query][0][1]
            return_dict[x] = y
        manager = Manager()
        return_dict = manager.dict()

        for num in range(num_workers):
            process_tmp = Process(target=worker, args=(num,process_query,return_dict))
            global_jobs.append(process_tmp)
            process_tmp.start()
        for proc in global_jobs:
            proc.join()

        return_dict = dict(return_dict)
        ind = max(return_dict.iteritems(), key=operator.itemgetter(1))[0]
        print corpus[ind]
        &gt;&gt;&gt; ""Graph minors A survey""
</code></pre>

<p>The problem I have with this is that, even though it outputs something, it doesn't give me a good similar query from my corpus even though it gets the max similarity of all the parts. </p>

<p>Am I doing something wrong?</p>
","2800939","","2800939","","2017-05-19 18:22:16","2017-06-24 12:46:43","Python Gensim how to make WMD similarity run faster with multiprocessing","<python><multithreading><multiprocessing><gensim>","2","4","2","","","CC BY-SA 3.0"
"52645618","1","","","2018-10-04 11:31:10","","1","205","<p>I have trained word2Vec model on a huge corpus using gensim. Then i have tried to find gender bias in the word2vec model. After this I am neutralizing the bias for few specific professional words. Referred the following link for gender de-biasing
<a href=""https://datascience-enthusiast.com/DL/Operations_on_word_vectors.html"" rel=""nofollow noreferrer"">https://datascience-enthusiast.com/DL/Operations_on_word_vectors.html</a>
On de-biasing or neutralizing a word, new vector of the specific word is returned. Is it possible to take such new vectors of existing words and retrain the previous model.
Please let me know if there are any techniques of doing so.
Thanks and Regards
AIMC</p>
","9018377","","","","","2018-10-04 11:31:10","retrain existing word2vec model with new word vectors","<python><gensim><word2vec>","0","1","","","","CC BY-SA 4.0"
"52469352","1","","","2018-09-23 19:04:52","","-1","650","<p>Edit: I'm asking this because I've spent over 40 hours experimenting with these packages, and feel as though I've gotten nowhere.</p>

<p>I'm pretty new to Python. I've done a RandomForestClassifier model successfully at my organization and the model is in production, but neural nets are beyond my current comprehension.</p>

<p>I'm working on a text classification problem in Python. I had 243 samples (rows) that are taken from 25 job postings. I have one column that is the string sentence, and one column that is the job posting that it came from. </p>

<p>I'm gunning for a promotion at work, and thought this would be a neat opportunity to learn about neural networks. (I'm not going for a data scientist-type role, this just fascinates me.) Each sample is one ""job duty"" from a job posting, and each ""document"" is a job posting. One job posting could have multiple duties that are nearly identical, every job posting should have 2-3 identical (abstracted) duties, and ultimately, I assume there will be 15-20 clusters of ""duties"" from my 25 job postings. </p>

<p>Essentially, my desired output is to classify each row (regardless of which job posting it came from; I don't think my document column is relevant) to n clusters. I don't expect labels for my clusters. </p>

<p>I've cleaned my 243 samples; removing punctuation and stopwords, and have it in a dataframe.</p>

<p>The packages I've experimented with so far are Keras, doc2vec, word2vec, nltk, and Soundex</p>

<ol>
<li><p>Is there a way to cluster my samples (unsupervised) without training data?</p></li>
<li><p>Do I need to upload a corpus to train? Does a corpus by default have classification labels? </p></li>
<li><p>What is the simplest (willing to sacrifice accuracy) to get n clusters out of 243 samples (I will go through the contents of each cluster and determine the label for the cluster post-processing)</p></li>
</ol>

<p>Just some vaguely directional guidance would really help me.</p>
","9363294","","9363294","","2018-09-23 19:20:06","2018-09-23 19:55:41","Python NLP, Neural Network, text clustering","<python><tensorflow><keras><nltk><gensim>","1","2","","","","CC BY-SA 4.0"
"44282320","1","","","2017-05-31 10:30:49","","2","5924","<p>I want to use gensim to train a word2vec model <br>
python 3.5.3 <br>
gensim 2.1.0 <br>
numpy 1.12.1+mkl <br>
scipy 0.19.0 <br></p>

<pre><code>import gensim
import codecs
class MySentences(object):
    def __init__(self,filename):
        self.filename=filename
    def __iter__(self):
        with codecs.open(self.filename) as f:
            for line in f.readlines():
                wordlist=list()
                for word in line:
                    wordlist.append(word)
                yield wordlist

sentences=MySentences('D:/Documents/Data/icwb2-data-processed/pku_training.rmspace.utf8')
model=gensim.models.Word2Vec(sentences)
model.save('w.model')
</code></pre>

<p>I run this code, and i cause the error:</p>

<blockquote>
  <p>AttributeError: module 'gensim' has no attribute 'models'</p>
</blockquote>

<p>I make this error due to i named this file 'gensim.py' <br>
thank @BurhanKhalid !!!</p>
","5537591","","1033581","","2018-04-15 07:55:44","2019-07-24 04:50:31","How to solve this error: module 'gensim' has no attribute 'models'","<python><python-3.x><gensim><word2vec>","2","3","","","","CC BY-SA 3.0"
"61303579","1","61309488","","2020-04-19 11:28:03","","0","115","<p>I am implementing a paper to compare our performance. In the paper, the uathor says </p>

<blockquote>
  <p>300-dimensional pre-trained word2vec vectors (Mikolov et al., 2013)</p>
</blockquote>

<p>I am wondering whether the pretrained word2vec Gensim model <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"" rel=""nofollow noreferrer"">here</a> is same as the pretrained embeddings on the official <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google site</a> (the GoogleNews-vectors-negative300.bin.gz file)</p>

<p><br>
My source of doubt arises from this line in Gensim documentation (in Word2Vec Demo section) </p>

<blockquote>
  <p>We will fetch the Word2Vec model trained on part of the Google News dataset, covering approximately 3 million words and phrases</p>
</blockquote>

<p>Does this mean the model on gensim is not fully trained? Is it different from the official embeddings by Mikolov? </p>
","8161586","","8161586","","2020-04-19 12:41:10","2020-04-20 19:56:12","Is the Gensim word2vec model same as the standard model by Mikolov?","<python><nlp><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"52686366","1","","","2018-10-07 07:28:40","","5","3613","<p>I train and save a gensim word2vec model:</p>

<pre><code>W2V_MODEL_FN = r""C:\Users\models\w2v.model""

model = Word2Vec(X, size=150, window=3, min_count=2, workers=10)
model.train(X, total_examples=len(X), epochs=50)
model.save(W2V_MODEL_FN)
</code></pre>

<p>And then:</p>

<pre><code>w2v_model = Word2Vec.load(W2V_MODEL_FN)
</code></pre>

<p>On one enviroment it works perfectly but in another I get the error:</p>

<blockquote>
  <p>{AttributeError}Can't get attribute 'Word2VecKeyedVectors' on  module
  'gensim.models.keyedvectors' from
  'C:\Users\Anaconda3_New\envs\ISP_env\lib\site-packages\gensim\models\keyedvectors.py'></p>
</blockquote>

<p>So I guess it might be a package version issue?</p>

<p>But I couldn't figure what it is.
Any ideas?</p>

<p>Thanks!</p>
","10117402","","","","","2020-06-05 17:46:41","Can't get attribute 'Word2VecKeyedVectors' on <module 'gensim.models.keyedvectors' >","<python><nlp><gensim><word2vec>","2","2","2","","","CC BY-SA 4.0"
"35609171","1","35642123","","2016-02-24 17:36:39","","2","1237","<p>Today I just started writing an script which trains LDA models on large corpora (minimum 30M sentences) using gensim library.
Here is the current code that I am using:</p>

<pre><code>from gensim import corpora, models, similarities, matutils

def train_model(fname):
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    dictionary = corpora.Dictionary(line.lower().split() for line in open(fname))
    print ""DOC2BOW""
    corpus = [dictionary.doc2bow(line.lower().split()) for line in open(fname)]

    print ""running LDA""
    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100, update_every=1, chunksize=10000, asses=1)
</code></pre>

<p>running this script on a small corpus (2M sentences) I realized that it needs about 7GB of RAM.
And when I try to run it on the larger corpora, it fails because of the memory issue.
The problem is obviously due to the fact that I am loading the corpus using this command:</p>

<pre><code>corpus = [dictionary.doc2bow(line.lower().split()) for line in open(fname)]
</code></pre>

<p>But, I think there is no other way because I would need it for calling the LdaModel() method:</p>

<pre><code>lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100, update_every=1, chunksize=10000, asses=1)
</code></pre>

<p>I searched for a solution to this problem but I could not find anything helpful.
I would imagine that it should be a common problem since we mostly train the models on very large corpora (usually wikipedia documents). So, it should be already a solution for it.</p>

<p>Any ideas about this issue and the solution for it?</p>
","2530859","","6573902","","2021-01-25 15:03:32","2021-01-25 15:03:32","Memory efficient LDA training using gensim library","<python><nlp><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"53247197","1","","","2018-11-11 08:57:45","","2","1195","<p>I want to measure the similarity between sentences. Can I use sklearn and Euclidean Distance to measure the semantic similarity between sentences. I read about Cosine similarity also. Can someone explain the difference of those to measures and what is the best approach to use?</p>
","7692210","","","","","2018-11-14 13:07:54","Does Euclidean Distance measure the semantic similarity?","<scikit-learn><gensim><euclidean-distance><cosine-similarity><sentence-similarity>","1","1","1","","","CC BY-SA 4.0"
"52919299","1","","","2018-10-21 19:57:40","","1","234","<p>So, I'm messing around with gensim and I've got it to print the top 5 topics and popular nouns associated with the topics (this was done using the example here <a href=""https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi"">Topic Distribution and clustering using LDA</a>). I'm working with 51 documents in my case. I'm having difficulty getting my last two clusters to work as I keep receiving a ""list index out of range"" error. I'm completely clueless about what changes I could make to fix my clusters. The method I attempted using if and else conditions gave an incorrect first cluster (you'll spot it commented out). Where exactly am I going wrong?</p>

<pre><code>    from gensim import corpora, models, similarities
from itertools import chain

# list of tokenised nouns from the noun documents
nounTokens = []

for index, row in df_Data.iterrows():
    nounTokens.append(df_Data.iloc[index]['Noun Tokens'])

# create a dictionary using noun Tokens
id2word = corpora.Dictionary(nounTokens)

# creates the bag of word corpus
mm = [id2word.doc2bow(noun) for noun in nounTokens]

# trains lda models
lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=5, update_every=1, chunksize=10000, passes=1)

# prints the topics of the corpus
for topics in lda.print_topics():
    print(topics)
print

lda_corpus = lda[mm]

# search for scores of all the words under each topic for all documents
scores = list(chain(*[[score for topic_id, score in topic] 
                      for topic in [doc for doc in lda_corpus]]))
# calculating the avg sum of all the probabilities to ensure we have a valid threshold.
threshold = sum(scores)/len(scores)
print(threshold)
print
# cluster1 = []
# cluster2 = []
# cluster3 = []

# for i,j in zip(lda_corpus, noun_Docs):
#     if len(i) &gt; 0:
#         if i[0][1] &gt; threshold:
#             cluster1.append(j)
#     elif len(i)&gt;1:
#         if i[1][1] &gt; threshold:
#             cluster2.append(j)
#     elif len(i) &gt; 2:
#         if i[2][1] &gt; threshold:
#             cluster3.append(j)

cluster1 = [j for i, j in zip(lda_corpus, noun_Docs) if i[0][1] &gt; threshold]
cluster2 = [j for i, j in zip(lda_corpus, noun_Docs) if i[1][1] &gt; threshold]
cluster3 = [j for i, j in zip(lda_corpus, noun_Docs) if i[2][1] &gt; threshold]
# for i,j in zip(lda_corpus, noun_Docs):
#     print(i)

print(cluster1)
# print(cluster2)
# print(cluster3)
</code></pre>
","5950637","","","","","2018-10-21 19:57:40","Topic distribution: Problem with clustering my documents using LDA","<python><nltk><gensim><lda>","0","0","","","","CC BY-SA 4.0"
"53159443","1","53159499","","2018-11-05 17:38:13","","4","82","<p>I have a list of sentences and I want to perform some action on two sentences each time, but not for al of the sentences.</p>

<p>for example:</p>

<pre><code>list= [""aaaaa"",""bbbbb"",""ccccc"",""ddddd"",""eeeee""]
similarity_a-d = sim(""aaaaa"",""ddddd"")
similarity_a-e = sim(""aaaaa"",""eeeee"")
similarity_b-d = sim(""bbbbb"",""ddddd"")
similarity_b-e = sim(""bbbbb"",""eeeee"")
similarity_c-d = sim(""ccccc"",""ddddd"")
similarity_c-e = sim(""ccccc"",""eeeee"")
</code></pre>

<p>That's what I tried:</p>

<pre><code>similarity={}
for i,vec_lda_topic in enumerate(vec_lda_topics)[:numOfUSs]:
    for j,vec_lda_topic in enumerate(vec_lda_topics)[numOfUSs:]:
        similarity[""sim{0}-{1}"".format(i,j)] = gensim.matutils.cossim(vec_lda_topics[i], vec_lda_topics[j])
        print('similarity between docs ', i, ' and ',j,': ', similarity[""sim{0}-{1}"".format(i,j)])
</code></pre>

<p>and receive the following error:</p>

<pre><code>TypeError: 'enumerate' object is not subscriptable
</code></pre>

<p>And besides the error, maybe there is a better way to do this?</p>
","5631372","","5631372","","2018-11-05 17:45:28","2018-11-05 17:45:28","nested loop over list and dynamically create variables","<python><python-3.x><for-loop><gensim>","2","0","","","","CC BY-SA 4.0"
"61029524","1","61032177","","2020-04-04 14:23:43","","1","621","<p>I have a dataset of reviews for different Hotels. 
I'm trying to find out similar hotels using the reviews of hotels. So, I'm using a <code>Doc2vec</code> algorithm to achieve this.</p>

<p>Is there any way to measure the accuracy of a <code>Doc2Vec</code> model using <code>Gensim</code>, rather than evaluating the results using <code>most_similar()</code> function of <code>Gensim</code>?</p>
","7434250","","5374999","","2020-04-06 20:06:45","2021-05-04 20:21:39","How to measure the accuracy of a Doc2vec model?","<gensim><unsupervised-learning><doc2vec>","2","0","","","","CC BY-SA 4.0"
"35372917","1","","","2016-02-12 22:01:00","","4","537","<p>I am using Gensim's LDAMulticore to perform LDA. I have around 28M small documents (around 100 characters each). </p>

<p>I have given workers argument to be 20 but the top shows it using only 4 processes. There are some discussions around it that it might be slow in reading corpus like:
<a href=""https://stackoverflow.com/questions/33929680/gensim-ldamulticore-not-multiprocessing"">gensim LdaMulticore not multiprocessing?</a> 
<a href=""https://github.com/piskvorky/gensim/issues/288"" rel=""nofollow noreferrer"">https://github.com/piskvorky/gensim/issues/288</a></p>

<p>But both of them uses MmCorpus . Although my corpus is completely in memory. I have machine with very large RAM (250 GB) and loading the corpus in memory takes around 40 GB. But even after that LDAMulticore is using just 4 processes. I created the corpus as:</p>

<p><code>corpus = [dictionary.doc2bow(text) for text in texts]</code></p>

<p>I am not able to understand what can be the limiting factor here?</p>
","2362514","","-1","","2017-05-23 12:25:52","2019-11-15 10:52:37","Gensim LdaMulticore is not multiprocessing properly (using just 4 workers)","<python><lda><gensim><topic-modeling>","1","2","1","","","CC BY-SA 3.0"
"52893017","1","52896103","","2018-10-19 13:08:32","","0","757","<p>Beginner here. </p>

<p>I have a large body of .txt files that I want to train a Doc2Vec model on. However, I am having trouble importing the data into python in a usable way.</p>

<p>To import data, I have used:</p>

<pre><code>docLabels = []
docLabels = [f for f in listdir(‚ÄúPATH TO YOU DOCUMENT FOLDER‚Äù) if 
f.endswith(‚Äò.txt‚Äô)]
data = []
for doc in docLabels:
    data.append(open(‚ÄòPATH TO YOU DOCUMENT FOLDER‚Äô + doc).read()) `
</code></pre>

<p>However, with this, I get a ""list"", which I can do no further work with. I cannot seem to find how to import text files in a way they can be used with NLTK / doc2vec anywhere on SO or in tutorials.</p>

<p>Help would be greatly appreciated. Thank you!</p>
","10485162","","7071577","","2018-10-19 13:21:41","2018-10-19 16:03:37","gensim Doc2Vec: Getting from txt files to TaggedDocuments","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"53343027","1","","","2018-11-16 17:52:59","","5","3890","<p>I am trying to use word2vec from gensim but I get this warning on running: 
 - C:\Users\user1PycharmProjects\FirstTest\venv\lib\site-packages\gensim\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
 - C:\Users\user1\PycharmProjects\FirstTest\venv\lib\site-packages\gensim\models\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.
  ""C extension not loaded, training will be slow. ""</p>

<p>I installed (&amp; configure path): </p>

<ul>
<li>mingw32-gcc-ada-bin</li>
<li>mingw32-gcc-fortran-bin</li>
<li>mingw32-gcc-g++-bin</li>
<li>mingw32-gcc-objc-bin</li>
</ul>

<p>I sure that I have a compiler (tested with a C script), but I don't know why I can't use the fast version of gensim !!!
I run my script on windows 10 with python3. </p>

<p>Thank you</p>
","7786168","","","","","2019-04-22 18:27:40","Gensim on windows: C extension not loaded, training will be slow","<python><pip><gensim>","1","1","","","","CC BY-SA 4.0"
"44352552","1","","","2017-06-04 09:22:06","","0","4507","<p>There is a dataframe like this:  </p>

<pre><code>  index  terms   
  1345  ['jays', 'place', 'great', 'subway']    
  1543  ['described', 'communicative', 'friendly']    
  9874  ['great', 'sarahs', 'apartament', 'back']    
  2456  ['great', 'sarahs', 'apartament', 'back']  
</code></pre>

<p>I try to create a dictionary from the corpus of comments[ 'terms' ], but I face an error message !  </p>

<pre><code>from gensim import corpora, models
dictionary = corpora.Dictionary( comments['terms'] )

TypeError: doc2bow expects an array of unicode tokens on input, not a single string
</code></pre>
","8000414","","","","","2021-05-26 22:48:37","TypeError: doc2bow expects an array of unicode tokens on input, not a single string when using gensim.corpora.Dictionary()","<python><dictionary><gensim>","2","1","","","","CC BY-SA 3.0"
"44571617","1","59049417","","2017-06-15 15:36:38","","5","1497","<p>Sometimes it returns probabilities for all topics and all is fine, but  sometimes it returns probabilities for just a few topics and they don't add up to one, it seems it depends on the document. Generally when it returns few topics, the probabilities add up to more or less 80%, so is it returning just the most relevant topics? Is there a way to force it to return all probabilities?</p>

<p>Maybe I'm missing something but I can't find any documentation of the method's parameters.</p>
","5016440","","","","","2020-04-27 16:51:03","probabilities returned by gensim's get_document_topics method doesn't add up to one","<text-mining><gensim><lda><topic-modeling>","2","0","1","","","CC BY-SA 3.0"
"61511101","1","61515577","","2020-04-29 20:29:46","","2","757","<p>Let's say we train a model with more than 1 million words. In order to find the most similar words we need to calculate the distance between the embedding of the test word and embeddings of all the 1 million words words, and then find the nearest words. It seems that Gensim calculate the results very fast. Although when I want to calculate the most similar, my function is extremely slow:</p>

<pre><code>def euclidean_most_similars (model, word, topn = 10):
  distances = {}
  vec1 = model[word]
  for item in model.wv.vocab:
    if item!= node:
      vec2 = model[item]
      dist = np.linalg.norm(vec1 - vec2)
      distances[(node, item)] = dist
  sorted_distances = sorted(distances.items(), key=operator.itemgetter(1))
</code></pre>

<p>I would like to know how Gensim manages to calculate the most nearest words so fast and what is an efficient way to calculate the most similares.</p>
","1056179","","","","","2021-07-20 15:52:24","How does gensim manage to find the most similar words so fast?","<python><time-complexity><gensim><word2vec><similarity>","1","2","","","","CC BY-SA 4.0"
"60988425","1","","","2020-04-02 09:34:03","","0","1057","<p>I am trying to apply LDA for topic modeling using the Mallet wrapper of Gensim on Python.
The code that I am running is as follows:</p>

<pre><code>MALLET_PATH = 'C:/mallet-2.0.8/bin/mallet'
lda_mallet = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=bow_corpus, 
                                              num_topics=TOTAL_TOPICS, id2word=dictionary,
                                              iterations=500, workers=16)
</code></pre>

<p>Mallet is installed in C-drive and is running on the Command Prompt (C:\mallet-2.0.8\bin\mallet).
The help command is also working (import-dir --). Java is also installed. The environment variable
and the path have also been set for both Mallet and Java.Yet the output shows the following error.</p>

<pre><code>CalledProcessError: Command 'mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\imibh\AppData\Local\Temp\a8b7e6_corpus.txt --output C:\Users\imibh\AppData\Local\Temp\a8b7e6_corpus.mallet' returned non-zero exit status 1.
</code></pre>

<p>Have already tried all the responses to past such queries on stack overflow without any improvement.</p>

<p>Would greatly appreciate any help.</p>

<p>Manit</p>
","13116459","","11736145","","2020-06-07 10:14:15","2020-11-05 10:26:23","Python Gensim Mallet","<python-3.x><jupyter-notebook><gensim><mallet>","2","0","","","","CC BY-SA 4.0"
"53249919","1","","","2018-11-11 14:54:07","","1","240","<p>i want to get the cosine similarity between sentences. I have tested doc2vec with gensim and trained it with only few sentences given in the code. But I want to train my model using a text document that have one sentence per each line. How can I use a document with sentences?</p>
","7692210","","","","","2018-11-11 15:33:01","How to import a document with sentences to train a doc2vec model?","<python><gensim><cosine-similarity><doc2vec><sentence-similarity>","1","1","","","","CC BY-SA 4.0"
"61280748","1","61294331","","2020-04-17 21:10:55","","0","167","<p>I am running word2vec models in gensim. I don't understand 2 metrics (in_qsize/out_qsize) reported by the log file. I've spent a bit of time searching and can't find an explanation. Here is a sample from my log files:</p>

<pre><code>2020-04-17 21:04:09,032 : INFO : EPOCH 5 - PROGRESS: at 68.67% examples, 657466 words/s, in_qsize 18, out_qsize 1
2020-04-17 21:04:10,038 : INFO : EPOCH 5 - PROGRESS: at 68.92% examples, 657527 words/s, in_qsize 20, out_qsize 0
2020-04-17 21:04:11,078 : INFO : EPOCH 5 - PROGRESS: at 69.14% examples, 657513 words/s, in_qsize 20, out_qsize 1
2020-04-17 21:04:12,136 : INFO : EPOCH 5 - PROGRESS: at 69.39% examples, 657458 words/s, in_qsize 18, out_qsize 1
2020-04-17 21:04:13,139 : INFO : EPOCH 5 - PROGRESS: at 69.68% examples, 657687 words/s, in_qsize 17, out_qsize 4
</code></pre>
","7864403","","2222","","2020-04-18 19:17:17","2020-04-18 19:17:17","meaning of in_qsize and out_qsize in gensim word2vec log files","<python><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"44146401","1","","","2017-05-23 23:03:42","","4","1275","<p>The title pretty much says it all. Here's some test code:</p>

<pre><code>import os
os.environ.update({'MALLET_HOME': r'C:/Users/somebody/a/place/LDA/mallet-2.0.8/',
                  'JAVA_HOME': r'C:/Program Files/Java/jdk1.8.0_131/'})

from gensim.corpora import mmcorpus, Dictionary
texts = [['human', 'interface', 'computer'],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]
dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

import gensim

mallet_path = r'C:\Users\somebody\a\place\LDA\mallet-2.0.\bin\mallet'
gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, id2word=dictionary, num_topics=5, alpha=1)
</code></pre>

<p>This raises the following error (truncated, since most of it is irrelevant stack info): </p>

<pre><code>CalledProcessError                        Traceback (most recent call last)
&lt;ipython-input-99-7343c192afd1&gt; in &lt;module&gt;()
      5 mallet_path = r'C:\Users\somebody\a\place\LDA\mallet-2.0.8\bin\mallet'
----&gt; 6 gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, id2word=dictionary, num_topics=5, alpha=1)
.
.
.
CalledProcessError: Command 'C:\Users\somebody\a\place\LDA\mallet-2.0.8\bin\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\somebody\AppData\Local\Temp\33b805_corpus.txt --output C:\Users\somebody\AppData\Local\Temp\33b805_corpus.mallet' returned non-zero exit status 1.
</code></pre>

<p>Fine, super, <em>but when I go run that exact command in cmd.exe, or in cygwin, There is no error, the code runs just fine!</em> I can even test the return code: ""echo $?"" in cygwin returns 0. Any help much appreciated!</p>
","7636950","","","","","2019-08-07 02:59:23","gensim LdaMallet raising CalledProcessError, but running mallet at command line runs with no error","<python><subprocess><gensim><lda><mallet>","1","0","","","","CC BY-SA 3.0"
"45948533","1","45950049","","2017-08-29 21:22:30","","2","1596","<p>Gensim implements a function called ""doesnt_match"" that return an outlier word from a list.</p>

<p>The function is called on a wordvector object.</p>

<p>model.wv.doesnt_match(""breakfast cereal dinner lunch"".split())
'cereal'</p>

<p>The documentation is not specifying how this function really work (what is the computation background)</p>

<p>Anyone knows ?</p>
","","user4402581","","","","2017-08-30 00:12:00","How is the Gensim doesnt_match function working?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"61572397","1","61572476","","2020-05-03 09:41:08","","0","209","<p>I copy a simple Python script by <a href=""https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html"" rel=""nofollow noreferrer"">Building a Wikipedia Text Corpus for Natural Language Processing</a> to build the corpus by stripping all Wikipedia markup from the articles, using gensim. This is the cose:</p>

<pre><code>""""""
Creates a corpus from Wikipedia dump file.
Inspired by:
https://github.com/panyang/Wikipedia_Word2vec/blob/master/v1/process_wiki.py
""""""

import sys
from gensim.corpora import WikiCorpus

    def make_corpus(in_f, out_f):

    """"""Convert Wikipedia xml dump file to text corpus""""""

    output = open(out_f, 'w')
    wiki = WikiCorpus(in_f)

    i = 0
    for text in wiki.get_texts():
        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
        i = i + 1
        if (i % 10000 == 0):
            print('Processed ' + str(i) + ' articles')
    output.close()
    print('Processing complete!')


if __name__ == '__main__':

    if len(sys.argv) != 3:
        print('Usage: python make_wiki_corpus.py &lt;wikipedia_dump_file&gt; &lt;processed_text_file&gt;')
        sys.exit(1)
    in_f = sys.argv[1]
    out_f = sys.argv[2]
    make_corpus(in_f, out_f)
</code></pre>

<p>Anyway, I obtained the error:</p>

<pre><code>ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>although I have installed the <code>gensim</code> package:</p>

<pre><code>python3 -m pip install gensim
</code></pre>

<p><strong>EDIT</strong>. If I try with</p>

<pre><code>pip install -U gensim
</code></pre>

<p>I obtain the error</p>

<pre><code> ImportError: cannot import name 'SourceDistribution' from 
 'pip._internal.distributions.source' (C:\Users\Standard\Anaconda3\lib\site- 
 packages\pip\_internal\distributions\source\__init__.py)
</code></pre>
","1306892","","1306892","","2020-05-03 11:09:39","2020-05-03 15:49:29","Build the corpus by Wikipedia: ModuleNotFoundError: No module named 'gensim'","<python><gensim>","1","2","","","","CC BY-SA 4.0"
"53570547","1","","","2018-12-01 11:59:55","","0","149","<p>I'm using Python, Django, IDE PyCharm. I imported Gensim for my project, however, seems like Django does not recognize it. Here are my versions: <strong>gensim-3.6.0 numpy-1.15.4 scipy-1.1.0 python 3.7.1</strong></p>

<p>The following error is:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Me/PycharmProjects/sma-capstone-2018/sma_core/brinfluence/lib/doc2vec.py"", line 4, in &lt;module&gt;
    import gensim
  File ""C:\Users\Me\venv\lib\site-packages\gensim\__init__.py"", line 5, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
  File ""C:\Users\Me\venv\lib\site-packages\gensim\parsing\__init__.py"", line 4, in &lt;module&gt;
    from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
  File ""C:\Users\Me\venv\lib\site-packages\gensim\parsing\preprocessing.py"", line 40, in &lt;module&gt;
    from gensim import utils
  File ""C:\Users\Me\venv\lib\site-packages\gensim\utils.py"", line 40, in &lt;module&gt;
    import scipy.sparse
  File ""C:\Users\Me\venv\lib\site-packages\scipy\sparse\__init__.py"", line 229, in &lt;module&gt;
    from .csr import *
  File ""C:\Users\Me\venv\lib\site-packages\scipy\sparse\csr.py"", line 15, in &lt;module&gt;
    from ._sparsetools import csr_tocsc, csr_tobsr, csr_count_blocks, \
ImportError: DLL load failed: The specified module could not be found.
</code></pre>
","9609791","","1033581","","2019-05-21 02:39:06","2019-05-21 02:39:06","Django doesn't recognize Gensim","<python><django><numpy><pycharm><gensim>","0","7","","","","CC BY-SA 4.0"
"53963743","1","","","2018-12-28 20:02:24","","3","824","<p>I've generated a PySpark Word2Vec model like so:</p>

<pre><code>from pyspark.ml.feature import Word2Vec

w2v = Word2Vec(vectorSize=100, minCount=1, inputCol='words', outputCol = 'vector')
model = w2v.fit(df)
</code></pre>

<p>(The data that I used to train the model on isn't relevant, what's important is that its all in the right format and successfully yields a <code>pyspark.ml.feature.Word2VecModel</code> object.)</p>

<p>Now I need to convert this model to a Gensim Word2Vec model. How would I go about this?</p>
","9898446","","","","","2018-12-29 06:51:33","Convert PySpark ML Word2Vec model to Gensim Word2Vec model","<pyspark><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"35252743","1","","","2016-02-07 11:13:24","","0","63","<p>I am trying to clean some text. I am keeping alphabets and numbers only. However, my text still contains other characters. </p>

<p>This is my function:</p>

<pre><code>def review_to_wordlist(review, remove_stopwords=False, remove_numbers = False ):
# Function to convert a document to a sequence of words,
# optionally removing stop words and numbers.  Returns a list of words.
#
# 1. Remove HTML
review_text = BeautifulSoup(review).get_text()
#
# 2. Remove non-letters
if True:
    review_text = re.sub(""[^a-zA-Z0-9]"","" "", review_text)
#
# 3. Convert words to lower case and split them
words = review_text.lower().split()
#
# 4. Optionally remove stop words (false by default)
if remove_stopwords:
    stops = set(stopwords.words(""english""))
    words = [w for w in words if not w in stops]
#
# 5. Return a list of words
return(words)
</code></pre>

<p>and this is one result that I get:</p>

<blockquote>
  <p>NuTone Central Vacuum System 45ÔøΩ Ell Ohio Steel Tandem Natural and
  Synthetic Turf Sweeping System Unique Home Designs 36 in. x 80 in. Su
  Casa Black Surface Mount Outswing Steel Security Door with Expanded
  Metal Screen Unique Home Designs 36 in. x 80 in. Su Casa Black Surface
  Mount Outswing Steel Security Door with Expanded Metal Screen Unique
  Home Designs 36 in. x 80 in. Su Casa Black Surface Mount Outswing
  Steel Security Door with Expanded Metal Screen MP Global Best 400 in.
  x 36 in. x 1/8 in. Acoustical Recycled Fiber Underlayment with Film
  for Laminate Wood MP Global Best 400 in. x 36 in. x 1/8 in. Acoustical
  Recycled Fiber Underlayment with Film for Laminate Wood Grip-Rite
  #10-1/4 in. x 2-1/2 in. 8ÔøΩ Bright Steel Ring-Shank Common Nails (1 lb.-Pack)</p>
</blockquote>

<p>the error that I get is:</p>

<pre><code>UnicodeDecodeError: 'utf8' codec can't decode bytes in position 5-6: unexpected end of data


676
Husky Pneumatic 3-1/2 in. 21ÔøΩ Full-Head Strip Framing Nailer
5157
RIDGID 3-1/2 in. 21ÔøΩ Round-Head Nailer
5158
RIDGID 3-1/2 in. 21ÔøΩ Round-Head Nailer
</code></pre>
","4127806","","4127806","","2016-02-07 12:16:23","2019-12-12 17:25:38","UnicodeDecodeError when cleaning text data","<python><regex><beautifulsoup><nlp><gensim>","1","14","","","","CC BY-SA 3.0"
"61952950","1","","","2020-05-22 10:29:01","","0","94","<p>I am looking for the most similar words for out-of-vocab OOV words using gensim. Something like this:</p>

<pre><code>    def get_word_vec(self, model, word):
    try:
        if word not in model.wv.vocab:
            mostSimWord = model.wv.similar_by_word(word)
            print(mostSimWord)
        else:
            print( word )
    except Exception as ex:
        print(ex)
</code></pre>

<p>Is there are way to achieve this task? Options other than gensim also welcomed.</p>
","1255393","","","","","2020-05-22 18:21:24","Find most similar words for OOV word","<python><nlp><gensim><similarity><oov>","1","0","","","","CC BY-SA 4.0"
"53941291","1","","","2018-12-27 07:24:25","","-1","48","<p>Pretrained models of English and other language wikipedia are available here... </p>

<p><a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">https://wikipedia2vec.github.io/wikipedia2vec/pretrained/</a></p>

<p>What is the difference between 100d and 500d in case of English wikipedia? </p>

<p>And what does these parameters mean to training (window=5, iteration=10, negative=15)</p>
","139150","","","","","2018-12-27 08:45:12","Wikipedia model training parameters","<machine-learning><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"27789298","1","27789586","","2015-01-05 22:52:13","","0","584","<p>I posted this issue on github (<a href=""https://github.com/piskvorky/gensim/issues/274"" rel=""nofollow"">https://github.com/piskvorky/gensim/issues/274</a>)</p>

<p>However, I need help with how to actually use the compatibility with numpy that gensim has.</p>

<p>I tried passing in None, <code>len(corpus)</code>, and 0-2 all failing.</p>

<p>The following is the corpus:</p>

<pre><code>[(0, 1.0), (1, 1.0), (2, 1.0)]
[(0, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0)]
[(2, 1.0), (5, 1.0), (7, 1.0), (8, 1.0)]
[(1, 1.0), (5, 2.0), (8, 1.0)]
[(3, 1.0), (6, 1.0), (7, 1.0)]
[(9, 1.0)]
[(9, 1.0), (10, 1.0)]
[(9, 1.0), (10, 1.0), (11, 1.0)]
[(4, 1.0), (10, 1.0), (11, 1.0)]
</code></pre>

<p>This is the code which doesn't work in my iPython notebook:</p>

<pre><code>from gensim import matutils
corpus = corpora.MmCorpus('/tmp/corpus.mm')
import numpy
numpy_matrix = matutils.corpus2dense(corpus)
</code></pre>

<p>Which throws IndexErrors</p>
","1377526","","","","","2015-01-05 23:17:12","corpus2dense requires two arguments, but tutorial example only uses one","<python><numpy><gensim>","1","3","","","","CC BY-SA 3.0"
"53503049","1","53506326","","2018-11-27 15:34:45","","6","6414","<p>I have already trained gensim doc2Vec model, which is finding most similar documents to an unknown one.</p>

<p>Now I need to find the similarity value between two unknown documents (which were not in the training data, so they can not be referenced by doc id)</p>

<pre><code>d2v_model = doc2vec.Doc2Vec.load(model_file)

string1 = 'this is some random paragraph'
string2 = 'this is another random paragraph'

vec1 = d2v_model.infer_vector(string1.split())
vec2 = d2v_model.infer_vector(string2.split())
</code></pre>

<p>in the code above vec1 and vec2 are successfully initialized to some values and of size - 'vector_size'</p>

<p>now looking through the gensim api and examples I could not find method that works for me, all of them are expecting TaggedDocument</p>

<p>Can I compare the feature vectors value by value and if they are closer => the texts are more similar?</p>
","5625696","","5625696","","2018-11-27 18:07:13","2020-12-13 21:41:02","Measure similarity between two documents using Doc2Vec","<python><machine-learning><nlp><gensim><doc2vec>","1","0","3","","","CC BY-SA 4.0"
"45960671","1","45970896","","2017-08-30 12:38:55","","3","2309","<p>I am using <code>gensim</code> <code>wmdistance</code> for calculating similarity between a reference sentence and 1000 other sentences. </p>

<pre><code>    model = gensim.models.KeyedVectors.load_word2vec_format(
     'GoogleNews-vectors-negative300.bin', binary=True)
    model.init_sims(replace=True)  

    reference_sentence = ""it is a reference sentence""
    other_sentences = [1000 sentences]
    index = 0
    for sentence in other_sentences: 
      distance [index] = model.wmdistance(refrence_sentence, other_sentences)
      index = index + 1
</code></pre>

<p>According to <code>gensim</code> <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">source code</a>, <code>model.wmdistance</code> returns the following:</p>

<pre><code>emd(d1, d2, distance_matrix)
</code></pre>

<p>where </p>

<pre><code>d1 =  # Compute nBOW representation of reference_setence.
d2 =  # Compute nBOW representation of other_sentence (one by one).
distance_matrix = see the source code as its a bit too much to paste it here.
</code></pre>

<p>This code is inefficient in two ways for my use case. </p>

<p>1) For the reference sentence, it is repeatedly calculating d1 (1000 times) for the distance function <code>emd(d1, d2, distance_matrix)</code>.</p>

<p>2) This distance function is called by multiple users from different points which repeat this whole process of <code>model.wmdistance(doc1, doc2)</code> for the same other_sentences and it is computationally expensive. For this 1000 comparisons, it takes around 7-8 seconds. </p>

<p>Therefore, I would like to isolate the two tasks. The final calculation of distance: <code>emd(d1, d2, distance_matrix)</code> and the preparation of these inputs: d1, d2, and distance matrix. As distance matrix depends on both so at least its input preparation should be isolated from the final matrix calculation. </p>

<p>My initial plan is to create three customized functions: </p>

<pre><code>d1 = prepared1(reference_sentence)
d2 = prepared2(other_sentence)
distance_matrix inputs = prepare inputs 
</code></pre>

<p>Is it possible to do this with this <code>gensim</code> function or should I just go my own customized version? Any ideas and solutions to deal with this problem in a better way?</p>
","1996842","","1996842","","2017-08-31 00:48:11","2018-12-30 06:14:30","Optimizing Gensim word mover's distance function for speed (wmdistance)","<python><nlp><nltk><gensim><word2vec>","1","1","1","","","CC BY-SA 3.0"
"27801403","1","","","2015-01-06 15:05:56","","1","781","<p>I'm trying to run this example code in Python 2.7 for LSI text clustering.</p>

<pre><code>import gensim
from gensim import corpora, models, similarities

documents = [""Human machine interface for lab abc computer applications"",
             ""A survey of user opinion of computer system response time"",
             ""The EPS user interface management system"",
             ""System and human system engineering testing of EPS"",
             ""Relation of user perceived response time to error measurement"",
             ""The generation of random binary unordered trees"",
             ""The intersection graph of paths in trees"",
             ""Graph minors IV Widths of trees and well quasi ordering"",
             ""Graph minors A survey""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)

texts = [[word for word in text if word not in tokens_once] for text in texts]

dictionary = corpora.Dictionary(texts)
corp = [dictionary.doc2bow(text) for text in texts]

# extract 400 LSI topics; use the default one-pass algorithm
lsi = gensim.models.lsimodel.LsiModel(corpus=corp, id2word=dictionary, num_topics=400)

# print the most contributing words (both positively and negatively) for each of the first ten topics
lsi.print_topics(10)
</code></pre>

<p>But it returns this error for the 2nd last command.</p>

<pre><code>Warning (from warnings module):
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py"", line 2499
    VisibleDeprecationWarning)
VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
</code></pre>

<p>Please let me know what I am doing wrong or if i need to update anything to make it work.</p>
","3571809","","","","","2015-01-06 15:05:56","LSI Clustering using gensim in python","<python><python-2.7><cluster-analysis><gensim>","0","2","1","","","CC BY-SA 3.0"
"54697748","1","54715357","","2019-02-14 19:26:53","","2","594","<p>I need to remove an invalid word from the vocab of a ""gensim.models.keyedvectors.Word2VecKeyedVectors"". </p>

<p>I tried to remove it using <code>del model.vocab[word]</code>, if I print the <code>model.vocab</code> the word disappeared, but when I run <code>model.most_similar</code> using other words the word that I deleted is still appearing as similar. 
So how can I delete a word from <code>model.vocab</code> in a way that affect the <code>model.most_similar</code> to not bring it?</p>
","2445630","","2445630","","2019-02-14 19:40:27","2019-04-18 07:55:12","Is there a way to remove a word from a KeyedVectors vocab?","<gensim><word2vec><embedding><glove>","2","1","1","","","CC BY-SA 4.0"
"53989210","1","56752428","","2018-12-31 15:52:48","","0","94","<p>I'm running this code</p>

<pre><code>from gensim.summarization import summarize
text = ""In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleones "" + \
       ""daughter Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando),""  + \
       ""the head of the Corleone Mafia family, is known to friends and associates as Godfather. ""  + \
       ""He and Tom Hagen (Robert Duvall), the Corleone family lawyer, are hearing requests for favors ""  + \
       ""because, according to Italian tradition, no Sicilian can refuse a request on his daughter's wedding "" + \
       "" day. One of the men who asks the Don for a favor is Amerigo Bonasera, a successful mortician ""  + \
       ""and acquaintance of the Don, whose daughter was brutally beaten by two young men because she""  + \
       ""refused their advances; the men received minimal punishment from the presiding judge. "" + \
       ""The Don is disappointed in Bonasera, who'd avoided most contact with the Don due to Corleone's"" + \
       ""nefarious business dealings. The Don's wife is godmother to Bonasera's shamed daughter, "" + \
       ""a relationship the Don uses to extract new loyalty from the undertaker. The Don agrees "" + \
       ""to have his men punish the young men responsible (in a non-lethal manner) in return for "" + \
        ""future service if necessary.""

print summarize(text)
</code></pre>

<p>It runs perfectly fine for the first time. But after that it shows me following error until I restart the kernel in spyder:</p>

<pre><code>File ""/home/taha/.local/lib/python2.7/site -packages/scipy/sparse/compressed.py"", line 50, in __init__ from .coo import coo_matrix
</code></pre>

<p><code>SystemError: Parent module 'scipy.sparse' not loaded, cannot perform relative import</code></p>

<p>I am using ubuntu 18.04</p>
","10852375","","4178623","","2018-12-31 17:35:23","2019-06-25 10:53:49","How to fix ""Relative import error"" in python (gensim.summarization)","<python><gensim><summarization>","2","0","","","","CC BY-SA 4.0"
"36014333","1","","","2016-03-15 14:37:41","","4","3096","<p>As I was trying to use Gensim to do some plain-text extraction from PDFs. However I encountered problems with using this library. </p>

<p>I followed the instructions on the <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow"">website</a> and it seemed to work properly. </p>

<p>I also have downloaded a Python IDE called Pycharm. But then when I am trying to do the ""quick example"" from <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow"">here</a>. I got some errors in my Pycharm. The logging activation has no errors, but the line <code>from gensim import corpora, models, similarities</code> isn't working. The IDE hints me that the word <code>gensim</code> (and the other three words) is <code>unresolved reference</code>.</p>

<p>So I think that maybe I need to do something to try to link Gensim as a reference library. But I am a totally newbie to python so I hope that someone can tell me how to do it. Or someone has worked with Gensim may also help me with this problem. Any ideas? </p>

<p>By the way, I am using python3 for the project as well. </p>
","2129773","","","","","2017-07-26 00:52:50","how to include gensim into pycharm","<python><python-3.x><ubuntu><pycharm><gensim>","2","1","1","","","CC BY-SA 3.0"
"53998446","1","54003011","","2019-01-01 19:47:37","","-1","1273","<p>Following reproducible script is used to compute the accuracy of a Word2Vec classifier with the <code>W2VTransformer</code> wrapper in gensim:</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from gensim.sklearn_api import W2VTransformer
from gensim.utils import simple_preprocess

# Load synthetic data
data = pd.read_csv('https://pastebin.com/raw/EPCmabvN')
data = data.head(10)

# Set random seed
np.random.seed(0)

# Tokenize text
X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
# Get labels
y_train = data.label

train_input = [x[0] for x in X_train]

# Train W2V Model
model = W2VTransformer(size=10, min_count=1)
model.fit(X_train)

clf = LogisticRegression(penalty='l2', C=0.1)
clf.fit(model.transform(train_input), y_train)

text_w2v = Pipeline(
    [('features', model),
     ('classifier', clf)])

score = text_w2v.score(train_input, y_train)
score
</code></pre>

<blockquote>
  <p>0.80000000000000004</p>
</blockquote>

<p>The problem with this script is that it <strong>only</strong> works when <code>train_input = [x[0] for x in X_train]</code>, which essentially is always the first word only. 
Once change to <code>train_input = X_train</code> (or <code>train_input</code> simply substituted by <code>X_train</code>), the script returns:</p>

<blockquote>
  <p>ValueError: cannot reshape array of size 10 into shape (10,10)</p>
</blockquote>

<p>How can I solve this issue, i.e. how can the classifier work with more than one word of input?</p>

<p><strong>Edit:</strong></p>

<p>Apparently, the W2V wrapper can't work with the variable-length train input, as compared to D2V. Here is a working D2V version:</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import Pipeline
from gensim.utils import simple_preprocess, lemmatize
from gensim.sklearn_api import D2VTransformer

data = pd.read_csv('https://pastebin.com/raw/bSGWiBfs')

np.random.seed(0)

X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
y_train = data.label

model = D2VTransformer(dm=1, size=50, min_count=2, iter=10, seed=0)
model.fit(X_train)

clf = LogisticRegression(penalty='l2', C=0.1, random_state=0)
clf.fit(model.transform(X_train), y_train)

pipeline = Pipeline([
        ('vec', model),
        ('clf', clf)
    ])

y_pred = pipeline.predict(X_train)
score = accuracy_score(y_train,y_pred)
print(score)
</code></pre>
","4697646","","4697646","","2019-01-02 09:13:48","2019-01-02 09:36:36","W2VTransformer: Only works with one word as input?","<scikit-learn><gensim><word2vec>","1","2","1","","","CC BY-SA 4.0"
"26145937","1","26148690","","2014-10-01 16:20:48","","1","1497","<p>I am trying to follow the tutorial on topic modelling / Latent Dirichlet Allocation (LDA) in the book Building Machine Learning Systems"" with Python.</p>

<p>I have not gone too far in this book, and the the first part of topic modelling returns errors for me:</p>

<pre><code>from gensim import corpora, models, similarities
corpus = corpora.BleiCorpus('./data/ap/ap.dat', './data/ap/vocab.txt')
</code></pre>

<p>Error:</p>

<pre><code>     63 
     64         self.fname = fname
---&gt; 65         with utils.smart_open(fname_vocab) as fin:
     66             words = [utils.to_unicode(word).rstrip() for word in fin]
     67         self.id2word = dict(enumerate(words))

/Users/user/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/gensim/utils.pyc in smart_open(fname, mode)
    659         from gzip import GzipFile
    660         return make_closing(GzipFile)(fname, mode)
--&gt; 661     return open(fname, mode)
    662 
    663 

IOError: [Errno 2] No such file or directory: './data/ap/vocab.txt'
</code></pre>

<p>The vocab.txt file does not exists, but switching to the directory where it is supposed to be, I find the following:</p>

<blockquote>
  <p>$ ls
  download_ap.sh        download_wp.sh      preprocess-wikidata.sh</p>
</blockquote>

<p>It looks like the ap data needs to be downloaded separately (not mentioned in the book), so by doing this:</p>

<pre><code>sh download_ap.sh
</code></pre>

<p>I get this:</p>

<pre><code>download_ap.sh: line 2: wget: command not found
tar: Error opening archive: Failed to open 'ap.tgz'
</code></pre>

<p>Does anybody knows how to solve this issue?</p>

<p>Thank you </p>
","1290147","","1290147","","2014-10-01 16:42:36","2014-10-01 19:03:55","BleiCorpus and Associated Press dataset in Gensim: IO Error","<python><enthought><lda><topic-modeling><gensim>","2","0","2","","","CC BY-SA 3.0"
"63419318","1","63421005","","2020-08-14 19:59:19","","0","130","<p>I have the following piece of code:</p>
<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load('model2')
X = model[model.wv.vocab]
</code></pre>
<p>This piece of code works on one of my machines but not another. The model file is the same. What's going on? The error message I get is the following:</p>
<pre><code>  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/word2vec.py&quot;, line 1330, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 427, in load
    obj._load_specials(fname, mmap, compress, subname)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 458, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 469, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/numpy/lib/npyio.py&quot;, line 440, in load
    pickle_kwargs=pickle_kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/numpy/lib/format.py&quot;, line 771, in read_array
    array.shape = shape
ValueError: cannot reshape array of size 16777184 into shape (134441,128)
</code></pre>
<p>To install gensim, I used <code>conda install -c anaconda gensim</code></p>
","8100895","","","","","2021-06-06 22:43:40","Array reshape error when loading word2vec model","<python><amazon-ec2><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"54005055","1","54005167","","2019-01-02 10:54:48","","0","42","<p>I just came across an error when trying to apply a cross-validation for a paragraph vector model: </p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from gensim.sklearn_api import D2VTransformer

data = pd.read_csv('https://pastebin.com/raw/bSGWiBfs')
np.random.seed(0)

X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
y_train = data.label

model = D2VTransformer(size=10, min_count=1, iter=5, seed=1)
clf = LogisticRegression(random_state=0)

pipeline = Pipeline([
        ('vec', model),
        ('clf', clf)
    ])

pipeline.fit(X_train, y_train)

score = pipeline.score(X_train, y_train)
print(""Score:"", score) # This works
cval = cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=3)
print(""Cross-Validation:"", cval) # This doesn't work
</code></pre>

<blockquote>
  <p>KeyError: 0</p>
</blockquote>

<p>I experimented by replacing <code>X_train</code> in <code>cross_val_score</code> with <code>model.transform(X_train)</code> or <code>model.fit_transform(X_train)</code>. Also, I tried the same with raw input data (<code>data.text</code>), instead of pre-processed text. I suspect that something must be wrong with the format of <code>X_train</code> for the cross-validation, as compared to the <code>.score</code> function for Pipeline, which works just fine. I also noted that the <code>cross_val_score</code> worked with <code>CountVectorizer()</code>.</p>

<p>Does anyone spot the mistake?</p>
","4697646","","","","","2019-01-02 11:04:00","Cross-validation for paragraph-vector model","<scikit-learn><transform><cross-validation><gensim>","1","0","","","","CC BY-SA 4.0"
"46064892","1","46065700","","2017-09-06 00:00:34","","0","467","<p>I'm trying to learn the skip-gram model within word2vec, however I'm confused by some of the basic concepts.  To start, here is my current understanding of the model motivated with an example.  I am using Python <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim</a> as I go.</p>

<p>Here I have a corpus with three sentences.</p>

<pre><code>sentences = [
    ['i', 'like', 'cats', 'and', 'dogs'],
    ['i', 'like', 'dogs'],
    ['dogs', 'like', 'dogs']
]
</code></pre>

<p>From this, I can determine my vocabulary, <code>V = ['and', 'cats', 'dogs', 'i', 'like']</code>.</p>

<p>Following <a href=""https://arxiv.org/pdf/1310.4546.pdf"" rel=""nofollow noreferrer"">this paper</a> by Tomas Mikolov (and others)</p>

<blockquote>
  <p>The basic Skip-gram formulation defines p(w_t+j |w_t) using the softmax
  function:</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/hza7Q.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hza7Q.gif"" alt=""skipgram""></a></p>

<blockquote>
  <p>where v_w and v‚Ä≤_w are the ‚Äúinput‚Äù and ‚Äúoutput‚Äù vector representations
  of w, and W is the number of words in the vocabulary.</p>
</blockquote>

<p>To my understanding, the skip-gram model involves two matrices (I'll call them <em>I</em> and <em>O</em>) which are the vector representations of ""input/center"" words and the vector representation of ""output/context"" words.  Assuming d = 2 (vector dimension or 'size' as its called in genism), <em>I</em> should be a 2x5 matrix and <em>O</em> should be a 5x2 matrix.  At the start of the training procedure, these matrices are filled with random values (yes?).  So we might have</p>

<pre><code>import numpy as np
np.random.seed(2017)

I = np.random.rand(5,2).round(2)  # 5 rows by 2 cols
[[ 0.02  0.77] # and
 [ 0.45  0.12] # cats
 [ 0.93  0.65] # dogs
 [ 0.14  0.23] # i
 [ 0.23  0.26]] # like

O = np.random.rand(2,5).round(2)  # 2 rows by 5 cols
  #and  #cats #dogs  #i   #like 
[[ 0.11  0.63  0.39  0.32  0.63]
 [ 0.29  0.94  0.15  0.08  0.7 ]]
</code></pre>

<p>Now if I want to calculate the probability that the word ""dogs"" appears in the context of ""cats"" I should do</p>

<p>exp([0.39, 0.15] * [0.45  0.12])/(...) = (0.1125)/(...)</p>

<p>A few questions on this:</p>

<ol>
<li>Is my understanding of the algorithm correct thus far?</li>
<li>Using genism, I can train a model on this data using</li>
</ol>

<p>&nbsp;</p>

<pre><code>import gensim
model = gensim.models.Word2Vec(sentences, sg = 1, size=2, window=1, min_count=1)
model.wv['dogs']  # array([ 0.06249372,  0.22618999], dtype=float32)
</code></pre>

<p>For the array given, is that the vector for ""dogs"" in the Input matrix or the Output matrix?  Is there a way to view both matrices in the final model?</p>

<ol start=""3"">
<li>Why does <code>model.wv.similarity('cats','cats')</code> = 1?  I thought this should be closer to 0, since the data would indicate that the word ""cats"" is unlikely to occur in the context of the word ""cats"".</li>
</ol>
","2146894","","2146894","","2017-09-06 00:31:04","2017-09-06 01:59:37","word2vec training procedure clarification","<gensim><word2vec>","1","1","1","","","CC BY-SA 3.0"
"54709178","1","54709303","","2019-02-15 12:15:36","","4","4272","<p>I have a list of ~10 million sentences, where each of them contains up to 70 words.</p>

<p>I'm running gensim word2vec on every word, and then taking the simple average of each sentence. The problem is that I use min_count=1000, so a lot of words are not in the vocab. </p>

<p>To solve that, I intersect the vocab array (that contains about 10000 words) with every sentence, and if there's at least one element left in that intersection, it returns its the simple average, otherwise, it returns a vector of zeros.</p>

<p>The issue is that calculating every average takes a very long time when I run it on the whole dataset, even when splitting into multiple threads, and I would like to get a better solution that could run faster.</p>

<p>I'm running this on an EC2 r4.4xlarge instance.</p>

<p>I already tried switching to doc2vec, which was way faster, but the results were not as good as word2vec's simple average.</p>

<pre><code>word2vec_aug_32x = Word2Vec(sentences=sentences, 
                        min_count=1000, 
                        size=32, 
                        window=2,
                        workers=16, 
                        sg=0)

vocab_arr = np.array(list(word2vec_aug_32x.wv.vocab.keys()))

def get_embedded_average(sentence):
    sentence = np.intersect1d(sentence, vocab_arr)
    if sentence.shape[0] &gt; 0:
        return np.mean(word2vec_aug_32x[sentence], axis=0).tolist()
    else:
        return np.zeros(32).tolist()

pool = multiprocessing.Pool(processes=16)

w2v_averages = np.asarray(pool.map(get_embedded_average, np.asarray(sentences)))
pool.close()
</code></pre>

<p>If you have any suggestions of different algorithms or techniques that have the same purpose of sentence embedding and could solve my problem, I would love to read about it.</p>
","7391932","","","","","2019-06-27 05:07:56","How to handle words that are not in word2vec's vocab optimally","<python><numpy><optimization><gensim><word2vec>","2","0","1","","","CC BY-SA 4.0"
"43633092","1","51293410","","2017-04-26 11:37:02","","6","2758","<p>I'm new to deeplearning4j, i want to make sentence classifier using words vector as input for the classifier. 
I was using python before, where the vector model was generated using gensim, and i want to use that model for this new classifier. 
Is it possible to use gensim's word2vec model in deeplearning4j.word2vec and how i can do that?</p>
","5182227","","5182227","","2019-02-04 21:31:28","2019-02-04 21:31:28","Is it possible to use gensim word2vec model in deeplearning4j.word2vec?","<java><gensim><word2vec><deeplearning4j>","1","3","2","","","CC BY-SA 4.0"
"52982761","1","","","2018-10-25 06:33:39","","2","475","<p>I want to train word2vec and fasttext to get vectors for a specific dataset that I have.</p>

<p>What should my model take as input?</p>

<p>My file is like this:</p>

<pre><code>Customer_4: I want to book a ticket to New York.
Agent_9: Okay, when do you want the tickets for
Customer_4: hmm, wait a sec
Agent_9: Sure
Customer_4: When is the least expensive to fly
</code></pre>

<p>Now, How should I prepare my data for word2vec to run? Does the word2vec model take inter sentence similaarity into account, i.e. should i not prepare the corpus sentence wise.</p>
","10555987","","","","","2018-10-28 23:51:00","How to prepare data for word2vec in gensim and fasttext?","<python><machine-learning><gensim><word2vec><fasttext>","1","0","","","","CC BY-SA 4.0"
"62343252","1","","","2020-06-12 11:22:16","","0","286","<p>I am trying to use gensim phraser on a column in a df. The sample df is given below</p>

<pre><code>col1   col2
1      ""this is test1 and is used for test1""
2      ""this is content of row which is second row""
3      ""this is the third row""
</code></pre>

<p>I have wrote a method for bigrams</p>

<pre><code>def bigrams(text):
    bigram = Phrases(text, min_count=1)
    bigram_mod = Phraser(bigram)
    return [bigram_mod[doc] for doc in text]
</code></pre>

<p>And I tried</p>

<pre><code>df['col2'].apply(bigrams)
df['col2'].apply(lambda x: bigrams([x])) - so that the text is enclosed in list
</code></pre>

<p>but I get the characters as output and not the bigrams. What am I missing here.</p>
","3910430","","","","","2020-06-13 11:27:23","Use gensim phraser on pandas column using apply method","<python><pandas><gensim><n-gram><phrase>","2","0","","","","CC BY-SA 4.0"
"53955958","1","","","2018-12-28 09:02:04","","1","400","<p>How to get most frequent context words from pretrained fasttext model?</p>

<p>For example:
For word 'football' and corpus <code>[""I like playing football with my friends""]</code></p>

<p>Get list of context words: <code>['playing', 'with','my','like']</code></p>

<p>I try to use 
<code>model_wiki = gensim.models.KeyedVectors.load_word2vec_format(""wiki.ru.vec"")
model.most_similar(""–±–ª–æ–∫"")</code></p>

<p>But it's not satisfied for me</p>
","7995764","","3091398","","2018-12-28 09:37:09","2019-02-26 21:25:21","How to get list of context words in Gensim","<python><gensim><word2vec><fasttext>","1","0","","","","CC BY-SA 4.0"
"37147955","1","","","2016-05-10 20:04:39","","0","267","<p>I just updated my version of Xcode to 7.3. When I run <code>pip install --upgrade gensim</code> the process completed without any issues. However, when I try import gensim within the python shell the terminal bars a bunch of C++ output with a block of execution errors that begins with:</p>

<p><code>Exception: Compilation failed (return status=1): clang: error: unsupported option '-b mi2'. clang: error: unsupported option '-b mi'. clang: error: unknown argument: '-target-feature'. clang: error: unknown argument: '-sse4a'. clang: error: unknown argument: '-target-feature'. clang: error: unknown argument: '-target-feature'. clang: error: unknown argument: '-target-feature'. clang: error: unknown argument: '-tbm'. clang: error: unknown argument: '-target-feature'. clang: error: unknown argument: '-target-feature'....</code></p>

<p>I think this has something to do with where gensim is looking for its header files, but I'm somewhat at a loss. Any help debugging would be greatly appreciated.</p>
","375267","","","","","2016-05-10 21:42:39","import gensim fails since updating to Xcode 7.3","<python><c++><xcode><gensim>","1","0","1","","","CC BY-SA 3.0"
"37350767","1","","","2016-05-20 15:49:17","","0","636","<p>I am training my own word2vec model on Gensim in python, on a relatively small dataset. The data consist of about 3000 short-text entries from different people, most of which are two or three sentences. I know this is small for a word2vec dataset, but I've seen similar ones work in the past.</p>

<p>For some reason, when I train my model all of the features are impractically close to one another. For instance:</p>

<pre><code>model.most_similar('jesus/NN')
[(u'person/NN', 0.9999418258666992),
 (u'used/VB', 0.9998890161514282),
 (u'so/RB', 0.9998359680175781),
 (u'question/NN', 0.9997845888137817),
 (u'just/RB', 0.9996646642684937),
 (u'other/NN', 0.9995589256286621),
 (u'allow/VB', 0.9995476603507996),
 (u'feel/VB', 0.9995381236076355),
 (u'attend/VB', 0.9995047450065613),
 (u'make/VB', 0.9994802474975586)]
</code></pre>

<p>The parts of speech are included because I lemmatize the data.</p>

<p>Here is my training code:</p>

<pre><code>cleanedResponses = []
#For every response
for rawValue in df['QB17_W6'].values:
    lemValue = lemmatize(rawValue)
    cleanedResponses.append(lemValue)

df['cleaned_responses'] = cleanedResponses
bigram_transformer = Phrases(df.cleaned_responses.values)

model = Word2Vec(bigram_transformer[df.cleaned_responses.values], size=5)
</code></pre>

<p>This also happens when I train without the bigram transformer. Does anybody have an idea as to why the distances are so close?</p>
","5424276","","","","","2016-05-20 15:49:17","Gensim Word2Vec distances are too close","<python><gensim><word2vec>","0","4","","","","CC BY-SA 3.0"
"52391572","1","","","2018-09-18 17:11:22","","0","680","<p>I am trying to use a <strong>text corpus file</strong> (<em>One sentence by line</em>) to extarct <strong>words co-occurrence</strong> from it in order to use them in a later traitement. So how can i extract word(statistical) co-occurrence from large corpus file using <strong>gensim</strong> and how to use them later ?  </p>
","4929556","","4929556","","2018-09-18 21:20:35","2018-09-18 21:20:35","Gensim: How to extract words co-occurrence?","<python><nlp><gensim><word2vec><corpus>","0","7","","","","CC BY-SA 4.0"
"63218694","1","","","2020-08-02 16:54:45","","0","130","<p>here is the problematic code:</p>
<pre><code>from gensim.corpora import Dictionary
tweets_dictionary = Dictionary(df.tokenized)
</code></pre>
<p>the Panda Dataframe df is build as followed with two columns &quot;created_at&quot; and &quot;tokenized&quot;. &quot;tokenized&quot; consists of a list of words:</p>
<p><a href=""https://i.stack.imgur.com/x3zxb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x3zxb.png"" alt=""df.head()"" /></a></p>
<p>I get the following error message while running the problematic code:</p>
<p><code>TypeError: doc2bow expects an array of unicode tokens on input, not a single string</code></p>
<p>This is very bizzare to me, as the column tokenized is not a single string. I have tried converting the column into a single list, a list of lists and a tuple, but nothing has worked so far....thanks in advance for your help!</p>
","12690267","","","","","2020-08-02 17:09:41","gensim.corpora Dictionary type error interprets tokenized column as single string","<python><dictionary><gensim>","1","0","","","","CC BY-SA 4.0"
"46065773","1","","","2017-09-06 02:12:56","","3","836","<p>In word2vec, after training, we get two weight matrixes:1.input-hidden weight matrix; 2.hidden-output weight matrix. and people will use the input-hidden weight matrix as the word vectors(each row corresponds to a word, namely, the word vectors).Here comes to my confusions:</p>

<ol>
<li>why people use input-hidden weight matrix as the word vectors instead of the hidden-output weight matrix.</li>
<li>why don't we just add softmax activation function to the hidden layers rather than output layers, thus preventing time-consuming.</li>
</ol>

<p>Plus, clarifying remarks on the intuition of how word vectors can be obtained like this will be appreciated.</p>
","8322066","","","","","2020-10-17 12:31:19","why we use input-hidden weight matrix to be the word vectors instead of hidden-output weight matrix?","<nlp><gensim><word2vec>","2","0","","","","CC BY-SA 3.0"
"61021821","1","","","2020-04-03 23:21:08","","0","58","<p>I am currently implementing a natural text generator for a school project. I have a dataset of sentences of predetermined lenght and key words, I convert them in vectors thanks to gensim and GoogleNews-vectors-negative300.bin.gz. I train a recurrent neural network to create a list of vectors that I compare to the list of vectors of the real sentence. So I try to get as close as possible to the ""real"" vectors.</p>

<p>My problem happens when I have to convert back vectors into words: my vectors aren't necessarily in the google set. So I would like to know if there is an efficient solution to get the closest vector in the Google set to an outpout vector.</p>

<p>I work with python 3 and Tensorflow </p>

<p>Thanks a lot, feel free to ask any questions about the project</p>

<p>Charles </p>
","13216555","","","","","2020-04-04 01:08:20","get closest vector from unknown vector with gensim","<python-3.x><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"44871728","1","","","2017-07-02 14:14:36","","4","1486","<p>I used the gensim package in Python to load the pre-trained Google word2vec dataset. I then want to use k-means to find meaningful clusters on my word vectors, and find the representative word for each cluster. I am thinking to use the word whose corresponding vector is closest to the centroid of a cluster to represent that cluster, but don't know whether this is a good idea as my experiment did not give me good results.</p>

<p>My example code is like below:</p>

<pre><code>import gensim
import numpy as np
import pandas as pd
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import pairwise_distances_argmin_min

model = gensim.models.KeyedVectors.load_word2vec_format('/home/Desktop/GoogleNews-vectors-negative300.bin', binary=True)  

K=3

words = [""ship"", ""car"", ""truck"", ""bus"", ""vehicle"", ""bike"", ""tractor"", ""boat"",
       ""apple"", ""banana"", ""fruit"", ""pear"", ""orange"", ""pineapple"", ""watermelon"",
       ""dog"", ""pig"", ""animal"", ""cat"", ""monkey"", ""snake"", ""tiger"", ""rat"", ""duck"", ""rabbit"", ""fox""]
NumOfWords = len(words)

# construct the n-dimentional array for input data, each row is a word vector
x = np.zeros((NumOfWords, model.vector_size))
for i in range(0, NumOfWords):
    x[i,]=model[words[i]] 

# train the k-means model
classifier = MiniBatchKMeans(n_clusters=K, random_state=1, max_iter=100)
classifier.fit(x)

# check whether the words are clustered correctly
print(classifier.predict(x))

# find the index and the distance of the closest points from x to each class centroid
close = pairwise_distances_argmin_min(classifier.cluster_centers_, x, metric='euclidean')
index_closest_points = close[0]
distance_closest_points = close[1]

for i in range(0, K):
    print(""The closest word to the centroid of class {0} is {1}, the distance is {2}"".format(i, words[index_closest_points[i]], distance_closest_points[i]))
</code></pre>

<p>The output is as below:</p>

<pre><code>[2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]
The closest word to the centroid of class 0 is rabbit, the distance is 1.578625818679259
The closest word to the centroid of class 1 is fruit, the distance is 1.8351978219013796
The closest word to the centroid of class 2 is car, the distance is 1.6586030662247868
</code></pre>

<p>In the code I have 3 categories of words: vehicle, fruit and animal. From the output we can see that k-means correctly clustered the words for all 3 categories, but the representative words derived using the centroid method are not very good, as for class 0 I want to see ""animal"" but it gives ""rabbit"", and for class 2 I want to see ""vehicle"" but it returns ""car"".</p>

<p>Any help or suggestion in finding the good representative word for each cluster will be highly appreciated.</p>
","4498726","","4498726","","2017-07-02 14:21:18","2017-07-03 11:38:53","How to find the meaningful word to represent each k-means cluster derived from word2vec vectors?","<python><k-means><gensim><word2vec>","1","0","1","","","CC BY-SA 3.0"
"63324116","1","63344486","","2020-08-09 08:24:40","","1","86","<p>Im using gensim version '3.8.3' <br></p>
<p>when im running for model Word2Vec and FastText <code>build_vocab</code> and <code>train</code> <br>
the logs from those functions are missing the values</p>
<p>for example part of the logs of  <code>build_vocab</code> of FastText</p>
<pre><code>08/09/2020 08:19:18 AM [INFO] collecting all words and their counts
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
</code></pre>
<p>the index is missing and printed as <code>i</code></p>
<p>is there a way to solve it? is it a version bug?</p>
","2228884","","","","","2020-08-10 16:47:51","word2vec logging missing values","<python-3.x><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"46083322","1","","","2017-09-06 19:52:17","","3","2711","<p>Is it possible to to train a doc2vec model where a single document has multiple tags?
For example, in movie reviews,</p>

<pre><code>doc0 = doc2vec.TaggedDocument(words=review0,tags=['UID_0','horror','action'])
doc1 = doc2vec.TaggedDocument(words=review1,tags=['UID_1','drama','action','romance'])
</code></pre>

<p>In such case where each document has a unique tag (UID) and multiple categorical tags, how do I access the vector after the training? For example, what would be the most proper syntax to call</p>

<pre><code>model['UID_1']
</code></pre>
","7415539","","","","","2017-09-08 05:41:33","Multiple tags for single document in doc2vec. TaggedDocument","<python><nlp><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 3.0"
"46072991","1","46081204","","2017-09-06 10:32:37","","2","299","<p>How does word2vec create vectors for words? I trained two word2vec models using two different files (from commoncrawl website) but I am getting same word vectors for a given word from both models. </p>

<p>Actually, I have created multiple word2vec models using different text files from the commoncrawl website. Now I want to check which model is better among all. How can select the best model out of all these models and why I am getting same word vectors for different models?</p>

<p>Sorry, If the question is not clear. </p>
","5059870","","","","","2017-09-06 17:32:11","How does word embedding/ word vectors work/created?","<neural-network><nlp><deep-learning><gensim><word2vec>","1","0","1","","","CC BY-SA 3.0"
"37461117","1","","","2016-05-26 12:39:01","","0","564","<p>I want to try to implement word2vec to Vietnamase language, but I'm confused about the pre-trained vectors, when I tried to use in the English language I use Google News-vectors-negative300.bin.gz (about 3.4GB) for pre-trained vectors and it works good. if i do with vietnam language should I make the data pre-trained vectors themselves ??
how to make a pre-trained vectors such as Google News-vectors-negative300.bin.gz, then I try to convert Google News-vectors-negative300.bin to text format the result as:</p>

<p>3000000 300</p>

<p> 0.001129 -0.000896 0.000319 0.001534 0.001106 -0.001404 -0.000031 -0.000420 -0.000576 0.001076 -0.001022 -0.000618 -0.000755 0.001404 -0.001640 -0.000633 0.001633 -0.001007 -0.001266 0.000652 -0.000416 -0.001076 0.001526 -0.000275 0.000140 0.001572 0.001358 -0.000832 -0.001404 0.001579 0.000254 -0.000732 -0.000105 -0.001167 0.001579</p>

<p>how to change a letter or word into the form above ??</p>
","6179731","","","","","2016-08-15 10:03:26","how to make a pre-trained vectors for other language (word2vec)?","<c><python-2.7><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"52425323","1","","","2018-09-20 12:36:04","","0","225","<p>I checked unsupervised clsutering on gensim, fasttext, sklearn but did not find any documentation where I can cluster my text data using unsupervised learn without mentioning numbers of cluster to be identified</p>

<p>for example in sklearn KMneans clustering </p>

<pre><code>km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100)
</code></pre>

<p>Where I have to provide n_clusters. </p>

<p>In my case, I have text and it should be automatically identify numbers of clusters in it and cluster the text. Any reference article or link much appreciated. </p>
","9433624","","6464269","","2018-09-21 10:37:41","2018-09-21 10:37:41","Is there any unsupervised clustering technique which can identify numbers clusters itself?","<tensorflow><scikit-learn><gensim><unsupervised-learning><fasttext>","1","1","","","","CC BY-SA 4.0"
"52402693","1","52857193","","2018-09-19 09:34:49","","0","209","<p>How can i generate non-english (french , spanish , italian ) word embedding from english word embedding ?</p>

<p>What are the best ways to generate high quality word embedding for non - english words .</p>

<p>Words may include (samsung-galaxy-s9)</p>
","7962476","","","","","2018-10-17 14:21:49","Non English Word Embedding from English Word Embedding","<tensorflow><nlp><gensim><word-embedding><chainer>","2","0","","","","CC BY-SA 4.0"
"54823225","1","","","2019-02-22 08:49:26","","0","215","<p>I have trained the LDA model to cluster 100 topic, and according to my knowledge, every topic should be outputted with a certain probabiliy, all adding up to 1.</p>

<p>But when I run this code, I am getting only 2 topics.</p>

<p>Please help.</p>

<pre><code>text = ""A blood cell, also called a hematocyte, is a cell produced by hematopoiesis and normally found in blood.""

# transform text into the bag-of-words space
bow_vector = dictionary.doc2bow(tokenize(text))
lda_vector = lda_model[bow_vector]
print(""LDA Output: "", lda_vector)
print(""\nTop Keywords from highest prob Topic: "",lda_model.print_topic(max(lda_vector, key=lambda item: item[1])[0]))
print(""\n\nAddition of all the probabilities from LDA output:"",functools.reduce(lambda x,y:x+y,[i[1] for i in lda_vector]))
</code></pre>

<blockquote>
  <p>LDA Output:  [(64, 0.6952628), (69, 0.18223721)]</p>
  
  <p>Top Keywords from highest prob Topic:  0.042*""health"" + 0.032*""medical"" + 0.017*""patients"" + 0.016*""cancer"" + 0.015*""hospital"" + 0.015*""said"" + 0.015*""treatment"" + 0.012*""doctors"" + 0.012*""care"" + 0.012*""drug""</p>
  
  <p>Addition of all the probabilities from LDA output: 0.8775</p>
</blockquote>
","10842858","","","","","2019-02-28 00:19:49","Gensim LDA giving output of Topic IDs but probabilities are not adding up to 1","<machine-learning><nlp><gensim><topic-modeling><unsupervised-learning>","1","0","","","","CC BY-SA 4.0"
"63549977","1","63550484","","2020-08-23 17:24:27","","0","149","<p>I am doing research that requires direct manipulation &amp; embedding of one-hot vectors and I am trying to use gensim to load a pretrained word2vec model for this.</p>
<p>The problem is they don't seem to have a direct api for working with 1-hot-vectors. And I am looking for work arounds.</p>
<p>So I wanted to know if anyone knows of a way to do this? Or more specifically if these vocab indices (which are defined quite ambiguously). Could be indices into corresponding 1-hot-vectors?</p>
<p>Context I have found:</p>
<ul>
<li>Seems <a href=""https://stackoverflow.com/questions/40458742/gensim-word2vec-accessing-in-out-vectors"">this question</a> is related but I tried accessing the 'input embeddings' (assuming they were one-hot representations), via model.syn0 (from link in answer), but I got a non-sparse matrix...</li>
<li>Also appears <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">they refer to word indices as 'doctags'</a> (search for Doctag/index).</li>
<li><a href=""https://stackoverflow.com/questions/47117569/how-to-get-word2index-from-gensim"">Here</a> is another question giving some context to the indices (although not quite answering my question).</li>
<li><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">Here</a> is the official documentation:</li>
</ul>
<p>################################################</p>
<p>class gensim.models.keyedvectors.Vocab(**kwargs)
Bases: object</p>
<p>A single vocabulary item, used internally for collecting per-word frequency/sampling info, and for constructing binary trees (incl. both word leaves and inner nodes).</p>
<p>################################################</p>
","1925652","","","","","2020-08-23 18:14:19","Is a gensim vocab index the index in the corresponding 1-hot-vector?","<gensim><word2vec><one-hot-encoding>","1","0","","","","CC BY-SA 4.0"
"52460163","1","","","2018-09-22 19:31:45","","0","486","<p>I'm using the pre-trained model from Google to do word embedding. What kind of string should I give to the model to receive and array full of zeros? I need to pad some sentences and I need an array of zeros to embed the padding. Maybe should I create the array of zeros instead of the model?</p>
","6660601","","","","","2018-09-22 19:31:45","Which Pad token in Google-News Word2vec Pre-trained Model","<python><gensim><word2vec><word-embedding>","0","5","","","","CC BY-SA 4.0"
"63551484","1","63552461","","2020-08-23 20:03:59","","0","76","<p>I'm testing feeding gensim's Word2Vec different sentences with the same overall vocabulary to see if some sentences carry &quot;better&quot; information than others. My method to train Word2Vec looks like this</p>
<pre><code>def encode_sentences(self, w2v_params, sentences):
    model = Word2Vec(sentences, **w2v_params)
    
    idx_order = torch.tensor([int(i) for i in model.wv.index2entity], dtype=torch.long)
    X = torch.zeros((idx_order.max()+1, w2v_params['size']), dtype=torch.float)
    
    # Put embeddings back in order
    X[idx_order] = torch.tensor(model.wv.vectors)    
    return X, y
</code></pre>
<p>What I'm hoping for here, is each time w2v runs, it starts with a fresh model and trains from scratch. However, I'm testing 3 kinds of sentences, so my test code looks like this:</p>
<pre><code>def test(sentence):
    w2v = {'size': 128, 'sg': 1}
    X = encode_sentences(w2v, sentence)
    evaluate(X) # Basic cluster analysis stuff here

# s1, s2 and s3 are the 3 sets of sentences with the same vocabulary in different order/frequency
[print(test(s) for s in [s1, s2, s3]]
</code></pre>
<p>However, I noticed if I remove one of the test sets, and only test <code>s1</code> and <code>s2</code> (or any combination of 2 sets of the three), the overall quality of the clusterings decreases. If I go back into <code>encode_sentences</code> and add <code>del model</code> before the <code>return</code> call, the overall cluster quality also goes down but remains consistent no matter how many datasets are tested.</p>
<p>What gives? Is the constructor not actually building a fresh model each time with new weights? The docs and source code give no indication of this. I'm quite sure it isn't my evaluation method, as everything was fixed after the <code>del model</code> was added. I'm at a loss here... Are these runs actually independent, or is each call to <code>Word2Vec(foo, ...)</code> equivalent to retraining the previous model with <code>foo</code> as new data?</p>
<p>And before you ask, no <code>model</code> is nowhere outside of the scope of the <code>encode_sentence</code> variable; that's the only time that variable name is used in the whole program. Very odd.</p>
<h3>Edit with more details</h3>
<hr />
<p>If it's important, I'm using Word2Vec to build node embeddings on a graph the way Node2Vec does with different walk strategies. These embeddings are then fed to a Logistic Regression model (<code>evaluate(X)</code>) and which calculates area under the roc.</p>
<p>Here is some sample output of the model before adding the <code>del model</code> call to the <code>encode_sentences</code> method averaged over 5 trials:</p>
<pre><code>Random walks:   0.9153 (+/-) 0.002
Policy walks:   0.9125 (+/-) 0.005
E-greedy walks: 0.8489 (+/-) 0.011
</code></pre>
<p>Here is the same output with the only difference being <code>del model</code> in the encoding method:</p>
<pre><code>Random walks:   0.8627 (+/-) 0.005
Policy walks:   0.8527 (+/-) 0.002
E-greedy walks: 0.8385 (+/-) 0.009
</code></pre>
<p>As you can see, in each case, the variance is very low (the +/- value is the standard error) but the difference between the two runs is almost a whole standard deviation. It seems odd that if each call to <code>Word2Vec</code> was truly independent that manually freeing the data structure would have such a large effect.</p>
","6136888","","6136888","","2020-08-24 13:48:04","2020-08-24 13:48:04","Does the gensim `Word2Vec()` constructor make a completely independent model?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"52486070","1","52489199","","2018-09-24 19:26:58","","0","466","<p>I am using the following code to get the ordered list of user posts.</p>

<pre><code>model = doc2vec.Doc2Vec.load(doc2vec_model_name)
doc_vectors = model.docvecs.doctag_syn0
doc_tags = model.docvecs.offset2doctag

for w, sim in model.docvecs.most_similar(positive=[model.infer_vector('phone_comments')], topn=4000):
        print(w, sim)
        fw.write(w)
        fw.write("" ("")
        fw.write(str(sim))
        fw.write("")"")
        fw.write(""\n"")

fw.close()
</code></pre>

<p>However, I am also getting the vector <code>""phone comments""</code> (that I use to find nearest neighbours) in like 6th place of the list. Is there any mistake I do in the code? or is it a issue in Gensim (becuase the vector cannot be a neighbour of itself)?</p>

<p><strong>EDIT</strong></p>

<p>Doc2vec model training code</p>

<pre><code>######Preprocessing
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for key, value in my_d.items():
    value = re.sub(""[^1-9a-zA-Z]"","" "", value)
    words = value.lower().split()
    tags = key.replace(' ', '_')
    docs.append(analyzedDocument(words, tags.split(' ')))

sentences = []  # Initialize an empty list of sentences
######Get n-grams
#Get list of lists of tokenised words. 1 sentence = 1 list
for item in docs:
    sentences.append(item.words)

#identify bigrams and trigrams (trigram_sentences_project) 
trigram_sentences_project = []
bigram = Phrases(sentences, min_count=5, delimiter=b' ')
trigram = Phrases(bigram[sentences], min_count=5, delimiter=b' ')

for sent in sentences:
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]
    trigram_sentences_project.append(trigrams_)

paper_count = 0
for item in trigram_sentences_project:
    docs[paper_count] = docs[paper_count]._replace(words=item)
    paper_count = paper_count+1

# Train model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 5, workers = 4, iter = 20)

#Save the trained model for later use to take the similarity values
model_name = user_defined_doc2vec_model_name
model.save(model_name)
</code></pre>
","9176239","","9176239","","2018-09-25 03:55:50","2018-09-25 03:55:50","Why Gensim most similar in doc2vec gives the same vector as the output?","<nlp><data-mining><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"54186233","1","54191144","","2019-01-14 17:15:22","","0","783","<p>I am generating a Doc2Vec embedding of a Pandas DataFrame by following the guidance provided <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
from gensim.test.test_doc2vec import ConcatenatedDoc2Vec
import gensim.models.doc2vec
from collections import OrderedDict
import pandas as pd
import numpy as np

cube_embedded =  # pandas cube
# convert the cube to documents
alldocs = [TaggedDocument(doc, [i]) for i, doc in enumerate(cube_embedded.values.tolist())]

# train models
simple_models = [
    # PV-DBOW plain
    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores),
    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes
    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),
    # PV-DM w/ concatenation - big, slow, experimental mode window=5 (both sides) approximates paper's apparent 10-word total window size
    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores),
]

for d2v_model in simple_models:
    d2v_model.build_vocab(alldocs)
    d2v_model.train(alldocs, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)

models_by_name = OrderedDict((str(d2v_model), d2v_model) for d2v_model in simple_models)
models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])
models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])
</code></pre>

<p>Given a document vector V, if I try to infer the most similar documents to the document vector V from a ConcatenatedDocvecs model, I get the following error:</p>

<pre><code>V = np.random.rand(200)
models_by_name['dbow+dmc'].docvecs.most_similar([V])

AttributeError: 'ConcatenatedDocvecs' object has no attribute 'most_similar'
</code></pre>

<p>Of course, I cannot use the simple models to infer similar documents as the produced vector embeddings have size 100 (and not 200 as the concatenated vectors do).</p>

<p>How can I get the list of most similar documents to a document vector from a ConcatenatedDocvecs model?</p>
","7936266","","7936266","","2019-01-14 17:22:07","2019-01-15 00:28:37","Doc2Vec: infer most similar vector from ConcatenatedDocvecs","<python><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"26202978","1","","","2014-10-05 13:27:25","","1","3033","<p>Based on my previous question <a href=""https://stackoverflow.com/questions/26177307/spark-and-python-use-custom-file-format-generator-as-input-for-rdd?noredirect=1#comment41082418_26177307"">Spark and Python use custom file format/generator as input for RDD</a> I think that I should be able to parse basically any input by sc.textFile() and then using my or from some library custom functions.</p>

<p>Now I am particularly trying to parse the wikipedia dump using gensim framework. I have already installed gensim on my master node and all my worker nodes and now I would like to use gensim build in function for parsing wikipedia pages inspired by this question <a href=""https://stackoverflow.com/questions/21096432/list-or-iterator-of-tuples-returned-by-map-pyspark"">List (or iterator) of tuples returned by MAP (PySpark)</a>.</p>

<p>My code is following:</p>

<pre><code>import sys
import gensim
from pyspark import SparkContext


if __name__ == ""__main__"":
    if len(sys.argv) != 2:
        print &gt;&gt; sys.stderr, ""Usage: wordcount &lt;file&gt;""
        exit(-1)

    sc = SparkContext(appName=""Process wiki - distributed RDD"")

    distData = sc.textFile(sys.argv[1])
    #take 10 only to see how the output would look like
    processed_data = distData.flatMap(gensim.corpora.wikicorpus.extract_pages).take(10)

    print processed_data
    sc.stop()
</code></pre>

<p>The source code of extract_pages can be found at <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/wikicorpus.py"" rel=""nofollow noreferrer"">https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/wikicorpus.py</a> and based on my going through it seems that it should work with Spark. </p>

<p>But unfortunately when I run the code I'm getting following error log:</p>

<pre><code>14/10/05 13:21:11 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, &lt;ip address&gt;.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
File ""/root/spark/python/pyspark/worker.py"", line 79, in main
serializer.dump_stream(func(split_index, iterator), outfile)
File ""/root/spark/python/pyspark/serializers.py"", line 196, in dump_stream
self.serializer.dump_stream(self._batched(iterator), stream)
File ""/root/spark/python/pyspark/serializers.py"", line 127, in dump_stream
for obj in iterator:
File ""/root/spark/python/pyspark/serializers.py"", line 185, in _batched
for item in iterator:
File ""/root/spark/python/pyspark/rdd.py"", line 1148, in takeUpToNumLeft
yield next(iterator)
File ""/usr/lib64/python2.6/site-packages/gensim/corpora/wikicorpus.py"", line 190, in extract_pages
elems = (elem for _, elem in iterparse(f, events=(""end"",)))
File ""&lt;string&gt;"", line 52, in __init__
IOError: [Errno 2] No such file or directory: u'&lt;mediawiki xmlns=""http://www.mediawiki.org/xml/export-0.9/"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://www.mediawiki.org/xml/export-0.9/ http://www.mediawiki.org/xml/export-0.9.xsd"" version=""0.9"" xml:lang=""en""&gt;'
    org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:124)
    org.apache.spark.api.python.PythonRDD$$anon$1.&lt;init&gt;(PythonRDD.scala:154)
    org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)
    org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
    org.apache.spark.scheduler.Task.run(Task.scala:54)
    org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>And then some probably Spark log:</p>

<pre><code>14/10/05 13:21:12 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
14/10/05 13:21:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
14/10/05 13:21:12 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
14/10/05 13:21:12 INFO scheduler.DAGScheduler: Failed to run runJob at PythonRDD.scala:296
</code></pre>

<p>and</p>

<pre><code>at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
at scala.Option.foreach(Option.scala:236)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
at akka.actor.ActorCell.invoke(ActorCell.scala:456)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
</code></pre>

<p>I've tried this without Spark successfully, so the problem should be somewhere in combination of Spark and gensim, but I don't much understand the error that I'm getting. I don't see any file reading in the line 190 of gensim wikicorpus.py. </p>

<p><strong>EDIT:</strong></p>

<p>Added some more logs from Spark:</p>

<p><strong>EDIT2:</strong></p>

<p>gensim uses from <code>xml.etree.cElementTree import iterparse</code>, documentation <a href=""https://docs.python.org/2/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse"" rel=""nofollow noreferrer"">here</a>, which might cause the problem. It actually expects file name or file containing the xml data. Can be RDD considered as file containing the xml data?</p>
","1499038","","-1","","2017-05-23 11:44:36","2015-04-06 18:51:43","Spark and Python trying to parse wikipedia using gensim","<python><apache-spark><gensim><wikimedia-dumps>","1","4","","","","CC BY-SA 3.0"
"46201029","1","46208661","","2017-09-13 15:09:18","","2","1360","<p>According to WMD <a href=""http://proceedings.mlr.press/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">paper</a>, it's inspired by word2vec model and use word2vec vector space for moving document 1 towards document 2 (in the context of Earth Mover Distance metric). From the paper:</p>

<pre><code>Assume we are provided with a word2vec embedding matrix
X ‚àà Rd√ón for a finite size vocabulary of n words. The 
ith column, xi ‚àà Rd, represents the embedding of the ith
word in d-dimensional space. We assume text documents
are represented as normalized bag-of-words (nBOW) vectors,
d ‚àà Rn. To be precise, if word i appears ci times in
the document, we denote di = ci/cj (for j=1 to n). An nBOW vector
d is naturally very sparse as most words will not appear in
any given document. (We remove stop words, which are
generally category independent.)
</code></pre>

<p>I understand the concept from the paper, however, I couldn't understand how wmd uses word2vec embedding space from the code in Gensim. </p>

<p><strong><em>Can someone explain it in a simple way? Does it calculate the word vectors in a different way because I couldn't understand where in this code word2vec embedding matrix is used?</em></strong> </p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">WMD Fucntion from Gensim:</a></p>

<pre><code>   def wmdistance(self, document1, document2):
    # Remove out-of-vocabulary words.
    len_pre_oov1 = len(document1)
    len_pre_oov2 = len(document2)
    document1 = [token for token in document1 if token in self]
    document2 = [token for token in document2 if token in self]

    dictionary = Dictionary(documents=[document1, document2])
    vocab_len = len(dictionary)

    # Sets for faster look-up.
    docset1 = set(document1)
    docset2 = set(document2)

    # Compute distance matrix.
    distance_matrix = zeros((vocab_len, vocab_len), dtype=double)
    for i, t1 in dictionary.items():
        for j, t2 in dictionary.items():
            if t1 not in docset1 or t2 not in docset2:
                continue
            # Compute Euclidean distance between word vectors.
            distance_matrix[i, j] = sqrt(np_sum((self[t1] - self[t2])**2))

    def nbow(document):
        d = zeros(vocab_len, dtype=double)
        nbow = dictionary.doc2bow(document)  # Word frequencies.
        doc_len = len(document)
        for idx, freq in nbow:
            d[idx] = freq / float(doc_len)  # Normalized word frequencies.
        return d

    # Compute nBOW representation of documents.
    d1 = nbow(document1)
    d2 = nbow(document2)

    # Compute WMD.
    return emd(d1, d2, distance_matrix)
</code></pre>
","1996842","","1996842","","2017-09-15 09:47:58","2017-09-15 09:47:58","How Word Mover's Distance (WMD) uses word2vec embedding space?","<nlp><nltk><gensim><word2vec><word-embedding>","1","0","1","","","CC BY-SA 3.0"
"54187308","1","","","2019-01-14 18:38:17","","0","142","<p>I installed Gensim in python3</p>

<p>when I call gensim I got this error. Can someone help?</p>

<pre><code>&gt;&gt;&gt; import gensim


AttributeError: 'tuple' object has no attribute 'type'
</code></pre>
","3479460","","3479460","","2019-01-14 21:11:44","2021-01-11 13:03:50","Gensim installation with python3","<python-3.5><gensim>","0","2","","","","CC BY-SA 4.0"
"54929726","1","54931526","","2019-02-28 16:02:09","","2","1599","<p>I'm new to Word2Vec and I am trying to cluster words based on their similarity.  To start I am using nltk to separate the sentences and then using the resulting list of sentences as the input into Word2Vec.  However, when I print the vocab, it is just a bunch of letters, numbers and symbols rather than words. To be specific, an example of one of the letters is ""&lt; gensim.models.keyedvectors.Vocab object at 0x00000238145AB438>, 'L':""</p>

<pre><code># imports needed and logging
import gensim
from gensim.models import word2vec
import logging

import nltk
#nltk.download('punkt')
#nltk.download('averaged_perceptron_tagger')
with open('C:\\Users\\Freddy\\Desktop\\Thesis\\Descriptions.txt','r') as f_open:
    text = f_open.read()
arr = []

sentences = nltk.sent_tokenize(text) # this gives a list of sentences

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)

model = word2vec.Word2Vec(sentences, size = 300)

print(model.wv.vocab)
</code></pre>
","10929013","","6573902","","2019-02-28 17:57:20","2019-02-28 17:57:20","Word2Vec vocab results in just letters and symbols","<python><python-3.x><tokenize><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"61080872","1","61082045","","2020-04-07 13:14:53","","1","80","<p>I'm currently trying to process a large amount of very big (>10k words) text files. In my data pipeline, I identified the gensim tokenize function as my bottleneck, the relevant part is provided in my MWE below:</p>

<pre><code>import re
import urllib.request

url='https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/genesis/english-web.txt'
doc=urllib.request.urlopen(url).read().decode('utf-8')

PAT_ALPHABETIC = re.compile('(((?![\d])\w)+)', re.UNICODE)

def tokenize(text):
    text.strip()
    for match in PAT_ALPHABETIC.finditer(text):
        yield match.group()

def preprocessing(doc):
    tokens = [token for token in tokenize(doc)]
    return tokens

foo=preprocessing(doc)
</code></pre>

<p>Calling the <code>preprocessing</code> function for the given example takes roughly <code>66ms</code> and I would like to improve this number. Is there anything I can still optimize in my code? Or is my hardware (Mid 2010s Consumer Notebook) the issue? I would be interested in the runtimes from people with some more recent hardware as well.</p>

<p>Thank you in advance</p>
","13109287","","","","","2020-04-07 14:13:00","Improve performance of large document text tokenization through Python + RegEx","<regex><python-3.x><nltk><gensim>","1","4","","","","CC BY-SA 4.0"
"54279792","1","","","2019-01-20 12:49:14","","0","50","<p>Recently, I am implementing an algorithm from a paper that I will be using in my master's work, but I've come across some problems regarding the time it is taking to perform some operations.</p>

<p>Before I get into details, I just want to add that my data set comprehends roughly 4kk entries of data points.</p>

<p>I have two lists of tuples that I've get from a framework (<a href=""https://github.com/spotify/annoy"" rel=""nofollow noreferrer"">annoy</a>) that calculates cosine similarity between a vector and every other vector in the dataset. The final format is like this:</p>

<pre><code>[(name1, cosine), (name2, cosine), ...]
</code></pre>

<p>Because of the algorithm, I have two of that lists with the same names (first value of the tuple) in it, but two different cosine similarities. What I have to do is to sum the cosines from both lists, and then order the array and get the top-N highest cosine values.</p>

<p>My issue is: is taking too long. My actual code for this implementation is as following:</p>

<pre><code>def topN(self, user, session):
    upref = self.m2vTN.get_user_preference(user)
    spref = self.sm2vTN.get_user_preference(session)

    # list of tuples 1
    most_su = self.indexer.most_similar(upref, len(self.m2v.wv.vocab))
    # list of tuples 2
    most_ss = self.indexer.most_similar(spref, len(self.m2v.wv.vocab))

    # concat both lists and add into a dict
    d       = defaultdict(int)      
    for l, v in (most_ss + most_su): 
        d[l] += v  


     # convert the dict into a list, and then sort it
    _list    = list(d.items())
    _list.sort(key=lambda x: x[1], reverse=True)

    return [x[0] for x in _list[:self.N]]
</code></pre>

<p>How do I make this code faster? I've tried using threads but I'm not sure if it will make it faster. Getting the lists is not the problem here, but the concatenation and sorting is.</p>

<p>Thanks! English is not my native language, so sorry for any misspelling.</p>
","5124600","Igor Santana","","","","2019-01-22 05:38:56","How do I speedup adding two big vectors of tuples?","<machine-learning><python><gensim>","1","1","1","","","CC BY-SA 4.0"
"54003616","1","54011266","","2019-01-02 09:03:17","","0","489","<p>I use following gensim wrapper to train a word-vector model:</p>

<pre><code>import numpy as np
import pandas as pd
from gensim.sklearn_api import W2VTransformer
from gensim.utils import simple_preprocess

# Load synthetic data
data = pd.read_csv('https://pastebin.com/raw/EPCmabvN')
data = data.head(10)
# Set random seed
np.random.seed(0)

X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
y_train = data.label

model = W2VTransformer(size=10, min_count=1)
model.fit(X_train)

model.wv.vocab
</code></pre>

<p>However, once I try to access the trained model, i.e. <code>model.wv.vocab</code>, it outputs the error:</p>

<blockquote>
  <p>AttributeError: 'W2VTransformer' object has no attribute 'wv'</p>
</blockquote>

<p>Can I somehow access the vocabulary and other model parameters, or is this not possible with the wrapper?</p>

<pre><code>Current workaround: 

from gensim.models.doc2vec import TaggedDocument
from gensim.models.doc2vec import Doc2Vec

#Defining model without wrapper
documents = data.apply(lambda r: TaggedDocument(words=simple_preprocess(r['text'], min_len=2), tags=[r.label]), axis=1)
d2v = Doc2Vec(documents, window=2, vector_size=10, min_count=1, seed=0)
d2v.wv.vocab
</code></pre>
","4697646","","4697646","","2019-01-02 09:52:17","2019-01-02 22:37:36","Accessing model in gensim wrapper","<model><wrapper><gensim>","1","0","","","","CC BY-SA 4.0"
"43772128","1","","","2017-05-04 00:51:45","","5","1063","<p>I have an LDA model trained through Mallet in Java. Three files are generated from the Mallet LDA model, which allow me to run the model from files and infer the topic distribution of a new text.</p>

<p>Now I would like to implement a Python tool which is able to infer a topic distribution given a new text, based on the trained LDA model. I do not want to re-trained the LDA model in Python. Therefore, I wonder if it is possible to load the trained Mallet LDA model into Gensim or any other python LDA package. If so, how can I do it?</p>

<p>Thanks for any answers or comments.</p>
","6079380","","","","","2019-04-02 18:10:44","Use Gensim or other python LDA packages to use trained LDA model from Mallet","<gensim><lda><mallet>","1","0","2","","","CC BY-SA 3.0"
"61185290","1","","","2020-04-13 09:52:56","","4","4206","<p>I have some text data for which I need to do sentiment classification. I don't have positive or negative labels on this data (unlabelled). I want to use the Gensim word2vec model for sentiment classification.<br>
Is it possible to do this? Because till now I couldn't find anything which does that?
Every blog and article are using some kind of labelled dataset (such as imdb dataset)to train and test the word2vec model. No one going further and predicting their own unlabelled data.</p>

<p>Can someone tell me the possibility of this (at least theoretically)?</p>

<p>Thanks in Advance!</p>
","11230191","","","","","2021-02-20 17:42:34","Is it possible to do sentiment analysis of unlabelled text using word2vec model?","<python-3.7><gensim><word2vec><sentiment-analysis>","4","6","2","","","CC BY-SA 4.0"
"37789724","1","","","2016-06-13 12:20:20","","0","368","<p>Aim- User inputs a string. I need to compare this input with Sentence 1 and Sentence 2 and find the maximum similarity with either of these sentences.</p>

<p>Current Approach- I tokenize the input and both sentences, find synonym sets of each token and compare maximum similarity by adding similarity for each token using nltk .path_similarity(token1,token2).</p>

<p>Problem- If sentence 1 is short and sentence 2 is long with many tokens, since I sum up individual similarities, the similarity of sentence 2 with input is always more even if most of tokens of input match with sentence 1.</p>

<p>One solution- I can divide the similarity of each sentence with length of sentence and hence I get similarity per token of Sentence. But this approach is too aggressive. Is there an industry standard approach for this?</p>
","3049870","","","","","2016-06-13 12:20:20","algorithm for sentence matching after calculating word similarity using nltk","<machine-learning><nlp><nltk><semantics><gensim>","0","2","","","","CC BY-SA 3.0"
"46998858","1","","","2017-10-29 09:56:08","","1","10290","<p>i am trying to install gensim using </p>

<pre><code>sudo -H pip install --upgrade gensim
</code></pre>

<p>but it is giving me this error :</p>

<pre><code>  File ""setup.py"", line 301, in &lt;module&gt;
    include_package_data=True,
  File ""/usr/lib/python2.7/distutils/core.py"", line 151, in setup
    dist.run_commands()
  File ""/usr/lib/python2.7/distutils/dist.py"", line 953, in run_commands
    self.run_command(cmd)
  File ""/usr/lib/python2.7/distutils/dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""/usr/local/lib/python2.7/dist-packages/setuptools/command                      /install.py"", line 67, in run
    self.do_egg_install()
  File ""/usr/local/lib/python2.7/dist-packages/setuptools/command /install.py"", line 98, in do_egg_install
    easy_install = self.distribution.get_command_class('easy_install')
  File ""/usr/local/lib/python2.7/dist-packages/setuptools/dist.py"", line 576, in get_command_class
    self.cmdclass[command] = cmdclass = ep.load()
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2229, in load
    return self.resolve()
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2235, in resolve
module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File ""/usr/local/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 47, in &lt;module&gt;
from setuptools.sandbox import run_setup
  File ""/usr/local/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 15, in &lt;module&gt;
    import pkg_resources.py31compat
ImportError: No module named py31compat
</code></pre>

<p>please help me, in installing gensim. i googled it, but i am not able to find the solution.</p>
","3270950","","","","","2020-01-21 11:01:35","ImportError: No module named py31compat","<python><python-2.7><importerror><gensim>","4","2","","","","CC BY-SA 3.0"
"45860212","1","","","2017-08-24 11:16:59","","1","147","<p>I have pre trained continuous bag of words word2vec model.How do I load and which type of tasks could I perform to evaluate it? </p>
","4692915","","","","","2017-08-24 11:16:59","How to load and evaluate pretrained word2vec model trained on CBOW algorithm","<python><gensim><word2vec>","0","0","","","","CC BY-SA 3.0"
"53971240","1","","","2018-12-29 16:22:00","","3","4075","<p>I have a pre-trained word embedding with vectors of different norms, and I want to normalize all vectors in the model. I am doing it with a for loop that iterates each word and normalizes its vector, but the model us huge and takes too much time. Does <code>gensim</code> include any way to do this faster? I cannot find it.</p>
<p>Thanks!!</p>
","9652994","","6573902","","2021-02-02 07:16:33","2021-02-02 07:16:33","Normalize vectors in gensim model","<python><nlp><gensim><word-embedding>","1","0","2","","","CC BY-SA 4.0"
"62222500","1","","","2020-06-05 19:04:11","","1","26","<p>I have run some gensim code to create an LDA model from a set of documents.  From there it has created a list of 10 topics.  Is it possible to then go back to all of those documents, and assign them to one of those 10 topics (or none if applicable)?</p>

<p>I have not been able to find any code samples to accomplish this online.</p>

<p>Thank you</p>
","939365","","","","","2020-06-05 19:04:11","After creating an LDA model from documents, how do I assign topics?","<python><machine-learning><nlp><gensim><lda>","0","0","1","","","CC BY-SA 4.0"
"54196106","1","","","2019-01-15 09:35:24","","1","623","<p>Why when I call Numpy, Scipy, Gensim with python3 in linux I have the following error?</p>

<pre><code>&gt;import gensim
_concrete_types = {v.type for k, v in _concrete_typeinfo.items()}
AttributeError: 'tuple' object has no attribute 'type'
</code></pre>
","10916130","","10916130","","2019-01-15 11:49:44","2019-01-17 01:27:20","Error with calling Numpy, Scipy, Gensim in python3","<python><python-3.x><numpy><scipy><gensim>","1","2","","","","CC BY-SA 4.0"
"62910645","1","","","2020-07-15 08:27:31","","0","255","<p>Need assistance regarding the Principal components to be displayed in pyLDAvis. It shows PC1 and PC2 by default, however I am interested in exploring other components as well.</p>
<p>I referred this <a href=""https://pyldavis.readthedocs.io/en/latest/modules/API.html"" rel=""nofollow noreferrer"">documentation</a> but the specified option <code>plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}</code> only changes plot labels.</p>
<p>I am using pyLDAvis through gensim:</p>
<pre><code>pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
</code></pre>
","4474531","","4474531","","2020-07-17 15:14:38","2020-08-10 19:40:30","pyLDAvis arguments for specifying principal components","<python-3.x><gensim><lda><topic-modeling><pyldavis>","1","0","","","","CC BY-SA 4.0"
"46786311","1","","","2017-10-17 08:59:39","","1","5659","<p>Does anyone know which function should I use if I want to use the pre-trained doc2vec models in this website <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">https://github.com/jhlau/doc2vec</a>?</p>

<p>I know we can use the <code>Keyvectors.load_word2vec_format()</code>to laod the word vectors from pre-trained word2vec models, but do we have a similar function to load pre-trained doc2vec models as well in gensim?</p>

<p>Thanks a lot.</p>
","8778369","","","","","2019-02-09 04:39:17","How to load the pre-trained doc2vec model and use it's vectors","<python><numpy><gensim><doc2vec>","2","0","","","","CC BY-SA 3.0"
"37763883","1","38151941","","2016-06-11 12:45:03","","5","4559","<p>Is there a way to get the document vectors of unseen and seen documents from Doc2Vec in the gensim 0.11.1 version? </p>

<ul>
<li><p>For example, suppose I trained the model on 1000 thousand - Can I get
the doc vector for those 1000 docs?     </p></li>
<li><p>Is there a way to get document vectors of unseen documents composed<br>
from the same vocabulary?</p></li>
</ul>
","3667569","","","","","2016-07-01 18:32:48","How to get the Document Vector from Doc2Vec in gensim 0.11.1?","<python><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 3.0"
"64563605","1","","","2020-10-27 22:10:51","","0","26","<p>I am using the functionality from the following script of the <code>gensim</code> package: <a href=""https://github.com/RaRe-Technologies/gensim/blob/3.8.0/gensim/summarization/keywords.py"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/3.8.0/gensim/summarization/keywords.py</a></p>
<p>How can I change the global variables <code>WINDOW_SIZE</code>, etc. before invoking the function <code>keywords()</code></p>
","3742823","","","","","2020-10-27 22:25:39","Change global variable of a package","<python><python-3.x><gensim>","2","0","","","","CC BY-SA 4.0"
"36013137","1","36034850","","2016-03-15 13:47:16","","0","334","<p>My objective is to find a vector representation of phrases. Below is the code I have, that works partially for bigrams using the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">Word2Vec</a> model provided by the <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow"">GenSim</a> library.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import word2vec

def bigram2vec(unigrams, bigram_to_search):
    bigrams = Phrases(unigrams)
    model = word2vec.Word2Vec(sentences=bigrams[unigrams], size=20, min_count=1, window=4, sg=1, hs=1, negative=0, trim_rule=None)
    if bigram_to_search in model.vocab.keys():
        return model[bigram_to_search]
    else:
        return None
</code></pre>

<p>The problem is that the Word2Vec model is seemingly doing automatic pruning of some of the bigrams, i.e. <code>len(model.vocab.keys()) != len(bigrams.vocab.keys())</code>. I've tried adjusting various parameters such as <code>trim_rule</code>, <code>min_count</code>, but they don't seem to affect the pruning.</p>

<p>PS - I am aware that bigrams to look up need to be represented using underscore instead of space, i.e. proper way to call my function would be <code>bigram2vec(unigrams, 'this_report')</code></p>
","863923","","","","","2016-03-16 11:49:08","GenSim Word2Vec unexpectedly pruning","<gensim><word2vec>","1","0","0","","","CC BY-SA 3.0"
"36013211","1","","","2016-03-15 13:50:36","","1","3221","<p>I read the Kaggle‚Äôs word2vec example from <a href=""https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors"" rel=""nofollow"">https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors</a>
and I can‚Äôt understand how come the model‚Äôs vocabulary length is different from the word vector length.</p>

<p>Doesn‚Äôt every cell at a word vector represent the relation with other word from the vocabulary, so each word has a relation to each other word?
If not, so what does each cell at the word vector represent?</p>

<p>Really appreciate any help.</p>
","4381411","","","","","2016-03-18 04:49:48","Why does word2vec vocabulary length is different from the word vector length","<machine-learning><text-classification><gensim><word2vec>","3","0","","","","CC BY-SA 3.0"
"46065514","1","","","2017-09-06 01:35:34","","1","722","<p>I am trying to get most probable sequence of word using gensim word2vec model. I have found a pretrained model which provides these files:</p>

<pre><code>word2vec.bin
word2vec.bin.syn0.npy
word2vec.bin.syn1neg.npy
</code></pre>

<p>This is my code trying to get the probability of the sentence with this model:</p>

<pre><code>model = model.wv.load(word_embedding_model_path)
model.hs = 1
model.negative = 0
print model.score(sentence.split("" ""))
</code></pre>

<p>While running this code I am getting this error:</p>

<pre><code>AttributeError: 'Word2Vec' object has no attribute 'syn1'
</code></pre>

<p>Can anyone help me figure out how to solve the problem. In general, I want to use some pretrained model to get the probability of sequence of word appearing together.</p>
","2462485","","","","","2017-09-06 17:42:38","Getting probability of the text given word embedding model in gensim word2vec model","<python><nlp><gensim><word2vec><language-model>","1","0","","","","CC BY-SA 3.0"
"62227888","1","","","2020-06-06 05:48:00","","0","50","<p>Can someone help me with the flow process using NLP and python? I'm totally new to this NLP. </p>
","13650267","","","","","2020-06-06 05:48:00","How to get similarity % from 2 resumes of which one resume is the best for x job role and other fresher resume needs to compare with the best one?","<python-3.x><nlp><nltk><gensim>","0","3","","","","CC BY-SA 4.0"
"54202117","1","","","2019-01-15 15:38:25","","1","1581","<p>I have two versions of python: python2, python3. </p>

<p>When I install gensim it goes to python2
 I install it with the following comand</p>

<pre><code>sudo pip3 install --upgrade gensim
</code></pre>

<p>how can I install it in python3?</p>
","10916130","","8565438","","2020-05-07 03:08:30","2020-05-07 03:08:30","Install Gensim in python3","<python><gensim>","2","4","","","","CC BY-SA 4.0"
"37818426","1","","","2016-06-14 17:22:31","","23","30351","<p>Using the <code>gensim.models.Word2Vec</code> library, you have the possibility to provide a model and a ""word"" for which you want to find the list of most similar words:</p>

<pre><code>model = gensim.models.Word2Vec.load_word2vec_format(model_file, binary=True)
model.most_similar(positive=[WORD], topn=N)
</code></pre>

<p>I wonder if there is a possibility to give the system as input the model and a ""vector"", and ask the system to return the top similar words (which their vectors is very close to the given vector). Something similar to:</p>

<pre><code>model.most_similar(positive=[VECTOR], topn=N)
</code></pre>

<p>I need this functionality for a bilingual setting, in which I have 2 models (English and German), as well as some English words for which I need to find their most similar German candidates.
What I want to do is to get the vector of each English word from the English model:</p>

<pre><code>model_EN = gensim.models.Word2Vec.load_word2vec_format(model_file_EN, binary=True)
vector_w_en=model_EN[WORD_EN]
</code></pre>

<p>and then query the German model with these vectors.</p>

<pre><code>model_DE = gensim.models.Word2Vec.load_word2vec_format(model_file_DE, binary=True)
model_DE.most_similar(positive=[vector_w_en], topn=N)
</code></pre>

<p>I have implemented this in C using the original distance function in the word2vec package. But, now I need it to be in python, in order to be able to integrate it with my other scripts.</p>

<p>Do you know if there is already a method in <code>gensim.models.Word2Vec</code> library or other similar libraries which does this? Do I need to implement it by myself?</p>
","2530859","","880772","","2016-12-17 05:20:11","2018-12-19 15:58:57","Get most similar words, given the vector of the word (not the word itself)","<python><gensim><word2vec>","2","4","9","","","CC BY-SA 3.0"
"46559980","1","46574804","","2017-10-04 08:09:53","","2","202","<p>I am trying to get the doc2vec function to work in python 3.
I Have the following code:</p>

<pre><code>tekstdata = [[ index, str(row[""StatementOfTargetFiguresAndPoliciesForTheUnderrepresentedGender""])] for index, row in data.iterrows()]
def prep (x):
    low = x.lower()
    return word_tokenize(low)

def cleanMuch(data, clean):
    output = []
    for x, y in data:
        z = clean(y)
        output.append([str(x), z])
    return output

tekstdata = cleanMuch(tekstdata, prep)

def tagdocs(docs):
    output = []    
    for x,y in docs:
        output.append(gensim.models.doc2vec.TaggedDocument(y, x))
    return output
    tekstdata = tagdocs(tekstdata)

    print(tekstdata[100])

vectorModel = gensim.models.doc2vec.Doc2Vec(tekstdata, size = 100, window = 4,min_count = 3, iter = 2)


ranks = []
second_ranks = []
for x, y in tekstdata:
 print (x)
 print (y)
 inferred_vector = vectorModel.infer_vector(y)
 sims = vectorModel.docvecs.most_similar([inferred_vector], topn=1001,   restrict_vocab = None)
rank = [docid for docid, sim in sims].index(y)
ranks.append(rank)
</code></pre>

<p>All works as far as I can understand until the rank function. 
The error I get is that there is no zero in my list e.g. the documents I am putting in does not have 10 in list:</p>

<pre><code>  File ""C:/Users/Niels Hels√∏/Documents/github/Speciale/Test/Data prep.py"", line 59, in &lt;module&gt;
rank = [docid for docid, sim in sims].index(y)

ValueError: '10' is not in list
</code></pre>

<p>It seems to me that it is the similar function that does not work. 
the model trains on my data (1000 documents) and build a vocab which is tagged.
The documentation I mainly have used is this: 
<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Gensim dokumentation</a>
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">Torturial</a></p>

<p>I hope that some one can help. If any additional info is need please let me know. 
best
Niels</p>
","8718589","","75103","","2017-10-04 08:17:03","2017-10-04 21:56:56","applying the Similar function in Gensim.Doc2Vec","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"55343781","1","","","2019-03-25 17:49:12","","0","1129","<p>I have a text file with million of rows which I wanted to convert into word vectors and later on I can compare these vectors with a search keyword and see which all texts are closer to the search keyword.</p>

<p>My Dilemma is all the training files that I have seen for the Word2vec are in the form of paragraphs so that each word has some contextual meaning within that file. Now my file here is independent and contains different keywords in each row.</p>

<p>My question is whether is it possible to create word embedding using this text file or not, if not then what's the best approach for searching a matching search keyword in this million of texts</p>

<p>**My File Structure: **</p>

<pre><code>Walmart
Home Depot
Home Depot
Sears
Walmart
Sams Club
GreenMile
Walgreen
</code></pre>

<p><strong>Expected</strong></p>

<pre><code>search Text : 'WAL'
</code></pre>

<p><strong>Result from My File:</strong></p>

<pre><code>WALGREEN
WALMART
WALMART
</code></pre>
","3704368","","3704368","","2019-03-26 07:47:49","2019-03-26 09:15:17","Convert list of words in Text file to Word Vectors","<python><machine-learning><nlp><gensim><word2vec>","2","5","","","","CC BY-SA 4.0"
"55812580","1","55819469","","2019-04-23 13:38:57","","3","602","<p>I am running Gensim with Python 3.6 on Windows 10. I have tried installing Visual Studio 2019 and MinGW (through TDM-GCC). I have uninstalled and reinstalled Gensim after both installations. I also did that after uninstalling and reinstalling Cython. </p>

<p>Regardless, it's not able to run the C extension, so I am stuck with the slower Numpy code.</p>

<p>I am not sure where the trouble lies, and I have run out of ideas on how to proceed. What can I do to make progress?</p>
","7189173","","","","","2019-05-06 12:04:23","Making Gensim FAST_VERSION work on Windows 10 (Python 3.6)","<python-3.x><gensim>","2","0","","","","CC BY-SA 4.0"
"29083865","1","","","2015-03-16 18:03:10","","0","635","<p>My ultimate goal is to produce a *.csv file containing <strong>labeled</strong> binary term vectors for each document. In essence, a term document matrix. </p>

<p>Using gensim, I can produce a file with an unlabeled term matrix.</p>

<p>I do this by essentially copying and pasting code from here: <a href=""http://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">http://radimrehurek.com/gensim/tut1.html</a></p>

<p>Given a list of documents called ""texts"".</p>

<pre><code>corpus = [dictionary.doc2bow(text) for text in texts]
print(corpus)
[(0, 1), (1, 1), (2, 1)]
[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]
[(2, 1), (5, 1), (7, 1), (8, 1)]
[(1, 1), (5, 2), (8, 1)]
[(3, 1), (6, 1), (7, 1)]
[(9, 1)]
[(9, 1), (10, 1)]
[(9, 1), (10, 1), (11, 1)]
[(4, 1), (10, 1), (11, 1)]
</code></pre>

<p>To convert the above vectors into a numpy matrix, I use:</p>

<pre><code>scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)
</code></pre>

<p>I then convert the sparse numpy matrix to a full array:</p>

<pre><code>full_matrix = csc_matrix(scipy_csc_matrix).toarray()
</code></pre>

<p>Finally, I output this to a file:</p>

<pre><code>with open('file.csv','wb') as f:
    writer = csv.writer(f)
    writer.writerows(full_matrix)
</code></pre>

<p>This produces a matrix of binomial vectors, but I do not know which vector represents which word. Is there an accurate way of matching words to vectors?</p>

<p>I've tried parsing the dictionary to creative a list of words which I would glue to the above full_matrix. </p>

<pre><code>#Retrive dictionary
tokenIDs = dictionary.token2id

#Retrieve keys from dictionary and concotanate those to full_matrix
for key, value in tokenIDs.iteritems():
    temp1 = unicodedata.normalize('NFKD', key).encode('ascii','ignore')
    temp = [temp1]
    dictlist.append(temp)

Keys = np.asarray(dictlist)

#Combine Keys and Matrix
labeled_full_matrix = np.concatenate((Keys, full_matrix), axis=1)
</code></pre>

<p>However, this does not work. The word ids (Keys) are not matched to the appropriate vectors. </p>

<p>I am under the assumption a much simpler and more elegant approach is possible. But after some time, I haven't been able to find it. Maybe someone here can help, or point me to something fundamental I've missed.</p>
","2167391","","","","","2015-06-09 01:06:23","Word Labels for Document Matrix in Gensim","<python-2.7><numpy><gensim>","1","0","1","","","CC BY-SA 3.0"
"55018426","1","55047853","","2019-03-06 08:17:24","","1","759","<p>Recently, I trained a FastText word embedding from <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">sentiment140</a> to get the representation for English words. However, today just for a trial, I run the FastText module on a couple of Chinese words, for instance:</p>

<pre><code>import gensim.models as gs

path = r'\data\word2vec'

w2v = gs.FastText.load(os.path.join(path, 'fasttext_model'))

w2v.wv['ÂìàÂìàÂìàÂìà']
</code></pre>

<p>It outputs:</p>

<pre><code>array([ 0.00303676,  0.02088235, -0.00815559,  0.00484574, -0.03576371,
       -0.02178247, -0.05090654,  0.03063928, -0.05999983,  0.04547168,
       -0.01778449, -0.02716631, -0.03326027, -0.00078981,  0.0168153 ,
        0.00773436,  0.01966593, -0.00756055,  0.02175765, -0.0050137 ,
        0.00241255, -0.03810823, -0.03386266,  0.01231019, -0.00621936,
       -0.00252419,  0.02280569,  0.00992453,  0.02770403,  0.00233192,
        0.0008545 , -0.01462698,  0.00454278,  0.0381292 , -0.02945416,
       -0.00305543, -0.00690968,  0.00144188,  0.00424266,  0.00391074,
        0.01969502,  0.02517333,  0.00875261,  0.02937791,  0.03234404,
       -0.01116276, -0.00362578,  0.00483239, -0.02257918,  0.00123061,
        0.00324584,  0.00432153,  0.01332884,  0.03186348, -0.04119627,
        0.01329033,  0.01382102, -0.01637722,  0.01464139,  0.02203292,
        0.0312229 ,  0.00636201, -0.00044287, -0.00489291,  0.0210293 ,
       -0.00379244, -0.01577058,  0.02185207,  0.02576622, -0.0054543 ,
       -0.03115215, -0.00337738, -0.01589811, -0.01608399, -0.0141606 ,
        0.0508234 ,  0.00775024,  0.00352813,  0.00573649, -0.02131752,
        0.01166397,  0.00940598,  0.04075769, -0.04704212,  0.0101376 ,
        0.01208556,  0.00402935,  0.0093914 ,  0.00136144,  0.03284211,
        0.01000613, -0.00563702,  0.00847146,  0.03236216, -0.01626745,
        0.04095127,  0.02858841,  0.0248084 ,  0.00455458,  0.01467448],
      dtype=float32)
</code></pre>

<p>Hence, I really want to know why the FastText module trained from <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">sentiment140</a> could do this. Thank you!</p>
","8079220","","","","","2019-03-07 15:57:35","Why the FastText word embedding could generate the representation of a word from another language?","<python><gensim><word-embedding><fasttext><natural-language-processing>","1","6","","","","CC BY-SA 4.0"
"55341145","1","","","2019-03-25 15:23:22","","0","350","<p>A <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer""><code>gensim.models.Word2Vec</code></a> class has method <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.predict_output_word"" rel=""nofollow noreferrer""><code>predict_output_word()</code></a>. Now I use prelearned model but it was saved in class <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer""><code>gensim.models.KeyedVectors</code></a>. Have a the class analogue method? Or how can I get instance of <code>gensim.models.Word2Vec</code> from gensim.models.KeyedVectors`?</p>

<p>I know about <code>most_similar()</code> but it something another.</p>
","7296020","","","","","2020-03-27 20:44:56","How to predict output word for KeyedVectors word2vec?","<python><python-3.x><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"29561063","1","","","2015-04-10 12:09:07","","0","209","<p>I'm using LDA to categorize small documents, about 4-5 lines. </p>

<p>I'm categorizing them into topics such as Technology, Politics, Art, Music etc etc</p>

<p>I'm using wikipedia to download articles in each category (Technology, Politics, Art etc etc) and training LDA for each category</p>

<p>Wikipedia is huge (about 8GB compressed), and computations take hours! and uses a huge space in my hard drive</p>

<p>Is there any toolkit that already provides ""ready-made"" generic topics which i can directly use for categorization? </p>
","4726262","","","","","2015-04-10 20:40:38","ready-made Topics in using LDA to categorize documents?","<python><nlp><text-processing><gensim>","1","0","1","2015-04-26 22:51:36","","CC BY-SA 3.0"
"64718989","1","","","2020-11-06 17:19:21","","0","21","<p>I trained my model using Gensim LDA. Training went okay but the evaluation of model did not go as expected. When I try to evaluate the model using test file in a folder, it outputs the following</p>
<p>&lt;gensim.interfaces.TransformedCorpus object at 0x12bba2b50&gt;</p>
<p>What am I doing wrong? Below is my python code:</p>
<pre><code>my_path = &quot;/path/to/files/training folder&quot;
files = os.listdir(my_path)
doc_set = []


for file in files:
    newpath = (os.path.join(my_path, file)) 
    newpath1 = textract.process(newpath)
    newpath2 = newpath1.decode(&quot;utf-8&quot;)
    doc_set.append(newpath2)

texts = []
for i in doc_set:
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)
    stopped_tokens = [i for i in tokens if not i in stopwords.words()]
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
texts.append(stemmed_tokens)

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]


ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, random_state=1, id2word = dictionary, passes=60)
print(ldamodel.print_topics(num_topics=4, num_words=25))

#Model evaluation

my_path1 = &quot;/Path/to/file/testing folder&quot;
files1 = os.listdir(my_path1)
doc_set1 = []

for file in files1:
    newpathone = (os.path.join(my_path, file)) 
    newpathtwo = textract.process(newpath)
    newpaththree = newpathtwo.decode(&quot;utf-8&quot;)
    doc_set1.append(newpaththree)

texts1 = []
for i in doc_set1:
    raw1 = i.lower()
    tokens1 = tokenizer.tokenize(raw1)
    stopped_tokens1 = [i for i in tokens1 if not i in stopwords.words()]
    stemmed_tokens1 = [p_stemmer.stem(i) for i in stopped_tokens1]
    texts1.append(stemmed_tokens1)


dictionary1 = corpora.Dictionary(texts1)
corpus1 = [dictionary1.doc2bow(text) for text in texts1]
print(ldamodel.get_document_topics(corpus1))
</code></pre>
","4936133","","","","","2020-11-06 17:19:21","How do I evaluate LDA gensim model using a set of test documents","<python><gensim><lda>","0","6","","","","CC BY-SA 4.0"
"37935785","1","","","2016-06-21 04:47:58","","1","266","<p>I am reading a paper about doc2vec. 
But I don't really get what is paragraph id and how it is trained...</p>

<p>I have tried to implement a sentiment analysis task with gensim package and succeeded, without knowing how exactly it works...</p>

<p>The paper said the Document vector is trained just like another word. But how is it processed? Is it trained at the same time with word2vec training? and how can it contain the message of the paragraph if it is treated as a word? 
And what is sentence label and one most confusing me is <strong>the matrix D</strong>...</p>

<p>Is there anybody can explain the process to me?
I got totally messed up... please help me...thx</p>
","3509168","","3509168","","2016-06-21 04:53:29","2016-06-21 04:53:29","what is document vector, paraghaph id in Doc2Vec","<python><gensim><doc2vec>","0","0","","","","CC BY-SA 3.0"
"55169721","1","55174632","","2019-03-14 18:30:43","","0","138","<p>is there a way to infer multiple documents at the same time to preserve the random state of the model using Gensim Doc2Vec?</p>

<p>The function infer_vector is defined as</p>

<pre><code>infer_vector(doc_words, alpha=None, min_alpha=None, epochs=None, steps=None)¬∂
</code></pre>

<p>where doc_words (list of str) ‚Äì A document for which the vector representation will be inferred. And I could not find any opther option to infer multiple documents at the same time.</p>
","9966339","","","","","2019-03-15 02:15:17","Can I preserve the random state of a doc2vec mode for each document I want to infer by infering all documents at the same time?","<gensim><word2vec><doc2vec>","1","1","","","","CC BY-SA 4.0"
"56128701","1","57237272","","2019-05-14 10:47:33","","1","1043","<p>I have a ndarray with words and their corresponding vector (with the size of 100 per word).
For example:</p>

<pre><code>Computer 0.11 0.41 ... 0.56
Ball     0.31 0.87 ... 0.32
</code></pre>

<p>And so on.</p>

<p>I want to create a word2vec model from it:</p>

<pre><code>model = load_from_ndarray(arr)
</code></pre>

<p>How can it be done? I saw </p>

<blockquote>
  <p>KeyedVectors</p>
</blockquote>

<p>but it only takes file and not array</p>
","6057371","","","","","2019-07-28 01:22:04","Python gensim create word2vec model from vectors (in ndarray)","<python><nlp><gensim><word2vec>","2","2","","","","CC BY-SA 4.0"
"29591581","1","29639149","","2015-04-12 16:14:54","","4","3358","<p>I am loading pre-trained vectors from a binary file generated from the word2vec C code with something like:</p>

<pre><code>model_1 = Word2Vec.load_word2vec_format('vectors.bin', binary=True)
</code></pre>

<p>I am using those vectors to generate vector representations of sentences that contain words that may not have already existing vectors in <code>vectors.bin</code>. For example, if <code>vectors.bin</code> has no associated vector for the word ""yogurt"", and I try</p>

<pre><code>yogurt_vector = model_1['yogurt']
</code></pre>

<p>I get <code>KeyError: 'yogurt'</code>, which makes good sense. What I want is to be able to take the sentence words that do not have corresponding vectors and add representations for them to <code>model_1</code>. I am aware from <a href=""https://stackoverflow.com/questions/22121028/update-gensim-word2vec-model"">this post</a> that you cannot continue to train the C vectors. Is there then a way to train a new model, say <code>model_2</code>, for the words with no vectors and merge <code>model_2</code> with <code>model_1</code>?</p>

<p>Alternatively, is there a way to test if the model contains a word before I actually try to retrieve it, so that I can at least avoid the KeyError?</p>
","3711266","","-1","","2017-05-23 12:17:44","2021-03-19 05:19:59","Gensim word2vec augment or merge pre-trained vectors","<python><gensim><keyerror><word2vec>","4","0","1","","","CC BY-SA 3.0"
"64377890","1","64379454","","2020-10-15 18:43:57","","1","67","<p>Is there a pretrained <code>Gensim</code>'s <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">Phrases</a> model? If not, would it be possible to reverse engineer and create a phrase model using a pretrained word embedding?</p>
<p>I am trying to use <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">GoogleNews-vectors-negative300.bin</a> with Gensim's <code>Word2Vec</code>. First, I need to map my words into phrases so that I can look up their vectors from the Google's pretrained embedding.</p>
<p>I search on the official Gensim's documentation but could not find any info. Thanks!</p>
","3128336","","3128336","","2020-10-16 15:22:27","2020-10-16 15:22:27","Is there a pretrained Gensim phrase model?","<python><machine-learning><gensim><word-embedding><phrase>","1","0","","","","CC BY-SA 4.0"
"47037276","1","47059291","","2017-10-31 14:01:09","","0","1146","<p>I wants to optimize gensim to run doc2vec in Window7</p>
<p>[1] C compiler</p>
<p>I installed gensim by following this instruction: <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/install.html</a></p>
<pre><code>pip install --upgrade gensim
</code></pre>
<p>However, in this page(<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a>), it is saying that C compiler is needed before installing gensim.</p>
<blockquote>
<p>Make sure you have a C compiler before installing gensim, to use optimized (compiled) doc2vec training (70x speedup [blog]).</p>
</blockquote>
<ol>
<li>Should I do something before using pip?</li>
</ol>
<p>[2] BLAS</p>
<p>In the tutorial, <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a> it is saying that</p>
<blockquote>
<p>Time to Train</p>
<p>If the BLAS library is being used, this should take no more than 3 seconds. If the BLAS library is not being used, this should take no more than 2 minutes, so use BLAS if you value your time.</p>
</blockquote>
<p>So it seems like I have to install BLAS for optimization,
but I have no idea what BLAS is and there are little and complex BLAS installation guides for window.</p>
<ol start=""2"">
<li>Which BLAS library should I install for running gensim in Window?</li>
<li>If I install BLAS library, will it be automatically linked to python code when I am running gensim doc2vec? or should I do something to link it to doc2vec code?</li>
</ol>
","6259476","","-1","","2020-06-20 09:12:55","2017-11-01 16:21:32","Optimizing gensim(C compilier and BLAS) in Window 7","<python-2.7><word2vec><gensim><blas><doc2vec>","1","0","","","","CC BY-SA 3.0"
"56130065","1","56226404","","2019-05-14 12:06:29","","2","741","<p>I‚Äôm working on a sentence similarity algorithm with the following use case: given a new sentence, I want to retrieve its n most similar sentences from a given set. I am using Gensim v.3.7.1, and I have trained both word2vec and doc2vec models. The results of the latter outperform word2vec‚Äôs, but I‚Äôm having trouble performing efficient queries with my Doc2Vec model. This model uses the distributed bag of words implementation (dm = 0).</p>

<p>I used to infer similarity using the built in method <code>model.most_similar()</code>, but this was not possible once I started training with more data that the one I want to query against. That's to say, <strong>I want to find the most similar sentence among a subset of my training dataset</strong>. My quick fix to this was comparing the vector of the new sentence with every vector on my set using cosine similarity, but obviously this does not scale as I have to compute loads of embeddings and make a lot of comparisons.</p>

<p>I successfully use <a href=""https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.WmdSimilarity"" rel=""nofollow noreferrer"">word-mover distance</a> for both of word2vec and doc2vec, but I get better results for doc2vec when using cosine similarity. How can I efficiently query a new document against my set using the PV-DBOW Doc2Vec model and a method from <a href=""https://radimrehurek.com/gensim/similarities/docsim.html#how-it-works"" rel=""nofollow noreferrer"">class Similarity</a>?</p>

<p>I'm looking for a similar approach to what I do with WMD, but for doc2vec cosine similarity:</p>

<pre class=""lang-py prettyprint-override""><code># set_to_query contains ~10% of the training data + some future updates
set_to_query_tokenized = [sentence.split() for sentence in set_to_query]
w2v_model = gensim.models.Word2Vec.load(""my_w2v_model"")
w2v_to_query = gensim.similarities.WmdSimilarity(
               corpus = set_to_query_tokenized,
               w2v_model = w2v_model,
               num_best=10
              )
new_query = ""I want to find the most similar sentence to this one"".split()
most_similar = w2v_to_query[new_query]
</code></pre>
","3170125","","3170125","","2019-05-15 10:33:15","2019-05-20 18:34:31","How to perform efficient queries with Gensim doc2vec?","<python><gensim><similarity><doc2vec><sentence-similarity>","1","4","0","","","CC BY-SA 4.0"
"56243568","1","","","2019-05-21 17:28:54","","1","7474","<p>My goal is to import gensim in Python 3 on Windows.</p>

<p>I am using Python 3.7.2 (checked by running <code>python -V</code> in Windows command prompt). I installed gensim by running <code>pip install gensim</code>. I checked the installation by running <code>pip freeze</code>, and saw the line <code>gensim==3.7.3</code>.</p>

<p>Then, I ran the command <code>py</code> to enter the interactive python mode (still in Windows command prompt). I ran the line <code>import gensim</code> and got the following output:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>I also tried <code>from gensim import test</code> and got the following output:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>Any suggestions? How do I install gensim on Windows with Python 3? How do I test gensim?</p>
","3851085","","","","","2020-09-21 16:47:50","ModuleNotFoundError: No module named 'gensim'","<python><python-3.x><pip><python-import><gensim>","6","4","1","","","CC BY-SA 4.0"
"38852074","1","38852397","","2016-08-09 13:23:48","","0","207","<p>Im pretty sure im using yield improperly:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-

import logging
from gensim import corpora, models, similarities
from collections import defaultdict
from pprint import pprint  # pretty-printer
from six import iteritems
import openpyxl
import string
from operator import itemgetter

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

#Creating a stoplist from file
with open('stop-word-list.txt') as f:
    stoplist = [x.strip('\n') for x in f.readlines()]

corpusFileName = 'content_sample_en.xlsx'
corpusSheetName = 'content_sample_en'

class MyCorpus(object):
    def __iter__(self):
        wb = openpyxl.load_workbook(corpusFileName)
        sheet = wb.get_sheet_by_name(corpusSheetName)
        for i in range(1, (sheet.max_row+1)/2):
            title = str(sheet.cell(row = i, column = 4).value.encode('utf-8'))
            summary = str(sheet.cell(row = i, column = 5).value.encode('utf-8'))
            content = str(sheet.cell(row = i, column = 10).value.encode('utf-8'))
            yield reBuildDoc(""{} {} {}"".format(title, summary, content))


def removeUnwantedPunctuations(doc):
    ""change all (/, \, &lt;, &gt;) into ' ' ""
    newDoc = """"
    for l in doc:
        if  l == ""&lt;"" or l == ""&gt;"" or l == ""/"" or l == ""\\"":
            newDoc += "" ""
        else:
            newDoc += l
    return newDoc

def reBuildDoc(doc):
    """"""
    :param doc:
    :return: document after being dissected to our needs.
    """"""
    doc = removeUnwantedPunctuations(doc).lower().translate(None, string.punctuation)
    newDoc = [word for word in doc.split() if word not in stoplist]
    return newDoc

corpus = MyCorpus()

tfidf = models.TfidfModel(corpus, normalize=True)
</code></pre>

<p>In the following example you can see me trying to create a corpus from an xlsx file. Im reading from the xlsx file 3 lines which are title summary and content and appending them into a big string. my <code>reBuildDoc()</code> and <code>removeUnwantedPunctuations()</code> functions then adjust the text to my needs and in the end returns a big list of words. (for ex: <code>[hello, piano, computer, etc... ]</code>) in the end I yield the result but I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Eran/PycharmProjects/tfidf/docproc.py"", line 101, in &lt;module&gt;
    tfidf = models.TfidfModel(corpus, normalize=True)
  File ""C:\Anaconda2\lib\site-packages\gensim-0.13.1-py2.7-win-amd64.egg\gensim\models\tfidfmodel.py"", line 96, in __init__
    self.initialize(corpus)
  File ""C:\Anaconda2\lib\site-packages\gensim-0.13.1-py2.7-win-amd64.egg\gensim\models\tfidfmodel.py"", line 119, in initialize
    for termid, _ in bow:
ValueError: too many values to unpack
</code></pre>

<p>I know the error is from the yield line because I had a different yield line that worked. It looked like this: </p>

<pre><code> yield [word for word in dictionary.doc2bow(""{} {} {}"".format(title, summary, content).lower().translate(None, string.punctuation).split()) if word not in stoplist]
</code></pre>

<p>It was abit messy and hard to put functionallity to it so I've changed it as you can see in the first example.</p>
","4948165","","4948165","","2016-08-09 13:37:02","2016-08-09 14:16:54","python - Yield improperly usage","<python><parsing><yield><gensim>","2","2","","","","CC BY-SA 3.0"
"55951158","1","55957512","","2019-05-02 11:10:59","","0","1063","<p>To my understanding, batch (vanilla) gradient descent makes one parameter update for all training data. Stochastic gradient descent (SGD) allows you to update parameter for each training sample, helping the model to converge faster, at the cost of high fluctuation in function loss. </p>

<p><a href=""https://i.stack.imgur.com/0LGA5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0LGA5.png"" alt=""enter image description here""></a></p>

<p>Batch (vanilla) gradient descent sets <code>batch_size=corpus_size</code>.</p>

<p>SGD sets <code>batch_size=1</code>.</p>

<p>And mini-batch gradient descent sets <code>batch_size=k</code>, in which <code>k</code> is usually 32, 64, 128...</p>

<p>How does gensim apply SGD or mini-batch gradient descent? It seems that <code>batch_words</code> is the equivalent of <code>batch_size</code>, but I want to be sure. </p>

<p>Is setting <code>batch_words=1</code> in gensim model equivalent to applying SGD?</p>
","7552761","","","","","2019-05-02 17:30:23","gensim Word2Vec - how to apply stochastic gradient descent?","<nlp><gensim><word2vec><gradient-descent><stochastic>","1","0","","","","CC BY-SA 4.0"
"65019412","1","65027443","","2020-11-26 09:39:11","","0","169","<p>I have trained a <code>doc2vec</code> model with <code>gensim</code> and like to import it into <code>Deeplearning4j</code> in order to deploy that model.</p>
<p>For <code>word2vec</code> models, I know that this is possible by saving the model with</p>
<pre><code>model.wv.save_word2vec_format(&quot;word2vec.bin&quot;, binary=True)
</code></pre>
<p>and importing if in Java with</p>
<pre><code>Word2Vec w2vModel = WordVectorSerializer.readWord2VecModel(&quot;word2vec.bin&quot;);
</code></pre>
<p>Is there a similar way to import a <code>doc2vec</code> model?</p>
","910411","","","","","2020-11-26 18:22:12","Importing a gensim doc2vec model in deeplearning4j","<java><gensim><word2vec><doc2vec><deeplearning4j>","1","0","","","","CC BY-SA 4.0"
"38852506","1","","","2016-08-09 13:43:38","","0","352","<p>I am trying to analyze news snippets in order to identify crisis periods. 
To do so, I have already downloaded news articles over the past 7 years and have those available. 
Now, I am applying a LDA (Latent Dirichlet Allocation) model on this dataset in order to identify those countries show signs of an economic crisis. </p>

<p>I am basing my code on a blog post by Jordan Barber (<a href=""https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"" rel=""nofollow"">https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html</a>) ‚Äì here is my code so far:</p>

<pre><code>import os, csv

#create list with text blocks in rows, based on csv file
list=[]

with open('Testfile.csv', 'r') as csvfile:
    emails = csv.reader(csvfile)
    for row in emails:
         list.append(row)

#create doc_set
doc_set=[]

for row in list:
    doc_set.append(row[0])

#import plugins - need to install gensim and stop_words manually for fresh python install
from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = get_stop_words('en')

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()


# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:

    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]

    # add tokens to list
    texts.append(stemmed_tokens)


# turn our tokenized documents into a id &lt;-&gt; term dictionary
dictionary = corpora.Dictionary(texts)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

# generate LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=10)

print(ldamodel.print_topics(num_topics=5, num_words=5))

# map topics to documents
doc_lda=ldamodel[corpus]

with open('doc_lda.csv', 'w') as outfile:
    writer = csv.writer(outfile)
    for row in doc_lda:
        writer.writerow(row)
</code></pre>

<p>Essentially, I identify a number of topics (5 in the code above ‚Äì to be checked), and using the last line I assign each news article a score, which indicates the probability of an article being related to one of these topics. 
Now, I can only manually make a qualitative assessment of whether a given topic is related to a crisis, which is bit unfortunate.
What I would much rather do, is to tell the algorithm whether an article was published during a crisis and use this additional piece of information to identify both topics for my ‚Äúcrisis years‚Äù as well as for my ‚Äúnon-crisis-years‚Äù. Simply splitting my dataset to just consider topics for my ‚Äúbads‚Äù (i.e. crisis years only) won‚Äôt work in my opinion, as I would still need to manually select which topics would actually be related to a crisis, and which topics would show up anyways (sports news, ‚Ä¶). </p>

<p>So, is there a way to adapt the code to a) incorporate the information of ‚Äúcrisis‚Äù vs ‚Äúnon-crisis‚Äù and b) to automatically chose the optimal number of topics / words to optimize the predictive power of the model?</p>

<p>Thanks a lot in advance!</p>
","5763269","","","","","2016-08-10 10:13:05","Stipulation of ""Good""/""Bad""-Cases in an LDA Model (Using gensim in Python)","<python><python-2.7><lda><gensim>","1","1","","","","CC BY-SA 3.0"
"37823014","1","","","2016-06-14 22:15:14","","1","852","<p>I am trying to categorize the blog content using Topic Modeling. Using LDA transformation, I couldn't find the correlation b/w topics. Say, cricket is a sub topic of Sports topic. However, I come to know that it could be achieved using HLDA. Could some one help me how to implement the HLDA transformation in python gensim package?</p>
","1462350","","","","","2016-07-26 18:43:49","How to implement hlda transformation to find correlation of topics in gensim?","<python><gensim><topic-modeling>","1","0","0","","","CC BY-SA 3.0"
"64719381","1","","","2020-11-06 17:49:34","","1","69","<p>I am trying to decide whether to use the gensim methods most_similar() and most_similar_cosmul() for a project where I am trying to find a set of words that are similar to a list of inputs.</p>
<p>While both methods provide an argument that restricts the number of words in the result set (default:10), I am trying to select words based on a similarity threshold (say 0.5). This works well between the two methods if there is a low number of positive words that contribute to the similarity.</p>
<p>However, while the regular most_similar()-method (linear) returns a relatively stable similarity score for each output-pair independent of the number of input words, the most_similar_cosmul()-method (multiplicative) returns increasingly small similarities.</p>
<p>I am wondering if there is any way to normalize the similarities provided by the most_similar_cosmul method so that they are independent of the number of input words. The most_similar_cosmul method is based on the paper ‚ÄúLinguistic Regularities in Sparse and Explicit Word Representations‚Äù by Levy and Yoav Goldberg.</p>
<pre><code>&gt;&gt;&gt; print(model.wv.most_similar(['digital_transformation','digital']))
[('social_mobile', 0.6574317216873169), ('social_networking', 0.653410792350769), ('mobile', 0.6508731245994568), ('mobile_social', 0.6483871340751648), ('digitization', 0.6388466358184814), ('digital_platform', 0.6366733908653259), ('digitalization', 0.6243988871574402), ('omni-channel', 0.6230252385139465), ('multi-channel', 0.6205648183822632), ('digital_marketing', 0.6161972284317017)]

&gt;&gt;&gt; print(model.wv.most_similar_cosmul(['digital_transformation','digital']))
[('social_mobile', 0.6048797965049744), ('social_networking', 0.6038507223129272), ('mobile_social', 0.5969080328941345), ('digitization', 0.594704270362854), ('mobile', 0.5940128564834595), ('digital_platform', 0.5880148410797119), ('digitalization', 0.585797905921936), ('omni-channel', 0.5844268798828125), ('multi-channel', 0.5779333710670471), ('digital_marketing', 0.575140118598938)]



&gt;&gt;&gt; print(model.wv.most_similar(['digital_transformation','digital','virtual']))
[('mobile', 0.6831567883491516), ('social_networking', 0.6692748665809631), ('social_mobile', 0.6688156127929688), ('cloud-based', 0.6612646579742432), ('cloud', 0.6573182344436646), ('mobile_social', 0.656197190284729), ('connected', 0.6240546107292175), ('physical_digital', 0.6219688653945923), ('digital_platform', 0.6217926740646362), ('digital_content', 0.6217813491821289)]

&gt;&gt;&gt; print(model.wv.most_similar_cosmul(['digital_transformation','digital','virtual']))
[('mobile', 0.4431973695755005), ('social_networking', 0.4394254982471466), ('social_mobile', 0.43778157234191895), ('cloud', 0.4324479103088379), ('cloud-based', 0.43241411447525024), ('mobile_social', 0.4277876913547516), ('connected', 0.4095992147922516), ('physical_digital', 0.40511685609817505), ('digital_content', 0.40396806597709656), ('digital_platform', 0.40359607338905334)]



&gt;&gt;&gt; print(model.wv.most_similar(['digital_transformation','digital','virtual', 'online']))
[('mobile', 0.7285350561141968), ('online_mobile', 0.6857519745826721), ('social_networking', 0.6757094264030457), ('mobile_social', 0.6697002053260803), ('digital_platform', 0.6669844388961792), ('social_mobile', 0.6654024720191956), ('digital_content', 0.6573283076286316), ('physical_digital', 0.6509564518928528), ('multi-channel', 0.6482810974121094), ('cloud-based', 0.64772629737854)]

&gt;&gt;&gt; print(model.wv.most_similar_cosmul(['digital_transformation','digital','virtual','online']))
[('mobile', 0.35193151235580444), ('social_networking', 0.3212329149246216), ('online_mobile', 0.31998541951179504), ('mobile_social', 0.31541353464126587), ('social_mobile', 0.3134937584400177), ('digital_platform', 0.31218039989471436), ('digital_content', 0.3066185712814331), ('physical_digital', 0.3035270869731903), ('cloud-based', 0.301998496055603), ('multi-channel', 0.30137890577316284)]
</code></pre>
","12872310","","","","","2020-11-06 17:49:34","Normalizing the similarity score provided by gensim method most_similar_cosmul","<python><gensim><word2vec><semantics><similarity>","0","2","","","","CC BY-SA 4.0"
"29589795","1","","","2015-04-12 13:06:28","","1","256","<p>I have 2 files, </p>

<p><strong>music.txt</strong> &amp; <strong>science.txt</strong></p>

<p>I'd lke to extract 2 topics from the above (<strong>Music</strong> , <strong>Science</strong>)</p>

<p>After creating the LDA model from these 2 files (setting num_topics=<strong>2</strong>)</p>

<pre><code>lda = gensim.models.ldamodel.LdaModel(corpus=my_corpus, id2word=corpus_dictionary, num_topics=2)

print(lda.print_topic(0))
print(lda.print_topic(1))
</code></pre>

<p>This is my output</p>

<pre><code>0.011*scientific + 0.010*musical + 0.007*music, + 0.006*music. + 0.006*study + 0.005*not + 0.005*research + 0.005*main

0.030*music + 0.013*science + 0.010*scientific + 0.009*musical + 0.006*not + 0.005*music. + 0.005*study + 0.005*music, + 0.005*their + 0.005*research
</code></pre>

<p>As you can see, <strong>both</strong> science and music are present in each of the 2 topics</p>

<p>I'd like to</p>

<ol>
<li>Use music.txt and create 1 topic <strong>Music</strong> LDA model</li>
<li>Use science.txt and create 1 topic <strong>Science</strong> LDA model</li>
<li>Combine the above 2 LDA models to give 1 LDA model with the above 2 topics</li>
</ol>

<p>is the above <strong>3rd</strong> step possible?  I'd like to have individual segregration of topics in my LDA model. If not, is there any alternative?</p>
","4726262","","4726262","","2015-04-12 17:11:30","2015-04-12 21:52:23","Text Processing, How to assign 1 topic -> 1 document using LDA?","<machine-learning><nlp><topic-modeling><text-classification><gensim>","1","0","1","","","CC BY-SA 3.0"
"47610985","1","","","2017-12-02 18:13:02","","2","139","<p>I'm trying to learn dynamic topic modeling(to capture the semantic changes in the words) from data scrapped from PUBMED. I was able to get the data in the form of xml and was able to extract the ""abstract"" text and the date information off of it and saved that in the csv format. (But this is just a part of the data.)</p>

<p>Format obtained</p>

<p>Year|month|day|abstractText</p>

<p>I'm planning on using gensim lda for my model</p>

<p>I've never really done topic modeling before and need your help with guiding me through this process one step at a time.</p>

<p>Questions:</p>

<ol>
<li>Is csv a preferred format to feed into gensim lda?</li>
<li>for dynamic modeling, how should the time aspect of the data be captured and used in the model?</li>
<li>is there a better way to organize the data than in csv files?</li>
<li>Should i use the bodytext instead of the abstract for this?</li>
</ol>

<p>Hope I learn a lot from this. Thanks in advance.</p>
","2283660","","1060350","","2017-12-03 10:25:43","2019-02-16 21:36:36","Setup data for dynamic topic modelling","<python><text-mining><gensim><topic-modeling><pubmed>","0","0","1","","","CC BY-SA 3.0"
"64901436","1","","","2020-11-18 21:10:05","","0","165","<p>I'm trying to create a Word2Vec model using gensim but I don't understand what Workers mean.
This is an examples from radimrehurek.com with <code>workers = 4</code> but there is no explanation for that.</p>
<p><code>Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)</code></p>
<p>I would be very thankful of anyone can help me.Thxx</p>
","14664843","","","","","2020-11-18 21:34:42","what does workers means in gensim Word2Vec?","<python><nlp><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"47735393","1","","","2017-12-10 02:49:16","","1","1782","<p>I am using Gensim Phrases to identify important n-grams in my text as follows.</p>

<pre><code>bigram = Phrases(documents, min_count=5)
trigram = Phrases(bigram[documents], min_count=5)

for sent in documents:
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]
</code></pre>

<p>However, this detects uninteresting n-grams such as <code>special issue</code>, <code>important matter</code>, <code>high risk</code> etc. I am particularly, interested in detecting concepts in the text such as <code>machine learning</code>, <code>human computer interaction</code> etc.</p>

<p>Is there a way to stop phrases detecting uninteresting  n-grams as I have mentioned above in my example?</p>
","","user8871463","","","","2017-12-11 02:37:00","Gensim Phrases usage to filter n-grams","<python><nlp><word2vec><gensim>","2","2","","","","CC BY-SA 3.0"
"46769705","1","","","2017-10-16 11:53:39","","2","745","<p>Is there any way to load doc2vec model saved using gensim into deeplearning4j's ParagraphVectors?</p>

<p>My gensim model is valid - I am able to load it using gensim with no problems.</p>

<p>When I call WordVectorSerializer.readParagraphVectors on my model from Java it throws exception:</p>

<pre><code>Exception in thread ""main"" java.util.zip.ZipException: error in opening zip file
    at java.util.zip.ZipFile.open(Native Method)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:219)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:149)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:163)
    at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.readWord2Vec(WordVectorSerializer.java:889)
    at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.readParagraphVectors(WordVectorSerializer.java:825)
    at pl.org.opi.main.main(main.java:17)
</code></pre>

<p>Upon debugging the code, I noticed that deeplearning4j expects a zip file with multiple txt files and a single json file inside of it. Is there a way to convert the gensim model to the zip expected by deeplearning4j or is there a dedicated method for this in dl4j API (couldn't find any using examples and javadoc)?</p>
","7271874","","","","","2017-10-16 11:53:39","Doc2Vec from gensim to deeplearning4j","<java><python><gensim><deeplearning4j><doc2vec>","0","1","1","","","CC BY-SA 3.0"
"37861873","1","","","2016-06-16 14:18:52","","1","599","<p>In <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow""><code>doc2vec</code> function</a>, there is a parameter called <code>size</code>.</p>

<p>I understand that, <code>size</code> is the dimension of output vector, and if <code>size=400</code> it will capture the content better than if <code>size=100</code>.</p>

<p>However, I do not understand, what does <code>size</code> stand for? Does it mean how far Doc2Vec will lookup from a word, to predict the next word? Or what does it mean?</p>

<p>Thanks a lot,</p>
","546678","","","","","2016-07-29 02:42:25","What does size parameter in gensim doc2vec represent","<gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"47427986","1","","","2017-11-22 06:15:32","","1","260","<p>I followed the example in this link and ran the following script to process the latest english wikipedia articles:</p>

<p><a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a></p>

<p>$ python -m gensim.scripts.make_wiki</p>

<p>The result of running the script after 9 hours is that I now have .mm and .txt files. I want to train a word2vec model but all the examples found start from the .bz2 file. </p>

<p>How do I train a word2vec model using the .mm files as input instead of the raw bz2 file? The link below shows how to train an LDA model. Can someone pls share syntax?</p>

<p><a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a></p>

<p>Thanks! </p>
","7743624","","","","","2017-11-22 06:15:32","Wikipedia word2vec","<gensim>","0","1","","","","CC BY-SA 3.0"
"47138149","1","","","2017-11-06 13:30:44","","0","128","<p>The LDA code generates topics say from 0 to 5 . Is there a standard way (a norm) used to link the generated topics and the documents themselves. Eg: doc1 is of Topic0 , doc5 is of topic Topic1 etc. 
One way i can think of is to string search each of geenrated key words in each topic on the docs , is there a generic way or practice followed for this?</p>

<p>Ex LDA code - <a href=""https://github.com/manhcompany/lda/blob/master/lda.py"" rel=""nofollow noreferrer"">https://github.com/manhcompany/lda/blob/master/lda.py</a> </p>
","7938352","","","","","2018-06-04 17:07:20","How do you link back topics generated by LDA model to actual document","<machine-learning><nlp><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"55248396","1","55249812","","2019-03-19 19:08:35","","1","6512","<p>Thanks for stopping by!  I had a quick question about appending stop words. I have a select few words that show up in my data set and I was hopping I could add them to gensims stop word list.  I've seen a lot of examples using nltk and I was hoping there would be a way to do the same in gensim.  I'll post my code below:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
            nltk.bigrams(token)
            result.append(lemmatize_stemming(token))
    return result</code></pre>
</div>
</div>
</p>
","11192516","","","","","2019-03-19 21:15:52","Add stop words in Gensim","<python><windows><nlp><gensim><stop-words>","2","0","","","","CC BY-SA 4.0"
"56251839","1","56265773","","2019-05-22 07:51:27","","0","1426","<blockquote>
  <p>edit</p>
</blockquote>

<p>The train corpus is a Spark dataframe I built before this step. I load it from parquet format and created a ""Feed"" class that give to Gensim lib the iterator on the train corpus :</p>

<pre><code>class Feed():
    def __init__(self, train_data):
        self.train_data = train_data

    def __iter__(self):
        for row in self.train_data.rdd.toLocalIterator():
            yield \
                gensim.models.doc2vec.TaggedDocument(\
                words=[kw.lower() for kw in row[""keywords""]] + list(row[""tokens_filtered""]),\
                tags=[row[""id""]])


sdf = spark.read.parquet(save_dirname)
train_corpus = Feed(sdf)
</code></pre>

<blockquote>
  <p>end edit</p>
</blockquote>

<p>I wish to train a Gensim Doc2Vec model on ~9 millions news text documents. Here is my model definition :</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(
        workers=8,
        vector_size=300,
        min_count=50,
        epochs=10)
</code></pre>

<p>The first step is getting the vocabulary :</p>

<pre><code>model.build_vocab(train_corpus)
</code></pre>

<p>It ends up in 90 minutes. Here is the logging info at the end of this process :</p>

<pre><code>INFO:gensim.models.doc2vec:collected 4202859 word types and 8950263 unique tags from a corpus of 8950339 examples and 1565845381 words
INFO:gensim.models.word2vec:Loading a fresh vocabulary
INFO:gensim.models.word2vec:min_count=50 retains 325027 unique words (7% of original 4202859, drops 3877832)
INFO:gensim.models.word2vec:min_count=50 leaves 1546772183 word corpus (98% of original 1565845381, drops 19073198)
INFO:gensim.models.word2vec:deleting the raw counts dictionary of 4202859 items
INFO:gensim.models.word2vec:sample=0.001 downsamples 9 most-common words
INFO:gensim.models.word2vec:downsampling leaves estimated 1536820314 word corpus (99.4% of prior 1546772183)
INFO:gensim.models.base_any2vec:estimated required memory for 325027 words and 300 dimensions: 13472946500 bytes
</code></pre>

<p>Then I train the model with an iterator class on the train corpus :</p>

<pre><code>model.train(train_corpus, total_examples=nb_rows, epochs=model.epochs)
</code></pre>

<p>The last training logs are :</p>

<pre><code>INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 99.99% examples, 201921 words/s, in_qsize 16, out_qsize 0
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads
</code></pre>

<p>But it never finish the remaining threads.
It's not the first time I encounter this problem, even with much smaller train corpus. Usually, I relaunch the entire process (vocabulary setting and model training) and it goes on.</p>

<p>By now, to save time, I wish to NOT calculate again the vocabulary, getting in place the previously succesfully calculated one, and only try to train again the model. Is there a way to save the vocab part only of the model, then load it to train the model directly on train corpus ?</p>
","11133272","","11133272","","2019-05-23 07:14:11","2019-05-23 07:14:11","Is there a way to save and load the vocabulary of a Gensim Doc2Vec model","<python><pyspark><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"34765622","1","","","2016-01-13 11:34:03","","2","367","<p>when I try and load the model and do anything with it I always get : ""<code>AttributeError: 'Projection' object has no attribute 'u'</code>""</p>

<p>I can't figure out what I am doing wrong here. Not sure if I am saving the model incorrectly or loading it incorrectly.</p>

<pre><code>index = similarities.MatrixSimilarity(lsiModel[tfIdfModel[myCorpus]])
File ""/home/analytics/.local/lib/python2.7/site-packages/gensim/models /lsimodel.py"", line 405, 
in __getitem__ 
assert self.projection.u is not None, ""decomposition not initialized yet""   
AttributeError: 'Projection' object has no attribute 'u'
</code></pre>
","5784001","","1202025","","2016-01-13 12:08:47","2017-01-01 21:27:27","AttributeError: 'Projection' object has no attribute 'u' gensim python lsi","<python-2.7><tf-idf><gensim>","0","1","","","","CC BY-SA 3.0"
"55263867","1","55265921","","2019-03-20 14:58:16","","2","1240","<p>I am training my <code>ldamodel</code> using <code>gensim</code>, and predicting using a test corpus like this <code>ldamodel[doc_term_matrix_test]</code>, it works just fine but I don't understand how the prediction is actually done using the trained model (what <code>ldamodel[doc_term_matrix_test]</code> is doing).</p>

<p>Here is the code :</p>

<pre><code>dictionary2 = corpora.Dictionary(test)
dictionary = corpora.Dictionary(train)
dictionary.merge_with(dictionary2)
doc_term_matrix2 = [dictionary.doc2bow(doc) for doc in test]
doc_term_matrix = [dictionary.doc2bow(doc) for doc in train]
Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(doc_term_matrix, num_topics=2, id2word = 
dictionary,random_state=100, iterations=50, passes=1)
topics = sorted(ldamodel[doc_term_matrix2],
                key=lambda 
                x:x[1],
                reverse=True)
</code></pre>
","9123977","","3935063","","2019-07-02 21:43:57","2019-07-02 21:43:57","How does LDA (Latent Dirichlet Allocation) inference from `gensim` work for a new data?","<python><gensim><lda><topic-modeling><inference>","1","4","","","","CC BY-SA 4.0"
"55278701","1","","","2019-03-21 10:47:33","","1","2745","<p>I am topic modelling Harvard Library book title and subjects.</p>

<p>I use Gensim Mallet Wrapper to model with Mallet's LDA.
When I try to get Coherence and Perplexity values to see how good the model is, perplexity fails to calculate with below exception.
I do not get the same error if I use Gensim's built-in LDA model instead of Mallet. 
My corpus holds 7M+ documents of length up to 50 words averaging 20. So documents are short.</p>

<p>Below is the related part of my code:</p>

<pre><code># TOPIC MODELING

from gensim.models import CoherenceModel
num_topics = 50

# Build Gensim's LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics,
                                       random_state=100,
                                       update_every=1,
                                       chunksize=100,
                                       passes=10,
                                       alpha='auto',
                                       per_word_topics=True)

# Compute Perplexity
print('\nPerplexity: ', lda_model.log_perplexity(corpus))  
# a measure of how good the model is. lower the better.
</code></pre>

<blockquote>
  <p>Perplexity:  -47.91929228302663</p>
</blockquote>

<pre><code># Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, 
texts=data_words_trigrams, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
</code></pre>

<blockquote>
  <p>Coherence Score:  0.28852857563541856</p>
</blockquote>

<p>LDA gave scores without problem. Now I model the same bag of words with MALLET</p>

<pre><code># Building LDA Mallet Model
mallet_path = '~/mallet-2.0.8/bin/mallet' # update this path
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, 
corpus=corpus, num_topics=num_topics, id2word=id2word)

# Convert mallet to gensim type
mallet_model = 
gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)

# Compute Coherence Score
coherence_model_ldamallet = CoherenceModel(model=mallet_model, 
texts=data_words_trigrams, dictionary=id2word, coherence='c_v')
coherence_ldamallet = coherence_model_ldamallet.get_coherence()
print('\nCoherence Score: ', coherence_ldamallet)
</code></pre>

<blockquote>
  <p>Coherence Score:  0.5994123896865993</p>
</blockquote>

<p>Then I ask for the Perplexity values and get below warnings and NaN value.</p>

<pre><code># Compute Perplexity
print('\nPerplexity: ', mallet_model.log_perplexity(corpus))
</code></pre>

<blockquote>
  <p>/app/app-py3/lib/python3.5/site-packages/gensim/models/ldamodel.py:1108:
  RuntimeWarning: invalid value encountered in multiply   score +=
  np.sum((self.eta - _lambda) * Elogbeta)</p>
  
  <p>Perplexity:  nan</p>
  
  <p>/app/app-py3/lib/python3.5/site-packages/gensim/models/ldamodel.py:1109:
  RuntimeWarning: invalid value encountered in subtract   score +=
  np.sum(gammaln(_lambda) - gammaln(self.eta))</p>
</blockquote>

<p>I realize this is a very Gensim specific question and requires deeper knowledge of this function: 
     gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)</p>

<p>Hence I would appreciate any comment on warnings and the Gensim domain.</p>
","8076949","","8076949","","2019-03-22 15:15:54","2020-12-20 16:41:52","Gensim Topic Modeling with Mallet Perplexity","<python><gensim><topic-modeling><mallet><perplexity>","2","0","","","","CC BY-SA 4.0"
"47424335","1","","","2017-11-21 23:19:15","","0","960","<p>I've trained an LDA model using gensim. I am under the impression that Lda reduces the data to two lower level matrices (ref: <a href=""https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"" rel=""nofollow noreferrer"">https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/</a>) but I cannot seem to figure out how to access the term-topic matrix. The only reference I could find in gensim's documentation is for the .get_topics() attribute, however the format it provides makes no sense to me.</p>

<p>It is easy enough to apply a transformation to retrieve the Document-topic matrix, like so:</p>

<pre><code>doc_topic_matrix = lda_model[doc_term_matrix]
</code></pre>

<p>so I am hoping that there is a similarly functional method to generate the topic-term matrix.</p>

<p>Ideally, output should look like this:</p>

<pre><code>         word1  word2  word3  word4  word5
topic_a   .12    .38    .07    .24    .19
topic_b   .41    .11    .04    .14    .30
</code></pre>

<p>Any thoughts on whether or not this is possible?</p>
","8746272","","","","","2021-04-22 15:49:31","Access Term Topic Matrix generated by Gensim LDA","<python><gensim><lda>","2","0","","","","CC BY-SA 3.0"
"64902215","1","64902545","","2020-11-18 22:13:28","","0","211","<p>Using the Gensim package (both LDA and Mallet), I noticed that when I create a model with more than 20 topics, and I use the print_topics function, it will print a maximum of 20 topics (note, not the first 20 topics, rather any 20 topics), and they will be out of order.</p>
<p>And so my question is, how do i get all of the topics to print? I am unsure if this is a bug or an issue on my end. I have looked back at my library of LDA models (over 5000, different data sources), and have noted this happens in all of them where topics are above 20.</p>
<p>Below is sample code with output. In the output, you will see the topics are not ordered (they should be) and topics are missing such as topic 3.</p>
<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus=jr_dict_corpus,
                                           id2word=jr_dict,
                                           num_topics=25, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)

pprint(lda_model.print_topics())
#note, if the model contained 20 topics, the topics would be listed in order 0-19
[(21,
  '0.001*&quot;commitment&quot; + 0.001*&quot;study&quot; + 0.001*&quot;evolve&quot; + 0.001*&quot;outlook&quot; + '
  '0.001*&quot;value&quot; + 0.001*&quot;people&quot; + 0.001*&quot;individual&quot; + 0.001*&quot;client&quot; + '
  '0.001*&quot;structure&quot; + 0.001*&quot;proposal&quot;'),
 (18,
  '0.001*&quot;self&quot; + 0.001*&quot;insurance&quot; + 0.001*&quot;need&quot; + 0.001*&quot;trend&quot; + '
  '0.001*&quot;statistic&quot; + 0.001*&quot;propose&quot; + 0.001*&quot;analysis&quot; + 0.001*&quot;perform&quot; + '
  '0.001*&quot;impact&quot; + 0.001*&quot;awareness&quot;'),
 (2,
  '0.001*&quot;link&quot; + 0.001*&quot;task&quot; + 0.001*&quot;collegiate&quot; + 0.001*&quot;universitie&quot; + '
  '0.001*&quot;banking&quot; + 0.001*&quot;origination&quot; + 0.001*&quot;security&quot; + 0.001*&quot;standard&quot; '
  '+ 0.001*&quot;qualifications_bachelor&quot; + 0.001*&quot;greenfield&quot;'),
 (11,
  '0.024*&quot;collegiate&quot; + 0.016*&quot;interpersonal&quot; + 0.016*&quot;prepare&quot; + '
  '0.016*&quot;invite&quot; + 0.016*&quot;aspect&quot; + 0.016*&quot;college&quot; + 0.016*&quot;statistic&quot; + '
  '0.016*&quot;continent&quot; + 0.016*&quot;structure&quot; + 0.016*&quot;project&quot;'),
 (10,
  '0.049*&quot;enjoy&quot; + 0.049*&quot;ambiguity&quot; + 0.017*&quot;accordance&quot; + 0.017*&quot;liberalize&quot; '
  '+ 0.017*&quot;developing&quot; + 0.017*&quot;application&quot; + 0.017*&quot;vacancie&quot; + '
  '0.017*&quot;service&quot; + 0.017*&quot;initiative&quot; + 0.017*&quot;discontinuing&quot;'),
 (20,
  '0.028*&quot;negotiation&quot; + 0.028*&quot;desk&quot; + 0.018*&quot;enhance&quot; + 0.018*&quot;engage&quot; + '
  '0.018*&quot;discussion&quot; + 0.018*&quot;ability&quot; + 0.018*&quot;depth&quot; + 0.018*&quot;derive&quot; + '
  '0.018*&quot;enjoy&quot; + 0.018*&quot;balance&quot;'),
 (12,
  '0.036*&quot;individual&quot; + 0.024*&quot;validate&quot; + 0.018*&quot;greenfield&quot; + '
  '0.018*&quot;capability&quot; + 0.018*&quot;coordinate&quot; + 0.018*&quot;create&quot; + '
  '0.018*&quot;programming&quot; + 0.018*&quot;safety&quot; + 0.010*&quot;evaluation&quot; + '
  '0.002*&quot;reliability&quot;'),
 (1,
  '0.028*&quot;negotiation&quot; + 0.021*&quot;responsibility&quot; + 0.014*&quot;master&quot; + '
  '0.014*&quot;mind&quot; + 0.014*&quot;experience&quot; + 0.014*&quot;worker&quot; + 0.014*&quot;ability&quot; + '
  '0.007*&quot;summary&quot; + 0.007*&quot;proposal&quot; + 0.007*&quot;alert&quot;'),
 (23,
  '0.043*&quot;banking&quot; + 0.026*&quot;origination&quot; + 0.026*&quot;round&quot; + 0.026*&quot;credibility&quot; '
  '+ 0.026*&quot;entity&quot; + 0.018*&quot;standard&quot; + 0.017*&quot;range&quot; + 0.017*&quot;pension&quot; + '
  '0.017*&quot;adapt&quot; + 0.017*&quot;information&quot;'),
 (13,
  '0.034*&quot;priority&quot; + 0.034*&quot;reconciliation&quot; + 0.034*&quot;purchaser&quot; + '
  '0.023*&quot;reporting&quot; + 0.023*&quot;offer&quot; + 0.023*&quot;investor&quot; + 0.023*&quot;share&quot; + '
  '0.023*&quot;region&quot; + 0.023*&quot;service&quot; + 0.023*&quot;manipulate&quot;'),
 (22,
  '0.017*&quot;analyst&quot; + 0.017*&quot;modelling&quot; + 0.016*&quot;producer&quot; + 0.016*&quot;return&quot; + '
  '0.016*&quot;self&quot; + 0.009*&quot;scope&quot; + 0.008*&quot;mind&quot; + 0.008*&quot;need&quot; + 0.008*&quot;detail&quot; '
  '+ 0.008*&quot;statistic&quot;'),
 (9,
  '0.021*&quot;decision&quot; + 0.014*&quot;invite&quot; + 0.014*&quot;balance&quot; + 0.014*&quot;commercialize&quot; '
  '+ 0.014*&quot;transform&quot; + 0.014*&quot;manage&quot; + 0.014*&quot;optionality&quot; + '
  '0.014*&quot;problem_solving&quot; + 0.014*&quot;fuel&quot; + 0.014*&quot;stay&quot;'),
 (7,
  '0.032*&quot;commitment&quot; + 0.032*&quot;study&quot; + 0.016*&quot;impact&quot; + 0.016*&quot;outlook&quot; + '
  '0.011*&quot;operation&quot; + 0.011*&quot;expand&quot; + 0.011*&quot;exchange&quot; + 0.011*&quot;management&quot; '
  '+ 0.011*&quot;conde&quot; + 0.011*&quot;evolve&quot;'),
 (15,
  '0.032*&quot;agility&quot; + 0.019*&quot;feasibility&quot; + 0.019*&quot;self&quot; + 0.014*&quot;deploy&quot; + '
  '0.014*&quot;define&quot; + 0.013*&quot;investment&quot; + 0.013*&quot;option&quot; + 0.013*&quot;control&quot; + '
  '0.013*&quot;action&quot; + 0.013*&quot;incubation&quot;'),
 (5,
  '0.020*&quot;desk&quot; + 0.018*&quot;agility&quot; + 0.016*&quot;vender&quot; + 0.016*&quot;coordinate&quot; + '
  '0.016*&quot;committee&quot; + 0.012*&quot;acquisition&quot; + 0.012*&quot;target&quot; + '
  '0.012*&quot;counterparty&quot; + 0.012*&quot;approval&quot; + 0.012*&quot;trend&quot;'),
 (17,
  '0.022*&quot;option&quot; + 0.017*&quot;working&quot; + 0.017*&quot;niche&quot; + 0.011*&quot;business&quot; + '
  '0.011*&quot;constrain&quot; + 0.011*&quot;meeting&quot; + 0.011*&quot;correspond&quot; + 0.011*&quot;exposure&quot; '
  '+ 0.011*&quot;element&quot; + 0.011*&quot;face&quot;'),
 (0,
  '0.025*&quot;expertise&quot; + 0.025*&quot;banking&quot; + 0.021*&quot;universitie&quot; + '
  '0.017*&quot;spreadsheet&quot; + 0.013*&quot;negotiation&quot; + 0.013*&quot;shipment&quot; + '
  '0.013*&quot;arise&quot; + 0.013*&quot;billing&quot; + 0.013*&quot;assistance&quot; + 0.013*&quot;sector&quot;'),
 (4,
  '0.024*&quot;provide&quot; + 0.017*&quot;consider&quot; + 0.017*&quot;allow&quot; + 0.015*&quot;outlook&quot; + '
  '0.015*&quot;value&quot; + 0.015*&quot;contract&quot; + 0.012*&quot;study&quot; + 0.012*&quot;technology&quot; + '
  '0.012*&quot;scenario&quot; + 0.012*&quot;indicator&quot;'),
 (6,
  '0.058*&quot;impulse&quot; + 0.027*&quot;shall&quot; + 0.027*&quot;shape&quot; + 0.024*&quot;marketer&quot; + '
  '0.017*&quot;availability&quot; + 0.014*&quot;determine&quot; + 0.014*&quot;load&quot; + '
  '0.014*&quot;constantly_change&quot; + 0.014*&quot;instrument&quot; + 0.014*&quot;interface&quot;'),
 (19,
  '0.042*&quot;task&quot; + 0.038*&quot;tariff&quot; + 0.038*&quot;recommend&quot; + 0.024*&quot;example&quot; + '
  '0.023*&quot;future&quot; + 0.021*&quot;people&quot; + 0.021*&quot;math&quot; + 0.021*&quot;capacity&quot; + '
  '0.021*&quot;spirit&quot; + 0.020*&quot;price&quot;')]
</code></pre>
<p>Same model as above, but using 20 topics. As you can see, the output is in order by topic # and it contains all the topics.</p>
<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus=jr_dict_corpus,
                                           id2word=jr_dict,
                                           num_topics=20, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)

pprint(lda_model.print_topics())

[(0,
  '0.031*&quot;enjoy&quot; + 0.031*&quot;ambiguity&quot; + 0.028*&quot;accordance&quot; + 0.016*&quot;statistic&quot; '
  '+ 0.016*&quot;initiative&quot; + 0.016*&quot;service&quot; + 0.016*&quot;liberalize&quot; + '
  '0.016*&quot;application&quot; + 0.011*&quot;community&quot; + 0.011*&quot;identifie&quot;'),
 (1,
  '0.016*&quot;transformation&quot; + 0.016*&quot;negotiation&quot; + 0.016*&quot;community&quot; + '
  '0.016*&quot;clock&quot; + 0.011*&quot;marketer&quot; + 0.011*&quot;desk&quot; + 0.011*&quot;mandate&quot; + '
  '0.011*&quot;closing&quot; + 0.011*&quot;initiative&quot; + 0.011*&quot;experience&quot;'),
 (2,
  '0.026*&quot;priority&quot; + 0.026*&quot;reconciliation&quot; + 0.026*&quot;purchaser&quot; + '
  '0.020*&quot;safety&quot; + 0.020*&quot;region&quot; + 0.020*&quot;query&quot; + 0.020*&quot;share&quot; + '
  '0.020*&quot;manipulate&quot; + 0.020*&quot;ibex&quot; + 0.020*&quot;investor&quot;'),
 (3,
  '0.022*&quot;improve&quot; + 0.021*&quot;committee&quot; + 0.021*&quot;affect&quot; + 0.012*&quot;target&quot; + '
  '0.012*&quot;acquisition&quot; + 0.011*&quot;basis&quot; + 0.011*&quot;profitability&quot; + '
  '0.011*&quot;economic&quot; + 0.011*&quot;natural&quot; + 0.011*&quot;profit&quot;'),
 (4,
  '0.024*&quot;provide&quot; + 0.019*&quot;value&quot; + 0.017*&quot;consider&quot; + 0.017*&quot;allow&quot; + '
  '0.015*&quot;scenario&quot; + 0.015*&quot;outlook&quot; + 0.015*&quot;contract&quot; + 0.014*&quot;forecast&quot; + '
  '0.014*&quot;decision&quot; + 0.012*&quot;indicator&quot;'),
 (5,
  '0.037*&quot;desk&quot; + 0.030*&quot;coordinate&quot; + 0.030*&quot;agility&quot; + 0.030*&quot;vender&quot; + '
  '0.023*&quot;counterparty&quot; + 0.023*&quot;immature_emerge&quot; + 0.023*&quot;metric&quot; + '
  '0.022*&quot;approval&quot; + 0.015*&quot;maximization&quot; + 0.015*&quot;undergraduate&quot;'),
 (6,
  '0.053*&quot;impulse&quot; + 0.025*&quot;shall&quot; + 0.025*&quot;shape&quot; + 0.018*&quot;availability&quot; + '
  '0.018*&quot;marketer&quot; + 0.012*&quot;determine&quot; + 0.012*&quot;language&quot; + '
  '0.012*&quot;monitoring&quot; + 0.012*&quot;integration&quot; + 0.012*&quot;month&quot;'),
 (7,
  '0.026*&quot;commitment&quot; + 0.026*&quot;study&quot; + 0.013*&quot;impact&quot; + 0.013*&quot;outlook&quot; + '
  '0.009*&quot;operation&quot; + 0.009*&quot;management&quot; + 0.009*&quot;expand&quot; + 0.009*&quot;exchange&quot; '
  '+ 0.009*&quot;conde&quot; + 0.009*&quot;balance&quot;'),
 (8,
  '0.057*&quot;insurance&quot; + 0.029*&quot;propose&quot; + 0.028*&quot;rule&quot; + 0.026*&quot;self&quot; + '
  '0.023*&quot;product&quot; + 0.023*&quot;asset&quot; + 0.023*&quot;pricing&quot; + 0.023*&quot;amount&quot; + '
  '0.023*&quot;result&quot; + 0.020*&quot;liquidity&quot;'),
 (9,
  '0.012*&quot;universitie&quot; + 0.012*&quot;need&quot; + 0.012*&quot;statistic&quot; + 0.012*&quot;trend&quot; + '
  '0.008*&quot;invite&quot; + 0.008*&quot;commercialize&quot; + 0.008*&quot;transform&quot; + 0.008*&quot;manage&quot; '
  '+ 0.008*&quot;problem_solving&quot; + 0.008*&quot;optionality&quot;'),
 (10,
  '0.024*&quot;background&quot; + 0.024*&quot;curve&quot; + 0.020*&quot;allow&quot; + 0.019*&quot;collect&quot; + '
  '0.019*&quot;basis&quot; + 0.017*&quot;accordance&quot; + 0.013*&quot;improve&quot; + 0.013*&quot;datum&quot; + '
  '0.013*&quot;component&quot; + 0.013*&quot;reliability&quot;'),
 (11,
  '0.054*&quot;task&quot; + 0.049*&quot;tariff&quot; + 0.049*&quot;recommend&quot; + 0.031*&quot;future&quot; + '
  '0.027*&quot;spirit&quot; + 0.027*&quot;capacity&quot; + 0.027*&quot;math&quot; + 0.022*&quot;ensure&quot; + '
  '0.022*&quot;profit&quot; + 0.022*&quot;variable_margin&quot;'),
 (12,
  '0.001*&quot;impulse&quot; + 0.001*&quot;availability&quot; + 0.001*&quot;reliability&quot; + '
  '0.001*&quot;shall&quot; + 0.001*&quot;component&quot; + 0.001*&quot;agent&quot; + 0.001*&quot;marketer&quot; + '
  '0.001*&quot;shape&quot; + 0.001*&quot;assisting&quot; + 0.001*&quot;supply&quot;'),
 (13,
  '0.021*&quot;region&quot; + 0.016*&quot;greenfield&quot; + 0.016*&quot;collegiate&quot; + 0.011*&quot;transfer&quot; '
  '+ 0.011*&quot;remuneration&quot; + 0.011*&quot;organization&quot; + 0.011*&quot;structure&quot; + '
  '0.011*&quot;continent&quot; + 0.011*&quot;project&quot; + 0.011*&quot;prepare&quot;'),
 (14,
  '0.033*&quot;originator&quot; + 0.025*&quot;vender&quot; + 0.025*&quot;expertise&quot; + 0.025*&quot;banking&quot; + '
  '0.019*&quot;evolve&quot; + 0.017*&quot;management&quot; + 0.017*&quot;market&quot; + 0.017*&quot;site&quot; + '
  '0.012*&quot;component&quot; + 0.012*&quot;discontinuing&quot;'),
 (15,
  '0.027*&quot;agility&quot; + 0.022*&quot;mind&quot; + 0.022*&quot;negotiation&quot; + 0.011*&quot;deploy&quot; + '
  '0.011*&quot;define&quot; + 0.011*&quot;ecosystem&quot; + 0.011*&quot;control&quot; + 0.011*&quot;lead&quot; + '
  '0.011*&quot;industry&quot; + 0.011*&quot;option&quot;'),
 (16,
  '0.001*&quot;region&quot; + 0.001*&quot;master&quot; + 0.001*&quot;orginiation&quot; + 0.001*&quot;greenfield&quot; '
  '+ 0.001*&quot;agent&quot; + 0.001*&quot;identifie&quot; + 0.001*&quot;remuneration&quot; + 0.001*&quot;mark&quot; + '
  '0.001*&quot;reviewing&quot; + 0.001*&quot;closing&quot;'),
 (17,
  '0.030*&quot;banking&quot; + 0.018*&quot;option&quot; + 0.018*&quot;round&quot; + 0.018*&quot;credibility&quot; + '
  '0.018*&quot;origination&quot; + 0.018*&quot;entity&quot; + 0.016*&quot;working&quot; + 0.015*&quot;niche&quot; + '
  '0.015*&quot;standard&quot; + 0.012*&quot;coordinate&quot;'),
 (18,
  '0.027*&quot;negotiation&quot; + 0.018*&quot;reporting&quot; + 0.018*&quot;perform&quot; + 0.018*&quot;world&quot; + '
  '0.015*&quot;offer&quot; + 0.015*&quot;manipulate&quot; + 0.011*&quot;query&quot; + 0.010*&quot;control&quot; + '
  '0.010*&quot;working&quot; + 0.009*&quot;self&quot;'),
 (19,
  '0.047*&quot;example&quot; + 0.039*&quot;people&quot; + 0.039*&quot;price&quot; + 0.039*&quot;excel&quot; + '
  '0.039*&quot;excellent&quot; + 0.038*&quot;base&quot; + 0.031*&quot;office&quot; + 0.031*&quot;optimizing&quot; + '
  '0.031*&quot;participate&quot; + 0.031*&quot;package&quot;')]
</code></pre>
","6251900","","6251900","","2020-11-18 22:22:48","2021-01-20 11:24:25","Python LDA Gensim model with over 20 topics does not print properly","<python><gensim><lda>","2","0","","","","CC BY-SA 4.0"
"65014553","1","65058015","","2020-11-26 00:29:10","","2","351","<p>I was running <code>gensim</code> <code>LdaMulticore</code> package for the topic modelling using Python.
I tried to understand the meaning of the parameters within <code>LdaMulticore</code> and found the website that provides some explanations on the usage of the parameters. As a non-expert, I have some difficulty understanding these intuitively. I also referred some other materials from the website but I guess this page gives relatively full explanations of every parameters.
<br>
<a href=""https://radimrehurek.com/gensim/models/ldamulticore.html"" rel=""nofollow noreferrer"">This page</a></p>
<ol>
<li><code>chunksize</code>
Number of documents to be used in each training chunk.
<br>-&gt;Does it mean that it determines how many documents to be analyzed (trained) at once?
<br>Does changing the <code>chunksize</code> number generate significantly different outcomes? or does it just matter to the running time?</li>
</ol>
<p>2.<code>alpha</code>, <code>eta</code>, <code>decay</code>
<br>-&gt;I kept reading the explanations but couldn't understand these at all.
<br>Could someone give me some intuitive explanations on what these are about/when do I need to adjust these?</p>
<p>3.iteration
<br>Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.
<br>-&gt;It seems that Python goes over n times of the entire corpus when I set it to n. So the higher the number, the more data is analyzed but takes longer time.</p>
<p>4.random state
<br>Either a <code>randomState</code> object or a seed to generate one. Useful for reproducibility.
<br>-&gt;I've seen people setting up this by putting a random number. But what is random state about?</p>
","7882846","","6573902","","2020-11-29 07:48:07","2020-12-09 16:56:39","How to tune the parameters for gensim `LdaMulticore` in Python","<python><python-3.x><nlp><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"43656999","1","","","2017-04-27 11:52:14","","0","1001","<p>I have unstructured data of about 150k documents. I am trying to group these documents using unsupervised learning algorithm. Currently I am using LDA (Latent Dirichlet allocation) in gensim Python. For LDAModel I have passed num_topics=20. Hence my whole 150k data is falling into 20 topics. </p>

<p>Now that I have these groups, I have 2 questions:</p>

<ol>
<li>How should I assign new documents to these topics? </li>
</ol>

<p>The approach I am taking is:
Calculate the sum of the word scores of the document per topic and assign the document to the topic with the highest score. However this is not giving me good results. </p>

<p>Is there any better way to get this?</p>

<ol start=""2"">
<li>How do I assign the main keywords that denote the topic? </li>
</ol>
","4145051","","4145051","","2017-05-02 06:18:57","2017-09-13 12:49:51","LDA - Assigning keywords to topics","<python-3.x><cluster-analysis><gensim><lda><unsupervised-learning>","1","2","","","","CC BY-SA 3.0"
"46197493","1","46198385","","2017-09-13 12:22:52","","1","979","<p>Im currently trying to implement a convolutional lstm network using keras. Instead of using keras' embedding layer, I used Gensim's doc2vec embeddings and created input data from it.</p>

<p><strong>preprocessing</strong></p>

<pre><code>preprocessed_train = utils.preprocess_text(train_vect)
preprocessed_test = utils.preprocess_text(test_vect)

print preprocessed_train[0]

result: [u'snes_classic', u'preorders_open', u'later_month', u'ever_since', u'nintendo', u'announce', u'snes_classic', u'edition', u'earlier', u'fan', u'desperate', u'register', u'interest', u'ensure', u'come', u'launch', u'however', u'although', u'system', u'pre-orders', u'make', u'available', u'retailers', u'every', u'store', u'plan', u'sell', u'console', u'allow', u'people', u'place', u'pre-orders', u'yet', u'today', u'though', u'nintendo', u'confirm', u'snes_classic', u'edition', u'pre-orders', u'soon', u'available', u'fan', u'post_official', u'facebook', u'company', u'console', u'make', u'available_pre-order', u'various_retailers', u'late', u'month', u'nintendo', u'appreciate', u'incredible', u'anticipation', u'hardware', u'reference', u'fact', u'snes_classic', u'edition', u'already', u'sell', u'many', u'place', u'across_globe', u'unfortunately', u'nintendo', u'clarify', u'exactly', u'retailers', u'open', u'snes_classic', u'pre-orders', u'provide', u'exact_date', u'however', u'stand_reason', u'wal-mart', u'retailers', u'force', u'cancel_pre-orders', u'hardware', u'website', u'error', u'saw', u'go_live', u'prematurely', u'currently_unclear', u'wal-mart', u'help', u'cancel', u'reservations', u'sign-up', u'pre-orders', u'go_live', u'properly', u'month', u'appreciate', u'incredible', u'anticipation', u'exist', u'super_nintendo', u'entertainment_system', u'super_nes', u'classic', u'post', u'nintendo', u'tuesday_august', u'1', u'2017', u'post', u'nintendo', u'mention', u'ship', u'significant_amount', u'snes_classic', u'edition', u'units', u'retailers', u'launch', u'company', u'make', u'units', u'available', u'throughout', u'balance', u'calendar', u'snes_classic', u'edition', u'first', u'announce', u'nintendo', u'explain', u'make', u'units', u'nes_classic', u'constantly', u'sell', u'leave', u'many', u'glad', u'nintendo', u'offer_clarification', u'others', u'however', u'remain_unconvinced', u'nintendo', u'able', u'keep', u'demand', u'console', u'incredibly_hard', u'fan', u'place', u'legitimate', u'order', u'snes_classic', u'edition', u'end', u'even_harder', u'find', u'throughout', u'scalpers', u'place', u'pre-orders', u'pick', u'console', u'post-launch', u'order', u'sell', u'higher_price', u'later_date', u'retailers', u'like', u'ebay', u'enforce_rule', u'scalpers', u'unclear_whether', u'enough', u'snes_classic', u'edition', u'launch', u'september_29', u'2017_source', u'nintendo', u'facebook']
</code></pre>

<p><strong>data labels</strong></p>

<pre><code>y_test = [x for x in test_data['slabel']]
y_train = [x for x in train_data['slabel']]

y_test = keras.utils.to_categorical(y_test)
y_train = keras.utils.to_categorical(y_train)

result:
array([[ 0.,  0.,  0.,  0.,  1.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.]])
</code></pre>

<p><strong>load doc2vec model</strong></p>

<pre><code>doc2vec_model = gensim.models.Doc2Vec.load('./doc2vec-models/dmbbv_300_epoch_500_size_model')
</code></pre>

<p><strong>infer data and create input vectors</strong>. The <em>infer_vector</em> function creates the document embeddings based on the doc2vec model that I created. </p>

<pre><code>X_train = []
for text in preprocessed_train:
    inferred_vec = doc2vec_model.infer_vector(text)
    X_train.append(inferred_vec)

X_test = []
for text in preprocessed_test:
    inferred_vec = doc2vec_model.infer_vector(text)
    X_test.append(inferred_vec)
</code></pre>

<p><strong>reshape data</strong></p>

<pre><code>X_train = np.array(X_train)
X_test = np.array(X_test)
X_train = X_train.reshape((X_train.shape[0],1,X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0],1,X_test.shape[1]))
X_train.shape,X_test.shape

result: ((1476, 1, 500), (370, 1, 500))
</code></pre>

<p><strong>building model</strong></p>

<pre><code>model = Sequential()
model.add(Conv1D(filters = 128,
                 kernel_size = 5,
                 input_shape = (X_train.shape[1],X_train.shape[2]), 
                 padding = 'valid',
                 activation = 'relu'))
model.add(MaxPooling1D(2))
model.add(LSTM(X_train.shape[1],return_sequences = True, 
               implementation=2, 
               kernel_regularizer=regularizers.l1_l2(0.001),
               activity_regularizer=regularizers.l1(0.01)
              ))
model.add(Dropout(0.7))
model.add(Activation('relu'))
model.add(LSTM(256,return_sequences = True))
model.add(Activation('relu'))
model.add(LSTM(128))
model.add(Activation('relu'))
model.add(LSTM(64,return_sequences = True))
model.add(Activation('relu'))
model.add(LSTM(32,return_sequences = True))
model.add(Activation('relu'))
model.add(LSTM(16))
model.add(Activation('relu'))
model.add(Dense(5, activation = 'sigmoid'))
model.compile(loss=""categorical_crossentropy"", optimizer='adamax',metrics=['categorical_accuracy', 'accuracy'])
</code></pre>

<p>then I get this error </p>

<hr>

<pre><code>-----------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-488-b29db30c3ee7&gt; in &lt;module&gt;()
      5 #                  use_bias=True,
      6                  padding = 'valid',
----&gt; 7                  activation = 'relu'))
      8 model.add(MaxPooling1D(2))
      9 model.add(LSTM(X_train.shape[1],return_sequences = True, 

/usr/local/lib/python2.7/dist-packages/keras/models.pyc in add(self, layer)
    434                 # and create the node connecting the current layer
    435                 # to the input layer we just created.
--&gt; 436                 layer(x)
    437 
    438             if len(layer.inbound_nodes) != 1:

/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in __call__(self, inputs, **kwargs)
    594 
    595             # Actually call the layer, collecting output(s), mask(s), and shape(s).
--&gt; 596             output = self.call(inputs, **kwargs)
    597             output_mask = self.compute_mask(inputs, previous_mask)
    598 

/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.pyc in call(self, inputs)
    154                 padding=self.padding,
    155                 data_format=self.data_format,
--&gt; 156                 dilation_rate=self.dilation_rate[0])
    157         if self.rank == 2:
    158             outputs = K.conv2d(

/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc in conv1d(x, kernel, strides, padding, data_format, dilation_rate)
   3114         strides=(strides,),
   3115         padding=padding,
-&gt; 3116         data_format=tf_data_format)
   3117     return x
   3118 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in convolution(input, filter, padding, strides, dilation_rate, name, data_format)
    670         dilation_rate=dilation_rate,
    671         padding=padding,
--&gt; 672         op=op)
    673 
    674 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in with_space_to_batch(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)
    336       raise ValueError(""dilation_rate must be positive"")
    337     if np.all(const_rate == 1):
--&gt; 338       return op(input, num_spatial_dims, padding)
    339 
    340   # We have two padding contributions. The first is used for converting ""SAME""

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in op(input_converted, _, padding)
    662           data_format=data_format,
    663           strides=strides,
--&gt; 664           name=name)
    665 
    666     return with_space_to_batch(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in _non_atrous_convolution(input, filter, padding, data_format, strides, name)
    114           padding=padding,
    115           data_format=data_format_2d,
--&gt; 116           name=scope)
    117     elif conv_dims == 2:
    118       if data_format is None or data_format == ""NHWC"":

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in conv1d(value, filters, stride, padding, use_cudnn_on_gpu, data_format, name)
   2011     result = gen_nn_ops.conv2d(value, filters, strides, padding,
   2012                                use_cudnn_on_gpu=use_cudnn_on_gpu,
-&gt; 2013                                data_format=data_format)
   2014     return array_ops.squeeze(result, [spatial_start_dim])
   2015 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.pyc in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)
    395                                 strides=strides, padding=padding,
    396                                 use_cudnn_on_gpu=use_cudnn_on_gpu,
--&gt; 397                                 data_format=data_format, name=name)
    398   return result
    399 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
    765         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    766                          input_types=input_types, attrs=attr_protos,
--&gt; 767                          op_def=op_def)
    768         if output_structure:
    769           outputs = op.outputs

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
   2630                     original_op=self._default_original_op, op_def=op_def)
   2631     if compute_shapes:
-&gt; 2632       set_shapes_for_outputs(ret)
   2633     self._add_op(ret)
   2634     self._record_op_seen_by_control_dependencies(ret)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)
   1909       shape_func = _call_cpp_shape_fn_and_require_op
   1910 
-&gt; 1911   shapes = shape_func(op)
   1912   if shapes is None:
   1913     raise RuntimeError(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in call_with_requiring(op)
   1859 
   1860   def call_with_requiring(op):
-&gt; 1861     return call_cpp_shape_fn(op, require_shape_fn=True)
   1862 
   1863   _call_cpp_shape_fn_and_require_op = call_with_requiring

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in call_cpp_shape_fn(op, require_shape_fn)
    593     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,
    594                                   input_tensors_as_shapes_needed,
--&gt; 595                                   require_shape_fn)
    596     if not isinstance(res, dict):
    597       # Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in _call_cpp_shape_fn_impl(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)
    657       missing_shape_fn = True
    658     else:
--&gt; 659       raise ValueError(err.message)
    660 
    661   if missing_shape_fn:

ValueError: Negative dimension size caused by subtracting 5 from 1 for 'conv1d_55/convolution/Conv2D' (op: 'Conv2D') with input shapes: [?,1,1,500], [1,5,500,128].
</code></pre>
","8262615","","8262615","","2017-09-13 12:28:03","2017-09-13 13:04:51","Using gensim doc2vec with Keras Conv1d. ValueError","<python><machine-learning><keras><gensim>","1","0","3","","","CC BY-SA 3.0"
"55789477","1","55820840","","2019-04-22 05:19:09","","1","945","<p>I have used Gensim LDAMallet for topic modelling but in what way we can predict sample paragraph and get their topic model using pretrained model.</p>

<pre><code># Build the bigram and trigram models
bigram = gensim.models.Phrases(t_preprocess(dataset.data), min_count=5, threshold=100)
bigram_mod = gensim.models.phrases.Phraser(bigram) 

def make_bigrams(texts):
   return [bigram_mod[doc] for doc in texts]

data_words_bigrams = make_bigrams(t_preprocess(dataset.data))

# Create Dictionary
id2word = corpora.Dictionary(data_words_bigrams)

# Create Corpus
texts = data_words_bigrams

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

mallet_path='/home/riteshjain/anaconda3/mallet/mallet2.0.8/bin/mallet' 
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path,corpus=corpus, num_topics=12, id2word=id2word, random_seed = 0)

coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=texts, dictionary=id2word, coherence='c_v')

a = ""When Honda builds a hybrid, you've got to be sure it‚Äö√Ñ√¥s a marvel. And an Accord Hybrid is when technology surpasses the known and takes a leap of faith into tomorrow. This is the next generation Accord, the ninth generation to be precise.""
</code></pre>

<p>How to use this text (a) to get its topic from the pretrained model. Please help.</p>
","11156911","","","","","2019-04-23 23:48:08","How to predict test data on Gensim Topic modelling","<python><jupyter-notebook><gensim><topic-modeling><mallet>","1","0","1","","","CC BY-SA 4.0"
"37930925","1","","","2016-06-20 20:04:51","","5","367","<p>I am using Gensim to train sentences with size 4 and I have 1192 unique words in training dataset. Number of words in model len(model.vocab) is 141 though that does not make sense. Is there any reason for seeing this? How I can change them model to have a key for every word in the training?
model = Word2Vec(windows,min_count=1)</p>
","3284804","","","","","2017-05-20 21:23:56","number of vocabulary in gensim is much lower than the ones in training data","<gensim><word2vec>","0","1","","","","CC BY-SA 3.0"
"55262684","1","","","2019-03-20 14:06:52","","0","82","<p>When working with a sparse matrix, it abruptly kills the kernel and exit code 139.
This happened when working with Gensim, which uses the sparse matrix format.</p>
<p>The failure happens when multiplying the matrix with another matrix, or even when using <code>matrix.sum()</code>.</p>
<p>the matrix was created using scipy:</p>
<pre><code>matrix = scipy.sparse.csc_matrix((data, indices, indptr), shape=(num_terms, num_docs), dtype=dtype)
</code></pre>
","8077762","","7274239","","2021-05-08 17:12:24","2021-05-08 17:12:24","sparse matrix causes Segmentation fault exit code 139","<scipy><segmentation-fault><sparse-matrix><gensim><exit-code>","1","0","","","","CC BY-SA 4.0"
"22121028","1","","","2014-03-01 22:08:11","","38","28332","<p>I have a word2vec model in gensim trained over 98892 documents. For any given sentence that is not present in the sentences array (i.e. the set over which I trained the model), I need to update the model with that sentence so that querying it next time gives out some results. I am doing it like this:</p>

<pre><code>new_sentence = ['moscow', 'weather', 'cold']
model.train(new_sentence)
</code></pre>

<p>and its printing this as logs:</p>

<pre><code>2014-03-01 16:46:58,061 : INFO : training model with 1 workers on 98892 vocabulary and 100 features
2014-03-01 16:46:58,211 : INFO : reached the end of input; waiting to finish 1 outstanding jobs
2014-03-01 16:46:58,235 : INFO : training on 10 words took 0.1s, 174 words/s
</code></pre>

<p>Now, when I query with similar new_sentence for most positives (as <code>model.most_similar(positive=new_sentence)</code>) it gives out error:</p>

<pre><code>Traceback (most recent call last):
 File ""&lt;pyshell#220&gt;"", line 1, in &lt;module&gt;
 model.most_similar(positive=['moscow', 'weather', 'cold'])
 File ""/Library/Python/2.7/site-packages/gensim/models/word2vec.py"", line 405, in most_similar
 raise KeyError(""word '%s' not in vocabulary"" % word)
  KeyError: ""word 'cold' not in vocabulary""
</code></pre>

<p>Which indicates that the word 'cold' is not part of the vocabulary over which i trained the thing (am I right)?</p>

<p>So the question is: How to update the model so that it gives out all the possible similarities for the given new sentence?</p>
","2480542","","","","","2019-09-25 01:49:40","Update gensim word2vec model","<gensim><word2vec>","6","1","10","","","CC BY-SA 3.0"
"55288724","1","55464300","","2019-03-21 20:24:44","","5","6120","<p>I'm getting an error while trying to access gensims mallet in jupyter notebooks.  I have the specified file 'mallet' in the same folder as my notebook, but cant seem to access it.  I tried routing to it from the C drive but I still get the same error.  Please help :)</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>import os
from gensim.models.wrappers import LdaMallet

#os.environ.update({'MALLET_HOME':r'C:/Users/new_mallet/mallet-2.0.8/'})

mallet_path = 'mallet' # update this path

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=bow_corpus, num_topics=20, id2word=dictionary)

result = (ldamallet.show_topics(num_topics=3, num_words=10,formatted=False))
for each in result:
    print (each)</code></pre>
</div>
</div>
</p>

<p><a href=""https://i.stack.imgur.com/xcjPF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xcjPF.png"" alt=""Mallet Error CalledProcessError""></a></p>

<p><a href=""https://i.stack.imgur.com/DZJFr.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DZJFr.png"" alt=""enter image description here""></a></p>
","11192516","","11192516","","2019-04-01 16:12:19","2021-09-25 09:07:46","Gensim mallet CalledProcessError: returned non-zero exit status","<python><windows><jupyter-notebook><gensim><mallet>","9","5","1","","","CC BY-SA 4.0"
"47441798","1","","","2017-11-22 18:40:07","","4","295","<p>I am trying to use Gensim with Glove instead of word2vec. To make the shape of Glove compatible with Gensim and use it, I am using the following lines of code:</p>

<pre><code>import gensim
from gensim.scripts.glove2word2vec import glove2word2vec
glove_in = 'glove.840B.300d.txt'
word2vec_format_out = 'glove.840B.300d.txt.word2vec'
glove2word2vec(glove_in, word2vec_format_out)
model =
gensim.models.KeyedVectors.load_word2vec_format(word2vec_format_out,
encoding='utf-8', binary=True)
</code></pre>

<p>However, this last line of code gives the following error: </p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbd in position 0:
invalid start byte
</code></pre>

<p>I have tried to open Glove first and then writing as a csv file, then re-open specifying encoding='utf-8'. I also tried several other things mentioned here, but the error keeps coming back. Does anyone know a solution for this?</p>
","8612523","","8612523","","2019-04-09 17:12:23","2019-04-09 17:12:23","Why does using Gensim with Glove continue to give a 'utf-8' UnicodeDecodeError?","<python-3.x><utf-8><gensim>","0","0","","","","CC BY-SA 4.0"
"17310933","1","19530686","","2013-06-26 03:13:39","","17","11117","<p>I've derived a LDA topic model using a toy corpus as follows:</p>

<pre><code>documents = ['Human machine interface for lab abc computer applications',
             'A survey of user opinion of computer system response time',
             'The EPS user interface management system',
             'System and human system engineering testing of EPS',
             'Relation of user perceived response time to error measurement',
             'The generation of random binary unordered trees',
             'The intersection graph of paths in trees',
             'Graph minors IV Widths of trees and well quasi ordering',
             'Graph minors A survey']

texts = [[word for word in document.lower().split()] for document in documents]
dictionary = corpora.Dictionary(texts)

id2word = {}
for word in dictionary.token2id:    
    id2word[dictionary.token2id[word]] = word
</code></pre>

<p>I found that when I use a small number of topics to derive the model, Gensim yields a full report of topical distribution over all potential topics for a test document. E.g.:</p>

<pre><code>test_lda = LdaModel(corpus,num_topics=5, id2word=id2word)
test_lda[dictionary.doc2bow('human system')]

Out[314]: [(0, 0.59751626959781134),
(1, 0.10001902477790173),
(2, 0.10001375856907335),
(3, 0.10005453508763221),
(4, 0.10239641196758137)]
</code></pre>

<p>However when I use a large number of topics, the report is no longer complete:</p>

<pre><code>test_lda = LdaModel(corpus,num_topics=100, id2word=id2word)

test_lda[dictionary.doc2bow('human system')]
Out[315]: [(73, 0.50499999999997613)]
</code></pre>

<p>It seems to me that topics with a probability less than some threshold (I observed 0.01 to be more specific) are omitted form the output.</p>

<p>I'm wondering if this behaviour is due to some aesthetic considerations? And how can I get the distribution of the probability mass residual over all other topics?</p>

<p>Thank you for your kind answer! </p>
","735445","","","","","2015-09-18 12:22:05","Document topical distribution in Gensim LDA","<python><lda><gensim>","2","3","4","","","CC BY-SA 3.0"
"55298609","1","","","2019-03-22 11:26:20","","0","300","<p>I am experiencing a problem when using a pre-trained fasttext.bin model (retreived from  <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a>). Checking most_similar for in-vocabulary-words returns sensible responses. However, when checking most_similar for an out-of-vocabulary word that only differs one character returns gibberish. </p>

<p>My question: Has this something to do with the model, or am I using it in the wrong way?</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.wrappers import FastText
model = FastText.load_fasttext_format('cc.en.300.bin')
model.most_similar(""universitet"")
[('Universitet', 0.8522759675979614),
 ('h√∂gskolan', 0.677900493144989),
 ('H√∂gskola', 0.6725144386291504),
 ('h√∂gskola', 0.6724666357040405),
 ('H√∂gskolan', 0.6600401997566223),
 ('Universitetet', 0.6519213318824768),
 ('H√∏gskolen', 0.647462010383606),
 ('Universiteti', 0.6399329900741577),
 ('forskning', 0.617483377456665),
 ('spr√•k', 0.6172543168067932)]

model.most_similar(""universitett"")
[('ESTATERETAILCONSUMERPHONESCARSBIKESAPPSINTERNETTABLETSCOMPUTERSSOCIETYPOLITICSLAWCRIMEENVIRONMENTSCIENCEARTSCELEBRITIESSPORTSSPECIALSFIRST',
  0.47905537486076355),
 ('Wikipedia-Page-Suzannah-B-Troy-6-yrs-after-Misogynist-Cyber-Vandalism-Censorship-via-Deletion-on-a-page-about-Censorship-Wikipedia-Agrees-to-retur',
  0.47733378410339355),
 ('DEky4M0BSpUOTPnSpkuL5I0GTSnRI4jMepcaFAoxIoFnX5kmJQk1aYvr2odGBAAIfkECQoABAAsCQAAABAAEgAACGcAARAYSLCgQQEABBokkFAhAQEQHQ4EMKCiQogRCVKsOOAiRocbLQ7EmJEhR4cfEWoUOTFhRIUNE44kGZOjSIQfG9rsyDCnzp0AaMYMyfNjS6JFZWpEKlDiUqALJ0KNatKmU4NDBwYEACH5BAkKAAQALAkAAAAQABIAAAhpAAEQGEiQIICDBAUgLEgAwICHAgkImBhxoMOHAyJOpGgQY8aBGxV2hJgwZMWLFTcCUIjwoEuLBym69PgxJMuDNAUqVDkz50qZLi',
  0.474983274936676),
 ('DEky4M0BSpUOTPnSpkuL5I0GTSnRI4jMepcaFAoxIoFnX5kmJQk1aYvr2odGBAAIfkECQoABAAsCQAAABAAEgAACGcAARAYSLCgQQEABBokkFAhAQEQHQ4EMKCiQogRCVKsOOAiRocbLQ7EmJEhR4cfEWoUOTFhRIUNE44kGZOjSIQfG9rsyDCnzp0AaMYMyfNjS6JFZWpEKlDiUqALJ0KNatKmU4NDBwYEACH5BAUKAAQALAkAAAAQABIAAAhpAAEQGEiQIICDBAUgLEgAwICHAgkImBhxoMOHAyJOpGgQY8aBGxV2hJgwZMWLFTcCUIjwoEuLBym69PgxJMuDNAUqVDkz50qZLi',
  0.47364047169685364),
 ('crescendosexibloguerobateyabsorbersexiindesignabledinerolatifundiosexibrezarcularsutesexirapoplinbrezarcorrentosoVd.lazadareflejoreglafeministabrezarchuzasexiouttiqueblogueroin',
  0.47090965509414673),
 ('QQFZAAEACwAAAAAGQASAAAIjgAJCBQIoGDBgQgTKiwooGHDgwshDgTgsOLDhAAGaAQwUYBBhx85EtS4cWLGjR5JSjxZkgDFkwwLohTJUqTLlANiwvQ4seVNjwwfBoVokKjFo0Jlksz506NFiklZtoQKFSjIoktLVv1YsahSn1WP0vzq02VYoAjJMsVYVKHZrDbdupW6Vq5cunHtRjQoMCAAIfkECRQABAAsCQADAAQABAAACAsABQgkILCgwYEBAQAh',
  0.46747487783432007),
 ('–∑–∞–ø–∏—Å–∏–¢–µ–ª–µ–ø—Ä–æ–≥—Ä–∞–º–º–∞VikerraadioOtseEsilehtJ√§relkuulamineSaatekavaPodcastidRaadioteaterRaadio',
  0.4659830331802368),
 ('deblogueroreflejoantecedentesexitlacuachebateysuteindesignableabsorbersexilatifundiosexibrezarsutemulti√©tnicosexiplinrapobrezarcorrentosoVd.lazadafisiochillidomabrezarsico-chuzaoutcolodrablogueroin',
  0.46159273386001587),
 ('2OtseEsilehtJ√§relkuulamineSaatedPodcastidKlassikaraadioOtseEsilehtJ√§relkuulamineSaatekavaPodcastidRaadio',
  0.4609595537185669),
 ('leilighetEiendomstypeSelveierleilighetPlass', 0.4550461769104004)]
</code></pre>
","8867146","","","","","2019-03-22 11:44:46","Pretrained Fasttext model returns gibberish for out-of-vocabulary words","<python><gensim><fasttext>","1","5","","","","CC BY-SA 4.0"
"55302607","1","","","2019-03-22 15:08:47","","0","211","<p>I am using a pre-trained  doc2vec model, when I try to find out most similar document to that of my sample document. It gives me  unsupported operand type(s) error.</p>

<pre><code>from gensim.models import Doc2Vec

filename = ""doc2vec.bin""
doc1 =[""This is a sample document.""]

model = Doc2Vec.load(filename)

inferred_vector = model.infer_vector(doc1)

sims = model.docvecs.most_similar(positive=[inferred_vector],topn=1)

print(sims)
</code></pre>

<p>This gives me following error</p>

<pre><code> File ""D:\doc2vectest.py"", line 10, in &lt;module&gt;
    sims = model.docvecs.most_similar(positive=[inferred_vector],topn=1)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 1667, in most_similar
    self.init_sims()
  File ""C:\Users\admin\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 1630, in init_sims
    self.vectors_docs_norm = _l2_norm(self.vectors_docs, replace=replace)
  File ""C:\Users\admin\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 2346, in _l2_norm
    dist = sqrt((m ** 2).sum(-1))[..., newaxis]
TypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'
</code></pre>
","8269023","","","","","2019-03-25 03:05:16","Gensim doc2vec most similar gives unsupported operand type(s) error","<machine-learning><nlp><gensim><word-embedding><doc2vec>","1","0","","","","CC BY-SA 4.0"
"55238328","1","","","2019-03-19 10:07:17","","0","83","<p>I've been trying to implement an embedding layer using gensim's word2vec. I have loaded my data using pandas, my data is text type, when it comes to the word2vec part:</p>

<pre><code>embedding_weights = train_word2vec(y_train, vocab['w2idx'], 
num_features=embedding_dim, min_word_count=min_word_count, context=context)
input_shape = (sequence_length,)
model_input = Input(shape=input_shape)
layer = Embedding(len(vocab['idx2w']), embedding_dim,
input_length=sequence_length, name=""embedding"")(model_input)
layer = Dropout(dropout_prob[0])(layer)
</code></pre>

<p>I keep getting this error:</p>

<pre><code>   File ""&lt;ipython-input-9-423d0e432e5b&gt;"", line 3, in &lt;module&gt;
   min_word_count=min_word_count, context=context)

   File ""C:\Users\ACER\Pod_Dsgn_Chatbot\Wor2vec.py"", line 43, in 
   train_word2vec
   for key, word in vocabulary_inv.items()}

   File ""C:\Users\ACER\Pod_Dsgn_Chatbot\Wor2vec.py"", line 43, in &lt;dictcomp&gt;
   for key, word in vocabulary_inv.items()}

   File ""C:\Users\ACER\Anaconda3\envs\py37\lib\site- 
   packages\gensim\utils.py"", line 1398, in new_func1
    return func(*args, **kwargs)

   File ""C:\Users\ACER\Anaconda3\envs\py37\lib\site- 
    packages\gensim\models\word2vec.py"", line 821, in __getitem__
     return self.wv.__getitem__(words)
   File ""C:\Users\ACER\Anaconda3\envs\py37\lib\site- 
   packages\gensim\models\keyedvectors.py"", line 171, in __getitem__
   return vstack([self.get_vector(entity) for entity in entities])

   TypeError: 'int' object is not iterable
</code></pre>

<p>i have no idea how to fix it, and there isn't much information on the internet, also the error  is related to the first line of the shown code. Finally note that i cleaned my data, and i removed every integer, thank you </p>

<p>Edit: And this is the function where the problem happens, Ps: i didn't develop this one </p>

<pre><code>def train_word2vec(sentence_matrix, vocabulary_inv,
               num_features=300, min_word_count=1, context=10):

model_dir = 'models'
model_name = ""{:d}features_{:d}minwords_{:d}context"".format(num_features, 
 min_word_count, context)
model_name = join(model_dir, model_name)
if exists(model_name):
    embedding_model = word2vec.Word2Vec.load(model_name)
    print('Load existing Word2Vec model \'%s\'' % split(model_name)[-1])
else:

    num_workers = 2   
    downsampling = 1e-3   


    print('Training Word2Vec model...')
    sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]
    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,
    size=num_features, min_count=min_word_count, window=context, 
    sample=downsampling)
    embedding_model.init_sims(replace=True)
    if not exists(model_dir):
        os.mkdir(model_dir)
    print('Saving Word2Vec model \'%s\'' % split(model_name)[-1])
    embedding_model.save(model_name)
    pyplot.scatter( embedding_model)
    embedding_weights = {key: embedding_model[word] if word in 
     embedding_model 
    else
                          np.random.uniform(-0.25, 0.25, 
      embedding_model.vector_size)
                     for key, word in vocabulary_inv.items()}
   return embedding_weights
</code></pre>
","11077864","","7067333","","2019-03-21 20:19:11","2019-03-21 20:19:11","Gensim/word2vec error: embedding layer error","<python><machine-learning><keras><gensim><word2vec>","0","3","","","","CC BY-SA 4.0"
"55304262","1","","","2019-03-22 16:44:49","","1","962","<p>I would like to tokenize a list of strings according to my self-defined dictionary.</p>

<p>The list of string looks like this:</p>

<pre><code>lst = ['vitamin c juice', 'organic supplement'] 
</code></pre>

<p>The self-defined dictionary: </p>

<pre><code>dct = {0: 'organic', 1: 'juice', 2: 'supplement', 3: 'vitamin c'}
</code></pre>

<p>My expected result:</p>

<p>vitamin c juice --> <code>[(3,1), (1,1)]</code>
organic supplement --> <code>[(0,1), (2,1)]</code></p>

<p>My current code:</p>

<pre><code>import gensim
import gensim.corpora as corpora
from gensim.utils import tokenize
dct = corpora.Dictionary([list(x) for x in tup_list]) 
corpus = [dct.doc2bow(text) for text in [s for s in lst]]
</code></pre>

<p>The error message I got is <code>TypeError: doc2bow expects an array of unicode tokens on input, not a single string</code> However, I do not want to simply tokenize ""vitamin c"" as <code>vitamin</code> and <code>c</code>. Instead, I want to tokenize based on my existing <code>dct</code> words. That is to say, it should be <code>vitamin c</code>.</p>
","8457035","","","","","2019-03-25 21:32:33","tokenize string based on self-defined dictionary","<python><nlp><nltk><tokenize><gensim>","2","0","","","","CC BY-SA 4.0"
"55309197","1","55330745","","2019-03-22 23:51:46","","3","3833","<p>I have a dataset of 6000 observations; a sample of it is the following:</p>

<pre><code>job_id      job_title                                           job_sector
30018141    Secondary Teaching Assistant                        Education
30006499    Legal Sales Assistant / Executive                   Sales
28661197    Private Client Practitioner                         Legal
28585608    Senior hydropower mechanical project manager        Engineering
28583146    Warehouse Stock Checker - Temp / Immediate Start    Transport &amp; Logistics
28542478    Security Architect Contract                         IT &amp; Telecoms
</code></pre>

<p>The goal is to predict the job sector of each row based on the job title.</p>

<p>Firstly, I apply some preprocessing on the <code>job_title</code> column:</p>

<pre><code>def preprocess(document):
    lemmatizer = WordNetLemmatizer()
    stemmer_1 = PorterStemmer()
    stemmer_2 = LancasterStemmer()
    stemmer_3 = SnowballStemmer(language='english')

    # Remove all the special characters
    document = re.sub(r'\W', ' ', document)

    # remove all single characters
    document = re.sub(r'\b[a-zA-Z]\b', ' ', document)

    # Substituting multiple spaces with single space
    document = re.sub(r' +', ' ', document, flags=re.I)

    # Converting to lowercase
    document = document.lower()

    # Tokenisation
    document = document.split()

    # Stemming
    document = [stemmer_3.stem(word) for word in document]

    document = ' '.join(document)

    return document

df_first = pd.read_csv('../data.csv', keep_default_na=True)

for index, row in df_first.iterrows():

    df_first.loc[index, 'job_title'] = preprocess(row['job_title'])
</code></pre>

<p>Then I do the following with <code>Gensim</code> and <code>Doc2Vec</code>:</p>

<pre><code>X = df_first.loc[:, 'job_title'].values
y = df_first.loc[:, 'job_sector'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)

tagged_train = TaggedDocument(words=X_train.tolist(), tags=y_train.tolist())
tagged_train = list(tagged_train)

tagged_test = TaggedDocument(words=X_test.tolist(), tags=y_test.tolist())
tagged_test = list(tagged_test)

model = Doc2Vec(vector_size=5, min_count=2, epochs=30)

training_set = [TaggedDocument(sentence, tag) for sentence, tag in zip(X_train.tolist(), y_train.tolist())]

model.build_vocab(training_set)

model.train(training_set, total_examples=model.corpus_count, epochs=model.epochs)   

test_set = [TaggedDocument(sentence, tag) for sentence, tag in zip(X_test.tolist(), y_test.tolist())]

predictors_train = []
for sentence in X_train.tolist():

    sentence = sentence.split()
    predictor = model.infer_vector(doc_words=sentence, steps=20, alpha=0.01)

    predictors_train.append(predictor.tolist())

predictors_test = []
for sentence in X_test.tolist():

    sentence = sentence.split()
    predictor = model.infer_vector(doc_words=sentence, steps=20, alpha=0.025)

    predictors_test.append(predictor.tolist())

sv_classifier = SVC(kernel='linear', class_weight='balanced', decision_function_shape='ovr', random_state=0)
sv_classifier.fit(predictors_train, y_train)

score = sv_classifier.score(predictors_test, y_test)
print('accuracy: {}%'.format(round(score*100, 1)))
</code></pre>

<p>However, the result which I am getting is 22% accuracy.</p>

<p>This makes me a lot suspicious especially because by using the <code>TfidfVectorizer</code> instead of the <code>Doc2Vec</code> (both with the same classifier) then I am getting 88% accuracy (!).</p>

<p>Therefore, I guess that I must be doing something wrong in how I apply the <code>Doc2Vec</code> of <code>Gensim</code>.</p>

<p>What is it and how can I fix it?</p>

<p>Or it it simply that my dataset is relatively small while more advanced methods such as word embeddings etc require way more data?</p>
","9024698","","9024698","","2019-03-25 09:32:48","2019-09-10 14:38:20","Doc2Vec & classification - very poor results","<python><classification><gensim><text-classification><doc2vec>","3","0","1","","","CC BY-SA 4.0"
"64740901","1","64740991","","2020-11-08 17:14:15","","1","101","<p>I want to know that is the best topic number (k) to feed to gensim for LDA, I've found an answer on StackOverflow. However, I got an error mentioned below.</p>
<p>Here is the link to the suggested way to feed the number of the optimal topics that I've found.</p>
<p><a href=""https://stackoverflow.com/questions/32313062/what-is-the-best-way-to-obtain-the-optimal-number-of-topics-for-a-lda-model-usin"">What is the best way to obtain the optimal number of topics for a LDA-Model using Gensim?</a></p>
<pre><code># import modules 

import seaborn as sns
import matplotlib.pyplot as plt
from gensim.models import LdaModel, CoherenceModel
from gensim import corpora

# make models with n k

dirichlet_dict = corpora.Dictionary(corpus)
bow_corpus = [dirichlet_dict.doc2bow(text) for text in corpus]

# Considering 1-15 topics, as the last is cut off
num_topics = list(range(16)[1:])
num_keywords = 15

LDA_models = {}
LDA_topics = {}
for i in num_topics:
    LDA_models[i] = LdaModel(corpus=bow_corpus,
                             id2word=dirichlet_dict,
                             num_topics=i,
                             update_every=1,
                             chunksize=len(bow_corpus),
                             passes=20,
                             alpha='auto',
                             random_state=42)

    shown_topics = LDA_models[i].show_topics(num_topics=num_topics, 
                                             num_words=num_keywords,
                                             formatted=False)
    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]
</code></pre>
<p>When I try to implent the code i got this error:</p>
<pre><code>-&gt; 1145         if num_topics &lt; 0 or num_topics &gt;= self.num_topics:
   1146             num_topics = self.num_topics
   1147             chosen_topics = range(num_topics)

TypeError: '&lt;' not supported between instances of 'list' and 'int'
</code></pre>
","10152208","","","","","2020-11-08 17:22:55","How to define the optimal number of topics (k)?","<python><python-3.x><gensim>","1","0","","","","CC BY-SA 4.0"
"55335354","1","","","2019-03-25 10:07:38","","3","512","<p>I've been using doc2vec in the most basic way so far with limited success. I'm able to find similar documents however often I get a lot of false positives.   My primary goal is to build a classification algorithm for user requirements. This is to help with user requirement analysis and search.</p>

<p>I know this is not really a large enough dataset so there are a few questions I'd like help with:</p>

<ol>
<li>How can a train on one set of documents and build vectors on another?</li>
<li>How do I go about tuning the model, specifically selecting the right number of dimensions for the vector space</li>
<li>How can I create a Hierarchical Clustering for the word vectors, should a do this with one model or should I create separate word and document classification models?</li>
<li>I don't have ground truth, this is unsupervised learning when tuning how do I measure the quality of the result?</li>
<li>And finally, are there any recommended online resource that might cover some of the above.</li>
</ol>

<p>I've been calling train once with 100 vectors on 2000 documents, each with about 100 words, each document has 22 columns which are tagged by both cell and row.</p>

<pre><code>def tag_dataframe(df, selected_cols):
    tagged_cells = []
    headers = list(df.columns.values)
    for index, row in df.iterrows():
        row_tag = 'row_' + str(index)
        for col_name in headers:
            if col_name in selected_cols:
                col_tag = 'col_' + col_name
                cell_tag = 'cell_' + str(index) + '_' + col_name
                cell_val = str(row[col_name])
                if cell_val == 'nan':
                    continue
                cleaned_text = clean_str(cell_val)
                if len(cleaned_text) == 0:
                    continue
                tagged_cells.append(
                    gensim.models.doc2vec.TaggedDocument(
                        cleaned_text,
                        [row_tag, cell_tag]))
    print('tagged rows')
    return tagged_cells

def load_or_build_vocab(model_path, tagged_cells):
    if os.path.exists(model_path):
        print('Loading vocab')
        d2vm = gensim.models.Doc2Vec.load(model_path)
    else:
        print('building vocab')
        d2vm = gensim.models.Doc2Vec(
            vector_size=100,
            min_count=0,
            alpha=0.025,
            min_alpha=0.001)
        d2vm.build_vocab(tagged_cells)
        print('    built')
        d2vm.save(model_path)
    return d2vm

def load_or_train_model(model_path, d2vm, tagged_cells):
    if os.path.exists(model_path):
        print('Loading Model')
        d2vm = gensim.models.Doc2Vec.load(model_path)
    else:
        print('Training Model')
        d2vm.train(
            tagged_cells,
            total_examples=len(tagged_cells),
            epochs=100)
        print('    trained')
        d2vm.save(model_path)
    return d2vm
</code></pre>

<p>What I hope to achieve is a set of document vectors which will help with finding similar user requirements from a free text and a Hierarchical Clustering to build navigation of the existing requirements.</p>
","1731392","","355230","","2019-03-25 10:54:21","2019-03-25 15:57:29","Doc2vec beyond beginner guidance","<python><dataframe><gensim><doc2vec>","1","1","","","","CC BY-SA 4.0"
"52364632","1","","","2018-09-17 09:30:43","","4","2879","<p>According to <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">https://code.google.com/archive/p/word2vec/</a>: </p>

<blockquote>
  <p>It was recently shown that the word vectors capture many linguistic
  regularities, for example vector operations vector('Paris') -
  vector('France') + vector('Italy') results in a vector that is very
  close to vector('Rome'), and vector('king') - vector('man') +
  vector('woman') is close to vector('queen') [3, 1]. You can try out a
  simple demo by running demo-analogy.sh.</p>
</blockquote>

<p>So we can try from the supplied demo script:</p>

<pre><code>+ ../bin/word-analogy ../data/text8-vector.bin
Enter three words (EXIT to break): paris france berlin

Word: paris  Position in vocabulary: 198365

Word: france  Position in vocabulary: 225534

Word: berlin  Position in vocabulary: 380477

                                              Word              Distance
------------------------------------------------------------------------
                                           germany      0.509434
                                          european      0.486505
</code></pre>

<p>Please note that <code>paris france berlin</code> is the input hint the demo suggest. The problem is that I'm unable to reproduce this behavior if I open the same word vectors in <code>Gensim</code> and try to compute the vectors myself. For example:</p>

<pre><code>&gt;&gt;&gt; word_vectors = KeyedVectors.load_word2vec_format(BIGDATA, binary=True)
&gt;&gt;&gt; v = word_vectors['paris'] - word_vectors['france'] + word_vectors['berlin']
&gt;&gt;&gt; word_vectors.most_similar(np.array([v]))
[('berlin', 0.7331711649894714), ('paris', 0.6669869422912598), ('kunst', 0.4056406617164612), ('inca', 0.4025722146034241), ('dubai', 0.3934606909751892), ('natalie_portman', 0.3909246325492859), ('joel', 0.3843030333518982), ('lil_kim', 0.3784593939781189), ('heidi', 0.3782389461994171), ('diy', 0.3767407238483429)]
</code></pre>

<p>So, what is the word analogy actually doing? How should I reproduce it?</p>
","1433971","","1433971","","2018-09-17 09:39:55","2018-09-17 20:09:53","What is the operation behind the word analogy in Word2vec?","<python><gensim><word2vec><word-embedding>","2","0","","","","CC BY-SA 4.0"
"61123616","1","","","2020-04-09 14:34:02","","1","471","<p>I'm trying to estimate the cosine similarity between <em>each</em> document <code>i</code> in a Corpus <code>A</code> and <em>all</em> documents in a Corpus <code>B</code>.</p>

<p>Any idea how I can do this efficiently? I'm working with pretty large datasets.</p>

<p>Essentially, I want to get the document(s) in Corpus <code>B</code> which is (are) <em>most similar</em> for each document within Corpus <code>A</code>.</p>
","9414906","","","","","2020-04-10 14:33:33","How to compute cosine similarity between 2 different CORPUSES?","<python><nlp><nltk><spacy><gensim>","2","3","1","","","CC BY-SA 4.0"
"54863236","1","","","2019-02-25 09:40:00","","2","9043","<p>I am trying to extract Indonesia titles from a wiki titles dump that's in a text file using word2vec-gensim in Python 3. The wiki dump contains titles in other languages also and some symbols. Below is my code:</p>

<pre><code>    if len(sys.argv) != 3: 
    namaFileInput = ""idwiki-latest-pages-articles.xml.bz2""
    namaFileOutput = ""wiki.id.case.text""
    sys.exit(1)
inp, outp = sys.argv[1:3]
space = "" ""
i = 0

output = open(namaFileOutput, 'w')

# lower=False: huruf kecil dan besar dibedakan
wiki = WikiCorpus(namaFileInput, lemmatize=False, dictionary={}, lower=False)
for text in wiki.get_texts():
    if six.PY3:
        output.write(b' '.join(text).encode('utf-8') + '\n')
    else:
        output.write(space.join(text) + ""\n"")
    i = i + 1
    if i % 10000 == 0:
        logger.info(""Saved "" + str(i) + "" articles"")

output.close()
logger.info(""Finished Saved "" + str(i) + "" articles"")
</code></pre>

<p>But I am getting and error:</p>

<pre><code>    TypeError                                 Traceback (most recent call last)
&lt;ipython-input-17-d4c686a9093a&gt; in &lt;module&gt;
     29 for text in wiki.get_texts():
     30     if six.PY3:
---&gt; 31         output.write(b' '.join(text).encode('utf-8') + '\n')
     32     else:
     33         output.write(space.join(text) + ""\n"")

TypeError: sequence item 0: expected a bytes-like object, str found
</code></pre>

<p>I have searched online but could not succeed. Any help will be appreciated.</p>
","8881708","","","","","2019-02-25 18:56:44","TypeError: sequence item 0: expected a bytes-like object, str found","<gensim><word2vec><python-3.7>","1","0","2","","","CC BY-SA 4.0"
"55230344","1","55284554","","2019-03-18 21:32:53","","0","100","<p>I'm building a LDA in python using Gensim and I'm struggling to increase the number of words printed per topic from the default of 10.  I'd like 20 topics with 30 words each.  Any advice would be greatly appreciated :)</p>

<pre><code># train the LDA model

lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)

# check out the topics

for idx, topic in lda_model.print_topics(-1):
   print('Topic: {} \nWords: {}'.format(idx, topic))
</code></pre>
","11192516","","2314737","","2019-04-12 22:23:37","2019-04-12 22:23:37","Add words per topic LDA","<python><windows><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"29676413","1","29756701","","2015-04-16 13:35:30","","1","314","<p>I am using the Doc2vec class from the gensim framework to compute the vectorial representation of each document in a corpus.</p>

<p>The corpus contains very short sentences, they can have even one word. I observed that for many sentences, especially the short ones, Doc2vec does not provide any representations. Could someone explain the reasons for this?</p>
","4576527","","4564945","","2015-04-20 19:49:30","2015-04-20 19:49:30","Missing sentences from the Doc2vec representation","<gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"65027694","1","65028926","","2020-11-26 18:45:04","","0","189","<p>First, I want to explain my task. I have a dataset of 300k documents with an average of 560 words (no stop word removal yet) 75% in German, 15% in English and the rest in different languages. The goal is to recommend similar documents based on an existing one. At the beginning I want to focus on the German and English documents. ¬†</p>
<p>To achieve this goal I looked into several methods on feature extraction for document similarity, especially the word embedding methods have impressed me because they are context aware in contrast to simple TF-IDF feature extraction and the calculation of cosine similarity.¬†</p>
<p>I'm overwhelmed by the amount of methods I could use and I haven't found a proper evaluation of those methods yet. I know for sure that the size of my documents are too big for BERT, but there is FastText, Sent2Vec, Doc2Vec and the Universal Sentence Encoder from Google. My favorite method based on my research is Doc2Vec even though there aren't any or old pre-trained models which means I have to do the training on my own.</p>
<p>Now that you know my task and goal, I have the following questions:</p>
<ul>
<li>Which method should I use for feature extraction based on the rough overview of my data?</li>
<li>My dataset is too small to train Doc2Vec on it. Do I achieve good results if I train the model on English / German Wikipedia?¬†</li>
</ul>
","11469656","","","","","2020-11-26 20:36:09","Which document embedding model for document similarity","<python><gensim><word-embedding><doc2vec><fasttext>","1","2","","","","CC BY-SA 4.0"
"64733334","1","","","2020-11-07 23:14:03","","-1","47","<p>Using this command it is possible to present 10 topics.</p>
<pre><code>ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=32, id2word=id2word)
pprint(ldamallet.show_topics(formatted=False))
</code></pre>
<p>How is it possible to print all topics and more than 10 words per topic?</p>
","14544121","","","","","2020-11-08 06:42:41","print all topic in the print version","<python><gensim>","1","2","","","","CC BY-SA 4.0"
"43668207","1","43756278","","2017-04-27 21:28:16","","2","1622","<p>I am unable to understand how to print the output for the below code</p>

<pre><code># make gensim dictionary and corpus
dictionary = gensim.corpora.Dictionary(boc_texts)
corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]
tfidf = gensim.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
</code></pre>

<p>I want to print the keyphrases and their tfidf scores</p>

<p>Thank you</p>
","7661819","","","","","2017-05-03 09:34:11","How to print gensim dictionary and corpus","<python><nlp><gensim>","1","0","","","","CC BY-SA 3.0"
"57579009","1","","","2019-08-20 17:44:10","","0","649","<p>The following code when run gives the cosine distance between two words.</p>

<p>model.wv.distance('word1','word2')</p>

<p>How do I find the euclidean distance between two words?
I am using gensim for word2vec implementation</p>
","","user8562180","","","","2019-08-20 18:01:59","How to change the code to find the euclidean distance (not cosine) between words in a word2vec impementation?","<python><gensim><word2vec><similarity><euclidean-distance>","1","0","","","","CC BY-SA 4.0"
"55813659","1","55833900","","2019-04-23 14:37:45","","0","670","<p>I hava a pandas dataframe that has one column with conversational data. I preprocessed it in the following way:</p>

<pre><code>def preprocessing(text):
     return [word for word in simple_preprocess(str(text), min_len = 2, deacc = True) if word not in stop_words]

dataset['preprocessed'] = dataset.apply(lambda row: preprocessing(row['msgText']), axis = 1)
</code></pre>

<p>To make it one-dimensional I used (both):</p>

<pre><code>processed_docs = data['preprocessed']
</code></pre>

<p>as well as:</p>

<pre><code>processed_docs = data['preprocessed'].tolist()
</code></pre>

<p>Which now looks as follows:</p>

<pre><code>&gt;&gt;&gt; processed_docs[:2]
0    ['klinkt', 'alsof', 'zwaar', 'dingen', 'spelen...
1    ['waar', 'liefst', 'meedenk', 'betekenen', 'pe...
</code></pre>

<p>For both cases, I used: </p>

<pre><code>dictionary = gensim.corpora.Dictionary(processed_docs)     
</code></pre>

<p>However, in both cases I got the error:</p>

<pre><code>TypeError: doc2bow expects an array of unicode tokens on input, not a single string
</code></pre>

<p>How can I modify my data, so that I don't get this TypeError?</p>

<hr>

<hr>

<p>Given that similar questions have been asked before, I've considered:</p>

<p><a href=""https://stackoverflow.com/questions/33229360/gensim-typeerror-doc2bow-expects-an-array-of-unicode-tokens-on-input-not-a-si#33230682"">Gensim: TypeError: doc2bow expects an array of unicode tokens on input, not a single string</a></p>

<p>Based on the first answer, I tried the solution of:</p>

<pre><code>dictionary = gensim.corpora.Dictionary([processed_docs.split()])
</code></pre>

<p>And got the error(/s):</p>

<pre><code>AttributeError: 'Series'('List') object has no attribute 'split'
</code></pre>

<p>And in the second answer someone says that the input needs to be tokens, which already holds for me. </p>

<p>Furthermore, based on (<a href=""https://stackoverflow.com/questions/44352552/typeerror-doc2bow-expects-an-array-of-unicode-tokens-on-input-not-a-single-str?noredirect=1&amp;lq=1"">TypeError: doc2bow expects an array of unicode tokens on input, not a single string when using gensim.corpora.Dictionary()</a>), I used the <code>.tolist()</code> approach as I described above, which does not work either.</p>
","7714681","","","","","2020-11-26 17:30:41","How to input a series/list consisting of different tokens in a Gensim Dictionary?","<python><dictionary><nlp><typeerror><gensim>","2","1","","","","CC BY-SA 4.0"
"38556496","1","38588677","","2016-07-24 20:31:38","","0","4487","<p>I need to do some experiments on text files using gensim on mac Yosemite.</p>

<p>I've already installed <code>numpy</code> and <code>scipy</code> but when I want to import <code>gensim</code>.</p>

<p>I'm facing this error:</p>

<pre><code>from six.moves.queue import Queue as _Queue
ImportError: No module named queue
</code></pre>

<p>I upgraded <code>numpy</code> and <code>scipy</code> to latest version and Python is 2.7.10.</p>

<p>I read that the problem may be solved by hacking the <code>gensim</code> code to <code>from Queue import Queue as _Queue</code> but I don't know how!</p>

<p>Is there any other way?</p>
","6632503","","5595825","","2016-07-24 20:34:23","2016-11-17 06:41:10","gensim can not be imported because ImportError: No module named queue?","<python-2.7><queue><gensim>","3","0","","","","CC BY-SA 3.0"
"57457214","1","57458009","","2019-08-12 07:35:16","","1","1431","<p>I'm using LDA with gensim for topic modeling. My data has 23 documents and I want separate topics/words for each document but gensim is giving topics for entire set of documents together. How to get it for  individual docs?</p>

<pre><code>dictionary = corpora.Dictionary(doc_clean)

# Converting list of documents (corpus) into Document Term Matrix using 
#dictionary prepared above.

corpus = [dictionary.doc2bow(doc) for doc in doc_clean]


# Creating the object for LDA model using gensim library
Lda = gensim.models.ldamodel.LdaModel

# Running and Trainign LDA model on the document term matrix.
ldamodel = Lda(corpus, num_topics=3, id2word = dictionary, passes=50)

result=ldamodel.print_topics(num_topics=3, num_words=3)
</code></pre>

<p>This is the output I'm getting:</p>

<pre><code>[(0, '0.011*""plex"" + 0.010*""game"" + 0.009*""racing""'),
(1, '0.008*""app"" + 0.008*""live"" + 0.007*""share""'),
(2, '0.015*""device"" + 0.009*""file"" + 0.008*""movie""')]
</code></pre>
","7458186","","","","","2019-08-12 10:53:42","How can I print document wise topics in Gensim?","<python><nltk><gensim><lda><topic-modeling>","2","0","","","","CC BY-SA 4.0"
"15184655","1","22824387","","2013-03-03 10:20:55","","4","3197","<p><strong>How do I load an LDA transformed corpus from python's <code>gensim</code> ?</strong> What i've tried:</p>

<pre><code>from gensim import corpora, models
import numpy.random
numpy.random.seed(10)

doc0 = [(0, 1), (1, 1)]
doc1 = [(0,1)]
doc2 = [(0, 1), (1, 1)]
doc3 = [(0, 3), (1, 1)]

corpus = [doc0,doc1,doc2,doc3]
dictionary = corpora.Dictionary(corpus)

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
corpus_tfidf.save('x.corpus_tfidf')

# To access the tfidf fitted corpus i've saved i used corpora.MmCorpus.load()
corpus_tfidf = corpora.MmCorpus.load('x.corpus_tfidf')

lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)
corpus_lda = lda[corpus]
corpus_lda.save('x.corpus_lda')

for i,j in enumerate(corpus_lda):
  print j, corpus[i]
</code></pre>

<p>The above code will output:</p>

<pre><code>[(0, 0.54259038344543631), (1, 0.45740961655456358)] [(0, 1), (1, 1)]
[(0, 0.56718063124157458), (1, 0.43281936875842542)] [(0, 1)]
[(0, 0.54255407573666647), (1, 0.45744592426333358)] [(0, 1), (1, 1)]
[(0, 0.75229707773868093), (1, 0.2477029222613191)] [(0, 3), (1, 1)]

# [(&lt;topic_number_from x.corpus_lda model&gt;, 
#   &lt;probability of this topic for this document&gt;), 
#  (&lt;topic# from lda model&gt;, &lt;prob of this top for this doc&gt;)] [&lt;document[i] from corpus&gt;]
</code></pre>

<p><strong>If i want to load the saved LDA transformed corpus, which class from <code>gensim</code> should i be using to load?</strong></p>

<p>I have tried using <code>corpora.MmCorpus.load()</code>, it doesn't give me the same output of the transformed corpus as shown above:</p>

<pre><code>&gt;&gt;&gt; lda_corpus = corpora.MmCorpus.load('x.corpus_lda')
&gt;&gt;&gt; for i,j in enumerate(lda_corpus):
...   print j, corpus[i]
... 
[(0, 0.55087839240547309), (1, 0.44912160759452685)] [(0, 1), (1, 1)]
[(0, 0.56715974584850259), (1, 0.43284025415149735)] [(0, 1)]
[(0, 0.54275680271070581), (1, 0.45724319728929413)] [(0, 1), (1, 1)]
[(0, 0.75233330695720912), (1, 0.24766669304279079)] [(0, 3), (1, 1)]
</code></pre>
","610569","","610569","","2013-03-19 12:29:25","2014-04-02 22:34:32","Which gensim corpora class should I use to load an LDA transformed corpus? - Python","<python><nlp><corpus><lda><gensim>","2","0","","","","CC BY-SA 3.0"
"54870236","1","54873809","","2019-02-25 16:03:52","","0","222","<p>I tried to load gensim in my code. Often it works fine. Today, I get the following exception:</p>

<pre><code>Traceback (most recent call last):
  File ""/project/6008168/tamouze/just.py"", line 2, in &lt;module&gt;
    import gensim
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/__init__.py"", line 5, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/parsing/__init__.py"", line 4, in &lt;module&gt;
    from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/parsing/preprocessing.py"", line 40, in &lt;module&gt;
    from gensim import utils
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/utils.py"", line 44, in &lt;module&gt;
    from smart_open import smart_open
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/smart_open/__init__.py"", line 1, in &lt;module&gt;
    from .smart_open_lib import *
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 29, in &lt;module&gt;
    import requests
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/requests/__init__.py"", line 97, in &lt;module&gt;
    from . import utils
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/requests/utils.py"", line 26, in &lt;module&gt;
    from ._internal_utils import to_native_string
ImportError: cannot import name to_native_string
</code></pre>

<p>Im using python 2.7.14 and gensim 3.4.0.
How can I solve this problem?</p>
","2869180","","","","","2019-02-25 20:03:46","Exception during calling gensim?","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"6522580","1","","","2011-06-29 14:45:38","","2","3747","<p>Am attempting to use <code>[gensim][1]</code>, a Vector Space Modelling package for python in some Machine Learning experiments of mine. I followed their installation instructions as said <a href=""http://nlp.fi.muni.cz/projekty/gensim/install.html"" rel=""nofollow"">here</a>, though installation of <code>scipy</code> on my OpenSuse 11.3 failed when using <code>easy_install</code> as they recommend, so I resorted to installing it from a package available on the Official package search portal <a href=""http://software.opensuse.org/search?q=scipy&amp;baseproject=openSUSE:11.3&amp;lang=en&amp;exclude_debug=true"" rel=""nofollow"">here</a>.</p>

<p>Installation of <code>scipy</code> went ok, and then I used <code>easy_install</code> to install <code>gensim</code> as they recommend. Which also went ok.</p>

<p>Now, after installation, I attempted to run the very first example they give of using the package, which starts with a humble import statement as follows:</p>

<pre><code>from gensim import corpora, models, similarities
</code></pre>

<p>Lo! When I attempted to run that in my python interpreter, the dear thing crashed with <code>Segmentation Fault</code>! This is what happened:</p>

<pre><code>&gt;&gt;&gt; from gensim import corpora, models, similarities
Segmentation fault
</code></pre>

<p>Someone save me, because I don't know where the error could possibly be coming from.</p>

<p>I understand the segfault usually is due to illegal memory access by a process, so could this be happening due to the import? or some error within <code>gensim</code>?</p>

<p>For more info, it is said that after installing the package it can be tested, so I did this to test my <code>gensim</code>, Lo! the same <code>Segmentation Fault</code>! Here is the ouput:</p>

<pre><code>python setup.py test
running test
running egg_info
writing requirements to gensim.egg-info/requires.txt
writing gensim.egg-info/PKG-INFO
writing top-level names to gensim.egg-info/top_level.txt
writing dependency_links to gensim.egg-info/dependency_links.txt
reading manifest file 'gensim.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no files found matching '*.sh' under directory '.'
writing manifest file 'gensim.egg-info/SOURCES.txt'
running build_ext
Segmentation fault
</code></pre>

<p>For the dependent packages, I have:</p>

<pre><code>&gt;&gt;&gt; numpy.version.version
'1.3.0'

&gt;&gt;&gt; scipy.version.version
'0.8.0'
</code></pre>

<p>Ok, as requested in the comments, I did hook gdb to the interpreter, and then tried the import statement again, then this is what gdb gave when the segfault occurred again:</p>

<pre><code>(gdb) continue
Continuing.
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/time.so
Try: zypper install -C ""debuginfo(build-id)=da29868e88d517efc61eed319c4a87b41404f932""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/strop.so
Try: zypper install -C ""debuginfo(build-id)=1a5723f070198420ae565b728f267f00ae7e9885""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/cStringIO.so
Try: zypper install -C ""debuginfo(build-id)=d02dafc8dd403786b35ee44d946fc67461c7af34""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/_functools.so
Try: zypper install -C ""debuginfo(build-id)=4d3d7d73a2d7abe3d4ac45bdc07a070abde67a3b""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/_collections.so
Try: zypper install -C ""debuginfo(build-id)=86c7e2481ef3930f858927648d270a96ef65e0d9""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/operator.so
Try: zypper install -C ""debuginfo(build-id)=ecdf6c9dfbb007d3698e4108e2412b575b14c3f0""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/unicodedata.so
Try: zypper install -C ""debuginfo(build-id)=b84b2bd4061ce43b8fe6e7319d0e3fe90431f3f9""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/cPickle.so
Try: zypper install -C ""debuginfo(build-id)=0cb3d3c8e51cd264b7fc0cfd6ad6cea7da6173f1""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/itertools.so
Try: zypper install -C ""debuginfo(build-id)=88125d7ede2ef83a18e46901c9b7bd938d7554b9""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/_bisect.so
Try: zypper install -C ""debuginfo(build-id)=e872da9d2f7456947a21d6cf8ac05115da084ee0""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/_heapq.so
Try: zypper install -C ""debuginfo(build-id)=0c250b23be656b9984a8fbf67c232930141c6a79""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/math.so
Try: zypper install -C ""debuginfo(build-id)=48f975758f43cffc37703cda98615cb2daaf8a08""
Missing separate debuginfo for /usr/lib/python2.6/site-packages/numpy/core/multiarray.so
Try: zypper install -C ""debuginfo(build-id)=adcbae28e6012eecb870c60af4805f25554c9148""
Missing separate debuginfo for /usr/lib/python2.6/site-packages/numpy/core/umath.so
Try: zypper install -C ""debuginfo(build-id)=1087f0837567a96e6db9fadb8258f21113173f01""
Missing separate debuginfo for /usr/lib/python2.6/site-packages/numpy/core/_sort.so
Try: zypper install -C ""debuginfo(build-id)=c70e9c08253546b727376f7643cc9b6cc796465e""
Missing separate debuginfo for /usr/lib/python2.6/site-packages/numpy/core/scalarmath.so
Try: zypper install -C ""debuginfo(build-id)=cbc4ec89676c6072f64ae92a2917548479141eee""
Missing separate debuginfo for /usr/lib/python2.6/site-packages/numpy/lib/_compiled_base.so
Try: zypper install -C ""debuginfo(build-id)=6f71bf761290527c07afe78736211d2393caa95e""
Missing separate debuginfo for /usr/lib/python2.6/site-packages/numpy/linalg/lapack_lite.so
Try: zypper install -C ""debuginfo(build-id)=ac2cb74a8a055e3d58e15f4ac0012159abbf7d1a""
Missing separate debuginfo for /usr/lib/liblapack.so.3
Try: zypper install -C ""debuginfo(build-id)=5c9cf054c3e366ea04681d3c3b1e4d1fa8b46da5""
Missing separate debuginfo for /usr/lib/libblas.so.3
Try: zypper install -C ""debuginfo(build-id)=c7ea0a3cdf0da62f1f07f81838207e6070e86449""
Missing separate debuginfo for /usr/lib/libgfortran.so.3
Try: zypper install -C ""debuginfo(build-id)=6889f5fdc16cb8d7cb4d5e97c59080336c2e6e01""
Missing separate debuginfo for /lib/libgcc_s.so.1
Try: zypper install -C ""debuginfo(build-id)=ea12a9f70518dd6b807755150f1d2c6ba8550fe1""
Missing separate debuginfo for /usr/lib/python2.6/site-packages/numpy/fft/fftpack_lite.so
Try: zypper install -C ""debuginfo(build-id)=32599ba87256834ebc65a962e4718aa1f9134b0e""
Missing separate debuginfo for /usr/lib/python2.6/site-packages/numpy/random/mtrand.so
Try: zypper install -C ""debuginfo(build-id)=e43ddcab2e8e2961f3ab58087ac55dffa4094993""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/_ctypes.so
Try: zypper install -C ""debuginfo(build-id)=40cde5dd7ee47a3caac1ce1f94b6ef7fa28792ff""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/_struct.so
Try: zypper install -C ""debuginfo(build-id)=a5c456fe75e29e3424d7881fc05be8321fa65707""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/binascii.so
Try: zypper install -C ""debuginfo(build-id)=d3a2d6b38432a2b5076e238aef398cd3776bed20""
Missing separate debuginfo for /lib/libz.so.1
Try: zypper install -C ""debuginfo(build-id)=afddd839a6c18dd308b04b5289c56cc3abd1384f""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/_random.so
Try: zypper install -C ""debuginfo(build-id)=683d2819c1613d54dcd68c9169fc043ecb1b5444""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/fcntl.so
Try: zypper install -C ""debuginfo(build-id)=aaabeb0dbb01b7a14698fd221d09fedc6de19521""
Missing separate debuginfo for /usr/lib/libjpeg.so.8
Try: zypper install -C ""debuginfo(build-id)=5656b9563c388beee6e716dbe832ecb4480895ba""
Missing separate debuginfo for /usr/lib/python2.6/lib-dynload/array.so
Try: zypper install -C ""debuginfo(build-id)=4e4aea1f2106d4a7a7b4dbd51199e337549b83e2""
Missing separate debuginfo for /usr/lib/libstdc++.so.6
Try: zypper install -C ""debuginfo(build-id)=181385b3f6f58b2e13543468f07e08c5edd2cd86""

Program received signal SIGSEGV, Segmentation fault.
0x00000000 in ?? ()
</code></pre>
","522150","","342473","","2012-03-20 11:29:04","2012-03-20 11:29:04","Why the Segmentation Fault! When I attempt to run / use Python Package gensim?","<python><segmentation-fault><gensim>","1","6","","","","CC BY-SA 3.0"
"49128847","1","","","2018-03-06 10:41:27","","2","214","<p>Suppose I have data to be trained in sentence_stream</p>

<pre><code>phrases = Phrases(sentence_stream)
bigram_model = Phraser(phrases)
</code></pre>

<p>Now, If I try my bigram_model on some test data and check the output</p>

<pre><code>sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
print(bigram_model[sent])
[u'the', u'mayor', u'of', u'new_york', u'was', u'there']
</code></pre>

<p>Now, Suppose If want to add custom bigrams like the_mayor in my bigram_model so that the output should contain</p>

<pre><code>[u'the_mayor', u'of', u'new_york', u'was', u'there']
</code></pre>

<p>Any suggestions on how to configure the bigram_model?</p>
","9261801","","","","","2018-03-06 10:41:27","how to configure bigram model in gensim to include custom bigrams?","<python-3.x><gensim>","0","0","1","","","CC BY-SA 3.0"
"24178843","1","","","2014-06-12 07:33:21","","4","4726","<p>I use <a href=""/questions/tagged/gensim"" class=""post-tag"" title=""show questions tagged &#39;gensim&#39;"" rel=""tag"">gensim</a> to build dictionary from a collection of documents. Each document is a list of tokens. this my code</p>

<pre><code>def constructModel(self, docTokens):
    """""" Given document tokens, constructs the tf-idf and similarity models""""""

    #construct dictionary for the BOW (vector-space) model : Dictionary = a mapping between words and their integer ids = collection of (word_index,word_string) pairs
    #print ""dictionary""
    self.dictionary = corpora.Dictionary(docTokens)

    # prune dictionary: remove words that appear too infrequently or too frequently
    print ""dictionary size before filter_extremes:"",self.dictionary#len(self.dictionary.values())
    #self.dictionary.filter_extremes(no_below=1, no_above=0.9, keep_n=100000)
    #self.dictionary.compactify()

    print ""dictionary size after filter_extremes:"",self.dictionary

    #construct the corpus bow vectors; bow vector = collection of (word_id,word_frequency) pairs
    corpus_bow = [self.dictionary.doc2bow(doc) for doc in docTokens]


    #construct the tf-idf model 
    self.model = models.TfidfModel(corpus_bow,normalize=True)
    corpus_tfidf = self.model[corpus_bow]   # first transform each raw bow vector in the corpus to the tfidf model's vector space
    self.similarityModel = similarities.MatrixSimilarity(corpus_tfidf)  # construct the term-document index
</code></pre>

<p>my question is how to add a new doc (tokens) to this dictionary and update it. I searched in gensim documents but I didn't find a solution </p>
","3336432","","6573902","","2020-01-30 18:07:54","2020-11-24 16:21:14","how to add tokens to gensim dictionary","<python><gensim><topic-modeling><topicmodels>","3","0","1","","","CC BY-SA 3.0"
"15260864","1","15352312","","2013-03-07 00:24:44","","0","947","<p>All,</p>

<p>This is a re-post to what I responded to over in <a href=""https://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python"">this thread</a>. I am getting some totally screwy results with trying to print LSI topics in gensim. Here is my code:</p>

<pre><code>try:
    from gensim import corpora, models
except ImportError as err:
    print err

class LSI:
    def topics(self, corpus):
        tfidf = models.TfidfModel(corpus)
        corpus_tfidf = tfidf[corpus]
        dictionary = corpora.Dictionary(corpus)
        lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)
        print lsi.show_topics()

if __name__ == '__main__':
    data = '../data/data.txt'
    corpus = corpora.textcorpus.TextCorpus(data)
    LSI().topics(corpus)
</code></pre>

<p>This prints the following to the console.</p>

<pre><code>-0.804*""(5, 1)"" + -0.246*""(856, 1)"" + -0.227*""(145, 1)"" + ......
</code></pre>

<p>I would like to be able to print out the topics like @2er0 did <a href=""https://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python"">over here</a> but I am getting results like these. See below and note that the second item that is printed is a tuple and I have no idea where it came from. data.txt is a text file with several paragraphs in it. That is all.</p>

<p>Any thoughts on this would be fantastic! Adam</p>
","759422","","-1","","2017-05-23 12:11:05","2013-03-12 02:34:07","Gensim topic printing errors/issues","<python><topic-modeling><gensim>","2","0","","","","CC BY-SA 3.0"
"46244286","1","","","2017-09-15 16:48:47","","3","3250","<p>I am currently using the Word2Vec model trained on Google News Corpus (from <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">here</a>)
Since this is trained on news only until 2013, I need to updated the vectors and also add new words in the vocabulary based on the news coming after 2013. </p>

<p>Suppose I have a new corpus of news after 2013. Can I re-train or fine tune or update the Google News Word2Vec model? Can it be done using Gensim? Can it be done using FastText?</p>
","386384","","","","","2019-05-02 15:11:14","fine tuning pre-trained word2vec Google News","<python><gensim><word2vec><google-news><fasttext>","2","0","1","","","CC BY-SA 3.0"
"64390330","1","","","2020-10-16 13:38:52","","2","558","<p>I am using python Gensim package to build LDA model (<a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#:%7E:text=Topic%20Modeling%20with%20Gensim%20(Python)&amp;text=Topic%20Modeling%20is%20a%20technique,in%20the%20Python%27s%20Gensim%20package"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#:~:text=Topic%20Modeling%20with%20Gensim%20(Python)&amp;text=Topic%20Modeling%20is%20a%20technique,in%20the%20Python's%20Gensim%20package</a>)</p>
<p>To choose the best number of Topics in the LDA, I calculated the Coherence score for (1-20)topics and then visulaize it</p>
<pre><code>    def compute_coherence_values(dictionary, corpus, texts, limit, start=1, step=1):
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        lda_model_coh = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, per_word_topics=True,update_every=1,
                                           chunksize=100,random_state=80,
                                           passes=10,
                                           alpha='auto')
        model_list.append(lda_model_coh)
        coherence_model_lda = CoherenceModel(model=lda_model_coh, texts=data_words_nostops, dictionary=id2word, coherence='c_v')
        coherence_values.append(coherence_model_lda.get_coherence())

    return model_list, coherence_values

model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words_nostops, start=1, limit=21, step=1)
print(coherence_values)
</code></pre>
<p>the out put is :</p>
<blockquote>
<p>[0.6110807023750182, 0.623346262237542, 0.611190819343431,
0.6150879617345366, 0.6661056841233617, 0.6460622418348893, 0.6684570240561849, 0.6603704258720786, 0.6781376351229919, 0.6686810583507139, 0.6704931154541898, 0.6209832171172912, 0.6223242456220992, 0.583528787158143, 0.5672411886488239, 0.5485767400671002, 0.5603438856884889, 0.538775236148759, 0.5424604528457801, 0.536498799229393]</p>
</blockquote>
<p><img src=""https://i.stack.imgur.com/ltqrC.png"" alt=""coherence score / topic numbers"" />][1]</p>
<p>As the chart shows, The coherence score value is highest score at the value 10. But when i visualize it using the intertopic Distance maps i found the topics are crowded and the overlaps between the topics is huge. So i visualize it using 5 and 7 topics.
<img src=""https://i.stack.imgur.com/JBgZZ.jpg"" alt=""intertopic distance map"" /><a href=""https://i.stack.imgur.com/JBgZZ.jpg"" rel=""nofollow noreferrer"">2</a>
I am not sure which number of topics is better to choose.
My question is, How to choose the best coherence value. a scintfic reference recommendation will be a good help for me.<br />
[1]: <a href=""https://i.stack.imgur.com/ltqrC.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/ltqrC.png</a></p>
","3427898","","3427898","","2020-10-19 13:05:39","2020-10-19 13:05:39","choose the best Coherence Score for LDA model","<python><gensim><lda><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"55487124","1","55500078","","2019-04-03 04:32:52","","0","242","<p>I am following the tutorial here:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>

<p>But when I get to this part:</p>

<pre><code>pre = Doc2Vec(min_count=0)
pre.scan_vocab(documents)
</code></pre>

<p>I get the following error on scan_vocab:</p>

<pre><code>    AttributeError: 'Doc2Vec' object has no attribute 'scan_vocab'
</code></pre>

<p>Does anyone know how to fix this? Thanks.</p>
","5476045","","","","","2019-04-03 16:31:38","Gensim Attribute Error when trying to use pre_scan on a doc2vec object","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"40230532","1","40234010","","2016-10-25 02:41:08","","1","830","<p>Here is my code for training my doc2vec model</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
from FileDocIterator import FileDocIterator

doc_file_name = 'doc_6million.txt'
docs = FileDocIterator(doc_file_name)
print ""Fitting started""
model = Doc2Vec(docs, size=100, window=5, min_count=5, negative=20, workers=6, iter=4)
print ""Saving model""
model.save(""doc2vec_model"")
print ""model saved""
</code></pre>

<p>Now lets take a look at <code>FileDocIterator</code></p>

<pre><code>import json

from gensim.models.doc2vec import TaggedDocument
from gensim.models import Phrases

class FileDocIterator(object):
    def __init__(self, fileName):
        self.fileName = fileName
        self.phrase = Phrases.load(""phrases"")

    def __iter__(self):
        for line in open(self.fileName):
            jsData = json.loads(line)
            yield TaggedDocument(words=jsData[""data""], tags=jsData[""id""])
</code></pre>

<p>Now I do understand that phrases isn't being used in this implementation, but bear with me here, lets take a look at how the data looks like. Here is the first data point</p>

<pre><code>{""data"":[""strategic"",""and"",""analytical"",""technical"",""program"",""director"",""and"",""innovator"",""who"",""inspires"",""calculated"",""risk-taking"",""in"",""emerging"",""technologies"","","",""such"",""as"",""cyber"",""security"","","",""risk"","","",""analytics"","","",""big"",""data"","","",""cloud"","","",""mobility"",""and"",""3d"",""printing"",""."",""known"",""for"",""growing"",""company"",""profit"",""through"",""innovative"",""thinking"",""aimed"",""at"",""improving"",""employee"",""productivity"",""and"",""providing"",""solutions"",""to"",""private"",""industry"",""and"",""government"",""customers"",""."",""recognized"",""for"",""invigorating"",""creative"",""thinking"",""and"",""collaboration"",""within"",""large"",""companies"",""to"",""leverage"",""their"",""economies"",""of"",""scale"",""to"",""capture"",""market"",""share"",""."",""successful"",""in"",""managing"",""the"",""risk"",""and"",""uncertainty"",""throughout"",""the"",""innovation"",""lifecycle"",""by"",""leveraging"",""an"",""innovation"",""management"",""framework"",""to"",""overcome"",""barriers"",""."",""track"",""record"",""of"",""producing"",""results"",""in"",""competitive"","","",""rapidly"",""changing"",""environments"",""where"",""innovation"",""and"",""customer"",""satisfaction"",""is"",""the"",""business"",""."",""competencies"",""include"","":"",""innovation"",""management"",""cyber"","","",""risk"","","",""analytics"","","",""cloud"",""computing"",""and"",""mobility"",""technology"",""development"",""security"",""compliance"","":"",""dod/ic"",""("",""nispom"","","",""icd"",""503"","","",""fedramp"","")"",""commercial"",""("",""iso/iec"",""27002"","","",""pci"",""dss"","")"",""relationship"",""management"","":"",""dod"","","",""public"",""sector"",""and"",""intelligence"",""community"",""change"",""management"",""it"",""security"",""&amp;"",""risk"",""management"",""("",""cissp"","")"",""program"","","",""product"",""&amp;"",""portfolio"",""management"",""("",""pmp"","")"",""data"",""analytics"",""management"",""("",""cchd"","")"",""itil"",""service"",
""management"",""("",""itilv3-expert"","")""],
""id"":""55c37f730d03382935e12767""}
</code></pre>

<p>My understanding is that the id, <code>55c37f730d03382935e12767</code> should be the id of the document, so doing the following ought to give me back a docVector.</p>

<pre><code>model.docvecs[""55c37f730d03382935e12767""]
</code></pre>

<p>Instead, this is what is outputed. </p>

<pre><code>&gt;&gt;&gt; model.docvecs[""55c37f730d03382935e12767""]
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/doc2vec.py"", line 341, in __getitem__
    return self.doctag_syn0[self._int_index(index)]
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/doc2vec.py"", line 315, in _int_index
    return self.max_rawint + 1 + self.doctags[index].offset
KeyError: '55c37f730d03382935e12767'
</code></pre>

<p>Trying to get most similar gives the following back</p>

<pre><code>&gt;&gt;&gt; model.docvecs.most_similar(""55c37f730d03382935e12767"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/doc2vec.py"", line 450, in most_similar
    raise KeyError(""doc '%s' not in trained set"" % doc)
KeyError: ""doc '55c37f730d03382935e12767' not in trained set""
</code></pre>

<p>What I'm trying to understand is how are doc vectors saved and what id's are used. What part of my approach isn't working above? </p>

<p>Now here's something interesting, if I do the following I get back similar doc vectors but they have no meaning to me. </p>

<pre><code>&gt;&gt;&gt; model.docvecs.most_similar(str(1))
[(u'8', 0.9000369906425476), (u'3', 0.8878246545791626), (u'7', 0.886141836643219), (u'2', 0.8834314942359924), (u'e', 0.8812381029129028), (u'a', 0.8648831248283386), (u'd', 0.8587037920951843), (u'0', 0.8413013219833374), (u'4', 0.8385311365127563), (u'c', 0.8290119767189026)]
</code></pre>
","3084800","","7117003","","2018-12-15 19:48:33","2018-12-15 19:48:33","gensim doc2vec documents not found by id","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"46238185","1","","","2017-09-15 11:09:31","","2","195","<p>I am a little new to doc2vec algorithm and using gensim for its implementation in python.</p>

<p>Following the gensim tutorial <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">""Gensim Doc2vec Tutorial on the IMDB Sentiment Dataset""</a> I have built vocab and trained a doc2vec model, and stored it on the disc using :</p>

<pre><code>model = Doc2Vec(dm=0, dbow_words=1, size=300, window=8, min_count=2, iter=10, workers=cores, alpha=0.025, min_alpha=0.025)
model.build_vocab(art_shuffle, progress_per=10000)
model.train(art_shuffle, total_examples=len(art_shuffle), epochs=10)
model.save('doc2vec_model')
</code></pre>

<p>It creates the following four files in my directory:</p>

<pre><code>doc2vec_model
doc2vec_model.docvecs.doctag_syn0.npy
doc2vec_model.syn1neg.npy
doc2vec_model.wv.syn0.npy
</code></pre>

<p>I load the model back, using the same filename I used to save it i.e.</p>

<pre><code>model = Doc2Vec.load('doc2vec_model')
</code></pre>

<p>After that, if I use this model to create a vector for my document I get an error</p>

<pre><code>model.infer_vector(tokenize(doc_text))

Traceback (most recent call last):
  File ""C:\Users\vipul\Documents\NLP_testing\python-nlp\doc2vec_trials\story_prediction_doc2vec.py"", line 394, in &lt;module&gt;
    inferred_vector = model.infer_vector(tokenize(doc_text))
  File ""C:\Python27\lib\site-packages\gensim\models\doc2vec.py"", line 743, in infer_vector
    doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
  File ""gensim\models\doc2vec_inner.pyx"", line 272, in gensim.models.doc2vec_inner.train_document_dbow (./gensim/models/doc2vec_inner.c:3535)
    _word_vectors = &lt;REAL_t *&gt;(np.PyArray_DATA(word_vectors))
TypeError: Cannot convert list to numpy.ndarray
</code></pre>

<p>Where am I going wrong?</p>

<p>Note : tokenize() function is returning a list of words using the nltk wordpunct_tokenizer</p>
","1798553","","","","","2017-09-15 11:09:31","TypeError while using infer_vector on a gensim Doc2Vec model loaded from memory","<python><gensim><doc2vec>","0","0","","","","CC BY-SA 3.0"
"55478104","1","55480359","","2019-04-02 15:12:48","","0","212","<p>I downloaded the full wikipedia archive 14.9gb and I am running thise line of code:</p>

<pre><code>wiki = WikiCorpus(""enwiki-latest-pages-articles.xml.bz2"")
</code></pre>

<p>My code doesn't seem to be getting past here and it has been running for an hour now, I understand that the target file is massive, but I was wondering how I could tell it is working, or what is the expected time for it to complete?</p>
","5476045","","","","","2019-04-02 17:20:24","How to tell if WikiCorpus from gensim is working?","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"24242287","1","","","2014-06-16 11:06:53","","1","550","<p>Does gensim give us hierarchy of topics?
I write a code to calculate topic of some documents, the output is words of each topic.
But I want hierarchy of topics.
this is my code:</p>

<p><a href=""https://gist.github.com/anonymous/2e3b2f3866e5029c55c3"" rel=""nofollow"">https://gist.github.com/anonymous/2e3b2f3866e5029c55c3</a></p>

<p>and this is output:</p>

<pre><code>2014-06-16 13:02:22,540 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-06-16 13:02:38,162 : INFO : built Dictionary(324451 unique tokens: [u'considered,', u'\x00\x00', u'\ufb90\ufee0\ufbff\ufeea', u'\u0627\ufee7\ufed4\ufb91\ufe8e\u0643', u'\u0627\u0628\u0631\u0631\u0627\u06cc\u0627\u0646\u0647']...) from 885 documents (total 3885556 corpus positions)
2014-06-16 13:02:38,545 : INFO : storing corpus in Matrix Market format to corpus.mm
2014-06-16 13:02:38,546 : INFO : saving sparse matrix to corpus.mm
2014-06-16 13:02:38,554 : INFO : PROGRESS: saving document #0
2014-06-16 13:02:45,290 : INFO : saved 884x79405 matrix, density=0.514% (360672/70194020)
2014-06-16 13:02:45,292 : INFO : saving MmCorpus index to corpus.mm.index
2014-06-16 13:02:45,293 : INFO : loaded corpus index from corpus.mm.index
2014-06-16 13:02:45,293 : INFO : initializing corpus reader from corpus.mm
2014-06-16 13:02:45,293 : INFO : accepted corpus with 884 documents, 79405 features, 360672 non-zero entries
2014-06-16 13:03:06,913 : INFO : topic 0: 0.010*ŸÖ€å + 0.006*ÿØŸáŸÜÿØ + 0.006*ÿ®ÿßÿ±Ÿá + 0.006*ÿ±Ÿà€å + 0.004*⁄©ÿ≥€å + 0.004*ÿ®€å + 0.004*ŸÖÿßŸÜŸÜÿØ + 0.004*ÿ¨ÿ≤ + 0.004*ÿ¥ŸàÿØ + 0.004*€å⁄©€å + 0.004*⁄ÜŸá + 0.004*ÿßŸÖÿß + 0.004*ÿØÿßÿ±ÿØ + 0.004*ÿØÿ± + 0.004*ÿ®ÿ± + 0.004*ÿ¢ŸÜ + 0.004*ÿßŸà + 0.004*ÿ≠ÿ™€å + 0.004*⁄©Ÿá + 0.004*Ÿáÿß€å
2014-06-16 13:03:07,097 : INFO : topic 1: 0.000*Ÿæÿ≥ÿ™ + 0.000*⁄Øÿ±ŸÅÿ™‚Äå + 0.000*Single + 0.000*Ô∫ßÔª®Ôªö + 0.000*Ô∫ëÔª§Ô∫éÔªßÔ∫™ + 0.000*ÿ≠ÿØŸàÿØ€å + 0.000*352 + 0.000*¬´ÿØŸäŸÜ + 0.000*⁄Øÿ±ŸàŸáŸä‚Äå + 0.000*Ô∫£ÔªîÔ∫é + 0.000*ŸÖ€å‚Äãÿ¥ŸàÿØ + 0.000*ÿ∫ŸÜ€å + 0.000*   ⁄©ÿ¥ÿ™Ÿä + 0.000*ÿ®ÿ≥ÿ™ÿß€åŸÖ. + 0.000*19-20. + 0.000*67 + 0.000*ÿ™ÿµÿ±ŸÅ + 0.000*ŸÖÿ∞ÿß⁄©ÿ±ÿßÿ™¬ª + 0.000*ÿßŸÑÿßÿ™ + 0.000*Ÿæÿ≥ÿ±ÿ¥
</code></pre>

<p>Is there any way to get the hierarchy of topics?</p>
","2228732","","","","","2016-07-26 19:00:57","Get hierarchy of topic from gensim","<python><scipy><gensim>","1","0","","","","CC BY-SA 3.0"
"24212368","1","24212440","","2014-06-13 19:09:45","","2","83","<p>I am working on an LDA model with gensim. For this, I am basically opening text files, building a dictionary, and then running the model. </p>

<p>To open the files I use this: </p>

<pre><code>files = [codecs.open(infile, 'r', 'utf-16', 'ignore') for infile in sample_list] 
</code></pre>

<p>in which sample_list is a list of paths to files. I need to use codecs.open because the texts are in a different language (and I haven't updated Python). </p>

<p>My problem is that I don't know how to close all the files after using them. Any ideas? I've tried a couple of things. I cannot use a regular loop here because of my following step is:</p>

<pre><code>texts = ["" "".join(file.readlines()[0:]) for file in files]
</code></pre>

<p>When I use over 5,000 files I get the error '' IOError: [Errno 24] Too many open files '' I am thinking that I could open a number of files at a time,  join them, close them, and repeat. Also, keeping the files open is just bad. 
Thank you!</p>
","3738814","","276949","","2014-06-13 19:10:52","2014-06-13 19:37:10","Python- Closing bunch of text files opened at the same time with list comprehensions","<python><loops><text><gensim>","1","2","1","","","CC BY-SA 3.0"
"55485908","1","","","2019-04-03 01:52:55","","1","495","<p>I'm getting a CalledProcessError ""non-zero exit status 1"" error when I run the Gensim LDAMallet model on my full corpus of ~16 million documents.
Interestingly enough, if I run the exact same code on a testing corpus of ~160,000 documents the code runs perfectly fine. Since it's working fine on my small corpus I'm inclined to think that the code is fine, but I'm not sure what else would/could cause this error...</p>

<p>I've tried editing the mallet.bat file as suggested <a href=""https://stackoverflow.com/questions/55288724/gensim-mallet-calledprocesserror-returned-non-zero-exit-status"">here</a>, but to no avail.
I've also double checked the paths, but that shouldn't be an issue given that it works with a smaller corpus.</p>

<pre><code>id2word = corpora.Dictionary(lists_of_words)
corpus =[id2word.doc2bow(doc) for doc in lists_of_words]
num_topics = 30
os.environ.update({'MALLET_HOME':r'C:/mallet-2.0.8/'})
mallet_path = r'C:/mallet-2.0.8/bin/mallet'
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)
</code></pre>

<p>Here's the full traceback and error:</p>

<pre><code>  File ""&lt;ipython-input-57-f0e794e174a6&gt;"", line 8, in &lt;module&gt;
    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py"", line 132, in __init__
    self.train(corpus)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py"", line 273, in train
    self.convert_input(corpus, infer=False)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\wrappers\ldamallet.py"", line 262, in convert_input
    check_output(args=cmd, shell=True)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\utils.py"", line 1918, in check_output
    raise error

CalledProcessError: Command 'C:/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\user\AppData\Local\Temp\2\e1ba4a_corpus.txt --output C:\Users\user\AppData\Local\Temp\2\e1ba4a_corpus.mallet' returned non-zero exit status 1.
</code></pre>
","8617685","","","","","2019-04-12 23:14:50","Python Gensim LDAMallet CalledProcessError with large corpus (runs fine with small corpus)","<python><gensim><lda><mallet>","1","0","","","","CC BY-SA 4.0"
"38324328","1","38649671","","2016-07-12 09:01:53","","1","2552","<p>I have been reading more modern posts about sentiment classification (analysis) such as <a href=""http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis"" rel=""nofollow"">this</a>.</p>

<p>Taking the IMDB dataset as an example I find that I get a similar accuracy percentage using Doc2Vec (88%), <strong>however a far better result using a simple tfidf vectoriser with tri-grams for feature extraction (91%)</strong>. I think this is similar to Table 2 in <a href=""http://arxiv.org/pdf/1412.5335v7.pdf"" rel=""nofollow"">Mikolov's 2015 paper</a>.</p>

<p>I thought that by using a bigger data-set this would change. So I re-ran my experiment using a breakdown of 1mill training and 1 mill test from <a href=""http://jmcauley.ucsd.edu/data/amazon/"" rel=""nofollow"">here</a>. Unfortunately, in that case my tfidf vectoriser feature extraction method increased to 93% but doc2vec fell to 85%.</p>

<p><strong>I was wondering if this is to be expected and that others find tfidf to be superior to doc2vec even for a large corpus?</strong></p>

<p>My data-cleaning is simple:</p>

<pre><code>def clean_review(review):
    temp = BeautifulSoup(review, ""lxml"").get_text()
    punctuation = """""".,?!:;(){}[]""""""
    for char in punctuation
        temp = temp.replace(char, ' ' + char + ' ')
    words = "" "".join(temp.lower().split()) + ""\n""
    return words
</code></pre>

<p>And I have tried using 400 and 1200 features for the Doc2Vec model:</p>

<pre><code>model = Doc2Vec(min_count=2, window=10, size=model_feat_size, sample=1e-4, negative=5, workers=cores)
</code></pre>

<p>Whereas my tfidf vectoriser has 40,000 max features:</p>

<pre><code>vectorizer = TfidfVectorizer(max_features = 40000, ngram_range = (1, 3), sublinear_tf = True)
</code></pre>

<p>For classification I experimented with a few linear methods, however found simple logistic regression to do OK...</p>
","4465819","","","","","2016-07-29 02:33:21","Is Doc2Vec suited for Sentiment Analysis?","<machine-learning><sentiment-analysis><gensim><word2vec><doc2vec>","1","0","2","","","CC BY-SA 3.0"
"55291657","1","55677954","","2019-03-22 01:38:09","","1","2266","<p>I have trained a LDA model on a corpus using Gensim. Now that I have the topic distribution for each document, how can I compare how similar two documents are in topics? I would like to have a summary measure. For example, the following are the topic distributions of two documents. There are totally 75 topics. For brevity, I show only the first 10 topics with largest probabilities (so the topics are not in order). (40, 0.5523168) means that topic #40 has a probability of 0.5523168 for DOC #1. Should I calculate the Euclidean or Cosine distance between the two vectors? And using this summary measure, can I say that, for example, DOC 1 is more similar to DOC2 than to DOC3, or DOC1 and DOC 2 are more similar to each other than DOC 3 and DOC 4 topically? Thank you!</p>

<pre><code>DOC #1:
[(40, 0.5523168), (60, 0.12225048), (43, 0.07556598), (41, 0.065885976), 
(22, 0.05838573), (24, 0.044774733), (74, 0.019839266), (65, 0.019544959), 
(51, 0.015470431), (36, 0.013449047)]


DOC #2:
[(73, 0.58864516), (41, 0.16827711), (51, 0.09783472), (63, 0.06510383), 
(24, 0.04722658), (32, 0.014467965), (44, 0.012267662), (47, 0.0031533625), 
(18, 0.0022214972), (0, 1.2154361e-05)]
</code></pre>
","9108781","","9108781","","2019-03-22 13:56:35","2019-04-14 17:15:23","How to compare the topical similarity between two documents in Python Gensim from their topic distributions?","<python><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"55331731","1","","","2019-03-25 05:32:41","","0","571","<p>I tried to load .bin embedding file using gensim but i got  errors. I tried all the methods provided by gensim but couldn't rectify the error</p>

<p><strong>Method 1</strong></p>

<pre><code>import gensim.models.keyedvectors as word2vec

model=word2vec.KeyedVectors.load_word2vec_format('Health_2.5reviews.s200.w10.n5.v10.cbow.bin', binary=True, unicode_errors=‚Äòignore')
</code></pre>

<p><strong>Method 2</strong></p>

<pre><code>from gensim.models import KeyedVectors

filename='Health_2.5reviews.s200.w10.n5.v10.cbow.bin'

model=KeyedVectors.load_word2vec_format(filename,binary=True,unicode_errors=‚Äòignore')
</code></pre>

<p><strong>Method 1 and 2 gave the error</strong></p>

<blockquote>
  <p>""UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbc in position
  0: invalid start byte""</p>
</blockquote>

<p><strong>Method 3</strong></p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec

filename='Health_2.5reviews.s200.w10.n5.v10.cbow.bin'

model=Word2Vec.load(filename)
</code></pre>

<p><strong>Method 3 gave the error</strong></p>

<blockquote>
  <p>UnpicklingError: invalid load key, '\xbc'.</p>
</blockquote>
","6151940","","6151940","","2019-03-25 15:04:06","2019-03-25 15:04:06","Error when loading .bin embedding file using gensim package","<python><gensim><word2vec>","0","6","","","","CC BY-SA 4.0"
"63590239","1","","","2020-08-26 03:54:04","","0","353","<p>So, I have a training data which is in a json file. Now, I am trying to use word Embedding(Word2Vec) on that. However, I have no idea how to proceed. Mostly everything I looked up just deals with direct sentences given and not with json file. Right now all I have done is to read a json file using python.</p>
<p>Please guide me on what could be done next. I trying to use gensim</p>
","8705853","","5404226","","2020-08-26 09:53:23","2020-08-26 09:53:23","Word2Vec with a json file","<python><json><gensim><word2vec>","0","5","","","","CC BY-SA 4.0"
"52415929","1","","","2018-09-20 00:42:39","","0","51","<p>I wish to use a variable like a model that has trained certain sentence for gensim. 
Example, I use gensim word2vec to train a sentence to find its vectors in another function and save it to a variable called ""model"".</p>

<p>Then I create another function to get the the vectors of each word and return it.</p>

<p>My code is as follow:</p>

<pre><code>def gen(sentence):
    model = gensim.models.word2vec.Word2Vec([sentence],min_count=1, workers=1, size=3)
    ....
    ....
    return ...

def name(sentence):
    for word in sentence:
        print(model.wv[word])
    return
</code></pre>

<p>Is there a way to pass the variable from function def ""gen"" named ""model"" to function def ""name""? I tried to use self but its not working. I wanted to just call the variable because I dont want to build another extra function just to link it.</p>
","10207014","","","","","2018-09-20 01:09:28","Is it possible to call a variable from another function in another function?","<python><gensim>","2","2","","","","CC BY-SA 4.0"
"55490182","1","","","2019-04-03 08:13:51","","0","755","<p>/Users/Barry/anaconda/lib/python2.7/site-packages/gensim/models/ldaseqmodel.py:217: RuntimeWarning: divide by zero encountered in double_scalars
  convergence = np.fabs((bound - old_bound) / old_bound)</p>

<pre><code>#dynamic topic model
def run_dtm(num_topics=18):
    docs, years, titles = preprocessing(datasetType=2)

    #resort document by years
    Z = zip(years, docs)
    Z = sorted(Z, reverse=False)
    years_new, docs_new = zip(*Z)

    #generate time slice
    time_slice = Counter(years_new).values()

    for year in Counter(years_new):
        print year,' --- ',Counter(years_new)[year]

    print '********* data set loaded ********'
    dictionary = corpora.Dictionary(docs_new)
    corpus = [dictionary.doc2bow(text) for text in docs_new]

    print '********* train lda seq model ********'
    ldaseq = ldaseqmodel.LdaSeqModel(corpus=corpus, id2word=dictionary, time_slice=time_slice, num_topics=num_topics)

    print '********* lda seq model done ********'
    ldaseq.print_topics(time=1)
</code></pre>

<p>Hey guys, I'm using the dynamic topic models in gensim package for topic analysis, following this tutorial, <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb</a>, however I always got the same unexpected error. Can anyone give me some guidance? I'm really puzzled even thought I have tried some different dataset for generating corpus and dictionary.
The error is like this:</p>

<p>/Users/Barry/anaconda/lib/python2.7/site-packages/gensim/models/ldaseqmodel.py:217: RuntimeWarning: divide by zero encountered in double_scalars
  convergence = np.fabs((bound - old_bound) / old_bound)</p>
","7894459","","","","","2020-12-01 09:58:46","gensim/models/ldaseqmodel.py:217: RuntimeWarning: divide by zero encountered in double_scalars","<gensim>","2","0","","","","CC BY-SA 4.0"
"55738632","1","","","2019-04-18 03:14:30","","0","222","<p>For example, I've already transformed all the words and numbers into one-hot coding. Then </p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import word2vec
</code></pre>

<p>and I want to use </p>

<pre class=""lang-py prettyprint-override""><code>word2vec.Word2Vec(sentences=one_hot_vectors)
</code></pre>

<p>However, it popped up an error: </p>

<pre class=""lang-py prettyprint-override""><code>ufunc 'add' did not contain a loop with signature matching types dtype('U32')dtype('U32') dtype('U32')
</code></pre>

<p>I think it is because I cannot directly input one-hot coding to <code>word2vec.Word2Vec</code>. I wonder that does python have any other modules to satisfy my needs. I just want to input one-hot coding vectors not the raw sentences directly into word2vec model. Thank you so much. </p>
","11376770","","1334330","","2019-04-18 03:50:48","2019-04-18 15:30:06","Can I input one-hot coding vectors not the raw sentences directly into PYTHON module word2vec.Word2Vec?","<python><gensim><word2vec><one-hot-encoding>","1","4","","","","CC BY-SA 4.0"
"61070763","1","","","2020-04-07 00:01:04","","0","670","<p>What is the appropriate way to save a <code>gensim</code> doc2vec model that has been transformed by T-SNE (from <code>sklearn.manifold</code>), e.g.</p>

<pre><code>x_full = model[doc_tags]
pca_full = PCA(n_components=50)
pca_result_full = pca_full.fit_transform(x_full)
tsne = TSNE(n_components=2, verbose=1)
x_tsne_full = tsne.fit_transform(pca_result_full)
</code></pre>

<p>where <code>doc_tags</code> is the tagged documents that the model was trained on. Reducing this model using T-SNE takes hours, so it would be good to save this for future use. </p>

<p>I have been putting it in a pandas dataframe, such as <code>df = pd.DataFrame(x_tsne_full, index=doc_tags, columns=['x', 'y'])</code> then saving this dataframe to CSV for future use. Is this the best way, or is there a T-SNE-appropriate way to save that is not apparent in its docs?</p>
","4237080","","4237080","","2020-04-07 00:07:00","2020-04-07 00:09:08","Save T-SNE results for future use","<python><gensim><dimensionality-reduction><saving-data>","0","3","","","","CC BY-SA 4.0"
"6615569","1","6615620","","2011-07-07 18:48:15","","1","4755","<p>I am having trouble getting <strong>PyDev on Eclipse</strong> to recognize installed modules (<strong>gensim</strong>), which work fine in IDLE. I am using Windows Vista, 32-bit. Python 2.7.</p>

<p>I have found this question asked: <a href=""https://stackoverflow.com/questions/6070423/adding-python-modules-to-pydev-in-eclipse-results-in-import-error"">here</a>, <a href=""https://stackoverflow.com/questions/2590435/eclipse-pydev-eclipse-telling-me-that-this-is-an-invalid-import"">here</a>, <a href=""https://stackoverflow.com/questions/4631377/unresolved-import-issues-with-pydev-and-eclipse"">here</a>, and <a href=""https://stackoverflow.com/questions/2983088/unresolved-import-models"">here</a>.</p>

<p>The recommended solution is to go to <strong>preferences > pydev > interpreter - python</strong>, and remove and re-add (w/ Auto Config) the python interpreter. I have done this, and have restarted Eclipse. In <code>PYTHONPATH</code>, <code>C:\Python27\lib\site-packages\gensim-0.8.0-py2.7.egg</code>, appears, but I still run into the import error. My code is:</p>

<pre><code>from gensim import corpora, models, similarities
</code></pre>

<p>And this yields:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\Jpan\Documents\workspace\FirstProject\src\gensim.py"", line 1, in &lt;module&gt;
    from gensim import corpora, models, similarities
  File ""C:\Users\Jpan\Documents\workspace\FirstProject\src\gensim.py"", line 1, in &lt;module&gt;
    from gensim import corpora, models, similarities
ImportError: cannot import name corpora
</code></pre>

<p>Another recommended solution is to manually add the folder by clicking ""New Folder"" in the bottom part of the interpreter - python screen and navigating to the location where gensim installed. I have also done this, and added <code>C:\Python27\lib\site-packages\gensim-0.8.0-py2.7.egg\gensim</code>, which has all the necessary <code>\__init__.py</code> files. But, I still get the <code>ImportError</code>.</p>

<p>Any suggestions for what else I could try?</p>
","834103","","-1","","2017-05-23 12:12:15","2012-03-20 11:29:55","Eclipse + PyDev ImportError","<python><eclipse><import><pydev><gensim>","1","0","","","","CC BY-SA 3.0"
"32543235","1","","","2015-09-12 20:09:35","","1","1797","<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from gensim import corpora, models, similarities
from nltk.corpus import stopwords
import codecs

documents = []
with codecs.open(""Master_File_for_Docs.txt"", encoding = 'utf-8', mode= ""r"") as fid:
   for line in fid:
       documents.append(line)
stoplist = []
x = stopwords.words('english')
for word in x:
    stoplist.append(word)

#Removes Stopwords
texts = [[word for word in document.lower().split() if word not in stoplist]
for document in documents]


dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda = models.LdaModel(corpus, id2word=dictionary, num_topics=100)
lda.print_topics(20)
#corpus_lda = lda[corpus]
#for doc in corpus_lda:
 #   print(doc)
</code></pre>

<p>I am running Gensim for topic modeling and trying to get the above code working. I know that this code works because my friend ran it from a mac computer and it worked successfully but when I run it from a windows computer the code gives me a </p>

<pre><code>MemoryError
</code></pre>

<p>Also the logging that I set on the second line also doesn't appear on my windows computer. Is there something in Windows that I need to fix in order for gensim to work?</p>
","4490935","","","","","2018-07-24 09:04:32","Python: Gensim Memory Error","<python><windows><gensim>","2","0","","","","CC BY-SA 3.0"
"34866830","1","","","2016-01-19 00:59:56","","5","3102","<p>I'm using the <code>most_similar()</code> method as below to get all the words similar to a given word:</p>

<pre><code>word,score= model.most_similar('apple',topn=sizeofdict)
</code></pre>

<p>AFAIK, what this does is, calculate the cosine similarity between the given word and all the other words in the dictionary. When i'm inspecting the words and scores, I can see there are words with negative score down the list. What does this mean? are them the words that has opposite meaning to the given word?</p>

<p>Also if it's using cosine similarity, how does it get a negative value? cosine similarity varies between 0-1 for two documents.</p>
","601357","","","","","2016-01-19 11:31:46","about word2vec most_similar() function","<text-mining><gensim><word2vec>","1","0","2","","","CC BY-SA 3.0"
"22129943","1","22130100","","2014-03-02 16:04:53","","135","111858","<p>According to the <a href=""http://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">Gensim Word2Vec</a>, I can use the word2vec model in gensim package to calculate the similarity between 2 words.</p>

<p>e.g.</p>

<pre><code>trained_model.similarity('woman', 'man') 
0.73723527
</code></pre>

<p>However, the word2vec model fails to predict the sentence similarity. I find out the LSI model with sentence similarity in gensim, but, which doesn't seem that can be combined with word2vec model. The length of corpus of each sentence I have is not very long (shorter than 10 words).  So, are there any simple ways to achieve the goal?</p>
","1237832","","2568747","","2016-04-12 13:10:22","2020-10-26 10:37:57","How to calculate the sentence similarity using word2vec model of gensim with python","<python><gensim><word2vec>","14","4","82","","","CC BY-SA 3.0"
"66135337","1","","","2021-02-10 10:43:54","","0","16","<p>I have a windows 10 pro PC and have python 3.9 installed on it.
Whenever I try to install gensim using pip install gensim, it failsm with the errod message:</p>
<p>ERROR: Command errored out with exit status 1: 'c:\program files\python39\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0]</p>
<pre><code>C:\Users\PS&gt;pip install gensim
Defaulting to user installation because normal site-packages is not writeable
Collecting gensim
  Using cached gensim-3.8.3.tar.gz (23.4 MB)
Requirement already satisfied: numpy&gt;=1.11.3 in c:\users\ps\appdata\roaming\python\python39\site-packages (from gensim) (1.20.1)
Requirement already satisfied: scipy&gt;=0.18.1 in c:\users\ps\appdata\roaming\python\python39\site-packages (from gensim) (1.6.0)
Requirement already satisfied: six&gt;=1.5.0 in c:\users\ps\appdata\roaming\python\python39\site-packages (from gensim) (1.15.0)
Requirement already satisfied: smart_open&gt;=1.8.1 in c:\users\ps\appdata\roaming\python\python39\site-packages (from gensim) (3.0.0)
Requirement already satisfied: requests in c:\users\ps\appdata\roaming\python\python39\site-packages (from smart_open&gt;=1.8.1-&gt;gensim) (2.25.1)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\ps\appdata\roaming\python\python39\site-packages (from requests-&gt;smart_open&gt;=1.8.1-&gt;gensim) (2020.12.5)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in c:\users\ps\appdata\roaming\python\python39\site-packages (from requests-&gt;smart_open&gt;=1.8.1-&gt;gensim) (2.10)
Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in c:\users\ps\appdata\roaming\python\python39\site-packages (from requests-&gt;smart_open&gt;=1.8.1-&gt;gensim) (4.0.0)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in c:\users\ps\appdata\roaming\python\python39\site-packages (from requests-&gt;smart_open&gt;=1.8.1-&gt;gensim) (1.26.3)
Using legacy 'setup.py install' for gensim, since package 'wheel' is not installed.
Installing collected packages: gensim
    Running setup.py install for gensim ... error
</code></pre>
<p>it runs fine till this point, and then:</p>
<pre><code>     Running setup.py install for gensim ... error
        ERROR: Command errored out with exit status 1:
         command: 'c:\program files\python39\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'C:\\Users\\PS\\AppData\\Local\\Temp\\pip-install-ba9y95a7\\gensim_d8e55913c8424bcea1d7f7d6b89f3a5c\\setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'C:\\Users\\PS\\AppData\\Local\\Temp\\pip-install-ba9y95a7\\gensim_d8e55913c8424bcea1d7f7d6b89f3a5c\\setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record 'C:\Users\PS\AppData\Local\Temp\pip-record-3mvtrgz3\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\Users\PS\AppData\Roaming\Python\Python39\Include\gensim'
             cwd: C:\Users\PS\AppData\Local\Temp\pip-install-ba9y95a7\gensim_d8e55913c8424bcea1d7f7d6b89f3a5c\
        Complete output (453 lines):
        running install
        running build
        running build_py
        creating build
        creating build\lib.win-amd64-3.9
        creating build\lib.win-amd64-3.9\gensim
        copying gensim\downloader.py -&gt; build\lib.win-amd64-3.9\gensim
        copying gensim\interfaces.py -&gt; build\lib.win-amd64-3.9\gensim
        copying gensim\matutils.py -&gt; build\lib.win-amd64-3.9\gensim
        copying gensim\nosy.py -&gt; build\lib.win-amd64-3.9\gensim
        copying gensim\utils.py -&gt; build\lib.win-amd64-3.9\gensim
        copying gensim\__init__.py -&gt; build\lib.win-amd64-3.9\gensim
        creating build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\bleicorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\csvcorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\dictionary.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\hashdictionary.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\indexedcorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\lowcorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\malletcorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\mmcorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\sharded_corpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\svmlightcorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\textcorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\ucicorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\wikicorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        copying gensim\corpora\__init__.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
        creating build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\atmodel.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\basemodel.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\base_any2vec.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\callbacks.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\coherencemodel.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\doc2vec.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\fasttext.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\hdpmodel.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\keyedvectors.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\ldamodel.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\ldamulticore.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\ldaseqmodel.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\lda_dispatcher.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\lda_worker.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\logentropy_model.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\lsimodel.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\lsi_dispatcher.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\lsi_worker.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\nmf.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\normmodel.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\phrases.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\poincare.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\rpmodel.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\tfidfmodel.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\translation_matrix.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\utils_any2vec.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\word2vec.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\_fasttext_bin.py -&gt; build\lib.win-amd64-3.9\gensim\models
        copying gensim\models\__init__.py -&gt; build\lib.win-amd64-3.9\gensim\models
        creating build\lib.win-amd64-3.9\gensim\parsing
        copying gensim\parsing\porter.py -&gt; build\lib.win-amd64-3.9\gensim\parsing
        copying gensim\parsing\preprocessing.py -&gt; build\lib.win-amd64-3.9\gensim\parsing
        copying gensim\parsing\__init__.py -&gt; build\lib.win-amd64-3.9\gensim\parsing
        creating build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\glove2word2vec.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\make_wiki.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\make_wikicorpus.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\make_wiki_lemma.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\make_wiki_online.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\make_wiki_online_lemma.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\make_wiki_online_nodebug.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\package_info.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\segment_wiki.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\word2vec2tensor.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\word2vec_standalone.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        copying gensim\scripts\__init__.py -&gt; build\lib.win-amd64-3.9\gensim\scripts
        creating build\lib.win-amd64-3.9\gensim\similarities
        copying gensim\similarities\docsim.py -&gt; build\lib.win-amd64-3.9\gensim\similarities
        copying gensim\similarities\index.py -&gt; build\lib.win-amd64-3.9\gensim\similarities
        copying gensim\similarities\levenshtein.py -&gt; build\lib.win-amd64-3.9\gensim\similarities
        copying gensim\similarities\nmslib.py -&gt; build\lib.win-amd64-3.9\gensim\similarities
        copying gensim\similarities\termsim.py -&gt; build\lib.win-amd64-3.9\gensim\similarities
        copying gensim\similarities\__init__.py -&gt; build\lib.win-amd64-3.9\gensim\similarities
        creating build\lib.win-amd64-3.9\gensim\sklearn_api
        copying gensim\sklearn_api\atmodel.py -&gt; build\lib.win-amd64-3.9\gensim\sklearn_api
        copying gensim\sklearn_api\d2vmodel.py -&gt; build\lib.win-amd64-3.9\gensim\sklearn_api
        copying gensim\sklearn_api\ftmodel.py -&gt; build\lib.win-amd64-3.9\gensim\sklearn_api
        copying gensim\sklearn_api\hdp.py -&gt; build\lib.win-amd64-3.9\gensim\sklearn_api
        copying gensim\sklearn_api\ldamodel.py -&gt; build\lib.win-amd64-3.9\gensim\sk
.
.
.
        creating build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_0.12.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_0.12.1.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_0.12.2.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_0.12.3.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_0.12.4.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_0.13.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_0.13.1.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_0.13.2.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_0.13.3.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_0.13.4.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_1.0.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_1.0.1.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_2.0.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_2.1.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_2.2.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_2.3.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_3.0.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_3.1.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_3.2.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_3.3.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        copying gensim\test\test_data\old_d2v_models\d2v_3.4.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_d2v_models
        creating build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_0.12.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_0.12.1.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_0.12.2.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_0.12.3.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_0.12.4.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_0.13.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_0.13.1.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_0.13.2.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_0.13.3.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_0.13.4.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_1.0.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_1.0.1.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_2.0.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_2.1.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_2.2.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_2.3.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_3.0.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_3.1.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_3.2.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_3.3.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        copying gensim\test\test_data\old_w2v_models\w2v_3.4.0.mdl -&gt; build\lib.win-amd64-3.9\gensim\test\test_data\old_w2v_models
        running build_ext
        building 'gensim.models.word2vec_inner' extension
        error: Microsoft Visual C++ 14.0 is required. Get it with &quot;Build Tools for Visual Studio&quot;: https://visualstudio.microsoft.com/downloads/
        ----------------------------------------
    ERROR: Command errored out with exit status 1: 'c:\program files\python39\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'C:\\Users\\PS\\AppData\\Local\\Temp\\pip-install-ba9y95a7\\gensim_d8e55913c8424bcea1d7f7d6b89f3a5c\\setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'C:\\Users\\PS\\AppData\\Local\\Temp\\pip-install-ba9y95a7\\gensim_d8e55913c8424bcea1d7f7d6b89f3a5c\\setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record 'C:\Users\PS\AppData\Local\Temp\pip-record-3mvtrgz3\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\Users\PS\AppData\Roaming\Python\Python39\Include\gensim' Check the logs for full command output.
</code></pre>
","14964517","","","","","2021-02-10 10:43:54","I'm having trouble installing Gensim on Windows 10 (Python 3.9)","<python><pip><windows-10><gensim>","0","1","","2021-02-10 11:38:02","","CC BY-SA 4.0"
"29837563","1","29886748","","2015-04-24 02:13:02","","1","292","<p>What is the difference between 
<code>gensim.models.ldamodel.LdaModel(...)</code> and <code>gensim.models.LdaModel(...)</code>?</p>

<p>The <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow"">docs</a> use <code>gensim.models.ldamodel.LdaModel(...)</code>.</p>

<p>However, <a href=""https://www.google.com/search?q=%22models.LdaModel%22&amp;oq=%22models.LdaModel%22&amp;aqs=chrome..69i57j0.9506j0j7&amp;sourceid=chrome&amp;es_sm=119&amp;ie=UTF-8#q=%22models.LdaModel(corpus%22&amp;tbas=0"" rel=""nofollow"">many people</a> seem to use <code>gensim.models.LdaModel(...)</code>.</p>
","3866874","","257465","","2015-04-27 03:41:47","2015-04-27 03:41:47","What is the difference between models.ldamodel.LdaModel and models.LdaModel?","<python><lda><gensim>","1","0","1","","","CC BY-SA 3.0"
"29939984","1","30160119","","2015-04-29 09:44:22","","2","4162","<p>Gensim is an optimized python port of Word2Vec (see <a href=""http://radimrehurek.com/2013/09/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow"">http://radimrehurek.com/2013/09/deep-learning-with-word2vec-and-gensim/</a>)</p>

<p>I am currently using these vectors: <a href=""http://clic.cimec.unitn.it/composes/semantic-vectors.html"" rel=""nofollow"">http://clic.cimec.unitn.it/composes/semantic-vectors.html</a></p>

<p>I am going to rerun the model training with gensim because there was some noisy tokens in their models. So i would like to find out what are some equivalent parameters for <code>word2vec</code> in <code>gensim</code></p>

<p>And the parameters they used from <code>word2vec</code> are:</p>

<ul>
<li>2-word context window, PMI weighting, no compression, 300K dimensions</li>
</ul>

<p>What is the gensim equivalence when i train a Word2Vec model?</p>

<p>Is it:</p>

<pre><code>&gt;&gt;&gt; model = Word2Vec(sentences, size=300000, window=2, min_count=5, workers=4)
</code></pre>

<p><strong>Is there a PMI weight option in gensim?</strong></p>

<p><strong>What is the default min_count used in word2vec?</strong></p>

<p>There's another set of parameters from word2vec as such:</p>

<ul>
<li>5-word context window, 10 negative samples, subsampling, 400 dimensions.</li>
</ul>

<p><strong>Is there a negative samples parameter in gensim?</strong></p>

<p><strong>What is the parameter equivalence of subsampling in gensim?</strong></p>
","610569","","610569","","2015-04-29 11:03:36","2015-05-11 05:20:29","Word2Vec and Gensim parameters equivalence","<python><nlp><neural-network><gensim><word2vec>","1","1","1","","","CC BY-SA 3.0"
"52488877","1","52489259","","2018-09-25 00:10:37","","1","568","<p>Using Gensim's Doc2Vec how would I find the distance between a <code>Doctag</code> and an <code>infer_vector()</code>?</p>

<p>Many thanks</p>
","1384522","","","","","2018-09-25 01:11:51","Finding the distance between 'Doctag' and 'infer_vector' with Gensim Doc2Vec?","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"47319033","1","47319235","","2017-11-15 23:25:04","","0","1853","<p>I have trained an topic model using a symmetric alpha in my lda distibution:</p>

<pre><code>model = gensim.models.ldamodel.LdaModel(bows, num_topics = 20, id2word = dictionary, passes = 100)
</code></pre>

<p>I can see that:</p>

<pre><code>model.alpha
array([ 0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,
    0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,
    0.05,  0.05])
</code></pre>

<p>where </p>

<pre><code>numpy.sum(model.alpha)
1.0000000000000002
</code></pre>

<p>I can't quite understand how gensim allows for lowering alpha parameter to allow each document to be a mixture of fewer topics?</p>
","1434041","","","","","2017-11-15 23:46:08","How to adjust alpha parameter in gensim LdaModel","<python><alpha><gensim><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"50046643","1","","","2018-04-26 15:18:50","","1","1488","<p>I'd like to use the reticulate package to run gensim from R. I'm not sure I fully understand the syntax of reticulate because I can get this to work with the default function settings but it fails when I try to pass more arguments. </p>

<pre><code>library(reticulate)
gensim &lt;- import(""gensim"")

model&lt;-gensim$models$Word2Vec$load(""word2vec_gensim"")

matrix(unlist(model$wv$most_similar(""queen"")),ncol=2,byrow=T)
      [,1]                           [,2]               
 [1,] ""princess""                     ""0.76466166973114"" 
 [2,] ""king""                         ""0.728749990463257""
 [3,] ""prince""                       ""0.653270363807678""
 [4,] ""lady""                         ""0.611525416374207""
 [5,] ""consort""                      ""0.609499335289001""
 [6,] ""duchess""                      ""0.608054518699646""
 [7,] ""monarch""                      ""0.606827557086945""
 [8,] ""lady-in-waiting""              ""0.605596661567688""
 [9,] ""empress""                      ""0.602727890014648""
 [10,] ""wiki/margrethe_ii_of_denmark"" ""0.59738427400589""
</code></pre>

<p>but...</p>

<pre><code>matrix(unlist(model$wv$most_similar(""queen"",topn = 25)),ncol=2,byrow=T)

Error in py_call_impl(callable, dots$args, dots$keywords) : 
TypeError: Partition index must be integer
</code></pre>

<p>Here ""word2vec_gensim"" is a pre-trained model, I can't include it because it is a large file but pick your fav pre-trained model. I think my issue is in how I'm providing additional args to the python function. </p>

<p>EDIT:I figured it out</p>

<p>looks like the R to python communication doesn't handle numbers as expected.</p>

<pre><code>matrix(unlist(model$wv$most_similar(""queen"",topn = as.integer(25))),ncol=2,byrow=T)
</code></pre>

<p>works</p>
","5265021","","5265021","","2018-04-26 16:40:55","2018-04-27 15:15:56","How to run a Python gensim functions from R with reticulate","<python><r><gensim><reticulate>","1","0","1","","","CC BY-SA 3.0"
"49279990","1","","","2018-03-14 14:14:05","","1","136","<p>I am trying to use gensim through IronPython, but installing this package seems to be impossible. I tried it with IronPython 2.7.7. with the following commands:</p>

<pre><code>ipy -X:Frames -m install ensurepip
ipy -X:Frames -m pip install setuptools
ipy -X:Frames -m pip install gensim
</code></pre>

<p>Does anyone know if it is possible to use the gensim-package in IronPython? Otherwise, I have to switch a part of my .NET application over to CPython, where I have gensim working.</p>

<p>Thanks in advance!</p>
","7890249","","5480409","","2018-03-14 14:18:10","2018-03-14 14:18:10","How to install gensim for IronPython?","<c#><python><pip><ironpython><gensim>","0","0","","","","CC BY-SA 3.0"
"55751027","1","55756270","","2019-04-18 17:25:33","","0","268","<p>I have a word2vec file in the standard format, but it is huge with 2M items. I also have a vocabulary file where each row is a word, the file has about ~800K rows. Now I want to load the embeddings from the word2vec file, and I want only embeddings for words in the vocabulary file. Is there an efficient implementation in gensim?</p>
","1363671","","","","","2019-04-19 03:36:17","How to load a word2vec txt file with vocabulary constraint","<gensim>","1","0","","","","CC BY-SA 4.0"
"57264086","1","57278871","","2019-07-30 03:56:34","","0","248","<p>I have a set of strings that I am tokenizing. I am sending each string into the <code>word2vec</code> model in <code>gensim</code>. Say, if there are 100 tokens (e.g. 'I', 'ate', 'pizza', etc.), it is generating a 100 * 100 3D matrix (<code>list</code> of <code>list</code> in <code>python</code>). How is it possible to convert the generated 3D token embeddings in to a 2D vector?  </p>

<p>I am sending this 3D into a model in <code>Tensorflow</code> library. I am doing the following,</p>

<p><code>model.add(Embedding(max_features, 128, input_length=maxlen))</code></p>

<p>Here max_features is the size of the token vector i.e. 100 and input_length is also the same. </p>

<p>But I am not sure If this is getting the job done. Is it the right way to convert 3D token embeddings in to 2D vectors? Ideally, I want to covert the embeddings into 2D vectors before sending into the model.</p>
","11839256","","","","","2019-07-30 19:32:43","How to combine 3D token embeddings into 2D vectors?","<python><tokenize><gensim><word2vec><word-embedding>","1","1","","","","CC BY-SA 4.0"
"50048484","1","50048953","","2018-04-26 17:03:20","","-1","804","<p>I am trying to join a lists of appended sentences into a large a string text object so that I can use it as an input for the Gensim summarize module. However, when I try to do this, it says the input sentences are less than 2. But when I run a split on the text, I see multiple sentences but it counts each sentence once instead of the total of sentences together. And the variable r is a string type object. I would like to concatenate the sentences together into one large string so it can be read through the Gensim summarize module.</p>

<p><strong>Sample Code:</strong></p>

<pre><code>import re
ruling_corpora  = re.findall(""\.?([^\.].\*?I find[^\.]*\. |[^\.]*$In sum[^\.]*\. |[^\.]*$agree[^\.]*\.)"", tokenized, re.I |re.DOTALL |re.M)[1:-1]

for r in ruling_corpora:                                   
    print(type(r))
    rc= ''.join(r)
    print(summarize(rc))
</code></pre>

<p>SAMPLE OUTPUT:</p>

<pre><code>raise ValueError(""input must have more than one sentence"")
ValueError: input must have more than one sentence
</code></pre>

<p>Here is an example of my input I want to summarize with the Gensim summarizer. The numbers underneath each string represent the count of sentences ending in periods:</p>

<pre><code>####Beginning of File### LUMB65.BL23607963.xml
Background Content: ANDERSON INITIAL DECISIONOn January 13, 2015, the appellant filed this appeal arguing that the agency's decision not to renew his term limited appointment which expired on January 28, 2015, is in error.  

 For the reasons discussed below, this appeal is DISMISSED for lack of jurisdiction without a hearing.
1
There is nothing in the agreement that curtails the agency's ability not to extend the term appointment. 
 IdIn reviewing the appellant's arguments, the appellant fails to establish that the Board has jurisdiction to review the agency's decision not to renew his time-limited appointment at issue in this appeal.
 Following a review of the record evidence, I find that the appellant has failed to non-frivolously allege Board jurisdiction over this appeal on any basis.
 Accordingly, this appeal must be dismissed for lack of jurisdiction.
1
####End of File### LUMB65.BL23607963.xml
</code></pre>
","9608799","","9608799","","2018-04-26 21:17:03","2018-04-27 13:31:50","joining sentences from a list in python3","<python><regex><list><join><gensim>","1","1","","","","CC BY-SA 3.0"
"61125887","1","","","2020-04-09 16:32:46","","0","57","<p>I am working with Word2vec (Gensim, in python) to understand meaning of sentences (by each word in them).
My goal is to be able to realize if the sentence indicates about the feeling of the speaker. 
Where can I find this kind of a dictionary of words?
For example one dictionary for words that indicate happiness and other for sadness.
Thanks </p>
","9746259","","","","","2020-04-11 19:03:50","build WORD2VEC words dictionary to indicate feelings","<python><gensim><word2vec><glove>","1","5","","","","CC BY-SA 4.0"
"14705944","1","","","2013-02-05 11:01:37","","2","507","<p>I am working on tf-idf model. I have little confusion as how this model is implemented. I have constructed model now when I am trying to print the model it is giving different value for the same term. For following two term are giving these result:</p>

<pre><code>doc_bow = [(0, 1), (1, 1)]
val1= tf_idf_corpus[doc_bow] 

doc_bow = [(0,1)]
val2=tf_idf_corpus[doc_bow] 
</code></pre>

<p>Following is the result:</p>

<pre><code>val1= [(0, 0.56486634414605663), (1, 0.82518241210720711)]
val2=[(0, 1.0)]
</code></pre>

<p>I am just curious to know, why tf-idf value of term 0 is 0.5648 in val1 and 1.0 in val2.</p>
","2632729","","610569","","2013-02-23 01:47:29","2013-12-04 22:09:11","Little confusion about how tf-idf model is implemented in gensim","<python><nlp><tf-idf><gensim>","1","0","","","","CC BY-SA 3.0"
"31814825","1","","","2015-08-04 16:42:17","","11","5209","<p>I am currently working on word2vec model using gensim in Python, and want to write a function that can help me find the antonyms and synonyms of a given word.
For example:
antonym(""sad"")=""happy""
synonym(""upset"")=""enraged""</p>

<p>Is there a way to do that in word2vec?</p>
","5115597","","","","","2019-08-28 14:29:27","How to obtain antonyms through word2vec?","<python><gensim><word2vec>","2","0","1","","","CC BY-SA 3.0"
"63998270","1","","","2020-09-21 18:46:59","","0","121","<p>I am trying to calculate the perplexity score in Spyder for different numbers of topics in order to find the best model parameters with gensim.</p>
<p>However, the perplexity score is not decreasing as it is supposed to [1].
Besides, there seem to be more persons experiencing this exact issue but no solution is available as far as I know.</p>
<p>Does anyone have any idea on how to solve the issue?</p>
<p>Code:</p>
<pre><code>X_train, X_test = train_test_split(corpus, train_size=0.9, test_size=0.1, random_state=1)

topic_range = [10, 20, 25, 30, 40, 50, 60, 70, 75, 90, 100, 150, 200]

def lda_function(X_train, X_test, dictionary, nr_topics):
    ldamodel2 = gensim.models.LdaModel(X_train,
                                       id2word=dictionary,
                                       num_topics=nr_topics,
                                       alpha='auto',
                                       eta=0.01,
                                       passes=10
                                       iterations=500, 
                                       random_state=42)
    return 2**(-1*ldamodel2.log_perplexity(X_test))

log_perplecs = [lda_function(X_train, X_test, dictionary, nr_topics=topic) for topic in topic_range]

print(&quot;\n&quot;,log_perplecs)

fig1, ax1 = plt.subplots(figsize=(7,5))
ax1.scatter(x=topic_range, y=log_perplecs)
fig1.tight_layout()

fig1.savefig(output_directory + &quot;Optimal Number of Topics (Perplexity Score).pdf&quot;, bbox_inches = 'tight')```




  [1]: https://i.stack.imgur.com/jFiF1.png
</code></pre>
","14316722","","6573902","","2020-10-02 05:45:45","2020-10-02 05:45:45","Gensim perplexity score increases","<python><python-3.x><gensim><lda><perplexity>","0","2","","","","CC BY-SA 4.0"
"49205736","1","","","2018-03-10 05:37:03","","4","1005","<p>For my application I'm comparing the similarity of one document against all other documents because I want to find the most similar other documents. In Gensim this can be done efficiently using the <a href=""https://radimrehurek.com/gensim/similarities/docsim.html"" rel=""nofollow noreferrer"">MatrixSimilarity method</a>. </p>

<p>In Spacy's <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer"">documentation</a> they have the example for comparing multiple documents, however for many documents the loop is not an efficient implementation:</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_lg')

doc1 = nlp(u""The labrador barked."")
doc2 = nlp(u""The labrador swam."")
doc3 = nlp(u""the labrador people live in canada."")

for doc in [doc1, doc2, doc3]:
    labrador = doc[1]
    dog = nlp(u""dog"")
    print(labrador.similarity(dog))
</code></pre>

<p>If someone could please suggest an efficient way compare one document to all others in Spacy it would be much appreciated. </p>

<p>I believe it may involve using a <a href=""https://spacy.io/usage/processing-pipelines"" rel=""nofollow noreferrer"">pipeline</a>, but I'm not sure how to use those. </p>

<p>I'll note that the example above from the documentation seems to have an <a href=""https://github.com/explosion/spaCy/issues/1791"" rel=""nofollow noreferrer"">issue</a>, so any ideas for how get around that issue are also welcome. </p>
","5539715","","5539715","","2018-03-10 14:43:52","2018-03-11 09:37:22","In Spacy how can I efficiently compare the similarity of one document to all others?","<performance><gensim><spacy>","1","1","1","","","CC BY-SA 3.0"
"48917449","1","48917515","","2018-02-21 23:52:20","","0","486","<p>I am converting a <code>gensim</code> w2v file to a <code>Tensorboard</code> tsv file with this code:</p>

<pre><code>with open(outfiletsv, 'w+b') as file_vector:
    with open(outfiletsvmeta, 'w+b') as file_metadata:
        for word in model.index2word:
            file_metadata.write(gensim.utils.to_utf8(word) + gensim.utils.to_utf8('\n'))
            vector_row = '\t'.join(str(x) for x in model[word])
            file_vector.write(vector_row + '\n')
</code></pre>

<p>It results in this error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
~\_repos\special\word2vec2tensor.py in &lt;module&gt;()
     79 
     80     logger.info(""running %s"", ' '.join(sys.argv))
---&gt; 81     word2vec2tensor(args.input, args.output, args.binary)
     82     logger.info(""finished running %s"", os.path.basename(sys.argv[0]))

~\_repos\special\word2vec2tensor.py in word2vec2tensor(word2vec_model_path, tensor_filename, binary)
     61                 file_metadata.write(gensim.utils.to_utf8(word) + gensim.utils.to_utf8('\n'))
     62                 vector_row = '\t'.join(str(x) for x in model[word])
---&gt; 63                 file_vector.write(vector_row + '\n')
     64 
     65     logger.info(""2D tensor file saved to %s"", outfiletsv)

TypeError: a bytes-like object is required, not 'str'
</code></pre>

<p>I added <code>b</code> to the original <code>w+</code> in the open file pieces to counteract the opposite issue (<code>TypeError: write() argument must be str, not bytes</code>). </p>

<p>I tried adding `vector_row = vector_row.encode('UTF-8'), but this did not work. </p>

<p>How do I remedy this <code>TypeError</code>?</p>
","7668467","","","","","2018-02-22 00:09:13","TypeError: a bytes-like object is required, not 'str' when converting gensim to tensorboard","<python><python-3.x><typeerror><gensim>","1","0","","","","CC BY-SA 3.0"
"37405958","1","","","2016-05-24 06:31:07","","0","555","<p>I would like to find or to design a parser that can find the wikipedia XML Dump but I am unable to find any nor do I have any idea on how to do it. </p>

<p>I have tried using wikiextractor but unfortunately it does not extract articles based on the user defined categories but instead it extract articles that contain the keyword.</p>

<p>For example in the xml file that I have downloaded, the wikipedia article ""Anarchism"".This article can be found in the following categories: </p>

<pre><code>[[Category:Anarchism| ]]
[[Category:Political culture]]
[[Category:Political ideologies]]
[[Category:Social theories]]
[[Category:Anti-fascism]]
[[Category:Anti-capitalism]]
[[Category:Far-left politics]]
</code></pre>

<p>For example part of the xml for Anarchism:</p>

<pre><code> &lt;page&gt;
    &lt;title&gt;Anarchism&lt;/title&gt;
    &lt;ns&gt;0&lt;/ns&gt;
    &lt;id&gt;12&lt;/id&gt;
    &lt;revision&gt;
      &lt;id&gt;716551092&lt;/id&gt;
      &lt;parentid&gt;714146352&lt;/parentid&gt;
      &lt;timestamp&gt;2016-04-22T10:19:33Z&lt;/timestamp&gt;
      &lt;contributor&gt;
        &lt;ip&gt;85.193.216.88&lt;/ip&gt;
      &lt;/contributor&gt;
      &lt;comment&gt;a better word;  use [[plain English]] -&amp;gt; [[WP:MOS]]&lt;/comment&gt;
      &lt;model&gt;wikitext&lt;/model&gt;
      &lt;format&gt;text/x-wiki&lt;/format&gt;
      &lt;text xml:space=""preserve""&gt;{{Redirect2|Anarchist|Anarchists|the fictional character|Anarchist (comics)|other uses|Anarchists (disambiguation)}}
{{pp-move-indef}}
{{Use British English|date=January 2014}}
{{Anarchism sidebar}}
'''Anarchism''' is a [[political philosophy]] that advocates [[self-governance|self-governed]] societies based on voluntary institutions. These are often described as [[stateless society|stateless societies]],&amp;lt;ref&amp;gt;&amp;quot;ANARCHISM, a social philosophy that rejects authoritarian government and maintains that voluntary institutions are best suited to express man's natural social tendencies.&amp;quot; George Woodcock. &amp;quot;Anarchism&amp;quot; at The Encyclopedia of Philosophy&amp;lt;/ref&amp;gt;&amp;lt;ref&amp;gt;&amp;quot;In a society developed on these lines, the voluntary associations which already now begin to cover all the fields of human activity would take a still greater extension so as to substitute themselves for the state in all its functions.&amp;quot; [http://www.theanarchistlibrary.org/HTML/Petr_Kropotkin___Anarchism__from_the_Encyclopaedia_Britannica.html Peter Kropotkin. &amp;quot;Anarchism&amp;quot; from the Encyclop√¶dia Britannica]&amp;lt;/ref&amp;gt;&amp;lt;ref&amp;gt;&amp;quot;Anarchism.&amp;quot; The Shorter Routledge Encyclopedia of Philosophy. 2005. p. 14 &amp;quot;Anarchism is the view that a society without the state, or government, is both possible and desirable.&amp;quot;&amp;lt;/ref&amp;gt;&amp;lt;ref&amp;gt;Sheehan, Sean. Anarchism, London: Reaktion Books Ltd., 2004. p. 85&amp;lt;/ref&amp;gt; although several authors have defined them more specifically as institutions based on non-[[Hierarchy|hierarchical]] [[Free association (communism and anarchism)|free associations]].&amp;lt;ref&amp;gt;&amp;quot;as many anarchists have stressed, it is not government as such that they find objectionable, but the hierarchical forms of government associated with the nation state.&amp;quot; Judith Suissa. ''Anarchism and Education: a Philosophical Perspective''. Routledge. New York. 2006. p. 7&amp;lt;/ref&amp;gt;&amp;lt;ref name=&amp;quot;iaf-ifa.org&amp;quot;/&amp;gt;&amp;lt;ref&amp;gt;&amp;quot;That is why Anarchy, when it works to destroy authority in all its aspects, when it demands the abrogation of laws and the abolition of the mechanism that serves to impose them, when it refuses all hierarchical organisation and preaches free agreement ‚Äî at the same time strives to maintain and enlarge the precious kernel of social customs without which no human or animal society can exist.&amp;quot; [[Peter Kropotkin]]. [http://www.theanarchistlibrary.org/HTML/Petr_Kropotkin__Anarchism__its_philosophy_and_ideal.html Anarchism: its philosophy and ideal]&amp;lt;/ref&amp;gt;&amp;lt;ref&amp;gt;&amp;quot;anarchists are opposed to irrational (e.g., illegitimate) authority, in other words, hierarchy ‚Äî hierarchy being the institutionalisation of authority within a society.&amp;quot; [http://www.theanarchistlibrary.org/HTML/The_Anarchist_FAQ_Editorial_Collective__An_Anarchist_FAQ__03_17_.html#toc2 &amp;quot;B.1 Why are anarchists against authority and hierarchy?&amp;quot;] in [[An Anarchist FAQ]]&amp;lt;/ref&amp;gt; Anarchism considers the [[state (polity)|state]] to be undesirable, unnecessary, and harmful.&amp;lt;ref name=&amp;quot;definition&amp;quot;&amp;gt;
</code></pre>

<p>I would like to search the Wikipedia XML dump for all articles that is contained in a particular category like for example, <code>[[Category:Anti-fascism]]</code> and to generate the XML file. I will then clean the XML file so that I can train it using gensim word2vec model.</p>

<p>Please advice me on how to do it, I have only basic programming experience and I need to do this in Python. </p>

<p>Thank you</p>
","5391187","","5391187","","2016-05-24 08:00:02","2016-05-24 08:00:02","Searching for articles in Wikipedia XML Dump based on its Categories","<python><wikipedia><gensim>","0","5","","","","CC BY-SA 3.0"
"38315158","1","","","2016-07-11 19:56:06","","0","132","<p>My goal is to convert a 6000-record CSV file into an array, clean and normalize it, so that I can read it into a corpus.Dictionary() to use in doc2bow in Gensim to perform a SparseMatrixSimiliarity query.  I was successful in reading in the CSV file at first, and it printed out an array I call ""definitions"" for each one of the 6000 sub-category record numbers.</p>

<pre><code> f = open('test.csv')
 csv_f = csv.reader(f)
 definitions = []

 for row in csv_f:
    definitions.append(row[2])

 print(definitions)
</code></pre>

<p>But then hit a wall with UTF-8 and ASCII errors.  Gensim has ""strict"" UTF-8 settings. </p>

<p>After several hours spent on Stack Overflow, researching, and <a href=""https://docs.python.org/2/library/csv.html"" rel=""nofollow"">trying to apply a few ""UTF-8"" encoders</a> per the Python CSV documentation, I read that since Python 2.7 doesn't have ""out of the box"" unicode-encoding using the import csv package, that I could use the codecs package. </p>

<p>I figured that instead of finding every line in my original ""definitions"" 6000-line array and decoding, that I could take an initial stab at decoding it right off the bat using codecs.  However, the below code fails to write anything to my definitions array. Being a newbie, I imagine that I may be using codecs the wrong way, and/or closing the wrong way.</p>

<pre><code> with codecs.open('test.csv', 'rb', encoding='utf-8') as f:    
     csv_f = csv.reader(f)
     definitions= []

     for row in csv_f:   
       definitions.append(np.array((array.float(i) for i in l)))

 f.close()        
 print(definitions)
</code></pre>

<p>I am a total newbie, apologies for any errors in my description.  Learning as I go, really appreciate any feedback and help. Perhaps I'm going about this the wrong way, and welcome any education.   Thank you again. </p>
","4399713","","","","","2016-07-11 19:56:06","Empty array after writing to CSV file python","<python><arrays><encoding><utf-8><gensim>","0","2","","","","CC BY-SA 3.0"
"46362028","1","","","2017-09-22 10:02:21","","0","245","<p>I am using PTVS in VS 2015 to write python code. When I write import gensim library and start debugging using F5, Debugger takes around 5 mins to load the library and move to next line.</p>

<pre><code>from gensim import utils
</code></pre>

<p>a. How do I fix this problem? 
b. If I start without debug(ctrl+F5) its fast to load. What code I should write to pause execution, attach the debugger then continue.</p>
","5113478","","5113478","","2017-09-22 10:07:51","2017-09-28 14:37:27","python tools for Visual studio 2015 debugging slow when importing gensim package","<python><visual-studio><gensim><ptvs>","1","0","","","","CC BY-SA 3.0"
"40379531","1","","","2016-11-02 12:04:37","","1","1478","<p>I have used three different ways to calculate the matching between the resume and the job description. Can anyone tell me that what method is the best and why?</p>

<ol>
<li><p>I used NLTK for keyword extraction and then RAKE for
keywords/keyphrase scoring, then I applied cosine similarity.</p></li>
<li><p>Scikit for keywords extraction, tf-idf and cosine similarity
calculation.</p></li>
<li><p>Gensim library with LSA/LSI model to extract keywords and calculate
cosine similarity between documents and query.</p></li>
</ol>
","1068837","","699305","","2016-11-02 16:32:20","2017-08-27 16:49:24","Best for resume, document matching","<scikit-learn><nltk><information-retrieval><tf-idf><gensim>","3","6","0","","","CC BY-SA 3.0"
"31821821","1","","","2015-08-05 01:06:27","","6","9165","<p><strong>Background</strong></p>

<p>I am trying to judge whether a phrase is semantically related to other words found in a corpus using Gensim.  For example, here is the corpus document pre-tokenized:</p>

<pre><code> **Corpus**
 Car Insurance
 Car Insurance Coverage
 Auto Insurance
 Best Insurance
 How much is car insurance
 Best auto coverage
 Auto policy
 Car Policy Insurance
</code></pre>

<p>My code (based on <a href=""https://radimrehurek.com/gensim/tut3.html#similarity-interface"" rel=""noreferrer"">this gensim tutorial</a>) judges the semantic relatendness of a phrase using cosine similarity against all strings in corpus.  </p>

<p><strong>Problem</strong></p>

<p>It seems that if a query contains ANY of the terms found within my dictionary, that phrase is judged as being semantically similar to the corpus (e.g.  **Giraffe Poop Car Murderer has a cosine similarity of 1 but SHOULD be semantically unrelated).  I am not sure how to solve for this issue.</p>

<p><strong>Code</strong></p>

<pre><code>#Tokenize Corpus and filter out anything that is a stop word or has a frequency &lt;1
texts = [[word for word in document if word not in stoplist]
        for document in documents]
from collections import defaultdict
frequency = defaultdict(int)
for text in texts:
    for token in text:
        frequency[token] += 1
texts = [[token for token in text if frequency[token] &gt; 1]
        for text in texts]
dictionary = corpora.Dictionary(texts)

# doc2bow counts the number of occurences of each distinct word, converts the word
# to its integer word id and returns the result as a sparse vector

corpus = [dictionary.doc2bow(text) for text in texts]  
lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
doc = ""giraffe poop car murderer""
vec_bow = dictionary.doc2bow(doc.lower().split())

#convert the query to LSI space
vec_lsi = lsi[vec_bow]              
index = similarities.MatrixSimilarity(lsi[corpus])

# perform a similarity query against the corpus
sims = index[vec_lsi]
sims = sorted(enumerate(sims), key=lambda item: -item[1])
</code></pre>
","3682157","","3682157","","2015-08-06 23:40:39","2015-08-14 14:18:21","Semantic Similarity between Phrases Using GenSim","<python-3.x><nltk><gensim>","1","0","5","","","CC BY-SA 3.0"
"55075312","1","55082751","","2019-03-09 08:10:46","","0","1386","<p>this is my code.it reads reviews from an excel file (rev column) and make a list of list.</p>

<p>xp is like this</p>

<pre><code>[""['intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one'],['better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', 'investigator', 'thrust', 'murder', 'investigation', 'invisible'],[ 'man', 'alone', 'tell', 'fun', 'flow', 'decent', 'clip', 'need', 'say', 'sequence', 'comedy', 'gold', 'like', 'scene', 'restaurant', 'excellent', 'costello', 'pretending', 'work', 'ball', 'gym', 'final', 'reel']""]
</code></pre>

<p>but when use list for model, it gives me error""TypeError: 'float' object is not iterable"".i don't know where is my problem.
Thanks.</p>

<pre><code>xp=[]
import gensim 
import logging
import pandas as pd 
file = r'FileNamelast.xlsx'
df = pd.read_excel(file,sheet_name='FileNamex')
pages = [i for i in range(0,1000)]


for page in  pages:

 text =df.loc[page,[""rev""]]
 xp.append(text[0])


model = gensim.models.Word2Vec (xp, size=150, window=10, min_count=2, 
workers=10)
model.train(xp,total_examples=len(xp),epochs=10)
</code></pre>

<p>this is what i got.TypeError: 'float' object is not iterable</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-32-aa34c0e432bf&gt; in &lt;module&gt;()
     14 
     15 
---&gt; 16 model = gensim.models.Word2Vec (xp, size=150, window=10, min_count=2, workers=10)
     17 model.train(xp,total_examples=len(xp),epochs=10)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in __init__(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)
    765             callbacks=callbacks, batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window,
    766             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,
--&gt; 767             fast_version=FAST_VERSION)
    768 
    769     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py in __init__(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)
    757                 raise TypeError(""You can't pass a generator as the sentences argument. Try an iterator."")
    758 
--&gt; 759             self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)
    760             self.train(
    761                 sentences=sentences, corpus_file=corpus_file, total_examples=self.corpus_count,

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py in build_vocab(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    934         """"""
    935         total_words, corpus_count = self.vocabulary.scan_vocab(
--&gt; 936             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)
    937         self.corpus_count = corpus_count
    938         self.corpus_total_words = total_words

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in scan_vocab(self, sentences, corpus_file, progress_per, workers, trim_rule)
   1569             sentences = LineSentence(corpus_file)
   1570 
-&gt; 1571         total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)
   1572 
   1573         logger.info(

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in _scan_vocab(self, sentences, progress_per, trim_rule)
   1552                     sentence_no, total_words, len(vocab)
   1553                 )
-&gt; 1554             for word in sentence:
   1555                 vocab[word] += 1
   1556             total_words += len(sentence)

TypeError: 'float' object is not iterable
</code></pre>
","","user10702710","","user10702710","2019-03-09 08:56:24","2019-03-09 22:49:31","train Word2vec model using Gensim","<python-3.x><gensim><word2vec>","1","2","","","","CC BY-SA 4.0"
"48914436","1","","","2018-02-21 19:59:24","","2","984","<p>I have a large sparse word embedding matrix that is trained from sklearn tfidf which has nothing to do with the Gensim word2vec. </p>

<p>It is very similar to: <a href=""https://stackoverflow.com/questions/47959639/gensim-word2vec-transfer-learning-from-a-non-gensim-model"">gensim Word2vec transfer learning (from a non-gensim model)</a> and <a href=""https://stackoverflow.com/questions/46297740/how-to-turn-embeddings-loaded-in-a-pandas-dataframe-into-a-gensim-model"">How to turn embeddings loaded in a Pandas DataFrame into a Gensim model?</a></p>

<p>However, given that the matrix is very sparse, I would like to store them more memore efficiently and reload them by gensim KeyedVecotrs. Or create the KeyedVectors instance without saving the sparse matrix and then directly save the gensim word2vec object. </p>

<p>Thanks. </p>

<hr>

<p>follow-up:</p>

<p>I end up successfully doing in this way:</p>

<pre><code>from collections import Counter, OrderedDict
import gensim
wc = Counter(corpus) # corpus = list of tokens in my documents
vocab = OrderedDict()
# building vocab for gensim
for idx, word in enumerate(words):
    tmp = gensim.models.keyedvectors.Vocab()
    tmp.index = idx
    tmp.count = wc[word]
    vocab[word] = tmp
w2v = gensim.models.keyedvectors.KeyedVectors()
w2v.index2word = words
w2v.vector_size = Xsparse.shape[1]
w2v.syn0 = Xsparse.toarray()  # have to make the matrix dense here
w2v.vocab = vocab
w2v.save(...)
</code></pre>

<p>However, I have to make the Xsparse word embedding matrix dense. Not sure if there is any more efficient way. Thanks for any answer.</p>
","9023765","","9023765","","2018-02-21 21:01:13","2018-02-21 21:01:13","How to efficiently transform a sparse word embedding matrix to gensim KeyedVectors object","<python><sparse-matrix><word2vec><gensim><word-embedding>","0","0","1","","","CC BY-SA 3.0"
"49514111","1","49528014","","2018-03-27 13:24:37","","0","1984","<p>I have trained an LDA model using gensim library and I am using it to extract topic vectors of a document and I am using the following code</p>

<pre><code>def clean_doc(data_string):    
    global en_stop
    tokenizer = RegexpTokenizer(r'\w+') #Create appropriate tokenizer
    p_stemmer = PorterStemmer() #Create object from Porter Stemmer
    #clean and tokenize document string
    raw = data_string.lower()
    tokens = tokenizer.tokenize(raw)
    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]
    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
    return stemmed_tokens

def infer_lda_vector(s, dictionary, model, dimensions):
    #s = s.decode('utf-8')
    vector = [0.0]*dimensions
    s = clean_doc(s)
    bow_vector = dictionary.doc2bow(s)   
    lda_vector = model[bow_vector]            
    for i in lda_vector:
        vector[i[0]] = i[1]
    return vector
</code></pre>

<p>I call it as follows:</p>

<pre><code>text = ""this a test""
lda_vector = infer_lda_vector(text, dictionary, lda_model, 300)
</code></pre>

<p>This exact piece of code was working when I was using Python2.7 but when I updated my system to Python3.x, its throwing the following error:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-36-723f03d03620&gt; in &lt;module&gt;()
      1 text = ""this a a test""
----&gt; 2 lda_vector = infer_lda_vector(text, dictionary, lda_model, 300)
      3 lda_vector

&lt;ipython-input-34-885205b68d9e&gt; in infer_lda_vector(s, dictionary, model, dimensions)
     34     s = clean_doc(s)
     35     bow_vector = dictionary.doc2bow(s)
---&gt; 36     lda_vector = model[bow_vector]
     37     for i in lda_vector:
     38         vector[i[0]] = i[1]

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\ldamodel.py in __getitem__(self, bow, eps)
   1158             `(topic_id, topic_probability)` 2-tuples.
   1159         """"""
-&gt; 1160         return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
   1161 
   1162     def save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs):

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\ldamodel.py in get_document_topics(self, bow, minimum_probability, minimum_phi_value, per_word_topics)
    979         if minimum_probability is None:
    980             minimum_probability = self.minimum_probability
--&gt; 981         minimum_probability = max(minimum_probability, 1e-8)  # never allow zero values in sparse output
    982 
    983         if minimum_phi_value is None:

TypeError: '&gt;' not supported between instances of 'float' and 'NoneType'
</code></pre>

<p>What am I doing wrong?</p>
","3667569","","3667569","","2018-03-27 13:39:46","2018-03-28 06:58:53","TypeError: '>' not supported between instances of 'float' and 'NoneType'","<python><gensim><lda>","1","2","1","","","CC BY-SA 3.0"
"49471037","1","","","2018-03-24 23:30:57","","1","2414","<p>I'm training a word2vec model on a corpus and then querying the model. </p>

<p>This works fine, but I am running an experiment and need to call the model for different conditions, save the model for each condition, query the model for each condition, and then save the output from the queries into a csv file, say, for further analyses of all the conditions.  </p>

<p>I've studied the gensim documentation and searched around, but can't figure out what to do. </p>

<p>I asked the gensim folks and they said that since the result of ""most_similar"" is a python object I can save it with pickle or save as txt, csv, whatever format I want. </p>

<p>Sounds great, but I don't have a clue how to start. Here's my code - could you help me ""fill in the blanks"" even with something simple that I can research further and expand on my own?</p>

<pre><code>#train the model
trained_model = gensim.models.Word2Vec(some hyperparamters)

#save the model in the format that is appropriate for querying by writing it to disk and call it stored_model
trained_model.save(some_filename)

#read in the stored model from disk and call it retrieved_model
retrieved_model = gensim.models.Word2Vec.load(some_filename)

#query the retrieved model
#each of these queries produces a tuple of 10 'word', cosine similarity pairs
retrieved_model.wv.most_similar(positive=['smartthings', 'amazon'], negative=['samsung'])
retrieved_model.wv.most_similar(positive=['light', 'nest'], negative=['hue'])
retrieved_model.wv.most_similar(positive=['shopping', 'new_york_times'], negative=['ebay'])
.
.
.
#store the results of all these queries in a csv so they can be analyzed.
?
</code></pre>
","8309998","","8309998","","2018-03-25 01:41:40","2020-07-30 09:51:14","save results of word2vec model query in a csv file?","<python><csv><word2vec><gensim>","2","2","2","","","CC BY-SA 3.0"
"49183643","1","49185137","","2018-03-08 22:36:26","","1","376","<p>I'm working with a large dataset of Yelp reviews for a machine learning research project. Gensim has worked well so far, however, when I build the vocabulary with <code>doc2vec.build_vocab()</code> on the over 5,000,000 documents I have...the indices appear to all be collected into a 64-key dictionary (which should certainly not be the case).</p>

<p>Below is the script I made for tagging the documents, building the vocabulary, and training the model.</p>

<pre><code>import os
import time
import pandas as pd
import numpy as np
from collections import namedtuple
from gensim.models.doc2vec import Doc2Vec
from keras.preprocessing.text import text_to_word_sequence

# keras helper function
def text2_word_seq(review):
  return text_to_word_sequence(review, 
       filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n', 
       lower=True, split="" "")

# instantiate the model
d2v = Doc2Vec(vector_size=300, 
  window=6, min_count=5, workers=os.cpu_count()-1)

chunksize = 5000
train_data = pd.read_json(""dataset/review.json"",
    chunksize=chunksize,
    lines=True)

Review = namedtuple('Review', 'words tags')
documents = list()
for i, data in enumerate(train_data):
    print(""Looked at %d chunks, %d documents"" % 
       (i, i*chunksize), end='\r', flush=True)
    users = data.user_id.values
    for j, review in enumerate(data.text):
        documents.append(Review(text2_word_seq(review), users[j]))

# build the vocabulary 
d2v.build_vocab(documents.__iter__(), update=False,
   progress_per=100000, keep_raw_vocab=False, trim_rule=None)

# train the model
d2v.train(documents, total_examples=len(documents), epochs=10)
d2v.save('d2v-model-v001')
</code></pre>

<p>After saving the model and loading it with <code>genim.models.Doc2Vec.load()</code>, the model's <code>docvecs.doctags</code> is of length 64. Each tag I am using when building the vocabulary is a user id. It is not necessarily unique, but there are thousands of users (not 64). Also, the tags appear as single characters - which is not expected...</p>

<pre><code>&gt;&gt;&gt; len(x.docvecs.doctags)
</code></pre>

<p>64</p>

<pre><code>&gt;&gt;&gt; x.docvecs.doctags

{'Y': Doctag(offset=27, word_count=195151634, doc_count=1727798), 
'j': Doctag(offset=47, word_count=198241878, doc_count=1739169), 
'4': Doctag(offset=17, word_count=195902251, doc_count=1728095), 
'J': Doctag(offset=50, word_count=197884244, doc_count=1741666), 
'W': Doctag(offset=41, word_count=198804200, doc_count=1741269), 
'O': Doctag(offset=23, word_count=196212468, doc_count=1728735), 
'o': Doctag(offset=9, word_count=194177928, doc_count=1709768), 
'n': Doctag(offset=3, word_count=193799059, doc_count=1714620), 
'3': Doctag(offset=34, word_count=197320036, doc_count=1725467), 
'F': Doctag(offset=10, word_count=195614702, doc_count=1729058) ...
</code></pre>

<p>What am I doing wrong here?  </p>
","7278264","","","","","2018-03-09 01:16:54","gensim docvecs.doctags incorrect indices","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"49274321","1","","","2018-03-14 09:43:55","","1","108","<p>I am using Word2Vec for word embeddings. I want to project a word W on an axis which represent the similarity of the word W to two given word W1 and W2 in a way that we can see to which word W is more similiar to, like this:
<a href=""https://i.stack.imgur.com/3NpB5.jpg"" rel=""nofollow noreferrer"">image</a></p>

<p>What is the best way to do it?</p>
","9160882","","9160882","","2018-03-14 13:13:09","2018-03-14 13:13:09","Project a word on an axis of similarity between 2 words","<vector><data-visualization><word2vec><gensim><word-embedding>","0","0","0","","","CC BY-SA 3.0"
"50084316","1","","","2018-04-29 07:25:27","","1","322","<p>I have been trying to install the gensim package in python using pip, using the <code>pip.main(['install','gensim'])</code>. It works and starts downloading, but then I get the following error:   </p>

<p><a href=""https://i.stack.imgur.com/hkgyw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hkgyw.png"" alt=""error in shell""></a></p>

<pre><code>THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.
gensim from https://files.pythonhosted.org/packages/82/f2/c2f2c87ed72483fce010fbfea1a3adbd168c0f0dafc878cbfb5a76381b03/gensim-3.4.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl#sha256=7bafe3f2fd49738942ef04396cb1e50a38283fe02203e5d4c66588daa01fb87c:
    Expected sha256 7bafe3f2fd49738942ef04396cb1e50a38283fe02203e5d4c66588daa01fb87c
         Got        728e9e79db209cfb0699c815c30a6169cefa61f46ef3471937100e4173fdbb3d
</code></pre>

<p>Any help would be appreciated </p>
","","user6912354","264837","","2018-04-29 08:02:00","2018-04-30 06:29:21","error when downloading the gensim package in python","<python><gensim>","2","2","","","","CC BY-SA 3.0"
"63979393","1","63979448","","2020-09-20 13:16:39","","0","93","<p>When I  try to install <code>gensim</code> through cmd prompt, it gives me following error:</p>
<blockquote>
<p>&quot;ERROR: Could not install packages due to an EnvironmentError:
[WinError 5] Access is denied:
'c:\programdata\anaconda3\lib\site-packages\<strong>pycache</strong>\cython.cpython-38.pyc'
Consider using the <code>--user</code> option or check the permissions. &quot;</p>
</blockquote>
<p>I'm unable to sort this issue, please help me out!</p>
","14242337","","5446749","","2020-09-20 13:34:01","2020-09-20 13:34:28","Unable to install 'gensim'","<python-3.x><gensim>","1","0","","","","CC BY-SA 4.0"
"24430238","1","","","2014-06-26 12:13:15","","5","3244","<p>EDIT: I've found an interesting issue here. <a href=""https://stackoverflow.com/questions/15067734/lda-model-generates-different-topics-everytime-i-train-on-the-same-corpus/15069580#15069580"">This link</a> shows that gensim uses randomness in both training and inference steps. So what it suggested here is to set a fixed seed in order to get same results every time. Why however am I getting for every topic the same probability?</p>

<p>What I want to do is to find for every twitter user her topics and  calculate the similarity between twitter users based on similarities in topics. Is there any possibility to calculate the same topics for every user in gensim or do I have to calculate a dictionary of topics and cluster every user topic?</p>

<p>In general, which is the best way to compare two twitter users based on topic-models extraction in gensim? My code is the following:</p>

<pre><code>   def preprocess(id): #Returns user word list (or list of user tweet)

        user_list =  user_corpus(id, 'user_'+str(id)+'.txt')
        documents = []
        for line in open('user_'+str(id)+'.txt'):
                 documents.append(line)
        #remove stop words
        lines = [line.rstrip() for line in open('stoplist.txt')]
        stoplist= set(lines)  
        texts = [[word for word in document.lower().split() if word not in stoplist]
                   for document in documents]
        # remove words that appear only once
        all_tokens = sum(texts, [])
        tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) &lt; 3)
        texts = [[word for word in text if word not in tokens_once]
                   for text in texts]
        words = []
        for text in texts:
            for word in text:
                words.append(word)

        return words


    words1 = preprocess(14937173)
    words2 = preprocess(15386966)
    #Load the trained model
    lda = ldamodel.LdaModel.load('tmp/fashion1.lda')
    dictionary = corpora.Dictionary.load('tmp/fashion1.dict') #Load the trained dict

    corpus = [dictionary.doc2bow(words1)]
    tfidf = models.TfidfModel(corpus)
    corpus_tfidf = tfidf[corpus]
    corpus_lda = lda[corpus_tfidf]

    list1 = []
    for item in corpus_lda:
      list1.append(item)

    print lda.show_topic(0)
    corpus2 = [dictionary.doc2bow(words2)]
    tfidf2 = models.TfidfModel(corpus2)
    corpus_tfidf2 = tfidf2[corpus2]
    corpus_lda2 = lda[corpus_tfidf2]

    list2 = []
    for it in corpus_lda2:
      list2.append(it)

    print corpus_lda.show_topic(0)  
</code></pre>

<p>Returned topic probabilities for user corpus (when using as a corpus a list of user words):</p>

<pre><code> [(0, 0.10000000000000002), (1, 0.10000000000000002), (2, 0.10000000000000002),
  (3, 0.10000000000000002), (4, 0.10000000000000002), (5, 0.10000000000000002),
  (6, 0.10000000000000002), (7, 0.10000000000000002), (8, 0.10000000000000002),
  (9, 0.10000000000000002)]
</code></pre>

<p>In the case where I use a list of user tweets, I get back calculated topics for every tweet.</p>

<p>Question 2: Does the following make sense: training LDA model with several twitter users and calculating the topic for every user (with every user corpus), using the LDA model calculated before?</p>

<p>In provided example, <code>list[0]</code> returns topic distribution with equal probabilities 0.1. 
Basically, every line of text corresponds to a different tweet. If I calculate corpus with <code>corpus = [dictionary.doc2bow(text) for text in texts]</code> it will give me the probabilities for every tweet separately. On the other hand, if I use <code>corpus = [dictionary.doc2bow(words)]</code> like the example, I'll have just all user words as corpus. In the second case, gensim returns the same probabilities for all topics. Thus, for both users I am getting the same topic distributions.</p>

<p>Should user text corpus be a list of words or a list of sentences (a list of tweets)?</p>

<p>Regarding the implementation of Qi He and Jianshu Weng in <a href=""http://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=1503&amp;context=sis_research"" rel=""nofollow noreferrer"">twitterRank approach</a> in page 264 it says that: we aggregate the tweets published by individual twitterer into a big document. Thus, each document corresponds to a twitterer. Ok I am confused, if document will be all user tweets then what should the corpus contain??</p>
","1194864","","-1","","2017-05-23 11:44:06","2017-12-20 04:52:18","LDA gensim implementation, distance between two different docs","<python><probability><gensim>","2","0","6","","","CC BY-SA 3.0"
"43776572","1","43956937","","2017-05-04 07:31:22","","20","17518","<p>I have trained a doc2vec and corresponding word2vec on my own corpus using gensim. I want to visualise the word2vec using t-sne with the words. As in, each dot in the figure has the ""word"" also with it.</p>

<p>I looked at a similar question here : <a href=""https://stackoverflow.com/questions/40581010/how-to-run-tsne-on-word2vec-created-from-gensim"">t-sne on word2vec</a></p>

<p>Following it, I have this code : </p>

<p>import gensim
import gensim.models as g</p>

<pre><code>from sklearn.manifold import TSNE
import re
import matplotlib.pyplot as plt

modelPath=""/Users/tarun/Desktop/PE/doc2vec/model3_100_newCorpus60_1min_6window_100trainEpoch.bin""
model = g.Doc2Vec.load(modelPath)

X = model[model.wv.vocab]
print len(X)
print X[0]
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X[:1000,:])

plt.scatter(X_tsne[:, 0], X_tsne[:, 1])
plt.show()
</code></pre>

<p>This gives a figure with dots but no words. That is I don't know which dot is representative of which word. How can I display the word with the dot?</p>
","4915693","","-1","","2017-05-23 12:26:07","2021-05-04 17:26:06","Visualise word2vec generated from gensim","<scikit-learn><data-visualization><gensim><word2vec>","2","0","11","","","CC BY-SA 3.0"
"49526981","1","","","2018-03-28 05:48:00","","1","2372","<p>I am using the below gibberish review data to train a doc2vec model in gensim. I face 2 errors.</p>

<p>1st : TaggedDocument takes 2 argument, I am unable to pass the <code>Sr</code> field as the 2nd argument so I resort to simple character<code>('tag')</code> in order to proceed further.</p>

<p>2nd: When I reach near the end of the code into for loop I get the following error.</p>

<p><strong><em>ValueError</strong>: You must specify either total_examples or total_words, for proper job parameters updationand progress calculations. The usual value is total_examples=model.corpus_count.</em></p>

<pre><code>| Sr   | review                                                     |
|------|------------------------------------------------------------|
| 123  | This is frustrating                                        |
| 456  | I am eating in a bowl and this is frustrating              |
| 678  | Summer has come and the weather is hot and I feel very hot |
| 1234 | When will winter come back I love the cool weather         |

import pandas as pd
import numpy as np
import gensim

file = pd.read_csv('/Users/test_text.csv')

file1 = [line.split() for line in file.review]

sent = [gensim.models.doc2vec.TaggedDocument(lines,'tag') for lines in file1]
model = gensim.models.Doc2Vec(alpha=0.025, min_alpha=0.025,min_count=1)  
model.build_vocab(sent)
for epoch in range(10):
        model.train(sent)
        model.alpha -= 0.002
        model.min_alpha = model.alpha 
</code></pre>
","1885727","","","","","2018-03-30 11:22:15","Doc2vec in gensim using csv","<python><nlp><data-science><gensim>","1","0","1","","","CC BY-SA 3.0"
"65978214","1","65983452","","2021-01-31 10:49:35","","0","260","<p>I'm using Gensim for building W2V models and, I didn't find a way for adding a vector for Unkown words or padding parts in Gensim and, I have to do it manually.
I also check the index of 0 in the created embedding and, it is also used for a specific word. This matter could cause a problem for padding words because they have the same index.</p>
<p>Am I missing something in here? Is Gensim handle this problem?</p>
<p>P.S: For handling this issue, I always append two vectors in the model weights after I train the model.</p>
","9099894","","9099894","","2021-01-31 11:28:49","2021-01-31 19:41:43","Does Gensim handling pad index and UNK index in W2V models?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"57496250","1","","","2019-08-14 14:00:39","","0","799","<p>I am pretty new to doc2vec then I made small research and found a couple of things. Here is my story: I am trying to learn using doc2vec 2.4 million documents. At first, I tried only doing so with a small model of 12 documents. I checked the results with infer vector of the first document and found it to be similar indeed to the first document by 0.97-0.99 cosine similarity measure. Which I found good, even though when I tried to enter a new document of completely different words I received a high score of 0.8 measure similarity. However, I had put it aside and tried to go on and build the full model with the 2.4 million documents. In this point, my problems began. The result was complete nonsense, I received in the most_similar function results with a similarity of 0.4-0.5 which were completely different from the new document checked. I tried to tune parameters but no result yet. I tried also to remove randomness both from the small and big model, however, I still got different vectors. Then I had tried to use get_latest_training_loss on each epoch in order to see how the loss changes over each epoch. This is my code: </p>

<pre><code>model = Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.025, pretrained_emb="".../glove.840B.300D/glove.840B.300d.txt"", seed=1, workers=1, compute_loss=True)

workers=1, compute_loss=True)
model.build_vocab(documents)

for epoch in range(10):
    for i in range(model_glove.epochs):
        model.train(documents, total_examples = token_count, epochs=1)
        training_loss = model.get_latest_training_loss()
        print(""Training Loss: "" + str(training_loss))

    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model.alpha # fix the learning rate, no decay
</code></pre>

<p>I know this code is a bit awkward, but it is used here only to follow the loss. 
The error I receive is: </p>

<pre><code>AttributeError: 'Doc2Vec' object has no attribute 'get_latest_training_loss'
</code></pre>

<p>I tried looking at model. and auto-complete and found that indeed there is no such function, I found something similar name training_loss, but it gives me the same error. </p>

<p>Anyone here can give me an idea? </p>

<p>Thanks in Advance</p>
","5565000","","","","","2019-08-14 18:26:04","'Doc2Vec' object has no attribute 'get_latest_training_loss'","<python><gensim><doc2vec><glove>","1","0","","","","CC BY-SA 4.0"
"49527668","1","","","2018-03-28 06:36:26","","2","2740","<p>After converting a list of text documents to corpora dictionary and then converting it to a bag of words model using:</p>

<pre><code>dictionary = gensim.corpora.Dictionary(docs) # docs is a list of text documents
corpus = [dictionary.doc2bow(doc) for doc in docs]
</code></pre>

<p>We can find out the index value of particular words in the dictionary using:</p>

<pre><code>dictionary.doc2idx([""righteous"",""height""])
</code></pre>

<p>Is there any way to find the word stored in dictionary at particular index?</p>
","9431817","","","","","2018-03-31 09:14:46","Understanding how words are stored in dictionary of gensim corpus after using ""gensim.corpora.Dictionary(TEXT)""","<python><gensim><corpus>","1","0","4","","","CC BY-SA 3.0"
"66343908","1","","","2021-02-24 02:38:22","","0","50","<p>I have made an LDA model in Python using gensim. The model has 5 topics. The gensim LDA model gives me the most significant terms for each topic in the model, along with their numerical value for that topic. However, I want to get the most significant/salient terms in the entire model so that I can accurately predict the terms that represent the entire document. How can I do this using gensim?</p>
","15271469","","","","","2021-02-24 02:38:22","Get the most significant terms from all topics of a gensim lda model","<gensim><lda><topic-modeling>","0","1","","","","CC BY-SA 4.0"
"64523731","1","64523739","","2020-10-25 12:34:04","","0","41","<p>I try to reproduce this lines of code from gensim</p>
<pre><code>import gensim
def coherence_values_computation(dictionary, corpus, texts, limit, start=2, step=3):
   coherence_values = []
   model_list = []
   for num_topics in range(start, limit, step):
      model = gensim.models.wrappers.LdaMallet(
         mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word
      )
      model_list.append(model)
   coherencemodel = CoherenceModel(
      model=model, texts=texts, dictionary=dictionary, coherence='c_v'
   )
   coherence_values.append(coherencemodel.get_coherence())
return model_list, coherence_values
</code></pre>
<p>However in the return function I receive this error:</p>
<blockquote>
<pre><code>File &quot;&lt;ipython-input-10-65490721eef3&gt;&quot;, line 13
    return model_list, coherence_values)
    ^
SyntaxError: invalid syntax
</code></pre>
</blockquote>
<p>Any idea with this happens?</p>
","14355763","","5820814","","2020-11-10 07:24:28","2020-11-10 07:24:28","Syntax for return after function","<python><python-3.x><gensim>","1","2","","2020-10-25 12:43:25","","CC BY-SA 4.0"
"31827623","1","31936431","","2015-08-05 08:47:26","","1","612","<p>In Gensim's Doc2Vec, how do you combine sentence vectors to make a single vector for a paragraph?  I realise you can train on the entire paragraph, but it would obviously be better to train on individual sentences, for context, etc. (I think...?)</p>

<p>Any advice or normal use case?</p>

<p>Also, how would I retrieve sentence/paragraph vectors from the model?</p>
","5189986","","","","","2015-08-11 08:09:34","Combining Doc2Vec sentences into paragraph vectors","<gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"49532089","1","","","2018-03-28 10:27:08","","2","2111","<p>I'm using gensim to do a LDA topic modeling work.
My data was pretreated by some other people. He gave me two things.
‚ë†the mmcorpus file(imported by <code>gensim.corpora.MmCorpus</code> function)
‚ë°the dictionary file(imported by <code>gensim.corpora.Dictionary.load</code> function)
I created the LDA model successfully and adjusted the superparameter ALPHA from 0.5-1.5 and I drew a visualized chart like this:
<a href=""https://i.stack.imgur.com/zRgHA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zRgHA.png"" alt=""enter image description here""></a>
I was confused why there are several tall bars there. And I found some strange words like this:
<a href=""https://i.stack.imgur.com/4dTsQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4dTsQ.png"" alt=""enter image description here""></a>
Interestingly the letter ""b"" which I haven't seen before appears. The man who gave me the data said the  letter ""b"" may generated automatically when he converted the data into bytes type. He doesn't know how to erase the ""b""  neither do I. How can I delete the ""b"" when I just have the mmcorpus file and the dictionary file?
Please!</p>
","8836671","","","","","2018-04-12 16:47:54","How to remove a word in LDA analysis by gensim","<python><text-mining><gensim><lda><stop-words>","1","0","1","","","CC BY-SA 3.0"
"40296765","1","","","2016-10-28 01:49:44","","1","1157","<p>I am preparing a Doc2Vec model using tweets. Each tweet's word array is considered as a separate document and is labeled as ""SENT_1"", SENT_2"" etc.</p>

<pre>
taggeddocs = []
for index,i in enumerate(cleaned_tweets):
    if len(i) > 2: # Non empty tweets
        sentence = TaggedDocument(words=gensim.utils.to_unicode(i).split(), tags=[u'SENT_{:d}'.format(index)])
        taggeddocs.append(sentence)

# build the model
model = gensim.models.Doc2Vec(taggeddocs, dm=0, alpha=0.025, size=20, min_alpha=0.025, min_count=0)

for epoch in range(200):
    if epoch % 20 == 0:
        print('Now training epoch %s' % epoch)
    model.train(taggeddocs)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model.alpha  # fix the learning rate, no decay
</pre>

<p>I wish to find tweets similar to a given tweet, say ""SENT_2"". How?</p>

<p>I get labels for similar tweets as:</p>

<pre>
sims = model.docvecs.most_similar('SENT_2')
for label, score in sims:
    print(label)
</pre>

<p>It prints as:</p>

<pre>
SENT_4372
SENT_1143
SENT_4024
SENT_4759
SENT_3497
SENT_5749
SENT_3189
SENT_1581
SENT_5127
SENT_3798
</pre>

<p>But given a label, how do I get original tweet words/sentence? E.g. what are the tweet words of, say, ""SENT_3497"". Can I query this to Doc2Vec model?</p>
","2838281","","","","","2017-01-19 03:54:21","How to extract words used for Doc2Vec","<python><nlp><gensim><doc2vec>","1","0","1","","","CC BY-SA 3.0"
"49478142","1","","","2018-03-25 16:30:27","","2","1033","<p>I have trained and saved a model with doc2vec in colab as</p>

<pre><code>model = gensim.models.Doc2Vec(vector_size=size_of_vector, window=10, min_count=5, workers=16,alpha=0.025, min_alpha=0.025, epochs=40)
model.build_vocab(allXs)
model.train(allXs, epochs=model.epochs, total_examples=model.corpus_count)
</code></pre>

<p>The model is saved in a folder not accessible from my drive but which I can see as:</p>

<pre><code>from os import listdir
from os.path import isfile, getsize
from operator import itemgetter

files = [(f, getsize(f)) for f in listdir('.') if isfile(f)]
files.sort(key=itemgetter(1), reverse=True)

for f, size in files:
    print ('{} {}'.format(size, f))
print ('({} files {} total size)'.format(len(files), sum(f[1] for f in files)))
</code></pre>

<p>The output is:</p>

<pre><code>79434928 Model_after_train.docvecs.vectors_docs.npy
9155086 Model_after_train
1024 .rnd
(3 files 88591038 total size)
</code></pre>

<p>To move the two files in the same shared directory as the notebook</p>

<pre><code>folder_id = FolderID

for f, size in files:
  if 'our_first_lda' in f:  
    file = drive.CreateFile({'parents':[{u'id': folder_id}]})
    file.SetContentFile(f)
    file.Upload()
</code></pre>

<p>The problem that I am facing now are two:
1) gensim creates two files when saving the model. Which one should I load?</p>

<p>2) when I try to load a file or the other with:</p>

<pre><code>from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

from googleapiclient.discovery import build
drive_service = build('drive', 'v3')

file_id = FileID


import io
from googleapiclient.http import MediaIoBaseDownload

request = drive_service.files().get_media(fileId=file_id)
downloaded = io.BytesIO()
downloader = MediaIoBaseDownload(downloaded, request)
done = False
while done is False:
  _, done = downloader.next_chunk()
model = doc2vec.Doc2Vec.load(downloaded.read())
</code></pre>

<p>I am not able to load the model getting the error:</p>

<pre><code>TypeError: file() argument 1 must be encoded string without null bytes, not str
</code></pre>

<p>Any suggestion?</p>
","1898959","","","","","2018-03-26 05:24:35","Load a saved Doc2Vec model in Colab","<python><gensim><google-colaboratory><doc2vec>","1","0","3","","","CC BY-SA 3.0"
"66191026","1","","","2021-02-13 23:34:28","","0","33","<p>I am trying to follow the Word2Vec tutorial on TensorFlow <a href=""https://www.tensorflow.org/tutorials/text/word2vec"" rel=""nofollow noreferrer"">here</a> with the Shakespeare dataset provided, and after being given the vectors.tsv and metadata.tsv file from the trained model, plugged them into the Embedding Projector <a href=""https://projector.tensorflow.org/"" rel=""nofollow noreferrer"">here</a>. The problem is when I tried to take these two files and plug them into Gensim by first combining them into one .vectors file, and processing them in my program. This .vectors file is formatted like the accepted answer <a href=""https://stackoverflow.com/questions/49750112/gensim-how-to-load-precomputed-word-vectors-from-text-file"">here</a>. The code I ran after creating the KeyedVectors object is this:</p>
<pre><code>word_vectors = KeyedVectors.load_word2vec_format('vectors.vectors', binary=False)
result = word_vectors.similar_by_word(&quot;queen&quot;)
</code></pre>
<p>The most similar word which (I think) is calculated by cosine similarity to &quot;queen&quot; is &quot;sacred&quot;, with score 0.389. However, after putting in the vectors and metadata into the Embedding projector, the most similar word is &quot;unrest&quot;, with score 0.609. In addition, the top 5 most frequent words in Gensim do not match the top 5 words in the Embedding Projector. Is my vectors file formatted incorrectly in Gensim, or does it use a different metric to calculate similarity? Or is there something else wrong entirely?</p>
","14623495","","","","","2021-02-13 23:34:28","Cosine similarity in Gensim differs from the Embedding Projector in Tensorflow","<python><python-3.x><tensorflow><nlp><gensim>","0","0","","","","CC BY-SA 4.0"
"17354417","1","17356483","","2013-06-27 22:39:49","","9","24002","<p>last parts of the code:   </p>

<pre><code>lda = LdaModel(corpus=corpus,id2word=dictionary, num_topics=2)
print lda
</code></pre>

<p>bash output:</p>

<pre><code>INFO : adding document #0 to Dictionary(0 unique tokens)
INFO : built Dictionary(18 unique tokens) from 5 documents (total  20 corpus positions)
INFO : using serial LDA version on this node
INFO : running online LDA training, 2 topics, 1 passes over the supplied corpus of 5 documents, updating model once every 5 documents
WARNING : too few updates, training might not converge; consider increasing the number of passes to improve accuracy
INFO : PROGRESS: iteration 0, at document #5/5
INFO : 2/5 documents converged within 50 iterations
INFO : topic #0: 0.079*cute + 0.076*broccoli + 0.070*adopted + 0.069*yesterday + 0.069*eat + 0.069*sister + 0.068*kitten + 0.068*kittens + 0.067*bananas + 0.067*chinchillas
INFO : topic #1: 0.082*broccoli + 0.079*cute + 0.071*piece + 0.070*munching + 0.069*spinach + 0.068*hamster + 0.068*ate + 0.067*banana + 0.066*breakfast + 0.066*smoothie
INFO : topic diff=0.470477, rho=1.000000
&lt;gensim.models.ldamodel.LdaModel object at 0x10f1f4050&gt;
</code></pre>

<p>So I'm wondering i'm able to save the resulting topics that it generated, to a readable format. I've tried the <code>.save()</code> methods, but it always outputs something unreadable. </p>
","2529936","","125617","","2016-08-16 14:42:13","2018-08-02 19:00:42","Gensim: How to save LDA model's produced topics to a readable format (csv,txt,etc)?","<python><lda><gensim>","4","1","5","","","CC BY-SA 3.0"
"49564330","1","49576266","","2018-03-29 20:15:54","","0","3337","<p>I am currently working with 9600 documents and applying gensim LDA. For training part, the process seems to take forever to get the model. I've tried to use multicore function as well, but it seems not working. I ran whole almost 3-days and I still can not get the lda model. I've checked some features of my data and the codes. I read this question <a href=""https://stackoverflow.com/questions/33929680/gensim-ldamulticore-not-multiprocessing"">gensim LdaMulticore not multiprocessing?</a>, but still don't get the solutions.</p>

<pre><code>corpora.MmCorpus.serialize('corpus_whole.mm', corpus)
corpus = gensim.corpora.MmCorpus('corpus_whole.mm')
dictionary = gensim.corpora.Dictionary.load('dictionary_whole.dict')

dictionary.num_pos
12796870

print(corpus)
MmCorpus(5275227 documents, 44 features, 11446976 non-zero entries)

# lda model training codes
lda = models.LdaModel(corpus, num_topics=45, id2word=dictionary,\
 update_every=5, chunksize=10000,  passes=100)

ldanulti = models.LdaMulticore(corpus, num_topics=45, id2word=dictionary,\
                            chunksize=10000, passes=100, workers=3)
</code></pre>

<p>This is my config to check BLAS, which I am not sure I installed proper one.
One thing I struggled here is, I can not use the command apt-get to install packages on my mac. I've installed Xcode but it still gives me an error. </p>

<pre><code>python -c 'import scipy; scipy.show_config()'
lapack_mkl_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
lapack_opt_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
blas_opt_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
blas_mkl_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
</code></pre>

<p>I have poor understanding on how to use shardedcorpus in python with my dictionary and corpora, so any helps will be appreciated! I haven't slept for 3 days to figure this problem!! Thanks!!</p>
","7170857","","","","","2018-03-30 14:28:54","Extremely slow LDA training model with large corpora python gensim","<python><machine-learning><multiprocessing><gensim><lda>","1","1","1","","","CC BY-SA 3.0"
"66195764","1","","","2021-02-14 13:13:05","","0","22","<p>I have two embedding spaces which I am applying a translation matrix to, in order to translate from embedding space V_i to embedding space V_j.</p>
<pre><code>source_model = Word2Vec.load('word2vec_1967')
target_model = Word2Vec.load('word2vec_1990')
</code></pre>
<p>I have created a set of word pairs, were in each pair, the first entry is the word from the source model and the second entry is its equivalent from the target model, as follows (think of them as standard stop word list since my goal is not to learn a mapping between embedding spaces NOT of two different languages, but rather embedding spaces of the same language in different times):</p>
<pre><code>word_pairs = [
    (&quot;w11&quot;, &quot;w12&quot;), (&quot;w21&quot;, &quot;w22&quot;), (&quot;w31&quot;, &quot;w32&quot;), etc
]
</code></pre>
<p>Now what I would do next is learn the translation matrix given the training words above:</p>
<pre><code>trans_model = TranslationMatrix(source_model, target_model, word_pairs=word_pairs)
</code></pre>
<p>My question is that, after learning the mapping, when we want to get the vector of the representative of word w_i from embedding space V_i (1967 in this example) in embedding space V_j (1990 in this example) should I do:</p>
<pre><code>trans_model.translate(w_i, topn=1) 
</code></pre>
<p>or should I do the matrix-vector multiplication (which will result in a vector; given that, for example, the translation matrix is 300 by 300 and the vector is 300 by 1 resulting in a new 300 by 1):</p>
<pre><code>trans_model.dot(source_model.wv[w_i])
</code></pre>
<p>What is the difference between the two methods and which one should I use?</p>
","11212687","","","","","2021-02-14 13:13:05","gensim TranslationMatrix .translate vs dot product","<python><matrix><gensim><word2vec><embedding>","0","0","","","","CC BY-SA 4.0"
"66197779","1","","","2021-02-14 16:56:32","","0","800","<pre><code>PAD = 0
UNK = 1
START = 2
END = 3
def make_vocab(wc, vocab_size):
    word2id, id2word = {}, {}
    word2id['&lt;pad&gt;'] = PAD
    word2id['&lt;unk&gt;'] = UNK
    word2id['&lt;start&gt;'] = START
    word2id['&lt;end&gt;'] = END
    for i, (w, _) in enumerate(wc.most_common(vocab_size), 4):
        word2id[w] = i
    return word2id
</code></pre>
<p>I got this error &quot;AttributeError: 'Word2Vec' object has no attribute 'most_common'&quot; when calling this function. I tried with different version of gensim. Could you give me some hints to  solve this.</p>
","10175905","","","","","2021-02-15 17:15:21","gensim - word2vec: AttributeError: 'Word2Vec' object has no attribute 'most_common'","<python><nlp><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"49628691","1","","","2018-04-03 11:15:01","","0","567","<p>I'm trying to load an already trained word2vec model downloaded from <a href=""http://hlt.isti.cnr.it/wordembeddings/"" rel=""nofollow noreferrer"">here</a> by using the following code, as suggested by the aforementioned website:</p>

<pre><code>from gensim.models import Word2Vec
model=Word2Vec.load('wiki_iter=5_algorithm=skipgram_window=10_size=300_neg-samples=10.m')
</code></pre>

<p>When I try to execute that code, I get the following error:</p>

<pre><code>UserWarning: detected Windows; aliasing chunkize to chunkize_serial
warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
Traceback (most recent call last):
File ""d:\DavideV\documents\visual studio 2017\Projects\tesi\tesi\tesi.py"", line 112, in &lt;module&gt;
model=Word2Vec.load('wiki_iter=5_algorithm=skipgram_window=10_size=300_neg-samples=10.m')
File ""C:\Users\admin\Anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 979, in load
return load_old_word2vec(*args, **kwargs)
File ""C:\Users\admin\Anaconda3\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 155, in load_old_word2vec
'size': old_model.vector_size,
AttributeError: 'Word2Vec' object has no attribute 'vector_size'
</code></pre>

<p>I suppose that this is due to the fact that the model has probably been trained with a previous version of gensim, but I would prefer to avoid to retrain it.</p>

<p>How can I solve this problem? Thanks.</p>
","6761184","","","","","2018-04-03 11:15:01","Gensim Word2Vec object has no attribute vector_size when loading file","<python-3.x><word2vec><gensim>","0","3","1","","","CC BY-SA 3.0"
"23473844","1","23706558","","2014-05-05 13:34:04","","0","930","<p>Problem statement: I have several documents(20k documents). I need to apply Topic modelling to find similar documents and then analyze those similar documents to find how those are different from each other. 
Q: Could anyone suggest me any Topic modelling package through which I can achieve this. I am exploring Mallet and Gensim Python. Not sure which would best fit in my requirement. </p>

<p>Any help would be highly appreciated. </p>
","3421622","","","","","2014-06-17 15:19:06","Topic Modelling and finding similarity in topics","<topic-modeling><gensim><mallet>","2","0","","","","CC BY-SA 3.0"
"49202413","1","","","2018-03-09 21:29:55","","1","894","<p>I set up an environment in anaconda for running gensim. it's been working great. today I updated gensim and some other packages in the environment. Now I get the following error in the terminal window. in my jupyter notebook, the kernel dies and it can't be restarted and when I try to import gensim it says module not found. I can't figure out where to start or what went wrong. I've been writing code for weeks and everything has been hunky-dory until i updated. any clues what to do?</p>

<p>Why will some code never be executed and what is ""image"" and why can't it find it?</p>

<p>Here is the complete terminal output for the failed session:</p>

<pre><code>Last login: Fri Mar  9 13:11:37 on ttys000
GWSB-FUN304-M1:~ dlhoffman$ /Users/dlhoffman/.anaconda/navigator/a.tool ; exit;
[I 13:11:41.634 NotebookApp] JupyterLab alpha preview extension loaded from /Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyterlab
JupyterLab v0.27.0
Known labextensions:
[I 13:11:41.636 NotebookApp] Running the core application with no additional extensions or settings
[I 13:11:41.640 NotebookApp] Serving notebooks from local directory: /Users/dlhoffman
[I 13:11:41.640 NotebookApp] 0 active kernels 
[I 13:11:41.640 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/?token=cb91dc32623736db4e7cf2baedee4284f4d3adc00cdf9f5c
[I 13:11:41.640 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 13:11:41.644 NotebookApp] 

    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8888/?token=cb91dc32623736db4e7cf2baedee4284f4d3adc00cdf9f5c
[I 13:11:41.758 NotebookApp] Accepting one-time-token-authenticated connection from ::1
[I 13:12:06.185 NotebookApp] Kernel started: 38a44a07-dfc6-4e61-8a69-df62ddc60d46
/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cffi/__pycache__/_cffi_ext.c:239:3: warning: 
      code will never be executed [-Wunreachable-code]
  _cffi_check__zmq_msg_t(0);
  ^~~~~~~~~~~~~~~~~~~~~~
/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cffi/__pycache__/_cffi_ext.c:272:3: warning: 
      code will never be executed [-Wunreachable-code]
  _cffi_check__zmq_pollitem_t(0);
  ^~~~~~~~~~~~~~~~~~~~~~~~~~~
2 warnings generated.
Traceback (most recent call last):
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/connect.py"", line 18, in &lt;module&gt;
    import jupyter_client
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/__init__.py"", line 4, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/connect.py"", line 22, in &lt;module&gt;
    import zmq
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/__init__.py"", line 47, in &lt;module&gt;
    from zmq import backend
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 40, in &lt;module&gt;
    reraise(*exc_info)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/utils/sixcerpt.py"", line 34, in reraise
    raise value
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 27, in &lt;module&gt;
    _ns = select_backend(first)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/select.py"", line 26, in select_backend
    mod = __import__(name, fromlist=public_api)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/__init__.py"", line 6, in &lt;module&gt;
    from . import (constants, error, message, context,
ImportError: dlopen(/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/error.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libsodium.23.dylib
  Referenced from: /Users/dlhoffman/anaconda3/lib/libzmq.5.dylib
  Reason: image not found
[I 13:12:09.186 NotebookApp] KernelRestarter: restarting kernel (1/5)
Traceback (most recent call last):
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/connect.py"", line 18, in &lt;module&gt;
    import jupyter_client
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/__init__.py"", line 4, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/connect.py"", line 22, in &lt;module&gt;
    import zmq
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/__init__.py"", line 47, in &lt;module&gt;
    from zmq import backend
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 40, in &lt;module&gt;
    reraise(*exc_info)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/utils/sixcerpt.py"", line 34, in reraise
    raise value
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 27, in &lt;module&gt;
    _ns = select_backend(first)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/select.py"", line 26, in select_backend
    mod = __import__(name, fromlist=public_api)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/__init__.py"", line 6, in &lt;module&gt;
    from . import (constants, error, message, context,
ImportError: dlopen(/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/error.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libsodium.23.dylib
  Referenced from: /Users/dlhoffman/anaconda3/lib/libzmq.5.dylib
  Reason: image not found
[I 13:12:12.194 NotebookApp] KernelRestarter: restarting kernel (2/5)
Traceback (most recent call last):
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/connect.py"", line 18, in &lt;module&gt;
    import jupyter_client
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/__init__.py"", line 4, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/connect.py"", line 22, in &lt;module&gt;
    import zmq
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/__init__.py"", line 47, in &lt;module&gt;
    from zmq import backend
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 40, in &lt;module&gt;
    reraise(*exc_info)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/utils/sixcerpt.py"", line 34, in reraise
    raise value
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 27, in &lt;module&gt;
    _ns = select_backend(first)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/select.py"", line 26, in select_backend
    mod = __import__(name, fromlist=public_api)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/__init__.py"", line 6, in &lt;module&gt;
    from . import (constants, error, message, context,
ImportError: dlopen(/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/error.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libsodium.23.dylib
  Referenced from: /Users/dlhoffman/anaconda3/lib/libzmq.5.dylib
  Reason: image not found
[I 13:12:15.202 NotebookApp] KernelRestarter: restarting kernel (3/5)
Traceback (most recent call last):
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/connect.py"", line 18, in &lt;module&gt;
    import jupyter_client
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/__init__.py"", line 4, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/connect.py"", line 22, in &lt;module&gt;
    import zmq
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/__init__.py"", line 47, in &lt;module&gt;
    from zmq import backend
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 40, in &lt;module&gt;
    reraise(*exc_info)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/utils/sixcerpt.py"", line 34, in reraise
    raise value
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 27, in &lt;module&gt;
    _ns = select_backend(first)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/select.py"", line 26, in select_backend
    mod = __import__(name, fromlist=public_api)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/__init__.py"", line 6, in &lt;module&gt;
    from . import (constants, error, message, context,
ImportError: dlopen(/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/error.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libsodium.23.dylib
  Referenced from: /Users/dlhoffman/anaconda3/lib/libzmq.5.dylib
  Reason: image not found
[W 13:12:16.348 NotebookApp] Timeout waiting for kernel_info reply from 38a44a07-dfc6-4e61-8a69-df62ddc60d46
[I 13:12:18.210 NotebookApp] KernelRestarter: restarting kernel (4/5)
kernel 38a44a07-dfc6-4e61-8a69-df62ddc60d46 restarted
Traceback (most recent call last):
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/ipykernel/connect.py"", line 18, in &lt;module&gt;
    import jupyter_client
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/__init__.py"", line 4, in &lt;module&gt;
    from .connect import *
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/jupyter_client/connect.py"", line 22, in &lt;module&gt;
    import zmq
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/__init__.py"", line 47, in &lt;module&gt;
    from zmq import backend
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 40, in &lt;module&gt;
    reraise(*exc_info)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/utils/sixcerpt.py"", line 34, in reraise
    raise value
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/__init__.py"", line 27, in &lt;module&gt;
    _ns = select_backend(first)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/select.py"", line 26, in select_backend
    mod = __import__(name, fromlist=public_api)
  File ""/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/__init__.py"", line 6, in &lt;module&gt;
    from . import (constants, error, message, context,
ImportError: dlopen(/Users/dlhoffman/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/error.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libsodium.23.dylib
  Referenced from: /Users/dlhoffman/anaconda3/lib/libzmq.5.dylib
  Reason: image not found
[W 13:12:21.219 NotebookApp] KernelRestarter: restart failed
[W 13:12:21.219 NotebookApp] Kernel 38a44a07-dfc6-4e61-8a69-df62ddc60d46 died, removing from map.
kernel 38a44a07-dfc6-4e61-8a69-df62ddc60d46 restarted failed!
[W 13:12:21.232 NotebookApp] Kernel deleted before session
[W 13:12:21.233 NotebookApp] 410 DELETE /api/sessions/b337ffd0-4c75-4d57-bb97-279e3a377e2d (::1) 1.80ms referer=http://localhost:8888/notebooks/Dropbox/_Tom%20and%20Donna/_Jupyter%20Donna/Donna%20pre-process%20the%20IFTTT%20data.ipynb
</code></pre>
","8309998","","","","","2018-05-15 22:06:17","updated environment in anaconda and now kernel dies and ""code will never be executed""","<python-3.x><image><kernel><anaconda><gensim>","2","0","","","","CC BY-SA 3.0"
"49635325","1","49635665","","2018-04-03 16:50:21","","1","1056","<p>I would like to tag a list of documents by <code>Gensim TaggedDocument()</code>, and then pass these documents as in input of <code>Doc2Vec()</code>. </p>

<p>I have read the documentation about <code>TaggedDocument</code> <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument"" rel=""nofollow noreferrer"">here</a>, but I don' t have understood what exactly are the parameters <code>words</code> and <code>tags</code>.</p>

<p>I have tried:</p>

<pre><code>texts = [[word for word in document.lower().split()]
          for document in X.values]

texts = [[token for token in text]
          for text in texts]

model = gensim.models.Doc2Vec(texts, vector_size=200)
model.train(texts, total_examples=len(texts), epochs=10)
</code></pre>

<p>But I get the error <code>'list' object has no attribute 'words'</code>.</p>
","7387749","","","","","2018-04-03 17:10:18","How to properly tag a list of documenta by Gensim TaggedDocument()","<nlp><gensim><doc2vec>","1","0","1","","","CC BY-SA 3.0"
"50373381","1","","","2018-05-16 14:17:03","","3","896","<p>While using ""<a href=""https://github.com/RaRe-Technologies/gensim/blob/8766edcd8e4baf3cfa08cdc22bb25cb9f2e0b55f/gensim/summarization/keywords.py#L200"" rel=""nofollow noreferrer"">keywords()</a>"" in summarization/keywords.py file, I am getting the same set of tags, no matter what value I choose for pos_tagger=['NN'], ['JJ'] or ['NN','JJ']</p>

<pre><code>from gensim.summarization import keywords
import requests

url='https://www.nytimes.com/2018/05/16/opinion/ramadan-spirit-america.html'
text = requests.get(url).text

print keywords(text,words=15,pos_filter=('NN'),lemmatize=True,scores=True)
print keywords(text,words=15,pos_filter('NN','JJ'),lemmatize=True,scores=True)
print keywords(text,words=15,pos_filter=('JJ'),lemmatize=True,scores=True)
</code></pre>

Expected Results

<p>If  I am giving pos_filter as 'NN', only nouns should come as tags, however, tags like ""started"", ""looking"" are also coming as output.
Similarly, there is no difference in the output irresepective of pos_filter='NN', pos_filter='NN','JJ', pos_filter='JJ' </p>

<p><strong>What is the correct way of using pos_filter to reflect appropriate output?</strong></p>

Actual Results

<p>student:0.20870111939889552, muslims:0.18960896637225794, americans:0.18895097005190414, ramadan:0.17605599898176202, month:0.12130699512494893, started:0.11817668681654464, community:0.11691583075245701, places:0.1117677772315554, spirituality:0.103727092629442, car:0.09988305780275739, white:0.09747271853405554, trump:0.09747271853405551, looking:0.09538360210000996, president:0.09538360210000986, black:0.0920316444206821</p>

<p>student:0.2087011193988958, muslims:0.18960896637225758, americans:0.1889509700519042, ramadan:0.17605599898176225, month:0.12130699512494901, started:0.11817668681654461, community:0.11691583075245732, places:0.11176777723155559, spirituality:0.10372709262944187, car:0.099883057802757, trump:0.09747271853405544, white:0.09747271853405512, president:0.0953836021000099, looking:0.09538360210000954, black:0.09203164442068222</p>

<p>student:0.20870111939889593, muslims:0.1896089663722575, americans:0.1889509700519037, ramadan:0.17605599898176255, month:0.1213069951249494, started:0.11817668681654483, community:0.11691583075245665, places:0.11176777723155547, spirituality:0.10372709262944207, car:0.09988305780275722, white:0.09747271853405541, trump:0.09747271853405526, looking:0.09538360210000975, president:0.0953836021000096, black:0.09203164442068222</p>
","6435600","","","","","2018-05-16 14:17:03","Correct way to use pos_tagger option in gensim + keywords extraction","<keyword><gensim><pos-tagger><summarization>","0","0","2","","","CC BY-SA 4.0"
"49585674","1","","","2018-03-31 08:07:33","","1","1532","<p>I will need a little help with diagnosing some problem I am experiencing with some text vector process. Actually, I am  trying apply doc2vec word embedding to obtain a vector for a classification task. After I run the code I get some errors which has been quite difficult to figure out, as I am pretty new. Below are the codes and the outputs</p>

<pre><code>    def constructLabeledSentences(data):
    sentences=[]
    for index, row in data.iteritems():
        sentences.append(TaggedDocument(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))
    return sentences

    x_raw_doc_sentences = constructLabeledSentences(x_raw_train['Text'])
x_raw_doc_model = Doc2Vec(min_count=5, window=5, vector_size=300, sample=0.001, negative=5, workers=4, epochs=10,seed=1)
x_raw_doc_model.build_vocab(x_raw_doc_sentences)
x_raw_doc_model.train(x_raw_doc_sentences, total_examples=x_raw_doc_model.corpus_count, epochs=x_raw_doc_model.epochs)
</code></pre>

<p>After running the model, I tried to extract the vectors with:</p>

<pre><code>x_raw_doc_train_arrays = np.zeros((x_raw_train.shape[0], 300))
for i in range (x_raw_train.shape[0]):
    x_raw_doc_train_arrays[i]=x_raw_doc_model.docvecs['Text_'+str(i)]
</code></pre>

<p>and this is the output i get:</p>

<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-106-bc0222fef295&gt; in &lt;module&gt;()
      1 x_raw_doc_train_arrays = np.zeros((x_raw_train.shape[0], 300))
      2 for i in range (x_raw_train.shape[0]):
----&gt; 3     x_raw_doc_train_arrays[i]=x_raw_doc_model.docvecs['Text_'+str(i)]
      4 
      5 

~\AppData\Local\Continuum\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in __getitem__(self, index)
   1197                 return self.vectors_docs[self._int_index(index, self.doctags, self.max_rawint)]
   1198             return vstack([self[i] for i in index])
-&gt; 1199         raise KeyError(""tag '%s' not seen in training corpus/invalid"" % index)
   1200 
   1201     def __contains__(self, index):

KeyError: ""tag 'Text_4' not seen in training corpus/invalid""
</code></pre>

<p>Is there anything I did wrong, or should be doing that I haven't?</p>
","7534616","","1222951","","2018-03-31 08:07:59","2018-03-31 16:39:21","tag 'Text_4' not seen in training corpus/invalid","<python><python-3.x><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"32056080","1","","","2015-08-17 17:11:15","","1","1491","<p>I'm trying to use Doc2Vec to read in a file that is a list of sentences like this:</p>

<pre><code>The elephant flaps its large ears to cool the blood in them and its body.

A house is a permanent building or structure for people or families to live in.

...
</code></pre>

<p>What I want to do is generate two files, one with unique words from these sentences and another that has one corresponding vector per line (if there's no vector output I want to output a vector of 0's)</p>

<p>I'm getting the vocab fine with my code but I can't seem to figure out how to print out the individual sentence vectors. I have looked through the documentation and haven't found much help.  Here is what my code looks like so far. </p>

<pre><code>sentences = []
for uid, line in enumerate(open(filename)):
    sentences.append(LabeledSentence(words=line.split(), labels=['SENT_%s' %       uid]))

model = Doc2Vec(alpha=0.025, min_alpha=0.025)
model.build_vocab(sentences)
for epoch in range(10):
    model.train(sentences)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
sent_reg = r'[SENT].*'
for item in model.vocab.keys():
    sent = re.search(sent_reg, item)
    if sent:
        continue
    else:
        print item

###I'm not sure how to produce the vectors from here and this doesn't work##   
sent_id = 0
for item in model:
    print model[""SENT_""+str(sent_id)]
    sent_id += 1
</code></pre>
","3834562","","4994021","","2015-08-31 23:06:17","2015-08-31 23:06:17","Using gensim's Doc2Vec to produce sentence vectors","<python><vector><gensim>","1","1","1","","","CC BY-SA 3.0"
"16645799","1","16739431","","2013-05-20 08:51:42","","45","100481","<p>From <a href=""https://stackoverflow.com/questions/15502802/creating-a-subset-of-words-from-a-corpus-in-r"">Creating a subset of words from a corpus in R</a>, the answerer can easily convert a <code>term-document matrix</code> into a word cloud easily.</p>

<p>Is there a similar function from python libraries that takes either a raw word textfile or <code>NLTK</code> corpus or <code>Gensim</code> Mmcorpus into a word cloud?</p>

<p>The result will look somewhat like this:
<img src=""https://i.stack.imgur.com/ieYK2.png"" alt=""enter image description here""></p>
","610569","","-1","","2017-05-23 12:10:24","2021-03-02 17:09:19","How to create a word cloud from a corpus in Python?","<python><nltk><corpus><gensim><word-cloud>","6","1","22","","","CC BY-SA 3.0"
"44050492","1","","","2017-05-18 14:23:39","","1","769","<p>I'm new to gensim, I was reading about <a href=""http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""nofollow noreferrer"">Experiments on the English Wikipedia</a> and from what I understand, it creates a model with topics and words and tries to relate them.</p>

<p>On my company, we have a list of phrases that we cluster manually after filtering them with a script that uses the Damerau-Levenshtein distance formula (actually, this data is on Elasticsearch and we use the fuzzyness search and the score to understand if the matching should be considered).</p>

<p>Example:</p>

<p><code>PHP Developer</code> is in the cluster <code>Developer</code>.</p>

<p>Let's say there is <code>Java Developer</code>, this too should be clustered as <code>Developer</code>.</p>

<p>The fuzzy search of Elasticsearch matches <code>Java Developer</code> to be similar to <code>PHP Developer</code> (Elasticsearch uses the Damerau-Levenshtein distance formula) so the script considers to put the same clusters of <code>PHP Developer</code> that are already validated (this validation is done manually).</p>

<p>My question is: can this gensim be useful to cluster words using Wikipedia's database as a ""dictionary""? </p>

<p>I also find this <a href=""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md?utm_campaign=buffer&amp;utm_content=buffer0df9b&amp;utm_medium=social&amp;utm_source=linkedin.com"" rel=""nofollow noreferrer"">pre-trained vectors</a> done by Facebook, I don't know if I can use this for my problem.</p>

<p>I tried to load one of the <code>.txt</code> files with this Python script:</p>

<pre><code>import gensim

sentences = gensim.models.KeyedVectors.load_word2vec_format('/Users/genesisxyz/Downloads/wiki.it.vec')
print(sentences)

p = sentences.similarity('uomo', 'donna')
print(p)
</code></pre>

<p>This was just a first experiment I was doing, but I still don't know where to start, I did a little of neural networks on other topics not related to words semantics, but here I have no clue.</p>

<p>Thanks in advance!</p>
","232194","","232194","","2017-05-19 07:50:36","2017-05-24 11:17:36","Word clustering with gensim","<python><neural-network><cluster-analysis><gensim><lda>","1","1","","","","CC BY-SA 3.0"
"44306123","1","44316225","","2017-06-01 11:20:24","","1","175","<p>Sentiment prediction based on document vectors works pretty well, as examples show:
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb</a>
<a href=""http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow noreferrer"">http://linanqiu.github.io/2015/10/07/word2vec-sentiment/</a></p>

<p>I wonder what pattern is in the vectors making that possible. I thought it should be similarity of vectors making that somehow possible. Gensim similarity measures rely on cosine similarity. Therefore, I tried the following:</p>

<p>Randomly initialised a fix ‚Äúcompare‚Äù vector, get cosine similarity of the ‚Äúcompare‚Äù vector with all other vectors in training and test set, use the similarities and the labels of the train set to estimate a logistic regression model, evaluate the model with the test set.</p>

<p>Looks like this, where train/test_arrays contain document vectors and train/test_labels labels either 0 or 1. (Notice, document vectors are obtained from genism doc2vec and are well trained, predicting the test set 80% right if directly used as input for the logistic regression):</p>

<pre><code>fix_vec = numpy.random.rand(100,1)
def cos_distance_to_fix(x):
    return scipy.spatial.distance.cosine(fix_vec, x)

train_arrays_cos =  numpy.reshape(numpy.apply_along_axis(cos_distance_to_fix, axis=1, arr=train_arrays), newshape=(-1,1))
test_arrays_cos = numpy.reshape(numpy.apply_along_axis(cos_distance_to_fix, axis=1, arr=test_arrays), newshape=(-1,1))

classifier = LogisticRegression()
classifier.fit(train_arrays_cos, train_labels)
classifier.score(test_arrays_cos, test_labels)
</code></pre>

<p>It turns out, that this approach does not work, predicting the test set only to 50%....
So, my question is, what ‚Äúinformation‚Äù is in the vectors, making the prediction based on vectors work, if it is not the similarity of vectors? Or is my approach simply not possible to capture similarity of vectors correct?</p>
","6933099","","130288","","2017-06-01 20:06:53","2017-06-02 01:06:04","What ‚Äúinformation‚Äù in document vectors makes sentiment prediction work?","<machine-learning><sentiment-analysis><gensim><feature-selection><doc2vec>","1","0","1","","","CC BY-SA 3.0"
"43617836","1","","","2017-04-25 17:57:08","","0","218","<p>I have trained a Latent Dirichlet Allocation (LDA) model on a corpus of documents using the <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">gensim package</a> in python. </p>

<p>I am able to retrieve the following:</p>

<ol>
<li>Distribution of topics over <strong>one document</strong></li>
<li>Distribution of words in a topic</li>
</ol>

<p>However, how can we obtain the distribution over topics (probability of a topic) in the <strong>entire corpus</strong>? </p>

<p>For example, if we 3 topics can be get a distribution as follows for the entire corpus (not just one document): 
[topic 1: 0.5, topic 2: 0.3, topic 3: 0.2]</p>

<p>Any help would be appreciated. Thank You. </p>
","2948166","","","","","2017-04-25 17:57:08","Distribution over topics for LDA using Gensim","<gensim><lda>","0","2","","","","CC BY-SA 3.0"
"23509699","1","","","2014-05-07 05:48:16","","6","2294","<p>I tried to examine the contents of the BOW corpus vs. the LDA[BOW Corpus] (transformed by LDA model trained on that corpus with, say,  35 topics)
I found the following output:</p>

<pre><code>DOC 1 : [(1522, 1), (2028, 1), (2082, 1), (6202, 1)]  
LDA 1 : [(29, 0.80571428571428572)]  
DOC 2 : [(1522, 1), (5364, 1), (6202, 1), (6661, 1), (6983, 1)]  
LDA 2 : [(29, 0.83809523809523812)]  
DOC 3 : [(3079, 1), (3395, 1), (4874, 1)]  
LDA 3 : [(34, 0.75714285714285712)]  
DOC 4 : [(1482, 1), (2806, 1), (3988, 1)]  
LDA 4 : [(22, 0.50714288283121989), (32, 0.25714283145449457)]  
DOC 5 : [(440, 1), (533, 1), (1264, 1), (2433, 1), (3012, 1), (3902, 1), (4037, 1), (4502, 1), (5027, 1), (5723, 1)]  
LDA 5 : [(12, 0.075870715371114297), (30, 0.088821329943986921), (31, 0.75219107156801579)]  
DOC 6 : [(705, 1), (3156, 1), (3284, 1), (3555, 1), (3920, 1), (4306, 1), (4581, 1), (4900, 1), (5224, 1), (6156, 1)]  
LDA 6 : [(6, 0.63896110435842401), (20, 0.18441557445724915), (28, 0.09350643806744402)]  
DOC 7 : [(470, 1), (1434, 1), (1741, 1), (3654, 1), (4261, 1)]  
LDA 7 : [(5, 0.17142855723258577), (13, 0.17142856888458904), (19, 0.50476192150187316)]  
DOC 8 : [(2227, 1), (2290, 1), (2549, 1), (5102, 1), (7651, 1)]  
LDA 8 : [(12, 0.16776844589094803), (19, 0.13980868559963203), (22, 0.1728575716782704), (28, 0.37194624921210206)]  
</code></pre>

<p>Where, 
    DOC N is the document from the BOW corpus 
    LDA N is the transformation of DOC N by that LDA model</p>

<p>Am I correct in understanding the output for each transformed document ""LDA N"" to be the topics that the document N belongs to? By that understanding, I can see some documents like 4, 5, 6, 7 and 8 to belong to more than 1 topic like DOC 8 belongs to topics 12, 19, 22 and 28 with the respective probabilities.</p>

<p>Could you please explain the output of LDA N and correct my understanding of this output, especially since in another thread <a href=""https://groups.google.com/forum/#!msg/gensim/Vv7SSC8HR8k/nQGNRE9HjacJ"" rel=""noreferrer"">HERE</a> - by the creator of Gensim himself, it's been mentioned that a document belongs to ONE topic? </p>
","215179","","215179","","2014-05-07 17:47:43","2018-05-16 10:20:33","Understanding LDA Transformed Corpus in Gensim","<python><nlp><lda><gensim>","1","0","1","","","CC BY-SA 3.0"
"35596031","1","35641434","","2016-02-24 07:39:48","","47","69930","<p>After training a word2vec model using python <a href=""http://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">gensim</a>, how do you find the number of words in the model's vocabulary?</p>
","2014591","","2956066","","2019-02-26 18:28:30","2021-10-20 17:52:46","gensim word2vec: Find number of words in vocabulary","<python><neural-network><nlp><gensim><word2vec>","3","0","11","","","CC BY-SA 3.0"
"44516295","1","44536662","","2017-06-13 08:37:05","","0","2567","<p>I have one tf-idf example from an ISI paper. I‚Äôm trying to validate my code by this example. But I get different result from my code.I don‚Äôt know what the reason is!</p>

<p>Term-document matrix from paper:  </p>

<pre><code>acceptance     [ 0 1 0 1 1 0
information      0 1 0 1 0 0
media            1 0 1 0 0 2
model            0 0 1 1 0 0
selection        1 0 1 0 0 0 
technology       0 1 0 1 1 0]
</code></pre>

<p>Tf-idf matrix from paper: </p>

<pre><code>acceptance     [ 0   0.4   0   0.3   0.7  0
information      0   0.7   0   0.5   0    0
media            0.3  0   0.2   0    0    1
model            0    0   0.6   0.5  0    0
selection        0.9  0   0.6   0    0    0 
technology       0   0.4   0   0.3   0.7  0]
</code></pre>

<p>My tf-idf matrix:</p>

<pre><code>acceptance     [ 0   0.4   0   0.3   0.7  0
information      0   0.7   0   0.5   0    0
media            0.5  0   0.4   0    0    1
model            0    0   0.6   0.5  0    0
selection        0.8  0   0.6   0    0    0 
technology       0   0.4   0   0.3   0.7  0]
</code></pre>

<p>My code:</p>

<pre><code>tfidf = models.TfidfModel(corpus)   
corpus_tfidf=tfidf[corpus]
</code></pre>

<p>I‚Äôve tried another code like this:</p>

<pre><code>transformer = TfidfTransformer()
tfidf=transformer.fit_transform(counts).toarray() ##counts is term-document matrix
</code></pre>

<p>But I didn‚Äôt get appropriate answer</p>
","5442235","","","","","2017-06-14 06:03:49","Tf-idf calculation using gensim","<python><tf-idf><gensim>","1","2","","","","CC BY-SA 3.0"
"44233296","1","44236287","","2017-05-29 00:37:15","","1","463","<p>I'm trying to use gensim's (ver 1.0.1) <code>doc2vec</code> to get the cosine similarities of documents. This should be relatively simple, but I'm having problems retrieving the vector of the documents so I can do cosine similarity. When I try to retrieve a document by the label I gave it in training, I get a key error. </p>

<p>For example, 
<code>print(model.docvecs['4_99.txt'])</code> 
will tell me that there is no such key as <code>4_99.txt</code>.</p>

<p>However if I print <code>print(model.docvecs.doctags)</code> I see things like this:
<code>'4_99.txt_3': Doctag(offset=1644, word_count=12, doc_count=1)</code></p>

<p>So it appears that for every document, <code>doc2vec</code> is saving each sentence as the ""document name underscore number""</p>

<p>So I'm either 
A) training incorrectly or
B) Don't understand how to retrieve the doc vector so that I can do <code>similarity(d1, d2)</code></p>

<p>Can anyone help me out here? </p>

<p>Here is how I train my doc2vec:</p>

<pre><code>#Obtain txt abstracts and txt patents 
filedir = os.path.abspath(os.path.join(os.path.dirname(__file__)))
files = os.listdir(filedir)

#Doc2Vec takes [['a', 'sentence'], 'and label']
docLabels = [f for f in files if f.endswith('.txt')]

sources = {}  #{'2_139.txt': '2_139.txt'}
for lable in docLabels:
    sources[lable] = lable
sentences = LabeledLineSentence(sources)


model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)
model.build_vocab(sentences.to_array())
for epoch in range(10):
    model.train(sentences.sentences_perm())

model.save('./a2v.d2v')
</code></pre>

<p>This uses this class </p>

<p><code>class LabeledLineSentence(object):</code></p>

<pre><code>def __init__(self, sources):
    self.sources = sources

    flipped = {}

    # make sure that keys are unique
    for key, value in sources.items():
        if value not in flipped:
            flipped[value] = [key]
        else:
            raise Exception('Non-unique prefix encountered')

def __iter__(self):
    for source, prefix in self.sources.items():
        with utils.smart_open(source) as fin:
            for item_no, line in enumerate(fin):
                yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])

def to_array(self):
    self.sentences = []
    for source, prefix in self.sources.items():
        with utils.smart_open(source) as fin:
            for item_no, line in enumerate(fin):
                self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))
    return self.sentences

def sentences_perm(self):
    shuffle(self.sentences)
    return self.sentences
</code></pre>

<p>I got this class from a web tutorial (<a href=""https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1"" rel=""nofollow noreferrer"">https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1</a>) to help me get around Doc2Vec's weird data formatting requirements and I don't completely understand it to be honest. It does look like this class written here is adding the <code>_n</code> for each sentence, but in the tutorial it seems that they still retrieve the document vector with just giving it the filename... So what am I doing wrong here?</p>
","5119424","","","","","2017-05-29 06:57:08","Problems accessing docvectors with gensim","<gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"61990540","1","","","2020-05-24 18:30:54","","1","304","<p>I have a large dataframe (~4.7M rows) where one of the columns contains document text. I am trying unsuccessfully to run Gensim summarize on a specific column for the entire dataframe.</p>

<pre class=""lang-py prettyprint-override""><code>df['summary'] = df['variable_content'].apply(lambda x: summarize(x, word_count=200))
</code></pre>

<p>Extracting each row of <code>variable_content</code> into a variable and running summarize works well, but is slow and ugly. I also get the error:</p>

<pre><code>ValueError: input must have more than one sentence
</code></pre>

<p>but can't find a row with only one sentence (most are hundreds/thousands). Can anyone help?</p>
","2366762","","4756173","","2020-05-25 06:00:51","2020-05-25 06:00:51","Summarize Pandas dataframe column","<python><pandas><dataframe><gensim>","1","9","","","","CC BY-SA 4.0"
"57688029","1","59787104","","2019-08-28 08:26:15","","1","53","<p>I am trying to process a large corpus but in preprocess_string( ) it returns an error shown below</p>

<pre><code>Traceback (most recent call last): File ""D:/Projects/docs_handler/data_preprocessing.py"", line 60, in &lt;module&gt; for temp in batch(iterator,1000): File ""D:/Projects/docs_handler/data_preprocessing.py"", line 30, in batch for item in iterable: File ""D:/Projects/docs_handler/data_preprocessing.py"", line 23, in iter_tokenized_documents document = preprocess_string(open(os.path.join(root, file)).read().strip(),filters=CUSTOM_FILTERS) File ""C:\Users\koradg\AppData\Local\Programs\Python\Python36\lib\encodings\cp1252.py"", line 23, in decode return codecs.charmap_decode(input,self.errors,decoding_table)[0] UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 16144: character maps to &lt;undefined&gt;
</code></pre>

<hr>

<pre><code>Versions
Windows-10-10.0.17763-SP0
Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)]
NumPy 1.17.0
SciPy 1.3.0
gensim 3.8.0
FAST_VERSION 0
</code></pre>

<hr>

<pre><code>def iter_tokenized_documents(input_directory):
    """"""Iterate over all documents, yielding a document (=list of utf8 tokens) at a time.""""""
    for root, dirs, files in os.walk(input_directory):
        for file in filter(lambda file: file.endswith('.txt'), files):
            document = preprocess_string(open(os.path.join(root, file)).read().strip(),filters=CUSTOM_FILTERS)
            if(len(document)):
                yield document
</code></pre>

<p>How to run it without any error?</p>
","7786625","","8321664","","2019-08-28 08:27:50","2020-04-23 08:07:00","Having issue with character encoding while processing a text","<python><encoding><gensim>","2","1","2","","","CC BY-SA 4.0"
"35738391","1","","","2016-03-02 04:15:29","","1","328","<p>I am using HDP (Hierarchical Dirichilet Process) package from gensim topic modelling software. Gensim HDP implementation expects user to provide number of topics (T) in advance.</p>

<p><code>hdpmodel.HdpModel(self, corpus, id2word,T=150)
</code></p>

<p>The documentation defines T as top level truncation level. </p>

<p>Can HDP determine number of topics on its own?
Is there an implementation of HDP which can detect number of topics? Any help is appreciated.</p>
","2355376","","2355376","","2016-03-02 04:28:02","2016-03-02 13:17:04","Can HDP (Hierarchical Dirichilet Process) detect the number of topics from the data?","<machine-learning><data-mining><gensim><topic-modeling><unsupervised-learning>","1","2","","","","CC BY-SA 3.0"
"53286476","1","","","2018-11-13 17:24:55","","1","58","<p>I am using Glove, <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Gensim-word2vec</a>, module and I can use it to return the similarity score between entities such as <code>'man'</code> and <code>'woman'</code> will return <code>0.89034</code>. But is there a way to return the semantic relationship between two entities? For example given the word <code>'people'</code> and a <code>'location'</code>, the result should be the relationship <code>'lives_in'</code>?</p>

<p>I can do something like: </p>

<pre><code>print(model.most_similar(positive=['king', 'woman'], negative=['man']))
</code></pre>

<p>Output is:</p>

<pre><code>[('queen', 0.775162398815155), ('prince', 0.6123066544532776), ('princess', 0.6016970872879028), ('kings', 0.5996100902557373), ('queens', 0.565579891204834), ('royal', 0.5646308660507202), ('throne', 0.5580971240997314), ('Queen', 0.5569202899932861), ('monarch', 0.5499411821365356), ('empress', 0.5295248627662659)]
</code></pre>

<p>Desired output:</p>

<pre><code>[(is_a, 0.3223), (same_as, 0349230), (people, 0302432) ...]
</code></pre>
","10072671","","10072671","","2018-11-13 18:27:15","2018-11-14 10:17:13","Is there a way to get the relationship from 'GloVe' word2vec?","<relationship><semantic-web><gensim><word2vec><glove>","2","1","","","","CC BY-SA 4.0"
"53376459","1","53393686","","2018-11-19 14:11:13","","2","1566","<p>I have a model based on <code>doc2vec</code> trained on multiple documents. I would like to use that model to infer the vectors of another document, which I want to use as the corpus for comparison. So, when I look for the most similar sentence to one I introduce, it uses this new document vectors instead of the trained corpus.
Currently, I am using the <code>infer_vector()</code> to compute the vector for each one of the sentences of the new document, but I can't use the <code>most_similar()</code> function with the list of vectors I obtain, it has to be <code>KeyedVectors</code>.</p>

<p>I would like to know if there's any way that I can compute these vectors for the new document that will allow the use of the <code>most_similar()</code> function, or if I have to compute the similarity between each one of the sentences of the new document and the sentence I introduce individually (in this case, is there any implementation in Gensim that allows me to compute the cosine similarity between 2 vectors?).</p>

<p>I am new to Gensim and NLP, and I'm open to your suggestions.</p>

<p>I can not provide the complete code, since it is a project for the university, but here are the main parts in which I'm having problems.</p>

<p>After doing some pre-processing of the data, this is how I train my model:</p>

<pre><code>documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_data)]
assert gensim.models.doc2vec.FAST_VERSION &gt; -1

cores = multiprocessing.cpu_count()

doc2vec_model = Doc2Vec(vector_size=200, window=5, workers=cores)
doc2vec_model.build_vocab(documents)
doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=30)
</code></pre>

<p>I try to compute the vectors for the new document this way:</p>

<pre><code>questions = [doc2vec_model.infer_vector(line) for line in lines_4]
</code></pre>

<p>And then I try to compute the similarity between the new document vectors and an input phrase:</p>

<pre><code>text = str(input('Me: '))

tokens = text.split()

new_vector = doc2vec_model.infer_vector(tokens)

index = questions[i].most_similar([new_vector])
</code></pre>
","6058501","","6058501","","2018-11-19 14:28:52","2018-11-20 13:33:34","Gensim Doc2vec model: how to compute similarity on a corpus obtained using a pre-trained doc2vec model?","<python><nlp><gensim><doc2vec>","1","2","1","","","CC BY-SA 4.0"
"49631758","1","49635851","","2018-04-03 13:47:23","","3","1388","<p>I am struggling with Doc2Vec and I cannot see what I am doing wrong.
I have a text file with sentences. I want to know, for a given sentence, what is the closest sentence we can find in that file.</p>

<p>Here is the code for model creation:</p>

<pre><code>sentences = LabeledLineSentence(filename)

model = models.Doc2Vec(size=300, min_count=1, workers=4, window=5, alpha=0.025, min_alpha=0.025)
model.build_vocab(sentences)
model.train(sentences, epochs=50, total_examples=model.corpus_count)
model.save(modelName)
</code></pre>

<p>For test purposes, here is my file:</p>

<pre><code>uduidhud duidihdd
dsfsdf sdf sddfv
dcv dfv dfvdf g fgbfgbfdgnb
i like dogs
sgfggggggggggggggggg ggfggg
</code></pre>

<p>And here is my test:</p>

<pre><code>test = ""i love dogs"".split()
print(model.docvecs.most_similar([model.infer_vector(test)]))
</code></pre>

<p>No matter what parameter for training, this should obviously tell me that the most similar sentence is the 4th one (SENT_3 or SENT_4, I don't know how their indexes work, but the sentence labels are this form). But here is the result:</p>

<pre><code>[('SENT_0', 0.15669342875480652),
 ('SENT_2', 0.0008485736325383186),
 ('SENT_4', -0.009077289141714573)]
</code></pre>

<p>What am I missing ? And if I try with the same sentence (I LIKE dogs), I have SENT_2, then 1 then 4... I really don't get it. And why such low numbers ? And when I run few times in a row with a load, I don't get the same results either.</p>

<p>Thanks for your help</p>
","9498501","","9498501","","2018-04-03 13:57:17","2018-04-03 17:22:25","Gensim Doc2Vec most_similar() method not working as expected","<python><nlp><gensim><doc2vec><sentence-similarity>","1","0","1","","","CC BY-SA 3.0"
"57760664","1","","","2019-09-02 17:11:25","","2","724","<p>I am trying to get unique words for each topic.</p>

<p>I am using gensim and this is the line that help me to generate my model</p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary)
</code></pre>

<p>But I have repeated words in two different topics, I would like to have different words per topic</p>
","11985685","","6275103","","2019-09-02 17:11:46","2020-04-23 01:57:03","How can I get unique words per each topic LDA?","<python><gensim><word><lda>","2","0","","","","CC BY-SA 4.0"
"44710644","1","44711455","","2017-06-22 23:06:43","","1","1581","<p>I trained a word2vec model using gensim package and saved it with the following name. </p>

<pre><code>model_name = ""300features_1minwords_10context""
model.save(model_name)
</code></pre>

<p>I got these log message info. while the model was getting trained and saved.</p>

<pre><code>INFO : not storing attribute syn0norm
INFO : not storing attribute cum_table
</code></pre>

<p>Then, I tried to load the model using this, </p>

<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load(""300features_1minwords_10context"")
</code></pre>

<p>I got the following error. </p>

<pre><code>2017-06-22 21:27:14,975 : INFO : loading Word2Vec object from 300features_1minwords_10context
2017-06-22 21:27:15,496 : INFO : loading wv recursively from 300features_1minwords_10context.wv.* with mmap=None
2017-06-22 21:27:15,497 : INFO : setting ignored attribute syn0norm to None
2017-06-22 21:27:15,498 : INFO : setting ignored attribute cum_table to None
2017-06-22 21:27:15,499 : INFO : loaded 300features_1minwords_10context
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-25-9d90db0f07c0&gt; in &lt;module&gt;()
      1 from gensim.models import Word2Vec
      2 model = Word2Vec.load(""300features_1minwords_10context"")
----&gt; 3 model.syn0.shape

AttributeError: 'Word2Vec' object has no attribute 'syn0'
</code></pre>

<p>Also, in the file ""300features_1minwords_10context"", it shows that </p>

<pre><code>""300features_1minwords_10context"" is not UTF-8 encoded
Saving disabled.
Open console for more details 
</code></pre>

<p>To fix the above attribute error, I have also tried the following from the google forum, </p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format(""300features_1minwords_10context"")
model.syn0.shape
</code></pre>

<p>It resulted in another error which is </p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>The model is trained with UTF-8 encoded sentences. I am not sure why is it throwing this error ?</p>

<p>More info : </p>

<pre><code>df = pd.read_csv('UNSPSCdataset.csv',encoding='mac_roman',low_memory=False)
features = ['MaterialDescription']
temp_features = df[features]
temp_features.to_csv('materialDescription', encoding='UTF-8')
X = pd.read_csv('materialDescription',encoding='UTF-8')
</code></pre>

<p>Here, I had to use 'mac_roman' encoding in order to access it using pandas dataframe. Since the text in the dataframe has to be in UTF-8 while training the model, I have saved that particular feature in a separate csv file by encoding it with UTF-8 and later, I have the accessed that particular column.</p>

<p>Any help is appreciable </p>
","6410520","","6410520","","2017-06-23 02:31:07","2017-06-23 07:37:30","Word2vec saved model is not UTF-8 encoded but the sentence input to the Word2vec model is UTF-8 encoded","<python-3.x><utf-8><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"43904029","1","","","2017-05-10 23:09:25","","1","1348","<p>I want to analyze the vectors looking for patterns and stuff, and use SVM on them to complete a classification task between class A and B, the task should be supervised. (I know it may sound odd but it's our homework.) so as a result I really need to know:</p>

<p>1- how to extract the coded vectors of a document using a trained model?</p>

<p>2- how to interpret them and how does word2vec code them?</p>

<p>I'm using gensim's word2vec.</p>
","4660314","","4660314","","2017-05-11 03:25:59","2017-05-15 10:16:43","How extract vocabulary vectors from gensim's word2vec?","<python><machine-learning><gensim><word2vec><text-classification>","1","1","","","","CC BY-SA 3.0"
"44022180","1","44037339","","2017-05-17 10:23:45","","1","4676","<p>I am trying to load a binary file using <code>gensim.Word2Vec.load(fname)</code> but I get the error:</p>

<blockquote>
  <p>File ""file.py"", line 24, in 
      model = gensim.models.Word2Vec.load('ammendment_vectors.model.bin')   </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 1396, in load
      model = super(Word2Vec, cls).load(*args, **kwargs) </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 271, in load
      obj = unpickle(fname)  </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 933, in unpickle
      return _pickle.load(f, encoding='latin1')</p>
  
  <p>_pickle.UnpicklingError: could not find MARK</p>
</blockquote>

<p>I googled but I am unable to figure out why this error is coming up. Please let me know if any other information is required.</p>
","8024795","","5293112","","2017-05-17 11:27:13","2017-05-18 01:56:16","Unpickling Error while using Word2Vec.load()","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"44163836","1","44164856","","2017-05-24 16:24:45","","1","526","<p>I trained a doc2vec model using train(..) with default settings. That worked, but now I'm wondering how infer_vector combines across input words, is it just the average of the individual word vectors?</p>

<pre><code>model.random.seed(0)
model.infer_vector(['cat', 'hat'])
model.random.seed(0)
model.infer_vector(['cat'])
model.infer_vector(['hat']) #doesn't average up to the ['cat', 'hat'] vector
model.random.seed(0)
model.infer_vector(['hat'])
model.infer_vector(['cat']) #doesn't average up to the ['cat', 'hat'] vector
</code></pre>

<p>Those don't add up, so I'm wondering what I'm misunderstanding. </p>
","2196059","","","","","2017-05-25 07:34:35","How does doc2vec.infer_vector combine across words?","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"53076731","1","","","2018-10-31 05:15:59","","0","83","<p>What I am trying to do is get a score of the likelihood of a search/query term in a single document/text/paragraph.</p>

<p>The score should tell how much the text is talking about the query term.</p>

<p>Here is what I have tried but failed:</p>

<pre><code>def score(text_data,query):

    texts = [str(doc).encode('utf-8').lower().split() for doc in text_data]

    dictionary = Dictionary(texts)
    corpus = [dictionary.doc2bow(line) for line in texts]

    tfidf_model = TfidfModel(corpus)

    query_vec = dictionary.doc2bow(query.lower().split())
    query_vec = tfidf_model[query_vec]

    index = similarities.MatrixSimilarity(tfidf_model[corpus])

    x = tfidf_model[query_vec]
    sims = index[x]
    score = list(sims)

    return score
</code></pre>

<p>I only need 1 value of the likelihood of the search term in the text data, between 0 to 1. What am I doing incorrectly ?</p>
","6604134","","4746283","","2019-03-07 15:20:52","2019-03-07 15:20:52","Text similarity score using a single query on a single document in Gensim","<python><gensim><tf-idf>","0","2","","","","CC BY-SA 4.0"
"58869364","1","","","2019-11-15 02:10:01","","0","368","<p>I have a dataset with 4.7 million questions, and I want to compare their tf-idf vectors and retrieve the most similar pair for each question.</p>
<p>According to the gensim documentation,</p>
<blockquote>
<p>There is also a special syntax for when you need similarity of documents in the index</p>
<p>to the index itself (i.e. queries=indexed documents themselves). This special syntax</p>
<p>uses the faster, batch queries internally and <strong>is ideal for all-vs-all pairwise similarities</strong>:</p>
<p><code>for similarities in index:  # yield similarities of the 1st indexed document, then 2nd... ...</code></p>
<p><code>pass</code></p>
</blockquote>
<p>However, since I have about 4.7 million documents, <code>similarities</code> should be a numpy array with length 4.7 million, which is very large too and I cannot store on memory.</p>
<pre><code>index = Similarity.load('out/corpus.index')
idx1 = 0
for similarities in index: # &lt;---- this part is slow
  idx1 += 1
  # and other stuff
</code></pre>
<p>Is there a way that I can get the most similar pair for each question?</p>
","8012572","","-1","","2020-06-20 09:12:55","2019-11-15 05:52:46","Gensim Similarity with very large dataset (~4.7 million)","<python><gensim>","1","2","","","","CC BY-SA 4.0"
"61472611","1","","","2020-04-28 04:26:29","","-2","95","<pre><code>from gensim.sklearn_api.phrases import PhrasesTransformer

# Create the model. Make sure no term is ignored and combinations seen 3+ times are captured.
m = PhrasesTransformer(min_count=1, threshold=3)
text = [['I', 'love', 'computer', 'science', 'computer', 'science', 'is', 'my', 'passion', 'I', 'studied', 'computer', 'science']]

# Use sklearn fit_transform to see the transformation.
# Since computer and science were seen together 3+ times they are considered a phrase.
m.fit_transform(text)
</code></pre>

<p>The above code does return computer_science as expected. But What is the right method to extract phrases pragmatically?</p>
","10308337","","","","","2020-04-29 20:11:25","Phrase detection using PhrasesTransformer","<nlp><gensim><n-gram><phrase>","1","4","","","","CC BY-SA 4.0"
"52941179","1","","","2018-10-23 04:28:28","","0","751","<p>I am using python gensim to create word2vec for my 93 million sentences. However, when I train my model, I am getting three files as output with extensions .bin.trainables.syn1neg.npy and .bin.wv.vectors.npy in addition to .bin. I went through the answer provided here: <a href=""https://stackoverflow.com/q/47173538/9526057"">Why are multiple model files created in gensim word2vec?</a> which gives reasoning of why this happens. However I would like to know if there is a way to convert these files into a normal single bin file?</p>
","9526057","","","","","2018-10-23 09:52:50","gensim creates files with extension .bin.trainables.syn1neg.npy and .bin.wv.vectors.npy in addition to .bin","<python-2.7><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"58871071","1","","","2019-11-15 05:51:44","","0","269","<p>I'm trying to find out the similarity between 2 documents i.e 'document_1' and 'document_2'.
I'm using <strong>Doc2Vec Gensim's <em>keyedvectors.py</em></strong> for finding similarity score.</p>

<pre><code>score = model.docvecs.similarity_unseen_docs(trainedModel, document_1, document_2)
print(score)
</code></pre>

<p>Where score is negative.</p>

<p>Here document_1 and document_2 are result of <em>NLTK's word_tokenize()</em></p>

<p><strong>What does Negative score mean when we try to find similarity between two ""<em>tokenized</em>"" documents?</strong></p>

<p><strong>P.S:</strong> Trained the model on 10 documents(2 Pages each)=20 Pages MS
word documents. </p>
","11586205","","11586205","","2019-11-15 14:34:24","2019-11-15 14:34:24","Getting negative score for model.docvecs.similarity_unseen_docs(document_1, document_2)","<python><nlp><nltk><gensim>","1","0","","","","CC BY-SA 4.0"
"33828304","1","33836687","","2015-11-20 13:47:58","","0","4506","<p>I'm applying TFIDF on text documents where I get varied length n dimensional vectors each corresponding to a document. </p>

<pre><code>    texts = [[token for token in text if frequency[token] &gt; 1] for text in texts]
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    lda = models.ldamodel.LdaModel(corpus, num_topics=100, id2word=dictionary)
    tfidf = models.TfidfModel(corpus)   
    corpus_tfidf = tfidf[corpus]
    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)
    corpus_lsi = lsi[corpus_tfidf]
    corpus_lda=lda[corpus]
    print ""TFIDF:""
    print corpus_tfidf[1]
    print ""__________________________________________""
    print corpus_tfidf[2]
</code></pre>

<p>The output to this is:</p>

<pre><code>TFIDF:
Vec1:    [(19, 0.06602704727889631), (32, 0.360417819987515), (33, 0.3078487494326974), (34, 0.360417819987515), (35, 0.2458968255872351), (36, 0.23680107692707422), (37, 0.29225639811281434), (38, 0.31741275088103), (39, 0.28571949457481044), (40, 0.32872456368129543), (41, 0.3855741727557306)]
    __________________________________________
Vec2:    [(5, 0.05617283528623041), (6, 0.10499864499395724), (8, 0.11265354901199849), (16, 0.028248249837939252), (19, 0.03948130674177094), (29, 0.07013501129200184), (33, 0.18408018239985235), (42, 0.14904146984986072), (43, 0.20484144632880313), (44, 0.215514203535732), (45, 0.15836501876891904), (46, 0.08505477582234795), (47, 0.07138425858136686), (48, 0.127695955436003), (49, 0.18408018239985235), (50, 0.2305566099597365), (51, 0.20484144632880313), (52, 0.2305566099597365), (53, 0.2305566099597365), (54, 0.053099690797234665), (55, 0.2305566099597365), (56, 0.2305566099597365), (57, 0.2305566099597365), (58, 0.0881162347543671), (59, 0.20484144632880313), (60, 0.16408387627386525), (61, 0.08256873616398946), (62, 0.215514203535732), (63, 0.2305566099597365), (64, 0.16731192344738707), (65, 0.2305566099597365), (66, 0.2305566099597365), (67, 0.07320703902661252), (68, 0.17912628269786976), (69, 0.12332630621892736)]
</code></pre>

<p>The vector points not represented are 0. Which means say (18, ....) does not exist in the vector, then it is 0. </p>

<p>I want to apply K means clustering on these vectors (Vec1 and Vec2)</p>

<p>Scikit's K means clustering needs vectors in equal dimension and in matrix format. What should be done about this? </p>
","3426358","","","","","2015-11-27 17:09:34","K means Clustering on n dimensional vectors.","<python><scikit-learn><k-means><gensim>","1","2","0","","","CC BY-SA 3.0"
"61944240","1","","","2020-05-21 21:36:07","","2","1878","<p>I was using Gensim 3.6.0 for loading a pre-trained Word2Vec and it showed the following error while calling <code>model.wv</code>.</p>

<pre><code>/anaconda/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).
  """"""Entry point for launching an IPython kernel.
</code></pre>

<p>Here is my code sample</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('/path/to/file/my-vec-300d-v2', binary=False)
print(model.wv['hello'].shape)
print(model.wv['hello']) 
</code></pre>
","3808276","","","","","2020-05-21 21:36:07","Gensim v3.6.0 Word2Vec DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead)","<python-3.x><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"27145452","1","","","2014-11-26 09:26:51","","3","896","<p>In the LDA model these are the two methods to inference the new documents using existing model i think. what are the differences between these two methods ?</p>
","3106424","","","","","2017-08-09 13:16:03","What is the deference between lda[doc_bow] and lda.inference(corpus)?","<python><lda><gensim>","1","1","1","","","CC BY-SA 3.0"
"44657908","1","","","2017-06-20 16:01:51","","1","45","<p>I am not expert in NLP related models. So I may be missing a simple point here. So please bear with me. </p>

<p>I have obtained topics and the corresponding keywords for each of those topic. I want to first cluster the documents based on these topics. That is, I want to see which document belongs to which topic. Next, given a new document, I need to find out which document clusters it belongs to. 
How do I do it ? </p>

<p>I think we need to pass the new document through the LDA model used to obtain the topics and then use the generated topics to measure its similarity with the previously obtained topics. However, I am not sure if this will work. For example, if the new document is a short document I don't know if it will work. </p>

<p>Any help will be extremely useful. </p>

<p>Thank you. </p>

<p>BTW: I am using Python 2.7 and the gensim package for the LDA algorithm</p>
","2307804","","","","","2017-06-20 16:01:51","Deciding which document cluster a new document belong to","<python-2.7><nltk><gensim>","0","0","","","","CC BY-SA 3.0"
"27147690","1","","","2014-11-26 11:14:04","","5","2091","<p>i am just wondering whether its either TFIDF corpus to be used or just corpus to be used when we are inference documents using LDA in gensim</p>

<p>Here is an example</p>

<pre><code>from gensim import corpora, models
import numpy.random
numpy.random.seed(10)

doc0 = [(0, 1), (1, 1)]
doc1 = [(0,1)] 
doc2 = [(0, 1), (1, 1)]
doc3 = [(0, 3), (1, 1)]

corpus = [doc0,doc1,doc2,doc3]
dictionary = corpora.Dictionary(corpus)

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
corpus_tfidf.save('x.corpus_tfidf')

corpus_tfidf = corpora.MmCorpus.load('x.corpus_tfidf')

lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)

#which one i should use from this   
**corpus_lda = lda[corpus]**          #this one 
**corpus_LDA = lda[corpus_tfidf ]**   #or this one?


corpus_lda.save('x.corpus_lda')

for i,j in enumerate(corpus_lda):
    print j, corpus[i]
</code></pre>
","3106424","","","","","2014-12-03 00:30:07","should i use tfidf corpus or just corpus to inference documents using LDA?","<python><lda><gensim>","1","0","0","","","CC BY-SA 3.0"
"39843584","1","","","2016-10-04 03:13:27","","55","15922","<p>I'm trying to compare my implementation of Doc2Vec (via tf) and gensims implementation. It seems atleast visually that the gensim ones are performing better.</p>

<p>I ran the following code to train the gensim model and the one below that for tensorflow model. My questions are as follows:</p>

<ol>
<li>Is my tf implementation of Doc2Vec correct. Basically is it supposed to be concatenating the word vectors and the document vector to predict the middle word in a certain context?</li>
<li>Does the <code>window=5</code> parameter in gensim mean that I am using two words on either side to predict the middle one? Or is it 5 on either side. Thing is there are quite a few documents that are smaller than length 10.</li>
<li>Any insights as to why Gensim is performing better? Is my model any different to how they implement it?</li>
<li>Considering that this is effectively a matrix factorisation problem, why is the TF model even getting an answer? There are infinite solutions to this since its a rank deficient problem. &lt;- This last question is simply a bonus.</li>
</ol>

<h3>Gensim</h3>

<pre><code>model = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=10, hs=0, min_count=2, workers=cores)
model.build_vocab(corpus)
epochs = 100
for i in range(epochs):
    model.train(corpus)
</code></pre>

<h3>TF</h3>

<pre><code>batch_size = 512
embedding_size = 100 # Dimension of the embedding vector.
num_sampled = 10 # Number of negative examples to sample.


graph = tf.Graph()

with graph.as_default(), tf.device('/cpu:0'):
    # Input data.
    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size/context_window])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size/context_window, 1])

    # The variables   
    word_embeddings =  tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))
    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))
    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, (context_window+1)*embedding_size],
                             stddev=1.0 / np.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))

    ###########################
    # Model.
    ###########################
    # Look up embeddings for inputs and stack words side by side
    embed_words = tf.reshape(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),
                            shape=[int(batch_size/context_window),-1])
    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)
    embed = tf.concat(1,[embed_words, embed_docs])
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
                                   train_labels, num_sampled, vocabulary_size))

    # Optimizer.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
</code></pre>

<h2>Update:</h2>

<p>Check out the jupyter notebook <a href=""https://github.com/sachinruk/doc2vec_tf"" rel=""noreferrer"">here</a> (I have both models working and tested in here). It still feels like the gensim model is performing better in this initial analysis.</p>
","2530674","","1259561","","2017-09-17 04:12:13","2017-09-17 04:12:13","gensim Doc2Vec vs tensorflow Doc2Vec","<python><tensorflow><nlp><gensim><doc2vec>","1","4","20","","","CC BY-SA 3.0"
"61638940","1","61653393","","2020-05-06 15:22:45","","0","277","<p>I've got an LDA model through using gensim. I can save it locally:</p>

<pre><code>ldamodel.save('models/lda/lda.model')
</code></pre>

<p>This results in four files in the specified place:</p>

<pre><code>lda.model
lda.model.expElogbeta.npy
lda.model.id2word
lda.model.state
</code></pre>

<p>Loading them back is as simple as </p>

<pre><code>ldamodel =  models.LdaModel.load('models/lda/lda.model')
</code></pre>

<p>However, I want this model to be saved on s3. I can work out how to save individual bits, for example:</p>

<pre><code>s3.meta.client.upload_file('models/lda/lda.model', 'bucket-name', 'lda.model')
</code></pre>

<p>But I can't work out how to actually meaningfully read them back in so they will function as expected as a coherent model. So the idea being that somebody other than me could take the files from s3 and use them as a model in Python. </p>

<p>Can anybody help?</p>
","8506921","","","","","2020-05-07 09:04:10","Save a gensim LDA model to s3","<python><amazon-s3><gensim>","1","0","1","","","CC BY-SA 4.0"
"53281744","1","53282039","","2018-11-13 13:10:21","","-1","1850","<p>I am supposed to do some exercises with python glove, most of it doesn't give me any problems but now i am supposed to find the 5 most similar words to ""norway - war + peace"" from the ""glove-wiki-gigaword-100"" package. But when i run my code it just says that the 'word' is not in the vocabulary. Now I'm guessing that this is some kind of formatting, but i don't know how to use it.</p>

<pre><code>import gensim.downloader as api
model = api.load(""glove-wiki-gigaword-100"")  # download the model and return as object ready for use

bests = model.most_similar(""norway - war + peace"", topn= 5)

print(""5 most similar words to 'norway - war + peace':"")

for best in bests:
    print(best)
</code></pre>
","6793326","","","","","2018-11-13 13:25:11","glove most similar to multiple words","<python><nlp><gensim><glove>","1","0","","","","CC BY-SA 4.0"
"44610300","1","","","2017-06-17 23:20:19","","0","1007","<p>I have a list of titles: </p>

<pre><code>&gt; print(data)
&gt; 
  0   Manager
  1   Electrician
  3   Carpenter
  4   Electrician &amp; Carpenter
  ...
</code></pre>

<p>I am trying to use gensim to find the closest related titles. </p>

<p>The code I have is: </p>

<pre><code>import os
import pandas as pd
import nltk
import gensim
from gensim import corpora, models, similarities
from nltk.tokenize import word_tokenize 

df = pd.read_csv('df.csv')
corpus = pd.DataFrame(df, columns=['Job Title'])
tokenized_sents = [word_tokenize(i) for i in corpus]

model = gensim.models.Word2Vec(tokenized_sents, min_count=1)

model.most_similar(""Electrician"")
</code></pre>

<p>When I am running tokenization to tokenize each title as a sentence  (tokenized_sents variable), it only tokenizes the header:</p>

<pre><code>&gt; tokenzied_sents 
&gt; [['Job', 'Title']]
</code></pre>

<p>What am I doing wrong?</p>
","1907382","","5014455","","2017-06-17 23:51:46","2017-06-17 23:51:46","Unable to tokenize sentences using gensim and nltk in python","<python><pandas><nltk><tokenize><gensim>","1","0","","","","CC BY-SA 3.0"
"57033566","1","57042036","","2019-07-15 05:14:25","","1","402","<p>I would like to train my own word embeddings with fastext. However, after following the tutorial I can not manage to do it properly. So far I tried:</p>

<p>In:</p>

<pre><code>from gensim.models.fasttext import FastText as FT_gensim

# Set file names for train and test data
corpus = df['sentences'].values.tolist()

model_gensim = FT_gensim(size=100)

# build the vocabulary
model_gensim.build_vocab(sentences=corpus)
model_gensim
</code></pre>

<p>Out:</p>

<pre><code>&lt;gensim.models.fasttext.FastText at 0x7f6087cc70f0&gt;
</code></pre>

<p>In:</p>

<pre><code># train the model
model_gensim.train(
    sentences = corpus, 
    epochs = model_gensim.epochs,
    total_examples = model_gensim.corpus_count, 
    total_words = model_gensim.corpus_total_words
)

print(model_gensim)
</code></pre>

<p>Out:</p>

<pre><code>FastText(vocab=107, size=100, alpha=0.025)
</code></pre>

<p>However, when I try to look in a vocabulary words:</p>

<pre><code>print('return' in model_gensim.wv.vocab)
</code></pre>

<p>I get <code>False</code>, even the word is present in the sentences I am passing to the fast text model. Also, when I check the most similar words to return I am getting characters:</p>

<pre><code>model_gensim.most_similar(""return"")

[('R', 0.15871645510196686),
 ('2', 0.08545402437448502),
 ('i', 0.08142799884080887),
 ('b', 0.07969795912504196),
 ('a', 0.05666942521929741),
 ('w', 0.03705815598368645),
 ('c', 0.032348938286304474),
 ('y', 0.0319858118891716),
 ('o', 0.027745068073272705),
 ('p', 0.026891689747571945)]
</code></pre>

<p>What is the correct way of using gensim's fasttext wrapper?</p>
","9978457","","","","","2020-08-04 13:29:14","How to train a word embedding representation with gensim fasttext wrapper?","<machine-learning><nlp><gensim><word-embedding><fasttext>","2","1","","","","CC BY-SA 4.0"
"48253833","1","","","2018-01-14 20:25:59","","1","297","<p>I need to print only the topic word (only one word). But it contains some number, But I can not get only the topic name like ""Happy"". My String word is ""Happy"", why it shows ""Happi""</p>

<pre><code>    import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models

import gensim
import string
tokenizer = RegexpTokenizer(r'\w+')
en_stop = get_stop_words('en')
p_stemmer = PorterStemmer()
fr = open('Happy DespicableMe.txt','r')
doc_a = fr.read()
fr.close()
doc_set = [doc_a]
texts = []
for i in doc_set:


    raw = i.lower()
    tokens = tokenizer.tokenize(raw)


    stopped_tokens = [i for i in tokens if not i in en_stop]


    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]


    texts.append(stemmed_tokens)


dictionary = corpora.Dictionary(texts)


corpus = [dictionary.doc2bow(text) for text in texts]


ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=20)



rafa = ldamodel.show_topics(num_topics=1, num_words=1, log=False , formatted=False)


print(rafa)
</code></pre>

<p>It only shows [(0, '0.142*""happi""')]. But I want to print only the word.</p>

<p><a href=""https://i.stack.imgur.com/SOHXi.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SOHXi.jpg"" alt=""Strings on the File""></a></p>
","5457779","","8291949","","2018-01-15 06:03:10","2018-02-05 00:08:46","Print only topic name using LDA with python","<python-3.x><nltk><gensim><lda><stemming>","1","4","","","","CC BY-SA 3.0"
"58876630","1","58885079","","2019-11-15 12:01:11","","3","2931","<p>I'm trying to export the fasttext model created by gensim to a binary file. But the docs are unclear about how to achieve this. 
What I've done so far: </p>

<pre><code>model.wv.save_word2vec_format('model.bin')
</code></pre>

<p>But this does not seems like the best solution. Since later when I want to load the model using the :</p>

<pre><code>fasttext.load_facebook_model('model.bin')
</code></pre>

<p>I get into an infinite loop. While loading the <code>fasttext.model</code> created by <code>model.save('fasttext.model)</code> function gets completed in around 30 seconds.</p>
","8265036","","","","","2021-06-21 11:27:58","How to export a fasttext model created by gensim, to a binary file?","<python><nlp><gensim><fasttext>","1","2","","","","CC BY-SA 4.0"
"65856634","1","65856800","","2021-01-23 07:02:50","","1","40","<p>I have sentences in corpus with mixed words (dictionary and non-dictionary words). Non-dictionary words are as important as they are domain specific. I'm not performing any nlp on non-dictionary words. Does doc2vec model compare non-dictionary words to same words in matching criteria?</p>
<p>Ex. I'm giving input ['AMDML','release']. Here AMDML is domain specific word. Will it match to same words if I've sentences in training model like ['AMDML','release','process'] or ['DML','release']. or only words like 'release' and 'process' is matched in most similar method?</p>
","15036998","","","","","2021-01-23 07:33:48","Does doc2vec model give accuracy on non-dictionary words?","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"50679537","1","55820776","","2018-06-04 11:27:52","","1","228","<p>I try to evaluate my dynamic topic models.
The model were generated with the gensim wrappers.
Are there any possible functions like perplexity or topic coherence equal to the ""normal"" topic modeling?</p>
","9751594","","","","","2019-04-23 23:39:07","Evaluation of Dynamic Topic Models","<gensim><evaluation><lda>","1","0","","","","CC BY-SA 4.0"
"40091380","1","","","2016-10-17 16:30:06","","-1","211","<p>I am using word vectors for text classification solution. I am using word vectors mainly to address the case of synonyms which are not there in the training set but will be present in the actual use-cases. By simply using word vectors, I am not getting a good enough accuracy in prediction. Can anyone please suggest some enhancements I can do over word vectors in order to improve accuracy?</p>
","1381301","","","","","2016-10-18 01:48:37","Enhancements for text classification using word vectors","<machine-learning><scikit-learn><text-classification><gensim><word2vec>","1","1","","","","CC BY-SA 3.0"
"41765951","1","41810748","","2017-01-20 14:28:54","","1","712","<p>I am applying the LDA method using Gensim to extract keywords from documents.
I can extract topics, and then assign these topics and key words associated to the documents.</p>

<p>I would like to have the ids of these terms (or key words) instead of the terms themselves. I know that <code>corpus[i]</code> extract a list of couples  [(term_id, term_frequency) ...] of document <code>i</code> but I can't see how could I use this in my code to extract only the ids and assign it to my results.</p>

<p>My code is as follows :</p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=passes, minimum_probability=0)

# Assinging the topics to the document in corpus
lda_corpus = ldamodel[corpus]

# Find the threshold, let's set the threshold to be 1/#clusters,
# To prove that the threshold is sane, we average the sum of all probabilities:
scores = list(chain(*[[score for topic_id,score in topic] \
                     for topic in [doc for doc in lda_corpus]]))

threshold = sum(scores)/len(scores)
print(threshold)

for t in range(len(topic_tuple)):  

    key_words.append([topic_tuple[t][j][0] for j in range(num_words)])
    df_key_words = pd.DataFrame({'key_words' : key_words})

    documents_corpus.append([j for i,j in zip(lda_corpus,doc_set) if i[t][1] &gt; threshold])
    df_documents_corpus = pd.DataFrame({'documents_corpus' : documents_corpus})

    documents_corpus_id.append([i for d,i in zip(lda_corpus, doc_set_id) if d[t][1] &gt; threshold])
    df_documents_corpus_id = pd.DataFrame({'documents_corpus_id' : documents_corpus_id})


result.append(pd.concat([df_key_words, df_documents_corpus, df_documents_corpus_id ], axis=1))
</code></pre>

<p>Thank you in advance and ask me if more information are needed.</p>
","7341902","","","","","2017-01-23 16:13:16","Python, LDA : How to get the id of keywords instead of the keywords themselves with Gensim?","<python><gensim><lda>","1","0","","","","CC BY-SA 3.0"
"33929680","1","34324435","","2015-11-26 02:31:03","","9","6030","<p>When I run gensim's <code>LdaMulticore</code> model on a machine with 12 cores, using:</p>

<pre><code>lda = LdaMulticore(corpus, num_topics=64, workers=10)
</code></pre>

<p>I get a logging message that says </p>

<pre><code>using serial LDA version on this node  
</code></pre>

<p>A few lines later, I see another loging message that says </p>

<pre><code>training LDA model using 10 processes
</code></pre>

<p>When I run top, I see 11 python processes have been spawned, but 9 are sleeping, I.e. only one worker is active.  The machine has 24 cores, and is not overwhelmed by any means.  Why isn't LdaMulticore running in parallel mode?</p>
","1467306","","1467306","","2015-12-11 23:45:39","2016-08-25 12:07:26","gensim LdaMulticore not multiprocessing?","<python><multiprocessing><lda><gensim>","1","1","2","","","CC BY-SA 3.0"
"66031545","1","66048268","","2021-02-03 16:21:47","","0","131","<p>I'm trying to use gensim <code>summarize()</code> to simplify paragraphs in job descriptions.
I webscraped a bunch of job descriptions using the selenium package and stored them in a list.</p>
<pre><code>descriptions=[]
for link in job_urls:
    driver.get(link)
    jd = driver.find_element_by_xpath('//div[@id=&quot;jobDescriptionText&quot;]').text
    #The form element with attribute id set to jobDescriptionText
    descriptions.append(jd)
</code></pre>
<p><a href=""https://i.stack.imgur.com/aXRFc.jpg"" rel=""nofollow noreferrer"">The output is a list of text; each item is multiple paragraphs.  EX:</a></p>
<p>If I summarize item one at a time with an index, the code works.:</p>
<pre><code>    text = descriptions[2] # Change index to desired job description.
    summarize(str(text), ratio=0.5)
'The core function of this opening is to conduct regional studies and mapping.\nAs the successful candidate you would be expected to conduct regional exploration studies and evaluations of the petroleum system elements, and possess the experience to integrate geological and geophysical data to create regional maps.\nYou should have the aptitude for, and tireless energy around data mining and analysis, with high level computer mapping skills.\nMinimum Requirements\nYou will be required to perform the following:\nConduct regional exploration studies and evaluations of the petroleum system elements, and integrate available geological and geophysical data to create regional maps.\nDevelop gross depositional environment maps, effectiveness maps, common risk segment maps of all petroleum system elements (source, reservoir seal), and composite common risk segment maps of different plays, to develop new play concepts and exploration opportunities.\nAnalyze data mining with high level of computer mapping skills, using major Exploration software packages, preferably Petrel.'
</code></pre>
<p>But if I loop through the list, the function throws the ValueError:</p>
<pre><code>for text in descriptions:
    text = str(text)
    summarize(text, ratio=0.5)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-40-b3969fcb2610&gt; in &lt;module&gt;
      4 for text in descriptions:
      5     text = str(text)
----&gt; 6     summarize(text, ratio=0.5)

~\Anaconda3\lib\site-packages\gensim\summarization\summarizer.py in summarize(text, ratio, word_count, split)
    426     # If only one sentence is present, the function raises an error (Avoids ZeroDivisionError).
    427     if len(sentences) == 1:
--&gt; 428         raise ValueError(&quot;input must have more than one sentence&quot;)
    429 
    430     # Warns if the text is too short.
ValueError: input must have more than one sentence
</code></pre>
<p>And with a list comprehension:</p>
<pre><code>summary = [summarize(str(text),ratio=0.5) for text in descriptions]

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-31-8d79c7c19d53&gt; in &lt;module&gt;
      1 #text = descriptions[2] # Change index to desired job description.
      2 #summarize(str(text), ratio=0.5)
----&gt; 3 summary = [summarize(str(text),ratio=0.5) for text in descriptions]
      4 #for text in descriptions:
      5    # print(str(text)+&quot;\n&quot;)

&lt;ipython-input-31-8d79c7c19d53&gt; in &lt;listcomp&gt;(.0)
      1 #text = descriptions[2] # Change index to desired job description.
      2 #summarize(str(text), ratio=0.5)
----&gt; 3 summary = [summarize(str(text),ratio=0.5) for text in descriptions]
      4 #for text in descriptions:
      5    # print(str(text)+&quot;\n&quot;)

~\Anaconda3\lib\site-packages\gensim\summarization\summarizer.py in summarize(text, ratio, word_count, split)
    426     # If only one sentence is present, the function raises an error (Avoids ZeroDivisionError).
    427     if len(sentences) == 1:
--&gt; 428         raise ValueError(&quot;input must have more than one sentence&quot;)
    429 
    430     # Warns if the text is too short.

ValueError: input must have more than one sentence
</code></pre>
<p>The items are more than one sentence and <code>summarize()</code> works individually.  Why would <code>summarize()</code> throw this error in a loop or list comprehension?</p>
","10781902","","7023590","","2021-02-03 21:03:29","2021-02-04 15:04:50","Gensim returns ""ValueError: input must have more than one sentence"" in for loop through list of paragraphs","<python><python-3.x><selenium><for-loop><gensim>","2","2","","","","CC BY-SA 4.0"
"48990935","1","49005051","","2018-02-26 14:36:55","","3","485","<p>I have a list of words in my python programme. Now I need to iterate through this list and find out the semantically similar words and put them into another list. I have been trying to do this using gensim with word2vec but could find a proper solution.This is what I have implemeted up to now. I need a help on how to iterate through the list of words in the variable sentences and find the semantically similar words and save it in another list.</p>

<pre><code>import gensim, logging

import textPreprocessing, frequentWords , summarizer
from gensim.models import Word2Vec, word2vec

import numpy as np
from scipy import spatial

sentences = summarizer.sorteddict

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model = word2vec.Word2Vec(sentences, iter=10, min_count=5, size=300, workers=4)
</code></pre>
","6919396","","712995","","2018-02-27 10:44:10","2018-02-27 10:44:10","How to find semantic similarity using gensim and word2vec in python","<python><machine-learning><nlp><word2vec><gensim>","1","0","1","","","CC BY-SA 3.0"
"33789541","1","33812230","","2015-11-18 20:16:25","","5","669","<p>I have been experimenting with the doc2vec module for sometime now. I can train my model and have the trained model output similar documents for a given document as follows :</p>

<pre><code>import re
modelloaded=Doc2Vec.load(""model_all_doc_dm_1"")

st = 'long description of a document as string'
doc = re.sub('[^a-zA-Z]', ' ', st).lower().split() 

new_doc_vec = modelloaded.infer_vector(doc)

modelloaded.docvecs.most_similar([new_doc_vec])
</code></pre>

<p>This works well, and gives me 10 results. Is there a way to get more than 10 results or is that the limit?</p>
","3197086","","","","","2015-11-19 18:59:15","Is there a limit in Gensim's Doc2Vec most_similar documents result set?","<python-3.x><gensim>","1","0","1","","","CC BY-SA 3.0"
"33808746","1","","","2015-11-19 16:02:16","","2","724","<p>I have a gensim LDA model that I am working on and I would like to fit into the sciKit Naive Bayes classifier, similar to sciKit's TfidfTransformer():</p>

<pre><code> lda = ldamodel.LdaModel(corpus=self.corpus, num_topics=num_topics)
</code></pre>

<p>The shape of the TfidfTransformer() is a sparse numpy matrix: </p>

<pre><code>&lt;2014x4604 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
    with 117869 stored elements in Compressed Sparse Row format&gt;
</code></pre>

<p>There have been a few posts on how to take the sciKit Vectorizer and feed it into a gensim model but I want to do the opposite, something like:</p>

<pre><code> classifier = MultinomialNB()
 classifier.fit(lda_model, train_class)
</code></pre>

<p>How would I go about doing this?  I assume that the matrix from TfidfTransformer.fit_transform() is a matrix of tfidf scores for each word in an array of documents like:</p>

<pre><code>  [[.1234, .234, .345], # document 1
  [.098, .987, .876],   # document 2
 [.555, .666, .777]]    # document 3
</code></pre>

<p>If that's the case, I can just replace every word with it's score and feed that matrix into the classifier. But I can't really tell because there's a lot of stuff going on under the hood that I can't see.</p>

<p>Edit:  I was able to print the first entry of the numpy array, which is itself a numpy array, so I printed the first entry of that and got: </p>

<pre><code> (0, 3911)     0.22756829025
 (0, 3826)     0.161385996776
 (0, 3815)     0.100930582918
 (0, 3627)     0.158255401295
 (0, 3621)     0.194939740341
 (0, 3620)     0.100757250634
 (0, 3527)     0.0734744409855
 (0, 3246)     0.109635312627
</code></pre>

<p>The column on the left must be tfidf scores but I'm not sure what the right is.  I assume wordids and the document id?</p>
","3951468","","3951468","","2015-11-28 08:14:59","2015-11-28 08:14:59","Fit Gensim LDA into SciKit Naive Bayes classifier","<python><scikit-learn><gensim>","0","6","2","","","CC BY-SA 3.0"
"48994062","1","48994438","","2018-02-26 17:37:19","","1","1891","<p>What I have achieved so far are models that can not be read by a person. I need to save the model as plain text to use it with a certain software, which requires that the model be this way.</p>

<p>I tried the following:</p>

<pre><code>model = models.doc2vec.Doc2Vec(size=300, min_count=0, alpha=0.025, min_alpha=0.025)
model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)
model.save('mymodel.txt')
</code></pre>

<p>But I get:</p>

<pre><code>Process finished with exit code -1073741571 (0xC00000FD)
</code></pre>

<p>I do not know if I should pass a specific parameter.</p>
","9025222","","","","","2018-02-26 18:01:22","Is there a way to save a Gensim doc2vec model as plain text (.txt)?","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"33976953","1","","","2015-11-28 22:05:32","","2","3770","<p>I am using gensim word2vec library in python and using pre-trained GoogleNews-vectors-negative300.bin model. But,</p>
<blockquote>
<p>I have words in my corpus for which i don't have word vectors and am
getting keyError for that how do i solve this problem?</p>
</blockquote>
<h1>Here is what i have tried so far,</h1>
<h1>1: Loading <code>GoogleNews-vectors-negative300.bin</code> per-trained model:</h1>
<pre><code>model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print &quot;model loaded...&quot;
</code></pre>
<h1>2: Build word vector for training set by using the average value of all word vectors in the tweet, then scale</h1>
<pre><code>def buildWordVector(text, size):
vec = np.zeros(size).reshape((1, size))
count = 0.
for word in text:
    try:
        vec += model[word].reshape((1, size))
        count += 1.
        #print &quot;found! &quot;,  word
    except KeyError:
        print &quot;not found! &quot;,  word #missing words
        continue
if count != 0:
    vec /= count
return vec

trained_vecs = np.concatenate([buildWordVector(z, n_dim) for z in x_train])
</code></pre>
<p>Please tell how it is possible to add new words in pre-trained Word2vec model?</p>
","2235817","","-1","","2020-06-20 09:12:55","2020-06-17 08:32:24","How to add missing words vectors in GoogleNews-vectors-negative300.bin pre-trained model?","<python><nlp><gensim><word2vec><word-embedding>","1","2","3","","","CC BY-SA 3.0"
"53430997","1","53443676","","2018-11-22 12:26:22","","1","2326","<p>I have a <code>pandas</code> dataframe containing descriptions. I would like to cluster descriptions based on meanings usign <code>CBOW</code>. My challenge for now is to document embed each row into equal dimensions vectors. At first I am training the word vectors using <code>gensim</code> as so:</p>

<pre><code>from gensim.models import Word2Vec

vocab = pd.concat((df['description'], df['more_description']))
model = Word2Vec(sentences=vocab, size=100, window=10, min_count=3, workers=4, sg=0)
</code></pre>

<p>I am however a bit confused now on how to replace the full sentences from my <code>df</code> with document vectors of equal dimensions.</p>

<p>For now, my workaround is repacing each word in each row with a vector then applying PCA dimentinality reduction to bring each vector to similar dimensions. Is there a better way of doing this though <code>gensim</code>, so that I could say something like this:</p>

<pre><code>df['description'].apply(model.vectorize)
</code></pre>
","10295388","","","","","2018-11-23 09:16:07","How to sentence embed from gensim Word2Vec embedding vectors?","<python-3.x><gensim><word2vec><word-embedding><doc2vec>","1","0","0","","","CC BY-SA 4.0"
"48234595","1","48248804","","2018-01-12 22:00:38","","1","624","<p>I have a working app using <code>doc2vec</code> from <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">gensim</a>. I know the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer""><code>KeyedVector</code></a> is now the recommended approach, and trying to port over however I am not sure what is the equivalent method for the <code>infer_vector</code> method in <code>Doc2Vec</code>?</p>

<p>Or better put, how do I obtain a document vector for an entire document using the <code>KeyedVector</code> model to write to my Annoy model?</p>
","1868436","","712995","","2018-01-14 11:27:02","2018-01-14 11:27:02","Gensim Doc2Vec.infer_vector() equivalent in KeyedVector","<machine-learning><nlp><word2vec><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"57335044","1","","","2019-08-03 02:26:38","","0","182","<p>I am trying to understand how to use Tensorflow2 to train word embeddings without the preset labels.</p>

<p>In the Tensorflow2 tutorial (<a href=""https://www.tensorflow.org/beta/tutorials/text/word_embeddings"" rel=""nofollow noreferrer"">https://www.tensorflow.org/beta/tutorials/text/word_embeddings</a>) it shows how to train word embeddings using pre-structured dataset with labels.</p>

<pre><code>imdb = keras.datasets.imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
    num_words=vocab_size)

embedding_dim=16

model = keras.Sequential([
layers.Embedding(vocab_size, embedding_dim, input_length=maxlen),
layers.GlobalAveragePooling1D(),
layers.Dense(16, activation='relu'),
layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
          loss='binary_crossentropy',
          metrics=['accuracy'])

history = model.fit(
    train_data,
    train_labels,
    epochs=30,
    batch_size=512,
    validation_data=(test_data, test_labels))
</code></pre>

<p>However, I wonder how to train - with Tensorflow2 - the embeddings on the non-labeled text, similar to what can be done with Gensim's Word2Vec?</p>
","6873010","","","","","2019-09-01 03:05:09","Word embeddings in Tensorflow2","<nlp><gensim><word2vec><word-embedding><tensorflow2.0>","1","2","0","","","CC BY-SA 4.0"
"58886579","1","58886600","","2019-11-16 01:04:05","","0","443","<p>I use GenSim and CBOW for training the corpus. How can I get the most similar words from a set of input words?</p>

<p>For example:
Given a set of input words: [""David"", ""Mary"", ""married""]. Can I infer some output words like: ""wedding"", ""husband"", ""wife"", ""couple"", etc?</p>
","6532658","","","","","2019-11-16 01:07:40","How to find the most similar words from a set of input words by CBOW (GenSim)?","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"33989826","1","33991111","","2015-11-30 00:30:37","","26","25622","<p>I know that this question has been asked already, but I was still not able to find a solution for it. </p>

<p>I would like to use gensim's <code>word2vec</code> on a custom data set, but now I'm still figuring out in what format the dataset has to be. I had a look at <a href=""http://streamhacker.com/2014/12/29/word2vec-nltk/"">this post</a> where the input is basically a list of lists (one big list containing other lists that are tokenized sentences from the NLTK Brown corpus). So I thought that this is the input format I have to use for the command <code>word2vec.Word2Vec()</code>. However, it won't work with my little test set and I don't understand why.</p>

<p>What I have tried:</p>

<p><strong>This worked</strong>:</p>

<pre><code>from gensim.models import word2vec
from nltk.corpus import brown
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

brown_vecs = word2vec.Word2Vec(brown.sents())
</code></pre>

<p><strong>This didn't work</strong>:</p>

<pre><code>sentences = [ ""the quick brown fox jumps over the lazy dogs"",""yoyoyo you go home now to sleep""]
vocab = [s.encode('utf-8').split() for s in sentences]
voc_vec = word2vec.Word2Vec(vocab)
</code></pre>

<p>I don't understand why it doesn't work with the ""mock"" data, even though it has the same data structure as the sentences from the Brown corpus:</p>

<p><strong>vocab</strong>:</p>

<pre><code>[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs'], ['yoyoyo', 'you', 'go', 'home', 'now', 'to', 'sleep']]
</code></pre>

<p><strong>brown.sents()</strong>: (the beginning of it)</p>

<pre><code>[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', ""Atlanta's"", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', ""''"", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', ""''"", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]
</code></pre>

<p>Can anyone please tell me what I'm doing wrong? </p>
","5232099","","","","","2018-03-07 06:14:11","Python: gensim: RuntimeError: you must first build vocabulary before training the model","<python><gensim><word2vec>","2","0","5","","","CC BY-SA 3.0"
"34021351","1","38128648","","2015-12-01 13:15:05","","0","1843","<p>I'm having a problem when trying to import gensim in python. When typing:</p>

<blockquote>
  <p>import gensim</p>
</blockquote>

<p>I got the following error:</p>

<p>Traceback (most recent call last):
  File """", line 1, in 
  File ""/Library/Python/2.7/site-packages/gensim/<strong>init</strong>.py"", line 6, in 
    from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
ImportError: cannot import name parsing</p>

<p>Also, when I view ""<strong>init</strong>.py"" it contains only the following lines:</p>

<blockquote>
  <h1>bring model classes directly into package namespace, to save some typing</h1>
  
  <p>from .summarizer import summarize, summarize_corpus</p>
  
  <p>from .keywords import keywords</p>
</blockquote>

<p>Any idea on how to solve this problem is highly appreciated.</p>

<p>I'm using:
MAC 10.10.5 and Python 2.7</p>

<p>Thank you</p>
","5289027","","","","","2016-06-30 16:24:07","importing gensim in mac","<python><gensim>","3","0","","","","CC BY-SA 3.0"
"34057374","1","34057537","","2015-12-03 03:28:22","","0","9264","<p>I'm trying to install gensim on Windows 7, with Python 3.4. According to <a href=""http://radimrehurek.com/gensim/install.html"" rel=""nofollow"">gensim official installation tutorial</a>, gensim depends on NumPy and SciPy, so I went to <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/"" rel=""nofollow"">here</a> to download .whl files for NumPy and SciPy installation. But when I used pip to install them, it gave me these errors:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;pyshell#2&gt;"", line 1, in &lt;module&gt;
import gensim
File ""C:\Python34\lib\site-packages\gensim\__init__.py"", line 6, in &lt;module&gt;
from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
File ""C:\Python34\lib\site-packages\gensim\matutils.py"", line 21, in &lt;module&gt;
import scipy.linalg
File ""C:\Python34\lib\site-packages\scipy\linalg\__init__.py"", line 172, in &lt;module&gt;
from .misc import *
File ""C:\Python34\lib\site-packages\scipy\linalg\misc.py"", line 5, in &lt;module&gt;
from .blas import get_blas_funcs
File ""C:\Python34\lib\site-packages\scipy\linalg\blas.py"", line 155, in &lt;module&gt;
from scipy.linalg import _fblas
ImportError: DLL load failed: Êâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊ®°Âùó„ÄÇ
</code></pre>

<p>""Êâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊ®°Âùó""means ""Cannot find the designated module"".
How can I resolve this?</p>
","5514910","","","","","2019-07-05 02:20:10","How to resolve error when installing gensim?","<python><gensim>","2","0","","","","CC BY-SA 3.0"
"34075899","1","","","2015-12-03 20:52:29","","0","872","<p>I am trying to analyze Wikipedia dump file. I am using gensim.scripts, a Python library, and running this command in Windows 10 cmd.exe:</p>

<pre><code>python -m gensim.scripts.make_wiki enwiki-latest-pages-articles.xml.bz2 wiki_en_output
</code></pre>

<p>This gives me the error:Microsoft Windows [Version 10.0.10586]
(c) 2015 Microsoft Corporation. All rights reserved.</p>

<pre><code>2015-12-03 15:47:20,459 : INFO : running C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\scripts\make_wiki.py enwiki-latest-pages-articles.xml.bz2 wiki_en_output
Traceback (most recent call last):
  File ""C:\Python27\lib\runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""C:\Python27\lib\runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\scripts\make_wiki.py"", line 84, in &lt;module&gt;
    wiki = WikiCorpus(inp, lemmatize=lemmatize) # takes about 9h on a macbook pro, for 3.5m articles (june 2011)
  File ""C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\corpora\wikicorpus.py"", line 270, in __init__
    self.dictionary = Dictionary(self.get_texts())
  File ""C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\corpora\dictionary.py"", line 58, in __init__
    self.add_documents(documents, prune_at=prune_at)
  File ""C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\corpora\dictionary.py"", line 119, in add_documents
    for docno, document in enumerate(documents):
  File ""C:\Python27\lib\site-packages\gensim-0.12.3-py2.7-win32.egg\gensim\corpora\wikicorpus.py"", line 290, in get_texts
    texts = ((text, self.lemmatize, title, pageid) for title, text, pageid in extract_pages(bz2.BZ2File(self.fname), self.filter_namespaces))
IOError: [Errno 2] No such file or directory: 'enwiki-latest-pages-articles.xml.bz2'
</code></pre>

<p>Thoughts on what I should do to fix this?</p>

<p>On Windows 10. gensim.scripts has been installed.</p>
","","user4974730","2128947","","2015-12-03 21:13:13","2015-12-03 23:04:41","gensim.scripts ""No such file or directory""","<python><cmd><gensim>","1","0","","","","CC BY-SA 3.0"
"50685212","1","","","2018-06-04 16:43:47","","0","346","<p>I am using gensim to create word vectors based on my corpus like the following:</p>

<pre><code>model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>I was wondering if it is possible to start (or somehow avoid having) words at index 0 and 1? I would like my vocabulary to start at index 2, because I need to do other operations and if I keep 0 and 1 as indexes it gets a little confusing.</p>

<p>Thanks for the help!</p>
","9355680","","","","","2018-06-04 19:59:31","Gensim word2vec - start vocabulary from index different than 0","<python><word2vec><gensim>","1","0","","","","CC BY-SA 4.0"
"67341054","1","","","2021-04-30 22:16:56","","0","117","<p>Hello i have a txt file in this form, in the first column is the word and in the second its vector.</p>
<pre><code>word 0.256 0.2659 0.326595
word1 0.528 0.6589 0.62326 ...
</code></pre>
<p>i am trying to load this as keyedvectors because I want to compute after the cosine similarity between the words and find the most similar words but I always get an error.</p>
","15212664","","15212664","","2021-05-01 08:27:14","2021-05-01 08:27:14","Load word vectors from a text file - GENSIM PYTHON","<python><gensim><word2vec>","1","2","","","","CC BY-SA 4.0"
"67344126","1","","","2021-05-01 08:03:39","","0","48","<p>I'm working on an NLP project and during word to vector transformation, I'm facing this problem.
Here is the code.</p>
<pre><code>from gensim.models import Word2Vec 
w2v_train = Word2Vec(min_count=2, corpus_file=train_corpus)  Can an
</code></pre>
","12085709","","6573902","","2021-05-03 11:29:28","2021-05-03 11:29:28","TypeError: stat: path should be string, bytes, OS.PathLike or integer, not list?","<python><nlp><gensim><word2vec>","0","2","","","","CC BY-SA 4.0"
"15016025","1","","","2013-02-22 02:47:42","","26","40147","<p>Using <code>gensim</code> I was able to extract topics from a set of documents in LSA but how do I access the topics generated from the LDA models?</p>

<p>When printing the <code>lda.print_topics(10)</code> the code gave the following error because <code>print_topics()</code> return a <code>NoneType</code>:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/alvas/workspace/XLINGTOP/xlingtop.py"", line 93, in &lt;module&gt;
    for top in lda.print_topics(2):
TypeError: 'NoneType' object is not iterable
</code></pre>

<p>The code:</p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models import hdpmodel, ldamodel
from itertools import izip

documents = [""Human machine interface for lab abc computer applications"",
              ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once]
         for text in texts]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# I can print out the topics for LSA
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)
corpus_lsi = lsi[corpus]

for l,t in izip(corpus_lsi,corpus):
  print l,""#"",t
print
for top in lsi.print_topics(2):
  print top

# I can print out the documents and which is the most probable topics for each doc.
lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)
corpus_lda = lda[corpus]

for l,t in izip(corpus_lda,corpus):
  print l,""#"",t
print

# But I am unable to print out the topics, how should i do it?
for top in lda.print_topics(10):
  print top
</code></pre>
","610569","","","","","2019-08-28 04:07:15","How to print the LDA topics models from gensim? Python","<python><nlp><lda><topic-modeling><gensim>","10","1","19","","","CC BY-SA 3.0"
"56999139","1","","","2019-07-12 01:29:56","","0","63","<p>I am trying to build a doc2vec model with more or less 10k sentences, after that I will use the model to find the most similar sentence in the model of some new sentences. </p>

<p>I have trained a gensim doc2vec model using the corpus(10k sentences) I have. This model can to some extend tell me if a new sentence is similar to some of the sentences in the corpus. 
But, there is a problem: it may happen that there are words in new sentences which don't exist in the corpus, which means that they don't have a word embedding. If this happens, the prediction result will not be good. 
As far as I know, the trained doc2vec model does have a matrix of doc vectors as well as a matrix of word vectors. So what I were thinking is to load a set of pre-trained word vectors, which contains a large number of words, and then train the model to get the doc vectors. Does it make sense? Is it possible with gensim? Or is there another way to do it?</p>
","10534864","","325727","","2019-07-12 02:24:39","2019-07-13 03:33:51","Is there a way to load pre-trained word vectors before training the doc2vec model?","<gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"57079642","1","57082110","","2019-07-17 15:39:01","","1","1206","<p>After reading the tutorial at gensim's <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb"" rel=""nofollow noreferrer"">docs</a>, I do not understand what is the correct way of generating new embeddings from a trained model. So far I have trained gensim's fast text embeddings like this:</p>

<pre><code>from gensim.models.fasttext import FastText as FT_gensim

model_gensim = FT_gensim(size=100)

# build the vocabulary
model_gensim.build_vocab(corpus_file=corpus_file)

# train the model
model_gensim.train(
    corpus_file=corpus_file, epochs=model_gensim.epochs,
    total_examples=model_gensim.corpus_count, total_words=model_gensim.corpus_total_words
)
</code></pre>

<p>Then, let's say I want to get the embeddings vectors associated with this sentences:</p>

<pre><code>sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()
sentence_president = 'The president greets the press in Chicago'.lower().split()
</code></pre>

<p>How can I get them with <code>model_gensim</code> that I trained previously?</p>
","9978457","","","","","2019-07-17 18:22:00","After training word embedding with gensim's fasttext's wrapper, how to embed new sentences?","<machine-learning><nlp><gensim><embedding>","1","0","","","","CC BY-SA 4.0"
"50692739","1","50709075","","2018-06-05 05:38:40","","0","295","<p>I am training with some documents with gensim's Doc2vec.  </p>

<p>I have two types of inputs:  </p>

<ol>
<li>Whole English Wikipedia: Each article of Wikipedia text is considered as one 
document for doc2vec training. (Total around 5.5 million articles or documents)</li>
<li>Some documents related to my project that are manually prepared and collected from some websites. (around 15000 documents).<br>
Where each document the size is around 100 sentences.</li>
</ol>

<p>Further, I want to use this model to infer sentences of size (10~20 words).</p>

<p>I request some clarification on my approach.<br>
Is the method of training over documents(size of each document approx. 100 sentences each) and then inferring over new sentence correct. ?</p>

<p>Or, should I train over only sentences and not documents and then infer over the new sentence.?</p>
","7397514","","","","","2018-06-05 21:18:42","Can doc2vec be useful if training on Documents and inferring on sentences only","<python><gensim><training-data><doc2vec>","1","0","2","","","CC BY-SA 4.0"
"49229336","1","","","2018-03-12 06:34:35","","0","687","<p>I am trying to train word embeddings(word2vec) on my own dataset using gensim library.</p>

<p><code>model = Word2Vec(sentences=alp[:20],size=100, window=6, min_count=5)
</code>
where alp is a list of list containing tokens of individual sentences in my corpus.</p>

<p>I get the following error whenever I try to train the w2v model.Please help.</p>

<pre><code>`Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 867, in worker_loop
    tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 785, in _do_train_job
    tally += train_batch_cbow(self, sentences, alpha, work, neu1, 
self.compute_loss)
  File ""gensim/models/word2vec_inner.pyx"", line 458, in gensim.models.word2vec_inner.train_batch_cbow (./gensim/models/word2vec_inner.c:5642)
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`

`Exception in thread Thread-1:
    Traceback (most recent call last):
      File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
        self.run()
      File ""/usr/lib/python3.5/threading.py"", line 862, in run
        self._target(*self._args, **self._kwargs)
      File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 867, in worker_loop
        tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))
      File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 785, in _do_train_job
        tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)
      File ""gensim/models/word2vec_inner.pyx"", line 458, in gensim.models.word2vec_inner.train_batch_cbow (./gensim/models/word2vec_inner.c:5642)
    ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`
</code></pre>

<p>`</p>
","8602998","","","","","2018-03-12 09:21:16","Trouble running gensim Word2Vec","<word2vec><gensim>","2","0","1","","","CC BY-SA 3.0"
"50684894","1","","","2018-06-04 16:21:57","","0","1189","<p>I have trained doc2vec model by following this tutorial for 500.000 documents.
<a href=""https://github.com/abtpst/Doc2Vec/blob/master/trainDoc2Vec.py"" rel=""nofollow noreferrer"">https://github.com/abtpst/Doc2Vec/blob/master/trainDoc2Vec.py</a></p>
<p>However, when I try to find most_similar documents for a given document, the results have similarity higher than 1. As you can see in the code and the outputs, docvecs.similarity returns 0.246 and docvecs.most_similar return 9.996 for similarity between same two documents. You can see the code and the output below:</p>
<pre><code>from gensim.models import doc2vec
def myhash(obj):
    return hash(obj) % (2 ** 32)    
model = doc2vec.Doc2Vec(hashfxn=myhash)
model = doc2vec.Doc2Vec.load(&quot;d2v.model&quot;)

tag1 = &quot;012020171590&quot;
tag2 = &quot;0109201716181&quot;

print(tag1 in model.docvecs.doctags)
print(tag2 in model.docvecs.doctags)

print(model.docvecs.similarity(tag1, tag2))
print(model.docvecs.most_similar(tag1))
</code></pre>
<p>Output:</p>
<blockquote>
<p>True</p>
<p>True</p>
<p>0.24682570854972158</p>
<p>[('0109201716181', 9.996172904968262), ('0120201611372', 9.853036880493164), ('010120166996', 9.613503456115723), ('012020173027', 8.97104263305664), ('01202017423', 8.886014938354492), ('01002009541', 8.783470153808594), ('00002004106', 8.682585716247559), ('0109201616963', 8.671405792236328), ('011020171931', 8.659266471862793), ('011020175199', 8.573907852172852)]</p>
</blockquote>
<p>Library and System Versions:</p>
<p>Linux-4.9.0-6-amd64-x86_64-with-debian-9.4</p>
<p>Python 3.5.3 (default, Jan 19 2017, 14:11:04)</p>
<p>NumPy 1.14.3</p>
<p>SciPy 1.1.0</p>
<p>gensim 3.4.0</p>
<p>FAST_VERSION 1</p>
","5355739","","-1","","2020-06-20 09:12:55","2018-06-04 16:21:57","Doc2vec most_similar method returns similarity score higher than 1","<python><python-3.x><gensim><doc2vec>","0","7","0","","","CC BY-SA 4.0"
"48999199","1","49040236","","2018-02-27 00:19:17","","2","1230","<p>I am a bit new to gensim and right now I am trying to solve the problem which involves using the doc2vec embeddings in keras. I wasn't able to find existing implementation of doc2vec in keras - as far as I see in all examples I found so far everyone just uses the gensim to get the document embeddings.</p>

<p>Once I trained my doc2vec model in gensim I need to export embeddings weights from genim into keras somehow and it is not really clear on how to do that. I see that</p>

<pre><code>model.syn0
</code></pre>

<p>Supposedly gives the word2vec embedding weights (according to <a href=""https://codekansas.github.io/blog/2016/gensim.html"" rel=""nofollow noreferrer"">this</a>). But it is unclear how to do the same export for document embeddings. Any advise?</p>

<p>I know that in general I can just get the embeddings for each document directly from gensim model but I want to fine-tune the embedding layer in keras later on, since doc embeddings will be used as a part of a larger task hence they might be fine-tuned a bit.</p>
","3633250","","","","","2020-01-06 09:54:11","Export gensim doc2vec embeddings into separate file to use with keras Embedding layer later","<keras><gensim><word-embedding><doc2vec>","1","1","","","","CC BY-SA 3.0"
"66221108","1","","","2021-02-16 08:41:55","","0","37","<p>I am trying to train a FastText model. The format of the file which the model is trained on is in a way that each line contains a sentence and sentences are separated from each other by a newline character. However, due to preprocessing, sometimes all words from a sentence are deleted, so the line becomes empty. As a result, out of 7 million lines, 72 of them are empty. I want to know whether this can cause a problem to how well the model is built. Does it make any difference? Does FastText ignore these empty lines or take them into account and as a result the model will not be trained well?</p>
","14251114","","","","","2021-02-16 10:00:53","the effect of empty lines on training a FastText model","<python><gensim><fasttext>","0","2","","","","CC BY-SA 4.0"
"67770231","1","","","2021-05-31 08:23:08","","-1","176","<p>I have just started with word2vec model and I want to make different cluster from my questions data.</p>
<p>So to make clusters what I got is, I have to</p>
<p>Create word embedding model
Get the word vector from the model
Create sentence vector from word vector
Using Kmeans cluster the questions data</p>
<p>So to get the word2vec word vector, <a href=""https://www.kaggle.com/chewzy/tutorial-how-to-train-your-custom-word-embedding"" rel=""nofollow noreferrer"">one of the article says</a></p>
<pre><code>def get_word2vec(tokenized_sentences):
    print(&quot;Getting word2vec model...&quot;)
    model = Word2Vec(tokenized_sentences, min_count=1)
    return model.wv
</code></pre>
<p>and then just create sentence vector and the Kmeans.</p>
<p><a href=""https://github.com/aravindmanoharan/Word2Vec-Using-Gensim/blob/master/word2vec.ipynb"" rel=""nofollow noreferrer"">and other article says</a>, after getting the word2vec model I have to build the vocab and then need to train the model. And then create sentence vector and then Kmeans/</p>
<pre><code>def get_word2vec_model(tokenized_sentences):
    start_time = time.time()
    print(&quot;Getting word2vec model...&quot;)
    model = Word2Vec(tokenized_sentences, sg=1, window=window_size,vector_size=size, min_count=min_count, workers=workers, epochs=epochs, sample=0.01)
    log_total_time(start_time)
    return model 


def get_word2vec_model_vector(model):
    start_time = time.time()
    print(&quot;Training...&quot;)
#     model = Word2Vec(tokenized_sentences, min_count=1)
    model.build_vocab(sentences=shuffle_corpus(tokenized_sentences), update=True)
    # Training the model
    for i in tqdm(range(5)):
        model.train(sentences=shuffle_corpus(tokenized_sentences), epochs=50, total_examples=model.corpus_count)
    log_total_time(start_time)
    return model.wv

def shuffle_corpus(sentences):
    shuffled = list(sentences)
    random.shuffle(shuffled)
    return shuffled
</code></pre>
<p>and this is how my tokenized_sentences look like</p>
<pre><code>8857                                     [, , , year, old]
11487     [, , birthday, canada, cant, share, job, friend]
20471                       [, , chat, people, also, talk]
5877                                           [, , found]
</code></pre>
<p><strong>Q1)</strong> the second approach gives the following error</p>
<pre><code>---&gt; 54     model.build_vocab(sentences=shuffle_corpus(tokenized_sentences), update=True)
     55     # Training the model
     56     for i in tqdm(range(5)):

~\AppData\Local\Programs\Python\Python38\lib\site-packages\gensim\models\word2vec.py in build_vocab(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    477 
    478         &quot;&quot;&quot;
--&gt; 479         self._check_corpus_sanity(corpus_iterable=corpus_iterable, corpus_file=corpus_file, passes=1)
    480         total_words, corpus_count = self.scan_vocab(
    481             corpus_iterable=corpus_iterable, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)

~\AppData\Local\Programs\Python\Python38\lib\site-packages\gensim\models\word2vec.py in _check_corpus_sanity(self, corpus_iterable, corpus_file, passes)
   1484         &quot;&quot;&quot;Checks whether the corpus parameters make sense.&quot;&quot;&quot;
   1485         if corpus_file is None and corpus_iterable is None:
-&gt; 1486             raise TypeError(&quot;Either one of corpus_file or corpus_iterable value must be provided&quot;)
   1487         if corpus_file is not None and corpus_iterable is not None:
   1488             raise TypeError(&quot;Both corpus_file and corpus_iterable must not be provided at the same time&quot;)

TypeError: Either one of corpus_file or corpus_iterable value must be provided
</code></pre>
<p>and</p>
<p><strong>Q2)</strong> Is is necessary build the vocab and then train the data? or getting model is the only thing i need to do?</p>
","2172547","","","","","2021-05-31 18:47:21","Either one of corpus_file or corpus_iterable value must be provided while training the word2vec model python","<python><machine-learning><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"48743444","1","","","2018-02-12 09:46:17","","0","771","<p><strong>Does <code>gensim.model.TfidfModel</code> have the term frequency saved?</strong></p>

<p>From the <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html"" rel=""nofollow noreferrer"">docs</a>, they use the formula:</p>

<pre><code>weights_i_j = frequency_i_j * log_2(D / doc_freq_i)
</code></pre>

<p>And when I prob the attributes of the <code>dir(model)</code> (TfidfModel object) with the following code:</p>

<pre><code>&gt;&gt;&gt; import gensim.downloader as api
&gt;&gt;&gt; from gensim.models import TfidfModel
&gt;&gt;&gt; from gensim.corpora import Dictionary
&gt;&gt;&gt;
&gt;&gt;&gt; dataset = api.load(""text8"")
&gt;&gt;&gt; dct = Dictionary(dataset)  # fit dictionary
&gt;&gt;&gt; corpus = [dct.doc2bow(line) for line in dataset]  # convert dataset to BoW format
&gt;&gt;&gt;
&gt;&gt;&gt; model = TfidfModel(corpus)  # fit model
&gt;&gt;&gt; dir(model)
</code></pre>

<p>I'm getting:</p>

<pre><code>['__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__getitem__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_adapt_by_suffix',
 '_apply',
 '_load_specials',
 '_save_specials',
 '_smart_save',
 'dfs',
 'id2word',
 'idfs',
 'initialize',
 'load',
 'normalize',
 'num_docs',
 'num_nnz',
 'save',
 'wglobal',
 'wlocal']
</code></pre>

<p>But I can't seem to find where are the term frequencies stored. </p>

<p><strong>If the term frequencies are not saved, is there a reason why?</strong> Since it's already stored to compute the weights anyways. </p>

<p><strong>Is there a way to retrieve the term frequencies somehow during the fitting process?</strong></p>
","610569","","","","","2018-06-20 10:33:04","Does gensim.model.TfidfModel have the term frequency saved?","<python><nlp><counter><gensim><tf-idf>","1","0","","","","CC BY-SA 3.0"
"57020405","1","","","2019-07-13 15:17:54","","0","78","<p>I am getting an import error as i try to import gensim, pandas and numpy in django views.py . It works fine in shell.
How can I import them in views.py. I have installed the libraries in virtual environment.</p>
","11780077","","11780077","","2019-07-14 11:26:16","2019-07-14 11:26:16","I am trying to import gensim, pandas and numpy in my django project but getting import error","<django><pandas><numpy><gensim>","0","6","","","","CC BY-SA 4.0"
"48749858","1","","","2018-02-12 15:27:30","","0","279","<p>I want to train word2vec (using gensim) on a large corpus data. The information I have is only co-occurence of any two words. My data has the format of<br>
word__tab__context_word__tab__Number<br>
(e.g: danger of 10, meaning '<em>danger</em>' and '<em>of</em>' co-occured 10 times in a window size of 5 in the corpus) for each line. 
Does word2vec of gensim take such input? I have searched through gensim tutorials and havn't seen any examples like this. </p>

<p>Thanks a lot for help. 
Li </p>
","9188095","","","","","2018-02-14 01:40:34","Can I use word-context-count pairs as input to gensim's Word2Vec","<word2vec><gensim>","1","1","","","","CC BY-SA 3.0"
"57060692","1","","","2019-07-16 15:25:27","","0","97","<p>So I created a Word2Vec model using Gensim using a document. Then I did some calculations on the vector array which I obtained by punching the code <code>model[model.wv.vocab]</code>. 
Now I have a new vector array with me and I want to ""devectorize"" this new vector array into text. How can this be done in python?</p>
","11792969","","","","","2019-07-16 15:25:27","How to ""devectorize"" a vector array (nx100) into text using a pre-trained model in Word2Vec using Gensim?","<python><nlp><artificial-intelligence><gensim><word2vec>","0","9","","","","CC BY-SA 4.0"
"50697092","1","","","2018-06-05 09:48:50","","6","2493","<p>I'm trying to get the text with its punctuation as it is important to consider the latter in my doc2vec model.  However, the wikicorpus retrieve only the text. After searching the web I found these pages:</p>

<ol>
<li>Page from gensim github issues section. It was a question by someone where the answer was to subclass WikiCorpus (answered by Piskvorky). Luckily, in the same page, there was a code representing the suggested 'subclass' solution. The code was provided by Rhazegh. (<a href=""https://github.com/RaRe-Technologies/gensim/issues/552"" rel=""noreferrer"">link</a>)</li>
<li>Page from stackoverflow with a title: ""Disabling Gensim's removal of punctuation etc. when parsing a wiki corpus"". However, no clear answer was provided and was treated in the context of spaCy. (<a href=""https://stackoverflow.com/questions/43500996/disabling-gensims-removal-of-punctuation-etc-when-parsing-a-wiki-corpus"">link</a>)</li>
</ol>

<p>I decided to use the code provided in page 1. My current code (mywikicorpus.py):</p>

<pre><code>import sys
import os
sys.path.append('C:\\Users\\Ghaliamus\\Anaconda2\\envs\\wiki\\Lib\\site-packages\\gensim\\corpora\\')

from wikicorpus import *

def tokenize(content):
    # override original method in wikicorpus.py
    return [token.encode('utf8') for token in utils.tokenize(content, lower=True, errors='ignore')
        if len(token) &lt;= 15 and not token.startswith('_')]

def process_article(args):
   # override original method in wikicorpus.py
    text, lemmatize, title, pageid = args
    text = filter_wiki(text)
    if lemmatize:
        result = utils.lemmatize(text)
    else:
        result = tokenize(text)
    return result, title, pageid


class MyWikiCorpus(WikiCorpus):
def __init__(self, fname, processes=None, lemmatize=utils.has_pattern(), dictionary=None, filter_namespaces=('0',)):
    WikiCorpus.__init__(self, fname, processes, lemmatize, dictionary, filter_namespaces)

    def get_texts(self):
        articles, articles_all = 0, 0
        positions, positions_all = 0, 0
        texts = ((text, self.lemmatize, title, pageid) for title, text, pageid in extract_pages(bz2.BZ2File(self.fname), self.filter_namespaces))
        pool = multiprocessing.Pool(self.processes)
        for group in utils.chunkize(texts, chunksize=10 * self.processes, maxsize=1):
            for tokens, title, pageid in pool.imap(process_article, group):  # chunksize=10):
                articles_all += 1
                positions_all += len(tokens)
            if len(tokens) &lt; ARTICLE_MIN_WORDS or any(title.startswith(ignore + ':') for ignore in IGNORED_NAMESPACES):
                continue
            articles += 1
            positions += len(tokens)
            if self.metadata:
                yield (tokens, (pageid, title))
            else:
                yield tokens
    pool.terminate()

    logger.info(
        ""finished iterating over Wikipedia corpus of %i documents with %i positions""
        "" (total %i articles, %i positions before pruning articles shorter than %i words)"",
        articles, positions, articles_all, positions_all, ARTICLE_MIN_WORDS)
    self.length = articles  # cache corpus length
</code></pre>

<p>And then, I used another code by Pan Yang (<a href=""https://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim"" rel=""noreferrer"">link</a>). This code initiates WikiCorpus object and retrieve the text. The only change in my current code is initiating MyWikiCorpus instead of WikiCorpus. The code (process_wiki.py):</p>

<pre><code>from __future__ import print_function
import logging
import os.path
import six
import sys
import mywikicorpus as myModule



if __name__ == '__main__':
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info(""running %s"" % ' '.join(sys.argv))

    # check and process input arguments
    if len(sys.argv) != 3:
        print(""Using: python process_wiki.py enwiki-20180601-pages-    articles.xml.bz2 wiki.en.text"")
        sys.exit(1)
    inp, outp = sys.argv[1:3]
    space = "" ""
    i = 0

    output = open(outp, 'w')
    wiki = myModule.MyWikiCorpus(inp, lemmatize=False, dictionary={})
    for text in wiki.get_texts():
        if six.PY3:
            output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
        else:
            output.write(space.join(text) + ""\n"")
        i = i + 1
        if (i % 10000 == 0):
            logger.info(""Saved "" + str(i) + "" articles"")

    output.close()
    logger.info(""Finished Saved "" + str(i) + "" articles"")
</code></pre>

<p>Through command line I ran the process_wiki.py code. I got text of the corpus with the last line in the command prompt: </p>

<p>(2018-06-05 09:18:16,480: INFO: Finished Saved 4526191 articles)</p>

<p>When I read the file in python, I checked the first article and it was without punctuation. Example:</p>

<p>(anarchism is a political philosophy that advocates self governed societies based on voluntary institutions these are often described as stateless societies although several authors have defined them more specifically as institutions based on non hierarchical or free associations anarchism holds the state to be undesirable unnecessary and harmful while opposition to the state is central anarchism specifically entails opposing authority or hierarchical)</p>

<p>My two relevant questions, and I wish you can help me with them, please:</p>

<ol>
<li>is there any thing wrong in my reported pipeline above?</li>
<li>regardless such pipeline, if I opened the gensim  wikicorpus python code (<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/wikicorpus.py"" rel=""noreferrer"">wikicorpus.py</a>) and wanted to edit it, what is the line that I should add it or remove it or update it (with what if possible) to get the same results but with punctuation?</li>
</ol>

<p>Many thanks for your time reading this long post. </p>

<p>Best wishes,</p>

<p>Ghaliamus </p>
","7735307","","","","","2021-01-05 10:53:29","How to get the wikipedia corpus text with punctuation by using gensim wikicorpus?","<python><nlp><gensim><doc2vec>","2","0","","","","CC BY-SA 4.0"
"49230376","1","49317637","","2018-03-12 07:50:42","","0","81","<p>I trained <code>LDA</code> model on my preprocessed corpus (i forget to save preprocessed data which was in  form of list of list) Is this possible to recover this data from trained model or not ?</p>
","2633704","","2633704","","2018-03-18 22:14:54","2018-03-18 22:14:54","is possible to extract bow from gensim lda model","<dataset><gensim>","1","0","","","","CC BY-SA 3.0"
"59397949","1","","","2019-12-18 18:15:49","","0","355","<p>I'm training some embeddings on a large corpus.  I gather from <code>gensim</code>'s documentation that it builds the vocabulary before beginning training.  In my case, building the vocabulary takes many hours.  I'd like to save time by re-using the vocabulary from the first model.  How can I do this?  the <code>.build_vocab</code> method can't take the <code>vocabulary</code> object from another model.  </p>

<p>Here's a dummy example:</p>

<pre><code>from gensim.models import FastText, Word2Vec
sentences = [""where are my goats"", ""yay i found my goats""]
m1 = Word2Vec(sentences, size  = 3)
m2 = Word2Vec(size = 4)
m2.build_vocab(m1.vocabulary) # doesn't work
</code></pre>
","1883795","","1883795","","2019-12-18 18:37:13","2019-12-19 18:23:47","How to initialize a gensim model with the vocabulary from another model?","<python><nlp><gensim>","2","0","","","","CC BY-SA 4.0"
"67770734","1","","","2021-05-31 09:00:00","","0","66","<p>I have a dataframe with an object column and over 100,000 rows like so:</p>
<pre><code>    df['words']
 0 the
 1 to
 2 of
 3 a
 4 with
 5 as
 6 job
 7 mobil
 8 market
 9 think
 10....
</code></pre>
<p>Desired Output with no stopwords:</p>
<pre><code>   df['words']
 0 way
 1 http
 2 internet
 3 car
 4 do
 5 want
 6 work
 7 uber
 8....
</code></pre>
<p>Is there a way to use gensim, spacy, or nltk to go through the usual stopwords in a single column?</p>
<p>I've tried:</p>
<pre><code>from gensim.parsing.preprocessing import remove_stopwords
stopwords.words('english')

df['words'] = df['words'].apply(lambda x: gensim.parsing.preprocessing.remove_stopwords(&quot; &quot;.join(x)))
</code></pre>
<p>But that results in:</p>
<pre><code>TypeError: can only join an iterable
</code></pre>
","13447271","","","","","2021-06-01 04:03:53","Remove stopwords-like words in a column","<python><pandas><nltk><gensim><stop-words>","1","4","","2021-06-01 05:19:26","","CC BY-SA 4.0"
"59419123","1","59453915","","2019-12-20 02:16:48","","0","1877","<p>I am new to gensim topic modeling. Here is my sample code:</p>

<pre><code>import nltk
nltk.download('stopwords')
import re
from pprint import pprint
# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
# spacy for lemmatization
import spacy
# Plotting tools
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
#%matplotlib inline
# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)
import warnings
warnings.filterwarnings(""ignore"",category=DeprecationWarning)
# NLTK Stop words
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
train=pd.DataFrame({'text':['find the most representative document for each topic',
                            'topic distribution across documents',
                            'to help with understanding the topic',
                            'one of the practical application of topic modeling is to determine']})
text=pd.DataFrame({'text':['how to find the optimal number of topics for topic modeling']})

data =  train.loc[:,'text'].values.tolist()

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

data_words = list(sent_to_words(data))
id2word = corpora.Dictionary(data_words)
corpus = [id2word.doc2bow(text) for text in data_words]
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                            id2word=id2word,
                                            num_topics=3)
</code></pre>

<p>So far so good. But I want to use lda_model to predict text. I at least need to know topic distribution over text and all the topic-word relation. </p>

<p>I think prediction is very common and important function for lda. But I do not know where I can find such function in gensim. Some answers says doc_lda = model[doc_bow] is prediction (<a href=""https://stackoverflow.com/questions/40924185/calculating-topic-distribution-of-an-unseen-document-on-gensim"">Calculating topic distribution of an unseen document on GenSim</a>). But I am not sure about it.</p>
","4827407","","472495","","2019-12-21 13:25:57","2020-02-11 10:49:42","How to use gensim topic modeling to predict new document?","<document><gensim><predict><lda>","2","3","","","","CC BY-SA 4.0"
"57543403","1","","","2019-08-18 09:53:41","","0","209","<p>Problem in reading the JSON dumps created from Wikipedia Dump using gensim</p>

<p>I am trying to follow the instructions given on this link to read wiki dump and create JSON file. </p>

<p><a href=""https://radimrehurek.com/gensim/scripts/segment_wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/scripts/segment_wiki.html</a></p>

<p>But the code is failing. </p>

<p>The code I am running is given below</p>

<pre><code>from gensim import utils
import json

 # iterate over the plain text data we just created
with utils.open('D:\\enwiki-latest.json.gz', 'rb') as f:
    for line in f:
      # decode each JSON line into a Python dictionary object
      article = json.loads(line)

      # each article has a ""title"", a mapping of interlinks and a list of ""section_titles"" and
       # ""section_texts"".
    print(""Article title: %s"" % article['title'])
    print(""Interlinks: %s"" + article['interlinks'])
    for section_title, section_text in zip(article['section_titles'], article['section_texts']):
        print(""Section text: %s"" % section_text)

and the stack trace is as follows

AttributeError                            Traceback (most recent call last)
&lt;ipython-input-1-8b1125fd41d0&gt; in &lt;module&gt;()
      3 
      4  # iterate over the plain text data we just created
----&gt; 5 with utils.open('D:\\enwiki-latest.json.gz', 'rb') as f:
      6     for line in f:
      7       # decode each JSON line into a Python dictionary object

AttributeError: module 'gensim.utils' has no attribute 'open'
</code></pre>

<p>Please help me understand whats wrong with my code. </p>

<p>I am running in the code on a Windows 10 machine using Anaconda</p>
","3910430","","","","","2019-08-18 09:53:41","Read JSON files created by gensim from Wikipedia dump","<json><gensim><wikipedia>","0","4","","","","CC BY-SA 4.0"
"49984763","1","","","2018-04-23 15:30:02","","0","314","<p>I apologise in advance as I cannot reproduce the dataset I'm working with. So I am just going to describe steps and hope someone is familiar with the whole process. </p>

<p>I'm trying to use LDA Gensim to extract topics from a list of text documents. </p>

<pre><code>from gensim.models import LdaModel
from gensim.corpora import Dictionary
</code></pre>

<p>I build <code>dictionary</code> and <code>corpus</code>:</p>

<pre><code>dictionary = Dictionary(final_docs)
corpus = [dictionary.doc2bow(doc) for doc in final_docs]
</code></pre>

<p>where <code>final_docs</code> is a list of lists with cleaned tokens for each text like this: </p>

<pre><code>final_docs = [['cat','dog','animal'],['school','university','education'],...['music','dj','pop']]
</code></pre>

<p>then I initiate the model like this:</p>

<pre><code># Set training parameters:
num_topics = 60
chunksize = 100
passes = 20
iterations = 400
eval_every = None 

# Make an index to word dictionary
temp = dictionary[0]  # This is only to ""load"" the dictionary.
id2word = dictionary.id2token

model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \
                           alpha='auto', eta='auto', \
                           iterations=iterations, num_topics=num_topics, \
                           passes=passes, eval_every=eval_every)
</code></pre>

<p>I can print topics and terms (10 most important). And they make sense. So it seems working fine.</p>

<pre><code>for idx in range(n_topics):
    print(""Topic #%s:"" % idx, model.print_topic(idx, 10))
</code></pre>

<p>BUT I struggle to plot all the documents as clusters using Bokeh. (And I really need Bokeh because I compare the same plot from different models). I know I have to reduce dimensionality to 2. And I try to do it using CountVectorizer and then T-sne:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
docs_vect = [' '.join(txt) for txt in final_docs]
cvectorizer = CountVectorizer(min_df=6, max_df=0.50, max_features=10000, stop_words=stop)
cvz = cvectorizer.fit_transform(docs_vect)
X_lda = model.fit_transform(cvz)
</code></pre>

<p>But I get this error: <code>AttributeError: 'LdaModel' object has no attribute 'fit_transform'</code>
I'm definitely doing something wrong with CountVectorizer. Could anyone help me out?</p>
","5967886","","3406693","","2018-04-23 16:42:25","2018-04-23 16:42:25","Plot clusters from LDA Gensim with Bokeh","<python-3.x><vectorization><gensim><dimensionality-reduction>","0","2","","","","CC BY-SA 3.0"
"23735576","1","23959741","","2014-05-19 10:37:21","","17","8935","<p>I am trying to train the word2vec model from <code>gensim</code> using the Italian wikipedia
""<a href=""http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2</a>""</p>

<p>However, I am not sure what is the best preprocessing for this corpus.</p>

<p><code>gensim</code> model accepts a list of tokenized sentences.
My first try is to just use the standard <code>WikipediaCorpus</code> preprocessor from <code>gensim</code>. This extract each article, remove punctuation and split words on spaces. With this tool each sentence would correspond to an entire model, and I am not sure of the impact of this fact on the model.</p>

<p>After this I train the model with default parameters. Unfortunately after training it seems that I do not manage to obtain very meaningful similarities.</p>

<p>What is the most appropriate preprocessing on the Wikipedia corpus for this task? (if this questions are too broad please help me by pointing to a relevant tutorial / article )</p>

<p>This the code of my first trial:</p>

<pre><code>from gensim.corpora import WikiCorpus
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
corpus = WikiCorpus('itwiki-latest-pages-articles.xml.bz2',dictionary=False)
max_sentence = -1

def generate_lines():
    for index, text in enumerate(corpus.get_texts()):
        if index &lt; max_sentence or max_sentence==-1:
            yield text
        else:
            break

from gensim.models.word2vec import BrownCorpus, Word2Vec
model = Word2Vec() 
model.build_vocab(generate_lines()) #This strangely builds a vocab of ""only"" 747904 words which is &lt;&lt; than those reported in the literature 10M words
model.train(generate_lines(),chunksize=500)
</code></pre>
","511005","","6573902","","2019-09-09 13:26:10","2019-09-09 13:26:10","Gensim train word2vec on wikipedia - preprocessing and parameters","<nlp><gensim><word2vec>","2","0","7","","","CC BY-SA 4.0"
"67865226","1","","","2021-06-07 02:14:50","","0","799","<p>I am trying to use Gensim packages as written below:</p>
<pre><code>import re, numpy as np, pandas as pd
from pprint import pprint

# Gensim
import gensim, spacy, logging, warnings
import gensim.corpora as corpora
from gensim.utils import lemmatize, simple_preprocess
from gensim.models import CoherenceModel
import matplotlib.pyplot as plt
</code></pre>
<p>But i keep getting the error:</p>
<pre><code>ImportError: cannot import name 'lemmatize' from 'gensim.utils' (/Users/xxx/opt/anaconda3/envs/virt_env/lib/python3.9/site-packages/gensim/utils.py)
</code></pre>
<p>I am using gensim v4.0.1, Python 3.8, numpy 1.20.0.</p>
<p>Has anyone encountered this kinda problem lately? Thank you</p>
","13737560","","","","","2021-06-07 16:21:10","Problem with importing Lemmatization from gensim","<python><numpy><gensim><lemmatization>","1","2","","","","CC BY-SA 4.0"
"67865773","1","67871758","","2021-06-07 03:56:42","","0","108","<p>I use python 3.9.1 on macOS Big Sur with an M1 chip.
And, gensim is 4.0.1</p>
<p>I tried to use the pre-trained Word2Vec model and I ran the code below:</p>
<pre><code>from gensim.models.word2vec import Word2Vec

model_path = '/path/to/word2vec.gensim.model'

model = Word2Vec.load(model_path)
</code></pre>
<p>However, I got an error below:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-11-4c1c2f93fadb&gt; in &lt;module&gt;
      1 from gensim.models.word2vec import Word2Vec
      2 
----&gt; 3 model = Word2Vec.load(model_path)

~/opt/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py in load(cls, rethrow, *args, **kwargs)
   1932                 &quot;Try loading older model using gensim-3.8.3, then re-saving, to restore &quot;
   1933                 &quot;compatibility with current code.&quot;)
-&gt; 1934             raise ae
   1935 
   1936     def _load_specials(self, *args, **kwargs):

~/opt/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py in load(cls, rethrow, *args, **kwargs)
   1920         &quot;&quot;&quot;
   1921         try:
-&gt; 1922             model = super(Word2Vec, cls).load(*args, **kwargs)
   1923             if not isinstance(model, Word2Vec):
   1924                 rethrow = True

~/opt/miniconda3/lib/python3.9/site-packages/gensim/utils.py in load(cls, fname, mmap)
    484         compress, subname = SaveLoad._adapt_by_suffix(fname)
    485 
--&gt; 486         obj = unpickle(fname)
    487         obj._load_specials(fname, mmap, compress, subname)
    488         obj.add_lifecycle_event(&quot;loaded&quot;, fname=fname)

~/opt/miniconda3/lib/python3.9/site-packages/gensim/utils.py in unpickle(fname)
   1456     &quot;&quot;&quot;
   1457     with open(fname, 'rb') as f:
-&gt; 1458         return _pickle.load(f, encoding='latin1')  # needed because loading from S3 doesn't support readline()
   1459 
   1460 

AttributeError: Can't get attribute 'Vocab' on &lt;module 'gensim.models.word2vec' from '/Users//opt/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py'&gt;

</code></pre>
<p>here is the link where I got the this model
<a href=""https://github.com/shiroyagicorp/japanese-word2vec-model-builder"" rel=""nofollow noreferrer"">https://github.com/shiroyagicorp/japanese-word2vec-model-builder</a></p>
<p>Thank you in advance.</p>
","15091297","","","","","2021-06-07 12:37:58","AttributeError: Can't get attribute on <module 'gensim.models.word2vec'","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"67804730","1","","","2021-06-02 12:02:39","","0","444","<p>while importing the below lines Jupyter compiler result in an error.</p>
<pre><code>ImportError: cannot import name 'deprecated' from 'gensim.utils
</code></pre>
<pre><code>from gensim.summarization.summarizer import summarize
from gensim.summarization import keywords**
</code></pre>
<p>Error as follows:</p>
<pre><code>~\AppData\Local\Programs\Python\Python39\Lib\site-packages\gensim\summarization\summarizer.py in &lt;module&gt;
     54 
     55 import logging
---&gt; 56 from gensim.utils import deprecated
     57 from gensim.summarization.pagerank_weighted import pagerank_weighted as _pagerank
     58 from gensim.summarization.textcleaner import clean_text_by_sentences as _clean_text_by_sentences

ImportError: cannot import name 'deprecated' from 'gensim.utils' (C:\Users\PavanKumar\AppData\Local\Programs\Python\Python39\Lib\site-packages\gensim\utils.py)
</code></pre>
","16077063","","3929826","","2021-06-02 12:08:21","2021-06-02 16:32:26","ImportError: cannot import name 'deprecated' from 'gensim.utils","<python><scipy><deprecated><gensim><summarize>","1","4","","","","CC BY-SA 4.0"
"67846338","1","","","2021-06-05 04:42:40","","0","70","<p>Want to convert gensim word2vec file to txt file, but I get this error:<br>
<strong>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte</strong>
<br>ps: The words is in Arabic language.</p>
<p>The mean goal is to build an embedding matrix, if any other solution is available I will be very gratefull.</p>
<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('SG\w2v_SG_300_3_400_10.model', binary=True)
model.save_word2vec_format('SG\w2v_SG_300_3_400_10.txt', binary=False)




``` unicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-239-fe28681b0b8d&gt; in &lt;module&gt;
     11 from gensim.models.keyedvectors import KeyedVectors
     12 
---&gt; 13 model = KeyedVectors.load_word2vec_format('SG\w2v_SG_300_3_400_10.model', binary=True)
     14 model.save_word2vec_format('SG\w2v_SG_300_3_400_10.txt', binary=False)

D:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)
   1628 
   1629         &quot;&quot;&quot;
-&gt; 1630         return _load_word2vec_format(
   1631             cls, fname, fvocab=fvocab, binary=binary, encoding=encoding, unicode_errors=unicode_errors,
   1632             limit=limit, datatype=datatype, no_header=no_header,

D:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in _load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)
   1900             fin = utils.open(fname, 'rb')
   1901         else:
-&gt; 1902             header = utils.to_unicode(fin.readline(), encoding=encoding)
   1903             vocab_size, vector_size = [int(x) for x in header.split()]  # throws for invalid file format
   1904         if limit:

D:\ProgramData\Anaconda3\lib\site-packages\gensim\utils.py in any2unicode(text, encoding, errors)
    363     if isinstance(text, str):
    364         return text
--&gt; 365     return str(text, encoding, errors=errors)
    366 
    367 

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte 
</code></pre>
<p>The error message.</p>
","9295970","","9295970","","2021-06-05 13:56:32","2021-06-05 17:44:32","Convert word2vec model file to text","<python><nlp><gensim><arabic><word-embedding>","1","2","","","","CC BY-SA 4.0"
"50009030","1","53813574","","2018-04-24 18:58:29","","1","4363","<p>What is the correct way to use gensim's Phrases and preprocess_string together ?, i am doing this way but it a little contrived.</p>

<pre><code>from gensim.models.phrases import Phrases
from gensim.parsing.preprocessing import preprocess_string
from gensim.parsing.preprocessing import strip_tags
from gensim.parsing.preprocessing import strip_short
from gensim.parsing.preprocessing import strip_multiple_whitespaces
from gensim.parsing.preprocessing import stem_text
from gensim.parsing.preprocessing import remove_stopwords
from gensim.parsing.preprocessing import strip_numeric
import re
from gensim import utils

# removed ""_"" from regular expression
punctuation = r""""""!""#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^`{|}~""""""

RE_PUNCT = re.compile(r'([%s])+' % re.escape(punctuation), re.UNICODE)


def strip_punctuation(s):
    """"""Replace punctuation characters with spaces in `s` using :const:`~gensim.parsing.preprocessing.RE_PUNCT`.

    Parameters
    ----------
    s : str

    Returns
    -------
    str
        Unicode string without punctuation characters.

    Examples
    --------
    &gt;&gt;&gt; from gensim.parsing.preprocessing import strip_punctuation
    &gt;&gt;&gt; strip_punctuation(""A semicolon is a stronger break than a comma, but not as much as a full stop!"")
    u'A semicolon is a stronger break than a comma  but not as much as a full stop '

    """"""
    s = utils.to_unicode(s)
    return RE_PUNCT.sub("" "", s)



my_filter = [
    lambda x: x.lower(), strip_tags, strip_punctuation,
    strip_multiple_whitespaces, strip_numeric,
    remove_stopwords, strip_short, stem_text
]


documents = [""the mayor of new york was there"", ""machine learning can be useful sometimes"",""new york mayor was present""]

sentence_stream = [doc.split("" "") for doc in documents]
bigram = Phrases(sentence_stream, min_count=1, threshold=2)
sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
test  = "" "".join(bigram[sent])


print(preprocess_string(test))
print(preprocess_string(test, filters=my_filter))
</code></pre>

<p>The result is:</p>

<pre><code>['mayor', 'new', 'york']
['mayor', 'new_york'] #correct
</code></pre>

<p>part of the code was taken from: <a href=""https://stackoverflow.com/questions/35716121/how-to-extract-phrases-from-corpus-using-gensim/35748858"">How to extract phrases from corpus using gensim</a></p>
","7480950","","6573902","","2020-09-24 08:09:27","2020-09-24 08:09:27","Correct way of using Phrases and preprocess_string gensim","<python><python-3.x><nlp><gensim>","1","1","1","","","CC BY-SA 3.0"
"59423553","1","","","2019-12-20 10:17:37","","1","223","<p>Dear all, I have trained word2vec in gensim using Wikipedia data and saved using following program.</p>

<pre><code>model = Word2Vec(LineSentence(inp), size=300, window=5, min_count=5, max_final_vocab=500000,
        workers=multiprocessing.cpu_count())

model.save(""outp1"")
</code></pre>

<p>I want use this model in keras for multi-class Text Classification, What changes I need to do in the following code</p>

<pre><code>model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(10, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 5
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, 
batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

accr = model.evaluate(X_test,Y_test) 
</code></pre>

<p>Actually I am new and trying to learn.</p>
","12437434","","","","","2019-12-20 16:07:31","Using pretrained gensim Word2vec embedding along with data set in keras","<python-3.x><machine-learning><keras><artificial-intelligence><gensim>","0","1","","","","CC BY-SA 4.0"
"49962749","1","49988058","","2018-04-22 05:18:53","","3","1523","<p>I'm trying to train a doc2vec model using the gensim library on 50 million sentences of variable length.</p>

<p>Some tutorials (eg. <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a>) have a <code>model.build_vocab</code> step before the actual training process. This part has been running for 3 hours now without any updates.</p>

<p>Is this step necessary for the training process? Why could this step be taking so long since it's just a linear pass over the data?</p>

<p>Using gensim version 3.4.0 with python 3.6.0</p>
","7977512","","","","","2018-04-26 06:33:49","Gensim build_vocab taking too long","<python-3.x><nlp><word2vec><gensim>","1","1","1","","","CC BY-SA 3.0"
"59510075","1","59513713","","2019-12-28 10:32:18","","0","91","<p>We have a <code>flask</code> application where we need to load a pretrained model located at the path <code>'/root/apps/mlapi/resources/emoji2vec.bin'</code> using <code>gensim</code>. While running the code I am getting below error</p>

<pre><code>File ""mlapi.py"", line 26, in &lt;module&gt;
    e2v_model = ModelEmoji2Vec()
  File ""/home/atinesh/Downloads/Current/vnc_chat/apps2/mlapi/models/susheels/text2emoji/vector_model/modelE2V.py"", line 19, in __init__
    self.e2v = gsm.KeyedVectors.load_word2vec_format(emoji2vec_path, binary=True)
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/gensim/models/keyedvectors.py"", line 1498, in load_word2vec_format
    limit=limit, datatype=datatype)
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/gensim/models/utils_any2vec.py"", line 342, in _load_word2vec_format
    with utils.open(fname, 'rb') as fin:
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/smart_open/smart_open_lib.py"", line 308, in open
    errors=errors,
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/smart_open/smart_open_lib.py"", line 517, in _shortcut_open
    return _builtin_open(parsed_uri.uri_path, mode, buffering=buffering, **open_kwargs)
PermissionError: [Errno 13] Permission denied: '/root/apps/mlapi/resources/emoji2vec.bin'
</code></pre>

<p>Basically, error is occurring at </p>

<pre><code>#modelE2V.py, line 19
self.e2v = gsm.KeyedVectors.load_word2vec_format(emoji2vec_path, binary=True)
</code></pre>

<p>It says that permission denied. But if I try to load the model using simple python script outside flask app it works fine, why this error is occurring inside flask application</p>
","653397","","","","","2019-12-28 18:24:51","Unable to load a file in flask application","<python><python-3.x><flask><gensim>","1","4","","","","CC BY-SA 4.0"
"59478986","1","59481692","","2019-12-25 13:45:50","","0","422","<p>I trained a doc2vec model using <code>python gensim</code> on a corpus of 40,000,000 documents. This model is used for infering docvec on millions of documents everyday. To ensure stability, I set <code>alpha</code> to a small value and a large <code>steps</code> instead of setting a constant random seed:</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
model = Doc2Vec.load('doc2vec_dm.model')
doc_demo = ['a','b']
# model.random.seed(0)
model.infer_vector(doc_demo, alpha=0.1, min_alpha=0.0001, steps=100)
</code></pre>

<p><code>doc2vec.infer_vector()</code> accepts only one documents each time and it takes almost 0.1 second to infer each docvec. Is there any <code>API</code> that can handle a series of documents in each infering step?</p>
","12463200","","","","","2019-12-25 21:19:29","How to perform doc2vec.infer_vector() on millions of documents?","<gensim><doc2vec>","1","0","0","","","CC BY-SA 4.0"
"59482140","1","59482484","","2019-12-25 22:01:34","","2","388","<p>Gensim Doc2Vec infer_vector on paragraphs with unseen words generates vectors that differ based on the characters in the unsween words.</p>

<pre><code>for i in range(0, 2):
    print(model.infer_vector([""zz""])[0:2])
    print(model.infer_vector([""zzz""])[0:2])
    print(model.infer_vector([""zzzz""])[0:2])
    print(""\n"")

[ 0.00152548 -0.00055992]
[-0.00165872 -0.00047997]
[0.00125548 0.00053445]


[ 0.00152548 -0.00055992] # same as in previous iteration
[-0.00165872 -0.00047997]
[0.00125548 0.00053445]
</code></pre>

<p>I am trying understand how unseen words affect initialization of the infer_vector. It looks like different characters will produce different vectors. Trying to understand why.</p>
","2649999","","","","","2019-12-25 23:16:57","Gensim Doc2Vec infer_vector on unseen words differs based on characters in these words","<gensim><word2vec><doc2vec>","1","0","1","","","CC BY-SA 4.0"
"59501121","1","59504071","","2019-12-27 12:55:42","","0","133","<p>I am confused as to how I can use <strong>Doc2Vec(using Gensim)</strong> for IMDB sentiment classification dataset. I have got the Doc2Vec embeddings after training on my corpus and built my Logistic Regression model using it. How do I use it to make predictions for new reviews? sklearn TF-IDF has a <em>transform</em> method that can be used on test data after training on training data, what is its equivalent in Gensim Doc2Vec?</p>
","10243253","","","","","2019-12-27 17:27:22","Sentiment Classification using Doc2Vec","<python><nlp><gensim><doc2vec>","2","0","","","","CC BY-SA 4.0"
"57804554","1","57804653","","2019-09-05 11:32:05","","1","293","<p>I am running a dockerfile on a ubuntu base image as follows :</p>

<pre><code>FROM ubuntu:14.04

# Install dependencies
RUN apt-get update 
RUN apt-get install -y \
    software-properties-common
RUN add-apt-repository universe
RUN apt-get install -y python3.5 \
    python3-pip 

RUN apt-get install libav-tools -y

RUN apt-get update 

RUN apt-get upgrade

#RUN  apt-get install google-cloud-sdk

RUN pip3 install --upgrade pip 
RUN pip3 install pandas 
RUN pip3 install glob3

RUN     pip3 install --upgrade pip 
#RUN    pip3 install pandas 
RUN pip3 install glob3
#RUN    pip3 install json
RUN pip3 install numpy
RUN pip3 install fuzzywuzzy
RUN pip3 install gensim
</code></pre>

<p>I have python 3.5 installed on this machine, but still I am getting the error as follows :</p>

<pre><code>Collecting gensim
  Downloading https://files.pythonhosted.org/packages/3a/bc/1415be59292a23ff123298b4b46ec4be80b3bfe72c8d188b58ab2653dee4/gensim-3.8.0.tar.gz (23.4MB)
    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-klg_2vmh/gensim/setup.py'""'""'; __file__='""'""'/tmp/pip-install-klg_2vmh/gensim/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base pip-egg-info
         cwd: /tmp/pip-install-klg_2vmh/gensim/
    Complete output (5 lines):
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/pip-install-klg_2vmh/gensim/setup.py"", line 23, in &lt;module&gt;
        raise Exception('This version of gensim needs Python 2.7, 3.5 or later.')
    Exception: This version of gensim needs Python 2.7, 3.5 or later.
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
</code></pre>

<p>Is there some specific version of Gensim that i need to download or this is some different error.</p>
","6294231","","","","","2019-09-05 12:09:15","unable to install Gensim with Python 3.5 on ubuntu","<python><docker><ubuntu><gensim>","1","0","1","","","CC BY-SA 4.0"
"40326300","1","","","2016-10-30 05:32:50","","2","513","<p>I am using Gensim's doc2vec method to read in my text file which contains 1 sentence per line. It reads my file into a dictionary where the keys are a tokenized list of terms and the values are the sentence number.</p>

<p>Here is my code:</p>

<pre><code>    from gensim import utils
    from gensim.models.doc2vec import LabeledSentence,TaggedLineDocument
    from gensim.models import Doc2Vec
    new_file = open('new_file.txt','w')
    with open('myfile.txt','r') as inp:
        for line in inp:
            utils.simple_preprocess(line)
            file1.write(str(utils.simple_preprocess(line)) + ""\n"")
    file1.close()
</code></pre>

<p>Example output of new file:</p>

<pre><code>[u'hi', u'how', u'are', u'you']
[u'its', u'such', u'great', u'day']
[u'its', u'such', u'great', u'day']
[u'its', u'such', u'great', u'day']
</code></pre>

<p>Then i feed that list into gensim's taggedlinedocument function:</p>

<pre><code>s = TaggedLineDocument('myfile.txt')
for k,v in s:
    print k, v
</code></pre>

<p>Example Output:</p>

<pre><code>[u'hi', u'how', u'are', u'you'] [0]
[u'hi', u'how', u'are', u'you'] [1]
[u'hi', u'how', u'are', u'you'] [2]
[u'its', u'such', u'a', u'great', u'day'] [3]
[u'its', u'such', u'a', u'great', u'day'] [4]
</code></pre>

<p>Question is, given the tag id (example 0), how do i get back the original sentence?</p>
","2800939","","2800939","","2016-10-30 05:37:52","2017-01-19 03:44:27","python gensim retrieve original sentences from doc2vec taggedlinedocument","<python><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 3.0"
"34153708","1","34154133","","2015-12-08 10:39:55","","0","302","<p>I use the next code to train the model:</p>

<pre><code>norms_train = [ [''], [ u'word', u'to', u'learn', ... ], ...]
model = word2vec.Word2Vec(norms_train, size=100, window=10)
</code></pre>

<p>With procedure to check the results:</p>

<pre><code>i, j = 0, 0
for text in norms_train:
    j += len(text)
    for word in text:
        if word not in model.vocab:
            i += 1
print i, '/', j
</code></pre>

<p>13129 / 185379</p>
","864113","","","","","2015-12-08 11:00:02","All of the words, those I use to train the word2vec model, must be in model.vocab, aren't they?","<python><gensim><training-data><word2vec>","1","0","","2015-12-08 13:09:54","","CC BY-SA 3.0"
"59418433","1","60721342","","2019-12-20 00:14:31","","1","549","<p>I just study gensim for topic modeling. when I use </p>

<pre><code>lda_model = gensim.models.ldamodel.LdaModel(...)
</code></pre>

<p>the result lda_model has two functions: get_topics() and get_document_topics(). I can find the topic-word and document-topics by them. But, I want to try:</p>

<pre><code>hdp_lda_model = gensim.models.hdpmodel.HdpModel(...)
</code></pre>

<p>I can only find there is get_topics() in its result, no something like get_document_topics(). So I cannot find the relation of document and topics. But it should be somewhere. I read some instruction from <a href=""https://radimrehurek.com/gensim/models/hdpmodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/hdpmodel.html</a>. But I did not find any (maybe I miss something?). So is there a function in hdp model, which is like get_document_topics() in lda model?</p>
","4827407","","472495","","2019-12-21 13:25:22","2020-03-17 11:20:32","How to get document-topics using models.hdpmodel ‚Äì Hierarchical Dirichlet Process in gensim","<document><gensim><word><lda><hdp>","1","0","","","","CC BY-SA 4.0"
"59521204","1","","","2019-12-29 16:29:13","","2","128","<p>I'm specifically using <code>gensim</code> to build a <code>TfidfModel</code> but I believe this is more of a general TF-IDF question...</p>

<p>Let's say I build a TF-IDF model with 10 documents. How can I use this model to detect words that are high-value in the model, but under-represented from a specific seen or unseen document?</p>

<p>For example, if documents 1-9 all use the word ""banana"" frequently, how can I discover that document 10 (or a document not used to build the model) doesn't use it at all?</p>

<p>I know that I could just pull a dictionary of words and values from the model and do my own kind of comparison but I'm wondering if there's a better way.</p>
","1171899","","","","","2019-12-29 16:29:13","How to use a TF-IDF model to find ""missing"" or under-represented words from a document?","<python><nlp><data-science><gensim><tf-idf>","0","3","","","","CC BY-SA 4.0"
"67785968","1","","","2021-06-01 09:09:41","","0","21","<p>I have some difficulties adjusting some gensim 3.x code to gensim 4. Specifically I have this code snippet:</p>
<pre><code>for new_index,word in enumerate(common_vocab):
            old_vocab_obj=old_vocab[word]
            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)
</code></pre>
<p>gensim 4 does not have the attribute Vocab anymore. What would I use instead?</p>
","12475000","","","","","2021-06-01 09:09:41","Creating a new Vocab in GenSim 4","<python><gensim>","0","1","","","","CC BY-SA 4.0"
"59465247","1","","","2019-12-24 07:20:17","","1","780","<p>I am doing topic modeling using gensim (in jupyter notebook). I successfully created a model and visualized it. Below is the code:</p>

<pre><code>import time
start_time = time.time()
import re
import spacy
import nltk
import pyLDAvis
import pyLDAvis.gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)
import warnings
warnings.filterwarnings(""ignore"",category=DeprecationWarning)
# nlp = spacy.load('en')
stop_word_list = nltk.corpus.stopwords.words('english')
stop_word_list.extend(['from', 'subject', 're', 'edu', 'use'])
df = pd.read_csv('Topic_modeling.csv')
data = df.Articles.values.tolist()

# Remove Emails
data = [re.sub('\S*@\S*\s?', '', sent) for sent in data]

# Remove new line characters
data = [re.sub('\s+', ' ', sent) for sent in data]

# Remove distracting single quotes
data = [re.sub(""\'"", """", sent) for sent in data]


def sent_to_words(sentences):
    for sentence in sentences:
        yield gensim.utils.simple_preprocess(str(sentence), deacc=True)  # deacc=True removes punctuations


data_words = list(sent_to_words(data))

# Build the bigram and trigram models
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)

# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# Define functions for stopwords, bigrams, trigrams and lemmatization
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_word_list] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """"""https://spacy.io/api/annotation""""""
    texts_out = []
    for sent in texts:
        doc = nlp("" "".join(sent))
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out


# Remove Stop Words
data_words_nostops = remove_stopwords(data_words)

# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# python3 -m spacy download en
nlp = spacy.load('en', disable=['parser', 'ner'])

# Do lemmatization keeping only noun, adj, vb, adv
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN','ADJ'])

# Create Dictionary
id2word = corpora.Dictionary(data_lemmatized)

# Create Corpus
texts = data_lemmatized

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]


# Build LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics= 3,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=20,
                                           alpha=0.4,
                                           eta=0.2,
                                           per_word_topics=True)

print(lda_model.print_topics())
doc_lda = lda_model[corpus]
</code></pre>

<p>Now I want to find dominant topics in each sentence. So I am using the below code:</p>

<pre><code>def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # =&gt; dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = "", "".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)


df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']

# Show
df_dominant_topic.head(10)
</code></pre>

<p>however, I am getting below error: </p>

<blockquote>
  <p>TypeError                                 Traceback (most recent call
  last)  in 
       22 
       23 
  ---> 24 df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)
       25 
       26 # Format</p>
  
  <p> in format_topics_sentences(ldamodel,
  corpus, texts)
        5     # Get main topic in each document
        6     for i, row in enumerate(ldamodel[corpus]):
  ----> 7         row = sorted(row, key=lambda x: (x[1]), reverse=True)
        8         # Get the Dominant topic, Perc Contribution and Keywords for each document
        9         for j, (topic_num, prop_topic) in enumerate(row):</p>
  
  <p>TypeError: '&lt;' not supported between instances of 'int' and 'tuple'</p>
</blockquote>

<p>I don't understand what is the problem. 
Can anyone help? </p>

<p>Thanks in advance!</p>
","11230191","","","","","2020-03-20 04:30:28","Type error while finding dominant topics in each sentence in Gensim","<jupyter-notebook><typeerror><python-3.7><gensim><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"67840689","1","","","2021-06-04 16:18:14","","1","17","<p>I'm working on the gensim‚Äôs word2vec model, but different runs on the same dataset produce the different model. I tried to set seed to a fixed number, including  PYTHONHASHSEED and set the number of workers being one. But all the above methods are not working.</p>
<p>I included my code here:</p>
<pre><code>def word2vec_model(data):
    model = gensim.models.Word2Vec(data, size=300, window=20, workers=4, min_count=1)
    model.wv.save(&quot;word2vec.wordvectors&quot;)
    embed = gensim.models.KeyedVectors.load(&quot;word2vec.wordvectors&quot;, mmap='r')
    return embed
</code></pre>
<p>I checked the following output:</p>
<pre><code>Cooking.similar_by_vector(Cooking['apple'], topn=10, restrict_vocab=None)
</code></pre>
<p>example output:</p>
<pre><code>[('apple', 0.9999999403953552),
 ('charcoal', 0.2554503381252289),
 ('response', 0.25395694375038147),
 ('boring', 0.2537640631198883),
 ('healthy', 0.24807702004909515),
 ('wrong', 0.24783077836036682),
 ('juice', 0.24270494282245636),
 ('lacta', 0.2373320758342743),
 ('saw', 0.2359238862991333),
 ('insufferable', 0.23015251755714417)]
</code></pre>
<p>Each run, I got different similar words.</p>
<p>Does anyone know how to solve itÔºüI appreciate any direct codes or documentation. Thank you in advance!</p>
","16129784","","","","","2021-06-04 17:51:57","How to control randomness when training word2vec in Gensim?","<python><random><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"50413059","1","55675388","","2018-05-18 14:02:29","","4","2747","<p>I want to use Dynamic Topic Modeling by Blei et al. (<a href=""http://www.cs.columbia.edu/~blei/papers/BleiLafferty2006a.pdf"" rel=""nofollow noreferrer"">http://www.cs.columbia.edu/~blei/papers/BleiLafferty2006a.pdf</a>) for a large corpus of nearly 3800 patent documents.
Does anybody has experience in using the DTM in the gensim package?
I identified two models: </p>

<ol>
<li>models.ldaseqmodel ‚Äì Dynamic Topic Modeling in Python <a href=""https://radimrehurek.com/gensim/models/ldaseqmodel.html"" rel=""nofollow noreferrer"">Link</a></li>
<li>models.wrappers.dtmmodel ‚Äì Dynamic Topic Models (DTM) <a href=""https://radimrehurek.com/gensim/models/wrappers/dtmmodel.html"" rel=""nofollow noreferrer"">Link</a></li>
</ol>

<p>Which one did you use, of if you used both, which one is ""better""? In better words, which one did/do you prefer?</p>
","9751594","","","","","2019-04-14 12:42:47","Dynamic Topic Modeling with Gensim / which code?","<python-3.x><gensim><lda>","1","0","2","","","CC BY-SA 4.0"
"59548233","1","","","2019-12-31 20:28:45","","0","110","<p>I've got several hundred pandas data frames, each of which has a column of very long strings that need to be processed/sentencized and finally tokenized before modeling with word2vec.</p>

<p>I can store them in any format on the disk, before I build a stream to pass them to gensim's word2vec function. </p>

<p>What format would be best, and why?  The most important criterion would be performance vis-a-vis training (which will take many days), but coherent structure to the filesystem would also be nice.  </p>

<p>Would it be crazy to store several million or maybe even a few billion text files containing one sentence each?  Or perhaps some sort of database?  If this was numerical data I'd use hdf5.  But it's text.  The cleanest would be to store them in the original data frames, but that seems less ideal from an i/o perspective, because I'd have to load each data frame (largish) every epoch.  </p>

<p>What makes the most sense here?</p>
","1883795","","","","","2020-01-02 05:00:26","Best way to store processed text data for streaming to gensim?","<stream><nlp><storage><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"59570511","1","","","2020-01-02 21:54:47","","2","619","<p>I downloaded <a href=""https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M-subword.bin.zip"" rel=""nofollow noreferrer"">wiki-news-300d-1M-subword.bin.zip</a> and loaded it as follows:</p>

<pre class=""lang-py prettyprint-override""><code>import gensim
print(gensim.__version__)
model = gensim.models.fasttext.load_facebook_model('./wiki-news-300d-1M-subword.bin')
print(type(model))
model_keyedvectors = model.wv
print(type(model_keyedvectors))
model_keyedvectors.save('./wiki-news-300d-1M-subword.keyedvectors')
</code></pre>

<p>As expected, I see the following output:</p>

<pre class=""lang-sh prettyprint-override""><code>3.8.1
&lt;class 'gensim.models.fasttext.FastText'&gt;
&lt;class 'gensim.models.keyedvectors.FastTextKeyedVectors'&gt;
</code></pre>

<p>I also see the following three numpy arrays serialized to the disk:</p>

<pre class=""lang-sh prettyprint-override""><code>$ du -h wiki-news-300d-1M-subword.keyedvectors*
127M    wiki-news-300d-1M-subword.keyedvectors
2.3G    wiki-news-300d-1M-subword.keyedvectors.vectors_ngrams.npy
2.3G    wiki-news-300d-1M-subword.keyedvectors.vectors.npy
2.3G    wiki-news-300d-1M-subword.keyedvectors.vectors_vocab.npy
</code></pre>

<p>I understand <code>vectors_vocab.npy</code> and <code>vectors_ngrams.npy</code>, however, what is <code>vectors.npy</code> is used for internally in <code>gensim.models.keyedvectors.FastTextKeyedVectors</code>? If I look at the source code for finding out <a href=""https://github.com/RaRe-Technologies/gensim/blob/3.8.1/gensim/models/keyedvectors.py#L2090"" rel=""nofollow noreferrer"">word vector</a>, I do not see how attribute <code>vectors</code> is being used anywhere. I see the attributes <code>vectors_vocab</code> and <code>vectors_ngrams</code> bing used. However, if I remove <code>vectors.npy</code> file, I am not able to load the model using <code>gensim.models.keyedvectors.FastTextKeyedVectors.load</code> method.</p>

<p>Can someone please explain where this variable is used? Can I remove it if all I am interested is in looking word vectors (to reduce memory footprint)?</p>

<p>Thanks. </p>
","1522429","","","","","2020-01-07 00:23:11","FastTextKeyedVectors difference between vectors, vectors_vocab and vectors_ngrams instance variables","<gensim><fasttext>","1","0","1","","","CC BY-SA 4.0"
"62939869","1","","","2020-07-16 17:02:29","","0","87","<p>The word embeddings when training with Gensim are meaningless: the similarities on known relationships (King - Man + Woman -&gt; Queen) and others simply don't hold (not even close), they might as well be random.</p>
<p>Using the same training data and equivalent parameters, I get meaningful results with Facebook's FastText. In comparison, I've tried the Gensim FastText and Word2Vec classes, both returning meaningless embeddings.</p>
<p>To narrow things down, here are some of the settings:</p>
<ul>
<li>30GB training data text file. Space separated tokens, newline separated sentences.</li>
<li>Final vocab size after training: 12M+ word vectors</li>
<li>Dimension = 100</li>
<li>Disabled subwords</li>
<li>Disabled n-grams</li>
<li>Negative sampling = 8</li>
<li>Skip-gram</li>
<li>Min-count = 20</li>
</ul>
<p>Training code below:</p>
<pre class=""lang-py prettyprint-override""><code>print(&quot;Setting Model&quot;)
model = FastText(
# model = Word2Vec(
  sg=1,
  size=dim, 
  max_vocab_size=max_vocab_size,
  window=10, 
  negative=8,
  min_count=20,
  max_n=0,
  min_n=0,
  workers=16,
  alpha=0.1,
)
print(&quot;Building vocab&quot;)
model.build_vocab(
  corpus_file=corpus_file,
  progress_per=1_000_000
)
print(&quot;Training model&quot;)
model.train(
  corpus_file=corpus_file,
  total_words=model.corpus_total_words,
  total_examples=model.corpus_count,
  epochs=5,
  word_ngrams=0,
)
print(&quot;Saving model&quot;)
model.save(model_name)
</code></pre>
<p>I would rather avoid using Facebook's FastText as it would require me changing its source code to achieve this vocab size.</p>
<p>What could be going wrong, or how can I debug this? I have been able to import Facebook's FastText vectors into Gensim and it works fine.</p>
","2628744","","6573902","","2020-07-16 18:19:04","2020-07-16 18:19:04","Gensim training meaningless word embeddings","<nlp><gensim><embedding><word-embedding><fasttext>","0","14","","","","CC BY-SA 4.0"
"59568131","1","","","2020-01-02 18:20:04","","0","205","<p>I am new to Machine Learning and it is the first time that I am using python's gensim in order to extract topics from text.</p>

<p>I successfully trained a model (for 100 topics) and then I had the idea to use that model in an HTTP API that I created using python flask. The endpoint gives as back terms for a given text.</p>

<p>Btw model is loaded when I initialize the API. </p>

<p>After trying this out on production, memory (on a small VM ~ 1GB Ram) exhausted and finally I got an error:</p>

<pre><code>tags = tags + lda.topic_words(topic_index, num_of_keywords_for_topic, model, words)
  File ""/var/app/tagbee/lda.py"", line 64, in topic_words
    x2 = model.get_topic_terms(topicid=topic_index, topn=number_of_keywords)
  File ""/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py"", line 1224, in get_topic_terms
    topic = self.get_topics()[topicid]
  File ""/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py"", line 1204, in get_topics
    topics = self.state.get_lambda()
  File ""/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py"", line 269, in get_lambda
    return self.eta + self.sstats
MemoryError: Unable to allocate 96.6 MiB for an array with shape (100, 253252) and data type float32
</code></pre>

<p>So I have some questions:</p>

<ul>
<li>Can a gensim LDA model be used that way, mean in an HTTP API?</li>
<li>If yes, what is the trick to make it happen? If it needs at least 90MB of memory per request, how does it scale?</li>
<li>Is there any alternative approach?</li>
</ul>

<p>Thank you in advance!</p>
","7707085","","7707085","","2020-01-03 20:03:26","2020-01-22 06:09:35","LDA gensim model in a flask HTTP API - Memory issues","<http><flask><out-of-memory><gensim><lda>","1","1","","","","CC BY-SA 4.0"
"31062273","1","","","2015-06-25 23:06:57","","10","790","<p>I'm trying to train a word2vec model using a file with about 170K lines, with one sentence per line.</p>

<p>I think I may represent a special use case because the ""sentences"" have arbitrary strings rather than dictionary words. Each sentence (line) has about 100 words and each ""word"" has about 20 characters, with characters like <code>""/""</code> and also numbers.</p>

<p>The training code is very simple:</p>

<pre><code># as shown in http://rare-technologies.com/word2vec-tutorial/
import gensim, logging, os

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

current_dir = os.path.dirname(os.path.realpath(__file__))

# each line represents a full chess match
input_dir = current_dir+""/../fen_output""
output_file = current_dir+""/../learned_vectors/output.model.bin""

sentences = MySentences(input_dir)

model = gensim.models.Word2Vec(sentences,workers=8)
</code></pre>

<p>Thing is, things work real quick up to 100K sentences (my RAM steadily going up) but then I run out of RAM and I can see my PC has started swapping, and training grinds to a halt. I don't have a lot of RAM available, only about 4GB and <code>word2vec</code> uses up all of it before starting to swap.</p>

<p>I think I have OpenBLAS correctly linked to numpy: this is what <code>numpy.show_config()</code> tells me:</p>

<pre><code>blas_info:
  libraries = ['blas']
  library_dirs = ['/usr/lib']
  language = f77
lapack_info:
  libraries = ['lapack']
  library_dirs = ['/usr/lib']
  language = f77
atlas_threads_info:
  NOT AVAILABLE
blas_opt_info:
  libraries = ['openblas']
  library_dirs = ['/usr/lib']
  language = f77
openblas_info:
  libraries = ['openblas']
  library_dirs = ['/usr/lib']
  language = f77
lapack_opt_info:
  libraries = ['lapack', 'blas']
  library_dirs = ['/usr/lib']
  language = f77
  define_macros = [('NO_ATLAS_INFO', 1)]
openblas_lapack_info:
  NOT AVAILABLE
lapack_mkl_info:
  NOT AVAILABLE
atlas_3_10_threads_info:
  NOT AVAILABLE
atlas_info:
  NOT AVAILABLE
atlas_3_10_info:
  NOT AVAILABLE
blas_mkl_info:
  NOT AVAILABLE
mkl_info:
  NOT AVAILABLE
</code></pre>

<p>My question is: <strong>is this expected on a machine that hasn't got a lot of available RAM (like mine) and I should get more RAM or train the model in smaller pieces?</strong> or <strong>does it look like my setup isn't configured properly (or my code is inefficient)?</strong></p>

<p>Thank you in advance.</p>
","436721","","436721","","2015-06-25 23:17:58","2015-09-30 21:00:52","Word2vec training using gensim starts swapping after 100K sentences","<python><numpy><blas><gensim><word2vec>","2","0","3","","","CC BY-SA 3.0"
"54476634","1","","","2019-02-01 09:34:42","","-1","653","<p>I am trying to feed sentence lists sequentially into gensim.models.Word2Vec, But it generates the TypeError: '_Token' object is not iterable. What should I do?</p>

<pre><code>    embedding_model= Word2Vec()
    for index, sentence_list in df.iterrows():
        embedding_model = Word2Vec(sentence_list, size=100, window=5, min_count=2, workers=2)
        embedding_model.train(tokenized_contents, total_examples=len(tsentence_list), epochs=10)
</code></pre>
","10856178","","","","","2019-02-01 10:57:26","word2vec error: '_Token' object is not iterable","<python><gensim><word2vec>","1","4","","","","CC BY-SA 4.0"
"67358081","1","","","2021-05-02 15:26:29","","0","54","<p>I saw the <code>gensim</code> documentation but I am stucked with the following problem:</p>
<p>-Once that I get a <code>mXj</code> similarity matrix where <code>m</code> is the number of documents and <code>j</code> is the total number of unique words I don't know how to extract the most similar <code>N</code> documents</p>
<p>-The long goal is to store and save in a <code>xlsx</code> or <code>csv</code> format, but this is another problem</p>
<p>Here <a href=""https://radimrehurek.com/gensim/similarities/docsim.html"" rel=""nofollow noreferrer"">the example from the documentation</a> using <code>Similarity</code></p>
<pre><code>from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile
index_tmpfile = get_tmpfile(&quot;index&quot;)

query = [(1, 2), (6, 1), (7, 2)]

index = Similarity(index_tmpfile, common_corpus, num_features=len(common_dictionary))  # build the index

similarities = index[query]  # get similarities between the query and all index documents
</code></pre>
<p>After that I get for example the top 10 most similar documents?</p>
<p>Expected Output:</p>
<pre><code>arr=[('This is the most similar',0.99),('This is one of the most similar',0.98),('This is another very similar docs,0.98)]
</code></pre>
<p>or</p>
<pre><code>arr=[['This is the most similar',0.99],['This is one of the most similar',0.98],['This is another very similar docs,0.98]]
</code></pre>
<p>In my personal situation I had the following code using <code>similarities.SparseMatrixSimilarity</code>:</p>
<pre><code>#type here the query
query_vector='house is clean'
tokenized_query_vector = regexp_tokenize(query_vector,r&quot;\w+&quot;)  

#Create an object of corpora.Dictionary() 
dictionary = corpora.Dictionary()
#Passing to dictionary.doc2bow() object
BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokenized_lines]
feature_cnt = len(dictionary.token2id)

#n=raw
#t=zero corrected idf
#c=cosine
tfidf = TfidfModel(BoW_corpus, smartirs='ntc')

query_vector = dictionary.doc2bow(tokenized_query_vector)

index = similarities.SparseMatrixSimilarity(tfidf[BoW_corpus], num_features = feature_cnt)
#this way I get the similarity between the query and the similarity matrix
sim = index[tfidf[query_vector]]
</code></pre>
","6290211","","","","","2021-05-02 15:26:29","Get Top N Most Similar Vector from SparseMatrixSimilarity gensim based on a specific query","<python><gensim><similarity><tf-idf><cosine-similarity>","0","0","","","","CC BY-SA 4.0"
"34249586","1","","","2015-12-13 09:15:04","","4","4822","<p>I'm currently use gensim to reproduce the result of example of Google provide. <a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">here</a></p>

<p>The problem is the accuracy test of gensim doesn't match with Google's result.</p>

<p>For example, the accuracy of capital-common-countries of Google is 82.02%, the best result of gensim of different parameter sets is 64.4%. There is a big gap here.</p>

<p>Here is the code snippet of train word2vec and accuracy by using gensim</p>

<pre><code>sentences = word2vec.Text8Corpus('./text8')
model = word2vec.Word2Vec(sentences, size=200, workers=12, min_count=5, sg=0, window=8, iter=15, sample=1e-4, negative=25)
model.accuracy(""./questions-words.txt"")[enter image description here][2]
</code></pre>

<p>Code snippet of Google's demo without changes any parameters</p>

<pre><code> ./demo-word-accuracy.sh
</code></pre>

<p><a href=""http://i.stack.imgur.com/2dhUK.png"" rel=""nofollow"">Accuracy comparison detail</a></p>

<p>Does anyone could help on this?</p>
","1534983","","4803609","","2015-12-13 10:04:51","2015-12-14 21:47:22","The accuracy test of word2vec in gensim","<gensim><word2vec>","1","0","3","","","CC BY-SA 3.0"
"67922777","1","67923409","","2021-06-10 14:04:46","","0","720","<p>I have my word2vec model.
I can use it in order to see the most similars words.
Now i create a function in order to plot the word as vector.
Here my function :</p>
<pre><code>def tsne_plot(model):

    labels = []
    tokens = []

    for word in model.wv.key_to_index:
        tokens.append(model[word])
        labels.append(word)
    
    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
    new_values = tsne_model.fit_transform(tokens)

    x = []
    y = []
for value in new_values:
    x.append(value[0])
    y.append(value[1])
    
plt.figure(figsize=(16, 16)) 
for i in range(len(x)):
    plt.scatter(x[i],y[i])
    plt.annotate(labels[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')
plt.show()
</code></pre>
<p>When i call the function, I have the following error :</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-47-d0f4ea6902bf&gt; in &lt;module&gt;
----&gt; 1 tsne_plot(model)

&lt;ipython-input-46-b4714ffe935b&gt; in tsne_plot(model)
      5 
      6     for word in model.wv.key_to_index:
----&gt; 7         tokens.append(model[word])
      8         labels.append(word)
      9 

TypeError: 'Word2Vec' object is not subscriptable
</code></pre>
<p>I really don't how to remove this error. I think it's maybe because the newest version of Gensim do not use array [...].
Thanks for advance !</p>
","15885112","","","","","2021-06-10 14:38:15","Ploting function word2vec Error 'Word2Vec' object is not subscriptable","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"67148633","1","67153031","","2021-04-18 12:40:07","","0","35","<p>I'm new to LDA and I want to calculate the topic similarity between words. Can I get the topic distribution of a word? If so, how can I do this in gensim.ldamodel?</p>
","13561540","","","","","2021-04-18 20:20:15","Can I get topics distribution of a word in LDA?","<python><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"46282473","1","","","2017-09-18 14:58:51","","0","2464","<p>I try to run the LDA model n pass LDA object to the get_coherence() it showing the error </p>

<pre><code>x.get_coherence()
</code></pre>

<p>*** TypeError: diags() takes at least 2 arguments (2 given)</p>

<p>My code :-</p>

<pre><code>iModel = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, 
num_topics=i, passes=10)

ldalist.append(iModel)

x = CoherenceModel(model=iModel, texts=tokenizedTexts, dictionary=dictionary, 
coherence=coherence)

cohValue = x.get_coherence()
</code></pre>
","3480223","","8580190","","2018-07-17 14:16:38","2019-08-14 18:09:19","error while Identify the coherence value from LDA model","<python><gensim><lda>","2","0","","","","CC BY-SA 4.0"
"50709355","1","50711175","","2018-06-05 21:41:04","","0","1241","<p>I am using <code>GoogleNews-vectors-negative300.bin</code> model and <a href=""https://github.com/ian-beaver/pycontractions"" rel=""nofollow noreferrer"">pycontractions</a> library to determinate with machine learning the best option to expand contractions when there are ambiguous meanings like <code>I'd</code> with can be <code>I would</code> and <code>I had</code>. The size of this model is very large, around to 3.5Gb.</p>

<p>I think that 3.5Gb is a very large model to use for my purpose. Probably I'll never use all words representations in this model. Is there a way to reduce the size extracting only a subset of words representations that are useful to my purposes?</p>
","1057007","","1057007","","2018-06-06 00:31:30","2018-06-06 01:48:32","Small model from Google news Word2Vec model","<machine-learning><models><word2vec><gensim>","1","0","","","","CC BY-SA 4.0"
"34309428","1","34312006","","2015-12-16 10:23:10","","0","1585","<p>I have created LDA model using gensim. Now, I wanted to visualise it using pyLDAvis library but getting :</p>

<pre><code>ImportError: cannot import name PCoA 
</code></pre>

<p>Can anyone help me with this or suggest some alternatives.</p>

<p>Thanks in advance.</p>
","4011400","","","","","2015-12-17 13:04:08","Cannot run pyLDAvis. Getting Error : ImportError: cannot import name PCoA","<python><scikit-learn><gensim><skbio>","2","0","","","","CC BY-SA 3.0"
"34166369","1","34166439","","2015-12-08 21:28:53","","25","6282","<p>I have an generator (a function that yields stuff), but when trying to pass it to <code>gensim.Word2Vec</code> I get the following error:</p>

<blockquote>
  <p>TypeError: You can't pass a generator as the sentences argument. Try an iterator.</p>
</blockquote>

<p>Isn't a generator a kind of iterator? If not, how do I make an iterator from it?</p>

<p>Looking at the library code, it seems to simply iterate over sentences like <code>for x in enumerate(sentences)</code>, which works just fine with my generator. What is causing the error then?</p>
","1537403","","2642204","","2015-12-08 21:32:25","2019-08-23 20:38:28","Generator is not an iterator?","<python><gensim><word2vec>","4","5","3","","","CC BY-SA 3.0"
"41815880","1","","","2017-01-23 21:22:23","","7","3050","<p>I'm analyzing a corpus of roughly 2M raw words. I build a model using gensim's word2vec, embed the vectors using sklearn TSNE, and cluster the vectors (from word2vec, not TSNE) using sklearn DBSCAN. The TSNE output looks about right: the layout of the words in 2D space seems to reflect their semantic meaning. There's a group of misspellings, clothes, etc.</p>

<p>However, I'm having trouble getting DBSCAN to output meaningful results. It seems to label almost everything in the ""0"" group (colored teal in the images). As I increase epsilon, the ""0"" group takes over everything. Here are screenshots with epsilon=10, and epsilon=12.5. With epsilon=20, almost everything is in the same group.</p>

<p><a href=""https://i.stack.imgur.com/e9QvG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e9QvG.png"" alt=""epsilon 10""></a>
<a href=""https://i.stack.imgur.com/LLkzu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LLkzu.png"" alt=""epsilon 12.5""></a></p>

<p>I would expect, for instance, the group of ""clothing"" words to all get clustered together (they're unclustered @ eps=10). I would also expect more on the order of 100 clusters, as opposed to 5 - 12 clusters, and to be able to control the size and number of the clusters using epsilon.</p>

<p>A few questions, then. Am I understanding the use of DBSCAN correctly? Is there another clustering algorithm that might be a better choice? How can I know what a good clustering algorithm for my data is?</p>

<p>Is it safe to assume my model is tuned pretty well, given that the TSNE looks about right?</p>

<p>What other techniques can I use in order to isolate the issue with clustering? How do I know if it's my word2vec model, my use of DBSCAN, or something else?</p>

<p>Here's the code I'm using to perform DBSCAN:</p>

<pre><code>import sys
import gensim
import json
from optparse import OptionParser

import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# snip option parsing

model = gensim.models.Word2Vec.load(options.file);
words = sorted(model.vocab.keys())
vectors = StandardScaler().fit_transform([model[w] for w in words])

db = DBSCAN(eps=options.epsilon).fit(vectors)
labels = db.labels_
core_indices = db.core_sample_indices_

n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
print(""Estimated {:d} clusters"".format(n_clusters), file=sys.stderr)

output = [{'word': w, 'label': np.asscalar(l), 'isCore': i in core_indices} for i, (l, w) in enumerate(zip(labels, words))]
print(json.dumps(output))
</code></pre>
","511328","","6408518","","2018-06-06 14:20:29","2018-06-08 18:29:36","Troubleshooting tips for clustering word2vec output with DBSCAN","<python><machine-learning><scikit-learn><word2vec><gensim>","2","1","3","","","CC BY-SA 3.0"
"58930298","1","","","2019-11-19 09:10:59","","3","2088","<p>I am building a machine learning model which will process documents and extract some key information from it. For this, I need to use word embedding for OCRed output. I have several different options for the embedding (Google's word2vec, Stanford's, Facebook's FastText) but my main concern is OOV words, as the OCR output will have a lot of misspelled words. For example, I want the embeddings such that the output for <strong>Embedding</strong> and <strong>Embdding</strong> (e missed by the OCR) should have a certain level of similarity. I don't care much about the associated contextual information. </p>

<p>I chose Facebook's FastText as it gives the embeddings for OOV words as well. My only concern is the size of the embeddings. The vector size of the FastText's model is of length 300. Is there a way to reduce the size of the returned word vector(I am thinking of using PCA or any other dimensionality reduction technique, but given the size of word vectors, it can be a time-consuming task)? </p>
","10564600","","130288","","2019-11-20 00:49:26","2020-04-30 18:02:23","Reducing size of Facebook's FastText Word2Vec","<data-science><gensim><word2vec><dimensionality-reduction><fasttext>","1","2","1","","","CC BY-SA 4.0"
"33229360","1","33230682","","2015-10-20 06:20:23","","13","25850","<p>I am starting with some python task, I am facing a problem while using gensim. I am trying to load files from my disk and process them (split them and lowercase() them)</p>

<p>The code I have is below:</p>

<pre><code>dictionary_arr=[]
for file_path in glob.glob(os.path.join(path, '*.txt')):
    with open (file_path, ""r"") as myfile:
        text=myfile.read()
        for words in text.lower().split():
            dictionary_arr.append(words)
dictionary = corpora.Dictionary(dictionary_arr)
</code></pre>

<p>The list (dictionary_arr) contains the list of all words across all the file, I then use gensim corpora.Dictionary to process the list. However I face a error.</p>

<pre><code>TypeError: doc2bow expects an array of unicode tokens on input, not a single string
</code></pre>

<p>I cant understand whats a problem, A little guidance would be appreciated.</p>
","2404193","","","","","2020-02-26 18:17:20","Gensim: TypeError: doc2bow expects an array of unicode tokens on input, not a single string","<python><gensim>","3","0","3","","","CC BY-SA 3.0"
"67944732","1","","","2021-06-12 00:46:19","","0","35","<p>In gensim 4.0 subclasses of gensim.corpora.textcorpus.TextCorpus apply a default preprocessing including remove_stopwords(). This function uses the stopword list stored in  gensim.parsing.preprocessing.STOPWORDS.</p>
<p>How can I replace this list with my own? I can do the following</p>
<pre><code>import gensim 
gensim.parsing.preprocessing.STOPWORDS = frozenset({'aber', 'alle', 'allem', 'allen' }) 
</code></pre>
<p>and it works with gensim.parsing.preprocessing.remove_stopwords(s). So this works as intended:</p>
<pre><code>gensim.parsing.preprocessing.remove_stopwords(&quot;aber alle lachten&quot;)
&gt; 'lachten'
</code></pre>
<p>But the list is not used when I process my files using the class gensim.corpora.textcorpus.TextDirectoryCorpus (which is a subclass of TextCorpus). For example:</p>
<pre><code>import os
os.mkdir('test123')
with open('test123/test.txt', 'w') as fout:
    fout.write('aber alle lachten \n allen gefallen \n')

corpus = gensim.corpora.textcorpus.TextDirectoryCorpus('test123')    

for text in corpus.get_texts():
    print(text)

&gt; ['aber', 'alle', 'lachten', 'allen', 'gefallen']
</code></pre>
<p>I know that I can write my own subclass and overwrite the method to preprocess the files, but this seems like overkill for replacing a stopword list.</p>
","2672805","","","","","2021-06-12 17:57:48","Using my own stopword list with gensim.corpora.textcorpus.TextCorpus","<gensim><stop-words>","1","0","","","","CC BY-SA 4.0"
"59605023","1","","","2020-01-05 23:12:48","","0","141","<p>I would like to load a gensim pretrained model and continue training it.</p>

<p>This example fails...I've tried many variants and gensim API, but what I'm trying to do doesn't seem possible.</p>

<pre><code>import gensim, logging, os
from gensim.models import KeyedVectors
from gensim.models import Word2Vec

import gensim.downloader as api
wv = api.load('word2vec-google-news-300')

# this fails - TypeError: 'int' object is not iterable
model = Word2Vec(wv)

class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

sentences = MySentences('all-tokenized-sentences.txt')

# Training the model with list of sentences (with 4 CPU cores)
model.train(sentences, workers=4)
</code></pre>
","7452559","","","","","2020-01-05 23:12:48","What is the process to convert gensim KeyedVector to model?","<python><model><gensim><word2vec>","0","3","","","","CC BY-SA 4.0"
"66517974","1","66821664","","2021-03-07 15:09:42","","0","176","<p>I want to train my word Embedding from scratch and I use gensim.models.word2vec as my model.
My corpus is so large that I can not read it at once , so I divide my corpus file into <strong>many parts</strong> and train my model iteratively„ÄÇI find this is helpful:</p>
<pre><code>train(corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)
</code></pre>
<h2>I confused about the parameter &quot;total_words&quot; .
Is it means total words of all my corpus or the part corpus trained now?</h2>
<p>UPDATE:</p>
<p>my code is like this:</p>
<pre><code>model =  gensim.models.word2vec.Word2Vec.load(init_model)  
for i in range(parts):
    model.build_vocab(corpus_file=this_part_file_name, update=True)
    model.train(corpus_file = this_part_file_name, 
                   total_words=word_count(this_part_file_name) )

</code></pre>
<p>Should the parameter total_words be <code>word_count(this_part_file_name)</code> or <code>word_count(ALL_my_corpus_file)</code> ?</p>
","14983196","","14983196","","2021-03-29 02:34:22","2021-03-29 02:34:22","what does 'corpus_count' in gensim word2vec?","<nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"59592326","1","","","2020-01-04 15:52:27","","0","1191","<p>I wonder which algorithm is the best for semantic similarity? Can anyone explain why? </p>

<p>Thank you!</p>
","11991961","","","","","2020-01-04 18:55:13","Word Mover's Distance vs Cosine Similarity","<python><nlp><gensim><semantics><cosine-similarity>","1","0","","","","CC BY-SA 4.0"
"67965553","1","","","2021-06-14 06:10:50","","1","26","<p>Am working with similarity matches using spacy. But as per my needs in the project, I am trying to build a custom word similarity. I was going through the internet to get leads, but dint find any.
Is there a way to actually build this ?</p>
<pre><code>My thoughts : {&quot;Account&quot;: [&quot;acct&quot;, &quot;account no&quot;, &quot;acount no&quot;, &quot;actno&quot;, &quot;act no&quot;, &quot;acct no&quot;]}
</code></pre>
<p>For example, if I want the word account which is represented as the list given above, it should match and give me the list of words present in a text.</p>
<pre><code>Output : Text = &quot;This a custom word similarity builder. The acct no for this can be STACKCODE. The only account no will be found in the stackoverflow&quot;
</code></pre>
<p>When I train it with all my inputs, and use word similarity, it should output me the words found.</p>
<pre><code>Output : [&quot;acct no&quot;, &quot;account no&quot;]
</code></pre>
","7166834","","","","","2021-06-14 18:43:05","Building a custom word similarity","<python><nlp><spacy><gensim>","1","2","","","","CC BY-SA 4.0"
"59631259","1","59633734","","2020-01-07 15:20:33","","0","121","<p><a href=""https://i.stack.imgur.com/SZxpG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SZxpG.png"" alt=""enter image description here""></a></p>

<p>Does it mean that I must provide tokenized words of a document as list of strings or simply a document as a list of string for the input doc_words. Please clarify</p>
","10900497","","","","","2020-01-07 17:59:36","Understanding of the parameter model.infer_vector for doc2vec gensim","<python><gensim><doc2vec>","1","2","","2020-01-08 07:55:52","","CC BY-SA 4.0"
"59634935","1","","","2020-01-07 19:31:03","","1","448","<p>I construct sentences using 3 words ""1"", ""2"", ""3"", in different ways, and observe that the word vectors are unchanged for each of these words.</p>

<p>Following are the different sentences</p>

<p>Type 1: [[""1"", ""2""], [""1"", ""3""]]</p>

<p>Type 2: [[""1"", ""2"", ""3""]]</p>

<p>Type 3: [[""1"", ""2""], [""3""]]</p>

<p>I am training <code>Word2Vec</code> model as follows</p>

<pre><code>model = Word2Vec(sentences,min_count=1,size=2)
print (model.wv.most_similar(""1""))
print (model.wv.most_similar(""2""))
print (model.wv.most_similar(""3""))
print (model.wv['1'])
print (model.wv['2'])
print (model.wv['3'])
</code></pre>

<p>And results are same on changing the sentence type</p>

<pre><code>[('3', 0.5377859473228455), ('2', -0.5831003785133362)]
[('1', -0.5831003189086914), ('3', -0.9985027313232422)]
[('1', 0.5377858281135559), ('2', -0.9985026717185974)]
[-0.24893647 -0.24495095]
[ 0.19231372 -0.03319569]
[-0.22207274  0.05098101]
</code></pre>

<p>Also when I change word ""1"" to suppose ""101"", the result changes</p>

<pre><code>[('3', 0.5407046675682068), ('2', -0.5859125256538391)]
[('101', -0.5859125256538391), ('3', -0.9985027313232422)]
[('101', 0.540704607963562), ('2', -0.9985026717185974)]
[-0.05898098 -0.0576357 ]
[ 0.19231372 -0.03319569]
[-0.22207274  0.05098101]
</code></pre>

<p>I wanted to know </p>

<ol>
<li><p>Why the results didn't change when I changed the sentences?</p></li>
<li><p>Why results changed when I just updated the value?</p></li>
</ol>
","10907314","","","","","2020-01-08 01:04:28","Understanding gensim Word2Vec most_similar results for 3 words","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"66521567","1","","","2021-03-07 21:23:04","","0","91","<p>I am trying to perform some NLP (more precisely a TF-IDF project) on a part of my bachelor thesis.</p>
<p>I exported a small part of it in a single document called 'thesis.txt' and it seems that I'm encountering an issue when fitting the cleaned textual data to gensim Dictionary.</p>
<p>All the words are tokenized, stored in a bag of words and I can't figure out what I am doing wrong.</p>
<p>Here's the error I got :</p>
<pre><code>    ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-317-73828cccaebe&gt; in &lt;module&gt;
     17 
     18 #Create dictionary
---&gt; 19 dictionary = Dictionary(tokens_no_stop)
     20 
     21 #Create bag of words

~/Library/Python/3.8/lib/python/site-packages/gensim/corpora/dictionary.py in __init__(self, documents, prune_at)
     89 
     90         if documents is not None:
---&gt; 91             self.add_documents(documents, prune_at=prune_at)
     92 
     93     def __getitem__(self, tokenid):

~/Library/Python/3.8/lib/python/site-packages/gensim/corpora/dictionary.py in add_documents(self, documents, prune_at)
    210 
    211             # update Dictionary with the document
--&gt; 212             self.doc2bow(document, allow_update=True)  # ignore the result, here we only care about updating token ids
    213 
    214         logger.info(

~/Library/Python/3.8/lib/python/site-packages/gensim/corpora/dictionary.py in doc2bow(self, document, allow_update, return_missing)
    250         &quot;&quot;&quot;
    251         if isinstance(document, string_types):
--&gt; 252             raise TypeError(&quot;doc2bow expects an array of unicode tokens on input, not a single string&quot;)
    253 
    254         # Construct (word, frequency) mapping.

TypeError: doc2bow expects an array of unicode tokens on input, not a single string
</code></pre>
<p>Thanks in advance for your help :) (Find below my code)</p>
<pre><code>from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter
from gensim.corpora import Dictionary
from gensim.models.tfidfmodel import TfidfModel

f = open('/Users/romeoleon/Desktop/Python &amp; R/NLP/TRIAL_THESIS/thesis.txt','r')
text = f.read()

#Tokenize text
Tokens = word_tokenize(text)

#Lower case everything
Tokens = [t.lower() for t in Tokens]

#Keep only leters
tokens_alpha = [t for t in Tokens if t.isalpha()]

#Remove stopwords
tokens_no_stop = [t for t in tokens_alpha if t not in stopwords.words('french')]

#Create Lemmatizer
lem = WordNetLemmatizer()
lemmatized = [lem.lemmatize(t) for t in tokens_no_stop]


#Create dictionary
dictionary = Dictionary(tokens_no_stop)

#Create bag of words
bow = [dictionary.doc2bow(line) for line in tokens_no_stop]

#Model TFID
tfidf = TfidfModel(bow)
bow_tfidf = tfidf[bow]
</code></pre>
","13136861","","","","","2021-03-08 14:39:00","Gensim - TF-IDF, how to perform a proper genesis TF-IDF?","<python><nlp><nltk><gensim>","1","0","","","","CC BY-SA 4.0"
"66537683","1","","","2021-03-08 21:46:14","","0","76","<p>I want to import the gensim library.</p>
<pre><code>import gensim 
</code></pre>
<p>However, the following error appears:</p>
<pre><code>Traceback (most recent call last):
  File &quot;untitled1/dfgd.py&quot;, line 1, in &lt;module&gt;
    import gensim
  File &quot;C:\Users\DSP\Virtual_env\lib\site-packages\gensim\__init__.py&quot;, line 5, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
  File &quot;C:\Users\DSP\Virtual_env\lib\site-packages\gensim\parsing\__init__.py&quot;, line 4, in &lt;module&gt;
    from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
  File &quot;C:\Users\DSP\Virtual_env\lib\site-packages\gensim\parsing\preprocessing.py&quot;, line 42, in &lt;module&gt;
    from gensim import utils
  File &quot;C:\Users\DSP\Virtual_env\lib\site-packages\gensim\utils.py&quot;, line 40, in &lt;module&gt;
    import scipy.sparse
  File &quot;C:\Users\DSP\Virtual_env\scipy\__init__.py&quot;, line 141, in &lt;module&gt;
    from scipy._lib._ccallback import LowLevelCallable
  File &quot;C:\Users\DSP\Virtual_env\scipy\_lib\_ccallback.py&quot;, line 1, in &lt;module&gt;
    from . import _ccallback_c
ImportError: cannot import name '_ccallback_c' from 'scipy._lib' (C:\Users\DSP\Virtual_env\scipy\_lib\__init__.py)
</code></pre>
<p>The same thing happens when importing sklearn. What does this error mean? how can I solve it? I have already uninstalled and installed scipy, but it wasn't helpful.</p>
","14251114","","","","","2021-03-08 21:46:14","ImportError: cannot import name '_ccallback_c' from 'scipy._lib' when importing gensim and sklearn","<python><import><scikit-learn><importerror><gensim>","0","1","","","","CC BY-SA 4.0"
"68025964","1","68034926","","2021-06-17 20:09:41","","1","152","<p>I have been tasked with putting a document vector model into production.
I am an R user, and so my original model is in R. One of the avenues we have is to recreate the code and the models in Python.</p>
<p><strong>I am confused by the Gensim implementation of Doc2vec</strong>.</p>
<p>The process that works in R goes like this:</p>
<p><strong>Offline</strong></p>
<hr />
<ul>
<li><p>Word vectors are trained using the functions in the <code>text2vec</code> package, namely GloVe or GlobalVectors, on a large corpus This gives me a large Word Vector text file.</p>
</li>
<li><p>Before the ML step takes place, the <code>Doc2Vec</code> function from the <code>TextTinyR</code> library is used to turn each piece of text from a smaller, more specific training corpus into a vector. <em>This is not a machine learning step. No model is trained</em>. The Doc2Vec function effectively aggregates the word vectors in the sentence, in the same sense that finding the sum or mean of vectors does, but in a way that preserves information about word order.</p>
</li>
<li><p>Various models are then trained on these smaller text corpuses.</p>
</li>
</ul>
<hr />
<p><strong>Online</strong></p>
<hr />
<ul>
<li>The new text is converted to Document Vectors using the pretrained word vectors.</li>
<li>The Document Vectors are fed into the pretrained model to obtain the output classification.</li>
</ul>
<hr />
<p><strong>The example code I have found for Gensim appears to be a radical departure from this.</strong></p>
<p>It appears in <code>gensim</code> that Doc vectors are a separate class of model from word vectors that you can train. It seems in some cases, the word vectors and doc vectors are all trained at once. Here are some examples from tutorials and stackoverflow answers:</p>
<p><a href=""https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"" rel=""nofollow noreferrer"">https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5</a></p>
<p><a href=""https://stackoverflow.com/questions/27470670/how-to-use-gensim-doc2vec-with-pre-trained-word-vectors"">How to use Gensim doc2vec with pre-trained word vectors?</a></p>
<p><a href=""https://stackoverflow.com/questions/36815038/how-to-load-pre-trained-model-with-in-gensim-and-train-doc2vec-with-it?rq=1"">How to load pre-trained model with in gensim and train doc2vec with it?</a></p>
<p><a href=""https://stackoverflow.com/questions/45037860/gensim1-0-1-doc2vec-with-google-pretrained-vectors?noredirect=1&amp;lq=1"">gensim(1.0.1) Doc2Vec with google pretrained vectors</a></p>
<p>So my questions are these:</p>
<p><strong>Is the gensim implementation of Doc2Vec fundamentally different from the TextTinyR implementation?</strong></p>
<p><strong>Or is the gensim doc2vec model basically just encapsulating the word2vec model and the doc2vec process into a single object?</strong></p>
<p><strong>Is there anything else I'm missing about the process?</strong></p>
","8968617","","","","","2021-06-19 17:28:20","Gensim: Is doc2vec a model or operation? Differences from R implementation","<python><r><gensim><word2vec><doc2vec>","3","0","1","","","CC BY-SA 4.0"
"59573454","1","","","2020-01-03 05:07:44","","0","4359","<p>I am trying to find a simple way to calculate soft cosine similarity between two sentences.</p>

<p>Here is my attempt and learning:</p>

<pre><code>from gensim.matutils import softcossim

sent_1 = 'Dravid is a cricket player and a opening batsman'.split()
sent_2 = 'Leo is a cricket player too He is a batsman,baller and keeper'.split()

print(softcossim(sent_1, sent_2, similarity_matrix))
</code></pre>

<p>I'm unable to understand about <code>similarity_matrix</code>. Please help me find so, and henceforth the soft cosine similarity in python.</p>
","4763959","","4960855","","2020-02-20 15:09:37","2021-06-01 10:06:05","Soft Cosine Similarity between two sentences","<python><gensim><cosine-similarity>","3","0","","","","CC BY-SA 4.0"
"68003709","1","68031931","","2021-06-16 13:31:21","","0","38","<p>I have a tuned and pretrainend LDA Model that I want to pass on to the ldaseq model in gensim, but don't understand how to do it. I've tried lda_model and sstats but it doesn'T seem to work, I still get this from the logging:</p>
<blockquote>
<p>running online (multi-pass) LDA training, 10 topics, 10 passes over
the supplied corpus of 1699 documents, updating model once every 1699
documents, evaluating perplexity every 1699 documents, iterating 50x
with a convergence threshold of 0.001000</p>
</blockquote>
","15265671","","","","","2021-06-18 08:34:07","How can you pass a pretrainend LDA Model to ldaseq in Gensim for DTM?","<gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"50447523","1","","","2018-05-21 11:06:45","","0","278","<p>I am a C# programmer and new to Python. Now I am working on Python form Visual Studio 2017.</p>
<p>I have already written a program in C# to do some text processing tasks, but now I need to accomplish my work in python as it provides advanced functions for dealing with Natural Language Processing.</p>
<p>Specifically, I need to pass a <code>List&lt;List&gt;</code> parameter from my C# application to the Python application.</p>
<p>The passed parameter will be used instead of <code>doc_term_matrix</code> inline 48 at the following python snippet:</p>
<p><a href=""https://i.stack.imgur.com/TsWk4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TsWk4.png"" alt=""enter image description here"" /></a></p>
<p>Another issue to be handled is the casting of the different data types, as I want to import a <code>List&lt;List&lt;double&gt;&gt;</code>, as shown below, from C# to be used as the parameter returned from the <code>dictionary.doc2bow(doc)</code>  function in Python.</p>
<p><a href=""https://i.stack.imgur.com/Fmq9E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fmq9E.png"" alt=""enter image description here"" /></a></p>
<p>I have no idea how to do so. So, please help me solve this issue.</p>
","1658160","","10534633","","2020-10-02 05:07:21","2020-10-02 05:07:21","Passing parameters from C# application to Python application [for using in gensim] in Visual Studio 2017","<c#><python><visual-studio-2017><parameter-passing><gensim>","1","0","","","","CC BY-SA 4.0"
"68039391","1","","","2021-06-18 17:37:58","","0","599","<p>I want to download a number of random Wikipedia articles (just the content) in order to introduce some randomness into the corpus and ultimately the topics. Then, I want to add the articles ‚ÄòCar‚Äô and ‚ÄòBus‚Äô to the corpus.</p>
<pre><code>import wikipedia

wikipedia_random_articles = wikipedia.random(5)
wikipedia_random_articles.append('Car')
wikipedia_random_articles.append('Bus')
</code></pre>
<p>This will return a list of seven article titles. These can then be downloaded:</p>
<p>wikipedia_articles = []
for wikipedia_article in wikipedia_random_articles:
wikipedia_articles.append([wikipedia_article, wikipedia.page(wikipedia_article).content])</p>
<p>Now, we have to clean, tokenize, and stem the articles. With the help of the NLTK:</p>
<pre><code>from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer

def clean(article):
    title, document = article
    tokens = RegexpTokenizer(r'\w+').tokenize(document.lower())
    tokens_clean = [token for token in tokens if token not in stopwords.words('english')]
    tokens_stemmed = [PorterStemmer().stem(token) for token in tokens_clean]
    return (title, tokens_stemmed)

wikipedia_articles_clean = list(map(clean, wikipedia_articles))
</code></pre>
<p>For the LDA model, we need a document-term matrix (a gensim dictionary) and all articles in vectorized format (we will be using a bag-of-words approach).</p>
<pre><code>from gensim import corpora, models
import gensim

article_contents = [article[1] for article in wikipedia_articles_clean]
dictionary = corpora.Dictionary(article_contents)
</code></pre>
<p>In order o constructing a vector representation of an article, I used following code:</p>
<p>bag_of_words = [dictionary.doc2bow(article_content)]</p>
<p>But I face with following error in Google Colab:</p>
<pre><code>&gt; TypeError                                 Traceback (most recent call
&gt; last) &lt;ipython-input-28-aca107b9a54d&gt; in &lt;module&gt;()
&gt; ----&gt; 1 bag_of_words = [dictionary.doc2bow(article_contents)]
&gt; 
&gt; /usr/local/lib/python3.7/dist-packages/gensim/corpora/dictionary.py in
&gt; doc2bow(self, document, allow_update, return_missing)
&gt;     243         counter = defaultdict(int)
&gt;     244         for w in document:
&gt; --&gt; 245             counter[w if isinstance(w, unicode) else unicode(w, 'utf-8')] += 1
&gt;     246 
&gt;     247         token2id = self.token2id
&gt; 
&gt; TypeError: decoding to str: need a bytes-like object, list found
</code></pre>
<p>Would you please help me to resolve this issue?</p>
","5655385","","610569","","2021-06-20 23:09:06","2021-06-20 23:09:06","When creating a gensim vocabulary why did I get ""decoding to str: need a bytes-like object, list found"" error?","<python><nltk><gensim><wikipedia><lda>","1","1","","","","CC BY-SA 4.0"
"50723303","1","50730108","","2018-06-06 14:46:50","","3","5609","<p>Say that I'm training a (Gensim) Word2Vec model with min_count=5. The documentation learns us what min_count does:</p>

<blockquote>
  <p>Ignores all words with total frequency lower than this.</p>
</blockquote>

<p>What is the effect of min_count on the context? Lets say that I have a sentence of frequent words (min_count > 5) and infrequent words (min_count &lt; 5), annotated with f and i:</p>

<blockquote>
  <p>This (f) is (f) a (f) test (i) sentence (i) which (f) is (f) shown (i) here (i)</p>
</blockquote>

<p>I just made up which word is frequently used and which word is not for demonstration purposes.</p>

<p>If I remove all infrequent words, we get a completely different context from which word2vec is trained. In this example, your sentence would be ""This is a which is"", which would then be a training sentence for Word2Vec. Moreover, if you have a lot of infrequent words, words that were originally very far away from each other are now placed within the same context.</p>

<p>Is this the correct interpretation of Word2Vec? Are we just assuming that you shouldn't have too many infrequent words in your dataset (or set a lower min_count threshold)?</p>
","3899735","","","","","2018-06-06 21:48:04","How is Word2Vec min_count applied","<python><word2vec><gensim>","1","0","1","","","CC BY-SA 4.0"
"67958280","1","","","2021-06-13 12:31:15","","0","179","<pre class=""lang-py prettyprint-override""><code>import gensim
</code></pre>
<pre><code>Traceback (most recent call last):
  File &quot;C:/Users/.../Old Files/Practice.py&quot;, line 2, in &lt;module&gt;
    import gensim
  File &quot;C:\ProgramData\Anaconda3\envs...site-packages\gensim\__init__.py&quot;, line 11, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:F401
  File &quot;C:\ProgramData\Anaconda3\envs...site-packages\gensim\parsing\__init__.py&quot;, line 4, in &lt;module&gt;
    from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
  File &quot;C:\ProgramData\Anaconda3\envs...site-packages\gensim\parsing\preprocessing.py&quot;, line 26, in &lt;module&gt;
    from gensim import utils
  File &quot;C:\ProgramData\Anaconda3\envs...site-packages\gensim\utils.py&quot;, line 62, in &lt;module&gt;
    default_prng = np.random.default_rng()
AttributeError: module 'numpy.random' has no attribute 'default_rng'
</code></pre>
","16214503","","10794031","","2021-06-13 12:33:07","2021-06-13 17:56:22","Getting following error while importing gensim package - ""AttributeError: module 'numpy.random' has no attribute 'default_rng' ""","<python><error-handling><pycharm><package><gensim>","1","1","","2021-06-14 12:56:42","","CC BY-SA 4.0"
"50723841","1","50730038","","2018-06-06 15:11:38","","2","768","<p>I would need to find something like the opposite of <code>model.most_similar()</code><br>
While <code>most_similar()</code> returns an array of words most similar to the one given as input, I need to find a sort of ""center"" of a list of words.</p>

<p>Is there a function in gensim or any other tool that could help me?</p>

<p>Example:<br>
Given <code>{'chimichanga', 'taco', 'burrito'}</code> the center would be maybe <code>mexico</code> or <code>food</code>, depending on the corpus that the model was trained on</p>
","6408518","","1060350","","2018-06-11 06:05:15","2018-06-11 06:05:15","Find the closest word to set of words","<python><nlp><word2vec><gensim>","1","0","","","","CC BY-SA 4.0"
"58927877","1","","","2019-11-19 06:27:33","","0","33","<p>I trained word embedding word2vec model using gensim and then used most_similar method to find the most associated words.</p>

<pre><code>Word to search:  forest 
</code></pre>

<p>The result is below:</p>

<pre><code>Most similar words:  [('wood', 0.2495424747467041), ('trees', 0.24147865176200867), ('distant', 0.2403097301721573), ('island', 0.2402323037)]
</code></pre>

<p>I wonder why the coefficient is very low, even the top word is less than 0.25.</p>

<p>Thank you!</p>
","11970084","","","","","2019-11-19 06:27:33","Gensim most_similar method coefficients are very low","<nlp><gensim><word2vec><word-embedding>","0","3","","","","CC BY-SA 4.0"
"67380180","1","","","2021-05-04 07:04:17","","1","22","<p>I have transformed my words from the pandas dataframe using <code>gensim</code> <code>word2vec</code> model and embeddings are done.</p>
<p>Now I want to feed these vectors into the neural network. how to do that?</p>
","15829986","","15829986","","2021-05-05 12:06:58","2021-05-05 12:06:58","How do i feed word vectors into my neural network after using word2vec","<python><nlp><gensim><word2vec>","0","4","","2021-05-04 14:45:23","","CC BY-SA 4.0"
"34396300","1","34452276","","2015-12-21 12:59:25","","2","949","<p>I've installed gensim on my MacBookPro (Yosemite 10.10.5 ) and I'm using anconda. The installation with <code>pip install --upgrade gensim</code> was working without error message. 
When I tried to run the code of the tutorials, there appears an error when calling serialization:
<code>corpora.MmCorpus.serialize('/temp/deerwester.mm', corpus)</code></p>

<p>Complete error message:</p>

<p><code>File ""/Users/sage/Desktop/gensim/test_gensim.py"", line 39, in &lt;module&gt;
    corpora.MmCorpus.serialize('/temp/deerwester.mm', corpus)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/corpora/indexedcorpus.py"", line 94, in serialize
    offsets = serializer.save_corpus(fname, corpus, id2word, metadata=metadata)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/corpora/mmcorpus.py"", line 49, in save_corpus
    return matutils.MmWriter.write_corpus(fname, corpus, num_terms=num_terms, index=True, progress_cnt=progress_cnt, metadata=metadata)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/matutils.py"", line 486, in write_corpus
    mw = MmWriter(fname)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/matutils.py"", line 436, in __init__
    self.fout = utils.smart_open(self.fname, 'wb+') # open for both reading and writing
  File ""/System/Library/anaconda/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 111, in smart_open
    raise NotImplementedError('unknown file mode %s' % mode)
NotImplementedError: unknown file mode wb+</code></p>

<p>When I downloaded the tar files and performed <code>python setup.py test</code>,
the error <code>NotImplementedError: unknown file mode wb+</code> occurred too. </p>

<p>How can I fix this?</p>
","4868210","","","","","2016-01-03 21:48:17","gensim installation on yosemite using anaconda","<python><python-2.7><anaconda><gensim>","1","2","1","","","CC BY-SA 3.0"
"41829323","1","41829441","","2017-01-24 13:21:52","","4","84387","<p>I have a list of 10k words in a text file like so:</p>

<p>G15
KDN
C30A
Action Standard
Air Brush
Air Dilution</p>

<p>I am trying to convert them into lower cased tokens using this code for subsequent processing with GenSim:</p>

<pre><code>data = [line.strip() for line in open(""C:\corpus\TermList.txt"", 'r')]
texts = [[word for word in data.lower().split()] for word in data]
</code></pre>

<p>and I get the following callback:</p>

<pre><code>AttributeErrorTraceback (most recent call last)
&lt;ipython-input-84-33bbe380449e&gt; in &lt;module&gt;()
      1 data = [line.strip() for line in open(""C:\corpus\TermList.txt"", 'r')]
----&gt; 2 texts = [[word for word in data.lower().split()] for word in data]
      3 
AttributeError: 'list' object has no attribute 'lower'
</code></pre>

<p>Any suggestions on what I am doing wrong and how to correct it would be greatly appreciated!!! Thank you!!</p>
","1801125","","","","","2018-09-26 09:29:55","AttributeError: 'list' object has no attribute 'lower' gensim","<python><string><split><gensim>","4","0","1","","","CC BY-SA 3.0"
"68018745","1","68023653","","2021-06-17 11:49:39","","2","872","<p>I have included the 2 import statements in my views.py</p>
<pre><code>from gensim.summarization.summarizer import summarizer
from gensim.summarization import keywords
</code></pre>
<p>However, even after I installed gensim using pip, I am getting the error:</p>
<pre><code>ModuleNotFoundError: No module named 'gensim.summarization'
</code></pre>
","15742403","","130288","","2021-06-17 17:16:11","2021-06-17 17:16:11","Not able to import from `gensim.summarization` module in Django","<python><django><nlp><gensim>","2","4","","","","CC BY-SA 4.0"
"66554689","1","66555436","","2021-03-09 20:58:10","","0","37","<p>I downloaded a word embedding already train in &quot;glove.txt&quot; format
I imported it in as a model of type gensim.models.keyedvectors.Word2VecKeyedVectors thanks to this documentation :</p>
<p><a href=""https://radimrehurek.com/gensim/scripts/glove2word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/scripts/glove2word2vec.html</a></p>
<p>But I would like a model of type gensim.models.word2vec.Word2Vec</p>
<p>Will there be a way to convert it or import it directly into the desired format?</p>
","12421776","","12421776","","2021-03-09 21:23:59","2021-03-09 22:03:09","pass from a model of type gensim.models.keyedvectors.Word2VecKeyedVectors to a model of type gensim.models.word2vec.Word2Vec","<python><deep-learning><nlp><gensim><word-embedding>","1","0","","","","CC BY-SA 4.0"
"34207097","1","","","2015-12-10 16:35:27","","1","989","<p>I am trying to learn topic modelling using Gensim python library.
I have tried so many different tutorials including official one.</p>

<p>Question:
How do I get document wise topic distribution using Gensim.</p>

<p>My current output is list of topics with its keywords and probability as shown below.</p>

<pre><code>(0, u'0.086*good + 0.086*brocolli + 0.086*health + 0.061*eat)
(1, u'0.068*mother + 0.068*brother + 0.068*drive + 0.041*pressur)
</code></pre>

<p>I would like to know if it is possible to have a list of say each document and top topics for that particular document?</p>

<p>My code is as following:</p>

<pre><code>tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = get_stop_words('en')

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

# create sample documents
doc_a = ""Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.""
doc_b = ""My mother spends a lot of time driving my brother around to baseball practice.""
doc_c = ""Some health experts suggest that driving may cause increased tension and blood pressure.""
doc_d = ""I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.""
doc_e = ""Health professionals say that brocolli is good for your health."" 

# compile sample documents into a list
doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]
num_topics=2;

# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:

    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]
    #print(stopped_tokens)

    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
    #print(""printing stemmed tokens"")
    #print(stemmed_tokens)

    # add tokens to list
    texts.append(stemmed_tokens)

# turn our tokenized documents into a id &lt;-&gt; term dictionary
dictionary = corpora.Dictionary(texts)
print(""printing each token/words along with unique integer id.."")
print(dictionary.token2id)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]
print(""printing sample bag of words"")
print(corpus[0])

# generate LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)
#print(ldamodel.print_topics(num_topics=3, num_words=3))
#print ldamodel.top_topics(corpus,2)
print(ldamodel.show_topics(num_topics=2, num_words=3, log=False, formatted=True))
print(ldamodel.show_topics())
print(""from for loop."")
for i in ldamodel.show_topics(len(dictionary)):
    print i
</code></pre>
","4695617","","","","","2017-04-27 14:12:17","Print document wise topics in Gensim","<python><gensim><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"68004371","1","","","2021-06-16 14:10:22","","0","21","<p>I'm working on word2vec model in order to analysis a corpus of newspaper.
I have a csv which contains some newspaper like tital, journal, and the content of the article.
I know how to train my model in order to get most similar words and their context.</p>
<p>However, I want to do a sentiment analysis on that. I found some ressources in order to do that but in all the test or train dataframe in the examples, there is already a column sentiment (0 or 1). Do you if it's possible to classify automaticaly texts by sentiment ? I mean put 0 or 1 to each text. I search but i don't find any references about that in the word2vec or doc2vec documentation...</p>
<p>Thanks for advance !</p>
","15885112","","","","","2021-06-16 16:00:10","How to get sentiment tag using word2vec","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"57834425","1","","","2019-09-07 14:05:51","","0","86","<p>I am trying to apply the doc2vec <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">tutorial</a> and instead of testing on a random test corpus document, testing on the entire test corpus</p>

<p>I just modified the following line:</p>

<p>code:</p>

<pre><code># Pick a random document from the test corpus and infer a vector from the model

#doc_id = random.randint(0, len(test_corpus) - 1)
doc_id = [index for index, text in enumerate(test_corpus)]

inferred_vector = model.infer_vector(test_corpus[doc_id])
sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))

# Compare and print the most/median/least similar documents from the train corpus
print('Test Document ({}): ¬´{}¬ª\n'.format(doc_id, ' '.join(test_corpus[doc_id])))
print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n' % model)
for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:
    print(u'%s %s: ¬´%s¬ª\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))
</code></pre>

<p>Error:</p>

<pre><code>TypeError: list indices must be integers or slices, not list
</code></pre>
","12006720","","","","","2019-09-07 14:38:33","Testing the Model doc2vec in all test corpus","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"67979656","1","","","2021-06-15 03:34:03","","0","419","<p>I want to import gensim, but it always have this problem</p>
<pre><code>ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject
</code></pre>
<p>And i tried some ways, but it doesn't work.</p>
<pre><code>pip uninstall numpy
pip install numpy
</code></pre>
<p>I have</p>
<pre><code>gensim==4.0.1
numpy==1.20.3
python3.9.1
</code></pre>
<p>Need help.</p>
","16222578","","","","","2021-06-15 03:34:03","RROR: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject","<python><pip><package><gensim>","0","1","","","","CC BY-SA 4.0"
"68023855","1","68024296","","2021-06-17 17:16:19","","0","38","<p>Gensim offers evaluate_word_pairs function for <a href=""https://radimrehurek.com/gensim_4.0.0/auto_examples/tutorials/run_word2vec.html#evaluating"" rel=""nofollow noreferrer"">evaluating semantic similarity</a>.</p>
<p>Here is an example from its page:</p>
<pre><code>model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))

Out:
((0.1014236962315867, 0.44065378924434523), SpearmanrResult(correlation=0.07441989763914543, pvalue=0.5719973648460552), 83.0028328611898)
</code></pre>
<p>I would like to know what metrics are used to generate each value(0.1014236962315867, 0.44065378924434523,...) in the output?</p>
","6221871","","6221871","","2021-06-17 17:45:22","2021-06-17 17:51:03","What Metrics Are Used in the Output of Gensim's evaluate_word_pairs?","<gensim><word2vec><similarity>","1","0","","","","CC BY-SA 4.0"
"34427678","1","34447464","","2015-12-23 02:24:53","","5","7785","<p>I have to use a word2vec module containing tons of Chinese characters. The module was trained by my coworkers using Java and is saved as a bin file. </p>

<p>I installed <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">gensim</a> and tries to load the module, but following error occurred: </p>

<pre><code>In [1]: import gensim  

In [2]: model = gensim.models.Word2Vec.load_word2vec_format('/data5/momo-projects/user_interest_classification/code/word2vec/vectors_groups_1105.bin', binary=True)

UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 96-97: unexpected end of data
</code></pre>

<p>I tried to load the module both in python 2.7 and 3.5, failed in the same way. So how can I load the module in gensim? Thanks.</p>
","975222","","975222","","2015-12-30 10:59:30","2017-04-27 09:55:13","'utf-8' decode error when loading a word2vec module","<python><nlp><gensim><word2vec>","2","5","1","","","CC BY-SA 3.0"
"68060054","1","","","2021-06-20 21:09:10","","0","175","<p>I'm using a i5 8600 (6 cores and no multithreading).</p>
<p>I'm comparing some topic modelling with LDA inside Gensim and I have no idea why I have these variatons <strong>shown below</strong>. I need to understand it so I can select and apply on a big data. <strong>Someone have a clue what's happening here?</strong></p>
<p><strong>1) If I use gensim.models.ldamulticore.LdaMulticore:</strong></p>
<pre><code>coherence = []
for k in range(5,10):
    print('Round: '+str(k))
    Lda = gensim.models.ldamulticore.LdaMulticore
    ldamodel = Lda(corpus, num_topics=k, \
               id2word = id2word, passes=40,\
               iterations=200, chunksize = 10000, eval_every = None, workers=6)
    
    cm = gensim.models.coherencemodel.CoherenceModel(\
         model=ldamodel, texts=texts,\
         dictionary=id2word, coherence='c_v')   
                                                
    coherence.append((k,cm.get_coherence()))
allDone()

executed in 1m 51.1s, finished 17:01:30 2021-06-20
</code></pre>
<p><strong>2) If I use gensim.models.LdaMulticore:</strong></p>
<pre><code>coherence = []
for k in range(5,10):
    print('Round: '+str(k))
    Lda = gensim.models.LdaMulticore
    ldamodel = Lda(corpus, num_topics=k, \
               id2word = id2word, passes=40,\
               iterations=200, chunksize = 10000, eval_every = None, workers=6)
    
    cm = gensim.models.coherencemodel.CoherenceModel(\
         model=ldamodel, texts=texts,\
         dictionary=id2word, coherence='c_v')   
                                                
    coherence.append((k,cm.get_coherence()))
allDone()

executed in 1m 54.8s, finished 17:03:40 2021-06-20
</code></pre>
<p><strong>3) If I use gensim.models.ldamodel.LdaModel:</strong></p>
<pre><code>coherence = []
for k in range(5,10):
    print('Round: '+str(k))
    Lda = gensim.models.ldamodel.LdaModel
    ldamodel = Lda(corpus, num_topics=k, \
               id2word = id2word, passes=40,\
               iterations=200, chunksize = 10000, eval_every = None)
    
    cm = gensim.models.coherencemodel.CoherenceModel(\
         model=ldamodel, texts=texts,\
         dictionary=id2word, coherence='c_v')   
                                                
    coherence.append((k,cm.get_coherence()))
allDone()

executed in 1m 37.1s, finished 17:05:17 2021-06-20
</code></pre>
<p><strong>4) If i use gensim.models.LdaModel:</strong></p>
<pre><code>coherence = []
for k in range(5,10):
    print('Round: '+str(k))
    Lda = gensim.models.LdaModel
    ldamodel = Lda(corpus, num_topics=k, \
               id2word = id2word, passes=40,\
               iterations=200, chunksize = 10000, eval_every = None)
    
    cm = gensim.models.coherencemodel.CoherenceModel(\
         model=ldamodel, texts=texts,\
         dictionary=id2word, coherence='c_v')   
                                                
    coherence.append((k,cm.get_coherence()))
allDone()

executed in 1m 37.3s, finished 17:06:54 2021-06-20
</code></pre>
<p><strong>QUESTIONS</strong>: Someone understand how Gensim functions works and can find the logic in it?</p>
<p>a) If I use Multicore is slower than single core.</p>
<p>b) And if I use  <code>.ldamodel.LdaModel</code> I get also another result than using only <code>.LdaModel</code> (and the same relation is applied for Multicore).</p>
<p>Many thanks to the community!</p>
","15673147","","6573902","","2021-06-23 07:56:18","2021-06-23 08:02:38","Why Python Gensim LDA model is slower when using multicore comparing when using single-core (post shows comparisions)?","<python><performance><gensim><multicore><lda>","1","5","","","","CC BY-SA 4.0"
"66562338","1","","","2021-03-10 09:57:25","","1","659","<p>I have installed both cython and gensim. However, when I import genism, this is what appears:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\DSP\untitled3\lib\site-packages\gensim\matutils.py&quot;, line 1426, in &lt;module&gt;
    from gensim.corpora._mmreader import MmReader  # noqa: F401
ModuleNotFoundError: No module named 'gensim.corpora._mmreader'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;untitled1\dfgd.py&quot;, line 3, in &lt;module&gt;
    from gensim.models.fasttext import FastText
  File &quot;C:\Users\DSP\untitled3\lib\site-packages\gensim\__init__.py&quot;, line 5, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
  File &quot;C:\Users\DSP\untitled3\lib\site-packages\gensim\corpora\__init__.py&quot;, line 6, in &lt;module&gt;
    from .indexedcorpus import IndexedCorpus  # noqa:F401 must appear before the other classes
  File &quot;C:\Users\DSP\untitled3\lib\site-packages\gensim\corpora\indexedcorpus.py&quot;, line 15, in &lt;module&gt;
    from gensim import interfaces, utils
  File &quot;C:\Users\DSP\untitled3\lib\site-packages\gensim\interfaces.py&quot;, line 21, in &lt;module&gt;
    from gensim import utils, matutils
  File &quot;C:\Users\DSP\untitled3\lib\site-packages\gensim\matutils.py&quot;, line 1428, in &lt;module&gt;
    raise utils.NO_CYTHON
RuntimeError: Cython extensions are unavailable. Without them, this gensim functionality is disabled. If you've installed from a package, ask the package maintainer to include Cython extensions. If you're building gensim from source yourself, run `python setup.py build_ext --inplace` and retry. 
</code></pre>
<p>How can I fix this problem?</p>
","14251114","","","","","2021-03-10 09:57:25","RuntimeError: Cython extensions are unavailable","<python><import><runtime-error><cython><gensim>","0","1","","","","CC BY-SA 4.0"
"59712626","1","","","2020-01-13 08:06:41","","0","283","<p>I am trying some unknown word and it give out 0% by putting ""Polytechnic"", ""Diploma"" that dictionary does not even have and i try to find sources that are able to add words into dictionary that i find are not able to find</p>

<p>Here is my function of code that i am calling</p>

<pre><code>def similarityChecker(txt1, txt2):
   result = 0.00
   file_docs = []

   tokens1 = sentence(txt1)
   for line in tokens1:
       file_docs.append(line)

   print(""Number of sentence:"",len(file_docs))

   gen_docs = [[w.lower() for w in removestop(text)]
            for text in file_docs]

   dictionary = gensim.corpora.Dictionary(gen_docs)

   corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]
   # This tf_idf cannot add unknown words?
   tf_idf = gensim.models.TfidfModel(corpus)

   # Gives out an empty array [] for using words not in the english dictionary
   for doc in tf_idf[corpus]:
       print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])

   # building the index
   sims = gensim.similarities.Similarity('/', tf_idf[corpus], num_features=len(dictionary))

   file2_docs = []

   tokens2 = sentence(txt2)
   for line in tokens2:
       file2_docs.append(line)

   print(""Number of sentence:"", len(file2_docs))

   avg_sims = []

   for line in file2_docs:
       # tokenize words
       query_doc = [w.lower() for w in removestop(line)]
       # create bag of words
       query_doc_bow = dictionary.doc2bow(query_doc)
       # find similarity for each document
       query_doc_tf_idf = tf_idf[query_doc_bow]
       # print (document_number, document_similarity)
       print('Comparing Result:', sims[query_doc_tf_idf])
       # calculate sum of similarities for each query doc
       sum_of_sims = (np.sum(sims[query_doc_tf_idf], dtype=np.float32))
       # calculate average of similarity for each query doc
       avg = sum_of_sims / len(file_docs)
       # print average of similarity for each query doc
       print(f'avg: {sum_of_sims / len(file_docs)}')
       # add average values into array
       avg_sims.append(avg)

   total_avg = np.sum(avg_sims, dtype=np.float)

   result = round(float(total_avg) * 100)

   if result &gt;= 100:
       result = 100

   return result
</code></pre>

<p>Some function i added is to call nltk which is working.
And I am new to this gensim coding i really need help.</p>
","12702715","","12702715","","2020-01-15 08:06:46","2020-01-15 08:06:46","Adding Unknown words to Gensim dictionary and teaching the model","<python><nltk><gensim><tf-idf>","0","4","","","","CC BY-SA 4.0"
"23853828","1","34235666","","2014-05-25 09:23:37","","2","2079","<p><a href=""https://stackoverflow.com/questions/21313493/indexerror-while-using-gensim-package-for-lda-topic-modelling"">Another thread</a> has a similar question to mine but leaves out reproducible code. </p>

<p>The goal with the script in question is to create a process that is as memory efficient as possible. So I tried to write a the class <code>corpus()</code> to take advantage of gensims' capabilities. However, I am running into an IndexError that I'm not sure how to resolve when creating <code>lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=checker.dictionary, num_topics=int(options.number_of_topics))</code>. </p>

<p>The documents that I am using are the same as used in the gensim tutorial, which I placed into tutorial_example.txt:</p>

<pre><code>$ cat tutorial_example.txt 
Human machine interface for lab abc computer applications
A survey of user opinion of computer system response time
The EPS user interface management system
System and human system engineering testing of EPS
Relation of user perceived response time to error measurement
The generation of random binary unordered trees
The intersection graph of paths in trees
Graph minors IV Widths of trees and well quasi ordering
Graph minors A survey
</code></pre>

<h2>Error received</h2>

<pre><code>$./gensim_topic_modeling.py -mn2 -w'english' -l1 tutorial_example.txt 
Traceback (most recent call last):
  File ""./gensim_topic_modeling.py"", line 98, in &lt;module&gt;
    lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=checker.dictionary, num_topics=int(options.number_of_topics))
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 306, in __init__
    self.update(corpus)
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 543, in update
    self.log_perplexity(chunk, total_docs=lencorpus)
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 454, in log_perplexity
    perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 630, in bound
    gammad, _ = self.inference([doc])
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 366, in inference
    expElogbetad = self.expElogbeta[:, ids]
IndexError: index 7 is out of bounds for axis 1 with size 7
</code></pre>

<p>Below is the <code>gensim_topic_modeling.py</code> script:</p>

<pre><code>##gensim_topic_modeling.py

#!/usr/bin/env python
# -*- coding: UTF-8 -*-
import sys
import re
import codecs
import logging
import fileinput
from operator import *
from itertools import *
from sklearn.cluster import KMeans
from gensim import corpora, models, similarities, matutils
import argparse
from nltk.corpus import stopwords

reload(sys)
sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
sys.stdin = codecs.getreader('utf-8')(sys.stdin)


##defs

def stop_word_gen():
    nltk_langs=['danish', 'dutch', 'english', 'french', 'german', 'italian','norwegian', 'portuguese', 'russian', 'spanish', 'swedish']
    stoplist = []
    for lang in options.stop_langs.split("",""):
        if lang not in nltk_langs:
            sys.stderr.write('\n'+""Language {0} not supported"".format(lang)+'\n')
            continue
        stoplist.extend(stopwords.words(lang))
    return stoplist


def clean_texts(texts):
    # remove tokens that appear only once
    all_tokens = sum(texts, [])
    tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
    return [[word for word in text if word not in tokens_once] for text in texts]

##class

class corpus(object):
    """"""sparse vector matrix and dictionary""""""
    def __iter__(self):
        first=True
        for line in fileinput.FileInput(options.input, openhook=fileinput.hook_encoded(""utf-8"")):
            # assume there's one document per line; tokenizer option determines how to split
            if options.space_tokenizer:
                rl = re.compile('\s+', re.UNICODE).split(unicode(line,'utf-8'))
            else:
                rl = re.compile('\W+', re.UNICODE).split(tagRE.sub(' ',line)) 
            # create dictionary
            tokens=[token.strip().lower() for token in rl if token != '' and token.strip().lower() not in stoplist]
            if first:
                first=False
                self.dictionary=corpora.Dictionary([tokens])
            else:
                self.dictionary.add_documents([tokens])
                self.dictionary.compactify
            yield self.dictionary.doc2bow(tokens)


##main 

if __name__ == '__main__':
    ##parser
    parser = argparse.ArgumentParser(
                description=""Topic model from a column of text.  Each line is a document in the corpus"")
    parser.add_argument(""input"", metavar=""args"")
    parser.add_argument(""-l"", ""--document-frequency-limit"", dest=""doc_freq_limit"", default=1,
                help=""Remove all tokens less than or equal to limit (default 1)"")
    parser.add_argument(""-m"", ""--create-model"", dest=""create_model"", default=False, action=""store_true"",
                help=""Create and save a model from existing dictionary and input corpus."")
    parser.add_argument(""-n"", ""--number-of-topics"", dest=""number_of_topics"", default=2,
                help=""Number of topics (default 2)"")
    parser.add_argument(""-t"", ""--space-tokenizer"", dest=""space_tokenizer"", default=False, action=""store_true"", 
                help=""Use alternate whitespace tokenizer"")
    parser.add_argument(""-w"", ""--stop-word-languages"", dest=""stop_langs"", default=""danish,dutch,english,french,german,italian,norwegian,portuguese,russian,spanish,swedish"",
                help=""Desired languages for stopword lists"")
    options = parser.parse_args()

    ##globals

    stoplist=set(stop_word_gen())  
    tagRE = re.compile(r'&lt;.*?&gt;', re.UNICODE)    # Remove xml/html tags
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO, filename=""topic-modeling-log"")
    logr = logging.getLogger(""topic_model"")
    logr.info(""#""*15 + "" started "" + ""#""*15)

    ##instance of class 

    checker=corpus()
    logr.info(""#""*15 + "" SPARSE MATRIX (pre-filter)"" + ""#""*15)

    ##view sparse matrix and dictionary

    for vector in checker: 
        logr.info(vector)
    logr.info(""#""*15 + "" DICTIONARY (pre-filter)"" + ""#""*15)
    logr.info(checker.dictionary)
    logr.info(checker.dictionary.token2id)
    #filter
    checker.dictionary.filter_extremes(no_below=int(options.doc_freq_limit)+1)
    logr.info(""#""*15 + "" DICTIONARY (post-filter)"" + ""#""*15)
    logr.info(checker.dictionary)
    logr.info(checker.dictionary.token2id)

    ##Create lda model

    if options.create_model:     
        tfidf = models.TfidfModel(checker,normalize=False)
        print tfidf
        logr.info(""#""*15 + "" corpus_tfidf "" + ""#""*15)
        corpus_tfidf = tfidf[checker]
        logr.info(""#""*15 + "" lda "" + ""#""*15)
        lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=checker.dictionary, num_topics=int(options.number_of_topics))
        logr.info(""#""*15 + "" corpus_lda "" + ""#""*15)
        corpus_lda = lda[corpus_tfidf] 

        ##Evaluate topics based on threshold

        scores = list(chain(*[[score for topic,score in topic] \
                      for topic in [doc for doc in corpus_lda]]))
        threshold = sum(scores)/len(scores)
        print ""threshold:"",threshold
        print
        cluster1 = [j for i,j in zip(corpus_lda,documents) if i[0][1] &gt; threshold]
        cluster2 = [j for i,j in zip(corpus_lda,documents) if i[1][1] &gt; threshold]
        cluster3 = [j for i,j in zip(corpus_lda,documents) if i[2][1] &gt; threshold]
</code></pre>

<p>The resulting <code>topic-modeling-log</code> file is below. Thanks in advance for any help!</p>

<h1>topic-modeling-log</h1>

<pre><code>2014-05-25 02:58:50,482 : INFO : ############### started ###############
2014-05-25 02:58:50,483 : INFO : ############### SPARSE MATRIX (pre-filter)###############
2014-05-25 02:58:50,483 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,483 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,483 : INFO : [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]
2014-05-25 02:58:50,483 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,483 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,483 : INFO : [(2, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]
2014-05-25 02:58:50,483 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(4, 1), (10, 1), (12, 1), (13, 1), (14, 1)]
2014-05-25 02:58:50,484 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(3, 1), (10, 2), (13, 1), (15, 1), (16, 1)]
2014-05-25 02:58:50,484 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)]
2014-05-25 02:58:50,484 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)]
2014-05-25 02:58:50,485 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,485 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,485 : INFO : [(24, 1), (26, 1), (27, 1), (28, 1)]
2014-05-25 02:58:50,485 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,485 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,485 : INFO : [(24, 1), (26, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]
2014-05-25 02:58:50,485 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,485 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
2014-05-25 02:58:50,485 : INFO : [(9, 1), (26, 1), (30, 1)]
2014-05-25 02:58:50,485 : INFO : ############### DICTIONARY (pre-filter)###############
2014-05-25 02:58:50,485 : INFO : Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,485 : INFO : {'minors': 30, 'generation': 22, 'testing': 16, 'iv': 29, 'engineering': 15, 'computer': 2, 'relation': 20, 'human': 3, 'measurement': 18, 'unordered': 25, 'binary': 21, 'abc': 0, 'ordering': 31, 'graph': 26, 'system': 10, 'machine': 6, 'quasi': 32, 'random': 23, 'paths': 28, 'error': 17, 'trees': 24, 'lab': 5, 'applications': 1, 'management': 14, 'user': 12, 'interface': 4, 'intersection': 27, 'response': 8, 'perceived': 19, 'widths': 34, 'well': 33, 'eps': 13, 'survey': 9, 'time': 11, 'opinion': 7}
2014-05-25 02:58:50,486 : INFO : keeping 12 tokens which were in no less than 2 and no more than 4 (=50.0%) documents
2014-05-25 02:58:50,486 : INFO : resulting dictionary: Dictionary(12 unique tokens: ['minors', 'graph', 'system', 'trees', 'eps']...)
2014-05-25 02:58:50,486 : INFO : ############### DICTIONARY (post-filter)###############
2014-05-25 02:58:50,486 : INFO : Dictionary(12 unique tokens: ['minors', 'graph', 'system', 'trees', 'eps']...)
2014-05-25 02:58:50,486 : INFO : {'minors': 0, 'graph': 1, 'system': 2, 'trees': 3, 'eps': 4, 'computer': 5, 'survey': 6, 'user': 7, 'human': 8, 'time': 9, 'interface': 10, 'response': 11}
2014-05-25 02:58:50,486 : INFO : collecting document frequencies
2014-05-25 02:58:50,486 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,486 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,486 : INFO : PROGRESS: processing document #0
2014-05-25 02:58:50,486 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,486 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,486 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,488 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,488 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,488 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,488 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
2014-05-25 02:58:50,488 : INFO : calculating IDF weights for 9 documents and 34 features (51 matrix non-zeros)
2014-05-25 02:58:50,488 : INFO : ############### corpus_tfidf ###############
2014-05-25 02:58:50,488 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,488 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,489 : INFO : ############### lda ###############
2014-05-25 02:58:50,489 : INFO : using symmetric alpha at 0.5
2014-05-25 02:58:50,489 : INFO : using serial LDA version on this node
2014-05-25 02:58:50,489 : WARNING : input corpus stream has no len(); counting documents
2014-05-25 02:58:50,489 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,489 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,489 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,489 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,489 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,491 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,491 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,491 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,491 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
2014-05-25 02:58:50,491 : INFO : running online LDA training, 2 topics, 1 passes over the supplied corpus of 9 documents, updating model once every 9 documents, evaluating perplexity every 9 documents, iterating 50 with a convergence threshold of 0
2014-05-25 02:58:50,491 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
2014-05-25 02:58:50,491 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,491 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
</code></pre>
","2322417","","-1","","2017-05-23 12:08:50","2015-12-12 02:45:06","python IndexError using gensim for LDA Topic Modeling","<python><lda><topic-modeling><gensim>","1","1","2","","","CC BY-SA 3.0"
"34384186","1","","","2015-12-20 18:14:30","","3","104","<p>I am trying to use gensim for some topic modelling. When I print the similarity queries, I only get it the id's and values instead of the actual strings. How do i print the string of my similarity queries ? 
below is the code i am using.</p>

<pre><code>index = similarities.MatrixSimilarity(lsi[corpus]) 
sims =index[vec_lsi] # perform a similarity query against the corpus
print(list(enumerate(sims))) # print (document_number, document_similarity) 
</code></pre>
","5700941","","","","","2015-12-20 18:14:30","Gensim similarity query-returns ids and values of vectors-no string","<python><gensim>","0","0","","","","CC BY-SA 3.0"
"66533269","1","","","2021-03-08 16:11:36","","0","159","<p>I am calculating the similarity between a query: <code>query2 = 'Audit and control, Board structure, Remuneration, Shareholder rights, Transparency and Performance'</code> and a document(in my case it is a company's annual report).</p>
<p>I am using glove vectors and calculating the soft cosine between vectors, however somehow I get the similarity score of 1 with two documents. <strong>How is that possible?</strong> For sure I know that the document does not contain only these query words. The document is a .txt file with cleaned text. And if the document matches exactly these words, then similarity can be 1 but I know it does not match exactly.</p>
<p>Code:</p>
<pre><code>if 'glove' not in locals():  
glove = api.load(&quot;glove-wiki-gigaword-50&quot;)
similarity_index = WordEmbeddingSimilarityIndex(glove)

def build_term(corpus, query):
dictionary = Dictionary(corpus+[query])
tfidf = TfidfModel(dictionary=dictionary)
similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf) 
return similarity_matrix

tfidf_model = build_term(corpus, query)

def doc_similarity_scores(query,similarity_matrix):
dictionary = Dictionary(corpus+[query])
tfidf = TfidfModel(dictionary=dictionary)
query_tf = tfidf[dictionary.doc2bow(query)]
index = SoftCosineSimilarity(tfidf[[dictionary.doc2bow(document) for document in corpus]],similarity_matrix)
doc_similarity_scores = index[query_tf]
return doc_similarity_scores

document_sim_scores = doc_similarity_scores(query,tfidf_model)

sorted_sim_scores = sort_similarity_scores_by_document(document_sim_scores)

doc_similar_terms = []
max_results_per_doc = 50
for term in query:
dictionary = Dictionary(corpus+[query])
idx1 = dictionary.token2id[term]
for document in corpus:
    results_this_doc = []
    for word in set(document):
        idx2 = dictionary.token2id[word]
        score = tfidf_model.matrix[idx1, idx2]
        if score &gt; 0.0:
            results_this_doc.append((word, score))
    results_this_doc = sorted(results_this_doc, reverse=True, key=lambda x: x[1])  
    results_this_doc = results_this_doc[:min(len(results_this_doc), max_results_per_doc)]  
    doc_similar_terms.append(results_this_doc)

for idx in sorted_sim_scores[:90]:
similar_terms_string = ', '.join([result[0] for result in doc_similar_terms[idx]])
print(f'{idx} \t {document_sim_scores[idx]:0.3f} \t {titles[idx]}')
</code></pre>
<p>Results:</p>
<pre><code>25   1.000   2019_q4_en_eur_con_00.txt
14   1.000   2017_q3_en_eur_con_00.txt
16   0.994   2018_ar_en_eur_con_00.txt
21   0.989   2019_ar_en_eur_con_00.txt
28   0.986   2020_q2_en_eur_con_00.txt
 1   0.963   2014_ar_en_eur_con_00.txt
</code></pre>
","6912505","","6912505","","2021-03-09 15:47:41","2021-03-09 15:47:41","Soft cosine similarity 1 between query and a document","<python><stanford-nlp><gensim><text-processing><cosine-similarity>","0","0","","","","CC BY-SA 4.0"
"68100358","1","68102773","","2021-06-23 13:01:01","","0","34","<p>I have read various research papers that one can retrofitting a fasttext model to improve its accuracy (<a href=""https://github.com/mfaruqui/retrofitting"" rel=""nofollow noreferrer"">https://github.com/mfaruqui/retrofitting</a>). However I am having trouble on how to implement it.</p>
<p>The github link above, will take one vector file and retrofitting it, output another vector file. I can load it using gensim library. However, since it is a vector file, it is no longer a model and it will not predict OOV (out-of-vocabulary) words. This makes it pointless. Is there a way to retrain the model somehow so it has better accuracy?</p>
","3899298","","","","","2021-06-23 15:24:59","How to retrofit a fasttext model?","<python><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"44051051","1","","","2017-05-18 14:48:50","","1","3544","<p>I am using gensim version <code>0.12.4</code> and have trained two separate word embeddings using the same text and same parameters. After training I am calculating the Pearsons correlation between the word occurrence-frequency and vector-length.  One model I trained using <code>save_word2vec_format(fname, binary=True)</code> and then loaded using <code>load_word2vec_format</code> the other I trained using <code>model.save(fname)</code> and then loaded using <code>Word2Vec.load()</code>. I understand that the word2vec algorithm is non deterministic so the results will vary however the difference in the correlation between the two models is quite drastic. Which method should I be using in this instance?</p>
","4793408","","","","","2017-05-18 15:11:01","Gensim save_word2vec_format() vs. model.save()","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"59736023","1","","","2020-01-14 14:41:59","","0","261","<p>I have a huge data frame that doesn't fit into memory. Thus I access it in Python via <code>dask</code> (distributed).</p>

<p>I want to train a Word2Vec/Doc2Vec model with the package <code>gensim</code> based on the entries of one column in the data frame, that's why I built an iterator like in <a href=""https://stackoverflow.com/questions/56681210/convert-a-column-in-a-dask-dataframe-to-a-taggeddocument-for-doc2vec"">this question</a>.</p>

<p>Now, <code>gensim</code> trains using multiple cores whose number I need to specify, and similarly <code>dask</code> allows me to use multiple cores, too. So far I gave all available cores to <code>dask</code> and the same number of cores to <code>gensim</code>. My reasoning would be that fetching data and training on the data are exclusive tasks that cannot be done at the same time, so <code>gensim</code> and <code>dask</code> shouldn't fight over the cores.</p>

<p>Indeed, there are no error messages, but still, training seems to be quite slow, and I suspect there's a better way to distribute the work. Does anyone have experience in this matter?</p>
","5986779","","","","","2020-01-17 03:22:18","Efficient use of multiple cores with dask distributed and gensim","<python><multithreading><dask><gensim><dask-distributed>","1","1","","","","CC BY-SA 4.0"
"67179473","1","67180499","","2021-04-20 13:00:41","","0","158","<p>I have noticed that my gensim Doc2Vec (DBOW) model is sensitive to document tags. My understanding was that these tags are cosmetic and so they should not influence the learned embeddings. Am I misunderstanding something? Here is a minimal example:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import numpy as np
import os
    
os.environ['PYTHONHASHSEED'] = '0'
    
reps = []
for a in [0,500]:
    documents = [TaggedDocument(doc, [i + a]) 
                 for i, doc in enumerate(common_texts)]
    model = Doc2Vec(documents, vector_size=100, window=2, min_count=0,
                    workers=1, epochs=10, dm=0, seed=0)
    reps.append(np.array([model.docvecs[k] for k in range(len(common_texts))])
    
reps[0].sum() == reps[1].sum()
</code></pre>
<p>This last line returns <code>False</code>. I am working with gensim 3.8.3 and Python 3.5.2. More generally, is there <em>any</em> role that the values of the tags play (assuming they are unique)? I ask because I have found that using different tags for documents in a classification task leads to widely varying performance.</p>
<p>Thanks in advance.</p>
","3703379","","130288","","2021-04-20 18:33:16","2021-06-24 03:58:28","Why is my Doc2Vec model in gensim not reproducible?","<gensim><word2vec><random-seed><doc2vec>","3","0","","","","CC BY-SA 4.0"
"45711628","1","","","2017-08-16 10:53:19","","4","1941","<p>I am currently working on LDA logarithm in python. I want to covert the topics into just a list of the top 20 words in each topic. I tried below code but got different output. 
I want my output in following format: <code>topic=2,words=20</code>.</p>

<pre><code>['men', 'kill', 'soldier', 'order', 'patient', 'night', 'priest', 'becom', 'new', 'speech', 'friend', 'decid', 'young', 'ward', 'state', 'front', 'would', 'home', 'two', 'father']

[""n't"", 'go', 'fight', 'doe', 'home', 'famili', 'car', 'night', 'say', 'next', 'ask', 'day', 'want', 'show', 'goe', 'friend', 'two', 'polic', 'name', 'meet']
</code></pre>

<p>I got below output:</p>

<pre><code>[""(u'ngma', 0.034841332255132154)"", ""(u'video', 0.0073756817356584745)"", ""(u'youtube', 0.006524039676605746)"", ""(u'liked', 0.0065240394176856644)"",]
[""(u'ngma', 0.024537057880333127)"", ""(u'photography', 0.0068263432438681482)"", ""(u'tvallwhite', 0.0029535361359022566)"", ""(u'3', 0.0029252727655122079)""]
</code></pre>

<p>My code:   </p>

<pre><code>`ldamodel = Lda(doc_term_matrix, num_topics=2, id2word = dictionary,passes=50)
lda=ldamodel.print_topics(num_topics=2, num_words=3)

f=open('LDA.txt','w')
f.write(str(lda))
f.close()

topics_matrix = ldamodel.show_topics(formatted=False,num_words=10)
topics_matrix = np.array((topics_matrix),dtype=list)
topic_words = topics_matrix[:, 1]
for i in topic_words:
    print([str(word) for word in i])
    print()`
</code></pre>

<p><strong>edit-1:</strong></p>

<pre><code>topic_words = []
for i in range(3):
    tt = ldamodel.get_topic_terms(i,10)
    topic_words.append([pair[0] for pair in tt])
    print topic_words
</code></pre>

<p>resulted in non expected output:</p>

<pre><code>[[1897, 135, 130, 127, 70, 162, 445, 656, 608, 1019], [1897, 364, 56, 1236, 181, 172, 449, 48, 15, 18], [1897, 163, 11, 70, 166, 345, 480, 9, 60, 351]]
</code></pre>
","8449495","","8449495","","2017-08-16 14:26:45","2017-08-16 18:05:16","how to convert the topics into just a list of the top 20 words in each topic in LDA in python","<python><gensim><lda>","1","0","1","","","CC BY-SA 3.0"
"68106023","1","","","2021-06-23 19:24:07","","1","923","<p>I have a compatibility issue when running :
<code>pip install numpy==1.19.4</code>
<code>pip install tensorflow=2.5.0</code>
<code>pip install gensim==4.0.1</code></p>
<p>On Ubuntu 18.04, with Python 3.9.5 (installs made inside docker container).</p>
<p>I get the following exception when trying to import gensim:
<code>ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject. </code>
Not sure how i can make this work, I tried downgrading several libraries but still not achieved to make it work in Ubuntu.</p>
<p>Edit : it works on Python 3.8.10</p>
","11774431","","11774431","","2021-06-24 06:46:10","2021-07-02 12:49:58","Tensorflow 2.5.0 and Gensim 4.0.1 compatibility with numpy","<python><numpy><tensorflow2.0><gensim>","1","0","","","","CC BY-SA 4.0"
"57895899","1","58001843","","2019-09-11 19:49:13","","0","145","<p>I am attempting to use Gensim's Mallet wrapper. When I run the following code:</p>

<pre class=""lang-py prettyprint-override""><code>import os
import gensim

os.environ.update({
        'MALLET_HOME':
        r"":C\Users\me\OneDrive - My Company\Documents\Projects\Current\mallet-2.0.8""
    })
lda_mallet = gensim.models.wrappers.LdaMallet(
        r""C:\Users\me\OneDrive - My Company\Documents\Projects\Current\mallet-2.0.8\bin\mallet"",
        corpus=corpus,
        num_topics=10,
        id2word=id_dict)
</code></pre>

<p>I am thrown the following errors:</p>

<pre><code>'C:\Users\me\OneDrive' is not recognized as an internal or external command,
operable program or batch file.

subprocess.CalledProcessError: Command 'C:\Users\me\OneDrive - My Company\Documents\Projects\Current\mallet-2.0.8\bin\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\me\AppData\Local\Temp\17fe21_corpus.txt --output C:\Users\me\AppData\Local\Temp\17fe21_corpus.mallet' returned non-zero exit status 1.
</code></pre>

<p>After exhaustive online searches, I have found many proposed solutions that unfortunately do not resolve my issue.</p>

<p>Since the first error message does not print the entire path, I believe the spaces are the cause of the issue. </p>

<p>Unfortunately, my company requires that I use this directory and I cannot change the name. Is there a way to ""escape"" the spaces in order to run my code?</p>
","11903343","","","","","2019-09-18 23:22:48","How do I pass a file path containing spaces to the Gensim LDA Mallet wrapper?","<python><bash><gensim><lda><mallet>","1","0","","","","CC BY-SA 4.0"
"34540518","1","","","2015-12-31 03:14:20","","4","2219","<p>Suppose I have a (possibly) large corpus, about 2.5M of them with 500 features (after running LSI on the original data with gensim). I need the corpus to train my classifiers using scikit-learn. However, I need to first convert the corpus into a numpy array. The corpus creation and classifier trainer are done in two different scripts.</p>

<p>So the problem is that, my collection size is expected to grow, and at this stage I already don't have enough memory (32GB on the machine) to convert all at once (with <code>gensim.matutils.corpus2dense</code>). In order to work around the problem I am converting one vector after another at a time, but it is very slow.</p>

<p>I have considered dumping the corpus into svmlight format, and have scikit-learn to load it with <code>sklearn.datasets.load_svmlight_file</code>. But then it would probably mean I will need to load everything into memory at once?</p>

<p>Is there anyway I can efficiently convert from gensim corpus to numpy array (or scipy sparse matrix)?</p>
","5742","","","","","2015-12-31 04:00:02","How to convert Gensim corpus to numpy array (or scipy sparse matrix) efficiently?","<python><scikit-learn><gensim>","1","2","1","","","CC BY-SA 3.0"
"59765941","1","59827145","","2020-01-16 08:53:24","","1","290","<p>I am using LDA for a Topic Modelling task. As suggested in various forums online, I have trained my model on a fairly large corpus : NYTimes news dataset (~ 200 MB csv file) which has reports regarding a wide variety of news topics.
Surprisingly the topics predicted out of it are mostly related to US politics and when I test it on a new document regarding 'how to educate children and parenting stuff' it predicts the most likely topic as this :</p>

<p>['two', 'may', 'make', 'company', 'house', 'things', 'case', 'use']</p>

<p>Kindly have a look at my model :</p>

<pre><code>def ldamodel_english(filepath, data):
  data_words = simple_preprocess(str(data), deacc=True)

  # Building the bigram model and removing stopwords
  bigram = Phrases(data_words, min_count=5, threshold=100)
  bigram_mod = Phraser(bigram)
  stop_words_english = stopwords.words('english')
  data_nostops = [[word for word in simple_preprocess(str(doc)) if word not in stop_words_english] 
for doc in data_words]
  data_bigrams = [bigram_mod[doc] for doc in data_nostops]
  data_bigrams = [x for x in data_bigrams if x != []]

  # Mapping indices to words for computation purpose
  id2word = corpora.Dictionary(data_bigrams)
  corpus = [id2word.doc2bow(text) for text in data_bigrams]

  # Building the LDA model. The parameters 'alpha' and 'eta' handle the number of topics per document and words per topic respectively
  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=20, random_state=10, iterations=100,
                                            update_every=1, chunksize=1000, passes=8, alpha=0.09, per_word_topics=True, eta=0.8)
  print('\nPerplexity Score: ' + str(lda_model.log_perplexity(corpus)) + '\n')
  for i, topic in lda_model.show_topics(formatted=True, num_topics=20, num_words=10):
      print('TOPIC #' + str(i) + ': ' + topic + '\n')
  coherence_model_lda = CoherenceModel(model=lda_model, texts=data_bigrams, dictionary=id2word, coherence='c_v')
  print('\nCoherence Score: ', coherence_model_lda.get_coherence())
  saved_model_path = os.path.join(filepath, 'ldamodel_english')
  lda_model.save(saved_model_path)

return saved_model_path, corpus, id2word
</code></pre>

<p>The 'data' part comes from the 'Content' section of the NYTimes News dataset and I used GENSIM library for LDA.</p>

<p>My question is if a well trained LDA model predicts so badly why there is such a hype and what is an effective alternative method?</p>
","8732457","","7117003","","2020-01-16 09:01:45","2020-01-20 16:14:41","LDA Topic Modelling : Topics predicted from huge corpus make no sense","<python><data-science><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"59690985","1","","","2020-01-11 01:48:35","","0","184","<p>I'm trying to train a gensim Word2Vec model with bigrams. To get the bigrams, I run the following code, with <code>sentences</code> standing for a long list of split sentences using <code>nltk.sent_tokenize</code>, lemmatized with Spacy and then lowercased:</p>

<pre><code>from gensim.models import Word2Vec, Phrases

bigrams = Phrases(sentences, min_count=20, threshold=10)
</code></pre>

<p>This could only include bigrams which occur >= 20 times. But when I run <code>bigrams.vocab</code>, I get:</p>

<pre><code>defaultdict(int,
             b'inflated': 237,
             b'the_inflated': 34,
             b'inflated_bag': 1,
             b'let': 6841,
             b'bag_let': 1,
             b'let_-pron-': 3723,
             ...)
</code></pre>

<p>From what I understand, <code>inflated_bag</code> and <code>let_-pron-</code> should not be present. Is there something I'm doing wrong? Or am I misinterpreting the output?</p>
","7542939","","","","","2020-01-11 09:00:28","gensim Phrases not observing min_count parameter","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"68089626","1","68090974","","2021-06-22 19:21:57","","0","17","<p>One of the tasks can be done with a word2vec model is to find most similar words for a give word using cosine similarity. What is the intuitive explanation for why similar words under a good word2vec model will be close to each other in the space?</p>
","6221871","","","","","2021-06-22 21:36:48","Why similar words will be close to each other under a word2vec model?","<gensim><word2vec><similarity><cosine-similarity>","1","0","","","","CC BY-SA 4.0"
"67170733","1","","","2021-04-20 00:00:21","","0","126","<p>Good evening,  I have a relatively simple question that primarily comes from my inexperience with python.  I would like to extract word embeddings for a list of words.  Here I have created a simple list:</p>
<pre><code>list_word = [['Word'],
 ['ant'],
 ['bear'],
 ['beaver'],
 ['bee'],
 ['bird']]
</code></pre>
<p>Then load gensim and other required libraries:</p>
<pre><code>#import tweepy           # Obtain Tweets via API
import re               # Obtain expressions 
from gensim.models import Word2Vec    #Import gensim Word2Fec

</code></pre>
<p>Now when I use the Word2Vec function I run the following:</p>
<pre><code>#extract embedding length 12
model = Word2Vec(list_word, min_count = 3, size = 12)
print(model)
</code></pre>
<p>When the model is run I then see that the vocab size is 1, when it should not be.  The output is the following:
Word2Vec(vocab=1, size=12, alpha=0.025)</p>
<p>I imagine that the imported data is not in the correct format and could use some advise or even example code on how to transform it into the correct format.  Thank you for your help.</p>
","12027237","","12027237","","2021-04-20 13:21:04","2021-04-20 13:21:04","Extract word embeddings from word2vec","<python><gensim><word2vec>","1","3","","","","CC BY-SA 4.0"
"48263122","1","48265130","","2018-01-15 12:32:11","","2","226","<p>I want to get a vector of words every few iter in <code>word2vec</code>, e.g., I would like to use the model below.</p>

<pre><code>embedding_model = Word2Vec(test_set, size=300, 
                           window=4, workers=6, 
                           iter=300, sg=1, min_count=10)
</code></pre>

<p>In this model, I want to get the 300-dimensional vectors learned for every 50 iterations, because I want to show continuous learning contents in html d3.</p>

<p>How can I do this?</p>
","9219332","","712995","","2018-01-15 16:01:58","2018-01-15 16:01:58","How can I get a vector after each training iter in word2vec?","<python-3.x><nlp><word2vec><gensim><word-embedding>","1","0","","","","CC BY-SA 3.0"
"48267720","1","48268360","","2018-01-15 17:10:08","","0","458","<p>The use-case I have is to have a collection of ""upvoted"" documents and ""downvoted"" documents and using those to re-order a set of results in a search.</p>

<p>I am using gensim <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">doc2vec</a> and am able to run the <code>most_similar</code> queries for word(s) and fetch matching words. But how would I be able to fetch the matching keywords given a vector fetched by a vector sum of the above doc vectors? </p>
","1868436","","","","","2018-01-15 17:58:30","doc2vec: any way to fetch closest matching terms for a given vector?","<word2vec><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"50737844","1","","","2018-06-07 09:33:45","","0","636","<p>I try to get LDAMallet in gensim working, but get the following error</p>

<p>'C:\...\AppData\Local\Temp\eb09f5_state.mallet.gz' not found</p>

<p>The code</p>

<pre><code>ldamallet = gensim.models.\
wrappers.LdaMallet(mallet_path,
                   corpus=corpus,
                   num_topics=5,
                   id2word=dictionary)
</code></pre>

<p>(the num_topics is extremely small, but the test goes over 5 sentences; this has no problem in the regular gensim LdaModel)</p>

<p>thanks,</p>
","9165100","","","","","2018-07-03 15:21:25","Mallet with Gensim: file-not-found","<gensim><lda><mallet>","1","3","","","","CC BY-SA 4.0"
"28508548","1","29087015","","2015-02-13 20:53:48","","3","4156","<p>I am trying to use <em>word2vec</em> for a project and after training I get:</p>

<pre><code>INFO : not storing attribute syn0norm
</code></pre>

<p>Is there any way I could save the <code>syn0norm</code>.</p>

<p>How can I do so?</p>
","3797381","","7483494","","2018-11-14 13:42:47","2018-11-14 13:42:47","Gensim Word2vec storing attribute syn0norm","<python><gensim><word2vec>","1","0","1","","","CC BY-SA 4.0"
"57896070","1","57897622","","2019-09-11 20:04:04","","0","34","<p>Given heavily cleaned input in the format</p>

<pre class=""lang-py prettyprint-override""><code>model_input = [['TWO people admitted fraudulently using bank cards (...)'],
               ['All tyrants believe forever',
                'But history especially People Power (...) first Bulatlat']]
</code></pre>

<p>word2vec is returning alongside the more obvious results super-specific vectors such as</p>

<pre class=""lang-py prettyprint-override""><code>{'A pilot shot dogfight Pakistani aircraft returned India Friday freed Islamabad called peace gesture following biggest standoff two countries years':
     &lt;gensim.models.keyedvectors.Vocab at 0x12a93572828&gt;,
 'This story published content partnership POLITICO':
     &lt;gensim.models.keyedvectors.Vocab at 0x12a93572a58&gt;,
 'Facebook says none 200 people watched live video New Zealand mosque shooting flagged moderators underlining challenge tech companies face policing violent disturbing content real time': 
    &lt;gensim.models.keyedvectors.Vocab at 0x12a93572ba8&gt;}
</code></pre>

<p>It appears to be occurring to more documents than not, and I have a hard time believing they each appear more than five times.</p>

<p>I'm using the following code to create my model:</p>

<pre class=""lang-py prettyprint-override""><code>TRAIN_EPOCHS = 30
WINDOW = 5
MIN_COUNT = 5 
DIMS = 250

vocab_model = gensim.models.Word2Vec(model_input,
                                     size=DIMS,
                                     window=WINDOW,
                                     iter=TRAIN_EPOCHS,
                                     min_count=MIN_COUNT)
</code></pre>

<p>What am I doing wrong that I'm getting such useless vectors?</p>
","379181","","379181","","2019-09-11 20:19:27","2019-09-11 22:35:47","Gensim's word2vec returning awkward vectors","<python><python-3.x><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"40121204","1","","","2016-10-19 02:42:17","","2","554","<p>I want to use Google word2vec (GoogleNews-vectors-negative300.bin) <br>
I downloaded it from <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow"">https://code.google.com/archive/p/word2vec/</a><br>
When I load it, the memory errors occured <br>
(Process finished with exit code 139 (interrupted by signal 11: SIGSEGV))</p>

<pre><code>from gensim.models.word2vec import Word2Vec
embedding_path = ""data/GoogleNews-vectors-negative300.bin""
word2vec = Word2Vec.load_word2vec_format(embedding_path, binary=True)
print word2vec
</code></pre>

<p>I use ubuntu 16.04 / GTX-1070(8gb) / Ram(16gb).
How can I fix it?!
 <a href=""https://i.stack.imgur.com/wreiV.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/wreiV.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/iaFpr.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/iaFpr.png"" alt=""enter image description here""></a></p>
","3704652","","","","","2018-06-09 15:00:59","Google word2vec load error","<gpu><segmentation-fault><gensim><word2vec>","1","5","","","","CC BY-SA 3.0"
"50752533","1","","","2018-06-08 02:31:19","","2","4780","<p>I am trying to fit a Word2Vec model. According to the documentation for Gensim's Word2Vec we do not need to call <code>model.build_vocabulary</code> before using it. 
But yet it is asking for me to do it. I have tried calling this function and it has not worked. I also fitted a Word2Vec model before without needing to call <code>model.build_vocabulary</code> . </p>

<p>Am I doing something wrong? Here is my code:</p>

<pre><code>from gensim.models import Word2Vec
dataset = pd.read_table('genemap_copy.txt',delimiter='\t', lineterminator='\n')

def row_to_sentences(dataframe):
    columns = dataframe.columns.values
    corpus = []
    for index,row in dataframe.iterrows():
        if index == 1000:
            break
        sentence = ''
        for column in columns:
            sentence += ' '+str(row[column])
        corpus.append([sentence])
    return corpus

corpus = row_to_sentences(dataset)
clean_corpus = [[sentence[0].lower()] for sentence in corpus ]


# model = Word2Vec()
# model.build_vocab(clean_corpus)
model = Word2Vec(clean_corpus, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>Help is greatly appreciated!
Also I am using macOS Sierra.
There is not much support online for using Gensim with Mac D: . </p>
","7363464","","","","","2018-06-08 13:32:16","Gensim Word2Vec 'you must first build vocabulary before training the model'","<python><nlp><word2vec><gensim>","3","1","","","","CC BY-SA 4.0"
"61367839","1","61379129","","2020-04-22 14:37:26","","1","510","<p>I have big gensim Doc2vec model, I only need to infer vectors while i am loading the training documents vectors from other source.
Is it possible to load it as is without the big npy file</p>

<p>I did </p>

<p><strong>Edit:</strong></p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
model_path = r'C:\model/model'
model = Doc2Vec.load(model_path)
model.delete_temporary_training_data(keep_doctags_vectors=False, keep_inference=True)
model.save(model_path)
</code></pre>

<p>remove the files <code>(model.trainables.syn1neg.npy,model.wv.vectors.npy)</code> <strong>manually</strong></p>

<pre><code>model = Doc2Vec.load(model_path)
</code></pre>

<p>but it ask for </p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-5-7f868a7dbe0c&gt;"", line 1, in &lt;module&gt;
    model = Doc2Vec.load(model_path)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\models\doc2vec.py"", line 1113, in load
    return super(Doc2Vec, cls).load(*args, **kwargs)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\models\base_any2vec.py"", line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\models\base_any2vec.py"", line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\utils.py"", line 427, in load
    obj._load_specials(fname, mmap, compress, subname)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\utils.py"", line 458, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\utils.py"", line 469, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\numpy\lib\npyio.py"", line 428, in load
    fid = open(os_fspath(file), ""rb"")

FileNotFoundError: [Errno 2] No such file or directory: 'C:\\model/model.trainables.syn1neg.npy'
</code></pre>

<p>Note:
Those files not exists in the directory,
The model run on a server and download the model file from the storage
My question is, Do the model must have those files for inference?
I want to run it as low memory consumption as possible.
Thanks.</p>

<p><strong>Edit:</strong>
Is the file model.trainables.syn1neg.npy is the model weights?
Is the file model.wv.vectors.npy is necessary for running an inference? </p>
","10377244","","10377244","","2020-04-22 21:30:03","2020-04-23 03:58:59","Load Doc2Vec without the docs vectors only for infer_vector","<gensim><doc2vec>","1","4","","","","CC BY-SA 4.0"
"68131731","1","","","2021-06-25 13:24:16","","1","176","<p>So I'm trying to use a pretrained Doc2vec for my semantic search project. I tried with this one <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">https://github.com/jhlau/doc2vec</a> (English Wikipedia DBOW) and with the forked version of Gensim (0.12.4) and python 2.7
It works fine when I use most_similar but when i try to use infer_vector I get this error:
<strong>AttributeError: 'Doc2Vec' object has no attribute 'neg_labels'</strong>
what can i do to make this work?</p>
","15823374","","","","","2021-06-26 10:37:44","Doc2Vec' object has no attribute 'neg_labels' when trying to use pretrained model","<nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"68158729","1","68160621","","2021-06-28 06:47:21","","1","37","<p><strong>1. For the below test text,</strong></p>
<pre><code>test=['test test', 'test toy']
</code></pre>
<p>the tf-idf score [without normalisation (smartirs: 'ntn')] is</p>
<pre><code>[['test', 1.17]]  
[['test', 0.58], ['toy', 1.58]]
</code></pre>
<p>This doesn't seem to tally with what I get via direct computation of</p>
<pre><code>tfidf (w, d) = tf x idf  
where idf(term)=log (total number of documents / number of documents containing term)   
tf = number of instances of word in d document / total number of words of d document  
</code></pre>
<p><strong>Eg</strong></p>
<pre><code>doc 1: 'test test'  
for &quot;test&quot; word  
tf= 1  
idf= log(2/2) = 0  
tf-idf = 0  
</code></pre>
<p>Can someone show me the computation using my above test text?</p>
<p><strong>2) When I change to cosine normalisation (smartirs:'ntc'), I get</strong></p>
<pre><code>[['test', 1.0]]  
[['test', 0.35], ['toy', 0.94]]
</code></pre>
<p>Can someone show me the computation too?</p>
<p>Thank you</p>
<pre><code>import gensim
from gensim import corpora
from gensim import models
import numpy as np
from gensim.utils import simple_preprocess

test=['test test', 'test toy']
 
texts = [simple_preprocess(doc) for doc in test]
 
mydict= corpora.Dictionary(texts)
mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in texts]
tfidf = models.TfidfModel(mycorpus, smartirs='ntn')
 
for doc in tfidf[mycorpus]:
    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])  
</code></pre>
","15745459","","6573902","","2021-06-28 09:00:56","2021-06-28 09:12:42","Python gensim (TfidfModel): How is the Tf-Idf computed?","<python><gensim><tf-idf>","1","0","","","","CC BY-SA 4.0"
"59775594","1","","","2020-01-16 18:15:10","","0","364","<p>Is there a way for Gensim to generate strictly the bigrams, trigrams in a list of words? </p>

<p>I can successfully generate the unigrams, bigrams, trigrams but I would like to extract only the bigrams, trigrams. </p>

<p>For example, in the list below:</p>

<pre><code>words = [['the', 'mayor', 'of', 'new', 'york', 'was', 'there'],[""i"",""love"",""new"",""york""],[""new"",""york"",""is"",""great""]]
</code></pre>

<p>I use </p>

<pre><code>bigram = gensim.models.Phrases(words, min_count=1, threshold=1)
bigram_mod = gensim.models.phrases.Phraser(bigram)
words_bigram = [bigram_mod[doc] for doc in words]
</code></pre>

<p>This creates a list of unigrams and bigrams as follows:</p>

<pre><code>[['the', 'mayor', 'of', 'new_york', 'was', 'there'],
 ['i', 'love', 'new_york'],
 ['new_york', 'is', 'great']]
</code></pre>

<p>My question is, is there a way (other than regular expressions) to extract strictly the bigrams, so that in this example only ""new_york"" would be a result?</p>
","6644716","","","","","2020-01-16 19:24:19","How to generate bigram/trigram corpus only","<python><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"59794837","1","","","2020-01-17 21:04:35","","1","350","<p>I'm working on an LDA model using <code>Gensim</code> and <code>spacy</code>. </p>

<p>Generically:</p>

<pre><code>ldamodel = Lda(doc_term_matrix, num_topics=4, random_state = 100, update_every=3, chunksize = 50, id2word = dictionary, passes=100, alpha='auto')
ldamodel.print_topics(num_topics=4, num_words=6)
</code></pre>

<p>I'm at the point where I have some output and I'd like to append my original Dataframe (from which the text came from) with the topics and a percent contribution for each document.</p>

<p>The original df looks like this</p>

<pre><code>id  group text
234 1     here is some text
837 7     here is some text
494 2     here is some text
223 1     here is some text
</code></pre>

<p>I do some standard preprocessing including lemmatization, removing stop words, etc. and then compute percent contributions for each document.</p>

<p>my output looks like this</p>

<pre><code>   Document_No  Dominant_Topic  ...                                           Keywords Text
0            0             1.0  ...  RT, new, work, amp, year, today, people, look,...    0
1            1             0.0  ...  like, time, good, know, day, find, research, a...    1
2            2             1.0  ...  RT, new, work, amp, year, today, people, look,...    2
3            3             3.0  ...  study, t, change, use, want, Trump, love, stud...    3
4            4             3.0  ...  study, t, change, use, want, Trump, love, stud...    4
</code></pre>

<p>I thought I could just concat the 2 dfs back together like so:</p>

<pre><code>results = pd.concat([df, results])
</code></pre>

<p>but when I do that the indices don't match and I'm left with a sort of Frankenstein df that looks like this</p>

<pre><code>id  group text                Document_No  Dominant_Topic  ...                                           
NaN NaN   NaN                 0            1.0             ...
NaN NaN   NaN                 1            0.0             ...
494 2     here is some text   NaN          NaN             ...
223 1     here is some text   NaN          NaN             ...
</code></pre>

<p>Happy to post fuller code if that would be helpful, but I'm hoping someone just knows a better way to do this from same point as I might print topics.</p>
","5703997","","5703997","","2020-01-17 21:18:50","2020-01-17 21:18:50","Append/Merge Dataframe with LDA output","<python><python-3.x><pandas><gensim>","0","5","","","","CC BY-SA 4.0"
"59813664","1","59813857","","2020-01-19 19:33:28","","0","4270","<p>I'm getting an <strong>AttributeError</strong> while trying to implement with embedding_vector:</p>

<pre><code>from gensim.models import KeyedVectors
embeddings_dictionary = KeyedVectors.load_word2vec_format('model', binary=True)

embedding_matrix = np.zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
</code></pre>

<blockquote>
  <p>AttributeError: 'Word2VecKeyedVectors' object has no attribute 'get'</p>
</blockquote>
","9630637","","5833590","","2020-01-19 19:34:20","2020-01-19 19:57:02","Error while implementing Word2Vec model with embedding_vector","<python><machine-learning><keras><gensim><word2vec>","2","1","1","","","CC BY-SA 4.0"
"66469391","1","","","2021-03-04 05:56:45","","0","13","<p>I am new to topic modeling and I was trying out sample code given on gensim documentation page. I want to create an end to end solution where as a user, someone can point to a pandas dataframe and train topic models , which can be later on saved and re-loaded for future scoring. With this context, I am trying out sample pieces of code wherein I want to train a topic model and first assign topic numbers to <code>training data</code> itself. Then when I am done evaluating the model, I want to <code>apply</code> this model to <code>unseen examples</code> and see what topics I get.</p>
<p>Here is my sample code</p>
<pre><code>import gensim
import gensim.corpora as corpora
from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel
from gensim.models.ldamodel import LdaModel
from gensim.test.utils import common_texts # loads sample data

# Create a corpus from a list of texts
common_dictionary = Dictionary(common_texts)
common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]
</code></pre>
<p>Here is the data -</p>
<pre><code>common_texts
[['human', 'interface', 'computer'],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]
</code></pre>
<p>I then train model on this corpus using below code -</p>
<pre><code>lda_model1 = LdaModel(corpus=common_corpus,
                   id2word=common_dictionary,
                   num_topics=3, 
                   random_state=0,
                   alpha='auto')

pprint(lda_model1.print_topics())
doc_lda = lda_model1[common_corpus]
</code></pre>
<p>And it prints top keywords in each topic -</p>
<pre><code>[(0,
  '0.215*&quot;graph&quot; + 0.152*&quot;trees&quot; + 0.151*&quot;minors&quot; + 0.089*&quot;computer&quot; + '
  '0.088*&quot;interface&quot; + 0.088*&quot;human&quot; + 0.086*&quot;survey&quot; + 0.027*&quot;user&quot; + '
  '0.027*&quot;system&quot; + 0.026*&quot;time&quot;'),
 (1,
  '0.249*&quot;system&quot; + 0.174*&quot;eps&quot; + 0.102*&quot;interface&quot; + 0.102*&quot;human&quot; + '
  '0.099*&quot;user&quot; + 0.097*&quot;trees&quot; + 0.032*&quot;graph&quot; + 0.031*&quot;survey&quot; + '
  '0.030*&quot;minors&quot; + 0.028*&quot;time&quot;'),
 (2,
  '0.174*&quot;user&quot; + 0.171*&quot;response&quot; + 0.170*&quot;time&quot; + 0.103*&quot;system&quot; + '
  '0.100*&quot;survey&quot; + 0.100*&quot;computer&quot; + 0.037*&quot;trees&quot; + 0.031*&quot;graph&quot; + '
  '0.029*&quot;minors&quot; + 0.029*&quot;human&quot;')]
</code></pre>
<p>I am now stuck at a step where I want to assign the <code>topic numbers</code> - <code>0,1 or 2</code> to each of the 9 data points on which I trained my model. How do I assign these topic numbers to the original data?</p>
<p>Also, how do I use this model to retrieve topics on unseen examples that model has never seen?</p>
","8229534","","","","","2021-03-04 05:56:45","How to assign topics on train data using gensim and load it for future use?","<python><gensim><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"67185941","1","67186195","","2021-04-20 20:32:21","","1","29","<p>I have some unsupervised data (100.000 files) and each file has a paragraph containing one sentence. The preprocessing went wrong and deleted all stop points (.).
I used word2vec on a small sample (2000 files) and it treated each document as one sentence.
Should I continue the process on all remaining files? Or this would result to a bad model ?</p>
<p>Thank you</p>
","9782974","","","","","2021-04-20 20:52:43","Word2vec on documents each one containing one sentence","<python><nlp><gensim><word2vec>","1","0","0","","","CC BY-SA 4.0"
"50438428","1","","","2018-05-20 19:03:03","","2","2651","<p>I am struggling to understand the usage of doc2vec. I trained a toy model on a set of documents using some sample code I saw on googling. Next I want to find the document that the model considers to be the closest match to documents in my training data. Say my document is ""This is a sample document"".</p>

<pre><code>test_data = word_tokenize(""This is a sample document"".lower())
v = model.infer_vector(test_data)
print(v)
# prints a numpy array.

# to find most similar doc using tags
similar_doc = model.docvecs.most_similar('1')
print(similar_doc)
# prints [('0', 0.8838234543800354), ('1', 0.875300943851471), ('3', 
#          0.8752948641777039), ('2', 0.865660548210144)]
</code></pre>

<p>I searched a fair bit but I am confused how to interpret similar_doc. I want to answer the question: ""which documents in my training data most closely match the document 'This is a sample document'"", so how do I map the similar_doc output back to the training data? I did not understand the array of tuples, the second half of each tuple must be a probability but what are '0', '1' etc?</p>
","9819934","","","","","2018-05-21 14:39:32","doc2vec get most similar document","<python><machine-learning><gensim><doc2vec>","1","2","","","","CC BY-SA 4.0"
"68048018","1","68048631","","2021-06-19 15:01:48","","0","47","<p>When trying to upload the fasttext model (cc.nl.300.bin) in gensim I get the following error:</p>
<pre><code>!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.bin.gz
!gunzip cc.nl.300.bin.gz
model = FastText_gensim.load_fasttext_format('cc.nl.300.bin')
model.build_vocab(cleaned_text, update=True)

AttributeError: 'FastTextTrainables' object has no attribute 'syn1neg'
</code></pre>
<p>The code goes wrong when building the vocab with my own dataset. The format of that dataset is all right, as I already used it to build and train other (not pre-trained) Word2Vec and FastText models.</p>
<p>I saw other had the same error on this blog, however their solution did not work for me: <a href=""https://github.com/RaRe-Technologies/gensim/issues/2588"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/2588</a></p>
<p>Also, I read somewhere that I should use 'load_facebook_model'? However I was not able to import load_facebook_model at all? Is this even a good way to solve this problem?</p>
<p>Any other suggestions?</p>
","15529066","","466862","","2021-06-19 15:06:21","2021-06-19 16:10:11","Dutch pre-trained model not working in gensim","<gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"67168343","1","","","2021-04-19 19:37:56","","2","359","<p>Any leads on how can I install gensim on the M1 chip on the new Mac community?
There hasn't been much news yet for this.
This is the only other question there -</p>
<p><a href=""https://stackoverflow.com/questions/65769069/cant-install-gensim-on-apple-m1"">can&#39;t install gensim on apple M1</a></p>
<p>(and I am not looking to use rosetta)</p>
<p><strong>ERROR</strong>:</p>
<blockquote>
<p>RuntimeError: Cython extensions are unavailable. Without them, this
gensim functionality is disabled. If you've installed from a package,
ask the package maintainer to include Cython extensions.</p>
</blockquote>
","3933055","","3933055","","2021-04-19 20:22:52","2021-04-19 20:22:52","Install Gensim on the new M1 chip Mac","<python><macos><scipy><gensim><apple-m1>","0","10","","","","CC BY-SA 4.0"
"34634367","1","","","2016-01-06 13:30:21","","1","333","<p>FYI, when I try to import gensim module in Django views.py, my server just got stuck, all my restful apis don't not working, not 5**, not 4**, not 3**, not 2**, just no response.</p>

<p>But if I just comment <code>import gensim</code> in views.py, it just back to the normal.</p>

<p>So where is the problem, and how to debug Django in this situation?</p>

<p>Django version: 1.8   Gensim version: 0.12.3</p>

<h2>UPDATE</h2>

<p>I can import gensim in Django shell
<a href=""https://i.stack.imgur.com/OzDnj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OzDnj.jpg"" alt=""enter image description here""></a></p>

<p>And same issue on Django 1.9</p>
","456105","","456105","","2016-01-06 15:42:22","2017-06-15 02:43:23","Get stuck when importing gensim in Django views.py","<python><django><gensim>","0","3","","","","CC BY-SA 3.0"
"34637242","1","","","2016-01-06 15:52:28","","1","87","<p>I was reading the <a href=""https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""nofollow"">Experiments on the English Wikipedia</a> tutorial and noticed that many of the topics generated by LSA and LDA contained multi-word terms that had clearly been concatenated e.g. <em>northamerica</em>, <em>hockeyarchives</em></p>

<p>Could someone indicate where this takes place. I have looked at <em>gensim.scripts.make_wiki</em>, <em>gensim.corpora.wikicorpus</em> and <em>genesis.utils</em>.</p>
","4955820","","","","","2016-01-06 15:52:28","Does Gensim handle multi-word terms when processing Wikipedia corpus?","<nlp><lda><gensim><topic-modeling><lsa>","0","0","0","","","CC BY-SA 3.0"
"41628856","1","41629091","","2017-01-13 06:46:13","","7","4220","<p>I am learning about word2vec and GloVe model in python so I am going through this available <a href=""http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python"" rel=""noreferrer"">here</a>. </p>

<p>After I compiled these code step by step in Idle3:</p>

<pre><code>&gt;&gt;&gt;from gensim.models import word2vec
&gt;&gt;&gt;import logging
&gt;&gt;&gt;logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
&gt;&gt;&gt;sentences = word2vec.Text8Corpus('text8')
&gt;&gt;&gt;model = word2vec.Word2Vec(sentences, size=200)
</code></pre>

<p>I am getting this error : </p>

<pre><code>2017-01-13 11:15:41,471 : INFO : collecting all words and their counts
Traceback (most recent call last):
  File ""&lt;pyshell#4&gt;"", line 1, in &lt;module&gt;
    model = word2vec.Word2Vec(sentences, size=200)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 469, in __init__
    self.build_vocab(sentences, trim_rule=trim_rule)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 533, in build_vocab
    self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 545, in scan_vocab
    for sentence_no, sentence in enumerate(sentences):
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 1536, in __iter__
    with utils.smart_open(self.fname) as fin:
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 127, in smart_open
    return file_smart_open(parsed_uri.uri_path, mode)
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 558, in file_smart_open
    return open(fname, mode)
FileNotFoundError: [Errno 2] No such file or directory: 'text8'
</code></pre>

<p>How do I rectify this ?
Thanks in advance for your help.</p>
","","user7399214","4952130","","2017-01-13 07:24:05","2017-05-28 15:35:19","gensim Getting Started Error: No such file or directory: 'text8'","<python><python-3.x><error-handling><gensim><word2vec>","1","0","1","","","CC BY-SA 3.0"
"59667894","1","","","2020-01-09 16:03:17","","0","67","<p>I have created a k means cluster from Gensim word2vec where the value of <code>k</code> is <code>3</code>. Now I want to retrieve the cluster and the values where the frequency is the most. </p>

<pre><code>import gensim
from gensim.models import Word2Vec
import nltk
from nltk.tokenize import sent_tokenize
from sklearn.cluster import KMeans
import numpy as np
text = ""Thank you for keeping me updated on this issue. I'm happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. Also many thanks for &lt;pre&gt; your suggestions. We hope to improve this feature in the future. In case you experience any &lt;pre&gt; further problems with the app, please don't hesitate to contact me again.""
sentences = sent_tokenize(text)
word_text = [[text for text in sentences.split()] for sentences in sentences]
model = Word2Vec(word_text, min_count=1)
x = model[model.wv.vocab]
n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters)
kmeans = kmeans.fit(x)
</code></pre>
","9297780","","10091597","","2020-01-09 16:38:01","2020-01-09 19:45:11","how to select cluster with maximum frequency in k means","<python><k-means><gensim><word2vec>","1","2","","","","CC BY-SA 4.0"
"68050134","1","68052249","","2021-06-19 19:34:52","","0","279","<p>I try to use <code>lemmatize()</code> function from <code>gensim</code>. They said I have to install <code>pattern</code> also to use this function. I have already install both <code>gensim</code> and pattern but every time I try to import lemmatize from <code>gensim</code>, it keeps showing this error</p>
<p>I use <code>pip install gensim</code> and <code>pip install pattern</code> to install the libraries. My gensim version is 4.0.1 and pattern is 3.6</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\huynh\Desktop\machine_learning\test.py&quot;, line 1, in &lt;module&gt;
    from gensim.utils import lemmatize
ImportError: cannot import name 'lemmatize' from 'gensim.utils' (C:\Users\huynh\AppData\Local\Programs\Python\Python39\lib\site-packages\gensim\utils.py)
</code></pre>
<p>I tried to look up documentation about this but all I could find is that I have to install <code>pattern</code> to be able to use it. Does anyone have an idea why I still don't have <code>lemmatize()</code>? Thank you</p>
","16269368","","1371329","","2021-06-21 13:57:32","2021-06-21 13:57:32","Cannot import name 'lemmatize' from 'gensim.utils' although I have installed Pattern","<python><gensim>","2","2","","","","CC BY-SA 4.0"
"32476336","1","33166451","","2015-09-09 09:51:02","","5","10006","<h1>LDA Original Output</h1>

<ul>
<li><p>Uni-grams </p>

<ul>
<li><p>topic1 -scuba,water,vapor,diving</p></li>
<li><p>topic2 -dioxide,plants,green,carbon</p></li>
</ul></li>
</ul>

<h1>Required Output</h1>

<ul>
<li><p>Bi-gram topics</p>

<ul>
<li><p>topic1 -scuba diving,water vapor</p></li>
<li><p>topic2 -green plants,carbon dioxide</p></li>
</ul></li>
</ul>

<p>Any idea?</p>
","4799994","","4799994","","2015-09-10 04:45:01","2018-11-13 02:02:23","How to abstract bigram topics instead of unigrams using Latent Dirichlet Allocation (LDA) in python- gensim?","<nlp><text-mining><lda><gensim>","2","0","5","","","CC BY-SA 3.0"
"50992153","1","50994033","","2018-06-22 16:29:52","","0","583","<p>running gensim Doc2Vec over ubuntu</p>

<p>Doc2Vec rejects my input with the error</p>

<blockquote>
  <p>AttributeError: 'list' object has no attribute 'words'</p>
</blockquote>

<pre><code>    import gensim from gensim.models  
    import doc2vec as dtv
    from nltk.corpus import brown
    documents = brown.tagged_sents()
    d2vmodel = &gt; dtv.Doc2Vec(documents, size=100, window=1, min_count=1, workers=1)
</code></pre>

<p>I have tried already from 
<a href=""https://stackoverflow.com/questions/36509957/why-gensim-doc2vec-give-attributeerror-list-object-has-no-attribute-words"">this SO question</a> and many variations with the same result</p>

<p>documents = [brown.tagged_sents()}
adding a hash function</p>

<p>If corpus is a .txt file I <em>can</em> utilize </p>

<pre><code>    documents=TaggedLineDocument(documents)
</code></pre>

<p>but that is often not possible</p>
","2363193","","","","","2018-06-22 18:49:34","Doc2Vec input format","<gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"58186670","1","","","2019-10-01 14:12:27","","5","2374","<p>I'm building a Word2Vec model for a category-recommendation on a dataset consisting of ~35.000 sentences for a total of ~500.000 words but only ~3.000 distinct ones.
I build the model basically like this :</p>

<pre class=""lang-py prettyprint-override""><code>def train_w2v_model(df, epochs):
    w2v_model = Word2Vec(min_count=5,
                                 window=100,
                                 size=230,
                                 sample=0,
                                 workers=cores-1,
                                 batch_words=100)
    vocab = df['sentences'].apply(list)
    w2v_model.build_vocab(vocab)
    w2v_model.train(vocab, total_examples=w2v_model.corpus_count, total_words=w2v_model.corpus_total_words, epochs=epochs, compute_loss=True)
    return w2v_model.get_latest_training_loss()
</code></pre>

<p>I tried to find the right number of epochs for such a model like this :</p>

<pre class=""lang-py prettyprint-override""><code>print(train_w2v_model(1))
=&gt;&gt; 86898.2109375
print(train_w2v_model(100))
=&gt;&gt; 5025273.0
</code></pre>

<p>I find the results very counterintuitive.
I do not understand how increasing the number of epochs could lead to lower the performance.
It seems not to be a misunderstanding from the function <code>get_latest_training_loss</code> since I observe the results with the function <code>most_similar</code> way better with only 1 epoch :</p>

<p>100 epochs :</p>

<pre class=""lang-py prettyprint-override""><code>w2v_model.wv.most_similar(['machine_learning'])
=&gt;&gt; [('salesforce', 0.3464601933956146),
 ('marketing_relationnel', 0.3125850558280945),
 ('batiment', 0.30903393030166626),
 ('go', 0.29414454102516174),
 ('simulation', 0.2930642068386078),
 ('data_management', 0.28968319296836853),
 ('scraping', 0.28260597586631775),
 ('virtualisation', 0.27560457587242126),
 ('dataviz', 0.26913416385650635),
 ('pandas', 0.2685554623603821)]
</code></pre>

<p>1 epoch :</p>

<pre><code>w2v_model.wv.most_similar(['machine_learning'])
=&gt;&gt; [('data_science', 0.9953729510307312),
 ('data_mining', 0.9930223822593689),
 ('big_data', 0.9894922375679016),
 ('spark', 0.9881765842437744),
 ('nlp', 0.9879133701324463),
 ('hadoop', 0.9834049344062805),
 ('deep_learning', 0.9831978678703308),
 ('r', 0.9827396273612976),
 ('data_visualisation', 0.9805369973182678),
 ('nltk', 0.9800992012023926)]
</code></pre>

<p>Any insight on why it behaves like this ? I would have think that increasing the number of epochs would have for sure a positive effect on the <strong>training</strong> loss.</p>
","4543908","","4543908","","2019-10-02 08:09:53","2019-10-02 08:09:53","Gensim Word2Vec model getting worse by increasing the number of epochs","<python><machine-learning><nlp><gensim><word2vec>","1","0","1","","","CC BY-SA 4.0"
"51030698","1","51030834","","2018-06-25 19:33:43","","2","719","<p>I am using gensim to create a bag of words model and I want to perform normalization. I found the documentation (<a href=""https://radimrehurek.com/gensim/models/normmodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/normmodel.html</a>), but I am confused as to how to implement that given the code I have. Conversations is a list of tokenized documents, so essentially a list of lists when each element is a document.</p>

<pre><code>id2word = corpora.Dictionary(conversations)
id2word.filter_extremes(keep_n=5000, keep_tokens=None) 
corpus = [id2word.doc2bow(text) for text in conversations]
norm_corpus = NormModel(corpus)
</code></pre>

<p>Corpus is a sparse matrix, I believe. For each document, it has the non-zero frequency terms and the corresponding counts: [[(0, 2), (1, 5), (2, 4)...(92, 2), (93, 3)],...].</p>

<p>The last line with <code>norm_corpus</code> does not work when I try to input it into the following: <code>models.LsiModel(norm_corpus, id2word=id2word, num_topics=12)</code>. I get the type error message, 'int' object is not iterable. However, the documentation says to pass in a corpus so I'm confused. I would appreciate any help -- thanks!</p>
","7375754","","7375754","","2018-06-25 19:48:52","2018-06-25 19:48:52","Normalizing bag of words data in Gensim","<python><normalization><gensim><corpus><term-document-matrix>","1","2","1","","","CC BY-SA 4.0"
"58182293","1","58185242","","2019-10-01 09:53:47","","0","250","<p>Hosting a word2vec model with gensim on AWS lambda </p>

<p>using python 2.7
boto==2.48.0
gensim==3.4.0</p>

<p>and I have a few lines in my function.py file where I load the model directly from s3</p>

<pre><code>print('################### connecting to s3...')
s3_conn = boto.s3.connect_to_region(
        region,
        aws_access_key_id = Aws_access_key_id,
        aws_secret_access_key = Aws_secret_access_key,
        is_secure = True,
        calling_format = OrdinaryCallingFormat()
        )
print('################### connected to s3...')
bucket = s3_conn.get_bucket(S3_BUCKET)
print('################### got bucket...')
key = bucket.get_key(S3_KEY)
print('################### got key...')
model =  KeyedVectors.load_word2vec_format(key, binary=True)
print('################### loaded model...')
</code></pre>

<p>on the model loading line</p>

<pre><code>    model =  KeyedVectors.load_word2vec_format(key, binary=True)
</code></pre>

<p>getting a mysterious error without much details:</p>

<p>on the cloud watch can see all of my print messages til '################### got key...' inclusive, 
then I get: </p>

<pre><code>START RequestId: {req_id} Version: $LATEST 
</code></pre>

<p>then right after it [no time delays between these two messages]</p>

<pre><code>module initialization error: __exit__ 
</code></pre>

<p>please, is there a way to get a detailed error or more info?</p>

<p>More background details :
I was able to download the model from s3 to /tmp/ and it did authorize and retrieve the model file, but it went out of space [file is ~2GB, /tmp/ is 512MB]</p>

<p>so, switched to directly loading the model by gensim as above and now getting that mysterious error.</p>

<p>running the function with python-lambda-local works without issues </p>

<p>so, this probably narrows it down to an issue with gensim's smart open or aws lambda, would appreciate any hints, thanks! </p>
","4703685","","174777","","2019-12-03 18:09:35","2019-12-03 18:09:35","AWS Lambda Boto gensim model module initialization error: __exit__","<python><amazon-web-services><aws-lambda><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"58195364","1","58209399","","2019-10-02 04:49:30","","0","213","<p>I am doing text classification using gensim and doc2vec. I am using two data-sets for testing this, one being a stack exchange data-set and a Reddit data-set. I am trying to classify between posts from one subreddit/stackexchange site on a particular subject and then using posts from other unrelated subreddit/stackexchange sites as negative examples.</p>

<p>I am using a data-set of 10k posts to train the model and a testing set of 5k divided in to 50% positive examples and 50% negative. I then use the infer_vector and most_similar functions to classify the entry as positive or negative. Before training the model I pre-process the data to remove any words, symbols, links etc just leaving the most significant words to train the model. Below is the code used to train the model.</p>

<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv(""fulltrainingset.csv"")

df.columns.values[0] = ""A""

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df[""A""])]

epoch_list = [1,5,10,15,25,50,100,200,300,400]
size_list = [1,5,10,15,25,50,100,200,300]

for x in epoch_list:
    for y in size_list:

        vec_size = y
        max_epochs = x
        minimum_count = 1
        mode = 0
        window_ = 15
        negative_sampling = 5
        subsampling = 1e-5
        alpha = 0.025
        minalpha = 0.00025

        model = Doc2Vec(alpha=alpha, min_alpha=minalpha, vector_size=vec_size, dm=mode, min_count=minimum_count, window =window_, sample=subsampling ,hs =negative_sampling)
        model.build_vocab(tagged_data)

        for epoch in range(max_epochs):
            print('iteration {0}'.format(epoch))
            model.train(tagged_data,
                        total_examples=model.corpus_count,
                        epochs=model.epochs)#self.epochs
            model.alpha -= 0.0002
            model.min_alpha = model.alpha


        model.save(str(y)+""s_""+str(x)+""e.model"")
</code></pre>

<p>This method is working and I can get results from it, but I would like to know if there is a different way of training to achieve better results. Currently I am just training many models with different epochs and vector_sizes, then using the infer_vector and most_similar functions to see if the vector score returned from the most_similar entry is greater than a certain number, but is there a way to improve upon this in the aspect of training the model?</p>

<p>Also, aiming to get better results I trained another model in the same way with a larger data-set (100k+ entries). When I used this model on the same data-set it produced similar but worse results to the models trained on smaller data-sets. I thought that more training data would have improved the results not made them worse, does anyone know a reason for this ?</p>

<p>Also, to further test I created a new but bigger test-set (15k entries) which did even worse then the original test-set. The data in this test-set although being unique is the same type of data used in the original test-set yet produces worse results, what may be the reason for this ?</p>

<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv(""all_sec_tweets.csv"")

df.columns.values[0] = ""A""

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df[""A""])]

epoch_list = [1,5,10,15,25,50,100]
size_list = [1,5,10,15,25,50,100]

for x in epoch_list:
    for y in size_list:

        vec_size = y
        max_epochs = x
        mode = 0
        window_ = 5
        subsampling = 1e-5

        model = Doc2Vec(vector_size=vec_size, dm=mode, window =window_, sample=subsampling,epochs=max_epochs)
        model.build_vocab(tagged_data)

        model.train(tagged_data,total_examples=model.corpus_count,epochs=model.epochs)

        model.save(str(y)+""s_""+str(x)+""e.model"")
</code></pre>
","1365234","","1365234","","2019-10-03 02:21:03","2019-10-03 02:21:03","text classification model using doc2vec and gensim","<python><machine-learning><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"67261993","1","67270635","","2021-04-26 06:47:10","","0","48","<p>This is my code</p>
<pre><code>w2v = Word2Vec(vector_size=150,min_count = 10)
w2v.build_vocab(x_train)
w2v.train(x_train)

def average_vec(text):
    vec = np.zeros(300).reshape((1,300))
    for word in text:
        try:
            vec += w2v[word].reshape((1,300))
        except KeyError:
            continue
        return vec
</code></pre>
<p><em><strong>And this throws the following error:</strong></em></p>
<pre><code>Traceback (most recent call last):   File
&quot;C:/Users/machao/Desktop/svm-master/word2vec.py&quot;, line 27, in &lt;module&gt;
    train_vec = np.concatenate([average_vec(z) for z in x_train])   File &quot;C:/Users/machao/Desktop/svm-master/word2vec.py&quot;, line 27, in
&lt;listcomp&gt;
    train_vec = np.concatenate([average_vec(z) for z in x_train])   File &quot;C:/Users/machao/Desktop/svm-master/word2vec.py&quot;, line 21, in
average_vec
    vec += w2v[word] TypeError: 'Word2Vec' object is not subscriptable

Process finished with exit code 1
</code></pre>
","15757898","","10315163","","2021-04-26 12:53:34","2021-04-26 16:46:06","I had a problem using word2vec. Maybe it's a version problem, but I don't know how to solve it Ôºü","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"67472355","1","","","2021-05-10 14:31:42","","0","14","<p>I have estimated numerous LDA models with different topic numbers, but based on the same corpus. There is a total of 17 models, with topic numbers starting at 10, all the way through 90, each with an incremental increase of 5 in the number of topics. That yields a total of 850 topics. How do i go about calculating the cosine similarity between each topic pair (722,500 in total)?</p>
<p>Here is the code for the first topic model with 10 topics:</p>
<pre><code>import numpy as np
a = np.arange(start=10, stop=91, step=5)
for i in a:
  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=10, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)```
</code></pre>
","12066890","","","","","2021-05-10 14:31:42","How do I calculate cosine similarity between topics from different lda models?","<python><cluster-analysis><gensim><lda><cosine-similarity>","0","0","","","","CC BY-SA 4.0"
"51062519","1","","","2018-06-27 12:12:53","","0","145","<p>I've used gensim for text summarizing in Python. I want my summarized output to be stored in a different column in the same dataframe.</p>

<p>I've used this code:</p>

<pre><code>for n, row in df_data_1.iterrows():
        text=df_data_1['Event Description (SAP)']
        print(text)
        *df_data_1['Summary']=summarize(text)*
print(df_data_1['Summary'])
</code></pre>

<p>The error is coming on line 4 of this code, which states: TypeError: expected string or bytes-like object.</p>

<p>How to store the processed text in the pandas dataframe</p>
","9999850","","2326911","","2018-06-27 12:14:34","2018-06-27 14:39:59","Storing processed text in pandas dataframe","<python><loops><gensim>","1","3","","","","CC BY-SA 4.0"
"59024220","1","59026174","","2019-11-25 01:48:29","","0","1313","<p>I am a beginner in NLP and it's my first time to do Topic Modeling. I was able to generate my model however I cannot produce the coherence metric.</p>

<p>Converting the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus</p>

<pre><code>sparse_counts = scipy.sparse.csr_matrix(data_dtm)
corpus = matutils.Sparse2Corpus(sparse_counts)
corpus
</code></pre>

<p><a href=""https://i.stack.imgur.com/EU6kl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EU6kl.png"" alt=""enter image description here""></a></p>

<pre><code>df_lemmatized.head()
</code></pre>

<p><a href=""https://i.stack.imgur.com/NUk6h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NUk6h.png"" alt=""enter image description here""></a></p>

<pre><code># Gensim also requires dictionary of the all terms and their respective location in the term-document matrix
tfidfv = pickle.load(open(""tfidf.pkl"", ""rb""))
id2word = dict((v, k) for k, v in tfidfv.vocabulary_.items())
id2word
</code></pre>

<p><a href=""https://i.stack.imgur.com/vN9Dm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vN9Dm.png"" alt=""enter image description here""></a></p>

<p>This is my model:</p>

<pre><code>lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=15, passes=10, random_state=43)
lda.print_topics()
</code></pre>

<p><a href=""https://i.stack.imgur.com/VXdpA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VXdpA.png"" alt=""enter image description here""></a></p>

<p>And finally, here is where I attempted to get Coherence Score Using Coherence Model:</p>

<pre><code># Compute Perplexity
print('\nPerplexity: ', lda.log_perplexity(corpus))  

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda, texts=df_lemmatized.long_title, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
</code></pre>

<p>This is the error:</p>

<p><em>---> 57     if not dictionary.id2token:  # may not be initialized in the standard gensim.corpora.Dictionary
     58         setattr(dictionary, 'id2token', {v: k for k, v in dictionary.token2id.items()})
     59 
AttributeError: 'dict' object has no attribute 'id2token'</em></p>
","9537509","","","","","2019-11-25 09:07:15","Error in Computing the Coherence Score ‚Äì AttributeError: 'dict' object has no attribute 'id2token'","<python><scipy><nlp><gensim><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"66508153","1","","","2021-03-06 16:46:51","","0","17","<p>I have an excel file, I want to filter out special characters in the column titled 'description'
(Assume that the column of data in this column has many special characters)</p>
<p><a href=""https://i.stack.imgur.com/XOCY9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XOCY9.png"" alt=""enter image description here"" /></a></p>
<pre><code>train_data = df[['description']]
for i in range(0,len(train_data)):
    text = gensim.utils.simple_preprocess(train_data.iloc[i, :].to_string())
    print(text)
</code></pre>
<blockquote>
<p>result: ['description', 'this', 'is', 'a', 'widget', 'that', 'is',
'blue']
['description', 'this', 'is', 'a', 'widget', 'that', 'is', 'red']</p>
</blockquote>
<p>I want the results below, what should I do?</p>
<blockquote>
<p>result: ['this', 'is', 'a', 'widget', 'that', 'is',
'blue']
['this', 'is', 'a', 'widget', 'that', 'is', 'red']</p>
</blockquote>
","11278588","","4685471","","2021-03-06 17:44:27","2021-03-06 17:44:27","Filtering special characters with gensim.utils.simple_preprocess failed","<python-3.x><nlp><gensim>","0","0","","","","CC BY-SA 4.0"
"26031958","1","","","2014-09-25 06:23:31","","2","498","<p>I'm trying to reproduce the results of Graber et al. in showing that when LDA is used with a multilingual corpus, the most probable terms for a topic (say, top 10) will come from a single language. Their paper is <a href=""http://arxiv.org/pdf/1205.2657.pdf"" rel=""nofollow"">here</a>.  </p>

<p>This is a reasonable sanity check to perform IMO, but I'm having difficulty. </p>

<p>I'm using the same corpus they used, the <a href=""http://www.statmt.org/europarl/"" rel=""nofollow"">Europarl corpus</a>, with the corpus composed of Bulgarian and English.  I concatenated the Bulgarian and English corpuses with </p>

<pre><code>cat corpusBg.txt corpusEn.txt &gt;&gt; corpusMixed.txt.  
</code></pre>

<p>This contains a sentence on each line, with the collection of lines in Bulgarian and the second collection in English. When I fit an LDA model with 4 topics, 3 contain only English terms in the top 10, and the fourth is mixed between English and Bulgarian. I'm using the default settings for LDA:</p>

<pre><code>texts = [[word for word in doc.lower().split()] for doc in open('corpusMixed.txt', 'r')]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(doc) for doc in texts]
lda = models.ldamodel.LdaModel(corpus, id2word = dictionary, num_topics = 4)
topics = lda.print_topics(lda.num_topics)

for t in topics:
    print t
</code></pre>

<p>Note that I have not removed stopwords or sparse terms, but I think that this shouldn't matter. There should intuitively be some topics with terms only in bulgarian and others with terms only in English, no? </p>
","3053861","","3053861","","2014-09-29 19:10:22","2014-09-29 19:10:22","Lda on Bi(multi)lingual Corpus","<lda><topic-modeling><gensim>","1","0","","","","CC BY-SA 3.0"
"67232788","1","","","2021-04-23 15:29:44","","0","24","<p>I can build a word2vec model and find similar words for any word within a corpus. What other NLP techniques can do this task?</p>
","6221871","","","","","2021-04-23 15:29:44","NLP techniques to find similar keywords within a corpus","<nlp><gensim><word2vec><similarity>","0","0","","2021-07-09 06:45:46","","CC BY-SA 4.0"
"50573054","1","50579950","","2018-05-28 20:25:36","","3","2724","<p><strong>Full Description</strong></p>

<p>I am starting to work with word embedding and found a great amount of information about it. I understand, this far, that I can train my own word vectors or use previously trained ones, such as Google's or Wikipedia's, which are available for the English language and aren't useful to me, since I am working with texts in <em>Brazilian Portuguese</em>. Therefore, I went on a hunt for pre-trained word vectors in Portuguese and I ended up finding <a href=""http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/"" rel=""nofollow noreferrer"">Hirosan's List of Pretrained Word Embeddings</a> which led me to Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a> from which I learned about Rami Al-Rfou's <a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a>. After downloading both, I unsuccessfully have been trying to simply load the word vectors.</p>

<p><strong>Short Description</strong></p>

<p>I can't load pre-trained word vectors; I am trying <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a> and <a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a>.</p>

<p><strong>Downloads</strong></p>

<ul>
<li><a href=""https://drive.google.com/open?id=0B0ZXk88koS2KRDcwcV9IVWFTeUE"" rel=""nofollow noreferrer"">Kyubyong's pre-trained word2vector format word vectors for Portuguese</a>;</li>
<li><a href=""https://doc-0g-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/c1ch6rdnp89glqmi8g81ev2somslu7cs/1527537600000/10341224892851088318/*/0B5lWReQPSvmGNEh0VTdmSHlHZ1k?e=download"" rel=""nofollow noreferrer"">Polyglot's pre-trained word vectors for Portuguese</a>;</li>
</ul>

<p><strong>Loading attempts</strong></p>

<p><em>Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a></em>
First attempt: using Gensim as suggested by <a href=""http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/"" rel=""nofollow noreferrer"">Hirosan</a>;</p>

<pre><code>from gensim.models import KeyedVectors
kyu_path = '.../pre-trained_word_vectors/kyubyong_pt/pt.bin'
word_vectors = KeyedVectors.load_word2vec_format(kyu_path, binary=True)
</code></pre>

<p>And the error returned:</p>

<pre><code>[...]
File ""/Users/luisflavio/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 359, in any2unicode
return unicode(text, encoding, errors=errors)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>The zip downloaded also contains other files but all of them return similar errors.</p>

<p><em><a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a></em>
First attempt: following <a href=""http://nbviewer.jupyter.org/gist/aboSamoor/6046170"" rel=""nofollow noreferrer"">Al-Rfous's instructions</a>;</p>

<pre><code>import pickle
import numpy
pol_path = '.../pre-trained_word_vectors/polyglot/polyglot-pt.pkl'
words, embeddings = pickle.load(open(pol_path, 'rb'))
</code></pre>

<p>And the error returned:</p>

<pre><code>File ""/Users/luisflavio/Desktop/Python/w2v_loading_tries.py"", line 14, in &lt;module&gt;
    words, embeddings = pickle.load(open(polyglot_path, ""rb""))

UnicodeDecodeError: 'ascii' codec can't decode byte 0xd4 in position 1: ordinal not in range(128)
</code></pre>

<p>Second attempt: using <a href=""https://polyglot.readthedocs.io/en/latest/Embeddings.html"" rel=""nofollow noreferrer"">Polyglot's word embedding load function</a>;</p>

<p>First, we have to install polyglot via pip:</p>

<pre><code>pip install polyglot
</code></pre>

<p>Now we can import it:</p>

<pre><code>from polyglot.mapping import Embedding
pol_path = '.../pre-trained_word_vectors/polyglot/polyglot-pt.pkl'
embeddings = Embedding.load(polyglot_path)
</code></pre>

<p>And the error returned:</p>

<pre><code>File ""/Users/luisflavio/anaconda3/lib/python3.6/codecs.py"", line 321, in decode
(result, consumed) = self._buffer_decode(data, self.errors, final)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p><strong>Extra Information</strong></p>

<p>I am using python 3 on MacOS High Sierra.</p>

<p><strong>Solutions</strong></p>

<p><em>Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a></em>
As pointed out by <a href=""https://stackoverflow.com/a/50579950?noredirect=1"">Aneesh Joshi</a>, the correct way to load Kyubyong's model is by calling the native load function of Word2Vec.</p>

<pre><code>from gensim.models import Word2Vec
kyu_path = '.../pre-trained_word_vectors/kyubyong_pt/pt.bin'
model = Word2Vec.load(kyu_path)
</code></pre>

<p>Even though I am more than grateful for Aneesh Joshi solution, polyglot seems to be a better model for working with Portuguese. Any ideas about that one?</p>
","8383446","","8383446","","2018-05-29 16:28:57","2020-04-19 12:57:13","UnicodeDecodeError error when loading word2vec","<python><word2vec><gensim><python-unicode><polyglot>","1","0","1","","","CC BY-SA 4.0"
"51092771","1","","","2018-06-29 00:30:12","","0","1096","<p>It has been suggested that initializing a topic model using clusters of words can lead to higher quality models or more robust (consistent) inference. I am talking about initializing the optimizer, not setting a prior. Here is some code to illustrate what I want to do:</p>

<p>Create an LdaModel object, but don't pass in a corpus. </p>

<pre><code>lda_model =
LdaModel(
         id2word=id2word,
         num_topics=30,
         eval_every=10,
         pass=40,
         iterations=5000)
</code></pre>

<p>Next assign some property of the object, corresponding to the probabilities of drawing each word from a topic to a matrix of my own construction. </p>

<pre><code>lda_model.topics = my_topic_mat
</code></pre>

<p>Then fit the corpus: </p>

<pre><code>lda_model.update(corpus)
</code></pre>

<p>Thanks for the help!</p>
","4241491","","4241491","","2018-06-30 20:28:56","2018-06-30 23:08:24","How can I initialize a gensim LDA topic model?","<python><gensim><topic-modeling>","1","3","","","","CC BY-SA 4.0"
"50516951","1","","","2018-05-24 19:36:38","","2","790","<p>My team has been given a task of looking at ~3,800 documents to see which are useful to be rebranded with our updated company branding guidelines.  It will take us forever, so I figured I would at least try to summarize the documents with Gensim, extract some keywords, and write the file name, summary, and keywords to a CSV. I'm doing this in the latest Jupyter Notebook using the Python 3 kernel.</p>

<p>I'm a python newbie, but I've done this in the past by looping through the directory and haven't had an issue.</p>

<p>This time after about 2 or so hours, the memory utilization starts creeping up to around 12GB and stays there for about an hour then returns down to normally fluctuationg between 100-300MB. Then an hour or two later the memory jumps back up and stays there and doesn't seem to process any more files...I left it running for 24 hours.  Being new to Python I'm not familiar with how to debug this and I'm not sure if it is because of my loop, some type of file encoding issues, or just a corrupt file or two in general.</p>

<p>Any help would be extremely greatful.  Code below.</p>

<pre><code>#Start java tika server before running

import os, sys, csv
import tika
from tika import parser
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
from gensim.summarization import summarize, keywords

# Set path to directory where files are
path = 'C:/Users/john/Desktop/WWS-Local' #directory has 3,800 files of multiple types
os.chdir(path)

exFileTypes = ('.zip', '.jpg', '.mp4', '.msg', '.oft', '.txt', '.png') #Don't care about these file types
with open('C:/Users/john/Desktop/winProcessedFiles.csv', 'w') as f:
    writer = csv.writer(f)
    writer.writerow(['File Name', 'Summary', 'Keywords'])
    for file in os.listdir('.'):
        if not file.endswith(exFileTypes):
            try:
                parsed = parser.from_file(file)
                text = parsed[""content""]
                text = text.strip('\n')
                text = text.encode('ascii','ignore').decode('ascii')
                summary = summarize(text, word_count=200)
                kw = keywords(text, words=15)
                writer.writerow([file, summary, kw])
            except Exception as e:
                print(file, e)
                writer.writerow([file, e, 'ERROR'])
            finally:
                pass
</code></pre>

<p>I started doing some basic logging and have been looking at my log file every so often.  My memory usage has spiked and it seems like the document being processed is taking an extremely long time.</p>

<p>I noticed this in the log:</p>

<p>INFO:root:102381-manufacturing-automotive-competitive-assessment-english-letter.pptx parsed in 10.553594589233398s
INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])
INFO:gensim.corpora.dictionary:adding document #10000 to Dictionary(2545 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:adding document #20000 to Dictionary(3386 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:adding document #30000 to Dictionary(3963 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:adding document #40000 to Dictionary(4335 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:adding document #50000 to Dictionary(4565 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:adding document #60000 to Dictionary(4736 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...)
INFO:gensim.corpora.dictionary:built Dictionary(4923 unique tokens: ['pictur', 'slide', 'titl', 'automot', 'manufactur']...) from 66286 documents (total 1009200 corpus positions)</p>

<p>Could this be the cause of the increased memory usage?  And why is it creating multiple dictionaries for this file when it looks like it only creates one dictionary for the other files?</p>
","6032082","","6032082","","2018-05-25 02:22:05","2018-05-25 02:22:05","Python - Extracting text and summarize multiple file types in directory with Tika and Gensim","<python-3.x><jupyter-notebook><gensim><apache-tika>","0","0","1","","","CC BY-SA 4.0"
"57903695","1","57910564","","2019-09-12 09:28:22","","0","784","<p>I want to use the read-only version of Gensim's FastText Embedding to save some RAM compared to the full model.</p>

<p>After loading the KeyVectors version, I get the following Error when fetching a vector:</p>

<p><code>IndexError: index 878080 is out of bounds for axis 0 with size 761210</code></p>

<p>The error occurs when using words that should be out-of-vocabulary e.g. ""lawyerxy"" instead of ""lawyer"". The full model returns a vector for both. </p>

<pre><code>from gensim.models import KeyedVectors
model = KeyedVectors.load(""model.kv"")
model .wv.__getitem__(""lawyerxy"")
</code></pre>

<p>So, my assumption is that the KeyedVectors do not offer FastText's out of vacabulary function - a key feature for my usecase. This limitation is not given in the documentation:
<a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a></p>

<p>Can anyone prove that assumption and/or name a fix to allow vectors for ""lawyerxy"" etc. ?</p>
","9032335","","","","","2019-09-12 15:59:50","Gensim's FastText KeyedVector out of vocab","<gensim><word-embedding><fasttext>","1","0","","","","CC BY-SA 4.0"
"66506701","1","","","2021-03-06 14:23:58","","0","13","<p>After running the follwing commands in terminal :</p>
<pre><code>import pickle 
f=open('deepwalk_node_vectors_rand_3.pkl','rb') 
unpackedlist=pickle.load(f) print(unpackedlist)
</code></pre>
<p>it printed a result in the terminal as given below:</p>
<pre><code>&lt;gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000021079E14D00&gt;
</code></pre>
<p>Can anyone help me to understand what it has generated?</p>
","11248139","","11248139","","2021-03-07 04:52:42","2021-03-07 04:52:42","How to open genesis modal keyvectors generated by pickling the file in python?","<python><vector><pickle><gensim><word2vec>","0","0","","","","CC BY-SA 4.0"
"51014463","1","51014994","","2018-06-24 22:25:09","","0","660","<p>What is the effect of assigning the same label to a bunch of sentences in doc2vec? I have a collection of documents that I want to learn vectors using gensim for a ""file"" classification task where file refers to a collection of documents for a given ID. I have several ways of labeling in mind and I want to know what would be the difference between them and which is the best - </p>

<ul>
<li><p>Take a document d1, assign label <code>doc1</code> to the tags and train. Repeat for others</p></li>
<li><p>Take a document d1, assign label <code>doc1</code> to the tags. Then tokenize document into sentences and assign label <code>doc1</code> to its tags and then train with both full document and individual sentences. Repeat for others</p></li>
</ul>

<p>For example (ignore that the sentence isn't tokenized) -</p>

<pre><code>Document -  ""It is small. It is rare"" 
TaggedDocument(words=[""It is small. It is rare""], tags=['doc1'])
TaggedDocument(words=[""It is small.""], tags=['doc1'])
TaggedDocument(words=[""It is rare.""], tags=['doc1'])
</code></pre>

<ul>
<li>Similar to above, but also assign a unique label for each sentence along with <code>doc1</code>. The full document has the all the sentence tags along with <code>doc1</code>.</li>
</ul>

<p>Example - </p>

<pre><code>Document -  ""It is small. It is rare"" 
TaggedDocument(words=[""It is small. It is rare""], tags=['doc1', 'doc1_sentence1', 'doc1_sentence2'])
TaggedDocument(words=[""It is small.""], tags=['doc1', 'doc1_sentence1'])
TaggedDocument(words=[""It is rare.""], tags=['doc1', 'doc1_sentence2'])
</code></pre>

<p>I also have some additional categorical tags that I'd be assigning. So what would be the best approach?</p>
","6832556","","","","","2018-06-25 00:27:46","Hierarchical training for doc2vec: how would assigning same labels to sentences of the same document work?","<python><nlp><word2vec><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"51132848","1","","","2018-07-02 09:25:08","","12","13649","<p>Is there a pre-trained doc2vec model with a large data set, like Wikipedia or similar?</p>
","10006087","","1270504","","2019-05-07 01:32:54","2019-05-07 01:32:54","Is there pre-trained doc2vec model?","<gensim><doc2vec>","2","0","1","","","CC BY-SA 4.0"
"58206571","1","58207763","","2019-10-02 17:39:01","","1","3918","<p>I am trying find similar sentence using doc2vec. What I am not able to find is actual sentence that is matching from the trained sentences.</p>
<p>Below is the code from <a href=""https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"" rel=""nofollow noreferrer"">this article</a>:</p>
<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
data = [&quot;I love machine learning. Its awesome.&quot;,
        &quot;I love coding in python&quot;,
        &quot;I love building chatbots&quot;,
        &quot;they chat amagingly well&quot;]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]
max_epochs = 100
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)
  
model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(&quot;d2v.model&quot;)
print(&quot;Model Saved&quot;)

model= Doc2Vec.load(&quot;d2v.model&quot;)
#to find the vector of a document which is not in training data
test_data = word_tokenize(&quot;I love building chatbots&quot;.lower())
v1 = model.infer_vector(test_data)
print(&quot;V1_infer&quot;, v1)

# to find most similar doc using tags
similar_doc = model.docvecs.most_similar('1')
print(similar_doc)


# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data
print(model.docvecs['1'])
</code></pre>
<p>But the above code only gives me vectors or numbers. But how can I get the actual sentence matched from training data. For Eg - In this case I am expecting the result as &quot;I love building chatbots&quot;.</p>
","646276","","472495","","2021-08-14 09:07:57","2021-08-14 09:08:12","Doc2Vec find the similar sentence","<python><nlp><gensim><doc2vec><sentence-similarity>","3","0","","","","CC BY-SA 4.0"
"51142294","1","","","2018-07-02 19:01:57","","1","491","<p>Been following the documentation <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">here</a> and as well as in this link: <a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""nofollow noreferrer"">Machine Learning Gensim Tutorial</a> and I'm at a complete loss for why this is happening. After tokenizing and lemmatizing my sentences, I put the sentences through a phraser, created a Dictionary, and inserted all the right variables into the model. Here is a sampling of my code:</p>

<pre><code>tokens =  [[euid, sent, gensim.parsing.preprocessing.preprocess_string(sent.lower(), filters=[strip_punctuation,
        strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, wordnet_stem])] for sent in sentences]
#these filters are all default gensim filters except for wordnet_stem, which uses a WordNetLemmatizer

 bigram = gensim.models.Phrases(bag_of_words)
bigram_mod = gensim.models.phrases.Phraser(bigram)
</code></pre>

<p>Sample token list looks like this: <code>['beautiful', 'Manager', 'tree', 'caring', 'great_place']</code> (completely made-up list)</p>

<pre><code>texts = [bigram_mod[t] for t in bag_of_words]
id2word = corpora.Dictionary(texts)
sent_wordfreq = [id2word.doc2bow(sent) for sent in texts]

lda_model = gensim.models.ldamodel.LdaModel(corpus=sent_wordfreq,
                                            id2word=id2word,
                                            num_topics=5,
                                            update_every=1,
                                            alpha='auto',
                                            per_word_topics=True)
</code></pre>

<p>Here are the topics I'm getting:</p>

<blockquote>
  <p>[(0, 'nan*""discovered"" + nan*""gained"" + nan*""send"" + ...
  (1, 'nan*""discovered"" + nan*""gained"" + nan*""send"" + ...
  and this continues on 3 more times</p>
</blockquote>

<p>So not only are all the topics the same, each's weight is nan. What could be the issue?</p>
","8250899","","8250899","","2018-07-02 19:18:52","2019-07-26 03:52:30","Gensim LDAmodel error: NaN and all topics the same","<python><pandas><nlp><gensim><lda>","1","0","1","","","CC BY-SA 4.0"
"58238043","1","58239907","","2019-10-04 14:08:54","","1","710","<p>After creating a FastText model using Gensim, I want to load it but am running into errors seemingly related to callbacks. </p>

<p>The code used to create the model is</p>

<pre class=""lang-py prettyprint-override""><code>TRAIN_EPOCHS = 30
WINDOW = 5
MIN_COUNT = 50
DIMS = 256

vocab_model = gensim.models.FastText(sentences=model_input,
                                     size=DIMS,
                                     window=WINDOW,
                                     iter=TRAIN_EPOCHS,
                                     workers=6,
                                     min_count=MIN_COUNT,
                                     callbacks=[EpochSaver(""./ftchkpts/"")])

vocab_model.save('ft_256_min_50_model_30eps')
</code></pre>

<p>and the callback <code>EpochSaver</code> is defined as</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.callbacks import CallbackAny2Vec

class EpochSaver(CallbackAny2Vec):
    '''Callback to save model after each epoch and show training parameters '''

    def __init__(self, savedir):
        self.savedir = savedir
        self.epoch = 0
        os.makedirs(self.savedir, exist_ok=True)

    def on_epoch_end(self, model):
        savepath = os.path.join(self.savedir, f""ft256_{self.epoch}e"")
        model.save(savepath)
        print(f""Epoch saved: {self.epoch + 1}"")
        if os.path.isfile(os.path.join(self.savedir, f""ft256_{self.epoch-1}e"")):
            os.remove(os.path.join(self.savedir,  f""ft256_{self.epoch-1}e""))
            print(""Previous model deleted "")
        self.epoch += 1
</code></pre>

<p>Aside from the type of model, this is identical to my process for Word2Vec which worked without issue. However when I open another file and try to load the model with</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import FastText
vocab = FastText.load(r'vocab/ft_256_min_50_model_30eps')
</code></pre>

<p>I'm greeted with the error</p>

<blockquote>
  <p><code>AttributeError: Can't get attribute 'EpochSaver' on &lt;module '__main__'&gt;</code></p>
</blockquote>

<p>What can I do to get the vocabulary to load so I can create the embedding layer for my keras model? If it's relevant, this is happening in JupyterLab.</p>
","379181","","379181","","2019-10-04 16:02:37","2019-10-04 16:08:37","Loading Gensim FastText Model with Callbacks Fails","<python><callback><gensim><jupyter-lab><fasttext>","1","0","","","","CC BY-SA 4.0"
"51160354","1","51161334","","2018-07-03 17:47:44","","1","854","<p>I used gensim to create a bag of words model. Although it is much longer in reality, here is the format outputted when creating a bag of words document-term matrix on the tokenized texts using Gensim:</p>

<pre><code>id2word = corpora.Dictionary(texts)
corpus = [id2word.doc2bow(text) for text in texts]

[[(0, 2),
  (1, 1),
  (2, 1),
  (3, 1),
  (4, 11),
  (385, 1),
  (386, 2),
  (387, 3),
  (388, 1),
  (389, 1),
  (390, 1)],
 [(4, 31),
  (8, 2),
  (13, 2),
  (16, 2),
  (17, 2),
  (26, 1),
  (28, 4),
  (29, 1),
  (30, 1)]]
</code></pre>

<p>This is a sparse matrix representation, and from what I understand other libraries represent the document-term matrix in a similar fashion as well. If the document-term matrix is non-sparse (meaning the zero entries are there as well), I know that I just have to (A.T*A), since A is of dimension (num. of documents by num. of terms), so multiplying the two will give the term co-occurrences. Ultimately, I want to get the top n co-occurrences (so get the top n term pairs that occur together in the same texts). How would I achieve this? I am not attached to Gensim for creating the BOW model. If another library like sklearn can do it more easily, I am very open. I would appreciate any advice/help/code with this problem -- thanks!</p>
","7375754","","","","","2018-07-04 18:40:37","Computing top n word pair co-occurrences from document term matrix","<python><matrix><scikit-learn><gensim><text-analysis>","1","0","","","","CC BY-SA 4.0"
"50579266","1","","","2018-05-29 08:02:10","","1","773","<p>According to the original paper <a href=""https://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">Distributed Representations of Sentences and Documents</a>, the inference on unseen paragraph can be done by</p>

<p><em>training ‚Äúthe inference stage‚Äù to get paragraph vectors <strong>D</strong> for new
paragraphs (never seen before) by adding more columns
in <strong>D</strong> and gradient descending on <strong>D</strong> while holding <strong>W, U, b</strong>
fixed</em></p>

<p>This <em>inference stage</em> can be done in gensim by <code>infer_vector()</code>.
If I have <code>window = 5</code> for doc2vec model, and attempts to infer paragraph with whose some sentences are <code>len(sentence) &lt; 5</code>.</p>

<p>such as :</p>

<p><code>model = Doc2Vec(window=5)
paragraph = [['I', 'am', 'groot'], ['I', 'am', 'groot', 'I', 'am', 'groot']]
model.infer_vector(paragraph)
</code></p>

<p>In this case, should I pre-pad my inferring vector with special NULL word symbol so that all length of sentences in the paragraph should be bigger than window size ? </p>

<p>such as :</p>

<p><code>paragraph = [['I', 'am', 'groot', NULL, NULL], ['I', 'am', 'groot', 'I', 'am', 'groot']]</code></p>
","7789561","","","","","2018-05-30 18:24:55","does doc2vec(gensim) infer_vector needs window-size padded sentence?","<gensim><doc2vec>","2","0","","","","CC BY-SA 4.0"
"66561475","1","","","2021-03-10 09:03:17","","0","40","<p>I have trained a FastText model in Python and saved the files into a folder. These are the contents of the folder:</p>
<pre><code>fasttext.model
fasttext.model.trainables.syn1neg.npy
fasttext.model.trainables.vectors_ngrams_lockf.npy
fasttext.model.trainables.vectors_vocab_lockf.npy
fasttext.model.wv.vectors.npy
fasttext.model.wv.vectors_ngrams.npy
fasttext.model.wv.vectors_vocab.npy
</code></pre>
<p>How can I load the model in MATLAB and extract the word embeddings of certain words?
This is what we do in Python:</p>
<pre><code>from gensim.models.fasttext import FastText
model = FastText.load(fasttext.model)
vector = model.wv[word]
</code></pre>
<p>Is there a similar thing in MATLAB? How can I get the word embeddings generated by a FastText model in Python in MATLAB and work with them?</p>
","14251114","","14251114","","2021-03-10 09:09:15","2021-07-07 03:48:11","loading a FastText model in MATLAB","<python><matlab><gensim><fasttext><language-model>","1","2","","","","CC BY-SA 4.0"
"68052035","1","","","2021-06-20 01:46:46","","0","19","<p>I have a vector of shape <code>[N,C,H,W] e.g. 12*384*38*25</code>. I want to convert it to non euclidean space such as on a hyperbolic or poincar√© space. But the tutorials online are quite confusing and I couldn't find any straightforward way to do it. There is a poincare model in the gensim library but  I am really confused how to use it.</p>
","4278798","","4278798","","2021-06-20 04:28:19","2021-06-20 04:28:19","Convert vector to non euclidean space such as Poincar√© space","<python><pytorch><gensim><hyperbolic-function>","0","0","","","","CC BY-SA 4.0"
"46157805","1","","","2017-09-11 14:12:49","","0","301","<p>I have created a doc2vec model of size of 100 dimensions. From what I understand from my reading that these dimensions are features of my model. How can I identify what these dimensions are exactly.</p>
","1897838","","","","","2017-09-11 17:51:30","Identify the dimensions in doc2vec model","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"58522356","1","","","2019-10-23 11:57:18","","-1","655","<p>I'm using anaconda enviroment python 3.7, gensim 3.8.0,  basically. I have my data as a dataframe tha tI separated in a test and training set, they both have this structure:</p>

<p>X_test and Xtrain dataframe format :</p>

<pre><code>        id                                            alltext  
1710  3264537  [exmodelo, karen, mcdougal, asegura, mantuvo, ...   
8211  3272079  [grupo, socialista, pionero, supone, apoyar, n...   
1885  3263933  [parte, entrenador, zaragoza, javier, aguirre,...   
2481  3263744  [fans, hielo, fuego, saga, literaria, dio, pie...   
2975  3265302  [actividad, busca, repetir, tres, ediciones, a... 
</code></pre>

<p>already preprocessed. </p>

<p>This is the code I use for creating my model</p>

<pre><code>id2word = corpora.Dictionary(X_train[""alltext""])   
texts = X_train[""alltext""]
corpus = [id2word.doc2bow(text) for text in texts]

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=20,
                                       random_state=100, 
                                       update_every=1, 
                                       chunksize=400, 
                                       passes=10, 
                                       alpha='auto',
                                       per_word_topics=True)enter code here
</code></pre>

<p>Until here, everything works fine. I can effectively use </p>

<pre><code>pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]
</code></pre>

<p>to get my topics.</p>

<p>The problem comes, when I try to compare similarity between a new document and the corpus. Here is the code I'm using</p>

<pre><code>newddoc = X_test[""alltext""][2730] #I get a particular instance of the test_set
new_doc_freq_vector = id2word.doc2bow(newddoc)  #vectorize its list of words
model_vec= lda_model[new_doc_freq_vector] #run the trained model on it
index = similarities.MatrixSimilarity(lda_model[corpus]) # error
sims = index[model_vec] #error
</code></pre>

<p>In the last two lines, I get this error: </p>

<pre><code>-------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-110-352248c464f8&gt; in &lt;module&gt;
      4 
      5 #index = Similarity('model/indexes/similarity_index_01', lda_model[corpus], num_features=len(id2word)) #the first argument, the place where the
----&gt; 6 index = similarities.MatrixSimilarity(lda_model[corpus]) # funciona si en vez de lda_model[corpus] usamos solo corpus
      7 index = similarities.MatrixSimilarity(model_vec)
      8 #sims = index[model_vec] #funciona si usamos index[new_doc_freq_vector] en vez de model_vec

~\AppData\Local\Continuum\anaconda3\envs\lda_henneo_01\lib\site-packages\gensim\similarities\docsim.py in __init__(self, corpus, num_best, dtype, num_features, chunksize, corpus_len)
    776                 ""scanning corpus to determine the number of features (consider setting `num_features` explicitly)""
    777             )
--&gt; 778             num_features = 1 + utils.get_max_id(corpus)
    779 
    780         self.num_features = num_features

~\AppData\Local\Continuum\anaconda3\envs\lda_henneo_01\lib\site-packages\gensim\utils.py in get_max_id(corpus)
    734     for document in corpus:
    735         if document:
--&gt; 736             maxid = max(maxid, max(fieldid for fieldid, _ in document))
    737     return maxid
    738 

~\AppData\Local\Continuum\anaconda3\envs\lda_henneo_01\lib\site-packages\gensim\utils.py in &lt;genexpr&gt;(.0)
    734     for document in corpus:
    735         if document:
--&gt; 736             maxid = max(maxid, max(fieldid for fieldid, _ in document))
    737     return maxid
    738 

ValueError: too many values to unpack (expected 2
</code></pre>

<p>No idea how to solve this, I have been trying to debug this for 3 hours now. , I believe I followed the same code many other people use fot getting similarity.</p>

<p>Things I have tried to solve this: </p>

<p>1)  Using </p>

<p><code>Similarity('model/indexes/similarity_index_01', lda_model[corpus], num_features=len(id2word))</code>.</p>

<p>But it did not work. Same error code was obtained.</p>

<p>2)  If I replace lda_model[corpus] with corpus, and index[model_vec] with index[new_doc_freq_vector], similarities.MatrixSimilarity() works. But I believe it does not give the proper result because, it does not have the model information in there. The fact that it works it tells me it has something to do with data types (?), if I print lda_model[corpus] I get </p>

<pre><code>&lt;gensim.interfaces.TransformedCorpus object at 0x00000221ECA8E148&gt;
</code></pre>

<p>no Idea what this means though. </p>
","9869300","","9869300","","2019-10-24 06:51:40","2019-11-10 14:47:41","Error ""too many values to unpack"" when trying to get similiraties in Gensim using LDA model","<python><gensim><similarity><recommendation-engine><lda>","2","0","","","","CC BY-SA 4.0"
"58286505","1","","","2019-10-08 12:36:41","","1","671","<p>I am a newbie in python and ML.
I found a nice script  (<a href=""https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/</a>) on how to get attributed topics to each document for LDA and I changed it to be able to use it with LSI as well.
The original code is:</p>

<pre><code>def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):
    # Init output
    sent_topics_df = pd.DataFrame()
    # Get main topic in each document
    for i, row_list in enumerate(ldamodel[corpus]):
        row = row_list[0] if ldamodel.per_word_topics else row_list            
        # print(row)
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # =&gt; dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = "", "".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
</code></pre>

<p>In order to use it for LSI, I changed it to:</p>

<pre><code>def format_topics_sentences_lsi(LsiModel=None, corpus=corpus, texts=data):
    """"""
    Extract all the information needed such as most predominant topic assigned to document and percentage of contribution
    LsiModel= model to be used
    corpus = corpus to be used
    texts = original text to be classify (for topic assignment)
    """"""
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row in enumerate(LsiModel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # =&gt; dominant topic
                wp = LsiModel.show_topic(topic_num)
                topic_keywords = "", "".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
</code></pre>

<ul>
<li>Is this the correct way?</li>
<li>As LSI is not based on probabilities the ""Perc_Contrib"" is above 100%. How should I interpret this number? </li>
<li>Apart from the script above, since LSI does not have get_document_topics, which function can I use to see the topic with the highest score?</li>
</ul>
","12182229","","","","","2019-10-08 12:36:41","How can I get the topic scores attributed to a document on gensim LSI?","<python><gensim><topic-modeling><latent-semantic-indexing>","0","0","1","","","CC BY-SA 4.0"
"51133162","1","","","2018-07-02 09:40:22","","0","1743","<p>I trained a w2v model on a big corpus, and I want to update it with a smaller one with new sentences (and new words).</p>

<p>In the first big training, I took the default parameters for alpha (0.025 with lin. decay  to 0.0001)
Now, I want to use <code>model.train</code> to update it. But from the doc I don't understand which (initial and final) learning rate will be used during this update of training.</p>

<p>From one side, if you also use 0.025 with lin. decay until 0.0001, it will be too strong for already existing words which appeared a lot in the first big corpus and that will be heavily changed, but from the other side for new words (added with model.build_vocab(sentences, update = True)) a low learning rate of 0.0001 is too small.</p>

<p>So my questions are : </p>

<ol>
<li>What is the default behaviour of <code>model.train</code> in the API on new sentences regarding the learning rate?</li>
<li><p>How I should choose the learning rate in order to take into account this issue of old/new words ?</p></li>
<li><p>[aside question] Why when I use 2 times model.train on the same sentences, the second time, it doesn't update the vectors ?</p></li>
</ol>
","10020702","","10020702","","2018-07-02 09:46:27","2018-07-02 15:17:31","word2vec gensim update learning rate","<python><machine-learning><word2vec><gensim>","1","0","1","","","CC BY-SA 4.0"
"51150702","1","","","2018-07-03 09:10:04","","0","445","<p>I am trying to find the dissimilarity between the two documents. I am using gensim and so far have obtained similarity score. </p>

<p>Is there any way to know the dissimilarity score and dissimilar features between two documents?
And how to evaluate it?</p>
","10025735","","1302488","","2019-09-02 12:18:27","2019-09-02 12:18:27","Dissimilar Features between two documents","<nlp><nltk><gensim><cosine-similarity>","1","2","1","","","CC BY-SA 4.0"
"51157395","1","","","2018-07-03 14:44:59","","1","589","<p>I am having a 1-word document that I want to transform to its bag-of-words representation:</p>

<p>so <code>doc</code> is <code>['party']</code> and <code>id2word.doc2bow(doc)</code> is <code>[(229, 1)]</code> which means the word is known.</p>

<p>However, if I call <code>get_document_topics()</code> with <code>doc_bow</code>, the result is an <em>empty</em> list:</p>

<pre class=""lang-py prettyprint-override""><code>id2word = lda.id2word 

# ..

doc_bow = id2word.doc2bow(doc)

t = lda.get_document_topics(doc_bow)

try:
    label, prob = sorted(t, key=lambda x: -x[1])[0]
except Exception as e:
    print('Error!')
    raise e
</code></pre>

<p>The only possible explanation I'd have here is that this document (the single word) cannot be assigned to <em>any</em> topic. Is this the reason why I am seeing this?</p>
","826983","","","","","2018-12-24 05:44:44","get_document_topics() returns empty list of topics","<gensim>","1","1","","","","CC BY-SA 4.0"
"50583937","1","","","2018-05-29 12:03:00","","5","528","<p>A few papers on the topics of word and document embeddings (word2vec, doc2vec) mention that they used the Stanford CoreNLP framework to tokenize/lemmatize/POS-tag the input words/sentences:</p>

<blockquote>
  <p>The  corpora  were  lemmatized and POS-tagged with the Stanford CoreNLP (Manning  et  al.,  2014)  and  each  token  was  replaced with its lemma and POS tag</p>
</blockquote>

<p>(<a href=""http://www.ep.liu.se/ecp/131/039/ecp17131039.pdf"" rel=""noreferrer"">http://www.ep.liu.se/ecp/131/039/ecp17131039.pdf</a>)</p>

<blockquote>
  <p>For pre-processing, we tokenise and lowercase the words using Stanford CoreNLP</p>
</blockquote>

<p>(<a href=""https://arxiv.org/pdf/1607.05368.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/1607.05368.pdf</a>)</p>

<p>So my questions are:</p>

<ul>
<li><p>Why does the first paper apply POS-tagging? Would each token then be replaced with something like <code>{lemma}_{POS}</code> and the whole thing used to train the model? Or are the tags used to filter tokens? 
For example, gensims WikiCorpus applies lemmatization per default and then only keeps a few types of part of speech (verbs, nouns, etc.) and gets rid of the rest. So what is the recommended way?</p></li>
<li><p>The quote from the second paper seems to me like they only split up words and then lowercase them. This is also what I first tried before I used WikiCorpus. In my opinion, this should give better results for document embeddings as most of POS types contribute to the meaning of a sentence. Am I right?</p></li>
</ul>

<p>In the original doc2vec paper I did not find details about their pre-processing.</p>
","3827381","","","","","2018-05-29 12:03:00","NLP: Pre-processing in doc2vec / word2vec","<nlp><stanford-nlp><word2vec><gensim><doc2vec>","0","1","1","","","CC BY-SA 4.0"
"51042031","1","","","2018-06-26 11:39:53","","1","41","<p>Our company has a lot of data that are issue which are stored in a database.We want to create a search engine so that people can check how the issues were previously dealt with.We cannot use any 3rd party api as there is sensitive data an we want to keep it as in house. Right now the approach is as following :- </p>

<ol>
<li>Clean up the data and then use a DOC2VEC to represent each issue as a vector .</li>
<li>Find the closest 5 issue using some distance metric.</li>
</ol>

<p>The problem is that the results are not at all useful.The problem is most of the data is one liner and some issue description.There are spelling mistakes and stack traces and other things. </p>

<p>Is this the right approch or should we switch to something else?
Right now we are testing on 200K data.
Thanks for the help.</p>
","3802935","","","","","2018-06-26 11:39:53","Trying to make a search engine for issues","<machine-learning><word2vec><gensim><information-retrieval><doc2vec>","0","2","0","","","CC BY-SA 4.0"
"41936775","1","41936843","","2017-01-30 13:10:29","","19","14605","<p>i am analysing text with topic modelling and using Gensim and pyLDAvis for that. Would like to share the results with distant colleagues, without a need for them to install python and all required libraries. 
Is there a way to export interactive graphs as HTML/JS files that could be uploaded to any web server?
I've found something mentioned in documentation, but have no idea how to implement it:
<strong><a href=""https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/_display.py"" rel=""noreferrer"">https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/_display.py</a></strong></p>
","2838794","","","","","2021-04-14 18:00:58","Export pyLDAvis graphs as standalone webpage","<python><gensim><lda><topic-modeling>","1","0","5","","","CC BY-SA 3.0"
"58253405","1","58261123","","2019-10-06 00:41:33","","1","1267","<p>I'm trying to use <a href=""https://github.com/idio/wiki2vec/"" rel=""nofollow noreferrer"">this</a> 1000 dimension wikipedia word2vec model to analyze some documents.</p>

<p>Using introspection I found out that the vector representation of a word is a 1000 dimension numpy.ndarray, however whenever I try to create an ndarray to find the nearest words I get a value error:</p>

<pre><code>ValueError: maximum supported dimension for an ndarray is 32, found 1000
</code></pre>

<p>and from what I can tell by looking around online 32 is indeed the maximum supported number of dimensions for an ndarray - so what gives? How is gensim able to output a 1000 dimension ndarray?</p>

<p>Here is some example code:</p>

<pre><code>doc = [model[word] for word in text if word in model.vocab]
out = []
n = len(doc[0])
print(n)
print(len(model[""hello""]))
print(type(doc[0]))
for i in range(n):
    sum = 0
    for d in doc:
        sum += d[i]
    out.append(sum/n)
out = np.ndarray(out)
</code></pre>

<p>which outputs:</p>

<pre><code>1000
1000
&lt;class 'numpy.ndarray'&gt;
ValueError: maximum supported dimension for an ndarray is 32, found 1000
</code></pre>

<p>The goal here would be to compute the average vector of all words in the corpus in a format that can be used to find nearby words in the model so any alternative suggestions to that effect are welcome.</p>
","4153995","","","","","2019-10-06 22:13:12","Gensim word2vec model outputs 1000 dimension ndarray but the maximum number of ndarray dimensions is 32 - how?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"51222901","1","","","2018-07-07 11:47:14","","0","72","<p>I trained my <code>word2vec</code> model using the <code>gensim</code> package on <strong>6.4 GB text data</strong> which is preprocessed using the following code snippet :</p>

<pre><code>def read_input(input_file):
  with open(input_file, ""r"") as inp:
    inp_str = inp.read()

  inp_str = inp_str.strip('\n')
  inp_str = re.sub(r'\n', ' ', inp_str)
  lowercase = inp_str.lower()
  punc = lowercase.translate(str.maketrans('', '', string.punctuation))

  return (punc.translate(str.maketrans('','','1234567890')))

def read_(input_file):
  return( gensim.utils.simple_preprocess(input_file, deacc=True, min_len=3))          

doc = read_input('../train1.txt')
documents = read_(doc)
logging.info (""Done reading data file"")
</code></pre>

<p>But every time I  train the model, its size is <strong>147 Kb</strong> which doesn't seems right and when I tried generating vectors from the trained model it says :</p>

<pre><code>KeyError: ""word 'type' not in vocabulary""
</code></pre>

<p>The following is the code I used for training my word2vec model :</p>

<pre><code>old_model = Word2Vec.load('../word2vec_model')
old_model.train(documents, total_examples=old_model.corpus_count, epochs=7)

old_model.save('../word2vec_model1')

logging.info (""Saved the new word2vec model"")
</code></pre>

<p>Please help me resolving this issue.</p>
","7482867","","7482867","","2018-07-07 12:54:32","2018-07-07 14:09:46","Word2vec Model size is very small and it is not recognizing words","<python><python-3.x><word2vec><gensim><word-embedding>","1","0","2","","","CC BY-SA 4.0"
"50521304","1","","","2018-05-25 03:57:03","","1","289","<p>I'm trying to cluster some descriptions using LSI. As the dataset that I have is too long, I'm clustering based on the vectors obtained from the models instead of using the similarity matrix, which requires too much memory, and if I pick a sample, the matrix generated doesn't correspond to a square (this precludes the use of MDS).</p>

<p>However, after running the model and looking for the vectors I'm getting different vector's lengths in the descriptions. Most of them have a length of 300 (the num_topics argument in the model), but some few, with the same description, present a length of 299.</p>

<p>Why is this happening? Is there a way to correct it?</p>

<pre><code>dictionary = gensim.corpora.Dictionary(totalvocab_lemmatized)
dictionary.compactify()

corpus = [dictionary.doc2bow(text) for text in totalvocab_lemmatized]

###tfidf model
tfidf = gensim.models.TfidfModel(corpus, normalize = True)
corpus_tfidf = tfidf[corpus]

###LSI model
lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)
vectors =[]
for n in lemmatized[:100]:
    vec_bow = dictionary.doc2bow(n)
    vec_lsi = lsi[vec_bow]
    print(len(vec_lsi))
</code></pre>
","7392900","","7392900","","2018-05-25 05:48:20","2019-05-27 15:30:46","Why I get different length of vectors using gensim LSI model?","<python><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"66488606","1","","","2021-03-05 07:44:15","","0","20","<p>I am using the &quot;.wv.similar_by_vector&quot; method, I want to compute both by Euclidean distance and cosine distance separately but can't find a flag to do so.</p>
<p>This is what I did</p>
<pre><code>list_nn = model.wv.similar_by_vector(vec, topn=10, restrict_vocab=None)
</code></pre>
<p>How can I change it to Euclidean distance?</p>
<p>*Judging by the results I conclude that it computes the cosine distance.</p>
<p>Thanks!</p>
","14385885","","","","","2021-03-05 07:44:15","how to control the distance metric in gensim ""similar_by_vector"" method","<gensim><nearest-neighbor><word-embedding>","0","0","","","","CC BY-SA 4.0"
"68179160","1","","","2021-06-29 12:58:27","","0","24","<p>I have read a good bit of LDA theory and am now learning to implement LDA in Gensim. I am aware that there are various options for setting the hyper-parameters $\alpha$ and $\eta$, including 'symmetric' and 'auto.' Choosing 'auto' means that Gensim will use Newton-Raphson updates (as described in <a href=""http://www.cs.columbia.edu/%7Eblei/papers/BleiNgJordan2003.pdf"" rel=""nofollow noreferrer"">Blei <em>et al.</em> (2003)</a> and <a href=""http://jonathan-huang.org/research/dirichlet/dirichlet.pdf"" rel=""nofollow noreferrer"">Huang</a>) in its variational inference algorithm to learn optimal values of that hyperparameter from the data. However, it seems that these updates are implemented only in the asymmetric dirichlet prior case - that is, where $\alpha$ and $\eta$ are vectors of length K (# of topics) and V (# of tokens in vocabulary) respectively rather than scalars. That is fine for $\alpha$, but I would like for the algorithm to use Newton-Raphson updates to learn a symmetric $\eta$ prior (this is recommended in <a href=""https://mimno.infosci.cornell.edu/papers/NIPS2009_0929.pdf"" rel=""nofollow noreferrer"">Wallach, Mimno, and McCallum</a>). Does anyone know if there is any way to do this in Gensim?</p>
<p>It seems to me that it should be possible in theory as I was able to derive symmetric case updates using the same approach used to derive the asymmetric case updates. If there is not a way to do this in Gensim, why not? Can anyone provide reasoning for why this feature would be unnecessary or undesirable?</p>
<p>An additional note: for either $\eta$ or $\alpha$ the 'symmetric' option sets the parameter equal to 1/K. In the case of $\eta$ for the V-word topics, this seems like a strange choice of default to me. As a secondary question, can anyone explain this choice?</p>
<p>Note: documentation for the code I'm describing is <a href=""https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.top_topics"" rel=""nofollow noreferrer"">here</a></p>
","16337512","","16337512","","2021-06-29 17:00:34","2021-06-29 17:00:34","Is it possible to learn symmetric priors via Newton-Raphson in the Gensim LDA implementation?","<gensim><lda><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"51105753","1","51133594","","2018-06-29 16:10:47","","0","3994","<p>I use the gensim library to create a word2vec model. It contains the function <code>predict_output_words()</code> which I understand as follows:</p>

<p>For example, I have a model that is trained with the sentence: ""Anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy.""</p>

<p>and then I use </p>

<p><code>model.predict_output_words(context_words_list=['Anarchism', 'does', 'not', 'offer', 'a', 'fixed', 'body', 'of', 'from', 'a', 'single', 'particular', 'world', 'view', 'instead', 'fluxing'], topn=10)</code>.</p>

<p>In this situation, could I get/predict the correct word or the omitted word 'doctrine'?</p>

<p>Is this the right way? Please explain this function in detail.</p>
","10011819","","4503898","","2018-06-29 18:49:25","2018-07-02 10:02:17","gensim function predict output words","<python><tensorflow><nlp><word2vec><gensim>","1","0","1","","","CC BY-SA 4.0"
"51135118","1","","","2018-07-02 11:26:43","","0","355","<p>Regarding word2vec with gensim,
Suppose you already trained a model on a big corpus, and you want to update it with new words from new sentences, but not update the words which already have a vector.
Is it possible to freeze the vectors of some words and update only some chosen words (like the new words) when calling <code>model.train</code> ?
Or maybe is there a trick to do it ?
Thanks.</p>
","10020702","","","","","2021-05-28 16:03:07","Gensim Word2vec Freeze some wordvectors and Update others","<python><word2vec><gensim>","1","0","","","","CC BY-SA 4.0"
"58245826","1","","","2019-10-05 06:08:32","","0","505","<p>I have already pretrained word2vec in gensim. In keras , I want to  use Word vector for word get from pretrained word2vec combined with that word's POS tag feature that i encode in one hot vector. In Keras, I  think use embedding matrix  So, I want to make embedding layer in Keras to achieve this so that It can be used in further layers(LSTM). Can you tell me in detail how to do this?</p>
","10154032","","","","","2019-12-01 03:21:55","How to combine POS tag feature with associated word vector for word get from Pretrained gensim word2vec ans use in embedding layer in keras","<python-3.x><keras><gensim>","1","0","","","","CC BY-SA 4.0"
"41690885","1","41696259","","2017-01-17 07:05:39","","2","1863","<p>There is Convolution1D example <a href=""https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py"" rel=""nofollow noreferrer"">https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py</a> without word2vec.</p>

<p>Currently, I am using gensim to train word2vec model.</p>

<p>I want to use word2vec and keras cnn(2D not 1D) to do document classifacation(Chinese Text). I learned the basic flow of text classification in cnn and want to do a test.</p>

<h2>For example(the steps I imagine):</h2>

<ol>
<li><p>Use a good Cinese Tokenized Text Set to train word2vec model</p>

<pre><code>model = gensim.models.Word2Vec(new_sentences, workers=10, size=200, min_count=2)
</code></pre></li>
<li><p>Tokenize my sentences dataset to words lists dataset(the longest sentence has over 8000 words, shortest is less 50)</p>

<pre><code>1     ['‰Ω†‰ª¨', 'Â•Ω', '‰ªäÂ§©', 'Â§©Ê∞î', 'Áúü', 'Â•Ω']
2     ['ÂóØ', 'ÂØπÁöÑ']
...
9999  ['Â•Ω', 'Â∞±', 'ËøôÊ†∑']
</code></pre></li>
<li><p>Use a method to transform words lists dataset to word2vec dataset</p>

<p>transform every word in every sencence to a vec by trained model.</p>

<pre><code>1     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
2     [[word2vec size=200], [word2vec size=200]]
...
9999  [[word2vec size=200], [word2vec size=200], [word2vec size=200]]
</code></pre></li>
<li><p>Pad  word2vec dataset (with size=200 zero array)</p>

<pre><code>1     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
2     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
....
9999  [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
</code></pre></li>
<li><p>go to the  CNN (using Convolution2D)</p></li>
</ol>

<hr>

<p>I search for a long time, but can't find any way to do the step 3 ( after step 3,  the parameter and layers setting in step 5 is hard to understand too). </p>
","1637673","","","","","2017-01-17 11:43:53","How to use word2vec with keras CNN (2D) to do text classification?","<neural-network><deep-learning><keras><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"58301450","1","58311363","","2019-10-09 09:49:46","","0","113","<p>I just want to be able to see the values in my word2vec model.</p>

<p>I have a  very small corpus. I just want to see exactly what happens in each step for this particular corpus.</p>

<p>A section of my code is below.</p>

<pre class=""lang-py prettyprint-override""><code>word2vec = Word2Vec(corpus, min_count=1)
word_vectors = word2vec.wv 

termsim_index = WordEmbeddingSimilarityIndex(word_vectors)


dictionary = corpora.Dictionary(food)
bow_corpus = [dictionary.doc2bow(doc) for doc in food]


similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)  
docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=10)

</code></pre>

<p>So I want to see what exactly is in <code>word_vectors</code>,<code>termsim_index</code>,<code>similarity_matrix</code> , <code>docsim_index</code></p>
","6302043","","","","","2019-10-09 19:53:26","How to view word2vec model","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"41689525","1","","","2017-01-17 05:18:53","","1","2266","<p>I am learning about <strong>Word2Vec</strong> and <strong>GloVe</strong> model in python so I am going through this getting started with <strong>GENSIM</strong> available <a href=""http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python"" rel=""nofollow noreferrer"">here</a>.</p>

<p>After I compiled these code step by step in Idle3:</p>

<pre><code>from gensim.models import word2vec
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
sentences = word2vec.Text8Corpus('text8')
sentences = word2vec.Text8Corpus('~/Desktop/text8')
model = word2vec.Word2Vec(sentences, size=200)
model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
model.most_similar(positive=['woman', 'king'], negative=['man'], topn=2)
model.most_similar(['man'])
model.save('text8.model')
model.save_word2vec_format('text.model.bin', binary=True)
model1 = word2vec.Word2Vec.load_word2vec_format('text.model.bin', binary=True)
model1.most_similar(['girl', 'father'], ['boy'], topn=3)
more_examples = [""he is she"", ""big bigger bad"", ""going went being""]
for example in more_examples:
    a, b, x = example.split()
    predicted = model.most_similar([x, b], [a])[0][0]
    print (""'%s' is to '%s' as '%s' is to '%s'"" % (a, b, x, predicted))
model_org = word2vec.Word2Vec.load_word2vec_format('vectors.bin', binary=True)
</code></pre>

<p>I am getting this error: </p>

<pre><code>2017-01-17 10:34:26,054 : INFO : loading projection weights from vectors.bin
Traceback (most recent call last):
  File ""&lt;pyshell#16&gt;"", line 1, in &lt;module&gt;
    model_org = word2vec.Word2Vec.load_word2vec_format('vectors.bin', binary=True)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 1172, in load_word2vec_format
    with utils.smart_open(fname) as fin:
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 127, in smart_open
    return file_smart_open(parsed_uri.uri_path, mode)
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 558, in file_smart_open
    return open(fname, mode)
FileNotFoundError: [Errno 2] No such file or directory: 'vectors.bin'
</code></pre>

<p>How do I rectify this. Where can I get the vector.bin file. 
Thanks for your help in advance.</p>
","","user7399214","","","","2017-02-09 08:12:02","Gensim getting started Error : No such file or directory: 'vectors.bin'","<python><error-handling><gensim><word2vec>","1","1","1","","","CC BY-SA 3.0"
"68195634","1","","","2021-06-30 13:22:07","","0","28","<p>I am training an LDA model. While I obtain decently interpretable topics (based on the top words), particular documents tend to load heavily on very &quot;generic&quot; topics rather than specialized ones -- even though the most frequent words in the document are specialized.</p>
<p>For example, I have a real estate report as a document. Top words by frequency are &quot;rent&quot;, &quot;reit&quot;, &quot;growth&quot;. Now, I have a &quot;specialized&quot; topic with top words being exactly those three. However, the loading of the specialized topic is 9%, and 32% goes to a topic which is very diffuse and the top words are rather common.</p>
<p>How can I increase the weight of &quot;specialized&quot; topics? Is it possible to truncate topics such that I only include the top 10 words and assign zero probability to anything else? Is it desirable to do so?</p>
<p>I am using the <code>gensim</code> package. Thank you!</p>
","15230939","","","","","2021-07-02 14:18:02","Truncate LDA topics","<nlp><gensim><lda>","2","0","","","","CC BY-SA 4.0"
"68201948","1","","","2021-06-30 21:48:51","","0","22","<p>I am using the Gensim KeyedVector most_similar function to find the top N word vectors similar to a subset of other word vectors. My model vocabulary is such that the top X are the word vectors I would like to <em>search through and return</em>, and indices X+1 and greater contain the words that I am searching <em>with</em>. I.e., when I search, I would not like to search through anything beyond index X.</p>
<p>I am using the restrict_vocab=X flag to only search through the top X of the vocab. I noticed that some of the nearest neighbors that are being returned are NOT from the top X and was wondering if anyone had any insights into what is going on here?</p>
<p>Specifically, I am using a version with an annoy indexer and a version without:</p>
<pre><code>neighbors = word_vec_model.most_similar(positive=[root_word], topn=N, restrict_vocab=X)
</code></pre>
<p>and</p>
<pre><code>neighbors = word_vec_model.most_similar(positive=[root_word], topn=N, restrict_vocab=X, indexer=annoy_index)
</code></pre>
<p>I verified that a word vector that is being returned is greater than X by getting the index in the vocab:</p>
<pre><code>list(word_vec_model.vocab.keys()).index(returned_neighbor)
</code></pre>
<p>and saw that it was a number greater than X.</p>
<p>EDIT:</p>
<pre><code>print(word_vec_model.vocab[returned_neighbor].index)
</code></pre>
<p>returned the same number as above.</p>
<p>I created my word_vec_model with the following line of code:</p>
<pre><code>word_vec_model = KeyedVectors.load_word2vec_format('embeddings.txt', binary=False)
</code></pre>
<p>where each line in the file is a keyword and a 512 dimensional vector. The file is formatted such that the first X are my <em>search through</em> words, and the X+1 and greater lines are the <em>search with</em> words. I expected the words to be read in in order and the .vocab attribute confirms this.</p>
","5504505","","5504505","","2021-07-01 19:08:56","2021-07-01 19:08:56","Argument restrict_vocab from Gensim KeyedVector most_similar not working as expected","<python><gensim><word-embedding>","0","4","","","","CC BY-SA 4.0"
"59863559","1","","","2020-01-22 15:53:08","","0","39","<p>I'm trying to predict big 5 personality traits (Extraversion, Neuroticism,  Agreeableness,  Conscientiousness, Openness) based on text analysis written by user. 
Here is my preprocessed data set:</p>

<p><img src=""https://i.stack.imgur.com/N6QHO.png"" alt=""df_processed""></p>

<p>And here is my word2vec model:</p>

<pre><code>model_wv = Word2Vec(df_processed['TEXT'], sg=1, size=300, window=10, min_count=1)
</code></pre>

<p>Vocabulary consists of 26126 words. </p>

<pre><code>features = ['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']
X = df_processed['TEXT']
y = df_processed[features]
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)

model = Sequential()
model.add(Embedding(max_features, 100)) 
model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(5, activation='softmax'))

model.compile(loss='binary_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs, validation_data=(X_test, y_test))
</code></pre>

<p>My question is, how can I use word2vec vectors in tensorflow model? </p>

<p>Should I replace every word in every row in ""X"" with the vector from word2vec model? I think it will be quite expensive for calculation but what will be another possibility? </p>

<p>Sorry if my questions sounds dummy, I' m just really new in NLP and tensorflow.</p>
","9342514","","3043944","","2020-01-22 17:01:36","2020-01-22 17:01:36","How to use Word2Vec for classification problem in Tensorflow","<python><tensorflow><nlp><gensim><word2vec>","0","2","","","","CC BY-SA 4.0"
"68222210","1","","","2021-07-02 08:45:20","","0","11","<p>I have a problem with calculating MatrixSimilarity with Gensim:</p>
<pre class=""lang-py prettyprint-override""><code>import copy
import math
import sys
from typing import List

import preprocessing_text
import spacy
from sortedcollections.ordereddict import SortedDict
import nltk.tokenize
from gensim import corpora, models, similarities

def build_similarity_matrix(texts:List[str]):
    '''restituisce la matrice di similarit√†'''
    texts_tokenized = [nltk.word_tokenize(text,LINGUA)  for text in texts]
    dictionary = corpora.Dictionary(texts_tokenized)
    feature_cnt = len(dictionary.token2id)
    corpus = [dictionary.doc2bow(text) for text in texts_tokenized]
    tfidf = models.TfidfModel(corpus)
    index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=feature_cnt)[corpus]
    return index

def test_matrix_similarity():
    text1 = &quot;&quot;&quot;Apple Inc. is an American multinational technology company that specializes in consumer electronics, computer software, and online services. Apple is the world's largest technology company by revenue.&quot;&quot;&quot;
    text2 = &quot;&quot;&quot;World War I or the First World War, often abbreviated as WWI or WW1, was a global war originating in Europe that lasted from 28 July 1914 to 11 November 1918. Contemporaneously known as the Great War or &quot;the war to end all wars\&quot;&quot;&quot;&quot;
    matrice = build_similarity_matrix([text1, text2])
    print(matrice)

if __name__==&quot;__main__&quot;:
    test_matrix_similarity()
</code></pre>
<p>Result:</p>
<pre><code>[[0.82065195 0.        ]
 [0.         0.8900606 ]]
</code></pre>
<p>Shouldn't they all have 1's on the diagonal? Why is this not the case here?</p>
","16068995","","16068995","","2021-07-11 08:24:11","2021-07-11 08:24:11","MatrixSimilarity doesn't have 1's on the diagonal","<python><matrix><gensim><similarity><cosine-similarity>","0","1","","","","CC BY-SA 4.0"
"58278111","1","","","2019-10-07 22:52:36","","0","420","<p>I have a TF Estimator that uses Feature Columns at its input layer. One of these is and <code>EmbeddingColumn</code> which I have been initializing randomly (the default behaviour). </p>

<p>Now I would like to pre-train my embeddings in gensim and transfer the learned embeddings into my TF model. The <code>embedding_column</code> accepts an initializer argument which expects a callable that can be <a href=""https://stackoverflow.com/questions/51237419/feature-column-pre-trained-embedding"">created</a> using <code>tf.contrib.framework.load_embedding_initializer</code>.</p>

<p>However, that function expects a saved TF checkpoint, which I don't have, because I trained my embeddings in gensim.</p>

<p>The question is: how do I save gensim word vectors (which are numpy arrays) as a tensor in the TF checkpoint format so that I can use that to initialize my embedding column?</p>
","4936825","","4936825","","2019-10-14 01:27:12","2019-10-14 01:27:12","Importing pre-trained embeddings into Tensorflow's Embedding Feature Column","<tensorflow><gensim><word2vec><transfer-learning>","1","0","1","","","CC BY-SA 4.0"
"59823688","1","","","2020-01-20 12:51:43","","1","696","<p>Trying to build gensim word2vec model. Corpus contains 1 M sentences.
I am using callback to print loss after every epochs. After few epochs loss becomes zero.
Any idea why loss becomes 0?</p>

<pre><code>Loss after epoch 0: 17300302.0
Loss after epoch 1: 11381698.0
Loss after epoch 2: 8893964.0
Loss after epoch 3: 7105532.0
           ...
           ...
           ...
Loss after epoch 54: 1283432.0
Loss after epoch 55: 1278048.0
Loss after epoch 56: 316968.0
Loss after epoch 57: 0.0
Loss after epoch 58: 0.0
Loss after epoch 59: 0.0
Loss after epoch 60: 0.0
Loss after epoch 61: 0.0
Loss after epoch 62: 0.0
Loss after epoch 63: 0.0
Loss after epoch 64: 0.0
Loss after epoch 65: 0.0
Loss after epoch 66: 0.0
</code></pre>
","2179376","","6573902","","2020-01-20 15:22:03","2020-01-27 10:04:08","Gensim word2vec model loss becomes 0 after few epochs","<deep-learning><gensim><word2vec><loss-function>","1","1","","","","CC BY-SA 4.0"
"59845191","1","59847592","","2020-01-21 16:17:13","","1","50","<p>I plan to use NLTK, Gensim and Scikit Learn for some NLP/text mining. But i will be using these libraries to work with my org data. The question is while using these libraries 'do they make API calls to process the data' or is the data taken out of the python shell to be processed. It is a security question, so was wondering if someone has any documentation for reference.</p>

<p>Appreciate any help on this.</p>
","10481278","","","","","2020-01-22 21:22:25","API calls from NLTK, Gensim, Scikit Learn","<python><api><nlp><nltk><gensim>","1","0","","","","CC BY-SA 4.0"
"68218550","1","68219012","","2021-07-02 01:17:29","","0","63","<p>I trained two versions of doc2vec models with two datasets.</p>
<p>The first dataset was made with 2400 documents and the second one was made with 3000 documents including the documents which were used in the first dataset.</p>
<p>For an example,</p>
<p>dataset 1 = doc1, doc2, ... doc2400</p>
<p>dataset 2 = doc1, doc2, ... doc2400, doc2401, ... doc3000</p>
<p>I thought that both doc2vec models should return the same similarity score between doc1 and doc2, however, they returned different scores.</p>
<p>Does doc2vec model's result change upon the datasets even they include the same documents?</p>
","15411718","","","","","2021-07-02 02:46:14","Gensim Doc2Vec model returns different cosine similarity depending on the dataset","<gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"67253213","1","67253427","","2021-04-25 12:06:15","","0","128","<p>I am implementing a simple <code>doc2vec</code> with <code>gensim</code>, <strong>not</strong> a <code>word2vec</code></p>
<p>I need to remove stopwords without losing the correct order to a list of list.</p>
<p>Each list is a document and, as I understood for doc2vec, the model will have as input a list of TaggedDocuments</p>
<p><code>model = Doc2Vec(lst_tag_documents, vector_size=5, window=2, min_count=1, workers=4)</code></p>
<pre><code>dataset = [['We should remove the stopwords from this example'],
     ['Otherwise the algo'],
     [&quot;will not work correctly&quot;],
     ['dont forget Gensim doc2vec takes list_of_list' ]]

STOPWORDS = ['we','i','will','the','this','from']


def word_filter(lst):
  lower=[word.lower() for word in lst]
  lst_ftred = [word for word in lower if not word in STOPWORDS]
  return lst_ftred

lst_lst_filtered= list(map(word_filter,dataset))
print(lst_lst_filtered)
</code></pre>
<p>Output:</p>
<pre><code>[['we should remove the stopwords from this example'], ['otherwise the algo'], ['will not work correctly'], ['dont forget gensim doc2vec takes list_of_list']]
</code></pre>
<p>Expected Output:</p>
<pre><code>[[' should remove the stopwords   example'], ['otherwise the algo'], [' not work correctly'], ['dont forget gensim doc2vec takes list_of_list']]
</code></pre>
<hr />
<ul>
<li><p><strong>What was my mistake and how to fix?</strong></p>
</li>
<li><p><strong>There are other efficient ways to solve this issue without losing the
proper order?</strong></p>
</li>
</ul>
<hr />
<p>List of questions I examined before asking:</p>
<p><a href=""https://stackoverflow.com/questions/30060970/how-to-apply-a-function-to-each-sublist-of-a-list-in-python"">How to apply a function to each sublist of a list in python?</a></p>
<ul>
<li>I studied this and tried to apply on my specific case</li>
</ul>
<p><a href=""https://stackoverflow.com/questions/65413876/removing-stopwords-from-list-of-lists"">Removing stopwords from list of lists</a></p>
<ul>
<li>The order is important I can't use set</li>
</ul>
<p><a href=""https://stackoverflow.com/questions/42042163/removing-stopwords-from-a-list-of-text-files"">Removing stopwords from a list of text files</a></p>
<ul>
<li>This could be a possible solution is similar to what I have implemented.</li>
<li>I undestood that the difference, but I don't know how to deal with it.
In my case the document is not tokenized (and should not be tokenized because is a doc2vec not a word2vec)</li>
</ul>
<p><a href=""https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python"">How to remove stop words using nltk or python</a></p>
<ul>
<li>In this question the SO is dealing with a list not a list of list</li>
</ul>
","6290211","","","","","2021-04-26 00:23:07","Preprocessing a list of list removing stopwords for doc2vec using map without losing words order","<python><list><gensim><stop-words>","2","0","","","","CC BY-SA 4.0"
"68181044","1","","","2021-06-29 14:47:25","","0","18","<p>Can't figure out if there's something wrong with the text file the function is reading or the code!</p>
<p><strong>TypeError: decoding to str: need a bytes-like object, builtin_function_or_method found</strong></p>
<pre><code>dictionary = gensim.corpora.Dictionary(gen_docs)
corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]
</code></pre>
<p><a href=""https://i.stack.imgur.com/VJZY2.png"" rel=""nofollow noreferrer"">Error Clipping</a>
<a href=""https://i.stack.imgur.com/Nf5ps.png"" rel=""nofollow noreferrer"">Text File</a></p>
","16343287","","","","","2021-06-29 14:47:25","Type Error: Built in function or module found?","<python><machine-learning><nlp><gensim>","0","1","","","","CC BY-SA 4.0"
"59835900","1","","","2020-01-21 07:16:40","","2","2711","<p>I am working on Word2Vec model. Is there any way to get the ideal value for one of its parameter i.e <code>iter</code>. Like the way we used do in K-Means (Elbo curve plot) to get the K value.Or is there any other way for parameter tuning on this model.</p>
","11903033","","","","","2020-01-21 19:33:13","Gensim Word2vec model parameter tuning","<gensim><word2vec>","1","0","2","","","CC BY-SA 4.0"
"58393090","1","58400524","","2019-10-15 10:58:26","","3","4238","<p>I have two lists, A is a list of words, for example [""hello"",""world"",......], Len(A) is 10000. List B contains the all pre-trained vectors corresponding to A, which is a [10000,512], 512 is the vector dimension. I want to convert two lists into gensim word2vec model format in order to load the model in later, such as <code>model = Word2Vec.load(""word2vec.model"")</code> how should I do this? </p>
","4921197","","4921197","","2019-10-15 12:48:05","2019-10-15 18:18:29","How to save as a gensim word2vec file?","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"59889710","1","59905717","","2020-01-24 02:33:11","","0","555","<p>I am trying to use gensim's file-based training (example from documentation below):</p>

<pre><code>from multiprocessing import cpu_count
from gensim.utils import save_as_line_sentence
from gensim.test.utils import get_tmpfile
from gensim.models import Word2Vec, Doc2Vec, FastText
 # Convert any corpus to the needed format: 1 document per line, words delimited by "" ""
corpus = api.load(""text8"")
corpus_fname = get_tmpfile(""text8-file-sentence.txt"")
save_as_line_sentence(corpus, corpus_fname)
 # Choose num of cores that you want to use (let's use all, models scale linearly now!)
num_cores = cpu_count()
 # Train models using all cores
w2v_model = Word2Vec(corpus_file=corpus_fname, workers=num_cores)
d2v_model = Doc2Vec(corpus_file=corpus_fname, workers=num_cores)
ft_model = FastText(corpus_file=corpus_fname, workers=num_cores)
</code></pre>

<p>However, my actual corpus contains many documents, each containing many sentences.
For example, let's assume my corpus is the plays of Shakespeare - Each play is a document, each document has many many sentences, and I would like to learn embeddings for each play, but the word embeddings only from within the same sentence.
Since the file-based training is meant to be one document per line, I assume that I should put one play per line. However, the documentation for file-based-training doesn't have an example of any documents with multiple sentences.
Is there a way to peek inside the model to see the documents and word context pairs that have been found before they are trained?</p>

<p>What is the correct way to build this file, maintaining sentence boundaries?</p>

<p>Thank you</p>
","8620682","","","","","2020-01-25 02:35:19","Correct way to represent documents containing multiple sentences in gensim file-based training","<gensim><corpus><doc2vec><sentence>","1","0","1","","","CC BY-SA 4.0"
"53852041","1","","","2018-12-19 13:15:16","","2","101","<p>I am using LDA for topic modelling but unfortunately my data is heavily skewed. I have documents from 10 different categories and would like each category to equally contribute to the LDA topics. </p>

<p>However, each category has a varying number of documents (one category for example holds more than 50% of the entire documents, while several categories hold only 1-2% of the documents).</p>

<p>What would be the best approach to assign weights to these categories, so they equally contribute to my topics? If I run the LDA without doing so, my topics will be largely based on the category, which holds over 50% of the documents in the corpus. I am exploring up-sampling but would prefer a solution that directly assigns weight in LDA.</p>
","7733199","","","","","2018-12-19 13:15:16","Assign more weight to certain documents within the corpus - LDA - Gensim","<python-3.x><nlp><gensim><lda><topic-modeling>","0","1","","","","CC BY-SA 4.0"
"59827730","1","59835066","","2020-01-20 16:48:53","","0","277","<p>I have been following the following example for using doc2vec for text classification:</p>

<p><a href=""https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb"" rel=""nofollow noreferrer"">https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb</a></p>

<p>I ran this notebook on my datasets and want to apply one of the doc2vec models to a 3rd dataset (eg, the overall dataset the test/train model was built on).  I tried:</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)
X_train = label_sentences(X_train, 'Train')
X_test = label_sentences(X_test, 'Test')

#added
big_text = label_sentences(big_text, 'Test') #big_text = larger dataframe

#old
#all_data = X_train + X_test

#new
all_data = X_train + X_test + big_text 
</code></pre>

<p>1 - this is not really practical for applied purposes.  The data that one wants to predict might not be available at the time of train/testing.</p>

<p>2 - the model performance decreased as a result</p>

<p>So how can I save once of the models and applying to a completely different dataset?  It would seems that I would need to update the doc2vec model with docs of the other dataset as well.</p>
","1624577","","1624577","","2020-01-20 16:54:48","2020-01-21 06:06:24","save/reuse doc2vec based model for further predictions","<machine-learning><scikit-learn><gensim>","1","2","1","","","CC BY-SA 4.0"
"68225624","1","68229590","","2021-07-02 12:56:58","","0","30","<p>I have a database containing about 2.8 million texts (more precisely tweets, so they are short texts). I put clean tweets (removing hashtags, tags, stop words...) in a list of lists of tokens called <code>sentences</code> (so it contains a list of tokens for each tweet).</p>
<p>After these steps, if I write</p>
<p><code>model = Word2Vec(sentences, min_count=1)</code></p>
<p>I obtain a vocabulary of about 400,000 words.</p>
<p>This was just an attempt, I would need some help to set the parameters (<code>size</code>, <code>window</code>, <code>min_count</code>, <code>workers</code>, <code>sg</code>) of <code>Word2Vec</code> in the most appropriate and consistent way.</p>
<p>Consider that my goal is to use</p>
<p><code>model.most_similar(terms)</code> (where <code>terms</code> is a list of words)</p>
<p>to find, within the list of lists of tokens <code>sentences</code>, the words most similar to those contained in <code>terms</code>.</p>
<p>The words in <code>terms</code> belong to the same topic and I would like to see if there are other words within the texts that could have to do with the topic.</p>
","13983136","","13983136","","2021-07-02 15:30:30","2021-07-02 18:03:48","Set the parameters of Word2Vec for a practical example","<python><nlp><gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"68234684","1","","","2021-07-03 08:54:01","","0","247","<pre><code>word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print('Found %s word vectors of word2vec' % len(word2vec.vocab))
</code></pre>
<p>In the word2vec.vocab it showing error, anyone have any idea to solve it??? i tried word2vec.wc.vocab &amp; word2vec.wc, they doesn't work</p>
","","user13312976","","","","2021-07-03 08:54:01","'KeyedVectors' object has no attribute 'wv' / The vocab attribute was removed from KeyedVector in Gensim 4.0.0","<python><jupyter><gensim><word-embedding>","0","1","","","","CC BY-SA 4.0"
"58412763","1","58417238","","2019-10-16 11:58:44","","1","782","<p>can anybody tell me which default values are used in <code>Doc2Vec()</code> for <code>alpha</code> and <code>min_alpha</code>? </p>
","11777111","","11777111","","2019-10-21 11:32:35","2021-08-17 04:16:03","Default values of doc2vec for alpha and min_alpha","<python><scikit-learn><gensim><doc2vec><hyperparameters>","1","0","","","","CC BY-SA 4.0"
"68240120","1","","","2021-07-03 21:35:30","","1","38","<p>I am running topic modeling using Gensim. Before creating the document-term matrix, one needs to create a dictionary of tokens.</p>
<pre><code>dictionary = corpora.Dictionary(tokenized_reviews)
doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_reviews]
</code></pre>
<p>But, I don't understand what kind of object &quot;dictionary&quot; is.</p>
<p>So, when I type:</p>
<pre><code>type(dictionary)
</code></pre>
<p>I get</p>
<blockquote>
<p>gensim.corpora.dictionary.Dictionary</p>
</blockquote>
<p>Is this a dictionary ( a kind of data structure)? If so, why can't I see the content (I am just curious)?</p>
<p>When I type</p>
<pre><code>dictionary
</code></pre>
<p>I get:</p>
<blockquote>
<p>&lt;gensim.corpora.dictionary.Dictionary at 0x1bac985ebe0&gt;</p>
</blockquote>
<p>The same issue exists with some of the objects in NLTK.</p>
<p>If this is a dictionary (as a data structure), why I am not able to see the keys and values like any other Python dictionary?</p>
<p>Thanks,
Navid</p>
","4710150","","","","","2021-07-05 02:11:44","How to see the content of a Gensim-generated dictionary?","<python><data-structures><nltk><gensim><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"59872029","1","59872652","","2020-01-23 05:25:42","","0","1173","<p>I am using word2vec (and doc2vec) to get embeddings for sentences, but i want to completely ignore word order.
I am currently using gensim, but can use other packages if necessary.</p>

<p>As an example, my text looks like this:</p>

<pre><code>[
['apple', 'banana','carrot','dates', 'elderberry', ..., 'zucchini'],
['aluminium', 'brass','copper', ..., 'zinc'],
...
]
</code></pre>

<p>I intentionally want 'apple' to be considered as close to 'zucchini' as it is to 'banana' so I have set the window size to a very large number, say 1000.
I am aware of 2 problems that may arise with this.</p>

<p>Problem 1:
The window might <em>roll</em> in at the start of a sentence creating the following training pairs:
<code>('apple', ('banana')), ('apple', ('banana', 'carrot')), ('apple', ('banana', 'carrot', 'date'))</code> before it eventually gets to the correct <code>('apple', ('banana','carrot', ..., 'zucchini'))</code>.
This would seem to have the effect of making 'apple' closer to 'banana' than 'zucchini',
since their are so many more pairs containing 'apple' and 'banana' than there are pairs containing 'apple' and 'zucchini'.</p>

<p>Problem 2:
I heard that pairs are sampled with inverse proportion to the distance from the target word to the context word- This also causes an issue making nearby words more seem more connected than I want them to be.</p>

<p>Is there a way around problems 1 and 2?
Should I be using cbow as opposed to sgns? Are there any other hyperparameters that I should be aware of?
What is the best way to go about removing/ignoring the order in this case?</p>

<p>Thank you</p>
","8620682","","","","","2020-01-23 18:48:16","word2vec window size at sentence boundaries","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"50598129","1","53999185","","2018-05-30 06:54:18","","4","1162","<p>Using <code>gensim</code>:</p>

<pre><code>from gensim.models import TfidfModel
from gensim.corpora import Dictionary

sent0 = ""The quick brown fox jumps over the lazy brown dog ."".lower().split()
sent1 = ""Mr brown jumps over the lazy fox ."".lower().split()

dataset = [sent0, sent1]
vocab = Dictionary(dataset)
corpus = [vocab.doc2bow(sent) for sent in dataset] 
model = TfidfModel(corpus)

# To retrieve the same pd.DataFrame format.
documents_tfidf_lol = [{vocab[word_idx]:tfidf_value for word_idx, tfidf_value in sent} for sent in model[corpus]]
documents_tfidf = pd.DataFrame(documents_tfidf_lol)
documents_tfidf.fillna(0, inplace=True)

documents_tfidf
</code></pre>

<p>[out]:</p>

<pre><code>    dog mr  quick
0   0.707107    0.0 0.707107
1   0.000000    1.0 0.000000
</code></pre>

<p>If we do the TF-IDF computation manually, </p>

<pre><code>sent0 = ""The quick brown fox jumps over the lazy brown dog ."".lower().split()
sent1 = ""Mr brown jumps over the lazy fox ."".lower().split()

documents = pd.DataFrame.from_dict(list(map(Counter, [sent0, sent1])))
documents.fillna(0, inplace=True, downcast='infer')
documents = documents.apply(lambda x: x/sum(x))  # Normalize the TF.
documents.head()

# To compute the IDF for all words.
num_sentences, num_words = documents.shape

idf_vector = [] # Lets save an ordered list of IDFS w.r.t. order of the column names.

for word in documents:
  word_idf = math.log(num_sentences/len(documents[word].nonzero()[0]))
  idf_vector.append(word_idf)

# Compute the TF-IDF table.
documents_tfidf = pd.DataFrame(documents.as_matrix() * np.array(idf_vector), 
                               columns=list(documents))
documents_tfidf
</code></pre>

<p>[out]:</p>

<pre><code>    .   brown   dog fox jumps   lazy    mr  over    quick   the
0   0.0 0.0 0.693147    0.0 0.0 0.0 0.000000    0.0 0.693147    0.0
1   0.0 0.0 0.000000    0.0 0.0 0.0 0.693147    0.0 0.000000    0.0
</code></pre>

<p>If we use <code>math.log2</code> instead of <code>math.log</code>:</p>

<pre><code>    .   brown   dog fox jumps   lazy    mr  over    quick   the
0   0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
1   0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
</code></pre>

<p>It looks like <code>gensim</code>:</p>

<ul>
<li>remove the non-salient words from the TF-IDF model, it's evident when we <code>print(model[corpus])</code></li>
<li>maybe the log base seem to be different from the log_2</li>
<li>maybe there's some normalization going on. </li>
</ul>

<p>Looking at <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel</a> , the <code>smart</code> scheme difference would have output different values but it's not clear in the docs what is the default value.</p>

<p><strong>What is the default smartirs for gensim TfidfModel?</strong></p>

<p><strong>What are the other default parameters that've caused the difference between a natively implemented TF-IDF and gensim's?</strong></p>
","610569","","610569","","2018-05-30 07:04:34","2019-01-04 05:36:44","What is the default smartirs for gensim TfidfModel?","<python><nlp><gensim><information-retrieval><tf-idf>","1","0","2","","","CC BY-SA 4.0"
"51168444","1","","","2018-07-04 07:49:20","","6","770","<p>I have check the previous post <a href=""https://stackoverflow.com/questions/22121028/update-gensim-word2vec-model"">link</a> but it doesn't seems to work for my case:-</p>

<p>I have pre trained word2vec model:</p>

<pre><code>import gensim    
model = Word2Vec.load('w2v_model')
</code></pre>

<p>Now I have a pandas dataframe with keywords:</p>

<pre><code>keyword
corruption
people
budget
cambodia
.......
......
</code></pre>

<p>All I want to add the vectors for each keyword in its corresponding columns but
when I use <code>model['cambodia']</code> it throw me error as <code>KeyError: ""word 'cambodia' not in vocabulary""</code></p>

<p>so I have update the keyword as:</p>

<pre><code>model.train(['cambodia'])
</code></pre>

<p>But this won't work out for me, when I use 
<code>model['cambodia']</code> </p>

<p>it still giving an error as <code>KeyError: ""word 'cambodia' not in vocabulary""</code>. How to update new words into word2vec vocabulary so i can get its vectors? Expected output will be:-</p>

<pre><code>keyword    V1         V2          V3         V4            V5         V6   
corruption 0.07397  0.290874    -0.170812   0.085428    -0.148551   0.38846 
people      ..............................................................
budget      ...........................................................
</code></pre>
","8568110","","8568110","","2018-07-06 06:47:36","2018-07-26 08:52:18","How I can get the vectors for words that were not present in word2vec vocabulary?","<python-3.x><pandas><word2vec><gensim><text-classification>","1","6","1","","","CC BY-SA 4.0"
"51180848","1","51199856","","2018-07-04 21:18:46","","0","293","<p>So I have started to learn gensim for both word2vec and doc2vec and it works. The similarity scores actually work really well. For an experiment, however, I wanted to optimize a key word based search algorithm by comparing a single word and getting how similar it is to a piece of text. </p>

<p>What is the best way to do this? I considered averaging the the word vectors of all words in the text (maybe remove fill and stop word first) and and comparing this to the search word? But this really is just intuition, what would be the best way to do this?</p>
","5272422","","","","","2018-07-05 21:00:30","How to I get the similiarity between a word to a document in gensim","<python><search><gensim><word2vec><doc2vec>","1","0","","2018-07-07 11:50:00","","CC BY-SA 4.0"
"59891168","1","","","2020-01-24 06:05:47","","-1","339","<p>I have a set of text documents(2000+) with labels (Liked/Disliked).Each document consists of 200+ words.
I am trying to do a supervised learning with these documents.
<strong>My approach would be:</strong></p>

<ol>
<li>Vectorize each document in the corpus. Say we have 2347 docs.</li>
<li>I can have 2347 rows with labels viz. Like as 1 and Dislike as 0.</li>
<li>Using any ML classification supervised model train above dataset with 2347 rows.</li>
</ol>

<p><strong>How to vectorize and create such dataset?</strong></p>
","11586205","","11586205","","2020-01-24 12:03:14","2020-01-24 12:50:18","How to do supervised learning with Gensim/Word2Vec/Doc2Vec having large corpus of text documents?","<python><nlp><gensim><word2vec><doc2vec>","1","1","","","","CC BY-SA 4.0"
"59925207","1","","","2020-01-27 04:37:37","","1","168","<p>I have created a word2vec model and have made a visualization of the top n similar words for a particular term using TSNE and matplotlib. What I do not understand is that when I run it multiple times, the same words are plotted to different positions even though the words and vectors are the same each time. Why is this the case? I have a feeling it has to do with the way TSNE reduces the dimensionality of the vectors. If this is the case is it really reliable to use this method of visualization since it will be different every time?</p>

<pre><code>model = Word2Vec.load(""a_w2v_model"")

topn_words_list = [x[0] for x in model.wv.most_similar(""king"",topn=3)]
topn_vectors_list = model[topn_words_list]

tsne = TSNE(n_components=2, verbose=1, perplexity=27, n_iter=300)
Y = tsne.fit_transform(topn_vectors_list)

fig, ax = plt.subplots()
ax.plot(Y[:, 0], Y[:, 1], 'o')
ax.set_yticklabels([]) #Hide ticks
ax.set_xticklabels([]) #Hide ticks

for i, word in enumerate(topn_words_list):
    plt.annotate(word, xy=(Y[i, 0], Y[i, 1]))
plt.show()
</code></pre>
","1365234","","","","","2020-03-03 16:21:42","random points when visualizing word2vec embeddings using TSNE","<python><matplotlib><pca><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"67245763","1","","","2021-04-24 17:46:43","","0","36","<p>word2vec = Word2VecProvider()
gensim.models.Word2Vec(&quot;glove.twitter.27B.200d.txt&quot;)</p>
<p>I have updated my gensim library twice but in vain. Any lead would be helpful.</p>
<p>Thanks!</p>
","15300767","","15300767","","2021-04-25 13:12:41","2021-04-25 20:17:05","Getting the Attribute Error: type object 'Word2Vec' has no attribute 'load_word2vec_format' while doing sentiment analysis","<python><gensim><word2vec><attributeerror><sentiment-analysis>","1","1","","","","CC BY-SA 4.0"
"68210732","1","68213523","","2021-07-01 13:02:47","","0","51","<p>I need to put the texts contained in a column of a MySQL database (about 3 million rows) into a list of lists of tokens. These texts (which are tweets, therefore they are generally short) must be preprocessed before being included in the list (stop words, hashtags, tags etc. must be removed). This list should be passed later as a <code>Word2Vec</code> parameter. This is the part of the code involved</p>
<pre><code>import mysql.connector
import re
from gensim.models import Word2Vec
import preprocessor as p
p.set_options(
    p.OPT.URL,
    p.OPT.MENTION,
    p.OPT.HASHTAG,
    p.OPT.NUMBER
)

conn = mysql.connector.connect(...)
cursor = conn.cursor()
query = &quot;SELECT text FROM tweet&quot;
cursor.execute(query)
table = cursor.fetchall()

stopwords = open('stopwords.txt', encoding='utf-8').read().split('\n')
sentences = []
for row in table:
    sentences = sentences + [[w for w in re.sub(r'[^\w\s-]', ' ', p.clean(row[0])).lower().split() if w not in stopwords and len(w) &gt; 2]]

cursor.close()
conn.close()

model = Word2Vec(sentences)
...
</code></pre>
<p>Obviously it takes a lot of time and I know that my method is probably inefficient. Can anyone recommend a better one? I know it is not a question directly related to <code>gensim</code> and <code>Word2Vec</code> but perhaps those who use them have already faced the problem of working with a large amount of texts.</p>
","13983136","","","","","2021-07-01 16:05:20","Gensim word2vec and large amount of texts","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"58407649","1","58417036","","2019-10-16 07:10:25","","1","1612","<p>I'm trying to load one of the FastText pre-trained models that has a form of a .bin file. The size of .bin file is 2.8GB and I have 8GB RAM and 8GB swap file. Unfortunately, the model starts loading and it occupies almost 15GB and then it breaks with the following error:</p>

<p><code>Process finished with exit code 137 (interrupted by signal 9: SIGKILL)</code></p>

<p>By observing the system monitor, I can see that RAM and swap are fully occupied, so I think it breaks because it is out of memory.</p>

<p>I'm trying to load the file using Gensim wrapper for FastText</p>

<p><code>from gensim.models.wrappers import FastText
 model = FastText.load_fasttext_format('../model/java_ftskip_dim100_ws5')</code></p>

<hr>

<p>My questions are the following:</p>

<p>1) Is there any way to fit this model in the current memory of my system?</p>

<p>2) Is it possible to reduce the size of this model? I tried the quantization using the following code</p>

<p><code>./fasttext quantize -output java_ftskip_dim100_ws5 -input unused_argument.txt</code></p>

<p>And I'm getting the following error:</p>

<p><code>terminate called after throwing an instance of 'std::invalid_argument'
  what():  For now we only support quantization of supervised models
Aborted (core dumped)</code></p>

<p>I would really appreciate your help!</p>
","4846259","","","","","2019-10-16 15:37:48","FastText .bin file cannot fit in memory, even though I have enough RAM","<python><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"59926638","1","59938117","","2020-01-27 07:22:16","","0","282","<p>This is the code for my model using <strong>Gensim.i</strong> run it and it returned a tuple. I wanna know that which one is the number of tokens?</p>

<pre><code>model = gensim.models.Word2Vec(mylist5,size=100, sg=0, window=5, alpha=0.05, min_count=5, workers=12, iter=20, cbow_mean=1, hs=0, negative=15)

model.train(mylist5, total_examples=len(mylist5), epochs=10)
</code></pre>

<p>The value that was returned by my model is: I need to know what is this?</p>

<pre><code> (167131589, 208757070)
</code></pre>

<p>I wanna know what is the number of tokens?</p>
","3782154","","3782154","","2020-01-27 07:30:45","2021-07-02 18:13:47","How to find number of tokens in gensim model","<python-3.x><gensim>","2","1","","","","CC BY-SA 4.0"
"59919462","1","","","2020-01-26 14:51:28","","3","2335","<p>I'm trying to build a Word2vec (or FastText) model using Gensim on a massive dataset which is composed of 1000 files, each contains ~210,000 sentences, and each sentence contains ~1000 words. The training was made on a 185gb RAM, 36-core machine.
I validated that</p>

<pre><code>gensim.models.word2vec.FAST_VERSION == 1
</code></pre>

<p>First, I've tried the following:</p>

<pre><code>files = gensim.models.word2vec.PathLineSentences('path/to/files')
model = gensim.models.word2vec.Word2Vec(files, workers=-1)
</code></pre>

<p>But after 13 hours I decided it is running for too long and stopped it.</p>

<p>Then I tried building the vocabulary based on a single file, and train based on all 1000 files as follows:</p>

<pre><code>files = os.listdir['path/to/files']
model = gensim.models.word2vec.Word2Vec(min_count=1, workers=-1)
model.build_vocab(corpus_file=files[0])
for file in files:
    model.train(corpus_file=file, total_words=model.corpus_total_words, epochs=1)
</code></pre>

<p>But I checked a sample of word vectors before and after the training, and there was no change, which means no actual training was done.</p>

<p>I can use some advise on how to run it quickly and successfully. Thanks!</p>

<p><strong>Update #1:</strong></p>

<p>Here is the code to check vector updates:</p>

<pre><code>file = 'path/to/single/gziped/file'
total_words = 197264406 # number of words in 'file'
total_examples = 209718 # number of records in 'file'
model = gensim.models.word2vec.Word2Vec(iter=5, workers=12)
model.build_vocab(corpus_file=file)
wv_before = model.wv['9995']
model.train(corpus_file=file, total_words=total_words, total_examples=total_examples, epochs=5)
wv_after = model.wv['9995']
</code></pre>

<p>so the vectors: <code>wv_before</code> and <code>wv_after</code> are exactly the same</p>
","5672316","","5672316","","2020-01-28 12:18:10","2020-01-28 12:18:10","Speed Up Gensim's Word2vec for a Massive Dataset","<gensim><word2vec><fasttext>","1","0","1","","","CC BY-SA 4.0"
"58497442","1","58509901","","2019-10-22 05:10:55","","1","1441","<p>I am trying to use doc2vec to do text classification based on document subject, for example, I want to classify all documents about sports as 1 and all other documents as 0. I want to do this by first training a doc2vec model with training data and then use a classification model such as logistic regression to classify the texts as positive or negative.</p>

<p>I have seen various examples online to do this [<a href=""https://fzr72725.github.io/2018/01/14/genism-guide.html"" rel=""nofollow noreferrer"">1</a>,<a href=""https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"" rel=""nofollow noreferrer"">2</a>] which employ different methods and I am unclear about some of the details as to why they are using certain methods, and which method is the best for text classification.</p>

<ol>
<li><p>Firstly Using the example above, is it better to train the model using just documents related to sports or documents on all subjects. My thinking was by training just on sports documents you could classify documents based on document similarity(although this wouldnt produce vectors for non sports documents to use to train the next model). Also, i feel like if training the model on all documents you would need a huge amount of documents to represent everything other than sports to get good classification. </p></li>
<li><p>Secondly, which features are actually used to train the logistic regression model. If training the model on all documents I assume you would track the documents using an index of some sort and then train the logistic regression model using the vectors with a class label, is this correct ? </p></li>
<li><p>Thirdly, I have seen various uses of TaggedDocument where a unique id is put for each document and also where a shared id is used to represent the same class, eg., 1 = sports 0 = non sports. From what I have read a shared id means the model has a single vector representing each class, while using a unique id provides unique vectors for each document, is this correct ?. If so, assuming that I need unique labeled vectors for training the logistic regression model what is the point of using a shared id ? Wouldnt this provide terrible classification results ?</p></li>
</ol>

<p>If anyone can help me with the questions above and generally what is the best way to do text classification using doc2vec vectors it would be greatly appreciated.</p>
","1365234","","","","","2021-08-08 09:50:38","best training methods for binary text classification using doc2vec gensim","<machine-learning><gensim><doc2vec>","1","1","","","","CC BY-SA 4.0"
"68222746","1","","","2021-07-02 09:25:34","","0","18","<p>I was going through old FastText code, and started to realize it doesn't work anymore and expects different parameters. When looking at the dcoumentation , it appears the documentation has been partially updated.</p>
<p><a href=""https://i.stack.imgur.com/3A9jB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3A9jB.png"" alt=""
"" /></a></p>
<p><a href=""https://i.stack.imgur.com/s2C9b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s2C9b.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/qCyLJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qCyLJ.png"" alt=""enter image description here"" /></a></p>
<p>Which it can be seen size and iter are not in the class definition shown in the docs despite being in the parameters. I was wondering if anyone knew exact version where this change has occured as it appears I've accidentally updated it to something newer.</p>
","6278900","","","","","2021-07-02 18:08:21","FastText version before most recent change","<python><nlp><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"59924168","1","59925929","","2020-01-27 00:56:17","","2","109","<p>I need to combine Word2Vec with my <code>CNN</code> model. To this end, I need to persist a flag (a binary one is enough) for each sentence as my corpus has two types (<em>a.k.a.</em> target classes) of sentences. So, I need to retrieve this flag of each vector after creation. How can I store and retrieve this information inside the input sentences of <code>Word2Vec</code> as I need both of them in order to train my deep neural network?</p>

<p>p.s. I'm using <code>Gensim</code> implementation of <code>Word2Vec</code>.</p>

<p>p.s. My corpus has <strong>6,925</strong> sentences, and <code>Word2Vec</code> produces <strong>5,260</strong> vectors.</p>

<p><strong>Edit:</strong> More detail regarding my corpus (as requested):</p>

<p>The structure of the corpus is as follows:</p>

<ol>
<li><p>sentences (label: <code>positive</code>) -- A <strong>Python list</strong></p>

<ul>
<li><code>Feature-A</code>: <strong>String</strong></li>
<li><code>Feature-B</code>: <strong>String</strong></li>
<li><code>Feature-C</code>: <strong>String</strong></li>
</ul></li>
<li><p>sentences (label: <code>negative</code>) -- A <strong>Python list</strong></p>

<ul>
<li><code>Feature-A</code>: <strong>String</strong></li>
<li><code>Feature-B</code>: <strong>String</strong></li>
<li><code>Feature-C</code>: <strong>String</strong></li>
</ul></li>
</ol>

<p>Then all the sentences were given as the input to <code>Word2Vec</code>.</p>

<pre><code>word2vec = Word2Vec(all_sentences, min_count=1)
</code></pre>

<p>I'll feed my CNN with the extracted features (which is the <code>vocabulary</code> in this case) and the <code>targets</code> of sentences. So, I need these labels of the sentences as well.</p>
","282855","","282855","","2020-01-27 19:06:27","2020-01-27 19:06:27","Word2Vec - How can I store and retrieve extra information regarding each instance of corpus?","<deep-learning><gensim><word2vec><one-hot-encoding><word-embedding>","1","0","","","","CC BY-SA 4.0"
"38682784","1","","","2016-07-31 09:55:43","","0","257","<p>Does anyone have any idea or can give me directions about how I can extract categories from an article?</p>

<p>What I have is a corpus of few thousands of articles (about sports, news, buisness etc.) I can work with.</p>

<p>For example, if theres an article about sports I would like my program to know if its soccer or basketball (or somthing else) so the output will be somthing like:</p>

<p>soccer 90% basketball 10%</p>
","1804659","","","","","2016-07-31 12:29:54","NLP - extract categories/tags from text","<python><nltk><gensim>","2","2","","","","CC BY-SA 3.0"
"59949098","1","","","2020-01-28 12:46:50","","0","25","<p>I try to build a gensim LDA model for topic modeling <a href=""https://i.stack.imgur.com/lmnU6.png"" rel=""nofollow noreferrer"">output of my trained model</a>. I need to give a real time input to check that document come under which topic. this model for question classification like Qunts, Reasoning and verbal question. Please help me.</p>
","12799145","","","","","2020-01-28 12:46:50","How to classifiy document the Topics using Gensim LDA model after the Training(How to deploy the code)?","<input><gensim><topic-modeling>","0","2","","","","CC BY-SA 4.0"
"68268176","1","","","2021-07-06 09:44:13","","0","30","<p>afternoon or evening.</p>
<p>Apologies in advance for disturbing the community with this. About the problem: The goal was to determine the similarity of emotion words (big five items) and words from the text corpus obtained from a person's reading behavior. I managed to write a code where it is possible for a list (see end of the post), but the goal would actually be a matrix. Or in other words, so far it looks like this</p>
<p>demoword \t emotionword \t similarityscore</p>
<p>my aim is something like that, every score tab seperated</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Word from Corpus</th>
<th style=""text-align: center;"">emotion word 1</th>
<th style=""text-align: right;"">emotion word 2</th>
<th style=""text-align: right;"">n emotion words</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Word one</td>
<td style=""text-align: center;"">score</td>
<td style=""text-align: right;"">score</td>
<td style=""text-align: right;"">score</td>
</tr>
<tr>
<td style=""text-align: left;"">Word two</td>
<td style=""text-align: center;"">score</td>
<td style=""text-align: right;"">score</td>
<td style=""text-align: right;"">score</td>
</tr>
<tr>
<td style=""text-align: left;"">Word n</td>
<td style=""text-align: center;"">score</td>
<td style=""text-align: right;"">score</td>
<td style=""text-align: right;"">score</td>
</tr>
</tbody>
</table>
</div>
<p>thanks everyone for reading, again am I am sorry if it is a simple solution and I am just not able to find it.</p>
<pre><code>from gensim.models import Word2Vec
import os

#Paths to the necessary files 
Model_Pfad = r'D:\OneDrive\Phyton\modelC.model'     #word2vec model
ausgabe= r'D:\OneDrive\Phyton\numbers.txt'          #file with the results
emo_file = r'D:\OneDrive\Phyton\test.txt'           #List of words of which the similarity is determined  
out_file= 'D:\OneDrive\Phyton\Ergebnisse.txt'       

model = Word2Vec.load(Model_Pfad)


x = list(model.wv.index_to_key[:500]) # creates a list with the 500 most common words in the w2v

corpus_words = &quot;\n&quot;.join(x)


#print(corpus_words, file = open (ausgabe,'a')) #just to ckeck the 500 words if necessary

corpus_wordsB = r'D:\OneDrive\Phyton\numbers.txt'


file = open(emo_file,'r') #load the target words
list_emo = []
for line in file:
    list_emo.append(line.lower())
file.close()

file = open(corpus_wordsB,'r') #load the words from the corpus
list_corpus = []
for line in file:
    list_corpus.append(line.lower())
file.close()


file = open(out_file,'w')
for x in range(0, len(list_emo)):
    w1 = list_emo[x].strip('\r\n') #get a word from the emo list

    for y in range(0, len(list_corpus)):
        w2 = list_corpus[y].strip('\r\n') #get a word from the corpus list
        try:
            distance = round(model.wv.similarity(w1,w2),5) # get the similarity between emotional word and word from corpus
        except KeyError:
            #print 'not in vocabulary'
            distance = 'N/A'
        file.write(w1+'\t'+w2+'\t')
        file.write(str(distance))
        file.write('\n')
file.close()

print ('done')
</code></pre>
","14686994","","14686994","","2021-07-07 12:40:33","2021-07-07 12:40:33","how to create a matrix with gensim similarities","<python><numpy><nlp><gensim><word2vec>","0","0","","","","CC BY-SA 4.0"
"51233632","1","51239329","","2018-07-08 15:46:00","","3","4926","<p>This problem is going completely over my head. I am training a Word2Vec model using gensim. I have provided data in multiple languages i.e. English and Hindi. When I am trying to find the words closest to 'man', this is what I am getting:</p>

<pre><code>model.wv.most_similar(positive = ['man'])
Out[14]: 
[('woman', 0.7380284070968628),
 ('lady', 0.6933152675628662),
 ('monk', 0.6662989258766174),
 ('guy', 0.6513140201568604),
 ('soldier', 0.6491742134094238),
 ('priest', 0.6440571546554565),
 ('farmer', 0.6366012692451477),
 ('sailor', 0.6297377943992615),
 ('knight', 0.6290514469146729),
 ('person', 0.6288090944290161)]
--------------------------------------------
</code></pre>

<p>Problem is, these are all English words. Then I tried to find similarity between same meaning Hindi and English words, </p>

<pre><code>model.similarity('man', '‡§Ü‡§¶‡§Æ‡•Ä')
__main__:1: DeprecationWarning: Call to deprecated `similarity` (Method will 
be removed in 4.0.0, use self.wv.similarity() instead).
Out[13]: 0.078265618974427215
</code></pre>

<p>This accuracy should have been better than all the other accuracies. The Hindi corpus I have has been made by translating the English one. Hence the words appear in similar contexts. Hence they should be close.</p>

<p>This is what I am doing here:</p>

<pre><code>#Combining all the words together.
all_reviews=HindiWordsList + EnglishWordsList

#Training FastText model
cpu_count=multiprocessing.cpu_count()
model=Word2Vec(size=300,window=5,min_count=1,alpha=0.025,workers=cpu_count,max_vocab_size=None,negative=10)
model.build_vocab(all_reviews)
model.train(all_reviews,total_examples=model.corpus_count,epochs=model.iter)
model.save(""word2vec_combined_50.bin"")
</code></pre>
","6083087","","6083087","","2018-07-10 10:20:52","2020-05-16 10:43:50","word2vec gensim multiple languages","<python><nlp><artificial-intelligence><word2vec><gensim>","3","6","","","","CC BY-SA 4.0"
"58397381","1","","","2019-10-15 14:48:54","","1","203","<p>I would like to create a LDA model (i.e. an instance of gensim.models.LdaModel) returning a pre-determined topic-word distribution. The reason for doing this is that I would like to leverage Gensim, pyLDAvis, etc to display the results on some topic-word distribution that I obtain from other algorithms. </p>

<p>Is that possible? Below I show some incomplete code (because I find the ML community intimidating):</p>

<pre><code>import gensim
texts=[['a','a','b'], ['a','b','c'],['b','c','c']]
d = gensim.corpora.Dictionary(texts)
bow = [d.doc2bow(doc) for doc in texts]

my_topics=[[2/3, 1/3, 0],[0, 2/3, 1/3]]
model = gensim.models.LdaModel(corpus=bow, id2word=d, num_topics=2, 
        eta=..., alpha..., passes=0, iterations=0, random_state=1)
model.show_topics(num_words=3)
# Hopefully return my_topics above
#[(0, '0.666*""a"" + 0.333*""b""'),
# (1, '0.666*""b"" + 0.333*""c""')]
</code></pre>

<p>I was hoping to achieve this by initializing eta in the LDA model and by setting iterations to 0. In my attempts (not shown) the resulting topics are not the same as 'my_topics'. </p>

<p><a href=""https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel"" rel=""nofollow noreferrer"">Gensim docs</a>:</p>

<blockquote>
  <p>eta ({float, np.array, str}, optional) ‚Äì</p>
  
  <p>A-priori belief on word probability, this can be:</p>

<pre><code>    scalar for a symmetric prior over topic/word probability,
    vector of length num_words to denote an asymmetric user defined probability for each word,
    matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,
    the string ‚Äòauto‚Äô to learn the asymmetric prior from the data.
</code></pre>
</blockquote>

<p>EDIT: This (calling reset on the model's state) seems to work, but I am not sure it makes theoretically sense</p>

<pre><code>model = gensim.models.LdaModel(corpus=bow, id2word=d, num_topics=2, 
    eta=my_topics, alpha=1, passes=0, iterations=0, random_state=1)
model.state.reset()
model.show_topics(num_words=3)
#array([[0.6666667 , 0.33333334, 0.        ],
#   [0.        , 0.6666667 , 0.33333334]], dtype=float32)
</code></pre>

<p>However, the topic distribution for a given document is not trained and I do not think it can be keeping the topics constant:</p>

<pre><code>model.get_document_topics(bow[2])
model[bow[2]]
#[(0, 0.55569696), (1, 0.44430298)]
</code></pre>
","3592827","","3592827","","2019-10-28 11:03:17","2019-10-28 11:03:17","Initialize Gensim LDA model with pre-determined priors","<python><gensim><lda>","0","0","","","","CC BY-SA 4.0"
"58451218","1","","","2019-10-18 12:44:40","","2","610","<p>I have a list of sentences. I want to cluster my sentences on similarity using the WMD (word mover's distance). I am using a word2vec model from gensim to create embeddings for my words.</p>

<p>The clustering algorithms I know (nltk, sklearn) use number vectors as input so I need to give the sentences as an array (or list) of the embeddings of the words in them. I think I can use the nltk clustering methods with a custom distance function. I want to use the WMD as his custom function. But the WMD function of gensim uses a 2 lists of strings as input. </p>

<p>Is there a prebuild WMD function that uses the embeddings and not the strings as input? Or is there a clustering (kmeans or something else) that can handle lists of strings as input and can have the WMD as custom distance function?</p>

<p>Thanks</p>
","11160436","","","","","2020-09-08 17:27:47","Use wmd function of gensim for sentence clustering","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"64871982","1","","","2020-11-17 08:50:27","","0","27","<p>I am performing topic modeling on year-on-year textual data. While fitting the model, I want to assign less weight to older documents and more weight to recent documents, so the topics are more in line with what is present in recent texts. Particularly, is there a technically correct way to do this using libraries like Gensim and Scikit-Learn?</p>
","14653528","","","","","2020-11-17 17:59:47","Is there any way to assign weight to historical data while fitting a topic model on text documents?","<scikit-learn><nlp><gensim><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"27957112","1","27957507","","2015-01-15 05:04:05","","2","1422","<p>This is my first question at Stack Overflow.</p>

<p>I have a DataFrame of Pandas like this.</p>

<pre><code>        a   b   c   d
one     0   1   2   3
two     4   5   6   7
three   8   9   0   1
four    2   1   1   5
five    1   1   8   9
</code></pre>

<p>I want to extract the pairs of column name and data whose data is 1 and each index is separate at array.</p>

<pre><code>[ [(b,1.0)], [(d,1.0)], [(b,1.0),(c,1.0)], [(a,1.0),(b,1.0)] ]
</code></pre>

<p>I want to use gensim of python library which requires corpus as this form.</p>

<p>Is there any smart way to do this or to apply gensim from pandas data?</p>
","4453357","","","","","2017-08-09 20:00:54","Extract array (column name, data) from Pandas DataFrame","<python><pandas><gensim>","2","0","","","","CC BY-SA 3.0"
"59909099","1","","","2020-01-25 12:13:50","","0","2442","<p>I'am using the word2vec model and I have a problem with storing and reading it.</p>

<pre><code>import gensim.models.keyedvectors as w2v
from gensim.models import KeyedVectors

word_vectors = w2v.wv
word_vectors.save(filepath + ""Vectors.bin"")

m = word2vec.KeyedVectors.load_word2vec_format(filepath + ""Vectors.bin"", binary=True)
</code></pre>

<p>I get following error:</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte

</code></pre>

<p>In the following way the laoding will work:</p>

<pre><code>vectors = KeyedVectors.load(filepath + ""Vectors.bin"", mmap='r')
</code></pre>

<p>But If I then call</p>

<pre><code>vectors.similar_by_word(""cat"")
</code></pre>

<p>I get following error: 
TypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'</p>

<p>What am I doing wrong?
How can I use the save_word2vec_format() function?</p>
","3008806","","3008806","","2020-01-25 20:26:11","2020-01-25 20:26:11","Gensim framework: Saving and storing word2vec keyed vectors","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"68259216","1","68951344","","2021-07-05 16:15:03","","1","26","<p>Is there a way to access MALLET's <a href=""http://mallet.cs.umass.edu/diagnostics.php"" rel=""nofollow noreferrer"">diagnostics file</a> or its content by using the provided API via <a href=""https://radimrehurek.com/gensim/index.html"" rel=""nofollow noreferrer"">Gensim</a> in Python?</p>
","16336153","","","","","2021-08-27 09:46:37","Accessing MALLET's diagnostics file via Gensim","<python><nlp><gensim><evaluation><mallet>","1","0","","","","CC BY-SA 4.0"
"68298289","1","68306381","","2021-07-08 08:42:56","","0","50","<p>With Gensim &lt; 4.0, we can retrain a word2vec model using the following code:</p>
<pre><code>model = Word2Vec.load_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True)
model.train(my_corpus, total_examples=len(my_corpus), epochs=model.epochs)
</code></pre>
<p>However, what I understand is that Gensim 4.0 is no longer supporting <code>Word2Vec.load_word2vec_format</code>. Instead, I can only load the keyedVectors.</p>
<p>How to fine-tune a pre-trained word2vec model (such as the model trained on GoogleNews) with my domain-specific corpus using Gensim 4.0?</p>
","3316585","","3316585","","2021-07-08 13:51:36","2021-07-08 17:40:54","Fine-tuning pre-trained Word2Vec model with Gensim 4.0","<gensim><word2vec><transfer-learning><pre-trained-model>","1","0","","","","CC BY-SA 4.0"
"64866067","1","64867002","","2020-11-16 21:37:08","","0","139","<p>I have recently started studying Doc2Vec model.
I have understood its mechanism and how it works.
I'm trying to implement it using gensim framework.
I have transormed my training data into TaggedDocument.
But i have one question :
What is the role of this line <code>model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])</code> ?
is it to create random vectors that represent text ?
Thank you for your help</p>
","14605205","","","","","2020-11-16 23:04:41","Understanding the role of the function build_vocab in Doc2Vec","<nlp><data-science><gensim><text-classification><doc2vec>","1","0","","","","CC BY-SA 4.0"
"52827465","1","","","2018-10-16 03:14:54","","1","526","<p>I am using <code>gensim</code> to do LDA on a corpus of arXiv abstracts in the category <a href=""https://arxiv.org/list/stat.ML/recent"" rel=""nofollow noreferrer"">stats.ML</a></p>

<p>My problem is that there is a lot of overlap between the topics (whether I pick 5, 10, or 50 topics). Every topic has a distribution of words like ""model"" ""algorithm"", or ""problem."" How can topics be considered differentiable if so many of them prominently feature the same terms? </p>

<p>Using pyLDAvis was instructive for me . This is the distribution for topic #3:
<a href=""https://i.stack.imgur.com/tLWCn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tLWCn.png"" alt=""topics with lambda equal one""></a></p>

<p>But when I turn down <code>lambda = 0.08</code>, the actual nature of the topic emerges (ML in medical applications):
<a href=""https://i.stack.imgur.com/rHyWn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rHyWn.png"" alt=""topics with low lambda""></a></p>

<p>So my question is, how could I uncover these distinctive terms in the course of training my LDA model (without pyLDAvis)? And also, does the performance (as opposed to interpret-ability) of the model improve if I can get it to ignore these common, non-discriminating terms?</p>

<p>I have several ideas to try but would like more guidance:</p>

<ul>
<li>Filtering the 50 most common terms from my dictionary. While I think it helped a bit, I'm not sure if it's the right approach</li>
<li>Tweaking <code>eta</code> parameter in <code>gensim.models.LdaModel</code></li>
</ul>

<p>My goal is ultimately to take a new document and do word coloring on it based on which words relate to which topics, and also offer the documents most similar to the input document.</p>

<p>I am pretty new with <code>gensim</code>, and this is my first SO question, so if I'm totally off-base with something, please let me know ;-). Thank you</p>
","10167851","","","","","2018-10-16 03:14:54","How to limit LDA topics to terms that are distinct?","<python><gensim><lda><topic-modeling>","0","1","1","","","CC BY-SA 4.0"
"58493250","1","","","2019-10-21 19:54:03","","0","241","<p>I am trying to code for a LDA Mallet Model...I ran this a couple months ago and it ran fine but it is no longer. There have been other posts on the same subject but the solutions have not yet helped me. Can anyone figure out what is wrong in my code and/or other solutions to fix the problem? The first two cells run fine. The third is where it breaks and it says it returns a non-zero exit status 1.</p>

<p><a href=""https://i.stack.imgur.com/0Uwij.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Uwij.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/5ap8D.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5ap8D.jpg"" alt=""enter image description here""></a></p>
","7374876","","","","","2019-10-21 20:24:11","Lda Mallet returned non-zero exit status 1","<python><gensim><mallet>","1","2","","","","CC BY-SA 4.0"
"47025885","1","","","2017-10-30 23:48:37","","1","447","<p>Does gensim <code>Word2Vec</code> have an option that is the equivalent of ""training steps"" in the TensorFlow word2vec example here: <a href=""https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"" title=""Word2Vec Basic"">Word2Vec Basic</a>? If not, what default value does gensim use? Is the gensim parameter <code>iter</code> related to training steps?</p>

<p>The TensorFlow script includes this section.</p>

<pre><code>with tf.Session(graph=graph) as session:
    # We must initialize all variables before we use them.
    init.run()
    print('Initialized')

    average_loss = 0
    for step in xrange(num_steps):
        batch_inputs, batch_labels = generate_batch(
            batch_size, num_skips, skip_window)
        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}

    # We perform one update step by evaluating the optimizer op (including it
    # in the list of returned values for session.run()
    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
    average_loss += loss_val

    if step % 2000 == 0:
        if step &gt; 0:
            average_loss /= 2000
        # The average loss is an estimate of the loss over the last 2000 batches.
        print('Average loss at step ', step, ': ', average_loss)
        average_loss = 0

    # Note that this is expensive (~20% slowdown if computed every 500 steps)
    if step % 10000 == 0:
        sim = similarity.eval()
        for i in xrange(valid_size):
            valid_word = reverse_dictionary[valid_examples[i]]
            top_k = 8  # number of nearest neighbors
            nearest = (-sim[i, :]).argsort()[1:top_k + 1]
            log_str = 'Nearest to %s:' % valid_word
            for k in xrange(top_k):
                close_word = reverse_dictionary[nearest[k]]
                log_str = '%s %s,' % (log_str, close_word)
            print(log_str)
  final_embeddings = normalized_embeddings.eval()
</code></pre>

<p>In the TensorFlow example, if I perform T-SNE on the embeddings and plot with matplotlib, the plot looks more reasonable to me when the number of steps is high. 
I am using a small corpus of 1,200 emails. One way it looks more reasonable is that numbers are clustered together. I would like to attain the same apparent level of quality using gensim.</p>
","814438","","712995","","2017-12-26 20:20:36","2017-12-26 20:20:36","Gensim equivalent of training steps","<python><tensorflow><nlp><word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"53852871","1","53856028","","2018-12-19 14:03:18","","1","528","<p>I am using Gensim's Doc2vec to train a model, and I use the infer_vector to infer the vector of a new document to compare the similarity document of the model. However, reusing the same document can have very different results. This way there is no way to accurately evaluate similar documents.<br>
The search network mentions that infer_vector has random characteristics, so each time a new text vector is produced, it will be different.<br>
Is there any way to solve this problem?</p>

<pre><code>model_dm =pickle.load(model_pickle)

inferred_vector_dm = model_dm.infer_vector(i)  

simsinput =model_dm.docvecs.most_similar([inferred_vector_dm],topn=10)
</code></pre>
","10811306","","10354154","","2018-12-19 16:04:58","2018-12-19 17:05:09","How to improve the reproducibility of Doc2vec cosine similarity","<python-3.x><nlp><gensim><similarity><doc2vec>","2","0","1","","","CC BY-SA 4.0"
"64876611","1","64880690","","2020-11-17 13:52:56","","0","179","<p>I'm trying to train a Doc2Vec model in order to create a multi-label text classifier.<br />
In order to do that, i have chosen a data set that contains approximately 70000 article, and every article contains between 1500 and 2000 words.<br />
These articles are divided into 5 classes.<br />
while setting up my input, i chosen as tag for my document their corresponding label.
I have done it as follow :
<code>tagged_article = data.apply(lambda r: TaggedDocument(words=r['article'].split(), tags=[r.labels]), axis=1)</code><br />
then i have trained my model with the following line codes:</p>
<pre><code>model_dbow = Doc2Vec(dm=1, vector_size=300, negative=5, min_count=10, workers=cores)
model_dbow.build_vocab([x for x in tqdm(tagged_article.values)])

print(&quot;Training the Doc2Vec model for &quot;, no_epochs, &quot;number of epochs&quot; )
for epoch in range(no_epochs):
     model_dbow.train(utils.shuffle([x for x in tqdm(tagged_article.values)]),total_examples=len(tagged_article.values), epochs=1)
     model_dbow.alpha -= 0.002
     model_dbow.min_alpha = model_dbow.alpha   
</code></pre>
<p>After that I have created a logistic regression model in order to predict tags for every article.</p>
<p>To do that I have created the following functions:\</p>
<pre><code>def vec_for_learning(model, tagged_docs):
sents = tagged_docs.values
targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=inference_steps)) for doc in tqdm(sents)])
return targets, regressors

y_train, X_train = vec_for_learning(model_dbow, tagged_article)

logreg = LogisticRegression(solver='lbfgs',max_iter=1000)
logreg.fit(X_train, y_train)
</code></pre>
<p>Unfortunately i am getting a very bad result. In fact I'm getting 22% as accuracy rate and 21 % as an F1 score</p>
<p>Can you please explain me why i am getting these bad results.</p>
","14605205","","","","","2020-11-17 17:54:54","Low accuracy rate after training Doc2Vec model","<python><nlp><data-science><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"52725193","1","","","2018-10-09 16:08:38","","2","2431","<p>I did some research and found that gensim has a script to convert glove to word2vec <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/scripts/glove2word2vec.py"" rel=""nofollow noreferrer"">GLove2Wrod2Vec</a>. I am looking to do the opposite.</p>

<p>Is there any simple way to convert using gensim or any other library</p>
","771318","","771318","","2018-10-09 18:40:03","2018-10-19 11:53:53","How to convert word2vec to glove format","<python><nlp><gensim><word2vec><word-embedding>","1","0","1","","","CC BY-SA 4.0"
"56106821","1","","","2019-05-13 06:29:31","","0","296","<p>I'm currently learning gensim doc2model in Python3.6 to see similarity between sentences.
I created a model but it returns <code>KeyError: ""word 'WORD' not in vocabulary""</code> when I input a word which obviously exists in the training dataset, to find a similar word/sentence.
Does it automatically skip some words not very important to define sentences? or is that simply a bug or something?
Very appreciated if I could have any way out to cover all the appearing words in the dataset. thanks.</p>
","9191983","","","","","2019-05-13 17:16:54","gensim doc2vec Model doesn't learn some words","<python><gensim><doc2vec>","1","3","","","","CC BY-SA 4.0"
"5497235","1","","","2011-03-31 08:41:48","","0","1194","<p>I have read several papers in which they actually initialize parameteres using a seed set of words for LDA. does anyone know how is this possible in gensim package?</p>
","313245","","342473","","2012-03-20 11:28:23","2012-03-20 11:28:23","How to initialize LDA with a set of seed words using gensim package","<python><gensim>","1","0","1","","","CC BY-SA 2.5"
"47598369","1","47599651","","2017-12-01 17:19:25","","0","122","<p>I found <a href=""https://stackoverflow.com/questions/36790867"">this question</a> which provides evidence that sentence order probably matters (but effect can be also a result of different random initialization).</p>

<p>I want to process <a href=""https://files.pushshift.io/reddit/comments/"" rel=""nofollow noreferrer"">Reddit comment dumps</a> for my project, but the strings extracted from json would be unsorted and belong to very different subreddits and topics, so I don't want to mess up contexts:</p>

<pre><code>{""gilded"":0,""author_flair_text"":""Male"",""author_flair_css_class"":""male"",""retrieved_on"":1425124228,""ups"":3,""subreddit_id"":""t5_2s30g"",""edited"":false,""controversiality"":0,""parent_id"":""t1_cnapn0k"",""subreddit"":""AskMen"",""body"":""I can't agree with passing the blame, but I'm glad to hear it's at least helping you with the anxiety. I went the other direction and started taking responsibility for everything. I had to realize that people make mistakes including myself and it's gonna be alright. I don't have to be shackled to my mistakes and I don't have to be afraid of making them. "",""created_utc"":""1420070668"",""downs"":0,""score"":3,""author"":""TheDukeofEtown"",""archived"":false,""distinguished"":null,""id"":""cnasd6x"",""score_hidden"":false,""name"":""t1_cnasd6x"",""link_id"":""t3_2qyhmp""}
</code></pre>

<p>So does the neighbor sentences matter for gensim word2vec and should I recover whole comment tree structure, or I can simply extract ""bag of sentences"" and train the model on it?</p>
","1692060","","","","","2017-12-01 18:44:09","Does word2vec realization from gensim go beyond sentence level when examining context?","<word2vec><gensim><word-embedding>","1","0","","","","CC BY-SA 3.0"
"45825532","1","45832150","","2017-08-22 19:28:32","","2","615","<p>I am training a word2vec model using gensim on 800k browser useragent. My dictionary size is between 300 and 1000 depending on the word frequency limit.
I am looking at few embedding vectors and similarities to see if the algorithm has been converged.
here is my code:</p>

<pre><code>wv_sim_min_count_stat={}
window=7;min_count=50;worker=10;size=128
total_iterate=1000
from copy import copy
for min_count in [50,100,500]:
    print(min_count)

    wv_sim_min_count_stat[min_count]={}
    model=gensim.models.Word2Vec(size=size,window=window,min_count=min_count,iter=1,sg=1)
    model.build_vocab(ua_parsed)


    wv_sim_min_count_stat[min_count]['vocab_counts']=[len(ua_parsed),len(model.wv.vocab),len(model.wv.vocab)/len(ua_parsed)]
    wv_sim_min_count_stat[min_count]['test']=[]

    alphas=np.arange(0.025,0.001,(0.001-0.025)/(total_iterate+1))
    for i in range(total_iterate):
        model.train(ua_parsed,total_examples=model.corpus_count,
                    epochs=model.iter,start_alpha=alphas[i],end_alpha=alphas[i+1])

        wv_sim_min_count_stat[min_count]['test'].append(
        (copy(model.wv['iphone']),copy(model.wv['(windows']),copy(model.wv['mobile']),copy(model.wv['(ipad;']),copy(model.wv['ios']),
         model.similarity('(ipad;','ios')))
</code></pre>

<p>unfortunately even after 1000 epochs there is no sign of convergence in embedding vectors. for example I plot embedding of the first dimension of '(ipad''s embedding vector vs number of epochs below:</p>

<pre><code>for min_count in [50,100,500]:
    plt.plot(np.stack(list(zip(*wv_sim_min_count_stat[min_count]['test']))[3])[:,1],label=str(min_count))

plt.legend() 
</code></pre>

<p><a href=""https://i.stack.imgur.com/dfsJI.png"" rel=""nofollow noreferrer"">embedding of '(ipad' vs number of epochs</a></p>

<p>I looked at many blogs and papers and it seems nobody trained the word2vec beyond 100 epochs. What I am missing here?</p>
","8502077","","","","","2017-08-23 06:29:04","embedded vectors doesn't converge in gensim","<python><gensim><convergence><word-embedding>","1","0","","","","CC BY-SA 3.0"
"57097233","1","","","2019-07-18 14:44:39","","1","429","<p>I want to compare the similarity between two strings, I can calculate the wmd distance with a word2vec model or with a doc2vec model in gensim. But I could not understand how does wmd work for a doc2vec model. </p>

<pre><code>def preprocess(doc):    
    return doc.lower().split()

s1 = 'i would like five rooms'
s2 = 'i would like four rooms'
s1 = preprocess(s1)
s2 = preprocess(s2)

model1 = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model, binary = False) 
d1 = model1.wmdistance(s1, s2)
print('wmd distance using a word2vec model:', d1)

model2 = gensim.models.Doc2Vec.load(doc2vec_model)
d2 = model2.wmdistance(s1, s2)
print('wmd distance using a doc2vec model:', d2)

# wmd distance using a word2vec model: 0.502799493163681
# wmd distance using a doc2vec model: 0.008121068463511764
</code></pre>

<p>Does wmd still take the word embeddings for each word from the doc2vec model as it does with a word2vec model? Is there no difference with a word2vec model or a doc2vec model to calculate the wmd distance? In the below example, the wmd distances calculated from these 2 models are very different, why is this? I understand how wmd works generally for two sentences, but I just cannot figure out how it works for a doc2vec model. I would appreciate it if someone can help me understand it.</p>
","11803564","","","","","2019-07-18 17:56:12","What is the wmdistance for a word2vec model and for a doc2vec model in gensim?","<gensim>","1","0","","","","CC BY-SA 4.0"
"51245689","1","51256902","","2018-07-09 12:32:10","","5","5222","<p>I am trying to understand the <code>epochs</code> parameter in the <code>Doc2Vec</code> function and <code>epochs</code> parameter in the <code>train</code> function. </p>

<p>In the following code snippet, I manually set up a loop of 4000 iterations. Is it required or passing 4000 as epochs parameter in the Doc2Vec enough? Also how <code>epochs</code> in <code>Doc2Vec</code> is different from epochs in <code>train</code>?</p>

<pre><code>documents = Documents(train_set)

model = Doc2Vec(vector_size=100, dbow_words=1, dm=0, epochs=4000,  window=5,
                seed=1337, min_count=5, workers=4, alpha=0.001, min_alpha=0.025)

model.build_vocab(documents)

for epoch in range(model.epochs):
    print(""epoch ""+str(epoch))
    model.train(documents, total_examples=total_length, epochs=1)
    ckpnt = model_name+""_epoch_""+str(epoch)
    model.save(ckpnt)
    print(""Saving {}"".format(ckpnt))
</code></pre>

<p>Also, how and when are the weights updated?</p>
","648138","","","","","2018-07-10 03:37:44","What does epochs mean in Doc2Vec and train when I have to manually run the iteration?","<python><gensim><doc2vec>","1","1","","","","CC BY-SA 4.0"
"56105853","1","","","2019-05-13 04:44:50","","0","171","<p>I have textual data that I want to discover topics it has, I used trained doc2vec on large corpus such as Wikipedia, but there is inconsistency in the results. Is there a better approach to discover topics.</p>
","4586806","","","","","2019-05-15 06:22:20","How to detect topics in arbitrary text file or data? not knowing number of topics beforehand","<python><nltk><gensim><word2vec><lda>","1","0","","","","CC BY-SA 4.0"
"50033595","1","50104495","","2018-04-26 01:39:21","","3","558","<p>I am trying to understand how LDA can be used for text-retrieval, and I am currently using the gensim's LdaModel model for implementing LDA, here: <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a>. </p>

<p>I have managed to identify the k topics and their most-used words, and I understand that LDA is about probabilistic distributions of topics and how words are distributed within those topics in the documents, so that much makes sense. </p>

<p>That said, I do not understand how to use the LdaModel to retrieve the documents that are relevant to a string input of search query eg ""negative effects of birth control"". I have tried inferring topic distributions on the search query and finding similarities between the topic distribution on the search query and the topic distributions from the corpus using gensim's similarities.MatrixSimilarity to compute cosine similarity like so:</p>

<p><code>lda = LdaModel(corpus, num_topics=10)
 index = similarities.MatrixSimilarity(lda[corpus])
 query = lda[query_bow]
 sims = index[query]</code></p>

<p>But the performance isn't really good. What I figure is that finding the topic distribution of the search query is not too meaningful because there is usually only 1 topic in the search query. But I don't know how else I could implement this on the LdaModel on gensim. Any advice would be really appreciated, I am new to topic modeling and maybe I am missing something that's glaringly obvious to me? Thanks!</p>
","9701095","","","","","2019-09-02 08:41:32","How to use gensim's LDA to conduct text-retrievals from queries?","<gensim><information-retrieval><lda><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"47598646","1","47805215","","2017-12-01 17:36:25","","0","661","<p>There are many different ways in which tf and idf can be calculated. I want to know which formula is used by gensim in its LSA model. I have been going through its source code <code>lsimodel.py</code>, but it is not obvious to me where the document-term matrix is created (probably because of memory optimizations).</p>

<p>In <a href=""http://www.ling.ohio-state.edu/~reidy.16/LSAtutorial.pdf"" rel=""nofollow noreferrer"">one LSA paper</a>, I read that each cell of the document-term matrix is the log-frequency of that word in that document, divided by the entropy of that word:</p>

<pre><code>tf(w, d) = log(1 + frequency(w, d))
idf(w, D) = 1 / (-Œ£_D p(w) log p(w))
</code></pre>

<p>However, this seems to be a very unusual formulation of tf-idf. A more familiar form of tf-idf is:</p>

<pre><code>tf(w, d) = frequency(w, d)
idf(w, D) = log(|D| / |{d ‚àà D: w ‚àà d}|)
</code></pre>

<p>I also notice that there is a <a href=""https://stackoverflow.com/questions/9470479/how-is-tf-idf-implemented-in-gensim-tool-in-python"">question on how the <code>TfIdfModel</code> itself is implemented in gensim</a>. However, I didn't see <code>lsimodel.py</code> importing <code>TfIdfModel</code>, and therefore can only assume that <code>lsimodel.py</code> has its own implementation of tf-idf.</p>
","2816194","","","","","2017-12-14 03:00:16","Which formula of tf-idf does the LSA model of gensim use?","<gensim><tf-idf><latent-semantic-indexing><latent-semantic-analysis>","1","0","","","","CC BY-SA 3.0"
"66056396","1","","","2021-02-05 01:36:23","","0","124","<p>I'm trying to download then load a pre-trained model with the gensim downloader to/from a specific location. It looks like I can successfully specify the download location using <code>gensim.downloader.BASE_DIR</code> but then I can't figure out how to load from that location.</p>
<p>I'm currently downloading the model using:</p>
<pre><code>import gensim.downloader as api

api.BASE_DIR= 'mnt/project/models'
</code></pre>
<p>This has worked so far - I can see the model being downloaded to the correct location - but then I can't access the model. This code:</p>
<pre><code>model = api.load('glove-twitter-25')
</code></pre>
<p>Results in an error:</p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/mnt/user/gensim-data/glove-twitter-25/glove-twitter-25.gz'
</code></pre>
<p>Is there a way to specify the directory to load the model from?</p>
","5504892","","","","","2021-02-05 02:38:39","Why can't I load from a specific filepath with gensim downloader?","<python><filepath><gensim>","1","0","","","","CC BY-SA 4.0"
"64887979","1","64899249","","2020-11-18 06:13:27","","1","1705","<p>Usually, I can use the following code to download the word vector package in jupyter lab:</p>
<pre><code>import gensim.downloader as api
word_vectors = api.load(&quot;glove-wiki-gigaword-50&quot;)
</code></pre>
<p>But now, i am using a windows server, which has a firewall. So this way does not work anymore. I also tried the way on <a href=""https://github.com/RaRe-Technologies/gensim-data"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim-data</a>:</p>
<pre><code>python -m gensim.downloader --download glove-twitter-25 
</code></pre>
<p>But I do not know how to set up the proxy in this line. If my proxy is <a href=""http://my-proxy.com:80"" rel=""nofollow noreferrer"">http://my-proxy.com:80</a>. Then how can I download the word vector behind a firewall?</p>
","4827407","","","","","2020-11-18 18:36:30","How to download glove-wiki-gigaword-100 or other word vector package using gensim.downloader behind a proxy?","<api><vector><download><gensim><word>","1","0","1","","","CC BY-SA 4.0"
"50492676","1","50499090","","2018-05-23 15:50:18","","12","6632","<p>I've only seen a few questions that ask this, and none of them have an answer yet, so I thought I might as well try. I've been using gensim's word2vec model to create some vectors. I exported them into text, and tried importing it on tensorflow's live model of the embedding projector. One problem. <em>It didn't work</em>. It told me that the tensors were improperly formatted. So, being a beginner, I thought I would ask some people with more experience about possible solutions.<br>
Equivalent to my code:  </p>

<pre><code>import gensim
corpus = [[""words"",""in"",""sentence"",""one""],[""words"",""in"",""sentence"",""two""]]
model = gensim.models.Word2Vec(iter = 5,size = 64)
model.build_vocab(corpus)
# save memory
vectors = model.wv
del model
vectors.save_word2vec_format(""vect.txt"",binary = False)
</code></pre>

<p>That creates the model, saves the vectors, and then prints the results out nice and pretty in a tab delimited file with values for all of the dimensions. I understand how to do what I'm doing, I just can't figure out what's wrong with the way I put it in tensorflow, as the documentation regarding that is pretty scarce as far as I can tell.<br>
One idea that has been presented to me is implementing the appropriate tensorflow code, but I don‚Äôt know how to code that, just import files in the live demo.  </p>

<p>Edit: I have a new problem now. The object I have my vectors in is non-iterable because gensim apparently decided to make its own data structures that are non-compatible with what I'm trying to do.<br>
  Ok. Done with that too! Thanks for your help!</p>
","9205824","","9205824","","2018-05-24 19:06:32","2020-03-11 14:31:33","Visualize Gensim Word2vec Embeddings in Tensorboard Projector","<python><tensorflow><gensim><tensorboard><word-embedding>","3","3","5","","","CC BY-SA 4.0"
"47604717","1","47604819","","2017-12-02 04:50:50","","1","360","<p>I'm using the <a href=""https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phraser.save"" rel=""nofollow noreferrer"">save method</a> on the Gensim Phrases class to store a model for future use but if I update my version of Gensim, I have problems loading that model back in. For example, I get the following error when loading a model in Gensim 2.3.0 that was made in 2.2.0:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;()

~/Stuff/Sources/anaconda3/envs/nlp/lib/python3.6/site-packages/gensim/models/phrases.py in __init__(self, phrases_model)
    395         self.min_count = phrases_model.min_count
    396         self.delimiter = phrases_model.delimiter
--&gt; 397         self.scoring = phrases_model.scoring
    398         self.phrasegrams = {}
    399         corpus = pseudocorpus(phrases_model.vocab, phrases_model.delimiter)

AttributeError: 'Phrases' object has no attribute 'scoring'
</code></pre>

<p>Is there a better way to ensure forwards compatibility?</p>
","600413","","","","","2017-12-02 05:18:39","How do I save a Gensim model while ensuring forwards compatibility?","<python><machine-learning><nlp><data-science><gensim>","1","0","1","","","CC BY-SA 3.0"
"59050644","1","59056615","","2019-11-26 12:07:13","","2","28323","<p>I am trying to train the word2vec model from Wikipedia text data, for that I am using following code.</p>

<pre><code>import logging
import os.path
import sys
import multiprocessing

from gensim.corpora import  WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence


if __name__ == '__main__':
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info(""running %s"" % ' '.join(sys.argv))

    # check and process input arguments

    if len(sys.argv) &lt; 3:
        print (globals()['__doc__'])
        sys.exit(1)
    inp, outp = sys.argv[1:3]

    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count())

    # trim unneeded model memory = use (much) less RAM
    model.init_sims(replace=True)

    model.save(outp)
</code></pre>

<p>But after 20 minutes of program running, I am getting following error</p>

<p><a href=""https://i.stack.imgur.com/6BjOz.png"" rel=""nofollow noreferrer"">Error message</a></p>
","12437434","","2973474","","2019-11-26 12:20:08","2021-07-27 17:58:45","MemoryError: unable to allocate array with shape and data type float32 while using word2vec in python","<python><multiprocessing><python-multiprocessing><gensim><word2vec>","3","3","","","","CC BY-SA 4.0"
"57100323","1","","","2019-07-18 18:03:42","","0","40","<p>I have this data (the data is generated in R) and I use the <code>reticulate</code> package to port over to Python. The problem is with my Python code.</p>

<p>R code :</p>

<pre><code>text &lt;- c(""Because I could not stop for Death -"",
          ""He kindly stopped for me -"",
          ""The Carriage held but just Ourselves -"",
          ""and Immortality"")

ID &lt;- c(1,2,3,4)    
df &lt;- data.frame(cbind(ID, text))
library(reticulate)

df_py &lt;- r_to_py(df)
repl_python()
</code></pre>

<p>Python code :</p>

<pre><code>import gensim
LabeledSentence1 = gensim.models.doc2vec.TaggedDocument
all_content_data = []
j = 0
for em in r.df_py['text'].values:
  all_content_data.append(LabeledSentence1(em,[j]))
j+=1
print('Number of texts processed: ', j)
</code></pre>

<p>Note: The <code>r.df_py['text']</code> is a ""special"" function which calls R data, it can be changed to <code>df_py['text']</code> if just using Python. </p>

<p>The data is supposed to process the documents but when I print it says <code>Number of texts processed:  1</code> when it should say <code>Number of texts processed:  4</code>. I just don't know where I am going wrong in that function. My data is a data frame and in each row I have a unique ""book"" all the text of that book is in one cell and I want to process that cell.</p>
","6447399","","5770501","","2019-07-18 18:06:23","2019-07-18 18:08:53","Number of texted processed = 1 when it should = 4 (function to process documents)","<python><r><gensim>","2","2","","","","CC BY-SA 4.0"
"68365830","1","","","2021-07-13 15:59:19","","0","13","<p>I am trying to implement a document similarity API using the LDA Model of Gensim. To experiment with the performance, I tried implementing it by training the LDA Model with TfIdf vectors instead of the normal BoW corpus as described in the documentation. The problem which I am facing is that while using the Similarity API of Gensim for creating the index and finding out the similarity score, what I encountered is that if I try to match the same document with itself, sometimes, the Similarity values are not ~1. The values which I get are as low as ~0.06. This does not occur ALL the time, but for some documents only. I tested this again with 229 documents matching each document with itself, and I found that 45 of the documents give results less than 0.98, sometimes giving values like 0.65, 0.41 and similar. I would like some help on this, whether I am doing something wrong or is it any other problem.</p>
<h4 id=""minimal-code-used-for-testing-6nj0"">Minimal Code used for testing:</h4>
<pre class=""lang-py prettyprint-override""><code>docs = [ 'Document 1 as a string', 'Document 2 as a string', 'Document 3 as a string', 'and so on.....' ]
cleaned_docs = list(map(clean_function, docs))                 # Here, clean_function return tokens for each string. So, cleaned_docs is essentially a list of list of strings List[List[str]]
bow_corpus = [dictionary.doc2bow(i) for i in cleaned_docs]
tfidf_corpus = tfidf_model[bow_corpus]
lda_corpus = lda_model[tfidf_corpus]
index = Similarity(lda_corpus)
sims = index[lda_corpus]                   # Getting similarity for all combinations. Got a (229, 229) array for my case
final_sims = np.diag(sims)                 # Getting similarity with itself
print(final_sims)                          # Getting very low score with some docs
</code></pre>
<p>Output Vectors of LDAModel for 3 documents out of the 45 I got low score for reference:</p>
<pre class=""lang-py prettyprint-override""><code>[[(0, 0.17789464), (2, 0.03806097), (12, 0.2273234), (14, 0.08613937), (21, 0.13261063), (22, 0.17807047), (36, 0.058883864)],
[(1, 0.43381935), (2, 0.14317065), (3, 0.07986226), (36, 0.062136874)], 
[(0, 0.32848448), (2, 0.16667062), (14, 0.0485237), (15, 0.11480027), (18, 0.086506054), (35, 0.059970867)]]
</code></pre>
","10212627","","","","","2021-07-13 15:59:19","Similarity Interface of Gensim giving low similarity score for exact same documents with TfIdf + LdaModel","<python><gensim><similarity><tf-idf><lda>","0","0","","","","CC BY-SA 4.0"
"50038347","1","50048276","","2018-04-26 08:31:46","","4","726","<p>While learning Doc2Vec library, I got stuck on the following question.</p>

<p><strong>Do gensim Doc2Vec distinguish between the same Sentence with positive and negative context?</strong></p>

<p>For Example:</p>

<p>Sentence A: ""I love Machine Learning""</p>

<p>Sentence B: ""I do not love Machine Learning""</p>

<p>If I train sentence A and B with doc2vec and find cosine similarity between their vectors:</p>

<ol>
<li>Will the model be able to distinguish the sentence and give a cosine similarity very less than 1 or negative?</li>
<li>Or Will the model represent both the sentences very close in vector space and give cosine similarity close to 1, as mostly all the words are same except the negative word (do not).</li>
</ol>

<p>Also, If I train only on sentence A and try to infer Sentence B, will both vectors be close to each other in vector space.?</p>

<p>I would request the NLP community and Doc2Vec experts for helping me out in understanding this.</p>

<p>Thanks in Advance !!</p>
","7397514","","7397514","","2018-04-26 08:39:38","2021-01-27 15:52:45","Do gensim Doc2Vec distinguish between same Sentence with positive and negative context.?","<python><nlp><gensim><doc2vec>","2","0","2","","","CC BY-SA 3.0"
"50038358","1","","","2018-04-26 08:32:17","","2","859","<p>What is the best way to visualize a Word2Vec model using TensorFlow's Embedding Projector?
is there a way to export the Word2Vec model's vectors to the format that Embedding Projector expects? or is there a built in function in tensorflow for that?</p>

<p>Thanks!</p>
","9160882","","","","","2019-05-07 18:07:39","Visualize a Word2Vec model using Embedding Projector","<tensorflow><nlp><data-visualization><word2vec><gensim>","1","2","1","","","CC BY-SA 3.0"
"51252324","1","51256843","","2018-07-09 18:57:07","","0","970","<p>I am trying to use gensim's doc2vec to create a model which will be trained on a set of documents and a set of labels. The labels were created manually and need to be put into the program to be trained on. So far I have 2 lists: a list of sentences, and a list of labels corresponding to that sentence. I need to use doc2vec specifically. Here is what I have tried so far.</p>

<pre><code>from gensim import utils
from gensim.models import Doc2Vec

tweets = [""A tweet"", ""Another tweet"", ""A third tweet"", ... , ""A thousandth-something tweet""]
labels_list = [1, 1, 3, ... , 16]

tagged_data = [tweets, labels_list]
model = Doc2Vec(size=20, alpha=0.025, min_alpha=0.00025, min_count=1, dm=1)
model.build_vocab(tagged_data)
for epoch in range(max_epochs):
    model.train(tagged_data, total_examples=model.corpus_count, 
epochs=model.iter)
    model.alpha -= 0.0002
    model.min_alpha = model.alpha
</code></pre>

<p>I am getting the error on the line with <code>model.build_vocab(tagged_data)</code> that there is an <code>AttributeError: 'list' object has no attribute 'words'</code>. I googled this and it says to put it into a labeled sentence object, but I am not sure if that will work if I have predefined labels. So does anyone know how to put pre-defined labels into doc2vec? Thanks in advance.</p>
","7700994","","","","","2018-07-10 03:30:43","Doc2Vec gensim with supervised data predefined labels","<python><gensim><supervised-learning><doc2vec>","1","0","","","","CC BY-SA 4.0"
"68377628","1","","","2021-07-14 11:56:04","","2","435","<p>As we all know the capability of <code>BERT</code> model for word embedding, it is probably better than the <code>word2vec</code> and any other models.</p>
<p>I want to create a model on <code>BERT</code> word embedding to generate synonyms or similar words. The same like we do in the <code>Gensim</code> <code>Word2Vec</code>. I want to create method of Gensim <code>model.most_similar()</code> into BERT word embedding.</p>
<p>I researched a lot about it, seems that it is possible to do that, but the problem is it is only showing the embeddings in the form of number, there is no way to get the actual word from it. Can anybody help me regarding this?</p>
","11152653","","","","","2021-07-14 14:05:33","Using BERT to generate similar word or synonyms through word embeddings","<python><nlp><gensim><word2vec><bert-language-model>","1","0","","","","CC BY-SA 4.0"
"66813857","1","66825689","","2021-03-26 09:05:13","","0","113","<p>I have a trained word2vec model which I need to train further with more data. I want to use the same hyperparameters that is used while training the model for the new model as well. But I don't want to hardcode it. Is there a method which I can use to get the hyperparameters used while training the existing model.
I am using Gensim word2vec.</p>
","12856596","","","","","2021-03-26 23:53:22","How to get back hyperparameters from a trained world2vec model gensim?","<python><gensim><word2vec><hyperparameters>","2","0","","","","CC BY-SA 4.0"
"63749621","1","","","2020-09-05 01:17:56","","0","717","<p>I have been trying topic modelling using gensim in Python. I have the following dataset:</p>
<p>Docs</p>
<pre><code>&quot;Sugar is bad to consume. My sister likes to have sugar, but not my father.&quot;
&quot;My father spends a lot of time driving my sister around to dance practice.&quot;
&quot;Doctors suggest that driving may cause increased stress and blood pressure.&quot;
&quot;Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.&quot;
&quot;Health experts say that Sugar is not good for your lifestyle.&quot;
</code></pre>
<p>I tried to lemmatise it as follows:</p>
<pre><code>texts = map(gensim.utils.lemmatize,Docs)
</code></pre>
<p>and run LDA:</p>
<pre><code>dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(doc) for doc in texts]
Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(corpus, num_topics=3, id2word = dictionary, passes=50)
ldamodel.print_topics()
</code></pre>
<p>However I am getting an error. Do you know how to fix it?</p>
<p>thanks</p>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-15-b36df3b5374b&gt; in &lt;module&gt;
----&gt; 1 import pattern
      2 
      3 dictionary = gensim.corpora.Dictionary(Docs)
      4 corpus = [dictionary.doc2bow(doc) for doc in Docs]
      5 Lda = gensim.models.ldamodel.LdaModel

ModuleNotFoundError: No module named 'pattern'
</code></pre>
<p>The whole error message:</p>
<pre><code>---&gt; 3 dictionary = gensim.corpora.Dictionary(Docs)
      4 corpus = [dictionary.doc2bow(doc) for doc in Docs]
      5 Lda = gensim.models.ldamodel.LdaModel

/anaconda3/lib/python3.7/site-packages/gensim/corpora/dictionary.py in __init__(self, documents, prune_at)
     82 
     83         if documents is not None:
---&gt; 84             self.add_documents(documents, prune_at=prune_at)
     85 
     86     def __getitem__(self, tokenid):

/anaconda3/lib/python3.7/site-packages/gensim/corpora/dictionary.py in add_documents(self, documents, prune_at)
    195 
    196         &quot;&quot;&quot;
--&gt; 197         for docno, document in enumerate(documents):
    198             # log progress &amp; run a regular check for pruning, once every 10k docs
    199             if docno % 10000 == 0:

/anaconda3/lib/python3.7/site-packages/gensim/utils.py in lemmatize(content, allowed_tags, light, stopwords, min_length, max_length)
   1676     if not has_pattern():
   1677         raise ImportError(
-&gt; 1678             &quot;Pattern library is not installed. Pattern library is needed in order to use lemmatize function&quot;
   1679         )
   1680     from pattern.en import parse

ImportError: Pattern library is not installed. Pattern library is needed in order to use lemmatize function
</code></pre>
","7788837","","7788837","","2020-09-05 21:22:51","2020-09-05 21:47:48","Topic modelling with gensim","<python><gensim><topic-modeling>","1","4","","","","CC BY-SA 4.0"
"60081431","1","","","2020-02-05 17:50:25","","1","186","<p>I am trying to load a fasttext .bin model in spanish, donwloaded from <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a> and continue training it with new sentences from the specific domain I am interested in.  </p>

<p>System: Anaconda, Jupyter Notebook, Python 3.6, Upgraded Gensim </p>

<p>My code (toy example):</p>

<pre><code>from gensim.models.fasttext import load_facebook_model
import os
os.chdir('path/to/directory')
model = load_facebook_model('cc.es.300.bin')

'enmadrarse' in model.wv.vocab
&gt;&gt;&gt; False
old_vector = np.copy(model.wv['enmadrarse'])

new_sentences = [['complexidad', 'cataratas', 'enmadrarse'],
['enmadrarse', 'cataratas', 'increibles'], 
['unidad','enmadrarse','complexa']]

model.build_vocab(new_sentences, update = True)
model.train(new_sentences, total_examples = len(new_sentences), epochs=model.epochs)

new_vector = np.copy(model.wv['enmadrarse'])
np.allclose(old_vector, new_vector, atol=1e-4)
&gt;&gt;&gt; True

'enmadrarse' in model.wv.vocab
&gt;&gt;&gt; False (still)
</code></pre>

<p>The old and new vectors of the word are equal and it remains out of the vocab so the model learnt nothing. What am I doing wrong? </p>
","6409883","","","","","2020-02-05 17:50:25","Fasttext model loaded with gensim won't continue training with new sentences","<python><gensim><fasttext><resuming-training>","0","2","1","","","CC BY-SA 4.0"
"46157937","1","46161753","","2017-09-11 14:19:26","","0","703","<p>I am running the gensim word2vec code on a corpus of resumes(stopwords removed) to identify similar context words in the corpus from a list of pre-defined keywords.</p>

<p>Despite several iterations with input parameters,stopword removal etc the similar context words are not at all making sense(in terms of distance or context)
Eg. correlation and matrix occurs in the same window several times yet matrix doesnt fall in the most_similar results for correlation </p>

<p>Following are the details of the system and codes
gensim 2.3.0 ,Running on Python 2.7 Anaconda
Training Resumes :55,418 sentences
Average words per sentence : 3-4 words(post stopwords removal)
Code :</p>

<pre><code>    wordvec_min_count=int()
    size = 50
    window=10
    min_count=5
    iter=50
    sample=0.001
    workers=multiprocessing.cpu_count()
    sg=1
    bigram = gensim.models.Phrases(sentences, min_count=10, threshold=5.0)
    trigram = gensim.models.Phrases(bigram[sentences], min_count=10, threshold=5.0)
    model=gensim.models.Word2Vec(sentences = trigram[sentences], size=size, alpha=0.005, window=window, min_count=min_count,max_vocab_size=None,sample=sample, seed=1, workers=workers, min_alpha=0.0001, sg=sg, hs=1, negative=0, cbow_mean=1,iter=iter)

model.wv.most_similar('correlation')
Out[20]: 
[(u'rankings', 0.5009744167327881),
 (u'salesmen', 0.4948525130748749),
 (u'hackathon', 0.47931140661239624),
 (u'sachin', 0.46358123421669006),
 (u'surveys', 0.4472047984600067),
 (u'anova', 0.44710394740104675),
 (u'bass', 0.4449636936187744),
 (u'goethe', 0.4413239061832428),
 (u'sold', 0.43735259771347046),
 (u'exceptional', 0.4313117265701294)]
</code></pre>

<p>I am lost as to why the results are so random ? Is there anyway to check the accuracy for word2vec ?</p>

<p>Also is there an alternative of word2vec for most_similar() function ? I read about gloVE but was not able to install the package.</p>

<p>Any information in this regard would be helpful</p>
","968170","","","","","2017-09-11 17:59:21","Why Word2Vec's most_similar() function is giving senseless results on training?","<python-2.7><gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 3.0"
"59938853","1","","","2020-01-27 21:01:18","","1","400","<p>I am trying to tokenize the following sentence type: </p>

<p><code>""The item at issue is no. 3553.""</code></p>

<p>Every tokenizer I've tried so far returns the following (including a Punkt tokenizer trained on my corpus):</p>

<p><code>[[""the"", ""item"", ""at"", ""issue"", ""is"", ""no.""], [""3553.""]]</code></p>

<p>Adding a ""no"" abbreviation to the tokenizer would be a problem for sentences ending in ""no.""</p>
","3763732","","3763732","","2020-01-27 21:31:27","2020-02-04 06:42:24","Stop sentence tokenizer from splitting sentence on ""no."" abbreviation","<python><nlp><nltk><gensim>","2","10","","","","CC BY-SA 4.0"
"57102264","1","","","2019-07-18 20:33:38","","-1","51","<p>Assume I have a number of pictures. Let‚Äôs say 10 pictures which are annotated by 50 people each.
So Pic 1 might be ‚Äûbeach, vacation, relax, sand, sun‚Ä¶‚Äú I now trained word2vec with a domain specific content. I have the vectors of each word and can represent them. But what I want now, is to create ONE final vector representing each picture. So one vector with represents the 50 annotations (beach, vacation, relax, sand, sun‚Ä¶)</p>

<p>Let‚Äôs assume each vector is represented with 100 dimensions ‚Äì do I just add the first dimension (the 100 dimensions) of all 50 vectors, than the 2nd dimension of all 50 vectors‚Ä¶ etc.</p>

<p>I am very thankful for any comments that might help me!</p>

<p>I tried this, but I am not sure if this is the right way to do it. 
I also tried doc2vec but I guess this is problematic as the word order of the annotations is irrelevant ‚Äì but relevant for doc2vec.???</p>
","5976308","","","","","2019-07-22 18:25:27","How to calculate vectors with word2vec","<gensim>","1","0","","","","CC BY-SA 4.0"
"68420041","1","","","2021-07-17 11:20:58","","0","44","<p>I am trying to train word2vec model on a simple toy dateset of 4 sentences.
The Word2vec version that I need is:</p>
<ul>
<li>Skip-gram model</li>
<li>no negative sampling</li>
<li>no hierarchical soft-max</li>
<li>no removal or down-scaling of frequent words</li>
<li>vector size of words is 2</li>
<li>Window size 4 i.e all the words in a sentence are considered context words of each other.</li>
<li>epochs can be varied from 1 to 500</li>
</ul>
<p><strong>Problem that I am facing is:</strong> No matter how I change the above parameters, the word vectors are not being updated/learned. The word vectors for epochs=1 and epochs=500 are being same.</p>
<pre><code>from gensim.models import Word2Vec
import numpy as np
import matplotlib.pyplot as plt
import nltk

# toy dataset with 4 sentences
sents = ['what is the time',
         'what is the day',
         'what time is the meeting',
         'cancel the meeting']

sents = [nltk.word_tokenize(string) for string in sents]

# model initialization and training
model = Word2Vec(alpha=0.5, min_alpha =0.25, min_count = 0, size=2, window=4,
                 workers=1, sg = 1, hs = 0, negative = 0, sample=0, seed = 42)

model.build_vocab(sents)
model.train(sents, total_examples=4, epochs=500)

# getting word vectors into array
vocab = model.wv.vocab.keys()
vocab_vectors = model.wv[vocab]
print(vocab)
print(vocab_vectors)

#plotting word vectors
plt.scatter(vocab_vectors[:,0], vocab_vectors[:,1], c =&quot;blue&quot;)
for i, word in enumerate(vocab):
    plt.annotate(word, (vocab_vectors[i,0], vocab_vectors[i,1]))
</code></pre>
<p>The out put of <code>print(vocab)</code> is as below</p>
<p><code>['what', 'is', 'time', 'cancel', 'the', 'meeting', 'day']</code></p>
<p>The output of <code>print(vocab_vectors)</code> is as below</p>
<pre><code>[[ 0.08136337 -0.05059118]
 [ 0.06549312 -0.22880174]
 [-0.08925873 -0.124718  ]
 [ 0.05645624 -0.03120007]
 [ 0.15067646 -0.14344342]
 [-0.12645201  0.06202405]
 [-0.22905378 -0.01489289]]
</code></pre>
<p>The plotted 2D vectors <a href=""https://i.stack.imgur.com/1BknG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1BknG.png"" alt=""Here!"" /></a></p>
<p><strong>Why do I think the vectors are not being learned?</strong> I am changing the epochs value to 1, 10, 50, 500... and running the whole code to check the output for each run. For epochs = #any_value &lt;1,10,50,500&gt;, the output (vocab, vocab_vectors, and the plot) is being same for all the runs.</p>
","16468524","","16468524","","2021-07-17 17:23:54","2021-07-17 22:20:51","Gensim 3.8.3 Word2Vec is not updating the weights/parameters on a toy dataset","<python><machine-learning><nlp><gensim><word2vec>","1","2","","","","CC BY-SA 4.0"
"68367520","1","68368636","","2021-07-13 18:15:39","","1","21","<p>I am building a topic model from scratch, one step of which uses the TfidfVectorizer method to get unigrams and bigrams from my corpus of texts:</p>
<pre><code>    tfidf_vectorizer = TfidfVectorizer(min_df=0.1, max_df=0.9, ngram_range = (1,2))
</code></pre>
<p>After topics are created, I use the similarity scores provided by gensim's Word2Vec to determine coherence of topics. I do this by training on the same corpus:</p>
<pre><code>    bigram_transformer = Phrases(corpus)
    model = Word2Vec(bigram_transformer[corpus], min_count=1)
</code></pre>
<p>For many of the bigrams in my topics however, I get a KeyError because that bigram was not picked up in the training of Word2Vec, despite them being trained on the same corpus. I think this is because Word2Vec decides on which bigrams to choose based on statistical analysis (<a href=""https://stackoverflow.com/questions/60108919/why-arent-all-bigrams-created-in-gensims-phrases-tool"">Why aren&#39;t all bigrams created in gensim&#39;s `Phrases` tool?</a>)</p>
<p>Is there a way to get the Word2Vec to include all those bigrams identified by TfidfVectorizer? I see trimming capabilities such as 'trim_rule' but not anything in the other direction.</p>
","12838566","","","","","2021-07-13 19:56:48","Inconsistencies between bigrams found by TfidfVectorizer and Word2Vec model","<python><nlp><gensim><word2vec><tfidfvectorizer>","1","0","","","","CC BY-SA 4.0"
"38772040","1","","","2016-08-04 15:50:35","","1","826","<p>I use Gensim Doc2vec model to train document vectors.
I printed out representations for the word 'good', but I found every epoch, I found not updating! While I printed out representations for the document with id '3', every epoch different! </p>

<p>My codes are below, do not know what is happening. </p>

<pre><code>model = gensim.models.Doc2Vec(dm = 0, alpha=0.1, size= 20, min_alpha=0.025)

model.build_vocab(documents)

print ('Building model....',(time4-time3))
for epoch in range(10):
    model.train(documents)

    print('Now training epoch %s' % epoch)
    print(model['good'])
    print(model.docvecs[str(3)])
</code></pre>
","4250174","","","","","2016-09-27 04:30:48","Doc2vec Gensim: the word embeddings not updating during each epoch","<nlp><gensim><word2vec><doc2vec>","3","0","","","","CC BY-SA 3.0"
"68434829","1","68435055","","2021-07-19 03:32:57","","1","23","<p>Is there a way to let <code>model.wv.most_similar</code> in gensim return positive-meaning words only (i.e. that shows synonyms but not antonyms)?</p>
<p>For example, if I do:</p>
<pre><code>import fasttext.util
from gensim.models.fasttext import load_facebook_model
from gensim.models.fasttext import FastTextKeyedVectors
fasttext.util.download_model('en', if_exists='ignore')  # English
model = load_facebook_model('cc.en.300.bin')
model.wv.most_similar(positive=['honest'], topn=2000)
</code></pre>
<p>Then the mode is also going to return words such as &quot;dishonest&quot;.</p>
<pre><code>('dishonest', 0.5542981028556824),
</code></pre>
<p>However, what if I want words with the positive-meaning only?</p>
<p>I have tried the following - subtracting &quot;not&quot; from &quot;honest&quot; in the vector space:</p>
<pre><code>import fasttext.util
from gensim.models.fasttext import load_facebook_model
from gensim.models.fasttext import FastTextKeyedVectors
fasttext.util.download_model('en', if_exists='ignore')  # English
model = load_facebook_model('cc.en.300.bin')
model.wv.most_similar(positive=['honest'], negative=['not'], topn=2000)
</code></pre>
<p>But somehow it is still returning &quot;dishonest&quot; somehow.</p>
<pre><code>('dishonest', 0.23721608519554138)
('dishonesties', 0.16536088287830353)
</code></pre>
<p>Any idea how to do this in a better way?</p>
","5705174","","5705174","","2021-07-19 03:40:21","2021-07-19 04:15:09","Genesis most_similar find synonym only (not antonyms)","<python><nlp><gensim><word2vec><fasttext>","1","0","","","","CC BY-SA 4.0"
"60108919","1","60119013","","2020-02-07 07:30:54","","1","669","<p>I have created a bigram model using gensim and the try to get the bigram sentences but it's not picking all bigram sentences why?</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models.phrases import Phrases, Phraser
phrases = Phrases(sentences, min_count=1, threshold=1)
bigram_model = Phraser(phrases)
sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
print(bigram_model[sent])
[u'the', u'mayor', u'of', u'new_york', u'was', u'there']
</code></pre>
<p>Can anyone explain how to get all bigrams.</p>
<h1>Why only 'new_york' not 'the_mayor' and others?</h1>
","10368811","","-1","","2020-06-20 09:12:55","2020-02-07 18:22:49","Why aren't all bigrams created in gensim's `Phrases` tool?","<python><nlp><gensim><n-gram><word-embedding>","1","2","","","","CC BY-SA 4.0"
"60085407","1","","","2020-02-05 23:06:39","","0","322","<p>I got an error:</p>

<pre><code>Error when checking input: 
expected embedding_1_input to have shape (50,) but got array with shape (1,)
</code></pre>

<p>When I change the input parameter <code>input_length</code> to 1, the error becomes:</p>

<pre><code>Error when checking input: 
expected embedding_1_input to have shape (1,) but got array with shape (50,)
</code></pre>

<p>My code is as below:</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
import numpy as np
import os
from keras import metrics
from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, BatchNormalization, Activation, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D
from keras.models import load_model
from keras.callbacks import ModelCheckpoint
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.preprocessing import sequence, text
import pandas as pd
from gensim import corpora
from gensim import models

maxlen = 50
batch_size = 128
np.random.seed(7)

df = pd.read_csv('C:/Users/DMY/Peer-logic-master/newdata/topnine.csv',encoding='utf-8')

x = df[""REVIEW""].fillna(""na"").values  
y = df[""TAG""]
encoder = LabelEncoder()
encoder.fit(y)
y = encoder.transform(y)

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1)

word_list = []

for i in range(len(x_train)):
    word_list.append(x_train[i].split(' '))


dictionary = corpora.Dictionary(word_list)
corpus = [dictionary.doc2bow(text) for text in word_list]


tfidf = models.TfidfModel(corpus)

X_train_id = [] 
word_id_dict = dictionary.token2id
for i in range(len(word_list)):
    sen_id = []
    word_sen = word_list[i]
    for j in range(len(word_sen)):       
        id = word_id_dict.get(word_sen[j])
        if id is None:
            id = 0 
        sen_id.append(id)
    X_train_id.append(sen_id)

X_train_tfidf_vec = []  
for i in range(len(x_train)):
    temp = {}
    string = x_train[i]
    string_bow = dictionary.doc2bow(string.lower().split()) 
    string_tfidf = tfidf[string_bow]

    for j in range(len(string_tfidf)):
#         print(string_tfidf[j][0])
        temp[string_tfidf[j][0]] = string_tfidf[j][1]
#         print(temp)
    X_train_tfidf_vec.append(temp)

X_train_tfidf = []  
for i in range(len(X_train_id)):
    sen_id = X_train_id[i]
    sen_id_tfidf = X_train_tfidf_vec[i]
    sen = []
    for j in range(len(sen_id)):
        word_id = sen_id[j]
        word_tfidf = sen_id_tfidf.get(word_id)
        if word_tfidf is None:
            word_tfidf = 0
        sen.append(word_tfidf)
    X_train_tfidf.append(sen)

x_train_tfidf = sequence.pad_sequences(X_train_tfidf, maxlen=maxlen,dtype='float64')
#print(len(x_train_tfidf))
#print(x_train_tfidf)

model4 = Sequential()
model4.add(Embedding(len(x_train_tfidf)+1, 100, input_length = ))#input_dim,output_dim,input_length
model4.add(Dropout(0.6))
model4.add(LSTM(100, recurrent_dropout=0.6))
model4.add(Dropout(0.6))
model4.add(Dense(1, activation='sigmoid'))
model4.compile('adam', 'binary_crossentropy', metrics=['accuracy'])
model4_history = model4.fit(x_train_tfidf, y_train, batch_size=batch_size, epochs=7,
                            validation_split=0.1)
score4, acc4 = model4.evaluate(x_test, y_test,
                               batch_size=batch_size)
print('Test accuracy for LSTM Model is:', acc4)
y_pred4 = model4.predict(x_test)
y_pred4 = (y_pred4 &gt; 0.5)
print(classification_report(y_test, y_pred4))
</code></pre>
","12848519","","7508700","","2020-02-06 00:13:06","2020-02-06 00:13:06","Error when checking input: expected embedding_1_input to have shape (50,) but got array with shape (1,)","<python><keras><lstm><gensim>","1","2","","","","CC BY-SA 4.0"
"47666699","1","47758616","","2017-12-06 04:16:35","","13","9631","<p><strong>BACKGROUND</strong></p>

<p>I have vectors with some sample data and each vector has a category name (Places,Colors,Names).</p>

<pre><code>['john','jay','dan','nathan','bob']  -&gt; 'Names'
['yellow', 'red','green'] -&gt; 'Colors'
['tokyo','bejing','washington','mumbai'] -&gt; 'Places'
</code></pre>

<p>My objective is to train a model that take a new input string and predict which category it belongs to. For example if a new input is ""purple"" then I should be able to predict 'Colors' as the correct category. If the new input is ""Calgary"" it should predict 'Places' as the correct category.</p>

<p><strong>APPROACH</strong></p>

<p>I did some research and came across <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">Word2vec</a>. This library has a ""similarity"" and ""mostsimilarity"" function which i can use. So one brute force approach I thought of is the following:</p>

<ol>
<li>Take new input.</li>
<li>Calculate it's similarity with each word in each vector and take an average.</li>
</ol>

<p>So for instance for input ""pink"" I can calculate its similarity with words in vector ""names"" take a average and then do that for the other 2 vectors also. The vector that gives me the highest similarity average would be the correct vector for the input to belong to.</p>

<p><strong>ISSUE</strong></p>

<p>Given my limited knowledge in NLP and machine learning I am not sure if that is the best approach and hence I am looking for help and suggestions on better approaches to solve my problem. I am open to all suggestions and also please point out any mistakes I may have made as I am new to machine learning and NLP world.</p>
","7042945","","","user9079195","2017-12-11 22:23:36","2018-05-14 16:43:38","Using word2vec to classify words in categories","<python><machine-learning><nlp><word2vec><gensim>","2","2","8","","","CC BY-SA 3.0"
"68429677","1","68430225","","2021-07-18 13:41:59","","0","39","<p>Hello Community Members,</p>
<p>I would like to output the 1000 most frequently used words with frequency from a Gensim Word2Vec model. However, I am not interested in certain words, which I therefore filter using numpy (np.stdiff1d).After that I create a new list using '/n'.join, but now I have the problem that every time I call an entry from the list '/n'.join is entered in front of the word (e.g. instead of house /nhouse), so I get a key error.</p>
<p>I tried to work around it by saving the list (corpus_words) as .txt and ‚Äúopen with‚Äú, but even then, there is a /n in front of each entry, when I try to get the frequency of the word.</p>
<p>to use a print statement beforer &quot;/n&quot;.join(new_list) did not help either.</p>
<p>is there any way to fix this?</p>
<pre><code>Model_Pfad = r'D:\OneDrive\Phyton\modelC.model'
ausgabe= open('D:\OneDrive\Phyton\wigbelsZahlen.txt', 'w')

model = Word2Vec.load(Model_Pfad)


x = list(model.wv.index_to_key[:1000])

stop_words = set ([&quot;an&quot;,
              'as',
              'art',
              'ab',
              'al',
            &quot;aber&quot;,
            &quot;abk.&quot;,
            &quot;alle&quot;,
            &quot;allem&quot;,
            &quot;allen&quot;,
            &quot;aller&quot;,
            &quot;alles&quot;,
            &quot;allg.&quot;
            ])

new_list = [item for item in x if item not in stop_words]

for i in new_list:
    result = model.wv.get_vecattr(i, &quot;count&quot;)
    ausgabe.write(i + '\t' + str(result))
    ausgabe.write('\n')
ausgabe.close

</code></pre>
","14686994","","14686994","","2021-07-21 15:59:54","2021-07-21 15:59:54","Want to call words from a list but there is always a /n before every entry","<python><list><nltk><gensim>","1","2","","","","CC BY-SA 4.0"
"30323899","1","","","2015-05-19 11:07:34","","1","2155","<p>I want to use the gensim library in a python code and need to execute that code from Java.</p>

<p>Following is the python code, sent2vec.py:</p>

<pre><code>import gensim
sentences = gensim.models.doc2vec.LabeledLineSentence('/tmp/sentence.tmp')
model = gensim.models.doc2vec.Doc2Vec(sentences, size=10, window=5, min_count=2, workers=4)
model.save_word2vec_format('/tmp/sentenceVectors.txt')
</code></pre>

<p>The code is running fine in a standalone mode, i.e. when executed by:</p>

<pre><code>python sent2vec.py
</code></pre>

<p>But I need to call this function from a java code. I tried using Jython but having errors. My java code looks like the following:</p>

<pre><code>StringWriter writer = new StringWriter(); //output will be stored here
ScriptEngineManager manager = new ScriptEngineManager();
ScriptContext context = new SimpleScriptContext();

context.setWriter(writer); //configures output redirection
ScriptEngine engine = manager.getEngineByName(""python"");
engine.eval(new FileReader(""sent2vec.py""), context);
System.out.println(writer.toString());
</code></pre>

<p>I am having this error:</p>

<pre><code>Exception in thread ""main"" javax.script.ScriptException: ImportError: No module named gensim in &lt;script&gt; at line number 1
at org.python.jsr223.PyScriptEngine.scriptException(PyScriptEngine.java:202)
at org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:42)
at org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:47)
Caused by: Traceback (most recent call last):
File ""&lt;script&gt;"", line 1, in &lt;module&gt;
ImportError: No module named gensim

at org.python.core.Py.ImportError(Py.java:328)
at org.python.core.imp.import_first(imp.java:877)
at org.python.core.imp.import_module_level(imp.java:972)
at org.python.core.imp.importName(imp.java:1062)
at org.python.core.ImportFunction.__call__(__builtin__.java:1280)
at org.python.core.PyObject.__call__(PyObject.java:431)
at org.python.core.__builtin__.__import__(__builtin__.java:1232)
at org.python.core.imp.importOne(imp.java:1081)
at org.python.pycode._pyx0.f$0(&lt;script&gt;:5)
at org.python.pycode._pyx0.call_function(&lt;script&gt;)
at org.python.core.PyTableCode.call(PyTableCode.java:167)
at org.python.core.PyCode.call(PyCode.java:18)
at org.python.core.Py.runCode(Py.java:1386)
at org.python.core.__builtin__.eval(__builtin__.java:497)
at org.python.core.__builtin__.eval(__builtin__.java:501)
at org.python.util.PythonInterpreter.eval(PythonInterpreter.java:259)
at org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:40)
... 3 more
</code></pre>

<p>Any help will be appreciated.
Thanks.</p>
","4915852","","","","","2020-05-28 20:05:25","Using gensim python from Java with jython","<java><python><jython><gensim>","0","2","","","","CC BY-SA 3.0"
"64956369","1","","","2020-11-22 16:07:17","","1","71","<p>I have two Doc2Vec models trained on the same corpus but with different parameters. I would like to concatenate the two of them and calculate the similarity of a given input word, choosing the returned vectors from the concatenated model. I read a lot of comments regarding the fact that this method may not be particularly suited for performance improvement and that it might be necessary to change the source code to the KeyedVector class in gensim to enable it. Up to now I attempted to do that using the Translation Matrix but it returns 5 features from the second model and I am not sure about whether it is performing the translations correctly or not.
Has anybody already encountered this issue? Is there another way to calculate the similarity for an input word in a concatenated doc2vec model?
Up to now I have been able to reproduce this:</p>
<pre><code>vocab1 = model1.wv
vocab2 = model2.wv

concatenated_vectors = {}
vocab_concatenated = vocab1
for i in range(len(vocab1.vectors)):
    v1 = vocab1.vectors[i] 
    v2 = vocab2.vectors[i]
    vocab_concatenated[list(vocab1.vocab.keys())[i]] = np.concatenate((v1, v2))
</code></pre>
<p>In order to re-calculate the <code>most_similar()</code> top-n features for a passed argument, how should I re-istantiate the newly created object? It seems that</p>
<pre><code>.add_vectors(list(vocab1.vocab.keys()), vocab_concatenated[list(vocab1.vocab.keys())])
</code></pre>
<p>is not working, but I am sure I am missing something.</p>
","14687076","","14687076","","2020-11-24 14:21:30","2020-11-25 11:29:02","Concatenated Doc2Vec - calculate similarities","<gensim><word2vec><doc2vec>","0","6","","","","CC BY-SA 4.0"
"64974507","1","64974659","","2020-11-23 18:48:18","","2","187","<p>I am using the pretrained word vectors from Wikipedia, <code>&quot;glove-wiki-gigaword-100&quot;</code>, in Gensim. As <a href=""https://github.com/kavgan/nlp-in-practice/blob/master/pre-trained-embeddings/Pre-trained%20embeddings.ipynb"" rel=""nofollow noreferrer"">this example documentation</a> shows, you can query the most similar words for a given word or set of words using</p>
<pre><code>model_gigaword.wv.most_similar(positive=['dirty','grimy'],topn=10)
</code></pre>
<p>However, I would like to query the most similar words to a <strong>given vector</strong>, specified as an array (of the same format as a word-vector from the pretrained model). For example, the result from adding or subtracting two word-vectors in the pretrained model, like</p>
<pre><code>vec = model_gigaword['king']-model_gigaword['man']
</code></pre>
<p>Output: (for <code>vec</code>)</p>
<pre><code>array([-0.696     , -1.26119   , -0.49109   ,  0.91179   ,  0.23077281,
       -0.18835002, -0.65568995, -0.29686698, -0.60074997, -1.35762   ,
       -0.11816999,  0.01779997, -0.74096   ,  0.21192   , -0.407071  ,
       -1.04871   , -0.480674  , -0.95541   , -0.06046999,  0.20678002,
       -1.1516    , -0.98955095,  0.44508   ,  0.32682198, -0.03306001,
       -0.31138003,  0.87721   ,  0.34279   ,  0.78621   , -0.297459  ,
        0.529243  , -0.07398   ,  0.551844  ,  0.54218   , -0.39394   ,
        0.96368   ,  0.22518003,  0.05197001, -0.912573  , -0.718755  ,
        0.08056   ,  0.421177  , -0.34256   , -0.71294   , -0.25391   ,
       -0.65362   , -0.31369498,  0.216278  ,  0.41873002, -0.21784998,
        0.21340999,  0.480393  ,  0.47077006, -1.00272   ,  0.16624999,
       -0.07340002,  0.09219003, -0.02021003, -0.58403   , -0.47306   ,
        0.05066001, -0.64416003,  0.80061007,  0.224344  , -0.20483994,
       -0.33785298, -1.24589   ,  0.08900005, -0.08385998, -0.195515  ,
        0.08500999, -0.55749   ,  0.19473001, -0.0751    , -0.61184   ,
       -0.08018   , -0.34303   ,  1.03759   , -0.36085004,  0.93508005,
       -0.00997001, -0.57282   ,  0.33101702,  0.271261  ,  0.47389007,
        1.1219599 , -0.00199997, -1.609     ,  0.57377803, -0.17023998,
       -0.22913098, -0.33818996, -0.367797  ,  0.367965  , -1.08955   ,
       -0.664806  ,  0.05213001,  0.40829998,  0.125692  , -0.44967002],
      dtype=float32)
</code></pre>
<p>How do I get the most similar words to <code>vec</code>?</p>
","12384851","","6573902","","2021-08-06 13:00:35","2021-08-06 13:00:35","Gensim most similar word to vector","<python><nlp><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"66808720","1","66809530","","2021-03-25 22:56:33","","0","45","<p>I would like to find the best hyperparameters for my model, but tuning 6 metaparameters over a total of 486 permutations and 200k documents takes a while. That's why I'm thinking about using the free credits on AWS. Ideally I want to run my script and get a .csv file as ouput.</p>
<pre><code>vector_size = [100, 200, 300]
window = [2, 5, 10]
epochs = [10, 20, 30]
count =[2, 5, 10] 
dm = [0,1]
sample = [10e-4, 10e-5, 10e-6 ]
</code></pre>
<p>The problem is that I've never used AWS and the amount of different services is overwhelming. Can you guys give me a hint which service is suitable for my problem?</p>
","11469656","","","","","2021-03-26 00:44:11","Which service to run doc2vec on AWS?","<python><amazon-web-services><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"41594324","1","","","2017-01-11 15:10:22","","0","392","<p>Problem Statement - Classify a product review </p>

<p>classes - Travel,Hotel,Cars,Electronics,Food,Movies</p>

<p>I am approaching this problem with the famous <code>Text Classification</code> problem. Feature set is prepared by using <code>Doc2Vec</code> default model from <code>gensim</code> and for classification I am using <code>Logistic Regression</code> oneVSrest from <code>sklearn</code>. </p>

<p>For every class I feed 10000 reviews to <code>Doc2Vec</code>.( I am following this <a href=""https://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow noreferrer"">Doc2Vec</a> tutorial). In this way the model learns vector for each sentence. From the resulting vectors, 80% from each class are given to <code>LogisticRegression</code> for training and 20% for testing. The accuracy of classifier is 98%. But for unseen data the accuracy is just 17%. Also <code>PCA</code> of all sentence vectors when plotted in a 2D graph resulted in one dense cluster. What I can conclude from the graph is that the data is inseparable but then how the classifier gave an accuracy of 98%? Also, why on unseen data the accuracy is very low? How can I evaluate/validate my results. </p>
","4555699","","","","","2017-01-11 15:10:22","Classifier Accuracy - Too good to believe","<python><pca><gensim><text-classification><doc2vec>","0","2","","","","CC BY-SA 3.0"
"64955331","1","64960852","","2020-11-22 14:28:03","","0","91","<p>Is there any way in gensim that i can force the learned vectors in word2vec to be all positive? (all the elements of vector be positive). i am working on a different task that needs these vectors to be positive ( the reason is really complicated so don't ask why )</p>
<p>so what is the easiest way for me to force gensim to learn positive vectors?</p>
","9557861","","","","","2020-11-22 23:45:27","Force gensim's word2vec vectors to be positive?","<gensim><word2vec>","1","6","","","","CC BY-SA 4.0"
"65006689","1","","","2020-11-25 14:30:23","","0","92","<p>Okay i want to wrap around the <code>Doc2Vec</code> class but for some reason that i don't fully understand afterwards it does not take my list of tagged documents:</p>
<p>this is what I'm trying todo:</p>
<pre><code>class doc2vec_model(Doc2Vec):
    
    def train(self,path):
        self._path = path
        self._tagged_documents()

        super(Doc2Vec, self).__init__(self._docs, min_count = 100, 
                                     vector_size=300, 
                                     epochs = 20, 
                                     negative = 5, 
                                     workers=20, 
                                     sample = 1e-5,
                                     alpha=0.01,
                                     min_alpha=0.0001)

    def _tagged_documents(self,):
        self.file_l = [name for name in glob.iglob(self._path, recursive=True)]
        self._docs = [] 
        for f_id, path in enumerate(self.file_l):
            with open(path,'r') as f:
                docu = f.read()
                docu = norm_string(docu)
                docu = docu.split(' ')
                chunk_size = 200 
                chunk_l = [docu[i:i+chunk_size] for i in range(0,len(docu),chunk_size)]
                for c_id, docu_chunk in enumerate(chunk_l):
                    self._docs.append(TaggedDocument(words=docu_chunk, tags=(f'DOC_{f_id}_{c_id}',)))
</code></pre>
<p>and then call it with:</p>
<pre><code>model = doc2vec_model()
model.train('/path/to/my/docs/*')
</code></pre>
<p>the error that I'm getting is the following:</p>
<pre><code>~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py in __init__(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)
    743                 raise TypeError(&quot;You can't pass a generator as the sentences argument. Try a sequence.&quot;)
    744 
--&gt; 745             self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)
    746             self.train(
    747                 sentences=sentences, corpus_file=corpus_file, total_examples=self.corpus_count,

~/.local/lib/python3.6/site-packages/gensim/models/doc2vec.py in build_vocab(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    926         total_words, corpus_count = self.vocabulary.scan_vocab(
    927             documents=documents, corpus_file=corpus_file, docvecs=self.docvecs,
--&gt; 928             progress_per=progress_per, trim_rule=trim_rule
    929         )
    930         self.corpus_count = corpus_count

~/.local/lib/python3.6/site-packages/gensim/models/doc2vec.py in scan_vocab(self, documents, corpus_file, docvecs, progress_per, trim_rule)
   1123             documents = TaggedLineDocument(corpus_file)
   1124 
-&gt; 1125         total_words, corpus_count = self._scan_vocab(documents, docvecs, progress_per, trim_rule)
   1126 
   1127         logger.info(

~/.local/lib/python3.6/site-packages/gensim/models/doc2vec.py in _scan_vocab(self, documents, docvecs, progress_per, trim_rule)
   1050         checked_string_types = 0
   1051         vocab = defaultdict(int)
-&gt; 1052         for document_no, document in enumerate(documents):
   1053             if not checked_string_types:
   1054                 if isinstance(document.words, string_types):

TypeError: 'NoneType' object is not iterable
</code></pre>
<p>and I don't understand why <code>self._docs</code> is a list full of <code>TaggedDocument</code> objects when I pass it to the <code>.__init__</code> of <code>Dov2Vec</code> so it should be iterable.</p>
","3749379","","6573902","","2020-11-30 18:04:57","2020-11-30 18:04:57","gensim taggeddocument list generates TypeError: 'NoneType' object is not iterable","<python><python-3.x><gensim>","0","2","","","","CC BY-SA 4.0"
"56213690","1","","","2019-05-20 02:34:12","","0","146","<p>I am new in natural language processing and I found <a href=""https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"" rel=""nofollow noreferrer"">this</a> interesting tutorial which describes how to do the topic modeling.</p>

<p>Available <a href=""https://www.kaggle.com/therohk/million-headlines/data"" rel=""nofollow noreferrer"">data</a> for this tutorial </p>

<p>Source code: <a href=""https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"" rel=""nofollow noreferrer"">here</a></p>

<p>The above code can provide topic modeling using LDA and generates the k number of topic. My question is how can I find which document belongs to which topic (cluster)? Like the example shown in figure <a href=""https://imgur.com/hA0dqUL"" rel=""nofollow noreferrer"">here</a>. I wondering something like:</p>

<blockquote>
  <p>publish_date:20030219 with text (aba ...) belongs to topic 1 cluster
  or ..</p>
</blockquote>

<p>I already read the post such as:
<a href=""https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi/20991190"">[1]</a> or <a href=""https://stackoverflow.com/questions/51448833/topicmodel-how-to-query-documents-by-topic-model-topic"">[2]</a> but still, I couldn't get my answer.</p>

<p>I also tried Matlab text analytic toolbox but I couldn't figure that out yet. </p>

<p>It would be great if you can provide me any help.</p>
","8921989","","8921989","","2019-05-22 18:27:53","2019-05-22 18:27:53","How to find which document is belong to which cluster?","<python><gensim><lda><topic-modeling><natural-language-processing>","1","0","1","","","CC BY-SA 4.0"
"47725012","1","","","2017-12-09 03:25:02","","0","109","<p>SOLVED: It seems that referencing my toUse variable does not work as an acceptable file path input. Changing it to the true path fixed the problem. </p>

<p>I'm relatively new to Python and am biting off more than I can chew but I don't understand how I keep getting this error in my code. It's just a simple I/O operation on closed file yet I don't know how my file is a closed file, error is derived from the corpora.MmCorpus.serialize() statement shown after this block of code.</p>

<pre><code>from gensim import corpora
temporary=open('C:\\Users\\A\\Horror and Suspense\\data\\inUse.txt','r')
toUse=open('C:\\Users\\A\\Horror and Suspense\\data\\parsing.txt','r+')
for line in temporary:
    toUse.write(line)
temporary.close()
corpus=corpora.textcorpus.TextCorpus(input=toUse)
corpora.MmCorpus.serialize('C:\\Users\\A\\Horror and 
Suspense\\data\\corpora.mm',corpus)
</code></pre>

<p>Here's the error:</p>

<pre><code>runfile('C:/Users/A/Horror and Suspense/System1/useFiles.py',         wdir='C:/Users/A/Horror and Suspense/System1')
Traceback (most recent call last):

  File ""&lt;ipython-input-37-ccfa33041487&gt;"", line 1, in &lt;module&gt;
runfile('C:/Users/A/Horror and Suspense/System1/useFiles.py', wdir='C:/Users/A/Horror and Suspense/System1')

  File ""C:\Users\A\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 710, in runfile
execfile(filename, namespace)

  File ""C:\Users\A\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 101, in execfile
exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/A/Horror and Suspense/System1/useFiles.py"", line 10, in &lt;module&gt;
corpora.MmCorpus.serialize('C:\\Users\\A\\Horror and Suspense\\data\\corpora.mm',corpus)

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\corpora\indexedcorpus.py"", line 93, in serialize
offsets = serializer.save_corpus(fname, corpus, id2word, **kwargs)

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\corpora\mmcorpus.py"", line 51, in save_corpus
fname, corpus, num_terms=num_terms, index=True, progress_cnt=progress_cnt, metadata=metadata

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\matutils.py"", line 723, in write_corpus
    for docno, doc in enumerate(corpus):

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\corpora\textcorpus.py"", line 184, in __iter__
    for text in self.get_texts():

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\corpora\textcorpus.py"", line 250, in get_texts
for line in lines:

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\corpora\textcorpus.py"", line 193, in getstream
with utils.file_or_filename(self.input) as f:

  File ""C:\Users\A\Anaconda3\lib\site-packages\gensim\utils.py"", line 148, in file_or_filename
input.seek(0)

ValueError: I/O operation on closed file.
</code></pre>
","4036635","","4036635","","2017-12-10 15:54:55","2017-12-10 15:54:55","Gensim ValueError: I/O operation on closed file","<python><gensim>","0","5","","","","CC BY-SA 3.0"
"57090689","1","","","2019-07-18 08:56:50","","0","99","<p>I have two lists (say  list A and list B) of texts. I want to find most similar texts in list B for each text in list A.
This I want to do using bag of words and later cosine similarity.</p>

<p>I created a dictionary using gensim</p>

<pre><code>tokensA = [preprocess_string(''.join(doc),CUSTOM_FILTERS) for doc in 
listA]
tokensB = [preprocess_string(''.join(doc),CUSTOM_FILTERS) for doc in 
listB]
# Create dictionary
dictionary = corpora.Dictionary(tokensA)
</code></pre>

<p>Then I obtained the occurences of words for each text in list B,</p>

<pre><code>mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokensB]

[[(9, 1), (25, 1), (611, 1), (627, 1), (1917, 1), (1918, 1)],
[(9, 1), (25, 1), (627, 1), (1918, 1), (1919, 1), (1920, 1)],...
</code></pre>

<p>How can I convert this to vectors so that I can do a cosine_similarity?</p>
","2804268","","","","","2019-07-18 08:56:50","How to create one hot vectors to find similarity of texts after gensim doc2bow implementation?","<nlp><gensim><cosine-similarity><one-hot-encoding>","0","3","","","","CC BY-SA 4.0"
"68350237","1","68351791","","2021-07-12 15:44:30","","0","48","<p>I trained and saved a word2Vec model 'myWord2Vec.model' to pass it to a logistic regression model for training, but the vector size is bigger than my training dataset so, I needed to reduce the vector size. I tried the code below:</p>
<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('myWord2Vec.model', limit=2021)
</code></pre>
<p>It gave me this error:</p>
<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
<p>I have no clue how to fix it nor how to reduce the vector size.
I would appreciate the help!</p>
","11213336","","","","","2021-07-12 17:42:10","Gensim W2V - UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte","<python-3.x><utf-8><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"30301922","1","42886555","","2015-05-18 11:24:45","","40","38855","<p>I have trained a word2vec model using a corpus of documents with Gensim. Once the model is training, I am writing the following piece of code to get the raw feature vector of a word say ""view"".</p>

<pre><code>myModel[""view""]
</code></pre>

<p>However, I get a KeyError for the word which is probably because this doesn't exist as a key in the list of keys indexed by word2vec. How can I check if a key exits in the index before trying to get the raw feature vector?</p>
","492372","","","","","2021-07-19 05:52:25","How to check if a key exists in a word2vec trained model or not","<python><gensim><word2vec>","5","0","7","","","CC BY-SA 3.0"
"68451937","1","68458078","","2021-07-20 08:40:25","","1","43","<p>It seems that when retrieving the most similar word vectors, sorting by word frequency will change the results in <code>Gensim</code>.</p>
<p>Before sorting:</p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import common_texts  # some example sentences
print(len(common_texts))
model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
model.build_vocab(corpus_iterable=common_texts)
model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=1)  

model.wv.most_similar(positive=[&quot;human&quot;])
</code></pre>
<blockquote>
<pre><code>[('interface', 0.7432922720909119),
 ('minors', 0.6719315052032471),
 ('time', 0.3513716757297516),
 ('computer', 0.05815044790506363),
 ('response', -0.11714297533035278),
 ('graph', -0.15643596649169922),
 ('eps', -0.2679084539413452),
 ('survey', -0.34035828709602356),
 ('trees', -0.63677978515625),
 ('user', -0.6500451564788818)]
</code></pre>
</blockquote>
<p>However, if I sort the vectors by descending frequency:</p>
<pre><code>model.wv.sort_by_descending_frequency()

model.wv.most_similar(positive=[&quot;human&quot;])
</code></pre>
<blockquote>
<pre><code>[('minors', 0.9638221263885498),
 ('time', 0.6335864067077637),
 ('interface', 0.40014874935150146),
 ('computer', 0.03224882856011391),
 ('response', -0.14850640296936035),
 ('graph', -0.2249641716480255),
 ('survey', -0.26847705245018005),
 ('user', -0.45202943682670593),
 ('eps', -0.497650682926178),
 ('trees', -0.6367797255516052)]
</code></pre>
</blockquote>
<p>The most similar word ranking as well as the word similarities change. Any idea why?</p>
<p><strong>Update:</strong></p>
<p>Before calling sort:</p>
<pre><code>model.wv.index_to_key
</code></pre>
<blockquote>
<pre><code>['system',
 'graph',
 'trees',
 'user',
 'minors',
 'eps',
 'time',
 'response',
 'survey',
 'computer',
 'interface',
 'human']
</code></pre>
</blockquote>
<pre><code>model.wv.expandos['count']
</code></pre>
<blockquote>
<p>array([4, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2])</p>
</blockquote>
<p>After calling sort:</p>
<pre><code>model.wv.index_to_key
</code></pre>
<blockquote>
<pre><code>['system',
 'user',
 'trees',
 'graph',
 'human',
 'interface',
 'computer',
 'survey',
 'response',
 'time',
 'eps',
 'minors']
</code></pre>
</blockquote>
<pre><code>model.wv.expandos['count']
</code></pre>
<blockquote>
<p>array([4, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2])</p>
</blockquote>
","5705174","","5705174","","2021-07-21 04:11:02","2021-07-21 04:11:02","Gensim sort_by_descending_frequency changes most_similar results","<python><nlp><gensim><word2vec><fasttext>","1","0","","","","CC BY-SA 4.0"
"68409432","1","","","2021-07-16 12:59:31","","0","21","<p>I am running the Word2Vec implementation from gensim twice, and I have a problem with the <code>save</code> function:</p>
<pre><code>model_ = gensim.models.Word2Vec(all_doc, size=int(config['MODEL']['embed_size']),
                                window=int(config['MODEL']['window']),
                                workers=multiprocessing.cpu_count(),
                                sg=1, iter=int(config['MODEL']['iteration']),
                                negative=int(config['MODEL']['negative']),
                                min_count=int(config['MODEL']['min_count']), seed=int(config['MODEL']['seed']))
</code></pre>
<p><code>model_.save(config['BASIC']['embedding_dir'])</code></p>
<p>I obtain different outputs for each time I run it. The first time it gives an &quot;output_embedding&quot;, an &quot;output_embedding.trainables.syn1neg.npy&quot; and an &quot;output_embedding.wv.vectors.npy&quot;. But the second time it does not give the two npy files, it just generates &quot;output_embedding&quot;.</p>
<p>The only thing I change from the first to the second time is the sentences I use as input (<code>all_doc</code>).</p>
<p>Why it does not generate the 3 files ?</p>
","15276782","","","","","2021-07-16 14:33:52","Gensim word2vec saves numpy arrays?","<numpy><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"68434314","1","","","2021-07-19 01:37:52","","0","38","<p>After I fix the number of Topics, contents of Topics will be varied by changing random_state argument in models.ldamodel() of gensim.
When I evaluate the LDA model w/ the fixed # of Topics, I always check coherence and perplexity of the model. These metrics are highly depend on the Topics.
I think that it means that the random_state argument is one of important variables of the LDA model.
Currently, I change the random_state ten times to find the best coherence and perplexity, but I am not quite comfortable since there might be better random_state rather than ten.
Dose someone knows if there is better manner to find better coherence and perplexity?</p>
<p>Metrics are given by following API of gensim, as you may know.</p>
<pre class=""lang-py prettyprint-override""><code>CoherenceModel()
log_perplexity()
</code></pre>
<p>Thank you.</p>
","13444774","","","","","2021-07-19 01:37:52","How can I know the coherence and perplexity is the best of LDA model","<gensim><lda>","0","1","","","","CC BY-SA 4.0"
"60185968","1","60186537","","2020-02-12 10:17:04","","0","455","<p>I am using the the Mallet LDA with gensims implemented wrapper.</p>

<p>Now I want to get the Topic distribution of several unseen documents, store it in a nested list and then print it out.</p>

<p>This is my code:</p>

<pre><code>other_texts = [
        ['wlan', 'usb', 'router'],
        ['auto', 'auto', 'auto'],
        ['human', 'system', 'computer']
 ]

corpus1 = [id2word.doc2bow(text) for text in other_texts]

to_pro = []
for t in corpus1:
    unseen_doc = corpus1
    vector = lda[unseen_doc] # get topic probability distribution for a document
    to_pro.append(vector)
</code></pre>

<p>If I try to print the list <code>vector</code> it yields this result:</p>

<pre><code>[&lt;gensim.interfaces.TransformedCorpus object at 0x0000024CC1DFC940&gt;, &lt;gensim.interfaces.TransformedCorpus object at 0x0000024CC1DFC320&gt;, &lt;gensim.interfaces.TransformedCorpus object at 0x0000024CC1DFC6A0&gt;]

</code></pre>

<p>I tried this code to print them out properly, but the probabilities of the topic distributuion are wrong:</p>

<pre><code>topic_dist = []
for line in to_pro:
    topic_dist += lda.get_document_topics(line)
td=[]
for topic in topic_dist:
    td.append(topic)
</code></pre>

<p>And I get this result:</p>

<pre><code>[[(0, 0.05458162849743133), (1, 0.05510823556400538), (2, 0.05603786367505091), (3, 0.05472256432318962), (4, 0.05471966342417517), (5, 0.05454446883678316), (6, 0.060267211268385176), (7, 0.05590590303517797), (8, 0.054558298009463865), (9, 0.0570497751708577), (10, 0.05586054626708894), (11, 0.05611284070096096), (12, 0.05483861615903838), (13, 0.054548627713420714), (14, 0.0548708631793431), (15, 0.055097199555668705), (16, 0.05572779508710042), (17, 0.05544789953285848)], [(0, 0.05457482739088479), (1, 0.05509130205455064), (2, 0.05599364448566309), (3, 0.05479472333893934), (4, 0.05489998490024729), (5, 0.054542940465732534), (6, 0.06014649090195501), (7, 0.0558787316629024), (8, 0.05455634249554292), (9, 0.05651159582517287), (10, 0.0558343047708517), (11, 0.05605027364084813), (12, 0.05483134591787102), (13, 0.054546952683828316), (14, 0.05488058477867337), (15, 0.0550725066190555), (16, 0.055951974201133244), (17, 0.055841473866147906)], [(0, 0.05457665942453363), (1, 0.055255130626316235), (2, 0.05616834056392741), (3, 0.05472749675259328), (4, 0.0547199851837743), (5, 0.054544546873748226), (6, 0.06037007389117332), (7, 0.05593838115178327), (8, 0.05456190582329174), (9, 0.056409168851414615), (10, 0.0559404965748031), (11, 0.05614914322415512), (12, 0.054842094317369555), (13, 0.054550171326841215), (14, 0.054870520851845996), (15, 0.05511732934346291), (16, 0.05579100118297473), (17, 0.05546755403599123)], [(0, 0.054581620307290336), (1, 0.05510823907508528), (2, 0.056037876384335425), (3, 0.05472256410518629), (4, 0.05471967034475046), (5, 0.05454446871605657), (6, 0.06026693118061518), (7, 0.05590622478877356), (8, 0.054558295773128575), (9, 0.05704995161755483), (10, 0.05586057502348091), (11, 0.056112803329985396), (12, 0.05483861481767718), (13, 0.05454862663175604), (14, 0.054870865577993026), (15, 0.055097113943380405), (16, 0.05572773919917307), (17, 0.055447819183777246)], [(0, 0.05457482815837349), (1, 0.05509132071436994), (2, 0.05599364089981504), (3, 0.05479471920764724), (4, 0.05489999995707833), (5, 0.05454293700828862), (6, 0.06014645177706313), (7, 0.05587868116251209), (8, 0.05455634846240247), (9, 0.056511585085478364), (10, 0.055834295810939794), (11, 0.056050296895854265), (12, 0.054831353686471636), (13, 0.05454695325610574), (14, 0.05488059866846103), (15, 0.055072528844072065), (16, 0.05595218064057245), (17, 0.05584127976449436)], [(0, 0.054576657976703774), (1, 0.05525504608539575), (2, 0.05616829811928526), (3, 0.05472749878845379), (4, 0.05471997497183866), (5, 0.054544547686709126), (6, 0.06037016659013718), (7, 0.05593821008515276), (8, 0.05456190840675052), (9, 0.05640917964821885), (10, 0.05594054039873076), (11, 0.05614912143569156), (12, 0.0548420823035294), (13, 0.054550172872614225), (14, 0.054870521717331436), (15, 0.055117319561282), (16, 0.05579110737872705), (17, 0.055467645973447846)], [(0, 0.054581639915369816), (1, 0.055108252268374285), (2, 0.056037916094392765), (3, 0.05472256597071497), (4, 0.05471966744573819), (5, 0.0545444687939403), (6, 0.06026693966026536), (7, 0.055906213964449725), (8, 0.05455829555351338), (9, 0.05704968653857304), (10, 0.0558606261827436), (11, 0.05611290790292455), (12, 0.05483860593828801), (13, 0.05454862649308445), (14, 0.05487085805236639), (15, 0.05509715099521129), (16, 0.05572773695595529), (17, 0.05544784127409454)], [(0, 0.05457482754746605), (1, 0.05509132328696252), (2, 0.055993666140583764), (3, 0.05479472184721206), (4, 0.05489996963702654), (5, 0.05454294168997213), (6, 0.060146365105445465), (7, 0.05587886571230439), (8, 0.05455633757025994), (9, 0.056511632004648656), (10, 0.055834239764847755), (11, 0.05605028881626678), (12, 0.054831347261978546), (13, 0.05454695137813789), (14, 0.05488060185684171), (15, 0.05507250450434276), (16, 0.055951827151308337), (17, 0.05584158872439472)], [(0, 0.05457665857245025), (1, 0.05525503335748317), (2, 0.05616811411295409), (3, 0.054727501563580076), (4, 0.054719978109952404), (5, 0.05454454660618627), (6, 0.060370135879343034), (7, 0.05593823717454384), (8, 0.05456190762146366), (9, 0.056409205316000424), (10, 0.05594060935464846), (11, 0.056149148701409454), (12, 0.05484207733245972), (13, 0.054550172010398135), (14, 0.05487051175914863), (15, 0.05511731933953272), (16, 0.055791267296383236), (17, 0.055467575892062346)]]
</code></pre>

<p>However printing one element from the list yields the correcr results:</p>

<pre><code>to_pro = []
for t in corpus1:
    unseen_doc = corpus1
    vector = lda[unseen_doc[1]] # specifying document at index 1
    to_pro.append(vector)
</code></pre>

<pre><code>[[(0, 0.052410901467505704), (1, 0.052410901467505704), (2, 0.052410901467505704), (3, 0.052410901467505704), (4, 0.052410901467505704), (5, 0.052410901467505704), (6, 0.052410901467505704), (7, 0.052410901467505704), (8, 0.052410901467505704), (9, 0.052410901467505704), (10, 0.052410901467505704), (11, 0.052410901467505704), (12, 0.052410901467505704), (13, 0.052410901467505704), (14, 0.10901467505240292), (15, 0.052410901467505704), (16, 0.052410901467505704), (17, 0.052410901467505704)], [(0, 0.052410901467505704), (1, 0.052410901467505704), (2, 0.052410901467505704), (3, 0.052410901467505704), (4, 0.052410901467505704), (5, 0.052410901467505704), (6, 0.052410901467505704), (7, 0.052410901467505704), (8, 0.052410901467505704), (9, 0.052410901467505704), (10, 0.052410901467505704), (11, 0.052410901467505704), (12, 0.052410901467505704), (13, 0.052410901467505704), (14, 0.10901467505240292), (15, 0.052410901467505704), (16, 0.052410901467505704), (17, 0.052410901467505704)], [(0, 0.052410901467505704), (1, 0.052410901467505704), (2, 0.052410901467505704), (3, 0.052410901467505704), (4, 0.052410901467505704), (5, 0.052410901467505704), (6, 0.052410901467505704), (7, 0.052410901467505704), (8, 0.052410901467505704), (9, 0.052410901467505704), (10, 0.052410901467505704), (11, 0.052410901467505704), (12, 0.052410901467505704), (13, 0.052410901467505704), (14, 0.10901467505240289), (15, 0.052410901467505704), (16, 0.052410901467505704), (17, 0.052410901467505704)]]
</code></pre>

<p>Another problem is, that for one document, the same distribution is printed 3 times. </p>

<p>I also looked into this answer: <a href=""https://stackoverflow.com/questions/45317151/gensim-interfaces-transformedcorpus-how-use"">gensim.interfaces.TransformedCorpus - How use?</a>, but it didnt help.</p>

<p>What am I doin wrong here? </p>
","12195215","","","","","2020-12-04 20:49:23","Transforming a gensim.interfaces.TransformedCorpus to a readable result","<python><gensim><lda>","2","0","","","","CC BY-SA 4.0"
"68521344","1","","","2021-07-25 18:11:03","","1","22","<p>Can I use gensim library to do summarization of texts in other languages (e.g. French, Swedish)? If so I would appreciate it if you could tell me how!</p>
","12264408","","","","","2021-07-25 18:11:03","Gensim summarization for other languages","<python><gensim>","0","4","","2021-07-26 02:05:01","","CC BY-SA 4.0"
"68552107","1","68552778","","2021-07-27 21:45:48","","0","22","<p>I want to train a word2vec model using Gensim. I preprocessed my corpus, which is made of hundreds of thousands of articles from a specific newspaper. I preprocessed them (lower casing, lemmatizing, removing stop words and punctuations, etc.) and then make a list of lists, in which each element is a list of words.</p>
<pre><code>corpus = [['first', 'sentence', 'second', 'dictum', 'third', 'saying', 'last', 'claim'],
          ['first', 'adage', 'second', 'sentence', 'third', 'judgment', 'last', 'pronouncement']]
</code></pre>
<p>I wanted to know if it is the right way, or it should be like the following:</p>
<pre><code>corpus = [['first', 'sentence'], ['second', 'dictum'], ['third', 'saying'], ['last', 'claim'], ['first', 'adage'], ['second', 'sentence'], ['third', 'judgment'], ['last', 'pronouncement']]
</code></pre>
","4168794","","","","","2021-07-27 23:32:17","Structure of Gensim Word Embedding corpus","<gensim><word2vec><word-embedding><corpus>","1","0","","","","CC BY-SA 4.0"
"68574477","1","","","2021-07-29 10:42:37","","0","36","<p>I have a pre-trained model, but I need to add some new words in it.</p>
<p>I tried:</p>
<pre><code>model.build_vocab([[new_word1, new_word2]], update=True)
model.train([[new_word1, new_word2]], total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>
<p>But when I check:</p>
<pre><code>model.wv[new_word1]
model.wv[new_word2]
</code></pre>
<p>I got</p>
<pre><code>KeyError: &quot;Key {new_word1} not present&quot;
</code></pre>
<p>same as new_word2</p>
<p>I have checked this
<a href=""https://stackoverflow.com/questions/58712856/how-to-add-words-and-vectors-manually-to-word2vec-gensim"">How to add words and vectors manually to Word2vec gensim?</a></p>
<p>How can I solve it?
Thanks</p>
","4478679","","","","","2021-07-29 16:50:15","Adding words to gensim word2vec model, but it's not shown in model.wv","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"68469974","1","","","2021-07-21 13:01:23","","0","10","<p>I am trying to follow this example <a href=""https://github.com/dsfsi/textaugment"" rel=""nofollow noreferrer"">https://github.com/dsfsi/textaugment</a> to upload a pre-trained Gensim model for data augmentation</p>
<pre><code>    import textaugment
    import gensim
    from textaugment import Word2vec
    model = gensim.models.KeyedVectors.load_word2vec_format(r'\GoogleNews-vectors-negative300.bin', binary=True)
    from textaugment import Word2vec
    t = Word2vec(model)
    t.augment('The stories are good')
</code></pre>
<p>but I get the following error:</p>
<pre><code>TypeError: __init__() takes 1 positional argument but 2 were given 
</code></pre>
<p>at line</p>
<pre><code>t = Word2vec(model)
</code></pre>
<p>What am I doing wrong?</p>
","9906395","","130288","","2021-07-21 17:07:59","2021-07-21 17:07:59","Uploading and running Gensim model for data augmentation","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"68516490","1","","","2021-07-25 07:33:37","","0","17","<p>I have run LSI using gensim, but I don't know how to get the cover rate, which is the number of topics n that describe the corpus.</p>
<p>I was able to get the Topics Vector, but do I use it?
Is it different from coherence or perplexity?</p>
<pre><code>def prepare_corpus(doc_clean):
    &quot;&quot;&quot;
    Input  : clean document
    Output : term dictionary and Document Term Matrix
    &quot;&quot;&quot;
    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)
    dictionary = corpora.Dictionary(doc_clean)
    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.
    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
    tfidf_instance=models.TfidfModel(doc_term_matrix)
    tfidf_corpus=tfidf_instance[doc_term_matrix]
    # generate LDA model
    return dictionary,doc_term_matrix,tfidf_corpus

def create_gensim_lsa_model(doc_clean,number_of_topics):
    &quot;&quot;&quot;
    Input  : clean document, number of topics and number of words associated with each topic
    Purpose: create LSA model using gensim
    Output : return LSA model
    &quot;&quot;&quot;
    dictionary,doc_term_matrix, tfidf_corpus =prepare_corpus(doc_clean)
    # generate LSA model
    #lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model
    lsamodel_tfidf = LsiModel(tfidf_corpus, num_topics=number_of_topics, id2word = dictionary)
    return lsamodel_tfidf
</code></pre>
","13831340","","","","","2021-07-25 07:33:37","How can I get the cover rate for a topic model?","<python><nlp><gensim><lda><latent-semantic-indexing>","0","0","","","","CC BY-SA 4.0"
"68554661","1","","","2021-07-28 05:04:45","","1","30","<p>I have a gensim doc2vec model trained on around 1000 documents. Now I have to incrementally update this existing model by adding 100 newly tagged documents. I am not able to incrementally retrain this model. Can anyone help me with the same.</p>
","5996176","","","","","2021-07-28 17:21:35","Incremental/ Continue Model Training for Gensin Doc2Vec model","<deep-learning><nlp><gensim><doc2vec>","1","1","","","","CC BY-SA 4.0"
"60248118","1","","","2020-02-16 11:46:01","","1","269","<p>I'm quite new to Python and coding in general, so I seem to have run into an issue.</p>

<p>I'm trying to run this code (credit to Matthew Mayo, whole thing can be found <a href=""https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html"" rel=""nofollow noreferrer"">here</a>):</p>

<pre><code># import warnings
# warnings.filterwarnings(action = 'ignore', category = UserWarning, module = 'gensim')
import sys
from gensim.corpora import WikiCorpus

def make_corpus (in_f, out_f):
    print(0)
    output = open(out_f, 'w', encoding = 'utf-8')
    print(1)
    wiki = WikiCorpus(in_f)
    print(2)
    i = 0
    for text in wiki.get_texts():
        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '/n')
        i += 1
        if i % 10000 == 0:
            print('Processed {} articles!'.format(i))
    print(3)
    output.close()
    print('Process complete!')



print('start')
if __name__ == '__main__':
    if len(sys.argv) != 3:
        print('Usage: python make_wiki_corpus.py &lt;wikipedia_dump_file&gt; &lt;processed_text_file&gt;')
        sys.exit(1)
    in_f = sys.argv[1]
    out_f = sys.argv[2]
    make_corpus(in_f, out_f)
else:
    print(__name__)
</code></pre>

<p>However, the function branch seems to run partly, stopping at the <code>wiki = WikiCorpus(in_f)</code> - it never makes it to <code>print(2)</code> - and then exiting and repeating the beginning of the code, yielding no results. No error actually comes up, only a warning (<code>UserWarning: detected Windows; aliasing chunkize to chunkize_serial warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")</code>).</p>

<p>The output is this:</p>

<pre><code>start
0
1
C:\Users\name\Anaconda3\lib\site-packages\gensim\utils.py:1254: UserWarning: detected Windows; aliasing chunkize to chunkize_serial warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
start
__mp_main__
start
__mp_main__
start
__mp_main__
</code></pre>

<p>I've tried uninstalling all required packages (numpy, smart_open), as well as gensim itself (in an active conda environment), but nothing has changed.
Also, what is the difference between the <strong>main</strong> and the multiprocessing one?</p>

<p>-- Specifications: win64, py 3.7.3</p>

<p>Edit: after running logging at the DEBUG level, logging file</p>

<pre><code>2020-02-16 22:49:00,061:start: :13396 
2020-02-16 22:49:00,061:0 :13396 
2020-02-16 22:49:00,061:1 :13396 
2020-02-16 22:49:01,493:start: :22356 
2020-02-16 22:49:01,493:3 :22356 
2020-02-16 22:49:01,496:start: :25332 
2020-02-16 22:49:01,497:3 :25332 
2020-02-16 22:49:01,530:start: :7120 
2020-02-16 22:49:01,530:3 :7120 
2020-02-16 22:49:01,541:adding document #0 to Dictionary(0 unique tokens: []):13396
</code></pre>

<p>(also, the '3' was added in the <code>else</code> branch:)</p>

<pre><code>else:
    logging.debug('3 ')
</code></pre>
","12907067","","12907067","","2020-02-17 12:59:56","2020-02-18 21:13:46","Problems with gensim WikiCorpus - aliasing chunkize to chunkize_serial; (__mp_main__ instead of __main__?)","<python><python-3.x><windows><nlp><gensim>","1","4","","","","CC BY-SA 4.0"
"68492838","1","68492990","","2021-07-23 00:38:59","","1","67","<p>I have a dataset that looks like this (not the actual values, but just to get the idea of it):</p>
<pre class=""lang-none prettyprint-override""><code>id  text                                      group 
1   what is the difference and why is it ...  2
2   let me introduce myself, first.           1 
</code></pre>
<p>The length of the &quot;text&quot; column can be from one sentence to many sentences. What I'm trying to do is to summarize each text from the row and save the summarized text in a new column. I'm using gensim for summarization.</p>
<p>My desired output is as follows, and please disregard the content.</p>
<pre class=""lang-none prettyprint-override""><code>id  text                                     group  text_summary 
1   what is the difference and why is it ...  2     the difference between object a and b 
2   let me introduce myself, first.           1     let me introduce myself, first.
</code></pre>
<p>Below is the code I used, but I'm getting the following error.</p>
<pre class=""lang-py prettyprint-override""><code>import gensim 
from gensim.summarization import summarize 
from gensim.summarization import keywords 

for i in range(0, df.shape[0]):
    text = df.iloc[i]['Answers']
    if len(text) &gt; 1:
        df.loc[i, 'summary_answer'] = summarize(text)
    else: 
        df.loc[i, 'summary_answer'] = text
</code></pre>
<p><a href=""https://i.stack.imgur.com/ytMQw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ytMQw.png"" alt=""enter image description here"" /></a></p>
<p>I understand the problem, but my <code>if/else</code> statement seems to not work in this case.</p>
","","user15851936","14531062","","2021-07-23 00:57:10","2021-07-23 22:12:27","Doing gensim text summarization in each row python","<python><gensim>","1","2","","","","CC BY-SA 4.0"
"60254825","1","","","2020-02-17 01:20:24","","0","66","<p>Below is the output that I get using Gensim Mallet wrapper. From this SO <a href=""https://stackoverflow.com/questions/8447393/how-to-understand-the-output-of-topic-model-class-in-mallet"">link</a> I understood that LL/token means ""<strong>model's log-liklihood divided by the total number of tokens</strong>"". 1) However, for few topics like (1,8,11 etc.) I do not see any terms at all. 2) I tried to run the code for a range of topics from (10,20,2) (step of 2 starting from 10-20). But the output shows 17 as the last topic generated. I am missing something here..</p>

<pre><code>0       2.77778 watch 
1       2.77778 
2       2.77778 receive tape hope purchase 
3       2.77778 dvds wildlife pass yr interested 
4       2.77778 dvd version walk bored 
5       2.77778 volume courtyard trilogy 
6       2.77778 crazy picture minute 
7       2.77778 neighbor 
8       2.77778 
9       2.77778 buy mice trouble stay versus feeder 
10      2.77778 inside stir tv mine life bird wonderful year fascinated 
11      2.77778 
12      2.77778 
13      2.77778 recommend test real prefer greenery 
14      2.77778 age 
15      2.77778 funny triliogy play friend full minute 
16      2.77778 
17      2.77778 time tree 

&lt;950&gt; LL/token: -22.17456
&lt;960&gt; LL/token: -22.22132
&lt;970&gt; LL/token: -22.24897
&lt;980&gt; LL/token: -22.11585
&lt;990&gt; LL/token: -22.38062
</code></pre>
","8382950","","","","","2020-02-17 15:19:52","Gensim Mallet: Output does not have terms for few topics","<nlp><gensim><lda><topic-modeling><mallet>","1","0","","","","CC BY-SA 4.0"
"60271345","1","","","2020-02-17 22:39:35","","0","239","<p>I am trying to understand how to use LDA in my case. I have a corpus of many documents and I want to see how a very specific set of words and ngrams are distributed across topics. Is there a way to specify a list of specific words as a vocabulary for topic modeling? </p>

<p>I have been working with the gensim implementation and I believe the argument <code>id2word</code> handles this, but the documentation is not clear to me. Is my understanding correct?</p>
","11666502","","","","","2020-02-20 15:56:27","Specifying vocabulary input in LDA","<python><nlp><cluster-analysis><gensim><lda>","2","2","1","","","CC BY-SA 4.0"
"60286735","1","60289229","","2020-02-18 17:47:38","","2","1323","<p>I don't have large corpus of data to train word similarities e.g. 'hot' is more similar to 'warm' than to 'cold'. However, I like to train doc2vec on a relatively small corpus ~100 docs so that it can classify my domain specific documents. </p>

<p><strong>To elaborate let me use this toy example.</strong> Assume I've only 4 training docs given  by 4 sentences - ""I love hot chocolate."", ""I hate hot chocolate."", ""I love hot tea."", and ""I love hot cake."".
Given a test document ""I adore hot chocolate"", I would expect, doc2vec will invariably return ""I love hot chocolate."" as the closest document. This expectation will be true if word2vec already supplies the knowledge that ""adore"" is very similar to ""love"". However, I'm getting most similar document as ""I hate hot chocolate"" -- which is a bizarre!!</p>

<p>Any suggestion on how to circumvent this, i.e. be able to use pre-trained word embeddings so that I don't need to venture into training ""adore"" is close to ""love"", ""hate"" is close to ""detest"", and so on.</p>

<p><strong>Code (Jupyter Nodebook. Python 3.7. Jensim 3.8.1)</strong></p>

<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
data = [""I love hot chocolate."",
        ""I hate hot chocolate"",
       ""I love hot tea."",
       ""I love hot cake.""]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]
print(tagged_data)
#Train and save
max_epochs = 10
vec_size = 5
alpha = 0.025


model = Doc2Vec(vector_size=vec_size, #it was size earlier
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    if epoch % 10 == 0:
        print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.epochs) #It was model.iter earlier
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

print(""Model Ready"")

test_sentence=""I adore hot chocolate""
test_data = word_tokenize(test_sentence.lower())
v1 = model.infer_vector(test_data)
#print(""V1_infer"", v1)

# to find most similar doc using tags
sims = model.docvecs.most_similar([v1])
print(""\nTest: %s\n"" %(test_sentence))
for indx, score in sims:
    print(""\t(score: %.4f) %s"" %(score, data[int(indx)]))
</code></pre>
","1029599","","1029599","","2020-02-18 18:21:27","2020-02-18 20:49:40","Gensim's Doc2Vec - How to use pre-trained word2vec (word similarities)","<python><nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"68577177","1","68578640","","2021-07-29 13:50:21","","0","17","<p>Seems like many people are having issues with Mallet.</p>
<pre><code>import os
from gensim.models.wrappers import LdaMallet

os.environ.update({'MALLET_HOME':r'C:/Users/myusername/Desktop/Topic_Modelling/mallet-2.0.8'})

mallet_path = r'C:/Users/myusername/Desktop/Topic_Modelling/mallet-2.0.8/bin/mallet' 

model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus,num_topics=num_topics, id2word=id2word)
</code></pre>
<p>Getting the following errors:</p>
<pre><code>/bin/sh: C:/Users/myusername/Desktop/Topic_Modelling/mallet-2.0.8/bin/mallet.bat: No such file or directory

CalledProcessError: Command 'C:/Users/myusername/Desktop/Topic_Modelling/mallet-2.0.8/bin/mallet.bat import-file --preserve-case --keep-sequence --remove-stopwords --token-regex &quot;\S+&quot; --input /var/folders/ml/lxzrtxwn02vbvq65c80g1b640000gn/T/c52cdc_corpus.txt --output /var/folders/ml/lxzrtxwn02vbvq65c80g1b640000gn/T/c52cdc_corpus.mallet' returned non-zero exit status 127.
</code></pre>
<p>I downloaded mallet from <a href=""http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip"" rel=""nofollow noreferrer"">http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip</a> and unzipped it in my directory. I've tried running the command in the error in the terminal and I'm getting the same 'no such file found' error, but it's there in my directory?</p>
<p>I've also followed this: <a href=""https://ps.au.dk/fileadmin/ingen_mappe_valgt/installing_mallet.pdf"" rel=""nofollow noreferrer"">https://ps.au.dk/fileadmin/ingen_mappe_valgt/installing_mallet.pdf</a></p>
<p>When I go to the directory via command line and type <code>./bin/mallet</code> I get a whole bunch of commands, which according to the instructions, is what I'm looking for to know that it's been installed ok.</p>
<p>I'm running the following on MacOS</p>
<ul>
<li>Python==3.9.6</li>
<li>gensim==3.8.3</li>
</ul>
<p>Anyone have any ideas?</p>
","5011954","","","","","2021-07-29 15:19:24","LDA Mallet Gensim CalledProcessError","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"60290296","1","","","2020-02-18 22:22:37","","3","1043","<p>I have trained several word2vec models using gensim for different languages, but the <code>size</code> is different for each of them.</p>

<p>vectors are obtained like this:</p>

<pre><code>vec_sp = word_vectors_sp.get_vector(""uno"")
</code></pre>

<p>How to use <code>vec_sp</code> as input for different model with different vector size:</p>

<pre><code>word_vectors_en.most_similar(positive=[vec_sp], topn=1)
</code></pre>

<p>to obtain the corresponding word in the second model</p>
","5625696","","","","","2020-02-19 21:31:06","Word2Vec compare vectors from different models with different sizes","<python><nlp><artificial-intelligence><gensim><word2vec>","2","3","","","","CC BY-SA 4.0"
"60307249","1","","","2020-02-19 18:46:02","","1","54","<p>I am working with a steadily growing corpus. I train my Document Vector with Doc2Vec which is implemented in Python. </p>

<p>Is it possible to update a Document Vector?</p>

<p>I want to use the Document Vector for Document recommendations.</p>
","12195215","","","","","2020-02-19 21:47:40","Is it possible to update a Doc2Vec Vector?","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"68622666","1","","","2021-08-02 13:43:36","","0","45","<p>I am working on Text classification where I want to classify movie genres. I want to give input as movie summary/plot and want output as Movie Genres. I have used FastText using Gensim library for obtaining vector representations for words and I want to feed the output of FastText in Neural Network for training so that I can give movie summary/plot as an input to Neural Network and get the output of Movie Genre such as Drama, Horror, etc. I have read many blogs and all are feeding TFIDF in Neural Network but no one is feeding the output of FastText in Neural Network. Can someone please explain to me if it is possible or you think otherwise.</p>
<p>Thank you for your cooperation and understanding in this regard.</p>
<pre><code>import time
from gensim.models import FastText
start = time.time()
model_ted = FastText(sentences=movies_new['genre_new'], size=100, window=5, min_count=5, workers=4,sg=1)
print(model_ted)
end = time.time()
print('Time to train fasttext from generator: %0.2fs' % (end - start))

model_ted.wv.most_similar(&quot;The Lemon Drop Kid , a New York City swindler, is illegally touting horses at a Florida racetrack. After several successful hustles, the Kid comes across a beautiful, but gullible, woman intending to bet a lot of money. The Kid convinces her to switch her bet, employing a prefabricated con. Unfortunately for the Kid, the woman belongs to notorious gangster Moose Moran , as does the money.&quot;)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>[('Foreign legion', 0.9828806519508362),
 ('Space opera', 0.9763268828392029),
 ('Cyberpunk', 0.9738191366195679),
 ('Reboot', 0.9718296527862549),
 ('Kafkaesque', 0.9635183215141296),
 ('Libraries and librarians', 0.9622164368629456),
 ('Parkour in popular culture', 0.961660623550415),
 ('Movies About Gladiators', 0.9592210650444031),
 ('Women in prison films', 0.9587017297744751),
 ('Outlaw', 0.9548137784004211)]
</code></pre>
","12217286","","","","","2021-08-02 17:59:12","NLP-How can we feed the output of FastText in Neural Network","<python><keras><nlp><gensim><fasttext>","1","3","","","","CC BY-SA 4.0"
"60211920","1","","","2020-02-13 15:57:49","","0","73","<p>I wrote the following code, 
But the following error shows, 
Please guide me.</p>

<pre><code>from gensim.models.wrappers import LdaMallet
import os
os.environ.update({'MALLET_HOME':r'C:/mallet'})
mallet_path = 'C:/mallet/bin/mallet'

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)
</code></pre>

<p><strong>Error:</strong></p>

<blockquote>
  <p>Command 'C:/mallet/bin/mallet import-file --preserve-case
  --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\MHRADM~1\AppData\Local\Temp\316a7f_corpus.txt --output
  C:\Users\MHRADM~1\AppData\Local\Temp\316a7f_corpus.mallet' returned
  non-zero exit status 1.</p>
</blockquote>
","12867109","","1581741","","2020-02-13 16:31:31","2020-02-13 16:31:31","Topic modeling in LdaMallet","<gensim><lda><topic-modeling><mallet>","0","2","","","","CC BY-SA 4.0"
"68604006","1","68605929","","2021-07-31 16:39:33","","0","16","<p>I'm trying to make documents vectors of gensim example using doc2vec.
I passed TaggedDocument which contains 9 docs and 9 tags.</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
idx = [0,1,2,3,4,5,6,7,100]
documents = [TaggedDocument(doc, [i]) for doc, i in zip(common_texts, idx)]
model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)
</code></pre>
<p>and it produces 101 vectors like this image.
<a href=""https://i.stack.imgur.com/b1ciY.png"" rel=""nofollow noreferrer"">gensim doc2vec produced 101 vectors</a></p>
<p>and what I want to know is</p>
<ol>
<li>How can I be sure that the tag I passed is attached to the right vector?</li>
<li>How did the vectors with the tags which I didn't pass (8~99 in my case) come out? Were they computed as a blank?</li>
</ol>
","16568137","","4685471","","2021-07-31 19:39:09","2021-07-31 21:26:59","Gensim doc2vec produce more vectors than given documents, when I pass unique integer id as tags","<machine-learning><gensim><word-embedding><doc2vec>","1","0","","","","CC BY-SA 4.0"
"60292744","1","60293149","","2020-02-19 03:51:08","","0","64","<p>I want to extract and print bigrams using Gensim. For this purpose I used that code in GoogleColab:</p>

<pre><code>import gensim.downloader as api
from gensim.models import Word2Vec
from gensim.corpora import WikiCorpus, Dictionary
from gensim.models import Phrases
from gensim.models.phrases import Phraser
from collections import Counter

data = api.load(""text8"") # wikipedia corpus
bigram = Phrases(data, min_count=3, threshold=10)


cntr = Counter()
for key in bigram.vocab.keys():
  if len(key.split('_')) &gt; 1:
    cntr[key] += bigram.vocab[key]

for key, counts in cntr.most_common(50):
  print(key, "" - "", counts)
</code></pre>

<p>But there's an error: </p>

<p><a href=""https://i.stack.imgur.com/eIjwF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eIjwF.png"" alt=""TypeError""></a></p>

<p>Then I tried this:</p>

<pre><code>cntr = Counter()
for key in bigram.vocab.keys():
  if len(key.split(b'_')) &gt; 1:
    cntr[key] += bigram.vocab[key]

for key, counts in cntr.most_common(50):
  print(key, "" - "", counts)
</code></pre>

<p>And then:</p>

<p><a href=""https://i.stack.imgur.com/ir6eB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ir6eB.png"" alt=""again""></a></p>

<p>What is wrong?</p>
","10536999","","","","","2020-02-19 05:04:05","TypeError during extracting bigrams with Gensim(Python)","<python><machine-learning><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"60318511","1","60318796","","2020-02-20 11:05:14","","0","1462","<p>i'm having this error problem, i have ran this script in jupyter notebook in base (root) environment, the log said that gensim library has been installed and i have run the command <strong>!pip install gensim</strong> before i import it, but it still can not be imported, and the error said <strong>ModuleNotFoundError: No module named 'gensim'</strong></p>

<pre><code>!pip install gensim
import gensim
from gensim.models import KeyedVectors
model = KeyedVectors.load('model_fasttext2.vec')
model.vector_size
------------------------------------------------------------------------
Requirement already satisfied: gensim in c:\users\ip-03\anaconda3\lib\site-packages (3.8.1)
Requirement already satisfied: scipy&gt;=0.18.1 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.4.1)
Requirement already satisfied: six&gt;=1.5.0 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.14.0)
Requirement already satisfied: smart-open&gt;=1.8.1 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.9.0)
Requirement already satisfied: numpy&gt;=1.11.3 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.18.1)
Requirement already satisfied: boto&gt;=2.32 in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (2.49.0)
Requirement already satisfied: boto3 in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (1.12.3)
Requirement already satisfied: bz2file in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (0.98)
Requirement already satisfied: requests in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (2.22.0)
Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.3.3)
Requirement already satisfied: botocore&lt;1.16.0,&gt;=1.15.3 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (1.15.3)
Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.9.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2019.11.28)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (3.0.4)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (1.25.8)
Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2.8)
Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in c:\users\ip-03\anaconda3\lib\site-packages (from botocore&lt;1.16.0,&gt;=1.15.3-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2.8.1)
Requirement already satisfied: docutils&lt;0.16,&gt;=0.10 in c:\users\ip-03\anaconda3\lib\site-packages (from botocore&lt;1.16.0,&gt;=1.15.3-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.15.2)
</code></pre>

<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-10-ee4a48d372cd&gt; in &lt;module&gt;
      1 get_ipython().system('pip install gensim')
----&gt; 2 import gensim
      3 from gensim.models import KeyedVectors
      4 model = KeyedVectors.load('model_fasttext2.vec')
      5 model.vector_size

ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>Is there anyone who can help this problem? i will really appreciate your help, it will help my thesis work, thank you for your attention </p>
","12917855","","","","","2020-02-20 11:21:27","No module named 'gensim' but already installed it","<python><machine-learning><jupyter-notebook><gensim><word-embedding>","1","0","","","","CC BY-SA 4.0"
"68468195","1","68487834","","2021-07-21 10:50:14","","1","72","<p>I was wondering if Spacy supports multi-GPU via <a href=""https://mpi4py.readthedocs.io/en/stable/tutorial.html#running-python-scripts-with-mpi"" rel=""nofollow noreferrer"">mpi4py</a>?</p>
<p>I am currently using Spacy's nlp.pipe for Named Entity Recognition on a high-performance-computing cluster that supports the MPI protocol and has many GPUs. It says <a href=""https://github.com/explosion/spaCy/issues/3394"" rel=""nofollow noreferrer"">here</a> that I would need to specify the GPU to use with cupy, but with PyMPI, I am not sure if the following will work (should I import spacy after calling cupy device?):</p>
<pre><code>
from mpi4py import MPI
import cupy

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    data = [&quot;His friend Nicolas J. Smith is here with Bart Simpon and Fred.&quot;*100]
else:
    data = None

unit = comm.scatter(data, root=0)

with cupy.cuda.Device(rank):
    import spacy
    from thinc.api import set_gpu_allocator, require_gpu
    set_gpu_allocator(&quot;pytorch&quot;)
    require_gpu(rank)
    nlp = spacy.load('en_core_web_lg')
    nlp.add_pipe(&quot;merge_entities&quot;)
    tmp_list = []
    for doc in nlp.pipe(unit):
        res = &quot; &quot;.join([t.text if not t.ent_type_ else t.ent_type_ for t in doc])
        tmp_list.append(res)

result = comm.gather(tmp_list, root=0)

if comm.rank == 0:
    print (result)
else:
    result = None

</code></pre>
<p>Or if i have 4 GPUs on the same machine and I do not want to use MPI, can I do the following:</p>
<pre><code>from joblib import Parallel, delayed
import cupy

rank = 0

def chunker(iterable, total_length, chunksize):
    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))

def flatten(list_of_lists):
    &quot;Flatten a list of lists to a combined list&quot;
    return [item for sublist in list_of_lists for item in sublist]

def process_chunk(texts):
    with cupy.cuda.Device(rank):
        import spacy
        from thinc.api import set_gpu_allocator, require_gpu
        set_gpu_allocator(&quot;pytorch&quot;)
        require_gpu(rank)
        preproc_pipe = []
        for doc in nlp.pipe(texts, batch_size=20):
            preproc_pipe.append(lemmatize_pipe(doc))
        rank+=1
        return preproc_pipe

def preprocess_parallel(texts, chunksize=100):
    executor = Parallel(n_jobs=4, backend='multiprocessing', prefer=&quot;processes&quot;)
    do = delayed(process_chunk)
    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))
    result = executor(tasks)
    return flatten(result)

preprocess_parallel(texts = [&quot;His friend Nicolas J. Smith is here with Bart Simpon and Fred.&quot;*100], chunksize=1000)
</code></pre>
","5705174","","5705174","","2021-07-21 11:33:27","2021-07-22 15:45:56","Does Spacy support multiple GPUs?","<python-3.x><nlp><mpi><spacy><gensim>","1","0","","","","CC BY-SA 4.0"
"60344269","1","60366904","","2020-02-21 18:17:58","","0","410","<p>I`ve faced with trouble to load model using gensim.model.FastText.load().</p>

<p>Here is some code and error which I get:</p>

<pre><code>from gensim.models import FastText

class FastTextModel:
    def __init__(self, model_path, dim=300):
        self.dim = dim
        self.model = FastText.load(model_path).wv

...

class GeneralModel:
    def __init__(self, config):
        if config[""type""] == ""fasttext"":
            # path - path to model
            # dim -  dimension, here 300
            self.model = FastTextModel(config[""path""], config[""dim""])
</code></pre>

<pre><code>  File ""/project/preprocessing/pipeline.py"", line 15, in __init__
    self.model_ru = GeneralModel(config[""models""][""ru""])
  File ""/project/models/nlp_models.py"", line 101, in __init__
    self.model = FastTextModel(config[""path""], config[""dim""])
  File ""/project/models/nlp_models.py"", line 16, in __init__
    self.model = FastText.load(model_path).wv
  File ""/usr/local/lib64/python3.6/site-packages/gensim/models/fasttext.py"", line 936, in load
    model = super(FastText, cls).load(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/models/base_any2vec.py"", line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/models/base_any2vec.py"", line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/utils.py"", line 423, in load
    obj._load_specials(fname, mmap, compress, subname)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/utils.py"", line 453, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/utils.py"", line 464, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)
  File ""/usr/local/lib64/python3.6/site-packages/numpy/lib/npyio.py"", line 447, in load
    pickle_kwargs=pickle_kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/numpy/lib/format.py"", line 738, in read_array
    array.shape = shape
ValueError: cannot reshape array of size 67239904 into shape (445446,300)
</code></pre>

<p>I've downloaded models from Google Drive folder, and though that it can somehow damage .npy files (as they are quite big), so I've downloaded each file (there 7 files for that model) separately, but this didn`t help me.</p>

<p>Also, I read that sometimes it can be caused because of bad unzipping in the 'load' method, but I'm passing already unzipped files into it, so this also don`t work for me.</p>

<p>Will be grateful for the help!</p>
","12884564","","","","","2020-02-23 21:12:55","Cannot load model with gensim FastText","<python><numpy><gensim>","1","0","","","","CC BY-SA 4.0"
"47681257","1","","","2017-12-06 18:37:35","","0","217","<p>On <a href=""https://radimrehurek.com/gensim/models/lsimodel.html#gensim.models.lsimodel.LsiModel.save"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/lsimodel.html#gensim.models.lsimodel.LsiModel.save</a>, function <code>save</code> has the prototype:</p>

<pre><code>save(fname, *args, **kwargs)
</code></pre>

<p>I'd like to understand what <code>args</code> and <code>kwargs</code> are, and how to pass them to <code>save</code>. However, the document doesn't explain these.</p>

<p>Could anyone help?</p>

<p>Thanks!</p>
","2291516","","","","","2018-07-17 02:26:53","How to call Gensim's LsiModel save?","<gensim>","1","1","","","","CC BY-SA 3.0"
"68472183","1","68473878","","2021-07-21 15:18:42","","0","26","<p>I'm wanna to load an pre-trained embendding to initalize my own unsupervise FastText model and retrain with my dataset.</p>
<p>The trained embendding file I have loads fine with <code>gensim.models.KeyedVectors.load_word2vec_format('model.txt')</code>. But when I try:
<code>FastText.load_fasttext_format('model.txt')</code> I get: <code>NotImplementedError: Supervised fastText models are not supported</code>.</p>
<p>Is there any way to convert supervised KeyedVectors to unsupervised FastText? And if possible, is it a bad idea?</p>
<p>I know that has an great difference between supervised and unsupervised models. But I really wanna try use/convert this and retrain it. I'm not finding a trained unsupervised model to load for my case (it's a portuguese dataset), and the best model I find <a href=""http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc"" rel=""nofollow noreferrer"">is that</a></p>
","9898033","","","","","2021-07-22 16:22:09","word-embendding: Convert supervised model into unsupervised model","<python><nlp><gensim><unsupervised-learning><fasttext>","1","0","","","","CC BY-SA 4.0"
"60142106","1","","","2020-02-09 22:52:43","","0","283","<p>Gensim <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"" rel=""nofollow noreferrer"">LDAModel</a> has parameters <code>iterations</code> and <code>passes</code> to control the number of training epochs, and callbacks to get information about convergence, but is there a possibility to stop the training when difference between two epochs is very small i.e. early stopping?</p>
","4016674","","6573902","","2020-02-12 15:53:35","2020-02-18 09:14:57","gensim LDAModel early stopping","<python><python-3.x><gensim><lda><early-stopping>","1","1","","","","CC BY-SA 4.0"
"60246570","1","60246843","","2020-02-16 08:03:17","","6","2419","<p>I created a Gensim LDA Model as shown in this tutorial: <a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""noreferrer"">https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/</a></p>

<pre><code>lda_model = gensim.models.LdaMulticore(data_df['bow_corpus'], num_topics=10, id2word=dictionary, random_state=100, chunksize=100, passes=10, per_word_topics=True)
</code></pre>

<p>And it generates 10 topics with a log_perplexity of: </p>

<blockquote>
  <p>lda_model.log_perplexity(data_df['bow_corpus']) = -5.325966117835991</p>
</blockquote>

<p>But when I run the coherence model on it to calculate coherence score, like so:</p>

<pre><code>coherence_model_lda = CoherenceModel(model=lda_model, texts=data_df['bow_corpus'].tolist(), dictionary=dictionary, coherence='c_v')
with np.errstate(invalid='ignore'):
    lda_score = coherence_model_lda.get_coherence()
</code></pre>

<p>My LDA-Score is nan. What am I doing wrong here?</p>
","12450117","","","","","2021-06-02 15:13:43","Gensim LDA Coherence Score Nan","<python><machine-learning><gensim><lda><topic-modeling>","2","0","1","","","CC BY-SA 4.0"
"68658833","1","","","2021-08-04 22:57:49","","0","32","<p>I am using two toy word sets to train my Word2Vec model with Gensim. The vocabulary in set 1 is <code>'x','y','c'</code> and in set 2 is <code>'a','b','c'</code>. After I trained the two sets separately with two different models, I found that the embedding vectors for the word <code>'c'</code> are very similar. My understanding is that the embedding is randomly initialized, so you probably even need to align the vectors for the same words trained with separate models in order to put them in the same space. Then why are my two vectors so similar? Here is my code.</p>
<pre><code>common_texts_1 = [['y', 'x', 'c', 'x', 'y', 'y']] +\
    [['y', 'c', 'c', 'c', 'x', 'y']] +\
    [['c', 'x', 'c', 'y', 'x', 'y']] +\
    [['y', 'c', 'x', 'c', 'c', 'y']] +\
    [['c', 'x', 'x', 'y', 'y', 'y']] +\
    [['c', 'x', 'x', 'y', 'y', 'y']] +\
    [['x', 'x', 'x', 'c', 'y', 'y']] +\
    [['y', 'x', 'c', 'y', 'y', 'y']] +\
    [['c', 'x', 'x', 'y', 'c', 'y']] +\
    [['c', 'y', 'y', 'y', 'y', 'y']] +\
    [['c', 'x', 'x', 'y', 'c', 'y']] +\
    [['c', 'x', 'x', 'y', 'y', 'y']] +\
    [['x', 'x', 'x', 'c', 'y', 'y']] +\
    [['x', 'x', 'x', 'y', 'y', 'c']] +\
    [['c', 'x', 'c', 'y', 'y', 'c']] +\
    [['x', 'x', 'c', 'y', 'y', 'y']] +\
    [['x', 'x', 'x', 'y', 'y', 'c']]
common_texts_2 = [['a', 'a', 'b', 'b', 'c', 'c']] +\
    [['a', 'c', 'b', 'b', 'c', 'c']] +\
    [['c', 'a', 'b', 'b', 'a', 'c']] +\
    [['a', 'c', 'b', 'b', 'b', 'c']] +\
    [['c', 'a', 'b', 'b', 'c', 'b']] +\
    [['b', 'a', 'b', 'c', 'c', 'a']] +\
    [['c', 'b', 'b', 'b', 'b', 'c']] +\
    [['c', 'a', 'b', 'b', 'c', 'c']] +\
    [['a', 'c', 'b', 'b', 'c', 'c']] +\
    [['a', 'c', 'b', 'b', 'c', 'c']] +\
    [['a', 'a', 'b', 'b', 'a', 'c']]
base_embed = gensim.models.Word2Vec(common_texts_1,
                            min_count=1,
                            size=2,
                            window=2,
                            workers=4)
other_embed = gensim.models.Word2Vec(common_texts_2,
                            min_count=1,
                            size=2,
                            window=2,
                            workers=4)
print(base_embed.wv.word_vec('c'))
print(other_embed.wv.word_vec('c'))

</code></pre>
","2547924","","","","","2021-08-05 00:43:47","Why the two embedding vectors for a same key from two Word2Vec models so similar?","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"60305405","1","","","2020-02-19 16:51:58","","1","115","<p>I am building a Document Vector in Gensim.</p>

<p>I have a custom list of tags (called url) I want to use for the TaggedDocument:</p>

<pre><code>documents = [[TaggedDocument(doc, str(ur)) for ur in url] for doc in space]
</code></pre>

<p>Every custom url is a sequence of 10 numbers.</p>

<p>The result looks like this:</p>

<pre><code>[TaggedDocument(words=['googl', 'ibm', 'quantenrechnpentium', 'fdiv', 'bug', 'vierteljahrhundert', 
'intel', 'geld', 'technikquantencomput', 'computibm', 'googl', 'beweis', 
'quantum', 'supremacyforsch', 'natursycamor', 'qubit', 'groessenordn', 'supercomput', 'bauartibm', 
'algorithmus', 'rechn', 'quant', 'quantenrechn', 'werkzeug', 'rechenfehl', 'intel', 'erfahr', 
'professor', 'drthomas', 'fdiv', 'bug', 'intel', 'pentium', 'intel', 'fehl', 'austauschprogramm', 
'intel', 'sorgdesktop', 'pc', 'prozessor', 'ghz', 'cor', 'fertigungstechn', 'stromauflag', 'pc', 
'bauvorschlaeg', 'amd', 'ryzdesktop', 'schwaech', 'intel', 'geldquartal', 'rekordumsatz', 
'milliard', 'dollarmilliard', 'preis', 'geld', 'fertigungsanlag', 'servermarkt', 'ausblick', 'amd', 
'umsatz', 'jahresvergleich', 'anleg', 'ryz', 'epycs', 'ausblick', 'amd', 'erfolgmicrosoft', 'ryz', 
'kund', 'surfac', 'notebook', 'gruend', 'journalist', 'surfac', 'testgeraet', 'ryz', 'ryz', 
'vergleich', 'intel', 'cor', 'processor', 'conferenc', 'intel', 'atomkern', 'tremont', 
'mikroarchitektur', 'atom', 'prozessor', 'nanomet', 'fertigelkhart', 'lak', 'embedded', 'systemiot', 
'devic', 'skyhawk', 'lak', 'chips', 'nasatom', 'celeron', 'pentium', 'silv', 'tremont', 'goldmont', 
'plusceleron', 'tremont', 'cor', 'kern', 'lakefield', 'microsoft', 'surfac', 'neo', 'kombination', 
'cor', 'tremont', 'intel', 'atom', 'kern', 'cach', 'serv', 'tremont', 'risc', 'spezialist', 'sifiv', 
'kern', 'aussichtnanomet', 'chipsarm', 'cortex', 'raspberry', 'pi', 'rechenpowspannend', 'erweiter', 
'sichsogenannt', 'world', 'ids', 'trennung', 'welt', 'risc', 'socsprozess', 'cpu', 'kerncach', 
'zeil', 'ram', 'adressbereich', 'world', 'id', 'sicherheitslueck', 'spectr', 'art', 'schreck', 
'prozessor', 'markt', 'microsoft', 'sich', 'business', 'notebook', 'sogenannt', 'secured', 'cor', 
'pcs', 'massnahmstart', 'betriebssystem', 'notebook', 'bios', 'bios', 'windows', 'updat', 'uefi', 
'capsul', 'updattechnik', 'linuxsecured', 'cor', 'pcs', 'microsoft', 'angab', 'mobilrechn', 
'business', 'version', 'surfacuefi', 'bios', 'kern', 'microsoft', 'rahm', 'project', 'artikel'], tags='4577911')
</code></pre>

<p>As you can see at end is the field <code>tags</code> with my custom number. </p>

<p>After training the model with:</p>

<pre><code>model = Doc2Vec(docu, vector_size=5, window=2, min_count=2, dm =1)
</code></pre>

<p>I am runnning a similarity query with unseen Test Documents:</p>

<pre><code>rank = []
for line in test:
    tokens = line.split()

    new_vector = model.infer_vector(tokens)
    sims = model.docvecs.most_similar([new_vector])
    rank.append(sims)
</code></pre>

<p>Which yields this for the first document:</p>

<pre><code>[('8', 0.9214882850646973),('9', 0.919198751449585), 
('0', 0.9049716591835022), ('1', 0.9047936797142029), 
('6', 0.9028873443603516), ('2', 0.8913612365722656), 
('3', 0.8857095837593079), ('7', 0.8747860789299011), 
('5', 0.8512719869613647), ('4', 0.8370641469955444)]
</code></pre>

<p>As you can see the tags at index 0 of every sublist are not custom tags but generic numbers.</p>

<p>What am I doing wrong here?</p>
","12195215","","12195215","","2020-02-19 16:58:09","2020-02-19 21:43:33","Using custom Tags for Tagged Documents in Gensim","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"68676637","1","68681997","","2021-08-06 05:41:06","","1","552","<p>I am using Word2Vec and using a wiki trained model that gives out the most similar words. I ran this before and it worked but now it gives me this error even after rerunning the whole program. I tried to take off  <code>return_path=True</code> but im still getting the same error</p>
<pre><code>print(api.load('glove-wiki-gigaword-50', return_path=True))
model.most_similar(&quot;glass&quot;)
</code></pre>
<p>#ERROR:</p>
<pre><code>/Users/me/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-153-3bf32168d154&gt; in &lt;module&gt;
      1 print(api.load('glove-wiki-gigaword-50', return_path=True))
----&gt; 2 model.most_similar(&quot;glass&quot;) 

AttributeError: 'Word2Vec' object has no attribute 'most_similar'
</code></pre>
<p>#MODEL
this is the model I used</p>
<pre><code>    print(
        '%s (%d records): %s' % (
            model_name,
            model_data.get('num_records', -1),
            model_data['description'][:40] + '...',
        )
    )
</code></pre>
<p>Edit: here is my gensim download &amp; output</p>
<pre><code>!python -m pip install -U gensim
</code></pre>
<p>OUTPUT:</p>
<p>Requirement already satisfied: gensim in ./opt/anaconda3/lib/python3.8/site-packages (4.0.1)</p>
<p>Requirement already satisfied: numpy&gt;=1.11.3 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)</p>
<p>Requirement already satisfied: smart-open&gt;=1.8.1 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (5.1.0)</p>
<p>Requirement already satisfied: scipy&gt;=0.18.1 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)</p>
","15316065","","15316065","","2021-08-06 17:32:29","2021-08-06 19:59:21","AttributeError: 'Word2Vec' object has no attribute 'most_similar' (Word2Vec)","<python><nlp><gensim><word2vec><doc2vec>","2","4","","","","CC BY-SA 4.0"
"60316242","1","","","2020-02-20 09:06:25","","0","160","<p>I am following this <a href=""https://medium.springboard.com/identifying-duplicate-questions-a-machine-learning-case-study-37117723844"" rel=""nofollow noreferrer"">tutorial</a> in which i have a following Dataset from Quora:</p>

<p><a href=""https://i.stack.imgur.com/dBzId.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dBzId.jpg"" alt=""enter image description here""></a></p>

<p>Here i have already cleaned and tokenize the data in column <strong>q1_clean</strong> &amp; <strong>q1_clean</strong>.</p>

<p>Now i have trained the <strong>W2vModel</strong> by using GoogleNews pretrained model with the following code. </p>

<pre><code># We are concating the two columns of Question1 and Question2

nData = pd.Series(pd.concat([data['q1_clean'], data['q2_clean']]))
model_w2v = Word2Vec(nData, size=300) 

# step 2: intersect the initialized word2vec model with the pre-trained fasttext model
model_w2v.intersect_word2vec_format('GoogleNews-vectors-negative300.bin',lockf=1.0,binary=True)

# step 3: improve model with transfer-learning using the training data
model_w2v.train(nData, total_examples=model_w2v.corpus_count, epochs= 10)

</code></pre>

<p>Now i have to do the feature analysis, for that i have following function to get the average computed distance.</p>

<pre><code>def get_pairwise_distance(word1, word2, weight1, weight2, method = 'euclidean'):
    if(word1.size==0 or word2.size==0):
        return np.nan
    dist_matrix = pairwise_distances(word1, word2, metric=method)
    return np.average(dist_matrix, weights=np.matmul(weight1.reshape(-1,1),weight2.reshape(-1,1).T))
</code></pre>

<p>Here i have computed the tfidf to use as a weights: </p>

<pre><code>X_train_tokens = get_tokenized_questions(data=X_train)

from sklearn.feature_extraction.text import TfidfVectorizer
pass_through = lambda x:x
tfidf = TfidfVectorizer(analyzer=pass_through)
# compute tf-idf weights for the words in the training set questions
X_tfidf = tfidf.fit_transform(X_train_tokens)

# split into two
# X1_tfidf -&gt; tf-idf weights of first question in question pair and 
# X2_tfidf -&gt; tf-idf weights of second question in question pair
X1_tfidf = X_tfidf[:len(X_train)]
X2_tfidf = X_tfidf[len(X_train):]
</code></pre>

<p>and i am calling this <strong>get_pairwise_distance</strong> function like in the <a href=""https://medium.springboard.com/identifying-duplicate-questions-a-machine-learning-case-study-37117723844"" rel=""nofollow noreferrer"">tutorial</a>.</p>

<pre><code>#cosine similarities
# here X1 and X2 are the embedded versions of the first and second questions in the question-pair data
# and X1_tfidf and X2_tfidf are the tf-idf weights of the first and second questions in the question-pair data

cosine = compute_pairwise_dist(X1, X2, X1_tfidf, X2_tfidf)
</code></pre>

<p><strong>For this function i need to pass the embedded version of <em>q1_clean</em> and <em>q2_clean</em> as X1 and X2 where weights are already computed using TFIDF. and i am getting no clue how to embed these two columns into vectors using pretrained model and pass it to the given function?</strong> </p>
","10048934","","10048934","","2020-02-20 10:15:34","2020-02-21 14:10:42","How to Embed your Dataframe using already trained model with Gensim (GoogleNews-vectors-negative300.bin)","<machine-learning><scikit-learn><nlp><nltk><gensim>","1","3","","","","CC BY-SA 4.0"
"62174945","1","","","2020-06-03 14:08:34","","0","463","<p>I have several gensim models fit to ~5 million documents. I'd like to pull the top 100 most representative documents from each of models for each topic to help me pick the best model. </p>

<p>Let's say I have a model <code>lda</code> and corpus <code>corpus</code>, I can get the topic probabilities in the following form:</p>

<pre><code>topic_probs = lda[corpus]
</code></pre>

<p>Where <code>topic_probs</code> is a list of tuples: <code>(topic_num, topic_prob)</code>.</p>

<p>How can I sort this list of tuples by topic, and then probability, then retrieve the top 100 documents from the corpus? I'm guessing the answer looks something like the <a href=""https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi"">method for assigning topics here</a>, but I'm struggling with how to work with a list of tuples while maintaining the document indices.</p>

<p>(This is somewhat complicated by the fact that I didn't know about the <code>minimum_probability</code> argument to <code>gensim.LdaModel</code>, so topics with &lt; 0.01 probability are suppressed. These models take 2-3 days to run each, so I'd like to avoid re-running them if possible).</p>
","8107978","","","","","2020-06-05 19:02:18","Gensim: extract 100 most representative documents for each topic","<python><tuples><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"68634515","1","68640229","","2021-08-03 10:35:03","","2","76","<p>I am trying to create an NLP model which can find similar sentences. For example, It should be able to say that &quot;Software Engineer&quot;, &quot;Software Developer&quot;, &quot;Software Dev&quot;, &quot;Soft Engineer&quot; are similar sentences.</p>
<p>I have a dataset with a list of roles such as Cheif Executives, Software Engineer and the variation of these terms will be unknown ( out of vocabulary).</p>
<p>I am trying to use fastText with Gensim but struggling.
Does anyone have suggested readings/ tutorials that might help me?</p>
","14431682","","","","","2021-08-04 16:09:19","How to find similar Sentences using FastText ( Sentences with Out of Vocabulary words)","<machine-learning><nlp><gensim><fasttext>","1","2","1","","","CC BY-SA 4.0"
"68700393","1","","","2021-08-08 11:38:11","","0","27","<p>I want to fetch vector representation of words.
I tried to use GENSIM api but got the same error as here (for Python 3.6):
<a href=""https://stackoverflow.com/questions/64425652/valueerror-when-downloading-gensim-data-set"">ValueError when downloading gensim data set</a></p>
<p>What is the best way to get the vector out of the pre-trained model?</p>
","6057371","","","","","2021-08-08 18:40:26","How to get word2vec from google's pre-trained model","<nlp><nltk><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"68722609","1","","","2021-08-10 07:26:41","","1","409","<p>i got a error message while visualizing LDA model</p>
<pre><code>import pyLDAvis
import pyLDAvis.gensim_models

pyLDAvis.enable_notebook()

vis = pyLDAvis.gensim_models.prepare(ldamodel,corpus, dictionary)
vis
</code></pre>
<p>and it gives me error message
&quot; import_optional_dependency() got an unexpected keyword argument 'errors'&quot;</p>
","16544304","","","","","2021-08-10 07:26:41","LDA visualization [ import_optional_dependency() got an unexpected keyword argument 'errors' ]","<python><gensim><lda><pyldavis>","0","1","1","","","CC BY-SA 4.0"
"53406593","1","","","2018-11-21 06:46:19","","2","274","<p>I'm trying to run a simple LDA model on Gensim:</p>

<pre><code>from gensim import corpora
#text_data here is a list of tokens
dictionary = corpora.Dictionary(text_data)
corpus = [dictionary.doc2bow(text) for text in text_data]

lda = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=5, id2word=dictionary, passes=15)
</code></pre>

<p>and getting the following error:</p>

<pre><code>File "".../anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py"", line 465, in __init__
    self.random_state = utils.get_random_state(random_state)

  File "".../anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 84, in get_random_state
    return np.random.mtrand._rand

AttributeError: module 'numpy.random' has no attribute 'mtrand'
</code></pre>

<p>I have tried <code>from numpy import random</code> but doesn't seem to work, and not really sure why this error is thrown. </p>

<p>Any suggestions?</p>

<p>EDIT: Restarting Anaconda and Spyder fixed the issue. </p>
","4639942","","4639942","","2018-11-21 22:07:55","2018-11-21 22:07:55","AttributeError from gensim running LDA model","<python><gensim><lda>","0","3","1","","","CC BY-SA 4.0"
"36256584","1","","","2016-03-28 05:35:38","","3","808","<p>I am unable to install any python package through </p>

<pre><code>pip install &lt;name&gt;
</code></pre>

<p>it always throws the error </p>

<pre><code>Command python setup.py egg_info failed with error code 1 in /tmp/pip-build-V2srLa/Gensim
Storing debug log for failure in /home/&lt;username&gt;/.pip/pip.log
</code></pre>

<p>for other packages using <code>sudo apt-get install python-&lt;name&gt;</code> used to work but not for gensim</p>

<p>I have tried to insttall/unzip the package and go <code>python setup.py test
python setup.py install</code> it throws <code>pkg_resources.DistributionNotFound: The 'ruamel.yaml&gt;=0.10.7' distribution was not found and is required by the application
</code></p>

<p>I have tried to install 'ruamel.yaml>=0.10.7' and it still doesn't work </p>

<p>Any help why this is failing ? and how can I solve it ? </p>

<p><strong>Update:</strong> moving to root (sudo su) worked for me  </p>
","5130728","","1307905","","2016-03-30 09:10:58","2019-08-10 20:22:11","Can't install python package gensim ubuntu","<python><pip><gensim><ruamel.yaml>","2","2","","","","CC BY-SA 3.0"
"68447480","1","","","2021-07-19 22:02:25","","1","28","<p>For my project I am trying to use unsupervised learning to identify different topics from application descriptions, but I am running into a strange problem. Firstly, I have 3 different datasets, one with 15k documents another with 50k documents and last with 2m documents. I am trying to test models with different number of topics (<strong>k</strong>) ranging from 5 to 100 with a step size of 5. This is in order to check which <strong>k</strong> results in the best model assessed with initially with the highest coherence score. For each <strong>k</strong>, I also build 3 different models with chunksize 10, 100 and 1000.</p>
<p>So now moving onto the problem I am having. Obviously my own machine is too slow and does not have enough cores for this kind of computation hence I am using my university's server. The problem here is my program seems to be consuming too much memory and I am unsure of the reason. I already made some adjustments such that the corpus is not loaded entirely to memory (or atleast I think I did). The dataset with 50k entries already at iteration <strong>k=50</strong> (so halfway) seems to have consumed the alloted 100GB of memory, which seems very huge.</p>
<p>I would appreciate any help in the right direction and thanks for taking the time to look at this. Below is the code from my <em>topic_modelling.py</em> file. Comments on the file are a bit outdated, sorry about that.</p>
<pre><code>class MyCorpus:
    texts: list
    dictionary: dict

    def __init__(self, descriptions, dictionary):
        self.texts = descriptions
        self.dictionary = dictionary

    def __iter__(self):
        for line in self.texts:
            try:
            # assume there's one document per line, tokens separated by whitespace
                yield self.dictionary.doc2bow(line)
            except StopIteration:
                pass

# Function given a dataframe creates a dictionary and corupus
# These are used to create an LDA model. Here we automatically use the Descriptionb column
# from each dataframe
def create_dict_and_corpus(df):
    text_descriptions = remove_characters_and_create_list(df, 'Description')
    # print(text_descriptions)
    dictionary = gensim.corpora.Dictionary(text_descriptions)
    corpus = MyCorpus(text_descriptions, dictionary)
    return text_descriptions, dictionary, corpus

# Given a dataframe remove and a column name in the data frame, extract all words and return a list
# Also to remove all chracters that are not alphanumeric or spaces
def remove_characters_and_create_list(df, column_name, split=True):
    df[column_name] = df[column_name].astype(str)
    texts = []
    for x in range(df[column_name].size):
        current_string = df[column_name][x]
        filtered_string = re.sub(r'[^A-Za-z0-9 ]+', '', current_string)
        if split:
            texts.append(filtered_string.split())
        else:
            texts.append(filtered_string)
    return texts


# This function given the parameters creates an LDA model for each number between
# the start limit and the end limit. After this the coherence and perplexity is calulated
# for each of those models and saved in a csv file to analyze later.
def test_lda_models(text, corpus, dictionary, start_limit, end_limit, path):
    results = []
    print(&quot;============Starting topic modelling============&quot;)
    for k in range(start_limit, end_limit+1, 5):
        for p in range(1, 4):
            chunk = pow(10, p)
            t0 = time.time()
            lda_model = gensim.models.ldamulticore.LdaMulticore(corpus,
                                            num_topics=k, 
                                            id2word=dictionary, 
                                            passes=p,
                                            chunksize=chunk)
            # To calculate the goodness of the model
            perplexity = lda_model.bound(corpus)
            coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=dictionary, coherence='c_v')
            coherence_lda = coherence_model_lda.get_coherence()
            t1 = time.time()
            print(f&quot;=====Done K={k} model with passes={p} and chunksize={chunk}, took {t1-t0} seconds=====&quot;)
            results.append((k, chunk, coherence_lda, perplexity))
        
    # Storing teh results in a csv file except the actual lda model (this would not make sense)
    path = make_dir_if_not_exists(path)
    list_tuples_to_csv(results, ['#OfTopics', 'ChunkSize', 'CoherenceScore', 'Perplexity'], f&quot;{path}/K={start_limit}to{end_limit}.csv&quot;)
    return results


# Function plot the visualization of an LDA model. This visualization is then
# saved as an html file inside the given path
def single_lda_model_visualization(k, c, corpus, dictionary, lda_model, path):
    vis = gensimvis.prepare(lda_model, corpus, dictionary)
    pyLDAvis.save_html(vis, f&quot;{path}/visualization.html&quot;)


# Given the results produced by test_lda_models, loop though the models and save the
# topic words of each model and the visualization of the topics in the given path
def save_lda_result(k, c, lda_model, corpus, dictionary, path):
    list_tuples_to_csv(lda_model.print_topics(num_topics=k), ['Topic#', 'Associated Words'], f&quot;{path}/associated_words.csv&quot;)
    single_lda_model_visualization(k, c, corpus, dictionary, lda_model, path)

# This is the entire pipeline that needs to be performed for a single dataset,
# which includes computing the LDA models from start to end limit and calculating 
# and saving the topic words and visual graphs for the top n topics with the highest
# coherence score.
def perform_topic_modelling_single_df(df, start_limit, end_limit, path):

    # Extracting the necessary data required for LDA model computation
    text_descriptions,dictionary, corpus = create_dict_and_corpus(df)

    results_lda = test_lda_models(text_descriptions, corpus, dictionary, start_limit, end_limit, path)
    # Sorting the results based on the 2nd tuple value returned which is 'coherence'
    results_lda.sort(key=lambda x:x[2],reverse=True)

    # Getting the top 5 results to save pass to save_lda_results function
    results = results_lda[:5]
    corpus_for_saving = [dictionary.doc2bow(text) for text in text_descriptions]
    texts = remove_characters_and_create_list(df, 'Description', split=False)

    # Perfrom application to topic modelling for the best lda model based on the
    # coherence score (TODO maybe test with other lda models?)
    print(&quot;getting descriptions for csv&quot;)
    for k, c, _, _ in results:
        dir_path = make_dir_if_not_exists(f&quot;{path}/k={k}_chunk={c}&quot;)
        p = int(math.log10(c))
        lda_model = gensim.models.ldamulticore.LdaMulticore(corpus,
                                            num_topics=k, 
                                            id2word=dictionary, 
                                            passes=p,
                                            chunksize=c)
        print(f&quot;=====REDOING K={k} model with passes={p} and chunksize={c}=====&quot;)
        save_lda_result(k,c, lda_model, corpus_for_saving, dictionary, dir_path)
        application_to_topic_modelling(df, k, c, lda_model, corpus_for_saving, texts, dir_path)


# Performs the whole topic modelling pipeline taking different genre data sets 
# and the entire dataset as a whole
def perform_topic_modelling_pipeline(path_ex):
    # entire_df = pd.read_csv(&quot;../data/preprocessed_data/preprocessed_10000_trial.csv&quot;)
    entire_df = pd.read_csv(os.path.join(ROOT_DIR, f&quot;data/preprocessed_data/preprocessedData_{path_ex}.csv&quot;))
    print(&quot;size of df&quot;)
    print(entire_df.shape)
    # For entire df go from start limit to ngenres to find best LDA model
    nGenres = row_counter(os.path.join(ROOT_DIR, f&quot;data/genre_wise_data/data{path_ex}/genre_frequency.csv&quot;))
    nGenres_rounded = math.ceil(nGenres / 5) * 5
    print(f&quot;Original number of genres should be {nGenres}, but we are rounding to {nGenres_rounded}&quot;)
    path = make_dir_if_not_exists(os.path.join(ROOT_DIR, f&quot;results/data{path_ex}/aall_data&quot;))
    perform_topic_modelling_single_df(entire_df, 5, 100, path)
</code></pre>
","16483691","","","","","2021-07-19 22:02:25","Solving memory issues when using Gensim LDA Multicore","<python><memory><gensim><lda><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"56177411","1","","","2019-05-16 22:35:37","","0","170","<p>Assume you have a (wikipedia) pre-trained word2vec model, and train it on an additional corpus (very small, 1000 scentences).</p>

<p>Can you imagine a way to limit a vector-search to the ""re-trained"" corpus only?</p>

<p>For example</p>

<pre><code>model.wv.similar_by_vector() 
</code></pre>

<p>will simply find the closest word for a given vector, no matter if it is part of the Wikipedia corpus, or the re-trained vocabulary.</p>

<p>On the other hand, for 'word' search the concept exists:</p>

<pre><code>most_similar_to_given('house',['garden','boat'])
</code></pre>

<p>I have tried to train based on the small corpus from scratch, and it somewhat works as expected. But of course could be much more powerful if the assigned vectors come from a pre-trained set.</p>
","2563693","","","","","2019-05-20 22:00:37","word2vec limit similar_by_vector() result to re-trained corpus","<nlp><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"64949799","1","64951046","","2020-11-22 01:23:40","","0","65","<p>I am trying to load a pretrained model <a href=""https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html#sphx-glr-auto-examples-howtos-run-downloader-api-py"" rel=""nofollow noreferrer"">listed here</a> to test the similarity of a handful of paragraphs.</p>
<p>Can gensim's pretrained models only be used with word-level vectors, or can the models also be used for document-length vectors?</p>
","823859","","","","","2020-11-22 05:46:57","Can the gensim pretrained models be used for doc2vec models?","<python><gensim><word-embedding><doc2vec>","1","0","","","","CC BY-SA 4.0"
"56236404","1","56244682","","2019-05-21 10:33:38","","0","90","<p>According to the docs and wikipedia:</p>

<p>mmap allows processes to share same chunk of ram</p>

<pre><code>word_vectors = KeyedVectors.load(config.get(wv_file))
</code></pre>

<p>This model loaded like this takes ~2.2 GB ram</p>

<pre><code>word_vectors = KeyedVectors.load(config.get(wv_file), mmap='r')
</code></pre>

<p>This model loaded like this takes ~1.2 GB ram</p>

<p>Why am I observing such drastic decrease in ram consumption?</p>

<p>Loading multiple models simultaneously, works as expected and models share the ~1 GM memory.  </p>
","5625696","","","","","2019-05-21 18:54:24","Why mmap flag reduces memory consumption for single Word2Vec instance","<ram><gensim><mmap><word2vec>","1","0","","","","CC BY-SA 4.0"
"60166663","1","","","2020-02-11 10:22:14","","1","1271","<p>While working on tasks like text classification, QA, the original vocabulary generated from the corpus is usually too large, containing a lot of 'unimportant' words. The most popular ways I've seen to reduce the vocabulary size are discarding stop words and words with low frequencies.</p>

<p>For example, in <code>gensim</code></p>

<pre><code>gensim.utils.prune_vocab(vocab, min_reduce, trim_rule=None):
    Remove all entries from the vocab dictionary with count smaller than min_reduce.
    Modifies vocab in place, returns the sum of all counts that were pruned.
</code></pre>

<p>But in practice, setting the minimum count is empirical and does not seems quite exact. I notice that the term frequency of each word in the vocabulary often follows long-tail distribution, is it a good way if I only keep the top-K words that occupies X% (95%, 90%, 85%, ...) of the total term frequency? Or are there any sensible ways to reduce the vocabulary, without seriously influencing the NLP task? </p>
","12557578","","","","","2020-02-11 19:46:53","Are there good ways to reduce the size of a vocabulary in natural language processing?","<machine-learning><deep-learning><nlp><gensim>","2","0","","","","CC BY-SA 4.0"
"62656022","1","","","2020-06-30 11:05:00","","0","24","<p>I have trained a LDA model using the following parameters:</p>
<pre><code>&gt;&gt; model = gensim.models.ldamodel.LdaModel(corpus=corpus,
 id2word=id2word,
 num_topics=25,
 passes=10,
 minimum_probability=0)
</code></pre>
<p>Then, I applied this model to a given corpus:</p>
<pre><code>&gt;&gt; lda_corpus = model[corpus]
</code></pre>
<p>I was expecting lda_corpus to be a list of lists or 2D matrix, where the number of rows is the number of docs and the number of columns is the number of topics and each element matrix, a tuple of the form (topic_index, probability). However I am getting this weird result where some elements are again a list:</p>
<pre><code>
&gt;&gt; print(lda_model_1[corpus[0]])


&gt;&gt; ([(0, 0.012841966), (3, 0.073988825), (4, 0.05184835), (8, 0.38537887), (10, 0.022958927), (11, 0.24562633), (13, 0.05168812), (17, 0.06522224), (21, 0.024792604)], [(0, [11]), (1, [8, 3, 17, 13]), (2, [3, 17, 8, 13]), (3, [8, 3]), (4, [11]), (5, [8, 17, 3]), (6, [4]), (7, [4, 8]), (8, [8, 13, 3]), (9, [11]), (10, [8, 0]), (11, [8, 13, 0]), (12, [21]), (13, [11]), (14, [11]), (15, [8]), (16, [8, 11, 13, 0]), (17, [11]), (18, [11, 17]), (19, [8, 13, 17, 3]), (20, [17, 13, 8]), (21, [17, 11, 8]), (22, [11]), (23, [8]), (24, [8, 13]), (25, [8, 3, 13])], [(0, [(11, 1.0)]), (1, [(3, 0.15384258), (8, 0.71774876), (13, 0.011975089), (17, 0.11643356)]), (2, [(3, 0.45133045), (8, 0.21692151), (13, 0.09479065), (17, 0.23232804)]), (3, [(3, 0.24423833), (8, 0.75576156)]), (4, [(11, 1.0)]), (5, [(3, 0.02001735), (8, 1.6895359), (17, 0.2904468)]), (6, [(4, 1.0)]), (7, [(4, 1.2565874), (8, 0.7367453)]), (8, [(3, 0.05150538), (8, 0.8553984), (13, 0.07775658)]), (9, [(11, 2.0)]), (10, [(0, 0.13937186), (8, 0.8588695)]), (11, [(0, 0.023420962), (8, 0.7131521), (13, 0.263427)]), (12, [(21, 1.0)]), (13, [(11, 0.99124163)]), (14, [(11, 2.0)]), (15, [(8, 1.0)]), (16, [(0, 0.011193657), (8, 1.7189965), (11, 0.23104382), (13, 0.029387457)]), (17, [(11, 1.9989293)]), (18, [(11, 0.9135094), (17, 0.08400644)]), (19, [(3, 0.07146881), (8, 2.1837764), (13, 0.38799366), (17, 0.352704)]), (20, [(8, 0.22638415), (13, 0.24114841), (17, 0.52740365)]), (21, [(8, 0.02224951), (11, 0.24574266), (17, 0.7231928)]), (22, [(11, 1.0)]), (23, [(8, 1.0)]), (24, [(8, 0.972818), (13, 0.027181994)]), (25, [(3, 0.16742931), (8, 0.7671518), (13, 0.05224549)])])
</code></pre>
<p>I would appreciate any help.</p>
","11478063","","","","","2020-07-08 10:12:52","Unexpected output when applying LDA trained model to given corpus","<python><gensim><lda><topic-modeling>","1","3","","","","CC BY-SA 4.0"
"68736698","1","","","2021-08-11 05:59:57","","0","34","<p>I would like to calculate/estimate the amount of memory required for a LSI model with Ticket Volume of 40,000, no. of topic 20,000 and dictionary size of 80,000. The data type used is float64 which consumes 8 bytes. I am relying on gensim package's lsi model. <a href=""https://radimrehurek.com/gensim/models/lsimodel.html"" rel=""nofollow noreferrer"">gensim reference</a>.</p>
<p>Can you someone please suggest the RAM memory calculation formula/method?</p>
","5238383","","","","","2021-08-12 12:43:12","RAM memory requirements for Machine Learning model (Latent semantic indexing)","<python><machine-learning><memory-management><nlp><gensim>","0","4","","","","CC BY-SA 4.0"
"53648464","1","","","2018-12-06 09:43:24","","1","464","<p>I am using Relaxed Word Mover's Distance in the package <code>text2vec</code> to compute the distance between documents, so as to identify the most similar document for each target document.  Word vectors are compiled using <code>FastText</code> available in the pacakage <code>gensim</code> in Python.  The length of the documents can vary from one word to over 50 words.  Some documents are duplicated in the corpus.  I assume that the distance between these duplicated should be very short and the value across different pair of the same documents should be the same.  However, what I observe is that the distance of these identical pair can vary from close to 0 to over 1, and some other less relevant documents are even concluded to be closer than these identical pair.  The command I use is as follows:</p>

<pre><code>library(text2vec)
tokens = word_tokenizer(tolower(data$item))

v = create_vocabulary(itoken(tokens))

v = prune_vocabulary(v, term_count_min = 12, term_count_max = 1000000)
it = itoken(tokens)

# Use our filtered vocabulary
vectorizer = vocab_vectorizer(v)

dtm = create_dtm(it, vectorizer)
tcm = create_tcm(it, vectorizer, skip_grams_window = 50)

#word vectors from FastText
wv_fasttext&lt;-as.data.frame(wv_fasttext)
rownames(wv_fasttext) &lt;- wv_fasttext[, 'na']

wv_fasttext$name&lt;- NULL
wv_fasttext&lt;- data.matrix(wv_fasttext, rownames.force = TRUE)

rwmd_model = RWMD$new(wv)

rwmd_distance = dist2(dtm[1:1000,], dtm[1:1000,], method = rwmd_model, norm 
= 'none')
</code></pre>

<p>Is there any problem with the above model? </p>
","10753818","","9592557","","2018-12-06 10:16:37","2018-12-06 10:16:37","Relaxed Word Mover's Distance in R","<python><r><gensim><wmd><text2vec>","0","2","","","","CC BY-SA 4.0"
"44983315","1","","","2017-07-08 06:40:33","","1","688","<p>I applied LDA from gensim package on the corpus and I get the probability with each term. My problem is how I get only the terms without their probability.
Here is my code:</p>

<pre><code>K = ldamodel.num_topics
t = 0
topicWordProbMat = ldamodel.print_topics(K)
for  topic_dist in topicWordProbMat:
    print('Topic #',t,topic_dist)
    t = t + 1
</code></pre>

<p>The output as example is like this:</p>

<pre><code>Topic # 0 '0.181*things + 0.181*amazon + 0.181*good
Topic # 1 '0.031*nokia + 0.031*microsoft + 0.031*apple  
</code></pre>

<p>and I want it as this:</p>

<pre><code>Topic # 0 things amazon good
Topic # 1 nokia microsoft apple
</code></pre>

<p>any idea how? Thanks in advance</p>
","6413657","","","","","2017-07-09 08:40:50","Get topics terms only with LDA","<python-3.x><gensim><lda>","1","0","1","","","CC BY-SA 3.0"
"45125798","1","","","2017-07-16 06:35:56","","14","21398","<p>I have two directories from which I want to read their text files and label them, but I don't know how to do this via <code>TaggedDocument</code>. I thought it would work as TaggedDocument([Strings],[Labels]) but this doesn't work apparently. </p>

<p>This is my code: </p>

<pre><code>from gensim import models
from gensim.models.doc2vec import TaggedDocument
import utilities as util
import os
from sklearn import svm
from nltk.tokenize import sent_tokenize
CogPath = ""./FixedCog/""
NotCogPath = ""./FixedNotCog/""
SamplePath =""./Sample/""
docs = []
tags = []
CogList = [p for p in os.listdir(CogPath) if p.endswith('.txt')]
NotCogList = [p for p in os.listdir(NotCogPath) if p.endswith('.txt')]
SampleList = [p for p in os.listdir(SamplePath) if p.endswith('.txt')]
for doc in CogList:
     str = open(CogPath+doc,'r').read().decode(""utf-8"")
     docs.append(str)
     print docs
     tags.append(doc)
     print ""###########""
     print tags
     print ""!!!!!!!!!!!""
for doc in NotCogList:
     str = open(NotCogPath+doc,'r').read().decode(""utf-8"")
     docs.append(str)
     tags.append(doc)
for doc in SampleList:
     str = open(SamplePath + doc, 'r').read().decode(""utf-8"")
     docs.append(str)
     tags.append(doc)

T = TaggedDocument(docs,tags)

model = models.Doc2Vec(T,alpha=.025, min_alpha=.025, min_count=1,size=50)
</code></pre>

<p>and this is the error I get: </p>

<pre><code>Traceback (most recent call last):
  File ""/home/farhood/PycharmProjects/word2vec_prj/doc2vec.py"", line 34, in &lt;module&gt;
    model = models.Doc2Vec(T,alpha=.025, min_alpha=.025, min_count=1,size=50)
  File ""/home/farhood/anaconda2/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 635, in __init__
    self.build_vocab(documents, trim_rule=trim_rule)
  File ""/home/farhood/anaconda2/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 544, in build_vocab
    self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
  File ""/home/farhood/anaconda2/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 674, in scan_vocab
    if isinstance(document.words, string_types):
AttributeError: 'list' object has no attribute 'words'
</code></pre>
","4660314","","7117003","","2018-12-15 19:32:14","2019-07-22 20:13:46","How to use TaggedDocument in gensim?","<python><nltk><gensim><word2vec><doc2vec>","3","3","","","","CC BY-SA 4.0"
"60301796","1","","","2020-02-19 13:45:52","","0","510","<p>I have a large number of sentences in a database and I want to find the most similar of those sentences to a single sentence that the user types in.  </p>

<p>It looks like I may be able to do this with <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_annoy.html"" rel=""nofollow noreferrer"">annoy and gensim</a>, but all the examples I can see are using word2vec which I believe is good for finding single similar words, but not for sentences.  However, I note that the AnnoyIndexer() can take a word2vec OR a doc2vec model.</p>

<p>Am I correct that the process is the same, but swapping the word2vec model with a doc2vec model and using a doc2vec vector of the search sentence?</p>

<p>Do I need to use pre-trained word embeddings in any way, or do I literally just train the doc2vec model with the corpus of sentences that I have in my database?</p>

<p>Thank you!</p>
","7412939","","","","","2020-02-19 21:40:11","Gensim and Annoy for finding similar sentences","<python><nlp><gensim>","1","1","","","","CC BY-SA 4.0"
"53536021","1","53539596","","2018-11-29 09:45:04","","0","37","<p>I am currently using custom corpus that wields Tagged Documents</p>

<pre><code>class ClassifyCorpus(object):
    def __iter__(self):
        with open(train_data) as fp:
            for line in fp:
                splt = line.split(':')
                id = splt[0]
                text = splt[1].replace('\n', '')
                yield TaggedDocument(text.split(), [id])
</code></pre>

<p>Looking at the source code of Brown Corpus, is see that it just reads from directory and handles the tagging of the documents for me.</p>

<p>I tested it and didn't see improvements in the training speed.</p>
","5625696","","","","","2018-11-29 12:58:10","Why use TaggedBrownCorpus when training gensim doc2vec","<python><gensim><corpus><doc2vec>","1","0","","","","CC BY-SA 4.0"
"36160322","1","36160533","","2016-03-22 16:43:32","","0","4400","<p>I am implementing the tutorial of gensim <a href=""http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow"">http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/</a> that includes the line 
    sentences = word2vec.Text8Corpus('/tmp/text8')
however when I run the program I get the error that text8 does not exist. Looking through the code I see that Text8Corpus is a method that accepts argument type object. The instructions indicate that it should be passed</p>

<p><a href=""http://mattmahoney.net/dc/text8.zip"" rel=""nofollow"">http://mattmahoney.net/dc/text8.zip</a></p>

<p>When I manually download this file and attempt to pass the resulting imbd uncompressed data set I am told that permissions denied. Does anyone have any insight into this problem ? Am I suppose to have downloaded the imdb dataset myself or was there suppose to be some pointers in the code that do it automatically ? </p>
","2033214","","","","","2016-03-22 16:53:18","what is ('/tmp/text8') gensim","<python><gensim>","1","1","","","","CC BY-SA 3.0"
"18840537","1","18855602","","2013-09-17 02:50:55","","-1","126","<p>What I want to do is, get a text training set (natural language) and increase this set with automatically created text that tries to mimic the text content. I'm using a bag-of-words assumption, sequence doesn't matter, syntax doesn't matter, I just want to create text that contains words that is pertinent with the general topic of the base.</p>

<p>Right now I'm using <strong>Latent Dirichlet Allocation</strong> to classify my documents in topics distributions, average the topic distribution of my set, and generate documents from these topic distribution.</p>

<p>I want to know two things:</p>

<blockquote>
  <p>1- Is there a better way to do that?</p>
  
  <p>2- Can I train LDA with texts that are not of the domain of my set,
  without tainting my topics: Eg. The set that I want to increase has
  texts about politics. Can I train my model with any kind of text
  (cars, fashion, musics) and classificates my base of politics text get its topics distributions and generates similar text from this distribution.</p>
</blockquote>

<p>I'm using python 2.7 and gensim.</p>
","2785926","","2785926","","2013-09-17 03:03:36","2013-09-17 16:56:07","How to generate pertinent text?","<algorithm><language-agnostic><nlp><probability-theory><gensim>","1","2","","2014-04-24 05:58:27","","CC BY-SA 3.0"
"60420718","1","60423491","","2020-02-26 18:57:43","","0","678","<p>I am training a LDA Model using Gensim:</p>

<pre><code>dictionary = corpora.Dictionary(section_2_sentence_df['Tokenized_Sentence'].tolist())
dictionary.filter_extremes(no_below=20, no_above=0.7)
corpus = [dictionary.doc2bow(text) for text in (section_2_sentence_df['Tokenized_Sentence'].tolist())]

num_topics = 15
passes = 200
chunksize = 100
lda_sentence_model = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=num_topics, 
                                                              id2word=dictionary, 
                                                              passes=passes, 
                                                              chunksize=chunksize,
                                                              random_state=100,
                                                              workers = 3)
</code></pre>

<p>After training i need the topics for further analysis. Unfortunately the show_topics function only returns <strong>10 topics</strong>. I expected the defined number of <strong>15 topics</strong>. Does anyone know if that is on purpose or an error that can be solved?</p>

<pre><code>print(len(lda_sentence_model.show_topics(formatted=False)))
</code></pre>
","8217561","","","","","2020-02-26 22:21:20","Python Gensim LDA Model show_topics funciton","<python><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"27177721","1","","","2014-11-27 19:37:11","","2","457","<p>I have a tf-idf matrix already, with rows for terms and columns for documents. Now I want to train a LDA model with the given terms-documents matrix. The first step seems to be using <code>gensim.matutils.Dense2Corpus</code> to convert the matrix into the corpus format. But how to construct the <code>id2word</code> parameter? I have the list of the terms (#terms==#rows) but I don't know the format of the dictionary so I cannot construct the dictionary from functions like <code>gensim.corpora.Dictionary.load_from_text</code>. Any suggestions? Thank you.</p>
","688080","","","","","2014-12-09 12:15:19","Training a LDA model with gensim from some external tf-idf matrix and term list","<python-3.x><tf-idf><lda><topic-modeling><gensim>","1","0","","","","CC BY-SA 3.0"
"44831480","1","44843293","","2017-06-29 17:25:57","","0","740","<p>Using <code>gensim</code> <code>word2vec</code> model in order to calculate similarities between two words. Training the model with a 250mb Wikipedia text gave a good result - about 0.7-0.8 similarity score for a related pair of words.</p>

<p>The problem is when I am using the <code>Phraser</code> model to add up phrases the similarity score drops to nearly zero for the same exact words.</p>

<p><strong>Results with the phrase model:</strong></p>

<pre><code>speed - velocity - 0.0203503432178
high - low - -0.0435703782446
tall - high - -0.0076987978333
nice - good - 0.0368784716958
computer - computational - 0.00487748035808
</code></pre>

<p>That probably means I am not using the Phraser model correctly.</p>

<p><strong>My Code:</strong></p>

<pre><code>    data_set_location = **
    sentences = SentenceIterator(data_set_location)

    # Train phrase locator model
    self.phraser = Phraser(Phrases(sentences))

    # Renewing the iterator because its empty
    sentences = SentenceIterator(data_set_location)

    # Train word to vector model or load it from disk
    self.model = Word2Vec(self.phraser[sentences], size=256, min_count=10, workers=10)



class SentenceIterator(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname), 'r', encoding='utf-8', errors='ignore'):
                yield line.lower().split()
</code></pre>

<p>Trying the pharser model alone looks like it worked fine:</p>

<p><code>&gt;&gt;&gt;vectorizer.phraser['new', 'york', 'city', 'the', 'san', 'francisco']
['new_york', 'city', 'the', 'san_francisco']</code></p>

<p>What can cause such behavior?</p>

<p><strong>Trying to figure out the solution:</strong></p>

<p>according to gojomo answer, I've tried to create a <code>PhraserIterator</code>:</p>

<pre><code>import os

class PhraseIterator(object):
def __init__(self, dirname, phraser):
    self.dirname = dirname
    self.phraser = phraser

def __iter__(self):
    for fname in os.listdir(self.dirname):
        for line in open(os.path.join(self.dirname, fname), 'r', encoding='utf-8', errors='ignore'):
            yield self.phraser[line.lower()]
</code></pre>

<p>using this iterator I've tried to train my <code>Word2vec</code> model.</p>

<pre><code>phrase_iterator = PhraseIterator(text_dir, self.phraser)
self.model = Word2Vec(phrase_iterator, size=256, min_count=10, workers=10
</code></pre>

<p>Word2vec training log:</p>

<pre><code>    Using TensorFlow backend.
2017-06-30 19:19:05,388 : INFO : collecting all words and their counts
2017-06-30 19:19:05,456 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types
2017-06-30 19:20:30,787 : INFO : collected 6227763 word types from a corpus of 28508701 words (unigram + bigrams) and 84 sentences
2017-06-30 19:20:30,793 : INFO : using 6227763 counts as vocab in Phrases&lt;0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000&gt;
2017-06-30 19:20:30,793 : INFO : source_vocab length 6227763
2017-06-30 19:21:46,573 : INFO : Phraser added 50000 phrasegrams
2017-06-30 19:22:22,015 : INFO : Phraser built with 70065 70065 phrasegrams
2017-06-30 19:22:23,089 : INFO : saving Phraser object under **/Models/word2vec/phrases_model, separately None
2017-06-30 19:22:23,441 : INFO : saved **/Models/word2vec/phrases_model
2017-06-30 19:22:23,442 : INFO : collecting all words and their counts
2017-06-30 19:22:29,347 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2017-06-30 19:33:06,667 : INFO : collected 143 word types from a corpus of 163438509 raw words and 84 sentences
2017-06-30 19:33:06,677 : INFO : Loading a fresh vocabulary
2017-06-30 19:33:06,678 : INFO : min_count=10 retains 95 unique words (66% of original 143, drops 48)
2017-06-30 19:33:06,679 : INFO : min_count=10 leaves 163438412 word corpus (99% of original 163438509, drops 97)
2017-06-30 19:33:06,683 : INFO : deleting the raw counts dictionary of 143 items
2017-06-30 19:33:06,683 : INFO : sample=0.001 downsamples 27 most-common words
2017-06-30 19:33:06,683 : INFO : downsampling leaves estimated 30341972 word corpus (18.6% of prior 163438412)
2017-06-30 19:33:06,684 : INFO : estimated required memory for 95 words and 256 dimensions: 242060 bytes
2017-06-30 19:33:06,685 : INFO : resetting layer weights
2017-06-30 19:33:06,724 : INFO : training model with 10 workers on 95 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2017-06-30 19:33:14,974 : INFO : PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0
2017-06-30 19:33:23,229 : INFO : PROGRESS: at 0.24% examples, 607 words/s, in_qsize 0, out_qsize 0
2017-06-30 19:33:31,445 : INFO : PROGRESS: at 0.48% examples, 810 words/s, 
...
2017-06-30 20:19:00,864 : INFO : PROGRESS: at 98.57% examples, 1436 words/s, in_qsize 0, out_qsize 1
2017-06-30 20:19:06,193 : INFO : PROGRESS: at 99.05% examples, 1437 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:11,886 : INFO : PROGRESS: at 99.29% examples, 1437 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:17,648 : INFO : PROGRESS: at 99.52% examples, 1438 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:22,870 : INFO : worker thread finished; awaiting finish of 9 more threads
2017-06-30 20:19:22,908 : INFO : worker thread finished; awaiting finish of 8 more threads
2017-06-30 20:19:22,947 : INFO : worker thread finished; awaiting finish of 7 more threads
2017-06-30 20:19:22,947 : INFO : PROGRESS: at 99.76% examples, 1439 words/s, in_qsize 0, out_qsize 8
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 6 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 5 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 4 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 3 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 2 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 1 more threads
2017-06-30 20:19:22,949 : INFO : worker thread finished; awaiting finish of 0 more threads
2017-06-30 20:19:22,949 : INFO : training on 817192545 raw words (4004752 effective words) took 2776.2s, 1443 effective words/s
2017-06-30 20:19:22,950 : INFO : saving Word2Vec object under **/Models/word2vec/word2vec_model, separately None
2017-06-30 20:19:22,951 : INFO : not storing attribute syn0norm
2017-06-30 20:19:22,951 : INFO : not storing attribute cum_table
2017-06-30 20:19:22,958 : INFO : saved **/Models/word2vec/word2vec_model
</code></pre>

<p>After this training - any of two similarity calculation produce zero:</p>

<pre><code>speed - velocity - 0
high - low - 0
</code></pre>

<p>So it seems that the iterator is not working well so I've checked it using gojomo trick:</p>

<pre><code>print(sum(1 for _ in s))
1

print(sum(1 for _ in s))
1
</code></pre>

<p>And its working. </p>

<p>What may be the problem?</p>
","4869599","","4869599","","2017-07-01 15:06:56","2017-07-01 15:57:13","Word2vec gensim - Calculating similarity between word isn't working when using phrases","<python><deep-learning><gensim><word2vec><phrases>","2","0","","","","CC BY-SA 3.0"
"53748921","1","","","2018-12-12 18:07:38","","2","134","<p>I have a question about topic modeling via python and gensim library: when I run the following code, it works well and comes up with the related topics but I want to see each topic per document listed in the .csv file but it shuffles. For example, 1st topic is from the 2nd document but the 2nd topic is from the 1st document, and the 3rd is from the 3rd one. And when I run the same code, it shuffles again. How can I fix that and get topics per document or/and linked topics directly to the Ids or authors of the documents that could be listed in the 1st column?</p>

<h1>Codes:</h1>

<h1>Step 1:</h1>

<pre><code> import nltk
 import csv
 import re
 import nltk.corpus
 import gensim
 from gensim import corpora
 from nltk.corpus import stopwords
 from nltk.stem.wordnet import WordNetLemmatizer
 import string
</code></pre>

<h1>Step 2: Loading data and Processing</h1>

<pre><code> doc_complete = open('/home/erdal/Desktop/big_data/abstract1.csv', 'r').readlines()
 stop = set(stopwords.words('english'))
 exclude = set(string.punctuation)
 lemma = WordNetLemmatizer()
 def clean(doc):
    stop_free = "" "".join([i for i in doc.lower().split() if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = "" "".join(lemma.lemmatize(word) for word in punc_free.split())
    return normalized
    doc_clean = [clean(doc).split() for doc in doc_complete]
    print(doc_clean)
    dictionary = corpora.Dictionary(doc_clean)
    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
    Lda = gensim.models.ldamodel.LdaModel
    ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)
    print(ldamodel.print_topics(num_topics=3, num_words=3))
</code></pre>

<h1>Step 3: Printing out the topics</h1>

<pre><code>  myData = ldamodel.print_topics(num_topics=3, num_words=3)
  myFile = open('/home/erdal/Desktop/big_data/all_data_1_topics.csv', 'w')
  with myFile:
   writer = csv.writer(myFile)
   writer.writerows(myData)
   print(""Writing complete"")
</code></pre>

<h1>Topics:</h1>

<pre><code> [(0, '0.036*""learning"" + 0.036*""student"" + 0.026*""intergroup""'), (1, '0.005*""abstract"" + 0.005*""significant"" + 0.005*""using""'), (2, '0.042*""clickers"" + 0.027*""motivation"" + 0.027*""student""')]
</code></pre>
","2699070","","2699070","","2018-12-12 22:33:55","2018-12-12 22:33:55","Topic Modeling, Gensim, Python, Getting Topic Models According to fixed IDs or Linked Data","<python-3.x><gensim><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"36263594","1","","","2016-03-28 13:41:49","","1","796","<p>i am using gensim in ubuntu. version is 0.12.4. my word2vec model is not consistent. every time i build the model based on the same exact sentences and same parameter it still have different presentations of the words.</p>

<p>here is the code (that i stole from the initial post)</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04913874,  0.04574081, -0.07402877, -0.03270053,  0.06598952,
        0.04157289,  0.05075986,  0.01770534, -0.03796235,  0.04594197], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04907205,  0.04569579, -0.07379777, -0.03273782,  0.06579078,
        0.04167712,  0.05083019,  0.01780009, -0.0378389 ,  0.04578455], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04906179,  0.04569826, -0.07382379, -0.03274316,  0.06583244,
        0.04166647,  0.0508585 ,  0.01777468, -0.03784611,  0.04578935], dtype=float32)
</code></pre>

<p>I have also tried to set seed to some fixed int but this didnt seem to help. i also tried to reinstall gensim which also didnt help.</p>

<p>Any idea how to stabilize my model??</p>
","5817030","","4414003","","2016-03-28 15:17:46","2017-03-22 10:38:11","gensim word2vec giving inconsistent results","<python><gensim><word2vec>","1","4","","","","CC BY-SA 3.0"
"27659985","1","","","2014-12-26 17:25:45","","13","21066","<p>I am trying to do the following <a href=""https://www.kaggle.com/c/word2vec-nlp-tutorial"" rel=""noreferrer"">kaggle assignmnet</a>. I am using gensim package to use word2vec. I am able to create the model and store it to disk. But when I am trying to load the file back I am getting the error below.</p>

<pre><code>    -HP-dx2280-MT-GR541AV:~$ python prog_w2v.py 
Traceback (most recent call last):
  File ""prog_w2v.py"", line 7, in &lt;module&gt;
    models = gensim.models.Word2Vec.load_word2vec_format('300features_40minwords_10context.txt', binary=True)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 579, in load_word2vec_format
    header = utils.to_unicode(fin.readline())
  File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 190, in any2unicode
    return unicode(text, encoding, errors=errors)
  File ""/usr/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>I find similar question. But I was unable to solve the problem. My prog_w2v.py is as below.</p>

<pre><code>import gensim
import time
start = time.time()    
models = gensim.models.Word2Vec.load_word2vec_format('300features_40minwords_10context.txt', binary=True) 
end = time.time()   
print end-start,""   seconds""
</code></pre>

<p>I am trying to generate the model using <a href=""http://ideone.com/9DXo4w"" rel=""noreferrer"">code here</a>. The program takes about half an hour to generate the model. Hence I am unable to run it many times to debug it. </p>
","3992452","","769871","","2017-03-02 17:58:03","2018-12-03 06:21:07","Error: 'utf8' codec can't decode byte 0x80 in position 0: invalid start byte","<python><character-encoding><gensim><word2vec><kaggle>","4","0","3","","","CC BY-SA 3.0"
"18867516","1","20387248","","2013-09-18 08:39:47","","2","2998","<p>I can save a serialized corpus into <code>foobar.mm</code> but when i try to load it, it gives <code>UnpicklingError</code>. Loading the dictionary seems fine though. <strong>Anyone knows how to resolve this? And why does this occur?</strong></p>

<pre><code>&gt;&gt;&gt; from gensim import corpora
&gt;&gt;&gt; docs = [""this is a foo bar"", ""you are a foo""]
&gt;&gt;&gt; texts = [[i for i in doc.lower().split()] for doc in docs]
&gt;&gt;&gt; print texts
[['this', 'is', 'a', 'foo', 'bar'], ['you', 'are', 'a', 'foo']]

&gt;&gt;&gt; dictionary = corpora.Dictionary(texts)
&gt;&gt;&gt; dictionary.save('foobar.dic')
&gt;&gt;&gt; print dictionary
Dictionary(7 unique tokens)
&gt;&gt;&gt; corpora.Dictionary.load('foobar.dic')
&lt;gensim.corpora.dictionary.Dictionary object at 0x329f910&gt;

&gt;&gt;&gt; corpus = [dictionary.doc2bow(text) for text in texts]
&gt;&gt;&gt; corpora.MmCorpus.serialize('foobar.mm', corpus)
&gt;&gt;&gt; corpus = corpora.MmCorpus.load('foobar.mm')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.8.6-py2.7.egg/gensim/utils.py"", line 166, in load
    return unpickle(fname)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.8.6-py2.7.egg/gensim/utils.py"", line 492, in unpickle
    return cPickle.load(open(fname, 'rb'))
cPickle.UnpicklingError: invalid load key, '%'.
</code></pre>
","610569","","","","","2013-12-04 22:15:34","How to resolve the unpicklingerror in loading gensim corpus? - python","<python><lda><topic-modeling><gensim>","2","0","1","","","CC BY-SA 3.0"
"60343072","1","","","2020-02-21 16:49:29","","1","343","<p>I have several thousand documents that I'd like to use in a gensim doc2vec model, but I only have 5grams for each of the documents, not the full texts in their original word order. In the doc2vec tutorial on the gensim website (<a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html</a>), a corpus is created with full texts and then the model is trained on that corpus. It looks something like this:</p>

<pre><code>[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern',...], tags=[1]), TaggedDocument(words=[.....], tags=[2]),...]
</code></pre>

<p>Is it possible create a training corpus where each document consists of a list of 5grams rather than a list of words in their original order?</p>
","519595","","","","","2020-02-23 21:01:13","Gensim doc2vec training on ngrams","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"60383368","1","","","2020-02-24 20:22:17","","1","100","<p>I am going through this Notebook about LDA and Document Similarity:</p>

<p><a href=""https://www.kaggle.com/ktattan/lda-and-document-similarity"" rel=""nofollow noreferrer"">https://www.kaggle.com/ktattan/lda-and-document-similarity</a></p>

<p>In this Notebook the Document similarity for a small set of documents gets computed however I want to compute the similarity for the whole corpus.</p>

<p>Instead of using test_df like in the Notebook:</p>

<pre><code>new_bow = dictionary.doc2bow(test_df.iloc[random_article_index,7])
new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])
</code></pre>

<p>I want to use train_df:</p>

<pre><code>new_bow= [id2word.doc2bow(doc) for doc in train_df['tokenized']]
new_doc_distribution = np.array([[tup[1] for tup in lst] for lst in model.get_document_topics(bow=new_bow)])
</code></pre>

<p>However this is does not work. My asumption is that its not possible because the lists that are used to create the numpy array (tup[1] in this case) are not of the same length. So its not possible to create a proper array which is needed to compute the Jensen Divergence.</p>

<p>Can somebody more experienced than me tell me if what I am trying is possible?</p>
","12195215","","","","","2020-03-01 16:34:15","Is it possible to compute Document similarity for every document in an LDA corpus?","<python><numpy><gensim><lda>","0","0","2","","","CC BY-SA 4.0"
"45454433","1","45454482","","2017-08-02 07:38:26","","1","919","<p>I'm new to doc2vec and I hope some one of you can help me with this issue.
I've asked a number of people about this issue, but nobody knows the solution.</p>

<p>What I wanto to do is cluster Doc2vec result into k-means. Please see below the code.</p>

<pre><code>mbk = MiniBatchKMeans(n_clusters=3, init_size=400, batch_size=300, verbose=1).fit(model_dm.docvecs[range([2000])                                                                                                 
MiniBatchKMeans.predict(mbk,mbk.labels_ )
</code></pre>

<p>I'm getting this Error.</p>

<pre><code>TypeErrorTraceback (most recent call last)
&lt;ipython-input-19-fbc57a13bf4b&gt; in &lt;module&gt;()
      6 
      7 
----&gt; 8 mbk = MiniBatchKMeans(n_clusters=3, init_size=400, batch_size=300, verbose=1).fit(model_dm.docvecs[:2000])
      9 
     10 #model_dm.docvecs.doctag_syn0[2000]

/usr/local/lib64/python2.7/site-packages/gensim/models/doc2vec.pyc in __getitem__(self, index)
    351             return self.doctag_syn0[self._int_index(index)]
    352 
--&gt; 353         return vstack([self[i] for i in index])
    354 
    355     def __len__(self):

TypeError: 'slice' object is not iterable
</code></pre>
","","user8400385","","user8400385","2017-08-02 09:09:16","2017-08-02 09:09:16","Gensim Doc2vec model clustering into K-means","<python><k-means><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"45478607","1","45490508","","2017-08-03 08:19:04","","2","878","<p>I would like to use Gensim's implemented Word2Vec with a list of context-word pairs as an input instead of sentences. I originally thought that entering the manually created context-word pairs as sentences would be equivalent to entering the raw sentences and setting the window parameter to 1, but the two approaches yield different results. How does Gensim's Word2Vec calculate the context-word pairs of sentences, and how should I enter my manually created pairs as an input to the function?</p>
","8410240","","","","","2017-08-03 17:02:20","Using gensim's Word2Vec with custom word-context pairs","<python><gensim>","1","0","","","","CC BY-SA 3.0"
"62827849","1","62829788","","2020-07-10 05:29:48","","0","139","<p>I know the <code>most_similar</code> method works when entering a previously added string, but how do you <em>reverse</em> search a numpy array of some word?</p>
<pre class=""lang-py prettyprint-override""><code>modelw2v = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True)
differenceArr = modelw2v[&quot;King&quot;] - modelw2v[&quot;Queen&quot;]


# This line does not work
modelw2v.most_similar(differenceArr) 
</code></pre>
","6412324","","6779252","","2020-07-10 07:25:10","2020-07-10 08:02:11","How to find most similar to an array in gensim","<python-3.x><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"36653882","1","","","2016-04-15 17:56:37","","9","9007","<p>I am trying to find the most important words in a corpus based on their TF-IDF scores.</p>

<p>Been following along the example at <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/tut2.html</a>. Based on</p>

<pre><code>&gt;&gt;&gt; for doc in corpus_tfidf:
...     print(doc)
</code></pre>

<p>the TF-IDF score is getting updated in each iteration. For example,</p>

<ul>
<li>Word 0 (""<em>computer</em>"" based on <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/tut1.html</a>), has a TF-IDF score of 0.5773 (Doc #1), 0.4442 (Doc #2).</li>
<li>Word 10 (""<em>graph</em>"") has a TF-IDF score of 0.7071 (Doc #7), 0.5080 (Doc #8), 0.4588 (Doc #9)</li>
</ul>

<p>So here's how I am currently getting the final TF-IDF score for each word,</p>

<pre><code>tfidf = gensim.models.tfidfmodel.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
d = {}
for doc in corpus_tfidf:
    for id, value in doc:
        word = dictionary.get(id)
        d[word] = value
</code></pre>

<p>Is there a better way?</p>

<p>Thanks in advance.</p>
","799188","","","","","2016-05-03 05:52:54","Getting TF-IDF Scores Of Words Using Gensim","<python><tf-idf><gensim>","1","0","","","","CC BY-SA 3.0"
"10559591","1","10559621","","2012-05-11 23:00:15","","4","1743","<p>Jep still playing around with Python. </p>

<p>I decided to try out Gensim, a tool to find out topics for a choosen word &amp; context. </p>

<p>So I wondered how to find a word in a section of text and extract 20 words together with it (as in 10 words before that spectic word and 10 words after that specific word) then to save it together with other such extractions so Gensim could be run on it.</p>

<p>What seems to be hard for me is to find a way to extract the 10 before and after words when the choosen word is found. I played with nltk before and by just tokenizing the text into words or sentences it was easy to get hold of the sentences. Still getting those words or the sentences before and after that specific sentence seems hard for me to figure out how to do.</p>

<p>For those who are confused (it's 1am here so I may be confusing) I'll show it with an example:</p>

<blockquote>
  <p>As soon as it had finished, all her blood rushed to her heart, for she
  was so angry to hear that Snow-White was yet living. ""But now,""
  thought she to herself, ""will I make something which shall destroy her
  completely."" Thus saying, she made a poisoned comb by arts which she
  understood, and then, disguising herself, she took the form of an old
  widow. She went over the seven hills to the house of the seven Dwarfs,
  and[15] knocking at the door, called out, ""Good wares to sell to-day!""</p>
</blockquote>

<p>If we say the word is Snow-White then I'd want to get this part extracted:</p>

<blockquote>
  <p>her heart, for she was so angry to hear that Snow-White was yet living. ""But now,""
  thought she to herself, ""will</p>
</blockquote>

<p>10 word before and after Snow-White. </p>

<p>It is also cool enough to instead get the sentence before and after the sentence Snow-White appeared in if this can be done in nltk and is easier. </p>

<p>I mean whatever works best I shall be happy with one of the two solutions if someone could help me.</p>

<p>If this can be done with Gensim too...and that is easier, then I shall be happy with that too. So any of the 3 ways will be fine...I just want to try and see how this can be done because atm my head is blank.</p>
","725826","","","","","2021-03-23 14:33:38","Extracting a word plus 20 more from a section (python)","<python><nltk><extraction><gensim>","3","0","3","","","CC BY-SA 3.0"
"45458493","1","45465462","","2017-08-02 10:37:59","","2","1358","<p>After creating word vectors in Gensim 2.2.0 from plain English text files with IMDB movie ratings:</p>

<pre><code>import gensim, logging
import smart_open, os
from nltk.tokenize import RegexpTokenizer

VEC_SIZE = 300 
MIN_COUNT = 5
WORKERS = 4
data_path = './data/'
vectors_path = 'vectors.bin.gz'

class AllSentences(object):
    def __init__(self, dirname):
        self.dirname = dirname
        self.read_err_cnt = 0
        self.tokenizer = RegexpTokenizer('[\'a-zA-Z]+', discard_empty=True)

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            print(fname)
            for line in open(os.path.join(self.dirname, fname)):
                words = []     
                try:
                    for word in self.tokenizer.tokenize(line):
                        words.append(word)
                    yield words
                except:
                    self.read_err_cnt += 1

sentences = AllSentences(data_path) 
</code></pre>

<p>Training and saving model:</p>

<pre><code>model = gensim.models.Word2Vec(sentences, size=VEC_SIZE, 
                               min_count=MIN_COUNT, workers=WORKERS)
word_vectors = model.wv
word_vectors.save(vectors_path)
</code></pre>

<p>And then trying to load it back:</p>

<pre><code>vectors = KeyedVectors.load_word2vec_format(vectors_path,
                                                    binary=True,
                                                    unicode_errors='ignore')
</code></pre>

<p>I get '<strong><em>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0</em></strong>' exception (see below). I tried different combinations of 'encoding' parameters including <strong><em>'ISO-8859-1'</em></strong> and <strong><em>'Latin1'</em></strong>. Also different combinations of <strong><em>'binary=True/False'</em></strong>. Nothing helps - the same exception, no matter what parameters are used. What is wrong? How to make loading vectors work?</p>

<p>Exception:</p>

<pre><code>UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-64-f353fa49685c&gt; in &lt;module&gt;()
----&gt; 1 w2v = get_w2v_vectors()

&lt;ipython-input-63-cbbe0a76e837&gt; in get_w2v_vectors()
      3     vectors = KeyedVectors.load_word2vec_format(word_vectors_path,
      4                                                     binary=True,
----&gt; 5                                                     unicode_errors='ignore')
      6 
      7                                                 #unicode_errors='ignore')

D:\usr\anaconda\lib\site-packages\gensim\models\keyedvectors.py in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)
    204         logger.info(""loading projection weights from %s"", fname)
    205         with utils.smart_open(fname) as fin:
--&gt; 206             header = utils.to_unicode(fin.readline(), encoding=encoding)
    207             vocab_size, vector_size = map(int, header.split())  # throws for invalid file format
    208             if limit:

D:\usr\anaconda\lib\site-packages\gensim\utils.py in any2unicode(text, encoding, errors)
    233     if isinstance(text, unicode):
    234         return text
--&gt; 235     return unicode(text, encoding, errors=errors)
    236 to_unicode = any2unicode
    237 

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
","1226649","","1226649","","2017-08-02 11:16:33","2017-08-02 15:48:40","Gensim: word vectors encoding problems","<python><gensim>","1","0","","","","CC BY-SA 3.0"
"18988886","1","19000922","","2013-09-24 18:07:35","","0","471","<p>In the <code>gensim</code> library, there is a <code>MmReader</code> class that converts a <a href=""http://bickson.blogspot.de/2012/02/matrix-market-format.html"" rel=""nofollow"">matrix market format</a> file into a python object. Sometimes it is necessary to <a href=""https://en.wikipedia.org/wiki/Transpose"" rel=""nofollow"">transpose the matrix</a>, hence the transposed parameter was introduced in the <code>MmReader</code>.</p>

<p>However, I am confused about why is it that at lines <code>525-526</code> and <code>567-568</code> of <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/matutils.py"" rel=""nofollow"">https://github.com/piskvorky/gensim/blob/develop/gensim/matutils.py</a> , the inversion of term-document values and id happens when <code>transposed == False</code>.</p>

<p>Anyone familiar with term-document matrices in information retrieval care to enlighten me?</p>

<pre><code>class MmReader(object):
    """"""
    Wrap a term-document matrix on disk (in matrix-market format), and present it
    as an object which supports iteration over the rows (~documents).

    Note that the file is read into memory one document at a time, not the whole
    matrix at once (unlike scipy.io.mmread). This allows us to process corpora
    which are larger than the available RAM.
    """"""
    def __init__(self, input, transposed=True):
        """"""
        Initialize the matrix reader.

        The `input` refers to a file on local filesystem, which is expected to
        be in the sparse (coordinate) Matrix Market format. Documents are assumed
        to be rows of the matrix (and document features are columns).

        `input` is either a string (file path) or a file-like object that supports
        `seek()` (e.g. gzip.GzipFile, bz2.BZ2File).
        """"""
        logger.info(""initializing corpus reader from %s"" % input)
        self.input, self.transposed = input, transposed
        if isinstance(input, basestring):
            input = open(input)
        header = input.next().strip()
        if not header.lower().startswith('%%matrixmarket matrix coordinate real general'):
            raise ValueError(""File %s not in Matrix Market format with coordinate real general; instead found: \n%s"" %
                             (self.input, header))
        self.num_docs = self.num_terms = self.num_nnz = 0
        for lineno, line in enumerate(input):
            if not line.startswith('%'):
                self.num_docs, self.num_terms, self.num_nnz = map(int, line.split())
                if not self.transposed: ## line 525
                    self.num_docs, self.num_terms = self.num_terms, self.num_docs
                break
        logger.info(""accepted corpus with %i documents, %i features, %i non-zero entries"" %
                     (self.num_docs, self.num_terms, self.num_nnz))

    def __len__(self):
        return self.num_docs

    def __str__(self):
        return (""MmCorpus(%i documents, %i features, %i non-zero entries)"" %
                (self.num_docs, self.num_terms, self.num_nnz))

    def skip_headers(self, input_file):
        """"""
        Skip file headers that appear before the first document.
        """"""
        for line in input_file:
            if line.startswith('%'):
                continue
            break

    def __iter__(self):
        """"""
        Iteratively yield vectors from the underlying file, in the format (row_no, vector),
        where vector is a list of (col_no, value) 2-tuples.

        Note that the total number of vectors returned is always equal to the
        number of rows specified in the header; empty documents are inserted and
        yielded where appropriate, even if they are not explicitly stored in the
        Matrix Market file.
        """"""
        if isinstance(self.input, basestring):
            fin = open(self.input)
        else:
            fin = self.input
            fin.seek(0)
        self.skip_headers(fin)

        previd = -1
        for line in fin:
            docid, termid, val = line.split()
            if not self.transposed:
                termid, docid = docid, termid
            docid, termid, val = int(docid) - 1, int(termid) - 1, float(val) # -1 because matrix market indexes are 1-based =&gt; convert to 0-based
            assert previd &lt;= docid, ""matrix columns must come in ascending order""
            if docid != previd:
                # change of document: return the document read so far (its id is prevId)
                if previd &gt;= 0:
                    yield previd, document

                # return implicit (empty) documents between previous id and new id
                # too, to keep consistent document numbering and corpus length
                for previd in xrange(previd + 1, docid):
                    yield previd, []

                # from now on start adding fields to a new document, with a new id
                previd = docid
                document = []

            document.append((termid, val,)) # add another field to the current document

        # handle the last document, as a special case
        if previd &gt;= 0:
            yield previd, document

        # return empty documents between the last explicit document and the number
        # of documents as specified in the header
        for previd in xrange(previd + 1, self.num_docs):
            yield previd, []


    def docbyoffset(self, offset):
        """"""Return document at file offset `offset` (in bytes)""""""
        # empty documents are not stored explicitly in MM format, so the index marks
        # them with a special offset, -1.
        if offset == -1:
            return []
        if isinstance(self.input, basestring):
            fin = open(self.input)
        else:
            fin = self.input

        fin.seek(offset) # works for gzip/bz2 input, too
        previd, document = -1, []
        for line in fin:
            docid, termid, val = line.split()
            if not self.transposed: ## line 567
                termid, docid = docid, termid
            docid, termid, val = int(docid) - 1, int(termid) - 1, float(val) # -1 because matrix market indexes are 1-based =&gt; convert to 0-based
            assert previd &lt;= docid, ""matrix columns must come in ascending order""
            if docid != previd:
                if previd &gt;= 0:
                    return document
                previd = docid

            document.append((termid, val,)) # add another field to the current document
        return document
#endclass MmReader
</code></pre>
","610569","","","","","2013-09-25 09:19:00","Transposed parameter in Matrix Market Format of gensim - python","<python><matrix><information-retrieval><tf-idf><gensim>","1","0","","","","CC BY-SA 3.0"
"45476932","1","","","2017-08-03 06:54:34","","3","436","<p>I want to use an LDA(Latent Dirichlet Allocation) model for an NLP purpose. 
To train a such a model from Wikipedia corpus takes about 5 to 6 hours and wiki corpus is about 8GB. Check the <a href=""http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""nofollow noreferrer"">Tutorial</a> </p>

<p>Rather than doing so, is there a place to download a built LDA model and use it directly with Gensim?</p>
","1867994","","355715","","2017-08-04 03:31:18","2017-08-05 15:52:03","Where to download a trained LDA model for Gensim?","<nlp><gensim><lda>","0","0","","","","CC BY-SA 3.0"
"53575141","1","","","2018-12-01 21:18:40","","-1","236","<p>Hello I am new in word2vec so I was trying a simple program to read file and get the vec of each word, but there's something wrong with the tokenization process, as word2vec takes into account each letter not word!</p>

<p>for instance my file contains ""hello this is my first trial""</p>

<pre><code>from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize


F = open('testfile')
f=F.read()
doc= word_tokenize(f)
print(f)

print(doc)

model = Word2Vec(doc,min_count=1)

# summarize the loaded model
print(model)

words = list(model.wv.vocab)
print(model['hello'])
</code></pre>

<p>I get an error that hello is not in the vocab, but when i use a letter 'h' it works</p>
","5712621","","5712621","","2018-12-01 21:24:08","2018-12-02 19:45:46","Trying to use word2Vec on file but not working?","<python><nlp><gensim><word2vec>","1","2","","","","CC BY-SA 4.0"
"27477084","1","27477536","","2014-12-15 03:51:47","","-1","185","<p>I am trying to run the GENSIM Topic modeling example in Canopy Express and get the following error on Sum() line.</p>

<pre><code>from gensim import corpora, models, similarities
from itertools import chain

"""""" DEMO """"""
documents = [""Human machine interface for lab abc computer applications"",
         ""A survey of user opinion of computer system response time"",
         ""The EPS user interface management system"",
         ""System and human system engineering testing of EPS"",
         ""Relation of user perceived response time to error measurement"",
         ""The generation of random binary unordered trees"",
         ""The intersection graph of paths in trees"",
         ""Graph minors IV Widths of trees and well quasi ordering"",
         ""Graph minors A survey""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
     for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once] for text in texts]
</code></pre>

<p>The error I get is TypeError: an integer is required.  It seems to be ok in regular Python but Canopy has an issue.  It seems it is how Canopy treats the sum statement but I'm not sure how to work around it.  Any ideas as I'm just getting started with Python and text analysis.</p>
","3890455","","","","","2014-12-15 04:47:30","GENSIM Error in Canopy Express","<python-2.7><canopy><gensim>","1","1","1","","","CC BY-SA 3.0"
"45459496","1","","","2017-08-02 11:23:27","","0","1468","<p>Based on this article: <a href=""http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"" rel=""nofollow noreferrer"">http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/</a> I am trying to implement a gensim word2vec model with the pretrained vectors of GloVe in a text classification task. However, I would like to do FeatureSelection also in my text data. I tried multiple sequences in the pipeline but i get fast a memory error which points to the transform part of TfidfEmbeddingVectorizer. </p>

<pre><code>   return np.array([
                np.mean([self.word2vec[w] * self.word2weight[w]
                         for w in words if w in self.word2vec] or
                        [np.zeros(self.dim)], axis=0)
                for words in X
</code></pre>

<p>If I replace the TfidfEmbeddingVectorizer class with a regular TfIdfVectorizer it works properly. Is there a way I could combine SelectFromModel and W2vec in the pipeline?</p>

<pre><code>from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np
import itertools
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import precision_recall_fscore_support as score, f1_score
import pickle
from sklearn.externals import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.pipeline import FeatureUnion
from sklearn.feature_extraction import DictVectorizer
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.svm import LinearSVC
import gensim
import collections

class ItemSelector(BaseEstimator, TransformerMixin):

    def __init__(self, column):
        self.column = column

    def fit(self, X, y=None, **fit_params):
        return self

    def transform(self, X):
        return (X[self.column])




class TextStats(BaseEstimator, TransformerMixin):
    """"""Extract features from each document for DictVectorizer""""""

    def fit(self, x, y=None):
        return self

    def transform(self, posts):
        return [{'REPORT_M': text}
                for text in posts]


class TfidfEmbeddingVectorizer(object):
  def __init__(self, word2vec):
    self.word2vec = word2vec
    self.word2weight = None
    self.dim = len(word2vec.values())

  def fit(self, X, y):
    tfidf = TfidfVectorizer(analyzer=lambda x: x)
    tfidf.fit(X)
    # if a word was never seen - it must be at least as infrequent
    # as any of the known words - so the default idf is the max of 
    # known idf's
    max_idf = max(tfidf.idf_)
    self.word2weight = collections.defaultdict(
        lambda: max_idf,
        [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

    return self

  def transform(self, X):
    return np.array([
            np.mean([self.word2vec[w] * self.word2weight[w]
                     for w in words if w in self.word2vec] or
                    [np.zeros(self.dim)], axis=0)
            for words in X
        ])


# training model
 def train(data_train, data_val):

    with open(""glove.6B/glove.6B.50d.txt"", ""rb"") as lines:
        w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))
               for line in lines}
    classifier = Pipeline([
                    ('union', FeatureUnion([

                            ('text', Pipeline([
                                ('selector', ItemSelector(column='TEXT')),
                                (""word2vec vectorizer"", TfidfEmbeddingVectorizer(w2v)),
                                ('feature_selection', SelectFromModel(LinearSVC(penalty=""l1"", dual=False),threshold=0.01))
                            ])),

                            ('category', Pipeline([
                                ('selector', ItemSelector(column='category')),
                                ('stats', TextStats()),
                                ('vect', DictVectorizer())
                            ])) 
    ])),
                    ('clf',ExtraTreesClassifier(n_estimators=200, max_depth=500, min_samples_split=6, class_weight= 'balanced'))])

    classifier.fit(data_train,data_train.CLASSES)
    predicted = classifier.predict(data_val)
</code></pre>
","3923918","","","","","2017-11-15 07:20:33","Combining w2vec and feature selection in pipeline","<python-3.x><gensim><text-classification><feature-selection>","1","0","","","","CC BY-SA 3.0"
"53885591","1","","","2018-12-21 13:28:12","","2","1501","<p>How to convert pretrained fastText vectors to gensim model?
I need predict_output_word method.</p>

<p>import gensim
from gensim.models import Word2Vec 
from gensim.models.wrappers import FastText</p>

<p>model_wiki = gensim.models.KeyedVectors.load_word2vec_format(""wiki.ru.vec"")
model3 = Word2Vec(sentences=model_wiki)</p>

<blockquote>
  <p>TypeError                                 Traceback (most recent call
  last)  in 
  ----> 1 model3 = Word2Vec(sentences=model_wiki)  # train a model from the corpus</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/word2vec.py
  in <strong>init</strong>(self, sentences, corpus_file, size, alpha, window,
  min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs,
  negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule,
  sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)
      765             callbacks=callbacks, batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window,
      766             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,
  --> 767             fast_version=FAST_VERSION)
      768 
      769     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/base_any2vec.py
  in <strong>init</strong>(self, sentences, corpus_file, workers, vector_size,
  epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed,
  hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss,
  fast_version, **kwargs)
      757                 raise TypeError(""You can't pass a generator as the sentences argument. Try an iterator."")
      758 
  --> 759             self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)
      760             self.train(
      761                 sentences=sentences, corpus_file=corpus_file, total_examples=self.corpus_count,</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/base_any2vec.py
  in build_vocab(self, sentences, corpus_file, update, progress_per,
  keep_raw_vocab, trim_rule, **kwargs)
      934         """"""
      935         total_words, corpus_count = self.vocabulary.scan_vocab(
  --> 936             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)
      937         self.corpus_count = corpus_count
      938         self.corpus_total_words = total_words</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/word2vec.py
  in scan_vocab(self, sentences, corpus_file, progress_per, workers,
  trim_rule)    1569             sentences = LineSentence(corpus_file)<br>
  1570 
  -> 1571         total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)    1572     1573 
  logger.info(</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/word2vec.py
  in _scan_vocab(self, sentences, progress_per, trim_rule)    1538<br>
  vocab = defaultdict(int)    1539         checked_string_types = 0
  -> 1540         for sentence_no, sentence in enumerate(sentences):    1541             if not checked_string_types:    1542<br>
  if isinstance(sentence, string_types):</p>
  
  <p>~/anaconda3/envs/pym/lib/python3.6/site-packages/gensim/models/keyedvectors.py
  in <strong>getitem</strong>(self, entities)
      337             return self.get_vector(entities)
      338 
  --> 339         return vstack([self.get_vector(entity) for entity in entities])
      340 
      341     def <strong>contains</strong>(self, entity):</p>
  
  <p>TypeError: 'int' object is not iterable</p>
</blockquote>
","7995764","","7995764","","2018-12-21 13:35:41","2018-12-21 14:16:01","How to convert pretrained fastText vectors to gensim model","<python><nlp><gensim><word2vec>","1","0","2","","","CC BY-SA 4.0"
"45467699","1","45473360","","2017-08-02 17:46:00","","0","251","<p>I am using gensim <strong>Doc2Vec</strong> model to generate my feature vectors. Here is the code I am using (I have explained what my problem is in the code):</p>

<pre><code>cores = multiprocessing.cpu_count()

# creating a list of tagged documents
training_docs = []

# all_docs: a list of 53 strings which are my documents and are very long (not just a couple of sentences)
for index, doc in enumerate(all_docs):
    # 'doc' is in unicode format and I have already preprocessed it
    training_docs.append(TaggedDocument(doc.split(), str(index+1)))

# at this point, I have 53 strings in my 'training_docs' list 

model = Doc2Vec(training_docs, size=400, window=8, min_count=1, workers=cores)

# now that I print the vectors, I only have 10 vectors while I should have 53 vectors for the 53 documents that I have in my training_docs list.
print(len(model.docvecs))
# output: 10
</code></pre>

<p>I am just wondering if I am doing a mistake or if there is any other parameter that I should set?</p>

<blockquote>
  <p><strong>UPDATE</strong>: I was playing with the <strong><em>tags</em></strong> parameter in <strong><em>TaggedDocument</em></strong>, and when I changed it to a mixture of text and numbers like: <em>Doc1, Doc2, ...</em> I see a different number for the count of generated vectors, but still I do not have the same number of feature vectors as expected.</p>
</blockquote>
","2347063","","2347063","","2017-08-02 22:06:59","2017-08-03 01:29:10","Gensim Doc2Vec model only generates a limited number of vectors","<python><nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"45502464","1","45552078","","2017-08-04 08:59:17","","4","1862","<p>I am using WMD to calculate the similarity scale between sentences. For example:</p>

<pre><code>distance = model.wmdistance(sentence_obama, sentence_president)
</code></pre>

<p>Reference: <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>

<p>However, there is also WMD based similarity method <code>(WmdSimilarity).</code></p>

<p>Reference: 
<a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>

<p>What is the difference between the two except the obvious that one is distance and another similarity? </p>

<p><strong>Update:</strong> Both are exactly the same except with their different representation. </p>

<pre><code>n_queries = len(query)
result = []
for qidx in range(n_queries):
    # Compute similarity for each query.
    qresult = [self.w2v_model.wmdistance(document, query[qidx]) for document in self.corpus]
    qresult = numpy.array(qresult)
    qresult = 1./(1.+qresult)  # Similarity is the negative of the distance.

    # Append single query result to list of all results.
    result.append(qresult)
</code></pre>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py</a></p>
","1996842","","1996842","","2017-08-07 10:03:44","2017-08-07 16:59:10","What is the difference between wmd (word mover distance) and wmd based similarity?","<nlp><nltk><gensim><word2vec><word-embedding>","1","3","","","","CC BY-SA 3.0"
"36790867","1","","","2016-04-22 10:08:03","","2","721","<p>In the gensim's documentation <code>window</code> size is defined as,</p>

<blockquote>
  <p><em>window</em> is the maximum distance between the current and predicted word within a sentence.</p>
</blockquote>

<p>which should mean when looking at context it doesn't go beyond the sentence boundary. right?</p>

<p>What i did was i created a document with several thousand tweets and selected a word (<code>q1</code>) and then selected most similar words to <code>q1</code> (using <code>model.most_similar('q1')</code>). But then, if I randomly shuffle the tweets in the input document and then did the same experiment (without changing word2vec parameters) I got a different set most_similar words to <code>q1</code>.</p>

<p>Can't really understand why that happens if only it's gonna look at is sentence level information? can anyone explain this?</p>

<p><b> EDIT: added model parameters and a graph </b> </p>

<p>used model parameters:</p>

<pre><code>model1 = word2vec.Word2Vec(sents1 , size=100, window=5, min_count=5, iter=n_iter, sg=0)
</code></pre>

<p><strong>Graph</strong>: 
To draw the graph what i did was I ran word2vec with above parameters for the original document (D) and the shuffled document (D') and took the top 10 or 20 (two bars) <code>most_similar('q')</code> words to a specific query word <code>q</code>, and calculated the jaccard similarity score between the two sets of words when iter=1,10,100. </p>

<p>It seems as the no of iterations increase, lesser and lesser similar words between the two sets of words got from running word2vec on D and D'.</p>

<p>can't really understand why this is happening or what's going on?</p>

<p><a href=""https://i.stack.imgur.com/MOl6R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MOl6R.png"" alt=""enter image description here""></a></p>
","601357","","601357","","2016-04-27 10:47:17","2016-04-27 10:47:17","Gensim Word2Vec changing the input sentence order?","<gensim><word2vec>","0","4","","","","CC BY-SA 3.0"
"53867257","1","","","2018-12-20 10:53:37","","1","193","<p>I have a word2vec model and I want to change it by adding some additional data beside the occurrence of the word itself.</p>

<p>For example:</p>

<p>Category (out of predefined 50), POS etc.</p>

<p>I thought of two ways to do it:</p>

<ol>
<li>Just concat the metadata to the word. (so that the word ""desk"" will be coded as ""desk-furniture-Noun""</li>
<li>The better way in my opinion: Create a new loss function that will be a function of the co-occurrences of the word itself, the co-occurrences of the category, the co-occurrences POS, etc.</li>
</ol>

<p>So my questions are:
1. What will be a better way?
2. How can I create a new loss function and optimize it in Word2Vec? Can I just pass a parameter to Gensim's Word2Vec or do I need to build a new Word2vec model from scratch?</p>
","10117402","","","","","2018-12-20 10:53:37","Adding metadata to words in word2vec","<nlp><gensim><word2vec><word-embedding>","0","0","","","","CC BY-SA 4.0"
"54277363","1","54301698","","2019-01-20 14:18:14","","0","422","<p>I've trained a word2vec model not for English but for an Asian language 'Sinhala'. in the later phase, I'm going to use this trained model to get the sentence similarities in order to detect plagiarism in Sinhala documents.
Please explain to me how to measure the accuracy of the trained model.I'm a university student. I have no previous knowledge of these things.</p>
","9048829","","","","","2019-01-22 05:20:40","How to measure the accuracy of Word2vec model Trained on another language?","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"65368524","1","","","2020-12-19 09:48:38","","0","668","<p>I want to import the &quot;genism&quot; library. I have previously successfully installed it by typing the following in the command prompt:</p>
<pre><code>pip install gensim
</code></pre>
<p>However, the following error appears when importing it:</p>
<pre><code>Traceback (most recent call last):
 File &quot;C:/Users/PycharmProjects/untitled/file.py&quot;, line 3, in &lt;module&gt;
  import gensim
 File &quot;C:\Users\AppData\Local\Programs\Python\Python35\lib\site-packages\gensim\__init__.py&quot;, line 5, in &lt;module&gt;
  from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
 File &quot;C:\Users\AppData\Local\Programs\Python\Python35\lib\site-packages\gensim\parsing\__init__.py&quot;, line 4, in &lt;module&gt;
  from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
 File &quot;C:\Users\AppData\Local\Programs\Python\Python35\lib\site-packages\gensim\parsing\preprocessing.py&quot;, line 42, in &lt;module&gt;
  from gensim import utils
 File &quot;C:\Users\AppData\Local\Programs\Python\Python35\lib\site-packages\gensim\utils.py&quot;, line 40, in &lt;module&gt;
  import scipy.sparse
 File &quot;C:\Users\AppData\Local\Programs\Python\Python35\lib\site-packages\scipy\__init__.py&quot;, line 156, in &lt;module&gt;
  from . import fft
 File &quot;C:\Users\AppData\Local\Programs\Python\Python35\lib\site-packages\scipy\fft\__init__.py&quot;, line 76, in &lt;module&gt;
  from ._basic import (
 File &quot;C:\Users\AppData\Local\Programs\Python\Python35\lib\site-packages\scipy\fft\_basic.py&quot;, line 1, in &lt;module&gt;
  from scipy._lib.uarray import generate_multimethod, Dispatchable
 File &quot;C:\Users\AppData\Local\Programs\Python\Python35\lib\site-packages\scipy\_lib\uarray.py&quot;, line 27, in &lt;module&gt;
  from ._uarray import *
 File &quot;C:\Users\AppData\Local\Programs\Python\Python35\lib\site-packages\scipy\_lib\_uarray\__init__.py&quot;, line 114, in &lt;module&gt;
  from ._backend import *
 File &quot;C:\Users\AppData\Local\Programs\Python\Python35\lib\site-packages\scipy\_lib\_uarray\_backend.py&quot;, line 1, in &lt;module&gt;
  from typing import (
ImportError: cannot import name 'Type'
</code></pre>
<p>I have also previously successfully installed the &quot;typing&quot; library&quot;:</p>
<pre><code>pip install typing
</code></pre>
<p>What should I do?</p>
","14251114","","","","","2020-12-19 10:19:29","ImportError: cannot import name 'Type' when importing 'gensim""","<python><pycharm><gensim>","1","3","","","","CC BY-SA 4.0"
"19615951","1","19761907","","2013-10-27 08:11:23","","1","5678","<p>I've been experimenting with LDA topic modelling using <a href=""http://radimrehurek.com/gensim/"" rel=""nofollow"">Gensim</a>. I couldn't seem to find any topic model evaluation facility in Gensim, which could report on the perplexity of a topic model on held-out evaluation texts thus facilitates subsequent fine tuning of LDA parameters (e.g. number of topics). It would be greatly appreciated if anyone could shed some light on how I can perform topic model evaluation in Gensim. This question has also been posted on <a href=""http://metaoptimize.com/qa/questions/14332/topic-models-evaluation-in-gensim"" rel=""nofollow"">metaoptimize</a>.</p>
","735445","","735445","","2013-10-27 09:07:27","2013-11-04 05:03:29","Topic models evaluation in Gensim","<lda><gensim>","1","0","1","","","CC BY-SA 3.0"
"54492390","1","54500667","","2019-02-02 11:00:22","","2","191","<p>I cannot find anything about the default values about the parameters for gensim fasttext <a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText"" rel=""nofollow noreferrer"">here</a></p>

<p>Or are they the same as for the original Facebook fasttext implementation?</p>
","","user9937436","10989396","","2019-02-02 13:24:38","2019-02-03 06:58:06","What are the defaults for gensim's fasttext?","<gensim><fasttext>","1","0","1","","","CC BY-SA 4.0"
"45847370","1","45848369","","2017-08-23 18:52:07","","1","674","<pre><code>from gensim.parsing import PorterStemmer
from gensim.models import Word2Vec, Phrases

class SentenceClass(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            with open(os.path.join(self.dirname,fname), 'r') as myfile:
                doc = myfile.read().replace('\n', ' ')
                for sent in tokenize.sent_tokenize(doc.lower()):
                    yield [Stemming.stem(word)\
                    for word in word_tokenize(re.sub(""[^A-Za-z]"", "" "",sent))\
                    if word not in stopwords]
</code></pre>

<p>Now of the two approaches:<br/>
1)</p>

<pre><code>model = Word2Vec(SentenceClass(data_dir_path), size=100, window=5, min_count=1, workers=4)
</code></pre>

<p>The above one runs really fine with no warning</p>

<p>2)</p>

<pre><code>bigram_transformer = Phrases(SentenceClass(data_dir_path), min_count=1)
model = Word2Vec(bigram_transformer[SentenceClass(data_dir_path)], size=100, window=5, min_count=1, workers=4)
</code></pre>

<p>produces the warning:</p>

<pre><code>WARNING:gensim.models.word2vec:train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).
WARNING:gensim.models.word2vec:supplied example count (0) did not equal expected count (30)
</code></pre>

<p>Now I do understand the difference between generator and iterators, and I am passing iterators, which is verified by printing below commands multiple times:</p>

<pre><code>print(list(SentenceClass(data_dir_path)))
print(list(SentenceClass(data_dir_path)))
print(list(bigram_transformer[SentenceClass(data_dir_path)]))
print(list(bigram_transformer[SentenceClass(data_dir_path)]))
</code></pre>

<p>And it prints thing fine, but I am still not sure why the warning of ""empty iterator"" for the second case, am I missing something here ? </p>
","3670532","","3670532","","2017-08-23 19:22:05","2017-08-23 20:00:37","Issue with gensim.models.Phrases","<python><nlp><gensim>","1","0","0","","","CC BY-SA 3.0"
"15036048","1","15403585","","2013-02-23 01:40:55","","2","2075","<p>Why did the tf-idf model in <code>gensim</code> throws away the terms and counts after i transform the corpus?</p>

<p>My code:</p>

<pre><code>from gensim import corpora, models, similarities

# Let's say you have a corpus made up of 2 documents.
doc0 = [(0, 1), (1, 1)]
doc1 = [(0,1)]
doc2 = [(0, 1), (1, 1)]
doc3 = [(0, 3), (1, 1)]

corpus = [doc0,doc1,doc2,doc3]

# Train a tfidf model using the corpus
tfidf = models.TfidfModel(corpus)

# Now if you print the corpus, it still remains as the flat frequency counts.
for d in corpus:
  print d
print 

# To convert the corpus into tfidf, re-initialize the corpus 
# according to the model to get the normalized frequencies.
corpus = tfidf[corpus]

for d in corpus:
  print d
</code></pre>

<p>Outputs:</p>

<pre><code>[(0, 1.0), (1, 1.0)]
[(0, 1.0)]
[(0, 1.0), (1, 1.0)]
[(0, 3.0), (1, 1.0)]

[(1, 1.0)]
[]
[(1, 1.0)]
[(1, 1.0)]
</code></pre>
","610569","","","","","2013-03-14 07:35:48","Why did the tf-idf model in `gensim` throws away the terms and counts after i transform the corpus?","<python><nlp><information-retrieval><tf-idf><gensim>","1","0","1","","","CC BY-SA 3.0"
"65435027","1","","","2020-12-24 06:57:54","","3","569","<p>I have 3 columns Smiles, Column, and mobile phase.</p>
<p>I want to convert my dataset to the corpus as a training dataset and apply the Gensim model.</p>
<p>Here is my dataset link</p>
<p><a href=""https://drive.google.com/file/d/1S80I_5zkjJfeTzby7OjIqrs1vMJI6jVo/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1S80I_5zkjJfeTzby7OjIqrs1vMJI6jVo/view?usp=sharing</a></p>
<p>I have referred this StackOverflow question but can't work</p>
<p><a href=""https://stackoverflow.com/questions/49088978/how-to-create-corpus-from-pandas-data-frame-to-operate-with-nltk/49104725"">How to create corpus from pandas data frame to operate with NLTK</a></p>
","12174604","","355230","","2020-12-24 08:40:12","2020-12-30 05:34:48","Is there any way to convert python pandas dataframe to NLP corpus or documentation?","<python><pandas><dataframe><gensim><corpus>","1","4","","","","CC BY-SA 4.0"
"65377859","1","","","2020-12-20 07:36:51","","0","34","<p>I want to train a Fasttext model in the following way:</p>
<pre><code>corpus_file = datapath('file.cor')
model = FastText(size=embedding_size,
                  window=window_size,
                  min_count=min_word,
                  sample=down_sampling,
                  sg=1,
                  iter=100)
model.build_vocab(corpus_file=corpus_file)
total_words = model.corpus_total_words
model.train(corpus_file=corpus_file, total_words=total_words, epochs=5)
</code></pre>
<p>Nevertheless, it won't show in which iteration or epoch it is currently training. I want to view the training steps in order to understand how my program is being processed and how far it has gone. What should I do?</p>
","14251114","","","","","2020-12-20 07:36:51","Viewing the training process of a Fasttext model","<python><gensim><fasttext>","0","1","","","","CC BY-SA 4.0"
"65884395","1","65887367","","2021-01-25 12:03:24","","1","111","<p>I would like to see how to access dictionary from gensim lda topic model. This is particularly important when you train lda model, save and load it later on. In the other words, suppose lda_model is the model trained on a collection of documents. To get document-topic matrix one can do something like below or something like the one explained in <a href=""https://www.kdnuggets.com/2019/09/overview-topics-extraction-python-latent-dirichlet-allocation.html"" rel=""nofollow noreferrer"">https://www.kdnuggets.com/2019/09/overview-topics-extraction-python-latent-dirichlet-allocation.html</a>:</p>
<pre><code>def regTokenize(text):
    # tokenize the text into words
    import re
    WORD = re.compile(r'\w+')
    words = WORD.findall(text)
    return words

from gensim.corpora.dictionary import Dictionary
ttext = [regTokenize(d) for d in text]  
dic = Dictionary(ttext)
ttext = [dic.doc2bow(text) for text in ttext]
ttext = lda_model.get_document_topics(ttext)
</code></pre>
<p>However, dictionary in trained <code>lda_model</code> might be different from new data and gives error for the last line, like:</p>
<pre><code>&quot;IndexError: index 41021 is out of bounds for axis 1 with size 41021&quot;
</code></pre>
<p>Is there any way (or parameter) to obtain dictionary from trained <code>lda_model</code>, to use it instead of <code>dic = Dictionary(ttext)</code>? Your help and answer much appreciated!</p>
","6395930","","6573902","","2021-01-25 15:06:30","2021-01-25 15:09:05","Access dictionary in Python gensim topic model","<python><dictionary><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"30663755","1","","","2015-06-05 10:02:59","","2","597","<p>I am new to stackoverflow. Please forgive my bad English.</p>

<p>I am using <strong>word2vec</strong> for a school project. I want to work with a domain specific corpus (like Physics Textbook) for creating the word vectors using <strong>Word2Vec</strong>. This standalone does not provide good results due to lesser size of the corpus. This especially hurts as we want to evaluate on words that may very well be outside the vocabulary of the text book.</p>

<p>We want the textbook to encode the domain specific relationships and semantic ""nearness"". ""Quantum"" and ""Heisenberg"" are especially close in this textbook for eg. which may not hold true for background corpus. To handle the generic words (like ""any"") we need the basic background model(like the one provided by Google on word2vec site).  </p>

<p>Is there any way that we can supplant to the background model using our newer corpus. Just training on the corpus etc. doesnot work well.</p>

<p>Are there any attempts to combine vector representations from two corpus- general and specific. I could not find any in my searches. </p>
","2263336","","3876141","","2015-06-05 10:54:52","2015-06-05 14:53:48","Biasing word2vec towards special corpus","<nlp><gensim><word2vec>","1","1","","","","CC BY-SA 3.0"
"62970416","1","","","2020-07-18 15:35:12","","-1","336","<p>I am working on a project which groups jobs posted on various job portals into clusters based on the description of the jobs using K-means.</p>
<p>I found the work vector using Word2Vec, but i guess this will not serve the purpose as I will need a vector of the whole job description.</p>
<p>I know that I can average out the word vector of a sentence to get the sentence vector but worried about the accuracy as this will loose the ordering of the words.</p>
<p>Is there any other way I can get the vectors ?</p>
","3754966","","","","","2020-07-20 00:57:27","Get sentence vector for a K-means clustering task","<machine-learning><nlp><vectorization><gensim><word2vec>","3","0","","","","CC BY-SA 4.0"
"63200943","1","","","2020-08-01 01:34:23","","-1","60","<p>I am trying to reproduce this code: <a href=""https://github.com/n0obcoder/skip-gram-model"" rel=""nofollow noreferrer"">https://github.com/n0obcoder/skip-gram-model</a></p>
<p>However I am getting this error:</p>
<pre><code>ModuleNotFoundError: No module named 'utils_modified'
</code></pre>
<p>could you please tell me what it is wrong there?</p>
","","user12809368","","","","2020-08-01 01:51:02","Word2vec from scratch: No module named 'utils_modified'","<python><gensim><word2vec><torch>","1","1","","","","CC BY-SA 4.0"
"53930617","1","","","2018-12-26 10:16:48","","2","74","<p>I have list of co-occurences and I want to train word2vec model with my own customized loss_function. </p>

<p>What is the best way to approach this?</p>

<ol>
<li>Is it possible to set gensim Word2Vec model with my own function? </li>
<li>If not, is there an example to an implementation with keras?</li>
<li>If not, must I define everything totally from scratch?</li>
</ol>

<p>Thanks!</p>
","10117402","","","","","2018-12-26 10:16:48","Customizg loss function in Word2vec","<keras><nlp><gensim><word2vec><word-embedding>","0","0","","","","CC BY-SA 4.0"
"65369269","1","65377810","","2020-12-19 11:22:01","","0","167","<p>I want to train a Fasttext model in Python using the &quot;gensim&quot; library. First, I should tokenize each sentences to its words, hence converting each sentence to a list of words. Then, this list should be appended to a final list. Therefore, at the end, I will have a  nested list containing all tokenized sentences:</p>
<pre><code>word_punctuation_tokenizer = nltk.WordPunctTokenizer()
word_tokenized_corpus = []
for line in open('sentences.txt'):
   new = line.strip()
   new = word_punctuation_tokenizer.tokenize(new)
   if len(new) != 0:
       word_tokenized_corpus.append(new)
</code></pre>
<p>Then, the model should be built as the following:</p>
<pre><code>embedding_size = 60
window_size = 40
min_word = 5
down_sampling = 1e-2
ft_model = FastText(word_tokenized_corpus,
                  size=embedding_size,
                  window=window_size,
                  min_count=min_word,
                  sample=down_sampling,
                  sg=1,
                  iter=100)
</code></pre>
<p>However, the number of sentences in &quot;word_tokenized_corpus&quot; is very large and the program can't handle it. Is it possible that I train the model by giving each tokenized sentence to it one by one, such as the following:?</p>
<pre><code> for line in open('sentences.txt'):
  new = line.strip()
  new = word_punctuation_tokenizer.tokenize(new)
  if len(new) != 0:
   ft_model = FastText(new,
              size=embedding_size,
              window=window_size,
              min_count=min_word,
              sample=down_sampling,
              sg=1,
              iter=100)
</code></pre>
<p>Does this make any difference to the final results? Is it possible to train the model without having to build such a large list and keeping it in the memory?</p>
","14251114","","","","","2021-02-07 16:10:22","training a Fasttext model","<python><gensim><fasttext>","2","0","","","","CC BY-SA 4.0"
"65394022","1","","","2020-12-21 13:54:30","","2","365","<p>I'm loading the model using:</p>
<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) 
</code></pre>
<p>Now every time i run the file in Pycharm, it loads the model again.</p>
<p>So, is there a way to load it once and be available whenever i run things like  <code>model['king']</code>  and <code>model.doesnt_match(&quot;house garage store dog&quot;.split())</code></p>
<p>because it takes alot of time whenever i wana check the similarity or words that don't match.
When i ran <code>model.most_similar('finance')</code> it was really slow and the whole laptop freezed for like 2 min. So, is there a way to make things faster, 'cause i wana use it in my project, but i can't let the user wait for this long.</p>
<p>Any suggestions?</p>
","14685734","","4685471","","2020-12-23 01:07:36","2020-12-23 01:07:36","How can a Word2Vec pretrained model be loaded in Gensim faster?","<python><machine-learning><nlp><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"27470670","1","","","2014-12-14 15:13:43","","44","39313","<p>I recently came across the doc2vec addition to Gensim. How can I use pre-trained word vectors (e.g. found in word2vec original website) with doc2vec?</p>

<p>Or is doc2vec getting the word vectors from the same sentences it uses for paragraph-vector training?</p>

<p>Thanks.</p>
","1735729","","2583490","","2020-04-05 17:56:23","2020-04-05 17:56:23","How to use Gensim doc2vec with pre-trained word vectors?","<python><nlp><gensim><word2vec><doc2vec>","4","0","28","","","CC BY-SA 3.0"
"36673316","1","","","2016-04-17 06:18:13","","0","746","<p>I have been generating topics with <a href=""https://www.yelp.com/dataset_challenge"" rel=""nofollow noreferrer"">yelp data</a> set of customer reviews by using Latent Dirichlet allocation(LDA) in python(gensim package). While generating tokens, I am selecting only the words having length >= 3 from the reviews( By using <code>RegexpTokenizer</code>):</p>

<pre><code>from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w{3,}')
tokens = tokenizer.tokenize(review)
</code></pre>

<p>This will allow us to filter out the noisy words of length less than 3, while creating the corpus document. <br/></p>

<p>How will filtering out these words effect performance with the LDA algorithm?</p>
","2301570","","721810","","2017-02-04 19:22:46","2019-04-12 23:37:18","Latent Dirichlet allocation(LDA) performance by limiting word size for Corpus Documents","<python><tokenize><lda><gensim><corpus>","2","0","","","","CC BY-SA 3.0"
"45758225","1","","","2017-08-18 13:45:36","","0","441","<p>I am using gensim LdaMulticore to extract topics.It works perfectly fine from Jupyter/Ipython notebook, but when I run from Command prompt, the loop runs indefinitely.
Once the execution arrives at the LdaMulticore function, the execution starts from first. 
Please help me as I am novice</p>

<pre><code>if __name__ == '__main__': 
    model = models.LdaMulticore(corpus=corpus_train, id2word=dictionary, num_topics=20, chunksize=4000, passes=30, alpha=0.5, eta=0.05, decay=0.5, eval_every=10, workers=3, minimum_probability=0)

**RESULTS:-**
Moving to Topics Extraction Script---------------------------------
2017-08-18 18:59:36,448 : INFO : using serial LDA version on this node
2017-08-18 18:59:37,183 : INFO : running online LDA training, 20 topics, 1 passes over the supplied corpus of 400 documents, updating every 12000 documents, evaluating every ~400 documents, iterating 50x with a convergence threshold of 0.001000    
2017-08-18 18:59:37,183 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
2017-08-18 18:59:37,183 : INFO : training LDA model using 3 processes
2017-08-18 18:59:37,214 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #400/400, outstanding queue size 1
Importing required Packages
</code></pre>

<p>Importing required Packages <a href=""https://i.stack.imgur.com/aJaSc.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aJaSc.jpg"" alt=""enter image description here""></a></p>
","8003392","","8060120","","2017-08-18 13:59:12","2018-08-24 08:04:50","gensim LdaMulticore is not running from Command Prompt","<python><nlp><multicore><gensim><lda>","1","1","","","","CC BY-SA 3.0"
"65406526","1","65412771","","2020-12-22 09:52:23","","0","101","<p>I'm trying to load wiki.ar.vec arabic word embedding file using word2vec function from gensim.</p>
<p>Below is the code use to load embedding file.</p>
<pre><code>import gensim.models.keyedvectors as word2vec 
print( &quot;Word Embedding is loading&quot;)
embedding = word2vec.KeyedVectors.load_word2vec_format('/home/user/Documents/wiki.ar.vec', binary=False)
print( &quot;Word Embedding is loaded&quot;)
</code></pre>
<p>Facing the Error describe in below screenshot:</p>
<p><a href=""https://i.stack.imgur.com/wZbVP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wZbVP.jpg"" alt=""enter image description here"" /></a></p>
<p>or any other way to load wiki.ar.vec embedding file?</p>
<p>Any suggestion and answers are highly appriciated.</p>
","13077856","","12305715","","2020-12-22 11:23:16","2020-12-22 16:59:39","ValueError: invalid vector on line 440902 | while loading wiki.ar.vec using gensim.models.keyedvectors.word2vec() function","<python><arabic><gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"65397810","1","","","2020-12-21 18:04:27","","0","284","<p>The official Python implementation offers the useful <code>get_sentence_vector()</code> which sums, normalizes (L2) and averages the words in the given sentence.</p>
<p>In other words, I'd like to embed a new sentence, given a trained model of Gensim's FastText.</p>
<p>Is there such method?</p>
","3880275","","","","","2020-12-22 09:27:26","What's the equivalent to get_sentence_vector for Gensim's FastText?","<machine-learning><nlp><gensim><word-embedding><fasttext>","1","1","","","","CC BY-SA 4.0"
"45234310","1","","","2017-07-21 09:40:29","","8","4036","<p>I'm training a <code>Word2Vec</code> model like:</p>

<pre><code>model = Word2Vec(documents, size=200, window=5, min_count=0, workers=4, iter=5, sg=1)
</code></pre>

<p>and <code>Doc2Vec</code> model like:</p>

<pre><code>doc2vec_model = Doc2Vec(size=200, window=5, min_count=0, iter=5, workers=4, dm=1)
doc2vec_model.build_vocab(doc2vec_tagged_documents)
doc2vec_model.train(doc2vec_tagged_documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.iter)
</code></pre>

<p>with the <strong>same data</strong> and comparable parameters.</p>

<p>After this I'm using these models for my classification task. And I have found out that simply averaging or summing the <code>word2vec</code> embeddings of a document performs considerably better than using the <code>doc2vec</code> vectors.  I also tried with much more <code>doc2vec</code> iterations (25, 80 and 150 - makes no difference).</p>

<p>Any tips or ideas why and how to improve <code>doc2vec</code> results?</p>

<p><strong>Update</strong>: This is how <code>doc2vec_tagged_documents</code> is created:</p>

<pre><code>doc2vec_tagged_documents = list()
counter = 0
for document in documents:
    doc2vec_tagged_documents.append(TaggedDocument(document, [counter]))
    counter += 1
</code></pre>

<p><strong>Some more facts about my data:</strong></p>

<ul>
<li>My training data contains 4000 documents</li>
<li>with 900 words on average.</li>
<li>My vocabulary size is about 1000 words.</li>
<li>My data for the classification task is much smaller on average (12 words on average), but I also tried to split the training data to lines and train the <code>doc2vec</code> model like this, but it's almost the same result.</li>
<li>My data is <strong>not</strong> about natural language, please keep this in mind.</li>
</ul>
","2612484","","2612484","","2017-07-22 13:14:19","2017-07-22 13:14:19","Doc2Vec Worse Than Mean or Sum of Word2Vec Vectors","<python><machine-learning><gensim><word2vec><doc2vec>","1","0","7","","","CC BY-SA 3.0"
"57179502","1","","","2019-07-24 09:23:30","","2","113","<p>All in all I need to run multiple word2vec over a period of time. For example I will be running word2vec once every month. To reduce computing workload I would like to run word2vec only on the data that was accumulated during the last month. My problem stems from the fact that for further processing I require the embeddings from the models I ran in previous months.</p>

<p>I know, also from reading other posts, that if the individual word2vec models are run on different samples which each are not a representative sample of an overarching corpus, obtaining word embeddings that are comparable is not possible. I have a similar problem, where I am analysing network data, which evolves over time (effectively doing a kind of graph2vec, but analysing node behaviour).</p>

<p>Yet I've been wondering if comparable embeddings can be achieved using PCA as follows:</p>

<ul>
<li>all models create ""node"" embeddings of length x</li>
<li>for each model: 

<ul>
<li>run PCA on the ""node"" embeddings and retain all x principal        components, whereby whitening is enabled</li>
<li>transform the individual ""node"" embeddings to their corresponding PCA coordinates</li>
</ul></li>
<li>since the individual samples used to train the individual models share a high proportion of nodes as existing ones tend to stay and
new ones are likely to be added do the following:

<ul>
<li>append all pca-transformed embeddings into one database</li>
<li>by nodeID calculate mean pca_transformed embedding</li>
</ul></li>
</ul>

<p>This would only work, if the PCA transformation of the embeddings of each model ensures that the resulting embeddings measure the ""same thing"". For example, the first principal component of each PCA should capture the same kind of information, etc. And that's what I'm not sure about.</p>
","11796763","","11796763","","2019-07-24 09:30:29","2019-07-24 19:14:06","Are Principal Components of different word2vec models measuring the same thing?","<math><gensim><word2vec><fasttext>","1","0","","","","CC BY-SA 4.0"
"36940334","1","","","2016-04-29 13:59:32","","2","717","<p>I have installed the 2 python libraries:</p>

<ol>
<li><p>NumPy - 1.10.4</p></li>
<li><p>Scipy - 0.17.0</p></li>
</ol>

<p>which are required for the successful installation of gensim as stated in: <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow"">https://radimrehurek.com/gensim/install.html</a>. I have used the wheel file from <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/"" rel=""nofollow"">http://www.lfd.uci.edu/~gohlke/pythonlibs/</a> for installation on a windows 7 64 bit machine with python 3.4. I am running into certain compatibility issues.</p>

<pre><code>   C:\Users\888537&gt;py -m pip install gensim-0.12.4-cp34-none-win_amd64.whl         
   Processing c:\users\888537\gensim-0.12.4-cp34-none-win_amd64.whl  
   Collecting numpy&gt;=1.3 (from gensim==0.12.4)                                         
   Using cached numpy-1.11.0-cp34-none-win_amd64.whl                                
   Collecting smart-open&gt;=1.2.1 (from gensim==0.12.4)                                
   Using cached smart_open-1.3.2.tar.gz                                              
   Complete output from command python setup.py egg_info:                          
   D:\Program Files\Python\lib\distutils\dist.py:260: UserWarning: Unknown distribution option: 'install_requires'                                                   warnings.warn(msg)                                                            
   D:\Program Files\Python\lib\distutils\dist.py:260: UserWarning: Unknown distribution option: 'test_suite'                                                         
   warnings.warn(msg)                                                            
   usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]                    
   or: -c --help [cmd1 cmd2 ...]                                                    
   or: -c --help-commands                                                          
   or: -c cmd --help                                                                                                                                            
   error: invalid command 'egg_info'                                                                                                                               ---------------------------------------- 
</code></pre>

<p>Error:Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\888537\AppData\Local\Temp\pip-build-7zxq63k_\smart-open\ </p>

<p>The same occurs during a pip installation:</p>

<pre><code>Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\888537\A
ppData\Local\Temp\pip-build-4o3kecba\gensim\
</code></pre>

<p>Is there a way out of this other than installation from the git repo? I was unsuccessful trying to import from the git repo too. :P</p>

<pre><code>C:\Users\888537&gt;py -m pip install -e git+ssh://git@github.com/piskvorky/gensim.g
it
--editable=git+ssh://git@github.com/piskvorky/gensim.git is not the right format
; it must have #egg=Package
</code></pre>
","6260947","","6260947","","2016-04-29 14:05:08","2016-05-02 13:32:27","Installation problems with Gensim library Python 3.4 : http://www.lfd.uci.edu/~gohlke/pythonlibs/","<python><git><numpy><scipy><gensim>","2","2","","","","CC BY-SA 3.0"
"23032745","1","23134272","","2014-04-12 15:59:46","","7","1758","<p>I know that after training the lda model for gensim, we can get the topic for an unseen document by:</p>

<pre><code>lda = LdaModel(corpus, num_topics=10)
doc_lda = lda[doc_bow]
</code></pre>

<p>But how about the documents that are already used for training? I mean is there a way to get the topic for a document in corpus that was used in training without treating it like a new document?</p>
","3450064","","","","","2014-04-17 13:18:16","Gensim get topic for a document (seen document)","<python><lda><gensim>","1","0","1","","","CC BY-SA 3.0"
"45276029","1","45278804","","2017-07-24 08:47:11","","3","1554","<p>According to the <strong>gensim.models.Word2Vec</strong> <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">API reference</a>, ""compute_loss"" is a valid keyword. However, I get an error that says it's an <code>unexpected keyword</code>.</p>

<p><strong>UPDATE</strong>:</p>

<p>The Word2Vec class on GitHub <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">does have</a> the 'compute_loss' keyword, but my local library does not.
I see that the gensim documentation and library deviate from each other.
I found that the <code>win-64/gensim-2.2.0-np113py35_0.tar.bz2</code>-file in <a href=""https://anaconda.org/anaconda/gensim/files"" rel=""nofollow noreferrer"">conda repository</a> is not up to date.</p>

<p>However after uninstalling gensim with conda, <code>pip install gensim</code> did not change anything as it still doesn't work.</p>

<p>Apparently, the source on GitHub and the distributed library are different, but the tutorial seems to assume code is as on GitHub.</p>

<p><strong>/END OF UPDATE</strong></p>

<p>I followed and downloaded the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb"" rel=""nofollow noreferrer"">tutorial notebook on Word2Vec</a>.</p>

<p>In input [25], first cell after ""Training Loss Computation"" headline, I get an error in the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Word2Vec</a> class' initializer. </p>

<p>Input:</p>

<pre><code># instantiating and training the Word2Vec model
model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, 
compute_loss=True, hs=0, sg=1, seed=42)

# getting the training loss value
training_loss = model_with_loss.get_latest_training_loss()
print(training_loss)
</code></pre>

<p>Output:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-25-c2933abf4b08&gt; in &lt;module&gt;()
      1 # instantiating and training the Word2Vec model
----&gt; 2 model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42)
      3 
      4 # getting the training loss value
      5 training_loss = model_with_loss.get_latest_training_loss()

TypeError: __init__() got an unexpected keyword argument 'compute_loss'
</code></pre>

<p>I have gensim 2.2.0 installed via conda and have a new new clone from the gensim repository (with the tutorial notebook). I'm using 64-bit Python 3.5.3 on windows 10. (Anaconda)</p>

<p>I've tried to search for others with same encounter, but I haven't been successful. </p>

<p>Do you know the reason for this, and how to fix this? Apparently, the source on GitHub and the distributed library are different, but the tutorial seems to assume code is as on GitHub.</p>

<p>I've also previously <a href=""https://groups.google.com/forum/#!topic/gensim/J1J2zTZwD7Q"" rel=""nofollow noreferrer"">posted the question</a> in the official mailing list.</p>
","1144382","","1144382","","2017-07-25 10:44:31","2017-07-31 09:04:15","Why doesn't gensim's Word2Vec recognize 'compute_loss' keyword?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"53901258","1","","","2018-12-23 04:44:30","","0","345","<p>I am using gensim for topic modeling. After training the lda model I call get_document_topics on a new document to get the topic distribution. However, for some documents, the return value is an empty list. Here is my code. Any idea what could have gone wrong?</p>

<pre><code> topic_vector = [ x[1] for x in self.ldamodel.get_document_topics(new_doc_bow , minimum_probability=
0.0, per_word_topics=False)]
</code></pre>
","2393015","","","","","2018-12-26 23:08:24","get_document_topics return an empty list.","<python><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"54400712","1","54522790","","2019-01-28 11:10:47","","0","55","<p>I have found so many similar questions none of them answer my problem can Someone help me . I have two legal documents I need to find which are contextually same or have same meaning what should be my approach. I thought of use something with LSTM wherever I see i get people having one or two sentences to compare . I want to do it for lot of docs and find out which of them are similar cannot get my head around how to begin my task  </p>
","8660575","","","","","2019-02-04 19:10:42","How can I find semantically similar paragraph in two different text files (two documents)","<nlp><gensim><recurrent-neural-network>","1","0","","","","CC BY-SA 4.0"
"53929657","1","","","2018-12-26 08:53:22","","2","210","<p>Hi I‚Äôm using Gensim to find similarity between documents to do so I make TF-IDF of documents and calculate cosine similarity. when I have new document I can calculate similarity of this document with previous documents using index[tfidf[vec]] but in this way TF-IDF doesn‚Äôt update and new words does not consider in similarity calculation is there any solution to update TF-IDF quickly without recalculating whole matrix or what is the best solution for my problem?</p>
","10128058","","10128058","","2019-01-12 10:55:25","2020-08-08 20:50:54","Updating TF-IDF using Gensim","<python><gensim><similarity><tf-idf>","1","0","","","","CC BY-SA 4.0"
"30654526","1","","","2015-06-04 21:30:42","","0","114","<p>so I am relatively new working with gensim and LDA, started about two weeks ago and I am having trouble trusting these results. The following are the topics produced by using 11 1-paragraph documents. </p>

<p>topic #0 (0.500): 0.059*island + 0.059*world + 0.057*computers + 0.056*presidential + 0.053*post + 0.047*posts + 0.046*tijuana + 0.045*vice + 0.045*tweets + 0.045*president</p>

<p>2015-06-04 16:22:07,891 : INFO : topic #1 (0.500): 0.093*computers + 0.064*world + 0.060*posts + 0.053*eurozone + 0.052*months + 0.049*tijuana + 0.048*island + 0.046*raise + 0.044*rates + 0.042*year</p>

<p>These topics just don't quite seem right. In fact they seem almost non-sensical. How exactly should I read these results? Also, is it normal that the topic distributions are exactly the same for both topics? </p>
","4975564","","","","","2015-06-05 01:10:59","LDA generated topics","<python><machine-learning><lda><topic-modeling><gensim>","1","2","","","","CC BY-SA 3.0"
"45243316","1","","","2017-07-21 17:19:59","","2","470","<p>I did document similarity on my corpus using Doc2Vec and it outputting not that good of similarities. I was wondering if I could do a topic model from what Doc2Vec is giving me to increase the accuracy of my model in order to get better similarities? </p>
","8249328","","","","","2017-07-21 23:46:52","Can I create a topic model (such as LDA) from the output of doc2vec model?","<nlp><gensim><lda><topic-modeling><doc2vec>","1","0","","","","CC BY-SA 3.0"
"30718471","1","30719725","","2015-06-08 20:27:31","","3","3674","<p>How to save the output? I am using the following code:    </p>

<pre><code>%time lda1 = models.LdaModel(corpus1, num_topics=20, id2word=dictionary1, update_every=5, chunksize=10000, passes=100)
</code></pre>
","1250155","","754176","","2016-01-07 17:15:14","2020-11-29 02:27:20","How to save gensim LDA topics output to csv along with the scores?","<python-2.7><gensim>","1","0","3","","","CC BY-SA 3.0"
"45280020","1","","","2017-07-24 11:56:23","","0","1101","<p>I have generated a word2vec model using gensim for  a huge corpus and I need to cluster the vocabularies using k means clustering for that i need:</p>

<ol>
<li>cosine distance matrix (word to word, so the size of the matrix the number_of_words x number_of_words )</li>
<li>features matrix (word to features, so the size of the matrix is the number_of_words x number_of_features(200) )</li>
</ol>

<p>for the feature matrix i tried to use x=model.wv and I got the object type as gensim.models.keyedvectors.KeyedVectors and its much smaller than what I expected a feature matrix will be </p>

<p>is there a way to use this object directly to generate the k-means clustering ?</p>
","8279790","","130288","","2017-07-24 20:03:44","2017-07-24 20:03:44","getting distance matrix and features matrix from word2vec model","<python><k-means><gensim><word2vec>","1","1","","","","CC BY-SA 3.0"
"53862627","1","53862779","","2018-12-20 04:59:23","","0","180","<p>I have a word2vec model using pre-trained GoogleNews-vectors-negative300.bin. The model works fine and I can get the similarities between the two words. For example:</p>

<pre><code>word2vec.similarity('culture','friendship')

0.2732939
</code></pre>

<p>Now, I want to use list elements instead of the words. For example, suppose that I have a list which its name is ""tag"". and the first two elements in the first row are culture and friendship. So, tag[0,0]= culture, and tag[0,1]=friendship.
I use the following code which gives me an error:</p>

<pre><code>word2vec.similarity(tag[0,0],tag[0,1])
</code></pre>

<p>the ""tag"" list is a <code>numpy.ndarray</code></p>

<p>the error is:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 992, in similarity
    return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2]))
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 337, in __getitem__
    return self.get_vector(entities)
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 455, in get_vector
    return self.word_vec(word)
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 452, in word_vec
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word ' friendship' not in vocabulary""
</code></pre>
","7189039","","8708527","","2018-12-20 05:16:46","2018-12-20 05:17:38","How to use a list in word2vec.similarity","<python><gensim><word2vec>","2","1","","","","CC BY-SA 4.0"
"54262583","1","","","2019-01-18 23:09:39","","0","1302","<p>I've loaded pretrained word2vec embeddings into a python dictionary of the form</p>

<p><code>{word: vector}</code></p>

<p>As an example, an element of this dictionary is</p>

<p><code>w2v_dict[""house""] = [1.1,2.0, ... , 0.2]</code></p>

<p>I would like to load this model into Gensim (or a similar library) so that I can find euclidean distances between embeddings.</p>

<p>I understand that pretrained embeddings typically come in a .bin file which can be loaded into Gensim. But if I only have a dictionary of this form, how would I load the vectors into a model?</p>
","2251258","","4001592","","2019-01-21 12:32:32","2019-11-09 06:41:57","Load word2vec dictionary into gensim","<nlp><gensim><word2vec><spacy><word-embedding>","2","0","","","","CC BY-SA 4.0"
"65370297","1","","","2020-12-19 13:23:29","","1","358","<p>I'm training a word embedding using GENSIM (word2vec) and use the trained model in a neural network in KERAS. A problem arises when I have an unknown (out-of-vocabulary) word so the neural network doesn't work anymore because it can't find weights for that specific word. I think one way to fix this problem is adding a new word (<code>&lt;unk&gt;</code>) to the pre-trained word embedding with zero weights (or maybe random weights? which one is better?) Is this approach fine? Also, for this word embedding, the weights are not trainable in this neural network.</p>
","2991243","","2991243","","2020-12-19 13:35:19","2020-12-20 04:17:32","Unknown words in a trained word embedding (Gensim) for using in Keras","<python><keras><gensim><word2vec><word-embedding>","1","2","1","","","CC BY-SA 4.0"
"62801052","1","62801053","","2020-07-08 18:10:33","","2","459","<p>I'm training a <code>Doc2Vec</code> model using the below code, where <code>tagged_data</code> is a list of <code>TaggedDocument</code> instances I set up before:</p>
<pre class=""lang-py prettyprint-override""><code>max_epochs = 40

model = Doc2Vec(alpha=0.025, 
                min_alpha=0.001)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.001
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(&quot;d2v.model&quot;)
print(&quot;Model Saved&quot;)
</code></pre>
<p>When I later check the model results, they're not good. What might have gone wrong?</p>
","130288","","130288","","2020-07-10 19:16:59","2020-10-23 16:58:39","My Doc2Vec code, after many loops of training, isn't giving good results. What might be wrong?","<gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"48808989","1","","","2018-02-15 13:56:11","","0","291","<pre><code>import os
import gensim.models as g
import logging
import gensim
os.chdir(""/home/ai/path"");
#doc2vec parameters
vector_size = 300
window_size = 5  
min_count = 1 
sampling_threshold = 1e-5 
negative_size = 5 
train_epoch = 100
dm= 0 

worker_count = 2 #number of parallel processes

#pretrained word embeddings
pretrained_emb = ""GoogleNews-vectors-negative300.bin""

#input corpus
train_corpus = ""mydata.txt""

#output model
saved_path = ""Googlemodel.bin""

#enable logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %
(message)s', 
level=logging.INFO)

#train doc2vec model
docs = g.doc2vec.TaggedLineDocument(train_corpus)
model = g.Doc2Vec(docs, size=vector_size, window=window_size, 
min_count=min_count, sample=sampling_threshold, workers=worker_count, 
hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, 
pretrained_emb=pretrained_emb, iter=train_epoch)
</code></pre>

<p>Size of <code>GoogleNews-vectors-negative300.bin</code> is 3.6 GB, my data size is <strong>455 MB</strong>. </p>

<p>After running this code or training process completion, my output model comes out with <strong>850 MB</strong> only.</p>
","8438023","","5377037","","2018-02-17 15:17:34","2018-02-17 15:17:34","Is there any way to retrain pretrained GoogleNews-vectors-negative300.bin model with our data?","<python><word2vec><gensim>","0","2","0","","","CC BY-SA 3.0"
"45289256","1","","","2017-07-24 20:13:05","","0","1152","<p>I have a big dataset and I'm trying to run Word2Vec model on it, but the vocabulary is constantly lowered to just 28.</p>

<pre><code>&gt;&gt;&gt; model = gensim.models.Word2Vec(sentences=sentences, window=5, min_count=1,trim_rule=None, workers=4,sg=0, hs=1)
&gt;&gt;&gt; len(model.wv.vocab)
28
</code></pre>

<p>I've tried with different setup of the constructor still the same.</p>

<p>My dataset consists of machine logs:</p>

<pre><code>wc eventlog_dataset
  4421775 124189284 978608310 eventlog_dataset
</code></pre>

<p>I previously ran tfidf model on this same dataset and I know for sure that I have ~100k unique words.</p>

<p>When I use a different dataset in gensim I have no such problem, so I definately know that the problem is my dataset, but I don't know why exactly...</p>

<p>Here's a sample :</p>

<pre><code>2017-05-16 10:55:58.91 CDT     3 61617032 Notification    Minor           Command error   sw_cli     {user super all {{0 8}} -1 10.0.188.216 3136} {Command: getfs  Error: Error: File Services is not configured on this array.} {}
2017-05-16 10:55:32.58 CDT     3 61616917 Notification    Minor           Command error   sw_cli     {user super all {{0 8}} -1 10.0.51.11 3727} {Command: getcage -e cage12 Error:    Opcode         = SCCMD_DOCDB    Node           = 253    Tpd error code = TE_INVALID          -- Invalid input parameter    Tpd error info = Cage (cage12) does not support this function } {}
</code></pre>

<p>As per the gensim documentation <code>trim_rule=None,min_count=1</code> should leave the full vocabulary.</p>

<p>Has anyone had such problems on datasets before ?</p>

<p><strong>EDIT</strong></p>

<p>Here's the code </p>

<pre><code>class FileToSent(object):
    def __init__(self, filename):
        self.filename = filename
       def __iter__(self):
            for line in open(self.filename, 'r'):
             ll = [i for i in unicode(line, 'utf-8').lower().split()]
             print ll
            yield ll


    sentences = FileToSent('/home/veselin/eventlog_dataset')
    model = gensim.models.Word2Vec(sentences=sentences, window=5, min_count=2,workers=4, hs=1)
</code></pre>

<p>And here's the output from the first line :</p>

<pre><code>/usr/bin/python2.7 /home/veselin/PycharmProjects/test/word2vec.py
[u'2016-10-16', u'17:55:19.55', u'cest', u'1', u'1788217', u'notification', u'minor', u'cli', u'command', u'error', u'sw_cli', u'{3parsvc', u'super', u'all', u'{{0', u'8}}', u'-1', u'172.16.24.110', u'12539}', u'{command:', u'getsralertcrit', u'all', u'error:', u'this', u'system', u'is', u'not', u'licensed', u'for', u'system', u'reporter', u'features}', u'{}']
</code></pre>

<p>You can see that words like cli,system or license, etc are not included in the vocabulary.</p>

<p>INFO logging (on full dataset)</p>

<pre><code>/usr/bin/python2.7 /home/veselin/PycharmProjects/test/word2vec.py
2017-07-28 11:32:56,966 : INFO : collecting all words and their counts
2017-07-28 11:33:35,580 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2017-07-28 11:33:35,582 : INFO : collected 28 word types from a corpus of 29 raw words and 1 sentences
2017-07-28 11:33:35,582 : INFO : Loading a fresh vocabulary
2017-07-28 11:33:35,582 : INFO : min_count=2 retains 1 unique words (3% of original 28, drops 27)
2017-07-28 11:33:35,582 : INFO : min_count=2 leaves 2 word corpus (6% of original 29, drops 27)
2017-07-28 11:33:35,583 : INFO : deleting the raw counts dictionary of 28 items
2017-07-28 11:33:35,584 : INFO : sample=0.001 downsamples 1 most-common words
2017-07-28 11:33:35,584 : INFO : downsampling leaves estimated 0 word corpus (3.3% of prior 2)
2017-07-28 11:33:35,584 : INFO : estimated required memory for 1 words and 100 dimensions: 1900 bytes
2017-07-28 11:33:35,584 : INFO : constructing a huffman tree from 1 words
2017-07-28 11:33:35,585 : INFO : built huffman tree with maximum node depth 0
2017-07-28 11:33:35,585 : INFO : resetting layer weights
2017-07-28 11:33:35,585 : INFO : training model with 4 workers on 1 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5
2017-07-28 11:36:43,871 : INFO : PROGRESS: at 100.00% examples, 0 words/s, in_qsize 2, out_qsize 2
2017-07-28 11:36:43,872 : INFO : worker thread finished; awaiting finish of 3 more threads
2017-07-28 11:36:43,873 : INFO : worker thread finished; awaiting finish of 2 more threads
2017-07-28 11:36:43,873 : INFO : worker thread finished; awaiting finish of 1 more threads
2017-07-28 11:36:43,873 : INFO : worker thread finished; awaiting finish of 0 more threads
2017-07-28 11:36:43,873 : INFO : training on 145 raw words (0 effective words) took 188.3s, 0 effective words/s
2017-07-28 11:36:43,873 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay

Process finished with exit code 0
</code></pre>
","4660571","","4660571","","2017-07-27 17:34:09","2020-01-13 15:05:14","gensim is always trimming the vocabulary","<python><gensim>","2","0","","","","CC BY-SA 3.0"
"48358161","1","50977776","","2018-01-20 16:03:38","","2","1816","<p>Is there a more direct or efficient method for getting the topic probabilities data from a gensim.interfaces.TransformedCorpus object into a numpy array (or alternatively, pandas dataframe) than the by-row method below?</p>

<pre><code>from gensim import models
import numpy as np

num_topics = 5
model = models.LdaMulticore(corpus, num_topics=num_topics, minimum_probability=0.0)

all_topics = model.get_document_topics(corpus)
num_docs = len(all_topics)

lda_scores = np.empty([num_docs, num_topics])

for i in range(0, num_docs):
    lda_scores[i] = np.array(all_topics[i]).transpose()[1]
</code></pre>
","4277212","","","","","2018-06-21 21:39:43","Efficient transformation of gensim TransformedCorpus data to array","<python><numpy><gensim><lda>","1","0","2","","","CC BY-SA 3.0"
"45310409","1","45331791","","2017-07-25 17:51:48","","16","15002","<p>I need to use gensim to get vector representations of words, and I figure the best thing to use would be a word2vec module that's pre-trained on the english wikipedia corpus. Does anyone know where to download it, how to install it, and how to use gensim to create the vectors?</p>
","7134235","","","","","2017-12-14 04:26:24","Using a Word2Vec model pre-trained on wikipedia","<wikipedia><gensim><word2vec>","2","2","7","","","CC BY-SA 3.0"
"45310925","1","45357701","","2017-07-25 18:21:31","","13","7887","<p>When I train my lda model as such</p>

<pre><code>dictionary = corpora.Dictionary(data)
corpus = [dictionary.doc2bow(doc) for doc in data]
num_cores = multiprocessing.cpu_count()
num_topics = 50
lda = LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, 
workers=num_cores, alpha=1e-5, eta=5e-1)
</code></pre>

<p>I want to get a full topic distribution for all <code>num_topics</code> for each and every document. That is, in this particular case, I want each document to have 50 topics contributing to the distribution <strong><em>and</em></strong> I want to be able to access all 50 topics' contribution. This output is what LDA should do if adhering strictly to the mathematics of LDA. However, gensim only outputs topics that exceed a certain threshold as shown <strong><a href=""https://stackoverflow.com/questions/23509699/understanding-lda-transformed-corpus-in-gensim/37708396?noredirect=1#comment77429460_37708396"">here</a></strong>. For example, if I try</p>

<pre><code>lda[corpus[89]]
&gt;&gt;&gt; [(2, 0.38951721864890398), (9, 0.15438596408262636), (37, 0.45607443684895665)]
</code></pre>

<p>which shows only 3 topics that contribute most to document 89. I have tried the solution in the link above, however this does not work for me. I still get the same output:</p>

<pre><code>theta, _ = lda.inference(corpus)
theta /= theta.sum(axis=1)[:, None]
</code></pre>

<p>produces the same output i.e. only 2,3 topics per document.</p>

<p>My question is how do I change this threshold so I can access the <strong><em>FULL</em></strong> topic distribution for <strong><em>each</em></strong> document? How can I access the full topic distribution, no matter how insignificant the contribution of a topic to a document? The reason I want the full distribution is so I can perform a <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""noreferrer"">KL similarity</a> search between documents' distribution.</p>

<p>Thanks in advance</p>
","4139143","","","","","2018-10-19 13:42:25","How to get a complete topic distribution for a document using gensim LDA?","<python><gensim><lda>","2","1","6","","","CC BY-SA 3.0"
"31512853","1","35124495","","2015-07-20 09:21:39","","2","1533","<p>I am trying to emulate streaming for some documents and update the LSI on additional documents streamed-in. I find this error:</p>

<pre><code>Traceback (most recent call last):
  File ""gensimStreamGen_tutorial5.py"", line 57, in &lt;module&gt;
    for vector in corpus_memory_friendly: # load one vector into memory at a time
  File ""gensimStreamGen_tutorial5.py"", line 44, in __iter__
    lsi = models.LsiModel(corpus, num_topics=10) # initialize an LSI transformation
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 331, in __init__
    self.add_documents(corpus)
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 388, in add_documents
    update = Projection(self.num_terms, self.num_topics, job, extra_dims=self.extra_samples, power_iters=self.power_iters)
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 126, in __init__
    extra_dims=self.extra_dims)
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 677, in stochastic_svd
    q, _ = matutils.qr_destroy(y) # orthonormalize the range
  File ""/Users/Desktop/gensim-0.12.0/gensim/matutils.py"", line 398, in qr_destroy
    qr, tau, work, info = geqrf(a, lwork=-1, overwrite_a=True)
ValueError: failed to create intent(cache|hide)|optional array-- must have defined dimensions but got (0,)
</code></pre>

<p>The code for streaming documents and updating LSI model:</p>

<pre><code>class MyCorpus(object):
    def __iter__(self):
        for document in documents:
            # Stream-in documents and build TF-IDF model to construct new_vec
            yield new_vec
            corpus.append(new_vec)
            tfidf = models.TfidfModel(corpus)
            corpus_tfidf = tfidf[corpus]
            lsi = models.LsiModel(corpus_tfidf,  num_topics=2)
            corpus_lsi = lsi[corpus_tfidf]
            lsi.print_topics(2)
            for doc in corpus_lsi:
                print(doc)

corpus_memory_friendly = MyCorpus()
for vector in corpus_memory_friendly:
    print(vector)
</code></pre>

<p>The corpus gets a new new_vec every iteration. The new_vec on each yield for different iterations:</p>

<pre><code>[]
[(0, 1)]
[(1, 1), (2, 1), (3, 1)]
[(3, 2), (4, 1), (5, 1)]
[(2, 1), (6, 1), (7, 1)]
[]
[(8, 1)]
[(8, 1), (9, 1)]
[(9, 1), (10, 1), (11, 1)]
</code></pre>

<p>The error appears on the first iteration (first line in expected new_vec). The rest is the expected output from new_vec.</p>
","4592660","","4592660","","2015-07-20 19:58:18","2016-02-01 06:49:44","Gensim: ValueError: failed to create intent(cache|hide)|optional array-- must have defined dimensions but got (0,)","<python><gensim><latent-semantic-indexing>","1","3","","","","CC BY-SA 3.0"
"45317151","1","45997591","","2017-07-26 03:54:12","","8","6331","<p>I'm relative new in the world of Latent Dirichlet Allocation.
I am able to generate a LDA Model following the Wikipedia tutorial and I'm able to generate a LDA model with my own documents.
My step now is try understand how can I use a previus generated model to classify unseen documents.
I'm saving my ""lda_wiki_model"" with</p>

<pre><code>id2word =gensim.corpora.Dictionary.load_from_text('ptwiki_wordids.txt.bz2')

    mm = gensim.corpora.MmCorpus('ptwiki_tfidf.mm')

    lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)
    lda.save('lda_wiki_model.lda')
</code></pre>

<p>And I'm loading the same model with:</p>

<pre><code>new_lda = gensim.models.LdaModel.load(path + 'lda_wiki_model.lda') #carrega o modelo
</code></pre>

<p>I have a ""new_doc.txt"", and I turn my document into a id&lt;-> term dictionary and converted this tokenized document to ""document-term matrix""</p>

<p>But when I run <code>new_topics = new_lda[corpus]</code> I receive a 
<strong><em>'gensim.interfaces.TransformedCorpus object at 0x7f0ecfa69d50'</em></strong></p>

<p>how can I extract topics from that?</p>

<p>I already tried </p>

<pre><code>`lsa = models.LdaModel(new_topics, id2word=dictionary, num_topics=1, passes=2)
corpus_lda = lsa[new_topics]
print(lsa.print_topics(num_topics=1, num_words=7)
</code></pre>

<p>and</p>

<p><code>print(corpus_lda.print_topics(num_topics=1, num_words=7</code>)
`</p>

<p>but that return topics not relationed to my new document.
Where is my mistake? I'm miss understanding something?</p>

<p>**If a run a new model using the dictionary and corpus created above, I receive the correct topics, my point is: how re-use my model? is correctly re-use that wiki_model?</p>

<p>Thank you.</p>
","6753946","","6753946","","2017-07-26 04:32:02","2020-12-04 20:49:32","gensim.interfaces.TransformedCorpus - How use?","<gensim><lda>","3","0","2","","","CC BY-SA 3.0"
"65372032","1","65384894","","2020-12-19 16:35:40","","2","529","<p>I am working on an NLP assignment and loaded the GloVe vectors provided by Gensim:</p>
<pre><code>import gensim.downloader
glove_vectors = gensim.downloader.load('glove-twitter-25')
</code></pre>
<p>I am trying to get the word embedding for each word in a sentence, but some of them are not in the vocabulary.</p>
<p>What is the best way to deal with it working with the Gensim API?</p>
<p>Thanks!</p>
","11556211","","","","","2020-12-23 05:09:03","Deal with Out of vocabulary word with Gensim pretrained GloVe","<nlp><stanford-nlp><gensim><word-embedding>","1","6","","","","CC BY-SA 4.0"
"36578341","1","","","2016-04-12 15:54:13","","3","7192","<p>How to use similarities.Similarity in gensim</p>

<p>Because if I use similarities.MatrixSimilarity:</p>

<p><code>index = similarities.MatrixSimilarity(tfidf[corpus])
</code>
It just told me:</p>

<pre><code>C:\Users\Administrator\AppData\Local\Enthought\Canopy\User\lib\site- packages\gensim-0.12.4-py2.7-win-amd64.egg\gensim\similarities\docsim.pyc in __init__(self, corpus, num_best, dtype, num_features, chunksize, corpus_len)
513                 raise ValueError(""cannot index a corpus with zero features (you must specify either `num_features` or a non-empty corpus in the constructor)"")
514             logger.info(""creating matrix with %i documents and %i features"", corpus_len, num_features)
515             self.index = numpy.empty(shape=(corpus_len, num_features), dtype=dtype)
516             # iterate over corpus, populating the numpy index matrix with (normalized)
517             # document vectors
</code></pre>

<p>MyProgram:</p>

<p>It works okay when the input content is less than 20,000 lines, but when the lines go more than 20,000, it just cannot build an index for 'corpus_tfidf'.</p>

<pre><code>    # -*- coding: utf-8 -*-
    import logging,time
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    start=time.clock()

    def mmin(a,b):
        if a&gt;b:
            return b
        else:
            return a
    from gensim import corpora, models, similarities

    fsource_01='E:\\cm_test\\ptfidf_test\\dim_items_terms_pre_pre.csv'
    fsource_02=fsource_01

    fcontent='E:\\cm_test\\ptfidf_test\\'

    f0=open(fsource_01)
    lines=f0.readlines()
    terms_list=[]
    for line in lines:
        line=line.strip('\n') #
        terms_sline=line.split(',') # es:['48909,53517,116593,55095']-&gt;['48909','53517','116593','55095']
        terms_list.append(terms_sline) 
    f0.close()

    from collections import defaultdict
    frequency = defaultdict(int)
    for text in terms_list:
            cnt_single=defaultdict(int)
            for token in text:
                frequency[token] += 1

    terms_list = [[token for token in text if frequency[token] &gt; 1] for text in terms_list]

    terms_list_qc=[]
    for ttext in terms_list:
        cnt_single=defaultdict(int)
        terms_list_qc_item=[]
        for token in ttext:
            cnt_single[token]+=1
            if(cnt_single[token]&lt;=1):
                terms_list_qc_item.append(str(token))
        terms_list_qc.append(terms_list_qc_item)

    dictionary = corpora.Dictionary(terms_list)
    #dictionary.save(fcontent+'dim_items_terms.dict')

    corpus = [dictionary.doc2bow(text) for text in terms_list]
    #corpora.MmCorpus.serialize(fcontent+'dim_items_terms.mm', corpus)  
    end1=time.clock()   
    print ""01.  Time Cost for trim_items_terms_to_sparse_matrix: %f s"" % (end1-start)

    tfidf = models.TfidfModel(corpus)
    corpus_tfidf = tfidf[corpus]
    end2=time.clock()   
    print ""2.   Time Cost for bagofwords_to_tfidf: %f s"" % (end2-end1)

    index = similarities.MatrixSimilarity(tfidf[corpus_tfidf])
    #index.save(fcontent+'dim_items_terms_tfidf.index')
    f0=open(fsource_02)
    lines=f0.readlines()
    f1=open(fcontent+'out_recordid_tfidf.txt',""w"")
    f2=open(fcontent+'out_cosine_tfidf.txt',""w"")
    for line in lines:
        line=line.strip('\n') 
        doc = line
        vec_bow = dictionary.doc2bow(doc.split(','))
        vec_lsi = tfidf[vec_bow]
        sims = index[vec_lsi]

        sims = sorted(enumerate(sims), key=lambda item: -item[1])
        osize=mmin(len(sims),400)
        for i in range(osize):
            f1.write(str(sims[i][0]+1)+',')
            f2.write(str(""%.2f""%sims[i][1])+',')
        f1.write('\n')
        f2.write('\n')
    f0.close()
    f1.close()
    f2.close()

    end3=time.clock()   
    print ""3.   Time Cost for get_sim_itemsid_top_fh: %f s"" % (end3-end2)
</code></pre>
","6192388","","53936","","2018-06-05 12:18:54","2018-06-05 12:18:54","How to use similarities.Similarity in gensim?","<python><gensim><cosine-similarity>","1","2","1","","","CC BY-SA 3.0"
"65895323","1","","","2021-01-26 02:59:50","","0","106","<p>I have some pretrained Word2vec models that I need to load both in Python and R. They are in txt.gz extension. I extracted them by 7-zip and could get the txt files. I use the following code to load them in Python:</p>
<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('syn0_ngram_1900_1909_full.txt', binary = False)
</code></pre>
<p>However, I get these errors:</p>
<pre><code>File &quot;&lt;ipython-input-6-31e3fee59511&gt;&quot;, line 1, in &lt;module&gt;
    model = gensim.models.KeyedVectors.load_word2vec_format('syn0_ngram_1900_1909_full.txt' , binary = False)

  File &quot;C:\Users\PSU\anaconda3\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 1547, in load_word2vec_format
    return _load_word2vec_format(

  File &quot;C:\Users\PSU\anaconda3\lib\site-packages\gensim\models\utils_any2vec.py&quot;, line 277, in _load_word2vec_format
    vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format

  File &quot;C:\Users\PSU\anaconda3\lib\site-packages\gensim\models\utils_any2vec.py&quot;, line 277, in &lt;genexpr&gt;
    vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format

ValueError: invalid literal for int() with base 10: '-1.552761644124984741e-01'
</code></pre>
<p>The first 3 lines of the file look like this:</p>
<pre><code>-1.552761644124984741e-01 -4.447535425424575806e-02 2.490501850843429565e-01 6.936315447092056274e-02 4.821906611323356628e-02 1.374670863151550293e-01 -3.902152925729751587e-02 -9.022397547960281372e-02 -2.184277474880218506e-01 2.209668904542922974e-01 -3.469136059284210205e-01 -1.411392092704772949e-01 -4.453907907009124756e-01 -1.025922745466232300e-01 8.120749890804290771e-02 -4.073899090290069580e-01 -1.905823498964309692e-02 -8.645015209913253784e-02 -7.905063778162002563e-02 7.617127150297164917e-02 1.504308432340621948e-01 8.758410811424255371e-03 3.396979570388793945e-01 -2.586390674114227295e-01 2.386739850044250488e-01 1.265343427658081055e-01 2.472167760133743286e-01 3.401717543601989746e-02 6.056435406208038330e-02 1.221914887428283691e-01 -8.905990421772003174e-02 -9.887123107910156250e-02 -9.832112491130828857e-02 -7.572189718484878540e-02 -7.369377184659242630e-03 1.302516758441925049e-01 -1.231815069913864136e-01 2.606352046132087708e-02 1.441473066806793213e-01 -1.717498451471328735e-01 9.777273982763290405e-03 6.302291993051767349e-03 8.990994654595851898e-03 -1.062645390629768372e-01 1.878743618726730347e-01 5.820669233798980713e-02 -1.301994770765304565e-01 1.007045060396194458e-01 -8.061264455318450928e-02 -1.923392526805400848e-02 9.648428112268447876e-02 -2.001685947179794312e-01 -1.039923876523971558e-01 1.369088292121887207e-01 3.344058617949485779e-02 -8.220246434211730957e-02 2.154016494750976562e-01 -1.533902585506439209e-01 -9.639452397823333740e-02 -1.077244579792022705e-01 3.839006647467613220e-02 -9.669522196054458618e-02 1.057831645011901855e-01 -1.731183379888534546e-01 -1.823752373456954956e-01 1.329025924205780029e-01 1.256246417760848999e-01 -5.657993257045745850e-02 -2.921316400170326233e-02 -6.282752007246017456e-02 1.006662324070930481e-01 6.356491148471832275e-02 1.589829623699188232e-01 1.770073324441909790e-01 4.362170770764350891e-02 -1.918367296457290649e-01 -3.448316827416419983e-02 -5.027920752763748169e-02 9.733915328979492188e-02 3.966502845287322998e-01 3.811245039105415344e-02 -2.094386219978332520e-01 3.425932824611663818e-01 -2.924242429435253143e-02 2.225339598953723907e-02 2.787167727947235107e-01 1.680488288402557373e-01 7.655870169401168823e-02 3.952257335186004639e-02 -2.619512081146240234e-01 -3.033895492553710938e-01 -5.149876475334167480e-01 -1.642060428857803345e-01 -1.959302574396133423e-01 1.126131117343902588e-01 -2.267295867204666138e-01 2.400911971926689148e-02 4.052775725722312927e-02 -3.044707141816616058e-02 -3.633485138416290283e-01 -2.818429283797740936e-02 -4.622202217578887939e-01 9.291686117649078369e-02 -2.956802845001220703e-01 1.862034201622009277e-01 1.242911815643310547e-01 1.026049628853797913e-01 1.160985007882118225e-01 -1.380904614925384521e-01 1.792961508035659790e-01 1.492877304553985596e-01 2.356165647506713867e-01 -2.932927012443542480e-02 1.063521653413772583e-01 3.353847563266754150e-01 1.908604428172111511e-02 3.782559633255004883e-01 -1.517397463321685791e-01 3.612821102142333984e-01 1.607065051794052124e-01 9.656509757041931152e-02 1.245319694280624390e-01 1.315144896507263184e-01 7.511320710182189941e-02 -4.755245521664619446e-02 -2.734144330024719238e-01 3.033797740936279297e-01 5.215175449848175049e-03 2.141999304294586182e-01 -1.597059220075607300e-01 3.182544559240341187e-02 4.125118851661682129e-01 -2.834559679031372070e-01 2.971411049365997314e-01 2.584041953086853027e-01 2.266484946012496948e-01 1.358106434345245361e-01 -8.042504638433456421e-02 -2.925538420677185059e-01 6.947112828493118286e-02 3.138780593872070312e-01 -1.517586410045623779e-01 -2.561317682266235352e-01 -1.843494027853012085e-01 2.936672978103160858e-02 -1.237718015909194946e-01 6.020113825798034668e-02 5.157970264554023743e-02 1.483027786016464233e-01 1.515904515981674194e-01 -7.338423281908035278e-02 1.898889243602752686e-02 2.750496566295623779e-02 -6.313492357730865479e-02 -2.602603659033775330e-02 -4.748337436467409134e-03 3.420833945274353027e-01 -5.720657855272293091e-02 -2.232243567705154419e-01 4.226108267903327942e-02 6.031884625554084778e-02 1.539045125246047974e-01 8.576720207929611206e-02 1.011675968766212463e-01 -3.795365989208221436e-01 -3.146133571863174438e-02 1.349445134401321411e-01 2.983746826648712158e-01 -2.938828170299530029e-01 1.533054113388061523e-01 -4.229364991188049316e-01 9.155936539173126221e-02 -2.974963048473000526e-03 -1.385585069656372070e-01 -1.053368579596281052e-02 1.153212636709213257e-01 3.379225432872772217e-01 -9.703439474105834961e-02 -1.578260511159896851e-01 -6.252604722976684570e-02 1.598290950059890747e-01 2.294627018272876740e-03 6.054456159472465515e-02 1.103171482682228088e-01 1.407995820045471191e-02 1.977602243423461914e-01 -7.971014082431793213e-02 6.747842580080032349e-02 -7.176994532346725464e-02 3.453086316585540771e-02 1.144322603940963745e-01 -1.870087534189224243e-01 -1.876662820577621460e-01 6.476462818682193756e-03 -8.064353466033935547e-02 -1.166440173983573914e-01 -3.607030212879180908e-02 -2.510503865778446198e-02 6.253489851951599121e-02 1.802610009908676147e-01 4.245756864547729492e-01 -1.071699485182762146e-01 -1.976074464619159698e-02 7.162892073392868042e-02 -2.126150727272033691e-01 -1.831589490175247192e-01 -7.786697894334793091e-02 1.421018242835998535e-01 2.083165943622589111e-01 -9.992305934429168701e-02 7.392542809247970581e-02 9.227126836776733398e-02 -1.524462252855300903e-01 2.111838459968566895e-01 1.633472144603729248e-01 6.497748196125030518e-02 6.825347244739532471e-02 -3.643653988838195801e-01 1.698636859655380249e-01 6.742136925458908081e-02 2.124408334493637085e-01 -2.609764039516448975e-01 -4.775075241923332214e-02 -1.276874262839555740e-02 -9.566855616867542267e-03 -7.416314631700515747e-02 -1.711301803588867188e-01 2.018006443977355957e-01 -4.967777058482170105e-03 7.954392582178115845e-02 -8.138674497604370117e-02 2.610500156879425049e-01 3.377711772918701172e-02 -2.635057568550109863e-01 7.423927634954452515e-02 -2.577809691429138184e-01 -7.702536880970001221e-02 1.627112627029418945e-01 1.897031962871551514e-01 -1.299263685941696167e-01 1.664789579808712006e-02 -6.737360358238220215e-02 -2.183234542608261108e-01 2.616149485111236572e-01 -1.861911714076995850e-01 -8.766605705022811890e-02 5.951049551367759705e-02 3.398019671440124512e-01 1.241989135742187500e-01 1.123771518468856812e-01 2.735071256756782532e-02 7.581159472465515137e-03 -1.705877929925918579e-01 9.298118948936462402e-02 -5.501312017440795898e-02 2.464835159480571747e-02 1.904888302087783813e-01 1.251959949731826782e-01 -9.753731638193130493e-02 4.099815338850021362e-02 -3.088685572147369385e-01 4.752117022871971130e-02 -1.016708761453628540e-01 2.049167454242706299e-01 -1.110423356294631958e-01 -2.558538317680358887e-02 9.703662991523742676e-02 1.440881937742233276e-01 -1.499230116605758667e-01 4.630966186523437500e-01 1.560464948415756226e-01 -2.473618537187576294e-01 7.339747250080108643e-02 -1.125243376009166241e-03 2.308040857315063477e-03 -7.349326461553573608e-02 -5.643999949097633362e-02 -1.791801899671554565e-01 3.374390304088592529e-02 5.359465628862380981e-02 4.016261696815490723e-01 -8.631563186645507812e-02 -1.041909903287887573e-01 -9.027398191392421722e-03 7.635752111673355103e-02 -1.177581623196601868e-01 6.990105658769607544e-02 -1.495847105979919434e-01 -1.948498487472534180e-01 -1.003706827759742737e-01 2.158978767693042755e-02 2.253228724002838135e-01 -8.305017650127410889e-02 9.877178817987442017e-02 -1.782058775424957275e-01 -4.364309012889862061e-01 2.809965051710605621e-02 5.815667286515235901e-02 9.305762499570846558e-02 9.939935058355331421e-02
-2.755518853664398193e-01 7.426643371582031250e-02 1.305104941129684448e-01 1.733209006488323212e-02 3.392809331417083740e-01 4.914091154932975769e-02 -5.487316101789474487e-02 -2.893702983856201172e-01 -3.995743691921234131e-01 1.019903868436813354e-01 -5.586374923586845398e-02 -2.909922003746032715e-01 -1.379316449165344238e-01 -1.213544141501188278e-02 -2.101085036993026733e-01 -4.060855805873870850e-01 2.363941520452499390e-01 -1.304764747619628906e-01 -1.898821741342544556e-01 7.960485666990280151e-02 6.144599989056587219e-02 -8.303866721689701080e-03 1.456501632928848267e-01 -1.511054039001464844e-01 3.446572422981262207e-01 1.809655129909515381e-01 3.376641869544982910e-01 -1.289701908826828003e-01 1.942324079573154449e-02 1.295022666454315186e-01 1.819744110107421875e-01 9.251490980386734009e-02 1.657947003841400146e-01 -4.376604557037353516e-01 2.938240170478820801e-01 -1.873110830783843994e-01 -1.355587989091873169e-01 -2.293781042098999023e-01 -9.990473277866840363e-03 -1.429447233676910400e-01 4.837138950824737549e-02 4.135683923959732056e-02 1.273282319307327271e-01 -1.000547260046005249e-01 3.860374540090560913e-02 3.943286091089248657e-02 7.455765455961227417e-02 -1.942279636859893799e-01 1.055958718061447144e-01 -1.248219236731529236e-01 6.977072358131408691e-02 8.551878482103347778e-02 4.604674875736236572e-02 -1.508192718029022217e-01 -2.823450267314910889e-01 -1.705607175827026367e-01 1.018783375620841980e-01 9.879937022924423218e-02 -4.601259529590606689e-02 -1.719024218618869781e-02 -1.294963657855987549e-01 -5.334546416997909546e-02 1.102923452854156494e-01 3.475880622863769531e-02 3.030833788216114044e-02 3.598376810550689697e-01 1.075935140252113342e-01 1.747883707284927368e-01 2.600349187850952148e-01 -4.294164106249809265e-02 3.064307570457458496e-01 3.595127537846565247e-02 8.350577205419540405e-02 4.761104285717010498e-02 1.397927701473236084e-01 6.383475847542285919e-03 -1.242930628359317780e-02 -6.513260304927825928e-02 -1.765230298042297363e-01 2.290750741958618164e-01 1.070840135216712952e-01 -1.611845940351486206e-01 2.256397455930709839e-01 3.962266817688941956e-02 -1.251329332590103149e-01 -8.839791268110275269e-02 -8.401984721422195435e-02 -1.068911850452423096e-01 4.183220565319061279e-01 -1.719796285033226013e-02 -1.992868930101394653e-01 -1.439917534589767456e-01 -3.158213943243026733e-02 1.782516241073608398e-01 -2.040623277425765991e-01 -2.465122565627098083e-02 3.390240948647260666e-03 -2.063101902604103088e-02 3.736664727330207825e-02 -1.950853466987609863e-01 7.347257435321807861e-02 -3.684818744659423828e-01 -3.807673603296279907e-02 -6.298073381185531616e-02 3.570814132690429688e-01 1.056838855147361755e-01 -6.606206297874450684e-02 1.103219836950302124e-01 1.340708583593368530e-01 1.316183954477310181e-01 1.801468431949615479e-01 2.364787608385086060e-01 -2.933555981144309044e-03 1.394167244434356689e-01 1.410789489746093750e-01 2.110916227102279663e-01 -3.954877331852912903e-02 -1.672693789005279541e-01 4.702991843223571777e-01 6.913857907056808472e-02 1.212535426020622253e-01 -8.544678986072540283e-02 1.632798463106155396e-01 -9.701278060674667358e-02 6.379171460866928101e-02 3.686433658003807068e-03 2.002136409282684326e-02 -2.272810041904449463e-01 2.025170065462589264e-02 -2.601526863873004913e-02 -2.050084173679351807e-01 1.567473113536834717e-01 9.391159564256668091e-02 -8.464719355106353760e-02 1.202475205063819885e-01 2.147839069366455078e-01 3.591004014015197754e-01 -1.711977422237396240e-01 -2.318718731403350830e-01 1.340079754590988159e-01 -3.448984399437904358e-02 2.319254539906978607e-02 -2.770109176635742188e-01 -3.476117849349975586e-01 -2.453668564558029175e-01 1.853164657950401306e-02 6.908061355352401733e-02 -8.381587266921997070e-02 -9.367397427558898926e-02 2.148762494325637817e-01 1.584969758987426758e-01 3.585815057158470154e-02 -2.491751015186309814e-01 -1.422119513154029846e-02 7.534956187009811401e-02 1.988542675971984863e-01 9.783649444580078125e-02 -1.565352529287338257e-01 -1.734849214553833008e-01 -6.991375982761383057e-02 3.639198243618011475e-01 1.621825546026229858e-01 -6.586146354675292969e-02 5.274389684200286865e-02 1.484267711639404297e-01 1.349460333585739136e-01 -1.060503944754600525e-01 2.191036790609359741e-01 4.246726259589195251e-02 -1.529514938592910767e-01 5.295782908797264099e-02 7.367381453514099121e-02 3.147679939866065979e-02 -1.366107761859893799e-01 -6.432162970304489136e-02 2.987476065754890442e-02 9.375686198472976685e-02 -3.127529919147491455e-01 -1.916600018739700317e-02 -1.505621522665023804e-01 -2.707700803875923157e-02 -3.426384180784225464e-03 -5.949690937995910645e-02 8.628486841917037964e-02 -1.401778589934110641e-02 2.023975551128387451e-01 -1.908088102936744690e-02 -2.119334191083908081e-01 1.441151797771453857e-01 1.514745354652404785e-01 -7.387769222259521484e-02 -1.496255546808242798e-01 -4.099091142416000366e-02 3.930757343769073486e-01 -1.470357030630111694e-01 -2.854492068290710449e-01 3.818158432841300964e-02 1.000983938574790955e-01 2.482949197292327881e-01 -1.042249724268913269e-01 1.192888319492340088e-01 -5.882263556122779846e-02 -1.760703176259994507e-01 1.793773612007498741e-03 -8.517112582921981812e-02 -8.584306389093399048e-02 -9.191264398396015167e-03 1.277624070644378662e-01 1.232503578066825867e-01 -5.207545310258865356e-02 -1.516743302345275879e-01 8.217832446098327637e-02 1.153566017746925354e-01 4.729519486427307129e-01 5.880970880389213562e-02 1.440012007951736450e-01 -2.476142048835754395e-01 -2.377158254384994507e-01 1.911669075489044189e-01 3.252712637186050415e-02 1.070735454559326172e-01 -6.179307028651237488e-02 -2.360517531633377075e-01 -2.577443122863769531e-01 -5.894837155938148499e-02 3.033825755119323730e-01 -3.594714030623435974e-02 3.514684969559311867e-03 -4.581563547253608704e-02 -1.858881562948226929e-01 1.027334555983543396e-01 2.197035700082778931e-01 2.374446019530296326e-02 -1.158262640237808228e-01 1.136242374777793884e-01 -2.748361043632030487e-02 4.605894163250923157e-02 -7.746911793947219849e-02 1.829507201910018921e-01 -6.030893698334693909e-02 -2.890438400208950043e-02 1.853499114513397217e-01 -2.043003737926483154e-01 3.536553680896759033e-02 -8.747279644012451172e-02 3.783982396125793457e-01 2.096020579338073730e-01 1.197393238544464111e-01 2.897605299949645996e-01 -1.817464530467987061e-01 -1.201971098780632019e-01 5.573973059654235840e-03 -3.500255197286605835e-02 -7.611036300659179688e-02 -2.485055625438690186e-01 6.760949641466140747e-02 -7.421203702688217163e-02 2.240369617938995361e-01 2.368878340348601341e-03 -1.559459716081619263e-01 -3.354331552982330322e-01 -1.983614265918731689e-01 -1.804647594690322876e-01 -1.304219365119934082e-01 -2.911436557769775391e-02 1.543108224868774414e-01 -1.778330653905868530e-01 3.935296833515167236e-01 -8.174671977758407593e-02 2.501682937145233154e-01 -7.979557663202285767e-02 6.083849817514419556e-02 8.713995665311813354e-02 4.636264592409133911e-02 8.727990835905075073e-02 -1.660248637199401855e-01 1.531319767236709595e-01 -6.790683255530893803e-04 7.505177706480026245e-02 -1.619962751865386963e-01 1.066124737262725830e-01 -3.226042985916137695e-01 2.093208255246281624e-03 1.322522610425949097e-01 -2.231481522321701050e-01 1.296667754650115967e-01 -1.306195557117462158e-02 -1.684510260820388794e-01 1.680857092142105103e-01 -5.792901664972305298e-02 1.396010369062423706e-01 1.932041347026824951e-01 3.012113086879253387e-02 2.267901003360748291e-01 -3.265710771083831787e-01 -3.679866790771484375e-01 -2.241524606943130493e-01 -9.129060804843902588e-02 -1.781184524297714233e-01 2.713193893432617188e-01
-2.774842977523803711e-01 -2.998521625995635986e-01 -7.971244305372238159e-02 8.230878040194511414e-03 -1.816957741975784302e-01 3.503366410732269287e-01 3.937914967536926270e-01 3.818871676921844482e-01 -9.332147240638732910e-02 -1.803085207939147949e-02 -2.010484933853149414e-01 1.534142941236495972e-01 -1.827129423618316650e-01 2.424736320972442627e-01 -2.587919868528842926e-02 -1.459656208753585815e-01 -2.334953695535659790e-01 -2.997140884399414062e-01 -1.362463384866714478e-01 8.161799609661102295e-02 1.432602256536483765e-01 1.230252087116241455e-01 -1.510256156325340271e-02 6.471955031156539917e-02 1.789489835500717163e-01 2.171420454978942871e-01 4.145196378231048584e-01 1.729734390974044800e-01 1.356951594352722168e-01 2.059405446052551270e-01 1.503511667251586914e-01 -2.932371757924556732e-02 8.221655152738094330e-03 -2.987872958183288574e-01 -1.650227159261703491e-01 1.787729188799858093e-02 -1.364763826131820679e-01 -5.834015086293220520e-02 2.543235421180725098e-01 -1.790037006139755249e-01 -1.532125100493431091e-02 5.038385838270187378e-02 9.709338843822479248e-02 -2.102746218442916870e-01 1.455358881503343582e-02 -3.748860061168670654e-01 -7.003118097782135010e-02 4.741380214691162109e-01 4.043326899409294128e-02 -4.256066307425498962e-02 3.166067302227020264e-01 -7.988628745079040527e-02 -3.403761088848114014e-01 -7.842956483364105225e-02 -3.211471065878868103e-02 -8.283370733261108398e-02 1.359094232320785522e-01 1.110327914357185364e-01 6.995534151792526245e-02 -1.054197996854782104e-01 5.218173563480377197e-02 -1.197223141789436340e-01 4.968923330307006836e-02 -1.623026579618453979e-01 -3.258561789989471436e-01 2.500229775905609131e-01 3.593124151229858398e-01 -3.386449441313743591e-02 8.114624768495559692e-03 1.578128151595592499e-02 3.713388741016387939e-01 3.886390849947929382e-02 7.361979037523269653e-02 4.777812063694000244e-01 -3.187358081340789795e-01 -1.472535170614719391e-02 -1.864034235477447510e-01 2.851904034614562988e-01 2.424101158976554871e-02 3.765774965286254883e-01 4.411956071853637695e-01 1.843919754028320312e-01 6.076631136238574982e-03 1.195349693298339844e-01 -4.604819789528846741e-02 3.925660848617553711e-01 -1.517851352691650391e-01 -1.246303766965866089e-01 1.766519099473953247e-01 -1.822839528322219849e-01 -4.482288956642150879e-01 -3.685613870620727539e-01 -2.097260802984237671e-01 1.643853932619094849e-01 -4.741436839103698730e-01 1.948721590451896191e-03 -9.952253662049770355e-03 -1.998928934335708618e-01 -2.408249676227569580e-01 -1.873278170824050903e-01 1.068998277187347412e-01 1.051706746220588684e-01 2.129557281732559204e-01 2.287544757127761841e-01 9.127160906791687012e-02 5.938585847616195679e-02 2.965441048145294189e-01 2.421934902667999268e-01 -8.174385130405426025e-02 2.093371152877807617e-01 1.487546563148498535e-01 2.000252306461334229e-01 -6.509444862604141235e-02 -7.295352220535278320e-02 3.018093109130859375e-01 2.694353833794593811e-02 3.530837222933769226e-02 -5.731752514839172363e-02 1.747614592313766479e-01 1.560003608465194702e-01 2.684080004692077637e-01 1.080089155584573746e-02 1.087081581354141235e-01 1.963618397712707520e-02 4.157029837369918823e-02 -1.816155910491943359e-01 2.548927962779998779e-01 4.838272556662559509e-02 -1.472955197095870972e-01 -1.429838240146636963e-01 -1.541285365819931030e-01 2.681677043437957764e-01 -4.319681525230407715e-01 1.616368293762207031e-01 -8.768530935049057007e-02 -1.047493219375610352e-01 -1.213742494583129883e-01 1.956912130117416382e-02 1.767091825604438782e-02 3.966970741748809814e-01 3.049480617046356201e-01 -3.431919217109680176e-01 -2.700016498565673828e-01 1.273563951253890991e-01 -7.174579054117202759e-02 1.201752051711082458e-01 -2.558999061584472656e-01 2.808746397495269775e-01 -1.585903167724609375e-01 3.504217267036437988e-01 1.641803234815597534e-01 -7.637239992618560791e-02 1.117228902876377106e-02 -1.290193665772676468e-02 5.763415247201919556e-02 2.977091446518898010e-02 1.087564900517463684e-01 5.177070945501327515e-02 1.507237739861011505e-02 1.421540975570678711e-02 -2.560492157936096191e-01 7.064723372459411621e-01 -3.870822861790657043e-02 -9.608229249715805054e-02 -4.009970650076866150e-02 2.679672539234161377e-01 -6.665153801441192627e-02 3.355098664760589600e-01 2.569686770439147949e-01 -2.018120288848876953e-01 -1.346995588392019272e-02 -5.782928317785263062e-02 8.600515872240066528e-02 3.889953345060348511e-02 2.605972588062286377e-01 -6.099622845649719238e-01 3.195210993289947510e-01 -2.089240849018096924e-01 1.928902976214885712e-02 -4.566932469606399536e-02 -1.053371950984001160e-01 -3.651003818958997726e-03 -5.018276348710060120e-02 -9.367980808019638062e-02 -3.558373451232910156e-02 -4.714028537273406982e-02 -1.415439844131469727e-01 1.963711977005004883e-01 2.375491559505462646e-01 -6.988362222909927368e-02 -1.764807403087615967e-01 -2.329608052968978882e-01 -2.117581516504287720e-01 1.594799607992172241e-01 -1.619364023208618164e-01 -2.535828948020935059e-01 -3.168850392103195190e-02 -1.248130872845649719e-01 -3.917082026600837708e-02 -1.555472612380981445e-02 1.310855150222778320e-01 -1.109836474061012268e-01 1.193420886993408203e-01 -7.499247789382934570e-02 2.165957689285278320e-01 2.515639737248420715e-02 -7.427005469799041748e-02 3.367302119731903076e-01 1.033056080341339111e-01 -2.416009753942489624e-01 1.372187584638595581e-01 1.164045780897140503e-01 -2.295079827308654785e-01 2.349957227706909180e-01 -1.768460124731063843e-01 2.567556798458099365e-01 -1.244153752923011780e-01 -1.948076933622360229e-01 2.246239781379699707e-01 2.920483946800231934e-01 1.322932094335556030e-01 -1.680658757686614990e-01 -2.810868918895721436e-01 -1.739274859428405762e-01 -6.817381829023361206e-02 1.097021810710430145e-02 -3.780493140220642090e-01 1.022090688347816467e-01 -3.455026149749755859e-01 -5.340179894119501114e-03 -1.941403299570083618e-01 2.195733934640884399e-01 1.599482595920562744e-01 -6.429063156247138977e-03 -1.994361579418182373e-01 7.429709285497665405e-02 -6.186648830771446228e-02 3.215630054473876953e-01 -3.681885451078414917e-02 -2.468834519386291504e-01 5.535753443837165833e-02 -1.222386434674263000e-01 -1.336591690778732300e-01 1.399028897285461426e-01 -1.366180181503295898e-01 1.522678881883621216e-01 1.511430181562900543e-02 2.401746958494186401e-01 2.649781107902526855e-01 -5.454543828964233398e-01 -1.092205718159675598e-01 -9.328306466341018677e-02 -3.384732306003570557e-01 -1.642127931118011475e-01 2.024921774864196777e-01 -3.152533620595932007e-02 2.340486049652099609e-01 8.750239759683609009e-02 -2.784305810928344727e-01 8.590795099735260010e-02 -1.133114993572235107e-01 -3.966497257351875305e-02 -3.721207976341247559e-01 9.668816626071929932e-02 -1.251616477966308594e-01 3.858697135001420975e-03 -2.339877784252166748e-01 5.868237018585205078e-01 4.996139090508222580e-03 1.808173656463623047e-01 3.835939243435859680e-02 -8.150214701890945435e-02 -1.084734220057725906e-02 -3.009911999106407166e-02 1.546976119279861450e-01 -9.382786601781845093e-02 2.376294881105422974e-01 6.217407062649726868e-03 -1.371953040361404419e-01 -2.346184253692626953e-01 3.121209144592285156e-01 3.882350400090217590e-02 -4.442759752273559570e-01 9.198225289583206177e-02 -1.463707238435745239e-01 -1.381492167711257935e-01 1.165479123592376709e-01 -3.256337344646453857e-01 -6.764362007379531860e-02 6.391936540603637695e-02 2.433998733758926392e-01 2.343472838401794434e-01 -1.262559294700622559e-01 8.547146618366241455e-02 -2.869825363159179688e-01 -5.394507665187120438e-03 -1.538014262914657593e-01 9.915013611316680908e-02 -6.313066929578781128e-02 2.360366135835647583e-01
</code></pre>
<p>I searched a lot and some people suggest that the Word2vec text files should have a first line containing the number of words and dimensions. I tried to add those to the file but it didn't work either:</p>
<pre><code>File &quot;&lt;ipython-input-5-9d20447a9587&gt;&quot;, line 1, in &lt;module&gt;
    model = gensim.models.KeyedVectors.load_word2vec_format('syn0_ngram_1900_1909_full_Copy.txt' , binary = False)

  File &quot;C:\Users\PSU\anaconda3\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 1547, in load_word2vec_format
    return _load_word2vec_format(

  File &quot;C:\Users\PSU\anaconda3\lib\site-packages\gensim\models\utils_any2vec.py&quot;, line 288, in _load_word2vec_format
    _word2vec_read_text(fin, result, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)

  File &quot;C:\Users\PSU\anaconda3\lib\site-packages\gensim\models\utils_any2vec.py&quot;, line 220, in _word2vec_read_text
    raise ValueError(&quot;invalid vector on line %s (is this really the text format?)&quot; % line_no)

ValueError: invalid vector on line 0 (is this really the text format?)
</code></pre>
<p>Finally, for the last line of error (ValueError: invalid literal for int() with base 10: '-1.552761644124984741e-01'), I believe there should be some issues with the file containing decimal values and not integers. I don't know how to change the default data type to float.</p>
<p>I haven't had a chance to investigate the issues with R much, but I get &quot;object X not found&quot; error when I run this code:</p>
<pre><code>df &lt;- read.wordvectors('syn0_ngram_1900_1909_full', type = 'txt', normalize = FALSE)
</code></pre>
<p>I've checked and I'm sure there's no issue with the directory or spelling.</p>
","14016238","","14016238","","2021-01-26 12:16:24","2021-01-26 12:16:24","Loading pretrained Word2vec model","<python><r><nlp><gensim><word2vec>","0","8","","","","CC BY-SA 4.0"
"65704851","1","65705829","","2021-01-13 15:22:59","","1","52","<p>I am using Gensim's Phraser model to find bigrams in some reviews, to be later used in an LDA topic modelling scenario. My issue is that the reviews mention the word &quot;service&quot; quite often and so Phraser finds lots of different bigrams with &quot;service&quot; as one of the pairs (e.g &quot;helpful_service&quot;, &quot;good_service&quot;, &quot;service_price&quot;).</p>
<p>These are then present across multiple topics in the final result*. I'm thinking that I could prevent this from occurring if I was able to tell Phraser <em>not</em> to include &quot;service&quot; when making bigrams. Is this possible?</p>
<p>(*) I am aware that &quot;service&quot;-related bigrams being present across multiple topics might indeed be the optimal result, but I just want to experiment with leaving them out.</p>
<p>Sample code:</p>
<pre><code># import gensim models
from gensim.models import Phrases
from gensim.models.phrases import Phraser

# sample data
data = [
    &quot;Very quick service left a big tip&quot;,
    &quot;Very bad service left a complaint to the manager&quot;
]
data_words = [doc.split(&quot; &quot;) for doc in data]

# build the bigram model
bigram_phrases = Phrases(data_words, min_count=2, threshold=0, scoring='npmi') 
# note I used the arguments above to force &quot;service&quot; based bigrams to be created for this example
bigram_phraser = Phraser(bigram_phrases)

# print the result
for word in data_words:
    tokens_ = bigram_phraser[word]
    print(tokens_)
</code></pre>
<p>The above prints:</p>
<pre><code>['Very', 'quick', 'service_left', 'a', 'big', 'tip']
['Very', 'bad', 'service_left', 'complaint', 'to', 'the', 'manager']
</code></pre>
","12129423","","","","","2021-01-13 16:19:49","How to prevent certain words from being included when building bigrams using Gensim's Phrases?","<python><nlp><gensim>","1","2","","","","CC BY-SA 4.0"
"62803770","1","","","2020-07-08 21:17:28","","0","38","<p>I'm trying to find the most similar documents to a new document. The doc2vec model was trained first, and now I'm introducing a new document; I've inferred the vector for the new document, but I don't know the ins and outs of doc2vec well... If the new document has a lot of words (in a row) that the old model never encountered, how will it be handled?</p>
","13421624","","10794031","","2020-07-11 00:44:53","2020-07-11 00:44:53","How does Doc2Vec assess new words?","<new-operator><gensim><word2vec><word><doc2vec>","1","0","","","","CC BY-SA 4.0"
"48371824","1","","","2018-01-21 21:19:23","","1","1191","<p>I have the following code (only relevant parts shown):</p>

<pre><code>def load_model(model_file):
    return Doc2Vec.load(model_file)

# infer 
def infer_docs(input_string, model_file, inferred_docs=5):
    model = load_model(model_file)
    processed_str = simple_preprocess(input_string, min_len=2, max_len=35)    
    inferred_vector = model.infer_vector(processed_str)
    return model.docvecs.most_similar([inferred_vector], topn=inferred_docs)
</code></pre>

<p>The code runs as a lambda on aws. It works fine when my model is small (I think that is the reason) but when I have a decent size model (~200mb) I get the following error</p>

<pre><code>[INFO]  2018-01-21T20:44:59.613Z    f2689816-feeb-11e7-b397-b7ff2947dcec    testing keys in event dict
[INFO]  2018-01-21T20:44:59.614Z    f2689816-feeb-11e7-b397-b7ff2947dcec    loading model from s3://data-d2v/trained_models/model_law
[INFO]  2018-01-21T20:44:59.614Z    f2689816-feeb-11e7-b397-b7ff2947dcec    loading Doc2Vec object from s3://data-d2v/trained_models/model_law
[INFO]  2018-01-21T20:44:59.650Z    f2689816-feeb-11e7-b397-b7ff2947dcec    Found credentials in environment variables.
[INFO]  2018-01-21T20:44:59.707Z    f2689816-feeb-11e7-b397-b7ff2947dcec    Starting new HTTPS connection (1): s3.eu-west-1.amazonaws.com
[INFO]  2018-01-21T20:44:59.801Z    f2689816-feeb-11e7-b397-b7ff2947dcec    Starting new HTTPS connection (2): s3.eu-west-1.amazonaws.com
[INFO]  2018-01-21T20:45:35.830Z    f2689816-feeb-11e7-b397-b7ff2947dcec    loading wv recursively from s3://data-d2v/trained_models/model_law.wv.* with mmap=None
[INFO]  2018-01-21T20:45:35.830Z    f2689816-feeb-11e7-b397-b7ff2947dcec    loading syn0 from s3://data-d2v/trained_models/model_law.wv.syn0.npy with mmap=None
[Errno 2] No such file or directory: 's3://data-d2v/trained_models/model_law.wv.syn0.npy': FileNotFoundError
Traceback (most recent call last):
  File ""/var/task/handler.py"", line 20, in infer_handler
    event['input_text'], event['model_file'], inferred_docs=10)
  File ""/var/task/infer_doc.py"", line 26, in infer_docs
    model = load_model(model_file)
  File ""/var/task/infer_doc.py"", line 21, in load_model
    return Doc2Vec.load(model_file)
  File ""/var/task/gensim/models/word2vec.py"", line 1569, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""/var/task/gensim/utils.py"", line 282, in load
    obj._load_specials(fname, mmap, compress, subname)
  File ""/var/task/gensim/models/word2vec.py"", line 1593, in _load_specials
    super(Word2Vec, self)._load_specials(*args, **kwargs)
  File ""/var/task/gensim/utils.py"", line 301, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)
  File ""/var/task/gensim/utils.py"", line 312, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)
  File ""/var/task/numpy/lib/npyio.py"", line 372, in load
    fid = open(file, ""rb"")
FileNotFoundError: [Errno 2] No such file or directory: 's3://data-d2v/trained_models/model_law.wv.syn0.npy'
</code></pre>

<p>First of all the file <code>s3://data-d2v/trained_models/model_law.wv.syn0.npy</code> exists and secondly to me it seems that the main model file <code>s3://data-d2v/trained_models/model_law</code> is loaded. </p>

<p>To validate access and existence of the file I added:</p>

<pre><code>import smart_open
with smart_open.smart_open('s3://data-d2v/trained_models/model_law.wv.syn0.npy') as prut:
    for line in prut:
        print(line)
</code></pre>

<p>which works just fine printing.</p>

<p>Can you help?  </p>
","6573904","","6573904","","2018-01-22 12:02:42","2018-01-23 12:38:54","gensim loading from s3 does not work","<amazon-web-services><amazon-s3><aws-lambda><gensim>","1","2","","","","CC BY-SA 3.0"
"55694532","1","","","2019-04-15 17:34:08","","3","930","<p>I'm trying to show learning progress in my LdaModel, but every sample I found on the web throws exceptions:</p>

<pre><code>l =  gensim.models.callbacks.CoherenceMetric(corpus=common_corpus, logger='shell')
lda = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=genres_count, id2word = common_corpus, passes=150, callbacks=[l])
</code></pre>

<p>Throws:</p>

<pre><code>  File ""&lt;ipython-input-165-6ad0e2e8516c&gt;"", line 2, in &lt;module&gt;
    lda = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=genres_count, id2word = common_corpus, passes=150, callbacks=[l])

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldamodel.py"", line 371, in __init__
    self.update(corpus, chunks_as_numpy=use_numpy)

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldamodel.py"", line 750, in update
    current_metrics = callback.on_epoch_end(pass_)

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\callbacks.py"", line 288, in on_epoch_end
    value = metric.get_value(topics=topics, model=self.model, other_model=self.previous)

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\callbacks.py"", line 105, in get_value
    coherence=self.coherence, topn=self.topn

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\coherencemodel.py"", line 190, in __init__
    self.window_size = SLIDING_WINDOW_SIZES[self.coherence]

KeyError: None
</code></pre>

<p>This code (found <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">here</a>):</p>

<pre><code>class EpochLogger(CallbackAny2Vec):
    '''Callback to log information about training'''

    def __init__(self):
        self.epoch = 0

    def on_epoch_begin(self, model):
        print(""Epoch #{} start"".format(self.epoch))

    def on_epoch_end(self, model):
        print(""Epoch #{} end"".format(self.epoch))
        self.epoch += 1

l = EpochLogger()
lda = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=genres_count, id2word = common_corpus, passes=150, callbacks=[l])
</code></pre>

<p>Throws:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-167-e89e2bf41977&gt;"", line 1, in &lt;module&gt;
    lda = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=genres_count, id2word = common_corpus, passes=150, callbacks=[l])

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldamodel.py"", line 371, in __init__
    self.update(corpus, chunks_as_numpy=use_numpy)

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldamodel.py"", line 688, in update
    callback.set_model(self)

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\callbacks.py"", line 264, in set_model
    if any(metric.logger == ""visdom"" for metric in self.metrics):

  File ""C:\Users\me\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\callbacks.py"", line 264, in &lt;genexpr&gt;
    if any(metric.logger == ""visdom"" for metric in self.metrics):

AttributeError: 'EpochLogger' object has no attribute 'logger'
</code></pre>

<p>Currently I'm mostly interested in monitoring learning progress (to eyeball ETA).</p>

<p>What would be the proper way of setting a callback?</p>
","672018","","","","","2019-08-26 20:46:28","How to log epochs in Gensim's LdaModel","<python><python-3.x><gensim>","1","1","","","","CC BY-SA 4.0"
"66096149","1","","","2021-02-08 05:02:25","","3","3229","<pre><code>import pyLDAvis.gensim
# Visualize the topics
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
vis
</code></pre>
<p>The above code displayed the visualization of LDA model in google colab but then after reopening the notebook it stopped displaying.
I even tried
<code>pyLDAvis.display(vis, template_type='notebook')</code>
still not working</p>
<p>When I set</p>
<p><code>pyLDAvis.enable_notebook(local=True)</code></p>
<p><a href=""https://i.stack.imgur.com/5G0es.png"" rel=""nofollow noreferrer"">enter image description here</a>
it does display the result but not the labels.. Any help would be appreciated!!</p>
","15166550","","","","","2021-08-21 10:37:14","pyLDAvis visualization from gensim not displaying the result in google colab","<visualization><gensim><lda><pyldavis>","2","0","","","","CC BY-SA 4.0"
"57189149","1","57254000","","2019-07-24 18:26:00","","0","85","<p>I try to compare texts in form of 'text-files' concerning their content.<br>
<strong>e.g.</strong>: I got 100 texts about animals and I want to analyze each text about what animals it discusses.<br>
I am looking for an analysis output like: <code>doc1: 60% cats, 10% rabbits, 10% dogs, 0% elephants, 20% else"", ""doc2: 0% cats, 10% rabbits, 40% dogs, ...</code></p>

<p>I have read a lot about Latent Dirichlet Allocation (and the word-probabilities for each topic) for Text Classification but a completely unsupervised approach seemed not to fit my set of documents.</p>

<p>Trying to implement the LDA-Stuff in Python I understood to prepare the data (tokenizing, lemmatizing/stemming) but I don't get the next steps. Do I have to generate training data for each topic (animal) and how could I implement this? </p>

<p>Also I've seen a tutorial manipulating the topics via the <code>eta-value</code> in <code>gensim</code> but I don't know how I could use this in my favor. </p>

<p>I am grateful for any advice that can lead me to the right direction. Thanks!</p>
","11823461","","8796240","","2019-07-24 20:22:56","2019-07-29 12:34:35","How to Implement a (statistical) Thematic Comparison of Texts via Text-Mining?","<python><text-mining><gensim><text-classification><lda>","1","2","","","","CC BY-SA 4.0"
"31507399","1","","","2015-07-20 00:30:45","","1","8640","<p>I try to use word2vec, but it gives an error when trying to do anything with any word. It seems to be an encoding issue, here is what I did: </p>

<h3>Init word2vec:</h3>

<pre><code>import gensim, logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

model = gensim.models.Word2Vec.load_word2vec_format('freebase-vectors-skipgram1000/knowledge-vectors-skipgram1000.bin', binary=True)
model.init_sims(replace=True)
</code></pre>

<h3>Test it a bit:</h3>

<pre><code>print(model)  
# prints: Word2Vec(vocab=1422903, size=1000, alpha=0.025)

print(model.index2word[0])  
# prints: u'/m/0dgps15'
# I would expect a readable word, how to fix that?
</code></pre>

<h3>The error:</h3>

<pre><code>print(model.similarity('word', 'sound'))
# An error happen: KeyError: 'word'
</code></pre>

<p>I also tried to load the model with <code>binary=False</code>, but this makes an error while loading. </p>
","2476920","","2476920","","2015-07-20 00:46:56","2016-06-20 09:52:11","Setting up word2vec - KeyError: ""word 'word' not in vocabulary""","<python><character-encoding><gensim><word2vec>","1","1","","","","CC BY-SA 3.0"
"47148615","1","","","2017-11-07 01:36:06","","0","674","<p>This is my first time using Doc2Vec
I'm trying to classify works of an author. I have trained a model with Labeled Sentences (paragraphs, or strings of specified length), with words = the list of words in the paragraph, and tags = author's name. In my case I only have two authors.
I tried accessing the docvecs attribute from the trained model but it only contains two elements, corresponding to the two tags I have when I trained the model. I'm trying to get the doc2vec numpy representations of each paragraph I fed in to the training so I can use that as training data later on. How can I do this?
Thanks.</p>
","6410915","","","","","2017-11-08 05:37:06","Getting numpy vector from a trained Doc2Vec model for each document","<python-3.x><nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"45346233","1","45357454","","2017-07-27 09:10:01","","1","337","<p>I wonder how the <a href=""https://radimrehurek.com/gensim/tut3.html"" rel=""nofollow noreferrer"">similarity</a> works with gensim ? How the different shards are created and does it increase performance when looking only for top-N similar document ? More generally, is there a documentation about the internal structures of gensim ?</p>
","6084245","","4542815","","2017-07-27 12:56:46","2017-07-27 17:32:34","Gensim's similarity: how does it work?","<python><nlp><gensim>","1","0","1","","","CC BY-SA 3.0"
"48402364","1","","","2018-01-23 12:57:48","","1","700","<p>I am trying to extract the documents vector to feed into a regression model for prediction.</p>

<p>I have fed around 1 400 000 of labelled sentences into doc2vec for training, however I was only able to retrieve only 10 vectors using model.docvecs.</p>

<p>This is a snapshot of the labelled sentences I used to trained the doc2vec model:</p>

<pre><code>In : documents[0]

Out: TaggedDocument(words=['descript', 'yet'], tags='0')

In : documents[-1]

Out: TaggedDocument(words=['new', 'tag', 'red', 'sparkl', 'firm', 'price', 'free', 'ship'], tags='1482534')
</code></pre>

<p>These are the code used to train the doc2vec model</p>

<pre><code>model = gensim.models.Doc2Vec(min_count=1, window=5, size=100, sample=1e-4, negative=5, workers=4)
model.build_vocab(documents)
model.train(documents, total_examples =len(documents), epochs=1)
</code></pre>

<p>This is the dimension of the documents vectors:</p>

<pre><code>In : model.docvecs.doctag_syn0.shape
Out: (10, 100)
</code></pre>

<p>On which part of the code did I mess up?</p>

<p><strong>Update:</strong></p>

<p>Adding on to the comment from <a href=""https://stackoverflow.com/users/6573902/sophros"">sophros</a>, it appear that i have made a mistake when I am creating the TaggedDocument prior to training which resulted in 1.4 mil Documents appearing as 10 Documents.</p>

<p>Courtesy of <a href=""https://ireneli.eu/2016/07/27/nlp-05-from-word2vec-to-doc2vec-a-simple-example-with-gensim/"" rel=""nofollow noreferrer"">Irene Li</a> on your tutorial on Doc2vec, I have made some slightly edit to the class she used to generate TaggedDocument</p>

<pre><code>def get_doc(data):

tokenizer = RegexpTokenizer(r'\w+')
en_stop = stopwords.words('english')
p_stemmer = PorterStemmer()

taggeddoc = []

texts = []
for index,i in enumerate(data):
    # for tagged doc
    wordslist = []
    tagslist = []
    i = str(i)
    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)
    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]
    # remove numbers
    number_tokens = [re.sub(r'[\d]', ' ', i) for i in stopped_tokens]
    number_tokens = ' '.join(number_tokens).split()
    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in number_tokens]
    # remove empty
    length_tokens = [i for i in stemmed_tokens if len(i) &gt; 1]
    # add tokens to list
    texts.append(length_tokens)

    td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(stemmed_tokens))).split(),str(index))

    taggeddoc.append(td)

return taggeddoc
</code></pre>

<p>The mistake was fixed when I made the change from </p>

<pre><code>td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(stemmed_tokens))).split(),str(index))
</code></pre>

<p>to this</p>

<pre><code>td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(stemmed_tokens))).split(),[str(index)])
</code></pre>

<p>It appear that the index of the TaggedDocument must be in the form of the list for TaggedDocument to work properly. For more details as to why, please refer to this answer by <a href=""https://stackoverflow.com/questions/47929028/doc2vec-model-docvecs-is-only-of-length-10"">gojomo</a>.</p>
","9256811","","9256811","","2018-01-24 06:38:06","2018-01-25 00:31:16","Extracting vectors from Doc2Vec","<gensim><doc2vec>","1","1","","","","CC BY-SA 3.0"
"62802812","1","62803006","","2020-07-08 20:05:57","","1","140","<p>I am trying to remove stopwords during an NLP pre-processing step. I use the <code>remove_stopwords()</code> function from <code>gensim</code> but would also like to add my own stopwords</p>
<pre><code># under this method, these custom stopwords still show up after processing
custom_stops = [&quot;stopword1&quot;, &quot;stopword2&quot;]
data_text['text'].apply(lambda x: [item for item in x if item not in custom_stops])
# remove stopwords with gensim
data_text['filtered_text'] = data_text['text'].apply(lambda x: remove_stopwords(x.lower()))
# split the sentences into a list
data_text['filtered_text'] = data_text['filtered_text'].apply(lambda x: str.split(x))
</code></pre>
","4907639","","","","","2020-07-08 20:19:22","Remove custom stopwords","<python><nlp><gensim>","2","0","","","","CC BY-SA 4.0"
"31166218","1","31168576","","2015-07-01 15:56:07","","1","590","<p>I am trying to use freebase along with gensim's word2vec to find similarity score between vectors of two word using following code.</p>

<pre><code>model = gensim.models.Word2Vec()
model = models.Word2Vec.load_word2vec_format('freebase-vectors-skipgram1000-en.bin.gz', binary=True)
</code></pre>

<p>after creating a model based on freebase my code is giving me key error for any word.</p>

<pre><code>model.similarity('microsoft', 'apple')
</code></pre>

<p>This is giving me <code>KeyError: 'microsoft'</code></p>

<p>But when I use googlenews instead of freebase it works fine. Any idea why?</p>
","2595210","","","","","2015-07-01 19:33:54","Unable to find words when using freebase with word2vec","<python-2.7><freebase><gensim><google-news>","2","0","","","","CC BY-SA 3.0"
"48362530","1","","","2018-01-21 00:31:48","","12","4165","<p>I am trying to follow the official Doc2Vec Gensim tutorial mentioned here - <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a></p>

<p>I modified the code in line 10 to determine best matching document for the given query and everytime I run, I get a completely different resultset. My new code iin line 10 of the notebook is:</p>

<p><code>
inferred_vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])
sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
rank = [docid for docid, sim in sims]
print(rank)
</code></p>

<p>Everytime I run the piece of code, I get different set of documents that are matching with this query: ""only you can prevent forest fires"". The difference is stark and just does not seem to match.</p>

<p>Is Doc2Vec not a suitable match for querying and information extraction? Or are there bugs?</p>
","2945609","","","","","2019-11-21 04:10:08","Doc2Vec.infer_vector keeps giving different result everytime on a particular trained model","<nlp><word2vec><gensim><doc2vec>","2","1","","","","CC BY-SA 3.0"
"55692700","1","55695487","","2019-04-15 15:36:47","","0","669","<p>Is it possible to apply a sentence-level LDA model using Gensim as proposed in Bao and Datta(2014)?  The paper <a href=""https://pubsonline.informs.org/doi/pdf/10.1287/mnsc.2014.1930"" rel=""nofollow noreferrer"">is here</a>.  </p>

<p>The distinct feature is that it makes the ""one topic per sentence assumption"" (p.1376). This is different from other sentence-level methods, which typically allow each sentence to include multiple topics. ""The most straightforward method is to treat each sentence as a document and apply the LDA model on the collection of sentences rather than documents."" (P.1376). But, I think it is more reasonable to assume that one sentence deals with one topic. </p>

<p>Thank you!</p>
","9108781","","6832816","","2019-04-15 19:36:21","2021-07-23 06:07:31","How to apply a sentence-level LDA model using Gensim?","<python><nlp><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"47155414","1","47172207","","2017-11-07 10:21:23","","1","1390","<p>I obtain word vectors from my code. e.g., </p>

<pre><code>array([ -3.09521449e-04,   2.73033947e-06,   2.15601496e-04, ...,
         5.12349070e-04,   5.04256517e-04,   8.16784304e-05], dtype=float32)
</code></pre>

<p>Now, I want to identify what is the word that represents this word vector in wor2vec genism.</p>

<p>I tried it using the below code. However it did not work.</p>

<pre><code>print(model.wv.index2word(kmeans_clustering.cluster_centers_))
</code></pre>

<p>Please help me.</p>
","","user8871463","","","","2017-11-08 05:32:32","Given a word vector get the word of it in word2vec","<python><word2vec><gensim><word-embedding><doc2vec>","1","4","1","","","CC BY-SA 3.0"
"55693826","1","55700805","","2019-04-15 16:45:59","","0","822","<p>I pretrained a word embedding using wang2vec (<a href=""https://github.com/wlin12/wang2vec"" rel=""nofollow noreferrer"">https://github.com/wlin12/wang2vec</a>), and i loaded it in python through gensim. When i tried to get the vector of some words not in vocabulary, i obviously get:</p>

<pre class=""lang-py prettyprint-override""><code>KeyError: ""word 'kjklk' not in vocabulary""
</code></pre>

<p>So, i thought about adding an item to the vocabulary to map oov (out of vocabulary) words, let's say <code>&lt;OOV&gt;</code>. Since the vocabulary is in <code>Dict</code> format, i would simply add the item <code>{""&lt;OOV&gt;"":0}</code>. </p>

<p>But, i searched an item of the vocabulary, with</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.KeyedVectors.load_word2vec_format(w2v_ext, binary=False, unicode_errors='ignore')
dict(list(model.vocab.items())[5:6])
</code></pre>

<p>The output was something like</p>

<pre class=""lang-py prettyprint-override""><code>{'word': &lt;gensim.models.keyedvectors.Vocab at 0x7fc5aa6007b8&gt;}
</code></pre>

<p>So, is there a way to add the <code>&lt;OOV&gt;</code> token to the vocabulary of a pretrained word embedding loaded through gensim, and avoid the KeyError? I looked at gensim doc and i found this: <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.build_vocab"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.build_vocab</a>
but it seems not work with the update parameter.</p>
","7781936","","7781936","","2019-04-15 18:25:40","2019-04-16 04:57:35","Manage KeyError with gensim and pretrained word2vec model","<python><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"62806942","1","","","2020-07-09 03:29:56","","1","91","<p>Below is the code, I am executing. The error occurs on the 3rd line (<code>vectors.init_sims(True)</code>)</p>
<pre><code>fname = get_tmpfile(path_to_embedding_file)
vectors = KeyedVectors.load(fname, mmap='r')

vectors.init_sims(True)
</code></pre>
<p>This is the error stack-</p>
<pre><code>Traceback (most recent call last):
  File &quot;generate_pair_histograms.py&quot;, line 82, in &lt;module&gt;
    vectors.init_sims(True) # normalize the vectors (!), so we can use the dot product as similarity measure
  File &quot;C:\users\janki\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 1354, in init_sims
    self.vectors_norm = _l2_norm(self.vectors, replace=replace)
  File &quot;C:\users\janki\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 2374, in _l2_norm
    m /= dist
ValueError: output array is read-only
</code></pre>
<p>Is this a known issue? Can someone provide a solution or workaround to this?</p>
","11631110","","","","","2020-07-09 17:40:24","Setting gensim wordVectors init_sim property as True error - ValueError: output array is read-only","<nlp><gensim><word-embedding>","1","0","","","","CC BY-SA 4.0"
"48443999","1","","","2018-01-25 13:28:06","","6","4330","<p>I am trying to continue training on an existing model,</p>

<pre><code>model = gensim.models.Word2Vec.load('model/corpus.zhwiki.word.model')
more_sentences = [['Advanced', 'users', 'can', 'load', 'a', 'model', 'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']]    
model.build_vocab(more_sentences, update=True)
model.train(more_sentences, total_examples=model.corpus_count, epochs=model.iter)
</code></pre>

<p>but I got an error with the last line:</p>

<p>AttributeError: 'Word2Vec' object has no attribute 'compute_loss'</p>

<p>Some posts said it's caused by using a earlier version of gensim, and I have tried to add this after loading the existing model and before train(). </p>

<pre><code>model.compute_loss = False
</code></pre>

<p>After that, it didn't give me the AttributeError, but the output of model.train() is 0, and model didn't trained with new sentences.</p>

<p><a href=""https://i.stack.imgur.com/ECtw0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ECtw0.png"" alt=""enter image description here""></a></p>

<p>How to solve this problem?</p>
","6166551","","446477","","2019-10-24 13:23:11","2019-10-24 13:23:11","gensim - Word2vec continue training on existing model - AttributeError: 'Word2Vec' object has no attribute 'compute_loss'","<python><nlp><word2vec><gensim>","2","0","3","","","CC BY-SA 4.0"
"45352522","1","45376287","","2017-07-27 13:39:51","","1","850","<p>I am trying to install gensim in Python on my Ubuntu. I tried with  easy_install but getting errors. Could someone help with identifying what is going wrong?</p>

<p><strong>easy_install</strong></p>

<pre><code>easy_install -U gensim

Running scipy-0.19.1/setup.py -q bdist_egg --dist-dir /tmp/easy_install-    QXO1dA/scipy-0.19.1/egg-dist-tmp-AxijnA
/tmp/easy_install-QXO1dA/scipy-0.19.1/setup.py:323: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates 

warnings.warn(""Unrecognized setuptools command, proceeding with ""                                                                 /usr/local/lib/python2.7/dist-packages/numpy/distutils/system_info.py:572: UserWarning: Atlas (http://math-atlas.sourceforge.net/) libraries not found.                                                                     
Directories to search for the libraries can be specified in the                                                                     numpy/distutils/site.cfg file (section [atlas]) or by setting                                                                       the ATLAS environment variable.    

self.calc_info()                                                                                                                    
/usr/local/lib/python2.7/dist-packages/numpy/distutils/system_info.py:572: UserWarning:                                                 
Lapack (http://www.netlib.org/lapack/) libraries not found.                                                                          
Directories to search for the libraries can be specified in the                                                                     
numpy/distutils/site.cfg file (section [lapack]) or by setting                                                                      
the LAPACK environment variable.                                                                                                  
self.calc_info()                                                                                                                  
/usr/local/lib/python2.7/dist-packages/numpy/distutils
/system_info.py:572: UserWarning:                                                   
Lapack (http://www.netlib.org/lapack/) sources not found.                                                                           
Directories to search for the sources can be specified in the                                                                       
numpy/distutils/site.cfg file (section [lapack_src]) or by setting                                                                  
the LAPACK_SRC environment variable.                                                                                              
self.calc_info()                                                                                                                  
Running from scipy source directory.                                                                                                
non-existing path in 'scipy/integrate': 'quadpack.h'                                                                                
Warning: Can't read registry to find the necessary compiler setting                                                                 
Make sure that Python modules _winreg, win32api or win32con are  installed.                                                          
error: no lapack/blas resources found
</code></pre>

<p>Thank you</p>
","1002903","","1002903","","2017-07-27 15:52:51","2017-07-28 14:40:22","Issues installing gensim on Ubuntu","<python><python-2.7><gensim>","1","1","","","","CC BY-SA 3.0"
"48444393","1","48514018","","2018-01-25 13:45:10","","0","144","<p>I recently came across following problem: When applying a topic model on a bunch of parsed PDF files, I discovered that content of the references unfortunately also counts for the model. I.e. words within the references appear in the tokenized list of words.</p>

<p><strong>Is there any known ""best-practice"" to solve this problem?</strong></p>

<p>I thought about a search strategy where the python code automatically removes all content after the last mention of ""references"" or ""bibliography"". If I would go by the first, or a random mention of ""references"" or ""bibliography"" within the full text, the parser might not capture the true full content.</p>

<p>The input PDF are all from different journals and thus have a different page structure.</p>
","4697646","","","","","2018-01-30 04:42:47","NLP Challenge: Automatically removing bibliography/references?","<nlp><gensim><topic-modeling>","2","0","","","","CC BY-SA 3.0"
"48403942","1","48419778","","2018-01-23 14:18:59","","0","228","<p>I am trying to create a gensim corpus and save it to arbitrary HDFS or regular FS path. I am using pyspark (2.2.1) and running a zeppelin notebook on a hadoop cluster. Here is my minimal example:</p>

<pre><code>from gensim import corpora
import os

path = ""/my/existing/hadoop/path""
corpus = [[(0,0), (1,2)]]
corpora.MmCorpus.serialize(os.path.join(path,""corpus.mm""), corpus)
</code></pre>

<p>This leads to error:</p>

<pre><code>[Errno 2] No such file or directory: '/my/existing/hadoop/path/corpus.mm' 
</code></pre>

<p>Although the path exists.</p>

<p>Running the following works.  </p>

<pre><code>corpora.MmCorpus.serialize(""corpus.mm"", corpus)
corpora.MmCorpus.serialize(os.path.join(""/tmp"",""corpus.mm""), corpus)
</code></pre>

<p>However, I can't find it. I checked <code>/tmp</code> and <code>hadoop fs -ls /tmp</code>
What kind of path is required when working with pyspark? </p>
","4842467","","4842467","","2018-01-24 09:05:34","2018-01-24 13:55:53","How to serialize gensim corpus in pyspark using apache-zeppelin notebook?","<hadoop><serialization><pyspark><gensim><apache-zeppelin>","1","0","","","","CC BY-SA 3.0"
"31728460","1","","","2015-07-30 15:48:51","","0","926","<p>I have a <code>Doc2Vec's</code> model and I want to create <code>Word2vec's</code> model with different dimension. How can I use Doc2Vec's model <code>vocab</code> for fast training? Or is it <code>feasible</code> to train like this? Does <code>vocab building</code> has any effect on <code>train</code>?</p>
","1215889","","1215889","","2015-07-30 16:00:45","2015-07-31 05:08:51","How to use Word2Vec's vocab of one model into another?","<python><deep-learning><gensim>","1","0","","","","CC BY-SA 3.0"
"66097756","1","","","2021-02-08 07:58:41","","0","123","<p>I am trying to find words that are similar to two different words. I know that I can find the most similar word with FastText but I was wondering if there is a way to find a keyword that is similar to two keywords. For example, &quot;apple&quot; is similar to &quot;orange&quot; and also similar to &quot;kiwi&quot;. So, what I want to do is if I have two words, &quot;organ&quot; and &quot;kiwi&quot;, then I would like to get a suggestion of the keyword &quot;apple&quot; or any other fruits. Is there a way to do this?</p>
","14625519","","","","","2021-02-09 08:11:57","How to find word that are similar to two keywords using FastText?","<nlp><gensim><word2vec><fasttext><conceptnet>","2","2","","","","CC BY-SA 4.0"
"57412511","1","57418446","","2019-08-08 12:23:40","","0","1076","<p>I was trying to use Gensim to import GoogelNews-pretrained model on some English words (sampled 15 ones here only stored in a txt file with each per line, and there are no more context as corpus). Then I could use ""model.most_similar()"" to get their similar words/phrases for them. But actually the file loaded from Python-Pickle method couldn't be used for gensim-built-in <code>model.load()</code> and <code>model.most_similar()</code> function directly. </p>

<p>how should I do to cluster the 15 English words (and more in the future), since I couldn't train and save and load a model  from the beginning?</p>

<pre><code>import gensim
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

GOOGLE_WORD2VEC_MODEL = '../GoogleNews-vectors-negative300.bin'

GOOGLE_ENGLISH_WORD_PATH = '../testwords.txt'

GOOGLE_WORD_FEATURE = '../word.google.vector'

model = gensim.models.KeyedVectors.load_word2vec_format(GOOGLE_WORD2VEC_MODEL, binary=True) 

word_vectors = {}

#load 15 words as a test to word_vectors

with open(GOOGLE_ENGLISH_WORD_PATH) as f:
    lines = f.readlines()
    for line in lines:
        line = line.strip('\n')
        if line:                
            word = line
            print(line)
            word_vectors[word]=None
try:
    import cPickle
except :
    import _pickle as cPickle

def save_model(clf,modelpath): 
    with open(modelpath, 'wb') as f: 
        cPickle.dump(clf, f) 

def load_model(modelpath): 
    try: 
        with open(modelpath, 'rb') as f: 
            rf = cPickle.load(f) 
            return rf 
    except Exception as e:        
        return None 

for word in word_vectors:
    try:
        v= model[word]
        word_vectors[word] = v
    except:
        pass

save_model(word_vectors,GOOGLE_WORD_FEATURE)

words_set = load_model(GOOGLE_WORD_FEATURE)

words_set.most_similar(""knit"", topn=3)
</code></pre>

<blockquote>
<pre><code>---------------error message--------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-8-86c15e366696&gt; in &lt;module&gt;
----&gt; 1 words_set.most_similar(""knit"", topn=3)

AttributeError: 'dict' object has no attribute 'most_similar'
---------------error message--------
</code></pre>
</blockquote>
","11366081","","11027506","","2019-08-08 14:28:21","2019-08-08 18:06:42","Gensim built-in model.load function and Python Pickle.load file","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"39890544","1","","","2016-10-06 07:54:48","","2","272","<p>The <code>gensim</code> <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""nofollow"">Dictionary</a> object keeps track of the vocabulary of the collection of documents (aka corpus). But to feed the data into the object, the data has to be fed into the memory, e.g.</p>

<pre><code>import io
from gensim.corpora import Dictionary

infile = '/path/to/data'

with io.open(infile, 'r', encoding='utf8') as fin:
    d = Dictionary(map(lambda x: x.split(), fin.readlines()))
    d.save('data.dict')
</code></pre>

<p><strong>Can I read a file object into a gensim Dictionary class?</strong></p>
","610569","","","","","2016-10-06 07:54:48","Can I read a file object into a gensim Dictionary class?","<python><dictionary><nlp><gensim><corpus>","0","0","","","","CC BY-SA 3.0"
"40181943","1","43424280","","2016-10-21 16:59:07","","1","1241","<p>I am trying to get the keys as well as the vectors in the vector <code>model.syn0</code> which gives vectors by <code>model.syn0[""word""]</code> which gives an n-dim vector. Is there a better way to create a list of all the words in the model in the same order as the the vectors of <code>syn0</code> than this? I have 350000 words and this would take too long.</p>

<pre><code>from gensim.models import word2vec as wv
model = wv.Word2Vec.load('model')
lab=[]
for i in model.syn0:
    lab.append(model.similar_by_vector(i)[0])

print(type(model.syn0))
    &lt;type 'numpy.ndarray'&gt;
</code></pre>
","5709404","","5709404","","2016-10-23 16:10:45","2017-04-15 09:13:11","How to get the key value pairs in numpy.ndarray? (Gensim Word2vec)","<python><performance><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"45424420","1","","","2017-07-31 20:04:41","","1","213","<p>Related to: <a href=""https://stackoverflow.com/questions/28488714/getting-string-version-of-document-by-id-in-gensim"">Getting string version of document by id in Gensim</a></p>

<p>I am implementing <code>gensim</code> for document similarity. After retrieving the <code>gensim</code> Similarity object index and similarity score for a new document, I would like to trace that index back to the original document in the corpus. Based on the answer in the question above, it looks like there is no built-in way to achieve this. </p>

<p>The df that feeds into my corpus looks like this:</p>

<pre><code>id    text
1-23  She loathes apples
1-52  I like rocks
1-43  You like ice cream
1-67  He hates bananas
</code></pre>

<p>After building the <code>gensim</code> model (tweaking it for dataframes from this tutorial: <a href=""https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python"" rel=""nofollow noreferrer"">https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python</a>), I run the following new document through:</p>

<pre><code>id    text
1-98  I like neutrons
</code></pre>

<p>I can get the max similarity score and the index of the Similarity object with this function:</p>

<pre><code>def search(row):
    query = [w.lower() for w in word_tokenize(row)]
    queryBOW = dictionary.doc2bow(query)
    queryTFIDF = tfidf[queryBOW]
    maxSim = max(sims[queryTFIDF])
    index = [i for i, j in enumerate(sims[queryTFIDF]) if j == maxSim]
    return maxSim, index
</code></pre>

<p>After unpacking the tuple into separate df columns:</p>

<pre><code>id    text              maxSim    index
1-98  I like neutrons   0.5678    4
</code></pre>

<p>The index of the Similarity object does not match the index of the original dataframe.</p>

<p>How would I carry the original dataframe index into the similarity object? Or, how would I carry the <code>id</code> field from the original dataframe object into the Similarity object? I understand I have to keep a separate mapping between Similarity object index and original dataframe index - what does that mapping look like?</p>

<p>Desired result:</p>

<pre><code>id    text              maxSim    index   originalTextID    originalText
1-98  I like neutrons   0.5678    4       1-52              I like rocks    
</code></pre>
","7668467","","7668467","","2017-07-31 20:19:49","2017-07-31 20:19:49","Maintain DataFrame Index Through Gensim","<python><python-3.x><pandas><indexing><gensim>","0","0","","","","CC BY-SA 3.0"
"45427552","1","","","2017-08-01 01:22:30","","0","923","<p>Hi I have files present in bbc folder like this
<a href=""https://i.stack.imgur.com/TVEiB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TVEiB.png"" alt=""Files present in parent folder""></a></p>

<p>Each sub folder inside bbc folder contains text files
<a href=""https://i.stack.imgur.com/beygd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/beygd.png"" alt=""Text files in sub folders""></a></p>

<p>This code helps accessing files inside the folder</p>

<pre><code>class MySentences(object):
def __init__(self, dirname):
    self.dirname = dirname

def __iter__(self):
    for fname in os.listdir(self.dirname):
        for line in open(os.path.join(self.dirname, fname)):
            yield line.split()
sentences = MySentences('C:/Users/JAYASHREE/Documents/NLP/bbc-fulltext/bbc/business')
</code></pre>

<p>But I want to access files from each subfolder.I am getting following error when I do this </p>

<pre><code>sentences = MySentences('C:/Users/JAYASHREE/Documents/NLP/bbc-fulltext/bbc')
IOError                                   Traceback (most recent call last)
&lt;ipython-input-29-26fb31de4fec&gt; in &lt;module&gt;()
      1 sentences = MySentences('C:/Users/JAYASHREE/Documents/NLP/bbc-fulltext/bbc') # a memory-friendly iterator
----&gt; 2 model = gensim.models.Word2Vec(sentences)

C:\Users\JAYASHREE\Anaconda2\lib\site-packages\gensim-2.3.0-py2.7-win-amd64.egg\gensim\models\word2vec.pyc in __init__(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss)
    501             if isinstance(sentences, GeneratorType):
    502                 raise TypeError(""You can't pass a generator as the sentences argument. Try an iterator."")
--&gt; 503             self.build_vocab(sentences, trim_rule=trim_rule)
    504             self.train(sentences, total_examples=self.corpus_count, epochs=self.iter,
    505                        start_alpha=self.alpha, end_alpha=self.min_alpha)

C:\Users\JAYASHREE\Anaconda2\lib\site-packages\gensim-2.3.0-py2.7-win-amd64.egg\gensim\models\word2vec.pyc in build_vocab(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)
    575 
    576         """"""
--&gt; 577         self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
    578         self.scale_vocab(keep_raw_vocab=keep_raw_vocab, trim_rule=trim_rule, update=update)  # trim by min_count &amp; precalculate downsampling
    579         self.finalize_vocab(update=update)  # build tables &amp; arrays

C:\Users\JAYASHREE\Anaconda2\lib\site-packages\gensim-2.3.0-py2.7-win-amd64.egg\gensim\models\word2vec.pyc in scan_vocab(self, sentences, progress_per, trim_rule)
    587         vocab = defaultdict(int)
    588         checked_string_types = 0
--&gt; 589         for sentence_no, sentence in enumerate(sentences):
    590             if not checked_string_types:
    591                 if isinstance(sentence, string_types):

&lt;ipython-input-28-48533b12127a&gt; in __iter__(self)
      5     def __iter__(self):
      6         for fname in os.listdir(self.dirname):
----&gt; 7             for line in open(os.path.join(self.dirname, fname)):
      8                 yield line.split()

IOError: [Errno 13] Permission denied: 'C:/Users/JAYASHREE/Documents/NLP/bbc-fulltext/bbc\\business'
</code></pre>

<p>Kindly suggest me changes in the code</p>
","7634981","","","","","2017-08-01 18:07:56","Read files present in subfolder","<python><gensim><word2vec>","1","1","","","","CC BY-SA 3.0"
"12713797","1","","","2012-10-03 17:32:57","","3","1285","<p>I am looking to compute similarities between users and text documents using their topic representations. I.e. each document and user is represented by a vector of topics (e.g. Neuroscience, Technology, etc) and how relevant that topic is to the user/document.</p>

<p>My goal is then to compute the similarity between these vectors, so that I can find similar users, articles and recommended articles.</p>

<p>I have tried to use Pearson Correlation but it ends up taking too much memory and time once it reaches ~40k articles and the vectors' length is around 10k.</p>

<p>I am using numpy.</p>

<p>Can you imagine a better way to do this? or is it inevitable (on a single machine)?</p>

<p>Thank you</p>
","1491915","","1491915","","2012-10-05 10:29:10","2013-02-25 12:18:03","Topic-based text and user similarity","<python><numpy><recommendation-engine><topic-modeling><gensim>","3","0","1","","","CC BY-SA 3.0"
"31742630","1","","","2015-07-31 09:24:14","","2","3505","<p>I post my question here because there are already some answers on how to use scikit methods with gensim like <a href=""https://stackoverflow.com/questions/19504898/use-sikit-tfidf-with-gensim-lda"">scikit vectorizers with gensim</a> or <a href=""https://stackoverflow.com/questions/15670525/how-do-you-initialize-a-gensim-corpus-variable-with-a-csr-matrix?rq=1"">this</a> but I haven't seen the whole pipeline to be used for text classification. I will try to explain a little bit my situation</p>

<p>I want to use gensim LDA implemented methods in order to proceed further to text classification. I have one dataset which is consisted from three parts(train(25K), test(25K) and unlabeled data(50K)). What I am trying to do is to learn the latent topics space using the unlabeled data and then transform the train and test set into this learned latent topic space. I am currently using the Scikit Learn implemented methods in order to extract the BoW representation. Later, I am transforming to the required inputs for the LDA implementation and at the end I am transforming the train and test set into the extracted latent topic space. Finally, I am going back to csr matrices in order to fit a classifier and to obtain the accuracy. <strong>Although, it seems to me that everything is fine, the performance of the classifier is almost 0%. I am attaching part of the code in order to get some additional help or if there is something obvious that I am currently missing.</strong></p>

<pre><code>#bow representations for the three sets unlabelled, train and test
vectorizer = CountVectorizer(max_features=3000,stop_words='english')


corpus_tfidf_unsuper = vectorizer.fit_transform(train_data_unsupervised[:,2])
corpus_tfidf_train = vectorizer.transform(train_ds[:,2])
corpus_tfidf_test= vectorizer.transform(test_ds[:,2])

#transform to gensim acceptable objects
vocab = vectorizer.get_feature_names()
id2word_unsuper=dict([(i, s) for i, s in enumerate(vocab)])
corpus_vect_gensim_unsuper = matutils.Sparse2Corpus(corpus_tfidf_unsuper.T)
corpus_vect_gensim_train = matutils.Sparse2Corpus(corpus_tfidf_train.T)
corpus_vect_gensim_test = matutils.Sparse2Corpus(corpus_tfidf_test.T)

#fit the model to the unlabelled data
lda = models.LdaModel(corpus_vect_gensim_unsuper, 
                  id2word = id2word_unsuper, 
                  num_topics = 10,
                  passes=1)
#transform the train and test set to the latent topic space
docTopicProbMat_train = lda[corpus_vect_gensim_train]
docTopicProbMat_test = lda[corpus_vect_gensim_test]
#transform to csr matrices
train_lda=matutils.corpus2csc(docTopicProbMat_train)
test_lda=matutils.corpus2csc(docTopicProbMat_test)
#fit the classifier and print the accuracy
clf =LogisticRegression()    
clf.fit(train_lda.transpose(), np.array(train_ds[:,0]).astype(int))     
ypred = clf.predict(test_lda.transpose())
print accuracy_score(test_ds[:,0].astype(int), ypred)
</code></pre>

<p>This is my first post, so if there are potential remarks, please do not hesitate to inform me.</p>
","5177161","","-1","","2017-05-23 11:45:52","2017-01-27 12:29:19","Gensim LDA for text classification","<python><scikit-learn><lda><topic-modeling><gensim>","0","8","","","","CC BY-SA 3.0"
"66112103","1","","","2021-02-09 02:22:09","","0","65","<p>I'm having a problem with Gensim's LSI Model. <br>
After pre-processing my text, I ended up with a few empty strings. <br>
I switched those empty strings into special tags: '@' <br>
Then I trained my LSI model with text containing several of those special tags as well as other text. <br>
However when <code>vec_bow = self.dictionary.doc2bow(self.processed[input_index])</code> and <code>vec_lsi = self.model[vec_bow]</code> were performed, <code>vec_lsi</code> ended up as an empty list. <br>
<br>
Here is the code for better understanding.</p>
<pre><code>def model_setup(self, train):
    self.dictionary = corpora.Dictionary(self.processed) #self.processed is the pre-processed text
    self.corpus = [self.dictionary.doc2bow(
        text) for text in self.processed] #using bag of words to create corpus

    if train:
        self.model = models.LsiModel(
            self.corpus, id2word=self.dictionary, num_topics=self.num_topics)
        self.model.save(&quot;eq_lsi.model&quot;)
    self.model = models.LsiModel.load(&quot;eq_lsi.model&quot;)

def getResult(self, input_index, n_top):
    vec_bow = self.dictionary.doc2bow(self.processed[input_index])
    print(vec_bow)
        
    print(vec_lsi)
    index = similarities.MatrixSimilarity(self.model[self.corpus])
    sims = index[vec_lsi]

    sims = sorted(enumerate(sims), key=lambda item: -item[1])
    print(sims)
</code></pre>
<p>for the case of '@' which is input index 1, <code>getResult(1, 3)</code> yields the following outputs
<br>
for print(vec_bow)</p>
<pre><code>[(3, 1)]
</code></pre>
<p>for print(vec_lsi)</p>
<pre><code>[]
</code></pre>
<p>for print(sims)</p>
<pre><code>[(0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.0), (12, 0.0), (13, 0.0), (14, 0.0), (15, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.0), (40, 0.0)]
</code></pre>
<p>(cf) I'm using a 5 topic LSI model.
<br>
<b> How can I get a non-empty list from vec_lsi? or is it impossible with this setup? </b></p>
","11335607","","","","","2021-02-09 02:22:09","Gensim LSI (Latent Semantic Indexing) Model Returning Empty Outputs","<python><nlp><gensim><similarity>","0","0","","","","CC BY-SA 4.0"
"45404027","1","45520833","","2017-07-30 19:48:11","","0","575","<p>So I am trying to run the demo from gensim for distributed LSI (You can find it <a href=""https://radimrehurek.com/gensim/dist_lsi.html"" rel=""nofollow noreferrer"">here</a>) Yet whenever I run the code I get the error</p>

<p><code>AttributeError: module 'Pyro4' has no attribute 'expose'</code></p>

<p>I have checked similar issues here on stackoverflow, and usually they are caused through misuse of the library.</p>

<p>However I am not using Pyro4 directly, I am using Distributed LSI introduced by gensim. So there is no room for mistakes on my side (or so I believe)</p>

<p>My code is really simple you can find it below</p>

<pre><code>from gensim import corpora, models, utils
import logging, os, Pyro4
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
os.environ[""PYRO_SERIALIZERS_ACCEPTED""] =  'pickle'
os.environ[""PYRO_SERIALIZER""] = 'pickle'

corpus = corpora.MmCorpus('wiki_corpus.mm') # load a corpus of nine documents, from the Tutorials
id2word = corpora.Dictionary.load('wiki_dict.dict')

lsi = models.LsiModel(corpus, id2word=id2word, num_topics=200, chunksize=1, distributed=True) # run distributed LSA on nine documents
</code></pre>
","1831518","","","","","2017-08-05 10:33:07","AttributeError module 'Pyro4' has no attribute 'expose' while running gensim distributed LSI","<python-2.7><gensim><latent-semantic-indexing><pyro4>","1","0","","","","CC BY-SA 3.0"
"31286574","1","31616089","","2015-07-08 07:48:57","","2","1496","<p>I'm studying word2vec, but when I use word2vec to train text data, occur OverFlowError with Numpy.</p>

<p>the message is,</p>

<pre><code>model.vocab[w].sample_int &gt; model.random.randint(2**32)]
Warning (from warnings module):
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 636
    warnings.warn(""C extension not loaded for Word2Vec, training will be slow. ""
UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""C:\Python34\lib\threading.py"", line 920, in _bootstrap_inner
    self.run()
  File ""C:\Python34\lib\threading.py"", line 868, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 675, in worker_loop
    if not worker_one_job(job, init):
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 666, in worker_one_job
    job_words = self._do_train_job(items, alpha, inits)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 623, in _do_train_job
    tally += train_sentence_sg(self, sentence, alpha, work)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 112, in train_sentence_sg
    word_vocabs = [model.vocab[w] for w in sentence if w in model.vocab and
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 113, in &lt;listcomp&gt;
    model.vocab[w].sample_int &gt; model.random.randint(2**32)]
  File ""mtrand.pyx"", line 935, in mtrand.RandomState.randint (numpy\random\mtrand\mtrand.c:9520)
OverflowError: Python int too large to convert to C long
</code></pre>

<p>Can you tell me the cases?</p>

<p>My machine is x64 and OS is windows 7, but python34 is 32bit. numpy and scipy are also 32bit.</p>
","5092693","","3401528","","2015-07-10 08:13:35","2015-07-28 20:00:23","Python34 word2vec.Word2Vec OverFlowError","<python-3.x><windows-7-x64><integer-overflow><gensim><word2vec>","1","0","1","","","CC BY-SA 3.0"
"55705634","1","55716588","","2019-04-16 10:07:28","","0","221","<p>i am training multiple word2vec models on the same corpus. (i am doing this to study the variation in learned word vectors)</p>

<p>i am using this tutorial as reference: <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">https://rare-technologies.com/word2vec-tutorial/</a></p>

<p>it is suggested that by default gensim.models.word2vec will iterate over the corpus at least twice. once for initialization and then again for training (iterating the number of epochs specified) </p>

<p>since i am always using the same corpus, i want to save time by initializing only once, and providing the same initialization as input to all successive models.</p>

<p>how can this be done?</p>

<p>this is my current setting:</p>

<pre class=""lang-py prettyprint-override""><code>subdirectory = 'corpus_directory'
for i in range(10):
    sentences = MySentences(subdirectory) # a memory-friendly iterator
    model = gensim.models.Word2Vec(sentences, min_count=20, size=100, workers=4)
    model.train(sentences, total_examples=model.corpus_count, epochs=1)
    word_vectors = model.wv
    fname = 'WV{}.kv'
    word_vectors.save(fname.format(i))
</code></pre>

<p>where MySentences is defined similarly to the tutorial:
(i made a slight change, so the order of corpus sentences would be shuffled with each initialization)</p>

<pre class=""lang-py prettyprint-override""><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname
        self.file_list = [fname for fname in os.listdir(dirname) if fname.endswith('.txt')]
        random.shuffle(self.file_list)

    def __iter__(self):
        for article in self.file_list:
            for line in open(os.path.join(self.dirname, article)):
                yield line.split()
</code></pre>
","6746604","","","","","2019-04-16 21:05:01","how to speed up gensim word2vec initialization with pre proccessed corpus?","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"53728556","1","53738781","","2018-12-11 16:36:43","","-2","223","<p>I would like to train a model with Gensim using news texts from electronic newspapers (in pdf format). What is the best way to extract texts from pdf files and to process the texts ready for training? Any sample codes?</p>
","10445343","","","","","2018-12-12 08:20:55","Extracting texts from pdf files for building a model with Gensim","<python-3.x><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"48455703","1","48467454","","2018-01-26 04:12:40","","4","1472","<p>I am using Gensim to train Word2Vec. I know word similarities are deteremined by if the words can replace each other and make sense in a sentence. But can word similarities be used to extract relationships between entities?</p>

<p>Example:
I have a bunch of interview documents and in each interview, the interviewee always says the name of their manager. If I wanted to extract the name of the manager from these interview transcripts could I just get a list of all human name's in the document (using nlp), and the name that is the most similar to the word ""manager"" using Word2Vec, is most likely the manager.</p>

<p>Does this thought process make any sense with Word2Vec? If it doesn't, would the ML solution to this problem then be to input my word embeddings into a sequence to sequence model?</p>
","8003832","","","","","2018-01-26 18:30:59","Can Word2Vec be used for information extraction?","<machine-learning><word2vec><gensim><rnn><information-extraction>","1","0","2","","","CC BY-SA 3.0"
"66111075","1","","","2021-02-08 23:55:37","","0","191","<p>Suppose I build a LDA topic model using gensim or sklearn and assign top topics to each document. But some of documents don't match top topics assigned. Besides trying out different numbers of topics or use coherence score to get the optimal number of topics, what other tricks can I use to improve my model?</p>
","6221871","","","","","2021-02-24 20:19:47","Tuning LDA Topic Models","<gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"54213078","1","54228800","","2019-01-16 08:37:46","","1","366","<p>I have a large language corpus and I use sklearn tfidf vectorizer and gensim Doc2Vec to compute language models. My total corpus has about 100,000 documents and I realized that my Jupyter notebook stops computing once I cross a certain threshold. I guess that the memory is full after applying the grid-search and cross-validation steps.</p>

<p>Even following example script already stops for Doc2Vec at some point:</p>

<pre><code>%%time
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.externals import joblib

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.sklearn_api import D2VTransformer

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from gensim.utils import simple_preprocess

np.random.seed(1)

data = pd.read_csv('https://pastebin.com/raw/dqKFZ12m')
X_train, X_test, y_train, y_test = train_test_split([simple_preprocess(doc) for doc in data.text],
                                                    data.label, random_state=1)

model_names = [
               'TfidfVectorizer',
               'Doc2Vec_PVDM',
              ]

models = [
    TfidfVectorizer(preprocessor=' '.join, tokenizer=None, min_df = 5),
    D2VTransformer(dm=0, hs=0, min_count=5, iter=5, seed=1, workers=1),
]

parameters = [
              {
              'model__smooth_idf': (True, False),
              'model__norm': ('l1', 'l2', None)
              },
              {
              'model__size': [200],
              'model__window': [4]
              }
              ]

for params, model, name in zip(parameters, models, model_names):

    pipeline = Pipeline([
      ('model', model),
      ('clf', LogisticRegression())
      ])

    grid = GridSearchCV(pipeline, params, verbose=1, cv=5, n_jobs=-1)
    grid.fit(X_train, y_train)
    print(grid.best_params_)

    cval = cross_val_score(grid.best_estimator_, X_train, y_train, scoring='accuracy', cv=5, n_jobs=-1)
    print(""Cross-Validation (Train):"", np.mean(cval))

print(""Finished."")
</code></pre>

<p><strong>Is there a way to ""stream"" each line in a document, instead of loading the full data into memory? Or another way to make it more memory efficient? I read a few articles on the topic but could not discover any that included a pipeline example.</strong></p>
","4697646","","4697646","","2019-01-31 19:06:12","2019-01-31 19:06:12","Streaming corpus to a vectorizer in a pipeline","<scikit-learn><streaming><gensim><corpus>","1","0","","","","CC BY-SA 4.0"
"48411780","1","","","2018-01-23 22:19:37","","4","725","<p>A little background about this project. I have copies with an identifier and the text, e.g. <code>{name: ""sports-football"", text: ""Content related to football sports""}</code>.</p>

<p>I need to find the right match for the given text input within this corpus.
However, I was able to achieve somewhat using Gensim. Similarity with LDA and LSI Model.</p>

<p>How to update the <code>Genism.Similarity</code> Index with new a document. The idea here is to keep training the model at live stage.</p>

<p>Here is the step I followed.</p>

<p>QueryText = ""Guardiola moved Lionel Messi to the No 9 role so that he didn't have to come deep and I think Aguero drops back into deeper positions too often.""</p>

<p>Note: some codes are just layman </p>

<p>The index is created using </p>

<pre><code>`similarities.Similarity(indexpath, model,topics)`
</code></pre>

<ol>
<li><p>Create A dictionary </p>

<p><code>dictionary = Dictionary(QueryText )</code></p></li>
<li><p>Create a corpus   </p>

<p><code>corpus = Corpus(QueryText, dictionary)</code></p></li>
<li><p>Create an LDA Model</p>

<p><code>LDAModel =  ldaModel(corpus,dictionary)</code></p></li>
</ol>

<p>Update existing dictionary, model, and index</p>

<p>Update existing dictionary</p>

<pre><code>existing_dictionary.add_document(dictionary)
</code></pre>

<p>Update existing LDA Model</p>

<pre><code>existing_lda_model.update(corpus)
</code></pre>

<p>Update existing Similarity index</p>

<pre><code>existing_index.add_dcoument(LDAModel[corpus])
</code></pre>

<p>Other than below warning update seems to be worked.</p>

<pre><code>gensim\models\ldamodel.py:535: RuntimeWarning: overflow encountered in exp2 perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words
</code></pre>

<p>Let's run the similarity for the query text</p>

<pre><code>vec_bow = dictionary.doc2bow(QueryText) 
vec_model = existing_lda_model[vec_bow] 
sims = existing_index[vec_model]
</code></pre>

<p>However, it failed with below error.</p>

<pre><code>Similarity index with 723 documents in 1 shards (stored under \Files\models\lda_model)
Similarity index with 725 documents in 0 shards (stored under \Files\models\lda_model)
\lib\site-packages\gensim\models\ldamodel.py:535: RuntimeWarning: overflow encountered in exp2
  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-3-8fe711724367&gt; in &lt;module&gt;()
     45 trigram = Trigram.apply_trigram_model(queryText, bigram, trigram)
     46 vec_bow = dictionry.doc2bow(trigram)
---&gt; 47 vec_model =  lda_model[vec_bow]
     48 print(vec_model)
     49 

~\Anaconda3\envs\lf\lib\site-packages\gensim\models\ldamodel.py in __getitem__(self, bow, eps)
   1103             `(topic_id, topic_probability)` 2-tuples.
   1104         """"""
-&gt; 1105         return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
   1106 
   1107     def save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs):

~\Anaconda3\envs\lf\lib\site-packages\gensim\models\ldamodel.py in get_document_topics(self, bow, minimum_probability, minimum_phi_value, per_word_topics)
    944             return self._apply(corpus, **kwargs)
    945 
--&gt; 946         gamma, phis = self.inference([bow], collect_sstats=per_word_topics)
    947         topic_dist = gamma[0] / sum(gamma[0])  # normalize distribution
    948 

~\Anaconda3\envs\lf\lib\site-packages\gensim\models\ldamodel.py in inference(self, chunk, collect_sstats)
    442             Elogthetad = Elogtheta[d, :]
    443             expElogthetad = expElogtheta[d, :]
--&gt; 444             expElogbetad = self.expElogbeta[:, ids]
    445 
    446             # The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.

IndexError: index 718 is out of bounds for axis 1 with size 713
</code></pre>

<p>I really appreciate, helping me with this.
Looking forward to awesome replies. </p>
","1891789","","1812322","","2018-02-02 12:04:15","2018-02-02 12:04:15","Gensim.Similarity Add document or Live training","<python><nlp><similarity><gensim>","1","0","","","","CC BY-SA 3.0"
"45419049","1","45424161","","2017-07-31 14:48:32","","0","209","<p>From the Doc2Vec wikipedia tutorial at <a href=""https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>

<pre><code>for num in range(0, 20):
    print('min_count: {}, size of vocab: '.format(num), 
           pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/700)
</code></pre>

<p>Output is:</p>

<pre><code>min_count: 0, size of vocab: 8545782.0
min_count: 1, size of vocab: 8545782.0
min_count: 2, size of vocab: 4227783.0
min_count: 3, size of vocab: 3008772.0
min_count: 4, size of vocab: 2439367.0
min_count: 5, size of vocab: 2090709.0
min_count: 6, size of vocab: 1856609.0
min_count: 7, size of vocab: 1681670.0
min_count: 8, size of vocab: 1546914.0
min_count: 9, size of vocab: 1437367.0
min_count: 10, size of vocab: 1346177.0
min_count: 11, size of vocab: 1267916.0
min_count: 12, size of vocab: 1201186.0
min_count: 13, size of vocab: 1142377.0
min_count: 14, size of vocab: 1090673.0
min_count: 15, size of vocab: 1043973.0
min_count: 16, size of vocab: 1002395.0
min_count: 17, size of vocab: 964684.0
min_count: 18, size of vocab: 930382.0
min_count: 19, size of vocab: 898725.0
</code></pre>

<blockquote>
  <p>In the original paper, they set the vocabulary size 915,715. It seems similar size of vocabulary if we set min_count = 19. (size of vocab = 898,725)</p>
</blockquote>

<p><code>700</code> seems rather arbitrary, and I don't see any mentioning of this in the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">docs</a>. </p>
","1144382","","","","","2017-07-31 19:47:46","Why is Doc2Vec.scale_vocab(...)['memory']['vocab'] divided by 700 to obtain vocabulary size?","<gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"19315338","1","","","2013-10-11 09:58:14","","8","3132","<p>I‚Äôm trying to get started by loading the pretrained .bin files from the google word2vec site ( freebase-vectors-skipgram1000.bin.gz) into the gensim implementation of word2vec. The model loads fine, </p>

<p>using ..</p>

<pre><code>model = word2vec.Word2Vec.load_word2vec_format('...../free....-en.bin', binary= True)
</code></pre>

<p>and creates a </p>

<pre><code>&gt;&gt;&gt; print model
&lt;gensim.models.word2vec.Word2Vec object at 0x105d87f50&gt;
</code></pre>

<p>but when I run the most similar function. It cant find the words in the vocabulary. My error code is below.</p>

<p>Any ideas where I‚Äôm going wrong?</p>

<pre><code>&gt;&gt;&gt; model.most_similar(['girl', 'father'], ['boy'], topn=3)
2013-10-11 10:22:00,562 : WARNING : word ‚Äògirl‚Äô not in vocabulary; ignoring it
2013-10-11 10:22:00,562 : WARNING : word ‚Äòfather‚Äô not in vocabulary; ignoring it
2013-10-11 10:22:00,563 : WARNING : word ‚Äòboy‚Äô not in vocabulary; ignoring it
Traceback (most recent call last):
File ‚Äú‚Äù, line 1, in
File ‚Äú/....../anaconda/python.app/Contents/lib/python2.7/site-packages/gensim-0.8.7/py2.7.egg/gensim/models/word2vec.py‚Äù, line 312, in most_similar
raise ValueError(‚Äúcannot compute similarity with no input‚Äù)
ValueError: cannot compute similarity with no input
</code></pre>
","2870492","","527702","","2014-01-16 05:40:33","2015-06-10 16:16:26","Working with google word2vec .bin files in gensim python","<python><gensim><word2vec>","2","0","1","","","CC BY-SA 3.0"
"23971900","1","","","2014-05-31 15:49:27","","1","320","<p>I do as the gensim tutorial to run LAD on 195145 documents, 6636308 features, 188901082 non-zero entries.
The code is simple:</p>

<pre><code>from gensim import corpora, models, similarities
class MyCorpus(object):
    def __iter__(self):
        for line in open('/home/pda/xxz149/LDA/DrugPatents.csv'):
            # assume there's one document per line, tokens separated by ','
            yield dictionary.doc2bow(line.lower().split(','))
dictionary = corpora.Dictionary.load('/home/pda/xxz149/LDA/DrugPatent.dict')
corpus = MyCorpus()
lda = models.ldamodel.LdaModel(corpus, num_topics = 300,id2word=dictionary,distributed = False,chunksize = 15, passes = 1 )
lda.save('/home/pda/xxz149/LDA/lda_DrugPatent.model')
</code></pre>

<p>But I meet the value error:</p>

<pre><code>  File ""/usr/lib/python2.6/site-packages/gensim-0.10.0rc1-py2.6.egg/gensim/models/ldamodel.py"", line 79, in __init__

    self.sstats = numpy.zeros(shape)
ValueError: array is too big.
</code></pre>

<p>The gensim is memory-friendly, Why this happen? How can I get through?</p>
","3694547","","3946766","","2014-10-05 14:54:06","2014-10-05 14:54:06","How to handle ""ValueError: array is too big."" in Gensim when run LDA?","<python><lda><gensim>","0","2","1","","","CC BY-SA 3.0"
"62861346","1","","","2020-07-12 12:54:23","","2","1448","<p>When I do the below:</p>
<pre><code>&gt;&gt;&gt; import gensim.downloader as api
&gt;&gt;&gt; model = api.load(&quot;glove-twitter-25&quot;)  # load glove vectors
</code></pre>
<p>the gensim.downloader API throws the below error:</p>
<blockquote>
<p>[Errno 2] No such file or directory:
'/Users/vtim/gensim-data/information.json'.</p>
</blockquote>
<p>What am I doing wrong?</p>
","8318130","","3235260","","2020-07-12 20:20:42","2020-08-11 16:13:40","Why can't I download a dataset with the Gensim download API","<python><download><dataset><gensim><glove>","1","0","1","","","CC BY-SA 4.0"
"50366293","1","","","2018-05-16 08:49:11","","3","1346","<p>I am working on a project to classify customer feedback into buckets based on the topic of the feedback comment. So , I need to classify the sentence into one of the topics among a list of pre-defined topics. </p>

<p><strong>For example :</strong></p>

<p>""I keep getting an error message every time I log in"" has to be tagged with ""login"" as the topic.</p>

<p>""make the screen more colorful"" has to be tagged with ""improvements"" as the topic.</p>

<p>So the <strong>topics are very specific to the product and the context</strong>.</p>

<p>LDA doesn't seem to work for me(correct me if i'm wrong). It detects the topics in a general sense like ""Sports"" , ""Politics"" , ""Technology"" etc. But I need to detect specific topics as mentioned above.</p>

<p>Also , I don't have labelled data for training. All I have is the comments.
So supervised learning approach doesn't look like an option.</p>

<p><strong>What I have tried so far:</strong></p>

<p>I trained a gensim model with google news corpus (its about 3.5 gb).
I am cleaning the sentence by removing stop words , punctuation marks etc.
I am finding , to what topic among the set of topics each word is closest to and tag the word to that topic. With an idea that the sentence might contain more words closer to the topic it is referring to than not , I am picking up the topic(s) to which maximum number of words in the sentence is mapped to.</p>

<p><strong>For example:</strong></p>

<p>If 3 words in a sentence is mapped to ""login"" topic and 2 words in the sentence is mapped to ""improvement"" topic , I am tagging the sentence to ""login"" topic. </p>

<p>If there is a clash between the count of multiple topics , I return all the topics with the maximum count as the topic list.</p>

<p>This approach is giving me fair results. But its not good enough.</p>

<p><strong>What will be the best approach to tackle this problem?</strong></p>
","5232932","","5232932","","2018-05-16 08:54:57","2020-10-06 14:42:33","How to classify a sentence into one of the pre-defined topic bucket using an unsupervised approach","<python><machine-learning><nlp><gensim><topic-modeling>","2","1","2","","","CC BY-SA 4.0"
"40643082","1","40767273","","2016-11-16 21:55:53","","0","622","<p>So I believe despite this being a common issue with many similar questions (especially on stackoverflow), the main reason behind this issue varies in each case</p>

<p>In my case I have a method named <code>readCorpus</code> (<strong>find code below</strong>) it reads a list of 21 files, extract docs from each file then yield them</p>

<p>The yield operation occurs at the end of reading each file</p>

<p>I have another method named <code>uploadCorpus</code> (<strong>find code below</strong>). The main aim of this method is to upload that corpus. </p>

<p>Obviously the main reason behind using yield is that the corpus can be very large and I only need to read it once.</p>

<p>Once I run the method <code>uploadCorpus</code> I receive the error below</p>

<p><code>TypeError: coercing to Unicode: need string or buffer, list found</code></p>

<p>The erros occurs at the line <code>self.readCorpus()])</code>. </p>

<p>Reading similar problems I came to understand that it happens when a list is misplaced .. I tried to uplate the line of question here to <code>docs for docs in self.readCorpus()])</code> but I ended with the same issue</p>

<p><strong>My code (uploadCorpus)</strong></p>

<pre><code>def uploadCorpus(self):
        #convert docs to corpus
        print ""uploading""

        utils.upload_chunked(
            self.service,
            [{'id': 'doc_%i' % num, 'tokens': utils.simple_preprocess(doc)}
            for num, doc in enumerate([ 
                self.readCorpus()])
                ],
            chunksize=1000) # send 1k docs at a time
</code></pre>

<p><strong>My code readCorpus()</strong></p>

<pre><code>def readCorpus(self):
    path = '../data/reuters'
    doc=''
    docs = []
    docStart=False

    fileCount=0

    print 'Reading Corpus'
    for name in glob.glob(os.path.join(path, '*.sgm')):
        print 'Reading File| ' + name
        docCount=0
        for line in open(name):
            if(len(re.findall(r'&lt;BODY&gt;', line)) &gt; 0 ): 
                docStart = True
                pattern = re.search(r'&lt;BODY&gt;.*', line)
                doc+= pattern.group()[6:]

            if(len(re.findall(r'&lt;/BODY&gt;\w*', line)) &gt; 0 ):
                docStart = False
                docs.append(doc)
                doc=''
                docCount+=1
                continue
                #break
            if(docStart):
                doc += line

        fileCount+=1
        print 'docuemnt[%d][%d]'%(fileCount,docCount)
        yield docs
        docs = []
</code></pre>
","1831518","","1831518","","2016-11-23 14:38:20","2016-11-23 14:38:20","python gensim TypeError: coercing to Unicode: need string or buffer, list found","<python><python-2.7><typeerror><iterable><gensim>","1","2","","","","CC BY-SA 3.0"
"53767024","1","53768789","","2018-12-13 17:17:43","","1","415","<p>Recently I switched to gensim 3.6 and the main reason was the optimized training process, which streams the training data directly from file, thus avoiding the GIL performance penalties.</p>

<p>This is how I used to trin my doc2vec:</p>

<pre><code>training_iterations = 20
d2v = Doc2Vec(vector_size=200, workers=cpu_count(), alpha=0.025, min_alpha=0.00025, dm=0)
d2v.build_vocab(corpus)

for epoch in range(training_iterations):
    d2v.train(corpus, total_examples=d2v.corpus_count, epochs=d2v.iter)
    d2v.alpha -= 0.0002
    d2v.min_alpha = d2v.alpha
</code></pre>

<p>And it is classifying documents quite well, only draw back is that when it is trained CPUs are utilized at 70%</p>

<p>So the new way:</p>

<pre><code>corpus_fname = ""spped.data""
save_as_line_sentence(corpus, corpus_fname)

# Choose num of cores that you want to use (let's use all, models scale linearly now!)
num_cores = cpu_count()

# Train models using all cores
d2v_model = Doc2Vec(corpus_file=corpus_fname, workers=num_cores, dm=0, vector_size=200, epochs=50)
</code></pre>

<p><strong>Now all CPUs are utilized at 100%</strong></p>

<p>but the model is performing very poorly.
According to the documentation, I should not use the train method also, I should use only epoch count and not iterations, also the min_aplpha and aplha values should not be touched.</p>

<p>The configuration of both Doc2Vec looks the same to me so is there an issue with my new set up or configuration, or there is something wrong with the new version of gensim?</p>

<p>P.S I am using the same corpus in both cases, also I tried epoch count = 100, also with smaller numbers like 5-20, but I had no luck</p>

<p><strong>EDIT</strong>: First model was doing 20 iterations 5 epoch each, second was doing 50 epoch, so having the second model make 100 epochs made it perform even better, since I was no longer managing the alpha by myself.</p>

<p>About the second issue that popped up: when providing file with line documents, the doc ids were not always corresponding to the lines, I didn't manage to figure out what could be causing this, it seems to work fine for small corpus, If I find out what I am doing wrong I will update this answer.</p>

<p>The final configuration for corpus of size 4GB looks like this</p>

<pre><code>    d2v = Doc2Vec(vector_size=200, workers=cpu_count(), alpha=0.025, min_alpha=0.00025, dm=0)
    d2v.build_vocab(corpus)
    d2v.train(corpus, total_examples=d2v.corpus_count, epochs=100)
</code></pre>
","5625696","","5625696","","2019-01-08 17:27:59","2019-01-08 17:27:59","Gensim doc2vec file stream training worse performance","<nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"54215456","1","54225979","","2019-01-16 10:52:02","","0","232","<p>Problem :</p>

<p>Im using glove pre-trained model with vectors to retrain my model with a specific domain say #cars, after training I want to find similar words within my domain but I got words not in my domain corpus, I believe it's from glove's vectors. </p>

<pre><code>model_2.most_similar(positive=['spacious'],    topn=10)

[('bedrooms', 0.6275501251220703),
 ('roomy', 0.6149100065231323),
 ('luxurious', 0.6105825901031494),
 ('rooms', 0.5935696363449097),
 ('furnished', 0.5897485613822937),
 ('cramped', 0.5892841219902039),
 ('courtyard', 0.5721820592880249),
 ('bathrooms', 0.5618442893028259),
 ('opulent', 0.5592212677001953),
 ('expansive', 0.555268406867981)]
</code></pre>

<p>Here I expect something like leg-room, car's spacious features mentioned in the domain's corpus. How can we exclude the glove vectors while having similar vectors?</p>

<p>Thanks  </p>
","8636829","","130288","","2019-01-17 07:01:51","2019-01-17 07:01:51","Gensim pretrained model similarity","<python><vector><nlp><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"40659565","1","","","2016-11-17 16:02:21","","2","903","<p>I'm using Gensim TfidfModel model. this is my code:</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split()) for line in open('aaa.txt'))

class MyCorpus(object):
    def __iter__(self):
        for line in open('aaa.txt'):
            yield dictionary.doc2bow(line.lower().split())

corpus = MyCorpus()

tfidf = models.TfidfModel(corpus)

corpus_tfidf = tfidf[corpus]
</code></pre>

<p>now I want to extract tf-idf value of each word, I know that they are in corpus_tfidf variable and I tried some codes like below to view all of words tf-idf but I have a word like 'banana' and I want to find its tf-idf value. there is a access to find each word in dictionary like dictionary.token2id['banana'] but how can I get tf-idf of each word?</p>

<pre><code>{dictionary.get(id): value for doc in corpus_tfidf for id, value in doc}
</code></pre>

<p>My corpus has 6501598 documents, 585499 features, 64106768 non-zero entries, and it's important to get value of each word in minimum time.</p>
","1013249","","","","","2016-11-17 16:02:21","Gensim Extracting TF-IDF value of a word in a corpus","<python><text><tf-idf><gensim>","0","0","","","","CC BY-SA 3.0"
"45420466","1","45420886","","2017-07-31 15:59:08","","23","38899","<p>I have a trained Word2vec model using Python's Gensim Library. I have a tokenized list as below. The vocab size is 34 but I am just giving few out of 34:</p>

<pre><code>b = ['let',
 'know',
 'buy',
 'someth',
 'featur',
 'mashabl',
 'might',
 'earn',
 'affili',
 'commiss',
 'fifti',
 'year',
 'ago',
 'graduat',
 '21yearold',
 'dustin',
 'hoffman',
 'pull',
 'asid',
 'given',
 'one',
 'piec',
 'unsolicit',
 'advic',
 'percent',
 'buy']
</code></pre>

<p><strong>Model</strong></p>

<pre><code>model = gensim.models.Word2Vec(b,min_count=1,size=32)
print(model) 
### prints: Word2Vec(vocab=34, size=32, alpha=0.025) ####
</code></pre>

<p>if I try to get the similarity score by doing <code>model['buy']</code> of one the words in the list, I get the </p>

<blockquote>
  <p>KeyError: ""word 'buy' not in vocabulary""</p>
</blockquote>

<p>Can you guys suggest me what I am doing wrong and what are the ways to check the model which can be further used to train PCA or t-sne in order to visualize similar words forming a topic? Thank you. </p>
","6395618","","","","","2018-07-16 05:46:34","Gensim: KeyError: ""word not in vocabulary""","<python><nlp><gensim><word2vec><topic-modeling>","2","0","6","","","CC BY-SA 3.0"
"49088689","1","","","2018-03-03 20:05:07","","7","2726","<p>Does anyone know how to load a tsv file with embeddings generated from StarSpace into Gensim? Gensim documentation seems to use Word2Vec a lot and I couldn't find a pertinent answer.</p>

<p>Thanks,</p>

<p>Amulya</p>
","9439502","","8332344","","2018-03-04 06:13:37","2021-07-07 17:05:41","How to load embeddings (in tsv file) generated from StarSpace","<gensim><word-embedding>","3","0","","","","CC BY-SA 3.0"
"56443667","1","","","2019-06-04 12:13:17","","0","1026","<p>ImportError: cannot import name 'mean_absolute_difference'</p>

<p>Tried uninstalling and installing again.</p>

<pre><code>import gensim
</code></pre>

<hr>

<pre><code>ImportError                          Traceback (most recent call last)
&lt;ipython-input-28-e70e92d32c6e&gt; in &lt;module&gt;()
----&gt; 1 import gensim
2 frames
/usr/local/lib/python3.6/dist-packages/gensim/models/hdpmodel.py in 
&lt;module&gt;()
 61 
 62 from gensim import interfaces, utils, matutils
 ---&gt; 63 from gensim.matutils import dirichlet_expectation, 
 mean_absolute_difference
 64 from gensim.models import basemodel, ldamodel
 65 

 ImportError: cannot import name 'mean_absolute_difference'
</code></pre>
","11585885","","","","","2020-05-04 15:40:01","Error while importing gensim package in colab","<python><importerror><gensim>","1","0","","","","CC BY-SA 4.0"
"48507404","1","","","2018-01-29 18:05:36","","0","1980","<p>I just came across <a href=""https://stackoverflow.com/questions/37190989/how-to-get-vocabulary-word-count-from-gensim-word2vec"">this StackOverflow post</a> on word counts in a doc2vec model vocabulary. I wonder if there is another method to retrieve the word frequency, other than </p>

<pre><code>for word, vocab_obj in model.wv.vocab.items():
    print(str(word) + str(vocab_obj.count))
</code></pre>

<p>Maybe there is a more elegant way via the gensim library (i.e. output words and frequencies in a txt file)?</p>
","4697646","","","","","2018-01-30 21:37:18","gensim: Retrieving word frequency in doc2vec vocabulary","<dictionary><word2vec><gensim><doc2vec><vocabulary>","1","0","","","","CC BY-SA 3.0"
"66952438","1","","","2021-04-05 11:41:46","","0","2117","<p>My problem is similar to <a href=""https://stackoverflow.com/questions/55577628/attributeerror-module-gensim-models-word2vec-has-no-attribute-load"">this</a> question. I tried both solutions posted on the question, but I still receive the error that the attribute &quot;Vocab&quot; is not available in the gensim.models.word2vec module.</p>
<p>The part of my code using this attribute is here</p>
<pre><code># if word in model.keys(): #use model.vocab for w2v model and model.keys() for Glove dicts
        if word in self.w2v_model.wv.vocab:
            vector = self.w2v_model.wv[word]
        else:
            vector = [0] * 100  
</code></pre>
","15557036","","","","","2021-04-05 20:37:51","AttributeError: Can't get attribute 'Vocab' on <module 'gensim.models.word2vec'","<python-3.x><error-handling><gensim><attribution>","2","0","","","","CC BY-SA 4.0"
"53788106","1","","","2018-12-14 23:21:10","","0","302","<p>My gensim model is like this:</p>

<pre><code>class MyCorpus(object):
    parametersList = []
    def __init__(self,dictionary):
       self.dictionary=dictionary
    def __iter__(self):
        #for line in open('mycorpus.txt'):
        for line in texts:
            # assume there's one document per line, tokens separated by whitespace
            yield self.dictionary.doc2bow(line[0].lower().split())




if __name__==""__main__"":
    texts=[['human human interface computer'],
             ['survey user user computer system system system response time'],
             ['eps user interface system'],
             ['system human system eps'],
             ['user response time'],
             ['trees'],
             ['graph trees'],
             ['graph minors trees'],
             ['graph minors minors survey survey survey']]


    dictionary = corpora.Dictionary(line[0].lower().split() for line in texts)

    corpus= MyCorpus(dictionary)
</code></pre>

<p>The frequency of each token in each document is automatically evaluated.</p>

<p>I also can define the tf-idf model and access the tf-idf statistic for each token in each document.</p>

<pre><code>model = TfidfModel(corpus)
</code></pre>

<p><strong>However, I have no clue how to count (memory-friendly) the number of documents that a given word arise. How can I do that?</strong> [Sure... I can use the values of tf-idf and document frequency to evaluate it... However, I would like to evaluate it directly from some counting process]</p>

<p>For instance, for the first document, I would like to get somenthing like</p>

<pre><code>[('human',2), ('interface',2), ('computer',2)]
</code></pre>

<p>since each token above arises twice in each document.</p>

<p>For the second.</p>

<pre><code>[('survey',2), ('user',3), ('computer',2),('system',3), ('response',2),('time',2)]
</code></pre>
","2065691","","2065691","","2018-12-14 23:46:59","2018-12-15 10:06:36","Gensim-python: Is there a simple way to get the number of times a given token arise in all documents?","<nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"40651699","1","","","2016-11-17 09:55:53","","1","205","<p>How can i compute the variance of the pre-trained word embeddings trained by word2Vec in gensim?<br>
In need the variance for the word2Vec-model trained by google (Google News corpus).<br>
Thanks.</p>
","5036074","","","","","2016-11-17 09:55:53","Variance of the pre-trained word embeddings trained by word2Vec (Google News Corpus)","<gensim><word2vec>","0","0","1","","","CC BY-SA 3.0"
"66958119","1","","","2021-04-05 18:44:12","","0","789","<p>Used command <code>pip install --upgrade gensim</code> from <a href=""https://pypi.org/project/gensim/"" rel=""nofollow noreferrer"">https://pypi.org/project/gensim/</a>
Anyone knows what might cause this?</p>
<pre><code>error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2
  ----------------------------------------
  ERROR: Failed building wheel for gensim
  Running setup.py clean for gensim
Failed to build gensim
Installing collected packages: gensim
    Running setup.py install for gensim ... error
    ERROR: Command errored out with exit status 1:
     command: 'c:\users\appdata\local\programs\python\python39\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'C:\\Users\\Andreea Elena\\AppData\\Local\\Temp\\pipinstall-khjrriwd\\gensim_18d18388d198487b8f7aebdfc3c97b94\\setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'C:\\Users\\AppData\\Local\\Temp\\pip-install-khjrriwd\\gensim_18d18388d198487b8f7aebdfc3c97b94\\stup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record 'C:\Users\AppData\Local\Temp\pip-record-c7348b68\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\appdata\local\programs\python\python39\Iclude\gensim'```
</code></pre>
","15499396","","","","","2021-04-05 20:00:33","Error when installing gensim using pip install","<python><gensim>","1","1","","","","CC BY-SA 4.0"
"47157286","1","","","2017-11-07 11:51:49","","3","3123","<p>I am using the following python code to generate similarity matrix of word vectors (My vocabulary size is <code>77</code>).</p>

<pre><code>similarity_matrix = []
index = gensim.similarities.MatrixSimilarity(gensim.matutils.Dense2Corpus(model.wv.syn0))

for sims in index:
    similarity_matrix.append(sims)
similarity_array = np.array(similarity_matrix)
</code></pre>

<p>The dimensionality of the <code>similarity_array</code> is <code>300 X 300</code>. However as I understand the dimensionality should be <code>77 x 77</code> (as my vocabulary size is 77).</p>

<pre><code>i.e.,
      word1, word2, ......, word77
word1 0.2,     0.8,    ...,  0.9
word2 0.1,     0.2,   ....,  1.0
...  ....,    ....., .....,   ....
word77 0.9,  0.8,    ...,    0.1
</code></pre>

<p>Please let me know what is wrong in my code.</p>

<p>Moreover, I want to know what is the order of the vocabulary <code>(word1, word2, ..., word77)</code> used to calculate this similarity matrix? Can I obtain this <code>order</code> from <code>model.wv.index2word</code>?</p>

<p>Please help me!</p>
","","user8871463","","","","2019-04-29 14:10:11","Get a similarity matrix from word2vec in python (Gensim)","<python><word2vec><gensim><word-embedding>","2","0","1","","","CC BY-SA 3.0"
"53765598","1","","","2018-12-13 15:54:49","","2","1064","<p>I trained Gensim W2V model on 500K sentences (around 60K) words and I want to calculate the perplexity.</p>

<ol>
<li>What will be the best way to do so?</li>
<li>for 60K words, how can I check what will be a proper amount of data?</li>
</ol>

<p>Thanks</p>
","10117402","","","","","2018-12-13 16:00:59","Calculate perplexity of word2vec model","<python><nlp><gensim><word2vec><language-model>","1","0","","","","CC BY-SA 4.0"
"56444845","1","","","2019-06-04 13:23:49","","1","551","<p>Lately I am doing a research with purpose of unsupervised clustering of a huge texts database. Firstly I tried bag-of-words and then several clustering algorithms which gave me a good result, but now I am trying to step into doc2vec representation and it seems to not be working for me, I cannot load prepared model and work with it, instead training my own doesnt prove any result.</p>

<p>I tried to train my model on 10k texts</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=500, min_count=2, epochs=100,workers=8)
</code></pre>

<p>(around 20-50 words each) but the similarity score which is proposed by gensim like</p>

<pre><code>sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
</code></pre>

<p>is working much worse than the same for Bag-of-words with my model.
By much worse i mean that identical or almost identical text have similarity score compatible to text which dont have any connection i can think about. So i decided to use model from <a href=""https://stackoverflow.com/questions/51132848/is-there-pre-trained-doc2vec-model"">Is there pre-trained doc2vec model?</a> to use some pretrained model which might have more connections between words. Sorry for somewhat long preambula but the question is how do i plug it in? Can someone provide some ideas how do i, using the loaded gensim model from <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">https://github.com/jhlau/doc2vec</a> convert my own dataset of text into vectors of same length? My data is preprocesssed (stemmed, no punctuation, lowercase, no nlst.corpus stopwords)and i can deliver it from list or dataframe or file if needed, the code question is how to pass my own data to pretrained model? Any help would be appreciated.</p>

<p>UPD: outputs that make me feel bad</p>

<blockquote>
  <p>Train Document (6134): ¬´use medium paper examination medium habit one
  week must chart daily use medium radio television newspaper magazine
  film video etc wake radio alarm listen traffic report commuting get
  news watch sport soap opera watch tv use internet work home read book
  see movie use data collect journal basis analysis examining
  information using us gratification model discussed textbook us
  gratification article provided perhaps carrying small notebook day
  inputting material evening help stay organized smartphone use note app
  track medium need turn diary trust tell tell immediately paper whether
  actually kept one begin medium diary soon possible order give ample
  time complete journal write paper completed diary need write page
  paper use medium functional analysis theory say something best
  understood understanding used us gratification model provides
  framework individual use medium basis analysis especially category
  discussed posted dominick article apply concept medium usage expected
  le medium use cognitive social utility affiliation withdrawal must
  draw conclusion use analyzing habit within framework idea discussed
  text article concept must clearly included articulated paper common
  mistake student make assignment tell medium habit fail analyze habit
  within context us gratification model must include idea paper¬ª</p>
  
  <p>Similar Document (6130, 0.6926988363265991): ¬´use medium paper examination medium habit one week must chart daily use medium radio
  television newspaper magazine film video etc wake radio alarm listen
  traffic report commuting get news watch sport soap opera watch tv use
  internet work home read book see movie use data collect journal basis
  analysis examining information using us gratification model discussed
  textbook us gratification article provided perhaps carrying small
  notebook day inputting material evening help stay organized smartphone
  use note app track medium need turn diary trust tell tell immediately
  paper whether actually kept one begin medium diary soon possible order
  give ample time complete journal write paper completed diary need
  write page paper use medium functional analysis theory say something
  best understood understanding used us gratification model provides
  framework individual use medium basis analysis especially category
  discussed posted dominick article apply concept medium usage expected
  le medium use cognitive social utility affiliation withdrawal must
  draw conclusion use analyzing habit within framework idea discussed
  text article concept must clearly included articulated paper common
  mistake student make assignment tell medium habit fail analyze habit
  within context us gratification model must include idea paper¬ª</p>
</blockquote>

<p>This looks perfectly ok, but looking on other outputs</p>

<blockquote>
  <p>Train Document (1185): ¬´photography garry winogrand would like paper
  life work garry winogrand famous street photographer also influenced
  street photography aim towards thoughtful imaginative treatment detail
  referencescite research material academic essay university level¬ª</p>
  
  <p>Similar Document (3449, 0.6901006698608398): ¬´tang dynasty write page
  essay tang dynasty essay discus buddhism tang dynasty name artifact
  tang dynasty discus them history put heading paragraph information
  tang dynasty discussed essay¬ª</p>
</blockquote>

<p>Shows us that the score of similarity between two exactly same texts which are the most similar in the system and two like super distinct is almost the same, which makes it problematic to do anything with the data.
To get most similar documents i use</p>

<pre><code> sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
</code></pre>
","8245211","","8245211","","2019-06-05 08:44:56","2019-06-05 08:44:56","is there a way to use pretrained doc2vec model to evaluate some document dataset","<python><numpy><gensim>","1","0","","","","CC BY-SA 4.0"
"66930434","1","","","2021-04-03 11:05:20","","0","661","<p>I am trying to find related words after creating a gensim model by processing a corpus (Text file A). Next I want to pass my own list of words (stored Text file B) to get most similar words to those in Text file B from the gensim model.</p>
<pre><code>model = gensim.models.Word2Vec(documents, window=5, min_count=1, workers=10)
model.train(documents, total_examples=len(documents), epochs=10)

w1 = &quot;beautiful&quot;
print(model.wv.most_similar(positive=w1))
</code></pre>
<p>This gives me a list of top 10 correlated words to the word &quot;beautiful&quot;.However, when I am trying to pass my list (text file B), it gets key error: key not present.</p>
<pre><code>mywords = read_med_terms(&quot;C:/Users/x/TextfileB.txt&quot;)


for word in mywords:
    try:
        print(model.wv.most_similar(positive=word))
    except KeyError:
        continue
</code></pre>
<p>How can I pass my list to get the set of related matching words available in the corpus? Sincerely appreciate your help.</p>
","11491586","","","","","2021-04-04 15:16:56","How to get related matching words using gensim in python?","<python><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"66975292","1","66976706","","2021-04-06 19:27:36","","0","62","<p>I use gensim 4.0.1 and train doc2vec:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
    
sentences = [['hello', 'world'], ['james', 'bond'], ['adam', 'smith']]
documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)]
model = Doc2Vec(documents, vector_size=5, window=5, min_count=0, workers=4) 
</code></pre>
<pre><code>documents
    [TaggedDocument(words=['hello', 'world'], tags=[0]),
    TaggedDocument(words=['james', 'bond'], tags=[1]),
    TaggedDocument(words=['adam', 'smith'], tags=[2])]
</code></pre>
<pre><code>model.dv[0],model.dv[1],model.dv[2]
        (array([-0.10461631, -0.11958256, -0.1976151 ,  0.1710569 ,  0.0713223 ],
               dtype=float32),
         array([ 0.00526548, -0.19761242, -0.10334401, -0.19437183,  0.04021204],
               dtype=float32),
         array([ 0.05662392,  0.09290017, -0.08597242, -0.06293383, -0.06159503],
               dtype=float32))
</code></pre>
<p>I expect to get a match on TaggedDocument #1</p>
<pre><code>seen = ['james','bond']
</code></pre>
<p>Surprisingly, that known text (james bond) produces a completely &quot;unseen&quot; vector:</p>
<pre><code>new_vector = model.infer_vector(seen)
new_vector
        
        array([-0.07762126,  0.03976333, -0.02985927,  0.07899596, -0.03556045],
              dtype=float32)
</code></pre>
<p>The <em>most_similar()</em> does not point to the expected Tag=1. Moreover, all 3 scores are quite weak implying completely unseen data.</p>
<pre><code>model.dv.most_similar_cosmul(positive=[new_vector]) 
[(0, 0.5322251915931702), (2, 0.4972134530544281), (1, 0.46321794390678406)]
</code></pre>
<p>What is wrong here, any ideas?</p>
","7206879","","","","","2021-04-06 21:30:47","How to interpret doc2vec results on previously seen data?","<gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"66959571","1","66959937","","2021-04-05 20:45:10","","0","245","<p>I'm trying to perform a simple task which requires iterations and interactions with specific vectors after loading it into gensim's Word2Vec.</p>
<p>Basically, given a txt file of the form:</p>
<pre><code>t1 -0.11307 -0.63909 -0.35103 -0.17906 -0.12349
t2 0.54553 0.18002 -0.21666 -0.090257 -0.13754
t3 0.22159 -0.13781 -0.37934 0.39926 -0.25967 
</code></pre>
<p>where t1 is the name of the vector and what follows are the vectors themselves. I load it in using the function <code>vecs = KeyedVectors.load_word2vec_format(datapath(f), binary=False)</code>.</p>
<p>Now, I want to iterate through the vectors I have and make a calculation, take summing up all of the vectors as an example. If this was read in using <code>with open(f)</code>, I know I can just use <code>.split(' ')</code> on it, but since this is now a KeyedVector object, I'm not sure what to do.</p>
<p>I've looked through the word2vec documentation, as well as used <code>dir(KeyedVectors)</code> but I'm still not sure if there is an attribute like <code>KeyedVectors.vectors</code> or something that allows me to perform this task.</p>
<p>Any tips/help/advice would be much appreciated!</p>
","11016170","","","","","2021-04-05 21:22:00","Is there a way to iterate through the vectors of Gensim's Word2Vec?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"56339518","1","56576598","","2019-05-28 09:52:26","","0","31","<p>I'm using gensim to perform cosine similarity on a bunch of documents getting the Segmentation fault: 11. Could you please help me to resolve this issue?</p>

<p><strong>Error Trace:</strong></p>

<pre><code>2019-05-28 15:11:22,779 : INFO : creating sparse index
2019-05-28 15:11:22,779 : INFO : creating sparse matrix from corpus
2019-05-28 15:11:22,780 : INFO : PROGRESS: at document #0/546
2019-05-28 15:11:22,790 : INFO : created &lt;546x430 sparse matrix of type '&lt;class 'numpy.float32'&gt;'
        with 2191 stored elements in Compressed Sparse Row format&gt;
2019-05-28 15:11:22,791 : INFO : creating sparse shard #0
2019-05-28 15:11:22,791 : INFO : saving index shard to /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,791 : INFO : saving SparseMatrixSimilarity object under /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0, separately None
2019-05-28 15:11:22,794 : INFO : saved /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,794 : INFO : loading SparseMatrixSimilarity object from /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,794 : INFO : loaded /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
Segmentation fault: 11
</code></pre>

<p><strong>Code</strong></p>

<pre><code>    def cosine_similarity(self,documents, query_docs=None, task='pairwise_similarity', metric_threshold=0.85, num_best=20):
        self.log('computing cosine similarity started')
        # Compute cosine similarity between the query_docs and the documents.
        dictionary = Dictionary(documents)
        corpus = [dictionary.doc2bow(doc) for doc in documents]
        # index_tmpfile = get_tmpfile(""index"")
        index = Similarity(output_prefix=None,corpus=corpus, num_best=num_best, num_features=len(dictionary))
        similarities = []
        if task == 'pairwise_similarity':
            self.log('computing pairwise_similarity')
            for sim in index:
                similarities.append(sim)
        elif task == 'batch_query':
            self.log('computing similarity using batch query')

            query_docs = [self.tfidf[self.dictionary.doc2bow(doc)] for doc in query_docs]
            for sim in index[query_docs]:
                similarities.append(sim)
        # filter results based on metric threshold
        filtered_results = []
        for ind_sim in similarities:
            filtered_results.append([item[0] for item in ind_sim if item[1] &gt;= metric_threshold])
        self.log('computing cosine similarity completed')
        return filtered_results
</code></pre>
","4065074","","","","","2019-06-13 08:38:24","Getting Segmentation fault: 11 while running gensim's Cosine Similarity function on a bunch of documents","<python-3.x><gensim>","1","1","0","","","CC BY-SA 4.0"
"40637537","1","","","2016-11-16 16:41:03","","3","1583","<p>I am new to topic modelling.
My aim is to find key topics from a document. I am planning to use lda for the purpose. But in lda the number of topics should be predefined.I believe if a document from some other domain which was not in the training corpus comes,it will not give proper results. Is there any alternative solution? Is my thought is correct?    </p>
","6289268","","","","","2017-05-05 21:48:42","Dynamic number of topics in topic models","<nlp><lda><gensim><topic-modeling>","1","1","3","","","CC BY-SA 3.0"
"47827130","1","","","2017-12-15 06:44:04","","0","61","<p>I am trying to do textual analysis on a bunch (about 140 ) of textual documents. Each document, after preprocessing and removing unnecessary words and stopwords, has about 7000 sentences (as determined by nlkt's sentence tokenizer) and each sentence has about 17 words on average. My job is to find hidden themes in those documents. </p>

<p>I have thought about doing topic modeling. However, I cannot decide if the data I have is enough to obtain meaningful results via LDA or is there anything else that I can do. </p>

<p>Also, how do I divide the texts into different documents? Is 140 documents (each with roughly 7000 x 17 words) enough ? or should I consider each sentence as a document. But then each document will have only 17 words on average; much like tweets. </p>

<p>Any suggestions would be helpful. 
Thanks in advance.  </p>
","2307804","","","","","2017-12-17 06:18:40","Suggestion on LDA","<python-3.x><nlp><gensim><text-analysis>","1","0","","","","CC BY-SA 3.0"
"56346717","1","","","2019-05-28 16:40:18","","3","1074","<p>I would like to see similarity between lists using <code>TFIDFVectorizer</code> and <code>CountVectorizer</code>.</p>

<p>I have lists like below:</p>

<pre class=""lang-py prettyprint-override""><code>list1 = [['i','love','machine','learning','its','awesome'],
         ['i', 'love', 'coding', 'in', 'python'],
         ['i', 'love', 'building', 'chatbots']]
list2 = ['i', 'love', 'chatbots']
</code></pre>

<p>I would like to see similarity between <code>list1[0]</code> and <code>list2</code>, <code>list1[1]</code> and <code>list2</code> , <code>list1[2]</code> and <code>list2</code>. </p>

<p>Expecting output should be like <code>[0.99 , 0.67, 0.54]</code></p>
","11568647","","1207193","","2020-06-19 14:16:10","2020-06-19 14:16:10","Calculate text similarity between lists using CountVectorizer, TFIDFVectorizer","<python><scikit-learn><gensim><countvectorizer><tfidfvectorizer>","1","0","","","","CC BY-SA 4.0"
"58619716","1","","","2019-10-30 06:28:27","","0","268","<p>My data has more than 1 million rows and while training gensim similarity model, it is making multiple .sav files (model.sav, model.sav.0, model.sav.1 and so on..). Problem is while loading, it is loading only one sub-part, instead of all the sub-parts, hence performing horribly in prediction. Parameters/options are not working as per gensim documentation.</p>

<p>As per the gensim documentation - <a href=""https://radimrehurek.com/gensim/similarities/docsim.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/similarities/docsim.html</a>
Saving as file handle and giving the following params should have worked - : </p>

<ol>
<li>model.save(fname_or_handle, separately = None)</li>
<li>model.load(filepath, mmap = 'r')</li>
</ol>

<p>Even tried to -</p>

<ol>
<li>pickle the .sav files ( this pickles the 1st shard only i.e. model.sav)</li>
<li>compressing all sub-parts as .gz file ( this compresses one shard only , not all the sub-parts) and also gives some sort of pickle error.</li>
</ol>

<pre><code>tf_idf = gensim.models.TfidfModel(corpus)

sims = gensim.similarities.Similarity('./models/model.sav',tf_idf[corpus],
                                          num_features=len(dictionary))
sims.save('./models/model.sav')

sims1 = gensim.similarities.Similarity.load(./models/model.sav)
</code></pre>

<p>Expected results should give all matching documents from corpus, but this gives only from model.sav (the file mentioned while loading). It does NOT even execute the other shards. I checked result from each shard.</p>

<p>Question: How do I use all the sub-files of gensim model to predict similarity of my test document, WITHOUT looping through every sub-file individually and then presenting union of those results.</p>
","8135339","","","","","2019-11-04 17:31:33","saving and loading multiple shards made by gensim similarity model","<python><model><gensim>","1","2","","","","CC BY-SA 4.0"
"65112423","1","","","2020-12-02 16:45:18","","0","73","<p>I am trying to reproduce the results from this repository: <a href=""https://github.com/danielricks/scholar"" rel=""nofollow noreferrer"">https://github.com/danielricks/scholar</a>. I do not have Linux and so cannot install the <code>word2vec</code> package the code uses, but it's only used for loading a pretrained word2vec model anyway, so Gensim should do the job.</p>
<p>The problem is that the pretrained model used by scholar is stored in a pickle file (provided in the <a href=""https://github.com/danielricks/scholar"" rel=""nofollow noreferrer"">Readme</a> under &quot;processed files&quot;), <code>postagged_wikipedia_for_word2vec_30kn3kv.pkl</code>.</p>
<p>When I try to open this file I got ModuleNotFoundError <code>No module named 'word2vec'</code>. I went inside the pickle file (in Notepad) and changed <code>word2vec</code> near the beginning to <code>gensim.models.word2vec</code>, but then I got ModuleNotFoundError <code>No module named 'gensim.models.word2vec'</code></p>
<p>I use Windows and so <code>word2vec</code> is <a href=""https://stackoverflow.com/questions/25643004/python-word2vec-not-installing"">not really feasible</a> to install. That is why I am trying to come up with a way to use Gensim here.</p>
","12384851","","","","","2020-12-02 17:29:25","Pickle with modulenotfound","<python><pickle><gensim>","2","0","","","","CC BY-SA 4.0"
"58610689","1","","","2019-10-29 15:35:10","","0","409","<p>I've produced GloVe vectors using the code provided by <a href=""https://github.com/stanfordnlp/GloVe/blob/master/demo.sh"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/GloVe/blob/master/demo.sh</a> using my own corpus. So, I have both the .bin file and .txt file vectors. I'm trying to import these files into gensim so I can work with them like I can word2vec vectors.</p>

<p>I've tried changing to load using both the binary format and text file format but only ended up getting a pickling error: </p>

<pre><code>models = gensim.models.Word2Vec.load(file)
</code></pre>

<p>I've tried ignoring the unicode error, which didn't work. I still got the unicode error. </p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format(file, binary=True, unicode_errors='ignore')
</code></pre>

<p>This is what I have for my code right now:</p>

<pre><code>from gensim.models import KeyedVectors
import gensim
from gensim.models import word2vec

file = 'vectors.bin'
model = KeyedVectors.load_word2vec_format(file, binary=True, unicode_errors='ignore')  
model.wv.most_similar(positive=['woman', 'king'], negative=['man'])
</code></pre>

<p>This is the error message I keep getting: </p>

<pre><code>Traceback (most recent call last):
  File ""glove_to_word2vec.py"", line 6, in &lt;module&gt;
    model = KeyedVectors.load_word2vec_format(file, binary=True)  # C  binary format
  File ""/home/users/epair/.local/lib/python3.6/site- packages/gensim/models/keyedvectors.py"", line 1498, in load_word2vec_format
    limit=limit, datatype=datatype)
  File ""/home/users/epair/.local/lib/python3.6/site-packages/gensim/models/utils_any2vec.py"", line 343, in _load_word2vec_format
    header = utils.to_unicode(fin.readline(), encoding=encoding)
  File ""/home/users/epair/.local/lib/python3.6/site-packages/gensim/utils.py"", line 359, in any2unicode
    return unicode(text, encoding, errors=errors)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe6 in position 0:  invalid continuation byte
</code></pre>

<p>The pickling error was something like this: <a href=""https://stackoverflow.com/questions/44022180/unpickling-error-while-using-word2vec-load"">Unpickling Error while using Word2Vec.load()</a></p>

<p><a href=""https://i.stack.imgur.com/celtK.png"" rel=""nofollow noreferrer"">Text file format</a></p>
","11865129","","11865129","","2019-10-30 17:07:07","2019-10-30 17:07:07","Importing GloVe vectors into gensim. UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe6 in position 0: invalid continuation byte","<python><gensim><word2vec><glove>","1","1","","","","CC BY-SA 4.0"
"65121932","1","","","2020-12-03 08:02:21","","2","373","<p>I was trying to understand the word2vec and I've decided to give it a go with a German word2vec model. Then I found <a href=""https://deepset.ai/german-word-embeddings"" rel=""nofollow noreferrer"">deepset's page</a> about their pre-trained models but I didn't understand how to use (load) word2vec model. I was expecting a single file but there are &quot;<a href=""https://int-emb-word2vec-de-wiki.s3.eu-central-1.amazonaws.com/vectors.txt"" rel=""nofollow noreferrer"">Vectors</a>&quot; and &quot;<a href=""https://int-emb-word2vec-de-wiki.s3.eu-central-1.amazonaws.com/vocab.txt"" rel=""nofollow noreferrer"">Vocab</a>&quot; text files. How can I use these files to load a pre-trained model using gensim (or any other tool)?</p>
<p><strong>UPDATE</strong>:
I've tried the answer of @gojomo and I received this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/bugra/word2vec_imp/pretrained_models/testtt.py&quot;, line 11, in &lt;module&gt;
    binary=False)
  File &quot;/home/bugra/word2vec_imp/project_envv/lib/python3.7/site-packages/gensim/models/keyedvectors.py&quot;, line 1549, in load_word2vec_format
    limit=limit, datatype=datatype)
  File &quot;/home/bugra/word2vec_imp/project_envv/lib/python3.7/site-packages/gensim/models/utils_any2vec.py&quot;, line 277, in _load_word2vec_format
    vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
  File &quot;/home/bugra/word2vec_imp/project_envv/lib/python3.7/site-packages/gensim/models/utils_any2vec.py&quot;, line 277, in &lt;genexpr&gt;
    vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
ValueError: invalid literal for int() with base 10: &quot;b'UNK'&quot;
</code></pre>
<p>So, in the Traceack, <code>vocab_size, vector_size = (int(x) for x in header.split())</code> the <code>header</code> is the first line of the Vector's text from the gensim's page. And it looks like this:</p>
<pre><code>b'UNK' -0.07903 0.01641 0.006979 -0.035038 0.006474 0.002469 -0.050103 0.142654 -0.03505 0.003106 -0.021312 0.094076 -0.018255 -0.098097 0.087143 0.105799 0.008606 -0.001315 0.069005 0.062015 0.019944 -0.007749 -0.007412 0.050015 -0.083615 0.007712 0.033161 0.017965 -0.06154 -0.017696 0.061967 0.053028 0.038143 -0.07057 0.01561 0.019588 -0.041708 0.034371 -0.066838 -0.059769 0.075711 -0.114826 0.014009 0.050187 -0.01899 -0.076014 -0.052502 0.086082 0.049812 0.008456 -0.01283 0.039918 -0.001924 -0.003752 0.031073 0.034325 0.040086 0.078946 -0.012194 0.056323 0.126129 -0.024503 0.026304 -0.074797 -0.098972 0.003672 0.051386 -0.017574 -0.050253 -0.07677 0.004362 -0.069935 -0.048108 0.020127 0.007066 -0.024247 0.041911 0.03377 -0.011906 -0.0168 -0.00355 -0.003168 0.05164 -0.055769 0.01488 -6e-06 0.094575 -0.066246 -0.111004 -0.031954 0.006958 0.005259 0.15825 0.102919 0.010383 -0.064236 -0.037729 -0.031751 -0.069492 -0.004198 -0.034654 -0.060518 -0.046611 -0.048463 -0.010096 -0.057894 -0.046687 0.062827 0.016907 0.096869 -0.036037 -0.106403 0.056466 0.095621 -0.046383 0.090213 -0.019204 -0.116271 -0.00824 -0.017732 0.037387 -0.021405 -0.040493 -0.059114 0.12289 0.032563 0.103712 0.072411 -0.106944 -0.110485 -0.027564 0.023977 -0.048099 0.036966 -0.11356 -0.009166 0.074402 0.128162 0.080086 0.112749 0.050494 0.064998 0.089217 0.029182 -0.07277 0.058653 0.061047 -0.05293 -0.01979 0.107459 0.002719 -0.008774 -0.098009 0.009321 0.099869 0.024181 -0.071247 -0.054372 0.019997 0.024442 0.108639 0.053727 -0.089804 0.118491 -0.044407 -0.045336 0.078483 0.059462 -0.012287 0.028941 0.064551 0.066738 0.029614 0.092768 0.021783 -0.018141 -0.032692 0.000178 0.021413 0.044657 -0.041903 0.027439 -0.029112 -0.027419 -0.091497 0.00712 -0.076297 -0.097602 -0.098875 -0.067403 -0.015912 0.055845 0.057585 -0.061145 -0.006828 0.044573 0.049632 0.014541 -0.024579 -0.045455 0.095474 -0.02978 -0.060053 -0.005672 -0.002711 0.059481 -0.060563 0.047562 -0.086001 0.064536 0.196527 -0.105742 -0.019043 0.038534 -0.099681 0.031009 -0.020548 -0.058781 0.064247 0.008213 0.126322 0.029859 0.013129 -0.021303 0.043993 0.033347 0.020245 0.037738 -0.02178 0.027693 -0.07024 0.004687 0.045271 -0.022966 0.014069 0.022861 -0.02787 0.082912 -0.049544 0.016079 -0.004684 0.000572 0.077382 0.036401 0.054974 -0.039538 0.002119 0.034002 -0.008836 -0.014758 0.00959 -0.064647 -0.034766 0.016912 -0.036381 -0.037106 0.073451 -0.098941 -0.092281 -0.018656 0.050538 0.041422 0.041235 0.011248 -0.106058 0.066443 0.083865 0.094636 0.004414 -0.092855 -0.027255 0.005234 0.066584 0.055394 0.023019 -0.001949 -0.066794 -0.064739 0.038924 -0.016647 0.000555 0.02428 0.016469 -0.0467 -0.035343 -0.066789 -0.025929 -0.023397 0.062855 0.020142 -0.047568 0.010299 -0.021509 -0.02826 0.029225 0.01803 0.024336 0.018226 -0.009453 -0.068584
</code></pre>
<p>Any help would be appreciated.</p>
","10183880","","3653343","","2021-06-27 10:32:55","2021-06-27 10:32:55","How to use deepset's word embedding pre-trained models using gensim?","<python><machine-learning><gensim><word2vec><word-embedding>","3","0","","","","CC BY-SA 4.0"
"57426745","1","57434746","","2019-08-09 09:01:22","","1","348","<p>What I want exactly is to cluster words and phrases, e.g.
knitting/knit loom/loom knitting/weaving loom/rainbow loom/home decoration accessories/loom knit/knitting loom/...And I don'd have corpus while I have only the words/phrases. Could I use a pre-trained model like the one from GoogleNews/Wikipedia/... to realise it?</p>

<p>I am trying now to use Gensim to load GoogleNews pre-trained model to get phrases similarity. I've been told that The GoogleNews model includes vectors of phrases and words. But I find that I could only get word-similarity while phrase-similarity fails with an error message that the phrase is not in the vocabulary. Please advise me. Thank you.</p>

<pre><code>import gensim
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

GOOGLE_MODEL = '../GoogleNews-vectors-negative300.bin'

model = gensim.models.KeyedVectors.load_word2vec_format(GOOGLE_MODEL, binary=True) 


# done well
model.most_similar(""computer"", topn=3) 

# done with error message ""computer_software"" is not in the vocabulory.
model.most_similar(""computer_software"", topn=3) 
</code></pre>
","11906455","","11906455","","2019-08-10 14:28:00","2019-08-10 14:28:00","How to Cluster words and phrases with pre-trained model on Gensim","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"36757409","1","36761265","","2016-04-21 00:13:12","","0","742","<p>I'm doing a small experiment where I have 2000 tweets as my input document. I train word2vec on this input tweets and then find the top 10 most similar words to a particular word - <code>w1</code>.</p>

<p>My concern is if I run word2vec 10 times (with same parameters) and inspect the top 10 most similar words to <code>w1</code>, gives me the same set of words (weights are also the same). </p>

<p>Now AFAIK word2vec initializes random weights at the beginning so why it's giving me the same output at different runs?</p>
","601357","","","","","2016-04-21 06:24:59","Word2Vec, most_similar(word1) returns same output on different runs","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"57757356","1","57778263","","2019-09-02 12:57:55","","0","24","<p>I have a test sentence (which define a skill, such as ""Perform equipment maintenance"") and a set of diplomas (10000 different diplomas) with description of the needed skills (=1 paragraph per diploma). My problem consists in finding the diploma closest to the test sentence in terms of semantic similarity.</p>

<p>I thought about creating a doc2vec model (multi-class, 1 class per diploma) in order to transform each diploma in feature vector, then infer vector for the test sentence and calculate cosine similarity with each feature vector. Yet, I only have one sample for each diploma. Will it still work? 
Or do I have to split the sentences of each diploma text in order to obtain several samples for a diploma ?</p>
","12009819","","","","","2019-09-03 20:21:06","Does doc2vec work with multi-class problem with only 1 sample per class?","<nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"40671057","1","41135204","","2016-11-18 06:53:42","","2","1141","<p>I've been trying to run an example of how t use word2vec from the gensim library of python but I keep getting this error </p>

<pre><code>    ValueError: The truth value of an array with more than one element is   ambiguous. Use a.any() or a.all()
</code></pre>

<p>This is my code, it's just a simple example :</p>

<pre><code>    from gensim.models import Word2Vec
    sentences = [['first', 'sentence'], ['second', 'sentence']]
    # train word2vec on the two sentences
    model = Word2Vec(sentences, min_count=1)
</code></pre>

<p>Note: I've made sure that gensim is installed with all its dependencies.</p>
","3811918","","","","","2019-12-16 13:36:25","use a.all() or a.any() error while trying to use gensim word2vec","<python-2.7><gensim><word2vec>","3","0","","","","CC BY-SA 3.0"
"58635642","1","58649746","","2019-10-31 01:07:27","","0","605","<p>I am training doc2vec with corpus file, which is very huge.     </p>

<pre><code>model = Doc2Vec(dm=1, vector_size=200, workers=cores, comment='d2v_model_unigram_dbow_200_v1.0')
model.build_vocab(corpus_file=path)
model.train(corpus_file=path, total_examples=model.corpus_count, epochs=model.iter)
</code></pre>

<p>I want to know how to get value of total_words.</p>

<p>Edit:</p>

<pre><code>total_words=model.corpus_total_words
</code></pre>

<p>Is this right?</p>
","2281101","","2281101","","2019-10-31 01:16:47","2019-10-31 22:08:43","total_words must be provided alongside corpus_file argument","<gensim>","1","0","","","","CC BY-SA 4.0"
"40660127","1","","","2016-11-17 16:28:43","","0","735","<p>My question is related to this post, <a href=""https://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda"">Document topical distribution in Gensim LDA</a>, the documentation for gensim.models.ldamodel states that ""minimum_probability controls filtering the topics returned for a document (bow)."" however, ldamodel[corpus] returns all possible topics with their probability (even below the number set in minimum_probability). what is the difference between these two? 
python 2.7.5
gensim 0.13.3</p>

<p>thank you</p>
","5031727","","-1","","2017-05-23 12:30:20","2017-09-13 13:08:12","gensim Latent Dirichlet Allocation minimum_probability vs print_topics","<python><lda><gensim>","1","1","","","","CC BY-SA 3.0"
"47812930","1","","","2017-12-14 12:02:09","","0","605","<p>I would like to know if there is any scientific explanation why word2vec models like CBOW perform poorly on small data. Here's what I tested;</p>

<pre><code>data=[[context1], [context2], [context3]......[contextn]]

model=trained word2vec model

model.most_similar('word')
output=[word not in even in top-10]
</code></pre>

<p>I retrained the model with 10 times the dataset.</p>

<pre><code>model.most_similar(word)
output=[word in the 10 most similar words]
</code></pre>

<p>Is there any scientific reason for the improvement in performance as the data size increased other than the increase in the word count with increase in data?</p>
","2274879","","472495","","2017-12-14 12:03:51","2017-12-14 18:09:56","Scientific explanation why word2vec models perform poorly on small data","<word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"58556924","1","58558025","","2019-10-25 10:37:05","","0","618","<p>I am new to NLP and Word Embeddings and still need to learn many concepts within these topics, so any pointers would be appreciated. This question is related to <a href=""https://stackoverflow.com/questions/45571295/semantic-similarity-across-multiple-languages"">this</a> and <a href=""https://stackoverflow.com/questions/51233632/word2vec-gensim-multiple-languages?noredirect=1&amp;lq=1"">this</a>, and I think there may have been developments since these questions had been asked. Facebook <a href=""https://arxiv.org/pdf/1710.04087.pdf"" rel=""nofollow noreferrer"">MUSE</a> provides aligned, supervised <a href=""https://github.com/facebookresearch/MUSE#multilingual-word-embeddings"" rel=""nofollow noreferrer"">word embeddings for 30 languages</a>, and it can be used to calculate word similarity across different languages. As far as I understand, The embeddings provided by MUSE satisfy the requirement of <a href=""https://stackoverflow.com/questions/45571295/semantic-similarity-across-multiple-languages"">coordinate space compatibilty</a>. It seems that it is possible to <a href=""https://datascience.stackexchange.com/questions/20071/how-do-i-load-fasttext-pretrained-model-with-gensim"">load these embeddings into libraries such as Gensim</a>, but I wonder: </p>

<ol>
<li>Is it possible to load multiple-language word embeddings 
into Gensim (or other libraries), and if so:</li>
<li>What type of similarity measure
might fit in this use case?</li>
<li>How to use these loaded word embeddings
to calculate cross-lingual similarity score of phrases* instead of
words?</li>
</ol>

<p>*e.g., ""<em>√ñPNV</em>"" in German vs ""<em>Trasporto pubblico locale</em>"" in Italian for the English term ""<em>Public Transport</em>"". </p>

<p>I am open o any implementation (libraries/languages/embeddings) though I may need some time to learn this topic. Thank you in advance.</p>
","548451","","548451","","2019-10-25 10:43:30","2020-09-21 14:40:30","Calculate Cross-Lingual Phrase Similarity (using e.g., MUSE and Gensim)","<python><nlp><multilingual><gensim><word-embedding>","2","0","","","","CC BY-SA 4.0"
"30770919","1","","","2015-06-11 03:13:23","","0","1768","<p>I am running (from Gensim)</p>

<pre><code>top_topics(corpus, num_topics=5, num_words=20)
</code></pre>

<p>I get the error:</p>

<pre><code>for topic in lda.top_topics(corpus=corpus, num_topics=5, num_words=20):
File ""/Library/Python/2.7/site-packages/gensim/models/ldamodel.py"", line 760, in top_topics
bestn = np.argsort(topic)[::-1][:num_words]
NameError: global name 'np' is not defined
</code></pre>

<p>I thought this was odd and I can see the file (indeed it has <code>import numpy</code> but not <code>import numpy as np</code>).</p>

<p>After adding <code>import numpy as np</code> and trying to run it again:</p>

<pre><code>  File ""/Library/Python/2.7/site-packages/gensim/models/ldamodel.py"", line 772, in top_topics
    if len(list(ifilter(lambda x: x[0] == id,corpus[document]))) &gt; 0:
</code></pre>

<p>NameError: global name 'ifilter' is not defined</p>

<p>Which was fixed by changing <code>from itertools import chain</code> to <code>from itertools import chain, ifilter</code>. The module then works perfectly. </p>

<p>So, I guess my question is whether this was an error specific to my system (is there some kind of 'import all' python trick that doesn't work for me?).</p>
","2795733","","2795733","","2015-06-11 03:42:18","2018-01-05 07:10:22","top_topics Gensim NameError: global name 'np' is not defined","<python><gensim>","1","2","","","","CC BY-SA 3.0"
"58574772","1","","","2019-10-26 21:12:45","","0","1729","<p>Getting the error below when I try to install <code>gensim</code> on <code>Python 3.8.0</code>. How can I make it work, is there any workaround?</p>

<p>Here is the full error stacktrace:</p>

<pre><code>The following command was executed:

C:\Users\talha\AppData\Local\Programs\Python\Python38\Scripts\pipenv.exe update --dev

The exit code: 1
The error output of the command:

Running $ pipenv lock then $ pipenv sync.
Installing dependencies from Pipfile.lock (ec3971)‚Ä¶
Installing initially failed dependencies‚Ä¶

Locking [dev-packages] dependencies‚Ä¶
Locking [packages] dependencies‚Ä¶

[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...
[=== ] Locking...
[ ===] Locking...
[  ==] Locking...
[   =] Locking...
[    ] Locking...
[   =] Locking...
[  ==] Locking...
[ ===] Locking...
[====] Locking...
[=== ] Locking...
[==  ] Locking...
[=   ] Locking...
[    ] Locking...
[=   ] Locking...
[==  ] Locking...Success! 
Updated Pipfile.lock (ec3971)!
An error occurred while installing scipy==1.3.1 --hash=sha256:0baa64bf42592032f6f6445a07144e355ca876b177f47ad8d0612901c9375bef --hash=sha256:243b04730d7223d2b844bda9500310eecc9eda0cba9ceaf0cde1839f8287dfa8 --hash=sha256:2643cfb46d97b7797d1dbdb6f3c23fe3402904e3c90e6facfe6a9b98d808c1b5 --hash=sha256:396eb4cdad421f846a1498299474f0a3752921229388f91f60dc3eda55a00488 --hash=sha256:3ae3692616975d3c10aca6d574d6b4ff95568768d4525f76222fb60f142075b9 --hash=sha256:435d19f80b4dcf67dc090cc04fde2c5c8a70b3372e64f6a9c58c5b806abfa5a8 --hash=sha256:46a5e55850cfe02332998b3aef481d33f1efee1960fe6cfee0202c7dd6fc21ab --hash=sha256:75b513c462e58eeca82b22fc00f0d1875a37b12913eee9d979233349fce5c8b2 --hash=sha256:7ccfa44a08226825126c4ef0027aa46a38c928a10f0a8a8483c80dd9f9a0ad44 --hash=sha256:89dd6a6d329e3f693d1204d5562dd63af0fd7a17854ced17f9cbc37d5b853c8d --hash=sha256:a81da2fe32f4eab8b60d56ad43e44d93d392da228a77e229e59b51508a00299c --hash=sha256:a9d606d11eb2eec7ef893eb825017fbb6eef1e1d0b98a5b7fc11446ebeb2b9b1 --hash=sha256:ac37eb652248e2d7cbbfd89619dce5ecfd27d657e714ed049d82f19b162e8d45 --hash=sha256:cbc0611699e420774e945f6a4e2830f7ca2b3ee3483fca1aa659100049487dd5 --hash=sha256:d02d813ec9958ed63b390ded463163685af6025cb2e9a226ec2c477df90c6957 --hash=sha256:dd3b52e00f93fd1c86f2d78243dfb0d02743c94dd1d34ffea10055438e63b99d! Will try again.
[pipenv.exceptions.InstallError]:   File ""c:\users\talha\appdata\local\programs\python\python38\lib\site-packages\pipenv\core.py"", line 2604, in do_sync
[pipenv.exceptions.InstallError]:       do_init(
[pipenv.exceptions.InstallError]:   File ""c:\users\talha\appdata\local\programs\python\python38\lib\site-packages\pipenv\core.py"", line 1246, in do_init
[pipenv.exceptions.InstallError]:       do_install_dependencies(
[pipenv.exceptions.InstallError]:   File ""c:\users\talha\appdata\local\programs\python\python38\lib\site-packages\pipenv\core.py"", line 858, in do_install_dependencies
[pipenv.exceptions.InstallError]:       batch_install(
[pipenv.exceptions.InstallError]:   File ""c:\users\talha\appdata\local\programs\python\python38\lib\site-packages\pipenv\core.py"", line 763, in batch_install
[pipenv.exceptions.InstallError]:       _cleanup_procs(procs, not blocking, failed_deps_queue, retry=retry)
[pipenv.exceptions.InstallError]:   File ""c:\users\talha\appdata\local\programs\python\python38\lib\site-packages\pipenv\core.py"", line 681, in _cleanup_procs
[pipenv.exceptions.InstallError]:       raise exceptions.InstallError(c.dep.name, extra=err_lines)
[pipenv.exceptions.InstallError]: ['Collecting scipy==1.3.1', '  Using cached https://files.pythonhosted.org/packages/ee/5b/5afcd1c46f97b3c2ac3489dbc95d6ca28eacf8e3634e51f495da68d97f0f/scipy-1.3.1.tar.gz', '  Installing build dependencies: started', '  Installing build dependencies: still running...', ""  Installing build dependencies: finished with status 'done'"", '  Getting requirements to build wheel: started', ""  Getting requirements to build wheel: finished with status 'done'"", '    Preparing wheel metadata: started', ""    Preparing wheel metadata: finished with status 'error'""]
[pipenv.exceptions.InstallError]: ['ERROR: Command errored out with exit status 1:', ""     command: 'd:\\.virtualenvs\\pyemoji-fylffy3g\\scripts\\python.exe' 'd:\\.virtualenvs\\pyemoji-fylffy3g\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\talha\\AppData\\Local\\Temp\\tmp88ndh32u'"", '         cwd: C:\\Users\\talha\\AppData\\Local\\Temp\\pip-install-ppv6gock\\scipy', '    Complete output (172 lines):', '    lapack_opt_info:', '    lapack_mkl_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""      libraries mkl_rt not found in ['d:\\\\.virtualenvs\\\\pyemoji-fylffy3g\\\\lib', 'C:\\\\']"", '      NOT AVAILABLE', '    ', '    openblas_lapack_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""      libraries openblas not found in ['d:\\\\.virtualenvs\\\\pyemoji-fylffy3g\\\\lib', 'C:\\\\']"", ""    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'"", '    customize GnuFCompiler', '    Could not locate executable g77', '    Could not locate executable f77', '    customize IntelVisualFCompiler', '    Could not locate executable ifort', '    Could not locate executable ifl', '    customize AbsoftFCompiler', '    Could not locate executable f90', '    customize CompaqVisualFCompiler', '    Could not locate executable DF', '    customize IntelItaniumVisualFCompiler', '    Could not locate executable efl', '    customize Gnu95FCompiler', '    Could not locate executable gfortran', '    Could not locate executable f95', '    customize G95FCompiler', '    Could not locate executable g95', '    customize IntelEM64VisualFCompiler', '    customize IntelEM64TFCompiler', '    Could not locate executable efort', '    Could not locate executable efc', '    customize PGroupFlangCompiler', '    Could not locate executable flang', ""    don't know how to compile Fortran code on platform 'nt'"", '      NOT AVAILABLE', '    ', '    openblas_clapack_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""      libraries openblas,lapack not found in ['d:\\\\.virtualenvs\\\\pyemoji-fylffy3g\\\\lib', 'C:\\\\']"", '      NOT AVAILABLE', '    ', '    atlas_3_10_threads_info:', '    Setting PTATLAS=ATLAS', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries tatlas,tatlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries tatlas,tatlas not found in C:\\', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in C:\\', ""    &lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&gt;"", '      NOT AVAILABLE', '    ', '    atlas_3_10_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries satlas,satlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries satlas,satlas not found in C:\\', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in C:\\', ""    &lt;class 'numpy.distutils.system_info.atlas_3_10_info'&gt;"", '      NOT AVAILABLE', '    ', '    atlas_threads_info:', '    Setting PTATLAS=ATLAS', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries ptf77blas,ptcblas,atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries ptf77blas,ptcblas,atlas not found in C:\\', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in C:\\', ""    &lt;class 'numpy.distutils.system_info.atlas_threads_info'&gt;"", '      NOT AVAILABLE', '    ', '    atlas_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries f77blas,cblas,atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in d:\\.virtualenvs\\pyemoji-fylffy3g\\lib', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries f77blas,cblas,atlas not found in C:\\', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', '      libraries lapack_atlas not found in C:\\', ""    &lt;class 'numpy.distutils.system_info.atlas_info'&gt;"", '      NOT AVAILABLE', '    ', '    lapack_info:', ""    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils"", '    customize MSVCCompiler', ""      libraries lapack not found in ['d:\\\\.virtualenvs\\\\pyemoji-fylffy3g\\\\lib', 'C:\\\\']"", '      NOT AVAILABLE', '    ', '    lapack_src_info:', '      NOT AVAILABLE', '    ', '      NOT AVAILABLE', '    ', '    C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\ma\\core.py:4462: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?', '      if self.shape is ():', ""    setup.py:386: UserWarning: Unrecognized setuptools command ('dist_info --egg-base C:\\Users\\talha\\AppData\\Local\\Temp\\pip-modern-metadata-kh8x6t2r'), proceeding with generating Cython sources and expanding templates"", '      warnings.warn(""Unrecognized setuptools command (\'{}\'), proceeding with ""', '    C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\distutils\\misc_util.py:464: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?', ""      return is_string(s) and ('*' in s or '?' is s)"", '    Running from scipy source directory.', '    C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\distutils\\system_info.py:624: UserWarning:', '        Atlas (http://math-atlas.sourceforge.net/) libraries not found.', '        Directories to search for the libraries can be specified in the', '        numpy/distutils/site.cfg file (section [atlas]) or by setting', '        the ATLAS environment variable.', '      self.calc_info()', '    C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\distutils\\system_info.py:624: UserWarning:', '        Lapack (http://www.netlib.org/lapack/) libraries not found.', '        Directories to search for the libraries can be specified in the', '        numpy/distutils/site.cfg file (section [lapack]) or by setting', '        the LAPACK environment variable.', '      self.calc_info()', '    C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\distutils\\system_info.py:624: UserWarning:', '        Lapack (http://www.netlib.org/lapack/) sources not found.', '        Directories to search for the sources can be specified in the', '        numpy/distutils/site.cfg file (section [lapack_src]) or by setting', '        the LAPACK_SRC environment variable.', '      self.calc_info()', '    Traceback (most recent call last):', '      File ""d:\\.virtualenvs\\pyemoji-fylffy3g\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py"", line 257, in &lt;module&gt;', '        main()', '      File ""d:\\.virtualenvs\\pyemoji-fylffy3g\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py"", line 240, in main', ""        json_out['return_val'] = hook(**hook_input['kwargs'])"", '      File ""d:\\.virtualenvs\\pyemoji-fylffy3g\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py"", line 110, in prepare_metadata_for_build_wheel', '        return hook(metadata_directory, config_settings)', '      File ""C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py"", line 156, in prepare_metadata_for_build_wheel', '        self.run_setup()', '      File ""C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py"", line 236, in run_setup', '        super(_BuildMetaLegacyBackend,', '      File ""C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py"", line 142, in run_setup', ""        exec(compile(code, __file__, 'exec'), locals())"", '      File ""setup.py"", line 505, in &lt;module&gt;', '        setup_package()', '      File ""setup.py"", line 501, in setup_package', '        setup(**metadata)', '      File ""C:\\Users\\talha\\AppData\\Local\\Temp\\pip-build-env-1qw9t5p7\\overlay\\Lib\\site-packages\\numpy\\distutils\\core.py"", line 135, in setup', '        config = configuration()', '      File ""setup.py"", line 403, in configuration', '        raise NotFoundError(msg)', '    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.', '    ----------------------------------------', ""ERROR: Command errored out with exit status 1: 'd:\\.virtualenvs\\pyemoji-fylffy3g\\scripts\\python.exe' 'd:\\.virtualenvs\\pyemoji-fylffy3g\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\talha\\AppData\\Local\\Temp\\tmp88ndh32u' Check the logs for full command output.""]
ERROR: ERROR: Package installation failed...

Error Running Pipenv
</code></pre>

<p>p.s. Using <code>pipenv</code> as the packaging tool.</p>
","282855","","282855","","2019-10-26 21:21:36","2019-10-27 18:03:04","Unable to install 'gensim' on Python 3.8.0","<python-3.x><scipy><gensim><pipenv><python-3.8>","1","4","","","","CC BY-SA 4.0"
"56341586","1","","","2019-05-28 11:48:09","","1","98","<p>I am trying to perform Topic modelling on Databricks using the Gesim wrapper for Mallet. </p>

<p>I already have running code for the same on my Local system.</p>

<p><strong>Here is some sample code that already works on my local System:</strong></p>

<pre><code>import os

os.environ['MALLET_HOME'] = 'C:/Users/Soumadiptya.c/Desktop/mallet-2.0.8'

mallet_path = 'C:/Users/Soumadiptya.c/Desktop/mallet-2.0.8/bin/mallet'

ldamallet_model = gensim.models.wrappers.ldamallet.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word, random_seed = 123)
</code></pre>

<p><strong>Here is what I am trying to execute on my Databricks instance:</strong></p>

<pre><code>os.environ['MALLET_HOME'] = '/dbfs/FileStore/tables/mallet-2.0.8'

mallet_path_new = '/dbfs/FileStore/tables/mallet-2.0.8/bin/mallet'

new_model = gensim.models.wrappers.ldamallet.LdaMallet(mallet_path_new, corpus=corpus, num_topics=20, id2word=id2word)
</code></pre>

<p>But this exits with the following error:</p>

<pre><code>CalledProcessError: Command '/dbfs/FileStore/tables/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input /tmp/e091ce_corpus.txt --output /tmp/e091ce_corpus.mallet' returned non-zero exit status 126
</code></pre>

<p>Please note that I have already imported the necessary mallet files to the mentioned directories and the paths themselves exist. I'm assuming the problem is something with setting up Env variables inside databricks but unable to figure out. Any help would be much appreciated.</p>
","6687283","","10010216","","2019-05-28 11:51:38","2019-05-28 11:51:38","Unable to perform Topic Modelling in Databricks with gensim mallet","<python><gensim><databricks><lda><mallet>","0","0","","","","CC BY-SA 4.0"
"51275484","1","","","2018-07-11 00:09:17","","0","73","<p>We intend to deploy a trained model in production. since we can not keep the same in the code base, we need to upload into the cloud and refer it on runtime.</p>

<p>We are using kubernetes, and I'm relatively new to it. Below is my stepwise understanding on how to solve this.</p>

<ol>
<li>build a persistent volume with my trained model (size around 30MB)</li>
<li>mount the persistent volume into pod with a single container.</li>
<li>keep this pod running. refer to the model from a python script via pod.</li>
</ol>

<p>I tried referring documentation <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/"" rel=""nofollow noreferrer"">pv</a> with no luck. I also tried to move the model to PV via ""kubectl cp"", with no success.</p>

<p>Any idea on how to resolve this? any helps would be appreciated. </p>
","8572337","","","","","2018-07-11 00:09:17","trained model on Kubernetes","<machine-learning><kubernetes><nlp><gensim><persistent-volumes>","0","3","0","","","CC BY-SA 4.0"
"65487215","1","","","2020-12-29 04:14:50","","0","228","<p>I wanna try this model doc_to_vec as my experiment</p>
<p><a href=""http://tutorialspoint.com/gensim/gensim_doc2vec_model.htm"" rel=""nofollow noreferrer"">http://tutorialspoint.com/gensim/gensim_doc2vec_model.htm</a></p>
<p>I want to convert my dataset to the corpus as a training dataset and apply the Gensim model.</p>
<p>Here is my dataset link</p>
<p><a href=""https://drive.google.com/file/d/1S80I_5zkjJfeTzby7OjIqrs1vMJI6jVo/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1S80I_5zkjJfeTzby7OjIqrs1vMJI6jVo/view?usp=sharing</a></p>
<p>I have referred this StackOverflow question but can't work</p>
<p><a href=""https://stackoverflow.com/questions/49088978/how-to-create-corpus-from-pandas-data-frame-to-operate-with-nltk/49104725"">How to create corpus from pandas data frame to operate with NLTK</a></p>
<p>you can also check my code here google colab</p>
<p><a href=""https://colab.research.google.com/drive/1BmBNrfsxQ0AIJH_1hfMaMAceQLh2Xk7Q?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1BmBNrfsxQ0AIJH_1hfMaMAceQLh2Xk7Q?usp=sharing</a></p>
<pre><code>import pandas as pd
dataset = pd.read_csv('ADL_Two_column_MoCo.csv',encoding = 'unicode_escape')
dataset = dataset.dropna()

import gensim
def tagged_document(list_of_list_of_words):
    for i, list_of_words in enumerate(list_of_list_of_words):
        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])


data = [dataset]
data
data_for_training = list(tagged_document(data))

model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)

model.build_vocab(data_for_training)

model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)

len(data_for_training)
1

data_for_training

    [TaggedDocument(words=                                       Smile Canonical              Column  \
 0                         C1=CC=C(C=C1)C2OC(C(O2)CO)CO        CHIRALPAK AD   
 1                     C1=CC=C(C=C1)C(C(C2=CC=CC=C2)O)O        CHIRALPAK AD   
 2                        CC(C1=CC=C(C=C1)C2=CC=CC=C2)O        CHIRALPAK AD   
 5    CC(C1=CC=CC=C1)OC(=O)C2=CC(=CC(=C2)[N+](=O)[O-...        CHIRALPAK AD   
 6       C1=CC=C2C(=C1)C=CC(=C2C3=C(C=CC4=CC=CC=C43)O)O        CHIRALPAK AD   
 ..                                                 ...                 ...   
 839             C1CC(=O)NC(=O)C1N2C(=O)C3=CC=CC=C3C2=O  CHROMEGACHIRAL CCJ   
 840              CC(C1=CC=C(S1)C(=O)C2=CC=CC=C2)C(=O)O  CHROMEGACHIRAL CCJ   
 841  CCC(COC(=O)C1=CC(=C(C(=C1)OC)OC)OC)(C2=CC=CC=C...  CHROMEGACHIRAL CCJ   
 842  CCC(COC(=O)C1=CC(=C(C(=C1)OC)OC)OC)(C2=CC=CC=C...  CHROMEGACHIRAL CCJ   
 843  CCC(COC(=O)C1=CC(=C(C(=C1)OC)OC)OC)(C2=CC=CC=C...  CHROMEGACHIRAL CCJ  




                                      Mobile phase  
 0                                        methanol  
 1                              n-hexane / ethanol  
 2                            water / acetonitrile  
 5                                        methanol  
 6                           n-hexane / 2-propanol  
 ..                                            ...  
 839                                      methanol  
 840  n-hexane / 2-propanol / trifluoroacetic acid  
 841         n-heptane / 2-propanol / diethylamine  
 842                         n-hexane / 2-propanol  
 843                       methanol / diethylamine  
 
 [828 rows x 3 columns], tags=[0])]
</code></pre>
<p>This is the value which I get.</p>
<p><a href=""https://i.stack.imgur.com/Lrgoe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Lrgoe.png"" alt=""Training data output"" /></a>
I'm getting this error</p>
<pre><code> RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-45-72344a512bb5&gt; in &lt;module&gt;
----&gt; 1 model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\doc2vec.py in train(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)
    555             sentences=documents, corpus_file=corpus_file, total_examples=total_examples, total_words=total_words,
    556             epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,
--&gt; 557             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)
    558 
    559     @classmethod

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py in train(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)
   1065             total_words=total_words, epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,
   1066             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks,
-&gt; 1067             **kwargs)
   1068 
   1069     def _get_job_params(self, cur_epoch):

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py in train(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)
    533             epochs=epochs,
    534             total_examples=total_examples,
--&gt; 535             total_words=total_words, **kwargs)
    536 
    537         for callback in self.callbacks:

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py in _check_training_sanity(self, epochs, total_examples, total_words, **kwargs)
   1171 
   1172         if not self.wv.vocab:  # should be set by `build_vocab`
-&gt; 1173             raise RuntimeError(&quot;you must first build vocabulary before training the model&quot;)
   1174         if not len(self.wv.vectors):
   1175             raise RuntimeError(&quot;you must initialize vectors before training the model&quot;)

RuntimeError: you must first build vocabulary before training the model
</code></pre>
<p>while I already have made vocabulary but the problem in the data frame.</p>
","13488976","","13488976","","2021-01-04 05:21:53","2021-01-27 16:25:19","Can I use Python pandas dataframe to NLP corpus or documentation?","<python><pandas><dataframe><gensim><corpus>","1","8","","","","CC BY-SA 4.0"
"27615804","1","","","2014-12-23 07:21:19","","1","1445","<p>I am trying to install Gensim which I downloaded from <a href=""https://pypi.python.org/pypi/gensim#downloads"" rel=""nofollow noreferrer"">this</a> site. The installation using pip is also not working for me. I am getting the error below.</p>

<pre><code>C:\Users\Anirudh\Downloads\gensim-0.10.3&gt;python setup.py install
running install
running bdist_egg
running egg_info
writing requirements to gensim.egg-info\requires.txt
writing gensim.egg-info\PKG-INFO
writing top-level names to gensim.egg-info\top_level.txt
writing dependency_links to gensim.egg-info\dependency_links.txt
reading manifest file 'gensim.egg-info\SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no files found matching '*.sh' under directory '.'
no previously-included directories found matching 'docs\src*'
writing manifest file 'gensim.egg-info\SOURCES.txt'
installing library code to build\bdist.win-amd64\egg
running install_lib
running build_py
running build_ext
building 'gensim.models.word2vec_inner' extension
Unable to find vcvarsall.bat
an integer is required
Traceback (most recent call last):
  File ""setup.py"", line 166, in &lt;module&gt;
    include_package_data=True,
  File ""C:\Python27\lib\distutils\core.py"", line 151, in setup
    dist.run_commands()
  File ""C:\Python27\lib\distutils\dist.py"", line 953, in run_commands
    self.run_command(cmd)
  File ""C:\Python27\lib\distutils\dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""C:\Python27\lib\site-packages\distribute-0.6.49-py2.7.egg\setuptools\com
mand\install.py"", line 73, in run
    self.do_egg_install()
  File ""C:\Python27\lib\site-packages\distribute-0.6.49-py2.7.egg\setuptools\com
mand\install.py"", line 93, in do_egg_install
    self.run_command('bdist_egg')
  File ""C:\Python27\lib\distutils\cmd.py"", line 326, in run_command
    self.distribution.run_command(command)
  File ""C:\Python27\lib\distutils\dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""C:\Python27\lib\site-packages\distribute-0.6.49-py2.7.egg\setuptools\com
mand\bdist_egg.py"", line 179, in run
    cmd = self.call_command('install_lib', warn_dir=0)
  File ""C:\Python27\lib\site-packages\distribute-0.6.49-py2.7.egg\setuptools\com
mand\bdist_egg.py"", line 166, in call_command
    self.run_command(cmdname)
  File ""C:\Python27\lib\distutils\cmd.py"", line 326, in run_command
    self.distribution.run_command(command)
  File ""C:\Python27\lib\distutils\dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""C:\Python27\lib\site-packages\distribute-0.6.49-py2.7.egg\setuptools\com
mand\install_lib.py"", line 20, in run
    self.build()
  File ""C:\Python27\lib\distutils\command\install_lib.py"", line 111, in build
    self.run_command('build_ext')
  File ""C:\Python27\lib\distutils\cmd.py"", line 326, in run_command
    self.distribution.run_command(command)
  File ""C:\Python27\lib\distutils\dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""setup.py"", line 71, in run
    ""There was an issue with your platform configuration - see above."")
TypeError: an integer is required
</code></pre>

<p>I have seen the <a href=""https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt"">same question</a> in stackoverflow. I also did what was mentioned in <a href=""https://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat"">this</a> question. I have installed Microsoft c++ compiler for python 2.7. Is there any windows binary for Gensim? How can I install it.</p>
","2598279","","-1","","2017-05-23 11:52:16","2014-12-31 14:07:59","Difficulty installing Gensim using from source and pip","<python><gensim>","2","0","1","","","CC BY-SA 3.0"
"57755481","1","","","2019-09-02 10:37:09","","1","423","<p>I cannot update the training of my gensim fasttext model with the command : model.build_vocab</p>

<p>I think the key is ""AttributeError: 'FastText' object has no attribute 'syn1neg'""</p>

<p>Please give me some suggestion. Thanks a lot</p>

<h3>Load the pre-trained model, not pretrained vector to make sure that i can train the model</h3>

<p>print('load fasttext pretrain model ')
pretrained_model=FastText_gensim.load(pretrained_model_file)</p>

<h3>Load the tokens of articles i wanna update and convert the tokens into list of list</h3>

<p>sent=token_df['token'].values.tolist()   </p>

use the "".build_vocab"" of  pretrain model and state ""update = True""

<p>pretrained_model.build_vocab(sent,update=True)</p>

<p>Traceback (most recent call last):
File ""C:/Users/marcus/PycharmProjects/DIVA_CWS/FastText_pretrain.py"", line 313, in 
pretrained_model.build_vocab(sent,update=True)
File ""C:\Users\marcus\Desktop\DIVA_CWS\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 712, in build_vocab
self.finalize_vocab(update=update)  # build tables &amp; arrays
File ""C:\Users\marcus\Desktop\DIVA_CWS\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 953, in finalize_vocab
self.update_weights()
File ""C:\Users\marcus\Desktop\DIVA_CWS\lib\site-packages\gensim\models\deprecated\word2vec.py"", line 1373, in update_weights
self.syn1neg = vstack([self.syn1neg, zeros((gained_vocab, self.layer1_size), dtype=REAL)])
AttributeError: 'FastText' object has no attribute 'syn1neg'</p>
","8501756","","","","","2019-09-20 10:52:09","Any bugs on FastText.build_vocab?","<python><gensim><word-embedding><fasttext><incremental-build>","1","1","","","","CC BY-SA 4.0"
"66955028","1","","","2021-04-05 14:57:47","","0","63","<p>Hello all I am having some trouble in transferring a trained LDA mallet model (using Gensim v 3.8.3 [as v 4.0.0 has dropped the mallet wrapper]) that was trained locally to an online instance of Colab getting consistently a</p>
<p><code>ValueError: EOF: reading array data, expected xx byes got yy</code></p>
<p>when using joblib to load the saved model.</p>
<p>When it comes to saving my model Gensim Mallet wrapper allows you to pass the string parameter preface which specifies a path for outputting sensitive mallet-related files:</p>
<pre><code>modelcorpus.txt
modeldoctopics.txt
modelinferencer.mallet
modelstate.mallet.gz (when unzipped produces modelstate.mallet)
modeltopickeys.txt
</code></pre>
<p>and then I use joblib to export:</p>
<pre><code>The corpus
id2word dictionary
mallet model itself
</code></pre>
<p>When attempting to rerun locally in a different console I am able to load the model and infer topics on a new document, but after uploading all those files to the Colab instance I get the aforementioned error.</p>
<p>Is there anyone who is familiar with the intricacies of Mallet and working pre-trained models on Colab?</p>
<p>Thank you</p>
","8218719","","","","","2021-04-05 14:57:47","Successfully running a pre-trained LDA-Mallet model in Google Colab and infering topics of unseen documents","<python><google-colaboratory><gensim><lda><mallet>","0","0","","","","CC BY-SA 4.0"
"53821303","1","","","2018-12-17 18:44:42","","1","144","<p>I am working on the 20newsgroup dataset using Python. After using CountVectorizer on it and then using the gensim api for augmented term frequency. I tried fitting it but am getting this error.</p>

<p>Here is my code:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=2000)
from sklearn.datasets import fetch_20newsgroups
twenty_train = fetch_20newsgroups(subset='train', shuffle=True)
X_train_counts = count_vect.fit_transform(twenty_train.data)
from gensim.sklearn_api import TfIdfTransformer
model = TfIdfTransformer(smartirs='atn')
tfidf_aug = model.fit_transform(X_train_counts())
</code></pre>

<p>After running the above code I get this error: </p>

<p><code>TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]</code></p>

<p>After using getnz() at the end like this.</p>

<pre><code>tfidf_aug = model.fit_transform(X_train_counts().getnnz())
</code></pre>

<p>I get this error:</p>

<p><code>TypeError: 'int' object is not iterable</code></p>
","10802686","","6347629","","2019-01-19 08:49:08","2019-01-19 08:49:08","Augmented Frequency on 20newsgroup dataset.TypeError: 'int' object is not iterable","<python><scikit-learn><nlp><gensim>","1","0","1","","","CC BY-SA 4.0"
"50607378","1","54354852","","2018-05-30 14:34:49","","3","3207","<p>I¬¥m currently trying to evaluate my topic models with gensim topiccoherencemodel:</p>

<pre><code>from gensim.models.coherencemodel import CoherenceModel
cm_u_mass = CoherenceModel(model = model1, corpus = corpus1, coherence = 'u_mass')
coherence_u_mass = cm_u_mass.get_coherence()

print('\nCoherence Score: ', coherence_u_mass)
</code></pre>

<p>The output is just negative values. Is this correct? Can anybody provide a formula or something how u_mass works?</p>
","9751594","","","","","2019-01-24 20:29:36","Negative Values: Evaluate Gensim LDA with Topic Coherence","<python-3.x><gensim><evaluation><topic-modeling>","1","0","1","","","CC BY-SA 4.0"
"47978579","1","","","2017-12-26 12:20:35","","0","509","<p>I am using the following function to load my word2vec model.</p>

<pre><code>def __init__(self, filename):
        print filename
        try:
            self.model = gensim.models.Word2Vec.load(filename)
        except cPickle.UnpicklingError:
            load = gensim.models.Word2Vec.load_word2vec_format
            self.model = load(filename, binary=True)
</code></pre>

<p>However, I am getting the following error when I try to do it.</p>

<pre><code>Traceback (most recent call last):
  File ""./explore"", line 70, in &lt;module&gt;
    api_controller.model = Model(sys.argv[1])
  File ""/home/volka/Documents/projects/word2vec-explorer/explorer.py"", line 77, in __init__
    self.model = gensim.models.Word2Vec.load(filename)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 1458, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 256, in load
    obj = unpickle(fname)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 920, in unpickle
    return _pickle.loads(f.read())
AttributeError: 'module' object has no attribute 'call_on_class_only'
</code></pre>

<p>The genism version I am using in both the versions are 0.12.3.</p>

<p>Please let me know where I am making it wrong?</p>

<hr>

<p>This is how I tried to remove call_on_class_only.</p>

<pre><code>model = word2vec.Word2Vec(text, sg=0, negative=5, hs=0)
model.save(""test_project"")

#load, delete and save
model_1 = word2vec.Word2Vec.load(""test_project"")
del model_1.call_on_class_only
model.save(model_name_2)
</code></pre>

<p>It gives me the following error: <code>AttributeError: call_on_class_only</code></p>

<p>Please help me.</p>
","","user8871463","","user8871463","2017-12-28 03:49:45","2017-12-28 03:49:45","Error when loading the word2vec model","<word2vec><gensim>","0","5","","","","CC BY-SA 3.0"
"54078386","1","","","2019-01-07 16:40:36","","1","656","<p>I have downloaded Google's pretrained word embeddings as a binary file here (GoogleNews-vectors-negative300.bin.gz). I want to be able to filter the embedding based on some vocabulary. </p>

<p>I first tried loading the bin file as a KeyedVector object, and then creating a dictionary that uses its vocabulary along with another vocabulary as a filter. However, it takes a long time.</p>

<pre><code>  # X is the vocabulary we are interested in 
  embeddings = KeyedVectors.load_word2vec_format('GoogleNews-vectors- 
  negative300.bin.gz', binary=True) 

  embeddings_filtered = dict((k, embeddings[k]) for k in X if k in list(embeddings.wv.vocab.keys()))
</code></pre>

<p>It takes a very long time to run. I am not sure if this is the most efficient solution. Should I filter it out in the <code> load_word2vec_format </code> step first?</p>
","10880194","","10880194","","2019-01-07 16:45:58","2019-01-07 17:16:25","Filtering Word Embeddings from word2vec","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"54089804","1","","","2019-01-08 10:26:54","","0","204","<p>Following code is used to preprocess text with a custom lemmatizer function:</p>

<pre><code>%%time
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from gensim.utils import simple_preprocess, lemmatize
from gensim.parsing.preprocessing import STOPWORDS
STOPWORDS = list(STOPWORDS)

def preprocessor(s):
    result = []
    for token in lemmatize(s, stopwords=STOPWORDS, min_length=2):
        result.append(token.decode('utf-8').split('/')[0])
    return result

data = pd.read_csv('https://pastebin.com/raw/dqKFZ12m')

%%time
X_train, X_test, y_train, y_test = train_test_split([preprocessor(x) for x in data.text],
                                                    data.label, test_size=0.2, random_state=0)
#10.8 seconds
</code></pre>

<p>Question: 
<strong>Can the speed of the lemmatization process be improved?</strong> </p>

<p>On a large corpus of about 80,000 documents, it currently takes about two hours. The <code>lemmatize()</code> function seems to be the main bottleneck, as a gensim function such as <code>simple_preprocess</code> is quite fast.</p>

<p>Thanks for your help!</p>
","4697646","","","","","2019-01-08 17:55:53","Improving the speed of preprocessing","<gensim><lemmatization>","1","0","","","","CC BY-SA 4.0"
"54228660","1","","","2019-01-17 03:37:37","","0","319","<p>I am using cosine similarity function in gensim module, which is similarities.SparseMatrixSimilarity(). And I want to get similarities between all index documents. The method have an attribute:index, but I don't know what are stored in it.</p>

<pre><code>    sim = similarities.SparseMatrixSimilarity(
        self.tfidf_vectors, num_features=self.featurenum)
    sim.save(path + '/model/train_index.index')
    print(sim.index.shape)
    print(sim.index.toarray().shape)
</code></pre>

<p>len(self.tfidf.vectors) is 9117, but sim.index.shape is (9117, 143807) and sim.index.toarray().shape is also (9117, 143807). I guess it should be (9117,9117). What is in sim.index ?</p>
","10925617","","","","","2019-07-10 02:29:13","What is stored in similarities.SparseMatrixSimilarity().index","<gensim><cosine-similarity>","1","0","","","","CC BY-SA 4.0"
"36763582","1","","","2016-04-21 08:10:31","","1","893","<p>I'm looking for an efficient way of creating a similarity vector of a single sentence against a list of sentences.</p>

<p>The trivial way of doing that is by iterating over the list of sentences and detect similarity between the single sentence and each one of the sentences in the list. This solution is too slow and I'm looking for a faster way of doing that. </p>

<p>My final goal is to detect if there is a really similar sentence in the list of sentences to the one I'm checking, if so I'll go to next sentence.</p>

<p>My solution right now is:</p>

<pre><code>for single_sentence in list_of_sentences:
    similarity_score = word2vec.sentences_similarity(sentence2test, single_sentence)
    if similarity_score &gt;= similarity_th:
       ignore_sent_flag = True
       break 
list_of_sentences.append(sentence2test)
</code></pre>

<p>Iv'e tried to put 'list_of_sentences' in a dictionary/set but the improvement in terms of time is minor.</p>

<p>I came across <a href=""https://github.com/piskvorky/gensim/pull/617"" rel=""nofollow"">this</a> solution but it is based on a Linux only package so no relevant for me. </p>
","4583366","","4583366","","2016-04-21 08:29:00","2016-04-25 07:52:27","Find similarity between a sentence to a list of sentences","<python><nlp><deep-learning><gensim><word2vec>","2","6","","","","CC BY-SA 3.0"
"62993607","1","","","2020-07-20 10:39:17","","0","89","<p>i am trying to vectorize a text and classify it by using gensim and tensorflow.keras.</p>
<p>Before the train I have shapes as follows:</p>
<pre><code>
X_train, y_train (1019471, 100, 1) (1019471, 5) 
X_validate, y_validate (127419, 100, 1) (127419, 5) 
X_test, y_test (127476, 100, 1) (127476, 5)

</code></pre>
<pre><code>    for function in functionSource:
        func_list.append([str(preprocess_text(function))])
    return func_list
</code></pre>
<pre><code>def preprocess_text(sen):
    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sen)

    # Single character removal
    sentence = re.sub(r&quot;\s+[a-zA-Z]\s+&quot;, ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence
</code></pre>
<pre><code>    embedding_dim = 100
    model = Word2Vec(func_ist, min_count=1,size= embedding_dim, workers=6, window =3, sg = 1)
</code></pre>
<p>After the train I have the following shapes:</p>
<pre><code>X_train, y_train (1018222, 100, 1) (1019471, 5) 
X_validate, y_validate (127398, 100, 1) (127419, 5) 
X_test, y_test (127461, 100, 1) (127476, 5)
</code></pre>
<p><strong>logging for gensim</strong></p>
<pre><code>INFO: 2020-07-20 12:44:54,936 - word2vec.py:1596 - scan_vocab : collected 1018222 word types from a corpus of 1019471 raw words and 1019471 sentences
INFO: 2020-07-20 12:44:54,936 - word2vec.py:1647 - prepare_vocab : Loading a fresh vocabulary
INFO: 2020-07-20 12:44:56,790 - word2vec.py:1671 - prepare_vocab : effective_min_count=1 retains 1018222 unique words (100% of original 1018222, drops 0)
INFO: 2020-07-20 12:44:56,790 - word2vec.py:1677 - prepare_vocab : effective_min_count=1 leaves 1019471 word corpus (100% of original 1019471, drops 0)
INFO: 2020-07-20 12:44:58,912 - word2vec.py:1736 - prepare_vocab : deleting the raw counts dictionary of 1018222 items
INFO: 2020-07-20 12:44:58,930 - word2vec.py:1739 - prepare_vocab : sample=0.001 downsamples 0 most-common words
INFO: 2020-07-20 12:44:58,930 - word2vec.py:1742 - prepare_vocab : downsampling leaves estimated 1019471 word corpus (100.0% of prior 1019471)
INFO: 2020-07-20 12:45:00,543 - base_any2vec.py:1022 - estimate_memory : estimated required memory for 1018222 words and 100 dimensions: 1323688600 bytes
INFO: 2020-07-20 12:45:00,543 - word2vec.py:1888 - reset_weights : resetting layer weights
</code></pre>
<p>I couldn't understand the part that my dataset losing some of the text, can anyone explain to me the reason why the shape gets smaller?</p>
<p>i am trying to learn about the topic. i would be grateful if anyone can explain it</p>
","11055992","","","","","2020-07-20 18:10:25","Gensim vector shape changing","<python><nlp><gensim><word2vec><shapes>","1","1","","","","CC BY-SA 4.0"
"41709318","1","41731695","","2017-01-18 00:15:02","","4","7050","<p><a href=""https://i.stack.imgur.com/ofJqR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ofJqR.png"" alt=""Doc2Vec Figure 2""></a></p>

<p>The above picture is from <a href=""https://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">Distributed Representations of Sentences and Documents</a>, the paper introducing Doc2Vec. I am using Gensim's implementation of Word2Vec and Doc2Vec, which are great, but I am looking for clarity on a few issues.</p>

<ol>
<li>For a given doc2vec model <code>dvm</code>, what is <code>dvm.docvecs</code>? My impression is that it is the averaged or concatenated vector that includes all of the word embedding <em>and</em> the paragraph vector, <code>d</code>. Is this correct, or is it d?</li>
<li>Supposing <code>dvm.docvecs</code> is not <code>d</code>, can one access d by itself? How?</li>
<li>As a bonus, how is <code>d</code> calculated? The paper only says:</li>
</ol>

<blockquote>
  <p>In our Paragraph Vector framework (see Figure 2), every
  paragraph is mapped to a unique vector, represented by a
  column in matrix D and every word is also mapped to a
  unique vector, represented by a column in matrix W.</p>
</blockquote>

<p>Thanks for any leads!</p>
","1753315","","","","","2017-01-19 00:14:55","What is gensim's 'docvecs'?","<python><nlp><gensim><doc2vec>","1","0","1","","","CC BY-SA 3.0"
"54063448","1","","","2019-01-06 16:15:09","","0","621","<p>I am using Gensim LDA for the topic modelling. I am using pandas DataFrame for the processing. but I am getting an error</p>

<blockquote>
  <p>TypeError: decoding to str: need a bytes-like object, Series found</p>
</blockquote>

<p>I need to process data using Pandas only, input data is like (one row)</p>

<pre><code> PMID           Text
12755608    The DNA complexation and condensation properties
12755609    Three proteins namely protective antigen PA edition
12755610    Lecithin retinol acyltransferase LRAT catalyze
</code></pre>

<p>My code is</p>

<pre><code>data = pd.read_csv(""h1.csv"", delimiter = ""\t"")
data = data.dropna(axis=0, subset=['Text'])
data['Index'] = data.index
data[""Text""] = data['Text'].str.replace('[^\w\s]','')
data.head()

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token):
            result.append(lemmatize_stemming(token))
    return result


input_data = data.Text.str.strip().str.split('[\W_]+')
print('\n\n tokenized and lemmatized document: ')
print(preprocess(input_data))
</code></pre>
","10826930","","4420967","","2019-01-08 14:41:07","2020-08-29 02:03:32","Error in Data Processing in Gensim LDA using Pandas Dataframe","<python><pandas><dataframe><gensim><lda>","1","2","","","","CC BY-SA 4.0"
"36885223","1","","","2016-04-27 09:07:25","","1","441","<p>I am trying to use gensim for topic classification. I already have all feature words from multiple documents in the following form:</p>

<pre><code>corpus = [['word1','word2',..],['A','B',...]] (python list of lists)
</code></pre>

<p>and also a term-frequency matrix in sparse form and a dict. </p>

<p>I was trying to train gensim LDA on this:</p>

<pre><code> lda_model = gensim.models.LdaModel(term_freq_matrix, num_topics=10, id2word=feature_names_dict, passes=4)
</code></pre>

<p>But I get the following error: </p>

<pre><code>  File ""/home/oliver/Environments/cmpdp/local/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 523, in &lt;genexpr&gt;
    corpus_words = sum(cnt for document in chunk for _, cnt in document)
ValueError: need more than 1 value to unpack
</code></pre>

<p>From this tutorial <a href=""http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"" rel=""nofollow"">http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html</a>
it seems like it all looks okay, only the sparse matrix form looks a bit different:</p>

<p>My corpus:</p>

<pre><code> print(next(iter(term_freq_matrix)))
  (0, 12036)    1
  (0, 12406)    2
...
  (0, 3916) 1
  (0, 3157) 1
</code></pre>

<p>Tutorial corpus:
print(next(iter(mm_corpus)))</p>

<pre><code>[(24, 1.0), (38, 1.0), (53, 1.0), (103, 1.0), (111, 1.0), (213, 3.0), (237, 1.0), (242, 2.0)]
</code></pre>

<p>What do you think?</p>
","5847056","","","","","2016-04-27 09:07:25","Create a gensim corpus from term-frequency matrix or from a collection of strings","<nlp><gensim><corpus><word2vec>","0","0","","","","CC BY-SA 3.0"
"54060506","1","54060824","","2019-01-06 10:11:56","","0","139","<p>following script is used to lemmatize a given input column with text:</p>

<pre><code>%%time
import pandas as pd
from gensim.utils import lemmatize
from gensim.parsing.preprocessing import STOPWORDS
STOPWORDS = list(STOPWORDS)

data = pd.read_csv('https://pastebin.com/raw/0SEv1RMf')

def lemmatization(s):
    result = []
    # lowercase, tokenize, remove stopwords, len&gt;3, lemmatize
    for token in lemmatize(s, stopwords=STOPWORDS, min_length=3):
        result.append(token.decode('utf-8').split('/')[0])
    # print(len(result)) &lt;- This didn't work.
    return result

X_train = data.apply(lambda r: lemmatization(r['text']), axis=1)
print(X_train)
</code></pre>

<p><strong>Question:</strong></p>

<p><strong>How can I print the progress of the lemmatization progress?</strong></p>
","4697646","","4697646","","2019-01-08 10:35:11","2019-01-08 10:35:11","Show progress in lemmatization","<gensim><lemmatization>","1","0","","","","CC BY-SA 4.0"
"63162298","1","","","2020-07-29 20:56:06","","0","41","<p>My dataset is in the following format where for each disease I am generating a 2D vector using word2vec.(Showing 2D vectors for example but in practice, vectors are in 100D )</p>
<pre><code>Disease                             Vectors

disease a, disease c         [[ 0.2520773 ,  0.433798],[0.38915345, 0.5541569]]

disease b                    [0.12321666, 0.64195603]

disease c, disease b         [[0.38915345, 0.5541569],[0.12321666, 0.64195603]]

disease c                    [0.38915345, 0.5541569]
</code></pre>
<p>From here I am generating a 1D array for each <code>disease/disease combination</code> by taking the average of the vectors. The issue with averaging word vectors is the fact that the combination of 2 or more diseases can have the same average vector as a totally different disease which is not at all relevant but the average vectors get matched. This makes the concept of averaging vectors flawed. To counter this, the understanding is with an increase in dimension of the vectors, this should be even less of a possibility.</p>
<p>So, couple of questions in all:</p>
<ol>
<li><p>Is there a better way than averaging the output from word2vec vectors to generate a 1D array?</p>
</li>
<li><p>These generated vectors will be treated as features to a classifier model that I am trying to build for each disease/disease combination so, if I generate a 100D feature vector from word2vec, shall I use something like a PCA on it to reduce the dimension or shall I just consider the 100D feature vector as 100 features to my classifier.</p>
</li>
</ol>
","13982723","","","","","2020-07-29 20:56:06","What is a good substitute for averaging vectors generated from Word2vec","<vector><pca><gensim><word2vec><word-embedding>","0","6","","","","CC BY-SA 4.0"
"37089933","1","","","2016-05-07 14:50:12","","5","6627","<p>Im trying to install gensim using pip but i'm getting:</p>

<pre><code>""Could not import setuptools which is required to install from a source distribution.
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip/req/req_install.py"", line 377, in setup_py
import setuptools  # noqa
  File ""/Library/Python/2.7/site-packages/setuptools/__init__.py"", line 11, in &lt;module&gt;
from setuptools.extern.six.moves import filterfalse, map
  File ""/Library/Python/2.7/site-packages/setuptools/extern/__init__.py"", line 1, in &lt;module&gt;
from pkg_resources.extern import VendorImporter
ImportError: No module named extern""
</code></pre>

<p>Other posts have suggested running </p>

<pre><code> pip install -U setuptools
</code></pre>

<p>which reports to have successfully installed the package.</p>

<pre><code>Installing collected packages: setuptools
Successfully installed setuptools-21.0.0
</code></pre>

<p>However, running the gensim pip install command:</p>

<pre><code>pip install gensim
</code></pre>

<p>just gives the first error again.</p>

<p>any ideas why this might be happening?</p>
","5446037","","892534","","2016-05-07 16:38:54","2017-05-09 07:13:31","Error installing Gensim: ""Could not import setuptools which is required to install from a source distribution.""","<python><pip><gensim>","5","1","","","","CC BY-SA 3.0"
"45565389","1","45574963","","2017-08-08 10:05:15","","2","1415","<p>I am looking at various semantic similarity methods such as word2vec, word mover distance (WMD), and fastText. fastText is not better than Word2Vec as for as semantic similarity is concerned. WMD and Word2Vec have almost similar results. </p>

<p>I was wondering if there is an alternative which has outperformed the Word2Vec model for semantic accuracy? </p>

<p><strong>My use case:</strong>
<em>Finding word embeddings for two sentences, and then use cosine similarity to find their similarity.</em> </p>
","1996842","","1996842","","2017-08-08 12:22:25","2017-08-08 17:52:23","Is there a semantic similarity method that outperforms word2vec approach for semantic accuracy?","<nlp><nltk><gensim><word2vec><fasttext>","1","2","1","","","CC BY-SA 3.0"
"63062583","1","","","2020-07-23 20:24:38","","0","338","<p>Now, from the outputs, I am unable to understand the embeddings or how these embeddings are changing or will change with new data.</p>
<p>Is this way correct for solving the problem statement and if so how can I optimize this to find the best embeddings for my dataset. Can anyone provide any suggestions on the same</p>
","13982723","","13982723","","2020-07-29 02:45:24","2020-07-29 02:45:24","Building a recommendation system using word2vec","<python><nlp><gensim><word2vec><word-embedding>","0","8","","","","CC BY-SA 4.0"
"45906577","1","","","2017-08-27 15:29:57","","1","63","<p>When I show the output of ldamodel learnt by gensim</p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)
ldamodel.print_topics(num_topics=2, num_words=4)
</code></pre>

<p>It will show all unicode character with \u</p>

<pre><code>[(0,
  u'0.128*""dddf"" + 0.128*""\u4f60\u597d"" + 0.046*""o"" + 0.046*""love""'),
 (1,
  u'0.057*""\u0646\u062f"" + 0.057*""\u0627\u06cc\u0646\u0647"" + 0.057*""\u0646\u0645\u06cc"" + 0.057*""\u0628\u06cc\u0637\u0631\u0641\u0647""')]
</code></pre>

<p>Those unicode characters are chinese or persian characters, how should I make it appear as those characters?</p>
","1497720","","1497720","","2017-08-27 15:35:14","2017-08-27 15:35:14","Make output of print_topics shows unicode character","<python><python-2.7><unicode><nltk><gensim>","0","1","","","","CC BY-SA 3.0"
"28488714","1","","","2015-02-12 22:04:23","","10","1881","<p>I am using Gensim for some topic modelling and I have gotten to the point where I am doing similarity queries using the LSI and tf-idf models. I get back the set of IDs and similarities, eg. <code>(299501, 0.64505910873413086)</code>. </p>

<p>How do I get the text document that is related to the ID, in this case 299501?</p>

<p>I have looked at the docs for corpus, dictionary, index, and the model and cannot seem to find it.</p>
","3790925","","209288","","2017-10-10 05:36:55","2017-10-10 06:28:46","Retrieve string version of document by ID in Gensim","<python><gensim>","2","0","3","","","CC BY-SA 3.0"
"46129335","1","46130945","","2017-09-09 09:49:15","","14","28296","<p>I am currently using uni-grams in my word2vec model as follows.</p>

<pre><code>def review_to_sentences( review, tokenizer, remove_stopwords=False ):
    #Returns a list of sentences, where each sentence is a list of words
    #
    #NLTK tokenizer to split the paragraph into sentences
    raw_sentences = tokenizer.tokenize(review.strip())

    sentences = []
    for raw_sentence in raw_sentences:
        # If a sentence is empty, skip it
        if len(raw_sentence) &gt; 0:
            # Otherwise, call review_to_wordlist to get a list of words
            sentences.append( review_to_wordlist( raw_sentence, \
              remove_stopwords ))
    #
    # Return the list of sentences (each sentence is a list of words,
    # so this returns a list of lists
    return sentences
</code></pre>

<p>However, then I will miss important bigrams and trigrams in my dataset.</p>

<pre><code>E.g.,
""team work"" -&gt; I am currently getting it as ""team"", ""work""
""New York"" -&gt; I am currently getting it as ""New"", ""York""
</code></pre>

<p>Hence, I want to capture the important bigrams, trigrams etc. in my dataset and input into my word2vec model.</p>

<p>I am new to wordvec and struggling how to do it. Please help me.</p>
","","user8566323","1060350","","2017-09-10 18:49:11","2019-05-09 06:01:54","Get bigrams and trigrams in word2vec Gensim","<python><tokenize><word2vec><gensim><n-gram>","3","2","9","","","CC BY-SA 3.0"
"51269058","1","","","2018-07-10 15:26:20","","0","85","<p>Gensim's <code>Word2Vec</code> model takes as an  input a list of lists with the inner list containing individual tokens/words of a sentence. As I understand <code>Word2Vec</code> is used to ""quantify"" the context of words within a text using vectors. 
I am currently dealing with a corpus of text that has already been split into individual tokens and no longer contains an obvious sentence format (punctuation has been removed). I was wondering how should I input this into the <code>Word2Vec</code> model? </p>

<p>Say if I simply split the corpus into ""sentences"" of uniform length (10 tokens per sentence for example), would this be a good way of inputting the data into the model? </p>

<p>Essentially, <strong>I am wondering how the format of the input sentences (list of lists) affects the output of Word2Vec?</strong> </p>
","10060066","","13860","","2018-07-10 17:59:54","2018-07-10 17:59:54","Use proxy sentences from cleaned data","<python><nlp><gensim><word2vec><word-embedding>","2","0","","","","CC BY-SA 4.0"
"45696948","1","","","2017-08-15 16:12:28","","2","802","<p>I have a class wrapping the various objects required for calculating LSI similarity:</p>

<pre><code>class SimilarityFiles:

    def __init__(self, file_name, tokenized_corpus, stoplist=None):
        if stoplist is None:
            self.filtered_corpus = tokenized_corpus
        else:
            self.filtered_corpus = []
            for convo in tokenized_corpus:
                self.filtered_corpus.append([token for token in convo if token not in stoplist])
        self.dictionary = corpora.Dictionary(self.filtered_corpus)
        self.corpus = [self.dictionary.doc2bow(text) for text in self.filtered_corpus]
        self.lsi = models.LsiModel(self.corpus, id2word=self.dictionary, num_topics=100)
        self.index = similarities.MatrixSimilarity(self.lsi[self.corpus])
</code></pre>

<p>I now want to add a function to the class to allow adding documents to the corpus and updating the model accordingly.
I've found <code>dictionary.add_documents</code>, and <code>model.add_documents</code>, but there are two things that aren't clear to me:</p>

<ol>
<li>When you originally create the LSI model, one of the parameters the function receives is <code>id2word=dictionary</code>. When updating the model, how do you tell it to use the updated dictionary? Is it actually unnecessary, or will it make a difference?</li>
<li>How do I update the index? It looks from the <a href=""https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.Similarity.add_documents"" rel=""nofollow noreferrer"">documentation</a> that if I use the <code>Similarity</code> class, and not the <code>MatrixSimilarity</code> class, I can add documents to the index, but I don't see such functionality for <code>MatrixSimilarity</code>. If I understood correctly, the <code>MatrixSimilarity</code> is better if my input corpus contains dense vectors (which is does, because I'm using the LSI model). Do I have to change it to <code>Similarity</code> just so that I can update the index? Or, conversely, what's the complexity of creating this index? If it's insignificant, should I just create a new index with my updated corpus, as follows:</li>
</ol>

<p>Code:</p>

<pre><code>self.dictionary.add_documents(new_docs)    # new_docs is already after filtering stop words
new_corpus = [self.dictionary.doc2bow(text) for text in new_docs]
self.lsi.add_documents(new_corpus)
self.index = similarities.MatrixSimilarity(self.lsi[self.corpus])
</code></pre>

<p>Thanks. :)</p>
","2986584","","","","","2018-09-27 18:44:12","Adding documents to gensim model","<python-3.x><gensim><lsa>","1","0","1","","","CC BY-SA 3.0"
"37190989","1","37995402","","2016-05-12 15:12:23","","12","27463","<p>I am using gensim word2vec package in python. I know how to get the vocabulary from the trained model. But how to get the word count for each word in vocabulary?</p>
","5969670","","","","","2021-03-16 21:30:45","How to get vocabulary word count from gensim word2vec?","<gensim><word2vec>","3","1","1","","","CC BY-SA 3.0"
"46137572","1","46143595","","2017-09-10 05:27:14","","3","1315","<p>I am trying to get the bigrams in the sentences using Phrases in Gensim as follows.</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser
documents = [""the mayor of new york was there"", ""machine learning can be useful sometimes"",""new york mayor was present""]

sentence_stream = [doc.split("" "") for doc in documents]
#print(sentence_stream)
bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
bigram_phraser = Phraser(bigram)

for sent in sentence_stream:
    tokens_ = bigram_phraser[sent]
    print(tokens_)
</code></pre>

<p>Even though it catches ""new"", ""york"" as ""new york"", it does not catch ""machine"", learning as ""machine learning""</p>

<p>However, in the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">example shown in Gensim Website</a> they were able to catch the words ""machine"", ""learning"" as ""machine learning"".</p>

<p>Please let me know how to get ""machine learning"" as a bigram in the above example</p>
","","user8510273","","","","2017-09-11 01:13:28","Error in extracting phrases using Gensim","<python><data-mining><text-mining><word2vec><gensim>","2","0","","","","CC BY-SA 3.0"
"47838719","1","","","2017-12-15 19:25:11","","2","1754","<p>I loaded google's news vector -300 dataset.  Each word is represented with a 300 point vector.  I want to use this in my neural network for classification.  But 300 for one word seems to be too big.  How can i reduce the vector from 300 to say 100 without compromising on the quality.  </p>
","7805829","","","","","2018-07-29 14:14:05","reducing word2vec dimension from Google News Vector Dataset","<python-3.x><gensim>","2","0","","","","CC BY-SA 3.0"
"53815402","1","53821335","","2018-12-17 12:36:39","","4","2108","<p>I just want to know the effect of the value of alpha in gensim <code>word2vec</code> and <code>fasttext</code> word-embedding models? I know that alpha is the <code>initial learning rate</code> and its default value is <code>0.075</code> form Radim blog.</p>

<p>What if I change this to a bit higher value i.e. 0.5 or 0.75? What will be its effect? Does it is allowed to change the same? However, I have changed this to 0.5 and experiment on a large-sized data with D = 200, window = 15, min_count = 5, iter = 10, workers = 4 and results are pretty much meaningful for the word2vec model. However, using the fasttext model, the results are bit scattered, means less related and unpredictable high-low similarity scores.</p>

<p>Why this imprecise result for same data with two popular models with different precision? Does the value of <code>alpha</code> plays such a crucial role during building of the model?</p>

<p>Any suggestion is appreciated.</p>
","3966705","","3966705","","2018-12-17 12:49:10","2018-12-17 18:47:42","Value of alpha in gensim word-embedding (Word2Vec and FastText) models?","<python-3.x><gensim><word2vec><word-embedding><fasttext>","1","0","1","","","CC BY-SA 4.0"
"54655604","1","54665637","","2019-02-12 17:28:57","","4","1781","<p>I would like to use pre-trained embeddings in my neural network architecture. The pre-trained embeddings are trained by gensim. I found <a href=""https://stackoverflow.com/a/49802495/1150683"">this informative answer</a> which indicates that we can load pre_trained models like so:</p>

<pre><code>import gensim
from torch import nn

model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')
weights = torch.FloatTensor(model.vectors)
emb = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors))
</code></pre>

<p>This seems to work correctly, also on 1.0.1. My question is, that I don't quite understand what I have to feed into such a layer to utilise it. Can I just feed the tokens (segmented sentence)? Do I need a mapping, for instance token-to-index? </p>

<p>I found that you can access a token's vector simply by something like</p>

<pre><code>print(weights['the'])
# [-1.1206588e+00  1.1578362e+00  2.8765252e-01 -1.1759659e+00 ... ]
</code></pre>

<p>What does that mean for an RNN architecture? Can we simply load in the tokens of the batch sequences? For instance:</p>

<pre><code>for seq_batch, y in batch_loader():
    # seq_batch is a batch of sequences (tokenized sentences)
    # e.g. [['i', 'like', 'cookies'],['it', 'is', 'raining'],['who', 'are', 'you']]
    output, hidden = model(seq_batch, hidden)
</code></pre>

<p>This does not seem to work so I am assuming you need to convert the tokens to its index in the final word2vec model. Is that true? I found that you can get the indices of words by using the word2vec model's <code>vocab</code>:</p>

<pre><code>weights.vocab['world'].index
# 147
</code></pre>

<p>So as an input to an Embedding layer, should I provide a tensor of <code>int</code> for a sequence of sentences that consist of a sequence of words? Example use with dummy dataloader (cf. example above) and dummy RNN welcome.</p>
","1150683","","1150683","","2019-02-12 19:32:26","2019-02-13 08:31:38","Expected input to torch Embedding layer with pre_trained vectors from gensim","<vector><pytorch><gensim><word2vec><recurrent-neural-network>","1","0","2","","","CC BY-SA 4.0"
"46258266","1","","","2017-09-16 20:42:10","","0","335","<p>I'm trying to the Q6 recipe shown <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ"" rel=""nofollow noreferrer"">here</a> but my corpus keep getting returned as [] even though I have checked and it does seem to be reading the document correctly.</p>

<p>So my code is:</p>

<pre><code>def iter_documents(top_directory):
    """"""Iterate over all documents, yielding a document (=list of utf8 tokens) at a time.""""""
    for root, dirs, files in os.walk(top_directory):
        for file in filter(lambda file: file.endswith('.txt'), files):
            document = open(os.path.join(root, file)).read() # read the entire document, as one big string
            yield utils.tokenize(document, lower=True) # or whatever tokenization suits you
class MyCorpus(object):
    # Used to create the object
    def __init__(self, top_dir):
        self.top_dir = top_dir
        self.dictionary = corpora.Dictionary(iter_documents(top_dir))
        self.dictionary.filter_extremes(no_below=1, keep_n=30000) # check API docs for pruning params
# Used if you ever need to iterate through the values
def __iter__(self):
    for tokens in iter_documents(self.top_dir):
        yield self.dictionary.doc2bow(tokens)
</code></pre>

<p>and the text file I'm using to test is <a href=""https://radimrehurek.com/gensim/mycorpus.txt"" rel=""nofollow noreferrer"">this</a>.</p>
","1761806","","","","","2017-09-16 22:23:05","Gensim - iterating over multiple documents","<gensim>","1","0","","","","CC BY-SA 3.0"
"63499110","1","63499255","","2020-08-20 05:56:51","","0","129","<p>Hoping you can help. I‚Äôm creating a web app with python and Flask. One of the the things that my web app will do is provide a smart document search. You can enter text and it will fetch results of documents similar to the portion of text you entered.</p>
<p>I‚Äôve used Flask for the front end to serve the HTML, manage any DB interactions required and display results. It will pass the query through to a Gensim similarity model and query it.</p>
<p>My question here is what is the best way to host these? I‚Äôve explored loading the model as part of loading flask but it slows things down quite a lot (it‚Äôs c. 6gb in memory) but it works. I can then query the model quite easily as it‚Äôs all within the same program scope.</p>
<p>My concern is that this would then not be scalable and possibly not best practice and that I may be better to host the model separately and make API calls to it from my Flask web app.</p>
<p>Thoughts and views would be much appreciated.</p>
<p>Thanks,
Pete</p>
","13003880","","","","","2020-08-20 06:36:30","Best practice for deploying machine learning web app with Flask","<python><flask><gensim>","1","0","","2020-08-20 21:29:48","","CC BY-SA 4.0"
"54878715","1","","","2019-02-26 05:02:26","","2","201","<p>I am getting this error when trying to get the coherence score for topic modelling from Gensim package. The error doesn't seem to be syntax related:</p>

<pre><code>    coherence_lda_gens = CoherenceModel(model=lda_gens, 
    texts=data_lemmatized, dictionary=dict_lemm, coherence='c_v')
    coherence = coherence_lda_gens.get_coherence()  #this line fails*emphasized text*
    print('\nCoherence Score: ',coherence)
</code></pre>

<p>and I get this error: </p>

<pre><code> File ""C:\Users\msharifpour\AppData\Local\Continuum\anaconda3\lib\multiprocessing\spawn.py"", line 172, in get_preparation_data
    main_mod_name = getattr(main_module.__spec__, ""name"", None)

AttributeError: module '__main__' has no attribute '__spec__'
</code></pre>

<p>I googled this problem quite a bit, but can't find where the problem comes from and what is the fix?</p>

<p>Thanks</p>
","6235045","","","","","2019-02-26 05:02:26","Error in Coherence score in Gensim package","<python><nlp><gensim>","0","0","","","","CC BY-SA 4.0"
"63779875","1","64595283","","2020-09-07 14:48:02","","2","1516","<p>I'd like to compare the difference among the same word mentioned in different sentences, for example &quot;travel&quot;.
What I would like to do is:</p>
<ul>
<li>Take the sentences mentioning the term &quot;travel&quot; as plain text;</li>
<li>In each sentence, replace 'travel' with travel_sent_x.</li>
<li>Train a word2vec model on these sentences.</li>
<li>Calculate the distance between travel_sent1, travel_sent2, and other relabelled mentions of &quot;travel&quot;
So each sentence's &quot;travel&quot; gets its own vector, which is used for comparison.</li>
</ul>
<p>I know that word2vec requires much more than several sentences to train reliable vectors. The official page recommends datasets including billions of words, but I have not a such number in my dataset(I have thousands of words).</p>
<p>I was trying to test the model with the following few sentences:</p>
<pre><code>    Sentences
    Hawaii makes a move to boost domestic travel and support local tourism
    Honolulu makes a move to boost travel and support local tourism
    Hawaii wants tourists to return so much it's offering to pay for half of their travel expenses
</code></pre>
<p>My approach to build the vectors has been:</p>
<pre><code>from gensim.models import Word2Vec

vocab = df['Sentences']))
model = Word2Vec(sentences=vocab, size=100, window=10, min_count=3, workers=4, sg=0)
df['Sentences'].apply(model.vectorize)
</code></pre>
<p>However I do not know how to visualise the results to see their similarity and get some useful insight.
Any help and advice will be welcome.</p>
<p>Update: I would use Principal Component Analysis algorithm to visualise embeddings in 3-dimensional space. I know how to do for each individual word, but I do not know how to do it in case of sentences.</p>
","","user13623188","","user13623188","2020-10-28 00:23:49","2020-10-29 17:43:29","Sentences embedding using word2vec","<python><gensim><word2vec><embedding>","3","2","1","","","CC BY-SA 4.0"
"46433778","1","55622285","","2017-09-26 18:51:02","","25","41433","<p>I am working on code using the gensim and having a tough time troubleshooting a ValueError within my code. I finally was able to zip GoogleNews-vectors-negative300.bin.gz file so I could implement it in my model. I also tried gzip which the results were unsuccessful. The error in the code occurs in the last line. I would like to know what can be done to fix the error. Is there any workarounds? Finally, is there a website that I could reference? </p>

<p>Thank you respectfully for your assistance! </p>

<pre><code>import gensim
from keras import backend
from keras.layers import Dense, Input, Lambda, LSTM, TimeDistributed
from keras.layers.merge import concatenate
from keras.layers.embeddings import Embedding
from keras.models import Mode

pretrained_embeddings_path = ""GoogleNews-vectors-negative300.bin""
word2vec = 
gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path, 
binary=True)

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-3-23bd96c1d6ab&gt; in &lt;module&gt;()
  1 pretrained_embeddings_path = ""GoogleNews-vectors-negative300.bin""
----&gt; 2 word2vec = 
gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path, 
binary=True)

C:\Users\green\Anaconda3\envs\py35\lib\site-
packages\gensim\models\keyedvectors.py in load_word2vec_format(cls, fname, 
fvocab, binary, encoding, unicode_errors, limit, datatype)
244                             word.append(ch)
245                     word = utils.to_unicode(b''.join(word), 
encoding=encoding, errors=unicode_errors)
--&gt; 246                     weights = fromstring(fin.read(binary_len), 
dtype=REAL)
247                     add_word(word, weights)
248             else:

ValueError: string size must be a multiple of element size
</code></pre>
","7297511","","","","","2021-05-01 20:55:23","Import GoogleNews-vectors-negative300.bin","<python><gensim>","3","4","2","","","CC BY-SA 3.0"
"44497215","1","","","2017-06-12 10:29:01","","0","444","<p>I am training a model with <code>gensim</code>, my corpus is many short sentences, and each sentence has a frequency which indicates times it occurs in total corpus. I implement it as follow, as you can see, I just choose to do repeat <code>freq</code> times. Any way, if the data is small, it should work, but when data grows, <strong>the frequency can be very large, it costs too much memory</strong> and my machine cannot afford it. </p>

<p>So 
1. <strong>can I just count the frequency in every record instead of repeat <code>freq</code> times?</strong> 2. <strong>Or any other ways to save memory?</strong></p>

<pre><code>class AddressSentences(object):
    def __init__(self, raw_path, path):
        self._path = path

    def __iter__(self):
        with open(self.path) as fi:
            headers = next(fi).split("","")
            i_address, i_freq = headers.index(""address""), headers.index(""freq"")
            index = 0
            for line in fi:
                cols = line.strip().split("","")
                freq = cols[i_freq]
                address = cols[i_address].split()
                # Here I do repeat
                for i in range(int(freq)):
                    yield TaggedDocument(address, [index])
                index += 1

print(""START %s"" % datetime.datetime.now())
train_corpus = list(AddressSentences(""/data/corpus.csv""))
model = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2, iter=55)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)
print(""END %s"" % datetime.datetime.now())
</code></pre>

<p>corpus is something like this:</p>

<pre><code>address,freq
Cecilia Chapman 711-2880 Nulla St.,1000
The Business Centre,1000
61 Wellfield Road,500
Celeste Slater 606-3727 Ullamcorper. Street,600
Theodore Lowe Azusa New York 39531,700
Kyla Olsen Ap #651-8679 Sodales Av.,300
</code></pre>
","2849157","","2849157","","2017-06-12 10:42:51","2017-06-12 18:46:45","How to count frequency in gensim.Doc2Vec?","<python><gensim><word2vec><doc2vec>","1","6","","","","CC BY-SA 3.0"
"28823948","1","","","2015-03-03 04:10:03","","1","394","<p>I have list of docs in <code>postCorp</code>. I am trying to get topics and corresponding probability using <code>lda model</code> that i have already trained using <code>gensim</code>.
Below is the code snippet where i am:
Getting each document
Converting it to BOW
Passing it to lda to return topics and 
And also calculating mean probability of each topic over all the document.</p>

<pre><code>with open('ldaOutputPost','w') as out:
    topicMean = defaultdict(float)
    count = 0
    for corp in postCorp:
        count += 1
        for (t,p) in lda[dictionary.doc2bow(corp)]:
            topicMean[t] += p 
        out.write(str(lda[dictionary.doc2bow(corp)])+'\n')
</code></pre>

<p>This works fine if i just pass bow of one document. But when I loop over all the documents it gives me following error which I am unable to find out why?</p>

<pre><code>Traceback (most recent call last):
  File ""C:\test.py"", line 110, in &lt;module&gt;
    for (t,p) in lda[dictionary.doc2bow(corp)]:
  File ""C:\Python27\lib\site-packages\gensim\models\ldamodel.py"", line 752, in __getitem__
    gamma, _ = self.inference([bow])
  File ""C:\Python27\lib\site-packages\gensim\models\ldamodel.py"", line 380, in inference
    expElogbetad = self.expElogbeta[:, ids]
IndexError: index 6770 is out of bounds for axis 1 with size 6770
</code></pre>

<p>Can any one please help.</p>

<p>P.S: There is no issue in BOW as <code>lda[dictionary.doc2bow(preCorp[0])</code> works fine and gives me correct output.</p>

<p>EDIT:
<code>postCorp</code> is list of list where inner list represents a list of all words in a document. Whereas dictionary is <code>dictionary = corpora.Dictionary(combined)</code> here combined is all corpus on which topic model was trained.</p>
","2595210","","2595210","","2015-03-03 05:00:42","2015-03-06 01:23:16","Error while while running trained topic model on new document","<python><lda><topic-modeling><gensim>","0","3","","","","CC BY-SA 3.0"
"46379763","1","","","2017-09-23 12:49:41","","5","4642","<p>I have a document Term matrix with nine documents:</p>

<p><a href=""https://i.stack.imgur.com/oKl7a.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/oKl7a.png"" alt=""enter image description here""></a></p>

<p>I am running the code as below:</p>

<pre><code>import pyLDAvis.gensim
topicData = pyLDAvis.gensim.prepare(ldamodel, docTermMatrix, dictionary)
pyLDAvis.display(topicData)  
</code></pre>

<p>I am getting the below error when executing pyLDAvis.display function:</p>

<blockquote>
  <p>TypeError: Object of type 'complex' is not JSON serializable</p>
</blockquote>

<p>Can someone guide here? What could be the reason?</p>
","6847248","","6847248","","2017-09-23 16:43:41","2018-11-21 07:42:59","TypeError: Object of type 'complex' is not JSON serializable while using pyLDAvis.display() function","<json><gensim><serializable>","2","0","1","","","CC BY-SA 3.0"
"46435220","1","46436370","","2017-09-26 20:25:36","","0","471","<p>I am have generated a tf-idf model on ~20,000,000 documents using the following code, which works well. The problem is when I try to calculate similarity scores when using linear_kernel the memory usage blows up:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

train_file = ""docs.txt""
train_docs = DocReader(train_file) #DocReader is a generator for individual documents

vectorizer = TfidfVectorizer(stop_words='english',max_df=0.2,min_df=5)
X = vectorizer.fit_transform(train_docs)

#predicting a new vector, this works well when I check the predictions
indoc = ""This is an example of a new doc to be predicted""
invec = vectorizer.transform([indoc])

#This is where the memory blows up
similarities = linear_kernel(invec, X).flatten()
</code></pre>

<p>Seems like this shouldn't take up much memory, doing a comparison of a 1-row-CSR to a 20mil-row-CSR should output a 1x20mil ndarray.</p>

<p>Justy FYI: X is a CSR matrix ~12 GB in memory (my computer only has 16). I have tried looking into gensim to replace this but I can't find a great example.</p>

<p>Any thoughts on what I am missing?</p>
","8041091","","","","","2017-09-26 21:49:09","Calculating similarity between Tfidf matrix and predicted vector causes memory overflow","<python><scikit-learn><gensim><tf-idf><csr>","1","0","","","","CC BY-SA 3.0"
"61921588","1","61925728","","2020-05-20 19:43:49","","2","387","<p>I am trying to load a pre-trained Doc2vec model using gensim and use it to map a paragraph to a vector. I am referring to <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">https://github.com/jhlau/doc2vec</a> and the pre-trained model I downloaded is the English Wikipedia DBOW, which is also in the same link. However, when I load the Doc2vec model on wikipedia and infer vectors using the following code:</p>

<pre><code>import gensim.models as g
import codecs

model=""wiki_sg/word2vec.bin""
test_docs=""test_docs.txt""
output_file=""test_vectors.txt""

#inference hyper-parameters
start_alpha=0.01
infer_epoch=1000

#load model
test_docs = [x.strip().split() for x in codecs.open(test_docs, ""r"", ""utf-8"").readlines()]
m = g.Doc2Vec.load(model)

#infer test vectors
output = open(output_file, ""w"")
for d in test_docs:
    output.write("" "".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + ""\n"")
output.flush()
output.close()
</code></pre>

<p>I get an error:</p>

<pre><code>/Users/zhangji/Desktop/CSE547/Project/NLP/venv/lib/python2.7/site-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function
  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL
Traceback (most recent call last):
  File ""/Users/zhangji/Desktop/CSE547/Project/NLP/AbstractMapping.py"", line 19, in &lt;module&gt;
    output.write("" "".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + ""\n"")
AttributeError: 'Word2Vec' object has no attribute 'infer_vector'
</code></pre>

<p>I know there are couple of threads regarding the infer_vector issue on stack overflow, but none of them resolved my problem. I downloaded the gensim package using</p>

<pre><code>pip install git+https://github.com/jhlau/gensim
</code></pre>

<p>In addition, after I looked at the source code in gensim package, I found that when I use Doc2vec.load(), the Doc2vec class doesn't really have a load() function by itself, but since it is a subclass of Word2vec, it calls the super method of load() in Word2vec and then make the model m a Word2vec object. However, the infer_vector() function is unique to Doc2vec and does not exist in Word2vec, and that's why it is causing the error. I also tried casting the model m to a Doc2vec, but I got this error:</p>

<pre><code>&gt;&gt;&gt; g.Doc2Vec(m)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/doc2vec.py"", line 599, in __init__
    self.build_vocab(documents, trim_rule=trim_rule)
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/word2vec.py"", line 513, in build_vocab
    self.scan_vocab(sentences, trim_rule=trim_rule)  # initial survey
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/doc2vec.py"", line 635, in scan_vocab
    for document_no, document in enumerate(documents):
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/word2vec.py"", line 1367, in __getitem__
    return vstack([self.syn0[self.vocab[word].index] for word in words])
TypeError: 'int' object is not iterable
</code></pre>

<p>In fact, all I want with gensim for now is to convert a paragraph to a vector using a pre-trained model that works well on academic articles. For some reasons I don't want to train the models on my own. I would be really grateful if someone can help me resolve the issue.</p>

<p>Btw, I am using python2.7, and the current gensim version is 0.12.4.</p>

<p>Thanks!</p>
","","user13584534","","","","2020-05-21 01:34:38","Cannot load Doc2vec object using gensim","<python><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"64280141","1","","","2020-10-09 12:32:56","","1","36","<p>I have written a function in Python to compute the similarity between PDF pages to return the most similar page mapping.</p>
<p>Function gets input the file and a list which has dictionary entries as:
thumbnail=[{'page': 1, 'text' : 'strin1'}, {'page': 2, 'text' : 'strin2'},...]</p>
<p>The function:</p>
<pre><code>import PyPDF2
import spacy

filename2=&quot;file.pdf&quot;
nlp = spacy.load('en_core_web_lg')


def checker(filename2, thumbnail):
    object = PyPDF2.PdfFileReader(filename2)
    NumPages = object.getNumPages()
    
    specialCharacters = {ord(c): &quot; &quot; for c in &quot;!@#$%^&amp;*()[]{};:,./&lt;&gt;?\|`~-=_+&quot;}
    
    # extract text and do the search
    output=[]
    for i in range(0, NumPages):
        temp_dict={}
        Text = object.getPage(i).extractText().translate(specialCharacters)
        Text=Text.replace('\n','')
        
        for item in thumbnail:
            sim= nlp(Text).similarity(nlp(item['text']))
            if sim&gt;0.98:
                temp_dict['page_thumbnail'] = item['page']
                temp_dict['page_file']=i+1
                temp_dict['sim'] = sim
                output.append(temp_dict)
    return output
</code></pre>
<p>This is taking a really long time for a PDF with 38 pages matched with a list of 38 entries using Spacy.
Any suggestion on how to make it scalable? Also the primary goal is to return the page number of the document (i) and the matched page for which the similarity score is the highest in the thumbnail (item['page']).</p>
","11913986","","11913986","","2020-10-09 12:53:54","2020-10-09 15:35:32","Document Similarity runtime exceeds using Spacy","<python><nlp><spacy><gensim><tf-idf>","1","0","","","","CC BY-SA 4.0"
"45802490","1","","","2017-08-21 17:11:02","","2","512","<p>I am doing topic modelling on linguistics papers and I am using the Gensim Phrases to identify frequent collocations. I want to be able to mark terms as 'do-support' and 'it-clefts' as one single word, since they are specific linguistic terminology. However, if I make the Gensim model after taking out stopwords, these collocations will not be found (since they contain stopwords), if I make the model after taking out stopwords (or stopwords not including 'it' or 'do'), it identifies a whole lot of irrelevant collocations. Is there a way to manually add phrases that should be recognised as collocations by the Gensim Phrases? 
Thanks!</p>
","8464691","","","","","2021-04-06 16:15:59","Manually add collocations to gensim phraser","<gensim><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"54559615","1","54560104","","2019-02-06 17:44:35","","3","1216","<p>When I start training my word2vec model, I am presented with the warning</p>

<blockquote>
  <p>consider setting layer size to a multiple of 4 for greater performance</p>
</blockquote>

<p>That sounds neat, but I can't find any reference to a <code>layer</code> argument or similar in <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">the documentation</a>. </p>

<p>So how can I increase the layer size, and how can I determine a good value?</p>
","1150683","","","","","2019-02-06 18:24:41","Layer size in gensim's word2vec","<python><python-3.x><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"37196520","1","","","2016-05-12 20:10:03","","7","1835","<p>I have some sample sentences that I want to run through a Doc2Vec model. My end goal is a matrix of size (num_sentences, num_features). </p>

<p>I'm using the Gensim package.</p>

<pre><code>from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec
# warning: long sample of data. It's just 40 sentences really though.
labeled_sents = [TaggedDocument(words=['u0644', 'u0646', 'u062f', 'u0646', 'u060c', 'u0628', 'u0631', 'u0637', 'u0627', 'u0646', 'u06cc', 'u06c1', 'u06a9', 'u0627'], tags='400'), TaggedDocument(words=['do', 'pan', 'en', '1713', 'o', 'soar', 'onde', 'se', 'sit', 'xfaa'], tags='401'), TaggedDocument(words=['u0420', 'u044c', 'u043e', 'u043d', 'u0442', 'u0433', 'u0435', 'u043d', '1901', 'xa0', 'u2022', 'u041b', 'u043e', 'u0440', 'u0435', 'u043d', 'u0446', 'xa0', 'u0417', 'u0435', 'u0435', 'u043c', 'u0430', 'u043d', '1902', 'xa0', 'u2022', 'u0411', 'u0435', 'u043a', 'u0435', 'u0440', 'u0435', 'u043b', 'xa0', 'u041f', 'u0438', 'u0435', 'u0440', 'u041a', 'u044e', 'u0440', 'u0438', 'xa0', 'u041c', 'u0430', 'u0440', 'u0438', 'u044f', 'u041a', 'u044e', 'u0440', 'u0438', '1903', 'xa0', 'u2022', 'u0420', 'u0435', 'u043b', 'u0435', 'u0439', '1904', 'xa0', 'u2022', 'u041b', 'u0435', 'u043d', 'u0430', 'u0440', 'u0434', '1905', 'xa0', 'u2022', 'u0414', 'u0436', 'u0414', 'u0436', 'u0422', 'u043e', 'u043c', 'u0441', 'u044a', 'u043d', '1906', 'xa0', 'u2022', 'u041c', 'u0430', 'u0439', 'u043a', 'u0435', 'u043b', 'u0441', 'u044a', 'u043d', '1907', 'xa0', 'u2022', 'u041b', 'u0438', 'u043f', 'u043c', 'u0430', 'u043d', '1908', 'xa0', 'u2022', 'u041c', 'u0430', 'u0440', 'u043a', 'u043e', 'u043d', 'u0438', 'xa0', 'u0411', 'u0440', 'u0430', 'u0443', 'u043d', '1909', 'xa0', 'u2022', 'u0412', 'u0430', 'u043d', 'xa0', 'u0434', 'u0435', 'u0440', 'xa0', 'u0412', 'u0430', 'u0430', 'u043b', 'u0441', '1910', 'xa0', 'u2022', 'u0412', 'u0438', 'u043d', '1911', 'xa0', 'u2022', 'u0414', 'u0430', 'u043b', 'u0435', 'u043d', '1912', 'xa0', 'u2022', 'u041a', 'u0430', 'u043c', 'u0435', 'u0440', 'u043b', 'u0438', 'u043d', 'u0433', 'xa0', 'u041e', 'u043d', 'u0435', 'u0441', '1913', 'xa0', 'u2022', 'u0424', 'u043e', 'u043d', 'xa0', 'u041b', 'u0430', 'u0443', 'u0435', '1914', 'xa0', 'u2022', 'u0423', 'u0438', 'u043b', 'u044f', 'u043c', 'u041b', 'u0411', 'u0440', 'u0430', 'u0433', 'xa0', 'u0423', 'u0438', 'u043b', 'u044f', 'u043c', 'u0425', 'u0411', 'u0440', 'u0430', 'u0433', '1915', 'xa0', 'u2022', 'u0411', 'u0430', 'u0440', 'u043a', 'u043b', 'u0430', '1917', 'xa0', 'u2022', 'u041f', 'u043b', 'u0430', 'u043d', 'u043a', '1918', 'xa0', 'u2022', 'u0429', 'u0430', 'u0440', 'u043a', '1919'], tags='402'), TaggedDocument(words=['nagusia', 'da'], tags='403'), TaggedDocument(words=['sino', 'que', 'los', 'ciudadanos', 'pueden', 'elegir', 'detraer', 'un', 'porcentaje', 'de', 'sus', 'impuestos', 'para', 'esta', 'causa', '68', '69', 'un', 'sistema', 'similar', 'se', 'da', 'en', 'alemania', 'o', 'austria', 'aunque', 'all', 'xed', 'se', 'impone', 'un', 'impuesto', 'eclesi', 'xe1stico'], tags='404'), TaggedDocument(words=['1244', 'c', 'xfc'], tags='405'), TaggedDocument(words=['u062a', 'u063a', 'u064a', 'u064a', 'u0631', 'u0644', 'u0641', 'u0638', 'u0627', 'u0644', 'u0643', 'u0644', 'u0645', 'u0629', 'u060c', 'u0641', 'u0645', 'u062b', 'u0644', 'u0627', 'u064b', 'rat', 'u062a', 'u0644', 'u0641', 'u0638', 'u0631', 'u0627', 'u062a'], tags='406'), TaggedDocument(words=['d', 'xfcrziler'], tags='407'), TaggedDocument(words=['xung', 'quanh', 'u0111', 'xf3'], tags='408'), TaggedDocument(words=['oblika', 'u0161to'], tags='409'), TaggedDocument(words=['u0432', 'u0430', 'u043b', 'u044e', 'u0442', 'u043d', 'u043e', 'u0433', 'u043e', 'u0441', 'u043e', 'u044e', 'u0437', 'u0443'], tags='410'), TaggedDocument(words=['sacerdotal', 'es'], tags='411'), TaggedDocument(words=['natoque', 'nisi'], tags='412'), TaggedDocument(words=['u0631', 'u0627', 'u0645', 'u06cc', 'u200c', 'u062a', 'u0648', 'u0627', 'u0646', 'u062f', 'u0631', 'u0627', 'u06cc', 'u0627', 'u0644', 'u0627', 'u062a', 'u0645', 'u062a', 'u062d', 'u062f', 'u0647', 'u0622', 'u0645', 'u0631', 'u06cc', 'u06a9', 'u0627', 'u06a9', 'u0627', 'u0646', 'u0627', 'u062f', 'u0627', 'u0628', 'u0631', 'u0632', 'u06cc', 'u0644', 'u0648', 'u0622', 'u0631', 'u0698', 'u0627', 'u0646', 'u062a', 'u06cc', 'u0646'], tags='413'), TaggedDocument(words=['u0423', 'u0439', 'u0433', 'u0443', 'u0440', 'u0441', 'u044c', 'u043a', 'u0430', 'u043c', 'u043e', 'u0432', 'u0430'], tags='414'), TaggedDocument(words=['termin', 'poznat', 'kao'], tags='415'), TaggedDocument(words=['les', 'fr', 'xe8res', 'lumi', 'xe8re'], tags='416'), TaggedDocument(words=['26', 'u03c0', 'u03b5', 'u03c1', 'u03af', 'u03c0', 'u03bf', 'u03c5', 'u03b1', 'u03b9', 'u03ce', 'u03bd', 'u03b5', 'u03c2', 'u03b7', 'u03c0', 'u03cc', 'u03bb', 'u03b7', 'u03c4', 'u03b7', 'u03c2', 'u0391', 'u03c5', 'u03bb', 'u03ce', 'u03bd', 'u03b1', 'u03c2', 'u03b5', 'u03af', 'u03bd', 'u03b1', 'u03b9', 'u03c3', 'u03ae', 'u03bc', 'u03b5', 'u03c1', 'u03b1'], tags='417'), TaggedDocument(words=['xcen', '13'], tags='418'), TaggedDocument(words=['acts', 'of', 'civil', 'disobedience', 'forced', 'the', 'head', 'of', 'the', 'local'], tags='419'), TaggedDocument(words=['hugo', 'az', 'xe1llamcs', 'xedny'], tags='420'), TaggedDocument(words=['f', 'xf8rste', 'nu', 'uofficielle', 'vers', 'forbindes', 'ofte', 'med', 'nynazistiske', 'synspunkter'], tags='421'), TaggedDocument(words=['gisulti', 'kanila', 'sa', 'mga', 'langyaw', 'nagtuong', 'gipangutana', 'sila', 'kon'], tags='422'), TaggedDocument(words=['u043d', 'u0430', 'u0438', 'u0432', 'u0440', 'u0438', 'u0442'], tags='423'), TaggedDocument(words=['its', 'influence'], tags='424'), TaggedDocument(words=['a', 'b', 'azerbaijan', 'homeowners', 'evicted', 'for', 'city'], tags='425'), TaggedDocument(words=['dinast', 'xeda', 'lunar', 'de'], tags='426'), TaggedDocument(words=['2', 'wyznawa', 'u0142o', 'judaizmu', '5', 'ponad'], tags='427'), TaggedDocument(words=['quyosh', 'vaqt', 'degani'], tags='428'), TaggedDocument(words=['u306e', 'u884c', 'u4fe1', 'u30fb', 'u91cd', 'u5f18', 'u3001', 'u9678', 'u5965', 'u56fd', 'u306e', 'u821e', 'u8349', 'u6d3e', 'u3001', 'u51fa', 'u7fbd', 'u56fd', 'u306e', 'u6708', 'u5c71', 'u6d3e', 'u3001', 'u4f2f', 'u8006', 'u56fd', 'u306e', 'u5b89', 'u92fc', 'u6d3e', 'u3001', 'u5099', 'u4e2d', 'u56fd', 'u306e', 'u53e4', 'u9752', 'u6c5f', 'u6d3e', 'u306e', 'u5b88', 'u6b21', 'u30fb', 'u6052', 'u6b21', 'u30fb', 'u5eb7', 'u6b21', 'u30fb', 'u8c9e', 'u6b21', 'u30fb', 'u52a9', 'u6b21', 'u30fb', 'u5bb6', 'u6b21', 'u30fb', 'u6b63', 'u6052', 'u3001', 'u8c4a', 'u5f8c', 'u56fd', 'u306e', 'u5b9a', 'u79c0', 'u6d3e', 'u3001', 'u85a9', 'u6469', 'u56fd', 'u306e', 'u53e4', 'u6ce2', 'u5e73', 'u6d3e', 'u306e', 'u884c', 'u5b89', 'u306a', 'u3069', 'u304c', 'u5b58', 'u5728', 'u3059', 'u308b', '7', '8', '9'], tags='429'), TaggedDocument(words=['p', 'xe5', '4'], tags='430'), TaggedDocument(words=['editovat'], tags='431'), TaggedDocument(words=['u0437', 'u0437', 'u0430', 'u0431', 'u043e', 'u0439', 'u0441', 'u0442', 'u0432', 'u0430', 'u043c', 'u0443'], tags='432'), TaggedDocument(words=['10', 'u043b', 'u0438', 'u043f', 'u043d', 'u044f', '1943', 'u0440', 'u043e', 'u043a', 'u0443', 'u0441', 'u043e', 'u044e', 'u0437', 'u043d', 'u0438', 'u043a', 'u0438', 'u0432', 'u0438', 'u0441', 'u0430', 'u0434', 'u0438', 'u043b', 'u0438', 'u0441', 'u044f', 'u0432', 'u0421', 'u0438', 'u0446', 'u0438', 'u043b', 'u0456', 'u0457', 'u0406', 'u0442', 'u0430', 'u043b', 'u0456', 'u0439', 'u0441', 'u044c', 'u043a', 'u0456'], tags='433'), TaggedDocument(words=['136', 'selvom', 'det', 'egentligt', 'ligger', 'i', 'sundby', 'p', 'xe5', 'lollandssiden', 'af', 'guldborgsund', 'centret', 'blev', 'grundlagt', 'i', '1989', 'da', 'byen', 'fejrede', '700', 'xe5rs', 'jubil', 'xe6um', 'bymuseet', 'rekonstruerede', 'som', 'de', 'f', 'xf8rste', 'i', 'verden', 'en', 'middelalderlig', 'kastemaskine', 'kaldet', 'en', 'blide'], tags='434'), TaggedDocument(words=['latine', 'redditur'], tags='435'), TaggedDocument(words=['ljubljani', 'in', 'njeni'], tags='436'), TaggedDocument(words=['u0442', 'u0430', 'u043d', 'u044b', 'u043c', 'u0430', 'u043b', 'u049b', 'u043e', 'u043d', 'u0430', 'u049b', 'u04af', 'u0439', 'u043b', 'u0435', 'u0440'], tags='437'), TaggedDocument(words=['u2022', 'hassib', 'ben'], tags='438'), TaggedDocument(words=['kurtulmu', 'u015f', 'olan', 'u0130talya'], tags='439')]

model = Doc2Vec(documents=labeled_sents, size=10, alpha=.035, window=4, 
    sample=1e-5, workers=4, min_count=1)
</code></pre>

<p>Now, I thought that <code>model.docvecs</code> would give me a list of arrays, with the first array corresponding to the vector for sentence 1, the second array corresponding to the vector for sentence 2, etc. But instead, it's got length 10! </p>

<p>I get <code>model.docvecs[0] = array([ 0.02312995, -0.00339695, -0.01273827,  0.01944644, -0.03247212, -0.04663946,  0.01369059,  0.03289782,  0.03516903, -0.03435936], dtype=float32)</code></p>

<p>What are these <code>docvecs</code> then? How do I get the output desired, which is a matrix of dimensions (40, 10) in this example?</p>

<hr>

<p>I saw this <a href=""https://stackoverflow.com/questions/31321209/doc2vec-how-to-get-document-vectors"">here</a>, and the correct answer says at the bottom ""where 99 is the document id whose vector we want."" So this makes me even more confused, as he seems to say that <code>model.docvecs</code> SHOULD be indexing a matrix where each row is a document vector!</p>
","6327449","","-1","","2017-05-23 11:53:08","2017-01-18 19:47:34","Understanding the output of Doc2Vec from Gensim package","<python><gensim><word2vec>","2","0","2","","","CC BY-SA 3.0"
"46368720","1","46387065","","2017-09-22 15:55:37","","1","909","<p>TypeError: 'TfidfModel' object is not callable</p>

<p><strong>Why can I not compute the TFIDF Matrix for each Doc after initializing?</strong></p>

<p>I started with 999 <em>documents</em>: 999 paragraphs with about 5-15 sentences each.
After spaCy tokenizing everything, I created the <em>dictionary</em> (~16k unique tokens) and <em>corpus</em> (a list of lists of tuples)</p>

<p>Now I'm ready to create the tfidf matrix (and later LDA and w2V matricies) for some ML; however, after initializing the tfidf model with my corpus (for calculation of the 'IDF')
<code>tfidf = models.TfidfModel(corpus)</code> I get the following error message when trying to see the tfidf of each doc <code>tfidf(corpus[5])</code>
<strong>TypeError: 'TfidfModel' object is not callable</strong></p>

<p>I am able to create this model using a differnt corpus where i have four docs each comprised of only a sentence.
There I can confirm that the expected corpus fomat is a list of lists of tuples: 
[doc1[(word1, count),(word2, count),...], doc2[(word3, count),(word4,count),...]...]</p>

<pre><code>from gensim import corpora, models, similarities

texts = [['teenager', 'martha', 'moxley'...], ['ok','like','kris','usual',...]...]
dictionary = corpora.Dictionary(texts)
&gt;&gt;&gt; Dictionary(15937 unique tokens: ['teenager', 'martha', 'moxley']...)

corpus = [dictionary.doc2bow(text) for text in texts]
&gt;&gt;&gt; [[(0, 2),(1, 2),(2, 1)...],[(3, 1),(4, 1)...]...]

tfidf = models.TfidfModel(corpus)
&gt;&gt;&gt; TfidfModel(num_docs=999, num_nnz=86642)

tfidf(corpus[0])
&gt;&gt;&gt; TypeError: 'TfidfModel' object is not callable

corpus[0]
&gt;&gt;&gt; [(0, 2),(1, 2),(2, 1)...]

print(type(corpus),type(corpus[1]),type(corpus[1][3]))
&gt;&gt;&gt; &lt;class 'list'&gt; &lt;class 'list'&gt; &lt;class 'tuple'&gt;
</code></pre>
","8075015","","","","","2017-09-24 05:55:14","TFIDIF Model Creation TypeError in Gensim","<python><nlp><gensim><tf-idf><language-features>","2","0","","","","CC BY-SA 3.0"
"46612949","1","46709591","","2017-10-06 19:45:38","","3","756","<p>Currently I have 1.2tb text data to build gensim's word2vec model. It is almost taking 15 to 20 days to complete. </p>

<p>I want to build model for 5tb of text data, then it might take few months to create model. I need to minimise this execution time. Is there any way we can use multiple big systems to create model? </p>

<p>Please suggest any way which can help me in reducing the execution time.</p>

<p>FYI, I have all my data in S3 and I use smart_open module to stream the data.</p>
","5059870","","","","","2021-02-05 04:09:08","Can we build word2vec model in a distributed way?","<nlp><deep-learning><distributed-computing><gensim><word2vec>","2","2","2","","","CC BY-SA 3.0"
"53265028","1","53265194","","2018-11-12 15:12:41","","0","147","<p>I have a code that converts word to vector. Below is my code:</p>

<pre><code># word_to_vec_demo.py

from gensim.models import word2vec
import logging

logging.basicConfig(format='%(asctime)s : \
%(levelname)s : %(message)s', level=logging.INFO)

sentences = [['In', 'the', 'beginning', 'Abba','Yahweh', 'created', 'the',
'heaven', 'and', 'the', 'earth.', 'And', 'the', 'earth', 'was',
'without', 'form,', 'and', 'void;', 'and', 'darkness', 'was',
'upon', 'the', 'face', 'of', 'the', 'deep.', 'And', 'the',
'Spirit', 'of', 'Yahweh', 'moved', 'upon', 'the', 'face',  'of',
'the', 'waters.']]

model = word2vec.Word2Vec(sentences, size=10, min_count=1)

print(""Vector for \'earth\' is: \n"")
print(model.wv['earth'])

print(""\nEnd demo"")
</code></pre>

<p>The output is </p>

<pre><code>Vector for 'earth' is: 

[-0.00402722  0.0034133   0.01583795  0.01997946  0.04112177  0.00291858
-0.03854967  0.01581967 -0.02399057  0.00539708]
</code></pre>

<p>Is it possible to encode from array of vector to words? If yes, how will I implement it in Python?</p>
","4780863","","4780863","","2018-11-13 06:17:04","2018-11-13 06:17:04","Python - Data Encoding Vector To Word","<python><machine-learning><nlp><gensim><word2vec>","1","2","","","","CC BY-SA 4.0"
"46441876","1","46442268","","2017-09-27 07:27:33","","5","1884","<p>I am building a word2vec model as follows.</p>

<pre><code>from gensim.models import word2vec, Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')
trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')

for sent in sentence_stream:
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]

    print(bigrams_)
    print(trigrams_)


# Set values for various parameters
num_features = 10    # Word vector dimensionality                      
min_word_count = 1   # Minimum word count                        
num_workers = 4       # Number of threads to run in parallel
context = 5          # Context window size                                                                                    
downsampling = 1e-3   # Downsample setting for frequent words


model = word2vec.Word2Vec(trigrams_, workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling)

vocab = list(model.wv.vocab.keys())
print(vocab[:10])
</code></pre>

<p>However, the output I get for the model's vocabulary is single character as follows. </p>

<pre><code>['h', 'u', 'm', 'a', 'n', ' ', 'c', 'o', 'p', 't']
</code></pre>

<p>I am getting the bigrams and trigrams correctly. Hence, I am just confused where I make the code wrong. Please let me know what is the problem?</p>
","","user8510273","","user8510273","2017-09-27 07:37:28","2017-09-27 07:49:52","Why do I get single letter vocabulary in Gensim word2vec?","<python><gensim><word2vec>","1","2","","","","CC BY-SA 3.0"
"63843793","1","63880910","","2020-09-11 08:53:05","","3","645","<p>The goal I want to achieve is to find a good word_and_phrase embedding model that can do:
(1) For the words and phrases that I am interested in, they have embeddings.
(2) I can use embeddings to compare similarity between two things(could be word or phrase)</p>
<p>So far I have tried two paths:</p>
<p>1: Some Gensim-loaded pre-trained models, for instance:</p>
<pre><code>from gensim.models.word2vec import Word2Vec
import gensim.downloader as api
# download the model and return as object ready for use
model_glove_twitter = api.load(&quot;fasttext-wiki-news-subwords-300&quot;)
model_glove_twitter.similarity('computer-science', 'machine-learning')
</code></pre>
<p>The problem with this path is that I do not know if a phrase has embedding. For this example, I got this error:</p>
<pre><code>KeyError: &quot;word 'computer-science' not in vocabulary&quot;
</code></pre>
<p>I will have to try different pre-trained models, such as word2vec-google-news-300, glove-wiki-gigaword-300, glove-twitter-200, etc. Results are similar, there are always phrases of interests not having embeddings.</p>
<ol start=""2"">
<li>Then I tried to use some BERT-based sentence embedding method: <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a>.</li>
</ol>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('distilbert-base-nli-mean-tokens')

from scipy.spatial.distance import cosine

def cosine_similarity(embedding_1, embedding_2):
    # Calculate the cosine similarity of the two embeddings.
    sim = 1 - cosine(embedding_1, embedding_2)
    print('Cosine similarity: {:.2}'.format(sim))

phrase_1 = 'baby girl'
phrase_2 = 'annual report'
embedding_1 = model.encode(phrase_1)
embedding_2 = model.encode(phrase_2)
cosine_similarity(embedding_1[0], embedding_2[0])
</code></pre>
<p>Using this method I was able to get embeddings for my phrases, but the similarity score was 0.93, which did not seem to be reasonable.</p>
<p>So what can I try else to achieve the two goals mentioned above?</p>
","13566331","","13566331","","2020-09-11 09:04:22","2020-09-14 21:40:28","Looking for an effective NLP Phrase Embedding model","<nlp><gensim><word2vec><fasttext>","1","7","2","","","CC BY-SA 4.0"
"54888490","1","54891714","","2019-02-26 15:05:38","","4","4777","<p>how to print to log (file or stout) the loss of each epoch in the training phase, when using gensim word2vec model. </p>

<p>I tried :</p>

<pre><code> logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
 logging.root.setLevel(level=logging.INFO)
</code></pre>

<p>But I didn't saw any loss printing.</p>
","2228884","","6573902","","2021-08-09 11:13:53","2021-08-09 11:13:53","gensim word2vec print log loss","<python><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"61873864","1","61998803","","2020-05-18 16:01:41","","0","114","<p>My issue is the following. I have some pretrained vectors saved in txt format, I load them in a dict. But when I try to save them after training them again in gensim it gives me an error, like the following:</p>

<pre><code>UnicodeDecodeError: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)
</code></pre>

<p>I'm using this code to create the gensim word2vec:</p>

<pre><code>w2vObject = gensim.models.Word2Vec(min_count=1, sample=threshold, sg=1,size=dimension, negative=15, iter=epochsNum, window=3) # create only the shell

print('Starting vocab build')
# t = time()
w2vObject.build_vocab(sentences, progress_per=10000) #here is the vocab being built as told in google groups gensim

print(w2vObject.wv['the'], 'before train')
</code></pre>

<p>Then I'm replacing the current untrained vectors with:</p>

<pre><code>f = codecs.open(f'../../../WordNetGraphHD/StorageEmbeddings/EmbeddingFormat{dimension}.txt')##os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))
embeddings_index = {}
for num, line in enumerate(f):
    values = my_split(line) # line.split('\t')
    word = values[0].rstrip()
    # vector = ''.join(num for num in values[1:])
    vector = values[1]
    if len(vector) != 300:
        print(line, 'here not 300')

    else:
        coefs = np.asarray(vector)

f.close()
</code></pre>

<p>This code replaces the untrained random vectors wit my own pretrained:</p>

<pre><code>i = 0
for elem in w2vObject.wv.vocab:
    if elem in embeddings_index.keys():
        w2vObject.wv[elem] = embeddings_index[elem]
        i += 1
        print('Found one', i)

print(i)
</code></pre>

<p>Next I train them again with gensim:</p>

<pre><code>w2vObject.train(sentences, total_examples=w2vObject.corpus_count, epochs=epochsNum)#w2vObject.iter)
</code></pre>

<p>Finally save them:</p>

<pre><code>print(w2vObject.wv, 'after train')
w2vObject.wv.save_word2vec_format('./GensimOneWNet.txt', binary=False)
print('saved')
</code></pre>

<p>If I don't replace the vectors with my own saving works, but I need to replace them and save them as txt, any help?</p>

<p>EDIT:</p>

<p>Here is my_split() function:</p>

<pre><code>def my_split(s):
    return list(re.split(""-?\d+.?\d*(?:[Ee]-\d+)?"", s))[0] ,list(re.findall(""-?\d+.?\d*(?:[Ee]-\d+)?"", s))
</code></pre>

<p>And here is a bit of data 300 dimensions for the embedding_index:</p>

<pre><code>'hood -0.013093032778433955 -0.004199660490964164 -0.013285915004532987 0.004154925177649314 -0.004331536946207293 -0.013220217973950956 -0.004774150107654365 0.004774714449991327 0.0040749706101727646...
's gravenhage 0.01400977963089465 -0.0047073654478706935 -0.004326147699308312 0.01323622314514233 -0.004702524319745591 0.004695915697719624 0.00497792763673179 -0.004391661500805715 0.0046651111592470...
'tween 0.008467020793348493 -0.008027116343722267 0.007882368315816719 0.00754852526967863 0.008563484027417608 0.00812782576892597 0.008192394872536986 0.0075759585496093206...
</code></pre>

<p>Added code here:
<a href=""https://pastebin.com/GKPnENxv"" rel=""nofollow noreferrer"">Python code runs fine without my vectors, crashes with them</a></p>

<p>Populate embedding_index I go through all the words and vectors in the txt and if for some reason a vector is not 300 dim, skip it:</p>

<pre><code>f = codecs.open(f'../../../WordNetGraphHD/StorageEmbeddings/EmbeddingFormat{dimension}.txt', encoding='utf-8')##os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))
embeddings_index = {}
for num, line in enumerate(f):
    values = my_split(line) # line.split('\t')
    word = values[0].rstrip()
    vector = values[1]
    if len(vector) != 300:
        print(line, 'here not 300')
    else:
        coefs = np.asarray(vector)
        embeddings_index[word] = coefs

f.close()
</code></pre>

<p>EDIT2:
Here is the stack trace of the full error:
Traceback (most recent call last):</p>

<pre><code>  File ""GensimTestSave.py"", line 136, in &lt;module&gt;
    w2vObject.wv.save_word2vec_format('./GensimOneWNet.txt', binary=False) #encoding='utf-8' )
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/keyedvectors.py"", line 1453, in save_word2vec_format
    fname, self.vocab, self.vectors, fvocab=fvocab, binary=binary, total_vec=total_vec)
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/utils_any2vec.py"", line 291, in _save_word2vec_format
    fout.write(utils.to_utf8(""%s %s\n"" % (word, ' '.join(repr(val) for val in row))))
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/utils_any2vec.py"", line 291, in &lt;genexpr&gt;
    fout.write(utils.to_utf8(""%s %s\n"" % (word, ' '.join(repr(val) for val in row))))
UnicodeDecodeError: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)
</code></pre>
","1423656","","1423656","","2020-05-19 16:56:55","2020-05-25 08:43:14","Gensim saving word vectors in txt format error","<python><gensim><word2vec>","1","8","","","","CC BY-SA 4.0"
"63815090","1","","","2020-09-09 15:41:49","","0","141","<p>(0,
'0.707*&quot;‡§â‡§§‡•ç‡§§‡§∞‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ‡•Ä&quot; + 0.707*&quot;‡§Ø‡•Ç‡§∞‡•ã‡§™&quot; + <strong><em>-0.000</em>&quot;‡§¨‡•Å‡§¶‡•ç‡§ß&quot;</strong> + -0.000*&quot;‡§ú‡§®‡•ç‡§Æ&quot; + '
'0.000*&quot;‡§¨‡•á‡§≤‡•ç‡§ú‡§ø‡§Ø‡§Æ&quot; + 0.000*&quot;‡§ï‡§ø‡§Ç‡§ó‡§°‡§Æ&quot; + 0.000*&quot;‡§®‡•á‡§™‡§æ‡§≤&quot; + 0.000*&quot;‡§ë‡§´‡§º&quot; + '
<strong><em><em>'-0.000</em>&quot;‡§Ø‡•Å‡§®&quot; + -0.000</em>&quot;‡§∏‡•ç‡§•‡§≤‡•Ä&quot;</strong>*')]<br />
Where as the documentation says <br />
<strong>show_topics(num_topics=-1, num_words=10, log=False, formatted=True)<br />
Return num_topics most significant topics (return all by default).
For each topic, show num_words most significant words (10 words by default).</strong></p>
<p>The topics are returned as a list ‚Äì a list of strings if formatted is True, or a list of (word, probability) 2-tuples if False.</p>
<p>If log is True, also output this result to log.</p>
<pre><code>def preprocessing(corpus):
    for document in corpus:
        doc = strip_short(document,3)
        doc = strip_punctuation(doc)
        yield word_tokenize(doc)
texts = preprocessing(corpus)
dictionary = corpora.Dictionary(texts)
dictionary.filter_extremes(no_below=1, keep_n=25000)

doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in preprocessing(corpus)]
tfidf = models.TfidfModel(doc_term_matrix)
corpus_tfidf = tfidf[doc_term_matrix]

lsi = models.LsiModel(corpus_tfidf, id2word=dictionary)
pprint(lsi.show_topics(num_topics=4, num_words=10))
</code></pre>
<pre><code>[(0,
  '0.707*&quot;‡§â‡§§‡•ç‡§§‡§∞‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ‡•Ä&quot; + 0.707*&quot;‡§Ø‡•Ç‡§∞‡•ã‡§™&quot; + -0.000*&quot;‡§¨‡•Å‡§¶‡•ç‡§ß&quot; + -0.000*&quot;‡§ú‡§®‡•ç‡§Æ&quot; + '
  '0.000*&quot;‡§¨‡•á‡§≤‡•ç‡§ú‡§ø‡§Ø‡§Æ&quot; + 0.000*&quot;‡§ï‡§ø‡§Ç‡§ó‡§°‡§Æ&quot; + 0.000*&quot;‡§®‡•á‡§™‡§æ‡§≤&quot; + 0.000*&quot;‡§ë‡§´‡§º&quot; + '
  '-0.000*&quot;‡§Ø‡•Å‡§®&quot; + -0.000*&quot;‡§∏‡•ç‡§•‡§≤‡•Ä&quot;'),
 (1,
  '0.577*&quot;‡§ï‡§ø‡§Ç‡§ó‡§°‡§Æ&quot; + 0.577*&quot;‡§¨‡•á‡§≤‡•ç‡§ú‡§ø‡§Ø‡§Æ&quot; + 0.577*&quot;‡§ë‡§´‡§º&quot; + -0.000*&quot;‡§ú‡§®‡•ç‡§Æ&quot; + '
  '-0.000*&quot;‡§¨‡•Å‡§¶‡•ç‡§ß&quot; + -0.000*&quot;‡§≠‡§ó‡§µ‡§æ‡§®&quot; + -0.000*&quot;‡§∏‡•ç‡§•‡§ø‡§§&quot; + -0.000*&quot;‡§≤‡•Å‡§Ç‡§¨‡§ø‡§®‡•Ä&quot; + '
  '-0.000*&quot;‡§â‡§§‡•ç‡§§‡§∞‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ‡•Ä&quot; + -0.000*&quot;‡§Ø‡•Ç‡§∞‡•ã‡§™&quot;'),
 (2,
  '0.354*&quot;‡§ú‡§®‡•ç‡§Æ&quot; + 0.354*&quot;‡§≠‡§ó‡§µ‡§æ‡§®&quot; + 0.354*&quot;‡§∏‡•ç‡§•‡§ø‡§§&quot; + 0.354*&quot;‡§∏‡•ç‡§•‡§≤‡•Ä&quot; + 0.354*&quot;‡§Ø‡•Å‡§®&quot; '
  '+ 0.354*&quot;‡§¨‡•Å‡§¶‡•ç‡§ß&quot; + 0.354*&quot;‡§≤‡•Å‡§Ç‡§¨‡§ø‡§®‡•Ä&quot; + 0.354*&quot;‡§®‡•á‡§™‡§æ‡§≤&quot; + 0.000*&quot;‡§â‡§§‡•ç‡§§‡§∞‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ‡•Ä&quot; + '
  '0.000*&quot;‡§Ø‡•Ç‡§∞‡•ã‡§™&quot;')]
</code></pre>
","9383746","","130288","","2020-09-09 23:00:50","2020-09-10 09:28:36","How come probabilities returned by Gensim LSI method show_topics are negative?","<python><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"35917500","1","","","2016-03-10 13:10:30","","0","481","<p>I'm playing around with Linan Qiu's <a href=""https://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow"">example</a> of a word2vec implementation (<a href=""https://github.com/linanqiu/word2vec-sentiments"" rel=""nofollow"">Github</a>), with the final goal to analyse a bunch of tweets. </p>

<p>The problem I'm facing is that I have no idea on how to extract positive/negative/polarity percentages from this implementation of word2vec. The code delivers an accuracy rate so I presume it must check the predicted value (POS/NEG) against the known value (in this case an entire .txt is filled with either POS or NEG). So my approach would be to get the predicted POS/NEG rating per document(in this case per review), then of course simply add those up (the number of ratings, I mean) and divide POS and NEG from it to get a percentage. This percentage would then cover all documents in that file. From this, the polarity could perhaps also be calculated but I'm trying to figure POS/NEG out first.</p>

<p>Would anyone have any idea on how to get those predicted ratings? Below is the post-vectorisation code, but it's rather similar (cough) to the standard used.</p>

<p>Thank you so much!</p>

<pre><code># gensim modules
from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec

# numpy
import numpy

# shuffle
from random import shuffle

# logging
import logging
import os.path
import sys
import cPickle as pickle

# logres
from sklearn.linear_model import LogisticRegression

#commit

model = Doc2Vec.load('./imdb.d2v')

train_arrays = numpy.zeros((25000, 100))
train_labels = numpy.zeros(25000)

for i in range(12500):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    train_arrays[i] = model.docvecs[prefix_train_pos]
    train_arrays[12500 + i] = model.docvecs[prefix_train_neg]
    train_labels[i] = 1
    train_labels[12500 + i] = 0



test_arrays = numpy.zeros((25000, 100))
test_labels = numpy.zeros(25000)

for i in range(12500):
    prefix_test_pos = 'TEST_POS_' + str(i)
    prefix_test_neg = 'TEST_NEG_' + str(i)
    test_arrays[i] = model.docvecs[prefix_test_pos]
    test_arrays[12500 + i] = model.docvecs[prefix_test_neg]
    test_labels[i] = 1
    test_labels[12500 + i] = 0


classifier = LogisticRegression()
classifier.fit(train_arrays, train_labels)

print classifier.score(test_arrays, test_labels)
</code></pre>
","6044838","","6044838","","2016-03-10 13:51:35","2016-04-21 04:39:32","Getting positive/negative percentages from word2vec","<python><sentiment-analysis><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"62125400","1","","","2020-06-01 03:49:11","","-1","235","<p>i don't know how to train model in multiples batches with doc2vec . Since i load all my data in ram and it't can not be loaded </p>

<pre><code>#Import all the dependencies
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
#import ReadExeFileCapstone
import update-doc2vec 
mapData = ReadExeFileCapstone.readData()

# print ('mapData', mapData)

max_epochs = 10000
vec_size = 200
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha,
                min_alpha=0.00025,
                min_count=1,
                dm =1)
data = []
for key in mapData:
    listData = mapData[key]
    # print (""listData: "", len(listData), listData)

    for i in range(len(listData)):
        listToStr = ' '.join([str(elem) for elem in listData[i]]) #convert array to list string
        data.append(listToStr)

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]


model.build_vocab(tagged_data)
#build vocab
for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha
# train model   
model.save(""d2v_ASM.model"")
print(""Model Saved"")
</code></pre>
","12323637","","130288","","2020-06-01 16:42:23","2020-06-02 21:07:58","Is there anyway to train doc2vec model in multiples batches","<gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"27291145","1","27320632","","2014-12-04 10:05:22","","1","260","<p>I can't install gensim successfully through many ways.For I'm  a freshman in coding,it's difficult for me to understand the following information.</p>

<hr>

<pre><code>**C:\Python27\Lib\site-packages\gensim-0.10.3&gt;python setup.py install
Traceback (most recent call last):
  File ""setup.py"", line 22, in &lt;module&gt;
    from setuptools import setup, find_packages, Extension
  File ""C:\Python27\lib\site-packages\setuptools\__init__.py"", line 12, in &lt;modu
le&gt;
    from setuptools.extension import Extension
  File ""C:\Python27\lib\site-packages\setuptools\extension.py"", line 7, in &lt;modu
le&gt;
    from setuptools.dist import _get_unpatched
  File ""C:\Python27\lib\site-packages\setuptools\dist.py"", line 16, in &lt;module&gt;
    from setuptools.compat import numeric_types, basestring
  File ""C:\Python27\lib\site-packages\setuptools\compat.py"", line 19, in &lt;module
&gt;
    from SimpleHTTPServer import SimpleHTTPRequestHandler
  File ""C:\Python27\lib\SimpleHTTPServer.py"", line 27, in &lt;module&gt;
    class SimpleHTTPRequestHandler(BaseHTTPServer.BaseHTTPRequestHandler):
  File ""C:\Python27\lib\SimpleHTTPServer.py"", line 208, in SimpleHTTPRequestHand
ler
    mimetypes.init() # try to read system mime.types
  File ""C:\Python27\lib\mimetypes.py"", line 358, in init
    db.read_windows_registry()
  File ""C:\Python27\lib\mimetypes.py"", line 258, in read_windows_registry
    for subkeyname in enum_types(hkcr):
  File ""C:\Python27\lib\mimetypes.py"", line 249, in enum_types
    ctype = ctype.encode(default_encoding) # omit in 3.x!
UnicodeDecodeError: 'ascii' codec can't decode byte 0xb6 in position 6: ordinal
not in range(128)**
</code></pre>

<hr>

<p>Thanks for help!</p>
","4323634","","","","","2014-12-05 16:39:46","A mistake in installing gensim","<python><lda><gensim>","1","0","1","","","CC BY-SA 3.0"
"46754881","1","","","2017-10-15 12:10:44","","2","639","<p>I use Python 3.6.3rc1. I get following message after executing my python script:</p>

<pre><code>Traceback (most recent call last):
  File ""main.py"", line 6, in &lt;module&gt;
    from train import train
  File ""C:\path\train.py"", line 2, in &lt;module&gt;
    import dataUtils
  File ""C:\path\dataUtils.py"", line 5, in &lt;module&gt;
    import gensim
ImportError: No module named gensim
</code></pre>

<p>But gensim is already installed. Command <code>pip3 freeze</code> gives following list:</p>

<pre><code>bleach==1.5.0
boto==2.48.0
bz2file==0.98
certifi==2017.7.27.1
chardet==3.0.4
gensim==3.0.1
html5lib==0.9999999
idna==2.6
jieba==0.39
Markdown==2.6.9
numpy==1.13.3+mkl
protobuf==3.4.0
requests==2.18.4
scipy==0.19.1
six==1.11.0
smart-open==1.5.3
tensorflow==1.3.0
tensorflow-tensorboard==0.1.7
urllib3==1.22
Werkzeug==0.12.2
</code></pre>

<p>When I imported KeyedVectors from gensim.models (<code>from gensim.models import KeyedVectors</code>) in another script, it worked.</p>

<p>Any suggestions?</p>

<p>Thanks for any reply.</p>
","7476166","","","","","2017-10-15 12:38:53","Python3.6 - Cannot import gensim in Windows","<python><python-3.x><pip><gensim>","2","0","","","","CC BY-SA 3.0"
"35716121","1","","","2016-03-01 06:30:21","","41","29429","<p>For preprocessing the corpus I was planing to extarct common phrases from the corpus, for this I tried using <strong>Phrases</strong> model in gensim, I tried below code but it's not giving me desired output.</p>

<p><strong>My code</strong></p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""machine learning can be useful sometimes""]

sentence_stream = [doc.split("" "") for doc in documents]
bigram = Phrases(sentence_stream)
sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
print(bigram[sent])
</code></pre>

<p><strong>Output</strong></p>

<pre><code>[u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
</code></pre>

<p><strong>But it should come as</strong> </p>

<pre><code>[u'the', u'mayor', u'of', u'new_york', u'was', u'there']
</code></pre>

<p>But when I tried to print vocab of train data, I can see bigram, but its not working with test data, where I am going wrong?</p>

<pre><code>print bigram.vocab

defaultdict(&lt;type 'int'&gt;, {'useful': 1, 'was_there': 1, 'learning_can': 1, 'learning': 1, 'of_new': 1, 'can_be': 1, 'mayor': 1, 'there': 1, 'machine': 1, 'new': 1, 'was': 1, 'useful_sometimes': 1, 'be': 1, 'mayor_of': 1, 'york_was': 1, 'york': 1, 'machine_learning': 1, 'the_mayor': 1, 'new_york': 1, 'of': 1, 'sometimes': 1, 'can': 1, 'be_useful': 1, 'the': 1}) 
</code></pre>
","5702489","","","","","2016-03-02 13:39:57","How to extract phrases from corpus using gensim","<python><nlp><gensim>","1","0","12","","","CC BY-SA 3.0"
"37593293","1","","","2016-06-02 13:28:33","","38","47624","<p>I want to calculate tf-idf from the documents below. I'm using python and pandas.</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'docId': [1,2,3], 
               'sent': ['This is the first sentence','This is the second sentence', 'This is the third sentence']})
</code></pre>

<p>First, I thought I would need to get word_count for each row. So I wrote a simple function:</p>

<pre><code>def word_count(sent):
    word2cnt = dict()
    for word in sent.split():
        if word in word2cnt: word2cnt[word] += 1
        else: word2cnt[word] = 1
return word2cnt
</code></pre>

<p>And then, I applied it to each row.</p>

<pre><code>df['word_count'] = df['sent'].apply(word_count)
</code></pre>

<p>But now I'm lost. I know there's an easy method to calculate tf-idf if I use Graphlab, but I want to stick with an open source option. Both Sklearn and gensim look overwhelming. What's the simplest solution to get tf-idf?</p>
","1610952","","7758804","","2020-09-20 18:19:29","2020-09-20 18:19:29","How to get tfidf with pandas dataframe?","<python><pandas><scikit-learn><tf-idf><gensim>","3","0","9","","","CC BY-SA 4.0"
"53467414","1","54102266","","2018-11-25 12:25:40","","2","1121","<p>I am trying to calculate similarity between two documents which are comprised of more than thousands sentences.</p>

<p>Baseline would be calculating cosine similarity using BOW.</p>

<p>However, I want to capture more of semantic difference between documents.</p>

<p>Hence, I built word embedding and calculated documents similarity by generating document vectors by simply averaging all the word vectors in each of documents and measure cosine similarity between these documents vectors. </p>

<p>However, since the size of each input document is rather big, the results I get from using the method above are very similar to simple BOW cosine similarity.</p>

<p>I have two questions, </p>

<p>Q1. I found gensim module offers soft cosine similarity. But I am having hard time understanding the difference from the methods I used above, and I think it may not be the mechanism to calculate similarity between million pairs of documents.</p>

<p>Q2. I found Doc2Vec by gensim would be more appropriate for my purpose. But I recognized that training Doc2Vec requires more RAM than I have (32GB) (the size of my entire documents is about 100GB). Would there be any way that I train the model with small part(like 20GB of them) of entire corpus, and use this model to calculate pairwise similarities of entire corpus?
If yes, then what would be the desirable train set size, and is there any tutorial that I can follow?</p>
","7837349","","","","","2019-01-09 18:30:41","Python Calculating similarity between two documents using word2vec, doc2vec","<python><similarity><gensim><word2vec><doc2vec>","1","0","2","","","CC BY-SA 4.0"
"53313575","1","","","2018-11-15 06:23:52","","6","4595","<p>I'm new to python and I need to construct a LDA project. After doing some preprocessing step, here is my code:</p>

<pre><code>dictionary = Dictionary(docs)
corpus = [dictionary.doc2bow(doc) for doc in docs]

from gensim.models import LdaModel
num_topics = 10
chunksize = 2000
passes = 20
iterations = 400
eval_every = None
temp = dictionary[0]
id2word = dictionary.id2token
model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \
                       alpha='auto', eta='auto', \
                       random_state=42, \
                       iterations=iterations, num_topics=num_topics, \
                       passes=passes, eval_every=eval_every)
</code></pre>

<p>I want to get a topic distribution of docs, all of the document and get 10 probability of topic distribution, but when I use:</p>

<pre><code>get_document_topics = model.get_document_topics(corpus)
print(get_document_topics)
</code></pre>

<p>The output only appear </p>

<pre><code>&lt;gensim.interfaces.TransformedCorpus object at 0x000001DF28708E10&gt;
</code></pre>

<p>How do I get a topic distribution of docs?</p>
","10595338","","10595338","","2018-11-15 06:45:12","2018-11-15 08:41:01","How to get document_topics distribution of all of the document in gensim LDA?","<python-3.x><gensim><lda><topic-modeling><probability-distribution>","1","0","1","","","CC BY-SA 4.0"
"55095368","1","55096238","","2019-03-11 04:47:00","","0","3838","<p>I am running the below code, but gensim word2vec is throwing a word not in vocabulary error. Can you let me know the solution?</p>

<p>this is my file(file.txt)</p>

<pre><code>'intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one', 'better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', ..
</code></pre>

<p>this is my code </p>

<pre><code> import gensim 
    with open('file.txt', 'r') as myfile:
      data = myfile.read()



    model = gensim.models.Word2Vec(data,min_count=1,size=32)
    w1 = ""good""
    model.wv.most_similar (positive=w1)
</code></pre>

<p>Output:</p>

<pre><code>KeyError: ""word 'good' not in vocabulary""


---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-34-22572d5a8082&gt; in &lt;module&gt;()
      7 model = gensim.models.Word2Vec(data,min_count=1,size=32)
      8 w1 = ""good""
----&gt; 9 model.wv.most_similar (positive=w1)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in most_similar(self, positive, negative, topn, restrict_vocab, indexer)
    529                 mean.append(weight * word)
    530             else:
--&gt; 531                 mean.append(weight * self.word_vec(word, use_norm=True))
    532                 if word in self.vocab:
    533                     all_words.add(self.vocab[word].index)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in word_vec(self, word, use_norm)
    450             return result
    451         else:
--&gt; 452             raise KeyError(""word '%s' not in vocabulary"" % word)
    453 
    454     def get_vector(self, word):

KeyError: ""word 'good' not in vocabulary""
</code></pre>

<p>‚Äã</p>
","","user10702710","","user10702710","2019-03-11 04:53:40","2019-03-12 09:15:30","gensim: KeyError: ‚Äúword 'good' not in vocabulary‚Äù","<python-3.x><gensim>","1","3","0","","","CC BY-SA 4.0"
"37749777","1","37893283","","2016-06-10 13:39:30","","1","567","<p>I have anaconda and Pip installed. I tried doing</p>

<pre><code>conda install -c anaconda gensim-0.11.1
</code></pre>

<p>but it couldn't find the package and the following msg was thrown on the PowerShell.</p>

<pre><code>Using Anaconda Cloud api site https://api.anaconda.org
Fetching package metadata: ......
Solving package specifications: .
Error:  Package missing in current win-64 channels:
  - gensim-0.11.1

You can search for this package on anaconda.org with

    anaconda search -t conda gensim-0.11.1
</code></pre>

<p>Any help would be appreciated. Thanks!</p>

<p>--Conda works well with the machine but even help with Pip would be appreciated.</p>
","3667569","","","","","2016-06-18 04:35:57","How to install Gensim version 0.11.1 on Windows 10 Machine?","<python><pip><conda><gensim>","1","0","","","","CC BY-SA 3.0"
"62181162","1","","","2020-06-03 19:29:13","","11","11340","<p>I was doing this and got this error :</p>

<pre><code>from gensim.models import Word2Vec

ImportError: cannot import name 'open' from 'smart_open' (C:\ProgramData\Anaconda3\lib\site-packages\smart_open\__init__.py)
</code></pre>

<p>Then I did this :</p>

<pre><code>import smart_open
dir(smart_open)

['BZ2File','BytesIO','DEFAULT_ERRORS','IS_PY2','P','PATHLIB_SUPPORT','SSLError','SYSTEM_ENCODING','Uri','__builtins__','__cached__','__doc__','__file__','__loader__','__name__','__package__','__path__','__spec__','boto','codecs','collections','gzip','hdfs','http','importlib','io','logger','logging','os','pathlib','pathlib_module','requests','s3','s3_iter_bucket','six','smart_open','smart_open_hdfs','smart_open_http','smart_open_lib','smart_open_s3','smart_open_webhdfs','sys','urlparse','urlsplit','warnings','webhdfs']
</code></pre>

<p>As you can see there is no 'open' in it so how should I solve this. I tried to install different versions
and I upgraded all version too.</p>
","13412418","","","","","2021-07-10 22:23:59","cannot import name 'open' from 'smart_open'","<deep-learning><nlp><importerror><gensim>","7","0","1","","","CC BY-SA 4.0"
"20954805","1","","","2014-01-06 16:42:42","","7","2329","<p><strong>Big picture goal:</strong> I am making an LDA model of product reviews in Python using NLTK and Gensim. I want to run this on varying n-grams. </p>

<p><strong>Problem:</strong> Everything is great with unigrams, but when I run with bigrams, I start to get topics with repeated information. For example, Topic 1 might contain: <code>['good product', 'good value']</code>, and Topic 4 might contain: <code>['great product', 'great value']</code>. To a human these are obviously conveying the same information, but obviously <code>'good product'</code> and <code>'great product'</code> are distinct bigrams. How do I algorithmically determine that <code>'good product'</code> and <code>'great product'</code> are similar enough, so I can translate all occurrences of one of them to the other (maybe the one that appears more often in the corpus)?</p>

<p><strong>What I've tried:</strong> I played around with WordNet's Synset tree, with little luck. It turns out that <code>good</code> is an 'adjective', but <code>great</code> is an 'adjective satellite', and therefore return <code>None</code> for path similarity. My thought process was to do the following:</p>

<ol>
<li>Part of speech tag the sentence</li>
<li>Use these POS to find the correct Synset</li>
<li>Compute similarity of the two Synsets</li>
<li>If they are above some threshold, compute occurrences of both words</li>
<li>Replace the least occurring word with the most occurring word</li>
</ol>

<p>Ideally, though, I'd like an algorithm that can determine that <code>good</code> and <code>great</code> are similar <em>in my corpus</em> (perhaps in a co-occurring sense), so that it can be extended to words that aren't part of the general English language, but appear in my corpus, and so that it can be extended to n-grams (maybe <code>Oracle</code> and <code>terrible</code> are synonymous in my corpus, or <code>feature engineering</code> and <code>feature creation</code> are similar).</p>

<p>Any suggestions on algorithms, or suggestions to get WordNet synset to behave?</p>
","2979931","","","","","2014-01-07 10:13:49","NLTK - Automatically translating similar words","<python><algorithm><nltk><wordnet><gensim>","2","3","4","","","CC BY-SA 3.0"
"64300122","1","","","2020-10-11 02:34:23","","0","226","<p>When using the ldaseqmodel in gensim, I got the running time warning:</p>
<blockquote>
<p>D:\Anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py:1474:
RuntimeWarning: invalid value encountered in double_scalars<br />
converged = np.fabs((lhood_old - lhood) / (lhood_old * total))
D:\Anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py:293:
RuntimeWarning: divide by zero encountered in double_scalars<br />
convergence = np.fabs((bound - old_bound) / old_bound)</p>
</blockquote>
<p>This is the code:</p>
<pre><code>ldaseq = LdaSeqModel(corpus=corpus_comments, id2word=dictionary_comments,time_slice=time_docs, num_topics=5, chunksize=10,lda_model=model)
</code></pre>
<p>The dataset is about 50,000 article in blog.</p>
<p>Please help me ! Thank you so much!</p>
","14428569","","6573902","","2020-10-13 16:11:14","2020-12-21 08:18:12","I get Runtime Warning: invalid value encountered in double_scalars and divide by zero encountered in double_scalars when using ldaseq","<python><python-3.x><numpy><gensim><lda>","1","1","","","","CC BY-SA 4.0"
"64267811","1","","","2020-10-08 17:32:23","","1","52","<p>I am working to calculate similarities of labels of materials. Where each label might have 1-10 words in it. I am using gensim word2vec to find cosine similarities.</p>
<p>My approach is simply to treat each label as a 'sentence' and tokenize each word.<br />
<strong>example:</strong><br />
<strong>labels</strong> = ['wooden desk cherry', 'long sleeve shirt cotton',..]</p>
<p><strong>sentences</strong> = [['wooden', 'desk', 'cherry'], ['long', 'sleeve', 'shirt', 'cotton'],..]</p>
<p>My question is does word2vec take neighboring sentences into its context. for example if i am using a window = 2, when looking at words around 'long', will 'cherry' be included or only 'sleeve'.</p>
<p>If neighboring sentences are considered is there a way to only consider words within the target words sentence.</p>
<p>Thanks for any help. I have read the Word2Vec documentation and couldn't find any information about this.</p>
","9708985","","9708985","","2020-10-08 17:53:30","2020-10-08 20:09:47","does gensim Word2Vec include neighboring sentences in context?","<python><machine-learning><nlp><gensim><word2vec>","1","2","","","","CC BY-SA 4.0"
"44693241","1","44694842","","2017-06-22 07:44:56","","1","3279","<p>The file <code>GoogleNews-vectors-negative300.bin</code> contains 300 million word-vectors. I think (not sure) this file is loaded when the following line is written:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
</code></pre>

<p>I want to download the vectors for words that I give externally in a list called <code>words</code>. This is my code to do this:</p>

<pre><code>import math
import sys
import gensim
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')

from gensim.models.keyedvectors import KeyedVectors

words = ['access', 'aeroway', 'airport', 'amenity', 'area', 'atm', 'barrier', 'bay', 'bench', 'boundary', 'bridge', 'building', 'bus', 'cafe', 'car', 'coast', 'continue', 'created', 'defibrillator', 'drinking', 'ele', 'embankment', 'entrance', 'ferry', 'foot', 'fountain', 'fuel', 'gate', 'golf', 'gps', 'grave', 'highway', 'horse', 'hospital', 'house', 'landuse', 'layer', 'leisure', 'man', 'manmade', 'market', 'marketplace', 'maxheight', 'name', 'natural', 'noexit', 'oneway', 'park', 'parking', 'pgs', 'place', 'worship', 'playground', 'police', 'police station', '', 'post', 'post box or mail', 'power', 'powerstation', 'private', 'public', 'railway', 'ref', 'residential', 'restaurant', 'road', 'route', 'school', 'shelter', 'shop', 'source', 'sport', 'toilet', 'toilets', 'tourism', 'unknown', 'vehicle', 'vending', 'vending machine', 'village', 'wall', 'waste', 'water', 'waterway', 'worship'];

model = gensim.models.KeyedVectors.load_word2vec_format(words, binary=True)

M = len(words)
count = 0
for i in range(1,M):
    wi = id2word[words[i]]
    if wi in word2vec.vocab:
        vector[:,count] = model[:,i]
        count = count+1

f = open('word_vectors.csv', 'w')
print(vector, file=f)
f.close()
</code></pre>

<p>But when I run the code, it just freezes up my system. Is it because it is loading the whole of the binary file before searching for the words in <code>words</code>? If yes, how do I get around this issue? I think of this as I get the following warning, which is why I use the <code>warning</code> package to suppress it:</p>

<pre><code>c:\Python35\lib\site-packages\gensim\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
</code></pre>

<p>And the error it gives is:</p>

<pre><code>Traceback (most recent call last):
  File ""word2vec.py"", line 18, in &lt;module&gt;
    model = gensim.models.KeyedVectors.load_word2vec_format(topic, binary=True) 
  File ""c:\Python35\lib\site-packages\gensim\models\keyedvectors.py"", line 196, in load_word2vec_format
    with utils.smart_open(fname) as fin:
  File ""c:\Python35\lib\site-packages\smart_open\smart_open_lib.py"", line 208, in smart_open
    raise TypeError('don\'t know how to handle uri %s' % repr(uri))
TypeError: don't know how to handle uri [['access'], ['aeroway'], ['airport'], ['amenity'], ['area'], ['atm'], ['barrier'], ['bay'], ['bench'], ['boundary'], ['bridge'], ['building'], ['bus'], ['cafe'], ['car'], ['coast'], ['continue'], ['created'], ['defibrillator'], ['drinking'], ['ele'], ['embankment'], ['entrance'], ['ferry'], ['foot'], ['fountain'], ['fuel'], ['gate'], ['golf'], ['gps'], ['grave'], ['highway'], ['horse'], ['hospital'], ['house'], ['landuse'], ['layer'], ['leisure'], ['man'], ['manmade'], ['market'], ['marketplace'], ['maxheight'], ['name'], ['natural'], ['noexit'], ['oneway'], ['park'], ['parking'], ['pgs'], ['place'], ['worship'], ['playground'], ['police'], ['police station'], [''], ['post'], ['post box or mail'], ['power'], ['powerstation'], ['private'], ['public'], ['railway'], ['ref'], ['residential'], ['restaurant'], ['road'], ['route'], ['school'], ['shelter'], ['shop'], ['source'], ['sport'], ['toilet'], ['toilets'], ['tourism'], ['unknown'], ['vehicle'], ['vending'], ['vending machine'], ['village'], ['wall'], ['waste'], ['water'], ['waterway'], ['worship']]
</code></pre>

<p>This I guess means that the program is not able to search for the words in the binary file. So, how to solve it? </p>
","5305512","","5305512","","2017-06-22 08:13:53","2017-06-22 09:01:44","How to extract a word vector from the Google pre-trained model for word2vec?","<python><file-handling><gensim><word2vec>","1","1","0","","","CC BY-SA 3.0"
"35914287","1","35979229","","2016-03-10 10:48:09","","3","1366","<p>I use ANN to predict words from words. The input and output are all words vectors. I do not know how to get words from the output of ANN. By the way,  it's gensim I am using</p>
","1891074","","58808","","2016-03-10 11:12:33","2016-08-01 09:48:57","word2vec how to get words from vectors?","<machine-learning><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"62179243","1","","","2020-06-03 17:40:03","","1","152","<p>everyone!
I'm trying to import gensim on jupyter but I got the following error:</p>

<p><code>ImportError: cannot import name 'deprecated'</code></p>

<p>What can I do?</p>

<p>PS: I tried in gensim version 3.8.1 and 3.8.3</p>
","8940208","","","","","2020-06-03 17:40:03","ImportError: cannot import name 'deprecated' when import gensim","<python-3.x><jupyter-notebook><gensim>","0","1","","","","CC BY-SA 4.0"
"37745250","1","37793466","","2016-06-10 09:55:13","","1","1011","<p>I am trying to apply the word2vec model implemented in the library gensim in python. I have a list of sentences (each sentences is a list of words).</p>

<p>For instance let us have:</p>

<pre><code>sentences=[['first','second','third','fourth']]*n
</code></pre>

<p>and I implement two identical models:</p>

<pre><code>model = gensim.models.Word2Vec(sententes, min_count=1,size=2)
model2=gensim.models.Word2Vec(sentences, min_count=1,size=2)
</code></pre>

<p>I realize that the models sometimes are the same, and sometimes are different, depending on the value of n. </p>

<p>For instance, if n=100 I obtain</p>

<pre><code>print(model['first']==model2['first'])
True
</code></pre>

<p>while, for n=1000:</p>

<pre><code>print(model['first']==model2['first'])
False
</code></pre>

<p>How is it possible?</p>

<p>Thank you very much!</p>
","4574836","","","","","2016-07-06 16:10:30","Different models with gensim Word2Vec on python","<python><nlp><gensim><word2vec>","1","5","","","","CC BY-SA 3.0"
"20953143","1","20953190","","2014-01-06 15:21:46","","1","3393","<p>I see the following script snippet from the <a href=""http://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">gensim tutorial page</a>.</p>

<p>What's the syntax of <strong>word for word</strong> in below Python script?</p>

<pre><code>&gt;&gt; texts = [[word for word in document.lower().split() if word not in stoplist]
&gt;&gt;          for document in documents]
</code></pre>
","264052","","","","","2014-01-06 15:43:39","What does ""word for word"" syntax mean in Python?","<python><gensim>","3","0","","","","CC BY-SA 3.0"
"36113487","1","","","2016-03-20 12:09:37","","1","720","<p>I have PC with NVIDIA gpu. I have installed OpenBLAS. I am trying to train word vectors using gensim's word2vec implementation. I have set number of workers =4. But when I run top command to see CPU usage. It is showing only 100%. Does it mean only one core is utilised? And my program does not show any speed-up.</p>

<p>My code snippet is:</p>

<pre><code>import gensim
import time
import numpy
class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname
    #called when Word2Vec is called
    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()


sentences=MySentences(""/home/lalchand/NewdatasetforAssgn2/tfidf/spam"")


start = time.time()

model = gensim.models.Word2Vec(sentences, min_count=1,iter=5,workers=4)
print(model.syn0.shape)
</code></pre>
","6089397","","690430","","2016-12-02 16:18:15","2016-12-02 16:18:28","word vectors using gensim's word2vec implementation and GPU does not show any speed-up","<gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"44993240","1","","","2017-07-09 05:19:44","","10","15341","<pre><code>def cosine(vector1,vector2):
    cosV12 = np.dot(vector1, vector2) / (linalg.norm(vector1) * linalg.norm(vector2))
    return cosV12
model=gensim.models.doc2vec.Doc2Vec.load('Model_D2V_Game')
string='Ê∞ëÁîü ‰∏∫‰∫Ü Áà∂‰∫≤ Êàë Ë¶Å ÂùöÂº∫ Âú∞ ...'
list=string.split(' ')
vector1=model.infer_vector(doc_words=list,alpha=0.1, min_alpha=0.0001,steps=5)
vector2=model.docvecs.doctag_syn0[0]
print cosine(vector2,vector1)
</code></pre>

<p>-0.0232586</p>

<p>I use a train data to train a <code>doc2vec</code> model. Then, I use <code>infer_vector()</code> to generate a vector given a document which is in trained data. But they are different. The value of cosine was so small (<code>-0.0232586</code>) distance between the <code>vector2</code> which was saved in <code>doc2vec</code> model and the <code>vector1</code> which was generated by <code>infer_vector()</code>. But this is not reasonable ah ...</p>

<p><strong>I find where i have error in. I should use 'string=u'Ê∞ëÁîü ‰∏∫‰∫Ü Áà∂‰∫≤ Êàë Ë¶Å ÂùöÂº∫ Âú∞ ...'' instead 'string='Ê∞ëÁîü ‰∏∫‰∫Ü Áà∂‰∫≤ Êàë Ë¶Å ÂùöÂº∫ Âú∞ ...''. When I correct this way, the cosine distance is up to 0.889342.</strong> </p>
","7368736","","7368736","","2017-07-09 06:15:57","2017-07-09 16:15:45","How to use the infer_vector in gensim.doc2vec?","<python><gensim><doc2vec>","1","3","4","","","CC BY-SA 3.0"
"64305110","1","","","2020-10-11 14:26:32","","0","136","<p><strong>BACKGROUND</strong></p>
<p>At the beginning of my project, the focus was to compare requests/questions received in terms of how they differ in terms of content. I trained a <code>Doc2Vec</code> model and the results were pretty good (for reference, my data included 14 million requests).</p>
<pre><code>class PhrasingIterable():
    def __init__(self, my_phraser, texts):
        self.my_phraser = my_phraser 
        self.texts = texts
    def __iter__(self):
        return iter(self.my_phraser[self.texts])

docs = DocumentIterator()
bigram_transformer = Phrases(docs, min_count=1, threshold=10)
bigram = Phraser(bigram_transformer)
corpus = PhrasingIterable(bigram, docs)
sentences = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]    
model = Doc2Vec(window=5,
                        vector_size=300,
                        min_count=10,
                        workers = multiprocessing.cpu_count(),
                        epochs = 10, 
                        compute_loss=True)
model.build_vocab(sentences)
model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>
<p>However, in a second stage, the focus of analysis shifted from requests to individuals per week. To measure how individuals requests differ from week to week I extracted all words from requests in a given week t and compared with all words from requests in the previous week t-1 using <code>d2v_model.wv.n_similarity</code>. Since I need to replicate this in other areas, occurred to me that I was wasting to much memory and time training <code>Doc2Vec</code> models when I could use <code>Word2Vec</code> to get the same measure. Thus, I trained the following <code>Word2Vec</code> model:</p>
<pre><code>docs = DocumentIterator()
bigram_transformer = gensim.models.Phrases(docs, min_count=1, threshold=10)
bigram = gensim.models.phrases.Phraser(bigram_transformer)
sentences = PhrasingIterable(bigram, docs)
model = Word2Vec(window=5,
                        size=300,
                        min_count=10,
                        workers = multiprocessing.cpu_count(),
                        iter = 10, 
                        compute_loss=True)
model.build_vocab(sentences)
model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>
<p>I used again the cosine similarity to compare the content from week to week <code>w2v_model.wv.n_similarity</code>. As a sanity check, I compared the similarities generated by <code>Word2Vec</code> and <code>Doc2vec</code>, the correlation coefficient among is around 0.70 and the scale differs a lot. My implied assumption is that comparing sets of extracted words using <code>d2v_model.wv.n_similarity</code> was taking advantage of the <code>Word2Vec</code> within the trained <code>Doc2Vec</code>.</p>
<p><strong>MY QUESTION</strong></p>
<p>Should cosine similarity measures between two sets of extracted words differ as we trade from <code>Doc2Vec</code> to <code>Word2Vec</code>? If so, why? I not, any suggestions on my code?</p>
","11086598","","11086598","","2020-10-12 10:24:30","2020-10-12 10:24:30","Different cosine similarity coefficients from Doc2Vec and Word2Vec","<python><gensim><word2vec><cosine-similarity><doc2vec>","0","13","","","","CC BY-SA 4.0"
"44849368","1","","","2017-06-30 14:56:28","","0","123","<p>I have multiple text files and I am trying to find a way to identify similar bodies of text. The files themselves consist of an ""average"" sized paragraph. On top of this I also have some data that could be used as lables for the data if I were to go down the root of a neural networks such as a saimese network.</p>

<p>While that was one option another possibility I was wondering about was using something such as doc2vec in order to process all of the paragraphs (with the removal of stopwords and such) and then attempting to find similar files of text based upon the cosine from doc2vec. </p>

<p>I was wondering how do the methods outlined above generally compare to each other in terms of results they produce and is doc2vec robust and accurate enough to consider it a viable option? Also I may be overlooking a good method for this.</p>
","8179088","","","","","2017-07-10 23:59:22","Training a network to find similar bodies of text","<nlp><nltk><gensim><spacy><doc2vec>","1","1","","","","CC BY-SA 3.0"
"55103288","1","55105555","","2019-03-11 13:47:51","","0","26","<p>How I can obtain specific doc vector values? By tag, like this:</p>

<pre><code>modelValues = model.docvecs['myDocTag']
</code></pre>

<p>or it is possible only by index, like this:</p>

<pre><code>modelValues = model.docvecs[12]
</code></pre>

<p>(in last case, I must know matching <code>tag</code>‚Üí<code>index</code>...)</p>
","4399478","","","","","2019-03-11 15:44:16","What is correct way to get doc vectors values?","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"46656290","1","","","2017-10-09 23:06:46","","0","1580","<p>I have a set of words (n-grams) where I need to calculate tf-idf values. These words are;</p>

<pre><code>myvocabulary = ['tim tam', 'jam', 'fresh milk', 'chocolates', 'biscuit pudding']
</code></pre>

<p>My corpus looks as follows.</p>

<pre><code>corpus = {1: ""making chocolates biscuit pudding easy first get your favourite biscuit chocolates"", 2: ""tim tam drink new recipe that yummy and tasty more thicker than typical milkshake that uses normal chocolates"", 3: ""making chocolates drink different way using fresh milk egg""}
</code></pre>

<p>I am currently getting tf-idf values for my n-grams in <code>myvocabulary</code> using sklearn as follows.</p>

<pre><code>tfidf = TfidfVectorizer(vocabulary = myvocabulary, ngram_range = (1,3))
tfs = tfidf.fit_transform(corpus.values())
</code></pre>

<p>However, I am interested in doing the same in Gensim. Forall the examples I came across in Gensim;</p>

<ol>
<li>uses only unigrams ( iwant it for bigrams and trigrams as well)</li>
<li>calculated for all the words (I only want to calculate for the words in <code>myvocabulary</code>)</li>
</ol>

<p>Hence, please help me to find out how to do the above two things in Gensim.</p>
","","user8566323","","","","2017-10-11 04:48:41","Calculate tf-idf in Gensim for my vocabulary","<python><gensim><tf-idf>","1","0","","","","CC BY-SA 3.0"
"46684810","1","46783315","","2017-10-11 09:37:55","","6","10101","<p><strong>Does gensim.corpora.Dictionary have term frequency saved?</strong> </p>

<p>From <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""noreferrer""><code>gensim.corpora.Dictionary</code></a>, it's possible to get the document frequency of the words (i.e. how many document did a particular word occur in):</p>

<pre><code>from nltk.corpus import brown
from gensim.corpora import Dictionary

documents = brown.sents()
brown_dict = Dictionary(documents)

# The 100th word in the dictionary: 'these'
print('The word ""' + brown_dict[100] + '"" appears in', brown_dict.dfs[100],'documents')
</code></pre>

<p>[out]:</p>

<pre><code>The word ""these"" appears in 1213 documents
</code></pre>

<p>And there is the <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_n_most_frequent"" rel=""noreferrer""><code>filter_n_most_frequent(remove_n)</code></a> function that can remove the n-th most frequent tokens: </p>

<blockquote>
  <p><code>filter_n_most_frequent(remove_n)</code>
  Filter out the ‚Äòremove_n‚Äô most frequent tokens that appear in the documents.</p>
  
  <p>After the pruning, shrink resulting gaps in word ids.</p>
  
  <p>Note: Due to the gap shrinking, the same word may have a different word id before and after the call to this function!</p>
</blockquote>

<p><strong>Is the <code>filter_n_most_frequent</code> function removing the n-th most frequent based on the document frequency or term frequency?</strong> </p>

<p>If it's the latter, <strong>is there some way to access the term frequency of the words in the <code>gensim.corpora.Dictionary</code> object?</strong></p>
","610569","","","","","2021-04-27 16:16:19","Does gensim.corpora.Dictionary have term frequency saved?","<python><dictionary><frequency><gensim><tf-idf>","6","0","","","","CC BY-SA 3.0"
"46609507","1","","","2017-10-06 15:50:21","","2","771","<p>I am trying to build an embedding for a corpus using Python's gensim's word2vec implementation. The catch is that I wish to have in the same embedding all of the unigrams and bigrams for the corpus.
Is there a way to embed in the same space both unigrams and bigrams?</p>
","1207545","","","","","2018-10-18 05:58:39","Mixed Unigram Bigram word2vec embedding","<python><text-mining><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"63831856","1","","","2020-09-10 14:27:47","","1","478","<p>I am trying to fine tunning for my problem a  FastText pretrained model using gensim wrapper but I am having problems.
I load the model embeddings successufully from the .bin file like this:</p>
<pre><code>from gensim.models.fasttext import FastText
model=FastText.load_fasttext_format(r_bin)
</code></pre>
<p>Nevertheless, I am struggling when I want to retrain the model using this 3 lines of code:</p>
<pre><code>sent = [['i', 'am ', 'interested', 'on', 'SPGB'], ['SPGB' 'is', 'a', 'good', 'choice']]
model.build_vocab(sent, update=True)
model.train(sentences=sent, total_examples = len(sent), epochs=5)
</code></pre>
<p>I get this error over and over no matter what do I change:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-91-6456730b1919&gt; in &lt;module&gt;
      1 sent = [['i', 'am', 'interested', 'on', 'SPGB'], ['SPGB' 'is', 'a', 'good', 'choice']]
----&gt; 2 model.build_vocab(sent, update=True)
      3 model.train(sentences=sent, total_examples = len(sent), epochs=5)

/opt/.../fasttext.py in build_vocab(self, sentences, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    380         return super(FastText, self).build_vocab(
    381             sentences, update=update, progress_per=progress_per,
--&gt; 382             keep_raw_vocab=keep_raw_vocab, trim_rule=trim_rule, **kwargs)
    383 
    384     def _set_train_params(self, **kwargs):

/opt/.../base_any2vec.py in build_vocab(self, sentences, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    484             trim_rule=trim_rule, **kwargs)
    485         report_values['memory'] = self.estimate_memory(vocab_size=report_values['num_retained_words'])
--&gt; 486         self.trainables.prepare_weights(self.hs, self.negative, self.wv, update=update, vocabulary=self.vocabulary)
    487 
    488     def build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False):

/opt/.../fasttext.py in prepare_weights(self, hs, negative, wv, update, vocabulary)
    752 
    753     def prepare_weights(self, hs, negative, wv, update=False, vocabulary=None):
--&gt; 754         super(FastTextTrainables, self).prepare_weights(hs, negative, wv, update=update, vocabulary=vocabulary)
    755         self.init_ngrams_weights(wv, update=update, vocabulary=vocabulary)
    756 

/opt/.../word2vec.py in prepare_weights(self, hs, negative, wv, update, vocabulary)
   1402             self.reset_weights(hs, negative, wv)
   1403         else:
-&gt; 1404             self.update_weights(hs, negative, wv)
   1405 
   1406     def seeded_vector(self, seed_string, vector_size):

/opt/.../word2vec.py in update_weights(self, hs, negative, wv)
   1452             self.syn1 = vstack([self.syn1, zeros((gained_vocab, self.layer1_size), dtype=REAL)])
   1453         if negative:
-&gt; 1454             self.syn1neg = vstack([self.syn1neg, zeros((gained_vocab, self.layer1_size), dtype=REAL)])
   1455         wv.vectors_norm = None
   1456 

AttributeError: 'FastTextTrainables' object has no attribute 'syn1neg'
</code></pre>
<p>Thanks for your help in advance</p>
","7035921","","7035921","","2020-09-15 06:51:04","2020-09-15 06:51:04","Problem retraining a FastText model from .bin file from Fasttext using Gensim. 'FastTextTrainables' object has no attribute 'syn1neg'","<python><nlp><gensim><pre-trained-model><fasttext>","1","2","","","","CC BY-SA 4.0"
"44787568","1","","","2017-06-27 18:37:50","","2","735","<p>I am trying to determine the optimum number of topics for my LDA model using log perplexity in python. That is, I am graphing the log perplexity for a range of topics and determining the minimum perplexity. However, the graph I have obtained has negative values for log perplexity, when it should have positive values between 0 and 1.</p>

<pre><code>#calculating the log perplexity per word as obtained by gensim code 
##https://radimrehurek.com/gensim/models/atmodel.html
#parameters: pass in trained corpus
#return: graph of perplexity per word for varying number of topics
parameter_list = range(1, 500, 100)
grid ={}

for parameter_value in parameter_list:
model = models.LdaMulticore(corpus=corpus, workers=None, id2word=None, 
                            num_topics=parameter_value, iterations=10)
grid[parameter_value]=[]

perplex=model.log_perplexity(corpus, total_docs=len(corpus))
grid[parameter_value].append(perplex)


df = pd.DataFrame(grid)
ax = plt.figure(figsize=(7, 4), dpi=300).add_subplot(111)
df.iloc[0].transpose().plot(ax=ax,  color=""#254F09"")
plt.xlim(parameter_list[0], parameter_list[-1])
plt.ylabel('Perplexity')
plt.xlabel('topics')
plt.show()     
</code></pre>
","8222177","","","","","2021-04-07 05:42:18","Determining log_perplexity using ldamulticore for optimum number of topics","<python-2.7><gensim><lda><topic-modeling><perplexity>","0","0","","","","CC BY-SA 3.0"
"46697503","1","","","2017-10-11 21:02:23","","1","468","<p>I have found successful weighting theme for adding word vectors which seems to work for sentence comparison in my case:</p>

<pre><code>query1 = vectorize_query(""human cat interaction"")
query2 = vectorize_query(""people and cats talk"")
query3 = vectorize_query(""monks predicted frost"")
query4 = vectorize_query(""man found his feline in the woods"")

&gt;&gt;&gt; print(1 - spatial.distance.cosine(query1, query2))
&gt;&gt;&gt; 0.7154500319

&gt;&gt;&gt; print(1 - spatial.distance.cosine(query1, query3))
&gt;&gt;&gt; 0.415183904078  

&gt;&gt;&gt; print(1 - spatial.distance.cosine(query1, query4))
&gt;&gt;&gt; 0.690741014142 
</code></pre>

<p>When I add additional information to the sentence which acts as noise I get decrease:</p>

<pre><code>&gt;&gt;&gt; query4 = vectorize_query(""man found his feline in the dark woods while picking white mushrooms and watching unicorns"")
&gt;&gt;&gt; print(1 - spatial.distance.cosine(query1, query4))
&gt;&gt;&gt; 0.618269123349
</code></pre>

<p>Are there any ways to deal with additional information when comparing using word vectors? When I know that some subset of the text can provide better match.</p>

<hr>

<p>UPD: edited the code above to make it more clear.</p>

<p><code>vectorize_query</code> in my case does so called smooth inverse frequency weighting, when word vectors from GloVe model (that can be word2vec as well, etc.) are added with weights <code>a/(a+w)</code>, where w should be the word frequency. I use there word's inverse tfidf score, i.e. <code>w = 1/tfidf(word)</code>. Coefficient <code>a</code> is typically taken 1e-3 in this approach. Taking just tfidf score as weight instead of that fraction gives almost similar result, I also played with normalization, etc. </p>

<p>But I wanted to have just ""vectorize sentence"" in my example to not overload the question as I think it does not depend on how I add word vectors using weighting theme - the problem is only that comparison works best when sentences have approximately the same number of meaning words.</p>

<p>I am aware of another approach when distance between sentence and text is being computed using the sum or mean of minimal pairwise word distances, e.g.
""Obama speaks to the media in Illinois"" &lt;-> ""The President greets the press in Chicago"" where we have <code>dist = d(Obama, president) + d(speaks, greets) + d(media, press) + d(Chicago, Illinois).</code> But this approach does not take into account that adjective can change the meaning of noun significantly, etc - which is more or less incorporated in vector models. Words like adjectives  'good', 'bad', 'nice', etc. become noise there, as they match in two texts and contribute as zero or low distances, thus decreasing the distance between sentence and text. </p>

<p>I played a bit with doc2vec models, it seems it was gensim doc2vec implementation and <code>skip-thoughts</code> embedding, but in my case (matching short query with much bigger amount of text) I had unsatisfactory results.</p>
","1692060","","1692060","","2017-10-13 13:06:51","2017-10-13 20:06:35","Is it possible to search for part the of text using word embeddings?","<gensim><word2vec><word-embedding><doc2vec>","2","7","","","","CC BY-SA 3.0"
"53628382","1","53661890","","2018-12-05 08:51:11","","0","197","<p>Is there a way to find similar docs like we do in word2vec</p>

<p>Like:</p>

<pre><code>  model2.most_similar(positive=['good','nice','best'],
    negative=['bad','poor'],
    topn=10)
</code></pre>

<p>I know we can use infer_vector,feed them to have similar ones, but I want to feed many positive and negative examples as we do in word2vec.</p>

<p>is there any way we can do that! thanks !</p>
","8636829","","","","","2018-12-07 01:07:31","Find similarity with doc2vec like word2vec","<python><nlp><gensim><word2vec><doc2vec>","2","0","","","","CC BY-SA 4.0"
"44964380","1","44969553","","2017-07-07 06:56:08","","0","854","<p>I have a corpus built out of Wikimedia Dump files stored at <strong><em>sentences.txt</em></strong>
I have a sentence say '‡§®‡•Ä‡§∞‡§ú‡§É ‡§π‡§æ‡§Å ‡§Æ‡§æ‡§§‡§æ ‡§ú‡•Ä! ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ñ‡§º‡§§‡•ç‡§Æ ‡§π‡•ã‡§§‡•á ‡§∏‡•Ä‡§ß‡§æ ‡§ò‡§∞ ‡§Ü‡§ä‡§Å‡§ó‡§æ'</p>

<p>Now when I try to extract the word vectors there is always one or two words which have been missed out while training (despite being included in the list to be trained upon) and I get the KeyError.
Is there any way to improve the training so that it doesn't miss out words that frequently? </p>

<p>Here is a proof that it does happen. <code>tok.wordtokenize</code> is a word tokenizer. <code>sent.drawlist()</code> as well as <code>sents.drawlist()</code> returns a list of sentences from the corpus stored inside <strong><em>sentences.txt</em></strong>. </p>

<hr>

<pre><code>&gt;&gt;&gt; sentence = '‡§®‡•Ä‡§∞‡§ú‡§É ‡§π‡§æ‡§Å ‡§Æ‡§æ‡§§‡§æ ‡§ú‡•Ä! ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ñ‡§º‡§§‡•ç‡§Æ ‡§π‡•ã‡§§‡•á ‡§∏‡•Ä‡§ß‡§æ ‡§ò‡§∞ ‡§Ü‡§ä‡§Å‡§ó‡§æ'
&gt;&gt;&gt; sentence = tok.wordtokenize(sentence) #tok.wordtokenize() is simply a word tokenizer.
&gt;&gt;&gt; sentences = sent.drawlist()
&gt;&gt;&gt; sentences = [tok.wordtokenize(i) for i in sentences]
&gt;&gt;&gt; sentences2 = sents.drawlist()
&gt;&gt;&gt; sentences2 = [tok.wordtokenize(i) for i in sentences2]
&gt;&gt;&gt; sentences = sentences2 + sentences + sentence
&gt;&gt;&gt; ""‡§®‡•Ä‡§∞‡§ú‡§É"" in sentences #proof that the word is present inside sentences
True
&gt;&gt;&gt; sentences[0:10] #list of tokenized sentences.
[['‡§µ‡§ø‡§∂‡•ç‡§µ', '‡§≠‡§∞', '‡§Æ‡•á‡§Ç', '‡§ï‡§∞‡•ã‡•ú‡•ã‡§Ç', '‡§ü‡•Ä‡§µ‡•Ä', '‡§¶‡§∞‡•ç‡§∂‡§ï‡•ã‡§Ç', '‡§ï‡•Ä', '‡§â‡§§‡•ç‡§∏‡•Å‡§ï‡§§‡§æ', '‡§≠‡§∞‡•Ä', '‡§®‡§ø‡§ó‡§æ‡§π', '‡§ï‡•á', '‡§¨‡•Ä‡§ö', '‡§Æ‡§ø‡§∏', '‡§ë‡§∏‡•ç‡§ü‡•ç‡§∞‡•á‡§≤‡§ø‡§Ø‡§æ', '‡§ú‡•á‡§®‡§ø‡§´‡§∞', '‡§π‡•â‡§ï‡§ø‡§Ç‡§∏', '‡§ï‡•ã', '‡§Æ‡§ø‡§∏', '‡§Ø‡•Ç‡§®‡§ø‡§µ‡§∞‡•ç‡§∏-‡•®‡•¶‡•¶‡•™', '‡§ï‡§æ', '‡§§‡§æ‡§ú', '‡§™‡§π‡§®‡§æ‡§Ø‡§æ', '‡§ó‡§Ø‡§æ'], ['‡§ï‡§∞‡•Ä‡§¨', '‡§¶‡•ã', '‡§ò‡§Ç‡§ü‡•á', '‡§ö‡§≤‡•á', '‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ', '‡§Æ‡•á‡§Ç', '‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§®', '‡§¶‡•á‡§∂‡•ã‡§Ç', '‡§ï‡•Ä', '‡•Æ‡•¶', '‡§∏‡•Å‡§Ç‡§¶‡§∞‡§ø‡§Ø‡•ã‡§Ç', '‡§ï‡•á', '‡§¨‡•Ä‡§ö', '‡•®‡•¶', '‡§µ‡§∞‡•ç‡§∑‡•Ä‡§Ø', '‡§π‡•â‡§ï‡§ø‡§Ç‡§∏', '‡§ï‡•ã', '‡§∏‡§∞‡•ç‡§µ‡§∂‡•ç‡§∞‡•á‡§∑‡•ç‡§†', '‡§Ü‡§Ç‡§ï‡§æ', '‡§ó‡§Ø‡§æ'], ['‡§Æ‡§ø‡§∏', '‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡§æ', '‡§∂‡•à‡§Ç‡§°‡•Ä', '‡§´‡§ø‡§®‡•á‡§ú‡•Ä', '‡§ï‡•ã', '‡§™‡•ç‡§∞‡§•‡§Æ', '‡§â‡§™', '‡§µ‡§ø‡§ú‡•á‡§§‡§æ', '‡§î‡§∞', '‡§Æ‡§ø‡§∏', '‡§™‡•ç‡§Ø‡•Ç‡§∞‡•á‡§ü‡•ã', '‡§∞‡§ø‡§ï‡•ã', '‡§Ö‡§≤‡•ç‡§¨‡§æ', '‡§∞‡•á‡§á‡§ú', '‡§¶‡•ç‡§µ‡§ø‡§§‡•Ä‡§Ø', '‡§â‡§™', '‡§µ‡§ø‡§ú‡•á‡§§‡§æ', '‡§ö‡•Å‡§®‡•Ä', '‡§ó‡§à'], ['‡§≠‡§æ‡§∞‡§§', '‡§ï‡•Ä', '‡§§‡§®‡•Å‡§∂‡•ç‡§∞‡•Ä', '‡§¶‡§§‡•ç‡§§‡§æ', '‡§Ö‡§Ç‡§§‡§ø‡§Æ', '‡•ß‡•¶', '‡§™‡•ç‡§∞‡§§‡§ø‡§≠‡§æ‡§ó‡§ø‡§Ø‡•ã‡§Ç', '‡§Æ‡•á‡§Ç', '‡§π‡•Ä', '‡§∏‡•ç‡§•‡§æ‡§®', '‡§¨‡§®‡§æ', '‡§™‡§æ‡§à'], ['‡§π‡•â‡§ï‡§ø‡§Ç‡§∏', '‡§®‡•á', '‡§ï‡§π‡§æ', '‡§ï‡§ø', '‡§ú‡•Ä‡§§', '‡§ï‡•á', '‡§¨‡§æ‡§∞‡•á', '‡§Æ‡•á‡§Ç', '‡§â‡§∏‡§®‡•á', '‡§∏‡§™‡§®‡•á', '‡§Æ‡•á‡§Ç', '‡§≠‡•Ä', '‡§®‡§π‡•Ä‡§Ç', '‡§∏‡•ã‡§ö‡§æ', '‡§•‡§æ'], ['‡§∏‡•å‡§Ç‡§¶‡§∞‡•ç‡§Ø', '‡§ï‡•Ä', '‡§Ø‡§π', '‡§∂‡•Ä‡§∞‡•ç‡§∑', '‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ', '‡§ï‡•ç‡§µ‡§ø‡§ü‡•ã', '‡§ï‡•á', '‡§ï‡§®‡•ç‡§µ‡•á‡§Ç‡§∂‡§®', '‡§∏‡•á‡§Ç‡§ü‡§∞', '‡§Æ‡•á‡§Ç', '‡§Æ‡§Ç‡§ó‡§≤‡§µ‡§æ‡§∞', '‡§¶‡•á‡§∞', '‡§∞‡§æ‡§§', '‡§∂‡•Å‡§∞‡•Ç', '‡§π‡•Å‡§à'], ['‡§ï‡§∞‡•Ä‡§¨', '‡•≠‡•´‡•¶‡•¶', '‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü', '‡§¶‡§∞‡•ç‡§∂‡§ï‡•ã‡§Ç', '‡§ï‡•Ä', '‡§Æ‡•å‡§ú‡•Ç‡§¶‡§ó‡•Ä', '‡§Æ‡•á‡§Ç', '‡§µ‡§ø‡§∂‡•ç‡§µ', '‡§ï‡•Ä', '‡§∏‡§∞‡•ç‡§µ‡§∂‡•ç‡§∞‡•á‡§∑‡•ç‡§†', '‡§∏‡•Å‡§Ç‡§¶‡§∞‡•Ä', '‡§ï‡•á', '‡§ö‡§Ø‡§®', '‡§ï‡•Ä', '‡§ï‡§µ‡§æ‡§Ø‡§¶', '‡§∂‡•Å‡§∞‡•Ç', '‡§π‡•Å‡§à'], ['‡§π‡§∞', '‡§ö‡§∞‡§£', '‡§ï‡•á', '‡§¨‡§æ‡§¶', '‡§≤‡•ã‡§ó‡•ã‡§Ç', '‡§ï‡•Ä', '‡§∏‡§æ‡§Ç‡§∏‡•á', '‡§•‡§Æ‡§®‡•á', '‡§≤‡§ó‡§§‡•Ä‡§Ç'], ['‡§ü‡•Ä‡§µ‡•Ä', '‡§™‡§∞', '‡§≤‡•Å‡§§‡•ç‡§´', '‡§â‡§†‡§æ', '‡§∞‡§π‡•á', '‡§¶‡§∞‡•ç‡§∂‡§ï', '‡§Ö‡§™‡§®‡•á', '‡§¶‡•á‡§∂', '‡§µ', '‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞', '‡§ï‡•Ä', '‡§∏‡•Å‡§Ç‡§¶‡§∞‡•Ä', '‡§ï‡•Ä', '‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ', '‡§Æ‡•á‡§Ç', '‡§∏‡•ç‡§•‡§ø‡§§‡§ø', '‡§ï‡•á', '‡§¨‡§æ‡§∞‡•á', '‡§Æ‡•á‡§Ç', '‡§µ‡•ç‡§Ø‡§ó‡•ç‡§∞', '‡§∞‡§π‡•á'], ['‡§´‡§æ‡§á‡§®‡§≤', '‡§Æ‡•á‡§Ç', '‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•á', '‡§µ‡§æ‡§≤‡•Ä', '‡§™‡§æ‡§Ç‡§ö', '‡§™‡•ç‡§∞‡§§‡§ø‡§≠‡§æ‡§ó‡§ø‡§Ø‡•ã‡§Ç', '‡§Æ‡•á‡§Ç', '‡§Æ‡§ø‡§∏', '‡§™‡•á‡§∞‡§æ‡§ó‡•ç‡§µ‡•á', '‡§Ø‡§æ‡§®‡§ø‡§®‡§æ', '‡§ó‡•ã‡§Ç‡§ú‡§æ‡§≤‡•á‡§ú', '‡§î‡§∞', '‡§Æ‡§ø‡§∏', '‡§§‡•ç‡§∞‡§ø‡§®‡§ø‡§¶‡§æ‡§¶', '‡§µ', '‡§ü‡•ã‡§¨‡•à‡§ó‡•ã', '‡§°‡•á‡§®‡§ø‡§Ø‡§≤', '‡§ú‡•ã‡§Ç‡§∏', '‡§≠‡•Ä', '‡§∂‡§æ‡§Æ‡§ø‡§≤', '‡§•‡•Ä‡§Ç']]
&gt;&gt;&gt; model = gensim.models.Word2Vec(sentences, size =10,  min_count=1) 
&gt;&gt;&gt; pred = []
&gt;&gt;&gt; for word in sentence:
...         pred.append(model.wv[word].tolist())
... 
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 2, in &lt;module&gt;
  File ""/home/djokester/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 574, in __getitem__
    return self.word_vec(words)
  File ""/home/djokester/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 273, in word_vec
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word '‡§®‡•Ä‡§∞‡§ú‡§É' not in vocabulary""
</code></pre>

<hr>

<p>As you can see, I check for the word ""‡§®‡•Ä‡§∞‡§ú‡§É"" inside the list of tokenized sentences. It is present in the list that I feed into the Word2Vec trainer and yet after training it is not in the vocabulary. </p>
","6503743","","6503743","","2017-07-08 22:14:19","2017-07-10 19:51:19","Gensim: Loss of Words/Tokens while Training","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"54085239","1","","","2019-01-08 04:31:48","","1","714","<p>I am trying to convert a KeyedVector word2vec object to a tsv file. Here is my code:</p>

<pre><code>wv_embeddings = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=100000)
</code></pre>

<p>Would you loop through each of the embeddings and save them to a tsv file?</p>
","10880194","","","","","2019-01-28 23:48:28","Converting KeyedVector to a tsv file","<python-3.x><gensim>","1","0","","","","CC BY-SA 4.0"
"46605194","1","","","2017-10-06 11:58:12","","2","5212","<p>i have implemented LDA in python.now i want to label the topics whichever i got from LDA.</p>

<pre><code>[(0, u'0.023*""alternate"" + 0.023*""transfervisions"" + 0.013*""tvcommunity""'), (1, u'0.026*""minimalism"" + 0.026*""minimalist"" + 0.018*""honking""'), (2, u'0.027*""videomaking"" + 0.019*""python"" + 0.019*""httpstcoc2ythrctki""')]
</code></pre>
","8449495","","","","","2018-05-16 00:59:41","how to label topics automatically after applying LDA","<python><nltk><gensim><lda><topic-modeling>","2","1","","","","CC BY-SA 3.0"
"46647945","1","46652450","","2017-10-09 13:41:42","","4","2387","<p>I have a Word2Vec model with a lot of word vectors. I can access a word vector as so. </p>

<pre><code>word_vectors = gensim.models.Word2Vec.load(wordspace_path)
print(word_vectors['boy'])
</code></pre>

<p><strong>Output</strong></p>

<pre><code>[ -5.48055351e-01   1.08748421e-01  -3.50534245e-02  -9.02988110e-03...]
</code></pre>

<p>Now I have a proper vector representation that I want to replace the word_vectors['boy'] with.</p>

<pre><code>word_vectors['boy'] = [ -7.48055351e-01   3.08748421e-01  -2.50534245e-02  -10.02988110e-03...]
</code></pre>

<p>But the following error is thrown</p>

<pre><code>TypeError: 'Word2Vec' object does not support item assignment
</code></pre>

<p>Is there any fashion or workaround to do this? That is manipulate word vectors manually once the model is trained? Is it possible in other platforms except Gensim? </p>
","5069280","","","","","2017-10-09 23:49:48","How to manually change the vector dimensions of a word in Gensim Word2Vec","<python><vector><gensim><word2vec><vector-space>","1","0","2","","","CC BY-SA 3.0"
"20984841","1","20991190","","2014-01-08 00:30:08","","29","24713","<p>I am able to run the LDA code from gensim and got the top 10 topics with their respective keywords.</p>

<p>Now I would like to go a step further to see how accurate the LDA algo is by seeing which document they cluster into each topic. Is this possible in gensim LDA?</p>

<p>Basically i would like to do something like this, but in python and using gensim.</p>

<p><a href=""https://stackoverflow.com/questions/14875493/lda-with-topicmodels-how-can-i-see-which-topics-different-documents-belong-to"">LDA with topicmodels, how can I see which topics different documents belong to?</a></p>
","2800939","","-1","","2017-05-23 12:34:08","2020-08-28 21:15:20","Topic distribution: How do we see which document belong to which topic after doing LDA in python","<python><nltk><lda><gensim>","3","1","22","","","CC BY-SA 3.0"
"47028943","1","","","2017-10-31 06:12:27","","1","6232","<p>I am running the below code, but gensim word2vec is throwing a word not in vocabulary error. Can you let me know the solution?</p>

<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

sentences = [[""The quick brown fox jumped over the lazy dog""], 
         [""The sun is shining bright""]]

from gensim.models import word2vec
model = word2vec.Word2Vec(sentences, iter=10, min_count=1, size=300, workers=4)

print(model['quick'])
</code></pre>

<p>Output:</p>

<pre><code>KeyError: ""word 'quick' not in vocabulary""
</code></pre>

<p>But If I use this</p>

<pre><code>print(model['The quick brown fox jumped over the lazy dog'])
</code></pre>

<p>it prints a list</p>

<pre><code>[  1.60348183e-03  -9.17983416e-04  -8.30831763e-04   9.46367683e-04
</code></pre>
","8505671","","8505671","","2017-10-31 06:23:23","2017-10-31 06:46:53","gensim: KeyError: ""word 'quick' not in vocabulary""","<python-3.x><word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"62211396","1","62219873","","2020-06-05 08:42:07","","0","236","<p>Refer to below image (the process of how word2vec skipgram extract training datasets-the word pair from the input sentences). </p>

<p>E.G. ""I love you."" ==> [(I,love), (I, you)]</p>

<p>May I ask what is the word pair when the sentence contains only one word? </p>

<p>Is it  ""Happy!"" ==> [(happy,happy)] ?</p>

<p>I tested the word2vec algorithm in genism, when there is just one word in the training set sentences, (and this word is not included in other sentences), the word2vec algorithm can still construct an embedding vector for this specific word. I am not sure how the algorithm is able to do so.</p>

<p><a href=""https://i.stack.imgur.com/zQPX6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zQPX6.png"" alt=""enter image description here""></a></p>

<p>===============UPDATE===============================</p>

<p>As the answer posted below, I think the word embedding vector created for the word in the 1-word-sentence is just the random initialization of neural network weights.</p>
","9827774","","9827774","","2020-06-14 12:52:43","2020-06-14 12:52:43","How does gensim word2vec word embedding extract training word pair for 1 word sentence?","<nlp><text-mining><gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"62348981","1","62349440","","2020-06-12 16:58:33","","0","69","<p>The code works absolutely fine for the data set containing 500000+ instances but whenever I reduce the data set to 5000/10000/15000 it throws a key error : word ""***"" not in vocabulary.Not for every data point but for most them it throws the error.The data set is in excel format.  [1]: <a href=""https://i.stack.imgur.com/YCBiQ.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/YCBiQ.png</a>
I don't know how to fix this problem since i have very little knowledge about it,,I am still learning.Please help me fix this problem! </p>

<pre><code>    purchases_train = []
    for i in tqdm(customers_train):
        temp = train_df[train_df[""CustomerID""] == i][""StockCode""].tolist()
        purchases_train.append(temp)

    purchases_val = []
    for i in tqdm(validation_df['CustomerID'].unique()):
        temp = validation_df[validation_df[""CustomerID""] == i][""StockCode""].tolist()
        purchases_val.append(temp)


    model = Word2Vec(window = 10, sg = 1, hs = 0,
                     negative = 10, # for negative sampling
                     alpha=0.03, min_alpha=0.0007,
                     seed = 14)

    model.build_vocab(purchases_train, progress_per=200)

    model.train(purchases_train, total_examples = model.corpus_count, 
                epochs=10, report_delay=1)


    model.save(""word2vec_2.model"")
    model.init_sims(replace=True)

    # extract all vectors
    X = model[model.wv.vocab]

    X.shape

    products = train_df[[""StockCode"", ""Description""]]

    products.drop_duplicates(inplace=True, subset='StockCode', keep=""last"")


 products_dict=products.groupby('StockCode'['Description'].apply(list).to_dict()

    def similar_products(v, n = 6):
        ms = model.similar_by_vector(v, topn= n+1)[1:]
        new_ms = []
        for j in ms:
            pair = (products_dict[j[0]][0], j[1])
            new_ms.append(pair)

        return new_ms

        similar_products(model['21883'])
</code></pre>
","11971562","","11971562","","2020-06-12 17:12:29","2020-06-12 17:26:42","word2vec recommendation system KeyError: ""word '21883' not in vocabulary""","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"61134211","1","","","2020-04-10 04:49:18","","0","281","<p>I trained and test the 'IMDb movie reviews dataset' using the Gensim word2vec model and I want to predict the sentiments of my own unlabelled data. I tried but got an error. I am reusing an open-source code. Below is the full code:</p>

<pre><code>import pandas as pd
import numpy as np
import text_normalizer as tn
import model_evaluation_utils as meu
np.set_printoptions(precision=2, linewidth=80)
import gensim
import keras
from keras.models import Sequential
from keras.layers import Dropout, Activation, Dense
from sklearn.preprocessing import LabelEncoder

dataset = pd.read_csv(r'imdb_reviews.csv')
new_data = pd.read_csv('abcd.csv', header=0)
# take a peek at the data
print(dataset.head())
reviews = np.array(dataset['reviews'])
sentiments = np.array(dataset['Sentiments'])

# build train and test datasets
train_reviews = reviews[:35000]
train_sentiments = sentiments[:35000]
test_reviews = reviews[35000:]
test_sentiments = sentiments[35000:]

# normalize datasets
norm_train_reviews = tn.normalize_corpus(train_reviews)
norm_test_reviews = tn.normalize_corpus(test_reviews)
le = LabelEncoder()
num_classes=2 
# tokenize train reviews &amp; encode train labels
tokenized_train = [tn.tokenizer.tokenize(text)
                   for text in norm_train_reviews]
y_tr = le.fit_transform(train_sentiments)
y_train = keras.utils.to_categorical(y_tr, num_classes)
# tokenize test reviews &amp; encode test labels
tokenized_test = [tn.tokenizer.tokenize(text)
                   for text in norm_test_reviews]
y_ts = le.fit_transform(test_sentiments)
y_test = keras.utils.to_categorical(y_ts, num_classes)
# print class label encoding map and encoded labels
print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))
print('Sample test label transformation:\n'+'-'*35,
      '\nActual Labels:', test_sentiments[:3], '\nEncoded Labels:', y_ts[:3], 
      '\nOne hot encoded Labels:\n', y_test[:3])
# build word2vec model
w2v_num_features = 500
w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,
                                   min_count=10, sample=1e-3)
def averaged_word2vec_vectorizer(corpus, model, num_features):
    vocabulary = set(model.wv.index2word)

    def average_word_vectors(words, model, vocabulary, num_features):
        feature_vector = np.zeros((num_features,), dtype=""float64"")
        nwords = 0.

        for word in words:
            if word in vocabulary: 
                nwords = nwords + 1.
                feature_vector = np.add(feature_vector, model[word])
        if nwords:
            feature_vector = np.divide(feature_vector, nwords)

        return feature_vector

    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)
                    for tokenized_sentence in corpus]
    return np.array(features)
# generate averaged word vector features from word2vec model
avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,
                                                     num_features=500)
avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,
                                                    num_features=500)
print('Word2Vec model:&gt; Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features
def construct_deepnn_architecture(num_input_features):
    dnn_model = Sequential()
    dnn_model.add(Dense(512, activation='relu', input_shape=(num_input_features,)))
    dnn_model.add(Dropout(0.2))
    dnn_model.add(Dense(512, activation='relu'))
    dnn_model.add(Dropout(0.2))
    dnn_model.add(Dense(512, activation='relu'))
    dnn_model.add(Dropout(0.2))
    dnn_model.add(Dense(2))
    dnn_model.add(Activation('softmax'))

    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',                 
                      metrics=['accuracy'])
    return dnn_model
w2v_dnn = construct_deepnn_architecture(num_input_features=500)
batch_size = 100
w2v_dnn.fit(avg_wv_train_features, y_train, epochs=15, batch_size=batch_size, 
            shuffle=True, validation_split=0.1, verbose=1)
y_pred = w2v_dnn.predict_classes(avg_wv_test_features)
predictions = le.inverse_transform(y_pred)
meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, 
                                      classes=['positive', 'negative'])
# This I added to predict and save the results of my own data
pred_y2 = w2v_dnn.predict_classes(new_data['Articles'])
print(pred_y2)
pd.DataFrame(pred_y2, columns=['Sentiments']).to_csv('abcd_sentiments.csv')
</code></pre>

<p>When I run this code I got the below error:</p>

<blockquote>
  <p>ValueError                                Traceback (most recent call
  last)  in 
  ----> 1 pred_y2 = w2v_dnn.predict_classes(new_data['Articles'])
        2 print(pred_y2)
        3 pd.DataFrame(pred_y2, columns=['Sentiments']).to_csv('abcd_sentiments.csv')</p>
  
  <p>~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/sequential.py
  in predict_classes(self, x, batch_size, verbose)
      266             A numpy array of class predictions.
      267         """"""
  --> 268         proba = self.predict(x, batch_size=batch_size, verbose=verbose)
      269         if proba.shape[-1] > 1:
      270             return proba.argmax(axis=-1)</p>
  
  <p>~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/training.py
  in predict(self, x, batch_size, verbose, steps, callbacks,
  max_queue_size, workers, use_multiprocessing)    1439     1440<br>
   # Case 2: Symbolic tensors or Numpy array-like.
  -> 1441         x, _, _ = self._standardize_user_data(x)    1442         if self.stateful:    1443             if x[0].shape[0] > batch_size
  and x[0].shape[0] % batch_size != 0:</p>
  
  <p>~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/training.py
  in _standardize_user_data(self, x, y, sample_weight, class_weight,
  check_array_lengths, batch_size)
      577             feed_input_shapes,
      578             check_batch_axis=False,  # Don't enforce the batch size.
  --> 579             exception_prefix='input')
      580 
      581         if y is not None:</p>
  
  <p>~/PycharmProjects/News/venv/lib/python3.7/site-packages/keras/engine/training_utils.py
  in standardize_input_data(data, names, shapes, check_batch_axis,
  exception_prefix)
      143                             ': expected ' + names[i] + ' to have shape ' +
      144                             str(shape) + ' but got array with shape ' +
  --> 145                             str(data_shape))
      146     return data
      147 </p>
  
  <p>ValueError: Error when checking input: expected dense_1_input to have
  shape (500,) but got array with shape (1,)</p>
</blockquote>

<p>Can somebody suggest me how to solve this error and predict sentiments of my unlabeled data?
I am using python 3.7 and jupyter notebook from Pycharm IDE.</p>

<p>Thanks in Advance.</p>
","11230191","","11230191","","2020-04-10 09:01:45","2020-04-10 09:01:45","How to predict unlabelled data's sentiment using Gensim word2vec model?","<python-3.7><gensim><word2vec><sentiment-analysis><valueerror>","0","4","","","","CC BY-SA 4.0"
"62620509","1","","","2020-06-28 09:13:55","","1","332","<p>I wrote LDA model in notebook.
I'm trying to wrap my gensim LDA model with mallet, getting the following error:</p>
<p>CalledProcessError: Command '../input/mymallet/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex &quot;\S+&quot; --input /tmp/fbcc4b_corpus.txt --output /tmp/fbcc4b_corpus.mallet' returned non-zero exit status 126.</p>
<p>The error raised because of the second line:</p>
<pre><code>mallet_path = '../input/mymallet/mallet-2.0.8/bin/mallet' # update this path
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)
</code></pre>
<p>The path is correct.</p>
<p>Tried this solution:  <a href=""https://stackoverflow.com/questions/55288724/gensim-mallet-calledprocesserror-returned-non-zero-exit-status"">Gensim mallet CalledProcessError: returned non-zero exit status</a></p>
<p>didn't work for me..</p>
","12640167","","12640167","","2020-06-28 10:24:23","2021-03-30 01:50:48","How to fix mallet on gensim","<gensim><lda><kaggle><mallet>","2","0","","","","CC BY-SA 4.0"
"44439291","1","","","2017-06-08 15:01:24","","4","757","<p>I have a a gensim Word2Vec KeyedVectors model. For speed purposes, I would like to parallelize my program so that it can run in a Spark environment. However, as far as I am aware Spark's RDDs only work on collections and iterables. I don't think I could actually see a performance boost by simply putting the KeyedVectors model into a RDD.</p>

<p>I have explored storing the model as a broadcast variable, but it is far too large. Partitioning (using an RDD) looks like the best option.</p>

<p>If I wanted to boost my program's performance by converting the model to a parallel collection in Spark, how would I go about doing this?</p>
","3180238","","","","","2017-06-08 15:01:24","Parallelizing a gensim KeyedVectors model in Spark","<apache-spark><pyspark><rdd><gensim><word2vec>","0","1","","","","CC BY-SA 3.0"
"62387623","1","","","2020-06-15 11:51:59","","0","1501","<pre><code>df_clean['message'] = df_clean['message'].apply(lambda x: gensim.parsing.preprocessing.remove_stopwords(x))
</code></pre>

<p>I tried this on a dataframe's column 'message' but I get the error: </p>

<pre><code>TypeError: decoding to str: need a bytes-like object, list found
</code></pre>
","10608841","","","","","2020-06-15 12:14:26","How to remove stopwords in gensim?","<python><gensim>","3","0","","","","CC BY-SA 4.0"
"46701173","1","46706139","","2017-10-12 03:59:07","","3","3037","<p>I have created word vectors using a distributed word2vec algorithm. Now I have words and their corresponding vectors. How to build a gensim word2vec model using these words and vectors? </p>
","5059870","","","","","2017-10-12 09:33:25","How to create gensim word2vec model using pre trained word vectors?","<nlp><gensim><word2vec><text-analysis><word-embedding>","1","0","","","","CC BY-SA 3.0"
"62344470","1","","","2020-06-12 12:39:35","","0","115","<p>i am doing synonym-detection based on word2vec models with gensim. What possibilities are for automatic calculate of recall and precision. I just want some metrics based on the trained Model whitout giving a list of correct synoynms. 
Cheers,
Marvin</p>
","13734142","","","","","2020-06-12 12:39:35","Automatic calculate Recall/ Precision to Word2vec model","<python><gensim><word2vec><synonym>","0","3","","","","CC BY-SA 4.0"
"61055072","1","","","2020-04-06 07:42:56","","1","3458","<pre><code>list(gensim.utils.simple_preprocess(""i you he she I it we you they"", deacc=True))
</code></pre>

<p>gives as result:</p>

<pre><code>['you', 'he', 'she', 'it', 'we', 'you', 'they']
</code></pre>

<p>Is it normal? Are there any words that it skips? Should I use another tokenizer?</p>

<p>BONUS QUESTION:
What does the ""deacc=True"" paramater mean?</p>
","11648332","","","","","2020-04-06 17:26:32","Why does gensim's simple_preprocess Python tokenizer seem to skip the ""i"" token?","<python><nlp><tokenize><gensim>","1","2","","","","CC BY-SA 4.0"
"43855348","1","43989840","","2017-05-08 19:01:26","","1","600","<p>I'm getting the following error when training a doc2vec model in a Jupyter notebook on OS X. The error is reproducible (although the specific thread in which it occurs changes) for my current dataset, although I have successfully trained models on other datasets. </p>

<pre><code>Exception in thread Thread-82:
Traceback (most recent call last):
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
self.run()
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 862, in run
self._target(*self._args, **self._kwargs)
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 822, in worker_loop
tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/doc2vec.py"", line 717, in _do_train_job
doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
File ""gensim/models/doc2vec_inner.pyx"", line 428, in gensim.models.doc2vec_inner.train_document_dm (./gensim/models/doc2vec_inner.c:5455)
File ""mtrand.pyx"", line 1266, in mtrand.RandomState.randint (numpy/random/mtrand/mtrand.c:15836)
RuntimeError: release unlocked lock
Exception in thread Thread-77:
Traceback (most recent call last):
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
self.run()
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 862, in run
self._target(*self._args, **self._kwargs)
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 822, in worker_loop
tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/doc2vec.py"", line 717, in _do_train_job
doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
File ""gensim/models/doc2vec_inner.pyx"", line 458, in gensim.models.doc2vec_inner.train_document_dm (./gensim/models/doc2vec_inner.c:5963)
File ""mtrand.pyx"", line 1266, in mtrand.RandomState.randint (numpy/random/mtrand/mtrand.c:15836)
RuntimeError: release unlocked lock
</code></pre>
","6287935","","","","","2017-05-15 22:36:58","RuntimeError: release unlocked lock while training doc2vec","<gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"46970376","1","","","2017-10-27 08:11:23","","0","497","<p>I am using Gensim wrapper to obtain wordRank embeddings (I am following their <a href=""https://github.com/parulsethi/gensim/blob/wordrank_wrapper/docs/notebooks/Wordrank_comparisons.ipynb"" rel=""nofollow noreferrer"">tutorial</a> to do this) as follows.</p>

<pre><code>from gensim.models.wrappers import Wordrank

model = Wordrank.train(wr_path = ""models"", corpus_file=""proc_brown_corp.txt"", 
out_name= ""wr_model"")

model.save(""wordrank"")
model.save_word2vec_format(""wordrank_in_word2vec.vec"")
</code></pre>

<p>However, I am getting the following error <code>FileNotFoundError: [WinError 2] The system cannot find the file specified</code>. I am just wondering what I have made wrong as everything looks correct to me. Please help me.</p>

<p>Moreover, I want to know if the way I am saving the model is correct. I saw that Gensim offers the method <code>save_word2vec_format</code>. What is the advantage of using it without directly using the original wordRank model?</p>
","","user8566323","","user8510273","2017-10-27 08:29:47","2018-09-24 00:07:54","Issues in Gensim WordRank Embeddings","<python><nlp><gensim><word-embedding>","1","1","","","","CC BY-SA 3.0"
"46674609","1","46677437","","2017-10-10 19:37:00","","0","1626","<p>Im trying to understand doc2vec and can I use it to solve my scenario. I want to label sentences with 1 or more tags using TaggedSentences([words], [tags]), but im unsure If my understanding is correct.</p>

<p>so basically, i need this to happen(or am I totally off the mark)</p>

<p>I create 2 TaggedDocuments</p>

<pre><code>TaggedDocument(words=[""the"", ""bird"", ""flew"", ""over"", ""the"", ""coocoos"", ""nest"", labels=[""animal"",""tree""])
TaggedDocument(words=[""this"", ""car"", ""is"", ""over"", ""one"", ""million"", ""dollars"", labels=[""motor"",""money""])
</code></pre>

<p>I build my model</p>

<pre><code>model = gensim.models.Doc2Vec(documents, dm=0, alpha=0.025, size=20, min_alpha=0.025, min_count=0)
</code></pre>

<p>Then I train my model</p>

<pre><code>model.train(documents, total_examples=len(documents), epochs=1)
</code></pre>

<p>So when I have all that done, what I expect is when I execute</p>

<pre><code>model.most_similar(positive=[""bird"", ""flew"", ""over"", ""nest])
</code></pre>

<p>is  [animal,tree], but I get</p>

<pre><code>[('the', 0.4732949137687683), 
('million', 0.34103643894195557),
('dollars', 0.26223617792129517),
('one', 0.16558100283145905),
('this', 0.07230066508054733),
('is', 0.012532509863376617),
('cocos', -0.1093338280916214),
('car', -0.13764989376068115)]
</code></pre>

<p>UPDATE:
when I infer</p>

<pre><code>vec_model = model.Word2Vec.load(os.path.join(""save"",""vec.w2v""))
infer = vec_model.infer_vector([""bird"", ""flew"", ""over"", ""nest""])
print(vec_model.most_similar(positive=[infer], topn=10))
</code></pre>

<p>I get</p>

<pre><code>[('bird', 0.5196993350982666),
('car', 0.3320297598838806), 
('the',  0.1573483943939209), 
('one', 0.1546170711517334), 
('million',  0.05099521577358246),
('over', -0.0021460093557834625), 
('is',  -0.02949431538581848),
('dollars', -0.03168443590402603), 
('flew', -0.08121247589588165),
('nest', -0.30139490962028503)]
</code></pre>

<p>So the elephant in the room, Is doc2vec what I need to accomplish the above scenario, or should I go back to bed and have a proper think about what Im trying to achieve in life :)</p>

<p>Any help greatly appreciated</p>
","4154338","","4154338","","2017-10-11 07:49:36","2017-10-11 07:49:36","Gensim doc2vec sentence tagging","<python><machine-learning><data-science><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"44449132","1","44491003","","2017-06-09 04:05:11","","2","179","<p>I have made doc2vec file by training data using gensim model now while processing it. I am getting an error.
I am running the below code:-</p>

<p>model = Doc2Vec.load('sentiment140.d2v')</p>

<pre><code>if len(sys.argv) &lt; 4:
    print (""Please input train_pos_count, train_neg_count and classifier!"")
    sys.exit()

train_pos_count = int(sys.argv[1])
train_neg_count = int(sys.argv[2])
test_pos_count = 144
test_neg_count = 144

print (train_pos_count)
print (train_neg_count)

vec_dim = 100

print (""Build training data set..."")
train_arrays = numpy.zeros((train_pos_count + train_neg_count, vec_dim))
train_labels = numpy.zeros(train_pos_count + train_neg_count)

for i in range(train_pos_count):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    train_arrays[i] = model.docvecs[prefix_train_pos]
    train_labels[i] = 1

for i in range(train_neg_count):
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    train_arrays[train_pos_count + i] = model.docvecs[prefix_train_neg]
    train_labels[train_pos_count + i] = 0


print (""Build testing data set..."")
test_arrays = numpy.zeros((test_pos_count + test_neg_count, vec_dim))
test_labels = numpy.zeros(test_pos_count + test_neg_count)

for i in range(test_pos_count):
    prefix_test_pos = 'TEST_POS_' + str(i)
    test_arrays[i] = model.docvecs[prefix_test_pos]
    test_labels[i] = 1

for i in range(test_neg_count):
    prefix_test_neg = 'TEST_NEG_' + str(i)
    test_arrays[test_pos_count + i] = model.docvecs[prefix_test_neg]
    test_labels[test_pos_count + i] = 0


print (""Begin classification..."")
classifier = None
if sys.argv[3] == '-lr':
    print (""Logistic Regressions is used..."")
    classifier = LogisticRegression()
elif sys.argv[3] == '-svm':
    print (""Support Vector Machine is used..."")
    classifier = SVC()
elif sys.argv[3] == '-knn':
    print (""K-Nearest Neighbors is used..."")
    classifier = KNeighborsClassifier(n_neighbors=10)
elif sys.argv[3] == '-rf':
    print (""Random Forest is used..."")
    classifier = RandomForestClassifier()

classifier.fit(train_arrays, train_labels)

print (""Accuracy:"", classifier.score(test_arrays, test_labels))
</code></pre>

<p>I am getting a Keyerror - ""TEST_POS_72""<a href=""https://i.stack.imgur.com/z9TEW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z9TEW.png"" alt=""ERROR""></a></p>

<p>I want to know what I am doing wrong.</p>
","8101003","","","","","2017-06-12 03:25:59","Getting error while using gensim model in python","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"62358583","1","62369680","","2020-06-13 10:38:34","","1","344","<p>I am trying to train Gensim Doc2Vec model on tagged documents. I have around 4000000 documents. Following is my code:</p>

<pre><code>import pandas as pd
import multiprocessing
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
import logging
from tqdm import tqdm
from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import os
import re



def text_process(text):
    logging.basicConfig(format=""%(levelname)s - %(asctime)s: %(message)s"", datefmt='%H:%M:%S', level=logging.INFO)
    stop_words_lst = ['mm', 'machine', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'first', 'second', 'third', 'plurality', 'one', 'more', 'least', 'at', 'example', 'memory', 'exemplary', 'fourth', 'fifth', 'sixth','a', 'A', 'an', 'the', 'system', 'method', 'apparatus', 'computer', 'program', 'product', 'instruction', 'code', 'configure', 'operable', 'couple', 'comprise', 'comprising', 'includes', 'cm', 'processor', 'hardware']
    stop_words = set(stopwords.words('english'))

    temp_corpus =[]
    text = re.sub(r'\d+', '', text)
    for w in stop_words_lst:
        stop_words.add(w)
    tokenizer = RegexpTokenizer(r'\w+')
    word_tokens = tokenizer.tokenize(text)
    lemmatizer= WordNetLemmatizer()
    for w in word_tokens:
        w = lemmatizer.lemmatize(w)
        if w not in stop_words:
            temp_corpus.append(str(w))
    return temp_corpus

chunk_patent = pd.DataFrame()
chunksize = 10 ** 5
cores = multiprocessing.cpu_count()
directory = os.getcwd()
for root,dirs,files in os.walk(directory):
    for file in files:
       if file.startswith(""patent_cpc -""):
           print(file)
           #f=open(file, 'r')
           #f.close()
           for chunk_patent_temp in pd.read_csv(file, chunksize=chunksize):
                #chunk_patent.sort_values(by=['cpc'], inplace=True)
                #chunk_patent_temp = chunk_patent_temp[chunk_patent_temp['cpc'] == ""G06K7""]
                if chunk_patent.empty:
                    chunk_patent = chunk_patent_temp
                else:
                    chunk_patent = chunk_patent.append(chunk_patent_temp)
train_tagged = chunk_patent.apply(lambda r: TaggedDocument(words=text_process(r['text']), tags=[r.cpc]), axis=1)
print(train_tagged.values)

if os.path.exists(""cpcpredict_doc2vec.model""):
    doc2vec_model = Doc2Vec.load(""cpcpredict_doc2vec.model"")
    doc2vec_model.build_vocab((x for x in tqdm(train_tagged.values)), update=True)
    doc2vec_model.train(train_tagged, total_examples=doc2vec_model.corpus_count, epochs=50)
    doc2vec_model.save(""cpcpredict_doc2vec.model"")
else:
    doc2vec_model = Doc2Vec(dm=0, vector_size=300, min_count=100, workers=cores-1)
    doc2vec_model.build_vocab((x for x in tqdm(train_tagged.values)))
    doc2vec_model.train(train_tagged, total_examples=doc2vec_model.corpus_count, epochs=50)
    doc2vec_model.save(""cpcpredict_doc2vec.model"")
</code></pre>

<p>I have tried modifying the Doc2vec parameters but without any luck.</p>

<p>On the same data I have trained Word2vec model, which is much accurate in comparison to the doc2vec model. Further, ""most_similar"" results for word2vec model is very different from the doc2vec model. </p>

<p>Following is the code for searching most similar results:</p>

<pre><code>from gensim.models import Word2Vec
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
import logging
from gensim.models import Doc2Vec
import re

def text_process(text):
    logging.basicConfig(format=""%(levelname)s - %(asctime)s: %(message)s"", datefmt='%H:%M:%S', level=logging.INFO)
    stop_words_lst = ['mm', 'machine', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'first', 'second', 'third', 'example', 'memory', 'exemplary', 'fourth', 'fifth', 'sixth','a', 'A', 'an', 'the', 'system', 'method', 'apparatus', 'computer', 'program', 'product', 'instruction', 'code', 'configure', 'operable', 'couple', 'comprise', 'comprising', 'includes', 'cm', 'processor', 'hardware']
    stop_words = set(stopwords.words('english'))
    #for index, row in df.iterrows():
    temp_corpus =[]
    text = re.sub(r'\d+', '', text)
    for w in stop_words_lst:
        stop_words.add(w)
    tokenizer = RegexpTokenizer(r'\w+')
    word_tokens = tokenizer.tokenize(text)
    lemmatizer= WordNetLemmatizer()
    for w in word_tokens:
        w = lemmatizer.lemmatize(w)
        if w not in stop_words:
            temp_corpus.append(str(w))
    return temp_corpus

model = Word2Vec.load(""cpc.model"")
print(model.most_similar(positive=['barcode'], topn=30))

model1 = Doc2Vec.load(""cpcpredict_doc2vec.model"")

pred_tags = model1.most_similar('barcode',topn=10)
print(pred_tags)
</code></pre>

<p>Further, the output of the aforementioned is cited below:</p>

<pre><code>[('indicium', 0.36468246579170227), ('symbology', 0.31725651025772095), ('G06K17', 0.29797130823135376), ('dataform', 0.29535001516342163), ('rogue', 0.29372256994247437), ('certification', 0.29178398847579956), ('reading', 0.27675414085388184), ('indicia', 0.27346929907798767), ('Contra', 0.2700084149837494), ('redemption', 0.26682156324386597)]

[('searched', 0.4693435728549957), ('automated', 0.4469209909439087), ('production', 0.4364866018295288), ('hardcopy', 0.42193126678466797), ('UWB', 0.4197841286659241), ('technique', 0.4149003326892853), ('authorized', 0.4134449362754822), ('issued', 0.4129987359046936), ('installing', 0.4093806743621826), ('thin', 0.4016669690608978)]
</code></pre>
","7472724","","","","","2020-06-14 15:57:14","Improving DOC2VEC Gensim efficiency","<python><nltk><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"36223864","1","36259938","","2016-03-25 16:25:17","","0","560","<p>I'm using doc2vec with a corpus of about 1 million titles. To train the corpus, I'm using the following code: </p>

<pre><code>model = gensim.models.Doc2Vec(min_count=1, window=10, size=300, workers=4)
model.build_vocab(corpus)
for epoch in range(10):
    model.train(corpus)
</code></pre>

<p>Everything seems to train properly and I am able to infer a vector using titles.most_similar. </p>

<p>I encounter a problem, however, when I try to use the vectors. It seems as though some documents are missing from the final model! I.e.: </p>

<pre><code>model.docvecs['SENT_157000']
</code></pre>

<blockquote>
  <p>KeyError: 'SENT_157000'</p>
</blockquote>

<p>I checked the gensim forum and stackoverflow and the only suggestion I could find was to ensure that the min_count = 1. I did that but I'm still having this issue.</p>
","875527","","670206","","2016-03-25 16:26:08","2018-03-23 14:40:42","KeyError in Doc2Vec model even when min_count set to 1 during training","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"53534262","1","","","2018-11-29 07:58:10","","0","91","<p>I use three txt file to do a LDA project
I try to separate these three txt file with two way 
The difference among the process is:</p>

<pre><code>docs = [[doc1.split(' ')], [doc2.split(' ')], [doc3.split(' ')]]
docs1 = [[''.join(i)] for i in re.split(r'\n{1,}', doc11)] + [[''.join(e)] for e in re.split(r'\n{1,}', doc22)] + [[''.join(t)] for t in re.split(r'\n{1,}', doc33)]    
dictionary = Dictionary(docs)
dictionary1 = Dictionary(docs1)
corpus = [dictionary.doc2bow(doc) for doc in docs]
corpus1 = [dictionary.doc2bow(doc) for doc in docs1]
</code></pre>

<p>And the document number is</p>

<pre><code>len(corpus)
len(corpus1)
3
1329
</code></pre>

<p>But the lda model create a rubbish result in <code>corpus</code> but a relatively good result in <code>corpus1</code></p>

<p>I use this model to train the document</p>

<pre><code>model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                    id2word=id2word,
                                    num_topics=10, 
                                    random_state=100,
                                    update_every=1,
                                    chunksize=100,
                                    passes=10,
                                    alpha='auto',
                                    per_word_topics=True)
</code></pre>

<p>The difference in the two model is the document number, everything else is the same</p>

<p>Why LDA create such a different result in this two model?</p>
","10595338","","","","","2018-12-02 05:57:03","How will the document number affect the result of Gensim LDA?","<python-3.x><text-mining><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"45108291","1","45113716","","2017-07-14 17:05:14","","1","633","<p>I would like to use embeddings made by w2v in order to obtain the most likely substitute words GIVEN a context (surrounding words), rather than supplying an individual word.</p>

<p>Example:
sentence = 'I would like to go to the park tomorrow after school'</p>

<p>If I want to find candidates similar to ""park"", typically I would just leverage the similarity function from the Gensim model</p>

<pre><code>model.most_similar('park')
</code></pre>

<p>and obtain semantically similar words. However this could give me similar words to the verb 'park' instead of the noun 'park', which I was after.</p>

<p>Is there any way to query the model and give it surrounding words as context to provide better candidates?</p>
","4031331","","","","","2017-07-15 01:55:41","python word2vec context similarity using surrounding words","<python><gensim><word2vec><word-embedding>","1","0","1","","","CC BY-SA 3.0"
"36250297","1","36250884","","2016-03-27 17:20:41","","5","2409","<p>Given such a data frame, including the item and corresponding review texts:</p>

<pre><code>item_id          review_text
B2JLCNJF16       i was attracted to this...
B0009VEM4U       great snippers...
</code></pre>

<p>I want to map the top <code>5000</code> most frequent word in <code>review_text</code>, so the resulting data frame should be like:</p>

<pre><code>item_id            review_text
B2JLCNJF16         1  2  3  4  5...
B0009VEM4U         6... #as the word ""snippers""  is out of the top 5000 most frequent word
</code></pre>

<p>Or, a bag-of-word vector is highly preferred:</p>

<pre><code>item_id            review_text
B2JLCNJF16         [1,1,1,1,1....]
B0009VEM4U         [0,0,0,0,0,1....] 
</code></pre>

<p>How can I do that? Thanks a lot!</p>

<p>EDIT:
I have tried @ayhan 's answer. Now I have successfully changed the review text to a <code>doc2bow</code> form:</p>

<pre><code>item_id            review_text
B2JLCNJF16         [(123,2),(130,3),(159,1)...]
B0009VEM4U         [(3,2),(110,2),(121,5)...]
</code></pre>

<p>It denotes the word of ID <code>123</code> has occurred <code>2</code> times in that document. Now I'd like to  transfer it to a vector like:</p>

<pre><code>[0,0,0,.....,2,0,0,0,....,3,0,0,0,......1...]
        #123rd         130th        159th
</code></pre>

<p>Do you how to do that? Thank you in advance!</p>
","","user4462740","","user4462740","2016-04-03 16:26:24","2016-04-03 16:48:13","How to map the word in data frame to integer ID with python-pandas and gensim?","<python><pandas><gensim>","1","0","","","","CC BY-SA 3.0"
"62636828","1","","","2020-06-29 11:09:49","","0","1414","<p>While trying to import gensim, I run into the following error</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\usr\Documents\hello\test.py&quot;, line 3, in &lt;module&gt;
    import gensim
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\gensim\__init__.py&quot;, line 5, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\gensim\corpora\__init__.py&quot;, line 6, in &lt;module&gt;
    from .indexedcorpus import IndexedCorpus  # noqa:F401 must appear before the other classes
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\gensim\corpora\indexedcorpus.py&quot;, line 15, in &lt;module&gt;
    from gensim import interfaces, utils
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\gensim\interfaces.py&quot;, line 21, in &lt;module&gt;
    from gensim import utils, matutils
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\gensim\matutils.py&quot;, line 21, in &lt;module&gt;
    from scipy.stats import entropy
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\scipy\stats\__init__.py&quot;, line 384, in &lt;module&gt;
    from .stats import *
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\scipy\stats\stats.py&quot;, line 179, in &lt;module&gt;
    from scipy.spatial.distance import cdist
  File &quot;C:\Users\usr\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\scipy\spatial\__init__.py&quot;, line 99, in &lt;module&gt;
    from .qhull import *
ImportError: DLL load failed: The specified module could not be found.
</code></pre>
<p>I have tried uninstalling numpy, scipy and gensim using <code>pip</code> in the command prompt and installing them again, but this does not resolve the issue.</p>
<p>I have also looked at the suggestions to a similar problem <a href=""https://stackoverflow.com/questions/34253458/installing-scipy-package-in-windows"">here</a>, and tried installing <a href=""https://www.lfd.uci.edu/%7Egohlke/pythonlibs/#numpy"" rel=""nofollow noreferrer"">numpy‚Äë1.19.0+mkl‚Äëcp37‚Äëcp37m‚Äëwin_amd64.whl</a>, but it resulted in a separate error <code>Importing the numpy c-extensions failed.</code> Thus, I have stuck to using numpy, scipy and gensim installed via <code>pip</code>.</p>
<p>Additionally, I installed scipy version 1.4.1 as the latest 1.5.0 version will give the following error as described in this link:
<a href=""https://stackoverflow.com/questions/62566691/error-when-loading-scipy-oserror-winerror-126-the-specified-module-could-not"">Error when loading scipy: OSError: [WinError 126] The specified module could not be found</a></p>
<p>Any help is greatly appreciated!</p>
<p>For additional information, I am using Python 3.7 and Windows 10.</p>
","13833839","","13833839","","2020-06-30 04:11:06","2020-06-30 07:09:11","""ImportError: DLL load failed: The specified module could not be found"" when trying to import gensim","<numpy><scipy><gensim>","0","5","","","","CC BY-SA 4.0"
"52734146","1","","","2018-10-10 06:43:12","","6","4786","<p>I'm working on a NLP application, where I have a corpus of text files. I would like to create word vectors using the <strong>Gensim word2vec algorithm</strong>. </p>

<p>I did a 90% training and 10% testing split. I trained the model on the appropriate set, but I would like to assess the accuracy of the model on the testing set.</p>

<p>I have surfed the internet for any documentation on accuracy assessment, but I could not find any methods that allowed me to do so. Does anyone know of a function that does accuracy analysis?</p>

<p>The way I processed my test data was that I extracted all the sentences from the text files in the test folder, and I turned it into a giant list of sentences. After that, I used a function that I though was the right one (turns out it wasn't as it gave me this error: <strong>TypeError: don't know how to handle uri</strong>). Here is how I went about doing this:</p>

<pre><code>test_filenames = glob.glob('./testing/*.txt')

print(""Found corpus of %s safety/incident reports:"" %len(test_filenames))

test_corpus_raw = u""""
for text_file in test_filenames:
    txt_file = open(text_file, 'r')
    test_corpus_raw += unicode(txt_file.readlines())
print(""Test Corpus is now {0} characters long"".format(len(test_corpus_raw)))

test_raw_sentences = tokenizer.tokenize(test_corpus_raw)

def sentence_to_wordlist(raw):
    clean = re.sub(""[^a-zA-Z]"","" "", raw)
    words = clean.split()
    return words

test_sentences = []
for raw_sentence in test_raw_sentences:
    if len(raw_sentence) &gt; 0:
        test_sentences.append(sentence_to_wordlist(raw_sentence))

test_token_count = sum([len(sentence) for sentence in test_sentences])
print(""The test corpus contains {0:,} tokens"".format(test_token_count))


####### THIS LAST LINE PRODUCES AN ERROR: TypeError: don't know how to handle uri 
texts2vec.wv.accuracy(test_sentences, case_insensitive=True)
</code></pre>

<p>I have no idea how to fix this last part. Please help. Thanks in advance!</p>
","9431573","","","","","2019-07-29 09:11:07","Word2vec Gensim Accuracy Analysis","<python><nlp><gensim><word2vec>","2","0","3","","","CC BY-SA 4.0"
"61693100","1","","","2020-05-09 07:12:38","","0","157","<p>I'm using windows 10 and python 3.3. I tried to download fasttext_model300 to calculate soft cosine similarity between documents, but when I run my python file, it stops after arriving at this statement:</p>

<pre><code>fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')
</code></pre>

<p>There are no errors or not responding, It just stops without any reaction.</p>

<p>Does anybody know why it happens?</p>

<p>Thanks</p>
","11344498","","1823841","","2020-05-10 05:53:37","2021-08-22 17:34:33","how to fix the problem of downloading fasttext-model300?","<text-mining><gensim><similarity><cosine-similarity><sentence-similarity>","2","0","","","","CC BY-SA 4.0"
"26286206","1","26286348","","2014-10-09 19:12:16","","-1","4089","<p>I am using gensim package for topic modelling in python.</p>

<p>I am trying to train the topic model using gensim. Below is the train.py module:</p>

<pre><code>class Corpus(object):
    def __init__(self, cursor, reviews_dictionary, corpus_path):
        self.cursor = cursor
        self.reviews_dictionary = reviews_dictionary
        self.corpus_path = corpus_path

    def __iter__(self):
        self.cursor.rewind()
        for review in self.cursor:
            yield self.reviews_dictionary.doc2bow(review[""words""])

    def serialize(self):
        BleiCorpus.serialize(self.corpus_path, self, id2word=self.reviews_dictionary)

        return self


class Dictionary(object):
    def __init__(self, cursor, dictionary_path):
        self.cursor = cursor
        self.dictionary_path = dictionary_path

    def build(self):
        self.cursor.rewind()
        dictionary = corpora.Dictionary(review[""words""] for review in self.cursor)
        dictionary.filter_extremes(keep_n=10000)
        dictionary.compactify()
        corpora.Dictionary.save(dictionary, self.dictionary_path)

        return dictionary


class Train:
    def __init__(self):
        pass

    @staticmethod
    def run(lda_model_path, corpus_path, num_topics, id2word):
        corpus = corpora.BleiCorpus(corpus_path)
        lda = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=id2word)
        lda.save(lda_model_path)

        return lda
</code></pre>

<p>I am getting the below error when I run this module:</p>

<pre><code>&gt; Traceback (most recent call last):


    File ""train.py"", line 74, in &lt;module&gt;
    main()

    File ""train.py"", line 68, in main
    dictionary = Dictionary(reviews_cursor, dictionary_path).build()
    File ""train.py"", line 38, in build
    corpora.Dictionary.save(dictionary, self.dictionary_path)
    File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 288, in save
    pickle(self, fname)
    File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 666, in pickle
    with smart_open(fname, 'wb') as fout: # 'b' for binary, needed on Windows
    File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 661, in smart_open
    return open(fname, mode)
    IOError: [Errno 2] No such file or directory: 'models/dictionary.dict'
</code></pre>

<p>Could anyone please help me figure out the issue?</p>
","3006723","","2676531","","2014-10-09 19:18:07","2014-10-09 19:22:09","Python:: IOError: [Errno 2] No such file or directory: 'models/dictionary.dict'","<python><gensim>","1","0","","","","CC BY-SA 3.0"
"43985180","1","44001668","","2017-05-15 17:07:37","","1","1234","<p>Loading the wiki-fasttext model with the gensim library takes <strong>six</strong> minutes. </p>

<p>I'm aware of ways to cache the model but I'm looking for ways to speedup the initial model loading. The specific api is below:</p>

<pre><code>en_model = KeyedVectors.load_word2vec_format(os.path.join(root_dir, model_file))
</code></pre>

<p>Granted, wiki-fasttext a very large model, however I have load the same model in many languages.  </p>
","1363774","","","","","2017-05-16 12:36:24","Is there a way to load the wiki-fasttext model faster with load_word2vec_format","<nlp><stanford-nlp><gensim><fasttext>","1","0","1","","","CC BY-SA 3.0"
"52757683","1","","","2018-10-11 10:20:39","","0","484","<p>Hello I have some word2vec models generated using Word2Vec java implementation in <a href=""https://deeplearning4j.org/docs/latest/deeplearning4j-nlp-word2vec"" rel=""nofollow noreferrer"">DL4J</a> and saved by calling </p>

<pre><code>writeWord2VecModel(Word2Vec vectors, String path)
</code></pre>

<p>The output of that is a zip file that contains a bunch of txt files.
I can successfully load and use the model in DL4j using </p>

<pre><code>Word2Vec readWord2VecModel(String path)
</code></pre>

<p>I am now trying to read that model in python, using <code>gensim</code></p>

<pre><code>import gensim

model = gensim.models.KeyedVectors.load_word2vec_format('file_path, binary=False)
</code></pre>

<p>But I get the following error:</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe0 in position 10: invalid continuation byte
</code></pre>

<p>I also tried with binary=True and get same results.</p>

<p>If I extract the model generated by DL4J I get the following files:</p>

<p><a href=""https://i.stack.imgur.com/zMowD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zMowD.png"" alt=""List Of Files""></a></p>

<p>Is there a way to read that model in python <code>genism</code>?</p>
","887835","","","","","2018-10-11 18:15:45","Loading DL4J trained Word2Vec Model into gensim","<python><gensim><word2vec><dl4j>","1","0","","","","CC BY-SA 4.0"
"46762366","1","46780344","","2017-10-16 03:06:11","","-1","471","<p>I load a KeyedVectors model and the word frequency seems like word index</p>

<p>And I miss something?</p>

<p><a href=""https://i.stack.imgur.com/Fy0vC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fy0vC.png"" alt=""enter image description here""></a></p>
","3909384","","","","","2017-10-16 23:18:30","gensim KeyedVectors object word count","<gensim>","1","0","","","","CC BY-SA 3.0"
"55121095","1","","","2019-03-12 12:06:41","","0","1843","<p>I'm new to DL and NLP, and recently started using a pre-trained fastText embedding model (cc.en.300.bin) through gensim.</p>

<p>I would like to be able to calculate vectors for out-of-vocabulary words myself, by splitting the word to n-grams and looking up the vector for every n-gram.</p>

<p>I could not find a way to export the n-gram vectors that are part of the model. I realize they are hashed, but perhaps there's a way (not necessarily using gensim) to get them?</p>

<p>Any insight will be appreciated!</p>
","10584991","","","","","2019-05-12 20:55:17","fasttext: is there a way export ngrams?","<export><gensim><n-gram><fasttext><oov>","2","0","","","","CC BY-SA 4.0"
"37696459","1","","","2016-06-08 07:53:53","","0","664","<p>I have 2 documents A-B (or 2 series of documents), and would like to get the 
a new document showing difference between the two document: A-B</p>

<p>By difference, there are several definitions, one is :
           List of words/""concept"" include in A but not in B.</p>

<p>I am thinking of using TF IDF for each sentence of A and B ,
such as :</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
d1 = [open(f1) for f1 in text_files]
tfidf = TfidfVectorizer().fit_transform(d1)
pairwise_similarity = tfidf * tfidf.T
</code></pre>

<p>I am not sure if this would be relevant to generate a new document C= ""A-B"",
especially am interested in ""semantic difference"" in the document C</p>
","6367065","","6367065","","2016-06-08 14:16:33","2016-06-08 14:16:33","NLP How to get the difference between 2 documents","<nlp><scikit-learn><stanford-nlp><gensim><spacy>","1","2","","","","CC BY-SA 3.0"
"18183810","1","18869058","","2013-08-12 09:38:20","","6","9517","<p>I was just curious about the gensim dictionary implementation. I have the following code:</p>

<pre><code>    def build_dictionary(documents):
        dictionary = corpora.Dictionary(documents)
        dictionary.save('/tmp/deerwester.dict') # store the dictionary
        return dictionary    
</code></pre>

<p>and I looked inside the file deerwester.dict and it looks like this:</p>

<pre><code>8002 6367 656e 7369 6d2e 636f 7270 6f72
612e 6469 6374 696f 6e61 7279 0a44 6963
7469 6f6e 6172 790a 7101 2981 7102 7d71
0328 5508 6e75 6d5f 646f 6373 7104 4b09
5508 ...
</code></pre>

<p>the following code, however,</p>

<pre><code>my_dict = dictionary.load('/tmp/deerwester.dict') 
print my_dict.token2id #view dictionary
</code></pre>

<p>yields this:</p>

<pre><code>{'minors': 30, 'generation': 22, 'testing': 16, 'iv': 29, 'engineering': 15, 'computer': 2, 'relation': 20, 'human': 3, 'measurement': 18, 'unordered': 25, 'binary': 21, 'abc': 0, 'ordering': 31, 'graph': 26, 'system': 10, 'machine': 6, 'quasi': 32, 'random': 23, 'paths': 28, 'error': 17, 'trees': 24, 'lab': 5, 'applications': 1, 'management': 14, 'user': 12, 'interface': 4, 'intersection': 27, 'response': 8, 'perceived': 19, 'widths': 34, 'well': 33, 'eps': 13, 'survey': 9, 'time': 11, 'opinion': 7}
</code></pre>

<p>So my question is, since I don't see the actual words inside the .dict file, what are all of the hexadecimal values stored there? Is this some kind of super compressed format? I'm curious because I feel like if it is, I should consider using it from now on.</p>
","1106919","","610569","","2013-09-18 10:01:44","2013-09-18 10:01:44","Gensim Dictionary Implementation","<python><nlp><topic-modeling><gensim>","1","0","1","","","CC BY-SA 3.0"
"61405111","1","61417290","","2020-04-24 09:24:06","","1","201","<p>I am currently training a Gensim FastText model with a document from a certain domain with the unsupervised training method from Gensim. </p>

<p>After this training of the word representations i would like to train a set of sentence+label lines and ultimately test the model and return a precision and recall value like it is possible in facebooks fastText implementation via train_supervised + test. Does GenSims implementation support the supervised training and testing? I couldnt get it to work / find the required methods.</p>

<p>Any help is much appreciated.</p>
","9819398","","","","","2020-04-24 20:54:30","Supervised training and testing in GenSims FastText implementation","<machine-learning><text><classification><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"36462394","1","","","2016-04-06 21:08:55","","2","1905","<p>I want to use gensim to convert Wikipedia dump to plain text using <code>python -m gensim.scripts.make_wiki</code> script.</p>

<p>I use it as :</p>

<pre><code>python -m gensim.scripts.make_wiki ./enwiki-latest-pages-articles.xml.bz2 ./results
</code></pre>

<p>gives me an error at the end:</p>

<pre><code>2016-04-06 20:43:46,471 : INFO : storing corpus in Matrix Market format to ./results/_bow.mm
Traceback (most recent call last):
  File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/scripts/make_wiki.py"", line 88, in &lt;module&gt;
    MmCorpus.serialize(outp + '_bow.mm', wiki, progress_cnt=10000) # another ~9h
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/corpora/indexedcorpus.py"", line 89, in serialize
    offsets = serializer.save_corpus(fname, corpus, id2word, progress_cnt=progress_cnt, metadata=metadata)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/corpora/mmcorpus.py"", line 49, in save_corpus
    return matutils.MmWriter.write_corpus(fname, corpus, num_terms=num_terms, index=True, progress_cnt=progress_cnt, metadata=metadata)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/matutils.py"", line 486, in write_corpus
    mw = MmWriter(fname)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.12.3-py2.7-linux-x86_64.egg/gensim/matutils.py"", line 436, in __init__
    self.fout = utils.smart_open(self.fname, 'wb+') # open for both reading and writing
  File ""build/bdist.linux-x86_64/egg/smart_open/smart_open_lib.py"", line 111, in smart_open
NotImplementedError: unknown file mode wb+
</code></pre>

<p>Does anybody know what is going on?</p>
","6169094","","6169094","","2016-04-07 01:48:40","2017-05-26 12:09:03","Convert Wikipedia dump to text using python -m gensim.scripts.make_wiki","<python><wikipedia><gensim>","1","0","","","","CC BY-SA 3.0"
"61403580","1","","","2020-04-24 07:49:32","","0","306","<p>I am trying to see what pre-trained model has included common phrases in news and I thought GoogleNews-vectors-negative300.bin should be a comprehensive one but it turned out that it does not even include deep_learning, machine_learning, social_network, social_responsibility. What pre-trained model could include those words that often occur in news, public reports?</p>

<pre><code>import gensim

# Load Google's pre-trained Word2Vec model.
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)

model.similarity('deep_learning', 'machine_learning')
</code></pre>
","10308337","","6573902","","2020-04-24 10:17:19","2020-04-24 21:00:40","Word not in vocabulary of GoogleNews-vectors-negative300.bin","<python><nlp><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"44090503","1","44458979","","2017-05-20 20:26:35","","3","972","<p>that might actually be a dumb question but I just can't figure out why my script with gensim.models.word2vec is not working. Here is the thing, I'm using the stanford sentiment analysis databank dataset (~11000 reviews), and i'm trying to build word2vec using gensim, this is my script: </p>

<pre><code>import gensim as gs 
import sys 

# open the datas
sentences = gs.models.word2vec.LineSentence('../processedWords.txt')
print(""size in RAM of the sentences: {}"".format(sys.getsizeof(sentences)))

# transform them
# bigram_transformer = gs.models.Phrases(sentences)

model = gs.models.word2vec.Word2Vec(sentences, min_count=10, size=100, window=5)
model.save('firstModel')
print(model.similarity('film', 'test'))
print(model.similarity('film', 'movie'))
</code></pre>

<p>Now, my problem is that the script runs in 2s, and gives only huge similarity between every pair of words. In addition, some words which are in the sentences are not in the built vocabulary. </p>

<p>I must be doing something obviously wrong, but can't figure what. </p>

<p>Thank you for your help. </p>
","4541360","","4541360","","2017-05-23 10:28:57","2017-06-09 13:22:44","Gensim Word2Vec: poor training performance.","<python-3.x><dataset><text-mining><gensim><word2vec>","1","5","1","","","CC BY-SA 3.0"
"62356383","1","","","2020-06-13 06:36:31","","0","430","<p>I am using Gensim's Mallet wrapper for topic modeling - </p>

<pre><code>LdaMallet(path_to_mallet_binary, corpus=corpus, num_topics=100, id2word=words, workers=6, random_seed=2)
</code></pre>

<p>While the above worked surprisingly fast, the step (see below) to obtain the topic distribution for each document (n=40,000) is taking a very long time.</p>

<pre><code>#Store topic distributuon for all documents
all_topics=[]
for x in tqdm(range(0, len(doc_list))):
    all_topics.append(lda_model[corpus[x]])
</code></pre>

<p>It has taken ~18 hours to complete 30,000 documents. Not sure what I am doing incorrectly. Is there a way to get topic distribution for all documents much faster?</p>
","7426950","","7426950","","2020-06-13 06:55:15","2021-01-03 16:02:43","Gensim Mallet Wrapper: How can I get all documents' topic weights?","<python><gensim><lda><topic-modeling><mallet>","2","0","","","","CC BY-SA 4.0"
"26977042","1","","","2014-11-17 16:19:21","","1","2910","<p>I am working on a project and I would like to use Latent Dirichlet Allocation in order to extract topics from a large amount of articles.</p>

<p>My code is this:</p>

<pre><code>import gensim
import csv
import json
import glob
from gensim import corpora, models
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from time import gmtime, strftime

tokenizer = RegexpTokenizer(r'\w+')
cachedStopWords = set(stopwords.words(""english""))
body = []
processed = []

with open('/‚Ä¶/file.json') as j:
    data = json.load(j)

for i in range(0,len(data)):
    body.append(data[i]['text'].lower())

for entry in body:
    row = tokenizer.tokenize(entry)
    processed.append([word for word in row if word not in cachedStopWords])

dictionary = corpora.Dictionary(processed)
corpus = [dictionary.doc2bow(text) for text in processed]
lda = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50, update_every=1, passes=1)
topics = lda.show_topics(num_topics=50, num_words=8)

other_doc = ""After being jailed for life in 1964, Nelson Mandela became a worldwide symbol of resistance to apartheid. But his opposition to racism began many years before.""
print lda[other_doc]

Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-  packages/gensim/models/ldamodel.py"", line 714, in __getitem__
gamma, _ = self.inference([bow])
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site
packages/gensim/models/ldamodel.py"", line 361, in inference ids = [id for id, _ in doc]
ValueError: need more than 1 value to unpack
</code></pre>

<p>I also tried to use LdaMulticore in 3 different ways :</p>

<pre><code>lda = gensim.models.LdaMulticore(corpus, id2word=dictionary, num_topics=100, workers=3)
lda = gensim.models.ldamodel.LdaMulticore(corpus, id2word=dictionary, num_topics=100, workers=3)
lda = models.LdaMulticore(corpus, id2word=dictionary, num_topics=100, workers=3)
</code></pre>

<p>And every time I got this error :</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'module' object has no attribute ‚ÄòLdaMulticore'
</code></pre>

<p>Any ideas?</p>

<p>Thank you in advance.</p>
","3797381","","","","","2016-06-21 13:51:35","Using Latent Dirichlet Allocation with Gensim","<python><lda><gensim>","2","0","2","","","CC BY-SA 3.0"
"17765509","1","17765879","","2013-07-20 18:45:27","","2","1129","<p>I am using this <a href=""http://radimrehurek.com/gensim/tut3.html#similarity-interface"" rel=""nofollow"">gensim</a> tutorial to find similarities between texts. Here is the code </p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models import hdpmodel, ldamodel
from itertools import izip

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

'''
documents = [""Human machine interface for lab abc computer applications"",
              ""bags loose tea water second ingredient tastes water"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey"",
              ""red cow butter oil""]
'''
documents = [""Human machine interface for lab abc computer applications"",
              ""bags loose tea water second ingredient tastes water""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once]
         for text in texts]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

#print corpus

tfidf = models.TfidfModel(corpus)

#print tfidf

corpus_tfidf = tfidf[corpus]

#print corpus_tfidf

lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)
lsi.print_topics(1)

lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)
lda.print_topics(1)

corpora.MmCorpus.serialize('dict.mm', corpus)
corpus = corpora.MmCorpus('dict.mm')
#print corpus

lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
doc = ""human computer interaction""
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lsi = lsi[vec_bow]
#print vec_lsi

index = similarities.MatrixSimilarity(lsi[corpus])
index.save('dict.index')
index = similarities.MatrixSimilarity.load('dict.index')

sims = index[vec_lsi]
#print list(enumerate(sims))

sims = sorted(enumerate(sims),key=lambda item: -item[1])
for sim in sims:
  print documents[sim[0]], "" ==&gt; "", sim[1]
</code></pre>

<p>There are two documents here. One has 10 texts and another has 2. One is commented out. If I use the first documents list everything goes fine and generates meaningful output. If I use the second document list(having 2 texts) an error occured. Here is it </p>

<pre><code>/usr/lib/python2.7/dist-packages/scipy/sparse/compressed.py:122: UserWarning: indices array has non-integer dtype (float64)
% self.indices.dtype.name )
</code></pre>

<p>What is the reason behind this error and how can I fix it?
I am using a 64bit machine.</p>
","848930","","","","","2013-12-04 22:44:49","python gensim: indices array has non-integer dtype (float64)","<python><gensim>","2","0","","","","CC BY-SA 3.0"
"62626282","1","","","2020-06-28 17:53:54","","0","142","<p>This might seem like a trivial question to many but its not clear for me.</p>
<p>So i have a model that looks like this</p>
<pre><code>w2v_model = gensim.models.word2vec.Word2Vec(size=300, 
                                        window=7, 
                                        min_count=10, 
                                        workers=8)
</code></pre>
<p>I also tokenize my x_train and x_test like this</p>
<pre><code>x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=300)
x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=300)
</code></pre>
<p>My embedding layer looks like this</p>
<pre><code>embedding_matrix = np.zeros((vocab_size, 300), dtype='float32')
 for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]

embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, 
trainable=False)
</code></pre>
<p>So I get that the 300 in the model is the dimension size and the 300 in the pad_sequences is the max sequence length aka max sentence length</p>
<p>I also get that the 300 in the embedding_matrix definition is the w2v dimension size again and that the first 300 in the embedding_layer is referring to the w2v dimension size</p>
<p>and the second 300 would be the max input length aka the same as the max length in the pad_sequences but what I don't get is.</p>
<p>If my dimensional size of my w2v model equals to let's say 240, does this mean my pad_sequences maxlen should also be 240 and also the input_length in my embedding_layer?</p>
<p>For example, the w2v size is 300 but my sentence length that I want to predict is 360 and my pad_sequences maxlen would be 500. Would my neural net be able to correctly predict this sentence of length 360?</p>
","10817423","","","","","2020-06-28 17:53:54","Is the size in a Word2Vec model equal to the max input size of a sentence?","<python><gensim><word2vec><word-embedding>","0","3","","","","CC BY-SA 4.0"
"62059196","1","62060613","","2020-05-28 07:27:12","","4","1819","<p>I've tried to load pre-trained FastText vectors from <a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""nofollow noreferrer"">fastext - wiki word vectors</a>.</p>

<p>My code is below, and it works well. </p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import FastText
model = FastText.load_fasttext_format('./wiki.en/wiki.en.bin')
</code></pre>

<p>but, the warning message is a little annoying. </p>

<pre><code>gensim_fasttext_pretrained_vector.py:13: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings)
</code></pre>

<p>The message said, <code>load_fasttext_format</code> will be deprecated so, it will be better to use <code>load_facebook_vectors</code>. </p>

<p>So I decided to changed the code. and My changed code is like below.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import FastText
model = FastText.load_facebook_vectors('./wiki.en/wiki.en.bin')
</code></pre>

<p><strong>But</strong>, the error occurred, the error message is like this. </p>

<pre><code>Traceback (most recent call last):
  File ""gensim_fasttext_pretrained_vector.py"", line 13, in &lt;module&gt;
    model = FastText.load_facebook_vectors('./wiki.en/wiki.en.bin')
AttributeError: type object 'FastText' has no attribute 'load_facebook_vectors'
</code></pre>

<p>I couldn't understand why these thing happen.
I just change what the messages said, but it doesn't work. 
If you know anything about this, please let me know. </p>

<p>Always, thanks for you guys help. </p>
","6773046","","","","","2020-05-28 08:50:09","gensim - fasttext - Why `load_facebook_vectors` doesn't work?","<python><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"35900953","1","","","2016-03-09 19:33:30","","4","532","<p>I'm trying to run the distributed LDA example as described here:</p>

<p><a href=""https://radimrehurek.com/gensim/dist_lda.html"" rel=""nofollow"">https://radimrehurek.com/gensim/dist_lda.html</a></p>

<p>I created the set of documents by following the tutorial here:</p>

<p><a href=""https://radimrehurek.com/gensim/dist_lsi.html"" rel=""nofollow"">https://radimrehurek.com/gensim/dist_lsi.html</a></p>

<p>by ""inflat[ing] the corpus to 1M documents, by repeating its documents over&amp;over"" as it suggests</p>

<p>I'm using python 3.3 and numpy 1.9.2
I keep getting the following error:</p>

<pre><code>Exception in thread oneway-call:
Traceback (most recent call last):
  File ""/usr/lib64/python3.3/threading.py"", line 901, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.3/site-packages/Pyro4/core.py"", line 1484, in run
    super(_OnewayCallThread, self).run()
  File ""/usr/lib64/python3.3/threading.py"", line 858, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/lib64/python3.3/site-packages/gensim/models/lda_worker.py"", line 71, in requestjob
    self.processjob(job)
  File ""/usr/lib64/python3.3/site-packages/gensim/utils.py"", line 98, in _synchronizer
    result = func(self, *args, **kwargs)
  File ""/usr/lib64/python3.3/site-packages/gensim/models/lda_worker.py"", line 80, in processjob
    self.model.do_estep(job)
  File ""/usr/lib64/python3.3/site-packages/gensim/models/ldamodel.py"", line 480, in do_estep
    gamma, sstats = self.inference(chunk, collect_sstats=True)
  File ""/usr/lib64/python3.3/site-packages/gensim/models/ldamodel.py"", line 423, in inference
    if doc and not isinstance(doc[0][0], six.integer_types):
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
</code></pre>

<p>I ran the distributed lsi example and it ran fine, but for some reason I couldn't seem to get the lda to work. </p>

<p>I tried changing line 423 in /usr/lib64/python3.3/site-packages/gensim/models/ldamodel.py to:</p>

<pre><code>if doc is not None and not isinstance(doc[0][0], six.integer_types):
</code></pre>

<p>The error went away, but I got a warning that </p>

<pre><code>FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.
</code></pre>

<p>Could someone explain what I've done wrong? Is my change to this file correct? Or should I be running LDA differently?</p>
","3363331","","","","","2018-04-11 09:04:08","Trouble running Gensim LDA","<python><numpy><lda><gensim>","1","0","1","","","CC BY-SA 3.0"
"34948650","1","","","2016-01-22 14:07:52","","12","8979","<p>I am using <code>Doc2Vec</code> function of <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""noreferrer"">gensim</a> in Python to convert a document to a vector.</p>

<p>An example of usage</p>

<p><code>model = Doc2Vec(documents, size=100, window=8, min_count=5, workers=4)</code></p>

<p>How should I interpret the <code>size</code> parameter. I know that if I set <code>size = 100</code>, the length of output vector will be 100, but what does it mean? For instance, if I increase <code>size</code> to 200, what is the difference?</p>
","5492392","","","","","2016-01-28 08:41:05","How should I interpret ""size"" parameter in Doc2Vec function of gensim?","<python><gensim><word2vec>","2","1","0","","","CC BY-SA 3.0"
"52785462","1","","","2018-10-12 18:48:56","","2","1074","<p>While implementing <code>Fasttext</code> in Python 3.7, I am facing an unexpected scenario related to <code>Exception in thread</code>, which leads to </p>

<blockquote>
  <p>NoneType' object is not subscriptable</p>
</blockquote>

<p>The error (screenshot) of full stack trace is as follows: 
<a href=""https://i.stack.imgur.com/MhNgz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MhNgz.png"" alt=""enter image description here""></a></p>

<p>What exactly is this issue in  gensim python?</p>

<p>The code I have tried:</p>

<pre><code>import nltk, re
import string
from collections import Counter 
from string import punctuation
from nltk.tokenize import word_tokenize
from nltk.corpus import gutenberg, stopwords
from nltk.stem import WordNetLemmatizer
from gensim.models import FastText

def preprocessing():
    raw_data = (gutenberg.raw('shakespeare-hamlet.txt'))
    tokens = word_tokenize(raw_data)
    tokens = [w.lower() for w in tokens]
    #remove punctuation from each word
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    global words
    words = [word for word in stripped if word.isalpha()]
    sw = (stopwords.words('english'))
    sw1= (['.', ',', '""', '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])

    stop=sw+sw1
    words = [w for w in words if not w in stop]
preprocessing()

def freq_count():
    fd = nltk.FreqDist(words)
freq_count()

def intialize_word_embedding():
    model = FastText([words], size = 100, sg = 1, window = 5, min_count = 5, workers = 4)
    model.train([words], total_examples=len(words), epochs=10)
    model.init_sims(replace=True)
    model_name = ""mcft""
    model.save(model_name)
    print(len(model.wv.vocab))
intialize_word_embedding()
def load_model():
    model = FastText.load('mcft')
    similarities = model.wv.most_similar('hamlet')
    for word, score in similarities:
        print(word , score)
    print(model.wv.similarity('hamlet', 'king'))
load_model()
</code></pre>

<p>Note: The model works perfectly, when I comment the </p>

<pre><code>model.train([words], total_examples=len(words), epochs=10)`
</code></pre>

<p>line in the above shown code.</p>
","3966705","","3966705","","2018-10-13 08:44:10","2018-10-13 08:44:10","gensim error : 'NoneType' object is not subscriptable during training in Fasttext","<python><python-3.x><nltk><gensim><fasttext>","0","5","1","","","CC BY-SA 4.0"
"44249358","1","44334452","","2017-05-29 19:32:17","","1","166","<p>I would like to find outliers in my dataset by using LDA. In order to specify outliers, For this case, I am planning to use a bound or perplexity value of the new unseen document on trained model?
After that, I will sort the values in ascending order to check whether it is the outlier or not? 
My issue is that I could not get a bound/perplex value of individual doc, the model throws me <strong>""TypeError: 'int' object is not subscriptable""</strong> error.</p>

<p>I would appreciate if you help me to solve my case?</p>

<p>Just in case, I am attaching my code : </p>

<pre><code>tokenized_corpora = dictionary.doc2bow(_acc[2])
total_number_of_words_tokenized_corpora = len(tokenized_corpora)
bound_corpora = ldaModel.bound(tokenized_corpora)
per_word_perplex_corpora = np.exp2(-bound_corpora / 
total_number_of_words_tokenized_corpora)
</code></pre>

<p>Thanks in advance. </p>
","3013290","","","","","2017-06-02 17:30:23","how to get a bound or perplexity value of the new unseen document on trained model?","<python><gensim>","1","0","1","","","CC BY-SA 3.0"
"46985320","1","46987193","","2017-10-28 01:08:17","","1","1340","<p>I am trying to use a pre-trained model and add additional vocabulary to it. I have a csv file with 1 column of sentences in it.</p>

<pre><code>import gensim

existing_model_fr = gensim.models.Word2Vec.load('./fr/fr.bin')

new_sentences = gensim.models.word2vec.LineSentence('./data/french.csv')
existing_model_fr.build_vocab(new_sentences, update=True)

existing_model_fr.train(new_sentences, total_examples=existing_model_fr.corpus_count, epochs=5)
existing_model_fr.save('new_model_fr')
</code></pre>

<p>I get following error on existing_model_fr.train() line. What am I missing?</p>

<blockquote>
  <p>AttributeError Traceback (most recent call last) in ()</p>
  
  <p>/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py in
  train(self, sentences, total_examples, total_words, epochs,
  start_alpha, end_alpha, word_count, queue_factor, report_delay,
  compute_loss) 863 is only called once, the model's cached iter value
  should be supplied as epochs value. 864 """""" --> 865 if
  self.model_trimmed_post_training: 866 raise RuntimeError(""Parameters
  for training were discarded using model_trimmed_post_training method"")
  867 if FAST_VERSION &lt; 0:</p>
  
  <p>AttributeError: 'Word2Vec' object has no attribute
  'model_trimmed_post_training'</p>
</blockquote>
","1550708","","","","","2017-10-28 07:06:33","gensim - Word2vec online training - AttributeError: 'Word2Vec' object has no attribute 'model_trimmed_post_training","<nlp><word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"44443675","1","","","2017-06-08 19:02:46","","1","1291","<p>I am using a pre-trained doc2vec BOW model <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">(AP-news)</a>. I am doing the following:</p>

<pre><code>import gensim.models as g 
start_alpha=0.01
infer_epoch=1000
model=""\\apnews_dbow\\doc2vec.bin""
m = g.Doc2Vec.load(model)
text='this is a sample text'
vec=m.infer_vector(text,alpha=start_alpha, steps=infer_epoch)
</code></pre>

<p>But if I compute the vec again for the same text then I am getting a different vector representation of the same text. Why is this happening and how can I aviod this. I want to have the same vector returned if I give exactly the same text. 
I tried following <a href=""https://github.com/RaRe-Technologies/gensim/issues/447"" rel=""nofollow noreferrer"">this post</a> but does not seem to help. </p>
","386384","","","","","2021-09-24 09:44:33","removing randomization of vector initialization for doc2vec","<python><random><gensim><doc2vec>","3","4","","","","CC BY-SA 3.0"
"53449019","1","54412320","","2018-11-23 15:06:20","","0","338","<p>My questioon is about cossim usage.</p>

<p>I have this fragment of a very big fuction:</p>

<pre><code>for elem in lList:
    temp = []
    try:
        x = dict(np.ndenumerate(np.asarray(model[elem])))
    except:
        if x not in embedDict.keys():
            x = np.random.uniform(low=0.0, high=1.0, size=300)
            embedDict[elem] = x
        else:
            x  =  dict(np.ndenumerate(np.asarray(embedDict[elem])))

    for w in ListWords:
        try:
            y =  dict(np.ndenumerate(np.asarray(model[w])))
        except:
            if y not in embedDict.keys():
                y = np.random.uniform(low=0.0, high=1.0, size=300)
                embedDict[w] = y
            else:
                y =  dict(np.ndenumerate(np.asarray(embedDict[w])))

        temp.append(gensim.matutils.cossim(x,y))
</code></pre>

<p>I get the following exception:</p>

<pre class=""lang-none prettyprint-override""><code>File ""./match.py"", line 129, in getEmbedding
    test.append(gensim.matutils.cossim(x,y))
  File ""./Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/matutils.py"", line 746, in cossim
    vec1, vec2 = dict(vec1), dict(vec2)
TypeError: cannot convert dictionary update sequence element #0 to a sequence
</code></pre>

<p>Can you please help me and explain to me what this exception means? </p>
","2869180","","3890632","","2018-11-23 15:07:23","2019-01-29 00:40:18","how to use cosssim in gensim","<python><python-2.7><gensim>","1","0","","","","CC BY-SA 4.0"
"61693901","1","","","2020-05-09 08:33:13","","0","589","<p>In my study, I am exploring if there is a statistically significant ideological bias in one set of media as compared to another. I was hoping to explore this using the word embeddings approach. </p>

<p>Let us take US and UK news media for example. If I build a corpora of all US media articles for a given time period and a separate corpora of all UK media articles for the same period, train them each using the same word embeddings algorithm (<code>gensim/word2vec/fasttext</code>) with the same set of parameters (e.g., window and vector size), is it possible to test if cosine similarity obtained between a pair of words in the US corpora is statistically significantly larger than cosine similarity obtained between the same pair of words in the UK corpora?</p>

<p>Many thanks for your help!</p>
","7426950","","7426950","","2020-05-09 08:53:22","2020-05-09 13:37:24","Is it possible to compare similarity scores across two word embeddings repository?","<nlp><stanford-nlp><gensim><word2vec><fasttext>","1","0","","","","CC BY-SA 4.0"
"44581914","1","44595917","","2017-06-16 06:07:00","","1","1965","<p>I am building a NLP chat application in Python using <code>gensim</code> library through <code>doc2vec</code> model. I have hard coded documents and given a set of training examples, I am testing the model by throwing a user question and then finding most similar documents as a first step. In this case my test question is an exact copy of a document from training example. </p>

<pre><code>import gensim
from gensim import models
sentence = models.doc2vec.LabeledSentence(words=[u'sampling',u'what',u'is',u'tell',u'me',u'about'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'eligibility',u'what',u'is',u'my',u'limit',u'how',u'much',u'can',u'I',u'claim'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'eligibility',u'I',u'am',u'retiring',u'how',u'much',u'can',u'claim',u'have', u'resigned'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'what',u'is',u'my',u'eligibility',u'post',u'my',u'promotion'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'what',u'is', u'my',u'eligibility' u'post',u'my',u'promotion'], tags=[""SENT_4""])
sentences = [sentence, sentence1, sentence2, sentence3, sentence4]
class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename
    def __iter__(self):
        for uid, line in enumerate(open(filename)):
            yield LabeledSentence(words=line.split(), labels=['SENT_%s' % uid])
model = models.Doc2Vec(alpha=0.03, min_alpha=.025, min_count=2)
model.build_vocab(sentences)
for epoch in range(30):
    model.train(sentences, total_examples=model.corpus_count, epochs = model.iter)
    model.alpha -= 0.002  # decrease the learning rate`
    model.min_alpha = model.alpha  # fix the learning rate, no decay
model.save(""my_model.doc2vec"")
model_loaded = models.Doc2Vec.load('my_model.doc2vec')
print (model_loaded.docvecs.most_similar([""SENT_4""]))
</code></pre>

<p>Result:</p>

<pre><code>[('SENT_1', 0.043695494532585144), ('SENT_2', 0.0017897281795740128), ('SENT_0', -0.018954679369926453), ('SENT_3', -0.08253869414329529)]
</code></pre>

<p>Similarity of <code>SENT_4</code> and <code>SENT_3</code> is only <code>-0.08253869414329529</code> when it should be 1 since they are exactly same. How should I improve this accuracy? Is there a specific way of training documents and I am missing something out?  </p>
","4542815","","4542815","","2017-06-16 06:23:46","2017-06-16 18:36:40","How can I improve the cosine similarity of two documents(sentences) in doc2vec model?","<python><nlp><gensim><word2vec><doc2vec>","1","2","1","","","CC BY-SA 3.0"
"53301916","1","53333072","","2018-11-14 13:56:33","","6","5789","<p>I know that in <em>gensims</em> <em><code>KeyedVectors</code>-model</em>, one can access the embedding matrix by the attribute <code>model.syn0</code>. There is also a <code>syn0norm</code>, which doesn't seem to work for the <em>glove</em> model I recently loaded. I think I also have seen <code>syn1</code> somewhere previously. </p>

<p>I haven't found a doc-string for this and I'm just wondering what's the logic behind this?</p>

<p>So if <code>syn0</code> is the embedding matrix, what is <code>syn0norm</code>? What would then <code>syn1</code> be and generally, what does <code>syn</code> stand for?</p>
","7483494","","7483494","","2018-11-14 19:21:24","2021-06-05 20:52:42","Python/Gensim - What is the meaning of syn0 and syn0norm?","<python><deep-learning><nlp><gensim><word-embedding>","1","0","3","","","CC BY-SA 4.0"
"26812617","1","27261800","","2014-11-08 01:13:02","","1","2547","<p>I read the docs I have</p>

<pre><code>corpusObj.readDocsSample(sampleFile)
</code></pre>

<p>Next,</p>

<pre><code>dictionary = corpusObj.buildDictionary()
</code></pre>

<p>Then I build a corpus:</p>

<pre><code>corpus = corpusObj.buildCorpus()
</code></pre>

<p>Definition of buildDictionary and buildCorpus:</p>

<pre><code>def buildDictionary(self):

         texts = [[word for word in self.docs[i]] for i in self.docs]
         self.dictionary = corpora.Dictionary(texts)
         return self.dictionary


def buildCorpus(self):
         return [self.dictionary.doc2bow(words) for words in self.docs.itervalues()]
</code></pre>

<p>Then I do stop words stuff:</p>

<pre><code>stop = corpus.readStopWords()
stopids = [dictionary.token2id[stopword] for stopword in stop
         if stopword in dictionary.token2id]
dictionary.filter_tokens(stopids)
dictionary.compactify()
</code></pre>

<p>Then I call:</p>

<pre><code> lda = gensim.models.ldamodel.LdaModel(corpus=corp, id2word=dictionary, num_topics=100, update_every=1, chunksize=1000, passes=1)
</code></pre>

<p>Here is the error:</p>

<pre><code> Traceback (most recent call last):
File ""/Users/jsuit/PycharmProjects/MyGensimPlaything/GensimPlayToy.py"", line 33, in &lt;module&gt;
lda = gensim.models.ldamodel.LdaModel(corpus=corp, id2word=dictionary, num_topics=100,     update_every=1, chunksize=1000, passes=1)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 313, in __init__
self.update(corpus)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 553, in update
self.log_perplexity(chunk, total_docs=lencorpus)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 464, in log_perplexity
perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 639, in bound
gammad, _ = self.inference([doc])
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 376, in inference
expElogbetad = self.expElogbeta[:, ids]
IndexError: index 46979 is out of bounds for axis 1 with size 46979
</code></pre>

<p>The logging information below shows that it gets started but then crashes.</p>

<pre><code>2014-11-07 19:31:56,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-11-07 19:32:00,458 : INFO : built Dictionary(47445 unique tokens: [u'Szczecin', u'pro-Soviet', u'Negroponte', u'1,800', u'woods']...) from 2250 documents (total 1050902 corpus positions)
2014-11-07 19:32:08,192 : DEBUG : rebuilding dictionary, shrinking gaps
2014-11-07 19:32:08,237 : INFO : using symmetric alpha at 0.01
2014-11-07 19:32:08,237 : INFO : using serial LDA version on this node
2014-11-07 19:32:08,856 : INFO : running online LDA training, 100 topics, 1 passes over   
the supplied corpus of 2250 documents, updating model once every 1000 documents,           
evaluating perplexity every 2250 documents, iterating 50x with a convergence threshold of 
0.001000
2014-11-07 19:32:08,856 : WARNING : too few updates, training might not converge; 
consider increasing the number of passes or iterations to improve accuracy
2014-11-07 19:32:08,931 : INFO : PROGRESS: pass 0, at document #1000/2250
2014-11-07 19:32:08,931 : DEBUG : performing inference on a chunk of 1000 documents
2014-11-07 19:32:15,414 : DEBUG : 22/1000 documents converged within 50 iterations
2014-11-07 19:32:15,432 : DEBUG : updating topics
2014-11-07 19:32:15,476 : INFO : merging changes from 1000 documents into a model of 2250  
documents
2014-11-07 19:32:16,222 : INFO : topic #60 (0.010): 0.057*Neeman + 0.042*woods + 0.039*needed + 0.024*timeout + 0.020*reggae + 0.020*Shocked + 0.019*Dexter + 0.015*nonsensical + 0.014*3-to-1 + 0.011*Mauritius
2014-11-07 19:32:16,237 : INFO : topic #45 (0.010): 0.049*needed + 0.047*Neeman + 0.042*woods + 0.024*reggae + 0.023*timeout + 0.022*Dexter + 0.022*Shocked + 0.019*nonsensical + 0.012*3-to-1 + 0.011*mid-week
2014-11-07 19:32:16,251 : INFO : topic #86 (0.010): 0.049*needed + 0.048*Neeman + 0.047*woods + 0.029*Shocked + 0.023*timeout + 0.017*nonsensical + 0.016*reggae + 0.016*3-to-1 + 0.014*Dexter + 0.014*Mauritius
2014-11-07 19:32:16,265 : INFO : topic #92 (0.010): 0.017*Neeman + 0.016*needed + 0.014*woods + 0.011*Dexter + 0.010*timeout + 0.009*reggae + 0.006*Shocked + 0.006*nonsensical + 0.005*22-month-old + 0.004*3-to-1
2014-11-07 19:32:16,279 : INFO : topic #95 (0.010): 0.045*needed + 0.041*woods + 0.032*Shocked + 0.028*Neeman + 0.022*timeout + 0.020*nonsensical + 0.018*reggae + 0.017*Dexter + 0.013*Mauritius + 0.010*3-to-1
2014-11-07 19:32:16,294 : INFO : topic #30 (0.010): 0.054*needed + 0.052*Neeman + 0.033*woods + 0.024*timeout + 0.022*nonsensical + 0.021*Dexter + 0.021*Shocked + 0.016*reggae + 0.013*Mauritius + 0.012*3-to-1
2014-11-07 19:32:16,307 : INFO : topic #51 (0.010): 0.000*expands + 0.000*Promotion + 0.000*Arnold + 0.000*1,320.75 + 0.000*credits + 0.000*tuition + 0.000*_Or + 0.000*Hunt + 0.000*Futrell + 0.000*stagecoaches
2014-11-07 19:32:16,321 : INFO : topic #41 (0.010): 0.045*Neeman + 0.032*needed + 0.031*woods + 0.016*Dexter + 0.013*nonsensical + 0.013*Shocked + 0.013*timeout + 0.011*reggae + 0.009*peux + 0.009*Mauritius
2014-11-07 19:32:16,336 : INFO : topic #28 (0.010): 0.052*Neeman + 0.046*needed + 0.040*woods + 0.030*timeout + 0.026*Shocked + 0.019*nonsensical + 0.018*Dexter + 0.014*reggae + 0.011*3-to-1 + 0.010*crouch
2014-11-07 19:32:16,351 : INFO : topic #11 (0.010): 0.046*Neeman + 0.044*woods + 0.037*needed + 0.031*Shocked + 0.021*Dexter + 0.021*reggae + 0.017*nonsensical + 0.017*timeout + 0.012*3-to-1 + 0.010*Mauritius
2014-11-07 19:32:16,365 : INFO : topic #20 (0.010): 0.067*Neeman + 0.036*woods + 0.035*needed + 0.028*timeout + 0.020*reggae + 0.018*Dexter + 0.016*Mauritius + 0.015*Shocked + 0.015*nonsensical + 0.014*mid-week
2014-11-07 19:32:16,379 : INFO : topic #31 (0.010): 0.001*Neeman + 0.001*woods + 0.001*timeout + 0.001*reggae + 0.000*needed + 0.000*Dexter + 0.000*3-to-1 + 0.000*Shocked + 0.000*romped + 0.000*1,800
2014-11-07 19:32:16,393 : INFO : topic #80 (0.010): 0.043*woods + 0.042*Neeman + 0.037*needed + 0.029*timeout + 0.024*Shocked + 0.017*nonsensical + 0.015*reggae + 0.014*Dexter + 0.011*1,800 + 0.011*3-to-1
2014-11-07 19:32:16,407 : INFO : topic #58 (0.010): 0.029*Neeman + 0.027*needed + 0.019*woods + 0.019*timeout + 0.013*Dexter + 0.010*Shocked + 0.008*nonsensical + 0.008*mid-week + 0.007*reggae + 0.007*3-to-1
 2014-11-07 19:32:16,421 : INFO : topic #79 (0.010): 0.002*woods + 0.002*needed + 0.002*sustaining + 0.001*Neeman + 0.001*timeout + 0.001*Godchaux + 0.001*Dexter + 0.001*dozen + 0.001*rumor + 0.001*Miami-based
 2014-11-07 19:32:16,437 : INFO : topic diff=78.067282, rho=1.000000  
 2014-11-07 19:32:16,523 : INFO : PROGRESS: pass 0, at document #2000/2250
 2014-11-07 19:32:16,523 : DEBUG : performing inference on a chunk of 1000 documents
 2014-11-07 19:32:22,841 : DEBUG : 38/1000 documents converged within 50 iterations
 2014-11-07 19:32:22,862 : DEBUG : updating topics
 2014-11-07 19:32:22,919 : INFO : merging changes from 1000 documents into a model of     
 2250 documents
 2014-11-07 19:32:23,640 : INFO : topic #63 (0.010): 0.017*Neeman + 0.016*needed + 0.014*autobiography + 0.013*teacher + 0.012*woods + 0.011*Mauritius + 0.010*Shocked + 0.009*timeout + 0.007*mid-week + 0.007*CFC
 2014-11-07 19:32:23,654 : INFO : topic #8 (0.010): 0.028*Neeman + 0.027*woods + 0.024*Shocked + 0.023*needed + 0.016*timeout + 0.013*Dexter + 0.010*reggae + 0.010*nonsensical + 0.007*Mauritius + 0.007*65-plus
 2014-11-07 19:32:23,669 : INFO : topic #85 (0.010): 0.054*needed + 0.041*woods + 0.036*Neeman + 0.036*Shocked + 0.031*timeout + 0.023*nonsensical + 0.016*reggae + 0.014*Dexter + 0.011*3-to-1 + 0.011*crouch
 2014-11-07 19:32:23,683 : INFO : topic #18 (0.010): 0.017*needed + 0.013*woods + 0.011*Neeman + 0.010*timeout + 0.009*Shocked + 0.008*reggae + 0.008*Guttierez + 0.006*livid + 0.006*Vermont + 0.006*Dexter
  2014-11-07 19:32:23,697 : INFO : topic #49 (0.010): 0.028*needed + 0.028*Neeman + 0.026*woods + 0.024*timeout + 0.019*Dexter + 0.017*nonsensical + 0.012*reggae + 0.009*Shocked + 0.007*3-to-1 + 0.007*crouch
 2014-11-07 19:32:23,712 : INFO : topic #53 (0.010): 0.035*mid-week + 0.034*Mauritius + 0.028*Neeman + 0.028*needed + 0.027*woods + 0.024*Tourism + 0.023*macho + 0.014*Shocked + 0.013*nonsensical + 0.012*timeout
 2014-11-07 19:32:23,726 : INFO : topic #32 (0.010): 0.071*Neeman + 0.031*woods + 0.022*needed + 0.015*timeout + 0.013*Shocked + 0.012*Dexter + 0.012*Mauritius + 0.009*nonsensical + 0.009*mid-week + 0.007*reggae
 2014-11-07 19:32:23,740 : INFO : topic #78 (0.010): 0.040*needed + 0.039*Neeman + 0.039*woods + 0.019*timeout + 0.019*Shocked + 0.017*Dexter + 0.017*reggae + 0.016*nonsensical + 0.015*3-to-1 + 0.011*mid-week
 2014-11-07 19:32:23,754 : INFO : topic #94 (0.010): 0.002*needed + 0.002*Neeman + 0.001*woods + 0.001*timeout + 0.001*Dexter + 0.001*nonsensical + 0.001*Shocked + 0.001*3-to-1 + 0.001*reggae + 0.000*dozen
 2014-11-07 19:32:23,768 : INFO : topic #17 (0.010): 0.023*needed + 0.022*woods + 0.018*Neeman + 0.018*timeout + 0.012*1,800 + 0.011*reggae + 0.011*Shocked + 0.010*Dexter + 0.008*Anderson + 0.007*Bovek
 2014-11-07 19:32:23,781 : INFO : topic #73 (0.010): 0.001*woods + 0.001*timeout + 0.001*Neeman + 0.001*Shocked + 0.001*reggae + 0.001*Falcon + 0.001*Dexter + 0.001*needed + 0.001*dozes + 0.001*dozen
 2014-11-07 19:32:23,796 : INFO : topic #96 (0.010): 0.049*Neeman + 0.048*needed + 0.043*woods + 0.025*timeout + 0.019*nonsensical + 0.018*Shocked + 0.015*Dexter + 0.013*3-to-1 + 0.011*reggae + 0.010*crouch
 2014-11-07 19:32:23,810 : INFO : topic #46 (0.010): 0.042*needed + 0.035*Neeman + 0.033*woods + 0.025*Shocked + 0.019*3-to-1 + 0.016*reggae + 0.016*timeout + 0.013*Dexter + 0.012*nonsensical + 0.011*Mauritius
 2014-11-07 19:32:23,824 : INFO : topic #39 (0.010): 0.002*needed + 0.002*Neeman + 0.001*woods + 0.001*Shocked + 0.001*Dexter + 0.001*mid-week + 0.001*timeout + 0.001*18th + 0.001*nonsensical + 0.001*Mauritius
 2014-11-07 19:32:23,838 : INFO : topic #66 (0.010): 0.036*Neeman + 0.032*woods + 0.021*needed + 0.020*timeout + 0.019*Shocked + 0.019*Blank + 0.014*cares + 0.013*Dexter + 0.011*reggae + 0.010*nonsensical
 2014-11-07 19:32:23,855 : INFO : topic diff=5.923197, rho=0.707107
 2014-11-07 19:32:24,260 : DEBUG : bound: at document #0
</code></pre>

<p>And then we get the error message I posted above.</p>
","3113501","","","","","2021-09-19 18:40:52","Index Error when running LDA in gensim","<python><lda><topic-modeling><gensim>","2","0","","","","CC BY-SA 3.0"
"61736874","1","61741677","","2020-05-11 18:38:20","","0","338","<p>I have two corpora - one with all women leader speeches and the other with men leader speeches. I would like to test the hypothesis that cosine similarity between two words in the one corpus is significantly different than cosine similarity between the same two words in another corpus. Is such a t-test (or equivalent) logical and possible?</p>

<p>Further, if the cosine similarities are different across the two corpora, how could I examine if cosine similarity between the same two words in a third corpus is more similar to the first or the second corpus?</p>
","7426950","","","","","2020-05-12 00:24:17","How to compare cosine similarities across three pretrained models?","<nlp><gensim><word2vec><word-embedding><glove>","1","0","1","","","CC BY-SA 4.0"
"44732303","1","44846497","","2017-06-24 02:56:11","","8","9019","<p>I am trying to build a Docker application that uses Python's gensim library, version 2.1.0, which is being installed via pip from a requirements.txt file.</p>

<p>However, Docker seems to have trouble installing numpy, scipy, and gensim. I googled the error messages and found other users who experienced the same problem, but in other environments. Many of their solutions do not seem to work in Docker.</p>

<p>The following is the error message:</p>

<pre><code>&lt;pre&gt; Step 4 : RUN pip install -r requirements.txt
 ---&gt; Running in a86d07e229d7
Collecting Flask==0.12 (from -r requirements.txt (line 1))
  Downloading Flask-0.12-py2.py3-none-any.whl (82kB)
Collecting requests==2.17.3 (from -r requirements.txt (line 2))
  Downloading requests-2.17.3-py2.py3-none-any.whl (87kB)
Collecting numpy==1.12.1 (from -r requirements.txt (line 3))
  Downloading numpy-1.12.1.zip (4.8MB)
Collecting nltk==3.2.2 (from -r requirements.txt (line 4))
  Downloading nltk-3.2.2.tar.gz (1.2MB)
Collecting scipy==0.19.0 (from -r requirements.txt (line 5))
  Downloading scipy-0.19.0.zip (15.3MB)
    Complete output from command python setup.py egg_info:
    /bin/sh: svnversion: not found
    /bin/sh: svnversion: not found
    non-existing path in 'numpy/distutils': 'site.cfg'
    Could not locate executable gfortran
    Could not locate executable f95
    Could not locate executable ifort
    Could not locate executable ifc
    Could not locate executable lf95
    Could not locate executable pgfortran
    Could not locate executable f90
    Could not locate executable f77
    Could not locate executable fort
    Could not locate executable efort
    Could not locate executable efc
    Could not locate executable g77
    Could not locate executable g95
    Could not locate executable pathf95
    don't know how to compile Fortran code on platform 'posix'
    Running from numpy source directory.
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py:367: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates
      run_build = parse_setuppy_commands()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Atlas (http://math-atlas.sourceforge.net/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [atlas]) or by setting
        the ATLAS environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Blas (http://www.netlib.org/blas/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [blas]) or by setting
        the BLAS environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Blas (http://www.netlib.org/blas/) sources not found.
        Directories to search for the sources can be specified in the
        numpy/distutils/site.cfg file (section [blas_src]) or by setting
        the BLAS_SRC environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Lapack (http://www.netlib.org/lapack/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [lapack]) or by setting
        the LAPACK environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Lapack (http://www.netlib.org/lapack/) sources not found.
        Directories to search for the sources can be specified in the
        numpy/distutils/site.cfg file (section [lapack_src]) or by setting
        the LAPACK_SRC environment variable.
      self.calc_info()
    /usr/local/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'define_macros'
      warnings.warn(msg)
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 158, in save_modules
        yield saved
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 199, in setup_context
        yield
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 254, in run_setup
        _execfile(setup_script, ns)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 48, in _execfile
        exec(code, globals, locals)
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 392, in &lt;module&gt;
        # higher up in this file.
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 384, in setup_package
        if ""--force"" in sys.argv:
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/core.py"", line 169, in setup
      File ""/usr/local/lib/python3.5/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/bdist_egg.py"", line 152, in run
        self.run_command(""egg_info"")
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/egg_info.py"", line 26, in run
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 148, in run
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 159, in build_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 294, in build_library_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 377, in generate_sources
      File ""numpy/core/setup.py"", line 674, in get_mathlib_info
    RuntimeError: Broken toolchain: cannot link a simple C program

    During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/pip-build-j8py_tat/scipy/setup.py"", line 416, in &lt;module&gt;
        setup_package()
      File ""/tmp/pip-build-j8py_tat/scipy/setup.py"", line 412, in setup_package
        setup(**metadata)
      File ""/usr/local/lib/python3.5/distutils/core.py"", line 108, in setup
        _setup_distribution = dist = klass(attrs)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/dist.py"", line 320, in __init__
        self.fetch_build_eggs(attrs['setup_requires'])
      File ""/usr/local/lib/python3.5/site-packages/setuptools/dist.py"", line 377, in fetch_build_eggs
        replace_conflicting=True,
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 852, in resolve
        dist = best[req.key] = env.best_match(req, ws, installer)
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1124, in best_match
        return self.obtain(req, installer)
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1136, in obtain
        return installer(requirement)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/dist.py"", line 445, in fetch_build_egg
        return cmd.easy_install(req)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 673, in easy_install
        return self.install_item(spec, dist.location, tmpdir, deps)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 699, in install_item
        dists = self.install_eggs(spec, download, tmpdir)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 880, in install_eggs
        return self.build_and_install(setup_script, setup_base)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1119, in build_and_install
        self.run_setup(setup_script, setup_base, args)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1105, in run_setup
        run_setup(setup_script, args)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 257, in run_setup
        raise
      File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
        self.gen.throw(type, value, traceback)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 199, in setup_context
        yield
      File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
        self.gen.throw(type, value, traceback)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 170, in save_modules
        saved_exc.resume()
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 145, in resume
        six.reraise(type, exc, self._tb)
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/_vendor/six.py"", line 685, in reraise
        raise value.with_traceback(tb)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 158, in save_modules
        yield saved
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 199, in setup_context
        yield
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 254, in run_setup
        _execfile(setup_script, ns)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 48, in _execfile
        exec(code, globals, locals)
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 392, in &lt;module&gt;
        # higher up in this file.
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 384, in setup_package
        if ""--force"" in sys.argv:
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/core.py"", line 169, in setup
      File ""/usr/local/lib/python3.5/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/bdist_egg.py"", line 152, in run
        self.run_command(""egg_info"")
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/egg_info.py"", line 26, in run
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 148, in run
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 159, in build_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 294, in build_library_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 377, in generate_sources
      File ""numpy/core/setup.py"", line 674, in get_mathlib_info
    RuntimeError: Broken toolchain: cannot link a simple C program
    /bin/sh: gcc: not found
    /bin/sh: gcc: not found

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-j8py_tat/scipy/
Removing intermediate container a86d07e229d7
The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1 &lt;/pre&gt;
</code></pre>

<p>I'm using the <code>python:3.5-alpine</code> image. The versions of the packages are <code>numpy==1.12.1</code>, <code>scipy==0.19.0</code>, and <code>gensim==2.1.0</code>.</p>
","3180238","","","","","2017-06-30 12:28:01","Docker unable to install numpy, scipy, or gensim","<python><numpy><docker><scipy><gensim>","1","4","","","","CC BY-SA 3.0"
"44781047","1","44789327","","2017-06-27 13:04:51","","10","11937","<p>I'm following the 'English Wikipedia' gensim tutorial at <a href=""https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""noreferrer"">https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation</a></p>

<p>where it explains that tf-idf is used during training (at least for LSA, not so clear with LDA).</p>

<p>I expected to apply a tf-idf transformer to new documents, but instead, at the end of the tut, it suggests to simply input a bag-of-words.</p>

<pre><code>doc_lda = lda[doc_bow]
</code></pre>

<p>Does LDA require bag-of-words vectors only?</p>
","314631","","","","","2018-08-20 10:01:09","Necessary to apply TF-IDF to new documents in gensim LDA model?","<gensim>","2","1","4","","","CC BY-SA 3.0"
"36001230","1","","","2016-03-15 01:42:14","","9","1770","<p>I know that there exists already an implementation of PV-DBOW (paragraph vector) in python (gensim).
But I'm interested in knowing how to implement it myself.
The explanation from the <a href=""https://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">official paper</a> for PV-DBOW is as follows:</p>

<blockquote>
  <p>Another way is to ignore the context words in the input, but force the model to predict words randomly sampled from the paragraph in the output. In reality, what this means is that at each iteration of stochastic gradient descent, we sample a text window, then sample a random word from the text window and form a classification task given the Paragraph Vector.</p>
</blockquote>

<p>According to the paper the word vectors are not stored
and PV-DBOW is said to work similar to skip gram in word2vec.</p>

<p>Skip-gram is explained in <a href=""http://arxiv.org/pdf/1411.2738v3.pdf"" rel=""nofollow noreferrer"">word2vec Parameter Learning</a>.
In the skip gram model the word vectors are mapped to the hidden layer.
The matrix that performs this mapping is updated during the training.
In PV-DBOW the dimension of the hidden layer should be the dimension of one paragraph vector. When I want to multiply the word vector of a sampled example with the paragraph vector they should have the same size.
The original representation of a word is of size (vocabulary size x 1). Which mapping is performed to get the right size (paragraph dimension x 1)
in the hidden layer. And how is this mapping performed when the word vectors are not stored?
I assume that word and paragraph representation should have the same size in the hidden layer because of equation 26 in <a href=""http://arxiv.org/pdf/1411.2738v3.pdf"" rel=""nofollow noreferrer"">word2vec Parameter Learning</a></p>
","4868210","","4868210","","2020-01-21 18:34:01","2020-01-21 18:34:01","doc2vec: How is PV-DBOW implemented","<machine-learning><nlp><neural-network><gensim><word2vec>","1","2","4","","","CC BY-SA 4.0"
"53130024","1","","","2018-11-03 09:26:21","","0","170","<p>I am trying to train <code>Gensim</code> library's <code>word2vec</code> on <code>NLTK</code> <code>Brown Corpus</code>, But facing issues while setting the path to the corpus</p>

<p><strong>Code</strong></p>

<pre><code>from gensim.models import word2vec

sentences = word2vec.BrownCorpus('/nltk_data/corpora/brown')

model = word2vec.Word2Vec(sentences, min_count=1)
</code></pre>

<p><strong>Error</strong></p>

<blockquote>
  <p>FileNotFoundError: [Errno 2] No such file or directory: '/nltk_data/corpora/brown'</p>
</blockquote>

<p>I have checked, Brown corpus data is present at the above path.</p>

<p>I know there is one another way to train Gensim word2vec on NLTK Brown Corpus as follows, But I want to know why the above method doesn't work</p>

<pre><code>from gensim.models import Word2Vec
from nltk.corpus import brown

sentences = brown.sents()

model = Word2Vec(sentences, min_count=1)
</code></pre>

<p>Feel free to drop any thoughts</p>
","653397","","","","","2018-11-03 09:26:21","Trouble training Gensim word2vec on NLTK Brown Corpus","<python><nltk><gensim><word2vec>","0","4","","","","CC BY-SA 4.0"
"44150201","1","","","2017-05-24 06:06:14","","0","1015","<p>My question concerns the proper training of the model for unique and really specific use of the Word2Vec model. <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">See Word2Vec details here</a></p>

<p>I am working on identifying noun-adjective (or ) relationships within the word embeddings.</p>

<p>(E.g. we have 'nice car' in a sentence of the data-set. Given the word embeddings of the corpus and the nouns and adjectives all labeled, I am trying to design a technique to find the proper vector that connects 'nice' with 'car'.)</p>

<p>Of course I am not trying to connect only that pair of words, but the technique should would for all  relationships. A supervised approach is taken at this moment, then try to work towards designing an unsupervised method.</p>

<p>Now that you understand what I am trying to do, I will explain the problem. I obviously know that word2vec needs to be trained on large amounts of data, to learn the proper embeddings as accurately as possible, but I am afraid to give it more data than the data-set with labelled  sentences (500-700).</p>

<p>I am afraid that if I give it more data to train on (e.g. Latest Wikipedia dump data-set), it will learn better vectors, but the extra data will influence the positioning of my  words, then this word relationship is biased by the extra training data. (e.g. what if there is also 'nice Apple' in the extra training data, then the positioning of the word 'nice' could be compromised).</p>

<p>Hopefully this makes sense and I am not making bad assumptions, but I am just in the dilemma of having bad vectors because of not enough training data, or having good vectors, but compromised  vector positioning in the word embeddings.</p>

<p>What would be the proper way to train on ? As much training data as possible (billions of words) or just the labelled data-set (500-700 sentences) ?</p>

<p>Thank you kindly for your time, and let me know if anything that I explained does not make sense.</p>
","2949252","","","","","2017-05-24 11:04:27","how to train Word2Vec model properly for a special purpose","<vector><deep-learning><gensim><word2vec><word-embedding>","1","5","1","","","CC BY-SA 3.0"
"27220927","1","27261545","","2014-12-01 02:40:15","","3","1441","<p>My term-document matrix is in a numpy matrix format, and I have a dictionary to represent the  of the term-document matrix.</p>

<p>Is there any way I can easily pass these two into Gensim's LDA model?</p>

<pre><code>tdMatrix = np.load('tdmatrix.npy')
dictionary = cPickle.load(open('dictionary.p', 'r')) # stores term represented by each column
</code></pre>

<p>Can I pass this somewhow to gensim.models.ldamodel.LDA?</p>
","3773235","","","","","2014-12-09 12:12:32","Passing Term-Document Matrix to Gensim LDA Model","<python><numpy><machine-learning><nlp><gensim>","2","0","","","","CC BY-SA 3.0"
"35616088","1","54973464","","2016-02-25 00:40:27","","11","3518","<p>I am looking at the <code>DeepDist</code> (<a href=""http://deepdist.com"" rel=""nofollow noreferrer"">link</a>) module and thinking to combine it with <code>Gensim</code>'s <code>Doc2Vec</code> API to train paragraph vectors on <code>PySpark</code>. The link actually provides with the following clean example for how to do it for <code>Gensim</code>'s <code>Word2Vec</code> model:</p>

<pre class=""lang-py prettyprint-override""><code>from deepdist import DeepDist
from gensim.models.word2vec import Word2Vec
from pyspark import SparkContext

sc = SparkContext()
corpus = sc.textFile('enwiki').map(lambda s: s.split())

def gradient(model, sentences):  # executes on workers
    syn0, syn1 = model.syn0.copy(), model.syn1.copy()   # previous weights
    model.train(sentences)
    return {'syn0': model.syn0 - syn0, 'syn1': model.syn1 - syn1}

def descent(model, update):      # executes on master
    model.syn0 += update['syn0']
    model.syn1 += update['syn1']

with DeepDist(Word2Vec(corpus.collect()) as dd:
    dd.train(corpus, gradient, descent)
    print dd.model.most_similar(positive=['woman', 'king'], negative=['man']) 
</code></pre>

<p>To my understanding, <code>DeepDist</code> is distributing the work of gradient descent into workers in batches, and the recombining them and updating at master. If I replace <code>Word2Vec</code> with <code>Doc2Vec</code>, there should be the document vectors that are being trained with the word vectors.</p>

<p>So I looked into the source code of <code>gensim.models.doc2vec</code> (<a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/doc2vec.py"" rel=""nofollow noreferrer"">link</a>). There are the following fields in the <code>Doc2Vec</code> model instance:</p>

<ol>
<li><code>model.syn0</code></li>
<li><code>model.syn0_lockf</code></li>
<li><code>model.docvecs.doctag_syn0</code></li>
<li><code>model.docvecs.doctag_syn0_lockf</code></li>
</ol>

<p>Comparing with the source code of <code>gensim.models.word2vec</code> (<a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">link</a>), the following fields went missing in <code>Doc2Vec</code> model:</p>

<ol start=""3"">
<li><code>model.syn1</code></li>
<li><code>model.syn1neg</code></li>
</ol>

<p>I think I do not touch the <code>lockf</code> vectors because they seem to be used after the training is done when new data points come in. Therefore my code should be something like </p>

<pre class=""lang-py prettyprint-override""><code>from deepdist import DeepDist
from gensim.models.doc2vec import Doc2Vec, LabeledSentence
from pyspark import SparkContext

sc = SparkContext()

# assume my dataset is in format 10-char-id followed by doc content
# 1 line per doc
corpus = sc.textFile('data_set').map(
    lambda s: LabeledSentence(words=s[10:].split(),labels=s[:10])
)

def gradient(model, sentence):  # executes on workers
    syn0, doctag_syn0 = model.syn0.copy(), model.docvecs.doctag_syn0.copy()   # previous weights
    model.train(sentence)
    return {'syn0': model.syn0 - syn0, 'doctag_syn0': model.docvecs.doctag_syn0 - doctag_syn0}

def descent(model, update):      # executes on master
    model.syn0 += update['syn0']
    model.docvecs.doctag_syn0 += update['doctag_syn0']

with DeepDist(Doc2Vec(corpus.collect()) as dd:
    dd.train(corpus, gradient, descent)
    print dd.model.most_similar(positive=['woman', 'king'], negative=['man']) 
</code></pre>

<p>Am I missing anything important here? For example:</p>

<ol>
<li>Should I care about <code>model.syn1</code> at all? What do they mean after all?</li>
<li>Am I right that <code>model.*_lockf</code> is the locked matrices after training?</li>
<li>Is it ok that I use <code>lambda s: LabeledSentence(words=s[10:].split(),labels=s[:10]</code> to parse my dataset, assuming I have each document in one line, prefixed by a 0-padded 10-digit id?</li>
</ol>

<p>Any suggestion/contribution are very appreciated. I will write up a blog post to summarize the result, mentioning contributors here, potentially to help others train Doc2Vec models on scaled distributed systems without spending much dev time trying to solve what I am solving now.</p>

<p>Thanks</p>

<hr>

<p>Update 06/13/2018</p>

<p>My apologies as I did not get to implement this. But there are better options nowaday, and <code>DeepDist</code> haven't been maintained for awhile now. Please read comment below.</p>

<p>If you insist on trying out my idea at the moment, be reminded you are proceeding with your own risk. Also, if someone knows that <code>DeepDist</code> still works, please report back in comments. It would help other readers. </p>
","2593536","","2593536","","2018-06-13 07:40:52","2019-03-03 20:34:51","Doc2Vec and PySpark: Gensim Doc2vec over DeepDist","<apache-spark><pyspark><gensim><word2vec>","1","3","5","","","CC BY-SA 4.0"
"36780138","1","","","2016-04-21 20:47:16","","2","3546","<p>So,I'm trying to learn and understand Doc2Vec. 
I'm following this <a href=""http://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow"">tutorial</a>. My input is a list of documents i.e list of lists of words. This is what my code looks like:  </p>

<pre><code>    input = [[""word1"",""word2"",...""wordn""],[""word1"",""word2"",...""wordn""],...] 

    documents = TaggedLineDocument(input)

    model = doc2vec.Doc2Vec(documents,size = 50, window = 10, min_count = 2, workers=2) 
</code></pre>

<p>But I am getting some unicode error(tried googling this error, but no good ):</p>

<pre><code>   TypeError('don\'t know how to handle uri %s' % repr(uri))
</code></pre>

<p>Can somebody please help me understand where i am going wrong ? Thank you ! </p>
","4351167","","","","","2017-09-22 08:06:08","Doc2vec : TaggedLineDocument()","<python><nlp><gensim>","2","0","","","","CC BY-SA 3.0"
"19522258","1","19522349","","2013-10-22 15:34:33","","0","3122","<p>When trying to install gensim (with pip install and setup install), it gives me this error:</p>

<pre><code>Traceback (most recent call last):
  File ""setup.py"", line 19, in &lt;module&gt;
    import ez_setup
  File ""C:\Users\User\Desktop\gensim-0.8.7\ez_setup.py"", line 106
    except pkg_resources.VersionConflict, e:
                                        ^
SyntaxError: invalid syntax
</code></pre>

<p>How can I solve this</p>
","1680859","","","","","2013-10-22 15:54:18","Can't install gensim","<python><gensim>","1","0","","","","CC BY-SA 3.0"
"52895284","1","","","2018-10-19 15:13:31","","0","205","<p>I am using gensim LDA for topic modelling.
I need to get <strong>the topic distribution of a corpus</strong>, not the individual documents.
Let say I have 1000 documents, which belongs to 10 different categories (let say 100 docs for each category).
After training the LDA model overall 1000 documents, then I want to see what are <strong>the dominant topics</strong> of each category. The following image illustrates my dataset and aim.</p>

<h2><a href=""https://i.stack.imgur.com/BRqsB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BRqsB.png"" alt=""enter image description here""></a></h2>

<p>So far I can think of two approaches, but I am not sure either is sane, I will be happy to know if there is a better way of doing it.</p>

<p>In the first approach, I can concatenate the documents of each category into one large document. So there will be only 10 large documents, hence for each document, I will be able to retrieve its topic distribution. </p>

<p>Another approach might be getting the topic distribution of all document, without concatenating documents. Hence for each category, we will have 100 documents topic distributions. To get the dominant topics for each category, I may sum the probability of each topic, and get only a few highest scored topics.
I am not sure any of this approaches are right, what would you suggest?</p>
","2234161","","","","","2018-10-21 17:50:12","Overall topic distribution of a corpus, not individual documents","<nlp><data-science><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"44323816","1","","","2017-06-02 08:05:15","","0","443","<p>I have about 30 word2vec models. When loading them in a python script each consumes a few GB of RAM so it is impossible to use all of them at once. Is there any way to use the models without loading the complete model into RAM?</p>
","407873","","","","","2017-06-02 08:30:22","word2vec - reduce RAM consumption when loading model","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"53353978","1","","","2018-11-17 17:57:54","","2","828","<p>I am new to using word embedding and want to know how i can project my model in Tensorflow. I was looking at the tensorflow website and it only accepts tsv file (vector/metadata), but don't know how to generate the required tsv files. I have tried looking it up and can't find any solutions regrading this. Will I try saving my model in a tsv file format, will i need to do some transformations? Any help will be appreciated.</p>

<p>I have saved my model as the following files, and just load it up when I need to use it:</p>

<blockquote>
  <p>word2vec.model</p>
  
  <p>word2vec.model.wv.vectors.npy</p>
</blockquote>
","9690788","","","","","2018-11-17 18:44:41","How to project my word2vec model in Tensorflow","<tensorflow><gensim><word2vec><tensorboard>","1","0","1","","","CC BY-SA 4.0"
"55373509","1","","","2019-03-27 09:15:34","","0","87","<p>I'm trying to implement <code>word2vec</code> in python to score the trained Skip-gram model on a pair of words. but I can't figure out the error:</p>

<blockquote>
  <p>only integers, slices (<code>:</code>), ellipsis (<code>...</code>), numpy.newaxis (<code>None</code>) and integer or boolean arrays are valid indices.</p>
</blockquote>

<p>Here is the code I tried :</p>

<pre><code>model = Word2Vec.load(r""C:\Users\Lenovo\model\word2vecforlaw.model"")
z=gensim.models.word2vec.score_sg_pair(model, ""patent"", ""law"")
print(z)
</code></pre>
","11238748","","5770501","","2019-03-27 10:05:44","2019-03-27 21:01:50","using word2vec.score.sg pair() raises Python error - only integers.....integer or boolean arrays are valid indices","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"55978013","1","","","2019-05-03 23:01:20","","5","8279","<p>I'm using the library node2vec, which is based on gensim word2vec model to encode nodes in an embedding space, but when i want to fit the word2vec object I get this warning:</p>

<blockquote>
  <p>C:\Users\lenovo\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py:743:
  UserWarning: C extension not loaded, training will be slow. Install a
  C compiler and reinstall gensim for fast training.</p>
</blockquote>

<p>Can any one help me to fix this issue please ?</p>
","11450077","","5167180","","2019-05-04 04:08:30","2019-10-25 07:18:00","How to fix 'C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.'","<python-3.x><jupyter-notebook><anaconda><gensim><word2vec>","4","7","","","","CC BY-SA 4.0"
"55376622","1","","","2019-03-27 11:56:23","","2","69","<p>I am trying to train a doc2vec model on a corpus of six novels and I need to build the corpus of Tagged Documents. 
Each novel is a txt file, already preprocessed and read into python using the <code>read()</code> method, so that it appears as a ""<code>long string</code>"". If I try to tag each novel using TaggedDocument form gensim, each novel gets only one tag, and the corpus of tagged documents has only six elements (which is not enough to train the doc2vec model). </p>

<p>I have been suggested to split each novel into sentences, then assign each sentence one tag for the ID of the sentence, and then one tag for the ID of the book it belongs to. I am, however, in trouble since I do not know how to structure the code.</p>

<p>This was the first code, i.e. the one using each novel in the format of a ""<code>long string</code>"":</p>

<pre><code>    `documents=[emma_text, persuasion_text, prideandprejudice_text,   
     janeeyre_text, shirley_text, professor_text] 
     corpus=[]`

    `for docid, document in enumerate(documents):
         corpus.append(TaggedDocument(document.split(), tags=
         [""{0:0&gt;4}"".format  
         (docid)]))`    

     `d2v_model = Doc2Vec(vector_size=100, 
                window=15,
                hs=0,
                sample=0.000001,
                min_count=100,
                workers=-1,
                epochs=500,
                dm=0, 
                dbow_words=1) 

    d2v_model.build_vocab(corpus)`

    `d2v_model.train(corpus, total_examples=d2v_model.corpus_count,    
     epochs=d2v_model.epochs)`
</code></pre>

<p>This, however, means that my corpus of tagged documents has only six elements and that my model has not enough elements on which to train. If for instance I try to apply the <code>.most_similar</code> method to a target book, I get completely wrong results</p>

<p>To sum up, I need help to assign each sentence of each book (I have already split the books into sentences) one tag for the ID of the sentence and one tag for the ID of the book it belongs to, using TaggedDocument to build the corpus on which I will train my model.</p>

<p>Thanks for the attention!</p>
","11264898","","6811216","","2019-03-27 14:18:02","2019-03-27 14:18:02","Doc2vec on a corpus of novels: how do I assign to each sentence of a novel one tag for the ID of the sentence and one tag for the ID of the book?","<python><gensim><doc2vec>","0","1","","","","CC BY-SA 4.0"
"53694381","1","","","2018-12-09 16:36:00","","2","1786","<p>I want to learn bigrams from a corpus using gensim, and then just print the bigrams learned. i've not seen an example that does this.
help appreciated</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream)

# how can I print all bigrams learned and just the bigrams, including ""new_york"" and ""human computer"" ?enter code here
</code></pre>
","52917","","52917","","2018-12-09 16:43:54","2020-02-13 20:34:21","print bigrams learned with gensim","<python><gensim><n-gram><topic-modeling><phrase>","2","0","","","","CC BY-SA 4.0"
"55424127","1","","","2019-03-29 19:10:45","","1","70","<p>I have a data-set that contains search queries terms and the ids of the users that typed/searched for those queries.
I want to get a user-embedding (user2vec) sort of thing to learn to  the type of thing the users search for.</p>

<p>Previously, I merged all the queries for each user id and tokenized it and tagged it using <code>gensim.doc2vec</code> Tagged function.</p>

<p>when, I tried using <code>gensim.doc2vec</code> to learn the embedding i kept getting memory error each time I run it.</p>

<p>I want to do it this time with Keras but don't know how input my data and how to do it.</p>

<p>I want to output to show a matrix <code>m</code> by <code>n</code>
where <code>m</code>, is the number of users 
<code>n</code>, is the dimension of the embedding(30)</p>
","10382230","","5786339","","2019-03-29 19:58:37","2019-07-11 14:09:26","How to input the data set in for doc2vec in Keras and train","<keras><out-of-memory><gensim><word-embedding><doc2vec>","0","0","","","","CC BY-SA 4.0"
"55393311","1","","","2019-03-28 08:44:39","","2","506","<p>Gensim uses text streaming to minimize memory requirements. This is at the cost of performance due to endless disk IO. Is there a trick to on the fly copy the complete file from disk (one disk IO) to a temporary in-memory file?
I like to keep the code as is (no recoding into a list structures), but this is not a great way of debugging functionality</p>

<p>Expected result: much faster code</p>

<h3>Some more background on the question</h3>

<p>The original code is at <a href=""https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb"" rel=""nofollow noreferrer"">https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb</a>. The example code is taken from the phrase modelling section</p>

<p>I'm calculating the unigrams. All reviews are at  </p>

<pre><code>review_txt_filepath = os.path.join(intermediate_directory,'review_text_all.txt'),
</code></pre>

<p>all unigrams should go to</p>

<pre><code>unigram_sentences_filepath = os.path.join(intermediate_directory, 'unigram_sentences_all.txt') 
</code></pre>

<p>The crucial routines are</p>

<pre><code>def punct_space(token):
    return token.is_punct or token.is_space

def line_review(filename):
    # generator function to read in reviews from the file
    with codecs.open(filename, encoding='utf_8') as f:
        for review in f:
            yield review.replace('\\n', '\n')

def lemmatized_sentence_corpus(filename):
    # generator function to use spaCy to parse reviews, lemmatize the text, and yield sentences

    for parsed_review in nlp.pipe(line_review(filename),
                              batch_size=10000, n_threads=4):
        for sent in parsed_review.sents:
            yield u' '.join([token.lemma_ for token in sent
                             if not punct_space(token)])
</code></pre>

<p>The unigrams are calculated as</p>

<pre><code>with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:
    for sentence in lemmatized_sentence_corpus(review_txt_filepath):
        f.write(sentence + '\n')
</code></pre>

<p>Doing this for 5000 lines requires some patience, 1h30m ;-)</p>

<p>I'm not that familiar with iterables, but do I understand it correctly that I first have to read the 
actual file (on disc) into a variable ""list_of_data"" and process that</p>

<pre><code>with (review_txt_filepath, 'r', encoding='utf_8') as f:
    list_of_data = f.read()

with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:
    for sentence in lemmatized_sentence_corpus(list_of_data):
        f.write(sentence + '\n')
</code></pre>

<p>So the strategy is</p>

<pre><code>1. read all data into a list in memory
2. process the data
3. write the results to disc
4. delete the list from memory by setting list_with_data = ()
</code></pre>

<p>A problem with this is obviously that line_review is doing the file reading               </p>
","9165100","","9165100","","2019-04-01 19:38:14","2019-04-01 19:38:14","Text streaming in Gensim","<inputstream><gensim>","1","0","","","","CC BY-SA 4.0"
"47060859","1","","","2017-11-01 17:51:04","","1","64","<p>I have a corpus file which was made using <code>dictionary.doc2bow</code> function of gensim but I have lost the dictionary file. Is there any way I can get the dictionary file by doing something on the corpus file using gensim</p>
","5282599","","4685471","","2017-11-02 11:08:36","2017-11-02 11:08:36","I have a corpus file which was made by doc2bow function of gensim but I have lost the dictionary file. How do I get the dictionary file back","<machine-learning><nlp><gensim>","0","0","","","","CC BY-SA 3.0"
"36315770","1","","","2016-03-30 17:31:01","","1","1034","<p>May we use Word2Vec for implementing Parts of Speech(PoS) Tagging, or similar other problems of label sequencing. 
I feel we may address this problem from clustering, so I was curious. 
Generally any good lead or discussion would be nice, but a Python example preferably with Gensim learn may be great, as it may have a Word2Vec implementation.  </p>
","1684021","","","","","2016-03-30 17:31:01","Word2Vec for PoS Tagging","<machine-learning><nlp><nltk><gensim><word2vec>","0","3","","","","CC BY-SA 3.0"
"64754994","1","","","2020-11-09 15:59:21","","1","158","<p>I need to return a text that contain the keyword. Let's consider the following example:</p>
<pre><code>keyword = &quot;configure&quot;
texts = [ 
   &quot;The system configuration document should be uploaded to the repository. Please contact the dev team.&quot;,
   &quot;To do the system setup, please follow the instructions.&quot; 
]
</code></pre>
<p>The keyword <code>configure</code> does not appear in any text. But the similar word <code>configuration</code> appears in the first sentence. Therefore the expected output is:</p>
<pre><code>The system configuration document should be uploaded to the repository. Please contact the dev team.
</code></pre>
<p>I know that it's possible to calculate the [semantic similarity between a word and texts][1]. However, it often returns inaccurate results for my case.</p>
<p>Another approach that I was evaluating is to apply stemming and lemmatization. However, <code>configure</code> and <code>configuration</code> have different stems.</p>
<p>Finally I also considered <code>Word2Vec</code> model... However, in this case I'm not sure how to efficiently use this approach.</p>
<pre><code>import gensim.downloader as api

word_vectors = api.load(&quot;glove-wiki-gigaword-100&quot;) 

word_vectors.similarity(&quot;configure&quot;,&quot;configuration&quot;)
</code></pre>
<p>Is there any state-of-the-art approach to deal with my task?
[1]: <a href=""https://medium.com/@adriensieg/text-similarities-da019229c894"" rel=""nofollow noreferrer"">https://medium.com/@adriensieg/text-similarities-da019229c894</a></p>
","11622712","","11622712","","2020-11-10 15:51:56","2020-11-10 15:51:56","How to find the similar sentence based on keyword that does not directly appear in sentences?","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"43942790","1","43960830","","2017-05-12 16:48:23","","2","1225","<p>I am executing the following line:</p>

<pre><code>id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')
</code></pre>

<p>This code is available at <strong>""<a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a>""</strong>. I downloaded the wikipedia corpus and generated the required files and wiki_en_wordids.txt is one of those files. This file is available in the following location:</p>

<pre><code>~/gensim/results/wiki_en
</code></pre>

<p>So when i execute the code mentioned above I get the following error:</p>

<pre><code>Traceback (most recent call last):
          File ""~\Python\Python36-32\temp.py"", line 5, in &lt;module&gt;
            id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')
          File ""~\Python\Python36-32\lib\site-packages\gensim\corpora\dictionary.py"", line 344, in load_from_text
            with utils.smart_open(fname) as f:
          File ""~\Python\Python36-32\lib\site-packages\smart_open\smart_open_lib.py"", line 129, in smart_open
            return file_smart_open(parsed_uri.uri_path, mode)
          File ""~\Python\Python36-32\lib\site-packages\smart_open\smart_open_lib.py"", line 613, in file_smart_open
            return open(fname, mode)
          FileNotFoundError: [Errno 2] No such file or directory: 'wiki_en_wordids.txt'
</code></pre>

<p>Even though the file is available in the required location I get that error. Should I place the file in any other location? How do I determine what the right location is?</p>
","2585848","","","","","2017-05-14 05:59:53","gensim file not found error","<python><python-3.x><gensim>","1","2","","","","CC BY-SA 3.0"
"36324016","1","","","2016-03-31 04:13:04","","0","1396","<p>I'm totally new in Python and gensim. I'm trying to use word2vec from gensim in Python 3.4 on windows7 (64).</p>

<pre><code>import csv
with open('Data.csv', 'r') as csvfile:
Word2VecTextTrain = csv.reader(csvfile, delimiter=' ')
   from gensim.models import Word2Vec
   model = Word2Vec( Word2VecTextTrain, size=100, window=3, min_count=5, workers=4)
</code></pre>

<p>""Data.csv"" contains 30k rows of texts. These texts are either a complete or incomplete sentences including up to 20 words. Some of them may contain ""/"" or numbers.</p>

<p>I'm facing this error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Home/PycharmProjects/Word2Vec Project/Word2Vec_2016_03_23"", line 26, in &lt;module&gt;
     model = Word2Vec( Word2VecTextTrain, size=100, window=5, min_count=5, workers=4)
   File ""C:\Users\Home\Miniconda3\lib\site-packages\gensim\models\word2vec.py"", line 431, in __init__
     self.build_vocab(sentences, trim_rule=trim_rule)
   File ""C:\Users\Home\Miniconda3\lib\site-packages\gensim\models\word2vec.py"", line 497, in build_vocab
     self.finalize_vocab()  # build tables &amp; arrays
   File ""C:\Users\Home\Miniconda3\lib\site-packages\gensim\models\word2vec.py"", line 625, in finalize_vocab
     self.reset_weights()
   File ""C:\Users\Home\Miniconda3\lib\site-packages\gensim\models\word2vec.py"", line 932, in reset_weights
     self.syn0[i] = self.seeded_vector(self.index2word[i] + str(self.seed))
   File ""C:\Users\Home\Miniconda3\lib\site-packages\gensim\models\word2vec.py"", line 946, in seeded_vector
     once = random.RandomState(uint32(self.hashfxn(seed_string)))
OverflowError: Python int too large to convert to C long

Process finished with exit code 1
</code></pre>

<p>I have no idea for the reason of this error. Any help is truly appreciated.</p>
","3439050","","","","","2016-04-01 04:51:41","Error in performing Word2Vec in Python","<python><gensim><word2vec>","1","0","1","","","CC BY-SA 3.0"
"28034688","1","28152488","","2015-01-19 22:35:40","","0","886","<p>I am trying to install <a href=""http://radimrehurek.com/gensim/index.html"" rel=""nofollow"">Gensim</a> but I am getting the errors displayed below. I am running Anaconda 2.1.0 with Python 2.7.8 and NumPy 1.9.0 on a Windows 8.1 machine. I already have the Windows SDK 8.1.</p>

<p>It says something about a deprecated NumPy version 1.7, which seems odd because I am running NumPy 1.9.0.</p>

<p>I also have a Anaconda3 installation with Python 3.4, though I removed those from my PATH in order to be able to run Python 2 in cmd, because I need to work on a project in Python 2. Running <code>python --version</code> returns ""Python 2.7.8"".</p>

<pre><code>In [9]: %run setup.py install
running install
running bdist_egg
running egg_info
writing requirements to gensim.egg-info\requires.txt
writing gensim.egg-info\PKG-INFO
writing top-level names to gensim.egg-info\top_level.txt
writing dependency_links to gensim.egg-info\dependency_links.txt
reading manifest file 'gensim.egg-info\SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no files found matching '*.sh' under directory '.'
no previously-included directories found matching 'docs\src*'
writing manifest file 'gensim.egg-info\SOURCES.txt'
installing library code to build\bdist.win-amd64\egg
running install_lib
running build_py
running build_ext
building 'gensim.models.word2vec_inner' extension
C:\Program Files (x86)\Haskell Platform\2013.2.0.0\mingw\bin\gcc.exe -DMS_WIN64
-mdll -O -Wall -IC:\Users\Robert-Jan\Downloads\gensim-0.10.3\gensim-0.10.3\gensi
m\models -IC:\Anaconda\include -IC:\Anaconda\PC -IC:\Anaconda\lib\site-packages\
numpy\core\include -c ./gensim/models/word2vec_inner.c -o build\temp.win-amd64-2
.7\Release\.\gensim\models\word2vec_inner.o
In file included from C:\Anaconda\lib\site-packages\numpy\core\include/numpy/nda
rraytypes.h:1804:0,
                 from C:\Anaconda\lib\site-packages\numpy\core\include/numpy/nda
rrayobject.h:17,
                 from C:\Anaconda\lib\site-packages\numpy\core\include/numpy/arr
ayobject.h:4,
                 from ./gensim/models/word2vec_inner.c:232:
C:\Anaconda\lib\site-packages\numpy\core\include/numpy/npy_1_7_deprecated_api.h:
12:9: note: #pragma message: C:\Anaconda\lib\site-packages\numpy\core\include/nu
mpy/npy_1_7_deprecated_api.h(12) : Warning Msg: Using deprecated NumPy API, disa
ble it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
./gensim/models/word2vec_inner.c: In function '__Pyx_RaiseArgtupleInvalid':
./gensim/models/word2vec_inner.c:9761:18: warning: unknown conversion type chara
cter 'z' in format
./gensim/models/word2vec_inner.c:9761:18: warning: format '%.1s' expects type 'c
har *', but argument 5 has type 'Py_ssize_t'
./gensim/models/word2vec_inner.c:9761:18: warning: unknown conversion type chara
cter 'z' in format
./gensim/models/word2vec_inner.c:9761:18: warning: too many arguments for format

./gensim/models/word2vec_inner.c: In function '__Pyx_RaiseTooManyValuesError':
./gensim/models/word2vec_inner.c:10235:18: warning: unknown conversion type char
acter 'z' in format
./gensim/models/word2vec_inner.c:10235:18: warning: too many arguments for forma
t
./gensim/models/word2vec_inner.c: In function '__Pyx_RaiseNeedMoreValuesError':
./gensim/models/word2vec_inner.c:10241:18: warning: unknown conversion type char
acter 'z' in format
./gensim/models/word2vec_inner.c:10241:18: warning: format '%.1s' expects type '
char *', but argument 3 has type 'Py_ssize_t'
./gensim/models/word2vec_inner.c:10241:18: warning: too many arguments for forma
t
./gensim/models/word2vec_inner.c: At top level:
C:\Anaconda\lib\site-packages\numpy\core\include/numpy/__multiarray_api.h:1629:1
: warning: '_import_array' defined but not used
C:\Anaconda\lib\site-packages\numpy\core\include/numpy/__ufunc_api.h:241:1: warn
ing: '_import_umath' defined but not used
./gensim/models/word2vec_inner.c: In function '__pyx_pf_5trunk_6gensim_6models_1
4word2vec_inner_train_sentence_sg':
./gensim/models/word2vec_inner.c:5271:59: warning: '__pyx_v_syn1' may be used un
initialized in this function
./gensim/models/word2vec_inner.c:5274:59: warning: '__pyx_v_syn1neg' may be used
 uninitialized in this function
./gensim/models/word2vec_inner.c:5275:28: warning: '__pyx_v_table' may be used u
ninitialized in this function
./gensim/models/word2vec_inner.c:5276:25: warning: '__pyx_v_table_len' may be us
ed uninitialized in this function
./gensim/models/word2vec_inner.c:5277:25: warning: '__pyx_v_next_random' may be
used uninitialized in this function
./gensim/models/word2vec_inner.c: In function '__pyx_pf_5trunk_6gensim_6models_1
4word2vec_inner_2train_sentence_cbow':
./gensim/models/word2vec_inner.c:6080:59: warning: '__pyx_v_syn1' may be used un
initialized in this function
./gensim/models/word2vec_inner.c:6083:59: warning: '__pyx_v_syn1neg' may be used
 uninitialized in this function
./gensim/models/word2vec_inner.c:6084:28: warning: '__pyx_v_table' may be used u
ninitialized in this function
./gensim/models/word2vec_inner.c:6085:25: warning: '__pyx_v_table_len' may be us
ed uninitialized in this function
./gensim/models/word2vec_inner.c:6086:25: warning: '__pyx_v_next_random' may be
used uninitialized in this function
writing build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.def
C:\Program Files (x86)\Haskell Platform\2013.2.0.0\mingw\bin\dllwrap.exe -DMS_WI
N64 -mdll -static --entry _DllMain@12 --output-lib build\temp.win-amd64-2.7\Rele
ase\.\gensim\models\libword2vec_inner.a --def build\temp.win-amd64-2.7\Release\.
\gensim\models\word2vec_inner.def -s build\temp.win-amd64-2.7\Release\.\gensim\m
odels\word2vec_inner.o -LC:\Anaconda\libs -LC:\Anaconda\PCbuild\amd64 -lpython27
 -lmsvcr90 -o build\lib.win-amd64-2.7\gensim\models\word2vec_inner.pyd
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x23fb): undefined reference to `_imp__PyExc_TypeError'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2406): undefined reference to `_imp__PyErr_Format'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2447): undefined reference to `_imp__PyDict_Next'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x246f): undefined reference to `_imp__PyString_Type'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x24aa): undefined reference to `_imp___PyString_Eq'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x251f): undefined reference to `_imp___PyString_Eq'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2555): undefined reference to `_imp__PyUnicodeUCS2_Compare'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2561): undefined reference to `_imp__PyErr_Occurred'
[...]
</code></pre>

<p>...and it goes on throwing up undefined references.</p>
","3348587","","","","","2015-12-22 13:37:31","Gensim not installing - Windows 8.1 - Python","<python><numpy><windows-8.1><anaconda><gensim>","2","0","","","","CC BY-SA 3.0"
"62941476","1","","","2020-07-16 18:39:05","","0","36","<p>I'm trying to use Doc2Vec to go through the classic exercise of training on Wikipedia articles, using the article title as the tag.</p>
<p>Here's my code and the results, is there something that I'm missing that they would not give the matching results with most_similar? Following <a href=""https://markroxor.github.io/gensim/static/notebooks/doc2vec-wikipedia.html"" rel=""nofollow noreferrer"">this tutorial</a>, but I used the wiki-english-20171001 dataset that came with gensim.</p>
<pre><code>import gensim.downloader as api
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import re

def cleanText(text):
    text = re.sub(r'\|\|\|', r' ', text)
    text = re.sub(r'http\S+', r'&lt;URL&gt;', text)
    text = text.lower()
    text = re.sub(r'[^\w\s]','',text)
    return text


wiki = api.load(&quot;wiki-english-20171001&quot;)

data = [d for d in wiki]

for i in range(10):
    print(data[i])
def my_create_tagged_docs(data):
    for wikiidx in range(len(data)):
        yield TaggedDocument([i for i in data[wikiidx].get('section_texts') for i in cleanText(i).split()], [data[wikiidx].get('title')])


wiki_data = my_create_tagged_docs(data)
del data
del wiki


model = Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter =10, epochs=40)
model.build_vocab(wiki_data)

model.train(wiki_data, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>
<pre><code>model.docvecs.most_similar(positive=[&quot;Lady Gaga&quot;], topn=10)
</code></pre>
<pre><code>[('Chlorothrix', 0.35521823167800903),
 (&quot;A Child's Garden of Verses&quot;, 0.3533579707145691),
 ('Fish Mooney', 0.35129639506340027),
 ('2000 Paris‚ÄìRoubaix', 0.3463437855243683),
 ('Calvin C. Chaffee', 0.3439667224884033),
 ('Murders of Eve Stratford and Lynne Weedon', 0.3397218585014343),
 ('Black Air', 0.3396576941013336),
 ('Turzyn', 0.3312540054321289),
 ('Scott Baker', 0.33018186688423157),
 ('Amongst the Waves', 0.3297169804573059)]
</code></pre>
<pre><code>model.docvecs.most_similar(positive=[&quot;Machine learning&quot;], topn=10)
</code></pre>
<pre><code>[('Wolf Rock, Connecticut', 0.3855834901332855),
 ('Am√°lia Rodrigues', 0.3349645137786865),
 ('Victoria Park, Leicester', 0.33312514424324036),
 ('List of visual anthropology films', 0.3311382532119751),
 ('Sadqay Teri Mout Tun', 0.3287636637687683),
 ('T. Damodaran', 0.32876330614089966),
 ('Urqu Jawira (Aroma)', 0.32281631231307983),
 ('Tiggy Wiggy', 0.3226730227470398),
 ('Fr√©d√©ric Brun (cyclist, born 1988)', 0.32106447219848633),
 ('Unholy Crusade', 0.3200794756412506)]
</code></pre>
","11402694","","","","","2020-07-16 19:04:06","Doc2Vec not providing adequate results in most_similar","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"45154833","1","","","2017-07-17 22:50:11","","0","216","<p>As you know, the skip-gram model learns vector representations of elements based on long sequences of elements and the contexts of each. This model has most commonly been applied to natural language by concatenating giant collections of text. These documents of are often concatenated into a single very long line of text, with no distinction of when a new document begins and ends. This ends up not being much of an issue in NLP because the percentage of the model training instances involving overlapping documents is a small percentage of the total number of instances. In Education data, this overlap can be much higher because of shorter sequences and high numbers of users (formerly ""documents"" in NLP). This is also a problem in other behavioral datasets, not just education. The problem manifests itself when inspecting the learned vectors and finding that the model has determined that many of the students' first encountered elements are very similar to the students' very last encountered elements. This is a bi-product of the ""wrapping"" of lines in the input to gensim (instances spanning the end of one student's sequence and the beginning of another). How can I identify where in the code this overlapping occurs and prohibit this overlap from happening during training in gensim. </p>
","8322066","","","","","2017-07-17 23:24:20","how to prevent overlapping in word2vec?","<python><nlp><gensim><word2vec>","1","0","","2017-07-18 08:55:16","","CC BY-SA 3.0"
"62737231","1","","","2020-07-05 05:37:05","","0","42","<p>I have the approximately 20gigabyte text data which is separated by \n.</p>
<p>I want to read that text as list to put it in the gensim library's Word2vec analysis(As far as I know, only the list form of input is allowed for the gensim Word2vec process).</p>
<p>I tried to read the text data line by line and append to the empty list. But it keep caused Memory error.</p>
<p>Since according to what I found in searching, read line by line does not really damage to the memory usage, I suspect it could be because of the process of appending results to the list. But I don't know how can I solve this problem.</p>
<p>My computer's RAM size is 32gb. and I use ubuntu 20.04 LTS and python 3 with pycharm.</p>
<p>Is it fundamentally impossible to load the 20gb of text file as a list in my computer..? which means that I better to add more RAM size to the computer.</p>
<p>Also, I wonder If it would be better to read as a csv not as a text because I found that pandas have good tools to read huge file. (I tried with this approach but I failed to convert the text column to the list because of the memory issue)</p>
<p>This is the code I used. Thanks for any advice.</p>
<pre><code>##tried with text file
corpus = []
with open(&quot;textfile.txt&quot;, &quot;r&quot;) as f:
    for line in f:
        corpus.append(line.splitlines()[0])


##tried with dataframe
iter_csv = pd.read_csv(&quot;textfile.csv&quot;, iterator=True, sep=',', chunksize=300, header=None
                       , usecols=[4], names=['text'],encoding='UTF-8')
df_forCount_tmp = pd.concat(iter_csv, axis=0, ignore_index=True) 
#memory error....
corpus = []
def convertTolist(text):
    return ast.literal_eval(text)

for dfi, df in df_forCount_tmp.iterrows():
    corpus.extend(convertTolist(df['text']))
</code></pre>
","6099444","","","","","2020-07-05 05:37:05","Read 20gigabyte text as a list for the gensim Word2vec analysis","<python><list><gensim><word2vec>","0","6","","","","CC BY-SA 4.0"
"45159693","1","45172706","","2017-07-18 06:59:03","","2","3581","<p>I am trying to make a word2vec model by Gensim on Persian language which has ""space"" as the character delimiter, I use python 3.5. The problem that I encounter was I gave a text file as input and it returns a model which only consists of each character separately instead of words. I also gave the input as a list of words which is recommended on :</p>

<p><a href=""https://stackoverflow.com/questions/43065843/python-gensim-word2vec-vocabulary-key]"">Python Gensim word2vec vocabulary key</a></p>

<p>It doesn't work for me and I think it doesn't consider sequence of words in a sentence so it wouldn't be correct.</p>

<p>I did some preprocessing on my input which consist of:</p>

<p>collapse multiple whitespaces into a single one<br>
tokenize by splitting on whitespace<br>
remove words less than 3 characters long
remove stop words</p>

<p>I gave the text to word2vec which gave me result correctly, but I need it on python so my choice is limited to use Gensim.</p>

<p>Also I tried to load the model which made by word2vec source on gensim I get error so I need create the word2vec model by Gensim.</p>

<p>my code is:</p>

<pre><code>  wfile = open('aggregate.txt','r')    
  wfileRead = wfile.read()    
  model = word2vec.Word2Vec(wfileRead , size=100)   
  model.save('Word2Vec.txt')
</code></pre>
","7930050","","1033581","","2019-07-07 05:05:36","2019-07-07 05:05:36","word2vec models consist of characters instead of words","<gensim><word2vec>","2","5","1","","","CC BY-SA 4.0"
"45164340","1","45172739","","2017-07-18 10:36:34","","0","591","<p>I am new to Gensim Word2Vec. I was trying to use Word2Vec to build word vectors for some raw html files. So I first convert the html file into txt file.</p>

<h3>My First Question:</h3>

<p>When I train the word2vec model, everything is fine. But when I want to test the accuracy of the model by doing</p>

<pre><code>model.accuracy(file_name)
</code></pre>

<p>it produced error: </p>

<pre><code>Traceback (most recent call last):
  File ""build_w2v.py"", line 82, in &lt;module&gt;
    main()
  File ""build_w2v.py"", line 77, in main
    gen_w2v_model()
  File ""build_w2v.py"", line 71, in gen_w2v_model
    accuracy = model.accuracy(target)
  File ""/home/k/shankai/app/anaconda2/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 1330, in accuracy
    return self.wv.accuracy(questions, restrict_vocab, most_similar, case_insensitive)
  File ""/home/k/shankai/app/anaconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.py"", line 679, in accuracy
    raise ValueError(""missing section header before line #%i in %s"" % (line_no, questions))
ValueError: missing section header before line #0
</code></pre>

<p>Below is the sample file:</p>

<pre><code>zGR='ca-about-health_js';var ziRfw=0;zobt="" Vision Ads"";zOBT="" Ads"";function zIpSS(u){zpu(0,u,280,375,""ssWin"")}function zIlb(l,t,f){zT(l,'18/1Pp/wX')}


zWASL=1;zGRH=1
#rs{margin:0 0 10px}#rs #n5{font-weight:bold}#rs a{padding:7px;text-transform:capitalize}Poking Eyelashes - Poking Eyelashes Problem


&lt;!--
zGOW=0;xd=0;zap="""";zAth='25752';zAthG='25752';zTt='11';zir='';zBTS=0;zBT=0;zSt='';zGz=''
ch='health';gs='vision';xg=""Vision"";zcs=''
zFDT='0'
zFST='0'
zOr='BA15WT26OkWA0O1b';zTbO=zRQO=1;zp0=zp1=zp2=zp3=zfs=0;zDc=1;
zSm=zSu=zhc=zpb=zgs=zdn='';zFS='BA110BA0110B00101';zFD='BA110BA0110B00101'
zDO=zis=1;zpid=zi=zRf=ztp=zpo=0;zdx=20;zfx=100;zJs=0;
zi=1;zz=';336280=2-1-1299;72890=2-1-1299;336155=2-1-12-1;93048=2-1-12-1;30050=2-1-12-1';zx='100';zde=15;zdp=1440;zds=1440;zfp=0;zfs=66;zfd=100;zdd=20;zaX=new Array(11, new Array(100,1051,8192,2,'336,300'),7, new Array(100,284,8196,12,'336,400'));zDc=1;;zDO=1;;zD336=1;zhc='';;zGTH=1;
zGo=0;zG=17;zTac=2;zDot=0;
zObT=""Vision"";zRad=5;var tp="" primedia_""+(zBT?"""":""non_"")+""site_targeting"";if(!this.zGCID)zGCID=tp
else zGCID+=tp;
if(zBT&gt;0){zOBR=1}
if(!this.uy)uy='about.com';if(typeof document.domain!=""undefined"")document.domain=uy;//--&gt;


function zob(p){if(!this.zOfs)return;var a=zOfs,t,i=0,l=a.length;if(l){w('&lt;div id=""oF""&gt;&lt;b&gt;'+(this.zobt?zobt:xg+' Ads')+'&lt;/b&gt;&lt;ul&gt;');while((i&lt;l)&amp;&amp;i&lt;zRad){t=a[i++].line1;w('&lt;li&gt;&lt;a href=""/z/js/o'+(p?p:'')+'.htm?k='+zUriS(t.toLowerCase())+(this.zobr?zobr:'')+'&amp;d='+zUriS(t)+'&amp;r='+zUriS(zWl)+'"" target=""_'+(this.zOBNW?'new'+zr(9999):'top')+'""&gt;'+t+'&lt;/a&gt;&lt;/li&gt;');}w('&lt;/ul&gt;&lt;/div&gt;')}}function rb600(){if(gEI('bb'))gEI('bb').height=600}zJs=10
zJs=11
zJs=12
zJs=13
zc(5,'jsc',zJs,9999999,'')
zDO=0
</code></pre>

<p>So This file actually begins with many (I don't know) space or \n. When I open in the vim.<a href=""https://i.stack.imgur.com/jOz4T.png"" rel=""nofollow noreferrer"">It looks like this</a>.</p>

<p><strong>So what is the problem here?</strong></p>

<h3>My second question:</h3>

<p>Also, I am doing text classification of some biomedical papers. The files I was given are all raw html files in either Japanese or English. After I do the ascii conversion and some stop_words cleaning, there are still many HTML code left in the file. </p>

<p>When I try to clean these files and restrict the characters to [a-zA-Z0-9], I found some medical terms like [4protein...] or something get not properly cleaned as well.</p>

<p><strong>Are there any suggestions in how to clean up these files?</strong></p>
","8318339","","","","","2017-07-18 16:46:33","Gensim Word2Vec Error: ValueError: missing section header before line #0","<python><html><nlp><gensim><word2vec>","1","4","0","","","CC BY-SA 3.0"
"21313493","1","","","2014-01-23 16:09:48","","1","705","<p>I have a total of 54892 documents which have 360331 unique tokens. The length of the dictionary is 88.</p>

<pre><code>mm = corpora.MmCorpus('PRC.mm')
dictionary = corpora.Dictionary('PRC.dict')
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=dictionary, num_topics=50, update_every=0, chunksize=19188, passes=650)
</code></pre>

<p>Whenever I run this script I get this error:</p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\modelDeTopics.py"", line 19, in &lt;module&gt;
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=dictionary, num_topics=50, update_every=0, chunksize=19188, passes=650)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 265, in __init__
self.update(corpus)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 445, in update
self.do_estep(chunk, other)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 365, in do_estep
gamma, sstats = self.inference(chunk, collect_sstats=True)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 318, in inference
expElogbetad = self.expElogbeta[:, ids]
IndexError: index 8 is out of bounds for axis 1 with size 8
</code></pre>

<p>I check on the internet, it is mentioned that i might be related to the RAM the computer has. I am using Windows 7 32-bit with 4 GB RAM. What change should I make in the script?</p>

<p>Please help!</p>
","1461896","","","","","2014-01-30 20:35:36","IndexError while using Gensim package for LDA Topic Modelling","<python><lda><topic-modeling><gensim>","1","0","","","","CC BY-SA 3.0"
"45186094","1","45199662","","2017-07-19 09:16:06","","1","1870","<p>I want to use Word2vec in a web server (production) in two different variants where I fetch two sentences from the web and compare it in real-time. For now, I am testing it on a local machine which has 16GB RAM. </p>

<p>Scenario:
w2v = load w2v model </p>

<pre><code>If condition 1 is true:
   if normalize:
      reverse normalize by w2v.init_sims(replace=False) (not sure if it will work)
   Loop through some items:
   calculate their vectors using w2v
else if condition 2 is true:
   if not normalized:
       w2v.init_sims(replace=True)
   Loop through some items:
   calculate their vectors using w2v
</code></pre>

<p>I have already read the solution about reducing the vocabulary size to a small size but I would like to use all the vocabulary. </p>

<p>Are there new workarounds on how to handle this? Is there a way to initially load a small portion of the vocabulary for first 1-2 minutes and in parallel keep loading the whole vocabulary? </p>
","1996842","","1996842","","2017-07-20 08:28:14","2018-06-04 20:45:33","How to make word2vec model's loading time and memory use more efficient?","<python><nlp><nltk><gensim><word2vec>","1","5","","","","CC BY-SA 3.0"
"45195169","1","45197639","","2017-07-19 15:37:04","","5","908","<p>I am trying to run doc2vec library from gensim package. My problem is that when I am training and saving the model the model file is rather large(2.5 GB) I tried using this line :</p>

<pre><code>model.estimate_memory()
</code></pre>

<p>But it didn't change anything. I also have tried to change max_vocab_size to decrease the space. But there was not luck. Can somebody help me with this matter?</p>
","2462485","","","","","2017-07-19 17:48:30","Gensim Doc2Vec generating huge file for model","<python><semantics><gensim><word2vec><doc2vec>","1","3","","2017-07-19 20:34:04","","CC BY-SA 3.0"
"47300015","1","","","2017-11-15 05:28:44","","0","290","<p>I am using Gensim to calculate tf-idf scores for my corpus mentioned below.</p>

<pre><code>corpus=['human interface computer',
 'survey user computer system response time',
 'eps user interface system',
 'system human system eps',
 'user response time']
</code></pre>

<p>My current code is as follows.</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in corpus)

class MyCorpus(object):
    def __iter__(self):
        for line in corpus:
            yield dictionary.doc2bow(line.lower().split())

corpus = MyCorpus()

tfidf = models.TfidfModel(corpus)

corpus_tfidf = tfidf[corpus]
</code></pre>

<p>However, I get the error <code>RecursionError: maximum recursion depth exceeded while calling a Python object</code> (PS: if my code is wrong I am happy to have a different code). Please help me to calculate tf-idf values for my current corpus. Moreover, I want to get the 3 terms that has the highest tf-idf score in my corpus.</p>

<p>Please help me!</p>
","","user8871463","","user8871463","2017-11-15 08:13:48","2017-11-15 08:13:48","Issues in calculating tf-idf in gensim","<python><gensim><tf-idf>","1","0","","","","CC BY-SA 3.0"
"52989568","1","","","2018-10-25 12:42:57","","5","408","<p>I'm a little bit confused about the comments to alpha in the documentation of LDA (Gensim).</p>

<p>In the ""regular"" Gensim LdaModel it says that if one sets alpha = 'asymmetric', Gensim uses a ""fixed normalized asymmetric prior of 1.0 / topicno"" (topicno is num_topics, right?!). But why it is called asymmetric? Isn't that the symmetric case? (see <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a>)</p>

<p>And whats the default number for alpha used by Mallet? 50? If so, why? As far as i know one should choose some value &lt;1 to get good results.
(see <a href=""https://radimrehurek.com/gensim/models/wrappers/ldamallet.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/wrappers/ldamallet.html</a>)</p>
","10478746","","","","","2018-10-25 12:42:57","LDA Gensim/Mallet documentation on alpha","<gensim><lda><mallet><dirichlet>","0","0","","","","CC BY-SA 4.0"
"54165109","1","54165744","","2019-01-13 00:26:23","","3","583","<p>I'm vectorizing words on a few different corpora with Gensim and am getting results that are making me rethink how Word2Vec functions. My understanding was that Word2Vec was deterministic, and that the position of a word in a vector space would not change from training to training. If ""My cat is running"" and ""your dog can't be running"" are the two sentences in the corpus, then the value of ""running"" (or its stem) seems necessarily fixed.</p>

<p>However, I've found that that value indeed does vary across models, and words keep changing where they are on a vector space when I train the model. The differences are not always hugely meaningful, but they do indicate the existence of some random process. What am I missing here?</p>
","7542939","","7542939","","2019-01-13 00:33:25","2019-01-14 20:43:08","What is the stochastic aspect of Word2Vec?","<nlp><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"43878332","1","43917056","","2017-05-09 19:26:59","","15","7656","<p>I have saved a Gensim dictionary to disk. When I load it, the <code>id2token</code> attribute dict is not populated.</p>

<p>A simple piece of the code that saves the dictionary:</p>

<pre><code>dictionary = corpora.Dictionary(tag_docs)
dictionary.save(""tag_dictionary_lda.pkl"")
</code></pre>

<p>Now when I load it (I'm loading it in an jupyter notebook), it still works fine for mapping tokens to IDs, but <code>id2token</code> does not work (I cannot map IDs to tokens) and in fact <code>id2token</code> is not populated at all.</p>

<pre><code>&gt; dictionary = corpora.Dictionary.load(""../data/tag_dictionary_lda.pkl"")
&gt; dictionary.token2id[""love""]
Out: 1613

&gt; dictionary.doc2bow([""love""])
Out: [(1613, 1)]

&gt; dictionary.id2token[1613]
Out: 
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input&gt; in &lt;module&gt;()
----&gt; 1 dictionary.id2token[1613]

KeyError: 1613

&gt; list(dictionary.id2token.keys())
Out: []
</code></pre>

<p>Any thoughts? </p>
","1772977","","","","","2017-05-11 13:36:18","Gensim saved dictionary has no id2token","<python><nlp><gensim>","1","0","1","","","CC BY-SA 3.0"
"53697450","1","","","2018-12-09 22:47:38","","0","940","<p>the version of python is 3.6
I tried to execute my code but, there are still some errors as below:</p>

<p>Traceback (most recent call last):</p>

<blockquote>
  <p>File
  ""C:\Users\tmdgu\Desktop\NLP-master1\NLP-master\Ontology_Construction.py"",
  line 55, in 
      , binary=True)</p>
  
  <p>File ""E:\Program
  Files\Python\Python35-32\lib\site-packages\gensim\models\word2vec.py"",
  line 1282, in load_word2vec_format
      raise DeprecationWarning(""Deprecated. Use gensim.models.KeyedVectors.load_word2vec_format instead."")</p>
  
  <p>DeprecationWarning: Deprecated. Use
  gensim.models.KeyedVectors.load_word2vec_format instead.</p>
</blockquote>

<p>how to fix the code? or is the path to data wrong?</p>
","8792931","","958529","","2018-12-10 01:58:24","2018-12-10 21:55:36","Error for word2vec with GoogleNews-vectors-negative300.bin","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"47303842","1","","","2017-11-15 09:36:41","","0","561","<p>when I use model.infer_vector  to compute the vectors, differ order of 
        document results different.</p>

<pre><code>size=200;negative=15; min_count=1;iterNum=20;
windows = 5
modelName = ""datasets/dm-sum.bin_""+str(windows)+""_"" 
+str(size)+""_""+str(negative)
model = loadDoc2vecModel(modelName)
vecNum = 200
</code></pre>

<p>call infer_vector</p>

<pre><code>test_docs = [ x.strip().split() for x in 
codecs.open(""datasets/test_keyword_f1"", ""r"", ""utf-8"").readlines() ]
for item in test_docs:

    print(""%s"" %(resStr.strip()))
    vecTmp = model.infer_vector(item,  alpha=0.05, steps=20)
    print(vecTmp)
</code></pre>

<p>When I executed call infer_vector twice, the results were as follows.</p>

<p>I don't know why did this happen.</p>

<p><a href=""https://i.stack.imgur.com/mN98V.png"" rel=""nofollow noreferrer"">this link is the result</a></p>
","8546996","","8546996","","2017-11-15 10:04:22","2019-08-02 17:09:11","gensim doc2vec, why the order of the sentences affects the doc2vec vector","<gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"52817087","1","52826998","","2018-10-15 12:44:07","","1","3700","<p>With this Gensim example in github, <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a> it provides examples at the end to find simalarities with phrases or keywords, like 'lady gaga' or 'machine learning'. However am looking to find similarity with actual document in plain text file, could this be done? and how can I do it? suppose text file is located on my local laptop in txt format.</p>
","4586806","","4586806","","2018-10-15 13:26:00","2018-10-16 02:11:40","Document similarity with doc2vec","<python><nlp><gensim><doc2vec>","1","3","","","","CC BY-SA 4.0"
"36509957","1","36617494","","2016-04-08 21:55:54","","10","6463","<p>I am trying to experiment gensim doc2vec, by using following code. As far as  I understand from tutorials, it should work. However it gives <strong>AttributeError: 'list' object has no attribute 'words'.</strong></p>

<pre><code>from gensim.models.doc2vec import LabeledSentence, Doc2Vec
document = LabeledSentence(words=['some', 'words', 'here'], tags=['SENT_1']) 
model = Doc2Vec(document, size = 100, window = 300, min_count = 10, workers=4)
</code></pre>

<p>So what did I do wrong? Any help please. Thank you. I am using python 3.5 and gensim 0.12.4</p>
","4345535","","4345535","","2016-04-11 11:04:55","2016-04-14 08:23:41","Why Gensim doc2vec give AttributeError: 'list' object has no attribute 'words'?","<python-3.x><gensim><word2vec>","1","3","3","","","CC BY-SA 3.0"
"65045215","1","","","2020-11-28 00:15:22","","0","62","<p>I have trained a LDA topic model on a corpus of 20,000 documents (training sample). I want to use the trained model on a new sample of 2,000 unseen documents and generate topics for those unseen documents based on the topics generated in the training sample.</p>
<p>I am struggling to figure out how to execute this. Code below.</p>
<pre><code># Build LDA model on original corpus of 20,000 documents
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=20, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)

#Load corpus of 2,000 unseen documents
unseen = pd.read_csv('pubs-testing.csv', header = 0, error_bad_lines=False)
unseen_text = unseen[['abstract']]
unseen_text['index'] = unseen_text
documents = unseen_text

#Run pre-defined function of pre-processing documents: lemmatize, remove stopwords, etc
processed_docs_testing = documents['abstract'].map(preprocess)

bow_vector = [dictionary.doc2bow(doc) for doc in processed_docs_testing]

#run original lda model on unseen documents
lda_vector = lda_model[bow_vector] 
</code></pre>
<p>Are the steps correct so far? Also, how do I observe the topic mixtures of each unseen document?</p>
<p>Thank you!</p>
","11068190","","","","","2020-11-28 00:15:22","Transforming unseen corpus using LDA topic modeling","<python><gensim><topic-modeling>","0","1","1","","","CC BY-SA 4.0"
"38985470","1","39370190","","2016-08-16 22:27:47","","2","937","<p>I am training on two identical sentences (documents) using from <code>gensim.models.doc2vec import Doc2Vec</code> and when checking out the vectors for each sentence they are completely different. Does the Neural Network have a different random initialisation per sentence?</p>

<pre><code># imports
from gensim.models.doc2vec import LabeledSentence
from gensim.models.doc2vec import Doc2Vec
from gensim import utils

# Document iteration class (turns many documents in to sentences
# each document being once sentence)
class LabeledDocs(object):
    def __init__(self, sources):
        self.sources = sources
        flipped = {}
        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')

    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                # print fin.read().strip(r""\n"")
                yield LabeledSentence(utils.to_unicode(fin.read()).split(),
                                      [prefix])

    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                #print fin, fin.read()
                self.sentences.append(
                    LabeledSentence(utils.to_unicode(fin.read()).split(),
                                    [prefix]))
        return self.sentences

# play and play3 are names of identical documents (diff gives nothing)
inp = LabeledDocs({""play"":""play"", ""play3"":""play3""})
model = Doc2Vec(size=20, window=8, min_count=2, workers=1, alpha=0.025,
                min_alpha=0.025, batch_words=1)
model.build_vocab(inp.to_array())
for epoch in range(10):
    model.train(inp)

# post to this model.docvecs[""play""] is very different from
# model.docvecs[""play3""]
</code></pre>

<p>Why is this ? Both <code>play</code> and <code>play3</code> contain :</p>

<pre class=""lang-none prettyprint-override""><code>foot ball is a sport
played with a ball where
teams of 11 each try to
score on different goals
and play with the ball
</code></pre>
","5252003","","355230","","2016-08-17 00:02:24","2016-09-07 22:03:16","Why does gensim Doc2Vec give me different vectors for the same sentence?","<python><neural-network><gensim>","1","0","","","","CC BY-SA 3.0"
"27324292","1","27329142","","2014-12-05 20:39:00","","67","51580","<p>From the <a href=""https://code.google.com/p/word2vec/"">word2vec</a> site I can download GoogleNews-vectors-negative300.bin.gz.  The .bin file (about 3.4GB) is a binary format not useful to me.  Tomas Mikolov <a href=""https://groups.google.com/d/msg/word2vec-toolkit/lxbl_MB29Ic/g4uEz5rNV08J"">assures us</a> that ""It should be fairly straightforward to convert the binary format to text format (though that will take more disk space). Check the code in the distance tool, it's rather trivial to read the binary file.""  Unfortunately, I don't know enough C to understand <a href=""http://word2vec.googlecode.com/svn/trunk/distance.c"">http://word2vec.googlecode.com/svn/trunk/distance.c</a>.</p>

<p>Supposedly <a href=""http://radimrehurek.com/2014/02/word2vec-tutorial/"">gensim</a> can do this also, but all the tutorials I've found seem to be about converting <em>from</em> text, not the other way.</p>

<p>Can someone suggest modifications to the C code or instructions for gensim to emit text?</p>
","29771","","29771","","2014-12-05 20:54:12","2017-05-04 08:30:48","Convert word2vec bin file to text","<python><c><gensim><word2vec>","10","0","29","","","CC BY-SA 3.0"
"53130738","1","53156039","","2018-11-03 11:10:38","","-2","252","<p>When running the below code. this Python 3.6, latest Gensim library in Jupyter</p>

<pre><code>for model in models:
       print(str(model))
       pprint(model.docvecs.most_similar(positive=[""Machine learning""], topn=20))
</code></pre>

<p>[1]: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a><img src=""https://i.stack.imgur.com/wI25F.png"" alt=""enter image description here""></p>
","4586806","","4586806","","2018-11-04 03:31:10","2018-11-05 14:09:27","Gensim example, TypeError:between str and int error","<python><nlp><gensim>","2","7","0","","","CC BY-SA 4.0"
"55384167","1","","","2019-03-27 18:24:15","","0","571","<p>I am getting repeated lines in my summarizer output. I am using genism in python for summarizing text documents. How to remove duplicate lines from the output of the summarizer. The output is coming with repeated content. How can I only keep unique lines in the output from the summarizer .The input file is as follows</p>

<pre><code>From: Jos
To: Halley, Ibizo /FR
Cc: pqr Secretariat; Bj√∂rnsson Ulrika
Subject: [EXTERNAL] pqr Response to Letter of Intent for a Variation WS procedure:SE/H/xxxx/WS/
Date: vendredi 1 juin 2018 13:16:48
Attachments: image001.jpg

A07_SE_xxx yy R&amp;D.PDF

Dear Ibizo,

Thank you for your letter of intent.

The pqr agrees, on the basis of the documentation provided, that the above mentioned work-
sharing application as specified in the enclosed letter of intent is acceptable for submission under
Article 20 of the Commission Regulation (EC) No 1234/2008 of 24 November 2008.

The reference authority for the worksharing procedure will be Sweden and the assigned work sharing
procedure number will be:

A07: SE/H/xxxx/WS/



Please be advised that this confirmation is not to be considered as validation of your application. The
validity of the worksharing application will be checked by the reference authority after submission.

Please liaise with the assigned reference authority for the further proceedings.


Kind regards,


Joe
Assistant Administrator
Parallel Distribution &amp; Certificates
Committees &amp; Inspections Department
Panthers Medicines Agency
30 ABC St, Michigan lane
Fax +44 (0)20 certificate@zz.europa.eu | www.zz.europa.eu


This message and any attachment contain information which may be confidential or otherwise
protected from disclosure. It is intended for the addressee(s) only and should not be relied upon as
legal advice unless it is otherwise stated. If you are not the intended recipient(s) (or authorised by
an addressee who received this message), access to this e-mail, or any disclosure or copying of its
contents, or any action taken (or not taken) in reliance on it is unauthorised and may be unlawful. If
you have received this e-mail in error, please inform the sender immediately.
P Please consider the environment and don't print this e-mail unless you really need to



From: Jos 
Sent: 30 April 2018 11:17
To: Ibizo.Halley@xxx.com
Cc: pqr Secretariat
Subject: RE: Alfuzosin Hydrochloride - Request for Worksharing procedure

Dear Ibizo,
Thank you for your zzil.
The letter of intent will be discussed in the May 2018 pqr meeting and you will receive feedback
within two weeks following the meeting.



Kind regards,


Joe
Assistant Administrator
Parallel Distribution &amp; Certificates
Committees &amp; Inspections Department

mailto:eretta.ab@zz.europa.eu
mailto:Ibizo.Halley@xxx.com
mailto:H-pqrSecretariat@zz.europa.eu
mailto:Ulrika.Bjornsson@mpa.se
mailto:certificate@zz.europa.eu

pqr/162/2010/Rev.2, August 2014 








26 April 2018 

pqr Secretariat 
Panthers Medicines Agency 
30 Bluegoon Place, ABC Wharf 
ABC E14 5EU  
United Kingdom 



Subject: Letter of intent for the submission of a worksharing procedure to the pqr according 


to Article 20 of Commission Regulation (EC) No 1234/2008 



Worksharing Applicant details: 


Name  : xxx-yy R&amp;D 


   Address : 1, lane Pierre Brossolette  
91385 Chilly-Maz 
Sw



Contact person details  
(i.e. name, address, e-mail 
address, phone number, fax 
number) 


: Ibizo Halley 
1, lane Pierre Brossolette  
91385 Chilly-Maz
Sw 
zzil: Ibizo.halley@xxx.com 
Tel : + 33 1 60 49 51 61 





Application details: 

This letter of intent for the submission of a Type II following a worksharing procedure according to 
Article 20 of Commission Regulation (EC) No 1234/2008, concerns the following medicinal products 
authorised via MRP and national procedures: 


Products authorized via MRP: 

Alfuzosin 2.5 mg film-coated tablets 

Product name Active 


substance(s) 
MRP number 


XATRAL Alfuzosin 
hydrochloride 


SE/H/0112/001 











mailto:Ibizo.halley@xxx.com





Alfuzosin 5 mg prolonged-release tablets 

Product name Active 


substance(s) 
MRP number 


XATRAL SR 5 MG Alfuzosin 
hydrochloride 


SE/H/0112/002 


XATRAL Alfuzosin 
hydrochloride 


SE/H/0112/002 



Alfuzosin 10 mg prolonged-release tablets 

Product name Active 


substance(s) 
MRP number 


XATRAL UNO       10 MG Alfuzosin 
hydrochloride 


SE/H/0112/003 


ALFUZOSIN WINTHROP 
UNO 10 MG 


Alfuzosin 
hydrochloride 


DE/H/2130/001 


ALFUZOSIN ZENTIVA 10 
MG 


Alfuzosin 
hydrochloride 


DE/H/2131/001/MR 


UROXATRAL Alfuzosin 
hydrochloride 


DE/H/2129/001 


Alfuzosin Zentiva    10 mg 
Retardtabletten 


Alfuzosin 
hydrochloride 


DE/H/2131/001 


XATRAL OD 10 MG Alfuzosin 
hydrochloride 


SE/H/0112/003 




Products authorised via national procedure:  

Alfuzosin 2.5 mg film-coated tablets 

Product name Active 


substance(s) 
National MA 


number 
Member state 


XATRAL Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10600 


Denmark 


XATRAL 2.5 MG Alfuzosin 
hydrochloride 


NL 14785 France 


ALFUZOSIN 
WINTHROP 2.5 MG 


Alfuzosin 
hydrochloride 


32177.00.00 Germany 


UROXATRAL Alfuzosin 
hydrochloride 


18111.00.00 Germany 


XATRAL Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10602 


Greece 


XATRAL 2.5 MG Alfuzosin 
hydrochloride 


PA 540/162/1 Ireland 


XATRAL Alfuzosin 
hydrochloride 


027314018 Italy 


MITTOVAL Alfuzosin 
hydrochloride 


026670024 Italy 


ALFUZOSINA 
ZENTIVA 


Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10163 


Italy 


XATRAL Alfuzosin 
hydrochloride 


RVG 13689 Netherlands 


DALFAZ Alfuzosin 
hydrochloride 


R/6812 Poland 


BENESTAN 2.5 MG Alfuzosin 
hydrochloride 


60031 Spain 


XATRAL 2.5 MG Alfuzosin 
hydrochloride 


PL 04425/0655 United Kingdom 







ALFUZOSIN 
HYDROCHLORIDE 


2.5MG 


Alfuzosin 
hydrochloride 


PL 17780/0220 United Kingdom 






Alfuzosin 5 mg prolonged-release tablets 

Product name Active 


substance(s) 
National MA 


number 
Member state 


XATRAL 5 RETARD Alfuzosin 
hydrochloride 


NAT-H-4908-01 Belgium 


XATRAL Alfuzosin 
hydrochloride 


17139 



Cyprus 


XATRAL LP 5 MG Alfuzosin 
hydrochloride 


NL 19090 France 


ALFUZOSIN 
WINTHROP 5 MG 


Alfuzosin 
hydrochloride 


34637.00.00 Germany 


XATRAL Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#10812 


Greece 


ALFETIM SR 5 MG Alfuzosin 
hydrochloride 


OGYI-T-4374/01 Hungary 


ALFUZOSINA 
ZENTIVA 


Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#8994 


Italy 


XATRAL 5 RETARD Alfuzosin 
hydrochloride 


583/98/12/4785 Luxembourg 


XATRAL SR 5 MG Alfuzosin 
hydrochloride 


MA082/05001 Malta 


DALFAZ SR Alfuzosin 
hydrochloride 


8127 Poland 


XATRAL LP 5 MG Alfuzosin 
hydrochloride 


1026/2008 Romania 


XATRAL 5-SR Alfuzosin 
hydrochloride 


77/0275/96-S  Slovakia 


BENESTAN 
RETARD 5 MG 


Alfuzosin 
hydrochloride 


60767 Spain 








Alfuzosin 10 mg prolonged-release tablets 

Product name Active 


substance(s) 
National MA 


number 
Member state 


XATRAL UNO       
10 MG 


Alfuzosin 
hydrochloride 


NAT-H-4908-04 Belgium 


XATRAL XL 10 MG Alfuzosin 
hydrochloride 


19244  Cyprus 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


345201 Estonia 


XATRAL CR 10 MG Alfuzosin 
hydrochloride 


13973 Finland 


ALFUZOSINE 
ZENTIVA LP 10 MG 


Alfuzosin 
hydrochloride 


NL 24407 France 


XATRAL LP 10 MG Alfuzosin 
hydrochloride 


NL 24386 France 


XATRAL OD Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#9520 


Greece 







ALFETIM UNO     
10 MG 


Alfuzosin 
hydrochloride 


OGYI-T-8022/01 Hungary 


XATRAL 10 MG Alfuzosin 
hydrochloride 


PA 540/162/3 Ireland 


MITTOVAL Alfuzosin 
hydrochloride 


026670048-051 Italy 


XATRAL 10 MG Alfuzosin 
hydrochloride 


027314044-057 Italy 


ALFUZOSINA 
ZENTIVA 


Alfuzosin 
hydrochloride 


NO APPLICATION 
CODE -#9579 


Italy 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


99-0702 Latvia 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


LT-2000/7118/10 Lithuania 


XATRAL UNO       
10 MG 


Alfuzosin 
hydrochloride 


0005/01/09/0045 Luxembourg 


XATRAL XL 10 MG Alfuzosin 
hydrochloride 


MA082/05002 Malta 


XATRAL XR 10 MG Alfuzosin 
hydrochloride 


RVG 23923 Netherlands 


DALFAZ UNO Alfuzosin 
hydrochloride 


8378 Poland 


BENESTAN OD    
10 MG 


Alfuzosin 
hydrochloride 


99/H/0006/01 Portugal 


ALFUZOSINA 
ZENTIVA, 10 MG 


Alfuzosin 
hydrochloride 


99/H/0007/001 Portugal 


XATRAL SR 10 MG Alfuzosin 
hydrochloride 


7893/2006 Romania 


UNIBENESTAN    
10 MG 


Alfuzosin 
hydrochloride 



63605 


Spain 


XATRAL XL 10 MG Alfuzosin 
hydrochloride 


PL 04425/0657 United Kingdom 


BESAVAR XL Alfuzosin 
hydrochloride 


PL 17780/0221 United Kingdom 








The following variation is intended to be part of the work-sharing procedure: 





Number as in the 
classification guideline: 


Title of variation as in the classification 
guideline 


Type of variation: 



C.I.4 



Changes in the Summary of Product 
Characteristics, Labelling or package 
Leaflet due new quality, preclinical, 
clinical or pharmacovigilance data 



Type II 








Justification for worksharing : xxx submitted for alfuzosin hydrochloride separate national and MRP variations for implementation of CCDS V13 including 
among other topics the addition of a contraindication to strong 
CYP3A4 inhibitors in the sections 4.3 and 4.5. 

The MAH received on 04 April 2018 a letter from pqr 
(zz/pqr/195547/2018) requesting to re-submit the variation 
for this contraindication as a work-sharing application including 







all MRP and nationally authorised products to harmonise the 
assessment of the contraindication in section 4.3 and 4.5 of the 
SmPC across the EU (provided in Annex I). 





Justification for grouping :  Not applicable 






Intended submission date : 30 June 2018 





Preferred Reference Authority 



: The Para Medical Products Agency, as RMS of the MRP 


procedure SE/H/0112/001-003 








Explanation that all MAs 
concerned belong to the 
same holder 


: I hereby confirm that all the marketing authorisations, listed in application details (refer above), concerned by the worksharing 
procedure belong to the same marketing authorisation holder, as 
they are part of the same mother company xxx, as per the 
Commission communication 98/C 229/03. 








Yours sincerely, 




Ibizo HALLEY 
xxx-yy R&amp;D, Europe Region 
Global Logistics Affairs Europe  






Please send this letter electronically to the pqr Secretariat (H-pqrSecretariat@zz.europa.eu) 
or RMS as relevant. 











mailto:H-pqrSecretariat@zz.europa.eu

























ANNEX 1 













30 Bluegoon Place ‚óè ABC Wharf ‚óè ABC E14 5EU ‚óè United Kingdom 






Telephone +44 (0)20 3660 6000 Facsimile +44 (0)20 3660 5520 

















Dr.ssa Maty Lecc
xxx S.p.A 


Viale L. Bodio 
20158 AUGB   
Italy 
E-mail: DRA@xxx.com 










4 April 2018 


zz/pqr/195547/2018 





Subject: Request for submission of variation worksharing procedure for Xatral (alfuzosin) 


and related names  





Dear Dr Maty Lecchi, 



During the March meeting, the pqr was informed that separate national and MRP variations have 


been submitted across EU Member States to request the inclusion of the below contraindication for 


Xatral (alfuzosin) and related names: 



Section 4.3 


Concomitant intake of strong inhibitors of CYP3A4 (see paragraph 4.5). 





The parallel submissions in several Member States have led to a disharmonised assessment of the 


contraindication. In the interest of public health across the Panthers Union, the pqr requests xxx 


to re-submit the variation as a worksharing application including all MRP, DCP and nationally 


authorised products to harmonise the assessment of the contraindication in section 4.3 of the SmPC 


across the EU. 


Please note that a separate letter on an independent issue to this has been sent to Esther de Bles, 


xxx-yy Netherlands B.V.. However, there are general concerns by the pqr on the lack of use 


of variation worksharing by xxx-yy in these cases.  



Kind Regards, 







Laura Oliveira Santamaria 


Chair of pqr 




mailto:DRA@xxx.com



        Worksharing Applicant details:

        Name 

        xxx-yy R&amp;D, Europe Region

        Global Logistics Affairs Europe






Panthers Medicines Agency
30 ABC St, Michigan lane
Fax +44 (0)20 3660 5525 certificate@zz.europa.eu | www.zz.europa.eu


This message and any attachment contain information which may be confidential or otherwise
protected from disclosure. It is intended for the addressee(s) only and should not be relied upon as
legal advice unless it is otherwise stated. If you are not the intended recipient(s) (or authorised by
an addressee who received this message), access to this e-mail, or any disclosure or copying of its
contents, or any action taken (or not taken) in reliance on it is unauthorised and may be unlawful. If
you have received this e-mail in error, please inform the sender immediately.
P Please consider the environment and don't print this e-mail unless you really need to



From: Ibizo.Halley@xxx.com [mailto:Ibizo.Halley@xxx.com] 
Sent: 27 April 2018 17:40
To: pqr Secretariat
Subject: Alfuzosin Hydrochloride - Request for Worksharing procedure

Dear Sirs, Madams,

We are pleased to send you a request for the submission of a Type II variation following a worksharing
procedure according to Article 20 of Commission Regulation (EC) No 1234/2008 for Alfuzosin
hydrochloride containing products.
The variation concerns the addition of a contraindication with strong CYP 3A4 inhibitors in section 4.3
and 4.5.
The worksharing procedure has been requested to xxx by the chair of pqr, Mme Oliveira
Santamaria, the letter is attached as Annex of the letter of intent attached.

Thank you in advance for your agreement.

Kind regards,

Ibizo Halley
GEM/EP and OTC switch
EU Regional Logistics Product manager
Global Logistics Affairs
xxx R&amp;D
Phone: +33 1 60 49 51 61



logoGRA 1



________________________________________________________________________

This e-mail has been scanned for all known viruses by Panthers Medicines Agency.
</code></pre>
","11118893","","","","","2019-03-27 19:09:42","Gensim summarization returning repeated lines as summary of text documents","<python><nlp><gensim><summarization><summarize>","2","3","","","","CC BY-SA 4.0"
"62730537","1","","","2020-07-04 14:46:03","","0","210","<p>I've installed gensim, however I keep getting an error when I try to import it
<code>from gensim.models import Word2Vec</code>
ImportError: cannot import name 'open'</p>
<p>I'm using the updated version of gensim 3.8.0 and smart_open 2.1.0.</p>
<p>I have reinstalled several times but still can't get it to work.</p>
","12379511","","","","","2021-04-06 15:16:48","Importing gensim","<gensim><word2vec>","1","4","1","","","CC BY-SA 4.0"
"44158856","1","","","2017-05-24 12:47:53","","0","620","<p>I have implemented finding similar documents based on a particular document using LDA Model (using Gensim). Next thing i want to do is if I have multiple documents then how to get similar document based on the multiple documents provided as input. </p>

<p>I implemented LDA using this <a href=""https://stackoverflow.com/questions/22433884/python-gensim-how-to-calculate-document-similarity-using-the-lda-model"">link</a></p>

<p>sample code for single query -</p>

<pre><code>dictionary = corpora.Dictionary.load('dictionary.dict')
corpus = corpora.MmCorpus(""corpus.mm"")
lda = models.LdaModel.load(""model.lda"") #result from running online lda (training)

index = similarities.MatrixSimilarity(lda[corpus])
index.save(""simIndex.index"")

docname = ""docs/the_doc.txt""
doc = open(docname, 'r').read()
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lda = lda[vec_bow]

sims = index[vec_lda]
sims = sorted(enumerate(sims), key=lambda item: -item[1])
print sims
</code></pre>

<p>Now if I have another doc then how to implement it.</p>
","3394021","","","","","2018-05-26 13:07:15","how to calculate document similarity using more than one query?","<python><gensim><lda>","2","0","","","","CC BY-SA 3.0"
"45193550","1","45208856","","2017-07-19 14:26:08","","0","382","<p>I recently updated a conda environment from python=3.4 to python 3.6. The environment is made for a project using <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow noreferrer"">gensim</a> wich worked perfectly on 3.4. After this update, using the library generates multiple errors such as:</p>

<pre><code>TypeError: object of type 'itertools.chain' has no len()
</code></pre>

<p>or</p>

<pre><code>AssertionError: decomposition not initialized yet
</code></pre>

<p>Do you guys know why this happens while gensim explicitly says python 3.5 and 3.6 are supported ?</p>

<p>The used code:</p>

<pre><code># Create Texts
texts = src.data.raw.extract_clean_merge_titles_abstracts(papers)
src.data.raw.train_phraser(texts)
texts = src.data.raw.tokenize_stream(texts)

print(""Size of corpus: "", len(texts)) # ERROR 1 HERE

# Create Dictionary
dictionary = gensim.corpora.dictionary.Dictionary(texts, prune_at=None)
dictionary.filter_extremes(no_below=3 ,no_above=0.1, keep_n=None)
dictionary.compactify()
print(dictionary)
dictionary.save(config.paths.PATH_DATA_GENSIM_TEMP_DICTIONARY)

# Create corpus
corpus = [dictionary.doc2bow(text) for text in texts]
#gensim.corpora.MmCorpus.serialize(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS, corpus)
corpus_index = gensim.similarities.docsim.Similarity(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_INDEX, corpus, num_features=len(dictionary))
corpus_index.save(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_INDEX)

#¬†tf-idf
tfidf = gensim.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]    #gensim.corpora.MmCorpus.serialize(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_TFIDF, corpus_tfidf)
tfidf.save(config.paths.PATH_DATA_GENSIM_TEMP_TFIDF)
corpus_tfidf_index = gensim.similarities.docsim.Similarity(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_TFIDF_INDEX, corpus_tfidf, num_features=len(dictionary))
corpus_tfidf_index.save(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_TFIDF_INDEX)

# lsa
lsa_num_topics = 100
lsa = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=lsa_num_topics)
corpus_lsa = lsa[corpus_tfidf] # ERROR 2 HERE
#gensim.corpora.MmCorpus.serialize(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_LSA, corpus_lsa)
lsa.save(config.paths.PATH_DATA_GENSIM_TEMP_LSA)
corpus_lsa_index = gensim.similarities.docsim.Similarity(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_LSA_INDEX, corpus_lsa, num_features=lsa_num_topics)
corpus_lsa_index.save(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_LSA_INDEX)
</code></pre>

<p>Here is the list of the packages installed:</p>

<pre><code>bkcharts                  0.2                      py36_0  
bokeh                     0.12.6                   py36_0  
boto                      2.47.0                   py36_0  
bz2file                   0.98                     py36_0  
cycler                    0.10.0                   py36_0  
dbus                      1.8.20                        1    ostrokach
decorator                 4.0.11                   py36_0  
expat                     2.1.0                         0    ostrokach
fontconfig                2.12.1                        3  
freetype                  2.5.5                         2  
gensim                    2.2.0               np113py36_0  
gettext                   0.19.5                        2    ostrokach
glib                      2.48.2                        0    ostrokach
gst-plugins-base          1.8.0                         0  
gstreamer                 1.8.0                         0  
icu                       54.1                          0    ostrokach
jinja2                    2.9.6                    py36_0  
jpeg                      9b                            0  
libffi                    3.2.1                         8    ostrokach
libgcc                    5.2.0                         0  
libgfortran               3.0.0                         1  
libiconv                  1.14                          0  
libpng                    1.6.27                        0  
libsigcpp                 2.4.1                         3    ostrokach
libxcb                    1.12                          1  
libxml2                   2.9.4                         0  
markupsafe                0.23                     py36_2  
matplotlib                2.0.2               np113py36_0  
mkl                       2017.0.3                      0  
networkx                  1.11                     py36_0  
nltk                      3.2.4                    py36_0  
numpy                     1.13.1                   py36_0  
openssl                   1.0.2l                        0  
pcre                      8.39                          1  
pip                       9.0.1                    py36_1  
pymysql                   0.7.9                    py36_0  
pyparsing                 2.1.4                    py36_0  
pyqt                      5.6.0                    py36_2  
python                    3.6.1                         2  
python-dateutil           2.6.0                    py36_0  
pytz                      2017.2                   py36_0  
pyyaml                    3.12                     py36_0  
qt                        5.6.2                         4  
readline                  6.2                           2  
requests                  2.14.2                   py36_0  
scikit-learn              0.18.2              np113py36_0  
scipy                     0.19.1              np113py36_0  
setuptools                27.2.0                   py36_0  
sip                       4.18                     py36_0  
six                       1.10.0                   py36_0  
smart_open                1.5.3                    py36_0  
sqlite                    3.13.0                        0  
system                    5.8                           2  
tk                        8.5.18                        0  
tornado                   4.5.1                    py36_0  
wheel                     0.29.0                   py36_0  
xz                        5.2.2                         1  
yaml                      0.1.6                         0  
zlib                      1.2.8                         3  
</code></pre>
","6084245","","6084245","","2017-07-20 07:27:28","2017-07-20 08:02:28","Gensim errors after updating python version with conda","<python-3.x><conda><gensim>","2","0","","","","CC BY-SA 3.0"
"64768423","1","","","2020-11-10 11:51:30","","2","89","<p>I am looking for a way to update the existing corpus with new docs using gensim. Here, I have created a dictionary from the existing corpus and a bag of words for the same. Later, I serialized it to .mm file and saved it to the disk locally. Now, I want to update my existing .mm file with new docs so that I can keep the representation of updated corpus so that on unseen data I will be able to use it for document similarity. Please assist me that how can I do it? What is the proper way to update the corpus? Moreover, I am aware that instead of .mm file I can add documents to the dictionary.</p>
<pre><code>from gensim import corpora, models, similarities
from gensim.parsing.preprocessing import STOPWORDS

tweets = [
    ['human', 'interface', 'computer'],
    ['survey', 'user', 'computer', 'system', 'response', 'time', 'survey'],
    ['eps', 'user', 'interface', 'system'],
    ['system', 'human', 'system', 'eps'],
    ['user', 'response', 'time'],
    ['trees'],
    ['graph', 'trees'],
    ['graph', 'minors', 'trees'],
    ['graph', 'minors', 'survey']
]

dictionary = corpora.Dictionary(tweets)
dictionary.save('tweets.dict')  # store the dictionary, for future reference

dictionary = corpora.Dictionary.load('tweets.dict')
print(f'Length of previous dict = {len(dictionary)}, tokens = {dictionary.token2id}')
raw_corpus = [dictionary.doc2bow(t) for t in tweets]
corpora.MmCorpus.serialize('tweets.mm', raw_corpus)  # store to disk
print(&quot;Save the vectorized corpus as a .mm file&quot;)

corpus = corpora.MmCorpus('tweets.mm') # loading saved .mm file
print(corpus)

new_docs = [
[&quot;user&quot;, &quot;response&quot;, &quot;system&quot;],
[&quot;trees&quot;, &quot;minor&quot;, &quot;surveys&quot;]
]

# how to add this new_docs corpus to tweets.mm
</code></pre>
<p>Can <code>tweets.mm</code> be updated? Or is it recommended?</p>
","5741062","","6573902","","2020-11-13 06:30:28","2020-11-13 06:30:28","How to update .mm (market matrix) file with new docs (corpus)?","<python><nlp><gensim><word2vec><similarity>","1","0","","","","CC BY-SA 4.0"
"36491071","1","36495932","","2016-04-08 03:35:44","","1","234","<p>What is a text mining tool with some easy tutorials and active community? I found some popular but not sure which one to start with.  </p>
","1718652","","","","","2016-04-08 09:19:31","An easy tutorial for a tool that supports text classification, clustering and topic modeling","<weka><text-mining><gensim><topic-modeling><mallet>","1","0","","","","CC BY-SA 3.0"
"45152693","1","","","2017-07-17 20:01:01","","0","123","<p>I have a panda series around 300k lines (one text per line). I wanted to extract the output array for each line of a trained W2V on a testing data set using the following code:</p>

<pre><code>import gensim

w2v = gensim.models.Word2Vec(list(X_train), hs=1, negative=0)
l = [w2v.score(e) for e in list(X_test)]
</code></pre>

<p>Here is the error I received (when it does not kill my ipython session)</p>

<pre><code>error: can't start new thread
</code></pre>

<p>How can I solve this?</p>
","7729091","","1670134","","2017-07-17 20:20:33","2017-07-17 20:20:33","Gensim Word2Vec - can't start new thread","<python><multithreading><gensim><word2vec>","0","4","","","","CC BY-SA 3.0"
"21498633","1","","","2014-02-01 13:28:14","","1","1911","<p>I am trying to classify emails based on the subject-line, and I have to get the LSI in order to train the classifier. I am getting tf-idf and further trying to get LSI model. However, It does not do any processing/write to any file at all. My code is as below:</p>

<pre><code>#reading the list of subjects for features
f = open('subject1000.csv','rb')
f500 = open('subject500.csv','wb')

with open('subject1000.csv') as myfile:
    head=list(islice(myfile,500))#only 500 subjects for training

for h in head:
    f500.write(h)
    #print h

f500.close()    
texts = (line.lower().split() for line in head) #creating texts of subjects

dictionary = corpora.Dictionary(texts) #all the words used to create dictionary
dictionary.compactify()
print dictionary #checkpoint - 2215 unique tokens -- 2215 unique words to 1418 for 500 topics

#corpus streaming 
class MyCorpus(object):
    def __iter__(self):
        for line in open('subject500.csv','rb'): #supposed to be one document per line -- open('subject1000.csv','rb')
            yield dictionary.doc2bow(line.lower().split())  #every line - converted to bag-of-words format = list of (token_id, token_count) 2-tuples          
print 'corpus created'
corpus = MyCorpus() # object created

for vector in corpus:
    print vector

tfidf = models.TfidfModel(corpus)
corpus_tfidf= tfidf[corpus]  #re-initialize the corpus according to the model to get the normalized frequencies.
corpora.MmCorpus.serialize('subject500-tfidf', corpus_tfidf)  #store to disk for later use

print 'TFIDF complete!' #check - till here its ok

lsi300 = models.LsiModel(corpus_tfidf, num_topics=300, id2word=dictionary) #using the trained corpus to use LSI indexing
corpus_lsi300 = lsi300[corpus_tfidf]
print corpus_lsi300 #checkpoint
lsi300.print_topics(10,5) #checks
corpora.BleiCorpus.serialize('subjects500-lsi-300', corpus_lsi300)
</code></pre>

<p>I get the output till 'TFIDF complete!' but then the program does not return anything for LSI. I am running through 500 subject lines for the above. Any ideas on what might be going wrong will be very much appreciated! Thanks.</p>

<p>The logged data is as below:</p>

<pre><code>INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens)
INFO:gensim.corpora.dictionary:built Dictionary(1418 unique tokens) from 500 documents (total 3109 corpus positions)
DEBUG:gensim.corpora.dictionary:rebuilding dictionary, shrinking gaps
INFO:gensim.models.tfidfmodel:collecting document frequencies
INFO:gensim.models.tfidfmodel:PROGRESS: processing document #0
INFO:gensim.models.tfidfmodel:calculating IDF weights for 500 documents and 1418 features (3081 matrix non-zeros)
INFO:gensim.corpora.mmcorpus:storing corpus in Matrix Market format to subject500-tfidf
INFO:gensim.matutils:saving sparse matrix to subject500-tfidf
INFO:gensim.matutils:PROGRESS: saving document #0
INFO:gensim.matutils:saved 500x1418 matrix, density=0.435% (3081/709000)
DEBUG:gensim.matutils:closing subject500-tfidf
DEBUG:gensim.matutils:closing subject500-tfidf
INFO:gensim.corpora.indexedcorpus:saving MmCorpus index to subject500-tfidf.index
INFO:gensim.models.lsimodel:using serial LSI version on this node
INFO:gensim.models.lsimodel:updating model with new documents
INFO:gensim.models.lsimodel:preparing a new chunk of documents
DEBUG:gensim.models.lsimodel:converting corpus to csc format
INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations
INFO:gensim.models.lsimodel:1st phase: constructing (1418, 400) action matrix
INFO:gensim.models.lsimodel:orthonormalizing (1418, 400) action matrix
DEBUG:gensim.matutils:computing QR of (1418, 400) dense matrix
DEBUG:gensim.models.lsimodel:running 2 power iterations
DEBUG:gensim.matutils:computing QR of (1418, 400) dense matrix
DEBUG:gensim.matutils:computing QR of (1418, 400) dense matrix
INFO:gensim.models.lsimodel:2nd phase: running dense svd on (400, 500) matrix
</code></pre>
","2916950","","2916950","","2014-02-01 19:08:10","2014-06-02 19:47:02","Python LSI using gensim not working","<python><text-processing><gensim>","3","0","","","","CC BY-SA 3.0"
"62797525","1","","","2020-07-08 14:48:50","","-1","297","<p>I have a set of 20 small document which talks about a particular kind of issue (training data). Now i want to identify those docs out of 10K documents, which are talking about the same issue.</p>
<p>For the purpose i am using the doc2vec implementation:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
    
# Tokenize_and_stem is creating the tokens and stemming and returning the list
# documents_prb store the list of 20 docs
tagged_data = [TaggedDocument(words=tokenize_and_stem(_d.lower()), tags=[str(i)]) for i, _d in enumerate(documents_prb)]
max_epochs = 20
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)

model.build_vocab(tagged_data)
for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(&quot;d2v.model&quot;)
print(&quot;Model Saved&quot;)

model= Doc2Vec.load(&quot;d2v.model&quot;)
#to find the vector of a document which is not in training data
    
def doc2vec_score(s):
    s_list = tokenize_and_stem(s)
    v1 = model.infer_vector(s_list)
    similar_doc = model.docvecs.most_similar([v1])
    original_match = (X[int(similar_doc[0][0])])
    score = similar_doc[0][1]
    match = similar_doc[0][0]
    return score,match


final_data  = []

# df_ws is the list of 10K docs for which i want to find the similarity with above 20 docs
for index, row in df_ws.iterrows():
    print(row['processed_description'])
    data = (doc2vec_score(row['processed_description']))
    L1=list(data)
    L1.append(row['Number'])
    final_data.append(L1)
     
with open('file_cosine_d2v.csv','w',newline='') as out:
    csv_out=csv.writer(out)
    csv_out.writerow(['score','match','INC_NUMBER'])
    for row in final_data:
        csv_out.writerow(row)
</code></pre>
<p>But, I am facing the strange issue, the results are highly un-reliable (Score is 0.9 even if there is not a slightest match) and score is changing with great margin every time. I am running the <code>doc2vec_score</code> function. Can someone please help me what is wrong here ?</p>
","865389","","5512911","","2020-07-09 00:53:04","2020-07-09 00:53:04","Why doc2vec is giving different and un-reliable results?","<machine-learning><nlp><gensim><similarity><doc2vec>","1","0","","","","CC BY-SA 4.0"
"47325484","1","","","2017-11-16 09:04:42","","1","813","<p>I have an assignment that's something like this:</p>

<pre><code>import gensim
from sklearn.feature_extraction.text import CountVectorizer

newsgroup_data = [""Human machine interface for lab abc computer applications"",
             ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]

vect = CountVectorizer(stop_words='english', 
                       token_pattern='(?u)\\b\\w\\w\\w+\\b')
X = vect.fit_transform(newsgroup_data)
corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)
id_map = dict((v, k) for k, v in vect.vocabulary_.items())
</code></pre>

<p>My task is to estimate LDA model parameters on the corpus, find a list of the 10 topics and the most significant 10 words in each topic, which I do as such:</p>

<pre><code>top10 = ldamodel.print_topics(num_topics=10, num_words=10)
ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus, 
              id2word=id_map, num_topics=10, minimum_probability=0)
</code></pre>

<p>Which passes the autograder fine.  The next task is to find the topic distribution of a new doc which I attempt to do as follows:</p>

<pre><code>new_doc = [""\n\nIt's my understanding that the freezing will start to occur because \
of the\ngrowing distance of Pluto and Charon from the Sun, due to it's\nelliptical orbit. \
It is not due to shadowing effects. \n\n\nPluto can shadow Charon, and vice-versa.\n\nGeorge \
Krumins\n-- ""]
newX = vect.transform(new_doc)
newC = gensim.matutils.Sparse2Corpus(newX, documents_columns=False)
print(ldamodel.get_document_topics(newC))
</code></pre>

<p>This however simply returns </p>

<p><code>gensim.interfaces.TransformedCorpus</code></p>

<p>I also see from the docs the statement: ""You can then infer topic distributions on new, unseen documents, with >>> doc_lda = lda[doc_bow]"" but have no success here either.  Any help appreciated.</p>
","2643911","","","","","2017-11-16 20:27:33","topic distristribution in gensim ldamodel trained with countvectorizer","<python-3.x><gensim><topic-modeling><countvectorizer>","1","0","","","","CC BY-SA 3.0"
"52514911","1","","","2018-09-26 09:54:01","","1","2182","<p>I have gensim installed in my system. I did the summarization with gensim. NOw I want to  find the similarity between the sentence and it showing an error. sample code is given below. I have downloaded the Google news vectors.</p>

<pre><code>from gensim.models import KeyedVectors

#two sample sentences
s1 = 'the first sentence'
s2 = 'the second text'
#model = gensim.models.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)
model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True)
#calculate distance between two sentences using WMD algorithm
distance = model.wmdistance(s1, s2)

print ('distance = %.3f' % distance)
</code></pre>

<blockquote>
  <p>Error#################################################</p>
  
  <p>****Traceback (most recent call last):   File ""/home/abhi/Desktop/CHiir/CLustering &amp;
  summarization/.idea/FInal_version/sentence_embedding.py"", line 7, in
  
      model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz',
  binary=True) NameError: name 'gensim' is not defined****</p>
</blockquote>
","5587508","","","","","2019-01-27 07:04:25","NameError: name 'gensim' is not defined (doc2vec similarity)","<similarity><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"61310229","1","61326599","","2020-04-19 19:17:45","","2","168","<p>I want to use the HDP model from <code>gensim</code> to get the number of topics for my corpus, I already used this corpus and dictionary to train a regular LDA model from <code>gensim</code> and it works fine. But now when I do</p>

<pre><code>hdp = models.HdpModel(bow_corpus, dictionary)
</code></pre>

<p>I get</p>

<pre><code>Traceback (most recent call last):
  File ""models.py"", line 185, in &lt;module&gt;
    hdp = models.HdpModel(bow_corpus, dictionary)
  File ""/usr/lib/python3.8/site-packages/gensim/models/hdpmodel.py"", line 391, in __init__
    self.update(corpus)
  File ""/usr/lib/python3.8/site-packages/gensim/models/hdpmodel.py"", line 467, in update
    start_time = time.clock()
AttributeError: module 'time' has no attribute 'clock'
</code></pre>

<p>Is this a bug?</p>

<pre><code>$ python --version
Python 3.8.2 (default, Feb 26 2020, 22:21:03) 
</code></pre>

<p>Edit to add more system information</p>

<pre><code>&gt;&gt;&gt; print(gensim.__version__)
3.8.1

uname -a
Linux ** 5.5.9-arch1-2 #1 SMP PREEMPT Thu, 12 Mar 2020 23:01:33 +0000 x86_64 GNU/Linux
</code></pre>
","1269703","","1269703","","2020-04-22 11:34:06","2020-04-22 11:34:06","Is this a bug on gensim hdp model for python 3.8?","<python-3.x><time><gensim><python-3.8>","1","1","","","","CC BY-SA 4.0"
"55815556","1","","","2019-04-23 16:23:02","","1","275","<p>LDA shows 10 number of words in a topic by default. I want to increase these numbers by 15. I have tried ""topn"" and ""num_words"" keywords but both are giving me an error. how can I change this default behaviour?</p>

<pre><code>model = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=8,topn=15,chunksize=10000, passes=30,iterations=300)
</code></pre>

<p>Error is </p>

<pre><code>    model = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=8,topn=15,chunksize=10000, passes=30,iterations=300)
TypeError: __init__() got an unexpected keyword argument 'topn'
</code></pre>
","3778289","","4076315","","2019-04-25 18:33:33","2019-04-25 18:33:33","How to change the default number of words in LdaMulticore?","<python><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"61746512","1","61761662","","2020-05-12 07:56:42","","0","490","<p>I am working with Gensim FASTText modeling and have the following questions.</p>

<ul>
<li>The output of ""ft_model.save(BASE_PATH + MODEL_PATH + fname)"" saves the following 3 files. Is this correct? is there a way to combine all three files? </li>
</ul>

<blockquote>
<pre><code>ft_gensim-v3
ft_gensim-v3.trainables.vectors_ngrams_lockf.npy
ft_gensim-v3.wv.vectors_ngrams.npy
</code></pre>
</blockquote>

<p>When I attempt to load the training file and then use it, I get the following error from <code>if model.wv.similarity(real_data, labelled['QueryText'][i]) &gt; maxSimilaity:</code>  </p>

<blockquote>
  <p>'function' object has no attribute 'wv'</p>
</blockquote>

<p>Finally, both models, is there a way not to have to store the output of <code>def read_train(path,label_path)</code> and <code>def lemmetize(df_col)</code>so I do not have to run this part of the code every time I want to train the model or compare? </p>

<p>Thanks for the assistance. </p>

<p><strong>Here is my FastText Train Model</strong></p>

<pre><code>import os
import logging
from config import BASE_PATH, DATA_PATH, MODEL_PATH
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from pprint import pprint as print
from gensim.models.fasttext import FastText as FT_gensim
from gensim.test.utils import datapath

#Read Training data
import pandas as pd
def read_train(path,label_path):
    d = []
    #e = []
    df = pd.read_excel(path)
    labelled = pd.read_csv(label_path)
    updated_col1 = lemmetize(df['query_text'])
    updated_col2 = lemmetize(labelled['QueryText'])
    for i in range(len(updated_col1)):
        d.append(updated_col1[i])
        #print(d)
    for i in range(len(updated_col2)):
        d.append(updated_col2[i])
    return d


from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import string
from nltk.stem import PorterStemmer

def lemmetize(df_col):
    df_updated_col = pd.Series(0, index = df_col.index)
    stop_words = set(stopwords.words('english'))
    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()
    ps = PorterStemmer()
    for i, j in zip(df_col, range(len(df_col))):
        lem = []
        t = str(i).lower()
        t = t.replace(""'s"","""")
        t = t.replace(""'"","""")
        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))
        t = t.translate(translator)
        word_tokens = word_tokenize(t)
        for i in range(len(word_tokens)):
            l1 = lemmatizer.lemmatize(word_tokens[i])
            s1 = ps.stem(word_tokens[i])
            if list(l1) != [''] and list(l1) != [' '] and l1 != '' and l1 != ' ':
                lem.append(l1)
        filtered_sentence = [w for w in lem if not w in stop_words]
        df_updated_col[j] = filtered_sentence
    return df_updated_col

#read test data
def read_test(path):
    return pd.read_excel(path)


#Read labelled data
def read_labelled(path):
    return pd.read_csv(path)


word_tokenized_corpus = read_train('Train Data.xlsx','SMEQueryText.csv')


#Train fasttext model
import tempfile
import os

from gensim.models import FastText
from gensim.test.utils import get_tmpfile
fname = get_tmpfile(""ft_gensime-v3"")

def train_fastText(data, embedding_size = 60, window_size = 40, min_word = 5, down_sampling = 1e-2, iter=100):
    ft_model = FastText(word_tokenized_corpus,
                      size=embedding_size,
                      window=window_size,
                      min_count=min_word,
                      sample=down_sampling,
                      sg=1,
                      iter=100)

    #with tempfile.NamedTemporaryFile(prefix=BASE_PATH + MODEL_PATH + 'ft_gensim_v2-', delete=False) as tmp:
    #    ft_model.save(tmp.name, separately=[])
    ft_model.save(BASE_PATH + MODEL_PATH + fname)
    return ft_model


# main function to output
def main(test_path, train_path, labelled):
    test_data = read_test(test_path)
    train_data = read_train(train_path,labelled)
    labelled = read_labelled(labelled)
    output_df = pd.DataFrame(index = range(len(test_data)))
    output_df['test_query'] = str()
    output_df['Similar word'] = str()
    output_df['category'] = str()
    output_df['similarity'] = float()
    model = train_fastText(train_data)

# run main
if __name__ == ""__main__"":
    output = main('Test Data.xlsx','Train Data.xlsx','QueryText.csv')
</code></pre>

<p><strong>Here is my Usage Model</strong></p>

<pre><code>import pandas as pd
from gensim.models import FastText
import gensim
from config import BASE_PATH, DATA_PATH, MODEL_PATH

#Read Training data
def read_train(path,label_path):
    d = []
    #e = []
    df = pd.read_excel(path)
    labelled = pd.read_csv(label_path)
    updated_col1 = lemmetize(df['query_text'])
    updated_col2 = lemmetize(labelled['QueryText'])
    for i in range(len(updated_col1)):
        d.append(updated_col1[i])
    for i in range(len(updated_col2)):
        d.append(updated_col2[i])
    return d

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import string
from nltk.stem import PorterStemmer

def lemmetize(df_col):
    df_updated_col = pd.Series(0, index = df_col.index)
    stop_words = set(stopwords.words('english'))
    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()
    ps = PorterStemmer()
    for i, j in zip(df_col, range(len(df_col))):
        lem = []
        t = str(i).lower()
        t = t.replace(""'s"","""")
        t = t.replace(""'"","""")
        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))
        t = t.translate(translator)
        word_tokens = word_tokenize(t)
        for i in range(len(word_tokens)):
            l1 = lemmatizer.lemmatize(word_tokens[i])
            s1 = ps.stem(word_tokens[i])
            if list(l1) != [''] and list(l1) != [' '] and l1 != '' and l1 != ' ':
                lem.append(l1)
        filtered_sentence = [w for w in lem if not w in stop_words]
        df_updated_col[j] = filtered_sentence
    return df_updated_col

#read test data
def read_test(path):
    return pd.read_excel(path)

#Read labelled data
def read_labelled(path):
    return pd.read_csv(path)

def load_training():
    return FT_gensim.load(BASE_PATH + MODEL_PATH +'ft_gensim-v3')

#compare similarity
def compare_similarity(model, real_data, labelled):
    maxWord = ''
    category = ''
    maxSimilaity = 0
    #print(""train data"",labelled[1])
    for i in range(len(labelled)):
        if model.similarity(real_data, labelled['QueryText'][i]) &gt; maxSimilaity:
            #print('labelled',labelled['QueryText'][i], 'i', i)
            maxWord = labelled['QueryText'][i]
            category = labelled['Subjectmatter'][i]
            maxSimilaity = model.similarity(real_data, labelled['QueryText'][i])

    return maxWord, category, maxSimilaity

# Output from Main to excel
from pandas import ExcelWriter
def export_Excel(data, aFile = 'FASTTEXTOutput.xlsx'):
    df = pd.DataFrame(data)
    writer = ExcelWriter(aFile)
    df.to_excel(writer,'Sheet1')
    writer.save()

# main function to output
def main(test_path, train_path, labelled):
    test_data = read_test(test_path)
    train_data = read_train(train_path,labelled)
    labelled = read_labelled(labelled)
    output_df = pd.DataFrame(index = range(len(test_data)))
    output_df['test_query'] = str()
    output_df['Similar word'] = str()
    output_df['category'] = str()
    output_df['similarity'] = float()
    model = load_training
    for i in range(len(test_data)):
        output_df['test_query'][i] = test_data['query_text'][i]
        #&lt;first change&gt;
        maxWord, category, maxSimilaity = compare_similarity(model, str(test_data['query_text'][i]), labelled)
        output_df['Similar word'][i] = maxWord
        output_df['category'][i] = category
        output_df['similarity'][i] = maxSimilaity
    #&lt;second change&gt;    
    return output_df

# run main
if __name__ == ""__main__"":
    output = main('Test Data.xlsx','Train Data.xlsx','SMEQueryText.csv')
    export_Excel(output)
</code></pre>

<p><strong>Here is the full tracible error message</strong></p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-22-57803b59c0b9&gt; in &lt;module&gt;
      1 # run main
      2 if __name__ == ""__main__"":
----&gt; 3     output = main('Test Data.xlsx','Train Data.xlsx','SMEQueryText.csv')
      4     export_Excel(output)

&lt;ipython-input-21-17cb88ee0f79&gt; in main(test_path, train_path, labelled)
     13         output_df['test_query'][i] = test_data['query_text'][i]
     14         #&lt;first change&gt;
---&gt; 15         maxWord, category, maxSimilaity = compare_similarity(model, str(test_data['query_text'][i]), labelled)
     16         output_df['Similar word'][i] = maxWord
     17         output_df['category'][i] = category

&lt;ipython-input-19-84d7f268d669&gt; in compare_similarity(model, real_data, labelled)
      6     #print(""train data"",labelled[1])
      7     for i in range(len(labelled)):
----&gt; 8         if model.wv.similarity(real_data, labelled['QueryText'][i]) &gt; maxSimilaity:
      9             #print('labelled',labelled['QueryText'][i], 'i', i)
     10             maxWord = labelled['QueryText'][i]

AttributeError: 'function' object has no attribute 'wv'
</code></pre>
","6195964","","6195964","","2020-05-12 17:31:40","2020-05-12 20:51:25","Python Gensim FastText Saving and Loading Model","<python><gensim><fasttext>","1","2","1","","","CC BY-SA 4.0"
"43922531","1","","","2017-05-11 17:55:58","","0","908","<p>In order to make it clear, I would like to get your feedback whether the following code/gensim-usage is right or not? </p>

<p>Thank you in advance for your valuable time. </p>

<pre><code>import gensim    

train = [""John likes to watch movies Mary likes movies too"" ,
         ""John also likes to watch football games"" ]

test = [""Football is my dream""]

train_texts = [[word for word in document.lower().split()] for document in train]
test_texts = [[word for word in document.lower().split()] for document in test]

dictionary =gensim.corpora.Dictionary(train_texts)

train_corpus = [dictionary.doc2bow(text) for text in train_texts]
test_corpus = [dictionary.doc2bow(text) for text in test_texts]

ldaModel = gensim.models.LdaModel(corpus=train_corpus , 
             id2word=dictionary , num_topics=2)
bound_perplex = ldaModel.bound(test_corpus)
</code></pre>
","3013290","","","","","2017-05-14 06:10:59","id2word_token2Id usage confusion in Gensim","<python><python-2.7><python-3.x><gensim>","1","1","1","","","CC BY-SA 3.0"
"45170589","1","","","2017-07-18 15:05:37","","0","390","<p>When training, what will word2vec do to cope with the words at the end of a sentence . Will it use the exact words at the beginning of another sentence as the context words of the center words which is 
at the end of last sentence.  </p>
","8322066","","","","","2017-07-18 16:48:58","How word2vec deal with the end of a sentence","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"52845345","1","","","2018-10-16 23:35:08","","3","1824","<p>I am trying to implement the following code:</p>

<pre><code>import os
os.environ.update({'MALLET_HOME':r'c:/mallet-2.0.8/'})

mallet_path = 'C:\\mallet-2.0.8\\bin\\mallet'
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=bow, num_topics=20, id2word=dictionary)
</code></pre>

<p>However, I keep getting this error: </p>

<blockquote>
  <p>CalledProcessError: Command 'C:\mallet-2.0.8\bin\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\Joshua\AppData\Local\Temp\98094d_corpus.txt --output C:\Users\Joshua\AppData\Local\Temp\98094d_corpus.mallet' returned non-zero exit status 1.</p>
</blockquote>

<p>I previously was able to execute this code on my laptop with the same directories yet it does not execute on my PC (where I am currently running python). </p>

<p>Could someone please let me know what I am doing wrong?</p>
","8705465","","","","","2019-04-02 17:54:16","LDA Mallet CalledProcessError","<python-3.x><gensim><lda><mallet>","1","0","","","","CC BY-SA 4.0"
"47300490","1","47314021","","2017-11-15 06:09:06","","2","1743","<p>I know to obtain a document vector for a given tag in doc2vec using <code>print(model.docvecs['recipe__11'])</code>.</p>

<p>My document vectors are either recipes (tags start with <code>recipe__</code>), newspapers (tags start with <code>news__</code>) or ingredients (tags start with <code>ingre__</code>)</p>

<p>Now I want to retrieve all the document vectors of recipes. The pattern of my recipe documents is <code>recipe__&lt;some number&gt;</code> (e.g., recipe__23, recipe__34). I am interested in knowing if it possible to obtain multiple document vectors using a pattern (e.g., tags starting with <code>recipe__</code>)</p>

<p>Please help me!</p>
","","user8566323","","user8566323","2017-11-15 06:27:25","2017-11-15 17:42:38","How to obtain document vectors in doc2vec in gensim","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"61625273","1","","","2020-05-06 00:22:50","","0","63","<p>I have tokenized my strings and made a Pandas column out of them and if I print the column <code>df['word_splits']</code> it looks like this.</p>

<pre><code>0    ['explanation', 'why', 'the', 'edits', 'made',...
1    [""d'aww"", '!', 'he', 'matches', 'this', 'backg...
2    ['hey', 'man', ',', ""i'm"", 'really', 'not', 't...
3    ['more', 'i', ""can't"", 'make', 'any', 'real', ...
4    ['you', ',', 'sir', ',', 'are', 'my', 'hero', ...
Name: word_splits, dtype: object
</code></pre>

<p>Next, I'm running Word2Vec</p>

<pre><code>model = gensim.models.Word2Vec(sentences=df[""word_splits""])
</code></pre>

<p>When I print out the vocabulary, using</p>

<pre><code>words = list(model.wv.vocab)
print(words)
</code></pre>

<p>I'm getting characters instead of a long list of words (vocabulary).</p>

<pre><code>['[', ""'"", 'e', 'x', 'p', 'l', 'a', 'n', 't', 'i', 'o', ',', ' ', 'w', 'h', 'y', 'd', 's', 'm', 'u', 'r', 'c', 'f', 'v', '?', '""', 'j', 'g', 'k', '.', ']', '!', 'b', '-', 'q', 'z']
</code></pre>

<p>Not sure what I'm doing wrong. </p>
","12486467","","","","","2020-05-06 04:40:51","Word2Vec Giving Characters instead of Words","<python><pandas><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"64630194","1","64630450","","2020-11-01 09:01:06","","2","507","<p>I have a text file that contains the content of a web page that I have extracted using BeautifulSoup. I need to find N similar words from the text file based on a given word. The process is as follows:</p>
<ol>
<li>The website from which text was extracted: <a href=""https://en.wikipedia.org/wiki/Football"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Football</a></li>
<li>The extracted text is saved to a text file.</li>
<li>The User inputs a word, ex: &quot;goal&quot; and I have to display the top N most similar words from the text file.</li>
</ol>
<p>I have only worked in Computer Vision and completely new to NLP. I'm currently stuck in step 3. I have tried Spacy and Gensim, but my approach is not at all efficient. I currently do this:</p>
<pre class=""lang-py prettyprint-override""><code>for word in ['goal', 'soccer']:
    # 1. compute similarity using spacy for each word in the text file with the given word.
    # 2. sort them based on the scores and choose the top N-words.
</code></pre>
<p>Is there any other approach or a simple solution to solve this problem? Any help is appreciated. Thanks!</p>
","9218531","","4317058","","2020-11-01 11:00:33","2020-11-01 11:00:33","Extract top N words that are most similar to an input word from a text file","<python><deep-learning><nlp><spacy><gensim>","1","2","1","","","CC BY-SA 4.0"
"64651943","1","64652058","","2020-11-02 19:19:35","","1","46","<p>I try to make an attempt like <a href=""https://stackoverflow.com/questions/32476336/how-to-abstract-bigram-topics-instead-of-unigrams-using-latent-dirichlet-allocat"">this</a> question</p>
<p>LDA Original Output</p>
<pre><code>Uni-grams

    topic1 -scuba,water,vapor,diving

    topic2 -dioxide,plants,green,carbon
</code></pre>
<p>Required Output</p>
<pre><code>Bi-gram topics

    topic1 -scuba diving,water vapor

    topic2 -green plants,carbon dioxide
</code></pre>
<p>And there is this answer</p>
<pre><code>from nltk.util import ngrams

for doc in docs:
    docs[doc] = docs[doc] + [&quot;_&quot;.join(w) for w in ngrams(docs[doc], 2)]
</code></pre>
<p>Any help what update should I make in order to have only bigrams?</p>
","14544121","","14544121","","2020-11-02 22:08:10","2020-11-02 22:08:10","Capture bigram topics instead of unigrams using latent dirichlet allocat","<python><nltk><gensim><n-gram>","1","0","","","","CC BY-SA 4.0"
"57599259","1","","","2019-08-21 21:14:47","","2","5824","<p>I am building a multilabel text classification program and I am trying to use OneVsRestClassifier+XGBClassifier to classify the text. Initially I used Sklearn's Tf-Idf Vectorization to vectorize the texts, which worked without error. Now I am using <strong>Gensim's Word2Vec</strong> to vectorize the texts. When I feed the vectorized data into the OneVsRestClassifier+XGBClassifier however, I get the following error on the line where I split the test and training data:</p>

<blockquote>
  <p>TypeError: Singleton array array(,
        dtype=object) cannot be considered a valid collection.</p>
</blockquote>

<p>I have tried converting the vectorized data into a feature array (np.array), but that hasn't seemed to work.
Below is my code:</p>

<pre><code>x = np.array(Word2Vec(textList, size=120, window=6, min_count=5, workers=7, iter=15))

vectorizer2 = MultiLabelBinarizer()
vectorizer2.fit(tagList)
y = vectorizer2.transform(tagList)

# Split test data and convert test data to arrays
xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.20)
</code></pre>

<p>The variables <code>textList</code> and <code>tagList</code> are a list of strings (textual descriptions I am trying to classify).</p>
","3950533","","3950533","","2019-08-21 21:36:39","2019-08-22 15:59:03","Sklearn+Gensim: How to use Gensim's Word2Vec embedding for Sklearn text classification","<python><machine-learning><scikit-learn><gensim>","1","0","","","","CC BY-SA 4.0"
"61337315","1","","","2020-04-21 06:34:44","","0","275","<p>When we visualize the LDA using pyLDAvis, we can see topic overlap. I want know the word that is causing this topic overlap. Or I want to know the words that are at the intersection of the topic bubbles. Any guidance is appreciated.</p>
","3910430","","","","","2020-04-21 13:02:09","Common words that cause topic overlap in gensim LDA","<python-3.x><gensim><lda><topic-modeling><pyldavis>","1","2","","","","CC BY-SA 4.0"
"61337725","1","61338326","","2020-04-21 07:03:49","","0","225","<p>I have some volunteer essay writings in the format of:</p>

<pre><code>volunteer_names, essay
[""emi"", ""jenne"", ""john""], [[""lets"", ""protect"", ""nature""], [""what"", ""is"", ""nature""], [""nature"", ""humans"", ""earth""]]
[""jenne"", ""li""], [[""lets"", ""manage"", ""waste""]]
[""emi"", ""li"", ""jim""], [[""python"", ""is"", ""cool""]]
...
...
...
</code></pre>

<p>I want to identify the similar users based on their essay writings. I feel like word2vec is more suitable in problems like this. However, since I want to embed user names too in the model I am not sure how to do it. The examples I found in the internet only uses the words (See example code).</p>

<pre><code>import gensim 
sentences = [['first', 'sentence'], ['second', 'sentence']]
# train word2vec on the two sentences
model = gensim.models.Word2Vec(sentences, min_count=1)
</code></pre>

<p>In that case, I am wondering if there is special way of doing this in word2vec or can I simply consider user names as just words to input to the model. please let me know your thoughts on this.</p>

<p>I am happy to provide more details if needed.</p>
","10704050","","10704050","","2020-04-21 08:16:26","2020-04-21 08:16:26","How to embed user names in word2vec model in gensim","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"29932784","1","","","2015-04-29 01:23:12","","5","1138","<p>I'm new to the <a href=""https://radimrehurek.com/gensim/install.html"" rel=""noreferrer"">gensim</a> package and vector space models in general, and I'm unsure of <em>what exactly I should do with my LSA output.</em> </p>

<p>To give a brief overview of my goal, I'd like to enhance Naive Bayes Classifier using topic modeling to improve classification of reviews (positive or negative). Here's a <a href=""http://www.aclweb.org/anthology/I13-1158"" rel=""noreferrer"">great paper</a> I've been reading that has shaped my ideas but left me still somewhat confused about implementation..</p>

<p>I've already got working code for Naive Bayes--currently, I'm just using unigram bag of words as my features and labels are either positive or negative.</p>

<p>Here's my gensim code</p>

<pre><code>from pprint import pprint # pretty printer
import gensim as gs

# tutorial sample documents
docs = [""Human machine interface for lab abc computer applications"",
              ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]


# stoplist removal, tokenization
stoplist = set('for a of the and to in'.split())
# for each document: lowercase document, split by whitespace, and add all its words not in stoplist to texts
texts = [[word for word in doc.lower().split() if word not in stoplist] for doc in docs]


# create dict
dict = gs.corpora.Dictionary(texts)
# create corpus
corpus = [dict.doc2bow(text) for text in texts]

# tf-idf
tfidf = gs.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

# latent semantic indexing with 10 topics
lsi = gs.models.LsiModel(corpus_tfidf, id2word=dict, num_topics =10)

for i in lsi.print_topics():
    print i
</code></pre>

<p>Here's output</p>

<pre><code>0.400*""system"" + 0.318*""survey"" + 0.290*""user"" + 0.274*""eps"" + 0.236*""management"" + 0.236*""opinion"" + 0.235*""response"" + 0.235*""time"" + 0.224*""interface"" + 0.224*""computer""
0.421*""minors"" + 0.420*""graph"" + 0.293*""survey"" + 0.239*""trees"" + 0.226*""paths"" + 0.226*""intersection"" + -0.204*""system"" + -0.196*""eps"" + 0.189*""widths"" + 0.189*""quasi""
-0.318*""time"" + -0.318*""response"" + -0.261*""error"" + -0.261*""measurement"" + -0.261*""perceived"" + -0.261*""relation"" + 0.248*""eps"" + -0.203*""opinion"" + 0.195*""human"" + 0.190*""testing""
0.416*""random"" + 0.416*""binary"" + 0.416*""generation"" + 0.416*""unordered"" + 0.256*""trees"" + -0.225*""minors"" + -0.177*""survey"" + 0.161*""paths"" + 0.161*""intersection"" + 0.119*""error""
-0.398*""abc"" + -0.398*""lab"" + -0.398*""machine"" + -0.398*""applications"" + -0.301*""computer"" + 0.242*""system"" + 0.237*""eps"" + 0.180*""testing"" + 0.180*""engineering"" + 0.166*""management""
</code></pre>

<p>Any suggestions or general comments would be appreciated.</p>
","4883434","","","","","2016-11-01 19:55:35","combining LSA/LSI with Naive Bayes for document classification","<document-classification><gensim><naivebayes><latent-semantic-indexing><latent-semantic-analysis>","1","1","","","","CC BY-SA 3.0"
"53030121","1","53032208","","2018-10-28 09:37:13","","3","203","<p>I am using Google-App-Engine standard (Not flex) Enviroment with Python2.7, and I need to load some pre-trained models (Gensim's Word2vec and Keras's LSTM).</p>

<p>I need to load it once (since it very slow - takes around 1.5 seconds) and keep it in faster access for several hours.</p>

<p>What is the best &amp; fastest way to do so? </p>

<p>Thanks!</p>
","10117402","","","","","2018-10-28 13:48:45","How to load files to Google-App-Engine in standard enviroment","<python><google-app-engine><keras><gensim><google-app-engine-python>","1","1","","","","CC BY-SA 4.0"
"52743468","1","","","2018-10-10 15:13:01","","0","213","<p>I'm trying to train a gensim sgns model and in the process I measure the loss during which I'm calculating as  </p>

<pre><code>loss = model.running_training_loss / model.corpus_count, 
</code></pre>

<p>however, I noticed that if I change my worker thread I get different losses keeping all other parameters same. Especially if I keep my worker thread a 1 I get a really high loss and If I increase threads I get less loss. An instance</p>

<pre><code>thread  loss
worker=1  20.40519721
worker=10   2.714875407
worker=16  1.239528453
</code></pre>
","2717058","","","","","2018-10-10 17:38:48","effect of increase worker thread in gensim word2vec","<multithreading><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"44371835","1","44374636","","2017-06-05 15:00:06","","1","1619","<p>I want to import word vecters created from tensorflow and utilize it at gensim.</p>

<p>there is a method <code>gensim.models.KeyedVectors.load_word2vec_format</code></p>

<p>so I tried this method by following exactly the same way in <a href=""https://stackoverflow.com/questions/42186543/training-wordvec-in-tensorflow-importing-to-gensim"">Training wordvec in Tensorflow, importing to Gensim</a></p>

<p>Example:</p>

<blockquote>
  <p>2 3</p>
  
  <p>word0 -0.000737 -0.002106 0.001851</p>
  
  <p>word1 -0.000878 -0.002106 0.002834</p>
</blockquote>

<p>Save the file and then load with kwarg binary=False:</p>

<pre><code>model = Word2Vec.load_word2vec_format(filename, binary=False)
</code></pre>

<p>but error like</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    model=gensim.models.KeyedVectors.load_word2vec_format('test.w2v')
  File ""C:\Users\cbj\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 243, in load_word2vec_format
    raise EOFError(""unexpected end of input; is count incorrect or file otherwise damaged?"")
EOFError: unexpected end of input; is count incorrect or file otherwise damaged?
</code></pre>

<p>raised</p>

<p>how can I solve this problem?</p>
","6580055","","","","","2017-06-05 17:41:28","Importing word vectors from tensorflow into gensim","<python><tensorflow><gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 3.0"
"43881924","1","43989899","","2017-05-10 00:49:37","","1","590","<p>I had trained a paragraph vector model from gensim by using a considerable amount text data. I did the next test: I verified the index of any sentence and then inferred a vector for it</p>

<pre><code>&gt;&gt;&gt; x=m.docvecs[18638]
&gt;&gt;&gt; g=m.infer_vector(""The seven OxyR target sequences analyzed previously and two new sites grxA at position 207 in GenBank entry M13449 and a second Mu phage mom site at position 59 in GenBank entry V01463 were used to generate an individual information weight matrix"".split())
</code></pre>

<p>When I computed the cosine similarity, it was very low (the opposite is expected). </p>

<pre><code>&gt;&gt;&gt; 1 - spatial.distance.cosine(g, x)
0.20437437837633066
</code></pre>

<p>Can someone tell me if I'm doing something wrong, please?</p>

<p>Thanks</p>
","3101193","","5545946","","2018-11-28 10:43:05","2018-11-28 10:43:05","inconsistent similarity betwen inferred and trained vectors in doc2vec","<python><gensim><doc2vec>","2","0","","","","CC BY-SA 3.0"
"64785169","1","","","2020-11-11 11:01:04","","1","419","<p>I'm using the function  <code>gensim.models.LdaMulticore</code> for training an LDA model on a large corpus. The problem is I have no idea when it's going to finish the process. Is there a way to show the progress in the process in this function?</p>
","5516760","","","","","2021-01-23 14:30:31","show progress in gensim LdaMulticore","<python><gensim>","1","3","","","","CC BY-SA 4.0"
"64633315","1","","","2020-11-01 15:01:41","","0","478","<p>I would like to use a Word2Vec model pre-trained on Italian wikipedia. Do you know if it already exists or, if it does not, if it would be possible to build one? I have no such experience with gensim, but I would need to use a good model with word-embeddings for Italian language.</p>
<p>Thank you for all your answers.</p>
","","user14289862","","","","2020-11-01 15:03:52","Word2Vec model on Italian Wikipedia","<python><gensim><word2vec><word-embedding>","1","0","","2020-11-02 01:35:37","","CC BY-SA 4.0"
"17662916","1","17663094","","2013-07-15 20:06:08","","8","6660","<p>The <code>lda.show_topics</code> module from the following code only prints the distribution of the top 10 words for each topic, how do i print out the full distribution of all the words in the corpus?</p>

<pre><code>from gensim import corpora, models

documents = [""Human machine interface for lab abc computer applications"",
""A survey of user opinion of computer system response time"",
""The EPS user interface management system"",
""System and human system engineering testing of EPS"",
""Relation of user perceived response time to error measurement"",
""The generation of random binary unordered trees"",
""The intersection graph of paths in trees"",
""Graph minors IV Widths of trees and well quasi ordering"",
""Graph minors A survey""]

stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)

for i in lda.show_topics():
    print i
</code></pre>
","610569","","792066","","2015-08-15 04:25:46","2017-10-19 18:32:08","How to print out the full distribution of words in an LDA topic in gensim?","<python><lda><topic-modeling><gensim>","3","3","2","","","CC BY-SA 3.0"
"47325820","1","","","2017-11-16 09:23:24","","2","5231","<p>I am using Gensim to load my fasttext <code>.vec</code> file as follows.</p>

<pre><code>m=load_word2vec_format(filename, binary=False)
</code></pre>

<p>However, I am just confused if I need to load <code>.bin</code> file to perform commands like <code>m.most_similar(""dog"")</code>, <code>m.wv.syn0</code>, <code>m.wv.vocab.keys()</code> etc.?  If so, how to do it?</p>

<p>Or <code>.bin</code> file is not important to perform this cosine similarity matching?</p>

<p>Please help me!</p>
","","user8871463","","user8871463","2017-11-16 11:05:13","2021-06-11 22:00:59","FastText in Gensim","<python><word2vec><gensim><fasttext>","4","0","","","","CC BY-SA 3.0"
"64628163","1","64628728","","2020-11-01 02:25:08","","1","52","<p>I have a dataset</p>
<pre><code>       Title                                                Year
0   Sport, there will be a match between United and Tottenham ...   2020
1   Forecasting says that it will be cold next week                 2019
2   Sport, Mourinho is approaching the anniversary at Tottenham     2020
3   Sport, Tottenham are sixth favourites for the title behind Arsenal. 2020
4   Pochettino says clear-out of fringe players at Tottenham is inevitable.     2018
... ... ...
</code></pre>
<p>I would like to study the text similarity within the same year, rather in the whole dataset. To find most similar texts, I am using the WM distance similarity.
For two text would be:</p>
<pre><code>word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
distance = word2vec_model.wmdistance(&quot;string 1&quot;.split(), &quot;string 2&quot;.split())
</code></pre>
<p>However I would need to iterate the distance through sentences in the same year to get the similarity of each text with others, creating a list of similar text per row in the dataframe.
Could you please tell me how to iterate the wmdistance function across text published in the same year, in order to get for each text the most similar ones within the same period?</p>
","","user12809368","","user12809368","2020-11-01 03:29:11","2020-11-01 04:43:21","Text similarity using WMD within the same time period","<python><pandas><gensim><word2vec><similarity>","1","0","","","","CC BY-SA 4.0"
"64638775","1","","","2020-11-02 01:38:43","","0","31","<p>The idea is to update a particular pre-trained word2vec model with different sets of new corpus. I have the following</p>
<pre><code># c1, c2 are each a list of 100 files
filelist = [c1, c2, c3, c4, c5, c6, c7, c8, c9, c10]

def update_model(files):
    # loading a pre-trained model
    trained_model = gensim.models.Word2Vec.load(&quot;model_both_100&quot;)
    # Document feeder is an iterable
    docs = DocumentFeeder(files)
    trained_model.build_vocab(docs, update=True)
    trained_model.train(docs, total_examples=trained_model.corpus_count, epochs=trained_model.epochs)

with Pool(processes=10) as P:
    P.map(update_model, filelist)
</code></pre>
<p>it takes about ~13 minutes to run. But the non-parallel version (looping over <code>filelist</code>) takes ~11 min. Why is this happening? Running on a 12 core cpu.</p>
","8464088","","","","","2020-11-02 02:37:40","Why parallel processing taking longer than usual code?","<parallel-processing><nlp><python-multiprocessing><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"44045881","1","","","2017-05-18 10:57:39","","0","2316","<p>I'm trying to load the pre-trained words2vecs which I've found here (<a href=""https://github.com/mmihaltz/word2vec-GoogleNews-vectors"" rel=""nofollow noreferrer"">https://github.com/mmihaltz/word2vec-GoogleNews-vectors</a>)
I used the following command:</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('word2vec.bin.gz', binary=False)
</code></pre>

<h2>And it throws this error:</h2>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/deeplearning/anaconda3/lib/python3.6/site-
packages/gensim/models/keyedvectors.py"", line 193, in 
 load_word2vec_format
    header = utils.to_unicode(fin.readline(), encoding=encoding)
  File ""/home/deeplearning/anaconda3/lib/python3.6/gzip.py"", line 374, 
in readline
    return self._buffer.readline(size)
  File ""/home/deeplearning/anaconda3/lib/python3.6/_compression.py"", 
line 68, in readinto
    data = self.read(len(byte_view))
  File ""/home/deeplearning/anaconda3/lib/python3.6/gzip.py"", line 463, 
in read
    if not self._read_gzip_header():
  File ""/home/deeplearning/anaconda3/lib/python3.6/gzip.py"", line 411, 
in _read_gzip_header
    raise OSError('Not a gzipped file (%r)' % magic)
OSError: Not a gzipped file (b've')
</code></pre>
","8024795","","130288","","2017-05-18 18:26:04","2020-09-26 12:29:49","Failed to load a .bin.gz pre trained words2vecx","<gensim><word2vec>","2","0","","","","CC BY-SA 3.0"
"56021542","1","56031430","","2019-05-07 11:24:55","","-1","37","<p>I am actually working with <code>doc2vec</code> from gensim library and I want to get all similarities with probabilites not only the top 10 similarities provided by <code>model.docvecs.most_similar()</code></p>

<p>Once my model is trained </p>

<pre><code>In [1]: print(model)
Out [1]: Doc2vec(...)
</code></pre>

<p>If I use <code>model.docvecs.most_similar()</code> I get only the Top 10 similar docs </p>

<pre><code>In [2]: model.docvecs.most_similar('1')
Out [2]: [('2007', 0.9171321988105774),
 ('606', 0.5638039708137512),
 ('2578', 0.530228853225708),
 ('4506', 0.5193327069282532),
 ('2550', 0.5178008675575256),
 ('4620', 0.5098666548728943),
 ('1296', 0.5071642994880676),
 ('3943', 0.5070815086364746),
 ('438', 0.5057751536369324),
 ('1922', 0.5048809051513672)]
</code></pre>

<p>And I am looking to get all probilities not only the top 10 for some analysis.</p>

<p>Thanks for your help :)</p>
","8098361","","8098361","","2019-05-07 11:48:01","2019-05-07 22:52:14","Get all similar documents with doc2vec","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"55872853","1","55875102","","2019-04-26 18:02:10","","2","1921","<p>I'm using Gensim with <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">Fasttext Word vectors</a> for return similar words.</p>

<p>This is my code:</p>

<pre><code>import gensim

model = gensim.models.KeyedVectors.load_word2vec_format('cc.it.300.vec')

words = model.most_similar(positive=['sole'],topn=10)

print(words)
</code></pre>

<p>This will return:</p>

<pre><code>[('sole.', 0.6860659122467041), ('sole.Ma', 0.6750558614730835), ('sole.Il', 0.6727924942970276), ('sole.E', 0.6680260896682739), ('sole.A', 0.6419174075126648), ('sole.√à', 0.6401025652885437), ('splende', 0.6336565613746643), ('sole.La', 0.6049465537071228), ('sole.I', 0.5922051668167114), ('sole.Un', 0.5904430150985718)]
</code></pre>

<p>The problem is that ""sole"" (""sun"", in english) return a series of words with a dot in it (like sole., sole.Ma, ecc...). Where is the problem? Why most_similar return this meaningless word?</p>

<p><strong>EDIT</strong></p>

<p>I tried with <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">english word vector</a> and the word ""sun"" return this:</p>

<pre><code>[('sunlight', 0.6970556974411011), ('sunshine', 0.6911839246749878), ('sun.', 0.6835992336273193), ('sun-', 0.6780728101730347), ('suns', 0.6730450391769409), ('moon', 0.6499731540679932), ('solar', 0.6437565088272095), ('rays', 0.6423950791358948), ('shade', 0.6366724371910095), ('sunrays', 0.6306195259094238)]¬†
</code></pre>

<p>Is it impossible to reproduce results like relatedwords.org?</p>
","2797134","","2797134","","2019-04-26 18:26:57","2019-04-26 21:23:49","Gensim most_similar() with Fasttext word vectors return useless/meaningless words","<gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"55874253","1","55874675","","2019-04-26 19:59:51","","0","852","<p>I am trying to get word2vec to work in python3, however as my dataset is too large to easily fit in memory I am loading it via an iterator (from zip files). However when I run it I get the error </p>

<pre><code>Traceback (most recent call last):
  File ""WordModel.py"", line 85, in &lt;module&gt;
    main()
  File ""WordModel.py"", line 15, in main
    word2vec = gensim.models.Word2Vec(data,workers=cpu_count())
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/word2vec.py"", line 783, in __init__
    fast_version=FAST_VERSION)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/base_any2vec.py"", line 759, in __init__
    self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/base_any2vec.py"", line 936, in build_vocab
    sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/word2vec.py"", line 1591, in scan_vocab
    total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/word2vec.py"", line 1576, in _scan_vocab
    total_words += len(sentence)
TypeError: object of type 'generator' has no len()
</code></pre>

<p>Here is the code:</p>

<pre><code>import zipfile
import os
from ast import literal_eval

from lxml import etree
import io
import gensim

from multiprocessing import cpu_count


def main():
    data = TrainingData(""/media/thijser/Data/DataSets/uit2"")
    print(len(data))
    word2vec = gensim.models.Word2Vec(data,workers=cpu_count())
    word2vec.save('word2vec.save')




class TrainingData:

    size=-1

    def __init__(self, dirname):
        self.data_location = dirname

    def __len__(self):
        if self.size&lt;0: 

            for zipfile in self.get_zips_in_folder(self.data_location): 
                for text_file in self.get_files_names_from_zip(zipfile):
                    self.size=self.size+1
        return self.size            

    def __iter__(self): #might not fit in memory otherwise
        yield self.get_data()

    def get_data(self):


        for zipfile in self.get_zips_in_folder(self.data_location): 
            for text_file in self.get_files_names_from_zip(zipfile):
                yield self.preproccess_text(text_file)


    def stripXMLtags(self,text):

        tree=etree.parse(text)
        notags=etree.tostring(tree, encoding='utf8', method='text')
        return notags.decode(""utf-8"") 

    def remove_newline(self,text):
        text.replace(""\\n"","" "")
        return text

    def preproccess_text(self,text):
        text=self.stripXMLtags(text)
        text=self.remove_newline(text)

        return text




    def get_files_names_from_zip(self,zip_location):
        files=[]
        archive = zipfile.ZipFile(zip_location, 'r')

        for info in archive.infolist():
            files.append(archive.open(info.filename))

        return files

    def get_zips_in_folder(self,location):
       zip_files = []
       for root, dirs, files in os.walk(location):
            for name in files:
                if name.endswith(("".zip"")): 
                    filepath=root+""/""+name
                    zip_files.append(filepath)

       return zip_files

main()


for d in data:
    for dd in d :
        print(type(dd))
</code></pre>

<p>Does show me that dd is of the type string and contains the correct preprocessed strings (with length somewhere between 50 and 5000 words each). </p>
","1930011","","1930011","","2019-04-27 02:04:23","2019-04-27 02:10:47","python gensim word2vec gives typeerror TypeError: object of type 'generator' has no len() on custom dataclass","<python><machine-learning><nlp><gensim><training-data>","1","0","","","","CC BY-SA 4.0"
"44490739","1","","","2017-06-12 02:46:16","","4","3556","<p>I am trying to run a program using the Gensim library of the Python with the version 3.6.<br>
Whenever I ran the program, I came across these statements: </p>

<pre class=""lang-none prettyprint-override""><code>C:\Python36\lib\site-packages\gensim-2.0.0-py3.6-win32.egg\gensim\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
Slow version of gensim.models.doc2vec is being used
</code></pre>

<p>I do not understand what is the meaning behind <code>Slow version of gensim.models.doc2vec is being used</code>. How the gensim is selecting the slow version and if I want the fastest version then what I need to do?   </p>
","4948889","","355230","","2017-06-12 03:04:24","2017-10-10 10:06:35","Using Gensim shows ""Slow version of gensim.models.doc2vec being used""","<python><python-3.x><gensim>","3","4","1","","","CC BY-SA 3.0"
"55428777","1","","","2019-03-30 06:23:48","","0","313","<p>I have some pre-trained word2vec model and I'd like to evaluate them using the same corpus. Is there a way I could get the raw training loss given a model dump file and the corpus in memory?</p>
","8845716","","","","","2019-03-30 21:27:20","How to get word2vec training loss in Gensim from pretrained models?","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"36328261","1","36339645","","2016-03-31 08:39:37","","3","1923","<p>I am getting the following error and am just not able to figure out why gensim cant be imported. I tried reimporting gensim again by creating virtual environment but that didnt work as well. 
I am new to python, please be generous.</p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\Tejasvi\workspace\major project\Tag Recommendation\test.py"", line 6, in &lt;module&gt;
import gensim
File ""C:\Python27\lib\site-packages\gensim\__init__.py"", line 6, in &lt;module&gt;
from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
File ""C:\Python27\lib\site-packages\gensim\models\__init__.py"", line 18, in &lt;module&gt;
from . import wrappers
File ""C:\Python27\lib\site-packages\gensim\models\wrappers\__init__.py"", line 5, in &lt;module&gt;
from .ldamallet import LdaMallet
File ""C:\Python27\lib\site-packages\gensim\models\wrappers\ldamallet.py"", line 40, in &lt;module&gt;
from smart_open import smart_open
File ""C:\Python27\lib\site-packages\smart_open\__init__.py"", line 1, in &lt;module&gt;
from .smart_open_lib import *
File ""C:\Python27\lib\site-packages\smart_open\smart_open_lib.py"", line 34, in &lt;module&gt;
from boto.compat import BytesIO, urlsplit, six
ImportError: cannot import name BytesIO
</code></pre>

<p>This is my code:</p>

<pre><code>import string
import re
import gensim
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
stop_words.update(['.', ',', '""', ""'"", '?', '!', ':', ';', '(', ')', '[',    ']',
'{', '}', 'lt', 'gt','xA', '/', 'lt'])

outFile = open('C:/Users/Tejasvi/Desktop/major/preprocessed/c#.txt', 'w')

with open('C:/Users/Tejasvi/Desktop/major/3/c#.txt') as f:
    for line in f:
        new_str = re.sub('[^a-zA-Z0-9\n\.]', ' ', line)
        new_string = ' '.join([w for w in new_str.split() if len(w)&gt;2])
        for c in string.punctuation:
            new_string=new_string.replace(c,"""")
            for w in new_string.split():
                if w.lower() not in stop_words:
                    outFile.write(w)
                    outFile.write("" "")
outFile.close()

from gensim import corpora
with open ('C:/Users/Tejasvi/Desktop/major/preprocessed/c#.txt', 'r') as f:
    for line in f:
        dictionary = corpora.Dictionary(line.strip().split())
</code></pre>

<p>here is the output of pip freeze</p>

<pre><code>-curses==2.2
alabaster==0.7.6
alembic==0.7.6
astroid==1.3.6
astropy==1.0.3
Babel==1.3
backports.datetime-timestamp==1.0.2.dev0
backports.functools-lru-cache==1.0.2.dev0
backports.method-request==1.0.1.dev0
backports.shutil-get-terminal-size==1.0.0
backports.ssl-match-hostname==3.4.0.2
Beaker==1.7.0
BeautifulSoup==3.2.1
beautifulsoup4==4.3.2
blinker==1.4.dev0
blosc==1.2.7
bloscpack==0.7.2
boto==2.24.0
Bottleneck==1.0.0
bz2file==0.98
CacheControl==0.11.5
cchardet==0.3.5
cdecimal==2.3
certifi==2015.4.28
cffi==1.1.2
chardet==2.3.0
colorama==0.3.3
configobj==5.0.6+xy.1
configparser==3.5.0b2
cov-core==1.15.0
coverage==3.7.1
cryptography==1.0.dev1
cssselect==0.9.1
cx-Freeze==4.3.4
cyordereddict==0.2.3.dev7
datrie==0.7.1.dev37
decorator==3.4.0
distlib==0.2.0
docutils==0.12
ecdsa==0.13.1.dev0
ed25519ll==0.6
enum34==1.0.4
faulthandler==2.4
formlayout==1.0.15
funcsigs==0.4
futures==3.0.3
gensim==0.12.4
gevent==1.0.2
gevent-websocket==0.9.5
GraphLab-Create==0.9.1
greenlet==0.4.7
grin==1.2.1+xy1
guidata==1.6.1
guiqwt==2.3.2
h5py==2.5.0
html5lib==0.99999
httpretty==0.8.10
idna==2.1.dev1
ipaddress==1.0.7
ipdb==0.8.1
ipdbplugin==1.4.2
ipython==2.4.1
jaraco.apt==1.0
jaraco.classes==1.2
jaraco.collections==1.1
jaraco.context==1.3
jaraco.functools==1.3
jaraco.structures==1.0
jaraco.text==1.4
jaraco.ui==1.3.1
jaraco.windows==3.4
jedi==0.9.0
Jinja2==2.7.3
keyring==5.3
lda==1.0.3
libnacl==1.4.3
librato-metrics==0.4.9
linecache2==1.0.0
lockfile==0.10.2.post7
logilab-common==0.63.2
lxml==3.4.4
mahotas==1.3.0
Mako==1.0.1
MarkupSafe==0.23
matplotlib==1.4.3
mixpanel-py==3.1.1
mock==1.0.1
modernize==0.4
more-itertools==2.3.dev0
ndg-httpsclient==0.4.0
netifaces==0.10.4
nltk==3.1
nose==1.3.7
nose-cov==1.6
nose-fixes==1.3
numexpr==2.4.3
numpy==1.10.4
numpydoc==0.6.dev0
oauthlib==0.7.3.dev0
objgraph==2.0.1.dev0
packaging==15.2
pandas==0.16.2
paramiko==1.15.2
pathlib==1.0.1
patsy==0.3.0
pbr==1.8.1
pep8==1.6.2
Pillow==2.8.2
ply==3.6
prettytable==0.7.2
psutil==1.1.3
psycopg2==2.6.1
py==1.4.30
py2exe==0.6.9
pyasn1==0.1.8
pyasn1-modules==0.0.6
PyAudio==0.2.8
pycparser==2.14
pycrypto==2.6.1
pyemf==2.0.0
pyflakes==0.9.2
Pygments==2.0.2
PyICU==1.9.2+xy.1
PyJWT==1.3.1.dev2
pylint==1.4.3
pyMinuit==1.2.1
PyOpenGL==3.1.0
PyOpenGL-accelerate==3.1.0
pyOpenSSL==0.15.1
pyparsing==2.0.3
PyQt4==4.11.3
pyreadline==2.0.6+xy.1
PyStemmer==1.3.0
python-dateutil==2.4.2
pytz==2015.4
pywin==0.3.1
pywin32==219
PyYAML==3.11
pyzmq==14.7.0
reportlab==3.2.0
requests==2.8.1
requests-oauthlib==0.5.0
rope==0.10.2
sampy==1.2.1
scandir==1.1.1.dev7
scikit-learn==0.17
scipy==0.15.1 
scp==0.10.2
singledispatch==3.4.0.3
six==1.9.0
smart-open==1.3.2
snowballstemmer==1.2.1.dev1
Sphinx==1.3.2
sphinx-rtd-theme==0.1.8
sphinxcontrib-plantuml==0.6
spyder==2.3.5.2
SQLAlchemy==1.0.6
statsmodels==0.6.1
stop-words==2015.2.23.1
tables==3.2.0
Tempita==0.5.2
textmining==1.0
tornado==3.2.1
traceback2==1.4.0
ujson==1.33
unittest2==1.0.1
urllib3==1.10.4
veusz==1.23.1
virtualenv==13.0.3
virtualenvwrapper-win==1.2.1
ViTables==2.2a1
wheel==0.24.0
Whoosh==2.7.0
wincertstore==0.2
wsaccel==0.6.2
wxPython==2.8.12.1
wxPython-common==2.8.12.1
yappi==0.94
yg.lockfile==2.0
zc.lockfile==1.1.0
</code></pre>

<p>I also checked if I had my own version of io.py but it doesn't exist.</p>
","5991248","","5991248","","2016-03-31 09:13:59","2016-03-31 17:12:22","ImportError: cannot import name BytesIO on eclipse","<python><lda><gensim><topic-modeling>","1","7","","","","CC BY-SA 3.0"
"38442161","1","","","2016-07-18 16:53:23","","0","661","<p>In the official explanation, there is no natural ordering between the topics in LDA.</p>

<p>As for the method show_topics(), if it returned num_topics &lt;= self.num_topics subset of all topics is therefore arbitrary and may change between two LDA training runs.</p>

<p>But I tends to find the top ten frequent topics of corpus. Is there any other ways to achieve this?</p>

<p>Many thanks.</p>
","6604481","","","","","2018-07-23 11:23:30","How to print top ten topics using Gensim?","<python><lda><gensim><topic-modeling>","2","0","","","","CC BY-SA 3.0"
"44101714","1","","","2017-05-21 20:56:02","","1","1707","<p>I've been trying to determine the similarity between a set of documents, and one of the methods I'm using is the cosine similarity with the results of the TF-IDF.</p>

<p>I tried to use both sklearn and gensim's implementations, which give me similar results, but my own implementation results in a different matrix.</p>

<p>After analyzing, I noticed that the their implementations are different from the ones I've studied and came across:</p>

<p>Sklearn and gensim use raw counts as the TF, and apply L2 norm
on the resulting vectors.</p>

<p>On the other side, the implementations I found will normalize the term count,
like</p>

<pre><code>TF = term count / sum of all term counts in the document
</code></pre>

<p>My question is, what is the difference with their implementations? Do they give better results in the end, for clustering or other purposes?</p>

<p>EDIT(So the question is clearer):
What is the difference between normalizing the end result vs normalizing the term count at the beggining?</p>
","2938803","","2938803","","2017-05-21 21:12:53","2017-05-23 14:26:36","Sklearn and gensim's TF-IDF implementation","<scikit-learn><tf-idf><gensim>","2","0","1","","","CC BY-SA 3.0"
"38630720","1","","","2016-07-28 08:16:09","","0","6153","<p>I have libbz2-dev installed however I am still getting the following  import error while importing gensim :</p>

<pre><code>&gt;&gt;&gt; import gensim
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/krishna/gensimenv/lib/python2.7/site-packages/gensim/__init__.py"", line 6, in &lt;module&gt;
    from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
  File ""/home/krishna/gensimenv/lib/python2.7/site-packages/gensim/corpora/__init__.py"", line 14, in &lt;module&gt;
    from .wikicorpus import WikiCorpus
  File ""/home/krishna/gensimenv/lib/python2.7/site-packages/gensim/corpora/wikicorpus.py"", line 21, in &lt;module&gt;
    import bz2
ImportError: No module named bz2
</code></pre>
","3805227","","","","","2017-07-17 16:48:05","Python import error no module named bz2","<python><gensim>","2","1","","","","CC BY-SA 3.0"
"66267818","1","66270436","","2021-02-18 20:39:04","","0","218","<p>I have a corpus of short text(~5000 sentences) which forms a vocabulary of ~2000 words. I used Gensim to build a Word2Vec model, but the output from most_similar doesn't look reasonable. Is this because I don't have enough words in the vocabulary? If so, is there any rule of thumbs for the vocabulary size?</p>
","6221871","","1453508","","2021-02-19 23:12:44","2021-02-19 23:12:44","Minimum number of words in the vocabulary for Word2Vec models?","<gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"47332205","1","47360978","","2017-11-16 14:28:15","","0","240","<p>I am using gensim doc2vec as below.</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple
import re

my_d = {'recipe__001__1': 'recipe 1 details should come here',
 'recipe__001__2': 'Ingredients of recipe 2 need to be added'}
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for key, value in my_d.items():
    value = re.sub(""[^a-zA-Z]"","" "", value)
    words = value.lower().split()
    tags = key
    docs.append(analyzedDocument(words, tags))
model = doc2vec.Doc2Vec(docs, size = 300, window = 10, dm=1, negative=5, hs=0, min_count = 1, workers = 4, iter = 20)
</code></pre>

<p>However, when I check <code>model.docvecs.offset2doctag</code> I get <code>['r', 'e', 'c', 'i', 'p', '_', '0', '1', '2']</code> as the output. The real output should be `'recipe__001__1' and 'recipe__001__2'.</p>

<p>When I use <code>len(model.docvecs.doctag_syn0)</code> I get <code>9</code> as the output. But the real value should be <code>2</code> because I only have 2 recipes in my test dictionary.</p>

<p>Please let me know, why this happens?</p>
","","user8566323","5545946","","2017-11-18 03:59:46","2017-11-18 03:59:46","Issues in doc2vec tags in Gensim","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"47507091","1","47515042","","2017-11-27 09:01:16","","2","83","<p>I have two different word vector models created using word2vec algorithm . Now issue i am facing is few words from first model is not there in second model . I want to create a third model from two different word vectors models where i can use word vectors from both models without loosing meaning and the context of word vectors. </p>

<p>Can I do this, and if so, how?</p>
","7962476","","130288","","2017-11-27 16:01:44","2017-11-27 16:01:44","Creating a wordvector model combining words from other models","<machine-learning><nlp><word2vec><gensim>","1","1","","","","CC BY-SA 3.0"
"47353341","1","","","2017-11-17 14:48:50","","1","1895","<p>I am calculating tf-idf as follows.</p>

<pre><code>texts=['human interface computer',
 'survey user computer system response time',
 'eps user interface system',
 'system human system eps',
 'user response time']

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
analyzedDocument = namedtuple('AnalyzedDocument', 'word tfidf_score')
d=[]
for doc in corpus_tfidf:
    for id, value in doc:
        word = dictionary.get(id)
        score = value
        d.append(analyzedDocument(word, score))
</code></pre>

<p>However, now I want to identify the most 3 important words in my corpus using the words that has the highest <code>idf</code> values. Please let me know how to do it?</p>
","","user8871463","","user8871463","2017-11-17 14:54:45","2017-11-18 00:35:33","Get the most important words in the corpus using tf-idf (Gensim)","<python><gensim><tf-idf>","1","0","1","","","CC BY-SA 3.0"
"66284750","1","","","2021-02-19 20:44:11","","0","149","<p>In Mallet, we can get a diagnostics file including measuring coherence for each topic
<a href=""http://mallet.cs.umass.edu/diagnostics.php"" rel=""nofollow noreferrer"">http://mallet.cs.umass.edu/diagnostics.php</a>. In the Gensim, we have an overall score for each set of topics and a single score for each topic (<a href=""https://radimrehurek.com/gensim/models/coherencemodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/coherencemodel.html</a>). I have two questions:</p>
<p>1- What is the name of the coherence method in the diagnostics file of Mallet?</p>
<p>2- If we want to measure an overall score using the coherence scores in the diagnostics file of Mallet, can we just measure the average of coherence scores?</p>
","9844472","","","","","2021-02-21 14:41:25","Coherence and Diagnostics File in Mallet","<gensim><topic-modeling><mallet>","1","0","","","","CC BY-SA 4.0"
"61361996","1","","","2020-04-22 09:45:35","","0","64","<p>I have a number of known themes and a dataset with phrases on these themes. 
As a result I need get the theme of a phrase which is not in the dataset.
I train gensim doc2vec model on this dataset using only phrases. Here TaggedDocument is a list of words of a phrase.</p>

<pre><code>train_data = list(create_tagged_document(data))
#&gt; [TaggedDocument(words=['anarchism', 'originated', ... 'social', 'or', 'emotional'], tags=[0])]
model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=1000)
model.build_vocab(train_data)
model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>

<p>Then I find the closest phrases to a new phrase. </p>

<pre><code>test_vec = model.infer_vector(['lorem', 'ipsum', 'new', 'phrase', 'words'])
similars = model.docvecs.most_similar([test_vec], topn=len(model.docvecs))
</code></pre>

<p>After that I check the themes of the closest phrases (based on cosine similarity probably) and if the closest phrases are close enough then decide that new phrase has the same theme as the closest.
Does this approach make sense? And did doc2vec model used correctly?</p>
","10963649","","","","","2020-04-22 09:45:35","Python, gensim, doc2vec. Classify phrases by themes","<python><gensim><doc2vec>","0","3","","","","CC BY-SA 4.0"
"56027806","1","56192682","","2019-05-07 17:34:08","","0","79","<p>Thanks for stopping by.  I have a directional question - I've built a Latent Dirichlet Allocation using Gensims Mallet wrapper.  I trained the model once on OldDataSet.csv and measured coherence.  I have been using it to pass NewDataSet.csv through for topic allocation.  I need some guidance on how I might be able to predict how accurately my pre-trained model is allocating NewDataSet.csv.  That coherence score only checks the accuracy of the pre-trained model not the allocated data set.  I'd like a way to  track the occurrence of historical topics and detect the emergence of new topics without re-training the model.  Like say these are the topics in OldDataSet.csv:</p>

<ol>
<li>whiskey</li>
<li>Tango</li>
<li>Foxtrot</li>
</ol>

<p>It will assign NewDataSet.csv 1. whiskey 2. Tango or 3. Foxtrot but a more accurate allocation might be:</p>

<ol>
<li>whiskey</li>
<li>Tango</li>
<li>Alpha</li>
</ol>

<p>If I keep running the same model I might miss this new topic.  If there exists a numeric score that would measure how closely the topics adhere to NewDataSet.csv that would be a huge time saver.  Thanks Stack you always save me :)</p>
","11192516","","","","","2019-05-17 19:45:20","LDA detect new emerging topics","<python><windows><machine-learning><gensim><lda>","1","4","","","","CC BY-SA 4.0"
"66283478","1","","","2021-02-19 18:55:34","","0","138","<p>I would like to know how the overall coherence is measured for u_mass', 'c_v', 'c_uci', 'c_npmi' for each set of topics in the gensim (<a href=""https://radimrehurek.com/gensim/models/coherencemodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/coherencemodel.html</a>)?</p>
<p>Is it based on the average of coherence values: (coherence topic 1 + coherence topic 2 + .... + coherence topic n)/n ?</p>
<p>For example, if we have 5 topics, the overall coherence would be (coherence topic 1 + coherence topic 2 + coherence topic 3 + coherence topic 4 + coherence topic 5) divided by 5</p>
","9844472","","","","","2021-02-24 20:10:36","Overal Coherence in Gensim and Mallet","<gensim><lda><topic-modeling><mallet>","1","0","","","","CC BY-SA 4.0"
"57617061","1","","","2019-08-22 21:05:49","","1","310","<p>I am building a program that assigns multiple labels/tags to textual descriptions. I am using Scikit-Learn's OneVsRestClassifier+XGBClassifier to classify the vectorized textual descriptions. I am using Gensim's Word2Vec to vectorize the texts. However, when I try to fit the classifier to the vectorized data, I get the following error: </p>

<blockquote>
  <p>IndexError: tuple index out of range</p>
</blockquote>

<p>Below is my code (the error happens on the last line where I try to fit the classifier):</p>

<pre><code>w2vModel = Word2Vec(sentences, size=150, window=10, min_count=2, workers=multiprocessing.cpu_count())
modelCorpus = list(w2vModel.wv.vocab)

descriptions = []
for sentence in sentences:
    wordList = []
    for word in sentence: 
        if (word in modelCorpus):
            wordList.append(w2vModel.wv[word])
    descriptions.append(np.concatenate(wordList))

x = np.array(descriptions)

# Vectorize ticket labels/tags using MultiLabelBinarizer
tagList = relevantDF.Tags # Retrieve list of tags
vectorizer2 = MultiLabelBinarizer()
vectorizer2.fit(tagList)
y = vectorizer2.transform(tagList)

# Split test data and convert test data to arrays
xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.20)
yTrain = csr_matrix(yTrain).toarray()

# Fit OneVsRestClassifier w/ XGBClassifier
clf = OneVsRestClassifier(XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.003))
clf.fit(xTrain, yTrain)
</code></pre>

<p>The shape of x is: (8347,)</p>

<p>The shape of y is: (8347, 24)</p>

<p>The shape of xTrain is: (6677,)</p>

<p>The shape of yTrain is: (6677, 24)</p>
","3950533","","4685471","","2019-08-22 21:23:11","2019-08-23 22:07:35","Sklearn classifier can't be trained with Gensim Word2Vec data","<python><machine-learning><scikit-learn><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"64919359","1","64930259","","2020-11-19 20:19:47","","1","61","<p>I am working on a dataset of amazon alexa reviews and wish to cluster them in positive and negative clusters. I am using Word2Vec for vectorization so wanted to know the difference between <strong>Text Embedding</strong> and <strong>Word Embedding</strong>. Also, which one of them will be useful for my clustering of reviews (Please consider that I want to predict the cluster of any reviews that I enter.)
Thanks in advance!</p>
","14654343","","","","","2020-11-20 13:31:44","Difference between Text Embedding and Word Embedding","<python-3.x><nlp><k-means><gensim><word2vec>","1","2","","2020-11-20 15:20:54","","CC BY-SA 4.0"
"56150678","1","67837895","","2019-05-15 13:33:38","","4","444","<p>I'm looking for a solution to use something like <code>most_similar()</code> from <code>Gensim</code> but using <code>Spacy</code>.
I want to find the most similar sentence in a list of sentences using NLP.</p>

<p>I tried to use <code>similarity()</code> from <code>Spacy</code> (e.g. <a href=""https://spacy.io/api/doc#similarity"" rel=""nofollow noreferrer"">https://spacy.io/api/doc#similarity</a>) one by one in loop, but it takes a very long time.</p>

<p>To go deeper :</p>

<p>I would like to put all these sentences in a graph (like <a href=""https://cdn-images-1.medium.com/max/1600/1*vvtIsW1AblmgLkq1peKfOg.png"" rel=""nofollow noreferrer"">this</a>) to find sentence clusters.</p>

<p>Any idea ?</p>
","11236608","","","","","2021-06-04 13:30:32","Use Spacy to find most similar sentences in doc","<gensim><similarity><spacy><doc2vec><sentence-similarity>","1","1","2","","","CC BY-SA 4.0"
"40413866","1","","","2016-11-04 01:18:02","","15","2126","<p>i am going thorugh this paper <a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"">http://cs.stanford.edu/~quocle/paragraph_vector.pdf</a></p>

<p>and it states that</p>

<blockquote>
  <p>"" Theparagraph vector and word vectors are averaged or concatenated
  to predict the next word in a context. In the experiments, we use
  concatenation as the method to combine the vectors.""</p>
</blockquote>

<p>How does concatenation or averaging work?</p>

<p>example (if paragraph 1 contain word1 and word2):</p>

<pre><code>word1 vector =[0.1,0.2,0.3]
word2 vector =[0.4,0.5,0.6]

concat method 
does paragraph vector = [0.1+0.4,0.2+0.5,0.3+0.6] ?

Average method 
does paragraph vector = [(0.1+0.4)/2,(0.2+0.5)/2,(0.3+0.6)/2] ?
</code></pre>

<p>Also from this image:</p>

<p>It is stated that :</p>

<blockquote>
  <p>The paragraph token can be thought of as another word. It acts as a
  memory that remembers what is missing from the current context ‚Äì or
  the topic of the paragraph. For this reason, we often call this model
  the Distributed Memory Model of Paragraph Vectors (PV-DM).</p>
</blockquote>

<p>Is the paragraph token equal to the paragraph vector which is equal to <code>on</code>?</p>

<p><a href=""https://i.stack.imgur.com/EQO9m.png""><img src=""https://i.stack.imgur.com/EQO9m.png"" alt=""enter image description here""></a></p>
","2800939","","2800939","","2016-11-04 06:53:29","2017-01-19 03:42:06","How does gensim calculate doc2vec paragraph vectors","<nlp><vectorization><gensim><word2vec><doc2vec>","2","0","3","","","CC BY-SA 3.0"
"56033651","1","56033914","","2019-05-08 04:40:58","","4","3091","<p>I am currently working with python where I train a Word2Vec model using sentences that I provide. Then, I save and load the model to get the word embedding of each and every word in the sentences that were used to train the model. However, I get the following error.</p>

<blockquote>
  <p>KeyError: ""word 'n1985_chicago_bears' not in vocabulary""</p>
</blockquote>

<p>whereas, one of the sentences provided during training is as follows.</p>

<pre><code>sportsteam n1985_chicago_bears teamplaysincity city chicago
</code></pre>

<p>Hence I would like to know why some words are missing from the vocabulary, despite being trained on those words from that sentence corpus. </p>

<p><strong>Training the word2vec model on own corpus</strong></p>

<pre><code>import nltk
import numpy as np
from termcolor import colored
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from sklearn.decomposition import PCA


#PREPARING DATA

fname = '../data/sentences.txt'

with open(fname) as f:
    content = f.readlines()

# remove whitespace characters like `\n` at the end of each line
content = [x.strip() for x in content]


#TOKENIZING SENTENCES

sentences = []

for x in content:
    nltk_tokens = nltk.word_tokenize(x)
    sentences.append(nltk_tokens)

#TRAINING THE WORD2VEC MODEL

model = Word2Vec(sentences)

words = list(model.wv.vocab)
model.wv.save_word2vec_format('model.bin')
</code></pre>

<p><strong>Sample sentences from sentences.txt</strong></p>

<pre><code>sportsteam hawks teamplaysincity city atlanta
stadiumoreventvenue honda_center stadiumlocatedincity city anaheim
sportsteam ducks teamplaysincity city anaheim
sportsteam n1985_chicago_bears teamplaysincity city chicago
stadiumoreventvenue philips_arena stadiumlocatedincity city atlanta
stadiumoreventvenue united_center stadiumlocatedincity city chicago
...
</code></pre>

<p>There are 1860 such lines in the <code>sentences.txt</code> file, each containing exactly 5 words and no stop words.</p>

<p>After saving the model, I tried to load it from a different python file within the same directory as the saved <code>model.bin</code> as shown below.</p>

<p><strong>Loading the saved model.bin</strong></p>

<pre><code>import nltk
import numpy as np
from gensim import models

w = models.KeyedVectors.load_word2vec_format('model.bin', binary=True)
print(w['n1985_chicago_bears'])
</code></pre>

<p>However, I end up with the following error</p>

<pre><code>KeyError: ""word 'n1985_chicago_bears' not in vocabulary""
</code></pre>

<p>Is there a way to get the word embedding for each and every word in the trained sentence corpus using the same method?</p>

<p>Any suggestions in this regard will be much appreciated.</p>
","6339494","","","","","2019-05-08 05:11:44","Words missing from trained word2vec model vocabulary","<python><tensorflow><nltk><gensim><word2vec>","1","0","2","","","CC BY-SA 4.0"
"52876014","1","","","2018-10-18 14:12:45","","1","1436","<p>I am trying to find best hyperparameters for my trained doc2vec gensim model which takes a document as an input and create its document embeddings. My train data consists of text documents but it doesn't have any labels. i.e. I just have 'X' but not 'y'.</p>

<p>I found some questions here related to what I am trying to do but all of the solutions are proposed for supervised models but none for unsupervised like mine.</p>

<p>Here is the code where I am training my doc2vec model:</p>

<pre><code>def train_doc2vec(
    self,
    X: List[List[str]],
    epochs: int=10,
    learning_rate: float=0.0002) -&gt; gensim.models.doc2vec:

    tagged_documents = list()

    for idx, w in enumerate(X):
        td = TaggedDocument(to_unicode(str.encode(' '.join(w))).split(), [str(idx)])
        tagged_documents.append(td)

    model = Doc2Vec(**self.params_doc2vec)
    model.build_vocab(tagged_documents)

    for epoch in range(epochs):
        model.train(tagged_documents,
                    total_examples=model.corpus_count,
                    epochs=model.epochs)
        # decrease the learning rate
        model.alpha -= learning_rate
        # fix the learning rate, no decay
        model.min_alpha = model.alpha

    return model
</code></pre>

<p>I need suggestions on how to proceed and find best hyperparameters for my trained model using GridSearch or any suggestions about some other technique. Help is much appreciated.</p>
","8210067","","8210067","","2018-10-18 14:35:55","2021-02-09 14:43:12","GridSearch for doc2vec model built using gensim","<machine-learning><gensim><grid-search><doc2vec><hyperparameters>","1","12","","","","CC BY-SA 4.0"
"38620124","1","38621889","","2016-07-27 18:00:35","","0","417","<p>I'm using Gensim for an NLP task and currently I have a corpus which includes empty documents.  I don't want to rerun my code, although that is an option, and would just like to remove the documents that don't have any content.  The documents are already saved as TF-IDF corpora and was wondering if there was a way to remove these documents that are empty.  I can figure out which documents are empty but the corpora file is an iterator and not any type of data structure ie list.  Thanks,</p>

<p>Cameron</p>
","3716368","","","","","2016-07-27 19:44:51","Removing documents in Gensim","<python><python-2.7><nlp><gensim>","1","0","","","","CC BY-SA 3.0"
"61596101","1","61599903","","2020-05-04 15:42:09","","1","1794","<p>I'm trying to calculate a between-topic cosine similarity score from a <code>Gensim</code> LDA topic model, but this proves more complicated than I first expected.</p>

<p><code>Gensim</code> has a method to calculate distances between topics <code>model.diff(model)</code>, but unfortunately cosine distance is not implemented; it has jaccard distance, but it is a bit too vector-length dependent (i.e., when comparing top 100 most important words per topic the distance is lower than comparing top 500, and the distance is 0 when full-length vectors are compared, as each topic includes all terms, but with different probabilities).</p>

<p>My problem is that the output from the model looks like this (only shown 4 top words):</p>

<pre><code>(30, '0.008*""tax"" + 0.004*""cut"" + 0.004*""bill"" + 0.004*""spending""')
(18, '0.009*""candidate"" + 0.009*""voter"" + 0.009*""vote"" + 0.009*""election""')
(42, '0.047*""shuttle"" + 0.034*""astronaut"" + 0.026*""launch"" + 0.025*""orbit""')
(22, '0.023*""boat"" + 0.020*""ship"" + 0.015*""migrant"" + 0.013*""vessel""')
</code></pre>

<p>So, in order to calculate the cosine sim/distance, I would have to parse the second element of the tuple (i.e., the  <code>'0.008*""tax"" +...'</code> part, which indicates term probabilities. </p>

<p>I was wondering whether there is an easier way to get cosine similarity out of the model? Or parsing each individual string of term/probabilities is really the only way to go?</p>

<p>Thanks for the help.</p>
","3590728","","","","","2020-05-04 19:17:29","Calculating cosine similarity from a Gensim model","<python><gensim><topic-modeling><cosine-similarity>","1","0","","","","CC BY-SA 4.0"
"49313542","1","","","2018-03-16 05:05:14","","1","1743","<p>I have trained LDA model on 2000 URL's(containing articles) on a particular topic in Python3. Can we predict new corpus based on the trained model?</p>
","8578853","","8578853","","2018-03-16 05:16:29","2019-12-08 13:31:52","Can we predict new corpus using trained LDA model?","<python-3.6><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"21552518","1","21553171","","2014-02-04 12:25:15","","20","8892","<p>I am trying to recycle scikit-learn vectorizer objects with gensim topic models. The reasons are simple: first of all, I already have a great deal of vectorized data; second, I prefer the interface and flexibility of scikit-learn vectorizers; third, even though topic modelling with gensim is very fast, computing its dictionaries (<code>Dictionary()</code>) is relatively slow in my experience.</p>

<p>Similar questions have been asked before, <a href=""https://stackoverflow.com/questions/19504898/use-sikit-tfidf-with-gensim-lda"">especially here</a> and <a href=""https://stackoverflow.com/questions/15670525/how-do-you-initialize-a-gensim-corpus-variable-with-a-csr-matrix"">here</a>, and the bridging solution is gensim's <code>Sparse2Corpus()</code> function which transforms a Scipy sparse matrix into a gensim corpus object.</p>

<p>However, this conversion does not make use of the <code>vocabulary_</code> attribute of sklearn vectorizers, which holds the mapping between words and feature ids. This mapping is necessary in order to print the discriminant words for each topic (<code>id2word</code> in gensim topic models, described as ""a a mapping from word ids (integers) to words (strings)"").</p>

<p>I am aware of the fact that gensim's <code>Dictionary</code> objects are much more complex (and slower to compute) than scikit's <code>vect.vocabulary_</code> (a simple Python <code>dict</code>)...</p>

<p>Any ideas to use <code>vect.vocabulary_</code> as <code>id2word</code> in gensim models?</p>

<p>Some example code:</p>

<pre><code># our data
documents = [u'Human machine interface for lab abc computer applications',
        u'A survey of user opinion of computer system response time',
        u'The EPS user interface management system',
        u'System and human system engineering testing of EPS',
        u'Relation of user perceived response time to error measurement',
        u'The generation of random binary unordered trees',
        u'The intersection graph of paths in trees',
        u'Graph minors IV Widths of trees and well quasi ordering',
        u'Graph minors A survey']

from sklearn.feature_extraction.text import CountVectorizer
# compute vector space with sklearn
vect = CountVectorizer(min_df=1, ngram_range=(1, 1), max_features=25000)
corpus_vect = vect.fit_transform(documents)
# each doc is a scipy sparse matrix
print vect.vocabulary_
#{u'and': 1, u'minors': 20, u'generation': 9, u'testing': 32, u'iv': 15, u'engineering': 5, u'computer': 4, u'relation': 28, u'human': 11, u'measurement': 19, u'unordered': 37, u'binary': 3, u'abc': 0, u'for': 8, u'ordering': 23, u'graph': 10, u'system': 31, u'machine': 17, u'to': 35, u'quasi': 26, u'time': 34, u'random': 27, u'paths': 24, u'of': 21, u'trees': 36, u'applications': 2, u'management': 18, u'lab': 16, u'interface': 13, u'intersection': 14, u'response': 29, u'perceived': 25, u'in': 12, u'widths': 40, u'well': 39, u'eps': 6, u'survey': 30, u'error': 7, u'opinion': 22, u'the': 33, u'user': 38}

import gensim
# transform sparse matrix into gensim corpus
corpus_vect_gensim = gensim.matutils.Sparse2Corpus(corpus_vect, documents_columns=False)
lsi = gensim.models.LsiModel(corpus_vect_gensim, num_topics=4)
# I instead would like something like this line below
# lsi = gensim.models.LsiModel(corpus_vect_gensim, id2word=vect.vocabulary_, num_topics=2)
print lsi.print_topics(2)
#['0.622*""21"" + 0.359*""31"" + 0.256*""38"" + 0.206*""29"" + 0.206*""34"" + 0.197*""36"" + 0.170*""33"" + 0.168*""1"" + 0.158*""10"" + 0.147*""4""', '0.399*""36"" + 0.364*""10"" + -0.295*""31"" + 0.245*""20"" + -0.226*""38"" + 0.194*""26"" + 0.194*""15"" + 0.194*""39"" + 0.194*""23"" + 0.194*""40""']
</code></pre>
","3248063","","-1","","2017-05-23 12:26:26","2019-07-31 18:31:07","Using scikit-learn vectorizers and vocabularies with gensim","<python><scikit-learn><topic-modeling><gensim>","5","0","15","","","CC BY-SA 3.0"
"56309232","1","","","2019-05-25 22:30:18","","0","290","<p>I am training a Phrases model to identify bigrams on large corpus using Gensim. When I reload the model I get the following error:</p>

<blockquote>
  <p>TypeError: 'float' object is not subscriptable</p>
</blockquote>

<p>My code to save the model (and also the training) is like this one:</p>

<pre><code>from gensim.models.phrases import Phrases, Phraser
def train_bigram(corpus):
    sentence_stream = [doc.split("" "") for doc in corpus]
    bigram_model = Phrases(sentence_stream, min_count=180, threshold=3.5)
    return bigram_model

print(""train bigram on cleaned text"")
phrases = train_bigram(corpus_cleaned)
print(""Build faster model for Gensim"")
bigram = Phraser(phrases)  # construct faster model (this is only an wrapper)
# Store the bigram model
bigram.save(path_to_save + ""bigram""
</code></pre>

<p>When I have to reload the model what cain I do? At the moment I am using the following:
<code>
bigram_reloaded = Phraser.load(path_to_save + 'bigram')
</code></p>

<p>but in this case I get the error shown before. Any idea or tips on how to solve?</p>
","11271317","","","","","2019-05-25 22:30:18","TypeError: 'float' object is not subscriptable when reloading a Gensim model","<python><nlp><save><gensim>","0","2","","","","CC BY-SA 4.0"
"15670525","1","15693931","","2013-03-27 22:12:52","","7","1745","<p>I have X as a csr_matrix that I obtained using scikit's tfidf vectorizer, and y which is an array</p>

<p>My plan is to create features using LDA, however, I failed to find how to initialize a gensim's corpus variable with X as a csr_matrix. In other words, I don't want to download a corpus as shown in gensim's documentation nor convert X to a dense matrix, since it would consume a lot of memory and the computer could hang.</p>

<p>In short, my questions are the following,</p>

<ol>
<li>How do you initialize a gensim corpus given that I have a csr_matrix (sparse) representing the whole corpus?</li>
<li>How do you use LDA to extract features?</li>
</ol>
","1248073","","298479","","2013-03-27 22:13:56","2013-03-28 23:27:52","How do you initialize a gensim corpus variable with a csr_matrix?","<python><scikit-learn><document-classification><lda><gensim>","1","1","5","","","CC BY-SA 3.0"
"50089525","1","","","2018-04-29 17:42:20","","1","570","<p>I am training document embeddings on a ~20 million sentences and using parallel processing in gensim. I'm creating my model and training with the following code</p>

<pre><code>class read_corpus(object):

    def __init__(self, fname, n):
        self.fname = fname
        self.n = n

    def __iter__(self):
        num_notes = 0
        with open(self.fname, 'r') as f:
            while num_notes &lt; n:
                note = next(f)
                sentence_id, sentence = note.split('\t')

                # remove the newline character after each line and split into words
                sentence = sentence[:-1].split(' ')

                # some processing


                yield TaggedDocument(sentence, [sentence_id])
                num_notes += 1


def model(fname, vector_size, min_count,
          n_epochs, model_name,
          n, prev_model_name=None):


    data = read_corpus(fname, n)

    if prev_model_name is not None:
        model = Doc2Vec.load(prev_model_name)
    else:
        model = Doc2Vec(vector_size=vector_size,
                        min_count=min_count,
                        workers=4,
                        window=8,
                        alpha=0.1,
                        min_alpha=0.0001)

        model.build_vocab(data)

    model.train(data, total_examples=model.corpus_count, epochs=n_epochs)
    model.save(model_name)
</code></pre>

<p>After 6 - 8 epochs, the logging information shows that the training gets stuck waiting for a worker thread.
Note: the logging information says ""EPOCH 1"" because I'm training in a for loop. </p>

<p><code>... 
INFO : EPOCH 1 - PROGRESS: at 99.71% examples, 162493 words/s, in_qsize 8, out_qsize 0
INFO : EPOCH 1 - PROGRESS: at 99.81% examples, 162528 words/s, in_qsize 7, out_qsize 0
INFO : EPOCH 1 - PROGRESS: at 99.91% examples, 162560 words/s, in_qsize 7, out_qsize 0
INFO : worker thread finished; awaiting finish of 3 more threads
INFO : worker thread finished; awaiting finish of 2 more threads</code></p>

<p>It's been stuck here for several hours.</p>

<p>I had a similar output on a previous run. But the logging stopped at <code>INFO : worker thread finished; awaiting finish of 3 more threads</code></p>
","7977512","","7977512","","2018-05-02 18:30:17","2018-11-07 01:46:56","Gensim worker thread stuck","<python><nlp><word2vec><gensim><doc2vec>","0","10","1","","","CC BY-SA 4.0"
"61244205","1","","","2020-04-16 06:42:59","","1","17","<p>I have only one document and use LDA to find just one topic. Here is what I got:</p>

<pre><code>[(0,
  '0.032*""women"" + 0.032*""gender"" + 0.032*""tech"" + 0.032*""amazons"" + 0.021*""peers"" + 0.021*""positions"" + 0.021*""policymakers"" + 0.021*""practices"" + 0.021*""markets"" + 0.021*""number""')]
</code></pre>

<p>How should I understand the number before each keyword? </p>
","10308337","","6573902","","2020-04-17 14:05:10","2020-04-17 14:05:10","What does the number before each keyword in a LDA topic mean?","<gensim><lda>","0","1","","","","CC BY-SA 4.0"
"55028281","1","55084983","","2019-03-06 16:50:55","","1","708","<p>I'm using <code>Gensim</code> for loading the german <code>.bin</code> files from <code>Fasttext</code> in order to get vector representations for out-of-vocabulary words and phrases. So far it works fine and I achieve good results overall.<br>
I am familiar with the <code>KeyError :'all ngrams for word &lt;word&gt; absent from model'.</code> Clearly the model doesn't provide a vector representation for every possible ngram combination.<br>
But now I ran into a confusing (at least for me) issue.<br>
I'll just give a quick example:<br>
the model provides a representation for the phrase <code>AuM Wert</code>.<br>
But when I want to get a representation for <code>AuM Wert 50 Mio. Eur</code>, I'll get the <code>KeyError</code> mentioned above. So the model obviously has a representation for the shorter phrase but not for the extended one.<br>
It even returns a representation for <code>AuM Wert 50 Mio.Eur</code> (I just removed the space between 'Mio' and 'Eur')<br>
I mean, the statement in the Error is simply not true, because the first example shows that it knows some of the ngrams. Can someone explain that to me? What don't I understand here? Is my understanding of ngrams wrong?</p>

<p>Heres the code: </p>

<pre><code>from gensim.models.wrappers import FastText
model = FastText.load_fasttext_format('cc.de.300.bin')
model.wv['AuM Wert'] #returns a vector
model.wv['AuM Wert 50 Mio.EUR'] #returns a vector
model.wv['AuM Wert 50 Mio. EUR'] #triggers the error
</code></pre>

<p>Thanks in advance,<br>
Amos</p>
","7495037","","7495037","","2019-03-10 00:04:23","2019-03-10 06:12:20","Fasttext representation for short phrase, but not for longer phrase containing the short one","<python><nlp><gensim><fasttext>","1","2","","","","CC BY-SA 4.0"
"57605476","1","","","2019-08-22 08:49:25","","1","261","<p>I am building a program that assigns multiple labels/tags to textual descriptions. I am using Gensim's Doc2Vec to vectorize each of the text descriptions. However, when I print out the length of the Doc2Vec's model's vectors, it returns the number of different tags there are, not the number of descriptions. In other words, it returns vectors representing the tags, not the documents. This inevitable leads to a ValueError when I try splitting the data (using sklearn):</p>

<blockquote>
  <p>ValueError: Found input variables with inconsistent numbers of
  samples: [64, 8370]</p>
</blockquote>

<p>Below is my code:</p>

<pre><code>textList = []

for i in range(0, len(unformattedText)):
    text = unformattedText[i]
    tag = tagList[i]
    textList.append(TaggedDocument(words=text.split("" ""), tags=[tag]))

numCores = multiprocessing.cpu_count() 
model = Doc2Vec(textList, workers=numCores, vector_size=100)

docVectors = []
for j in range(0, len(model.docvecs)):
    docVectors.append(model.docvecs[j])
x = docVectors

vectorizer2 = MultiLabelBinarizer()
vectorizer2.fit(tagList)
y = vectorizer2.transform(tagList)

xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.20)
</code></pre>

<p>Variable dimensions: <code>x</code> is an array of length 64 and <code>y.shape = (8370, 24)</code></p>
","3950533","","4685471","","2019-08-22 11:58:16","2019-08-23 08:33:22","Gensim Doc2Vec: Less vectors generated than expected","<python><machine-learning><scikit-learn><gensim><doc2vec>","2","1","","","","CC BY-SA 4.0"
"44301061","1","47001435","","2017-06-01 07:23:07","","1","324","<p>I have a set of documents. I also have the title of topics based on which I want to categorize documents. My preference is to use LDA in Gensim. is there any way to feed my own list of topics in the topic modeling algorithm ?</p>
","5070831","","","","","2017-10-29 14:37:35","LDA/LSI Topic modelling in Gensim with predefined list of topics","<gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"43918566","1","","","2017-05-11 14:37:58","","0","1159","<p>I'm trying to mimick the n_gram parameter in CountVectorizer() with gensim. My goal is to be able to use LDA with Scikit or Gensim and to find very similar bigrams. </p>

<p>For example, we can find the following bigrams with scikit: ""abc computer"", ""binary unordered"" and with gensim ""A survey"", ""Graph minors""...</p>

<p>I have attached my code below to make a comparison between Gensim and Scikit in terms of bigrams/unigrams.</p>

<p>Thanks for your help</p>

<pre><code>documents = [[""Human"" ,""machine"" ,""interface"" ,""for"" ,""lab"", ""abc"" ,""computer"" ,""applications""],
      [""A"", ""survey"", ""of"", ""user"", ""opinion"", ""of"", ""computer"", ""system"", ""response"", ""time""],
      [""The"", ""EPS"", ""user"", ""interface"", ""management"", ""system""],
      [""System"", ""and"", ""human"", ""system"", ""engineering"", ""testing"", ""of"", ""EPS""],
      [""Relation"", ""of"", ""user"", ""perceived"", ""response"", ""time"", ""to"", ""error"", ""measurement""],
      [""The"", ""generation"", ""of"", ""random"", ""binary"", ""unordered"", ""trees""],
      [""The"", ""intersection"", ""graph"", ""of"", ""paths"", ""in"", ""trees""],
      [""Graph"", ""minors"", ""IV"", ""Widths"", ""of"", ""trees"", ""and"", ""well"", ""quasi"", ""ordering""],
      [""Graph"", ""minors"", ""A"", ""survey""]]
</code></pre>

<p>With the gensim model we find 48 unique tokens, we can print the unigram/bigrams with print(dictionary.token2id)</p>

<pre><code># 1. Gensim
from gensim.models import Phrases

# Add bigrams and trigrams to docs (only ones that appear 20 times or more).
bigram = Phrases(documents, min_count=1)
for idx in range(len(documents)):
    for token in bigram[documents[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            documents[idx].append(token)

documents = [[doc.replace(""_"", "" "") for doc in docs] for docs in documents]
print(documents)

dictionary = corpora.Dictionary(documents)
print(dictionary.token2id)
</code></pre>

<p>And with the scikit 96 unique tokens, we can print scikit's vocabulary with print(vocab)</p>

<pre><code># 2. Scikit
import re
token_pattern = re.compile(r""\b\w\w+\b"", re.U)

def custom_tokenizer( s, min_term_length = 1 ):
    """"""
    Tokenizer to split text based on any whitespace, keeping only terms of at least a certain length which start with an alphabetic character.
    """"""
    return [x.lower() for x in token_pattern.findall(s) if (len(x) &gt;= min_term_length and x[0].isalpha() ) ]

from sklearn.feature_extraction.text import CountVectorizer

def preprocess(docs, min_df = 1, min_term_length = 1, ngram_range = (1,1), tokenizer=custom_tokenizer ):
    """"""
    Preprocess a list containing text documents stored as strings.
    doc : list de string (pas tokeniz√©)
    """"""
    # Build the Vector Space Model, apply TF-IDF and normalize lines to unit length all in one call
    vec = CountVectorizer(lowercase=True,
                      strip_accents=""unicode"",
                      tokenizer=tokenizer,
                      min_df = min_df,
                      ngram_range = ngram_range,
                      stop_words = None
                     ) 
    X = vec.fit_transform(docs)
    vocab = vec.get_feature_names()

    return (X,vocab)

docs_join = list()

for i in documents:
    docs_join.append(' '.join(i))

(X, vocab) = preprocess(docs_join, ngram_range = (1,2))

print(vocab)
</code></pre>
","7708272","","","","","2017-05-11 16:25:48","Trying to mimick Scikit ngram with gensim","<python><scikit-learn><gensim>","1","0","","","","CC BY-SA 3.0"
"63769246","1","","","2020-09-06 21:56:50","","0","254","<p>I'm having an issue topic modeling with a lot of data. I am trying to do both LDA and NMF topic modeling which I have done before, but not with the great volume of data I am currently working with. The main issue is that i can't hold all my data in memory while also creating the models.</p>
<p>I need both the models and associated metrics. Here is the code for how i make my models currently</p>
<pre><code>def make_lda(dictionary, corpus, num_topics):
    passes = 3

    # Make a index to word dictionary.
    temp = dictionary[0]  # This is only to &quot;load&quot; the dictionary.
    id2word = dictionary.id2token

    model = LdaMulticore(
        corpus=corpus,
        id2word=id2word,
        passes=passes,
        num_topics=num_topics
    )
    
    return model

def make_nmf(dictionary, corpus, num_topics):
    
    passes = 3

    # Make a index to word dictionary.
    temp = dictionary[0]  # This is only to &quot;load&quot; the dictionary.
    id2word = dictionary.id2token
    
    model = Nmf(
        corpus=corpus,
        id2word=id2word,
        passes=passes,
        num_topics=num_topics
    )
    
    return model
</code></pre>
<p>And here is how I get the coherence measures and some other statistics</p>
<pre><code>def get_model_stats(model, model_type, docs, dictionary, corpus, num_topics, verbose=False, get_topics=False):
    if model_type == 'lda':
        top_topics = model.top_topics(texts=docs, dictionary=dictionary, coherence='c_v') #, num_words=20)
    elif model_type == 'nmf':
        top_topics = model.top_topics(corpus=corpus, texts=docs, dictionary=dictionary, coherence='c_v') #, num_words=20)

    # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.
    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics
    rstd_atc = np.std([t[1] for t in top_topics]) / avg_topic_coherence
  
    if verbose:
        print('Average topic coherence: ', avg_topic_coherence)
        print('Relative Standard Deviation of ATC: ', rstd_atc)
    
    if get_topics:
        return avg_topic_coherence, rstd_atc, top_topics
    
    return avg_topic_coherence, rstd_atc
</code></pre>
<p>As you can see, I need my dictionary, texts, corpus, and id2token objects in memory at different times, sometimes all at the same time. But I can't do that since something like my texts use up a ton of memory. My machine just does not have enough.</p>
<p>I know I can pay to get a virtual machine with crazy amounts of RAM, but I want to know if there is a better solution. I can store all of my data on disk. Is there a way to run these models were the data is not in memory? Is there some other solution where I don't overload my memory?</p>
","11531123","","","","","2020-09-07 20:41:29","Topic Modeling Memory Error: How to do gensim topic modelling when with large amounts of data","<python><machine-learning><gensim><topic-modeling>","2","0","","","","CC BY-SA 4.0"
"46421771","1","46520884","","2017-09-26 08:47:19","","1","688","<p>I want to make a word2vec model with more n-grams that usual. As I found, Phrase class in gensim.models.phrase can find phrases that I want and it's possible to use phrases on corpus and use it's result model for word2vec train function.</p>

<p>So first of all I do something like below, exactly like sample codes in <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim documentation</a>.</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield word_tokenize(line)

sentences = MySentences('sentences_directory')

bigram = gensim.models.Phrases(sentences)

model = gensim.models.Word2Vec(bigram['sentences'], size=300, window=5, workers=8)
</code></pre>

<p>model has been created but without any good result in evaluation and a warning :</p>

<pre><code>WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable)
</code></pre>

<p>I searched for it and I found <a href=""https://groups.google.com/forum/#!topic/gensim/XWQ8fPMFSi0"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/gensim/XWQ8fPMFSi0</a> and changed my code:</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield word_tokenize(line)

class PhraseItertor(object):
    def __init__(self, my_phraser, data):
        self.my_phraser, self.data = my_phraser, data

    def __iter__(self):
        yield self.my_phraser[self.data]


sentences = MySentences('sentences_directory')

bigram_transformer = gensim.models.Phrases(sentences)

bigram = gensim.models.phrases.Phraser(bigram_transformer)

corpus = PhraseItertor(bigram, sentences)

model = gensim.models.Word2Vec(corpus, size=300, window=5, workers=8)
</code></pre>

<p>I get error:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/fatemeh/Desktop/Thesis/bigramModeler.py"", line 36, in &lt;module&gt;
    model = gensim.models.Word2Vec(corpus, size=300, window=5, workers=8)
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 478, in init
    self.build_vocab(sentences, trim_rule=trim_rule)
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 553, in build_vocab
    self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 575, in scan_vocab
    vocab[word] += 1
TypeError: unhashable type: 'list'
</code></pre>

<p>Now I want to know that what is wrong in my codes.</p>
","1013249","","","","","2017-10-02 07:08:45","Text Processing - Word2Vec training after phrase detection (bigram model)","<python><text-processing><gensim><word2vec><python-textprocessing>","1","0","1","","","CC BY-SA 3.0"
"56148576","1","61181845","","2019-05-15 11:44:23","","0","1523","<p>When I try to run:</p>

<pre><code>def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod1[doc] for doc in texts]

# Remove Stop Words
data_words_nostops1 = remove_stopwords(data_words1)

# Form Bigrams
data_words_bigrams1 = make_bigrams(data_words_nostops1)    
# Create Dictionary
    id2word1 = corpora.Dictionary(data_words_bigrams1)

# Create Corpus
texts1 = data_words_bigrams1

# Term Document Frequency
corpus1 = [id2word1.doc2bow(text) for text in texts1]

mallet_path = 'T:Python/Mallet/mallet-2.0.8/bin/mallet'

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus1, num_topics=15, id2word=id2word1)
</code></pre>

<p>I get the following error:</p>

<pre><code>CalledProcessError: Command 'T:/Python/Mallet/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\E26E5~1.RIJ\AppData\Local\Temp\3\a66fc0_corpus.txt --output C:\Users\E26E5~1.RIJ\AppData\Local\Temp\3\a66fc0_corpus.mallet' returned non-zero exit status 1.
</code></pre>

<p>What can I do in my code specifically to make it work? </p>

<p>Furthermore, the question on this error has been asked a few times before. However, each answer seems so specific to a particular case, that I don't see what I can change on my code now so that it will work. Can someone elaborate on the meaning of this problem?</p>
","7714681","","","","","2020-04-13 05:10:23","CalledProcessError: Returned non-zero exit status 1","<python><gensim><lda><mallet>","1","1","","","","CC BY-SA 4.0"
"24688116","1","","","2014-07-10 23:53:45","","8","6687","<p>I am using <code>gensim</code> for some NLP task. I've created a corpus from <code>dictionary.doc2bow</code> where <code>dictionary</code> is an object of <code>corpora.Dictionary</code>. Now I want to filter out the terms with low tf-idf values before running an LDA model. I looked into the <a href=""http://radimrehurek.com/gensim/corpora/mmcorpus.html"" rel=""noreferrer"">documentation</a> of the corpus class but cannot find a way to access the terms. Any ideas? Thank you.</p>
","688080","","688080","","2014-07-11 00:00:06","2018-11-01 03:30:20","How to filter out words with low tf-idf in a corpus with gensim?","<python><nlp><gensim>","4","1","1","","","CC BY-SA 3.0"
"46574720","1","","","2017-10-04 21:48:23","","3","1180","<p>I'm using LDA from gensim to perform a topic modeling. I know how to convert the raw text data to corpus and get the topics. But, after I get the topics, can I label or add back the topics results to the raw document? </p>

<p><strong>Here are my codes:</strong></p>

<pre><code>movie_reviews = pd.read_csv(data_path + 'movie_review.tsv',header=0,delimiter='\t',quoting=3) 

reviews = []
for i in range(len(movie_reviews['review'])):
reviews.append(review_to_words(movie_reviews['review']
              [i],stops=stopwords.words('english')))
from gensim import corpora
dictionary = corpora.Dictionary(reviews)
corpus = [dictionary.doc2bow(review) for review in reviews]
from gensim import models
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=10)
corpus_lda = lda[corpus_tfidf]
lda.print_topics(10)
[(0,
  u'0.001*ben + 0.001*sinatra + 0.001*santa + 0.001*henry + 0.001*band + 0.001*william + 0.001*fool + 0.001*tragic + 0.001*favourite + 0.001*bed'),
 (1,
  u'0.002*dentist + 0.002*homeless + 0.002*connery + 0.002*hawn + 0.002*judas + 0.002*bogus + 0.001*dickens + 0.001*hilarity + 0.001*snuff + 0.001*chong'),
 (2,
  u'0.002*freddy + 0.002*mst + 0.001*summary + 0.001*aliens + 0.001*fred + 0.001*broke + 0.001*express + 0.001*cube + 0.001*perfection + 0.001*struck'),
 (3,
  u'0.004*ned + 0.003*kidman + 0.002*nicole + 0.002*chuck + 0.002*hart + 0.002*sabrina + 0.002*miyazaki + 0.002*roberts + 0.002*amitabh + 0.001*educational'),
 (4,
  u'0.002*seagal + 0.002*buffy + 0.002*caprica + 0.002*stargate + 0.002*clown + 0.002*travolta + 0.001*bsg + 0.001*goat + 0.001*insomnia + 0.001*update'),
 (5,
  u'0.003*cinderella + 0.002*envy + 0.002*homicide + 0.002*sucker + 0.002*quantum + 0.002*stallone + 0.002*elvira + 0.002*walt + 0.002*lundgren + 0.001*boobs'),
 (6,
  u'0.002*pickford + 0.002*guaranteed + 0.002*swearing + 0.002*eleniak + 0.002*biko + 0.002*tremendously + 0.001*characterisation + 0.001*arnie + 0.001*radical + 0.001*generate'),
 (7,
  u'0.003*sandler + 0.002*dont + 0.002*buff + 0.002*ustinov + 0.002*brosnan + 0.001*amazon + 0.001*perry + 0.001*link + 0.001*maker + 0.001*adam'),
 (8,
  u'0.002*gandhi + 0.002*scarecrow + 0.002*frankie + 0.002*boxing + 0.002*creep + 0.002*worms + 0.002*mcqueen + 0.002*sellers + 0.002*duchovny + 0.002*appearances'),
 (9,
  u'0.002*sentinel + 0.002*scrooge + 0.002*che + 0.002*robots + 0.002*betty + 0.002*wtf + 0.002*redneck + 0.002*unexplained + 0.002*stiller + 0.002*groups') 

print corpus_lda[0]]
[(1, 0.032862717742657352), (2, 0.061544456899498043), (3, 0.17498689066920223), (5, 0.034931340026756269), (6, 0.01142214861116901), (7, 0.01368447078032208), (8, 0.014051012107502465), (9, 0.58954345105937356)]
</code></pre>

<p>The last code shows the distribution of each topic in document 1. Now, my question is: how can I convert it into a numeric variable for each topic with its weight for each document? </p>

<p>Desired output in Dataframe:</p>

<pre><code>Document ID  Topic1   Topic2   Topic3.... 
0           0.032    0.062   0.175  
</code></pre>

<p>As you can see, this is a DataFrame with the topic as column name and weight as the value. </p>

<p>Also, can I link this topic variable back to the raw document, which is movie_review here? </p>
","8722795","","4909087","","2017-10-04 21:49:23","2020-04-23 22:01:18","Python gensim LDA: add the topic to the document after getting the topics","<python><gensim><lda>","1","0","5","","","CC BY-SA 3.0"
"43896195","1","43909272","","2017-05-10 14:59:56","","0","365","<p>I have trained an LDA algorithm on a corpus , and what I'd like to do is getting for each sentence the topic on which it corresponds, in order, to make a comparison between what the algorithm finds and the labels I have.</p>

<p>I have tried with the code below, but the results are quite bad I find a great deal of topic 17 (maybe 25% of the volume, it should be closer to 5%)</p>

<p>Thanks for your help</p>

<pre><code># text lemmatized: list of string lemmatized
dico = Dictionary(texts_lemmatized)
corpus_lda = [dico.doc2bow(text) for text in texts_lemmatized]

lda_ = LdaModel(corpus_lda, num_topics=18)

df_ = pd.DataFrame([])
data = []

# theme_commentaire = label of the string
for i in range(0, len(theme_commentaire)):
     # lda_.get_document_topics() gives the distribution of all topic for a specific sentence
     algo = max(lda_.get_document_topics(corpus_lda[i]))[0]
     human = theme_commentaire[i]
     data.append([str(algo), human])

cols = ['algo', 'human']
df_ = pd.DataFrame(data, columns=cols)
df_.head()
</code></pre>
","7708272","","","","","2017-05-11 07:37:11","Gensim find topics in sentences","<python><gensim>","1","2","0","","","CC BY-SA 3.0"
"28979559","1","","","2015-03-11 05:55:35","","0","2344","<pre><code>corpus = PlaintextCorpusReader(""path"",'.*',encoding=""latin1"")
docs = [corpus.words(f)for f in corpus.fileids()]
docs2 = [[w.lower()for w in doc]for doc in docs]
docs3 = [[w for w in doc if re.search('^[a-z]+$', w)]for doc in docs2]
from nltk.corpus import stopwords
stop_list = stopwords.words('english')

docs4 = [[w for w in doc if w not in stop_list]for doc in docs3]
</code></pre>

<p>I have written the following code , which reads a corpus of files. FOllowed by that i have done some preprocessing steps ir removing punctuations , stopwords etc. I would now like to perform a word count and find most frequent words used in the text. I used the following code below to do so so.
for word in docs4:</p>

<pre><code>if word in word_counter:
    word_counter[word] += 1
else:
    word_counter[word] = 1

popular_words = sorted(word_counter, key = word_counter.get, reverse = True)
</code></pre>

<p>However i get the following error. --</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/rohanhm.2014/PycharmProjects/untitled1/bp.py"", line 18, in &lt;module&gt;
    if word in word_counter:
TypeError: unhashable type: 'list'
</code></pre>

<p>Any suggestions?</p>
","4222982","","","","","2015-03-11 09:19:10","Unhashable type 'list' - Wordcount","<python><regex><nltk><word-count><gensim>","3","2","","","","CC BY-SA 3.0"
"37570696","1","45776156","","2016-06-01 13:50:52","","13","5701","<p>I can't seem to find it or probably my knowledge on statistics and its terms are the problem here but I want to achieve something similar to the graph found on the bottom page of the <a href=""http://pythonhosted.org/lda/getting_started.html"" rel=""noreferrer"" title=""LDA"">LDA lib from PyPI</a> and observe the uniformity/convergence of the lines. How can I achieve this with <a href=""https://radimrehurek.com/gensim/models/ldamulticore.html#module-gensim.models.ldamulticore"" rel=""noreferrer"" title=""Gensim LDA MultiCore"">Gensim LDA</a>?</p>
","3783238","","","","","2021-06-11 12:34:51","How to monitor convergence of Gensim LDA model?","<python><lda><gensim><convergence>","1","0","2","","","CC BY-SA 3.0"
"46576437","1","","","2017-10-05 01:17:40","","0","133","<p>I have a dataframe it looks like this :</p>

<pre><code>id  created_at  text    month

0   911721027587231746  2017-09-23 22:36:46 ÿ™ŸÅÿßÿµŸäŸÑ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿ≥Ÿäÿßÿ±ÿßÿ™ ÿßŸÑÿ•ÿ≥ÿπÿßŸÅ ŸÑÿ™Ÿáÿ±Ÿäÿ® ÿßŸÑŸÖŸàÿßÿØ ÿßŸÑ...   9
1   911719688257851397  2017-09-23 22:31:27 ÿ™ÿ∑ŸàŸäÿ± ŸÑŸÇÿßÿ≠ ÿ¨ÿØŸäÿØ ŸÑŸÖÿ≠ÿßÿ±ÿ®ÿ© ÿ™ÿ≥Ÿàÿ≥ ÿßŸÑÿ£ÿ≥ŸÜÿßŸÜ\n https:/...   9
2   911715658395725826  2017-09-23 22:15:26 ""ÿ≠ŸÖÿØŸä ÿßŸÑŸÖŸäÿ±ÿ∫ŸÜŸä"" Ÿäÿ¥ÿßÿ±ŸÉ ÿ¨ŸÖŸáŸàÿ±Ÿá ÿ®ÿµŸàÿ±ÿ© ÿ¨ÿØŸäÿØÿ© ŸÖŸÜ ÿ¥Ÿá...   9
3   911715466166587392  2017-09-23 22:14:40 ÿ¥ÿÆÿµŸäÿ© ŸÖÿµÿ±.. ŸÅŸä ÿπŸäŸàŸÜ ÿ¨ŸÖÿßŸÑ ÿ≠ŸÖÿØÿßŸÜ (2) https://t.c...   9
</code></pre>

<p>month column has values that range from 1 to 11 and I want to build a model on the text data based on the number of the month and I'm trying to get the output and save it to a txt file but when I open the files I find it only contains one line each. </p>

<p>what I want is to get 11 text file each named per index and each one should contain 12 lines .</p>

<p>this is my code</p>

<pre><code>def model(final_text):

    sentences = [clean(raw_sentence) for raw_sentence in final_text]
    doc_clean = [i.split() for i in sentences]
    dictionary = corpora.Dictionary(doc_clean)
    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
    Lda = gensim.models.ldamodel.LdaModel
    ldamodel = Lda(doc_term_matrix, num_topics=12, id2word = dictionary, passes = 100, alpha='auto', update_every=5)
    x = ldamodel.print_topics(num_topics=12, num_words=5)

    y = ldamodel.show_topics(num_topics=12, num_words=5, formatted=False)
    topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in y]
    for topic,words in topics_words:
        #print("" "".join(words).encode('utf-8'))
        #print(words)

        f = open(str(i)+'.txt', 'wb')
        f.write("" "".join(words).encode('utf-8'))
        #f.write(words.encode('utf-8'))
    f.close()

#clean is just a function for cleaning data and it returns text

for i in range(1,12):
    df = parsed[parsed['month'] == i]
    text = df.text
    model(text)
</code></pre>

<p>what am I doing wrong here?</p>

<p>Thanks in advance</p>
","4676565","","","","","2017-10-05 03:16:10","how to write output data into a text file iteratively?","<python><string><pandas><encoding><gensim>","2","0","","","","CC BY-SA 3.0"
"46326173","1","46328592","","2017-09-20 15:30:07","","9","6042","<p>I'm new to topic modelling / Latent Dirichlet Allocation and have trouble understanding how I can apply the concept to my dataset (or whether it's the correct approach).</p>

<p>I have a small number of literary texts (novels) and would like to extract some general topics using LDA.</p>

<p>I'm using the <code>gensim</code> module in Python along with some <code>nltk</code> features. For a test I've split up my original texts (just 6) into 30 chunks with 1000 words each. Then I converted the chunks into document-term matrices and ran the algorithm. This is the code (although I think it doesn't matter for the question) :</p>

<pre><code># chunks is a 30x1000 words matrix

dictionary = gensim.corpora.dictionary.Dictionary(chunks)
corpus = [ dictionary.doc2bow(chunk) for chunk in chunks ]
lda = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = dictionary,
    num_topics = 10)
topics = lda.show_topics(5, 5)
</code></pre>

<p>However the result is completely different from any example I've seen in that the topics are full of meaningless words that can be found in <em>all</em> source documents, e.g. ""I"", ""he"", ""said"", ""like"", ... example:</p>

<pre><code>[(2, '0.009*""I"" + 0.007*""\'s"" + 0.007*""The"" + 0.005*""would"" + 0.004*""He""'), 
(8, '0.012*""I"" + 0.010*""He"" + 0.008*""\'s"" + 0.006*""n\'t"" + 0.005*""The""'), 
(9, '0.022*""I"" + 0.014*""\'s"" + 0.009*""``"" + 0.007*""\'\'"" + 0.007*""like""'), 
(7, '0.010*""\'s"" + 0.009*""I"" + 0.006*""He"" + 0.005*""The"" + 0.005*""said""'), 
(1, '0.009*""I"" + 0.009*""\'s"" + 0.007*""n\'t"" + 0.007*""The"" + 0.006*""He""')]
</code></pre>

<p>I don't quite understand why that happens, or why it doesn't happen with the examples I've seen. How do I get the LDA model to find more distinctive topics with less overlap? Is it a matter of filtering out more common words first? How can I adjust how many times the model runs? Is the number of original texts too small?</p>
","4745643","","","","","2017-09-20 17:42:59","Understanding LDA / topic modelling -- too much topic overlap","<python><nlp><gensim><lda><topic-modeling>","1","0","5","","","CC BY-SA 3.0"
"52693004","1","52694835","","2018-10-07 21:18:10","","0","2381","<p>I have a sample of ~60,000 documents.  We've hand coded 700 of them as having a certain type of content.  Now we'd like to find the ""most similar"" documents to the 700 we already hand-coded.  We're using gensim doc2vec and I can't quite figure out the best way to do this.</p>

<p>Here's what my code looks like:</p>

<pre><code>cores = multiprocessing.cpu_count()

model = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, 
        epochs=10, workers=cores, dbow_words=1, train_lbls=False)

all_docs = load_all_files() # this function returns a named tuple
random.shuffle(all_docs)
print(""Docs loaded!"")
model.build_vocab(all_docs)
model.train(all_docs, total_examples=model.corpus_count, epochs=5)
</code></pre>

<p>I can't figure out the right way to go forward.  Is this something that doc2vec can do?  In the end, I'd like to have a ranked list of the 60,000 documents, where the first one is the ""most similar"" document.</p>

<p>Thanks for any help you might have!  I've spent a lot of time reading the gensim help documents and the various tutorials floating around and haven't been able to figure it out.</p>

<p>EDIT: I can use this code to get the documents most similar to a short sentence:</p>

<pre><code>token = ""words associated with my research questions"".split()
new_vector = model.infer_vector(token)
sims = model.docvecs.most_similar([new_vector])
for x in sims:
    print(' '.join(all_docs[x[0]][0]))
</code></pre>

<p>If there's a way to modify this to instead get the documents most similar to the 700 coded documents, I'd love to learn how to do it!</p>
","10470286","","10470286","","2018-10-07 21:30:30","2018-10-22 19:04:54","Doc2Vec: Similarity Between Coded Documents and Unseen Documents","<python><nlp><gensim><word2vec><doc2vec>","3","2","","","","CC BY-SA 4.0"
"61407835","1","","","2020-04-24 11:59:17","","0","41","<p>When I tried to run below code, I get keyerror: </p>

<pre><code>KeyError: word fransƒ±z not in vocabulary. 
</code></pre>

<p>What is the issue?</p>

<pre><code>import numpy as np
from gensim.models import Word2Vec
from nltk.tokenize import sent_tokenize,word_tokenize
import string
text=""Victor Marie Hugo, Romantik akƒ±ma baƒülƒ± Fransƒ±z ≈üair, romancƒ± ve oyun yazarƒ±. En b√ºy√ºk ve √ºnl√º Fransƒ±z yazarlardan biri kabul edilir. Hugo'nun Fransa'daki edebi √ºn√º ilk olarak ≈üiirlerinden sonra da romanlarƒ±ndan ve tiyatro oyunlarƒ±ndan gelir. Pek √ßok ≈üiirinin i√ßinde √∂zellikle Les Contemplations ve La L√©gende des si√®cles b√ºy√ºk saygƒ± g√∂r√ºr. Fransa dƒ±≈üƒ±nda en √ßok Sefiller ve Notre Dame'ƒ±n Kamburu romanlarƒ±yla tanƒ±nƒ±r.Gen√ßliƒüinde ≈üiddetli bir kral yanlƒ±sƒ± olsa da, g√∂r√º≈ü√º yƒ±llar i√ßinde deƒüi≈üti ve tutkulu bir cumhuriyet destek√ßisi oldu. Eserleri zamanƒ±nƒ±n politik ve sosyal sorunlarƒ±na ve de sanatsal akƒ±mlarƒ±na deƒüinir. Hugo'nun cenazesi 1885'te Panth√©on'da g√∂m√ºld√º. Hugo hakkƒ±nda en √ßok eser yazƒ±lan ilk 100 ki≈üi listesinde yer almaktadƒ±r. Victor Hugo, Joseph L√©opold Sigisbert Hugo (1773‚Äì1828) ve Sophie Tr√©buchet (1772‚Äì1821) √ßiftinin √º√ß√ºnc√º oƒüluydu; Abel Joseph Hugo (1798‚Äì1855) ve Eug√®ne Hugo (1800‚Äì1837) isminde iki aƒüabeyi vardƒ±. 1802'de Besan√ßon'da doƒüdu. Napolyon'un bir kahraman olduƒüunu d√º≈ü√ºnen serbest fikirli bir cumhuriyet√ßiydi. Annesi 1812'de Napolyon'a kar≈üƒ± komplo kurduƒüu i√ßin idam edilen General Victor Lahorie ile sevgili olduƒüu d√º≈ü√ºn√ºlen Katolik bir Kralcƒ±ydƒ±.Hugo'nun √ßocukluƒüu √ºlkede siyasi karma≈üƒ±klƒ±ƒüƒ±n olduƒüu bir d√∂nemde ge√ßti. Doƒüumundan iki yƒ±l sonra Napolyon ƒ∞mparator ilan edilmi≈ü, 18 ya≈üƒ±ndayken de Bourbon Monar≈üisi yeniden tahta ge√ßirilmi≈üti. Hugo'nun ailesinin ters dini ve politik g√∂r√º≈üleri Fransa'da egemenlik m√ºcadelesi veren kuvvetleri yansƒ±tƒ±yordu. Hugo'nun babasƒ± ƒ∞spanya'da yenilene kadar orduda y√ºksek r√ºtbeli bir subaydƒ±.Babasƒ± subay olduƒüu s√ºrece aile sƒ±k sƒ±k ta≈üƒ±ndƒ± ve bu yolculuklar sƒ±rasƒ±nda Hugo pek √ßok ≈üey √∂ƒürendi. √áocukluƒüunda Napoli'ye giderken geni≈ü Alpler'deki ge√ßitleri ve karlƒ± zirveleri, muhte≈üem Akdeniz mavisini ve ≈üenlikler yapƒ±lan Roma'yƒ± g√∂rd√º. 5 ya≈üƒ±nda olmasƒ±na raƒümen bu 6 aylƒ±k geziyi her zaman aklƒ±nda tuttu. Aile Napoli'de birka√ß ay kalƒ±p doƒüruca Paris'e d√∂nd√º.Hugo'nun annesi Sophie evliliƒüinin ba≈üƒ±nda kocasƒ±na ƒ∞talya (Leopold Napoli'ye yakƒ±n bir vilayette valiydi) ve ƒ∞spanya'ya (√º√ß vilayette g√∂rev almƒ±≈ütƒ±) kadar e≈ülik etti. Askeri hayatƒ±n getirdiƒüi yorucu yolculuklar ve kocasƒ±nƒ±n inancƒ±nƒ±n zayƒ±flƒ±ƒüƒ± nedeniyle ters d√º≈ümelerinden dolayƒ± Sophie 1803'te Leopold'dan bir s√ºreliƒüine ayrƒ±lƒ±p √º√ß √ßocuƒüuyla Paris'e yerle≈üti. Bundan sonra Hugo'nun eƒüitimi ve yeti≈ümesi √ºzerine eƒüildi. Bu y√ºzden Hugo'nun kariyerinin ilk d√∂nemindeki ≈üiir ve kurgu √ßalƒ±≈ümalarƒ± annesinin inancƒ±nƒ±n ve krala baƒülƒ±lƒ±ƒüƒ±nƒ±n yansƒ±masƒ±ydƒ±. Ama ba≈üƒ±nƒ± Fransa'daki 1848 Devrimi'nin √ßektiƒüi olaylar sƒ±rasƒ±nda Katolik Kralcƒ± yanlƒ±sƒ± eƒüitime ba≈ükaldƒ±rƒ±p Cumhuriyet√ßiliƒüi ve √ñzg√ºr d√º≈ü√ºnceyi desteklemeye ba≈üladƒ±.Gen√ßliƒüinde a≈üƒ±k oldu ve annesinin isteklerine kar≈üƒ± gelip √ßocukluk arkada≈üƒ± Ad√®le Foucher (1803‚Äì1868) ile gizlice ni≈üanlandƒ±. Annesi ile yakƒ±n ili≈ükisinden dolayƒ± Ad√®le ile evlenmek i√ßin annesinin √∂l√ºm√ºne (1821) kadar bekledi ve 1822'de evlendi.Ad√®le ve Victor Hugo'nun ilk √ßocuƒüu Leopold 1823'te doƒüdu ama doƒüduktan kƒ±sa s√ºre sonra √∂ld√º. Sonraki sene kƒ±zlarƒ± 28 Aƒüustos 1824'te L√©opoldine doƒüdu. Onu 4 Kasƒ±m 1826'da doƒüan Charles, 28 Ekim 1828'de doƒüan Fran√ßois-Victor, ve 24 Aƒüustos 1830'da doƒüan Ad√®le takip etti.Hugo'nun en b√ºy√ºk ve en sevdiƒüi kƒ±zƒ± L√©opoldine, Charles Vacquerie ile evliliƒüinden kƒ±sa s√ºre sonra 19 ya≈üƒ±ndayken 1843'te √∂ld√º. 4 Eyl√ºl 1843'te Seine nehrinde boƒüuldu. Gemi alabaro olduƒüundan aƒüƒ±r eteƒüi tarafƒ±ndan dibe doƒüru √ßekildi ve kocasƒ± Charles Vacquerie de onu kurtarmaya √ßalƒ±≈üƒ±rken √∂ld√º. O zaman metresi ile Fransa'nƒ±n g√ºneyinde seyahat etmekte olan Hugo kƒ±zƒ±nƒ±n √∂l√ºm√ºn√º oturduƒüu cafede okuduƒüu bir gazeteden √∂ƒürendi. Kƒ±zƒ±nƒ±n √∂l√ºm√º Hugo'yu olduk√ßa harap etti.III. Napolyon'un 1851 yƒ±lƒ±nƒ±n sonundaki askeri darbesi sebebiyle s√ºrg√ºne √ßƒ±ktƒ±. Fransa'dan ayrƒ±ldƒ±ktan sonra, Channel Adalarƒ±'na gitmeden √∂nce kƒ±sa bir s√ºre Br√ºksel'de ya≈üadƒ±. 1852'den 1855'e kadar Jersey'de ya≈üadƒ±. 1855'te 15 yƒ±l ya≈üayacaƒüƒ± Guernsey'e ta≈üƒ±ndƒ±. III. Napolyon 1859'da genel af ilan ettiƒüinde √ºlkesine d√∂nme fƒ±rsatƒ± elde ettiyse de s√ºrg√ºnde kalmayƒ± tercih etti. Kaybedilen Fransa-Prusya Sava≈üƒ±'nƒ±n sonucu olarak III. Napolyon iktidardan √ßekilmek zorunda kalƒ±nca √ºlkesine d√∂nd√º. Paris Ku≈üatmasƒ±'ndan sonra hayatƒ±nƒ±n geri kalanƒ±nƒ± Fransa'da ge√ßirmek i√ßin geri d√∂nmeden √∂nce tekrar Guernsey'e ta≈üƒ±nƒ±p 1872 ve 1873 arasƒ± orada kaldƒ±. Hugo ilk romanƒ±nƒ± (Han d'Islande, 1823) evliliƒüinden bir yƒ±l sonra yayƒ±mladƒ±. √ú√ß yƒ±l sonra da ikinci romanƒ± (Bug-Jargal, 1826) basƒ±ldƒ±. 1829 ve 1840 arasƒ±nda zamanƒ±nƒ±n en iyi ≈üairlerinden biri olarak √ºn√ºn√º peki≈ütiren be≈ü ≈üiir kitabƒ± (Les Orientales, 1829; Les Feuilles d'automne, 1831; Les Chants du cr√©puscule, 1835; Les Voix int√©rieures, 1837; ve Les Rayons et les ombres, 1840) yayƒ±nladƒ±.""
punctuations = "",;:()[]/{}''""
sentence=""!.?""
no_punct = """"
for char in text:
   if char not in punctuations:
       no_punct = no_punct + char
t_sen = """"
for char in no_punct:
   if char in sentence:
       t_sen = no_punct.split(char)

corpus=[]
for cumle in t_sen:
    corpus.append(cumle.split())

model=Word2Vec(corpus,size=30,window=5,min_count=5,sg=1)
model.wv.most_similar('fransƒ±z')
</code></pre>
","7683846","","6573902","","2020-04-24 17:09:32","2020-04-25 04:56:41","KeyError: word fransƒ±z not in vocabulary","<python><nlp><data-science><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"35470805","1","","","2016-02-18 00:56:37","","4","210","<p>I am looking to calculate the similarity between documents using gensim on Python.</p>

<p>I want a way to be able to restrict the calculations to only a subset of the corpus. Specifically, my documents have an associated year, and i want a way of only computing similarities between the search document and other document which have the same value for that variable. </p>

<p>I can not see any instructions on e.g. <a href=""http://radimrehurek.com/gensim/simserver.html"" rel=""nofollow"">http://radimrehurek.com/gensim/simserver.html</a> on how to associate additional variables with each document, and in turn how to restrict the similarities to only those documents - and indeed what i am trying to do may not be feasible. My question is thus, is this it is possible, or is the only way to achieve this to  use multiple corpuses.</p>
","2801069","","","","","2017-04-07 20:09:15","Restrict gensim similarity calculations to a subset of a corpus","<python><gensim>","1","0","0","","","CC BY-SA 3.0"
"36958388","1","","","2016-04-30 18:05:48","","2","413","<p>I would like to use genism doc2vec model for a classification task.
However, It seems like the gensim implementation of doc2vec requires to see all documents (train and test) to build the vocabulary before training the model. Otherwise, you get keyerror if you want to get document vector of a document that was not present when building the vocabulary. I wonder if my understanding is correct! In practice, one does not have access to the test data at the time of training.</p>

<p>Is there any way to update the vocabulary at the test time to be able to get document representation of test documents?</p>
","2994860","","","","","2016-05-28 19:27:46","getting paragraph representation for unseen paragraphs in doc2vec","<classification><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"54521323","1","54523227","","2019-02-04 17:28:50","","1","554","<p>I have protein sequences and want to do doc2vec. My goal is to have one vector for each sentence/sequence.</p>

<p>I have 1612 sentences/sequences and 30 classes so the label is not unique and many documents share the same labels.</p>

<p>So when I first tried doc2vec, it gave my just 30 vectors which is the number of unique labels. Then I decided to have multiple tags to get a vector for each sentence.</p>

<p>When I did this I ended up having more vectors than my sentences. Any explanations what might have gone wrong?</p>

<p><a href=""https://i.stack.imgur.com/0c734.jpg"" rel=""nofollow noreferrer"">Screenshot of my data</a></p>

<p><a href=""https://i.stack.imgur.com/MVLN3.jpg"" rel=""nofollow noreferrer"">Screenshot of corpus</a></p>

<p><code>tagged = data.apply(lambda r: TaggedDocument(words=(r[""A""]), tags=[r.label,r.id]), axis=1)</code></p>

<p><code>print(len(tagged))</code></p>

<p><code>1612</code></p>

<p><code>sents = tagged.values</code></p>

<p><code>model = Doc2Vec(sents, size=5, window=5, iter=20, min_count = 0)</code></p>

<p><code>sents.shape</code></p>

<p><code>(1612,)</code></p>

<p><code>model.docvecs.vectors_docs.shape</code></p>

<p><code>(1643,5)</code></p>

<p><a href=""https://i.stack.imgur.com/0c734.jpg"" rel=""nofollow noreferrer"">Screenshot of my data</a></p>
","10950908","","10950908","","2019-02-04 18:45:41","2019-02-04 19:40:49","I get more vectors than my documents size - gensim doc2vec","<python><tags><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"53587960","1","53601635","","2018-12-03 05:29:29","","2","1742","<p>Assume that we have 1000 words (A1, A2,..., A1000) in a dictionary. As fa as I understand, in words embedding or word2vec method, it aims to represent each word in the dictionary by a vector where each element represents the similarity of that word with the remaining words in the dictionary. Is it correct to say there should be 999 dimensions in each vector, or the size of each word2vec vector should be 999?</p>

<p>But with Gensim Python, we can modify the value of ""size"" parameter for Word2vec, let's say size = 100 in this case. So what does ""size=100"" mean? If we extract the output vector of A1, denoted (x1,x2,...,x100), what do x1,x2,...,x100 represent in this case?</p>
","10737255","","","","","2018-12-03 20:47:17","What is the meaning of ""size"" of word2vec vectors [gensim library]?","<python><gensim><word2vec><word-embedding>","1","1","","","","CC BY-SA 4.0"
"62304813","1","","","2020-06-10 13:29:10","","2","92","<p>I am working on a NLP problem with gensim that requires the use of multilingual embeddings. I have the already pretrained and aligned .txt embeddings that <a href=""https://fasttext.cc/docs/en/aligned-vectors.html"" rel=""nofollow noreferrer"">FastText provides in their web</a>. Sadly, they don't provide the full model, but these vectors are missing some important vocabulary on my problem and the per-character embedding ability of a FastText model here comes very handy for these cases.</p>

<p>My question:</p>

<ol>
<li>Is there a way to recreate the entire model so I can infer new vocabulary that is also in the vector space of aligned embeddings?</li>
<li>If not, Is there still a way to obtain those terms in that aligned embedding space? Without having to retrain a new entire FastText and then align it the already pre-trained ones? </li>
</ol>
","1160393","","","","","2020-06-10 13:29:10","Full FastText model from KeyedVectors to infer new words in aligned space","<nlp><gensim><fasttext>","0","0","","","","CC BY-SA 4.0"
"62308418","1","62310583","","2020-06-10 16:24:41","","0","931","<p>I am trying to load the pretrained vec file of Facebook fasttext crawl-300d-2M.vec with the next code:</p>

<pre><code>from gensim.models.fasttext import load_facebook_model, load_facebook_vectors

model_facebook = load_facebook_vectors('fasttext/crawl-300d-2M.vec')
</code></pre>

<p>But it fails with the next error:</p>

<pre><code>NotImplementedError: Supervised fastText models are not supported
</code></pre>

<p>It is not possible to load this vector?</p>

<p>If it is possible, afterwards can I train it with my own sentences?</p>

<p>Thanks in advance.</p>

<p>Whole error trace:</p>

<pre><code>---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-181-f8262e0857b8&gt; in &lt;module&gt;
----&gt; 1 model_facebook = load_facebook_vectors('fasttext/crawl-300d-2M.vec')

/opt/conda/lib/python3.7/site-packages/gensim/models/fasttext.py in load_facebook_vectors(path, encoding)
   1196 
   1197     """"""
-&gt; 1198     model_wrapper = _load_fasttext_format(path, encoding=encoding, full_model=False)
   1199     return model_wrapper.wv
   1200 

/opt/conda/lib/python3.7/site-packages/gensim/models/fasttext.py in _load_fasttext_format(model_file, encoding, full_model)
   1220     """"""
   1221     with gensim.utils.open(model_file, 'rb') as fin:
-&gt; 1222         m = gensim.models._fasttext_bin.load(fin, encoding=encoding, full_model=full_model)
   1223 
   1224     model = FastText(

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in load(fin, encoding, full_model)
    339         model.update(dim=magic, ws=version)
    340 
--&gt; 341     raw_vocab, vocab_size, nwords, ntokens = _load_vocab(fin, new_format, encoding=encoding)
    342     model.update(raw_vocab=raw_vocab, vocab_size=vocab_size, nwords=nwords, ntokens=ntokens)
    343 

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in _load_vocab(fin, new_format, encoding)
    192     # Vocab stored by [Dictionary::save](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc)
    193     if nlabels &gt; 0:
--&gt; 194         raise NotImplementedError(""Supervised fastText models are not supported"")
    195     logger.info(""loading %s words for fastText model from %s"", vocab_size, fin.name)
    196 

NotImplementedError: Supervised fastText models are not supported
</code></pre>
","5857214","","10883094","","2020-06-11 09:43:31","2020-06-11 09:43:31","Word embedding with gensim and FastText, training on pretrained vectors","<python><gensim><word-embedding><fasttext>","1","2","","","","CC BY-SA 4.0"
"54623849","1","54625881","","2019-02-11 04:08:38","","0","310","<p>What is the similarity score in the genism similar_by_word function?</p>

<p>I was reading here about the genism similar_by_word function:
<a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html</a></p>

<p>The similar_by_word function returns a sequence of (word, similarity). What is the definition by similarity here and how is it calculated?</p>
","4984897","","","","","2019-02-11 07:39:27","What is the similarity score in the gensim similar_by_word function?","<gensim>","1","0","","","","CC BY-SA 4.0"
"61357566","1","","","2020-04-22 04:56:09","","0","98","<p>I'm working with around 8k documents and all of them are based on a single topic. However, the documents cover various different events that happened across the world, related to that single topic. I want to find these subtopics (or events) from the documents. Now to achieve this, I'm using the gensim LDA model:</p>

<pre><code>corpus = [dictionary.doc2bow(doc) for doc in docTrain]

model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=17, chunksize=10000, id2word=dictionary,random_state=123, alpha = 0.01, eta = 0.9, passes = 10 )

coherencemodel = gensim.models.CoherenceModel(model=model, texts=data, dictionary=dictionary, coherence='c_v')
</code></pre>

<p>Since I was unaware of the number of topics in this case, I used elbow method to determine the optimal number of topics in this case, which comes out to be 17 or 18. Also, the coherence score is not increasing beyond 0.4.</p>

<p>I want to know what is going wrong and if there is any other approach that would help me solve this problem in a better way. Please let me know if any other information regarding my approach is required. </p>
","9987916","","","","","2020-04-22 04:56:09","Topic modeling - How to get different sub-topics from a single topic","<machine-learning><data-science><gensim><lda><topic-modeling>","0","5","","","","CC BY-SA 4.0"
"61521498","1","","","2020-04-30 10:39:00","","1","403","<p>I am trying to run Python's gensim package in R environment via reticulate. More specifically, I am trying to build a doc2vec model, for which a corpus of tokens and tags needs to be prepared.</p>

<p>The TaggedDocument function is where I am having problems. Here's an example in python of what I am trying to reproduce in R:</p>

<pre><code>import pandas as pd
import numpy as np
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize

data = [""this is the first sentence"",
        ""running doc2vec via reticulate"",
        ""r and python bff forever"",
        ""this aint working""]

tags = [""a"",""b"",""a"",""c""]

corpus = pd.DataFrame({""sentences"": data, ""labels"": tags})

tagged_data = corpus.apply(
    lambda r: TaggedDocument(words=word_tokenize(r['sentences']), tags=[r.labels]), axis=1)

</code></pre>

<p>This results in an output of this kind:</p>

<pre><code>0       ([this, is, the, first, sentence], [a])
1    ([running, doc2vec, via, reticulate], [b])
2         ([r, and, python, bff, forever], [a])
3                  ([this, aint, working], [c])
dtype: object
</code></pre>

<p>which can be used to build a vocabulary and train a doc2vec model.</p>

<p>How can I get to the same result in R (possibly without loops)?</p>

<pre><code>library(reticulate)

gensim &lt;- import(""gensim"") 
Doc2Vec &lt;- gensim$models$Doc2Vec 
TaggedDocument &lt;- gensim$models$doc2vec$TaggedDocument


sentences &lt;- c(""this is the first sentence"",
        ""running doc2vec via reticulate"",
        ""r and python bff forever"",
        ""this aint working"")

labels &lt;- c(""a"",""b"",""c"",""a"")
</code></pre>

<p>Thanks in advance!</p>

<p><strong>* EDIT *</strong> </p>

<p>I have been trying an even simpler setting: </p>

<pre><code>library(reticulate)

gensim &lt;- import(""gensim"") 
Doc2Vec &lt;- gensim$models$Doc2Vec 
TaggedDocument &lt;- gensim$models$doc2vec$TaggedDocument


sentences &lt;- c(""this is the first sentence"")
tags &lt;- c(""a"")

df &lt;- data.frame (tokens= sentences, labels = tags)

tagged_docs &lt;- TaggedDocument(words = df$tokens, tags = df$labels)
</code></pre>

<p>but I keep receiving the same error message: </p>

<blockquote>
  <p>Blockquote
  Error in py_call_impl(callable, dots$args, dots$keywords) : 
    AttributeError: 'str' object has no attribute 'words' - Detailed traceback: 
    File ""C:\Anaconda\lib\site-packages\gensim\models\doc2vec.py"", line 1184, in build_vocab
      progress_per=progress_per, trim_rule=trim_rule
    File ""C:\Anaconda\lib\site-packages\gensim\models\doc2vec.py"", line 1381, in scan_vocab
      total_words, corpus_count = self._scan_vocab(documents, docvecs, progress_per, trim_rule)
    File ""C:\Anaconda\lib\site-packages\gensim\models\doc2vec.py"", line 1310, in _scan_vocab
      if isinstance(document.words, string_types):</p>
</blockquote>

<p><strong>---</strong> </p>

<p>What am I doing wrong?</p>
","13083886","","13083886","","2020-05-05 15:32:44","2020-10-04 10:07:32","How can I use the TaggedDocument function (Gensim \ Doc2Vec) via Reticulate in R?","<r><gensim><doc2vec><reticulate>","1","0","","","","CC BY-SA 4.0"
"44910999","1","","","2017-07-04 16:54:25","","4","437","<p>I am fairly new to the NLP embedding world. I used gensim's word2vec model and tensorflow vector representation.</p>

<p>I have a question that while training gensim's word2vec model it takes tokenize sentences, while tensorflow takes a long list of words. How does it differ in training. Is there any quality impact?
Also how does then tensorflow cater to the needs of skip-gram as now the data is a list of words and no more sentences. 
I am referring to the tensorflow's tutorial found at link <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/word2vec</a></p>

<p>Pardon me if my understanding in this domain is wrong would appreciate if my understanding is cleared.<br>
Thank you for your guidance and help.</p>
","7220545","","","","","2017-07-04 16:54:25","how does gensim's word2vec differ from tensorflow vector representation?","<vector><tensorflow><nlp><gensim><word-embedding>","0","0","0","","","CC BY-SA 3.0"
"54627037","1","54632732","","2019-02-11 09:05:05","","0","156","<p>I am trying to rewrite algorithm that basically takes a input text file and compares with different documents and results the similarities.</p>

<p>Now I want to print output of unmatched words and output a new textile with unmatched words.</p>

<p>From this code, ""hello force"" is the input and is checked against the raw_documents and prints out rank for matched document between 0-1(word ""force"" is matched with second document and ouput gives more rank to second document but ""hello"" is not in any raw_document i want to print unmatched word ""hello"" as not matched ), But what i want is to print unmatched input word that was not matched with any of the raw_document</p>

<pre><code>import gensim
import nltk

from nltk.tokenize import word_tokenize

raw_documents = [""I'm taking the show on the road"",
                 ""My socks are a force multiplier."",
             ""I am the barber who cuts everyone's hair who doesn't 
cut their own."",
             ""Legend has it that the mind is a mad monkey."",
            ""I make my own fun.""]

gen_docs = [[w.lower() for w in word_tokenize(text)]
            for text in raw_documents]

dictionary = gensim.corpora.Dictionary(gen_docs)

corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]

tf_idf = gensim.models.TfidfModel(corpus)
s = 0
for i in corpus:
    s += len(i)
sims =gensim.similarities.Similarity('/usr/workdir/',tf_idf[corpus],
                                  num_features=len(dictionary))
query_doc = [w.lower() for w in word_tokenize(""hello force"")]

query_doc_bow = dictionary.doc2bow(query_doc)

query_doc_tf_idf = tf_idf[query_doc_bow]
result = sims[query_doc_tf_idf] 
print result
</code></pre>
","10037769","","","","","2019-02-11 14:27:55","How can I find and print unmatched/dissimilar words from the documents?","<python><scikit-learn><nltk><gensim>","1","1","","","","CC BY-SA 4.0"
"54839158","1","54874823","","2019-02-23 07:09:09","","0","1477","<p>After I trained word embeddings, I saved it as npz format.
While I am trying to load it as KeyedVectors format, it makes errors.
How can I load numpy array as gensim.KeyedVectors format?
I really need it because I need to use functions like most_similar() not just vector values.</p>

<p>in model.py with tensorflow,</p>

<pre><code>self.verb_embeddings = tf.Variable(np.load(cfg.pretrained_target)[""embeddings""],
                                               name=""verb_embeddings"",
                                               dtype=tf.float32,
                                               trainable=cfg.tune_emb)
</code></pre>

<p>in saving.py</p>

<pre><code>target_emb = sess.run(model.verb_embeddings)
np.savez_compressed(""trained_target_emb.npz"", embeddings=target_emb)
</code></pre>

<p>in main.py</p>

<pre><code> model = KeyedVectors.load('trained_target_emb.npz')
</code></pre>

<p>I got</p>

<pre><code>_pickle.UnpicklingError: A load persistent id instruction was encountered, but no persistent_load function was specified.
</code></pre>

<p>also tried</p>

<pre><code> model = KeyedVectors.load_word2vec_format('trained_target_emb.npz')
</code></pre>

<p>but got</p>

<pre><code> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xde in position 14: invalid continuation byte
</code></pre>
","7574561","","","","","2019-02-25 21:18:57","How to load numpy array to gensim Keyedvector format?","<python><numpy><tensorflow><gensim><embedding>","1","0","","","","CC BY-SA 4.0"
"62310124","1","62562168","","2020-06-10 18:01:13","","0","498","<p>I am trying to save a custom FastText model trained with gensim. I want to save the binary files to have the possibility of training again the model, if it may.</p>

<p>The code to save the binary file is the next one:</p>

<pre><code>from gensim.models.fasttext import save_facebook_model

save_facebook_model(model,'own_fasttext_model.bin')
</code></pre>

<p>But I am obtaining the next error in that same line:</p>

<pre><code>---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-192-c9c2c41985af&gt; in &lt;module&gt;
      2 from gensim.models.fasttext import save_facebook_model
      3 
----&gt; 4 save_facebook_model(model,'own_fasttext_model.bin')

/opt/conda/lib/python3.7/site-packages/gensim/models/fasttext.py in save_facebook_model(model, path, encoding, lr_update_rate, word_ngrams)
   1334     """"""
   1335     fb_fasttext_parameters = {""lr_update_rate"": lr_update_rate, ""word_ngrams"": word_ngrams}
-&gt; 1336     gensim.models._fasttext_bin.save(model, path, fb_fasttext_parameters, encoding)

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in save(model, fout, fb_fasttext_parameters, encoding)
    666     if isinstance(fout, str):
    667         with open(fout, ""wb"") as fout_stream:
--&gt; 668             _save_to_stream(model, fout_stream, fb_fasttext_parameters, encoding)
    669     else:
    670         _save_to_stream(model, fout, fb_fasttext_parameters, encoding)

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in _save_to_stream(model, fout, fb_fasttext_parameters, encoding)
    629 
    630     # Save words and ngrams vectors
--&gt; 631     _input_save(fout, model)
    632     fout.write(struct.pack('@?', False))  # Save 'quot_', which is False for unsupervised models
    633 

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in _input_save(fout, model)
    573 
    574     assert vocab_dim == ngrams_dim
--&gt; 575     assert vocab_n == len(model.wv.vocab)
    576     assert ngrams_n == model.wv.bucket
    577 

AssertionError: 
</code></pre>

<p>Any clue on what could be happening?</p>

<p>Thanks in advance.</p>
","5857214","","","","","2020-06-24 18:53:06","Saving FastText custom model binary with Gensim","<save><gensim><fasttext>","2","0","","","","CC BY-SA 4.0"
"61622340","1","","","2020-05-05 20:20:44","","0","310","<p>Do we have an option to save a trained Gensim Word2Vec model as a saved model using tf 2.0 <code>tf.saved_model.save</code>? In other words, how can I save a trained embedding vector as a saved model signature to work with tensorflow 2.0. The following steps are not correct normally:</p>

<pre><code>model = gensim.models.Word2Vec(...)

model.init_sims(..)

model.train(..)

model.save(..)

module = gensim.models.KeyedVectors.load_word2vec(...)

tf.saved_model.save(
    module, 
    export_dir
)
</code></pre>

<p>EDIT:</p>

<p>This example helped me about how to do it : <a href=""https://keras.io/examples/nlp/pretrained_word_embeddings/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/pretrained_word_embeddings/</a></p>
","904745","","904745","","2020-06-16 08:47:14","2020-06-16 08:47:14","Save trained gensim word2vec model as a tensorflow SavedModel","<tensorflow><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"63567713","1","63567996","","2020-08-24 19:40:43","","0","354","<p>I have trained a word2vec model called <code>word_vectors</code>, using the Gensim package with size = 512.</p>
<pre><code>fname = get_tmpfile('word2vec.model')
word_vectors = KeyedVectors.load(fname, mmap='r')
</code></pre>
<p>Now, I have created a new Numpy array (also of size 512) which I have added to the word2vec as follows:</p>
<pre><code>vector = (rand(512)-0.5) *20
word_vectors.add('koffie', vector)
</code></pre>
<p>Doing this seems to go fine and even when I call</p>
<pre><code>word_vectors['koffie']
</code></pre>
<p>I get the array as output, as expected.</p>
<p>However, when I want to look for the most similar words in my model and run the following code:</p>
<pre><code>word_vectors.most_similar('koffie')
</code></pre>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-283-ce992786ce89&gt;&quot;, line 1, in &lt;module&gt;
    word_vectors.most_similar('koffie')

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\envs\ldaword2vec\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 553, in most_similar
    mean.append(weight * self.word_vec(word, use_norm=True))

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\envs\ldaword2vec\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 461, in word_vec
    result = self.vectors_norm[self.vocab[word].index]

IndexError: index 146139 is out of bounds for axis 0 with size 146138


word_vector.size()
Traceback (most recent call last):

  File &quot;&lt;ipython-input-284-2606aca38446&gt;&quot;, line 1, in &lt;module&gt;
    word_vector.size()

NameError: name 'word_vector' is not defined
</code></pre>
<p>The error seems to indicate that my indexing isn't correct here. But since I am only indexing indirectly (with a key rather than an actual numeric index), I don't see what I need to change here.</p>
<p>Who knows what goes wrong here? And what can I do to overcome this error?</p>
","7714681","","","","","2020-08-24 21:10:23","IndexError: index is out of bounds - word2vec","<python-3.x><numpy><gensim><word2vec><index-error>","1","0","","","","CC BY-SA 4.0"
"11471376","1","11473707","","2012-07-13 13:22:10","","6","11156","<p>I am using Gensim to do some large-scale topic modeling. I am having difficulty understanding how to determine predicted topics for an unseen (non-indexed) document. For example: I have 25 million documents which I have converted to vectors in LSA (and LDA) space. I now want to figure out the topics of a new document, lets call it x.</p>

<p>According to the Gensim documentation, I can use:</p>

<pre><code>topics = lsi[doc(x)]
</code></pre>

<p>where doc(x) is a function that converts x into a vector.</p>

<p>The problem is, however, that the above variable, topics, returns a vector. The vector is useful if I am comparing x to additional documents because it allows me to find the cosine similarity between them, but I am unable to actually return specific words that are associated with x itself.</p>

<p>Am I missing something, or does Gensim not have this capability?</p>

<p>Thank you,</p>

<p><strong>EDIT</strong></p>

<p>Larsmans has the answer.</p>

<p>I was able to show the topics by using:</p>

<pre><code>for t in topics:
    print lsi.show_topics(t[0])
</code></pre>
","1509597","","166749","","2012-07-14 13:02:21","2014-05-17 16:43:02","Finding topics of an unseen document via Gensim","<python><nlp><latent-semantic-indexing><gensim>","2","1","2","","","CC BY-SA 3.0"
"37487504","1","37530523","","2016-05-27 15:40:47","","0","3283","<p>I computed my LDA model, I retrieved my topics and now I am looking for the way to compute the weight/percentage of each topic on the corpus. Surprisingly I cannot find the way to do this, so far my code looks like:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

## Tokenizing
tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = stopwords.words('english')

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

import json
import nltk
import re
import pandas

appended_data = []

#for i in range(20014,2016):
#    df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
#    appended_data.append(df0)

for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)


appended_data = pandas.concat(appended_data)
# doc_set = df1.body

doc_set = appended_data.body

# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:

    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    # add tokens to list
    texts.append(stopped_tokens)

# turn our tokenized documents into a id &lt;-&gt; term dictionary
dictionary = corpora.Dictionary(texts)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

# generate LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=50)
ldamodel.save(""model.lda0"")  
</code></pre>

<p>So far, what I have seen in other forums is to do the following:</p>

<pre><code>from itertools import chain
print(type(doc_set))
print(len(doc_set))

for top in ldamodel.print_topics():
  print(top)
print

# Assinging the topics to the document in corpus
lda_corpus = ldamodel[corpus]
#print(lda_corpus)

# Find the threshold, let's set the threshold to be 1/#clusters,
# To prove that the threshold is sane, we average the sum of all probabilities:
scores = list(chain(*[[score for topic_id,score in topic] \
                     for topic in [doc for doc in lda_corpus]]))
print(sum(scores))
print(len(scores))
threshold = sum(scores)/len(scores)
print(threshold)

cluster1 = [j for i,j in zip(lda_corpus,doc_set) if i[0][1] &gt; threshold]
cluster2 = [j for i,j in zip(lda_corpus,doc_set) if i[1][1] &gt; threshold]
cluster3 = [j for i,j in zip(lda_corpus,doc_set) if i[2][1] &gt; threshold] 
</code></pre>

<p>However I get the error in the cluster two: <code>IndexError: list index out of range</code>. Any idea why?</p>
","5510540","","5483914","","2019-01-21 22:16:17","2019-01-21 22:16:17","computing the weight of LDA topic for all the documents in the corpus","<python><lda><gensim><corpus>","1","0","1","","","CC BY-SA 4.0"
"35398153","1","","","2016-02-14 21:23:16","","0","63","<p>Hello stack overflow community,</p>

<p>I have the following problem: I am currently mining a database of support tickets and would like to use e.g. Doc2Vec to check for similarities between tickets. However, the text contains huge strings, produced by the OS or compiler commands. So it would not be clever to use those strings as single words in the model. What is a good practice here? Does have anyone experience with something like that?</p>

<p>Best
Thorsten </p>
","3273188","","","","","2016-02-14 21:23:16","NLP on compiler output and other system error messages","<python><nlp><nltk><gensim>","0","2","","","","CC BY-SA 3.0"
"64805244","1","","","2020-11-12 13:54:12","","0","253","<p>I am working on a entity similarity project. The <code>most_similar</code> in word2vec gensim model works fine in this regard. However, I also want the search term itself to be included in the outcome. It should be something like this:</p>
<pre><code>&gt;&gt;&gt; model = Word2Vec(sw_token, min_count=2)
&gt;&gt;&gt; model = gensim.models.KeyedVectors.load(&quot;model.bin&quot;)
&gt;&gt;&gt; model.wv.most_similar(&quot;melanoma&quot;, topn=5)

[('melanoma', 1.000000),
 ('cutaneous', 0.6512814164161682),
 ('uveal', 0.6295092701911926),
 ('gp100', 0.617050290107727),
 ('ligand-bearing', 0.614188551902771)]
</code></pre>
<p>The official documents doesn't shows anything which can help me here. Also, if there are terms such as <code>melanoma xyz</code> how can we get such a word as closer in most_similar? I understand that it will take word into account so two words count as 2 not one therefore, they are not similar here. maybe. Thanks.</p>
","7890734","","","","","2020-11-23 19:07:59","Why most_similar in word2vec doesn't consider the term itself?","<python><nlp><gensim><word2vec><similarity>","3","0","","","","CC BY-SA 4.0"
"52762875","1","","","2018-10-11 14:42:14","","2","1153","<p>I had been trying to keep an output of topic modeling stable by using mallet as a library in gensim. However, I found out that mallet can set random-seed but I do not see any parameter in gensim to set it.  </p>
","9997755","","","","","2019-04-19 21:08:20","How can I set random-seed of topic model using mallet in gensim?","<python><gensim><topic-modeling><mallet>","2","0","","","","CC BY-SA 4.0"
"54623993","1","56730211","","2019-02-11 04:31:22","","5","787","<p>I am following the following gensim tutorial to transform my word2vec model to tensor.
Link to the tutorial: <a href=""https://radimrehurek.com/gensim/scripts/word2vec2tensor.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/scripts/word2vec2tensor.html</a></p>

<p>More specifically, I ran the following command</p>

<pre><code>python -m gensim.scripts.word2vec2tensor -i C:\Users\Emi\Desktop\word2vec\model_name -o C:\Users\Emi\Desktop\word2vec
</code></pre>

<p>However, I get the following error for the above command.</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>When I use <code>model.wv.save_word2vec_format(model_name)</code> to save my model (as mentioned in the following link: <a href=""https://github.com/RaRe-Technologies/gensim/issues/1847"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/1847</a>) and then use the above command I get the following error.</p>

<pre><code>ValueError: invalid vector on line 1 (is this really the text format?)
</code></pre>

<p>Just wondering if I have made any mistakes in the syntax of the commads. Please let me know how to resolve this issue.</p>

<p>I am happy to provide more details if needed.</p>
","10704050","","10704050","","2019-02-11 04:38:09","2020-06-30 20:57:28","How to use word2vec2tensor in gensim?","<python><gensim><word2vec>","2","2","","","","CC BY-SA 4.0"
"62315518","1","","","2020-06-11 01:09:41","","0","81","<p>I have a word array from a pickle file, and a corresponding vector array from an npy file, how do I combine them to make a Gensim W2V model?</p>
","13724435","","","","","2020-06-11 16:39:08","Combining a word array and a vector array to make a Gensim W2V model","<gensim><word2vec><embedding><word-embedding>","1","0","","","","CC BY-SA 4.0"
"63777101","1","63783510","","2020-09-07 11:52:42","","2","492","<p>Is there a way in python to map documents belonging to a certain topic. For example a list of documents that are primarily &quot;Topic 0&quot;. I know there are ways to list topics for each document but how do I do it the other way around?</p>
<p>Edit:</p>
<p>I am using the following script for LDA:</p>
<pre class=""lang-py prettyprint-override""><code>    doc_set = []
    for file in files:
        newpath = (os.path.join(my_path, file)) 
        newpath1 = textract.process(newpath)
        newpath2 = newpath1.decode(&quot;utf-8&quot;)
        doc_set.append(newpath2)

    texts = []
    for i in doc_set:
        raw = i.lower()
        tokens = tokenizer.tokenize(raw)
        stopped_tokens = [i for i in tokens if not i in stopwords.words()]
        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
        texts.append(stemmed_tokens)

    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, random_state=0, id2word = dictionary, passes=1)
</code></pre>
","4936133","","4936133","","2020-09-13 12:15:19","2020-09-13 19:11:54","Topic wise document distribution in Gensim LDA","<python><gensim><lda>","1","2","","","","CC BY-SA 4.0"
"61460683","1","61465687","","2020-04-27 14:03:20","","0","48","<p>I would like to train a <code>word2vec</code> model using what is an unordered list of keywords and categories for each document. Therefore my vocabulary is quite small around 2.5k tokens.</p>

<p>Would the performance be improved if at the training step, I used actual sentences from the document?</p>

<p>From example:</p>

<pre><code>doc_keywords = ['beach', 'holiday', 'warm']
doc_body = 'Going on a beach holiday it can be very warm'
</code></pre>

<p>If there is a benefit to using the full documents, could someone also explain why this is the case?</p>

<p>Since the model predicts the next word in a document, what would be the benefit to it learning <code>very -&gt; warm</code> as two words which often come together, given that <code>very</code> is not in my vocabulary.</p>
","4896449","","","","","2020-04-27 18:18:40","word2vec, using document body or keywords as training corpus","<machine-learning><nlp><gensim><word2vec><doc2vec>","2","0","","","","CC BY-SA 4.0"
"52840791","1","","","2018-10-16 17:16:54","","0","4486","<p>I have installed gensim for Windows by the command:</p>

<blockquote>
  <p>conda install -c anaconda gensim</p>
</blockquote>

<p>I have an environment py35. I'm trying to run a Python script using the import statement: <code>from gensim.models import Word2Vec</code> where there is an error <code>ImportError: No module named 'gensim'</code>. Any idea what to be done?</p>
","1138158","","","","","2018-10-17 11:23:40","ImportError: No module named 'gensim'","<python><gensim>","1","6","","","","CC BY-SA 4.0"
"44929582","1","","","2017-07-05 14:50:39","","0","2260","<p>I am using windows 7. I have installed python 2.7 and gensim using (pip install gensim). When I try to import gensim in a python console, I get the following error : </p>

<pre><code>C:\HOMEWARE\Anaconda\lib\site-packages\gensim\utils.py:860:¬†UserWarning:¬†detected¬†Windows;¬†aliasing¬†chunkize¬†to¬†chunkize_serial
¬†¬†warnings.warn(""detected¬†Windows;¬†aliasing¬†chunkize¬†to¬†chunkize_serial"")
Traceback¬†(most¬†recent¬†call¬†last):
¬†¬†File¬†""&lt;stdin&gt;"",¬†line¬†1,¬†in¬†&lt;module&gt;
¬†¬†File¬†""C:\HOMEWARE\Anaconda\lib\site-packages\gensim\__init__.py"",¬†line¬†6,¬†in¬†&lt;module&gt;
¬†¬†¬†¬†from¬†gensim¬†import¬†parsing,¬†matutils,¬†interfaces,¬†corpora,¬†models,¬†similarities,¬†summarization
¬†¬†File¬†""C:\HOMEWARE\Anaconda\lib\site-packages\gensim\matutils.py"",¬†line¬†21,¬†in¬†&lt;module&gt;
¬†¬†¬†¬†from¬†scipy.stats¬†import¬†entropy
¬†¬†File¬†""C:\HOMEWARE\Anaconda\lib\site-packages\scipy\stats\__init__.py"",¬†line¬†348,¬†in¬†&lt;module&gt;
¬†¬†¬†¬†from¬†.stats¬†import¬†*
¬†¬†File¬†""C:\HOMEWARE\Anaconda\lib\site-packages\scipy\stats\stats.py"",¬†line¬†175,¬†in¬†&lt;module&gt;
¬†¬†¬†¬†import¬†scipy.special¬†as¬†special
¬†¬†File¬†""C:\HOMEWARE\Anaconda\lib\site-packages\scipy\special\__init__.py"",¬†line¬†640,¬†in¬†&lt;module&gt;
¬†¬†¬†¬†from¬†._ufuncs¬†import¬†*
ImportError:¬†DLL¬†load¬†failed:¬†The¬†specified¬†module¬†could¬†not¬†be¬†found.
</code></pre>

<p>I have seen similar error on stackoverflow <a href=""https://stackoverflow.com/questions/20201868/importerror-dll-load-failed-the-specified-module-could-not-be-found"">here</a> and <a href=""https://stackoverflow.com/questions/8111664/boost-python-examples-windows-7-x64-importerror-dll-load-failed-the-specifi"">here</a> but it doesn't seem to do the trick for me. </p>

<p>Thank you for your help !</p>
","8259884","","","","","2017-10-07 14:54:53","Gensim : ImportError: DLL load failed: The specified module could not be found","<python><windows><installation><gensim>","1","0","","","","CC BY-SA 3.0"
"55710967","1","55716394","","2019-04-16 14:48:12","","1","142","<p>Lets say I am trying to compute the average distance between a word and a document using distances() or compute cosine similarity between two documents using n_similarity(). However, lets say these new documents contain words that the original model did not. How does gensim deal with that?</p>

<p>I have been reading through the documentation and cannot find what gensim does with unfound words.</p>

<p>I would prefer gensim to not count those in towards the average. So, in the case of distances(), it should simply not return anything or something I can easily delete later before I compute the mean using numpy. In the case of n_similarity, gensim of course has to do it by itself....</p>

<p>I am asking because the documents and words that my program will have to classify will in some instances contain unknown words, names, brands etc that I do not want to be taken into consideration during classification. So, I want to know if I'll have to preprocess every document that I am trying to classify. </p>
","7189173","","","","","2020-03-19 17:54:08","Dealing with new words in gensim not found in model","<python><nlp><gensim>","2","2","","","","CC BY-SA 4.0"
"36815038","1","38649708","","2016-04-23 18:52:33","","1","2503","<p>I am having a ready to go word2vec model that I already trained. I have serialized it as a CSV file:</p>

<pre><code>word,  v0,     v1,     ..., vN
house, 0.1234, 0.4567, ..., 0.3461
car,   0.456,  0.677,  ..., 0.3461
</code></pre>

<p>What I'd like to know is how I can load that word vector model in <code>gensim</code> and use that to train a paragraph or doc2vec model.</p>

<p>This <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">Doc2Vec tutorial</a> says I can load a model in form of a ""<code># C text format</code>"" but I have no idea what that actually means. What is ""C text format"" in the first place but more important: </p>

<ul>
<li>How can I load my word2vec model and use it for doc2vec training?</li>
</ul>

<p>How do I build the vocabulary from my word2vec model?</p>
","826983","","826983","","2016-04-23 19:54:15","2016-07-29 02:38:08","How to load pre-trained model with in gensim and train doc2vec with it?","<python><gensim><word2vec><doc2vec>","1","1","1","","","CC BY-SA 3.0"
"54289583","1","","","2019-01-21 12:03:10","","1","136","<p>I am trying to train a word2vec model using gensim. This is the line I am using:</p>

<pre><code>model = Word2Vec(training_texts, size=50, window=5, min_count=1, workers=4, max_vocab_size=20000)
</code></pre>

<p>Where training_texts is a list of lists of strings representing words. The corpora I am using has 8924372 sentences with 141,985,244 words and 1,531,477 unique words. After training, only 15642 words are present in the model:</p>

<pre><code>len(list(model.wv.vocab))
# returns 15642
</code></pre>

<p>Shouldn't the model have 20,000 words, as specified max_vocab_size? Why is it missing most of the training words?</p>

<p>Thanks!!</p>
","9652994","","9652994","","2019-01-21 16:35:51","2019-01-22 08:41:49","Missing words when training word2vec model","<nlp><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"40356631","1","40362385","","2016-11-01 08:44:08","","3","654","<p>I have a set of documents and I want to know the topic distribution for each document (for different values of number of topics). I have taken a toy program from <a href=""https://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda"">this question</a>.
I have first used LDA provided by gensim and then I am again giving test data as my training data itself to get the topic distribution of each doc in training data . But I am getting uniform topic distribution always.</p>

<p>Here is the toy code I used</p>

<pre><code>import gensim
import logging
logging.basicConfig(filename=""logfile"",format='%(message)s', level=logging.INFO)


def get_doc_topics(lda, bow):
    gamma, _ = lda.inference([bow])
    topic_dist = gamma[0] / sum(gamma[0])  # normalize distribution

documents = ['Human machine interface for lab abc computer applications',
             'A survey of user opinion of computer system response time',
             'The EPS user interface management system',
             'System and human system engineering testing of EPS',
             'Relation of user perceived response time to error measurement',
             'The generation of random binary unordered trees',
             'The intersection graph of paths in trees',
             'Graph minors IV Widths of trees and well quasi ordering',
             'Graph minors A survey']

texts = [[word for word in document.lower().split()] for document in documents]
dictionary = gensim.corpora.Dictionary(texts)
id2word = {}
for word in dictionary.token2id:    
    id2word[dictionary.token2id[word]] = word
mm = [dictionary.doc2bow(text) for text in texts]
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=2, update_every=1, chunksize=10000, passes=1,minimum_probability=0.0)

newdocs=[""human system""]
print lda[dictionary.doc2bow(newdocs)]

newdocs=[""Human machine interface for lab abc computer applications""] #same as 1st doc in training
print lda[dictionary.doc2bow(newdocs)]
</code></pre>

<p>Here is the output:</p>

<pre><code>[(0, 0.5), (1, 0.5)]
[(0, 0.5), (1, 0.5)]
</code></pre>

<p>I have checked with some more examples but all ended up giving the same equiprobable result.</p>

<p>Here is the logfile generated(i.e output of logger)</p>

<pre><code>adding document #0 to Dictionary(0 unique tokens: [])
built Dictionary(42 unique tokens: [u'and', u'minors', u'generation', u'testing', u'iv']...) from 9 documents (total 69 corpus positions)
using symmetric alpha at 0.5
using symmetric eta at 0.5
using serial LDA version on this node
running online LDA training, 2 topics, 1 passes over the supplied corpus of 9 documents, updating model once every 9 documents, evaluating perplexity every 9 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
-5.796 per-word bound, 55.6 perplexity estimate based on a held-out corpus of 9 documents with 69 words
PROGRESS: pass 0, at document #9/9
topic #0 (0.500): 0.057*""of"" + 0.043*""user"" + 0.041*""the"" + 0.040*""trees"" + 0.039*""interface"" + 0.036*""graph"" + 0.030*""system"" + 0.027*""time"" + 0.027*""response"" + 0.026*""eps""
topic #1 (0.500): 0.088*""of"" + 0.061*""system"" + 0.043*""survey"" + 0.040*""a"" + 0.036*""graph"" + 0.032*""trees"" + 0.032*""and"" + 0.032*""minors"" + 0.031*""the"" + 0.029*""computer""
topic diff=0.539396, rho=1.000000
</code></pre>

<p>It says ' too few updates, training might not converge' so I have tried increasing no of passes to 1000 but the output is still same.
(though it is not related to convergence , I have also tried increasing no of topics)</p>
","6354442","","-1","","2017-05-23 11:46:16","2017-01-10 11:31:12","gensim LDA module : Always getting uniform topical distribution while predicting","<python><lda><gensim>","1","0","1","","","CC BY-SA 3.0"
"54950481","1","54962988","","2019-03-01 18:39:02","","3","1217","<p>I have googled this issue but I cannot find any reliable solution (some sources gives log(V) some log(V/2). But what is the time complexity of the word2vec model with the following parameters:</p>

<p><code>Word2Vec(corpus, size=4000, window=30, min_count=1, workers=50, iter=100, alpha=0.0001)</code></p>

<p>I have a vocabulary that equals to 10000 words (unique words). </p>
","6799297","","202229","","2020-07-25 19:07:00","2020-07-25 20:30:20","Word2Vec time complexity","<python><time-complexity><big-o><gensim><word2vec>","1","0","3","","","CC BY-SA 4.0"
"6287411","1","6343197","","2011-06-09 02:20:59","","4","7266","<p>I'm using Python's gensim library to do latent semantic indexing.  I followed the tutorials on the website, and it works pretty well.  Now I'm trying to modify it a bit; I want to be run the lsi model each time a document is added.</p>

<p>Here is my code:</p>

<pre><code>stoplist = set('for a of the and to in'.split())
num_factors=3
corpus = []

for i in range(len(urls)):
 print ""Importing"", urls[i]
 doc = getwords(urls[i])
 cleandoc = [word for word in doc.lower().split() if word not in stoplist]
 if i == 0:
  dictionary = corpora.Dictionary([cleandoc])
 else:
  dictionary.addDocuments([cleandoc])
 newVec = dictionary.doc2bow(cleandoc)
 corpus.append(newVec)
 tfidf = models.TfidfModel(corpus)
 corpus_tfidf = tfidf[corpus]
 lsi = models.LsiModel(corpus_tfidf, numTopics=num_factors, id2word=dictionary)
 corpus_lsi = lsi[corpus_tfidf]
</code></pre>

<p>geturls is function I wrote that returns the contents of a website as a string.  Again, it works if I wait until I process all of the documents before doing tfidf and lsi, but that's not what I want.  I want to do it on each iteration.  Unfortunately, I get this error:</p>

<pre><code>    Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""streamlsa.py"", line 51, in &lt;module&gt;
    lsi = models.LsiModel(corpus_tfidf, numTopics=num_factors, id2word=dictionary)
  File ""/Library/Python/2.6/site-packages/gensim-0.7.8-py2.6.egg/gensim/models/lsimodel.py"", line 303, in __init__
    self.addDocuments(corpus)
  File ""/Library/Python/2.6/site-packages/gensim-0.7.8-py2.6.egg/gensim/models/lsimodel.py"", line 365, in addDocuments
    self.printTopics(5) # TODO see if printDebug works and remove one of these..
  File ""/Library/Python/2.6/site-packages/gensim-0.7.8-py2.6.egg/gensim/models/lsimodel.py"", line 441, in printTopics
    self.printTopic(i, topN = numWords)))
  File ""/Library/Python/2.6/site-packages/gensim-0.7.8-py2.6.egg/gensim/models/lsimodel.py"", line 433, in printTopic
    return ' + '.join(['%.3f*""%s""' % (1.0 * c[val] / norm, self.id2word[val]) for val in most])
  File ""/Library/Python/2.6/site-packages/gensim-0.7.8-py2.6.egg/gensim/corpora/dictionary.py"", line 52, in __getitem__
    return self.id2token[tokenid] # will throw for non-existent ids
KeyError: 1248
</code></pre>

<p>Usually the error pops up on the second document.  I think I understand what it's telling me (the dictionary indices are bad), I just can't figure out WHY.  I've tried lots of different things and nothing seems to work.  Does anyone know what's going on?</p>

<p>Thanks!</p>
","353278","","342473","","2012-03-20 11:28:28","2014-04-25 08:44:18","LSI using gensim in python","<python><latent-semantic-indexing><gensim>","3","2","1","","","CC BY-SA 3.0"
"19474333","1","19474377","","2013-10-20 05:46:31","","2","151","<p>I am new to Python and Gensim.  I am currently working through one of the tutorials on <code>gensim</code> (<a href=""http://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">http://radimrehurek.com/gensim/tut1.html</a>).  I have two question about this line of code:</p>

<pre><code># collect statistics about all tokens
&gt;&gt;&gt; dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))
</code></pre>

<p>1) Is the file <code>mycorpus.txt</code> fully loaded into memory before the Dictionary starts to get built?    The tutorial explicitly says no:</p>

<pre><code>Similarly, to construct the dictionary without loading all texts into memory
</code></pre>

<p>but when I monitor RAM usage in my Activity Monitor, the Python process hits 1 gig for a 3 gig file (I killed the process midway).  This is strange, as I assumed the dictionary for my 3 gig text file would be MUCH smaller.  Can someone clarify this point for me?</p>

<p>2) How can I recode this line so that I can do stuff between each line read?  I want to print to screen to see the progress.  Here is my attempt:</p>

<pre><code>i = 1

for line in f:
    if i % 1000 == 0:
        print i
    dictionary = corpora.Dictionary([line.lower().split()])
    i += 1
</code></pre>

<p>This doesn't work because dictionary is being reinitialized for every line.</p>

<p>I realize these are very n00b questions - appreciate your help and patience.</p>
","1583516","","","","","2013-10-20 05:55:13","Build Dictionary without Loading All Texts","<python><dictionary><gensim>","1","0","1","","","CC BY-SA 3.0"
"46086858","1","46086985","","2017-09-07 02:16:18","","0","740","<p>I am new in 'Word2Vec' in Gensim. I want to build a Word2Vec model for the text (Extracted from Wikipedia: Machine Learning) and find <strong>most similar words</strong> to 'Machine Learning'.</p>

<p>My current code is as follows.</p>

<pre><code># import modules &amp; set up logging
from gensim.models import Word2Vec

sentences = ""Machine learning is the subfield of computer science that, according to Arthur Samuel, gives computers the ability to learn without being explicitly programmed.[1][2][verify] Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term machine learning in 1959 while at IBM. Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] ‚Äì such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.""
# train word2vec on the sentences
model = Word2Vec(sentences, min_count=1)
vocab = list(model.wv.vocab.keys())
print(vocab[:10])
</code></pre>

<p>However, for vocab I get one character output.</p>

<pre><code>['M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'l', 'r']
</code></pre>

<p>Please help me to get the most_similar_words by using using <strong>model.most_similar</strong></p>
","","user8566323","","user8566323","2017-09-07 02:26:59","2017-09-07 02:34:23","Word2Vec in Gensim using model.most_similar","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"63564117","1","","","2020-08-24 15:31:32","","1","496","<p>I was thinking underlying dictionary are the same for both supervised (classification) and unsupervised(Word Embedding). I want to analyze the classification model (word vectors) that I built for supervised problem using Gensim. But I got following error. I know <strong>Gensim has not implemented Supervised learning</strong> part of Fastext and only focusing on Word Embedding. But I just want to load the dictionary to analyze. Any pointers?</p>
<pre><code>Traceback (most recent call last):
  File &quot;fasttext_model_analysis.py&quot;, line 2, in &lt;module&gt;
    model = FastText.load_fasttext_format('model_ups_tickets_rca.bin')
  File &quot;/usr/local/lib/python3.5/dist-packages/gensim/models/deprecated/fasttext_wrapper.py&quot;, line 274, in load_fasttext_format
    model.load_binary_data(encoding=encoding)
  File &quot;/usr/local/lib/python3.5/dist-packages/gensim/models/deprecated/fasttext_wrapper.py&quot;, line 301, in load_binary_data
    self.load_dict(f, encoding=encoding)
  File &quot;/usr/local/lib/python3.5/dist-packages/gensim/models/deprecated/fasttext_wrapper.py&quot;, line 332, in load_dict
    raise NotImplementedError(&quot;Supervised fastText models are not supported&quot;)
NotImplementedError: Supervised fastText models are not supported
</code></pre>
","3708401","","","","","2020-08-25 15:57:13","Why does Gensim reject to load supervised model dict built by Fasttext (Facebook) library?","<gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"48559764","1","","","2018-02-01 09:52:09","","1","515","<p>As i say in the title i would like to load pre-tranined model </p>

<p>using gensim is possibile for example but with fasttext say:</p>

<p><a href=""https://radimrehurek.com/gensim/models/wrappers/fasttext.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/wrappers/fasttext.html</a></p>

<pre><code> ""Note that due to limitations in the FastText API, you cannot continue training with a model loaded this way, though you can query for word similarity etc.""
</code></pre>

<p>With Word2vec say it is possibile continue the traning of your own model not a pretranind end i do not know with Glove.</p>

<p>Can you point me to any library or something for load a pre-tranined model and continue the traning on my own sentences ?</p>

<p>Or in case i can load the pretranined model into a neural network and after continue the traning with my own vectors ?  (maybe using get_keras_embedding ? )</p>
","448381","","","","","2018-02-01 09:52:09","NLU FastText, Glove or Word2Vec Load Pre-trained model and Add new word to vocabulary","<neural-network><nlp><word2vec><gensim><fasttext>","0","0","","","","CC BY-SA 3.0"
"48562396","1","","","2018-02-01 12:06:32","","5","3814","<p>I am new in NLP. I am trying to extract the summary of the paragraphs using Gensim in python. </p>

<p>I am facing a problem with a short paragraph, it is giving me a warning as given below and doesn't give me a summary of the short paragraph.</p>

<p>Here is my code in Python:</p>

<pre><code> import logging
 logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
 from gensim.summarization import summarize

text = ""short paragraph""
print ('Summary:')
print (summarize(text))
</code></pre>

<p>It is giving me warning as follows:</p>

<pre><code>2018-02-01 17:31:47,247 : WARNING : Input text is expected to have at least 10 sentences.
2018-02-01 17:31:47,253 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2018-02-01 17:31:47,258 : INFO : built Dictionary(52 unique tokens: ['clearli', 'adult', 'chang', 'member', 'visit']...) from 4 documents (total 70 corpus positions)
2018-02-01 17:31:47,262 : WARNING : Input corpus is expected to have at least 10 documents.
2018-02-01 17:31:47,285 : WARNING : Couldn't get relevant sentences.
</code></pre>

<p>The output is(Printing only summary label not the actual summary of the short paragraph):</p>

<pre><code>Summary:
</code></pre>

<p>Am I missing something? Is there any other library for the same.</p>
","7741625","","","","","2018-02-01 13:18:46","Text Summarization with Gensim with short paragraph","<python><python-3.x><gensim>","1","0","1","","","CC BY-SA 3.0"
"45783781","1","","","2017-08-20 15:25:44","","0","286","<p>I am trying to save gensim Doc2vec model. The model is trained on 9M document vectors and vocabulary of around 1M words. But I am getting pickel error. ""top"" shows that the program uses around 13GB of RAM. Also I think since I need to re-train the model for new documents as and when required,  saving all parameters is necessary.</p>

<pre><code>Traceback (most recent call last):
 File ""doc_2_vec.py"", line 61, in &lt;module&gt;

model.save(""/data/model_wl_videos/model"",pickle_protocol=2)
 File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 1406, in save
super(Word2Vec, self).save(*args, **kwargs)
 File ""/home/meghana.negi/.local/lib/python2.7/site-packages/gensim/utils.py"", line 504, in save
pickle_protocol=pickle_protocol)
 File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/utils.py"", line 376, in _smart_save
pickle(self, fname, protocol=pickle_protocol)
 File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/utils.py"", line 930, in pickle
_pickle.dump(obj, fout, protocol=protocol)
</code></pre>

<p>MemoryError</p>
","4883595","","","","","2017-08-20 15:25:44","Pickel Error while storing Doc2vec gensim model","<nlp><pickle><gensim><doc2vec>","0","5","","","","CC BY-SA 3.0"
"63147796","1","63152639","","2020-07-29 06:44:37","","0","415","<p>I am using LDA for Topic Modelling in Python.Gensim implementation of LDA allows us to set alpha as 'auto' as below:</p>
<pre><code>alpha ({numpy.ndarray, str}, optional) ‚Äì
    
            ‚Äôasymmetric‚Äô: Uses a fixed normalized asymmetric prior of 1.0 / topicno.
    
            ‚Äôauto‚Äô: Learns an asymmetric prior from the corpus (not available if distributed==True).
</code></pre>
<p>For LDA Mallet wrapper provided in Gensim there is no option of setting alpha as auto.</p>
<p>Is there way to learn alpha from the corpus in LDA Mallet?</p>
","4712585","","","","","2020-07-29 11:31:32","LDA Gensim Mallet setting alpha as 'auto'","<python><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"19652908","1","","","2013-10-29 08:15:26","","1","1355","<p>I have been using LsiModel in gensim for modelling topics from a corpus of 10000 mails. I am able to get the words and word scores for each topic  and store them in a file. I have tried using <strong>print_topics()</strong> and <strong>show_topics()</strong> but both return only the words &amp; score associated with those words. But I also need the topic score that it outputs to the log file, I want those values in a variable.  Like for this example log output below: </p>

<pre><code>2010-11-03 16:08:27,602 : INFO : topic #0(200.990): -0.475*""delete"" + -0.383*""deletion"" + -0.275*""debate"" + -0.223*""comments"" + -0.220*""edits"" + -0.213*""modify"" + -0.208*""appropriate"" + -0.194*""subsequent"" + -0.155*""wp"" + -0.117*""notability""
2010-11-03 16:08:27,626 : INFO : topic #1(143.129): -0.320*""diff"" + -0.305*""link"" + -0.199*""image"" + -0.171*""www"" + -0.162*""user"" + 0.149*""delete"" + -0.147*""undo"" + -0.144*""contribs"" + -0.122*""album"" + 0.113*""deletion""
2010-11-03 16:08:27,651 : INFO : topic #2(135.665): -0.437*""diff"" + -0.400*""link"" + -0.202*""undo"" + -0.192*""user"" + -0.182*""www"" + -0.176*""contribs"" + 0.168*""image"" + -0.109*""added"" + 0.106*""album"" + 0.097*""copyright""
2010-11-03 16:08:27,677 : INFO : topic #3(125.027): -0.354*""image"" + 0.239*""age"" + 0.218*""median"" + -0.213*""copyright"" + 0.204*""population"" + -0.195*""fair"" + 0.195*""income"" + 0.167*""census"" + 0.165*""km"" + 0.162*""households""
2010-11-03 16:08:27,701 : INFO : topic #4(116.927): -0.307*""image"" + 0.195*""players"" + 0.184*""median"" + -0.184*""copyright"" + -0.181*""age"" + -0.167*""fair"" + -0.162*""income"" + -0.151*""population"" + -0.136*""households"" + -0.134*""census""
</code></pre>

<p>I need these score in a variable.   </p>

<pre><code>topic #0 : 200.990 
topic #1 : 143.129
topic #2 : 135.665
topic #3 : 125.027
topic #4 : 116.927
</code></pre>

<p>Is there any method in the package to get these outputs? Please help. </p>
","807916","","","","","2013-12-09 19:40:18","How to obtain the topic score in LSI model of Gensim?","<python><gensim><latent-semantic-indexing>","1","0","","","","CC BY-SA 3.0"
"54422810","1","54423541","","2019-01-29 14:03:15","","4","2480","<p>I'm pretty new to Gensim and I'm trying to train my first model using word2vec model. I see that all the parameters are pretty straightforward and easy to understand, however I don't know how to track the loss of the model to see the progress. Also, I would like to be able to get the embeddings after each epoch so that I can also <em>show</em> that the predictions also get more <em>logical</em> with after each epoch. How can I do that?</p>

<p>OR, is it better to train for <em>iter=1</em> each time and save the loss and embeddings after each epoch? Sounds not too efficient.</p>

<p>Not much to show with the code but still posting it below:</p>

<pre><code>model = Word2Vec(sentences = trainset, 
             iter = 5, # epoch
             min_count = 10, 
             size = 150, 
             workers = 4, 
             sg = 1, 
             hs = 1, 
             negative = 0, 
             window = 9999)
</code></pre>
","10868996","","","","","2019-01-29 14:43:54","Tracking loss and embeddings in Gensim word2vec model","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"44011706","1","44013893","","2017-05-16 21:15:43","","7","4663","<p>I read this <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""noreferrer"">page</a> but I do not understand what is different between models which are built based on the following codes.
I know when dbow_words is 0, training of doc-vectors is faster.</p>

<p>First model</p>

<pre><code>model = doc2vec.Doc2Vec(documents1, size = 100, window = 300, min_count = 10, workers=4)
</code></pre>

<p>Second model</p>

<pre><code>model = doc2vec.Doc2Vec(documents1, size = 100, window = 300, min_count = 10, workers=4,dbow_words=1)
</code></pre>
","3092781","","","","","2017-05-17 01:07:38","What is different between doc2vec models when the dbow_words is set to 1 or 0?","<gensim><doc2vec>","1","0","2","","","CC BY-SA 3.0"
"52842474","1","52844360","","2018-10-16 19:11:06","","0","342","<p>I am trying to build doc2vec model, using gensim + sklearn to perform sentiment analysis on short sentences, like comments, tweets, reviews etc.</p>

<p>I downloaded <a href=""http://jmcauley.ucsd.edu/data/amazon/"" rel=""nofollow noreferrer"">amazon product review data set</a>, <a href=""https://www.kaggle.com/c/twitter-sentiment-analysis2"" rel=""nofollow noreferrer"">twitter sentiment analysis data set</a> and <a href=""https://www.kaggle.com/utathya/imdb-review-dataset"" rel=""nofollow noreferrer"">imbd movie review data set</a>.</p>

<p>Then combined these in 3 categories, positive, negative and neutral.</p>

<p>Next I trinaed gensim doc2vec model on the above data so I can obtain the input vectors for the classifying neural net.</p>

<p>And used sklearn LinearReggression model to predict on my test data, which is about 10% from each of the above three data sets.</p>

<p>Unfortunately the results were not good as I expected. Most of the tutorials out there seem to focus only on one specific task, 'classify amazon reviews only' or 'twitter sentiments only', I couldn't manage to find anything that is more general purpose.</p>

<p>Can some one share his/her thought on this? </p>
","5625696","","","","","2018-10-16 21:35:58","Data set for Doc2Vec general sentiment analysis","<dataset><artificial-intelligence><gensim><sentiment-analysis><doc2vec>","1","1","","","","CC BY-SA 4.0"
"54537417","1","","","2019-02-05 15:09:35","","0","37","<p>I am trying to rewrite algorithm that basically takes a input text file and compares with different documents and results the similarities.</p>

<p>Now I want to print output of unmatched words and output a new textile with unmatched words. </p>

<p>From this code, ""hello force"" is the input and is checked against the raw_documents and prints out rank for matched document between 0-1(word ""force"" is matched with second document and ouput gives more rank to second document but ""hello"" is not in any raw_document i want to print unmatched word ""hello"" as not matched ), But what i want is to print unmatched input word that was not matched with any of the raw_document </p>

<pre><code>import gensim
import nltk

from nltk.tokenize import word_tokenize

raw_documents = [""I'm taking the show on the road"",
                 ""My socks are a force multiplier."",
             ""I am the barber who cuts everyone's hair who doesn't cut their own."",
             ""Legend has it that the mind is a mad monkey."",
            ""I make my own fun.""]

gen_docs = [[w.lower() for w in word_tokenize(text)]
            for text in raw_documents]

dictionary = gensim.corpora.Dictionary(gen_docs)

corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]

tf_idf = gensim.models.TfidfModel(corpus)
s = 0
for i in corpus:
    s += len(i)
sims = gensim.similarities.Similarity('/usr/workdir/',tf_idf[corpus],
                                      num_features=len(dictionary))
query_doc = [w.lower() for w in word_tokenize(""hello force"")]

query_doc_bow = dictionary.doc2bow(query_doc)

query_doc_tf_idf = tf_idf[query_doc_bow]
result = sims[query_doc_tf_idf]
print result
</code></pre>
","10037769","","10037769","","2019-02-06 08:35:32","2019-02-06 08:35:32","How can I find and print unmatched/dissimilar words from the documents(dataset)?","<python><dictionary><nltk><gensim><nltk-trainer>","0","5","","","","CC BY-SA 4.0"
"35502363","1","","","2016-02-19 09:57:58","","1","623","<p>I am new to the LDA and I have three questions. I would like to classify my text (tags) with the LDA. First I filter the words, which have been used only by one user, machine tags, tags containing only digits and tags with the frequency less than 3.
Then, I calculate the amount of topics with the Elbow method and there I get the memory error (this will be the third question). So the amount of topics suggested by the Elbow method is 8 (I have filtered some more tags to overcome the memory issue but I would need to apply it to bigger datasets in the future).</p>

<ol>
<li><p>Should I use tf-idf as a preprocessing step for the LDA? Or if I filter the ""useless"" tags before it doesn't make sense? I think I don't understand what is going on exactly in the LDA.</p>

<pre><code>dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
lda = ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, alpha = 0.1, num_topics=8)
corpus_lda = lda[corpus_tfidf]
</code></pre></li>
<li><p>Does it make sense to validate the topics quality with the LSI? As I understand the LSI is a method for dimensionality reduction, so I use it to apply K-Means and to see if the 8 clusters of the topics actually look like clusters. But to be honest I don't really understand what exactly I am visualising.</p>

<pre><code>lsi = models.LsiModel(corpus_lda, id2word=dictionary, num_topics=2)
lsi_coord = ""file.csv""
fcoords = codecs.open(lsi_coord,'w','utf-8')
for vector in lsi[corpus_lda]:
    if len(vector) != 2:
    continue
    fcoords.writelines(""%6.12f\t%6.12f\n"" % (vector[0][1],vector[1][1]))
fcoords.close()
num_topics = 8
X = np.loadtxt(lsi_coord, delimiter=""\t"")
my_kmeans = KMeans(num_topics).fit(X)
k_means_labels = my_kmeans.labels_
k_means_cluster_centers = my_kmeans.cluster_centers_
colors = ['b','g','r','c','m','y','k','greenyellow']
for k, col in zip(range(num_topics), colors):
my_members = k_means_labels == k
plt.scatter(X[my_members, 0], X[my_members, 1], s=30, c=colors[k], zorder=10)
cluster_center = k_means_cluster_centers[k]
plt.scatter(cluster_center[0], cluster_center[1], marker='x', s=30, linewidths=3, color='r', zorder=10)
plt.title('K-means clustering')
plt.show()
</code></pre></li>
<li><p>Memory issues. I am trying to create a matrix which has values for every unique term. So if the term is not in the document it gets zero. So it is a sparse matrix, because I have around 1300 unique terms and every document has about 5. And the memory issue arise at the converting to np.array. I guess I have to optimize the matrix somehow.</p>

<pre><code> # creating term-by-document matrix
Y = []
for z in corpus_lda:
    Y1=[]
    temp_dict={}
    for g in z:
        temp_dict.update({g[0]:g[1]})
     counter=0
     while counter &lt; len(dictionary.keys()):
        if counter in temp_dict.keys():
            Y1.append(temp_dict[counter])
        else:
            Y1.append(0)
        counter+=1
    Y.append(Y1)
Y = np.array(Y)
</code></pre></li>
</ol>

<p>The following code I took from here : <a href=""https://stackoverflow.com/questions/6645895/calculating-the-percentage-of-variance-measure-for-k-means"">Calculating the percentage of variance measure for k-means?</a></p>

<pre><code>    K = range(1,30) # amount of clusters 
    KM = [kmeans(Y,k) for k in K] 
    KM = []
    for k in K:
        KM_result = kmeans(Y,k)
        KM.append(KM_result)

    centroids = [cent for (cent,var) in KM]

    scipy.spatial.distance import cdist
    D_k = [cdist(Y, cent, 'euclidean') for cent in centroids] 
    cIdx = [np.argmin(D,axis=1) for D in D_k]
    dist = [np.min(D,axis=1) for D in D_k]
    avgWithinSS = [sum(d)/Y.shape[0] for d in dist]  
    kIdx = 8

    # elbow curve
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(K, avgWithinSS, 'b*-')
    ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=12, markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')
    plt.grid(True)
    plt.xlabel('Number of clusters')
    plt.ylabel('Average within-cluster sum of squares')
    plt.title('Elbow for KMeans clustering')
</code></pre>

<p>Any ideas for any of the questions are highly appreciated!</p>
","5672618","","-1","","2017-05-23 10:29:31","2016-02-19 09:57:58","is it necessary/appropriate to calculate tf-idf as a preprocessing for LDA (Gensim)?","<python><tf-idf><lda><gensim>","0","0","1","","","CC BY-SA 3.0"
"9470479","1","","","2012-02-27 18:48:16","","4","5727","<p>From the documents which i found out from the net i figured out the expression used to determine the Term Frequency and Inverse Document frequency weights of terms in a corpus to be</p>

<p>tf-idf(wt)= tf * log(|N|/d);</p>

<p>I was going through the implementation of tf-idf mentioned in gensim.
The example given in the documentation is</p>

<pre><code>&gt;&gt;&gt; doc_bow = [(0, 1), (1, 1)]
&gt;&gt;&gt; print tfidf[doc_bow] # step 2 -- use the model to transform vectors
[(0, 0.70710678), (1, 0.70710678)] 
</code></pre>

<p>Which apparently does not follow the standard implementation of Tf-IDF.
What is the difference between both the models?</p>

<p>Note: 0.70710678 is the value 2^(-1/2) which is used usually in eigen value calculation.
So how does eigen value come into the TF-IDF model?</p>
","885386","","342473","","2012-03-20 11:28:36","2015-06-13 18:27:33","How is TF-IDF implemented in gensim tool in python?","<python><tf-idf><latent-semantic-indexing><gensim>","2","0","1","","","CC BY-SA 3.0"
"15067734","1","15069580","","2013-02-25 13:08:28","","18","15702","<p>I am using python <code>gensim</code> to train an Latent Dirichlet Allocation (LDA) model from a small corpus of 231 sentences. However, each time i repeat the process, it generates different topics. </p>

<p><strong>Why does the same LDA parameters and corpus generate different topics everytime?</strong></p>

<p><strong>And how do i stabilize the topic generation?</strong></p>

<p>I'm using this corpus (<a href=""http://pastebin.com/WptkKVF0"">http://pastebin.com/WptkKVF0</a>) and this list of stopwords (<a href=""http://pastebin.com/LL7dqLcj"">http://pastebin.com/LL7dqLcj</a>) and here's my code:</p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models import hdpmodel, ldamodel
from itertools import izip
from collections import defaultdict
import codecs, os, glob, math

stopwords = [i.strip() for i in codecs.open('stopmild','r','utf8').readlines() if i[0] != ""#"" and i != """"]

def generateTopics(corpus, dictionary):
    # Build LDA model using the above corpus
    lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)
    corpus_lda = lda[corpus]

    # Group topics with similar words together.
    tops = set(lda.show_topics(50))
    top_clusters = []
    for l in tops:
        top = []
        for t in l.split("" + ""):
            top.append((t.split(""*"")[0], t.split(""*"")[1]))
        top_clusters.append(top)

    # Generate word only topics
    top_wordonly = []
    for i in top_clusters:
        top_wordonly.append("":"".join([j[1] for j in i]))

    return lda, corpus_lda, top_clusters, top_wordonly

####################################################################### 

# Read textfile, build dictionary and bag-of-words corpus
documents = []
for line in codecs.open(""./europarl-mini2/map/coach.en-es.all"",""r"",""utf8""):
    lemma = line.split(""\t"")[3]
    documents.append(lemma)
texts = [[word for word in document.lower().split() if word not in stopwords]
             for document in documents]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda, corpus_lda, topic_clusters, topic_wordonly = generateTopics(corpus, dictionary)

for i in topic_wordonly:
    print i
</code></pre>
","610569","","","","","2020-03-12 20:09:37","LDA model generates different topics everytime i train on the same corpus","<python><nlp><lda><topic-modeling><gensim>","4","0","8","","","CC BY-SA 3.0"
"31524898","1","","","2015-07-20 19:35:07","","13","5760","<p>I wanted to know the difference between gensim word2vec's two similarity measures  : most_similar() and most_similar_cosmul(). I know that the first one works using cosine similarity of word vectors while other one uses using the multiplicative combination objective proposed by Omer Levy and Yoav Goldberg. I want to know how it affects the results? Which one gives semantic similarity ? etc.
Eg :</p>

<pre><code>model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
model.most_similar(positive=['woman', 'king'], negative=['man'])               
</code></pre>

<p>Result : [('queen', 0.50882536), ...]</p>

<pre><code>model.most_similar_cosmul(positive=['baghdad', 'england'], negative=['london'])
</code></pre>

<p>Result : [(u'iraq', 0.8488819003105164), ...]</p>
","4663523","","","","","2015-07-30 10:11:30","Gensim Word2vec : Semantic Similarity","<python><semantics><similarity><gensim><word2vec>","1","0","5","","","CC BY-SA 3.0"
"64494914","1","","","2020-10-23 06:25:31","","0","33","<p>I am following the code from <a href=""https://medium.com/better-programming/introduction-to-gensim-calculating-text-similarity-9e8b55de342d"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I have a csv file of 8000 questions and answers and I have made an LSI model with 1000 topics, from a tfidf corpus, using gensim as follows. I only consider questions as part of the text not the answers.</p>
<pre><code>texts = [jieba.lcut(text) for text in document]
# tk = WhitespaceTokenizer() 
# texts = [tk.tokenize(text) for text in document]
dictionary = corpora.Dictionary(texts)
feature_cnt = len(dictionary.token2id)
corpus = [dictionary.doc2bow(text) for text in texts]
tfidf = models.TfidfModel(corpus)
tfidf_corpus = tfidf[corpus]
lsi_model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=1000)
corpus_lsi = lsi_model[tfidf_corpus]
index = similarities.SparseMatrixSimilarity(corpus_lsi, num_features = feature_cnt)
</code></pre>
<p>Before this I am also preprocessing the data by removing stopwords using nltk and replacing punctuations using regex and lemmatizing, using wordnet and nltk.</p>
<p>I understand that jieba is not a tokenizer suited for english because it tokenizes spaces as well like this:</p>
<pre><code>Sample: This is untokenized text
Tokenized: 'This',' ','is',' ','untokenized', ' ', 'text'
</code></pre>
<p>When I switch from jieba to nltk whitespace tokenizer, strange thing happens, my accuracy suddenly drops that is when I a new sentence using the following code I get worse results</p>
<pre><code>keyword = &quot;New sentence the similarity of which is to be found to the main corpus&quot;
kw_vector = dictionary.doc2bow(jieba.lcut(keyword)) # jieba.lcut can be replaced by tk.tokenize()
sim = index[lsi_model[tfidf[kw_vector]]]                         
x = [sim[i] for i in np.argsort(sim)[-2:]]
</code></pre>
<p>My understanding is that extra and useless words and characters like whitespaces should decrease accuracy but here I observe an opposite effect. What could be the possible reasons?</p>
<p>One possible explanation I came up with is that most of the questions are short only 5 to 6 words like</p>
<ol>
<li>What is the office address?</li>
<li>Who to contact for X?</li>
<li>Where to find document Y?</li>
</ol>
","12056943","","","","","2020-10-23 06:25:31","Inefficient tokenization leading to better results","<nlp><tokenize><gensim>","0","4","","","","CC BY-SA 4.0"
"39944487","1","","","2016-10-09 14:07:01","","0","1635","<p>This is the structure I'm dealing with:</p>

<pre><code>src/
    processing/
        station_level/
            train_paragraph_vectors.py
    doc2vec_ext.py
    word_embeddings_station_level.py
</code></pre>

<p>I have trained and stored a model in <code>word_embeddings_station_level.py</code> like this:</p>

<pre><code>from src.doc2vec_ext import WeightedDoc2Vec

# ...

model = WeightedDoc2Vec(
    # ...
)

train(model, vocab, station_sentences, num_epochs)

# Saving the model -&gt; pickles it
model.save(open(model_file, ""w""))
</code></pre>

<p>This is working fine so far. However, I want to load that model in <code>train_paragraph_vectors.py</code> like this:</p>

<pre><code>import sys
from src import doc2vec_ext
sys.modules[""doc2vec_ext""] = doc2vec_ext

if __name__ == ""__main__"":
# ...
    model = doc2vec_ext.WeightedDoc2Vec.load(station_level_sentence_vectors)
</code></pre>

<p>but I'm getting:</p>

<pre><code>Traceback (most recent call last):
  File ""E:/python/kaggle/seizure_prediction/src/processing/station_level/train_paragraph_vectors.py"", line 57, in &lt;module&gt;
    model = doc2vec_ext.WeightedDoc2Vec.load(station_level_sentence_vectors)
  File ""C:\Python27\lib\site-packages\gensim\models\word2vec.py"", line 1684, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 248, in load
    obj = unpickle(fname)
  File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 911, in unpickle
    return _pickle.loads(f.read())
ImportError: No module named doc2vec_ext
</code></pre>

<p><strong>doc2vec_ext.py</strong></p>

<p>Here you can see, that I just inherit from the <code>gensim.models.Doc2Vec</code> class and do some stuff:</p>

<pre><code>class WeightedDoc2Vec(Doc2Vec):

    def __init__(self, dm=1,window=5, f_size=0, size=100, min_count=1, negative=0, dbow_words=1, alpha=0.015, workers=8, seed=42, dm_weighted=False, dm_stacked=False):
        Doc2Vec.__init__(self,
            # Constructor arguments ..
            )

        # ...
</code></pre>

<p>I don't know what's the problem here. I've tried to do the <code>sys.modules[]</code> but it's still not working properly.</p>

<p>How can I load my stored model?</p>

<hr>

<p><strong>Important:</strong></p>

<p>I noticed that I can't even load from the same module. If I try to load the model in the file where it was created (here <code>word_embeddings_station_level.py</code>) it's still not working giving me the same error.</p>
","826983","","826983","","2016-10-09 14:36:22","2016-10-09 18:17:26","Pickle load: ImportError: No module named doc2vec_ext","<python><pickle><gensim>","1","4","","","","CC BY-SA 3.0"
"38098824","1","","","2016-06-29 11:56:19","","1","669","<p>I am taking different documents from a database and I check with LDA (gensim), what kind of latent topics are there in these documents. This works pretty well. What I would like to do is to save in the database for every document what is its most probable topic. And I am not sure what is the best solution for it. I could, for example, at the beginning extract a unique id of every document from the database together with the text_column and somehow process it that I know at the end which id belongs to which topic number. Or may be I should do it in the last part, where I print the documents and their topics. But I don't know how to connect it back to the database. By the comparison of the text_column with the document and assigning the corresponding topic number? Would be grateful for any comment.</p>

<pre><code>stop = stopwords.words('english')

sql = """"""SELECT text_column FROM table where NULLIF(text_column, '') IS NOT NULL;""""""
cur.execute(sql)
dbrows = cur.fetchall()
conn.commit()

documents = []
    for i in dbrows:
    documents = documents + list(i)

# remove all the words from the stoplist and tokenize
stoplist = stopwords.words('english')

additional_list = set(""``;''"".split("";""))

texts = [[word.lower() for word in document.split() if word.lower() not                 in stoplist and word not in string.punctuation and word.lower() not in additional_list] 
     for document in documents]

# remove words that appear less or equal of 2 times
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) &lt;= 2)
texts = [[word for word in text if word not in tokens_once]
     for text in texts]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
my_num_topics = 10

# lda itself
lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=my_num_topics)
corpus_lda = lda[corpus]

# print the most contributing words for selected topics
for top in lda.show_topics(my_num_topics):
    print top

# print the most probable topic and the document
for l,t in izip(corpus_lda,documents):
    selected_topic = max(l,key=lambda item:item[1])
    if selected_topic[1] != 1/my_num_topics:
        selected_topic_number = selected_topic[0]
        print selected_topic
        print t
</code></pre>
","5672618","","","","","2016-07-04 09:43:16","LDA gensim. How to update a Postgres database with the correct topic number for every document?","<python><postgresql><lda><gensim>","1","3","1","","","CC BY-SA 3.0"
"49262453","1","","","2018-03-13 17:35:03","","1","31","<p>Is it possible to leverage the pretrained model e.g. GLOVE and use it to further train a corpus. </p>

<p>Any example will be very helpful.</p>
","7984336","","","","","2018-03-13 17:35:03","Use pretrained models to further train current corpus","<nlp><word2vec><gensim>","0","0","","","","CC BY-SA 3.0"
"31543542","1","35966104","","2015-07-21 15:34:47","","14","11064","<p>I am using the Gensim HDP module on a set of documents. </p>

<pre><code>&gt;&gt;&gt; hdp = models.HdpModel(corpusB, id2word=dictionaryB)
&gt;&gt;&gt; topics = hdp.print_topics(topics=-1, topn=20)
&gt;&gt;&gt; len(topics)
150
&gt;&gt;&gt; hdp = models.HdpModel(corpusA, id2word=dictionaryA)
&gt;&gt;&gt; topics = hdp.print_topics(topics=-1, topn=20)
&gt;&gt;&gt; len(topics)
150
&gt;&gt;&gt; len(corpusA)
1113
&gt;&gt;&gt; len(corpusB)
17
</code></pre>

<p>Why is the number of topics independent of corpus length?</p>
","2795733","","","","","2020-10-17 18:56:09","Hierarchical Dirichlet Process Gensim topic number independent of corpus size","<python><nlp><lda><gensim>","7","0","6","","","CC BY-SA 3.0"
"54644228","1","","","2019-02-12 06:43:41","","0","504","<p>How do you use the Gensim predict output word function?</p>

<pre><code>model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)

model.predict_output_word(['Hi', 'how', 'you'], topn=10)

AttributeError: 'Word2VecKeyedVectors' object has no attribute 'predict_output_word'
</code></pre>

<p>I tried Word2Vec.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True), which was deprecated as well.</p>
","4984897","","","","","2019-02-12 07:51:24","Gensim Predict Output Word Function Syntax","<gensim>","1","0","","","","CC BY-SA 4.0"
"38005590","1","","","2016-06-24 04:26:32","","5","7415","<p>I have installed Anacoda Python v2.7 and Gensim v 0.13.0</p>

<p>I am using Spyder as IDE</p>

<p>I have the following simple code:</p>

<pre><code> from gensim import corpora
</code></pre>

<hr>

<p>I got the following error:</p>

<pre><code>from gensim import corpora

  File ""gensim.py"", line 7, in &lt;module&gt;

ImportError: cannot import name corpora
</code></pre>

<p>I reinstalled:
- Gensim
- Scipy
- Numpy
but still have the same issue.</p>
","3055644","","","","","2017-11-07 07:29:49","ImportError: cannot import name corpora with Gensim","<python-2.7><nltk><lda><gensim>","3","2","","","","CC BY-SA 3.0"
"48590383","1","","","2018-02-02 20:25:44","","1","1211","<p>I would like to apply ""graph2vec"" code to a my own dataset. However I can not figure out how to properly format the input data nor understand the input data format of examples available on ""github"" page of the authors. A network in my dataset has integer nodes and binary label, so it is a dataframe with three columns. I appreciate if anyone can point me to the right direction.</p>

<p>""graph2vec"" on github:   <a href=""https://github.com/MLDroid/graph2vec_tf"" rel=""nofollow noreferrer"">https://github.com/MLDroid/graph2vec_tf</a></p>

<p>""graph2vec"" on arxiv:   <a href=""https://arxiv.org/pdf/1707.05005.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1707.05005.pdf</a> </p>
","6784445","","6784445","","2018-02-02 20:39:09","2018-05-16 10:18:05","""graph2vec"" input data format","<python-3.x><tensorflow><graph><word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"47173538","1","47213557","","2017-11-08 07:07:15","","17","3963","<p>When I try to create a word2vec model (skipgram with negative sampling) I received 3 files as output as follows.</p>

<pre><code>word2vec (File)
word2vec.syn1nef.npy (NPY file)
word2vec.wv.syn0.npy (NPY file)
</code></pre>

<p>I am just worried why this happens as for my previous test examples in word2vec I only received one model(no npy files).</p>

<p>Please help me.</p>
","","user8871463","130288","","2017-11-13 16:24:46","2017-11-13 16:24:46","Why are multiple model files created in gensim word2vec?","<python><word2vec><gensim><word-embedding>","1","0","1","","","CC BY-SA 3.0"
"38054356","1","","","2016-06-27 12:45:42","","6","521","<p>Using <code>gensim</code>, I want to calculate the similarity within a list of documents. This library is excellent at handling the amounts of data that I have got. The documents are all reduced to timestamps and I have got a function <code>time_similarity</code> to compare them. <code>gensim</code> however, uses the cosine similarity.</p>

<p>I am wondering if anyone has attemted this before or has a different solution.  </p>
","470433","","","","","2016-07-06 00:14:37","gensim: custom similarity measure","<python><time><similarity><gensim>","1","0","","","","CC BY-SA 3.0"
"29259416","1","","","2015-03-25 15:08:27","","1","401","<p>I'm trying to replicate the tutorial for the Mallet wrapper in gensim. <a href=""http://radimrehurek.com/2014/03/tutorial-on-mallet-in-python/"" rel=""nofollow"">http://radimrehurek.com/2014/03/tutorial-on-mallet-in-python/</a></p>

<p>When I fit the model with</p>

<pre><code>model = models.LdaMallet(mallet_path, corpus, num_topics=10, id2word=corpus.dictionary)
</code></pre>

<p>I get an error message:</p>

<pre><code>C:\Anaconda\lib\site-packages\gensim\models\ldamallet.py:234: RuntimeWarning: invalid value encountered in divide topic = topic / topic.sum() # normalize to probability dist
</code></pre>

<p>When I use the model to infer the topic distribution of the example the distribution is uniform:</p>

<pre><code>doc = ""Don't sell coffee, wheat nor sugar; trade gold, oil and gas instead.""
bow = corpus.dictionary.doc2bow(utils.simple_preprocess(doc))
print model[bow]
</code></pre>

<p>My output:</p>

<pre><code>[(0, 0.10000000000000002), (1, 0.10000000000000002), (2, 0.10000000000000002), (3, 0.10000000000000002), (4, 0.10000000000000002), (5, 0.10000000000000002), (6, 0.10000000000000002), (7, 0.10000000000000002), (8, 0.10000000000000002), (9, 0.10000000000000002)]
</code></pre>

<p>Is this a problem in the functioning of the wrapper or in mallet? I've managed to replicate the mallet tutorial here: <a href=""http://programminghistorian.org/lessons/topic-modeling-and-mallet"" rel=""nofollow"">http://programminghistorian.org/lessons/topic-modeling-and-mallet</a></p>
","2998998","","","","","2015-03-25 15:08:27","Gensim LdaMallet division error","<python><machine-learning><topic-modeling><gensim><mallet>","0","0","","","","CC BY-SA 3.0"
"56456051","1","56469545","","2019-06-05 07:36:06","","3","2594","<p>I get the following deprecation warning when saving/loading a gensim word embedding:</p>

<pre><code>model.save(""mymodel.model"")

/home/.../lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: 
UserWarning: This function is deprecated, use smart_open.open instead. 
See the migration notes for details:
</code></pre>

<p><a href=""https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function</a></p>

<pre><code>  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL
</code></pre>

<p>I don't understand what to do following the notes on the page.
So, <strong>how should I save and open my models instead?</strong></p>

<p>I use python 3.7 , gensim 3.7.3. and smart_open 1.8.4. I think I did not get the warning when using gensim 3.7.1. and python 3.5. smart_open should have been 1.8.4. </p>
","9032335","","","","","2019-06-06 00:16:02","gensim save load model deprecation warning","<gensim>","1","0","","","","CC BY-SA 4.0"
"48821863","1","","","2018-02-16 07:21:15","","0","715","<p>I Already have a Doc2Vec model. I have trained it with my train data.</p>

<p>Now after a while I want to use Doc2Vec for my test data. I want to add my test data vocabulary to my existing model's vocabulary. How can I do this?
I mean how can I update my vocabulary?</p>

<p>Here is my model:</p>

<pre><code>    model = model.load('my_model.Doc2vec')
</code></pre>
","6569505","","","","","2018-02-16 23:25:57","add new vocabulary to existing Doc2vec model","<word2vec><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"57200067","1","","","2019-07-25 10:41:26","","0","173","<p>I have a code that loads a file, strips each sentence and then removes some stopwords and returns the tokens.</p>

<p>So far so good.. If I include a <code>print()</code> statement or do a simple example, I see that stopwords are removed BUT..
when I run the sentences in my word2vec model, the model still creates a wordvector for stopwords like 'the' .. is there an error in my code??</p>

<pre><code>class Raw_Sentences(object):

    def __init__(self, dirname):
        self.dirname = dirname
    def __iter__(self):
        for file in file_loads: # list with the according file names e.g. 'Users/file1.txt'
                       with open(file,'r', buffering=20000000, encoding='utf-8') as t:     
                for sentence in tokenizer.tokenize(t.read().replace('\n', ' ').lower()):
                    sent = remove_stopwords(sentence)
                    print(sent)
                    yield gensim.utils.simple_preprocess(sent, deacc=True)
</code></pre>

<p>Then I run:</p>

<pre><code>sentences = Raw_Sentences(directory)
num_features = 200  
min_word_count = 2 
num_workers = cpu_count()
context_size = 4  
downsampling = 1e-5  
seed = 2 

model = gensim.models.Word2Vec(sentences,
                               sg=1, #skip-gram
                               seed=seed,
                               workers=num_workers,
                               size=num_features,
                               min_count=min_word_count,
                               window=context_size,
                               sample=downsampling)     

model.most_similar('the')
</code></pre>

<p>and it returns similar words.. But the word 'the' should have been removed...</p>

<p>crying out loud</p>

<p><code>remove_stopwords</code> is a gensim function <code>from gensim.parsing.preprocessing import remove_stopwords</code>  which takes a set of stopwords <code>stoplist = set(stop_words)</code> and removes them <code>def remove_stopwords(s):       ## del 
    s = utils.to_unicode(s)
    return "" "".join(w for w in s.split() if w not in stoplist)</code></p>
","10053244","","","","","2019-07-25 17:20:22","Code removes stopwords but Word2vec still creates wordvector for stopword?","<python><nltk><gensim><stop-words>","1","0","","","","CC BY-SA 4.0"
"54650673","1","","","2019-02-12 12:59:53","","0","1635","<p>When i try to import WordEmbeddingSimilarityIndex, it's giving me the following error:</p>
<pre><code>&gt;&gt; from gensim.models import WordEmbeddingSimilarityIndex
ImportError: cannot import name 'WordEmbeddingSimilarityIndex
</code></pre>
<p>The same issue occurs for <code>SparseTermSimilarityMatrix</code> function:</p>
<pre><code>&gt;&gt; from gensim.similarities import SparseTermSimilarityMatrix
ImportError: cannot import name 'SparseTermSimilarityMatrix
</code></pre>
<p>Note: I have installed and imported gensim, gensim.models and gensim.similarities. But still it's giving me the <code>ImportError</code> while importing the above mentioned functions.</p>
<p>Can you tell me what I am doing wrong, please?</p>
","10639106","","7122272","","2021-07-31 09:16:52","2021-07-31 09:16:52","How to import WordEmbeddingSimilarityIndex function from gensim module?","<python-3.x><gensim>","2","0","","","","CC BY-SA 4.0"
"31524433","1","","","2015-07-20 19:08:05","","1","1401","<p>I am not able to reproduce the word2vec results using Gensim, and some of the results do not make sense. Gensim is an open-source toolkit, is intended for handling large text collections using efficient online algorithms, including the <a href=""http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow noreferrer"">python implementation of Google's word2vec algorithm</a>.</p>
<p>I am following an <a href=""http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python"" rel=""nofollow noreferrer"">online tutorial</a> and am not able reproduece the results.  The most similar words for (positive=['woman', 'king'], negative=['man']) were supposed to to 'wenceslaus'and 'queen'.  In stead, I got 'u'eleonore' and 'iv'.  The most similar for 'fast' was slow and for 'quick' was 'mitsumi'.</p>
<p>Any insights?  Below are my codes and results:</p>
<blockquote>
<p>&gt;&gt;&gt; from gensim.models import word2vec</p>
<p>&gt;&gt;&gt; import logging</p>
<p>&gt;&gt;&gt; logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)</p>
<p>&gt;&gt;&gt; sentences = word2vec.Text8Corpus('\tmp\text8')</p>
<p>&gt;&gt;&gt; model = word2vec.Word2Vec(sentences, size=200)</p>
<p>&gt;&gt;&gt; model.most_similar(positive=['woman', 'king'], negative=['man'], topn=2)</p>
<p>out[63]: [(u'eleonore', 0.5138808...), (u'iv',0.510519325...)]</p>
<p>&gt;&gt;&gt; model.most_similar(positive=['fast'])</p>
<p>Out[64]: [(u'slow', 0.48932...), (u'paced', 0.46925...)...]</p>
<p>&gt;&gt;&gt; model.most_similar(positive=['quick'],topn=1)</p>
<p>out[65]: [(u'mitsumi', 0.48545..)]</p>
</blockquote>
","4978118","","-1","","2020-06-20 09:12:55","2021-09-07 14:41:10","Why I cannot reproduce word2vec results using gensim","<python><gensim><word2vec>","2","0","1","","","CC BY-SA 3.0"
"63472132","1","","","2020-08-18 15:45:17","","1","132","<p>I have created GloVE vectors in R previously using <code>text2vec</code> library.</p>
<p>Is there any easy way to export these for use in Python where I have scripts to compare/contract with Gensim created word vectors?  I know there is a specific word2vec c_format, but Im not sure if R has the capability of producing this.</p>
","4115123","","","","","2020-08-21 13:19:34","Export R text2vec Vectors for use in Gensim in Python","<python><r><gensim><text2vec>","1","1","","","","CC BY-SA 4.0"
"54754669","1","","","2019-02-18 20:11:40","","0","512","<p>I try to train model to get sentence similarity (In my case names of some organization)</p>

<p>I use to train model</p>

<pre><code>names_tok = [TaggedDocument(words=word_tokenize(name.lower()), tags=[str(i)])
                        for (i, name) in enumerate(names)]

# train model
max_epochs = 50
vec_size = 50
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha,
                min_alpha=0.00025,
                min_count=1,
                dm=1)
</code></pre>

<p>And I get results with </p>

<pre><code>name = word_tokenize(name.lower())
infer_v = model.infer_vector(name)
results = model.docvecs.most_similar([infer_v]))
</code></pre>

<p>And it returns strange results for all tests. I've already tried to use example from train data and I haven't got it with similarity. For example <code>–§–∏–ª–∏–ø –ú–æ—Ä—Ä–∏—Å –ü—Ä–æ–¥–∞–∫—Ç—Å –°.–ê.</code> I get next results</p>

<pre><code>–ù–∞–π–∫ –ò–Ω–Ω–æ—É–≤–µ–π—Ç –°.–í.: 0.9336682558059692

–°–û–°–¨–ï–¢–ï –î–ï –ü–†–û–î–Æ–ò –ù–ï–°–¢–õ–ï –°.–ê.: 0.9370058178901672

–Æ–Ω–∏–ª–µ–≤–µ—Ä –ù.–í.: 0.9347286224365234

–ú–µ—Ä–∫ –®–∞—Ä–ø –∏ –î–æ—É–º –ö–æ—Ä–ø.: 0.9339677095413208
</code></pre>

<p>And I can't understand Why I get this.
I have 180 000 examples of train data.
How can I improve results of my model?</p>
","6840039","","6840039","","2019-02-19 06:31:54","2019-02-20 00:55:19","Doc2Vec: strange results with model.docvecs.most_similar","<python><gensim><doc2vec>","1","3","","","","CC BY-SA 4.0"
"57443879","1","57455261","","2019-08-10 16:33:19","","0","82","<p>I'm training a Doc2Vec model from the french wikipedia.</p>

<p>My code is based on this notebook :
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>

<p>It's actually in the training phase, but, I don't know how to vectorize new sentences after that.</p>

<p>Should I just use : model.infer_vector[""Example sentence here""] ?
But in this case, how to make the same processing than the Wikicorpus method does ? (This is not explained here : <a href=""https://radimrehurek.com/gensim/corpora/wikicorpus.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/corpora/wikicorpus.html</a>)</p>

<p>Thanks!</p>
","3827807","","3827807","","2019-08-10 16:40:01","2019-08-12 03:10:32","New sentence from doc2vec model trained with wikicorpus","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"45687700","1","","","2017-08-15 06:44:54","","1","713","<p>When I use the python library <code>gensim</code> and train a Word2Vec model, I can call the function like this <code>word2vec_result.similarity('apple','banana')</code> to get the cosine similarity between apple and banana at local machine. <br>
But in <code>pyspark(version2.2)</code>, I can't find the same function in the document after the model built.<br></p>

<p>Code:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-
from pyspark.mllib.feature import Word2Vec
from pyspark.mllib.feature import Word2VecModel
from pyspark import SparkConf, SparkContext
import logging
directory = ""data_path""  
inp = sc.textFile(directory).map(lambda row: row.split("" ""))
model = word2vec_run(inp)
model.save(sc, ""/data/word2vec_model"")
</code></pre>

<p>Are there any simple ways to achieve the goal?</p>
","5771675","","","","","2017-08-15 06:44:54","How to compute cosine similarity between two words in Word2Vec model in pyspark","<python><pyspark><word2vec><gensim>","0","2","","","","CC BY-SA 3.0"
"39948442","1","","","2016-10-09 20:56:26","","0","821","<p>I am trying to visualize LDA topics in Python using PyLDAVis but I can't seem to get it right. My model has a vocab size of 150K words and about 16 Million tokens were taken to train it.</p>

<p>I am doing it outside of an iPython notebook and this is the code that I wrote to do it.</p>

<pre><code>model_filename = ""150k_LdaModel_topics_""+ topics +""_passes_""+passes +"".model""

dictionary = gensim.corpora.Dictionary.load('LDADictSpecialRemoved150k.dict')
corpus = gensim.corpora.MmCorpus('LDACorpusSpecialRemoved150k.mm')
ldamodel = gensim.models.ldamodel.LdaModel.load(model_filename)

import pyLDAvis.gensim
vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
pyLDAvis.save_html(vis, ""topic_viz_""+topics+""_passes_""+passes+"".html"")
</code></pre>

<p>I get the following error after 2-3 hours of running code on a high speed server with >30GBs of RAM. Can someone help where I am going wrong?</p>

<pre><code>Traceback (most recent call last):
  File ""create_vis.py"", line 36, in &lt;module&gt;
    vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
  File ""/local/lib/python2.7/site-packages/pyLDAvis/gensim.py"", line 110, in prepare
    return vis_prepare(**opts)
  File ""/local/lib/python2.7/site-packages/pyLDAvis/_prepare.py"", line 398, in prepare
    token_table        = _token_table(topic_info, term_topic_freq, vocab, term_frequency)
  File ""/local/lib/python2.7/site-packages/pyLDAvis/_prepare.py"", line 267, in _token_table
    term_ix.sort()
  File ""/local/lib/python2.7/site-packages/pandas/indexes/base.py"", line 1703, in sort
    raise TypeError(""cannot sort an Index object in-place, use ""
TypeError: cannot sort an Index object in-place, use sort_values instead
</code></pre>
","3667569","","","","","2016-10-12 12:30:24","PyLdaVis : TypeError: cannot sort an Index object in-place, use sort_values instead","<python><visualization><lda><gensim><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"46013294","1","46027746","","2017-09-02 11:26:06","","2","4114","<p>Just reading through the doc2vec commands on the gensim page. </p>

<p>I am curious about  the command""intersect_word2vec_format"" . </p>

<p>My understanding of this command is it lets me inject vector values from a pretrained word2vec model into my doc2vec model and then train my doc2vec model using the pretrained word2vec values rather than generating the word vector values from my document corpus. The result is that I get a more accurate doc2vec model because I am using pretrained w2v values which was generated from a much larger corpus of data compared to my relatively small document corpus. </p>

<p>Is my understanding of this command correct or not even close?  ;-) </p>
","8455177","","","","","2017-09-03 20:54:19","gensim doc2vec ""intersect_word2vec_format"" command","<nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"64017591","1","","","2020-09-22 21:00:18","","1","154","<p>In my work, I used my own corpus to train a Word2Vec model using gensim. Then I used several small corpus to &quot;update&quot; that model (producing different sets of vectors). This process well documented in gensim.</p>
<p>I am trying to replicate a similar process with GloVe model. I could find the code to train my own GloVe model <a href=""https://github.com/stanfordnlp/GloVe"" rel=""nofollow noreferrer"">here</a>.
However, I am not sure how to go about updating this model with different new corpus. Does it even make sense to &quot;update&quot; a GloVe model?</p>
<p><a href=""https://stackoverflow.com/a/56383823/8464088"">This</a> answer says no. But strong confirmation will help.</p>
","8464088","","","","","2020-09-22 21:00:18","How to update GloVe models?","<nlp><stanford-nlp><gensim><word-embedding>","0","8","1","","","CC BY-SA 4.0"
"47171777","1","","","2017-11-08 04:47:30","","1","548","<p>I have a set of document vectors generated using gensim doc2vec (~500K vectors of 150 dimensions). I wish to cluster similar documents for which i want to generate a n*n similarity matrix over which i can run my clustering algorithm.</p>

<p>I tried instructions of this link <a href=""https://github.com/RaRe-Technologies/gensim/issues/140"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/140</a> using the gensim.similarities but the output for 500k records was 500k*150 matrix. I dont understand the output. Shouldn't it be 500k * 500k ? am i missing something?</p>
","8432128","","","","","2017-11-08 07:45:16","doc2vec clustering n*n similarity between documents","<cluster-analysis><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"40205725","1","40427138","","2016-10-23 17:23:45","","1","584","<p>So I am trying to use gensim to generate an LSI model along with corpus_lsi following <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow"">this</a> tutorial.</p>

<p>I start with a corpus and a dictionary that I generated myself.
The list of documents are too small (9 lines = 9 documents), which is the sample list provided in <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">gensim</a> tutorials</p>

<p>However, pythos just crashes when it reaches the line for generating LSI_model.
You can see below my code along with the generated output</p>

<p><strong>Code</strong></p>

<pre><code>#!/usr/bin/env python
import os
from gensim import corpora, models, similarities
import logging

#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

if __name__ == '__main__':
    if (os.path.exists(""tmp\dictionary.dict"")):
        dictionary = corpora.Dictionary.load('tmp\dictionary.dict')
        corpus = corpora.MmCorpus('tmp\corpus.mm')
        print(""Used files generated Dataset Generator"")
    else:
        print(""Please run dataset generator"")

print (""generating tf-idf model ..."")
tfidf = models.TfidfModel(corpus)   # Generate tfidf matrix (tf-idf model)
print (""generating corpus_tf-idf model ..."")
corpus_tfidf = tfidf[corpus]    #use the model to transform vectors

print (""generating LSI model ..."")
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation
print (""generating corpus_lsi model ..."")
corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow-&gt;tfidf-&gt;fold-in-lsi

lsi.print_topics(2)
</code></pre>

<p><strong>Output</strong></p>

<pre><code>Used files generated Dataset Generator
generating tf-idf model ...
generating corpus_tf-idf model ...
generating LSI model ...
</code></pre>

<p>After printing ""generating LSI model"" it crashes</p>

<p>Any suggestions ?</p>

<p><strong>Other things I tried</strong></p>

<ul>
<li>Changing python version to python 2.6</li>
<li>Removing gensim and installing it again from github (instead of conda) </li>
</ul>
","1831518","","1831518","","2016-10-24 07:22:42","2016-11-04 16:05:48","gensim Generating LSI model causes ""Python has stopped working""","<python><python-3.x><gensim><latent-semantic-indexing><latent-semantic-analysis>","1","0","","","","CC BY-SA 3.0"
"66247207","1","","","2021-02-17 17:39:00","","0","67","<p>The following command will load a word2vec format file into a KeyedVectors object:</p>
<pre><code>w2v_model = KeyedVectors.load_word2vec_format(&quot;w2v_model.bin&quot;, binary=True)
</code></pre>
<p>My question is how to convert this KeyedVectors object into gensim.models.fasttext.FastText or gensim.models.word2vec.Word2Vec object.</p>
","6221871","","","","","2021-02-18 12:49:23","How to convert Word2VecKeyedVectors to FastText or Word2Vec object in Gensim?","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"64041275","1","64041352","","2020-09-24 07:14:27","","0","225","<p>I have a very large amount of sentences, the problem is i cannot load them all at once in memory, specially when i tokenize the sentences and split them into list of words my RAM goes full really fast.</p>
<p>but i couldn't find any example of how can i train the gensim word2vec with batches, meaning in each epoch i guess i have to somehow load batches of data from disk, tokenize them and give it to the model then unload it and load the next batch.</p>
<p>how can i overcome this problem and train a word2vec model when i don't have enough ram to load all the sentences (not even 20% of them).</p>
<p>my sentences are basically in a text file, each line representing a sentence.</p>
","9557861","","","","","2020-09-24 16:59:06","How to deal with large amount of sentences with gensim word2vec?","<nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"24126187","1","","","2014-06-09 18:08:45","","4","3367","<p>I was following the instructions on this link (""<a href=""http://radimrehurek.com/2014/03/tutorial-on-mallet-in-python/"" rel=""nofollow"">http://radimrehurek.com/2014/03/tutorial-on-mallet-in-python/</a>""), however I came across an error when I tried to train the model:</p>

<pre><code>    model = models.LdaMallet(mallet_path, corpus, num_topics =10, id2word = corpus.dictionary)
    IOError: [Errno 2] No such file or directory: 'c:\\users\\brlu\\appdata\\local\\temp\\c6a13a_state.mallet.gz'
</code></pre>

<p>Please share any thoughts you might have. </p>

<p>Thanks.</p>
","3038725","","","","","2018-10-13 01:33:20","Error when implementing gensim.LdaMallet","<python><lda><gensim>","5","1","2","","","CC BY-SA 3.0"
"64018628","1","64025130","","2020-09-22 22:43:07","","0","56","<p>I'm running the LSI program from Gensim's <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html"" rel=""nofollow noreferrer"">Topics and Transformations tutorial</a> and for some reason, the signs of the topic weights keep switching from positive to negative and vice versa. For example, this is what I get when I print using the line</p>
<pre><code>for doc, as_text in zip(corpus_lsi, documents):
    print(doc, as_text)

Run 1
[(0, 0.066007833960900791), (1, 0.52007033063618491), (2, -0.37649581219168904)]
[(0, 0.196675928591421), (1, 0.7609563167700063), (2, 0.5080674581001664)]
[(0, 0.089926399724459982), (1, 0.72418606267525132), (2, -0.408989731553764)]
[(0, 0.075858476521777865), (1, 0.63205515860034334), (2, -0.53935336057339001)]
[(0, 0.10150299184979866), (1, 0.57373084830029653), (2, 0.67093385852959075)]
[(0, 0.70321089393783254), (1, -0.1611518021402539), (2, -0.18266089635241448)]
[(0, 0.87747876731198449), (1, -0.16758906864658912), (2, -0.10880822642632856)]
[(0, 0.90986246868185872), (1, -0.14086553628718496), (2, 0.00087117874886860625)]
[(0, 0.61658253505692762), (1, 0.053929075663897361), (2, 0.25568697959599318)]

Run 2
[(0, 0.066007833960908563), (1, -0.52007033063618446), (2, -0.37649581219168959)]
[(0, 0.19667592859143226), (1, -0.76095631677000253), (2, 0.50806745810016629)]
[(0, 0.089926399724470751), (1, -0.72418606267525032), (2, -0.40898973155376284)]
[(0, 0.075858476521787177), (1, -0.63205515860034223), (2, -0.5393533605733889)]
[(0, 0.10150299184980684), (1, -0.57373084830029419), (2, 0.67093385852959098)]
[(0, 0.70321089393782976), (1, 0.16115180214026417), (2, -0.18266089635241456)]
[(0, 0.87747876731198149), (1, 0.16758906864660211), (2, -0.10880822642632891)]
[(0, 0.90986246868185627), (1, 0.14086553628719861), (2, 0.00087117874886795399)]
[(0, 0.61658253505692828), (1, -0.053929075663887563), (2, 0.25568697959599251)]

Run 3
[(0, 0.066007833960902929), (1, -0.52007033063618535), (2, 0.37649581219168821)]
[(0, 0.19667592859142491), (1, -0.76095631677000497), (2, -0.50806745810016662)]
[(0, 0.089926399724463771), (1, -0.7241860626752511), (2, 0.40898973155376317)]
[(0, 0.075858476521781085), (1, -0.63205515860034334), (2, 0.5393533605733889)]
[(0, 0.10150299184980124), (1, -0.57373084830029542), (2, -0.67093385852959064)]
[(0, 0.70321089393783143), (1, 0.16115180214025732), (2, 0.18266089635241564)]
[(0, 0.87747876731198304), (1, 0.16758906864659326), (2, 0.10880822642632952)]
[(0, 0.90986246868185761), (1, 0.1408655362871892), (2, -0.00087117874886778746)]
[(0, 0.61658253505692784), (1, -0.053929075663894419), (2, -0.25568697959599318)]
</code></pre>
<p>I am running Python 3.5.2 on a PC, coding in IntelliJ.</p>
<p>Anyone encountered this problem, using the Gensim library or elsewhere?</p>
","8315960","","6573902","","2020-09-23 09:09:30","2020-09-23 09:39:27","Why are the signs of my topic weights changing from run to run?","<python><python-3.x><gensim><topic-modeling><latent-semantic-indexing>","2","3","","","","CC BY-SA 4.0"
"31321209","1","","","2015-07-09 14:57:45","","57","72066","<p>How to get document vectors of two text documents using Doc2vec?
I am new to this, so it would be helpful if someone could point me in the right direction / help me with some tutorial</p>

<p>I am using gensim.</p>

<pre><code>doc1=[""This is a sentence"",""This is another sentence""]
documents1=[doc.strip().split("" "") for doc in doc1 ]
model = doc2vec.Doc2Vec(documents1, size = 100, window = 300, min_count = 10, workers=4)
</code></pre>

<p>I get </p>

<blockquote>
  <p>AttributeError: 'list' object has no attribute 'words'</p>
</blockquote>

<p>whenever I run this.</p>
","4663523","","7117003","","2018-12-15 19:33:57","2019-06-04 10:17:50","Doc2vec: How to get document vectors","<python><gensim><word2vec>","4","0","26","","","CC BY-SA 4.0"
"23348819","1","23349062","","2014-04-28 18:41:57","","0","687","<p>I believe my issue is that python does not play nicely with the character encoding of a column in a SQL table:</p>

<pre><code>| column | varchar(255) | latin1_swedish_ci | YES  |     | NULL              |                             | select,insert,update,references |    | 
</code></pre>

<p>The above shows the output for this column. It has type <code>varchar(255)</code> and has encoding <code>latin1_swedish_ci.</code> </p>

<p>Now when I try to make python play with this data, I am getting the following error: </p>

<pre><code> dictionary = gs.corpora.Dictionary(tweets)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 50, in __init__
    self.add_documents(documents)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 97, in add_documents
    _ = self.doc2bow(document, allow_update=True) # ignore the result, here we only care about updating token ids
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 121, in doc2bow
    document = sorted(utils.to_utf8(token) for token in document)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 121, in &lt;genexpr&gt;
    document = sorted(utils.to_utf8(token) for token in document)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/utils.py"", line 164, in any2utf8
    return unicode(text, encoding, errors=errors).encode('utf8')
  File ""/usr/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x96 in position 0: invalid start byte
</code></pre>

<p><code>gs</code> is the <a href=""http://radimrehurek.com/gensim/"" rel=""nofollow"">gensim</a> topic modeling library. I believe that the problem is that gensim requires unicode encodings. </p>

<ol>
<li>How can I change the character encoding (collation?) for this column in my database?</li>
<li>Is there an alternative solution?</li>
</ol>

<p>Thanks for all the help!</p>
","1104823","","","","","2018-12-07 08:15:15","Python MySQLdb change string encoding","<python><mysql><encoding><collation><gensim>","3","0","","","","CC BY-SA 3.0"
"66125475","1","","","2021-02-09 19:06:20","","1","186","<p>I'm training my Doc2Vec model on 106k documents (100-600 words per document). The goal is to retrieve similar documents for a target document.</p>
<p>Since Doc2Vec is an unsupervised model there is no real evaluation possible except to test how it performs on your downstream task.
So, I created a small dataset containing about 200 target documents and 5 similar documents per target.</p>
<p>My idea is to calculate the cosine similarity for every document against all other documents in my test dataset and get top 5 similar documents per target document.</p>
<p>Is there an efficient way to create a cosine similarity matrix with Doc2Vec? The <code>most_similar</code> function is impractical as it retrieves every similar document used for training.</p>
","15178277","","15178277","","2021-02-09 20:46:12","2021-02-10 13:25:46","evaluating Doc2Vec - cosine similarity matrix","<python><nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"56468865","1","64440004","","2019-06-05 22:29:49","","2","1277","<p>I am relatively new to NLP and I am trying to create my own words embeddings trained in my personal corpus of docs.</p>

<p>I am trying to implement the following code to create my own wordembedings:</p>

<pre><code>model = gensim.models.Word2Vec(sentences)
</code></pre>

<p>with sentences being a list of sentences.
Since I can not pass thousands and thousands of sentences I need an iterator</p>

<pre><code># with mini batch_dir a directory with the text files
# MySentences is a class iterating over sentences.
sentences = MySentences(minibatch_dir) # a memory-friendly iterator
</code></pre>

<p>I found this solution by the creator of gensim:</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()
</code></pre>

<p>It does not work for me.
How can I create an iterator if I know how to get the list of sentences from every document?</p>

<p>And second very related question:
If I am aiming to compare documents similarity in a particular corpus, is always better to create from scratch word embeddings with all the documents of that particular corpus than using GloVec or word2vec? 
The amount of docs is around 40000.</p>

<p>cheers</p>

<p>More pre</p>
","7168098","","","","","2020-10-20 06:59:14","Sentence iterator to pass to Gensim language model","<python><nlp><gensim><word2vec><word-embedding>","1","5","","","","CC BY-SA 4.0"
"38062337","1","","","2016-06-27 20:05:11","","3","484","<p>I am working with the Doc2Vec and Word2Vec deep learning algorithms (<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">Doc2Vec API  description from Gensim</a>). <a href=""http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow"">More description here</a></p>

<p>Currently I am interested in using the <code>model.n_similarity(wordSet1, wordSet2)</code> method which basically computes the  cosine similarity between two sets of words.</p>

<p>I am interested in any ways of validating the models performance, not just on the <code>n_similiarity()</code> function, but overall how accurate or realistic results can the model provide. Since it performs deep learning, I do not know if there is any ways of knowing how well does it perform. </p>

<p>Are there any techniques that I should look up, then use or is there a data-set that has results and I should compare ?</p>

<p>Any suggestion is much appreciated. Thank you.</p>
","2949252","","","","","2016-06-27 20:05:11","Is there any way to validate the performance of a Doc2Vec/ Word2Vec Deep Learning model?","<python><deep-learning><gensim><word2vec><doc2vec>","0","0","","","","CC BY-SA 3.0"
"48842866","1","48843030","","2018-02-17 15:28:11","","0","1895","<p>I am using gensim in python 3 as shown within image below </p>

<p><a href=""https://i.stack.imgur.com/xMbap.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xMbap.png"" alt="" Image""></a></p>

<p>In line no 11 I am getting the following error:</p>

<p><a href=""https://i.stack.imgur.com/NoGO4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NoGO4.png"" alt=""Image""></a></p>
","9373885","","340277","","2018-02-17 18:52:31","2018-05-08 15:25:45","'gensim.models.doc2vec' has no attribute 'LabeledSentence'","<python-3.x><sublimetext3><sentiment-analysis><gensim>","1","1","","","","CC BY-SA 3.0"
"64011983","1","","","2020-09-22 14:34:03","","0","37","<p>Is there any way I can see the distribution over topics (topic mixtures) per document for the Dynamic Topic Model in Spyder using the Gensim module?</p>
<p>I am only aware of 'print_topic_times' that shows one topic (distribution over words) over all time slices.
However, is there any code that allows to see the topix mixture of one document for each time slice?</p>
","14316722","","","","","2020-09-24 16:32:26","Dynamic Topic Model: Topic Mixture","<spyder><gensim><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"48623214","1","48639835","","2018-02-05 13:01:28","","0","167","<p>I have a 60000 documents which i processed in <code>gensim</code> and got a 60000*300 matrix. I exported this as a <code>csv</code> file. When i import this in <code>ELKI</code> environment and run <code>Kmeans</code> clustering, i am getting below error.</p>

<pre><code>Task failed
de.lmu.ifi.dbs.elki.data.type.NoSupportedDataTypeException: No data type found satisfying: NumberVector,field AND NumberVector,variable
Available types: DBID DoubleVector,variable,mindim=266,maxdim=300 LabelList
    at de.lmu.ifi.dbs.elki.database.AbstractDatabase.getRelation(AbstractDatabase.java:126)
    at de.lmu.ifi.dbs.elki.algorithm.AbstractAlgorithm.run(AbstractAlgorithm.java:81)
    at de.lmu.ifi.dbs.elki.workflow.AlgorithmStep.runAlgorithms(AlgorithmStep.java:105)
    at de.lmu.ifi.dbs.elki.KDDTask.run(KDDTask.java:112)
    at de.lmu.ifi.dbs.elki.application.KDDCLIApplication.run(KDDCLIApplication.java:61)
    at [...]
</code></pre>

<p>Below is the ELKI settings i have used
<a href=""https://i.stack.imgur.com/QdG3V.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QdG3V.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/ECMlW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ECMlW.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/XyMJu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XyMJu.jpg"" alt=""enter image description here""></a></p>
","4566277","","4566277","","2018-02-06 09:42:32","2018-02-09 09:25:14","ELKI Kmeans clustering Task failed error for high dimensional data","<cluster-analysis><k-means><gensim><doc2vec><elki>","2","0","","","","CC BY-SA 3.0"
"47022246","1","47038874","","2017-10-30 18:46:21","","1","697","<p>When I tried to import gensim module in Windows, I end up with below error.</p>

<blockquote>
  <p>c:\python27\lib\site-packages\gensim-3.0.1-py2.7-win-amd64.egg\gensim\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
    <strong>warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")</strong></p>
</blockquote>

<p>Is there any possibility to overcome this warning?</p>
","8698105","","472495","","2018-06-12 21:46:39","2018-06-12 21:46:39","Warning message after importing gensim module in Windows","<python><nlp><warnings><semantics><gensim>","1","1","","","","CC BY-SA 4.0"
"65495775","1","65740567","","2020-12-29 16:55:46","","1","378","<p>I want to use the Gensim (word2vec model) trained vectors inside a neural network (Tensorflow). There are two kinds of weights I can use for this purpose. The first group is <code>model.syn0</code> and the second group is <code>model.vectors_norm</code> (after calling <code>model.init_sims(replace=True)</code>). The second one is the group of vectors we use for calculating similarity. Which one has the correct order (match with <code>model.wv.index2word</code> and <code>model.wv.vocab[X].index</code>) and weights for the embedding layer of a neural network?</p>
","2991243","","","","","2021-01-15 17:13:49","Which trained embeddings vectors from Gensim (word2vec model) should be used for Tensorflow? Unnormalised or normalised ones?","<tensorflow><keras><gensim><word2vec><word-embedding>","1","0","3","","","CC BY-SA 4.0"
"55520565","1","","","2019-04-04 16:07:48","","1","142","<p><strong>Background :</strong></p>

<p>Given a corpus I want to train it with an implementation of word2wec (Gensim). </p>

<p>Want to understand if the final similarity between 2 tokens is dependent on the frequency of A and B in the corpus <em>(all contexts preserved)</em>, or agnostic of it.</p>

<p><em>Example</em>: 
(May not be ideal, but using it to elaborate the problem statement) </p>

<p>Suppose word 'A' is being used in 3 different contexts within the corpus : </p>

<pre><code>Context 1 : 1000 times
Context 2 : 50000 times
Context 3 : 50000 times
</code></pre>

<p>'B' is being used in 2 different contexts :</p>

<pre><code>Context 1 : 300 times 
Context 5 : 1000 time
</code></pre>

<p><strong>Question :</strong> </p>

<p>If I change the frequency of 'A' in my corpus (ensuring no context is lost, i.e. 'A' is still being used at least once in all the contexts as in the original corpus), is the similarity between A snd B going to be the same ?</p>

<p>New distribution of 'A' across contexts</p>

<pre><code> Context 1 : 5 times
 Context 2 : 10 times
 Context 3 : 5000 times
</code></pre>

<p>Any leads appreciated</p>
","6046655","","6046655","","2019-04-04 16:14:09","2019-04-04 17:21:20","Semantic similarity between words A and B : Dependency on frequency of A and B in corpus?","<python><nlp><gensim><word2vec><word-embedding>","3","0","","","","CC BY-SA 4.0"
"48009532","1","53907235","","2017-12-28 14:47:23","","4","4679","<p>I have generated word vectors from a corpus, but I am facing out of vocabulary issues for many words. How can I generate word vectors for OOV words on the fly using existing word embedding?</p>
","7962476","","","","","2019-05-08 14:45:29","Word embedding for OOV words","<machine-learning><nlp><word2vec><gensim>","1","4","3","","","CC BY-SA 3.0"
"65520223","1","","","2020-12-31 11:24:08","","0","22","<p>Could someone please explain to me what does the parameter threshold do in the Phrase model of Gensim? and how to choose it when it comes to train a dataset that contains a lot of data?
I searched among many tutorials but still didn't get a clear idea about it
thank you</p>
","12775879","","","","","2020-12-31 18:06:21","Gensim Phrase model parameters (threshold)","<model><gensim><threshold><phrase>","1","0","","","","CC BY-SA 4.0"
"31338082","1","","","2015-07-10 10:05:38","","2","956","<p>I want to process the wikipedia using <code>gensim.corpora.wikicorpus</code>. My final objective is to train a <a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">Word2Vec</a> Model from it.</p>

<p>I have it working but I have a problem with the accented vowels of Spanish: √°, √©, √≠, √≥, √∫.</p>

<p>I want to normalize them to a, e, i, o, u.</p>

<p>I have seem that there is a <a href=""https://radimrehurek.com/gensim/utils.html#gensim.utils.deaccent"" rel=""nofollow"">deaccent</a> function in gensim but I dwould like to apply it directly while I am building the corpus. Can this be done?</p>

<p>Here is a working example:</p>

<pre><code>from gensim.corpora import WikiCorpus
from gensim.models.word2vec import  Word2Vec
import logging

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',
                level=logging.INFO)

# I would like to include here the normalization
corpus = WikiCorpus('/Users/jesusfbes/Desktop/eswiki-latest-pages-articles.xml.bz2', dictionary=False)


max_sentence = -1


def generate_lines():
    for index, text in enumerate(corpus.get_texts()):
        if index &lt; max_sentence or max_sentence == -1:
            yield text
        else:
            break

model = Word2Vec(size=400, window=5, min_count=5)
model.build_vocab(generate_lines())
model.train(generate_lines(), chunksize=500)

model.save('mymodel')
</code></pre>
","5102327","","","","","2017-05-10 08:50:04","Spanish Wikipedia processing using Gensim","<python><wikipedia><gensim><word2vec>","1","1","","","","CC BY-SA 3.0"
"64039454","1","64039527","","2020-09-24 04:15:04","","0","92","<p>I am new to NLP and I am running into this issue that I do not understand at all:</p>
<p>I have a text file with gloVe vectors.
I converted it to Word2Vec using</p>
<pre><code>glove2word2vec(TXT_FILE_PATH, KV_FILE_PATH)
</code></pre>
<p>this creates a KV file in my path which can then be loaded using</p>
<pre><code>word_vectors = KeyedVectors.load_word2vec_format(KV_FILE_PATH, binary=False)
</code></pre>
<p>I then save it using</p>
<pre><code>word_vectors.save(KV_FILE_PATH)
</code></pre>
<p>But when I now try to use the new KV file in intersect_word2vec_format it gives me an encoding error</p>
<pre><code>---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-11-d975bb14af37&gt; in &lt;module&gt;
      6 
      7 print(&quot;Intersect with pre-trained model...&quot;)
----&gt; 8 model.intersect_word2vec_format(KV_FILE_PATH, binary=False)
      9 
     10 print(&quot;Train custom word2vec model...&quot;)

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/word2vec.py in intersect_word2vec_format(self, fname, lockf, binary, encoding, unicode_errors)
    890         logger.info(&quot;loading projection weights from %s&quot;, fname)
    891         with utils.open(fname, 'rb') as fin:
--&gt; 892             header = utils.to_unicode(fin.readline(), encoding=encoding)
    893             vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    894             if not vector_size == self.wv.vector_size:

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/utils.py in any2unicode(text, encoding, errors)
    366     if isinstance(text, unicode):
    367         return text
--&gt; 368     return unicode(text, encoding, errors=errors)
    369 
    370 

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
","9665188","","","","","2020-09-24 04:25:22","How is the output of glove2word2vec() different from keyed_vectors.save()","<python><nlp><stanford-nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"48614304","1","","","2018-02-05 00:14:38","","1","291","<p>I'm trying to build a Gensim word2vec model by using an external vocabulary. I know Gensim has an internal vocabulary generator however I do not have the same control over them. My problem code is simply. </p>

<pre><code>import gensim
from sklearn.feature_extraction.text import CountVectorizer

corpus = corpusCleaner(raw_corpus)
vocabularyGenerator = CountVectorizer(strip_accents=""ascii"", stop_words=""english"")
vocabularyGenerator.fit(corpus)
vocabulary = vocabularyGenerator.vocabulary_
model = gensim.models.Word2Vec()
model.build_vocab_from_freq(vocabulary)
</code></pre>

<p>I'm getting
C:\Anaconda3\envs\workflow\lib\site-packages\gensim\models\word2vec.py:1235: RuntimeWarning: overflow encountered in int_scalars
  retain_pct = retain_total * 100 / max(original_total, 1)</p>
","6486877","","","","","2018-02-05 00:14:38","Gensim build_vocab_from_freq overflow Error","<python><nlp><overflow><gensim><vocabulary>","0","1","","","","CC BY-SA 3.0"
"65788950","1","65797938","","2021-01-19 09:46:55","","1","249","<p>I want to create a <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""nofollow noreferrer"">gensim dictionary</a> from lines of a dataframe. The <code>df.preprocessed_text</code> is a list of words.</p>
<pre><code>from gensim.models.phrases import Phrases, Phraser
from gensim.corpora.dictionary import Dictionary


def create_dict(df, bigram=True, min_occ_token=3):

    token_ = df.preprocessed_text.values
    if not bigram:
        return Dictionary(token_)
    
    bigram = Phrases(token_,
                     min_count=3,
                     threshold=1,
                     delimiter=b' ')

    bigram_phraser = Phraser(bigram)

    bigram_token = []
    for sent in token_:
        bigram_token.append(bigram_phraser[sent])
    
    dictionary = Dictionary(bigram_token)
    dictionary.filter_extremes(no_above=0.8, no_below=min_occ_token)
    dictionary.compactify() 
    
    return dictionary
</code></pre>
<p>I couldn't find a progress bar option for it and the <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">callbacks</a> doesn't seem to work for it too. Since my corpus is huge, I really appreciate a way to show the progress. Is there any?</p>
","6355816","","","","","2021-01-19 19:09:19","Add progress bar (verbose) when creating gensim dictionary","<python><dictionary><text><progress-bar><gensim>","1","3","","","","CC BY-SA 4.0"
"56483468","1","56486335","","2019-06-06 18:51:00","","2","62","<p>I am trying to use gensim for doc2vec and word2vec.</p>

<p>Since PV-DM approach can generate word2vec and doc2vec at the same time,
I thought PV-DM is the right model to use.</p>

<p>So, I created a model using <code>gensim</code> by specifying <code>dm=1</code> for PV-DM</p>

<p>My questions are followings:</p>

<ol>
<li><p>Is it true that word2vec model gets trained along with doc2vec when I call <code>train</code> on Doc2vec object??</p></li>
<li><p>it seems like property <code>wv</code> contains word2vec and available even before training. Is this static version of word2vec?</p></li>
<li><p>I also created DBOW model and noticed that it also contains <code>wv</code>. Is this also the same static version of word2vec that I mentioned in the previous question?</p></li>
</ol>
","8128407","","8128407","","2019-06-06 19:47:10","2019-06-06 23:41:25","Where is word2vec mapping coming from for DBOW doc2vec in gensim implementation?","<gensim><word2vec><doc2vec>","1","0","1","","","CC BY-SA 4.0"
"14468078","1","14637784","","2013-01-22 21:11:09","","2","888","<p>I am using Gensim python toolkit to build tf-idf model for documents. So I need to create a dictionary for all documents first. However, I found Gensim does not use stemming before creating the dictionary and corpus. Am I right ? </p>
","1971501","","1313757","","2013-01-22 21:12:28","2016-02-25 05:36:03","Is stemming used when gensim creates a dictionary for tf-idf model?","<python><nlp><gensim>","2","0","","","","CC BY-SA 3.0"
"65757921","1","","","2021-01-17 06:26:13","","0","346","<p>I have a function to extract the pre trained embeddings from <code>GloVe.txt</code> and load them as <code>Kears Embedding Layer</code> weights but how can I do for the same for the given two files?</p>
<p><a href=""https://stackoverflow.com/questions/47118678/difference-between-fasttext-vec-and-bin-file"">This accepted stackoverflow answer</a> gave me a a feel that  <strong><code>.vec</code> can be seen as <code>.txt</code></strong> and we <strong>might</strong> use the same technique to extract the <code>fasttext.vec</code> which we use for <code>glove.txt</code>. Is my understanding correct?</p>
<p>I went through a lot of blogs and stack answers to find what to do with the binary file? And I found <a href=""https://stackoverflow.com/questions/27324292/convert-word2vec-bin-file-to-text"">in this stack answer</a> that binary or <code>.bin</code> file is the <strong>MODEL</strong> itself not the embeddings and you can convert the bin file to text file using <code>Gensim</code>. I think it does something to save the embeddings and we can load the pre trained embeddings just like we load <code>Glove</code>. Is my understanding correct?</p>
<p>Here is the code to do that. I want to know if I'm on the right path because I could not find a satisfactory answer to my question anywhere.</p>
<pre><code>     tokenizer.fit_on_texts(data) # tokenizer is Keras Tokenizer()
     vocab_size = len(tokenizer.word_index) + 1 # extra 1 for unknown words
     encoded_docs = tokenizer.texts_to_sequences(data) # data is lists of lists of sentences
     padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')   # max_length is say 30  


     model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) # this will load the binary Word2Vec model
     model.save_word2vec_format('GoogleNews-vectors-negative300.txt', binary=False) # this will save the VECTORS in a text file. Can load it using the below function?


    def load_embeddings(vocab_size,fitted_tokenizer,emb_file_path,emb_dim=300):
        '''
        It can load GloVe.txt for sure. But is it the right way to load paragram.txt, fasttext.vec and word2vec.bin if converted to .txt?
        '''
        embeddings_index = dict()
        f = open(emb_file_path)
        for line in f:
            values = line.split()
            word = values[0]
            coefs = asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs
        f.close()

        embedding_matrix = zeros((vocab_size, emb_dim))
        for word, i in tokenizer.word_index.items():
            embedding_vector = embeddings_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[i] = embedding_vector
                
        return embedding_matrix
</code></pre>
<p>My question is that <strong>Can we load the <code>.vec</code> file directly AND can we load the <code>.bin</code> file as I have described above with the given <code>load_embeddings()</code> function?</strong></p>
","11725056","","","","","2021-01-17 09:55:39","How to use Wiki: Fasttext.vec and Google News: Word2vec.bin pre trained files as weights for Keras Embedding layer","<python><tensorflow><keras><gensim><word-embedding>","1","0","","","","CC BY-SA 4.0"
"30804073","1","","","2015-06-12 13:15:54","","1","529","<p>I'm trying to fit an LDA to a corpus in LDA-C format.  I've got it working for a HDP model but I can't seem to make it work for LDA in gensim.  I'm looking to get the topic probability vector for each document as well as the probability distribution over words for each topic.</p>

<p>Here is the HDP model which works fine</p>

<p>.dat file has the corpus in LDA-C format and .vocab file has unique words</p>

<pre><code>corpus = gensim.corpora.belicorpus.BeliCorpus('ap.dat','ap.vocab')  
d = gensim.corpora.Dictionary()
d.token2id = dict(enumerate(l[:-1] for l in open('ap.vocab')))
hdp = gensim.models.HdpModel(corpus,d.token2id)
alpha, beta = hdp.hdp_to_lda()

# save topic prior
numpy.savetxt(corpus_name+'.alpha',alpha)

# save word distribution for each topic
numpy.savetxt(corpus_name+'.beta',beta)

# save topic distribution for each document in market matrix format
doc_hdp = hdp[corpus]
gensim.corpora.MmCorpus.save_corpus(corpus_name+'.mm',doc_hdp)
</code></pre>

<p>Here is the LDA implementation, I get the proper vectors but I can't seem to find a function that will give me the priors or word distribution/topic:</p>

<pre><code>corpus=gensim.corpora.bleicorpus.BleiCorpus('ap.dat','ap.vocab') 
d = gensim.corpora.Dictionary()
d.token2id = dict(enumerate(l[:-1] for l in open('ap.vocab')))
lda = gensim.models.LdaModel(corpus,num_topics=10,id2word=d.token2id)
</code></pre>
","3834562","","3834562","","2015-06-12 18:59:23","2016-03-03 13:19:01","Fitting LDA to corpus in LDA-C format in gensim","<lda><topic-modeling><gensim>","0","0","","","","CC BY-SA 3.0"
"48606331","1","48614697","","2018-02-04 08:50:48","","2","1090","<p>I am new to natural language processing.
I have a list of blog titles, for example (Not real data, but you get the point):</p>

<pre><code>docs = [""Places to Eat"", ""Places to Visit"", ""Top 10 Things to Do in Singapore""]...
</code></pre>

<p>There are about 3000 over titles and I want to use LDA in Python to generate topics for each of this title. Assuming that I have already cleaned and tokenised these texts using nltk package and removed the stopwords, I will end up with:</p>

<pre><code>texts = [[""places"",""eat""],[""places"",""visit""]]...
</code></pre>

<p>I then proceed to convert these texts into Bag-of-words:</p>

<pre><code>from gensim import corpora, models
dictionary = corpora.Dictionary(texts)

corpus = [dictionary.doc2bow(text) for text in texts]
</code></pre>

<p>Corpus data looks like this:</p>

<pre><code>[(0, 1), (1, 1)]...
</code></pre>

<p>Model creation:</p>

<pre><code>import gensim
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=30, id2word = dictionary, passes=20)
</code></pre>

<p>How do I make use of this model to generate a list of topics - For example ""Eat"", ""Visit"", etc. for each of this titles? I understand that the output might contain probabilities but I would like to string them together with only the text.</p>
","5892703","","","","","2018-02-05 01:33:41","How to generate a topic from a list of titles using LDA (Python)?","<python><nlp><nltk><gensim><lda>","1","3","","","","CC BY-SA 3.0"
"65769069","1","","","2021-01-18 05:03:17","","0","626","<p>I am trying to install python gensim module but getting errors.</p>
<pre><code> Running from numpy source directory.
      /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'define_macros'
        warnings.warn(msg)
      error: Command &quot;clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64 -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c numpy/core/src/multiarray/buffer.c -o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/buffer.o -MMD -MF build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/buffer.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers&quot; failed with exit status 1
      ----------------------------------------
      ERROR: Failed building wheel for numpy
    ERROR: Failed to build one or more wheels
</code></pre>
<pre><code>  distutils.errors.DistutilsError: Command '['/Library/Developer/CommandLineTools/usr/bin/python3', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/var/folders/5d/9599hjpx0tsbp22mcpdsqdc80000gn/T/tmp56vlhe_y', '--quiet', 'numpy&gt;=1.11.3']' returned non-zero exit status 1.
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
</code></pre>
<p>I am not sure why its building numpy</p>
<p>I installed numpy separately using the conda miniforge as mentioned here: <a href=""https://towardsdatascience.com/new-apple-silicon-m1-macbook-air-the-dream-laptop-for-machine-learning-engineers-a1590fbd170f"" rel=""nofollow noreferrer"">https://towardsdatascience.com/new-apple-silicon-m1-macbook-air-the-dream-laptop-for-machine-learning-engineers-a1590fbd170f</a></p>
<p>Can someone help if they have more experience with this?</p>
","15027214","","","","","2021-01-19 19:09:02","can't install gensim on apple M1","<python><numpy><gensim><apple-m1>","1","3","","","","CC BY-SA 4.0"
"48683025","1","","","2018-02-08 10:18:25","","0","359","<p>I have a question regarding concatenating two doc2vec models. I followed the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">official gensim IMDB example</a> on <code>doc2vec</code> and implemented example data.</p>

<p>When concatenating two models (PV-DM + PV-DBOW), as outlined in the original paper, I wondered that the concatenated model appears not to have 200-dim, like the two input models, but 400-dim:</p>

<pre><code>Shape Train(11948, **400**)
Shape Test(2987, **400**)
</code></pre>

<p>The input shapes were each:</p>

<pre><code>np.asarray(X_train).shape)
(11948, **200**)
(2987, **200**)
</code></pre>

<p><strong>Is this correct?</strong> I expected the number of dimensions to be 200 again. </p>
","4697646","","","","","2018-02-08 13:21:14","Concatenating two doc2vec models: Vector dimensions doubled","<machine-learning><concatenation><word2vec><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"39252860","1","41732835","","2016-08-31 14:51:26","","0","223","<p>I'm trying to understand the PV-DM implementation with averaging in gensim.
In the function <code>train_document_dm</code> in <code>doc2vec.py</code> the return value (""errors"") of <code>train_cbow_pair</code> is in the case of averaging (<code>cbow_mean=1</code>) not divided by the number of input vectors (<code>count</code>).
According to this explanation there should be a division by the number of documents in the case of averaging the input vectors: <a href=""http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf"" rel=""nofollow"">word2vec Parameter Learning Explained, equation (23)</a>.
Here is the code from <code>train_document_dm</code>:</p>

<pre class=""lang-py prettyprint-override""><code>l1 = np_sum(word_vectors[word2_indexes], axis=0)+np_sum(doctag_vectors[doctag_indexes], axis=0)  
count = len(word2_indexes) + len(doctag_indexes)  
if model.cbow_mean and count &gt; 1:  
    l1 /= count  
neu1e = train_cbow_pair(model, word, word2_indexes, l1, alpha,
                                learn_vectors=False,  learn_hidden=learn_hidden)  
if not model.cbow_mean and count &gt; 1:  
    neu1e /= count  
if learn_doctags:  
    for i in doctag_indexes:  
        doctag_vectors[i] += neu1e * doctag_locks[i]  
if learn_words:  
    for i in word2_indexes:  
        word_vectors[i] += neu1e * word_locks[i]  
</code></pre>
","4868210","","","","","2017-01-19 02:39:59","updates of the document vectors in doc2vec (PV-DM) in gensim","<python><numpy><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 3.0"
"39294921","1","39295492","","2016-09-02 14:33:13","","0","1279","<p>I'm using <code>gensim</code>'s <code>LdaModel</code>, which, according to the <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow"">documentation</a>, has the parameter <code>random_state</code>. However, I'm getting an error that says:</p>

<pre><code> TypeError: __init__() got an unexpected keyword argument 'random_state'
</code></pre>

<p>Without the <code>random_state</code> parameter, the function works as expected. So, the workflow looks like this for those that want to know what else is happening...</p>

<pre><code>from gensim import corpora, models
import numpy as np

# pseudo code of text pre-processing all on ""comments"" variable
# stop words
# remove punctuation (optional)
# keep alpha only
# stemming
# get bigrams and integrate with corpus (gensim makes this very easy)


dictionary = corpora.Dictionary(comments)
corpus = [dictionary.doc2bow(comm) for comm in comments]
tfidf = models.TfidfModel(corpus) # change weights
corp_tfidf = tfidf[corpus] # apply them to corpus

# set random seed
random_seed = 135
state = np.random.RandomState(random_seed)

# train model
num_topics = 3
lda_mod = models.LdaModel(corp_tfidf, # corpus
                          num_topics=num_topics, # number of topics we want back
                          id2word=dictionary, # our id-word map
                          passes=10, # how many passes to take over the data
                          random_state=state) # reproduce the results
</code></pre>

<p>Which results in the error message above...</p>

<pre><code>TypeError: __init__() got an unexpected keyword argument 'random_state'
</code></pre>

<p>I'd like to be able to recreate my results, if possible.</p>
","5066768","","5066768","","2016-09-02 14:40:45","2016-09-02 15:01:40","LdaModel - random_state parameter not recognized - gensim","<python-3.x><numpy><gensim><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"48635364","1","","","2018-02-06 04:09:16","","0","804","<p>I have read lots of examples regarding doc2vec, but I couldn't find any answer. Like a real example, I want to build a model with doc2vec and then train it with some ML models. after that, how can I get the vector of a raw string with the exact trained Doc2vec model? because I need to predict with my ML model with the same size and logical vector</p>
","6569505","","","","","2018-02-06 22:20:19","load Doc2Vec model and get new sentence's vectors for test","<nlp><word2vec><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"65796905","1","65798065","","2021-01-19 17:55:53","","1","30","<p>Purpose: We are exploring the use of word2vec models in clustering our data. We are looking for the ideal model to fit our needs and have been playing with using (1) existing models offered via Spacy and Gensim (trained on internet data only), (2) creating our own custom models with Gensim (trained on our technical data only) and (3) now looking into creating hybrid models that add our technical data to existing models (trained on internet + our data).</p>
<p>Here is how we created our hybrid model of adding our data to an existing Gensim model:</p>
<pre><code>model = api.load(&quot;word2vec-google-news-300&quot;)
model = Word2Vec(size=300, min_count =1)
model.build_vocab(our_data)
model.train(our_data, total_examples=2, epochs =1)
model.wv.vocab
</code></pre>
<p>Question: Did we do this correctly in terms of our intentions of having a model that is trained on the internet and layered with our data?</p>
<p>Concerns: We are wondering if our data was really added to the model. When using the most similar function, we see really high correlations with more general words with this model. Our custom model has much lower correlations with more technical words. See output below.</p>
<pre><code>Most Similar results for 'Python'

This model (internet + our data):
'technicians' = .99
'system'      = .99
'working'     = .99

Custom model (just our data):
'scripting'   = .65
'perl'        = .63
'julia'       = .58
</code></pre>
","15038798","","","","","2021-01-19 19:18:09","Did we update an existing gemsim model with our own data correctly?","<python><nlp><spacy><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"39296592","1","42389608","","2016-09-02 16:02:43","","1","3313","<p>I will like to plot in a simple vector space graph the similarity between different words. I have calculated them using the model <code>word2vec</code> given by gensim but I cannot find any graphical examples in the literature. My code is as follows:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

import json
import nltk
import re
import pandas


appended_data = []


#for i in range(20014,2016):
#    df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
#    appended_data.append(df0)

for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)


appended_data = pandas.concat(appended_data)
# doc_set = df1.body

doc_set = appended_data.body

## Building the deep learning model
import itertools

sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
sentenized = doc_set.apply(sent_detector.tokenize)
sentences = itertools.chain.from_iterable(sentenized.tolist()) # just to flatten

from gensim.models import word2vec


result = []
for sent in sentences:
    result += [nltk.word_tokenize(sent)]

model = gensim.models.Word2Vec(result)
</code></pre>

<p>In a simple vector space graph, I will like to place the following words: bank, finance, market, property, oil, energy, business and economy. I can easily calculate the similarity of these pairs of words with the function:</p>

<pre><code>model.similarity('bank', 'property')
0.25089364531360675
</code></pre>

<p>Thanks a lot</p>
","5510540","","","","","2017-02-22 11:11:00","Graphical plot of words similarity given by Word2Vec","<python><graph><deep-learning><gensim><word2vec>","1","4","","","","CC BY-SA 3.0"
"65767390","1","65780258","","2021-01-18 00:24:18","","0","156","<p>I am interested in placing a callback on the Gensim word2vec model to trigger some function after each batch. Per <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">documentation</a>, it is possible to place a callback on batch end or epoch end. However, as shown in the MVE below, only the epoch callback actually triggers.</p>
<p>To run the sample, let <code>corpus_filepath</code> direct to a line separated file of unpunctuated sentences (words in a sentence on given a line should be space separated). You may also need to change <code>workers</code> in the <code>Word2Vec</code> instantiation.</p>
<pre><code>from gensim.models import Word2Vec
from gensim.models.callbacks import CallbackAny2Vec

corpus_filepath = 'train.txt'
out_filepath = 'out.txt'

class MyCallback(CallbackAny2Vec):
    def __init__(self):
        pass

    def on_batch_end(self, model):
        print('batch end')

    def on_epoch_end(self, model):
        print('epoch end')


callback = MyCallback()
model = Word2Vec(size=300, window=5, min_count=0, workers=64)
print('Making vocabulary...')
model.build_vocab(corpus_file=corpus_filepath)
print('Beginning training...')
model.train(corpus_file=corpus_filepath, epochs=5, total_words=model.corpus_total_words, callbacks=[callback])
</code></pre>
<p>Incorrect output (missing batch printouts):</p>
<pre><code>Making vocabulary...
Beginning training...
epoch end
epoch end
epoch end
epoch end
epoch end
</code></pre>
<p>What am I doing wrong?</p>
","6772171","","","","","2021-01-18 18:38:55","Gensim word2vec training doesn't callback on batch end","<python><machine-learning><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"48644212","1","","","2018-02-06 13:27:58","","0","345","<p>I am trying to save lda output in a dictionary, in which words and their probabilities will be keys and values and then save this dictionary in json but I don't know how to achieve this. When I simply try to save it in json it is some kind of binary format. Here is the code which I have tried so far:</p>

<pre><code>   filename = sys.argv[1]
    lda = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary,alpha = 0.001, , passes=50,iterations=300)
    for i in range(0, lda.num_topics):
        with open(filename, 'w') as outfile:
            outfile.write(json.dumps('{}\n'.format('Topic #' + str(i + 1) + ': ')))
            for word, prob in lda.show_topic(i, topn=10):
                outfile.write(json.dumps('{}:{}\n'.format(word,prob)))
            outfile.write('\n')
</code></pre>
","3778289","","4762738","","2018-02-06 16:17:37","2018-02-06 16:17:37","How to make dictionary of lda outputs and then save it into json","<python><json><python-3.x><gensim><lda>","0","4","0","","","CC BY-SA 3.0"
"48690415","1","","","2018-02-08 16:35:04","","4","2509","<p>I'm attempting to load some pre-trained vectors into a gensim <code>Word2Vec</code> model, so they can be retrained with new data. My understanding is I can do the retraining with <code>gensim.Word2Vec.train()</code>. However, the only way I can find to load the vectors is with <code>gensim.models.KeyedVectors.load_word2vec_format('path/to/file.bin', binary=True)</code> which creates an object of what is usually the <code>wv</code> attribute of a <code>gensim.Word2Vec</code> model. But this object, on it's own, does not have a <code>train()</code> method, which is what I need to retrain the vectors. </p>

<p>So how do I get these vectors into an actual <code>gensim.Word2Vec</code> model?</p>
","6137760","","712995","","2018-02-08 20:20:43","2018-02-08 20:20:43","Load vectors into gensim Word2Vec model - not KeyedVectors","<machine-learning><nlp><word2vec><gensim><word-embedding>","1","0","","","","CC BY-SA 3.0"
"39570120","1","","","2016-09-19 09:49:46","","1","1151","<p>I'm dealing with topic-modelling of Twitter to define profiles of invidual Twitter users. I'm using Gensim module to generate a LDA model. My question is about choosing good input data. I'd like to generate topics which then I'd assign to specific users. Question is about input data. Now I'm using a supervised method of choosing users from different categories on my own (sports, IT, politics etc) and putting their tweets into the model but it's not very efficient and effective.</p>

<p>What would be a good method for generating meaningful topics of the whole Twitter?</p>
","5666764","","","","","2017-05-17 19:38:06","Generating a good LDA model of Twitter in Python with correct input data","<python><twitter><lda><gensim><topic-modeling>","1","1","1","","","CC BY-SA 3.0"
"48674084","1","","","2018-02-07 22:08:08","","0","567","<p>I am using gensim lda for topic modeling and getting the results like so:</p>

<p>Topic 1: word1 word2 word3 word4</p>

<p>Topic 2: word4 word1 word2 word5</p>

<p>Topic 3: word1 word4 word5 word6</p>

<p>However using mallet on same lda does not produce duplicate words across topics. I have ~20 documents with >1000 words each that I train the lda on. How to get rid of words appearing across multiple topics?</p>
","5421102","","","","","2018-04-10 09:21:58","Words appearing across all topics in lda","<python><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"39247496","1","39357806","","2016-08-31 10:45:53","","0","372","<p>I wanted to output the log-probability during learning of the word and doc vectors in gensim. I have taken a look at the implementation of the score function in the ""slow plain numpy"" version.</p>

<pre class=""lang-py prettyprint-override""><code>def score_cbow_pair(model, word, word2_indices, l1):
    l2a = model.syn1[word.point]  # 2d matrix, codelen x layer1_size
    sgn = (-1.0)**word.code  # ch function, 0-&gt; 1, 1 -&gt; -1
    lprob = -log(1.0 + exp(-sgn*dot(l1, l2a.T)))
    return sum(lprob)
</code></pre>

<p>The score function should make use of the parameters learned during hierarchical softmax training. But in the calculation of the log-probability there is supposed to be a sigmoid function( <a href=""http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf"" rel=""nofollow"">word2vec Parameter Learning Explained equation (45)</a>).
So does gensim really calculate the log-probability in <code>lprob</code> or is it just a score for comparison purposes.</p>

<p>I would have calculated the log-probability as follows:
<code>-log(1.0/(1.0+exp(-sgn*dot(l1, l2a.T))))</code></p>

<p>Is this equation not used because it explodes for values close to zero or is it in general wrong?</p>
","4868210","","1481986","","2016-08-31 11:03:21","2016-09-06 21:03:22","score_cbow_pair in word2vec (gensim)","<python><numpy><probability><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"39549248","1","39662736","","2016-09-17 16:40:54","","11","21787","<p>I want to use a pre-trained <code>word2vec</code> model, but I don't know how to load it in python.</p>

<p>This file is a MODEL file (703 MB).
It can be downloaded here:<br>
<a href=""http://devmount.github.io/GermanWordEmbeddings/"" rel=""noreferrer"">http://devmount.github.io/GermanWordEmbeddings/</a></p>
","6248445","","2145789","","2017-11-29 04:56:30","2021-10-05 09:33:05","How to load a pre-trained Word2vec MODEL File and reuse it?","<python><file><model><word2vec><gensim>","2","0","2","","","CC BY-SA 3.0"
"48661163","1","","","2018-02-07 10:16:11","","0","832","<p>I want to get the similarity of one document to other documents. I use gensim. The program can run correctly, but after some steps it exits with Segmentation fault.</p>

<p>Below is my code:</p>

<pre><code>from gensim import corpora, models, similarities
docs = [['Looking', 'for', 'the', 'meanings', 'of', 'words'],
        ['phrases'],
        ['and', 'expressions'],
        ['We', 'provide', 'hundreds', 'of', 'thousands', 'of', 'definitions'],
        ['synonyms'],
        ['antonyms'],
        ['and', 'pronunciations', 'for', 'English', 'and', 'other', 'languages'],
        ['derived', 'from', 'our', 'language', 'research', 'and', 'expert', 'analysis'],
        ['We', 'also', 'offer', 'a', 'unique', 'set', 'of', 'examples', 'of', 'real', 'usage'],
        ['as', 'well', 'as', 'guides', 'to:']]
dictionary = corpora.Dictionary(docs)
corpus = [dictionary.doc2bow(text) for text in docs]
nf=len(dictionary.dfs)
index = similarities.SparseMatrixSimilarity(corpus, num_features=nf)
phrases = [['This',
            'section',
            'gives',
            'guidelines',
            'on',
            'writing',
            'in',
            'everyday',
            'situations'],
           ['from',
            'applying',
            'for',
            'a',
            'job',
            'to',
            'composing',
            'letters',
            'of',
            'complaint',
            'or',
            'making',
            'an',
            'insurance',
            'claim'],
           ['There',
            'are',
            'plenty',
            'of',
            'sample',
            'documents',
            'to',
            'help',
            'you',
            'get',
            'it',
            'right',
            'every',
            'time'],
           ['create',
            'a',
            'good',
            'impression'],
           ['and',
            'increase',
            'the',
            'likelihood',
            'of',
            'achieving',
            'your',
            'desired',
            'outcome']]
phrase2word=[dictionary.doc2bow(text,allow_update=True) for text in phrases]
sims=index[phrase2word]
</code></pre>

<p>It can run normally until get sims, but it cannot get sims, and using <code>gdb</code> gets the following info:</p>

<blockquote>
  <p>Program received signal SIGSEGV, Segmentation fault.
  0x00007fffd881d809 in csr_tocsc (n_row=5, n_col=39,
  Ap=0x4a4eb10, Aj=0x9fc6ec0, Ax=0x1be4a00, Bp=0xa15f6a0, Bi=0x9f3ee80,
  Bx=0x9f85f60) at scipy/sparse/sparsetools/csr.h:411 411<br>
  scipy/sparse/sparsetools/csr.h: Ê≤°ÊúâÈÇ£‰∏™Êñá‰ª∂ÊàñÁõÆÂΩï.</p>
</blockquote>
","9326542","","9326542","","2018-02-08 06:30:03","2018-02-09 05:36:37","gensim.similarities.SparseMatrixSimilarity get segmentation-fault","<python><c++><segmentation-fault><gensim>","1","1","","","","CC BY-SA 3.0"
"39615420","1","39715845","","2016-09-21 11:33:05","","2","1905","<p>In gensim, when I give a string as input for training doc2vec model,  I get this error : </p>

<blockquote>
  <p>TypeError('don\'t know how to handle uri %s' % repr(uri))</p>
</blockquote>

<p>I referred to this question <a href=""https://stackoverflow.com/questions/36780138/doc2vec-taggedlinedocument"">Doc2vec : TaggedLineDocument()</a>
but still have a doubt about the input format. </p>

<p><code>documents = TaggedLineDocument('myfile.txt')</code></p>

<p>Should the myFile.txt have tokens as list of lists or separate list in each line for each document or a string? </p>

<p><code>For eg</code> - I have 2 documents.</p>

<p>Doc 1 : Machine learning is a subfield of computer science that evolved from the study of pattern recognition.</p>

<p>Doc 2 :  Arthur Samuel defined machine learning as a ""Field of study that gives computers the ability to learn"".</p>

<p>So, what should the <code>myFile.txt</code> look like?</p>

<p>Case 1 : simple text of each document in each line</p>

<p>Machine learning is a subfield of computer science that evolved from the study of pattern recognition</p>

<p>Arthur Samuel defined machine learning as a Field of study that gives computers the ability to learn</p>

<p>Case 2 : a list of lists having tokens of each document</p>

<p><code>[ [""Machine"", ""learning"", ""is"", ""a"", ""subfield"", ""of"", ""computer"", ""science"", ""that"", ""evolved"", ""from"", ""the"", ""study"", ""of"", ""pattern"", ""recognition""]</code>,</p>

<pre><code>[""Arthur"", ""Samuel"", ""defined"", ""machine"", ""learning"", ""as"", ""a"", ""Field"", ""of"", ""study"", ""that"", ""gives"", ""computers"" ,""the"", ""ability"", ""to"", ""learn""] ]
</code></pre>

<p>Case 3 : list of tokens of each document in a separate line</p>

<pre><code>[""Machine"", ""learning"", ""is"", ""a"", ""subfield"", ""of"", ""computer"", ""science"", ""that"", ""evolved"", ""from"", ""the"", ""study"", ""of"", ""pattern"", ""recognition""]

[""Arthur"", ""Samuel"", ""defined"", ""machine"", ""learning"", ""as"", ""a"", ""Field"", ""of"", ""study"", ""that"", ""gives"", ""computers"" ,""the"", ""ability"", ""to"", ""learn""]
</code></pre>

<p>And when I am running it on the test data, what should be the format of the sentence which i want to predict the doc vector for? Should it be like case 1 or case 2 below or something else?</p>

<p><code>model.infer_vector(testSentence, alpha=start_alpha, steps=infer_epoch)</code></p>

<p>Should the testSentence be :</p>

<p>Case 1 : string</p>

<pre><code>testSentence = ""Machine learning is an evolving field""
</code></pre>

<p>Case 2 : list of tokens</p>

<pre><code>testSentence = [""Machine"", ""learning"", ""is"", ""an"", ""evolving"", ""field""]
</code></pre>
","4915693","","4915693","","2017-12-20 11:55:30","2017-12-20 11:55:30","doc2vec - Input Format for doc2vec training and infer_vector() in python","<python><gensim><word2vec><doc2vec>","1","0","1","","","CC BY-SA 3.0"
"39615436","1","39651384","","2016-09-21 11:33:45","","0","2003","<p>When using LDA model, I get different topics each time and I want to replicate the same set. I have searched for the similar question in Google such as <a href=""https://groups.google.com/forum/#!topic/gensim/s1EiOUsqT8s"" rel=""nofollow"">this</a>.</p>

<p>I fix the seed as shown in the article by <code>num.random.seed(1000)</code> but it doesn't work. I read the <code>ldamodel.py</code> and find the code below:</p>

<pre><code>def get_random_state(seed):

    """"""
    Turn seed into a np.random.RandomState instance.
    Method originally from maciejkula/glove-python, and written by @joshloyal
    """"""
     if seed is None or seed is numpy.random:
         return numpy.random.mtrand._rand
     if isinstance(seed, (numbers.Integral, numpy.integer)):
         return numpy.random.RandomState(seed)
     if isinstance(seed, numpy.random.RandomState):
        return seed
     raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
                      ' instance' % seed)
</code></pre>

<p>So I use the code:</p>

<pre><code>lda = models.LdaModel(
    corpus_tfidf,
    id2word=dic,
    num_topics=2,
    random_state=numpy.random.RandomState(10)
)
</code></pre>

<p>But it's still not working.</p>
","6858032","","3714940","","2016-09-21 11:55:49","2018-06-25 05:55:06","Fails to fix the seed value in LDA model in gensim","<python><numpy><gensim>","2","1","1","","","CC BY-SA 3.0"
"56478384","1","56482529","","2019-06-06 13:14:00","","2","750","<p>Doc2vec while creating the vocabulary has possibility to put minimum occurence of the word in documents to be included in vocabulary as parameter <code>min_count</code>.</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=3, epochs=100,workers=8)
</code></pre>

<p>How is it possible to exclude words which appear far too often, with some parameter?</p>

<p>I know that one way is to do this in preprocessing step by manually deleting those words, and counting each, but would be nice to know if there is maybe some built in method to do so, as it gives more space for testing.
Many thanks for the answer.</p>
","8245211","","130288","","2019-06-06 17:23:24","2019-06-06 17:34:37","How to put maximum vocabulary frequency in doc2vec","<python><gensim><word2vec><doc2vec>","1","4","","","","CC BY-SA 4.0"
"31384947","1","31401184","","2015-07-13 13:38:52","","0","509","<p>I am trying to install <code>gensim</code> lib on Ubuntu using:</p>

<pre><code>pip install --upgrade gensim
</code></pre>

<p>However, I got an error like this:</p>

<pre><code>Requirement already up-to-date: gensim in /usr/local/lib/python3.4/dist-packages/gensim-0.12.0-py3.4-linux-x86_64.egg
Collecting numpy&gt;=1.3 (from gensim)
Downloading numpy-1.9.2.tar.gz (4.0MB)
100% |################################| 4.0MB 146kB/s 
Collecting scipy&gt;=0.7.0 (from gensim)
Downloading scipy-0.15.1.tar.gz (11.4MB)
100% |################################| 11.4MB 55kB/s
Requirement already up-to-date: six&gt;=1.2.0 in /usr/lib/python3/dist-packages (from gensim)
Collecting smart-open&gt;=1.2.1 (from gensim)
Downloading smart_open-1.2.1.tar.gz
Complete output from command python setup.py egg_info:
File ""/tmp/pip-build-_nbem_oq/smart-open/setup.py"", line 28, in &lt;module&gt;
    long_description = read('README.rst'),
  File ""/tmp/pip-build-_nbem_oq/smart-open/setup.py"", line 21, in read
    return open(os.path.join(os.path.dirname(__file__), fname)).read()
  File ""/usr/lib/python3.4/encodings/ascii.py"", line 26, in decode
    return codecs.ascii_decode(input, self.errors)[0]
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc5 in position 4344: ordinal not in range(128)
----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-_nbem_oq/smart-open
</code></pre>

<p>Does anyone know how to fix it? </p>
","1300344","","","","","2015-07-14 08:08:47","Getting UnicodeDecodeError when installing gensim on Ubuntu","<ubuntu><pip><gensim>","1","0","","","","CC BY-SA 3.0"
"39252207","1","","","2016-08-31 14:21:26","","2","2259","<p>With Doc2Vec modelling, I have trained a model and saved following files:</p>

<pre><code>1. model
2. model.docvecs.doctag_syn0.npy
3. model.syn0.npy
4. model.syn1.npy
5. model.syn1neg.npy
</code></pre>

<p>However, I have a new way to label the documents and want to train the model again. since the word vectors already obtained from previous version. Is there any way to reuse that model (e.g., taking the previous w2v results as initial vectors for training)? Any one know how to do it? </p>
","4373432","","6573902","","2021-01-25 15:02:20","2021-01-25 15:02:20","Gensim: how to retrain doc2vec model using previous word2vec model","<python><nlp><gensim><word2vec><doc2vec>","1","0","1","","","CC BY-SA 3.0"
"39969919","1","54268813","","2016-10-11 03:07:18","","9","2014","<p>I am hoping to assign each document to one topic using LDA. Now I realise that what you get is a distribution over topics from LDA. However as you see from the last line below I assign it to the most probable topic.</p>

<p>My question is this. I have to run <code>lda[corpus]</code> for somewhat the second time in order to get these topics. Is there some other builtin gensim function that will give me this topic assignment vectors directly? Especially since the LDA algorithm has passed through the documents it might have saved these topic assignments?</p>

<pre class=""lang-py prettyprint-override""><code>    # Get the Dictionary and BoW of the corpus after some stemming/ cleansing
    texts = [[stem(word) for word in document.split() if word not in STOPWORDS] for document in cleanDF.text.values]
    dictionary = corpora.Dictionary(texts)
    dictionary.filter_extremes(no_below=5, no_above=0.9)
    corpus = [dictionary.doc2bow(text) for text in texts]

    # The actual LDA component
    lda = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=30, chunksize=10000, passes=10,workers=4) 

    # Assign each document to most prevalent topic
    lda_topic_assignment = [max(p,key=lambda item: item[1]) for p in lda[corpus]]
</code></pre>
","2530674","","2530674","","2020-04-20 03:31:54","2020-10-20 09:16:32","Gensim LDA topic assignment","<gensim><lda><topic-modeling>","2","0","1","","","CC BY-SA 4.0"
"48017343","1","48029141","","2017-12-29 04:18:46","","3","2360","<p>I have a Word2Vec model which was trained on a huge corpus. While using this model for Neural network application I came across quite a few ""Out of Vocabulary"" words. Now I need to find word embeddings for these ""Out of Vocabulary"" words. So I did some googling and found that Facebook has recently released a FastText library for this. Now my question is how can I convert my existing word2vec model or Keyedvectors to FastText model?</p>
","5059870","","","","","2018-05-17 09:53:40","How to convert gensim Word2Vec model to FastText model?","<nlp><word2vec><gensim><word-embedding><fasttext>","2","0","1","","","CC BY-SA 3.0"
"39275547","1","39277615","","2016-09-01 15:27:12","","3","3798","<p>I will like to analyze my first deep learning model using Python and in order to do so I have to first split my corpus (8807 articles) into sentences. My corpus is built as follows:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

import json
import nltk
import re
import pandas


appended_data = []


#for i in range(20014,2016):
#    df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
#    appended_data.append(df0)

for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)


appended_data = pandas.concat(appended_data)
# doc_set = df1.body

doc_set = appended_data.body
</code></pre>

<p>I am trying to use the function <code>Word2Vec.load_word2vec_format</code> from the library <code>gensim.models</code> but I have to first split my corpus (<code>doc_set</code>) into sentences.</p>

<pre><code>from gensim.models import word2vec
model = Word2Vec.load_word2vec_format(doc_set, binary=False)
</code></pre>

<p>Any recommendations? </p>

<p>cheers</p>
","5510540","","","","","2016-09-01 17:29:38","Tokenizing a corpus composed of articles into sentences Python","<python><deep-learning><gensim><word2vec>","1","3","1","","","CC BY-SA 3.0"
"39552088","1","","","2016-09-17 21:54:26","","8","5717","<p>Is it possible to choose between the <code>Skip-gram</code> and the <code>CBOW</code> model in <em>Gensim</em> when training a <em>Word2Vec</em> model?</p>
","5036074","","8239061","","2020-10-13 21:33:47","2020-10-13 21:33:47","Select between skip-gram and CBOW model for training word2Vec in gensim","<nlp><gensim><word2vec>","1","0","2","","","CC BY-SA 4.0"
"39657215","1","39718081","","2016-09-23 09:24:33","","5","2284","<p>The Code is in python. I loaded up the binary model into gensim on python, &amp; used the ""init_sims"" option to make the execution faster. The OS is OS X.
It takes almost 50-60 seconds to load it up. And an equivalent time to find ""most_similar"". Is this normal? Before using the init_sims option, it took almost double the time! I have a feeling it might be an OS RAM allocation issue.</p>

<pre><code>model=Word2Vec.load_word2vec_format('GoogleNewsvectorsnegative300.bin',binary=True)
model.init_sims(replace=True)
model.save('SmallerFile')
#MODEL SAVED INTO SMALLERFILE &amp; NEXT LOAD FROM IT
model=Word2Vec.load('SmallerFile',mmap='r')
#GIVE RESULT SER!
print model.most_similar(positive=['woman', 'king'], negative=['man'])
</code></pre>
","5030608","","","","","2016-09-27 07:01:49","Word2Vec: Using Gensim and Google-News dataset- Very Slow Execution Time","<python><gensim><word2vec>","1","1","3","","","CC BY-SA 3.0"
"48703067","1","48725677","","2018-02-09 09:53:11","","3","7806","<ol>
<li>Build_vocab extend my old vocabulary? </li>
</ol>

<p>For example, my idea is when I use doc2vec(s) to train a model, it just builds the vocabulary from the datasets.  If I want to extend it, I need to use build_vocab()</p>

<ol start=""2"">
<li>Where should I use it?  Should I put it after ""gensim.doc2vec()""?  </li>
</ol>

<p>For example:</p>

<pre><code>sentences = gensim.models.doc2vec.TaggedLineDocument(f_path)
dm_model = gensim.models.doc2vec.Doc2Vec(sentences, dm=1, size=300, window=8, min_count=5, workers=4)
dm_model.build_vocab()
</code></pre>
","8026780","","3661748","","2020-01-28 20:23:07","2020-01-28 20:23:07","how to use build_vocab in gensim?","<nlp><word2vec><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"65817456","1","65822091","","2021-01-20 20:49:55","","1","333","<p>Why am I getting same set of topics # words in gensim lda model? I used these parameters. I checked there are no duplicate documents in my corpus.</p>
<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus=MY_CORPUS,
                                           id2word=WORD_AND_ID,
                                           num_topics=4, 
                                           minimum_probability=minimum_probability,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto', # symmetric, asymmetric
                                           per_word_topics=True)
</code></pre>
<h1>Results</h1>
<pre><code>[
(0, '0.004*lily + 0.01*rose + 0.00*jasmine'),
(1, '0.005*geometry + 0.07*algebra + 0.01*calculation'),
(2, '0.003*painting + 0.001*brush + 0.01*colors'),
(3, '0.005*geometry + 0.07*algebra + 0.01*calculation')
]
</code></pre>
<p>Notice: Topic #1 and #3 are identical.</p>
","4150078","","6573902","","2021-01-21 06:21:09","2021-01-21 06:21:09","LDA: topic model gensim gives same set of topics","<python><nlp><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"39738327","1","39742604","","2016-09-28 04:54:24","","1","186","<p>I am trying to make a Apple Siri-like application in python in which you give it vocal commands or questions through a microphone, it determines the text version of the inputted audio, and then determines the appropriate action to take based on the meaning of the command/question. I am going to be using the Speech Recognition library to accept microphone input and convert from speech to text (via the IBM Watson Speech to Text API).</p>

<p>The main problem I have with it right now is that when I define an action for the app to execute when the appropriate command is given/question is asked, I don't know how to determine if the said command/question is denoting that action. Let me clarify what I mean by that with an example:</p>

<p>Say we have a action called <code>hello</code>. There are multiple ways for somebody to say ""hello"" to another person (or in this case, my application), such as:</p>

<ul>
<li>""Hello""</li>
<li>""Hi""</li>
<li>""Howdy""</li>
<li>...Etcetera...</li>
</ul>

<p>Of course, I want all of these ways of saying ""hello"" to be classified under the action of <code>hello</code>. That is, when someone says ""hello"", ""hi"", or ""howdy"", the response for the action <code>hello</code> should be executed (most likely just the app saying ""hello"" back in this case).</p>

<p>My first thought on how to solve this was to supply the app with all of or the most common ways to say a certain command/question. So, if I follow the previous example, I would tell the computer that ""hello"", ""hi"", and ""howdy"" all meant the same thing: the <code>hello</code> action. However, this method has a couple flaws. First off, it simply wouldn't understand ways of saying ""hello"" that weren't hardcoded in, such as ""hey"". Second off, once the responses for new commands/questions start getting coded in, it would become very tedious entering all the ways to say a certain phrase.</p>

<p>So then, because of the aforementioned problems, I started looking into ways to calculate the similarities between a group of sentences, and a single query. I eventually came across the Gensim library for python. I looked into it and found some very promising information on complex processes such as latent semantic indexing/analysis (LSI/LSA) and Tf-idf. However, it seemed to me like these things were mainly for comparing documents with large word counts as they rely on the frequency of certain terms. Assuming this is true, these processes wouldn't really provide me with accurate results as the commands/questions given to my app will probably be about eight words on average. I could be completely wrong, after all I know very little about these processes.</p>

<p>I also discovered WordNet, and how to work with it in python using the Natural Language Toolkit (NLTK). It looks like it could be useful, but I'm not sure how.</p>

<p>So, finally, I guess my real question here is what would be the best solution to the problem I've mentioned? Should I use one of the methods I've mentioned? Or is there a better way to do what I want that I don't know about?</p>

<p>Any help at all would be greatly appreciated. Thanks in advance.</p>

<p>P.S. Sorry for the wordy explanation; I wanted to be sure I was clear :P</p>
","5017282","","","","","2016-09-28 09:38:24","Siri-like app: calculating similarities between a query and a predefined set of control phrases","<python><nlp><nltk><wordnet><gensim>","1","0","","","","CC BY-SA 3.0"
"31350481","1","32756797","","2015-07-10 21:21:53","","1","185","<p>I'd like to use gensim's Python wrapper for Dynamic Topic Models. Essentially, it is a topic modeling approach that slices the corpus by date (i.e. years) and looks at how topics evolve over time. However, I am finding nothing online that specifies how <a href=""https://radimrehurek.com/gensim/models/dtmmodel.html#gensim.models.dtmmodel.DtmModel.ftimeslices"" rel=""nofollow"">my_timeslices</a> should be formatted. Does anyone have an example of a file and/or preparation?</p>
","1721444","","","","","2020-07-27 06:50:07","Gensim - Timeslice Data Format?","<python><python-2.7><gensim>","1","2","","","","CC BY-SA 3.0"
"56508631","1","","","2019-06-08 17:25:40","","1","1007","<p>I am trying to create a W2V model and then generate train and test data to be used for my model.My question is how can I generate test data after I am done with creating a W2V model with my train data.</p>
","7386656","","","","","2019-06-09 18:46:13","Creating train,test data for Word2Vec model","<python><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"39558642","1","","","2016-09-18 14:20:48","","20","15975","<p>Using Word2vec and Doc2vec methods provided by Gensim, they have a distributed version which uses BLAS, ATLAS, etc to speedup (details <a href=""http://rare-technologies.com/word2vec-in-python-part-two-optimizing/"" rel=""noreferrer"">here</a>). However, is it supporting GPU mode? Is it possible to get GPU working if using Gensim?</p>
","4250174","","","","","2019-07-13 00:33:28","Does Gensim library support GPU acceleration?","<optimization><gpu><gensim><deeplearning4j>","1","1","3","","","CC BY-SA 3.0"
"39580232","1","","","2016-09-19 18:57:36","","4","2228","<p>I have trained paragraph vectors for around 2300 paragraphs(between 2000-12000 words each) each with vector size of 300. Now, I need to infer paragraph vectors of around 100,000 sentences which I have considered as paragraphs(each sentence is around 10-30 words each corresponding to the earlier 2300 paragraphs already trained).</p>

<p>So, am using </p>

<p><code>model.infer_vector(sentence)</code></p>

<p>But, the problem is it is taking too long, and it does not take any arguments such as ""<code>workers</code>"" .! Is there a way I can speed up the process by threading or some other way? I am using a machine with 8gb ram and when I checked the available cores using</p>

<pre><code>cores = multiprocessing.cpu_count()
</code></pre>

<p>it comes out to be 8.</p>

<p>I need this for answering multiple choice questions. Also, are there any other libraries/models such as <code>doc2vec</code> which can help in this task?</p>

<p>Thanks in advance for your time.</p>
","4915693","","4915693","","2017-12-20 11:56:10","2020-05-31 17:48:15","doc2vec - How to infer vectors of documents faster?","<python><gensim><word2vec><doc2vec>","2","0","1","","","CC BY-SA 3.0"
"39644667","1","50287612","","2016-09-22 16:48:13","","4","5149","<p>I will like to know more about whether or not there are any rule to set the hyper-parameters alpha and theta in the LDA model. I run an LDA model given by the library <code>gensim</code>:</p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=30, id2word = dictionary, passes=50, minimum_probability=0)
</code></pre>

<p>But I have my doubts on the specification of the hyper-parameters. From what I red in the library documentation, both hyper-parameters are set to 1/number of topics. Given that my model has 30 topics, both hyper-parameters are set to a common value 1/30. I am running the model in news-articles that describe the economic activity. For this reason, I expect that the document-topic distribution (theta) to be high (similar topics in documents),while the topic-word distribution (alpha) be high as well (topics sharing many words in common, or, words not being so exclusive for each topic). For this reason, and given that my understanding of the hyper-parameters is correct, is 1/30 a correct specification value?</p>
","5510540","","","","","2018-05-11 07:54:50","Rules to set hyper-parameters alpha and theta in LDA model","<lda><gensim>","1","0","1","","","CC BY-SA 3.0"
"56563339","1","","","2019-06-12 13:24:12","","0","353","<p>I tried to use <code>gensim.downloader</code> to download <code>word2vec-google-news-300</code>, but my network isn't very reliable, so I downloaded <code>word2vec-google-news-300.gz</code> and <code>__init__.py</code> from github and put them into <code>~/gensim-data/word2vec-google-news-300/</code>. </p>

<p>But when I use <code>api.load(""word2vec-google-news-300"")</code> to load this model, I received error like this:</p>

<blockquote>
  <p>AttributeError: module 'word2vec-google-news-300' has no attribute 'load_data'</p>
</blockquote>

<p>My code:</p>

<pre class=""lang-py prettyprint-override""><code>import gensim.downloader as api
model = api.load(""word2vec-google-news-300"")
</code></pre>
","11637049","","7106850","","2019-06-12 13:36:59","2020-03-16 15:50:49","module 'word2vec-google-news-300' has no attribute 'load_data'","<python-3.x><gensim>","2","0","0","","","CC BY-SA 4.0"
"57244699","1","57245511","","2019-07-28 20:25:02","","0","113","<p>I am trying to learn word2vec.</p>

<p>I am using the code below to load the Google pre-trained word2vec model in Python 3. But I am unsure how to turn a list such as :[""I"", ""ate"", ""apple""] to a list of vectors (ie how to get vectors from this model?).</p>

<pre><code>import nltk
import gensim

# Load Google's pre-trained Word2Vec model.
model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>
","8853131","","","","","2019-07-28 22:46:18","How to turn a list of words into a list of vectors using a pre-trained word2vec model(Google)?","<python-3.x><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"56553753","1","56555646","","2019-06-12 02:07:03","","0","518","<p>I am using Doc2Vec to analysis some paragraph and wish to get deterministic vector representation of the train data. Based on the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">official documentation</a>, it seems that I need to set the parameters ""seed"" and ""workers"", as well as the PYTHONHASHSEED environment variable in Python 3. Therefore, I wrote the script as follows.</p>

<pre class=""lang-py prettyprint-override""><code>import os
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec


def main():
    # Check whether the environment variable has been set successfully
    print(os.environ.get('PYTHONHASHSEED'))

    docs = [TaggedDocument(['Apple', 'round', 'apple', 'red', 'Apple', 'juicy', 'apple', 'sweet'], ['A']),
            TaggedDocument(['I', 'have', 'a', 'little', 'frog', 'His', 'name', 'is', 'Tiny', 'Tim'], ['B']),
            TaggedDocument(['On', 'top', 'of', 'spaghetti', 'all', 'covered', 'with', 'cheese'], ['C'])]

    # Loop 3 times to check whether consistent results are produced within each run
    for i in range(3):
        model = Doc2Vec(min_count=1, seed=12345, workers=1)
        model.build_vocab(docs)
        model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)
        print(model.docvecs['B'])


if __name__ == '__main__':
    os.environ['PYTHONHASHSEED'] = '12345'
    main()
</code></pre>

<p>The problem is that within each run it does produce deterministic results, but when I run the whole script again it gives different results. Is there any problem with my environment variable setting, or am I missing out something else?</p>

<p>I am on Python 3.6.5.</p>
","10561864","","10561864","","2019-06-12 02:12:08","2021-06-24 04:05:40","How to get deterministic train results in Doc2Vec?","<python><gensim><doc2vec>","2","0","","","","CC BY-SA 4.0"
"39806859","1","","","2016-10-01 12:55:42","","0","544","<p>I am attempting to build a model that will attempt to identify the interest category / topic of supplied text. For example:</p>

<blockquote>
  <p>Shop for Bridal Wedding Sarees from our exhausting variety of beautiful and designer sarees. Get great deals, quality stitching and
  Free International delivery.</p>
</blockquote>

<p>would resolve to a top level category like:</p>

<blockquote>
  <p>Fashion or Wedding Fashion</p>
</blockquote>

<p>To acheive this, I have used Latent Dirichlet allocation (LDA) which is a topic model that generates topics based on word frequency from a set of documents. </p>

<p>So I got topics of document as below but don't find way to map them to human understandable format</p>

<blockquote>
  <p>topic #0 (0.500): 0.100*sare + 0.060*intern + 0.060*get + 0.060*deal +
  0.060*exhaust + 0.060*design + 0.060*free + 0.060*qualiti + 0.060*shop + 0.060*great</p>
  
  <p>topic #1 (0.500): 0.063*sare + 0.063*beauti + 0.063*deliveri +
  0.063*stitch + 0.063*varieti + 0.063*wed + 0.062*bridal + 0.062*great + 0.062*shop + 0.062*qualiti</p>
</blockquote>

<p>I have used this <a href=""https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"" rel=""nofollow"">script</a> to implement above things.</p>

<p>So the Question is How to map above identified topics to human readable category like Fashion?</p>
","1495125","","1495125","","2016-10-03 04:39:56","2018-02-01 20:44:56","Identifying interest / topic from text","<python><nltk><lda><gensim><nltk-trainer>","0","11","","","","CC BY-SA 3.0"
"48705138","1","48706010","","2018-02-09 11:47:28","","4","264","<p>I noticed that when adding documents to a gensim Dictionary, execution time jumps from 0.2s to more than 6s when reaching 2 million words.</p>

<p>The code below is a quick example. I loop through int and add the number to the dictionary at each iteraion.</p>

<pre><code>from gensim import corpora
import time



dict_transcript = corpora.Dictionary()


for i in range(1,10000000):

    start_time = time.time()

    doc = [str(i)]

    dict_transcript.add_documents([doc])

    print(""Iter ""+str(i)+"" done in "" + str(time.time() - start_time) + ' w/ '+str(len(doc)) + ' words and dico size ' +
          str(len(dict_transcript)))
</code></pre>

<p>I do get the following output when reaching 2 million words:</p>

<pre><code>Iter 1999999 done in 0.0 w/ 1 words and dico size 1999999
Iter 2000000 done in 0.0 w/ 1 words and dico size 2000000
Iter 2000001 done in 0.0 w/ 1 words and dico size 2000001
Iter 2000002 done in 7.940511226654053 w/ 1 words and dico size 2000001
</code></pre>

<p>Is there any reason why? And does anyone know how to bypass that problem?
I'm using this dictionary on a big corpus that I tokenize into bigrams so I'm expecting the dictionary to be a few million rows.</p>

<p>Many thanks</p>
","5682776","","5682776","","2018-02-09 12:10:13","2018-02-09 12:36:39","Why adding documents to gensim Dictionary gets slow when reaching 2 million words?","<python><dictionary><nlp><gensim>","1","0","","","","CC BY-SA 3.0"
"56599306","1","56599395","","2019-06-14 13:33:49","","1","481","<p>So I want to use word-embeddings in order to get some handy dandy cosine similarity values. After creating the model and checking for similarity of the word ""not"" (which is in the data I give the model) it tells me that the word is not in the vocabulary.</p>

<p>Why can't it find the similarity for the word 'not'?</p>

<p>the description data looks as follows:<br>
[['not', 'only', 'do', 'angles', 'make', 'joints', 'stronger', 'they', 'also', 'provide', 'more', 'consistent', 'straight', 'corners', 'simpson', 'strongtie', 'offers', 'a', 'wide', 'variety', 'of', 'angles', 'in', 'various', 'sizes', 'and', 'thicknesses', 'to', 'handle', 'lightduty', 'jobs', 'or', 'projects', 'where', 'a', 'structural', 'connection', 'is', 'needed', 'some', 'can', 'be', 'bent', 'skewed', 'to', 'match', 'the', 'project', 'for', 'outdoor', 'projects', 'or', 'those', 'where', 'moisture', 'is', 'present', 'use', 'our', 'zmax', 'zinccoated', 'connectors', 'which', 'provide', 'extra', 'resistance', 'against', 'corrosion', 'look', 'for', 'a', 'z', 'at', 'the', 'end', 'of', 'the', 'model', 'numberversatile', 'connector', 'for', 'various', 'connections', 'and', 'home', 'repair', 'projectsstronger', 'than', 'angled', 'nailing', 'or', 'screw', 'fastening', 'alonehelp', 'ensure', 'joints', 'are', 'consistently', 'straight', 'and', 'strongdimensions', 'in', 'x', 'in', 'x', 'inmade', 'from', 'gauge', 'steelgalvanized', 'for', 'extra', 'corrosion', 'resistanceinstall', 'with', 'd', 'common', 'nails', 'or', 'x', 'in', 'strongdrive', 'sd', 'screws']]</p>

<p>Note that I've already tried to give the data as separate sentences instead of separate words.</p>

<pre><code>def word_vec_sim_sum(row):
    description = row.product_description.split()
    description_embedding = gensim.models.Word2Vec([description], size=150,
        window=10,
        min_count=2,
        workers=10,
        iter=10)       
    print(description_embedding.wv.most_similar(positive=""not""))
</code></pre>
","8526970","","1752496","","2019-06-14 13:38:37","2019-06-14 13:39:52","word not in vocabulary after training gensim word2vec model, why?","<python><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"56605373","1","56611764","","2019-06-14 21:32:01","","0","193","<p>The question is two-fold:
1.  How to select the ideal value for <code>size</code>?
2.  How to get the vocabulary size dynamically (per row as I intend) to set that ideal size? </p>

<p>My data looks like the following (example)‚Äîjust one row and one column:</p>

<p>Row 1</p>

<pre><code>{kfhahf}    
Lfhslnf;
.
.
. 
</code></pre>

<p>Row 2</p>

<pre><code>(stdgff  ksshu, hsihf)
asgasf;
.
.
. 
</code></pre>

<p>Etc.</p>

<p>Based on this post: <a href=""https://stackoverflow.com/questions/45444964/python-what-is-the-size-parameter-in-gensim-word2vec-model-class"">Python: What is the &quot;size&quot; parameter in Gensim Word2vec model class</a> The <code>size</code> parameter should be less than (or equal to?) the vocabulary size. So, I am trying to dynamically assign the size as following:</p>

<pre><code>from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

# I do Word2Vec for each row
For item in dataset:
    Tokenized = word_tokenize(item)
    model = Word2Vec([Tokenized], min_count=1)
</code></pre>

<p>I get the vocabulary size here. So I create a second model:</p>

<pre><code>model1 = Word2Vec([Tokenized], min_count=1, size=len(model.wv.vocab))
</code></pre>

<p>This sets the <code>size</code> value to the current vocab value of the current row, as I intended. But is it the right way to do? What is the right size for a small vocabulary text?</p>
","5405358","","","","","2019-06-15 15:33:06","How to dynamically assign the right ""size"" for Word2Vec?","<python><python-3.x><nltk><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"39781812","1","39940007","","2016-09-30 00:09:37","","14","6597","<p>I am trying to use Gensim's Word2Vec implementation. Gensim warns that if you don't have a C compiler, the training will be 70% slower.  Is there away to verify that Gensim is correctly using the C Compiler I have installed?</p>

<p>I am using Anaconda Python 3.5 on Windows 10.</p>
","6516892","","6516892","","2016-10-12 02:57:51","2016-10-12 02:57:51","How can I tell if Gensim Word2Vec is using the C compiler?","<python><compilation><installation><gensim><word2vec>","2","2","1","","","CC BY-SA 3.0"
"49142365","1","","","2018-03-07 00:46:23","","0","2007","<p>I have built a gensim Doc2vec model. Let's call it doc2vec. Now I want to find the most relevant words to a given document according to my doc2vec model. </p>

<p>For example, I have a document about ""java"" with the tag ""doc_about_java"". When I ask for similar documents, I get documents about other programming languages and topics related to java. So my document model works well.</p>

<p>Now I want to find the most relevant words to ""doc_about_java"".</p>

<p>I follow the solution from the closed question <a href=""https://stackoverflow.com/questions/46047506/how-to-find-most-similar-terms-words-of-a-document-in-doc2vec"">How to find most similar terms/words of a document in doc2vec?</a> and it gives me seemingly random words, the word ""java"" is not even among the first 100 similar words:</p>

<pre><code>docvec = doc2vec.docvecs['doc_about_java']
print doc2vec.most_similar(positive=[docvec], topn=100)
</code></pre>

<p>I also tried like this:</p>

<pre><code>print doc2vec.wv.similar_by_vector(doc2vec[""doc_about_java""])
</code></pre>

<p>but it didn't change anything. How can I find the most similar words to a given document?</p>
","1372785","","1372785","","2018-03-07 00:56:19","2018-03-19 18:35:21","How to get most similar words to a document in gensim doc2vec?","<word2vec><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"31800674","1","","","2015-08-04 04:23:15","","1","693","<p>It is my first time to use gensim package to run LDA model, and the problem happened as followed:</p>

<p>I had train and save lda model to the local file 'lda.model' last night,   and when I try to usinfer topic distributions on new, unseen documents, with run() funciton:</p>

<pre><code>self.lda = models.LdaMulticore.load('./lda_model/lda.model')
print self.lda[self.corpus_tfidf]
</code></pre>

<p>then Error happend!</p>

<pre><code>AttributeError: 'LdaMulticore' object has no attribute 'minimum_probability'
</code></pre>

<p>The following is my source code,Could you help me?</p>

<pre><code># -*- coding: utf-8 -*-
#@author: chenbjin
#@time:  2015-08-3
import jieba, os, logging
from base_func import *
from gensim import corpora, models, similarities

class LDAModel(object):
    """"""docstring for LDAModel""""""
    def __init__(self, train_data='./data/train_set.txt'):
        super(LDAModel, self).__init__()
        self.dic = None
        self.corpus = None
        self.tfidf_model = None
        self.corpus_tfidf = None
        self.lda_model = None
        self.train_set = load_train_set(train_data)
        self.test_set = None

    def train(self):
        self.dic = corpora.Dictionary(self.train_set)
        self.corpus = [ self.dic.doc2bow(text) for text in self.train_set ]
        self.tfidf_model = models.TfidfModel.load('./tfidf_model/tfidf.model')
        #self.tfidf_model = models.TfidfModel(self.corpus)
        #self.tfidf_model.save('./tfidf_model/tfidf.model')
        self.corpus_tfidf = self.tfidf_model[self.corpus]

        self.lda_model = models.LdaMulticore(self.corpus_tfidf, id2word = self.dic, num_topics = 50)
        self.lda_model.save('./lda_model/lda.model')

    def run(self, test_set='./data/test_set.txt'):
        self.test_set = load_train_set(test_set)
        self.dic = corpora.Dictionary(self.test_set)
        self.corpus = [ self.dic.doc2bow(text) for text in self.test_set ]

        self.tfidf_model = models.TfidfModel.load('./tfidf_model/tfidf.model')
        self.corpus_tfidf = self.tfidf_model[self.corpus]

        self.lda = models.LdaMulticore.load('./lda_model/lda.model')
        print self.lda[self.corpus_tfidf]


def main():
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    lda = LDAModel(train_data=None)
    lda.run()

if __name__ == '__main__':
    main()
</code></pre>
","4694072","","","","","2015-08-04 04:23:15","'LdaMulticore' object has no attribute 'minimum_probability'?","<lda><gensim>","0","0","","","","CC BY-SA 3.0"
"65811834","1","","","2021-01-20 14:46:03","","0","87","<p>I am experimenting with topic modelling in Gensim and SciKit learn (Python 3) and would like to know more about adjusting hyperparamters in either package.</p>
<p>I found some related discussions on GITHUB, e.g. <a href=""https://github.com/RaRe-Technologies/gensim/issues/105"" rel=""nofollow noreferrer"">&quot;optimize hyperparameters as part of LDA training&quot;</a>, but no complete documentation.</p>
<p>I am not even sure hyperparametre optimisation is at all possible in SciKit learn.</p>
<p>I am very grateful for links or book recommendations.</p>
","10962766","","","","","2021-01-20 15:07:23","Adjusting LDA hyperparameters in SciKit learn or Gensim (Python 3)?","<python><scikit-learn><nlp><gensim><hyperparameters>","1","0","","","","CC BY-SA 4.0"
"65816743","1","","","2021-01-20 19:58:44","","0","100","<p>I am using gensim LDA to build a topic model for a bunch of documents that I have stored in a pandas data frame. Once the model is built, I can call <code>model.get_document_topics(model_corpus)</code> to get a list of list of tuples showing the topic distribution for each document. For example, when I am working with 20 topics, I might get the following for the first three documents in my data frame:</p>
<pre><code>[(5, 0.11253482), (7, 0.75876033)]
[(19, 0.96343607)]
[(0, 0.010002977),
 (1, 0.010002977),
 (2, 0.010002977),
 (3, 0.010002979),
 (4, 0.8099435),
 (5, 0.010002977),
 (6, 0.010002977),
 (7, 0.010002977),
 (8, 0.010002977),
 (9, 0.010002977),
 (10, 0.010002977),
 (11, 0.010002977),
 (12, 0.010002977),
 (13, 0.010002977),
 (14, 0.010002977),
 (15, 0.010002977),
 (16, 0.010002977),
 (17, 0.010002977),
 (18, 0.010002977),
 (19, 0.010002977)]
</code></pre>
<p>This means that the most likely topic for document_1 is 7, for document_2 is 19, and for document_3 is 4. The primary output that I would like to see is simply this most likely topic for each document. The way I'm doing this now is using a loop:</p>
<pre><code>import numpy as np
import pandas as pd

def get_max(doc):
        idx,l = zip(*doc)
        return idx[np.argmax(l)]

data['doc_topic'] = [get_max(doc) for doc in model.get_document_topics(model_corpus)]
</code></pre>
<p>I have around 80k documents in my data frame, so this code takes about 45 seconds to execute. But since gensim has already done all the computations, I keep thinking that that 45 seconds of computational time is simply spent on reorganizing data, so there must be a more efficient way of doing this.</p>
<p>If possible, a secondary output that would be nice to have is the document-topic matrix, such that each row corresponds to a document in my data frame, and each column represents the probability (or similarity) of the document to the topic. So this would yield a DxT matrix, where D is the number of documents, and T is the number of topics.</p>
","13014864","","13014864","","2021-01-20 20:16:34","2021-01-20 20:16:34","How to get a document-topic distribution matrix from gensim LDA","<python><pandas><nlp><gensim>","0","0","","","","CC BY-SA 4.0"
"56582711","1","56585193","","2019-06-13 14:25:26","","0","159","<p>I've already built my Doc2Vec model, using around 20.000 files. I'm looking for a way to find the string representation of a given vector/ID, which might be similar to Word2Vec's index2entity. I'm able to get the vector itself, using model['n'], but now I'm wondering whether there's a way to get some sort of string representation of it as well.</p>
","11642944","","","","","2019-06-13 16:48:03","Python3 - Doc2Vec: Get document by vector/ID","<python><nlp><gensim><word2vec><doc2vec>","1","2","","","","CC BY-SA 4.0"
"48898325","1","48913627","","2018-02-21 04:45:14","","2","1662","<p>I am trying to train with new labelled document(TaggedDocument) with the pre-trained model.</p>

<p>Pretrained model is the trained model with documents which the unique id with label1_index, for instance, Good_0, Good_1 to Good_999
And the total size of trained data is about 7000</p>

<p>Now, I want to train the pre-trained model with new documents which the unique id with label2_index, for instance, Bad_0, Bad_1... to Bad_1211
And the total size of trained data is about 1211</p>

<p>The train itself was successful without any error, but the problem is that whenever I try to use 'most_similar' it only suggests the similar document labelled with Good_... where I expect the labelled with Bad_.</p>

<p>If I train altogether from the beginning, it gives me the answers I expected - it infers a newly given document similar to either labelled with Good or Bad. </p>

<p>However, the practice above will not work as the one trained altogether from the beginning.</p>

<p>Is continuing train not working properly or did I make some mistake?</p>
","5572627","","7357920","","2018-02-21 04:51:02","2018-02-21 19:08:43","gensim doc2vec train more documents from pre-trained model","<gensim><doc2vec><pre-trained-model><resuming-training>","1","0","","","","CC BY-SA 3.0"
"56605777","1","","","2019-06-14 22:24:15","","0","29","<p>I would like to compare two documents semantically and generate a similarity score. The following docs are from wikipedia and when compare them, I expect to see a higher score for world_1 and world_2 as they have similar context.</p>

<p>Would training a Doc2vec model on ""world_1"" and testing other two docs with that model be a good approach? </p>

<p>thermo = ""Thermodynamics is principally based on a set of four laws which are universally valid when applied to systems that fall within the constraints implied by each. In the various theoretical descriptions of thermodynamics these laws may be expressed in seemingly differing forms, but the most prominent formulations are the following:Zeroth law of thermodynamics:If two systems are each in thermal equilibrium with a third, they are also in thermal equilibrium with each other.This statement implies that thermal equilibrium is an equivalence relation on the set of thermodynamic systems under consideration.""</p>

<p>world_1 = ""World War I (often abbreviated as WWI or WW1), also known as the First World War or the Great War, was a global war originating in Europe that lasted from 28 July 1914 to 11 November 1918. Contemporaneously described as the war to end all wars,[7] it led to the mobilisation of more than 70 million military personnel, including 60 million Europeans, making it one of the largest wars in history.[8][9] It is also one of the deadliest conflicts in history,[10] with an estimated nine million combatants and seven million civilian deaths as a direct result of the war, while resulting genocides and the 1918 influenza pandemic caused another 50 to 100 million deaths worldwide. On 28 June 1914, Gavrilo Princip, a Bosnian Serb Yugoslav nationalist, assassinated the Austro-Hungarian heir Archduke Franz Ferdinand in Sarajevo, leading to the July Crisis.""</p>

<p>world_2 = ""World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries‚Äîincluding all the great powers‚Äîeventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 50 to 85 million fatalities, most of whom were civilians in the Soviet Union and China.""</p>
","11650169","","","","","2019-06-15 15:18:24","How to generate a similarity score for two documents","<gensim><word2vec><similarity><cosine-similarity><doc2vec>","1","0","","","","CC BY-SA 4.0"
"44338377","1","","","2017-06-02 22:41:55","","1","440","<p>I am trying, unsuccefully, to install a python package (gensim).</p>

<pre><code>System details:
I am using Python 3.6.0 :: Anaconda custom (64-bit) with Ubuntu16.04LTS.
</code></pre>

<ol>
<li><p>First I followed the directions from <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow noreferrer"">here</a> (including updating before trying to install), I entered </p>

<pre><code>easy_install --upgrade gensim
</code></pre>

<p>and got the following error massage:</p>

<pre><code>Searching for gensim
Reading https://pypi.python.org/simple/gensim/
Download error on https://pypi.python.org/simple/gensim/: unknown urltype: https -- Some packages may not be found!
Couldn't find index page for 'gensim' (maybe misspelled?)
Scanning index of all packages (this may take a while)
Reading https://pypi.python.org/simple/
Download error on https://pypi.python.org/simple/: unknown url type:https -- Some packages may not be found!
No local packages or working download links found for gensim 
error: Could not find suitable distribution for 
Requirement.parse('gensim')
</code></pre></li>
<li><p>When I tried to install using anaconda, </p>

<pre><code>conda install -c anaconda gensim=1.0.1
</code></pre>

<p>as described <a href=""https://anaconda.org/anaconda/gensim"" rel=""nofollow noreferrer"">here</a>, I got </p>

<pre><code>Fetching package metadata ...
CondaHTTPError: HTTP None None for url 
&lt;https://conda.anaconda.org/anaconda/linux-64/repodata.json&gt;
Elapsed: None

An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your way.
SSLError(SSLError(""Can't connect to HTTPS URL because the SSL module is not available."",),)
</code></pre></li>
<li><p>When I tried to install directly from git</p>

<pre><code>pip install https://github.com/RaRe-Technologies/gensim.git
</code></pre>

<p>I got a similar error</p>

<pre><code>SSLError(""Can't connect to HTTPS URL because the SSLpip._vendor.requests.packages.urllib3.exceptions.SSLError: 
Can't connect to HTTPS URL because the SSL module is not available.
</code></pre></li>
<li><p>I tried to work my way around this by using</p>

<pre><code>PYTHONUSERBASE=/home/usr/anaconda3 pip3 install --user --upgrade gensim
</code></pre>

<p>which was able to install the gensim package, but under a newly created folder <code>/home/usr/anaconda3/python3.5</code> even though my default python is python3.6</p></li>
</ol>

<p><strong>Conclusion(?)</strong>: </p>

<p>From <a href=""https://docs.python.org/3/library/http.client.html"" rel=""nofollow noreferrer"">this</a> I understood that probably my Python was not compiled with SSL support, and if I fix this I may be able to win this long battle. BUT I don't understand how I can fix this D_:</p>

<p>PLUS, I don't understand why it insists on being installed under python3.5 when the <a href=""https://pypi.python.org/pypi/gensim"" rel=""nofollow noreferrer"">manual</a> says the package is also compatible with python3.6.</p>
","2934048","","","","","2017-06-06 13:35:50","Can't install gensim with Ubuntu16.04 and anaconda","<ubuntu><ssl><anaconda><gensim><python-3.6>","1","0","","","","CC BY-SA 3.0"
"39973361","1","","","2016-10-11 08:36:47","","1","88","<p>I was wondering what changed in Gensim Word2Vec model between 0.12.3 and 0.13.2.</p>

<p>When I train a small sample of sentences on 0.12.3 (setting the size=2 for visualization). The distribution looks as follows:
<a href=""https://i.stack.imgur.com/zMOcc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zMOcc.png"" alt=""enter image description here""></a></p>

<p>When I do the same on 0.13.2 it looks like this:
<a href=""https://i.stack.imgur.com/NYUxP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NYUxP.png"" alt=""enter image description here""></a></p>

<p>Did the dimensions change to polar or something?</p>
","394594","","","","","2016-10-11 08:36:47","Gensim Word2Vec dimension change between 0.12.3 and 0.13.2","<python><gensim><word2vec>","0","0","","","","CC BY-SA 3.0"
"49155392","1","","","2018-03-07 15:19:24","","3","1809","<p>When I train Doc2vec (using Gensim's Doc2vec in Python) on corpus of about 10k documents (each has few hundred words) and then infer document vectors using the same documents, they are not at all similar to the trained document vectors. I would expect they would be at least somewhat similar.</p>

<p>That is I do <code>model.docvecs['some_doc_id']</code> and <code>model.infer_vector(documents['some_doc_id'])</code>.</p>

<p>Cosine distances between trained and inferred vectors for few first documents:</p>

<pre><code>0.38277733326
0.284007549286
0.286488652229
0.173178792
0.370117008686
0.275438070297
0.377647638321
0.171194493771
0.350615143776
0.311795353889
0.342757165432
</code></pre>

<p>As you can see, they are not really similar. If the similarity is so terrible even for documents used for training, I can't even begin to try to infer unseen documents.</p>

<p>Training configuration:</p>

<pre><code>model = Doc2Vec(documents=documents, dm=1, size=100, window=6, alpha=0.1, workers=4, 
seed=44, sample=1e-5, iter=15, hs=0, negative=8, dm_mean=1, min_alpha=0.01, min_count=2)
</code></pre>

<p>Inferring:</p>

<pre><code>model.infer_vector(tokens, steps=20, alpha=0.025)
</code></pre>

<p>Note on the side: Documents are always preprocessed the same way (I checked that the same list of tokens goes into training and into inferring).</p>

<p>Also I played with parameters around a bit, too, and results were similar. So if your suggestion would be something like ""try increasing or decreasing this or that training parameter"", I've most likely tried it. Maybe I just didn't come across the 'correct' parameters though.</p>

<p>Thanks for any suggestions as to what can I do to make it work better.</p>

<p>EDIT: I am willing and able to use any other available Python implementation of paragraph vectors (doc2vec). It doesn't have to be this one. If you know of another that can achieve better results.</p>

<p>EDIT: <strong>Minimal working example</strong></p>

<pre><code>import fnmatch
import os
from scipy.spatial.distance import cosine
from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
from keras.preprocessing.text import text_to_word_sequence

files = {}
folder = 'some path'  # each file contains few regular sentences
for f in fnmatch.filter(os.listdir(folder), '*.sent'):
    files[f] = open(folder + '/' + f, 'r', encoding=""UTF-8"").read()

documents = []
for k, v in files.items():
    words = text_to_word_sequence(v, lower=True)  # converts string to list of words, removes commas etc.
    documents.append(TaggedDocument(tags=[k], words=words))

d2 = Doc2Vec(size=200, documents=documents)

for doc in documents:
    trained = d2.docvecs[doc.tags[0]]
    inferred = d2.infer_vector(doc.words, steps=50)
    print(cosine(trained, inferred))  # cosine similarity from scipy
</code></pre>
","4146818","","4146818","","2018-03-08 16:41:54","2018-03-08 16:41:54","Gensim's Doc2vec - inferred vector isn't similar","<python><gensim><doc2vec>","1","0","1","","","CC BY-SA 3.0"
"30155506","1","30201869","","2015-05-10 19:04:33","","3","3596","<p>Gensim's <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">official tutorial</a> explicitly states that it is possible to continue training a (loaded) model. I'm aware that according to the documentation it is not possible to continue training a model that was loaded from the <code>word2vec</code> format. But even when one generates a model from scratch and then tries to call the <code>train</code> method, it is not possible to access the newly created labels for the <code>LabeledSentence</code> instances supplied to <code>train</code>.</p>

<pre><code>&gt;&gt;&gt; sentences = [LabeledSentence(['first', 'sentence'], ['SENT_0']), LabeledSentence(['second', 'sentence'], ['SENT_1'])]
&gt;&gt;&gt; model = Doc2Vec(sentences, min_count=1)
&gt;&gt;&gt; print(model.vocab.keys())
dict_keys(['SENT_0', 'SENT_1', 'sentence', 'first', 'second'])
&gt;&gt;&gt; sentence = LabeledSentence(['third', 'sentence'], ['SENT_2'])
&gt;&gt;&gt; model.train([sentence])
&gt;&gt;&gt; print(model.vocab.keys())

# At this point I would expect the key 'SENT_2' to be present in the vocabulary, but it isn't
dict_keys(['SENT_0', 'SENT_1', 'sentence', 'first', 'second'])
</code></pre>

<p>Is it at all possible to continue the training of a Doc2Vec model in Gensim with new sentences? If so, how can this be achieved?</p>
","3144403","","","","","2018-03-18 21:56:13","Continue training a Doc2Vec model","<neural-network><gensim>","2","0","","","","CC BY-SA 3.0"
"56577964","1","","","2019-06-13 09:58:31","","2","70","<p><strong>Problem description</strong></p>

<p>Unable to run gensims Distributed LSI due to this <code>failed to initialize distributed LSI (Failed to locate the nameserver)</code></p>

<p><strong>Steps/code/corpus to reproduce</strong></p>

<pre><code>from gensim.corpora import Dictionary
from gensim.models import TfidfModel, LsiModel
from gensim.similarities import Similarity
from gensim.test.utils import get_tmpfile
import sys
import time, traceback
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

documents=['simple document','trying to reproduce lsi error','example document']
documents=[doc.split(' ') for doc in documents]    
dictionary = Dictionary(documents)
tfidf = TfidfModel(dictionary=dictionary)
corpus = [dictionary.doc2bow(doc) for doc in documents]
model = LsiModel(corpus, id2word=dictionary,num_topics=200,distributed=True)
</code></pre>

<p><strong>Log Trace:</strong></p>

<pre><code>
2019-06-13 15:15:40,268 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2019-06-13 15:15:40,269 : INFO : built Dictionary(11 unique tokens: ['document', 'simple', 'error', 'lsi', 'reproduce']...) from 5 documents (total 17 corpus positions)
2019-06-13 15:15:40,292 : INFO : looking for dispatcher at PYRONAME:gensim.lsi_dispatcher
2019-06-13 15:15:42,414 : ERROR : failed to initialize distributed LSI (Failed to locate the nameserver)
---------------------------------------------------------------------------
ConnectionRefusedError                    Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in connect_and_handshake(conn)
    514                                                nodelay=config.SOCK_NODELAY,
--&gt; 515                                                sslContext=sslContext)
    516                 conn = socketutil.SocketConnection(sock, uri.object)

~/anaconda3/lib/python3.6/site-packages/Pyro4/socketutil.py in createSocket(bind, connect, reuseaddr, keepalive, timeout, noinherit, ipv6, nodelay, sslContext)
    306         try:
--&gt; 307             sock.connect(connect)
    308         except socket.error:

ConnectionRefusedError: [Errno 61] Connection refused

The above exception was the direct cause of the following exception:

CommunicationError                        Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in _locateNS(host, port, broadcast, hmac_key)
   2004     try:
-&gt; 2005         proxy._pyroBind()
   2006         log.debug(""located NS"")

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in _pyroBind(self)
    407         """"""
--&gt; 408         return self.__pyroCreateConnection(True)
    409 

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in __pyroCreateConnection(self, replaceUri, connected_socket)
    595             else:
--&gt; 596                 connect_and_handshake(conn)
    597             if config.METADATA:

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in connect_and_handshake(conn)
    548                         ce.__cause__ = x
--&gt; 549                     raise ce
    550             else:

CommunicationError: cannot connect to ('localhost', 9090): [Errno 61] Connection refused

The above exception was the direct cause of the following exception:

NamingError                               Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/gensim/models/lsimodel.py in __init__(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples, dtype)
    431                 logger.info(""looking for dispatcher at %s"", str(dispatcher._pyroUri))
--&gt; 432                 dispatcher.initialize(
    433                     id2word=self.id2word, num_topics=num_topics, chunksize=chunksize, decay=decay,

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in __getattr__(self, name)
    274             if not self._pyroMethods and not self._pyroAttrs:
--&gt; 275                 self._pyroGetMetadata()
    276         if name in self._pyroAttrs:

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in _pyroGetMetadata(self, objectId, known_metadata)
    614             try:
--&gt; 615                 self.__pyroCreateConnection()
    616             except errors.PyroError:

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in __pyroCreateConnection(self, replaceUri, connected_socket)
    587             else:
--&gt; 588                 uri = _resolve(self._pyroUri, self._pyroHmacKey)
    589             # socket connection (normal or Unix domain socket)

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in _resolve(uri, hmac_key)
   1909     if uri.protocol == ""PYRONAME"":
-&gt; 1910         with _locateNS(uri.host, uri.port, hmac_key=hmac_key) as nameserver:
   1911             return nameserver.lookup(uri.object)

~/anaconda3/lib/python3.6/site-packages/Pyro4/core.py in _locateNS(host, port, broadcast, hmac_key)
   2011             e.__cause__ = x
-&gt; 2012         raise e
   2013 

NamingError: Failed to locate the nameserver

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-5-d3777d1d3a97&gt; in &lt;module&gt;()
     24 tfidf = TfidfModel(dictionary=dictionary)
     25 corpus = [dictionary.doc2bow(doc) for doc in documents]
---&gt; 26 model = LsiModel(corpus, id2word=dictionary,num_topics=200,distributed=True)
     27 corpus = [model[tfidf[doc]] for doc in corpus]
     28 index_tmpfile = get_tmpfile(""index"")

~/anaconda3/lib/python3.6/site-packages/gensim/models/lsimodel.py in __init__(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples, dtype)
    440                 # distributed version was specifically requested, so this is an error state
    441                 logger.error(""failed to initialize distributed LSI (%s)"", err)
--&gt; 442                 raise RuntimeError(""failed to initialize distributed LSI (%s)"" % err)
    443 
    444         if corpus is not None:

RuntimeError: failed to initialize distributed LSI (Failed to locate the nameserver)
</code></pre>

<p><strong>Versions</strong></p>

<pre><code>Python 3.6.4
NumPy 1.15.4
SciPy 1.1.0
gensim 3.7.1
FAST_VERSION 1
</code></pre>
","4065074","","1000551","","2019-06-13 10:11:39","2019-06-13 10:11:39","Unable to run gensims Distributed LSI","<python><python-3.6><gensim><latent-semantic-indexing><pyro4>","0","3","","","","CC BY-SA 4.0"
"56591149","1","56601802","","2019-06-14 03:52:24","","0","349","<p>I wanted to see if I can simply set new weights for gensim's Word2Vec without training. I get the 20 News Group data set from scikit-learn (from sklearn.datasets import fetch_20newsgroups) and trained an instance of Word2Vec on it:</p>

<pre><code>model_w2v = models.Word2Vec(sg = 1, size=300)
model_w2v.build_vocab(all_tokens)
model_w2v.train(all_tokens, total_examples=model_w2v.corpus_count, epochs = 30)
</code></pre>

<p>Here all_tokens is the tokenized data set. 
Then I created a new instance of Word2Vec without training </p>

<pre><code>model_w2v_new = models.Word2Vec(sg = 1, size=300)
model_w2v_new.build_vocab(all_tokens)
</code></pre>

<p>and set the embeddings of the new Word2Vec equal to the first one</p>

<pre><code>model_w2v_new.wv.vectors = model_w2v.wv.vectors
</code></pre>

<p>Most of the functions work as expected, e.g.</p>

<pre><code>model_w2v.wv.similarity( w1='religion', w2 = 'religions')
&gt; 0.4796233
model_w2v_new.wv.similarity( w1='religion', w2 = 'religions')
&gt; 0.4796233
</code></pre>

<p>and</p>

<pre><code>model_w2v.wv.words_closer_than(w1='religion', w2 = 'judaism')
&gt; ['religions']
model_w2v_new.wv.words_closer_than(w1='religion', w2 = 'judaism')
&gt; ['religions']
</code></pre>

<p>and</p>

<pre><code>entities_list = list(model_w2v.wv.vocab.keys()).remove('religion')

model_w2v.wv.most_similar_to_given(entity1='religion',entities_list = entities_list)
&gt; 'religions'
model_w2v_new.wv.most_similar_to_given(entity1='religion',entities_list = entities_list)
&gt; 'religions'
</code></pre>

<p>However, most_similar doesn't work:</p>

<pre><code>model_w2v.wv.most_similar(positive=['religion'], topn=3)
[('religions', 0.4796232581138611),
 ('judaism', 0.4426296651363373),
 ('theists', 0.43141329288482666)]

model_w2v_new.wv.most_similar(positive=['religion'], topn=3)
&gt;[('roderick', 0.22643062472343445),
&gt; ('nci', 0.21744996309280396),
&gt; ('soviet', 0.20012077689170837)]
</code></pre>

<p>What am I missing? </p>

<p>Disclaimer. I posted this question on <a href=""https://datascience.stackexchange.com/questions/53601/copying-embeddings-for-gensim-word2vec"">datascience.stackexchange</a> but got no response, hoping to have a better luck here. </p>
","2299692","","","","","2019-06-14 16:06:36","Copying embeddings for gensim word2vec","<gensim><word2vec><word-embedding>","1","2","","","","CC BY-SA 4.0"
"50146901","1","","","2018-05-03 04:19:39","","0","1261","<p>code like this:</p>

<pre><code>train_corpus = ""sentence_all.txt""
sentences = LineSentence(train_corpus)
model = Word2Vec(sentences, size=vector_size,  window=window_size, min_count=min_count, workers=worker_count, iter=train_epoch)
print(model['‰∏Ä‰πù‰πùÂÖ´Âπ¥Êñ∞Âπ¥'])
</code></pre>

<p>the corpus file has been processd as list of token by LineSentence in gensim like this:</p>

<pre><code>['Êú¨Êä•', 'ËÆØ', 'Ê≤≥ÂåóÈÇ¢Âè∞‰∏≠Ê°•ÂïÜÂú∫', '‰ª•', 'ËØö', 'ÂæÖÂÆ¢', ',', '‰ª•', 'ÁúüÂìÅ', 'Ëµ¢', 'Êù•', 'ÂõûÂ§¥ÂÆ¢', '„ÄÇ', '1997Âπ¥', ',', 'ÂïÜÂú∫', 'Âà©Á®é', 'ÊØî', '‰∏äÂπ¥', 'Áøª', '‰∫Ü', '‰∏ÄÁï™', 'Â§ö', ',', 'ÂëòÂ∑•', '‰∫∫Âùá', 'Âπ¥', 'ÈîÄÂîÆÈ¢ù', 'Ëææ', '22.1‰∏á', 'ÂÖÉ', '„ÄÇ']
['‰∏≠Ê°•ÂïÜÂú∫', 'ËôΩ', 'Âú∞Â§Ñ', 'ÈÇ¢Âè∞Â∏Ç', ',', '‰ΩÜ', '‰∏∫‰∫Ü', 'Êâ©Â§ß', 'ÈîÄÂîÆ', 'ÂçäÂæÑ', ',', '‰ªñ‰ª¨', 'ÊäïËµÑ', 'Ëøë', '‰∏á', 'ÂÖÉ', ',', 'Âêë', 'ÈÇ¢Âè∞Â∏Ç', '19', '‰∏™', 'Âéø', '„ÄÅ', 'Â∏Ç', '„ÄÅ', 'Âå∫', 'ÈÉ®ÂàÜ', 'È°æÂÆ¢', 'Ëµ†ÈòÖ', '„Ää', 'ÂÖ¨ÂÖ≥', '‰∏ñÁïå', '„Äã', 'Âèä', '„Ää', '‰∏≠ÂõΩ', 'Ë¥®Èáè', '‰∏á', 'Èáå', 'Ë°å', '„Äã', 'ÊùÇÂøó', ',', 'Êâ©Â§ß', '‰∫Ü', 'ÂïÜÂ∫ó', 'ÁöÑ', 'ÂΩ±Âìç', '„ÄÇ']
</code></pre>

<p>then get the error:</p>

<pre><code>KeyError: ""word '‰∏Ä‰πù‰πùÂÖ´Âπ¥Êñ∞Âπ¥' not in vocabulary""
</code></pre>

<p>but only a few tokens are not in vocabulary, the others can get their word vector, then I don't know the reason.</p>
","6796943","","","","","2018-06-24 17:26:21","KeyError: word not in vocabularyÔºå when I use gensim.Word2Vec to process chinese tokens","<python><nlp><word2vec><gensim><keyerror>","2","2","","","","CC BY-SA 4.0"
"56316903","1","56464767","","2019-05-26 19:53:31","","7","8665","<p>I am interested in calculating similarity between vectors, however this similarity has to be a number between 0 and 1. There are many questions concerning tf-idf and cosine similarity, all indicating that the value lies between 0 and 1. From <a href=""https://en.wikipedia.org/wiki/Cosine_similarity#Soft_cosine_measure"" rel=""noreferrer"">Wikipedia</a>:</p>

<blockquote>
  <p>In the case of information retrieval, the cosine similarity of two
  documents will range from 0 to 1, since the term frequencies (using
  tf‚Äìidf weights) cannot be negative. The angle between two term
  frequency vectors cannot be greater than 90¬∞.</p>
</blockquote>

<p>The peculiarity is that I wish to calculate the similarity between two vectors from two different word2vec models. These models have been aligned, though, so they should in fact represent their words in the same vector space. I can calculate the similarity between a word in <code>model_a</code> and a word in <code>model_b</code> like so</p>

<pre class=""lang-py prettyprint-override""><code>import gensim as gs
from sklearn.metrics.pairwise import cosine_similarity

model_a = gs.models.KeyedVectors.load_word2vec_format(model_a_path, binary=False)
model_b = gs.models.KeyedVectors.load_word2vec_format(model_b_path, binary=False)

vector_a = model_a[word_a].reshape(1, -1)
vector_b = model_b[word_b].reshape(1, -1)

sim = cosine_similarity(vector_a, vector_b).item(0)
</code></pre>

<p>But <code>sim</code> is then a similarity metric in the [-1,1] range. Is there a scientifically sound way to map this to the [0,1] range? Intuitively I would think that something like</p>

<pre><code>norm_sim = (sim + 1) / 2
</code></pre>

<p>is okay, but I'm not sure whether that is good practice with respect to the actual meaning of cosine similarity. If not, are other similarity metrics advised? </p>

<p>The reason why I am trying to get the values to be between 0 and 1 is because the data will be transferred to a colleague who will use it as a feature for her machine learning system, which expects all values to be between 0 and 1. Her intuition was to take the absolute value, but that seems to me to be a worse alternative because then you map opposites to be identical. Considering the actual meaning of cosine similarity, though, I might be wrong. So if taking the absolute value is the good approach, we can do that as well.</p>
","1150683","","1150683","","2019-05-28 09:32:48","2019-08-22 08:51:51","Cosine similarity between 0 and 1","<python><scikit-learn><gensim><similarity><cosine-similarity>","2","7","4","","","CC BY-SA 4.0"
"56551612","1","56555460","","2019-06-11 21:01:09","","0","24","<p>I am asking this question as a lazy researcher who just wants to try out random crazy ideas quickly, without spending a ton of time reinventing wheels. I completely understand these aren't the intended use cases.</p>

<p>To test a number of hypothesis, I would love to</p>

<ul>
<li>generate the (target, context, +1) tuples differently, instead of the default sliding window.</li>
<li>generate the negative samples (target, random_context, -1) tuples based on some rules, instead of from random NCE draws.</li>
</ul>

<p>For example, I can get the parse tree of a sentence and use parent-child relationship to generate tuples, which is a non-linear window(somebody already tried it in NLP research community, hand-coded ofc...). I can also get an antonyms dictionary to lookup and to generate more negative samples in addition to the random ones (not sure, may help with faster convergence).</p>

<p>Are there some private member functions (something that starts with <code>_XX</code>)I can override to achieve these?</p>
","2593536","","","","","2019-06-12 05:53:39","Gensim: Manual generation of training tuples of (target, context, label)","<python><nlp><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"53037373","1","53037416","","2018-10-29 00:39:53","","2","1222","<pre><code>import gensim
corpus = [[""a"",""b"",""c""],[""a"",""d"",""e""],[""a"",""f"",""g""]]
from gensim.corpora import Dictionary
dct = Dictionary(corpus)
print(dct)
dct.filter_extremes(no_below=1)
print(dct)
</code></pre>

<p>When I ran the code above, my output was -</p>

<pre><code>Dictionary(7 unique tokens: ['a', 'b', 'c', 'd', 'e']...)
Dictionary(6 unique tokens: ['b', 'c', 'd', 'e', 'f']...)
</code></pre>

<p>I supposed that since 'a' occurs in two documents, it should not be removed. However, this is not the case. Am I missing something?</p>
","8695113","","5067311","","2018-10-29 00:50:53","2018-10-29 00:57:02","Misunderstanding the use of filter_extreme in gensim","<python><python-2.7><gensim>","1","0","","","","CC BY-SA 4.0"
"66853259","1","","","2021-03-29 11:16:45","","0","46","<p>I need to create a topic modelling for my uni project, and what I'm doing is trying to repeat what this guy is doing: <a href=""https://www.youtube.com/watch?v=TgXLq1XIdA0"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=TgXLq1XIdA0</a></p>
<p>So far I'm running his code till the line 45, I'm copying it here:</p>
<pre><code>#creating our first topic model :)
#importing the necessary libraries
import gensim, nltk, os

#defining lists to work on with
#tokens from each file stored here
texts = []
#files' names
labels = []

#iterating through all files in the corpus folder
for root, dirs, files in os.walk('CEOs_speeches_all'):
    for file_name in files:
        with open(os.path.join(root, file_name),encoding='utf8') as rf:
            #opening each file as a text for reading
            text = rf.read()
            #tokenising words with nltk
            tokens = nltk.word_tokenize(text)
            #getting rid of all tokens that are not numerical or alphabetical i.e. spaces, punctuation
            cleaned = [word for word in tokens if word.isalnum()]
            texts.append(cleaned)
            #putting files' names without extension (.txt) into the list
            labels.append(file_name[:-4])

#transforming the corpus to run in gensim
corpus_dictionary = gensim.corpora.Dictionary(texts)
corpus_dictionary.filter_extremes(no_below=5)
#transforming the corpus text list into the list of bags of words
processed_corpus = [corpus_dictionary.doc2bow(text) for text in texts]

#printing the first file for checking
#print(processed_corpus[0])

#defining the number of topics
number_of_topics = 10

#specifying where mallet lives
mallet_path = os.path.join(&quot;C:\mallet&quot;, &quot;bin&quot;, &quot;mallet&quot;)

#creating the mallet modeling object
lda_model = gensim.models.wrappers.ldamallet.LdaMallet(
    mallet_path,
    corpus=processed_corpus,
    id2word=corpus_dictionary,
    num_topics=number_of_topics,
    optimize_interval=10,
    prefix='fed_'
)
</code></pre>
<p>and I get this error message:
subprocess.CalledProcessError: Command 'C:\mallet/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex &quot;\S+&quot; --input fed_corpus.txt --output fed_corpus.mallet' returned non-zero exit status 127.</p>
<p>Could someone help to fix, please? Any ideas?</p>
<p>UPDATE: I changed the line mallet_path = os.path.join(&quot;C:\mallet&quot;, &quot;bin&quot;, &quot;mallet&quot;) to mallet_path = &quot;~/mallet/bin/mallet&quot; but it still shows the same error :/</p>
","14593215","","14593215","","2021-03-29 11:44:13","2021-03-29 11:44:13","Topic modelling with Python subprocess.CalledProcessError","<python><gensim><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"16553252","1","16555463","","2013-05-14 21:35:04","","1","6597","<p>I am on Mac OS X 10.8.3 (Mountain Lion) and am trying to run a script in PyCharm.  Python 2.7.2 is installed, I have installed Canopy and Gensim.  I just do not understand what could be causing the error that I'm getting.</p>

<pre><code>scipy.__version__ 
</code></pre>

<p>shows that v 0.11 is installed.</p>

<p>Here is the entirety of my output following a run of the script:</p>

<pre><code>/System/Library/Frameworks/Python.framework/Versions/2.7/bin/python ""/util/LSA/Base LSA.py""

Traceback (most recent call last):

File ""/util/LSA/Base LSA.py"", line 8, in &lt;module&gt;
    from gensim import corpora, models, similarities, matutils
File ""/Library/Python/2.7/site-packages/gensim-0.8.6-py2.7.egg/gensim/__init__.py"", line 7, in &lt;module&gt;
    import utils, matutils, interfaces, corpora, models, similarities
File ""/Library/Python/2.7/site-packages/gensim-0.8.6-py2.7.egg/gensim/matutils.py"", line 20, in &lt;module&gt;
    import scipy.sparse
ImportError: No module named scipy.sparse

Process finished with exit code 1
</code></pre>
","2271170","","","","","2013-05-15 01:44:50","Gensim ImportError in PyCharm: No module named scipy.sparse","<python><scipy><pycharm><lda><gensim>","1","2","","","","CC BY-SA 3.0"
"57630389","1","57630793","","2019-08-23 16:57:30","","0","466","<p>I trained the LDA model on my PC and saved it locally by using model.save() command. I can load this model and output topics in PyCharm, but when I try to load the same model in Jupiter Notebook I get an error.</p>

<p>Did anyone encounter the same problem and fix it?
Below is the full error output:</p>

<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-14-043e6d6083e2&gt; in &lt;module&gt;
      1 # Loading saved model
----&gt; 2 model = models.LdaModel.load('information_extraction/optimal_LDA3.model')
      3 # model_topics = model.show_topics(formatted=True)
      4 # pprint.pprint(model.print_topics(num_words=15))

~/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py in load(cls, fname, *args, **kwargs)
   1636         """"""
   1637         kwargs['mmap'] = kwargs.get('mmap', None)
-&gt; 1638         result = super(LdaModel, cls).load(fname, *args, **kwargs)
   1639 
   1640         # check if `random_state` attribute has been set after main pickle load

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in load(cls, fname, mmap)
    424         compress, subname = SaveLoad._adapt_by_suffix(fname)
    425 
--&gt; 426         obj = unpickle(fname)
    427         obj._load_specials(fname, mmap, compress, subname)
    428         logger.info(""loaded %s"", fname)

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in unpickle(fname)
   1382         # Because of loading from S3 load can't be used (missing readline in smart_open)
   1383         if sys.version_info &gt; (3, 0):
-&gt; 1384             return _pickle.load(f, encoding='latin1')
   1385         else:
   1386             return _pickle.loads(f.read())

ModuleNotFoundError: No module named 'numpy.random._pickle'
</code></pre>
","11968600","","","","","2019-08-23 17:35:44","How to load pre-trained LDA model to Jupiter Notebook?","<pycharm><jupyter-notebook><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"64820037","1","","","2020-11-13 11:23:10","","0","83","<p>As we know machines can't understand the text but it understands numbers so in NLP we convert text to some numeric representation and one of them is BOW representation. Here, my objective is to convert every document to some numeric representation and save it for future use. And I am following the below way to do that by converting text to BOW and saving it in a pickle file. My question is, whether we can do this in a better and reliable way? so that every document can be saved as some vector into a file and new documents are appended in the same way without losing any structure or information.</p>
<pre><code>from gensim import corpora
import pickle

tokenized_corpus = [
    ['human', 'interface', 'computer'],
    ['survey', 'user', 'computer', 'system', 'response', 'time', 'survey'],
    ['eps', 'user', 'interface', 'system'],
    ['system', 'human', 'system', 'eps'],
    ['user', 'response', 'time'],
    ['trees'],
    ['graph', 'trees'],
    ['graph', 'minors', 'trees'],
    ['graph', 'minors', 'survey'],
    ['hello', 'system', 'i', 'love', 'graph', 'minor', 'trees']
]

file_name = 'corpus_sparse_rep.pkl'
bow = []
dct = corpora.Dictionary([tokenized_corpus[0]])  # added first doc as it needs corpus as argument
with open(file_name, 'wb+') as fp:
    # adding each doc sequentially
    for doc in tokenized_corpus:
        dct.add_documents([doc])  # updating vocab in dictionary
        bow.append(dct.doc2bow(doc))  # adding file representation to bow just to check contents before and after in
        # pickle
        pickle.dump(dct.doc2bow(doc), fp)
print(f'Saving bow data to pickle = {bow}')
print(f'Dictionary = {dct}')

# To load bow data from pickle file
pickle_data = []
with open(file_name, 'rb') as fr:
    while True:
        try:
            pickle_data.append(pickle.load(fr))
        except EOFError:
            break
print(f'Loading bow data from pickle = {pickle_data}')
# corpora.MmCorpus.serialize('t.mm', bow) # serialize data and save to market matrix (.mm) format

# Output
# Saving bow data to pickle = [[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)], [(5, 1), (9, 1), (10, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]
# Dictionary = Dictionary(16 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)
# Loading bow data from pickle = [[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)], [(5, 1), (9, 1), (10, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]
</code></pre>
","5741062","","","","","2020-11-13 11:23:10","What is the reliable way to convert text data (document) to numerical data (vector) and save it for further use?","<python><vector><nlp><gensim><doc2vec>","0","10","","","","CC BY-SA 4.0"
"57475889","1","57476857","","2019-08-13 10:30:07","","0","1184","<p>I found it was a failure that I had used Gensim with GoogleNews pre-trained model to cluster phrases like:</p>

<ul>
<li>knitting</li>
<li>knit loom</li>
<li>loom knitting</li>
<li>weaving loom</li>
<li>rainbow loom</li>
<li>home decoration accessories</li>
<li>loom knit/knitting loom</li>
<li>...</li>
</ul>

<p>I am advised that <a href=""https://stackoverflow.com/questions/57426745/how-to-cluster-words-and-phrases-with-pre-trained-model-on-gensim"">GoogleNews model does't have the phrases in it</a>. The phrases I have are a little specific to GoogleNews model while I don't have corpus to train a new model. I have only the phrases. And now I am considering to turn to BERT. But could BERT do that as I expected as above? Thank you.</p>
","11906455","","","","","2019-08-13 11:30:49","Could I use BERT to Cluster phrases with pre-trained model","<tensorflow><nlp><pytorch><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"50117021","1","","","2018-05-01 13:06:14","","2","1294","<p>I am new to <a href=""https://fasttext.cc/docs/en/support.html"" rel=""nofollow noreferrer"">fastText</a>, a library for efficient learning of word representations and sentence classification. I am trying to <a href=""https://fasttext.cc/docs/en/unsupervised-tutorial.html"" rel=""nofollow noreferrer"">generate word-vector for huge data set</a>. But in single process it's taking significantly long time.</p>

<p>So let me put my questions clearly:</p>

<ul>
<li>Are there any options which I can use to speedup the single fastText process?</li>
<li>Is there any way to generate word-vector in parallel fastText processes?</li>
<li>Are there any other implementation or workaround available which can solve the problem, as I read <a href=""https://fasttext.cc/docs/en/faqs.html"" rel=""nofollow noreferrer"">caffe2 implementation is available</a>, but I am unable to find it.</li>
</ul>

<p>Thanks</p>
","8420618","","6771046","","2018-05-01 17:04:42","2018-05-01 17:04:42","Is there a way to use fastText's word representation process in parallel?","<nlp><word2vec><gensim><fasttext><caffe2>","2","1","","","","CC BY-SA 3.0"
"35591567","1","35592209","","2016-02-24 01:22:29","","2","1313","<p>I believe my question is easy, but I'm very new to python and I think that is blinding me a bit.</p>

<p>I've downloaded a Wikipedia dump as explained under ""Preparing the Corpus"" here: <a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow"">https://radimrehurek.com/gensim/wiki.html</a>.  Then I ran the following lines of code:</p>

<pre><code>import gensim

# these next two lines take around 16 hours
wikiDocs = gensim.corpora.wikicorpus.WikiCorpus('enwiki-latest-pages-articles.xml.bz2')
gensim.corpora.MmCorpus.serialize('wiki_en_vocab200k', wikiDocs)
</code></pre>

<p>These lines of code are taken from the link above.  Now, in a separate script I've done some text analysis.  The result of that text analysis is a number representing the index of a particular article in the wikiDocs corpus.  The problem, I don't know how to print out the text of that article.  The obvious thing to try is:</p>

<pre><code>wikiDocs[index_of_article]
</code></pre>

<p>but that returns the error</p>

<pre><code>TypeError: 'WikiCorpus' object does not support indexing
</code></pre>

<p>I've tried a few other things but I'm stuck.  Thanks for any help.</p>
","2019896","","2681088","","2016-02-24 01:37:59","2016-02-24 02:35:06","Print Wikipedia Article Title from Gensim WikiCorpus","<python><nlp><wikipedia><gensim><text-analysis>","1","1","","","","CC BY-SA 3.0"
"30446268","1","","","2015-05-25 21:30:00","","2","262","<p>I have many independent tasks that read <em>but not write to</em> the same gensim model which is about 3.6GB in size. (Gensim is a topic modelling library built upon numpy.) So I decide to parallelize them by first loading the gensim model from a file:</p>

<pre><code>from gensim.models.word2vec import Word2Vec
from multiprocessing import Pool
model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>And then pass the <code>model</code> as a parameter to a pool of processes to run <code>doWork</code>:</p>

<pre><code>def doWork(experiment, doc):
  print ""Begin working""
  # do some work here; access model by experiment.model

class Experiment(object):
   def __init__(self, model, docs):
     self.model = model
     self.docs = docs
   def run(self):
     pool = Pool(processes = 4)
     print ""Done preparing""
     results = pool.map(doWork, [(self, doc) for doc in self.docs])
     return results

experiment = Experiment(model, ['doc1.txt', 'doc2.txt'])
experiment.run()
</code></pre>

<p>When I ran this script (the two segments I show here are a runnable script; please copy), it got stuck on the <code>pool.map</code> line and a <code>SystemError</code> occurred. The output was:</p>

<pre><code>Done preparing
Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 810, in __bootstrap_inner
    self.run()
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 763, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/pool.py"", line 342, in _handle_tasks
    put(task)
SystemError: NULL result without error in PyObject_Call
</code></pre>

<p>The error had never occurred before I introduced gensim to my program. (Gensim without multiprocessing also works for me.) I think it may be related to the interoperation with C code underlying gensim and numpy (BLAS). <strong>I wanted to know the reason of this error and how to fix it.</strong> If I can't use gensim with subprocessing, what are the alternatives?</p>

<p>I don't think the <code>model</code> would be copied, because my OS (Mac OS X) should be using the copy-on-write strategy. I don't think it is related to memory synchronization either, because not a line of ""Begin working"" is printed, i.e. the <code>model</code> has not been accessed by my code. The error is in passing the <code>model</code> to the sub-processes.</p>
","1654411","","1654411","","2015-05-25 21:45:47","2015-05-25 21:45:47","SystemError when sharing a gensim (numpy) model in multiprocessing","<python><numpy><python-multiprocessing><gensim><word2vec>","0","2","","","","CC BY-SA 3.0"
"47799657","1","","","2017-12-13 18:13:04","","0","354","<p>my task is to assign tags (descriptive words) to documents or posts from the list of available tags. I'm working with Doc2vec available in Gensim. I read that doc2vec can be used for document tagging. But i could not get the suitable parameter values for this task. Till now, i have tested it by changing value of parameters named 'size' and 'window'. The results i'm getting are too nonsense and also by changing values of these parameters i haven't find any trend in results i.e. at some values results got little bit improved and at some values results fall down. Can anyone suggest what should be suitable parameter values for this task? I found that 'size'(defines size if feature vector) should be large if we have enough training data. But about the rest of parameters, i am not getting sure! </p>
","8622544","","","","","2017-12-14 05:08:30","Parameter values of Doc2vec for Document Tagging - Gensim","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"46807010","1","46816101","","2017-10-18 09:33:27","","3","4003","<p>I am new to doc2vec. I was initially trying to understand doc2vec and mentioned below is my code that uses Gensim. As I want I get a trained model and document vectors for the two documents.</p>

<p>However, I would like to know the benefits of retraining the model in several epoches and how to do it in Gensim? Can we do it using <code>iter</code> or <code>alpha</code> parameter or do we have to train it in a seperate <code>for loop</code>? Please let me know how I should change the following code to train the model for 20 epoches.</p>

<p>Also, I am interested in knowing is the multiple training iterations are needed for word2vec model as well.</p>

<pre><code># Import libraries
from gensim.models import doc2vec
from collections import namedtuple

# Load data
doc1 = [""This is a sentence"", ""This is another sentence""]

# Transform data
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for i, text in enumerate(doc1):
    words = text.lower().split()
    tags = [i]
    docs.append(analyzedDocument(words, tags))

# Train model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)

# Get the vectors
model.docvecs[0]
model.docvecs[1]
</code></pre>
","","user8566323","","user8566323","2017-10-18 09:47:15","2018-05-17 13:23:11","What are doc2vec training iterations?","<python><deep-learning><word2vec><gensim><doc2vec>","1","0","1","","","CC BY-SA 3.0"
"56045317","1","","","2019-05-08 16:36:29","","1","78","<p>I have a LSI model stored and the model is getting stored as model.pkl and model.pkl.projection.</p>

<p>However, when I try to load the model the loading is failing because its trying to look for projection file with .npy</p>

<pre><code>loading LsiModel object from /var/app/data/lsi_model.pkl.projection
loading u from /var/app/data/lsi_model.pkl.projection.u.npy with mmap=r
failed to load projection from /var/app/data/lsi_model.pkl.projection: 
[Errno 2] No such file or directory: 
'/var/app/data/lsi_model.pkl.projection.u.npy'
</code></pre>

<p>Any idea why this would be happening?</p>
","1547498","","","","","2020-04-03 11:18:36","LSI Model fails to load the model","<scikit-learn><gensim><latent-semantic-indexing>","0","4","","","","CC BY-SA 4.0"
"66867375","1","66877399","","2021-03-30 08:33:10","","1","68","<p>I've built a Doc2Vec model with around 3M documents, now I want to compare it to another model I've previously built. The second model has been scaled to 0-1 so I now also want to scale the gensim model to the same range so that they are comparable.
This is my first time using gensim so I'm not sure how this is done. It's nothing fancy but this is the code I have so far (model generation code ommited). I thought about scaling (minmax scaling with max/min in the union of  vectors) the inferred vectors (v1 and v2) but I don't think this would be correct approach.
The idea here is to compare two documents (with tokens likely to be in the corpus) and output a similarity score between them. I've seen a few Gensim's tutorials and they often compare a single string to the corpus' documents, which is not really the idea here.</p>
<blockquote>
<pre><code> def get_similarity_score(self,string_1, string_2):
    split_tokens1 = string_1.split()
    split_tokens2 = string_2.split()
    v1 = self.model.infer_vector(split_tokens1)
    v2 = self.model.infer_vector(split_tokens2)
    text_score = nltk.cluster.util.cosine_distance(v1, v2)
    return text_score
</code></pre>
</blockquote>
<p>Any recommendations?</p>
","6846396","","","","","2021-04-01 17:27:25","Scale cosine distance to 0-1 using Gensim","<python><math><nlp><text-mining><gensim>","1","0","","","","CC BY-SA 4.0"
"64817706","1","64829979","","2020-11-13 08:35:56","","1","44","<p>I use FastText from the <code>gensim</code> package, and I use the code below to transform my text into a dense a representation but it takes many times when I have a huge dataset.
Could you help me to accelerate it?</p>
<pre class=""lang-py prettyprint-override""><code>def word2vec_features(self, templates, model):
    if self.method == 'mean':
        feats = np.vstack([sum_vectors(p, model) / len(p) for p in templates])
    else:
        feats = np.vstack([sum_vectors(p, model) for p in templates])
    return feats

def get_vect(word, model):
    try:
        return model.wv[word]
    except KeyError:
        return np.zeros((model.size,))


def sum_vectors(phrase, model):
    return sum(get_vect(w, model) for w in phrase)
</code></pre>
","12935107","","6573902","","2020-11-13 16:44:30","2020-11-14 01:08:48","How can i optimize my Embedding transformation on a huge dataset?","<python><python-3.x><numpy><gensim><fasttext>","1","1","","","","CC BY-SA 4.0"
"56323377","1","","","2019-05-27 09:34:45","","3","1566","<p>I'm trying to find out the <em>similarity between 2 documents</em>. I'm using <strong>Doc2vec Gensim</strong> to train around <strong>10k documents</strong>. There are around <strong>10 string type of tags</strong>. Each tag consists of a unique word and contains some sort of documents. Model is trained using <strong>distributed memory method</strong>.</p>

<pre><code>Doc2Vec(alpha=0.025, min_alpha=0.0001, min_count=2, window=10, dm=1, dm_mean=1, epochs=50, seed=25, vector_size=100, workers=1)
</code></pre>

<p>I've tried both <strong>dm</strong> and <strong>dbow</strong> as well. <strong>dm</strong> gives better <em>result(similarity score)</em> as compared to <strong>dbow</strong>. I understood the concepts of <strong>dm vs dbow</strong>. But don't know which method is good for similarity measures between two documents. </p>

<p>First question: <strong>Which method is the best to perform well on similarities?</strong></p>

<p><code>model.wv.n_similarity(&lt;words_1&gt;, &lt;words_2&gt;)</code> gives similarity score using <strong>word vectors</strong>.</p>

<p><code>model.docvecs.similarity_unseen_docs(model, doc1, doc2)</code> gives similarity score using <strong>doc vectors</strong> where doc1 and doc2 are not tags/ or indexes of doctags. <em>Each doc1 and doc2 contains 10-20 words kind of sentences.</em></p>

<p>Both <strong>wv.n_similarity</strong> and <strong>docvecs.similarity_unseen_docs</strong> provide different similarity scores on same types of documents. </p>

<p><strong>docvecs.similarity_unseen_docs</strong> gives little bit good results as compared to <strong>wv.n_similarity</strong> but <strong>wv.n_similarity</strong> sometimes also gives good results.</p>

<p>Question: <strong>What is the difference between docvecs.similarity_unseen_docs and wv.n_similarity? Can I use docvecs.similarity_unseen_docs to find the similarity score between unseen data (It might be a silly question)?</strong> </p>

<p>Why I asked because <strong>docvecs.similarity_unseen_docs</strong> provides similarity score on tags, not on actual words belonging to their tags. I'm not sure, please correct me here, if I'm wrong.</p>

<p><strong>How can I convert cosine similarity score to probability?</strong></p>

<p>Thanks.</p>

<pre class=""lang-py prettyprint-override""><code>model = Doc2Vec(alpha=0.025, min_alpha=0.0001, min_count=2, window=10, dm=1, dm_mean=1, epochs=50, seed=25, vector_size=100, workers=4)
# Training of the model
tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(&lt;list_of_list_of_tokens&gt;)]
model.build_vocab(tagged_data)
model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)

# Finding similarity score
model.wv.n_similarity(&lt;doc_words1&gt;, &lt;doc_words2&gt;)
model.random.seed(25)
model.docvecs.similarity_unseen_docs(model, &lt;doc_words1&gt;, &lt;doc_words2&gt;)
</code></pre>
","1749606","","1749606","","2019-05-29 08:04:10","2019-05-29 08:04:10","Which method dm or dbow works well for document similarity using Doc2Vec?","<python-3.x><gensim><similarity><doc2vec>","1","0","1","","","CC BY-SA 4.0"
"50161445","1","","","2018-05-03 18:11:44","","0","1222","<p>I trained a FastText model in Gensim. I want to use it to encode my sentences. Specifically, I want to use this feature from native FastText:</p>

<pre><code>./fasttext print-word-vectors model.bin &lt; queries.txt
</code></pre>

<p>How to I save the model in Gensim so that it is the correct binary format that can be understood by native FastText?</p>

<p>I am using FastText 0.1.0 and Gensim 3.4.0 under Python 3.4.3.</p>

<p>In essence, I need the inverse of the load_binary_data() as given in the <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">Gensim FastText doc</a>.</p>
","2744662","","2744662","","2018-05-04 04:41:01","2018-05-05 15:43:05","How to load Gensim FastText model in native FastText","<gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"44461656","1","45501112","","2017-06-09 15:32:12","","0","1463","<p>My setup is as follows:</p>

<p>Python version: 3.6.0</p>

<p>Numpy version: 1.13.0</p>

<p>Scipy version: 0.19.0</p>

<p>Gensim version: 2.1.0</p>

<p>GCC Compiler version: 5.3.0</p>

<p>System: Windows 7, 64bit</p>

<p>I get the following error with the setup above</p>

<pre><code>import gensim
&gt;&gt;&gt;Slow version of gensim.models.doc2vec is being used
</code></pre>

<p>This makes the run time far too slow when training models on gensim. I feel there is some problem with the package versions I am using or how I installed them because: I had to install numpy using <code>pip</code>; I had to install scipy using <code>conda</code>; and I had to install gensim using <code>pip</code> again. The reason for this setup, is because if I try to install scipy using <code>pip</code>, I get the error</p>

<pre><code>&gt;&gt;&gt;ImportError: DLL load failed: The specified procedure could not be found.
</code></pre>

<p>So I had to install scipy via <code>conda</code>. Also, if I try to install gensim using</p>

<p><code>conda install gensim</code></p>

<p>or</p>

<p><code>conda update gensim</code></p>

<p>it only installs version 1 - I have tried <code>conda install -c anaconda gensim=2.1.0</code> but I get the error</p>

<pre><code>PackageNotFoundError: Package missing in current win-64 channels:
- gensim 2.1.0*
</code></pre>

<p>Numpy and Scipy work fine independently when I import them into a script - that is, they import fine and I can use all their functionality. However, when they are being used by Gensim, clearly there is a problem and I don't know why.</p>

<p><strong>Would anyone be able to advise possible fixes? Ideally I would like to keep all the latest versions of these packages if possible. Thank you in advance</strong></p>

<p><strong>NOTE: Gensim works fine with the ""fast"" version when I have Gensim version 1 installed and with the same versions of the dependencies above!</strong></p>
","4139143","","4139143","","2017-06-09 16:01:30","2017-10-10 10:02:38","Gensim: Slow version of gensim.models.doc2vec is being used","<python><numpy><scipy><pip><gensim>","2","0","","","","CC BY-SA 3.0"
"50195948","1","","","2018-05-06 03:17:06","","0","585","<h2>Word2Vec</h2>

<p>Currently I am trying to perform text classification on a text corpus. In order to do so, I have decided to perform <code>word2vec</code> with the help of <code>gensim</code>. In order to do so, I have the code below: </p>

<pre><code>sentences = MySentences(""./corpus_samples"") # a memory-friendly iterator
model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>My sentences is basically a class that handles the File <em>I/O</em></p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()
</code></pre>

<p>Now we can get the vocabulary of the model that has been created through these lines: </p>

<pre><code>print(model.wv.vocab)
</code></pre>

<p>The output of which is below(sample): </p>

<pre><code>t at 0x106f19438&gt;, 'raining.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19470&gt;, 'fly': &lt;gensim.models.keyedvectors.Vocab object at 0x106f194a8&gt;, 'rain.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f194e0&gt;, 'So‚Ä¶': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19518&gt;, 'Ohhh,': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19550&gt;, 'weird.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19588&gt;}
</code></pre>

<p>As of now, the dictionary that is the vocabulary, contains the word string and a <code>&lt;gensim.models.keyedvectors.Vocab object at 0x106f19588&gt;</code> object or such. I want to be able to query an index of a particular word. In order to make my training data like: </p>

<pre><code>w91874 w2300 w6 w25363 w6332 w11 w767 w297441 w12480 w256 w23270 w13482 w22236 w259 w11 w26959 w25 w1613 w25363 w111 __label__4531492575592394249
w17314 w5521 w7729 w767 w10147 w111 __label__1315009618498473661
w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492
w30877 w72 w11 w2828 w141417 w77033 w10147 w111 __label__4970306416006110305
w3332 w1107 w4809 w1009 w327 w84792 w6 w922 w11 w2182 w79887 w1099 w111 __label__-3645735357732416904
w471 w14752 w1637 w12348 w72 w31330 w930 w11569 w863 w25 w1439 w72 w111 __label__-5932391056759866388
w8081 w5324 w91048 w875 w13449 w1733 w111 __label__3812457715228923422
</code></pre>

<p>Where the <code>wxxxx</code> represents the index of the word within the vocabulary and the label represents the class. </p>

<hr>

<h2>Corpora</h2>

<p>Some of the solutions that I have been experimenting with, is the <code>corpora</code> utility of <code>gensim</code>:  </p>

<pre><code>corpora = gensim.corpora.dictionary.Dictionary(sentences, prune_at=2000000)
print(corpora)
print(getKey(corpora,'am'))
</code></pre>

<p>This gives me a nice dictionary of the words, but this corpora vocabulary is not the same as the one created by the <code>word2vec</code> function mentioned above. </p>
","4805357","","","","","2018-05-07 06:36:10","I am trying to get the key of a particular word from a Word2Vec Vocabulary","<python><dictionary><nlp><word2vec><gensim>","1","0","","","","CC BY-SA 4.0"
"66868221","1","","","2021-03-30 09:28:12","","9","11237","<p>I have trained a Word2Vec model using Gensim 3.8.0. Later I tried to use the pretrained model using Gensim 4.0.o on GCP. I used the following code:</p>
<pre><code>model = KeyedVectors.load_word2vec_format(wv_path, binary= False)
words = model.wv.vocab.keys()
self.word2vec = {word:model.wv[word]%EMBEDDING_DIM for word in words}
</code></pre>
<p>I was getting error that &quot;model.mv&quot; has been removed from Gensim 4.0.0.
Then I used the following code:</p>
<pre><code>model = KeyedVectors.load_word2vec_format(wv_path, binary= False)
words = model.vocab.keys()
word2vec = {word:model[word]%EMBEDDING_DIM for word in words}
</code></pre>
<p>And getting the following error:</p>
<pre><code>AttributeError: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.
Use KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.
See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4
</code></pre>
<p>Can anyone please suggest that how can I use the pretrained model &amp; return a dictionary in Gensim 4.0.0?</p>
","15377660","","","","","2021-08-25 22:58:23","Gensim 3.8.0 to Gensim 4.0.0","<python><nlp><gensim><word2vec><word-embedding>","3","0","","","","CC BY-SA 4.0"
"66884353","1","66891019","","2021-03-31 08:41:10","","1","1167","<p>I am trying to use LDA MAllet model. but I am facing with &quot;No module named 'gensim.models.wrappers'&quot; error.</p>
<ul>
<li><p>I have gensim installed and  ' gensim.models.LdaMulticore' works properly.</p>
</li>
<li><p>Java developer‚Äôs kit is installed</p>
</li>
<li><p>I have already downloaded mallet-2.0.8.zip and unzipped it on c:\ drive.</p>
</li>
<li><p>This is the code I am trying to use:</p>
<pre><code>import os
from gensim.models.wrappers import LdaMallet
os.environ.update({'MALLET_HOME':r'C:/mallet-2.0.8/'}) 
mallet_path = r'C:/mallet-2.0.8/bin/mallet' 

</code></pre>
</li>
</ul>
<p>Does anyone know what is wrong here? Many thanks!</p>
","13922736","","","","","2021-04-19 03:09:41","ModuleNotFoundError: No module named 'gensim.models.wrappers'","<python><gensim><lda><mallet><modulenotfounderror>","2","0","","","","CC BY-SA 4.0"
"53187257","1","53190227","","2018-11-07 10:05:24","","0","241","<p>I'm pretty new to MySQL, Gensim, and Word2Vec, and I'm still learning how to use by working on my personal project.</p>
<p>I have data that I got by doing web scraping so it's not hard coded.
(I used Instagram account to get hashtag data from several post, so my data is
Instagram hashtags)</p>
<p>I'm trying to use that data in this code below:</p>
<pre><code>import pymysql.cursors
import re
from gensim.models import Word2Vec

# Connect to the database
connection = pymysql.connect(host=secrets[0],
user=username,
password=password,
db='test',
charset='charsetExample',
cursorclass=pymysql.cursors.DictCursor)

try:
    # connection to database
    with connection.cursor() as cursor:
    # cursor is iterator / 'Select' - caption is column 
     # post is the table 
     cursor.execute(&quot;SELECT caption FROM posts LIMIT 1000&quot;)
     data = cursor.fetchall()
     # list of captions
      captions = [d['caption'].lower() for d in data]
     # hashtags = [re.findall(r&quot;#([A-Za-z_0-9]+)&quot;, caption) for caption in captions]
    # hashtags = [hashtag for hashtag in hashtags if hashtag != []]
    model = Word2Vec(captions, min_count=1)
    model = Word2Vec(hashtags) 
    res = model.wv.most_similar(&quot;fitness&quot;)

    print(captions)
    print(res)

finally:
    connection.close()
</code></pre>
<p>This is the part that I'm working on and not really sure how to do:</p>
<pre><code>res = model.wv.most_similar(&quot;fitness&quot;)
</code></pre>
<p>For now I was trying to use <code>most_similar()</code> method to see how it works.
What I'm trying to do is in the <code>most_similar(&quot;value&quot;)</code> I want to use my data
which will be each hashtags that I got by scraping the Instagram website as the value.</p>
<p>Thank you!</p>
","10548370","","2318649","","2021-02-03 11:16:39","2021-02-03 11:16:39","How to use scraped data from website to Word2vec Gensim","<python><mysql><gensim><word2vec>","1","2","","","","CC BY-SA 4.0"
"66864011","1","","","2021-03-30 02:29:30","","0","111","<p>Gensim Word2Vec Model has a great method which allows you to find the top n most similar words in the models vocabulary given a list of positive words and negative words.</p>
<pre><code>wv.most_similar(positive=['word1', 'word2', 'word3'], 
                negative=['word4','word5'], topn=10)
</code></pre>
<p>What I am looking to do is create word vector that represents an averaged or summed vector of the input positive and negative words. I am hoping to use this new vector to compare to other vectors.
Something like this:</p>
<pre><code>newVector = 'word1' + 'word2' + 'word3' - 'word4' - 'word5'
</code></pre>
<p>I know that vectors can be summed, but I am not sure if that is the best option. I am hoping to find out exactly how the above function (most_similar) combines the positive vectors and negative vectors, and if Gensim has a function to do so. Thank you in advance.</p>
","9708985","","","","","2021-03-30 12:52:21","Combining vectors in Gensim Word2Vec vocabulary","<nlp><gensim><word2vec><word-embedding>","2","0","","","","CC BY-SA 4.0"
"50191231","1","50191902","","2018-05-05 15:44:52","","1","2344","<p>Having loaded a pre-trained word2vec model with the gensim toolkit, I would like to find a synonym of a word given a context such as intelligent for 'she is a bright person'.</p>
","4503898","","6573902","","2018-11-15 22:04:04","2018-11-15 22:04:04","How do I find a synonym of a word or multi-word paraphrase using the gensim toolkit","<python><nlp><word2vec><gensim><word-sense-disambiguation>","1","1","1","","","CC BY-SA 4.0"
"44506950","1","","","2017-06-12 19:05:05","","0","194","<p>I'm trying to pull into python the English Wikipedia corpus (<a href=""https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2</a>) to perform some deep learning. I'm using gensim.</p>

<p>It's 16GB and I've got it sitting on a large EC2 machine in AWS. I load it with</p>

<pre><code>from gensim.corpora.wikicorpus import WikiCorpus
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from pprint import pprint
import multiprocessing

wiki = WikiCorpus(""enwiki-latest-pages-articles.xml.bz2"")
</code></pre>

<p>I run this in a jupyter notebook, but its basically hung trying to load this. I'm watching memory consumption and its loading extremely slowly. (12+ hours and only ~2 GB). Any way I can speed this up?</p>
","5244410","","27678","","2017-06-12 19:06:23","2017-06-22 09:48:08","Can I speed up loading xml bz2 files into memory?","<python><deep-learning><gensim>","1","0","","","","CC BY-SA 3.0"
"50212449","1","","","2018-05-07 10:50:07","","1","2189","<p>I try to map sentences to a vector in order to make sentences comparable to each other. To test gensim's Doc2Vec model, I downloaded sklearn's newsgroup dataset and trained the model on it.</p>
<p>In order to compare two sentences, I use model.infer_vector() and I am wondering why two calls using the same sentence delivers me different vectors:</p>
<pre><code>model = Doc2Vec(vector_size=100, window=8, min_count=5, workers=6)
model.build_vocab(documents)

epochs=10
for epoch in range(epochs):
    print(&quot;Training epoch %d&quot; % (epoch+1))
    model.train(documents,  total_examples=len(documents), epochs=epochs)

    v1 = model.infer_vector(&quot;I feel good&quot;)
    v2 = model.infer_vector(&quot;I feel good&quot;)
    print(np.linalg.norm(v1-v2)) 
</code></pre>
<p>Output:</p>
<blockquote>
<p>Training epoch 1</p>
<p>0.41606528</p>
<p>Training epoch 2</p>
<p>0.43440753</p>
<p>Training epoch 3</p>
<p>0.3203116</p>
<p>Training epoch 4</p>
<p>0.3039317</p>
<p>Training epoch 5</p>
<p>0.68224543</p>
<p>Training epoch 6</p>
<p>0.5862567</p>
<p>Training epoch 7</p>
<p>0.5424634</p>
<p>Training epoch 8</p>
<p>0.7618142</p>
<p>Training epoch 9</p>
<p>0.8170159</p>
<p>Training epoch 10</p>
<p>0.6028216</p>
</blockquote>
<p>If I set alpha and min_alpha = 0 I get consistent vectors for the &quot;I feel fine&quot; and &quot;I feel good&quot;, but the model gives me the same vector in every epoch, so it does not seem to learn anything:</p>
<blockquote>
<p>Training epoch 1</p>
<p>0.043668125</p>
<p>Training epoch 2</p>
<p>0.043668125</p>
<p>Training epoch 3</p>
<p>0.043668125</p>
<p>Training epoch 4</p>
<p>0.043668125</p>
<p>Training epoch 5</p>
<p>0.043668125</p>
<p>Training epoch 6</p>
<p>0.043668125</p>
<p>Training epoch 7</p>
<p>0.043668125</p>
<p>Training epoch 8</p>
<p>0.043668125</p>
<p>Training epoch 9</p>
<p>0.043668125</p>
<p>Training epoch 10</p>
<p>0.043668125</p>
</blockquote>
<p>So my questions are:</p>
<ol>
<li><p>Why do I even have the possibility to specify a learning rate for inference? I would expect that the model is only changed during training and not during inference.</p>
</li>
<li><p>If I specify alpha=0 for inference, why does the distance between those two vectors not change during different epochs?</p>
</li>
</ol>
","3827381","","-1","","2020-06-20 09:12:55","2018-05-07 18:20:02","Gensim Doc2Vec - Why does infer_vector() use alpha?","<gensim><embedding><sentence><doc2vec>","1","0","1","","","CC BY-SA 4.0"
"50214899","1","50374592","","2018-05-07 13:02:05","","3","984","<p>I am facing the following error when trying to update my gensim's <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">LdaModel</a>: </p>

<blockquote>
  <p>IndexError: index 6614 is out of bounds for axis 1 with size 6614</p>
</blockquote>

<p>I checked why were other people having this issue on <a href=""https://groups.google.com/forum/#!topic/gensim/T0GMxE7YZqM"" rel=""nofollow noreferrer"">this thread</a>, but I am using the same dictionary from the beginning to the end, which was their error.</p>

<p>As I have a big dataset, I am loading it chunk by chunk (using pickle.load). I am building the dictionary in this way, iteratively, thanks to this piece of code :¬†</p>

<p>¬†</p>

<pre><code> fr_documents_lda = open(""documents_lda_40_rails_30_ruby_full.dat"", 'rb')
¬†dictionary = Dictionary()
¬†chunk_no = 0
¬†while 1:
¬† ¬† ¬†try:
¬† ¬† ¬† ¬† ¬†t0 = time()
¬† ¬† ¬† ¬† ¬†documents_lda = pickle.load(fr_documents_lda)
¬† ¬† ¬† ¬† ¬†chunk_no += 1
¬† ¬† ¬† ¬† ¬†dictionary.add_documents(documents_lda)
¬† ¬† ¬† ¬† ¬†t1 = time()
¬† ¬† ¬† ¬† ¬†print(""Chunk number {0} took {1:.2f}s"".format(chunk_no, t1-t0))
¬† ¬† ¬†except EOFError:
¬† ¬† ¬† ¬† ¬†print(""Finished going through pickle"")
¬† ¬† ¬† ¬† ¬†break
</code></pre>

<p>Once built for the whole dataset, I am training the model in the same fashion, iteratively, this way :</p>

<pre><code>fr_documents_lda = open(""documents_lda_40_rails_30_ruby_full.dat"", 'rb')
first_iter = True
chunk_no = 0
lda_gensim = None
while 1:
¬† ¬† try:
¬† ¬† ¬† ¬† t0 = time()
¬† ¬† ¬† ¬† documents_lda = pickle.load(fr_documents_lda) 
¬† ¬† ¬† ¬† chunk_no += 1
¬† ¬† ¬† ¬† corpus = [dictionary.doc2bow(text) for text in documents_lda]
¬† ¬† ¬† ¬† if first_iter:
¬† ¬† ¬† ¬† ¬† ¬† first_iter = False
¬† ¬† ¬† ¬† ¬† ¬† lda_gensim = LdaModel(corpus, num_topics=no_topics, iterations=100, offset=50., random_state=0, alpha='auto')
¬† ¬† ¬† ¬† else:
¬† ¬† ¬† ¬† ¬† ¬† lda_gensim.update(corpus)
¬† ¬† ¬† ¬† t1 = time()
¬† ¬† ¬† ¬† print(""Chunk number {0} took {1:.2f}s"".format(chunk_no, t1-t0))
¬† ¬† except EOFError:
¬† ¬† ¬† ¬† print(""Finished going through pickle"")
¬† ¬† ¬† ¬† break
</code></pre>

<p>I also tried updating the dictionary at every chunk, i.e. having
¬†</p>

<pre><code>dictionary.add_documents(documents_lda)
</code></pre>

<p>right before
¬†</p>

<pre><code>corpus = [dictionary.doc2bow(text) for text in documents_lda]
</code></pre>

<p>¬†in the last piece of code. Finally, I tried setting the allow_update argument of doc2bow to True. Nothing works.</p>

<p>FYI, the size of my final dictionary is 85k. The size of my dictionary built only from the first chunk is 10k. The error occurs on the second iteration, when it passes in the else condition, when calling the update method.</p>

<p>The error is raised by the line  <code>expElogbetad = self.expElogbeta[:, ids]</code>
, called by <code>gamma, sstats = self.inference(chunk, collect_sstats=True)</code>, itself called by <code>gammat = self.do_estep(chunk, other)</code>, itself called by <code>lda_gensim.update(corpus)</code>.</p>

<p>Is anyone having an idea on how to fix this, or what is happening ?</p>

<p>Thank you in advance.</p>
","6245540","","","","","2020-06-24 07:55:06","IndexError when trying to update gensim's LdaModel","<python-3.x><gensim><lda><topic-modeling><index-error>","1","0","","","","CC BY-SA 4.0"
"56431471","1","","","2019-06-03 16:44:56","","1","286","<p>I'm training word2vec model where each word belongs to a specific class. </p>

<p>I want my embeddings to learn differences of words within each class, but don't want them to learn the differences between classes. </p>

<p>This can be achieved by negative sampling from only the words of same class as the target word. </p>

<p>In gensim <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">word2vec</a>, we can specify the number of words to negative sample using <code>negative</code> parameter, but it doesn't mention any options to modify/filter the sampling function. </p>

<p>Is there any method to achieve this?</p>

<p><strong>Update:</strong></p>

<p>Consider the classes to be like languages. So I have words from different languages. In training data, each sentence/document contains mostly words from same language, but sometimes from other languages. </p>

<p>Now I want embeddings where words with similar meanings are together irrespective of the language.</p>

<p>But because words from different languages do not occur together as frequently as words from same language, the embeddings basically groups words from same language together. </p>

<p>Because of this, I wanted to try negative sampling target words with words from same language so that it learns to distinguish the words within same language. </p>
","3273991","","3273991","","2019-06-04 07:44:59","2019-06-04 07:44:59","Specify condition for negative sampling in gensim word2vec","<gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"56579523","1","","","2019-06-13 11:33:16","","1","2283","<p>I want to retrain pre-trained word embeddings in Python using Gensim. The pre-trained embeddings I want to use is Google's Word2Vec in the file GoogleNews-vectors-negative300.bin.</p>

<p>Following Gensim's word2vec tutorial, ""it‚Äôs not possible to resume training with models generated by the C tool, load_word2vec_format(). You can still use them for querying/similarity, but information vital for training (the vocab tree) is missing there."" 
Therefore I can't use the KeyedVectors and for training a model the tutorial suggests to use:</p>

<pre><code>    model = gensim.models.Word2Vec.load('/tmp/mymodel')
    model.train(more_sentences)
</code></pre>

<p>(<a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">https://rare-technologies.com/word2vec-tutorial/</a>)</p>

<p>However, when I try this:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec
model = Word2Vec.load('data/GoogleNews-vectors-negative300.bin')
</code></pre>

<p>I get an error message:</p>

<pre><code>    1330         # Because of loading from S3 load can't be used (missing readline in smart_open)
    1331         if sys.version_info &gt; (3, 0):
    -&gt; 1332             return _pickle.load(f, encoding='latin1')
    1333         else:
    1334             return _pickle.loads(f.read())

    UnpicklingError: invalid load key, '3'.
</code></pre>

<p>I didn't find a way to convert the binary google new file into a text file properly, and even if so I'm not sure whether that would solve my problem.</p>

<p>Does anyone have a solution to this problem or knows about a different way to retrain pre-trained word embeddings?</p>
","11641873","","","","","2019-06-13 17:06:51","Retraining pre-trained word embeddings in Python using Gensim","<python-3.x><gensim><word2vec>","2","0","1","","","CC BY-SA 4.0"
"66160326","1","","","2021-02-11 18:07:40","","0","33","<p>I want to write a program that takes a word embedding as input, and I want to build it in such a fashion that I have an initial set of nodes, then their children are their direct neighbours from word2vec, and then for each child I add their child (neighbours of neighbours). At some point in time in my program, I must be able to retrieve the neighbours that are accesseble by <code>k</code> edges. I have built a semi program for thatt using general tree structure. But I am pretty sure this is very compute intensive. I am trying to incorporate <code>multiprocessing</code>, but I even want it more optimized because my word embeddings are very huge in size. I am struggling to make it faster.</p>
<p>My code:</p>
<pre><code>import os, pickle
import numpy as np
from gensim.models import Word2Vec
from smart_open import open
import time
import multiprocessing
from utilities import cossim
from nltk.corpus import stopwords
stopwords_list = stopwords.words('arabic')


class Node(object):
    def __init__(self, data):
        self.data = data
        self.children = []

    def add_children(self, obj):
        self.children = []
        for o in obj:
            self.children.append(self.__class__(o))


class GraphNeighbourBased:

    def __init__(self, model1, model2, t, topK=100):
        v1 = list(model1.wv.vocab.keys())
        v2 = list(model2.wv.vocab.keys())
        # the intersection of the vocabs of two embedding spaces
        self.v = list(set(v1) &amp; set(v2))
        # number of iterations - Algorithm 1 in the paper
        self.t = t
        # two embedding spaces
        self.model1, self.model2 = model1, model2
        # top N neighbours to include in Algorithm 1
        self.topK = topK
        # vector containing stability of each word in vocab
        svals = [1] * len(self.v)
        # dictionary containing stability values of each word in v
        self.stabilities = dict(zip(self.v, svals))
        # after initialization build the graph
        self._build_graph()

    def _build_graph(self):
        t1 = time.time()
        self.root_nodes = [Node(w) for w in self.v]
        self.graph1 = [Node(w) for w in self.v]
        self.graph2 = [Node(w) for w in self.v]
        # keep adding neighbours until depth=t
        for iter in range(self.t):
            print('building graph: iteration {}'.format(iter))
            if iter == 0:
                p1 = multiprocessing.Process(target=self._add_first_children2leaves_graph1)
                p2 = multiprocessing.Process(target=self._add_first_children2leaves_graph2)

                p1.start()
                p2.start()

                p1.join()
                p2.join()

            else:
                p1 = multiprocessing.Process(target=self._add_children2leaves_graph1)
                p2 = multiprocessing.Process(target=self.add_children2leaves_graph2)

                p1.start()
                p2.start()

                p1.join()
                p2.join()

        print('done building graph ...')
        t2 = time.time()
        print('building graph took: {}'.format((t2-t1)/60))
        print('saving graphs ...')
        with open(&quot;graph1.dat&quot;, &quot;wb&quot;) as f:
            pickle.dump(self.graph1, f)
        with open(&quot;graph2.dat&quot;, &quot;wb&quot;) as f:
            pickle.dump(self.graph2, f)
        print('done saving graphs')

    def _add_first_children2leaves_graph1(self):
        for i, w in enumerate(self.graph1):
            nei1 = self.model1.wv.most_similar(positive=[w.data], topn=self.topK)
            self.graph1[i].add_children([n for n, _ in nei1])

    def _add_first_children2leaves_graph2(self):
        for i, w in enumerate(self.graph2):
            nei2 = self.model2.wv.most_similar(positive=[w.data], topn=self.topK)
            self.graph2[i].add_children([n for n, _ in nei2])

    def _add_children2leaves_graph1(self):
        for node in self.graph1:
            leaves = [leaf for leaf in self.get_leaves(node)]
            for l in leaves:
                neighs = self.model1.wv.most_similar(positive=[l.data], topn=self.topK)
                l.add_children([n for n, _ in neighs])

    def add_children2leaves_graph2(self):
        for node in self.graph2:
            leaves = [leaf for leaf in self.get_leaves(node)]
            for l in leaves:
                neighs = self.model2.wv.most_similar(positive=[l.data], topn=self.topK)
                l.add_children([n for n, _ in neighs])

    def get_leaves(self, w):
        if not w.children:
            print('leaf: {}'.format(w.data))
            yield w.data

        else:
            for i in range(len(w.children)):
                print('processing child: {}'.format(w.children[i].data))
                yield from self.get_leaves(w.children[i])
        # result = []
        # if not w.children:
        #     result = w
        # else:
        #     result.extend([self.get_leaves(w.children[i]) for i in range(len(w.children))])
        # return result

    def get_neighbours(self, w, iter):
        if iter == 0:
            for child in w.children:
                yield child
        else:
            for i in range(len(w.children)):
                yield from self.get_neighbours(w.children[i], iter-1)

if __name__ == '__main__':
    model1 = Word2Vec.load('word2vec_1933')
    model2 = Word2Vec.load('word2vec_1990')
    graph_nei = GraphNeighbourBased(model1=model1, model2=model2, t=5, topK=100)
</code></pre>
","11212687","","","","","2021-02-11 18:07:40","tree/graph for word embedding","<python><tree><gensim><word-embedding>","0","0","","","","CC BY-SA 4.0"
"66892154","1","66899277","","2021-03-31 17:01:34","","1","51","<p>I was wondering whether <code>spacy</code> has some APIs to do phrase* extraction as one would do when using <code>word2phrase</code> or the <code>Phrases</code> class from <code>gensim</code>. Thank you.</p>
<p>PS. Phrases are also called collocations in Linguistics.</p>
","1177868","","","","","2021-04-03 04:48:34","Phrase extraction with Spacy","<nlp><spacy><gensim><phrase>","2","0","","","","CC BY-SA 4.0"
"50213754","1","","","2018-05-07 12:01:11","","1","409","<p>I'm working on something using gensim.</p>

<p>In gensim, var <code>index</code> usually means an object of <code>gensim.similarities.&lt;cls&gt;</code>.</p>

<p>At first, I use <code>gensim.similarities.Similarity(filepath, ...)</code> to save index as a file, and then loads it by <code>gensim.similarities.Similarity.load(filepath + '.0')</code>. Because <code>gensim.similarities.Similarity</code> default save index to shards file like <code>index.0</code>.</p>

<p>When index file becoming larger, it automatically seperate into more shards, like <code>index.0</code>,<code>index.1</code>,<code>index.2</code>......</p>

<p>How can I load these shards file? <code>gensim.similarities.Similarity.load()</code> can only load one file.</p>

<p>BTW: I have try to find the answer in gensim's doc, but failed.</p>
","6159371","","","","","2019-11-04 15:50:00","How to load index shards by gensim.similarities.SimilarityÔºü","<python><gensim>","2","0","","","","CC BY-SA 4.0"
"65452387","1","","","2020-12-25 22:51:22","","0","32","<p>I want to <em>import Word2Vec</em> from <em>gensim.models</em>. The interpreter is telling me that there is an error like this:</p>
<blockquote>
<p>from gensim.models import Word2Vec</p>
<p>ImportError: cannot import name 'open' from 'smart_open' (C:\Users\DELL\anaconda3\envs\tensoflow\lib\site-packages\smart_open_<em>init</em>_.py)</p>
</blockquote>
","14855431","","3789665","","2020-12-26 12:01:17","2020-12-26 12:01:17","Gensim import failed","<gensim>","0","1","","","","CC BY-SA 4.0"
"40436110","1","","","2016-11-05 08:12:51","","1","813","<p>I am trying to calculate similarity. First of all i used RAKE library to extract the keywords from the crawled jobs. Then I put the keywords of every jobs into separate array and then combined all those arrays into documentArray.</p>
<blockquote>
<p>documentArray = ['Anger
command,Assertiveness,Approachability,Adaptability,Authenticity,Aggressiveness,Analytical
thinking,Molecular Biology,Molecular Biology,Molecular
Biology,molecular biology,molecular biology,Master,English,Molecular
Biology,,Islamabad,Islamabad District,Islamabad Capital
Territory,Pakistan,,Rawalpindi,Rawalpindi,Punjab,Pakistan'&quot;],
['competitive compensation,assay design,positive attitude,regular
basis,motivate others,meetings related,improve state,travel on,phd
degree,meeting abstracts,benefits package,daily basis,scientific
papers,application notes']</p>
</blockquote>
<hr />
<blockquote>
<p>queryStr = 'In Vitro,Biochemistry,PCR,Western
Blotting,Neuroscience,Molecular Biology,Cell
biology,Immunohistochemistry,Microscopy,Animal
Models,Presentations,Immunoprecipitation,Cell biology,Master's
Degree,Bachelor's Degree,,,,,'</p>
</blockquote>
<p>Then I wrote the following GENSIM code,</p>
<blockquote>
<p>class Gensim:</p>
<pre><code>def __init__(self):
    print(&quot;Init&quot;)

def calculateGensimSimilarity(self, texts, query):
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
    lda = models.LdaModel(corpus, id2word=dictionary, num_topics=2)
    index_lsi = similarities.MatrixSimilarity(lsi[corpus])
    index_lda = similarities.MatrixSimilarity(lda[corpus])
    vec_bow = dictionary.doc2bow(query.lower().split())
    vec_lsi = lsi[vec_bow]
    vec_lda = lda[vec_bow]
    print(&quot;LSI Model&quot;)
    sims_lsi = index_lsi[vec_lsi]
    print(&quot;LDA Model&quot;)
    print(sims_lsi)
    sims_lda = index_lda[vec_lda]
    print(sims_lda)
</code></pre>
</blockquote>
<p>It is printing LSA score 0 and LDA score 90%+ match. Kindly let me know where I am wrong and how can i modify to calculate the correct cosine similarity.</p>
<blockquote>
<p>LSA Score[ 0.  0.]
LDA Score[ 0.94234258  0.9477495 ]</p>
</blockquote>
","1068837","","-1","","2020-06-20 09:12:55","2016-11-05 11:25:48","RAKE with GENSIM","<python><rake><information-retrieval><gensim><cosine-similarity>","0","0","","","","CC BY-SA 3.0"
"66875191","1","","","2021-03-30 16:54:50","","0","12","<p>I am working on Topic Modelling using pyLDAvis and gensim. I provided the number of topics as 27 for the topic modeling. However, pyLDAvis does not display the top 30 keywords for topic 24 to topic 27. Attached is the screenshot of the pyLDAvis output.
<a href=""https://i.stack.imgur.com/QOk5d.png"" rel=""nofollow noreferrer"">pyLDAVis Output Screenshot</a></p>
<p>Why does pyLDAvis not display the keywords for certain topics?</p>
","12405148","","","","","2021-03-30 16:54:50","pyLDAVis is not Showing the Top 30 keywords for Few Topics","<gensim><topic-modeling><pyldavis>","0","0","","","","CC BY-SA 4.0"
"47930809","1","","","2017-12-21 18:34:44","","2","2132","<p>In the doc2vec model, Can we cluster on the vectors themselves? Should we cluster each resulting <code>model.docvecs[1]</code>vector? How to implement the clustering model?</p>

<pre><code> model = gensim.models.doc2vec.Doc2Vec(size= 100, min_count = 5,window=4, iter = 50, workers=cores)
    model.build_vocab(res) 
    model.train(res, total_examples=model.corpus_count, epochs=model.iter)


    # each of length 100
    len(model.docvecs[1])
</code></pre>
","8382950","","","","","2018-02-12 13:29:33","Doc2vec: clustering resulting vectors","<python><nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"30742973","1","","","2015-06-09 21:15:31","","2","164","<p>So I have a general bow corpus that I have created that yields documents per the format that <code>gensim</code> requires (<a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">see here</a>.)</p>

<p>However those documents have a lot of words that are used extremely often.  So I wanted to use a <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">tfidf</a> to balance that out.</p>

<p>So I do something like</p>

<pre><code>tfidf_model = TfidfModel(corpus)
new_corpus = tfidf_model[corpus]
</code></pre>

<p>Now I want to train my LDA</p>

<pre><code>lda = LdaModel(corpus=new_corpus, num_topics=16)
</code></pre>

<p>And it trains and converges fine...great.  Now I have a new unseen document that I want to project onto my lda topics.  Do I always need to project this new doc with the <code>tfidf_model</code> first?  i.e.</p>

<pre><code>transformed_doc = tfidf_model[unseen_doc]
projections = lda[transformed_doc]
</code></pre>

<p>Or can <code>gensim</code> take the original and know to apply the <code>tfidf</code> first then project onto the <code>lda</code>.  </p>

<pre><code>projections = lda[unseen_doc]
</code></pre>

<p>The <code>gensim</code> docs are a little unclear on whether or not the model knows any other previous transformations were applied to a corpus.</p>
","2236401","","","","","2016-11-22 02:02:56","Do I need to transform unseen documents before projecting them onto model topics?","<python><tf-idf><lda><gensim>","0","0","","","","CC BY-SA 3.0"
"35778075","1","","","2016-03-03 16:46:19","","2","943","<p>I use the Gensim package for topic modelling. The idea is to understand what are the topics in the flickr tags.
Till now I am using this code (document are tags):</p>

<pre><code>    texts = [[word for word in document.split("";"") if word not in stoplist] for document in documents]
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    lda = ldamodel.LdaModel(corpus, id2word=dictionary, alpha = 0.1, num_topics=10)
    topic = []
    for f in lda.print_topics(num_topics=4, num_words=10):
        topic_number = f[0]
        keywords = f[1]
        keywords = keywords.split("" + "")
        keywords_update = {}
        for ii in keywords:
            ii = str(ii)
            keyword = ii[6:]
            probab = ii[0:5]
            probab = float(probab)
            if probab &gt; 0.02:
                keywords_update.update({keyword:probab})
        topic.append(keywords_update)
    print topic
</code></pre>

<p>So basically I train the LDA on all my documents and then print the 10 most probable words for every topic. Is it correct? Or do I have to train the data on some part of the documents and then use corpus_lda = lda[corpus] in order to apply the trained model on the unseen documents?
If the results are different every time I run the model, does it mean that the amount of the topics is not correct? What is the best way to evaluate the results?</p>
","5672618","","","","","2017-05-29 18:11:11","LDA for tags (gensim)","<python><lda><gensim>","1","1","0","","","CC BY-SA 3.0"
"50227208","1","","","2018-05-08 06:26:16","","0","173","<p>Using <a href=""https://github.com/dav/word2vec"" rel=""nofollow noreferrer"">https://github.com/dav/word2vec</a>, I have built Word2Vec embedding. I have to use that output in Python's Gensim's Word2Vec. </p>

<p>I have VectorsFile.txt and VocabuloryFile.txt for my Corpus.txt.  </p>

<p>How to do that .. </p>

<p>Can you please .. ?  </p>
","7955314","","","","","2018-05-08 06:26:16","Converting Word2Vec embedding in C to Python's Gensim Word2vec","<python><c><tensorflow><word2vec><gensim>","0","2","","","","CC BY-SA 4.0"
"46860197","1","46867142","","2017-10-21 04:58:22","","6","4136","<p>My current doc2vec code is as follows.</p>

<pre><code># Train doc2vec model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4, iter = 20)
</code></pre>

<p>I also have a word2vec code as below.</p>

<pre><code> # Train word2vec model
model = word2vec.Word2Vec(sentences, size=300, sample = 1e-3, sg=1, iter = 20)
</code></pre>

<p>I am interested in using both DM and DBOW in <strong>doc2vec</strong> AND both Skip-gram and CBOW in <strong>word2vec</strong>.</p>

<p>In Gensim I found the below mentioned sentence:
<strong>""Produce word vectors with deep learning via word2vec‚Äôs ‚Äúskip-gram and CBOW models‚Äù, using either hierarchical softmax or negative sampling""</strong></p>

<p>Thus, I am confused either to use hierarchical softmax or negative sampling. Please let me know what are the <strong>differences</strong> in these two methods.</p>

<p>Also, I am interested in knowing <strong>what are the parameters that need to be changed</strong> to use <strong>hierarchical softmax</strong> AND/OR <strong>negative sampling</strong> with respect to <strong>dm, DBOW, Skip-gram and CBOW</strong>?</p>

<p>P.s. my application is a recommendation system :)</p>
","","user8566323","","","","2017-10-23 01:25:42","Doc2vec and word2vec with negative sampling","<python><nlp><word2vec><gensim><doc2vec>","1","0","4","","","CC BY-SA 3.0"
"30745184","1","30793067","","2015-06-10 00:37:51","","3","5752","<p>I wish to know the default number of iterations in <strong>gensim</strong>'s LDA (Latent Dirichlet Allocation) algorithm. I don't think the documentation talks about this. (Number of iterations is denoted by the parameter <strong>iterations</strong> while initializing the <strong>LdaModel</strong> ). Thanks !</p>
","2233336","","2233336","","2015-06-10 01:09:05","2017-12-12 00:14:42","Gensim LDA - Default number of iterations","<python><topic-modeling><gensim>","3","0","","","","CC BY-SA 3.0"
"61983014","1","","","2020-05-24 07:45:57","","0","250","<p>I try import gensim library in python3. 
all of the libraries are last version.
in first import got an <code>TypeError: expected bytes, Descriptor found</code>
and in second import got an this error:</p>

<pre><code> import gensim

&gt; AttributeError                            Traceback (most recent call last)
&lt;ipython-input-7-e70e92d32c6e&gt; in &lt;module&gt;
----&gt; 1 import gensim

~\Anaconda3\lib\site-packages\gensim\__init__.py in &lt;module&gt;
      3 """"""
      4 

~\Anaconda3\lib\site-packages\smart_open\transport.py in &lt;module&gt;
     20 NO_SCHEME = ''
     21 
---&gt; 22 _REGISTRY = {NO_SCHEME: smart_open.local_file}
     23 
     24 

AttributeError: module 'smart_open' has no attribute 'local_file'
</code></pre>
","13260089","","","","","2020-05-27 06:37:21","import gensim and got an TypeError: expected bytes, Descriptor found","<python><import><typeerror><gensim><attributeerror>","1","2","","","","CC BY-SA 4.0"
"44605649","1","44616754","","2017-06-17 14:15:59","","0","1139","<p>I'm recently interested in NLP, and would like to build up search engine for product recommendation. (Actually I'm always wondering about how search engine for Google/Amazon is built up)</p>

<p>Take Amazon product as example, where I could access all ""word"" information about one product:</p>

<pre><code>Product_Name    Description      ReviewText
""XXX brand""    ""Pain relief""    ""This is super effective""
</code></pre>

<p>By applying <code>nltk</code> and <code>gensim</code> packages I could easily compare similarity of different products and make recommendations.</p>

<p>But here's another question I feel very vague about:
How to build a search engine for such products? </p>

<p>For example, if I feel pain and would like to search for medicine online, I'd like to type-in <code>""pain relief""</code> or <code>""pain""</code>, whose searching results should include <code>""XXX brand""</code>.</p>

<p>So this sounds more like keyword extraction/tagging question? How should this be done in NLP? I know <strong>corpus</strong> should contain <strong>all</strong> but <strong>single</strong> words, so it's like:</p>

<pre><code>[""XXX brand"" : (""pain"", 1),(""relief"", 1)]
</code></pre>

<p>So if I typed in either <code>""pain""</code> or <code>""relief""</code> I could get <code>""XXX brand""</code>; but what about I searched <code>""pain relief""</code>?</p>

<p>I could come up with idea that directly call python in my javascript for calculate similarities of input words <code>""pain relief""</code> on browser-based server and make recommendation; but that's kind of do-able? </p>

<p>I still prefer to build up very big lists of keywords at backends, stored in datasets/database and directly visualized in web page of search engine.</p>

<p>Thanks!</p>
","815408","","","","","2017-06-18 15:47:35","Natural language processing keywords for building search engine","<nlp><nltk><search-engine><gensim><corpus>","1","1","","","","CC BY-SA 3.0"
"65456176","1","","","2020-12-26 12:02:45","","0","41","<p>While preparing dataset for the Latent Dirchlet Allocation model (gensim library), I removed all common stop-words, did the tokenization, lemmatization, bigrams, BoW etc. (it might be important that my model does not work with English language, I found an appropriate dictionary in Spacy libraries though). Additionally, I created another file with stop words, that are not so common, but in my opinion are definitely useless and have no predictive power - let say my dataset is created from chats form helpdesk - words like 'man', 'woman', 'last', 'monday' are not neccessary, but were not included in basic 'stop-words file'. I did it also for reducing a dictionaery - it contains (together with bigrams) around 1800 words now.</p>
<p>The problem I face, is that each time I run the LDA model I get different topics - I use pyLDAvis to visualization - each time I have a feeling that key words for topic are different, so are 'distances' between topics. In other words I would not say, that the model has any reproducibility in its predictions. What I have tried already:</p>
<ul>
<li>checked coherence measure to choose the optimal number of topics</li>
<li>tried many values for random_state parameter</li>
<li>set 'iterations' parameter much higher than in default (to allow the model get the 'true' convergence each time)</li>
<li>tried to run the model on bigger and smaller datasets</li>
<li>tried to run the model on random subsamples of the dataset (let say 5k sample from 50k records) few times to check if result are stable</li>
</ul>
<p>Because results still seems to be unsatisfied, I added and added another words to my 'stop-words file' to reduce the dictionary again, and was hoping it'll make the model more reproducible. Anyway, while there is around 700 houndreds words treating as stop words, I still cannot say the model works as I expected.</p>
<p>What can I do more? Is there any possibility that the dataset is 'too bad' to be used (it's a real data set)?
Maybe making my own 'stop words file' was a mistake? Regarding the second question, I was also trying to remove only the common stop words like 'we, I, you, me' etc. and then do the Tf-Idf, which should do this task for me. But then, I read, that it's usually better to use the 'bag of words set' as the input for the LDA model, not the Tf-Idf representation - indeed, during tests, Tf-Idf representation of data set caused even worse results.</p>
<p>I will appreciate any help,</p>
<p>Thanks</p>
","8308727","","","","","2020-12-26 12:02:45","Text cleaning for topic modeling - how far to do so?","<python><gensim><text-processing><topic-modeling><text-recognition>","0","0","","","","CC BY-SA 4.0"
"47959639","1","","","2017-12-24 09:58:48","","1","1278","<p>I have a set of embeddings trained with a neural network that has nothing to do with gensim's word2vec.</p>

<p>I want to use these embeddings as the initial weights in <code>gensim.Word2vec</code>.</p>

<p>Now what I did see is that I can <code>model.load(SOME_MODEL)</code> and then continue training, but it requires a gensim modle as input. Also <code>reset_from()</code> seems to only accept other gensim model.
But in my case, I don't have a gensim model to start from, but a text file in word2vec format of embeddings.</p>

<p>So how do I start transfer learning from an word2vec text file to <code>gensim.Word2vec</code>?</p>
","5368083","","","","","2017-12-24 15:02:56","gensim Word2vec transfer learning (from a non-gensim model)","<python><word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"27139908","1","27462798","","2014-11-26 01:35:41","","24","16851","<p>I am using the Gensim Python package to learn a neural language model, and I know that you can provide a training corpus to learn the model. However, there already exist many precomputed word vectors available in text format (e.g. <a href=""http://www-nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">http://www-nlp.stanford.edu/projects/glove/</a>). Is there some way to initialize a Gensim Word2Vec model that just makes use of some precomputed vectors, rather than having to learn the vectors from scratch?</p>

<p>Thanks! </p>
","1927885","","","","","2018-12-15 00:16:31","Load PreComputed Vectors Gensim","<python><nlp><gensim><word2vec>","3","0","9","","","CC BY-SA 3.0"
"35727272","1","","","2016-03-01 15:46:52","","1","97","<p>I tried to install gensim for word2vec project and it stuck at the point like this:</p>

<pre><code>----------------------------------------
  Rolling back uninstall of scipy
Command /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c ""import setuptools;__file__='/var/folders/6t/wqrgwp2s5zv5tnyz3rh85mvh0000gn/T/pip-build/scipy/setup.py';exec(compile(open(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /var/folders/6t/wqrgwp2s5zv5tnyz3rh85mvh0000gn/T/pip-IDV4D9-record/install-record.txt --single-version-externally-managed failed with error code 1 in /var/folders/6t/wqrgwp2s5zv5tnyz3rh85mvh0000gn/T/pip-build/scipy
Storing complete log in /Users/Myname/.pip/pip.log
</code></pre>

<p>And I did some research I realize it's sth wrong with the scipy. I tried to upgrade it (since I've installed long time ago) and it repetitively show this error. I tried ""sudo"" and install libblas-dev but still show this error.</p>

<pre><code>   ----------------------------------------
  Rolling back uninstall of scipy
Command /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c ""import setuptools;__file__='/tmp/pip-build/scipy/setup.py';exec(compile(open(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-7Z45Gz-record/install-record.txt --single-version-externally-managed failed with error code 1 in /tmp/pip-build/scipy
Storing complete log in /Users/MyName/.pip/pip.log
</code></pre>

<p>I guess it's the same core problem essentially and I stuck here for long time.</p>

<p>Could someone help me please?</p>
","4196483","","","","","2016-03-01 15:46:52","When upgrading scipy, I came across Storing complete log in pip.log","<python><scipy><gensim>","0","1","","","","CC BY-SA 3.0"
"16259652","1","","","2013-04-28 04:37:21","","2","1768","<p>I have corpus with around 20,000 documents and I have to train that data set for topic modelling using LDA.</p>

<pre><code>import logging, gensim

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
id2word = gensim.corpora.Dictionary('questions.dict')
mm = gensim.corpora.MmCorpus('questions.mm')
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=0, chunksize=3000, passes=20)
lda.print_topics(20)
</code></pre>

<p>Whenever I run this program I come across this error:</p>

<pre><code>2013-04-28 09:57:09,750 : INFO : adding document #0 to Dictionary(0 unique tokens)
2013-04-28 09:57:09,759 : INFO : built Dictionary(11 unique tokens) from 14 documents (total 14 corpus positions)
2013-04-28 09:57:09,785 : INFO : loaded corpus index from questions.mm.index
2013-04-28 09:57:09,790 : INFO : initializing corpus reader from questions.mm
2013-04-28 09:57:09,796 : INFO : accepted corpus with 19188 documents, 15791 features, 106222 non-zero entries
2013-04-28 09:57:09,802 : INFO : using serial LDA version on this node
2013-04-28 09:57:09,808 : INFO : running batch LDA training, 100 topics, 20 passes over the supplied corpus of 19188 documents, updating model once every 19188 documents
2013-04-28 09:57:10,267 : INFO : PROGRESS: iteration 0, at document #3000/19188

Traceback (most recent call last):
File ""C:/Users/Animesh/Desktop/NLP/topicmodel/lda.py"", line 10, in &lt;module&gt;
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=0, chunksize=3000, passes=20)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 265, in __init__
self.update(corpus)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 445, in update
self.do_estep(chunk, other)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 365, in do_estep
gamma, sstats = self.inference(chunk, collect_sstats=True)
File ""C:\Python27\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\models\ldamodel.py"", line 318, in inference
expElogbetad = self.expElogbeta[:, ids]
IndexError: index (11) out of range (0&lt;=index&lt;10) in dimension 1
</code></pre>

<p>I even tried to change the values in <code>LdaModel</code> function but I always get the same error!</p>

<p>What should be done ?</p>
","1461896","","6573902","","2021-01-25 14:58:36","2021-01-25 14:58:36","Applying LDA to a corpus for training using gensim","<python><nlp><gensim><lda><topic-modeling>","1","1","2","","","CC BY-SA 3.0"
"35681403","1","","","2016-02-28 10:35:06","","1","250","<p>I am trying to find out new concepts in a Corpus from Konkani language.
I had trained two models on 1) a domain specific corpus 2) on newspaper corpus.</p>

<p>I have used Gensim word2vec to train the model however I am unable to get the terms of similar meaning on close proximity in vector space.</p>

<p>The closes words show no relation of being synonym with each other. Their similarity is as good as just some random words.</p>

<p>What am i doing wrong?</p>
","1516947","","3907250","","2017-03-21 21:25:35","2017-03-21 21:25:35","Finding concepts from a large corpus using Word embeddings","<gensim><word2vec><word-embedding>","1","0","0","","","CC BY-SA 3.0"
"32744732","1","","","2015-09-23 16:23:21","","1","1062","<p>I am learning about Doc2Vec and the gensim library. I have been able to train my model by creating a corpus of documents such as</p>

<pre><code>LabeledSentence(['what', 'happens', 'when', 'an', 'army', 'of', 'wetbacks', 'towelheads', 'and', 'godless', 'eastern', 'european', 'commies', 'gather', 'their', 'forces', 'south', 'of', 'the', 'border', 'gary', 'busey', 'kicks', 'their', 'butts', 'of', 'course', 'another', 'laughable', 'example', 'of', 'reagan-era', 'cultural', 'fallout', 'bulletproof', 'wastes', 'a', 'decent', 'supporting', 'cast', 'headed', 'by', 'l', 'q', 'jones', 'and', 'thalmus', 'rasulala'], ['LABELED_10', '0'])`
</code></pre>

<p>note that this particular document has two tags, namely 'LABELED_10' and  '0'.</p>

<p>Now after i load my model and perform </p>

<pre><code>print(model.docvecs.most_similar(""LABELED_10""))
</code></pre>

<p>i get </p>

<pre><code>[('LABELED_107', 0.48432376980781555), ('LABELED_110', 0.4827481508255005), ('LABELED_214', 0.48039984703063965), ('LABELED_207', 0.479473352432251), ('LABELED_315', 0.47931796312332153), ('LABELED_307', 0.47898322343826294), ('LABELED_124', 0.4776897132396698), ('LABELED_222', 0.4768940210342407), ('LABELED_413', 0.47479286789894104), ('LABELED_735', 0.47462597489356995)]
</code></pre>

<p>which is perfect ! as i get all the tags most similar to LABELED_10. </p>

<p>Now i would like to have a feedback loop while training my model. So if i give my model a new document, i would like to know how good or bad the model's classification is before tagging and adding that document to my corpus. How would i do that using Doc2Vec? So how do i know whether the documents for LABELED_107 and LABELED_10 are actually similar or not. Here is one approach that i have in mind. Here is the code for my random forest classifier</p>

<pre><code>result = cfun.rfClassifer(n_estimators, trainingDataFV, train[""sentiment""],testDataFV)
</code></pre>

<p>and here is the function</p>

<pre><code>def rfClassifer(n_estimators, trainingSet, label, testSet):

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    forest = RandomForestClassifier(n_estimators)
    forest = forest.fit(trainingSet, label)
    result = forest.predict(testSet)

    return result
</code></pre>

<p>and finally i can do</p>

<pre><code>output = pd.DataFrame(data={""id"": test[""id""], ""sentiment"": result})

output.to_csv(""../../submits/Doc2Vec_AvgVecPredict.csv"", index=False, quoting=3)
</code></pre>

<p>Feedback process</p>

<ol>
<li><p>Keep a validation set which is tagged correctly.</p></li>
<li><p>Feed the tagged validation set to the classifier after removing the tags and save the result in a csv.</p></li>
<li><p>Compare the result with another csv that has the correct tags.</p></li>
<li><p>For every mismatch, add those documents to the labeled training set and train the model again.</p></li>
<li><p>Repeat for more validation sets.</p></li>
</ol>

<p>Is this approach correct? Also, can i incrementally train the doc2vec model? Lets say that initially i trained my doc2vec model with 100k tagged docs. Now after the validation step, i need my model to be trained on a further 10k documents. Will i have to train my model from the very beginning ? Meaning will i need to train my model on the initial 100k tagged docs again?</p>

<p>I would really appreciate your insights.</p>

<p>Thanks </p>
","2334092","","2334092","","2015-09-23 19:44:26","2016-03-28 16:15:32","Doc2Vec: Best practice for feedback loop for training the model","<nlp><python-3.4><gensim><word2vec>","1","2","0","","","CC BY-SA 3.0"
"47929028","1","","","2017-12-21 16:27:43","","2","653","<p>I am trying doc2vec for 600000 rows of sentences and my code is below:</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(size= 100, min_count = 5,window=4, iter = 50, workers=cores)
model.build_vocab(res) 
model.train(res, total_examples=model.corpus_count, epochs=model.iter)

#len(res) = 663406

#length of unique words 15581
print(len(model.wv.vocab))

#length of doc vectors is 10
len(model.docvecs)

# each of length 100
len(model.docvecs[1])
</code></pre>

<p>How do I interpret this result? why is the length of vector only 10 with each of size 100? when the length of 'res' is 663406, it does not make sense. I know something is wrong here.</p>

<p>In <a href=""https://stackoverflow.com/questions/37196520/understanding-the-output-of-doc2vec-from-gensim-package?rq=1"">Understanding the output of Doc2Vec from Gensim package</a>, they mention that the length of docvec is determined by 'size' which is not clear. </p>
","8382950","","","","","2018-01-05 19:42:01","Doc2vec: model.docvecs is only of length 10","<python><nlp><gensim><doc2vec>","1","0","1","","","CC BY-SA 3.0"
"66908630","1","","","2021-04-01 16:52:53","","0","38","<p>I mean without having to manually compute perplexity/coherence score, and then provide LDA with the number of topics. I was able to find packages such &quot;ldatuning&quot; in other languages but nothing for gensim (python). The packages I found in other languages basically look at a couple of metrics such as perplexity, coherence, etc and automatically assign the model the optimal number of topics</p>
","4936133","","","","","2021-04-01 16:52:53","Is there a way to make gensim LDA automatically pick the optimal number of topics","<python><gensim><lda>","0","0","","","","CC BY-SA 4.0"
"66915255","1","","","2021-04-02 06:25:36","","0","30","<p>I am using the wikipidia corpus (17G) and python language to obtain a n-gram model. I've used <code>sklearn.feature_extraction.text.CountVectorizer</code> but it does not has any progress bar or verbose option. Is there a recommended way?</p>
","12892962","","","","","2021-04-03 16:28:54","Generate n-gram from a large corpus","<scikit-learn><nlp><nltk><gensim><n-gram>","1","3","","","","CC BY-SA 4.0"
"16262016","1","16301403","","2013-04-28 10:39:43","","9","15359","<p>I have trained a corpus for LDA topic modelling using gensim.</p>

<p>Going through the tutorial on the gensim website (this is not the whole code):</p>

<pre><code>question = 'Changelog generation from Github issues?';

temp = question.lower()
for i in range(len(punctuation_string)):
    temp = temp.replace(punctuation_string[i], '')

words = re.findall(r'\w+', temp, flags = re.UNICODE | re.LOCALE)
important_words = []
important_words = filter(lambda x: x not in stoplist, words)
print important_words
dictionary = corpora.Dictionary.load('questions.dict')
ques_vec = []
ques_vec = dictionary.doc2bow(important_words)
print dictionary
print ques_vec
print lda[ques_vec]
</code></pre>

<p>This is the output that I get:</p>

<pre><code>['changelog', 'generation', 'github', 'issues']
Dictionary(15791 unique tokens)
[(514, 1), (3625, 1), (3626, 1), (3627, 1)]
[(4, 0.20400000000000032), (11, 0.20400000000000032), (19, 0.20263215848547525), (29, 0.20536784151452539)]
</code></pre>

<p>I don't know how the last output is going to help me find the possible topic for the <code>question</code> !!!</p>

<p>Please help!</p>
","1461896","","1461896","","2013-04-28 10:48:48","2016-06-18 05:00:52","How to predict the topic of a new query using a trained LDA model using gensim?","<python><nlp><lda><topic-modeling><gensim>","3","0","11","","","CC BY-SA 3.0"
"50275623","1","50277499","","2018-05-10 14:44:57","","9","8228","<p>I was confused with the results of most_similar and similar_by_vector from gensim's Word2vecKeyedVectors. They are supposed to calculate cosine similarities in the same way - however:</p>

<p>Running them with one word gives identical results, for example:
model.most_similar(['obama']) and similar_by_vector(model['obama'])</p>

<p>but if I give it an equation:</p>

<pre><code>model.most_similar(positive=['king', 'woman'], negative=['man'])
</code></pre>

<p>gives:</p>

<pre><code>[('queen', 0.7515910863876343), ('monarch', 0.6741327047348022), ('princess', 0.6713887453079224), ('kings', 0.6698989868164062), ('kingdom', 0.5971318483352661), ('royal', 0.5921063423156738), ('uncrowned', 0.5911505818367004), ('prince', 0.5909028053283691), ('lady', 0.5904011130332947), ('monarchs', 0.5884358286857605)]
</code></pre>

<p>while with:</p>

<pre><code>q = model['king'] - model['man'] + model['woman']
model.similar_by_vector(q)
</code></pre>

<p>gives:</p>

<pre><code>[('king', 0.8655095100402832), ('queen', 0.7673765420913696), ('monarch', 0.695580005645752), ('kings', 0.6929547786712646), ('princess', 0.6909604668617249), ('woman', 0.6528975963592529), ('lady', 0.6286187767982483), ('prince', 0.6222133636474609), ('kingdom', 0.6208546161651611), ('royal', 0.6090123653411865)]
</code></pre>

<p>There is a noticable difference in cosine distance of the words queen, monarch... etc. I'm wondering why?</p>

<p>Thanks!</p>
","7463805","","","","","2018-05-10 16:32:01","Difference between most_similar and similar_by_vector in gensim word2vec?","<nlp><word2vec><gensim>","1","0","","","","CC BY-SA 4.0"
"49339461","1","","","2018-03-17 16:53:25","","0","513","<p>I want to do word_embedding using LDA to represent each of documents in my corpus with a vector that each dimension shows one of the detected topics by LDA model, but I don't know how to do it. Any suggestion will be appreciated.
I use python 3.6 and gensim library for LDA.</p>
","667355","","","","","2018-03-17 16:53:25","Using LDA for word embedding","<machine-learning><gensim><lda><topic-modeling><word-embedding>","0","2","1","","","CC BY-SA 3.0"
"46888984","1","","","2017-10-23 12:07:55","","2","200","<p>The code found test.py runs correctly. But when importing test.py in the test2.py file the line that creates the LdaMulticore model seems to be stuck.</p>

<p>I have added the example code to illustrate the problem. Is there a solution for this? </p>

<p>test.py:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from gensim import corpora, models
from gensim.matutils import Sparse2Corpus
import time

data_clean = [""This is the first example document."",""This is the second example document."",""This is the third and last exaple document""]

vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0.0001, max_df=0.8,stop_words = 'english')
matrix =  vectorizer.fit_transform(data_clean)

id2words = dict()
for k, v in vectorizer.vocabulary_.iteritems():
    id2words[v] = k

train_corpus = Sparse2Corpus(matrix, documents_columns=False)

if __name__ == 'test':
    print ""The file is being imported""
    model = models.LdaMulticore(train_corpus ,id2word=id2words,num_topics=10, workers=4)
else:
    print ""The file is directly executed""
    model = models.LdaMulticore(train_corpus ,id2word=id2words,num_topics=10, workers=4)
</code></pre>

<p>test2.py:</p>

<pre><code>import test
</code></pre>

<p>Terminal output : 
<a href=""https://i.stack.imgur.com/m8x2V.png"" rel=""nofollow noreferrer"">terminal output when running test2.py</a></p>
","8731742","","","","","2017-10-23 12:07:55","Gensim models.LdaMulticore() not executing when imported trough other file","<python><machine-learning><gensim><lda>","0","0","","","","CC BY-SA 3.0"
"50278744","1","50286638","","2018-05-10 17:52:21","","7","1939","<p>I currently have following script that helps to find the best model for a doc2vec model. It works like this: First train a few models based on given parameters and then test against a classifier. Finally, it outputs the best model and classifier (I hope).</p>

<p><strong>Data</strong></p>

<p>Example data (data.csv) can be downloaded here: <a href=""https://pastebin.com/takYp6T8"" rel=""noreferrer"">https://pastebin.com/takYp6T8</a>
Note that the data has a structure that should make an ideal classifier with 1.0 accuracy.</p>

<p><strong>Script</strong></p>

<pre><code>import sys
import os
from time import time
from operator import itemgetter
import pickle
import pandas as pd
import numpy as np
from argparse import ArgumentParser

from gensim.models.doc2vec import Doc2Vec
from gensim.models import Doc2Vec
import gensim.models.doc2vec
from gensim.models import KeyedVectors
from gensim.models.doc2vec import TaggedDocument, Doc2Vec

from sklearn.base import BaseEstimator
from gensim import corpora

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


dataset = pd.read_csv(""data.csv"")

class Doc2VecModel(BaseEstimator):

    def __init__(self, dm=1, size=1, window=1):
        self.d2v_model = None
        self.size = size
        self.window = window
        self.dm = dm

    def fit(self, raw_documents, y=None):
        # Initialize model
        self.d2v_model = Doc2Vec(size=self.size, window=self.window, dm=self.dm, iter=5, alpha=0.025, min_alpha=0.001)
        # Tag docs
        tagged_documents = []
        for index, row in raw_documents.iteritems():
            tag = '{}_{}'.format(""type"", index)
            tokens = row.split()
            tagged_documents.append(TaggedDocument(words=tokens, tags=[tag]))
        # Build vocabulary
        self.d2v_model.build_vocab(tagged_documents)
        # Train model
        self.d2v_model.train(tagged_documents, total_examples=len(tagged_documents), epochs=self.d2v_model.iter)
        return self

    def transform(self, raw_documents):
        X = []
        for index, row in raw_documents.iteritems():
            X.append(self.d2v_model.infer_vector(row))
        X = pd.DataFrame(X, index=raw_documents.index)
        return X

    def fit_transform(self, raw_documents, y=None):
        self.fit(raw_documents)
        return self.transform(raw_documents)


param_grid = {'doc2vec__window': [2, 3],
              'doc2vec__dm': [0,1],
              'doc2vec__size': [100,200],
              'logreg__C': [0.1, 1],
}

pipe_log = Pipeline([('doc2vec', Doc2VecModel()), ('log', LogisticRegression())])

log_grid = GridSearchCV(pipe_log, 
                        param_grid=param_grid,
                        scoring=""accuracy"",
                        verbose=3,
                        n_jobs=1)

fitted = log_grid.fit(dataset[""posts""], dataset[""type""])

# Best parameters
print(""Best Parameters: {}\n"".format(log_grid.best_params_))
print(""Best accuracy: {}\n"".format(log_grid.best_score_))
print(""Finished."")
</code></pre>

<p>I do have following questions regarding my script (I combine them here to avoid three posts with the same code snippet):</p>

<ol>
<li>What's the purpose of <code>def __init__(self, dm=1, size=1, window=1):</code>? Can I possibly remove this part, somehow (tried unsuccessfully)?</li>
<li>How can I add a <code>RandomForest</code> classifier (or others) to the GridSearch workflow/pipeline?</li>
<li>How could a train/test data split added to the code above, as the current script only trains on the full dataset?</li>
</ol>
","4697646","","4697646","","2019-01-31 19:00:03","2019-01-31 19:00:03","Pipeline and GridSearch for Doc2Vec","<scikit-learn><pipeline><gensim><grid-search>","1","2","1","","","CC BY-SA 4.0"
"46899062","1","46902581","","2017-10-23 21:48:12","","1","3350","<p>I want to train a word2vec model on a tokenized file of size 400MB. I have been trying to run this python code :
</p>

<pre><code>import operator
import gensim, logging, os
from gensim.models import Word2Vec
from gensim.models import *

class Sentences(object):
    def __init__(self, filename):
        self.filename = filename

    def __iter__(self):
        for line in open(self.filename):
            yield line.split()

def runTraining(input_file,output_file):
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    sentences = Sentences(input_file)
    model = gensim.models.Word2Vec(sentences, size=200)
    model.save(output_file)
</code></pre>

<p></p>

<p>When I call this function on my file, I get this :</p>

<pre><code>2017-10-23 17:57:00,211 : INFO : collecting all words and their counts
2017-10-23 17:57:04,071 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2017-10-23 17:57:16,116 : INFO : collected 4735816 word types from a corpus of 47054017 raw words and 1 sentences
2017-10-23 17:57:16,781 : INFO : Loading a fresh vocabulary
2017-10-23 17:57:18,873 : INFO : min_count=5 retains 290537 unique words (6% of original 4735816, drops 4445279)
2017-10-23 17:57:18,873 : INFO : min_count=5 leaves 42158450 word corpus (89% of original 47054017, drops 4895567)
2017-10-23 17:57:19,563 : INFO : deleting the raw counts dictionary of 4735816 items
2017-10-23 17:57:20,217 : INFO : sample=0.001 downsamples 34 most-common words
2017-10-23 17:57:20,217 : INFO : downsampling leaves estimated 35587188 word corpus (84.4% of prior 42158450)
2017-10-23 17:57:20,218 : INFO : estimated required memory for 290537 words and 200 dimensions: 610127700 bytes
2017-10-23 17:57:21,182 : INFO : resetting layer weights
2017-10-23 17:57:24,493 : INFO : training model with 3 workers on 290537 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2017-10-23 17:57:28,216 : INFO : PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:32,107 : INFO : PROGRESS: at 20.00% examples, 1314 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:36,071 : INFO : PROGRESS: at 40.00% examples, 1728 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:41,059 : INFO : PROGRESS: at 60.00% examples, 1811 words/s, in_qsize 0, out_qsize 0
Killed
</code></pre>

<p>I know that word2vec needs a lot of space, but I still think there is a problem here. As you see the estimated memory for this model is of 600MB, while my computer has 16GB of RAM. Yet monitoring the process while the code runs shows that it occupies all of my memory and then gets killed.</p>

<p>As other posts advise I have tried to increase min_count and decrease size. But even with ridiculous values (min_count=50, size=10) the process stops at 60%.</p>

<p>I also tried to make python an exception to OOM so that the process doesn't get killed. When I do that, I have a MemoryError instead of the killing.</p>

<p>What is going on ?</p>

<p>(I use a recent laptop with Ubuntu 17.04, 16GB RAM and a Nvidia GTX 960M. I run python 3.6 from Anaconda and gensim 3.0, but it does'nt do better with gensim 2.3)</p>
","8821703","","8821703","","2017-10-23 21:59:42","2017-10-24 05:04:45","Gensim Word2Vec uses too much memory","<python-3.x><memory><word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"57226900","1","","","2019-07-26 20:54:44","","-1","243","<p>I have a quite long text document describing behaviours of different animals. I want to extract text about a specific animal and haven't figured out how this can be done.</p>

<p>So for example, if the document descibes 15 different animals, I want my alorithm to output all information from the input file that related to lions. Lions described and discussed in several different places of the document - how do I do ""selective extraction"" for text that is only related to lions, does anyone know?</p>

<h1>EDIT - inputs and outputs</h1>

<p>Inputs:
(1) Text file (e.g. ""document.txt"")
(2) Key word(s) (e.g. ""lion"")</p>

<p>Output (example):
""Lions are large felines that are traditionally depicted as the 'king of the jungle.' These big cats once roamed Africa, Asia and Europe. [...] Males are generally larger than females and have a distinctive mane of hair around their heads [...] Asiatic lions eat large animals as well, such as goats, nilgai, chital, sambhar and buffaloes. [...] Females have a gestation period of around four months. She will give birth to her young away from others and hide the cubs for the first six weeks of their lives.""</p>
","11843347","","11843347","","2019-07-26 21:04:42","2019-07-28 01:30:55","Selective text extraction in Python based on certain topics or keywords","<python><nlp><nltk><gensim><pythonanywhere>","1","2","","","","CC BY-SA 4.0"
"44606735","1","44695691","","2017-06-17 16:17:55","","0","917","<p>I want to extract topics from articles, the test article is ""<a href=""https://julien.danjou.info/blog/2017/announcing-scaling-python"" rel=""nofollow noreferrer"">https://julien.danjou.info/blog/2017/announcing-scaling-python</a>"".</p>

<p>It's an aticle about python and scalling. I've tried lsi and lda, most of time , lda seems works better. But the output of both of them isn't stable. </p>

<p>Of course, the first three or five keywords seem to hit the target. ""python"", ""book"", 'project' ( I don't think 'project' should be an useful topic and will drop it in stopwords list.) , scaling or scalable or openstack should be in keywords list, but not stable at all.</p>

<p>Topic list and stopwords list might improve the results, but it's not scalable. I have to maintain different list for different domain.</p>

<p>So the question here, is there any better solution to improve the algorithm?</p>

<pre><code>num_topics = 1
num_words = 10
passes = 20
</code></pre>

<h3>lda model demo code, code of lsi is the same.</h3>

<pre><code>for topic in lda.print_topics(num_words=num_words):
    termNumber = topic[0]
    print(topic[0], ':', sep='')
    listOfTerms = topic[1].split('+')
    for term in listOfTerms:
        listItems = term.split('*')
        print('  ', listItems[1], '(', listItems[0], ')', sep='')
        lda_list.append(listItems[1])
</code></pre>

<h3>Test Result 1</h3>

<pre><code>Dictionary(81 unique tokens: ['dig', 'shoot', 'lot', 'world', 'possible']...)
# lsi result
0:
  ""python"" (0.457)
  ""book"" ( 0.391)
  ""project"" ( 0.261)
  ""like"" ( 0.196)
  ""application"" ( 0.130)
  ""topic"" ( 0.130)
  ""new"" ( 0.130)
  ""openstack"" ( 0.130)
  ""way"" ( 0.130)
  ""decided""( 0.130)

# lda result
0:
  ""python"" (0.041)
  ""book"" ( 0.036)
  ""project"" ( 0.026)
  ""like"" ( 0.021)
  ""scalable"" ( 0.015)
  ""turn"" ( 0.015)
  ""working"" ( 0.015)
  ""openstack"" ( 0.015)
  ""scaling"" ( 0.015)
  ""different""( 0.015)
</code></pre>

<h3>Test Result 2</h3>

<pre><code>Dictionary(81 unique tokens: ['happy', 'idea', 'tool', 'new', 'shoot']...)
# lsi result
0:
  ""python"" (0.457)
  ""book"" ( 0.391)
  ""project"" ( 0.261)
  ""like"" ( 0.196)
  ""scaling"" ( 0.130)
  ""application"" ( 0.130)
  ""turn"" ( 0.130)
  ""working"" ( 0.130)
  ""openstack"" ( 0.130)
  ""topic""( 0.130)
# lda result
0:
  ""python"" (0.041)
  ""book"" ( 0.036)
  ""project"" ( 0.026)
  ""like"" ( 0.021)
  ""decided"" ( 0.015)
  ""different"" ( 0.015)
  ""turn"" ( 0.015)
  ""writing"" ( 0.015)
  ""working"" ( 0.015)
  ""application""( 0.015)
</code></pre>
","2672481","","","","","2017-06-22 09:39:25","how to improve topic model of gensim","<python><gensim><topic-modeling>","1","0","1","","","CC BY-SA 3.0"
"50253681","1","","","2018-05-09 12:40:01","","2","2062","<p>I am working on a word embedding project. I am using Amazon SageMaker for this purpose. The BlazingText algorithm in the Amazon SageMaker produced fast result than the other options. But I don't see any facility to get the prediction model or the weights. The output consists only the vectors file from which I cannot generate the model.
Is there any way by which I can get the model with the vector file? I need this to predict new words. Thanks in advance.</p>
","7239592","","3115607","","2018-05-09 12:44:23","2019-09-10 08:52:20","Amazon SageMaker BlazingText","<amazon-web-services><nlp><word2vec><gensim><amazon-sagemaker>","3","3","","","","CC BY-SA 4.0"
"16254207","1","16260043","","2013-04-27 16:05:52","","9","7180","<p>I have to apply LDA (Latent Dirichlet Allocation) to get the possible topics from a data base of 20,000 documents that I collected.</p>

<p>How can I use these documents rather than the other corpus available like the Brown Corpus or English Wikipedia as training corpus ?</p>

<p>You can refer <a href=""http://radimrehurek.com/gensim/models/ldamodel.html"">this</a> page.</p>
","1461896","","1461896","","2013-04-27 16:23:15","2017-11-07 15:56:26","Can we use a self made corpus for training for LDA using gensim?","<python><lda><gensim>","1","5","7","","","CC BY-SA 3.0"
"62085134","1","62091487","","2020-05-29 11:39:24","","0","59","<p>I wrote the code below, I used Used spacy to restrict the words in the tweets to content words, i.e., nouns, verbs, and adjectives. Transform the words to lower case and add the POS with an underderscore. E.g.:</p>

<p>love_VERB old-fashioneds_NOUN</p>

<p>now I want to Train 4 more Word2vec models and average the resulting embedding matrices.
but I dont have any idea for it, can you help me please ?</p>

<pre><code># Tokenization of each document
from gensim.models.word2vec import FAST_VERSION
from gensim.models import Word2Vec
import spacy
import pandas as pd
from zipfile import ZipFile
import wget

url = 'https://raw.githubusercontent.com/dirkhovy/NLPclass/master/data/reviews.full.tsv.zip'
wget.download(url, 'reviews.full.tsv.zip')

with ZipFile('reviews.full.tsv.zip', 'r') as zf:
    zf.extractall()

# nrows , max amount of rows
df = pd.read_csv('reviews.full.tsv', sep='\t', nrows=100000)
documents = df.text.values.tolist()

nlp = spacy.load('en_core_web_sm')  # you can use other methods
# excluded tags
included_tags = {""NOUN"", ""VERB"", ""ADJ""}


vocab = [s for s in new_sentences]

sentences = documents[:103]  # first 10 sentences
new_sentences = []
for sentence in sentences:
    new_sentence = []
    for token in nlp(sentence):
        if token.pos_ in included_tags:
            new_sentence.append(token.text.lower()+'_'+token.pos_)
    new_sentences.append(new_sentence)


# initialize model
w2v_model = Word2Vec(
                     size=100,
                     window=15,
                     sample=0.0001,
                     iter=200,
                     negative=5,
                     min_count=1,  # &lt;-- it seems your min_count was too high
                     workers=-1,
                     hs=0
                     )


new_sentences


w2v_model.build_vocab(vocab)

w2v_model.train(vocab, 
                total_examples=w2v_model.corpus_count, 
                epochs=w2v_model.epochs)
w2v_model.wv['car_NOUN']
</code></pre>
","13197554","","","","","2020-05-29 17:26:43","I want to Train 4 more Word2vec models and average the resulting embedding matrices","<python><pandas><nlp><spacy><gensim>","1","0","","","","CC BY-SA 4.0"
"46885454","1","46886353","","2017-10-23 09:00:26","","0","2184","<p>I tried to follow this documentation:
nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb
Where I have the following code snippet:</p>

<pre><code>ordered_vocab = [(term, voc.index, voc.count)
             for term, voc in food2vec.vocab.iteritems()]

ordered_vocab = sorted(ordered_vocab, key=lambda (term, index, count): -count)

ordered_terms, term_indices, term_counts = zip(*ordered_vocab)

word_vectors = pd.DataFrame(food2vec.syn0norm[term_indices, :],
                        index=ordered_terms
</code></pre>

<p>To get it to run i have change it to following:</p>

<pre><code>ordered_vocab = [(term, voc.index, voc.count)
             for term, voc in word2vecda.wv.vocab.items()]
ordered_vocab = sorted(ordered_vocab)
ordered_terms, term_indices, term_counts = zip(*ordered_vocab)
word_vectorsda = pd.DataFrame(word2vecda.wv.syn0norm[term_indices,],index=ordered_terms)
word_vectorsda [:20]
</code></pre>

<p>But the last line before I print the DataFrame give me an error I cannot get my head around. It keeps return that the noneType object cannot be in this line. To me, it looks like it is Term_indices there tracking it, but I do not get why? </p>

<pre><code> TypeError: 'NoneType' object is not subscriptable
</code></pre>

<p>Can any help me with this? Any inputs are most welcome
Best Niels</p>
","8718589","","3711601","","2017-10-23 09:36:02","2017-10-23 09:47:57","How to create a DataFrame with the word2ve vectors as data, and the terms as row labels?","<python-3.x><pandas><word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"46889727","1","48320687","","2017-10-23 12:44:40","","17","14970","<p>I am working on a recurrent language model. To learn word embeddings that can be used to initialize my language model, I am using gensim's word2vec model. 
After training, the word2vec model holds two vectors for each word in the vocabulary: the word embedding (rows of input/hidden matrix) and the context embedding (columns of hidden/output matrix).</p>

<p>As outlined in <a href=""https://stackoverflow.com/questions/36731784/wordvectors-how-to-concatenate-word-vectors-to-form-sentence-vector"">this post</a> there are at least three common ways to combine these two embedding vectors:</p>

<ol>
<li>summing the context and word vector for each word</li>
<li>summing &amp; averaging</li>
<li>concatenating the context and word vector</li>
</ol>

<p>However, I couldn't find proper papers or reports on the best strategy. So my questions are:</p>

<ol>
<li>Is there a common solution whether to sum, average or concatenate the vectors?</li>
<li>Or does the best way depend entirely on the task in question? If so, what strategy is best for a word-level language model?</li>
<li>Why combine the vectors at all? Why not use the ""original"" word embeddings for each word, i.e. those contained in the weight matrix between input and hidden neurons.</li>
</ol>

<p>Related (but unanswered) questions: </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/42119824/word2vec-summing-concatenate-inside-and-outside-vector?rq=1"">word2vec: Summing/concatenate inside and outside vector</a> </li>
<li><a href=""https://stackoverflow.com/questions/46065773/why-we-use-input-hidden-weight-matrix-to-be-the-word-vectors-instead-of-hidden-o?rq=1"">why we use input-hidden weight matrix to be the word vectors instead of hidden-output weight matrix?</a></li>
</ul>
","7384906","","7384906","","2017-10-24 07:38:32","2020-04-10 02:42:38","word2vec - what is best? add, concatenate or average word vectors?","<python><word2vec><gensim><word-embedding><language-model>","4","5","10","","","CC BY-SA 3.0"
"46914400","1","","","2017-10-24 15:25:38","","0","1847","<p>I want to construct word embeddings for documents using GloVe. I know how to obtain vector embeddings for single words (unigrams) as follows (for their example text document).</p>

<pre><code>$ git clone http://github.com/stanfordnlp/glove
$ cd glove &amp;&amp; make
$ ./demo.sh
</code></pre>

<p>Now, I want to obtain vector embeddings for bigrams. For example;</p>

<ol>
<li>""New york"" -> instead of ""New"", and ""york""</li>
<li>""machine learning"" -> instead of ""machine"", and ""learning""</li>
</ol>

<p>Is it possible to do in GloVe? If yes, how?</p>
","","user8566323","","","","2017-10-25 13:01:55","N-grams in GloVe","<nlp><stanford-nlp><data-mining><gensim><word-embedding>","1","2","","","","CC BY-SA 3.0"
"53232894","1","","","2018-11-09 20:32:30","","2","539","<p>Folks,</p>

<p>I have searched Google for different type of papers/blogs/tutorials etc but haven't found anything helpful. I would appreciate if anyone can help me. <strong>Please note that I am not asking for code step-by-step but rather an idea/blog/paper or some tutorial.</strong>   </p>

<p>Here's my problem statement:  </p>

<blockquote>
  <p>Just like sentiment analysis is used for identifying positive and
  negative tone of a sentence, I want to find whether a sentence is
  forward-looking (future outlook) statement or not.</p>
</blockquote>

<p>I do not want to use bag of words approach to sum up the number of forward-looking words/phrases such as <em>""going forward""</em>, ""<em>in near future</em>"" or ""<em>In 5 years from now</em>"" etc. I am not sure if word2vec or doc2vec can be used. Please enlighten me.  </p>

<p>Thanks. </p>
","202375","","","","","2018-11-10 07:39:21","Unsupervised sentiment Analysis using doc2vec","<nlp><gensim><word2vec><sentiment-analysis><doc2vec>","1","1","","","","CC BY-SA 4.0"
"66916861","1","","","2021-04-02 08:57:05","","0","20","<p>I have downloaded a file which includes the TransE embeddings of FreeBase's entities.
I want to know how to read and fetch the required data from it. I am using the following code:</p>
<pre><code>from gensim.models import FastText
model = FastText.load_fasttext_format('relation2vec.bin')
</code></pre>
<p>This error is thrown:</p>
<p>NotImplementedError: Supervised fastText models are not supported</p>
","14796335","","","","","2021-04-02 08:57:05","read a binary file including embeddings in Python","<python><vector><binaryfiles><gensim><embedding>","0","0","","","","CC BY-SA 4.0"
"66918124","1","","","2021-04-02 10:43:50","","0","19","<p>For example, how perplexed is our model assigning the topic that it assigned to a document. The reason I want to know this is because I want to weed out low frequency documents that were wrongly assigned to high frequency topics</p>
","4936133","","","","","2021-04-02 10:43:50","Is there a way to get document wise perplexity in gensim LDA","<gensim><lda><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"50237247","1","50237579","","2018-05-08 15:28:24","","1","3558","<p>I get this error when I load the google pre-trained word2vec to train doc2vec model with my own data. Here is part of my code:</p>

<pre><code>model_dm=doc2vec.Doc2Vec(dm=1,dbow_words=1,vector_size=400,window=8,workers=4)
model_dm.build_vocab(document)
model_dm.intersect_word2vec_format('home/xxw/Downloads/GoogleNews-vectors-negative300.bin',binary=True)
model_dm.train(document)
</code></pre>

<p>But I got this error:</p>

<blockquote>
  <p>'Doc2Vec' object has no attribute 'intersect_word2vec_format'</p>
</blockquote>

<p>Can you help me with the error? I get the google model from <a href=""https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz</a>, and my gensim is the latest version I think.</p>
","9759340","","7264964","","2018-05-08 17:53:42","2018-05-08 17:53:42","gensim: 'Doc2Vec' object has no attribute 'intersect_word2vec_format' when I load the Google pre-trained word2vec model","<word2vec><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"46914513","1","46917782","","2017-10-24 15:31:58","","1","341","<p>I'm using <code>gensim 3.0.1</code>. </p>

<p>I have a list of <code>TaggedDocument</code> with unique labels of the form <code>""label_17""</code>, but when I train Doc2Vec model, it somehow splits the labels to symbols, so the output for <code>model.docvecs.doctags</code> is the following:
</p>

<pre><code>{'0': Doctag(offset=5, word_count=378, doc_count=40),
 '1': Doctag(offset=6, word_count=1330, doc_count=141),
 '2': Doctag(offset=7, word_count=413, doc_count=50),
 '3': Doctag(offset=8, word_count=365, doc_count=41),
 '4': Doctag(offset=9, word_count=395, doc_count=41),
 '5': Doctag(offset=10, word_count=420, doc_count=41),
 '6': Doctag(offset=11, word_count=408, doc_count=41),
 '7': Doctag(offset=12, word_count=426, doc_count=41),
 '8': Doctag(offset=13, word_count=385, doc_count=41),
 '9': Doctag(offset=14, word_count=376, doc_count=40),
 '_': Doctag(offset=4, word_count=2009, doc_count=209),
 'a': Doctag(offset=1, word_count=2009, doc_count=209),
 'b': Doctag(offset=2, word_count=2009, doc_count=209),
 'e': Doctag(offset=3, word_count=2009, doc_count=209),
 'l': Doctag(offset=0, word_count=4018, doc_count=418)}
</code></pre>

<p>but in the initial list of tagged document each document has its own unique label.</p>

<p>The code for model training is the following:
</p>

<pre><code>model = Doc2Vec(size=300, sample=1e-4, workers=2)
print('Building Vocabulary')
model.build_vocab(data)
print('Training...')
model.train(data, total_words=total_words_count, epochs=20)
</code></pre>

<p>Therefore I can't index my documents like <code>model.docvecs['label_17']</code> and get <code>KeyError</code>.</p>

<p>The same thing if I pass data to the constructor instead of building the vocabulary.</p>

<p>Why is this happening? Thanks.</p>
","2402613","","","","","2017-10-24 18:39:14","Doc2Vec model splits documents tags in symbols","<python-3.x><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"53142322","1","","","2018-11-04 15:23:48","","0","234","<p>I tried to reproduce <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb"" rel=""nofollow noreferrer"">this tutorial</a> on my local machine to get used to gensim fasttext functionalities. Fasttext and gensim libraries are correctly installed. By calling the train method of gensim fasttext wrapper</p>

<pre><code>model_wrapper = FT_wrapper.train(ft_home, lee_train_file)
</code></pre>

<p>I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
PermissionError                           Traceback (most recent call last)
&lt;ipython-input-19-0815ab031d23&gt; in &lt;module&gt;()
      3 
      4 # train the model
----&gt; 5 model_wrapper = FT_wrapper.train(ft_home, lee_train_file)
      6 
      7 print(model_wrapper)

~/anaconda3/lib/python3.6/site-packages/gensim/models/deprecated/fasttext_wrapper.py in train(cls, ft_path, corpus_file, output_file, model, size, alpha, window, min_count, word_ngrams, loss, sample, negative, iter, min_n, max_n, sorted_vocab, threads)
    240             cmd.append(str(value))
    241 
--&gt; 242         utils.check_output(args=cmd)
    243         model = cls.load_fasttext_format(output_file)
    244         cls.delete_training_files(output_file)

~/anaconda3/lib/python3.6/site-packages/gensim/utils.py in check_output(stdout, *popenargs, **kwargs)
   1795     try:
   1796         logger.debug(""COMMAND: %s %s"", popenargs, kwargs)
-&gt; 1797         process = subprocess.Popen(stdout=stdout, *popenargs, **kwargs)
   1798         output, unused_err = process.communicate()
   1799         retcode = process.poll()

~/anaconda3/lib/python3.6/subprocess.py in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)
    707                                 c2pread, c2pwrite,
    708                                 errread, errwrite,
--&gt; 709                                 restore_signals, start_new_session)
    710         except:
    711             # Cleanup if the child failed starting.

~/anaconda3/lib/python3.6/subprocess.py in _execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)
   1342                         if errno_num == errno.ENOENT:
   1343                             err_msg += ': ' + repr(err_filename)
-&gt; 1344                     raise child_exception_type(errno_num, err_msg, err_filename)
   1345                 raise child_exception_type(err_msg)
   1346 

PermissionError: [Errno 13] Permission denied: '/Users/marcomattioli/fastText'
</code></pre>

<p>Note that I have <strong>-rwxr-xr-x</strong> rights on the fasttext executable. Any help appreciated how to fix this.</p>
","8321695","","","","","2019-11-05 08:32:16","Gensim fasttext wrapper returns permission error 13 while model training","<file-permissions><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"35721503","1","35990848","","2016-03-01 11:20:22","","11","3262","<p>I need to train a word2vec representation on tweets using gensim. Unlike most tutorials and code I've seen on gensim my data is not raw, but has already been preprocessed. I have a dictionary in a text document containing 65k words (incl. an ""unknown"" token and a EOL token) and the tweets are saved as a numpy matrix with indices into this dictionary. A simple example of the data format can be seen below:</p>

<p><strong>dict.txt</strong></p>

<pre><code>you
love
this
code
</code></pre>

<p><strong>tweets (5 is unknown and 6 is EOL)</strong></p>

<pre><code>[[0, 1, 2, 3, 6],
 [3, 5, 5, 1, 6],
 [0, 1, 3, 6, 6]]
</code></pre>

<p>I'm unsure how I should handle the indices representation. An easy way is just to convert the list of indices to a list of strings (i.e. [0, 1, 2, 3, 6] -> ['0', '1', '2', '3', '6']) as I read it into the word2vec model. However, this must be inefficient as gensim then will try to look up the internal index used for e.g. '2'.</p>

<p>How do I load this data and create the word2vec representation in an efficient manner using gensim? </p>
","1452257","","1452257","","2016-03-12 19:02:16","2016-11-25 15:52:52","Gensim word2vec on predefined dictionary and word-indices data","<python><nlp><gensim><word2vec>","2","0","3","","","CC BY-SA 3.0"
"50306710","1","50307360","","2018-05-12 13:22:37","","3","7016","<p>I am running gensim on Linux Suse. I can start my python program but on startup I get: </p>

<blockquote>
  <p>C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.</p>
</blockquote>

<p>GCC is installed. Does anyone know what I have to do?</p>
","7125403","","712995","","2018-05-12 17:56:14","2018-05-12 17:56:14","Gensim: ""C extension not loaded, training will be slow.""","<pip><word2vec><gensim><opensuse><suse>","1","0","","","","CC BY-SA 4.0"
"53183341","1","","","2018-11-07 03:51:37","","3","1597","<p>I am having trouble converting a fast FastText vector back to a word.
Here is my python code: </p>

<pre><code>from gensim.models import KeyedVectors
en_model = KeyedVectors.load_word2vec_format('wiki.en/wiki.en.vec')
vect = en_model.get_vector(""turtles"")
</code></pre>

<p>How can I take the vector (especially an arbitrary vector with the proper dimensions) and have it spit out a word?</p>
","10616459","","","","","2018-11-07 04:06:36","Converting Fasttext vector to word","<python><nlp><data-science><gensim><fasttext>","1","1","1","","","CC BY-SA 4.0"
"64915422","1","","","2020-11-19 16:04:40","","0","77","<p>I'm training a Word2Vec model using gensim and want to evaluate the model using a validation loss calculation. I'm trying to implement a validation loss callback, but there doesn't seem to be a straightforward way to implement a loss calculation using non-training data.</p>
<p>I've tried to search the GitHub repo for code and can't seem to find any. I've also tried to use the <code>score</code> method, but I get the below error:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; rna2vec = gensim.models.Word2Vec(sentences=sentence_train,    
                                     size=100,
                                     window=25,
                                     min_count=5,
                                     iter = 500,
                                     workers=6,
                                     compute_loss=True,
                                     seed = 255)

&gt;&gt;&gt; rna2vec.score(sentence_val)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-62-8e37dbdc5104&gt; in &lt;module&gt;
----&gt; 1 rna2vec.score(sentence_val)

~/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py in score(self, sentences, total_sentences, chunksize, queue_factor, report_delay)
    771         if not self.hs:
    772             raise RuntimeError(
--&gt; 773                 &quot;We have currently only implemented score for the hierarchical softmax scheme, &quot;
    774                 &quot;so you need to have run word2vec with hs=1 and negative=0 for this to work.&quot;
    775             )

RuntimeError: We have currently only implemented score for the hierarchical softmax scheme, so you need to have run word2vec with hs=1 and negative=0 for this to work.
</code></pre>
<p>But I would ideally like to keep <code>negative</code> when training, so it looks like the score function is out. Anyone have some thoughts on how to implement a val loss calculation?</p>
","6739275","","","","","2020-11-19 16:04:40","gensim validation loss callback","<python><gensim>","0","0","","","","CC BY-SA 4.0"
"27032517","1","27258823","","2014-11-20 05:40:50","","22","11409","<p><a href=""https://code.google.com/p/word2vec/"" rel=""noreferrer"">word2vec</a> is a open source tool by Google: </p>

<ul>
<li><p>For each word it provides a vector of float values, what exactly do they represent?</p></li>
<li><p>There is also a paper on <a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""noreferrer"">paragraph vector</a> can anyone explain how they are using word2vec in order to obtain fixed length vector for a paragraph.</p></li>
</ul>
","3992452","","1988825","","2016-11-03 16:44:38","2018-07-30 07:18:51","what does the vector of a word in word2vec represents?","<machine-learning><nlp><neural-network><gensim>","2","2","18","","","CC BY-SA 3.0"
"61977341","1","","","2020-05-23 19:11:08","","0","106","<p>I used following code lemmatize texts that were already excluding stop words and kept words longer than 3. However, after using following code, it split existing words such as 'wheres' to ['where', 's']; 'youre' to ['-PRON-','be']. I didn't expect 's', '-PRON-', 'be' these results in my text, what caused this behaviour and what I can do?</p>

<pre><code>def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
""""""https://spacy.io/api/annotation""""""

texts_out = []
for sent in texts:
    doc = nlp("" "".join(sent)) 
    texts_out.append([token.lemma_ for token in doc]) # though rare, if only keep the tokens with given posttags, add 'if token.pos_ in allowed_postags'
return texts_out

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
nlp = spacy.load('en', disable=['parser', 'ner'])

data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
</code></pre>
","8426105","","","","","2020-05-23 19:11:08","Unexpected lemmatize result from gensim","<nlp><nltk><gensim><lemmatization>","0","2","","","","CC BY-SA 4.0"
"46915589","1","","","2017-10-24 16:27:57","","0","631","<p>I am trying to summarise some text using Gensim in python and want exactly 3 sentences in my summary. There doesn't seem to be an option to do this so I have done the following workaround:</p>

<pre><code>with open ('speeches//'+speech, ""r"") as myfile:
    speech=myfile.read()
    sentences = speech.count('.')
    x = gensim.summarization.summarize(speech, ratio=3.0/sentences)
</code></pre>

<p>However this code is only giving me two sentences. Furthermore, as I incrementally increase 3 to 5 still nothing happens.</p>

<p>Any help would be most appreciated.</p>
","7958305","","5381204","","2018-01-09 14:25:05","2018-04-04 13:20:15","NLP: How to get an exact number of sentences for a text summary using Gensim","<nlp><text-processing><gensim>","1","0","","","","CC BY-SA 3.0"
"47624644","1","","","2017-12-03 23:53:47","","2","608","<p>I do not understand why words are not present in gensim model vocabulary after training</p>

<pre><code>model = gensim.models.Word2Vec(sentences, min_count=1, size=200, iter=1)
print ""AMBER"" in sentences
vec = model.wv[""AMBER""]
print vec
</code></pre>

<p>gives the following</p>

<p>True</p>

<pre><code>Traceback (most recent call last):
  File ""model.py"", line 38, in &lt;module&gt;
    vec = model.wv[""AMBER""]
  File ""/Users/nadiia/miniconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.py"", line 601, in __getitem__
    return self.word_vec(words)
  File ""/Users/nadiia/miniconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.py"", line 288, in word_vec
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word 'AMBER' not in vocabulary""
</code></pre>

<p>I do not understand why...
I am specifically running this model to learn embeddings of every word in list <em>sentences</em>, but then I cannot find any word in model's vocabulary. I cannot get any embeddings. </p>

<p>What's wrong with it?</p>

<p>Thanks</p>

<p>EDIT:
This helped to solve the problem</p>

<pre><code>text = [nltk.word_tokenize(sent.decode(""utf-8"")) for sent in sentences]
</code></pre>
","3849781","","3849781","","2017-12-04 04:11:43","2017-12-04 04:11:43","gensim model KeyError","<python><gensim>","0","5","1","","","CC BY-SA 3.0"
"38739250","1","38739531","","2016-08-03 09:11:31","","1","42848","<p>Not able to install gensim on windows.Please help me I need to gensim Immediately and tell me installation steps with More details and other software that needs to be installed before it. thanks</p>
","","user6671441","","","","2020-08-12 19:07:10","How to install gensim on windows","<python><gensim>","6","1","3","","","CC BY-SA 3.0"
"54348402","1","","","2019-01-24 14:01:22","","0","402","<p>I am creating document vectors with a trained FastText model on my computer. Gensim's FastText, as far as I know, doesn't have an option to create document vectors (better known as Paragraph Vectors [PV]). Therefore I have calculated them manually by taking the average of the sum of the words available in a document. This task alone doesn't take as much time.</p>

<p>If I want to append several other numerical features to the calculated PV, 5 millionen docs take about 30 minutes to create. I thought this process could be improved by splitting the work onto several cores on my computer with the multiprocessing library of Python, which works right now but only to a certain extent.</p>

<p>There were a few problems I had to solve before getting to this stage. Since I am using the Jupyter Notebook to execute the code I had to place some methods in a seperate Python script to be able to use mulitprocessing in a Jupyter Notebook. This is the code that's available in the Jupyter Notebook. It imports the module ""m_helpers"" which has the methods to
create the document vectors:</p>

<pre><code>import multiprocessing
import m_helpers

# Define number of workers.
num_processes = 3

if __name__ == ""__main__"":
    # This pool spawns several processes to built the
    # document vectors with the FastText model
    with multiprocessing.Pool(processes = num_processes, 
                              initializer = m_helpers.init_vars, 
                              initargs = (fasttext_model, vars_df)) as pool:
        results = pool.map(m_helpers.create_docvecs,
                           data_df.itertuples(name = False), 
                           chunksize = 512)
        output = [x for x in results]

    # Print length of output to see whether everything was processed
    print(""Length of output (document vectors): {0}"".format(len(output)))
</code></pre>

<p>m_helpers.py:</p>

<pre><code>fasttext_model = None
vars_df = None

def init_vars(model, df):
    global fasttext_model
    fasttext_model = model
    global vars_df
    vars_df = df

def create_docvecs(data):
    word_vectors = [fasttext_model.wv[word] for word in data[-1].str.split()]
    document_vector = sum(word_vectors) / len(word_vectors)
    feature_vector = vars_df.loc[data[0], :].values
    # Further code to combine both vectors
    return document_vector
</code></pre>

<p>I have a computer with 6 cores / 12 threads. However I can make this code work only for 3 cores. Using more cores always results in an error caused by using up all the memory (RAM). I think this is caused by all those copies of objects for each process. </p>

<p>Now there seems to be ways to create a shared memory for all processes to access. There is a dataframe I am iterating over to access the text data. The method that is called for all processes uses an other dataframe and the fasttext model. All of those objects are read only to create the PV and append values from the other dataframe. I could merge the text dataframe and feature dataframe before. However, I would still need to share at least the fasttext_model object. So the question would be, how to do it? Is it possible at all? I have read several questions regarding this problem on stackoverflow but I couldn't make much out of it. Maybe I need to use something different than Pool?</p>
","9685648","","","","","2019-01-24 14:01:22","Shared memory for several read-only objects with multiprocessing Pool","<python><multiprocessing><jupyter-notebook><shared-memory><gensim>","0","3","","","","CC BY-SA 4.0"
"22283396","1","","","2014-03-09 14:25:43","","0","2758","<p>I am trying to get related documents for a list of 10,000 documents from the same set of 10,000 docs. I am using two algorithms for testing: gensim lsi and gensim similarity. Both give terrible results. How can I improve it?</p>

<pre><code>from gensim import corpora, models, similarities
from nltk.corpus import stopwords
import re

def cleanword(word):
    return re.sub(r'\W+', '', word).strip()

def create_corpus(documents):

    # remove common words and tokenize
    stoplist = stopwords.words('english')
    stoplist.append('')
    texts = [[cleanword(word) for word in document.lower().split() if cleanword(word) not in stoplist]
             for document in documents]

    # remove words that appear only once
    all_tokens = sum(texts, [])
    tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)

    texts = [[word for word in text if word not in tokens_once] for text in texts]

    dictionary = corpora.Dictionary(texts)
    corp = [dictionary.doc2bow(text) for text in texts]

def create_lsi(documents):

    corp = create_corpus(documents)
    # extract 400 LSI topics; use the default one-pass algorithm
    lsi = models.lsimodel.LsiModel(corpus=corp, id2word=dictionary, num_topics=400)
    # print the most contributing words (both positively and negatively) for each of the first ten topics
    lsi.print_topics(10)

def create_sim_index(documents):
    corp = create_corpus(documents)
    index = similarities.Similarity('/tmp/tst', corp, num_features=12)
    return index
</code></pre>
","692977","","","","","2015-10-16 07:03:51","Document Similarity Gensim","<python><nlp><gensim>","3","3","1","","","CC BY-SA 3.0"
"54431187","1","","","2019-01-29 23:28:46","","0","1179","<p>I'm running the following python script on a large dataset (around 100 000 items). Currently the execution is unacceptably slow, it would probably take a month to finish at least (no exaggeration). Obviously I would like it to run faster.</p>

<p>I've added a comment belong to highlight where I think the bottleneck is. I have written my own database functions which are imported.  </p>

<p>Any help is appreciated!</p>

<pre><code># -*- coding: utf-8 -*-
import database
from gensim import corpora, models, similarities, matutils
from gensim.models.ldamulticore import LdaMulticore
import pandas as pd
from sklearn import preprocessing



def getTopFiveSimilarAuthors(author, authors, ldamodel, dictionary):
    vec_bow = dictionary.doc2bow([researcher['full_proposal_text']])
    vec_lda = ldamodel[vec_bow]

    # normalization
    try:
        vec_lda = preprocessing.normalize(vec_lda)
    except:
        pass

    similar_authors = []

    for index, other_author in authors.iterrows():
        if(other_author['id'] != author['id']):
            other_vec_bow = dictionary.doc2bow([other_author['full_proposal_text']])

            other_vec_lda = ldamodel[other_vec_bow]
            # normalization
            try:
                other_vec_lda = preprocessing.normalize(vec_lda)
            except:
                pass

            sim = matutils.cossim(vec_lda, other_vec_lda)
            similar_authors.append({'id': other_author['id'], 'cosim': sim})
    similar_authors = sorted(similar_authors, key=lambda k: k['cosim'], reverse=True)
    return similar_authors[:5]


def get_top_five_similar(author, authors, ldamodel, dictionary):
    top_five_similar_authors = getTopFiveSimilarAuthors(author, authors, ldamodel, dictionary)
    database.insert_top_five_similar_authors(author['id'], top_five_similar_authors, cursor)

connection = database.connect()
authors = []
authors = pd.read_sql(""SELECT id, full_text FROM author WHERE full_text IS NOT NULL;"", connection)

# create the dictionary
dictionary = corpora.Dictionary([authors[""full_text""].tolist()])

# create the corpus/ldamodel
author_text = []

for text in author_text['full_text'].tolist():
    word_list = []
    for word in text:
        word_list.append(word)
        author_text.append(word_list)

corpus = [dictionary.doc2bow(text) for text in author_text]
ldamodel = LdaMulticore(corpus, num_topics=50, id2word = dictionary, workers=30)

#BOTTLENECK: the script hangs after this point. 
authors.apply(lambda x: get_top_five_similar(x, authors, ldamodel, dictionary), axis=1)
</code></pre>
","4437631","","","","","2019-01-30 00:51:24","Gensim LDA Multicore Python script runs much too slow","<python><mysql><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"50264369","1","50265639","","2018-05-10 01:53:40","","3","386","<p>We are having n number of documents. Upon submission of new document by user, our goal is to inform him about possible duplication of existing document (just like stackoverflow suggests questions may already have answer).</p>

<p>In our system, new document is uploaded every minute and mostly about the same topic (where there are more chance of duplication).</p>

<p>Our current implementation includes gensim doc2vec model trained on documents (tagged with unique document ids). We infer vector for new document and find most_similar docs (ids) with it. Reason behind choosing doc2vec model is that we wanted to take advantage of semantics to improve results. As far as we know, it does not support online training, so we might have to schedule a cron or something that periodically updates the model. But scheduling cron will be disadvantageous as documents come in a burst. User may upload duplicates while model is not yet trained for new data. Also given huge amount of data, training time will be higher.</p>

<p>So i would like to know how such cases are handled in big companies. Are there any better alternative? or better algorithm for such problem?</p>
","2578525","","","","","2018-05-10 04:45:59","Document similarity in production environment","<python><machine-learning><nlp><gensim><doc2vec>","1","0","2","","","CC BY-SA 4.0"
"47775557","1","47799544","","2017-12-12 14:56:40","","2","2704","<p>I have an existing gensim Doc2Vec model, and I'm trying to do iterative updates to the training set, and by extension, the model.</p>

<p>I take the new documents, and perform preproecssing as normal:</p>

<pre><code>stoplist = nltk.corpus.stopwords.words('english')
train_corpus= []
for i, document in enumerate(corpus_update['body'].values.tolist()):
     train_corpus.append(gensim.models.doc2vec.TaggedDocument([word for word in gensim.utils.simple_preprocess(document) if word not in stoplist], [i]))
</code></pre>

<p>I then load the original model, update the vocabulary, and retrain:</p>

<pre><code>#### Original model
## model = gensim.models.doc2vec.Doc2Vec(dm=0, size=300, hs=1, min_count=10, dbow_words= 1, negative=5, workers=cores)

model = Doc2Vec.load('pvdbow_model_6_06_12_17.doc2vec')

model.build_vocab(train_corpus, update=True)

model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)
</code></pre>

<p>I then update the training set Pandas dataframe by appending the new data, and reset the index.</p>

<pre><code>corpus = corpus.append(corpus_update)
corpus = corpus.reset_index(drop=True)
</code></pre>

<p>However, when I try to use infer_vector() with the <em>updated</em> model:</p>

<pre><code>inferred_vector = model1.infer_vector(tokens)
sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
</code></pre>

<p>the result quality is poor, suggesting that the indices from the model and the training set dataframe no longer match. </p>

<p>When I compare it against the <em>non-updated</em> training set dataframe (again using the updated model) the results are fine - though, obviously I'm missing the new documents.</p>

<p>Is there anyway to have both updated, as I want to be able to make frequent updates to the model without a full retrain of the model?</p>
","3564977","","","","","2017-12-13 18:04:31","Updating training documents for gensim Doc2Vec model","<gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"53368915","1","53371517","","2018-11-19 05:41:32","","1","61","<p>In each tiny step of doc2vec training process, it takes a word and its neighbors within certain length(called window size). The neighbors are summed up, averaged, or concated, and so on and so on.</p>

<p>My question is, what if the window exceed the boundary of a certain doc, like 
<a href=""https://i.stack.imgur.com/Iy5e0.png"" rel=""nofollow noreferrer"">this</a></p>

<p>Then how are the neighbors summed up, averaged, or concated? Or they are just simply discarded? </p>

<p>I am doing some nlp work and most doc in my dataset are quite short. Appeciate for any idea.</p>
","8712683","","","","","2018-11-19 17:18:14","Genisim doc2vec: how is short doc processed?","<machine-learning><nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"44698910","1","44709375","","2017-06-22 12:02:19","","1","7213","<p>I'm building a NLP chat application using Doc2Vec technique in Python using its <code>gensim</code> package. I have already done tokenizing and stemming. I want to remove the stop words (to test if it works better) from both the training set as well as the question which user throws. </p>

<p>Here is my code.</p>

<pre><code>import gensim
import nltk
from gensim import models
from gensim import utils
from gensim import corpora
from nltk.stem import PorterStemmer
ps = PorterStemmer()

sentence0 = models.doc2vec.LabeledSentence(words=[u'sampl',u'what',u'is'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'sampl',u'tell',u'me',u'about'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'elig',u'what',u'is',u'my'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'limit', u'what',u'is',u'my'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'claim',u'how',u'much',u'can',u'I'],tags=[""SENT_4""])
sentence5 = models.doc2vec.LabeledSentence(words=[u'retir',u'i',u'am',u'how',u'much',u'can',u'elig',u'claim'],tags=[""SENT_5""])
sentence6 = models.doc2vec.LabeledSentence(words=[u'resign',u'i',u'have',u'how',u'much',u'can',u'i',u'claim',u'elig'],tags=[""SENT_6""])
sentence7 = models.doc2vec.LabeledSentence(words=[u'promot',u'what',u'is',u'my',u'elig',u'post',u'my'],tags=[""SENT_7""])
sentence8 = models.doc2vec.LabeledSentence(words=[u'claim',u'can,',u'i',u'for'],tags=[""SENT_8""])
sentence9 = models.doc2vec.LabeledSentence(words=[u'product',u'coverag',u'cover',u'what',u'all',u'are'],tags=[""SENT_9""])
sentence10 = models.doc2vec.LabeledSentence(words=[u'hotel',u'coverag',u'cover',u'what',u'all',u'are'],tags=[""SENT_10""])
sentence11 = models.doc2vec.LabeledSentence(words=[u'onlin',u'product',u'can',u'i',u'for',u'bought',u'through',u'claim',u'sampl'],tags=[""SENT_11""])
sentence12 = models.doc2vec.LabeledSentence(words=[u'reimburs',u'guidelin',u'where',u'do',u'i',u'apply',u'form',u'sampl'],tags=[""SENT_12""])
sentence13 = models.doc2vec.LabeledSentence(words=[u'reimburs',u'procedur',u'rule',u'and',u'regul',u'what',u'is',u'the',u'for'],tags=[""SENT_13""])
sentence14 = models.doc2vec.LabeledSentence(words=[u'can',u'i',u'submit',u'expenditur',u'on',u'behalf',u'of',u'my',u'friend',u'and',u'famili',u'claim',u'and',u'reimburs'],tags=[""SENT_14""])
sentence15 = models.doc2vec.LabeledSentence(words=[u'invoic',u'bills',u'procedur',u'can',u'i',u'submit',u'from',u'shopper stop',u'claim'],tags=[""SENT_15""])
sentence16 = models.doc2vec.LabeledSentence(words=[u'invoic',u'bills',u'can',u'i',u'submit',u'from',u'pantaloon',u'claim'],tags=[""SENT_16""])
sentence17 = models.doc2vec.LabeledSentence(words=[u'invoic',u'procedur',u'can',u'i',u'submit',u'invoic',u'from',u'spencer',u'claim'],tags=[""SENT_17""])

# User asks a question.

document = input(""Ask a question:"")
tokenized_document = list(gensim.utils.tokenize(document, lowercase = True, deacc = True))
#print(type(tokenized_document))
stemmed_document = []
for w in tokenized_document:
    stemmed_document.append(ps.stem(w))
sentence19 = models.doc2vec.LabeledSentence(words= stemmed_document, tags=[""SENT_19""])

# Building vocab.
sentences = [sentence0,sentence1,sentence2,sentence3, sentence4, sentence5,sentence6, sentence7, sentence8, sentence9, sentence10, sentence11, sentence12, sentence13, sentence14, sentence15, sentence16, sentence17, sentence19]

#I tried to remove the stop words but it didn't work out as LabeledSentence object has no attribute lower.
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
          for document in sentences]
..
</code></pre>

<p>Is there a way I can remove stop words from <code>sentences</code> directly and get a new set of vocab without stop words ?</p>
","4542815","","","","","2017-06-22 21:05:31","How to remove stop words from documents in gensim?","<python><nlp><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 3.0"
"54432791","1","","","2019-01-30 03:02:01","","0","806","<p>I am running LDAMulticore from the python gensim library, and the script cannot seem to create more than one thread. Here is the error:</p>

<pre><code>  Traceback (most recent call last):
  File ""/usr/lib64/python2.7/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/usr/lib64/python2.7/multiprocessing/process.py"", line 114, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/lib64/python2.7/multiprocessing/pool.py"", line 97, in worker
    initializer(*initargs)
  File ""/usr/lib64/python2.7/site-packages/gensim/models/ldamulticore.py"", line 333, in worker_e_step
    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?
  File ""/usr/lib64/python2.7/site-packages/gensim/models/ldamodel.py"", line 725, in do_estep
    gamma, sstats = self.inference(chunk, collect_sstats=True)
  File ""/usr/lib64/python2.7/site-packages/gensim/models/ldamodel.py"", line 655, in inference
    ids = [int(idx) for idx, _ in doc]
TypeError: 'int' object is not iterable
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/threading.py"", line 812, in __bootstrap_inner
    self.run()
  File ""/usr/lib64/python2.7/threading.py"", line 765, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/lib64/python2.7/multiprocessing/pool.py"", line 325, in _handle_workers
    pool._maintain_pool()
  File ""/usr/lib64/python2.7/multiprocessing/pool.py"", line 229, in _maintain_pool
    self._repopulate_pool()
  File ""/usr/lib64/python2.7/multiprocessing/pool.py"", line 222, in _repopulate_pool
    w.start()
  File ""/usr/lib64/python2.7/multiprocessing/process.py"", line 130, in start
    self._popen = Popen(self)
  File ""/usr/lib64/python2.7/multiprocessing/forking.py"", line 121, in __init__
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory
</code></pre>

<p>I'm creating my LDA model like this:</p>

<pre><code>ldamodel = LdaMulticore(corpus, num_topics=50, id2word = dictionary, workers=3)
</code></pre>

<p>I have actually asked another question about this script, so the full script can be found here: </p>

<p><a href=""https://stackoverflow.com/questions/54431187/gensim-lda-multicore-python-script-runs-much-too-slow"">Gensim LDA Multicore Python script runs much too slow</a></p>

<p>If it's relevant, I'm running this on a CentOS server. Let me know if I should include any other information. </p>

<p>Any help is appreciated!</p>
","4437631","","","","","2019-01-30 07:19:06","gensim.LDAMulticore throwing exception:","<python><gensim><multicore><lda>","1","0","","","","CC BY-SA 4.0"
"53227410","1","","","2018-11-09 14:16:51","","1","188","<p>import gensim</p>

<p>Load Google's pre-trained Word2Vec model.</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)

embed = model.get_keras_embedding()

print(embed.weights)
</code></pre>

<p>output: []</p>

<p>How to print the weights of embedding layer in Keras, currently it is printing empty list?</p>
","7227411","","2413201","","2018-11-09 14:29:10","2018-11-09 14:29:10","How to print the weights of Keras embedding?","<keras><gensim>","0","0","","","","CC BY-SA 4.0"
"53320951","1","","","2018-11-15 13:50:25","","1","165","<p>I am trying to perform LDATransformer using gensim api and then I want to get the topic words only using following code:</p>

<pre><code>from gensim.sklearn_api.ldamodel import LdaTransformer
   print(""Loading docs for lda input..."")
   docs = get_lda_input_from_corpus_folder(CORPUS_PATH)
    print(""Topic modeling using LdaTransformer.."")
    dictionary = Dictionary(docs)
    corpus = [dictionary.doc2bow(text) for text in docs]
    model =  LdaTransformer(id2word=dictionary, num_topics=n_topics,iterations=lda_n_iter,random_state=n_random)
    model.fit(corpus)
    print(""\nTopical words:"")
    print(""-"" * 20)
    topic_word = model.topic_word_
    n_top_words = 8
 for i, topic_dist in enumerate(topic_word):
     topic_words = np.argsort(topic_dist)[:-(n_top_words+1):-1]
     print('Topic {}: {}'.format(i, ' '.join(topic_words)))
</code></pre>

<p>but when I print these topic words i have received an error as:</p>

<pre><code>AttributeError: 'LdaTransformer' object has no attribute 'topic_word_'
</code></pre>

<p>any other method which I can use to extract words from this model?</p>
","2305101","","","","","2018-11-15 13:50:25","how to display topic words using sklearn api in gensim","<python-3.x><scikit-learn><gensim><lda>","0","0","","","","CC BY-SA 4.0"
"22272370","1","22315107","","2014-03-08 17:07:51","","21","23639","<p>I am trying to train a word2vec model on very short phrases (5 grams). Since each sentence or example is very short, I believe the window size I can use can atmost be 2. I am trying to understand what the implications of such a small window size are on the quality of the learned model, so that I can understand whether my model has learnt something meaningful or not. I tried training a word2vec model on 5-grams but it appears the learnt model does not capture semantics etc very well.</p>

<p>I am using the following test to evaluate the accuracy of model:
<a href=""https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt"" rel=""noreferrer"">https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt</a></p>

<p>I used gensim.Word2Vec to train a model and here is a snippet of my accuracy scores (using a window size of 2)</p>

<pre><code>[{'correct': 2, 'incorrect': 304, 'section': 'capital-common-countries'},
 {'correct': 2, 'incorrect': 453, 'section': 'capital-world'},
 {'correct': 0, 'incorrect': 86, 'section': 'currency'},
 {'correct': 2, 'incorrect': 703, 'section': 'city-in-state'},
 {'correct': 123, 'incorrect': 183, 'section': 'family'},
 {'correct': 21, 'incorrect': 791, 'section': 'gram1-adjective-to-adverb'},
 {'correct': 8, 'incorrect': 544, 'section': 'gram2-opposite'},
 {'correct': 284, 'incorrect': 976, 'section': 'gram3-comparative'},
 {'correct': 67, 'incorrect': 863, 'section': 'gram4-superlative'},
 {'correct': 41, 'incorrect': 951, 'section': 'gram5-present-participle'},
 {'correct': 6, 'incorrect': 1089, 'section': 'gram6-nationality-adjective'},
 {'correct': 171, 'incorrect': 1389, 'section': 'gram7-past-tense'},
 {'correct': 56, 'incorrect': 936, 'section': 'gram8-plural'},
 {'correct': 52, 'incorrect': 705, 'section': 'gram9-plural-verbs'},
 {'correct': 835, 'incorrect': 9973, 'section': 'total'}]
</code></pre>

<p>I also tried running the demo-word-accuracy.sh script outlined here with a window size of 2 and get poor accuracy as well:</p>

<pre><code>Sample output:
    capital-common-countries:
    ACCURACY TOP1: 19.37 %  (98 / 506)
    Total accuracy: 19.37 %   Semantic accuracy: 19.37 %   Syntactic accuracy: -nan % 
    capital-world:
    ACCURACY TOP1: 10.26 %  (149 / 1452)
    Total accuracy: 12.61 %   Semantic accuracy: 12.61 %   Syntactic accuracy: -nan % 
    currency:
    ACCURACY TOP1: 6.34 %  (17 / 268)
    Total accuracy: 11.86 %   Semantic accuracy: 11.86 %   Syntactic accuracy: -nan % 
    city-in-state:
    ACCURACY TOP1: 11.78 %  (185 / 1571)
    Total accuracy: 11.83 %   Semantic accuracy: 11.83 %   Syntactic accuracy: -nan % 
    family:
    ACCURACY TOP1: 57.19 %  (175 / 306)
    Total accuracy: 15.21 %   Semantic accuracy: 15.21 %   Syntactic accuracy: -nan % 
    gram1-adjective-to-adverb:
    ACCURACY TOP1: 6.48 %  (49 / 756)
    Total accuracy: 13.85 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 6.48 % 
    gram2-opposite:
    ACCURACY TOP1: 17.97 %  (55 / 306)
    Total accuracy: 14.09 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 9.79 % 
    gram3-comparative:
    ACCURACY TOP1: 34.68 %  (437 / 1260)
    Total accuracy: 18.13 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 23.30 % 
    gram4-superlative:
    ACCURACY TOP1: 14.82 %  (75 / 506)
    Total accuracy: 17.89 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 21.78 % 
    gram5-present-participle:
    ACCURACY TOP1: 19.96 %  (198 / 992)
    Total accuracy: 18.15 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 21.31 % 
    gram6-nationality-adjective:
    ACCURACY TOP1: 35.81 %  (491 / 1371)
    Total accuracy: 20.76 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.14 % 
    gram7-past-tense:
    ACCURACY TOP1: 19.67 %  (262 / 1332)
    Total accuracy: 20.62 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 24.02 % 
    gram8-plural:
    ACCURACY TOP1: 35.38 %  (351 / 992)
    Total accuracy: 21.88 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.52 % 
    gram9-plural-verbs:
    ACCURACY TOP1: 20.00 %  (130 / 650)
    Total accuracy: 21.78 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.08 % 
    Questions seen / total: 12268 19544   62.77 % 
</code></pre>

<p>However the word2vec site claims its possible to obtain an accuracy of ~60% on these tasks.
Hence I would like to gain some insights into the effect of these hyperparameters like window size and how they affect quality of learnt models.</p>
","1846250","","","","","2021-09-21 09:48:21","Word2Vec: Effect of window size used","<gensim><word2vec>","2","0","10","","","CC BY-SA 3.0"
"65070534","1","65076789","","2020-11-30 08:57:09","","0","72","<p>I have two dataframe. both have two columns. I want to use wmd to find closest match for each entity in column <code>source_label</code> to entities in column <code>target_label</code> However, at the end I would like to have a DataFrame with all the 4 columns with respect to the entities.</p>
<h3>df1</h3>
<pre><code>,source_Label,source_uri
'neuronal ceroid lipofuscinosis 8',&quot;http://purl.obolibrary.org/obo/DOID_0110723&quot;
'autosomal dominant distal hereditary motor neuronopathy',&quot;http://purl.obolibrary.org/obo/DOID_0111198&quot;
</code></pre>
<h3>df2</h3>
<pre><code>,target_label,target_uri
'neuronal ceroid ',&quot;http://purl.obolibrary.org/obo/DOID_0110748&quot;
'autosomal dominanthereditary',&quot;http://purl.obolibrary.org/obo/DOID_0111110&quot;
</code></pre>
<h3>Expected result</h3>
<pre><code>,source_label, target_label, source_uri, target_uri, wmd score
'neuronal ceroid lipofuscinosis 8', 'neuronal ceroid ', &quot;http://purl.obolibrary.org/obo/DOID_0110723&quot;, &quot;http://purl.obolibrary.org/obo/DOID_0110748&quot;, 0.98
'autosomal dominant distal hereditary motor neuronopathy', 'autosomal dominanthereditary', &quot;http://purl.obolibrary.org/obo/DOID_0111198&quot;, &quot;http://purl.obolibrary.org/obo/DOID_0111110&quot;, 0.65
</code></pre>
<p>The dataframe is so big that I am looking for some faster way to iterate over both label columns. So far I tried this:</p>
<pre><code>list_distances = []
temp = []

def preprocess(sentence):
    return [w for w in sentence.lower().split()]

entity = df1['source_label']
target = df2['target_label']

 for i in tqdm(entity):
    for j in target:
        wmd_distance = model.wmdistance(preprocess(i), preprocess(j))
        temp.append(wmd_distance)
    list_distances.append(min(temp))
# print(&quot;list_distances&quot;, list_distances)
WMD_Dataframe = pd.DataFrame({'source_label': pd.Series(entity),
                              'target_label': pd.Series(target),
                              'source_uri': df1['source_uri'],
                              'target_uri': df2['target_uri'],
                              'wmd_Score': pd.Series(list_distances)}).sort_values(by=['wmd_Score'])
WMD_Dataframe = WMD_Dataframe.reset_index()

</code></pre>
<p>First of all this code is not working well as the other two columns are coming directly from the dfs' and do not take entities relation with the uri into consideration.
How one can make it faster as the entities are in millions. Thanks in advance.</p>
","7890734","","7890734","","2020-11-30 16:30:44","2020-11-30 16:30:44","loop over pandas column for wmd similarity","<python><pandas><numpy><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"45685055","1","","","2017-08-15 00:54:35","","0","300","<p>This is my code:</p>

<pre><code>def build_trigram_model(corpus):
    corpus = lemmer(nlp(corpus))
    bigram = phrases.Phrases(corpus, min_count=2, threshold=40)
    bigram_phraser = phrases.Phraser(bigram)
    trigram = phrases.Phrases(bigram_phraser[corpus], min_count=2, threshold=50)
    trigram_phraser = phrases.Phraser(trigram)
    return bigram_phraser, trigram_phraser

def punct_space(token):
    """"""
    helper function to eliminate punctuation, spaces and numbers.
    """"""
    return token.is_punct or token.is_space or token.like_num

def lemmer(tokens):
    word_space = []
    stemmer = SnowballStemmer(""english"")
    for token in tokens:
        if not punct_space(token):
            word_space.append(stemmer.stem(str(token).lower()))
    return word_space


bigram_phraser, trigram_phraser = build_trigram_model(corpus)
</code></pre>

<p>This returns something like:</p>

<pre><code>bigram_phraser.phrasegrams

{(b'\xef\xa3\xaf', b'\xef\xa3\xb0'): (189, 49.569954185342624),
 (b'\xef\xa3\xba', b'\xef\xa3\xbb'): (189, 49.162979913552),
 (b'\xce\xbf', b'\xcf\x82'): (11, 52.7203947368421),
 (b'\xef\xa3\xb1', b'\xef\xa3\xb2'): (13, 78.54622410336378),
 (b'\xef\xa3\xb2', b'\xef\xa3\xb3'): (12, 74.75279850746269),
 (b'\xef\xa3\xbc', b'\xef\xa3\xbd'): (8, 73.94232987312573),
 (b'\xef\xa3\xbd', b'\xef\xa3\xbe'): (7, 65.46977124183007),
 (b'\xc9\xa1', b'\xcc\x8a'): (29, 64.73618071658315),
 (b'\xef\x9d\xb4', b'\xef\x9d\xb2'): (51, 105.61094674556212),
 (b'\xef\x9d\xa3', b'\xef\x9d\xac'): (26, 53.88736340711684)}
</code></pre>

<p>When running a new text through the model, no collocations are found.</p>

<p>However, when I use the following stemmer:</p>

<pre><code>def lemmer(tokens):
    """"""lemmatize words""""""
    word_space = []
    for sent in tokens.sents:
        sentence = []
        for token in sent:
            if not punct_space(token):
                if token.lemma_=='-PRON-':
                    sentence.append(token.lower_)
                else:
                    sentence.append(token.lemma_)
        word_space.append(sentence)
    return word_space
</code></pre>

<p>Everything works just as it is supposed to. The dtype returned by both stemmers is list of strings, so that cannot be the problem. Any ideas why this happens? Thanks!</p>

<ul>
<li><strong>Platform:</strong> Linux-4.4.0-1030-aws-x86_64-with-debian-stretch-sid</li>
<li><strong>Python version:</strong> 3.6.1</li>
</ul>
","8464691","","107625","","2017-08-15 06:29:19","2017-08-15 06:29:19","Gensim.models.phrases.Phrases does not work when input comes from NLTK stemmed tokens","<python><nltk><gensim><spacy>","0","3","","","","CC BY-SA 3.0"
"53417171","1","53418932","","2018-11-21 17:02:37","","0","762","<p>I use gensim LDA topic modelling to find topics for each document and to check the similarity between documents by comparing the received topics vectors.
Each document is given a different number of matching topics, so the comparison of the vector (by cosine similarity) is incorrect because vectors of the same length are required.</p>

<p>This is the related code:</p>

<pre><code>lda_model_bow = models.LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=3, passes=1, random_state=47)

#---------------Calculating and Viewing the topics----------------------------
vec_bows = [dictionary.doc2bow(filtered_text.split()) for filtered_text in filtered_texts]

vec_lda_topics=[lda_model_bow[vec_bow] for vec_bow in vec_bows]

for id,vec_lda_topic in enumerate(vec_lda_topics):
    print ('document ' ,id, 'topics: ', vec_lda_topic)
</code></pre>

<p>The output vectors is:</p>

<pre><code>document  0 topics:  [(1, 0.25697246), (2, 0.08026043), (3, 0.65391296)]
document  1 topics:  [(2, 0.93666667)]
document  2 topics:  [(2, 0.07910537), (3, 0.20132676)]
.....
</code></pre>

<p>As you can see, each vector has a different length, so it is not possible to perform cosine similarity between them.</p>

<p>I would like the output to be:</p>

<pre><code>document  0 topics:  [(1, 0.25697246), (2, 0.08026043), (3, 0.65391296)]
document  1 topics:  [(1, 0.0), (2, 0.93666667), (3, 0.0)]
document  2 topics:  [(1, 0.0), (2, 0.07910537), (3, 0.20132676)]
.....
</code></pre>

<p>Any ideas how to do it? tnx</p>
","5631372","","","","","2018-11-21 19:02:42","fixed-size topics vector in gensim LDA topic modelling for finding similar texts","<python><gensim><lda><topic-modeling><cosine-similarity>","2","0","","","","CC BY-SA 4.0"
"61789168","1","","","2020-05-14 04:09:52","","0","407","<p>I am trying to parse the <a href=""https://www.kaggle.com/watts2/glove6b50dtxt"" rel=""nofollow noreferrer"">Glove6b50d data from Kaggle</a> in via Google Colab, then run it through the word2vec process (apologies for the huge URL - it's the fastest link I've found). However, I'm hitting a bug where '-' tokens are not parsed correctly, resulting in the above error. </p>

<p>I have attempted to handle this in a few ways. I've also looked into the load_word2vec_format method itself and tried to ignore errors, however it doesn't seem to make a difference. I've tried a map method on line two, following combinations of advice from these links: <a href=""https://stackoverflow.com/questions/45269652/python-convert-string-to-float-error-with-negative-numbers"">[a]</a> and <a href=""https://stackoverflow.com/questions/21385673/shortest-way-to-replace-parts-of-strings-in-numpy-array"">[b]</a>. This hasn't fixed or changed the error message received (i.e. removing it changes nothing in the text). </p>

<pre><code>gloveFile = pd.read_fwf(""https://storage.googleapis.com/kagglesdsdata/datasets/652874/1154868/glove.6B.50d.txt?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&amp;Expires=1589683535&amp;Signature=kaS%2FTkSmvp7lhqwLJ%2B1lyuvP76PcDpwK1dnsCZEO0AiVXqQm7jsBc1r5g9af%2BuVkOSvMgqUDXYL4O%2BN43pnL5RLs7ns%2B3w%2BEtCYDTfJz6q1O0zfPz4%2BTcD3GV7UAGgVjVNIvncC9fHWcd2YuKwiZaTvKL%2BGRnMkf9b%2BYnOweYeXEeA1sX005krj%2FLMBbVTXmDTwOtN4HwVNb3%2BrbezkWkoEC6sxLPnGcsEKaBe%2Biv%2FuVSQG5FsQlwvRgsSU%2FMgk0c4bi%2FHxF04lrQW0E0s767TIXwHeodRHYpk5KQeKmyd91uKD2Zb8v8xQcf2%2BkmSNGQHbX0mDz8HBwYEmOdV7aMQ%3D%3D&amp;response-content-disposition=attachment%3B+filename%3Dglove.6B.50d.txt"",
                    delimiter=""\n\t\s+"", header=None)

map(lambda gloveFile: gloveFile.replace(r'[^\x00-\x7F]+' , '-'), gloveFile[0])

numpy.savetxt(r'/usr/local/lib/python3.6/dist-packages/gensim/test/test_data/glove6b50d.txt', gloveFile.values, fmt=""%s"")

from gensim.models import KeyedVectors
from gensim.test.utils import datapath, get_tmpfile
from gensim.scripts.glove2word2vec import glove2word2vec

glove_file = datapath('glove6b50d.txt')

glove2word2vec(glove_file, ""glove6b50d_word2vec.txt"")

model = KeyedVectors.load_word2vec_format(""glove6b50d_word2vec.txt"", binary=False)
</code></pre>

<p>Per the comment below, the exact error I'm getting is as follows:</p>

<pre><code>/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function
  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-132-6ad5a51f4fb3&gt; in &lt;module&gt;()
      9 glove2word2vec(glove_file, ""glove6b50d_word2vec.txt"")
     10 
---&gt; 11 model = KeyedVectors.load_word2vec_format(""glove6b50d_word2vec.txt"", binary=False)
     12 

2 frames
/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py in &lt;listcomp&gt;(.0)
    220                 if len(parts) != vector_size + 1:
    221                     raise ValueError(""invalid vector on line %s (is this really the text format?)"" % line_no)
--&gt; 222                 word, weights = parts[0], [datatype(x) for x in parts[1:]]
    223                 add_word(word, weights)
    224     if result.vectors.shape[0] != len(result.vocab):

ValueError: could not convert string to float: '-'
</code></pre>

<p>The system works fine using a text file containing only: ""test -1.0 1.526 -2.55"" or ""- -1.0 1.526 -2.55"". Additionally, searching the source text file (glove.6B.50d.txt) for occurrences of "" - "" comes up with no results. I'm on Windows, so I have done so by executing: </p>

<pre><code>findstr /C:"" - "" glove.6B.50d.txt
</code></pre>

<p>Calling <code>print(gloveFile)</code> both pre- and post-map call provide the following output. Note that I've kept the mapping call in for completeness of my efforts, not for its effect. </p>

<pre><code>0       the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.0...
1       , 0.013441 0.23682 -0.16899 0.40951 0.63812 0....
2       . 0.15164 0.30177 -0.16763 0.17684 0.31719 0.3...
3       of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.7...
4       to 0.68047 -0.039263 0.30186 -0.17792 0.42962 ...
...                                                   ...
399995  chanty 0.23204 0.025672 -0.70699 -0.045465 0.1...
399996  kronik -0.60921 -0.67218 0.23521 -0.11195 -0.4...
399997  rolonda -0.51181 0.058706 1.0913 -0.55163 -0.1...
399998  zsombor -0.75898 -0.47426 0.4737 0.7725 -0.780...
399999  andberger 0.072617 -0.51393 0.4728 -0.52202 -0...
</code></pre>

<p>If I print the first ten lines of the <code>glove6b50d_word2vec.txt</code> file, I get the following text, which matches the word2vec format. Additionally, if I count the occurrences of the string <code>"" - ""</code> in the document, I find none. </p>

<pre><code>['400000 50\n', 'the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n', ', 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n', '. 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n', 'of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n', 'to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n', 'and 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\n', 'in 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\n', 'a 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n', '"" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n']
</code></pre>

<p>My search methods are evidently thusfar ineffective. Would really appreciate some help. </p>
","6036772","","6036772","","2020-05-15 03:08:57","2020-05-15 22:58:14","Glove6b50d parsing: could not convert string to float: '-'","<python><text><gensim><word-embedding><glove>","1","9","","","","CC BY-SA 4.0"
"22196248","1","","","2014-03-05 11:20:51","","4","2283","<p>I'm trying to use <code>gensim's lda</code> model. If I create the lda model with a given corpus, and then I want to update it with a new corpus that contains words that aren't seen in the first corpus, how do I do this? When I try to just call <code>lda_model.update(new_corpus)</code>, I get the following error:</p>

<pre><code>/Library/Python/2.7/site-packages/gensim/models/ldamodel.pyc in inference(self, chunk, collect_sstats)
    361             Elogthetad = Elogtheta[d, :]
    362             expElogthetad = expElogtheta[d, :]
 --&gt;363             expElogbetad = self.expElogbeta[:, ids]
    364 
    365             # The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.
   IndexError: index 57 is out of bounds for axis 1 with size 57
</code></pre>

<p>I initialized lda_model with a corpus consisting of only 57 words, so that's why we see the size <code>57</code> bound. Then I wanted to call update on it with a corpus of many more words, and this fails.</p>

<p>How do I get around this? I want to be able to update my lda model with a new corpus with new words is this possible?</p>
","3121136","","2805846","","2014-03-05 11:41:42","2017-06-05 08:31:15","gensim lda model - calling update on a corpus with unseen words","<lda><gensim>","1","0","","","","CC BY-SA 3.0"
"65064458","1","","","2020-11-29 19:49:50","","0","147","<p>I want to use <code>wrappers.dtmmodel</code> by Mac. For Mac I need to use pre-compiled binaries for OS version unterladen <a href=""https://github.com/magsilva/dtm/tree/master/bin"" rel=""nofollow noreferrer"">https://github.com/magsilva/dtm/tree/master/bin</a>
but wenn I download the <code>dtm-darwin64</code>, and try to run the code, the error shows:</p>
<blockquote>
<p>dtm_path must point to the binary file, not to a folder</p>
</blockquote>
<p>What can I do to deal with this problem?</p>
<p>the code:</p>
<pre class=""lang-py prettyprint-override""><code>
from gensim.models.wrappers.dtmmodel import DtmModel

from gensim.corpora import Dictionary, bleicorpus

dtm_path = &quot;/Users/123/Desktop/dtm-darwin64&quot;

dtm_model = DtmModel(dtm_path, corpus, time_slice, num_topics=3, id2word=dictionary, initialize_lda=True)
</code></pre>
<p>And the error is :</p>
<blockquote>
<p>ValueError: dtm_path must point to the binary file, not to a folder</p>
</blockquote>
<p>thanks</p>
","12379330","","6573902","","2020-12-13 06:29:23","2021-05-21 07:07:53","how to use dtm model/ wrappers dtm by mac?","<python><macos><nlp><gensim>","2","1","","","","CC BY-SA 4.0"
"35981178","1","","","2016-03-14 06:49:59","","3","879","<p>I know that word2vec in gensim can compute similarity between words. But now I want to compute word similarity using TF-IDF or LSA with <strong>gensim</strong>. How to do it? </p>

<p>note: Computing document similarity using LSA with gensim is easy: <a href=""http://radimrehurek.com/gensim/wiki.html"" rel=""nofollow"">http://radimrehurek.com/gensim/wiki.html</a></p>
","6059430","","6059430","","2016-03-14 10:31:31","2016-03-14 10:31:31","How to compute word similarity using TF-IDF or LSA with gensim?","<python><nlp><tf-idf><gensim><lsa>","1","0","","","","CC BY-SA 3.0"
"48281026","1","","","2018-01-16 12:14:07","","0","382","<p>I want to cluster 3.5M 300-dimensional word2vec vectors from my custom gensim model to determine whether I can use those clustering to find topic-related words. It is not the same as <code>model.most_similar_...</code>, as I hope to attach quite distant, but still related words.</p>

<p>The overall size of the model (after normalization of vectors, i.e. <code>model.init_sims(replace=True)</code>) in memory is 4GB:</p>

<pre><code>words = sorted(model.wv.vocab.keys())
vectors = np.array([model.wv[w] for w in words]) 
sys.getsizeof(vectors)
4456416112
</code></pre>

<p>I tried both scikit's DBSCAN and some other implementations from GitHub, but they seem to consume more and more RAM during processing and crash with <code>std::bad_alloc</code> after some time. I have 32 GB of RAM and 130GB swap.</p>

<p>Metric is euclidean, <a href=""https://stackoverflow.com/questions/32745541/dbscan-error-with-cosine-metric-in-python/40368885#40368885"">I convert my cosine distance threshold cos=0.48 as eps=sqrt(2-2*0.48)</a>, so all the optimizations should be applied. </p>

<p>The problem is that I don't know the number of clusters and want to determine them by setting the threshold for closely related words (let it be <code>cos&lt;0.48</code> or <code>d_l2 &lt; sqrt(2-2*0.48)</code>). DBSCAN seems working on small subsets, but I can't pass the computation on the full data.</p>

<p>Is there any algorithm or workaround in Python which can help with that?</p>

<p>EDIT: Distance matrix seem to be for a size(float)=4bytes: 3.5M*3.5M*4/1024(KB)/1024(MB)/1024(GB)/1024(TB) = 44.5 TB, so it's impossible to precompute it.</p>

<p>EDIT2: Currently trying ELKI, but cannot make it to cluster data on toy subset properly. </p>
","1692060","","1692060","","2018-01-16 16:12:23","2019-02-18 11:32:29","Fixed RAM DBSCAN or another clustering algorithm without predefined number of clusters?","<python><optimization><cluster-analysis><gensim>","0","4","","","","CC BY-SA 3.0"
"47882600","1","","","2017-12-19 08:16:30","","1","109","<p>I have a class <code>TfidfRecommendations</code> with several methods and inputs. Some of the inputs are the trained model objects of <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow noreferrer"">Gensims TfIDF model</a> (the function <code>train_tfidf</code> below):</p>

<pre><code>import gensim
from gensim import models, corpora, similarities
def train_tfidf(data):
    dictionary = corpora.Dictionary(data)
    corpus = [dictionary.doc2bow(doc) for doc in data]
    tfidf = models.TfidfModel(corpus)
    index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary))
    return tfidf_model, tfidf_dictionary, tfidf_index
</code></pre>

<p>Where <code>data</code> is a pandas dataframe of text documents (one document per row).</p>

<p>I use the output from <code>train_tfidf</code> above, as input into <code>TfidfRecommendations</code>:</p>

<pre><code>class TfidfRecommendations:

    def __init__(self, data, tfidf_dictionary, tfidf_model, tfidf_index):
        self.data               = data
        self.tfidf_dictionary   = tfidf_dictionary
        self.tfidf_model        = tfidf_model
        self.tfidf_index        = tfidf_index

    ...

    def get_sims(self, query):
        # query is a list of strings to be compared to the corpus data
        vec_bow = self.tfidf_dictionary.doc2bow(query)
        sims = self.tfidf_index[self.tfidf_model[vec_bow]]
        return sims
</code></pre>

<p>the problem with the class <code>TfidfRecommendations</code> is that it returns a list of tuples for <code>sims</code> which is incorrect:</p>

<pre><code>tfidf_model, tfidf_dictionary, tfidf_index = train_tfidf(data)
TFIDF = TfidfRecommendations(data, tfidf_dictionary, tfidf_model, tfidf_index)
sims = TFIDF.get_sims(query_text) # query_text is a list of string tokens
print(sims)
&gt;&gt;&gt;[(4, 0.004360197614450217),
   (19, 0.044387503503385946),
   (46, 0.10344463256852278),
   (82, 0.01845695743910715),
   (125, 0.024611722270581393),
   (133, 0.045794061264144204)]
</code></pre>

<p>Whereas it should return a numpy array of length <code>len(data)</code> with each entry being a cosine similarity between <code>query_text</code> and each row in <code>data</code>. This works fine if <code>get_sims</code> is its own independent function outside of the class <code>TfidfRecommendations</code></p>

<pre><code>def get_sims(query, tfidf_dictionary, tfidf_index, tfidf_model):
    # query is a list of strings to be compared
    # to the corpus data
    vec_bow = tfidf_dictionary.doc2bow(query)
    sims = tfidf_index[tfidf_model[vec_bow]]
    return sims

get_sims(query, tfidf_dictionary, tfidf_index, tfidf_model)
&gt;&gt;&gt; array([ 0.00123292,  0.0080641 ,  0.00420302, ...,  0.        ,
    0.0101376 ,  0.00987199], dtype=float32)
</code></pre>

<p><strong>What is going wrong here?</strong> Why can't gensim model objects be used with <code>self.</code> inside a class? Any help would be much appreciated.</p>
","4139143","","4139143","","2017-12-19 08:35:06","2017-12-19 08:35:06","Using self. in a class with Gensim TfIDF","<python><class><gensim><tf-idf>","0","2","1","","","CC BY-SA 3.0"
"65163881","1","65534378","","2020-12-06 01:45:44","","3","1074","<p>I am trying to run the w2v on this sample of data</p>
<pre><code>Statement              Label
Says the Annies List political group supports third-trimester abortions on demand.       FALSE
When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.         TRUE
&quot;Hillary Clinton agrees with John McCain &quot;&quot;by voting to give George Bush the benefit of the doubt on Iran.&quot;&quot;&quot;     TRUE
Health care reform legislation is likely to mandate free sex change surgeries.    FALSE
The economic turnaround started at the end of my term.     TRUE
The Chicago Bears have had more starting quarterbacks in the last 10 years than the total number of tenured (UW) faculty fired during the last two decades.    TRUE
Jim Dunnam has not lived in the district he represents for years now.    FALSE
</code></pre>
<p>using the code provided in this GitHub folder (FeatureSelection.py):</p>
<p><a href=""https://github.com/nishitpatel01/Fake_News_Detection"" rel=""nofollow noreferrer"">https://github.com/nishitpatel01/Fake_News_Detection</a></p>
<p>I would like to include word2vec features in my Naive Bayes model.
First I considered X and y and used train_test_split:</p>
<pre><code>X = df['Statement']
y = df['Label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)

dataset = pd.concat([X_train, y_train], axis=1)
</code></pre>
<p>This is the code I am currently using:</p>
<pre><code>#Using Word2Vec 
with open(&quot;glove.6B.50d.txt&quot;, &quot;rb&quot;) as lines:
    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))
           for line in lines}

training_sentences = DataPrep.train_news['Statement']

model = gensim.models.Word2Vec(training_sentences, size=100) # x be tokenized text
w2v = dict(zip(model.wv.index2word, model.wv.syn0))


class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(word2vec.itervalues().next())

    def fit(self, X, y): # what are X and y?
        return self

    def transform(self, X): # should it be training_sentences?
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])


&quot;&quot;&quot;
class TfidfEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.word2weight = None
        self.dim = len(word2vec.itervalues().next())
    def fit(self, X, y):
        tfidf = TfidfVectorizer(analyzer=lambda x: x)
        tfidf.fit(X)
        # if a word was never seen - it must be at least as infrequent
        # as any of the known words - so the default idf is the max of 
        # known idf's
        max_idf = max(tfidf.idf_)
        self.word2weight = defaultdict(
            lambda: max_idf,
            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])
        return self
    def transform(self, X):
        return np.array([
                np.mean([self.word2vec[w] * self.word2weight[w]
                         for w in words if w in self.word2vec] or
                        [np.zeros(self.dim)], axis=0)
                for words in X
            ])
&quot;&quot;&quot;
</code></pre>
<p>and in classifier.py, I am running</p>
<pre><code>nb_pipeline = Pipeline([
        ('NBCV',FeaturesSelection.w2v),
        ('nb_clf',MultinomialNB())])
</code></pre>
<p>However this is not working and I am getting this error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-14-07045943a69c&gt; in &lt;module&gt;
      2 nb_pipeline = Pipeline([
      3         ('NBCV',FeaturesSelection.w2v),
----&gt; 4         ('nb_clf',MultinomialNB())])

/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)
     71                           FutureWarning)
     72         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
---&gt; 73         return f(**kwargs)
     74     return inner_f
     75 

/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py in __init__(self, steps, memory, verbose)
    112         self.memory = memory
    113         self.verbose = verbose
--&gt; 114         self._validate_steps()
    115 
    116     def get_params(self, deep=True):

/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py in _validate_steps(self)
    160                                 &quot;transformers and implement fit and transform &quot;
    161                                 &quot;or be the string 'passthrough' &quot;
--&gt; 162                                 &quot;'%s' (type %s) doesn't&quot; % (t, type(t)))
    163 
    164         # We allow last estimator to be None as an identity transformation

TypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '{' ': array([-0.17019527,  0.32363772, -0.0770281 , -0.0278154 , -0.05182227, ....
</code></pre>
<p>I am using all the programs in that folder, so the code can be reproducible if you use them.</p>
<p>If you could explain me how to fix it and what other changes in the code would be necessary, it would be great. My goal is to compare models (naive bayes, random forest,...) with BoW, TF-IDF and Word2Vec.</p>
<p>Update:</p>
<p>After the answer below (from Ismail), I updated the code as follows:</p>
<pre><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec, size=100):
        self.word2vec = word2vec
        self.dim = size
</code></pre>
<p>and</p>
<pre><code>#building Linear SVM classfier
svm_pipeline = Pipeline([
        ('svmCV',FeaturesSelection_W2V.MeanEmbeddingVectorizer(FeaturesSelection_W2V.w2v)),
        ('svm_clf',svm.LinearSVC())
        ])

svm_pipeline.fit(DataPrep.train_news['Statement'], DataPrep.train_news['Label'])
predicted_svm = svm_pipeline.predict(DataPrep.test_news['Statement'])
np.mean(predicted_svm == DataPrep.test_news['Label'])
</code></pre>
<p>However, I am still getting errors.</p>
","14684140","","14684140","","2021-01-04 09:03:22","2021-01-04 09:05:30","Using Word2Vec in scikit-learn pipeline","<python><scikit-learn><gensim><word2vec>","1","6","3","","","CC BY-SA 4.0"
"22361438","1","22493433","","2014-03-12 19:06:18","","1","268","<p>I have bunch of html documents 10-15 on which i have to apply LDA algorithm in gensim
I am stuck on creating the corpus as i don't understand how i design a corpus for a collection of html documents. The example on the site shows the creation of them on wikipedia compressed file .xml.bz</p>

<p>Anyone please guide me how can i apply LDA on bunch of html documents.
Thanks in advance</p>
","3318134","","3318134","","2014-03-12 20:49:18","2014-03-18 23:45:01","LDA for Html Documents in Genism","<python><gensim>","1","0","","","","CC BY-SA 3.0"
"48290403","1","48313226","","2018-01-16 21:51:28","","13","47523","<p>I am working on node2vec in Python, which uses Gensim's <code>Word2Vec</code> internally.</p>
<p>When I am using small dataset the code works well. But as soon as I try to run the same code on large dataset, the code crashes.</p>
<p><code>Error:  Process finished with exit code 134 (interrupted by signal 6: SIGABRT).</code></p>
<p>The line which is giving error is</p>
<pre><code>model = Word2Vec(walks, size=args.dimensions,
                 window=args.window_size, min_count=0, sg=1,
                 workers=args.workers, iter=args.iter)
</code></pre>
<p>I am using pycharm and python 3.5.</p>
<p>Any idea what is happening? I could not found any post which could solve my problem.</p>
","1070734","","130288","","2021-09-17 20:58:04","2021-09-17 21:43:43","Python node2vec (Gensim Word2Vec) ""Process finished with exit code 134 (interrupted by signal 6: SIGABRT)""","<python><pycharm><word2vec><gensim>","5","0","1","","","CC BY-SA 4.0"
"62235365","1","62239946","","2020-06-06 17:23:19","","1","147","<p>We developed a Jupyter Notebook in a local machine to train models with the Python (V3) libraries <code>sklearn</code> and <code>gensim</code>.
As we set the <code>random_state</code> variable to a fixed integer, the results were always the same.</p>

<p>After this, we tried moving the notebook to a workspace in Azure Machine Learning Studio (classic), but the results differ even if we leave the <code>random_state</code> the same.</p>

<p>As suggested in the following links, we installed the same libraries versions and checked the <code>MKL</code> version was the same and the <code>MKL_CBWR</code> variable was set to <code>AUTO</code>.</p>

<p><a href=""https://stackoverflow.com/questions/46766714/t-sne-generates-different-results-on-different-machines"">t-SNE generates different results on different machines</a></p>

<p><a href=""https://stackoverflow.com/questions/38228088/same-python-code-same-data-different-results-on-different-machines"">Same Python code, same data, different results on different machines</a></p>

<p>Still, we are not able to get the same results.</p>

<p>What else should we check or why is this happening?</p>

<p><strong>Update</strong></p>

<p>If we generate a <code>pkl</code> file in the local machine and import it in AML, the results are the same (as the intention of the pkl file is).</p>

<p>Still, we are looking to get the same results (if possible) without importing the pkl file.</p>

<p><strong>Library versions</strong></p>

<pre><code>gensim 3.8.3.
sklearn 0.19.2.
matplotlib 2.2.3.
numpy 1.17.2.
scipy 1.1.0.
</code></pre>

<p><strong>Code</strong></p>

<p>Full code can be found <a href=""https://t.ly/YlCi"" rel=""nofollow noreferrer"">here</a>, sample data link inside.</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib
from matplotlib import pyplot as plt

from gensim.models import KeyedVectors
%matplotlib inline

import time

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import seaborn as sns

wordvectors_file_vec = '../libraries/embeddings-new_large-general_3B_fasttext.vec'
wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)

math_quests = # some transformations using wordvectors

df_subset = pd.DataFrame()

pca = PCA(n_components=3, random_state = 42)
pca_result = pca.fit_transform(mat_quests)
df_subset['pca-one'] = pca_result[:,0]
df_subset['pca-two'] = pca_result[:,1] 

time_start = time.time()
tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state = 42)
tsne_results = tsne.fit_transform(mat_quests)

df_subset['tsne-2d-one'] = tsne_results[:,0]
df_subset['tsne-2d-two'] = tsne_results[:,1]

pca_50 = PCA(n_components=50, random_state = 42)
pca_result_50 = pca_50.fit_transform(mat_quests)
print('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))

time_start = time.time()
tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300, random_state = 42)
tsne_pca_results = tsne.fit_transform(pca_result_50)
print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))
</code></pre>
","13155092","","13155092","","2020-06-07 00:25:15","2020-06-07 01:37:03","Models generate different results when moving to Azure Machine Learning Studio","<python><scikit-learn><gensim><azure-machine-learning-studio>","1","5","","","","CC BY-SA 4.0"
"39006270","1","39006865","","2016-08-17 21:05:21","","1","195","<p>Given a matrix </p>

<p>----<code>d1 d2 d3
 a: v1  0  v2
 b: v1  v3  0</code></p>

<p>I want </p>

<p>----<code>v1 v2 v3
 a: 1  1   0
 b: 1  0   1</code></p>

<p>I remember vaguely that this can be done with <code>Gensim</code>...but there must also be some module in pandas? I have tried to do <code>for v in v: for el in [a,b]</code>(happy to post the code, but I think that the example is clear enough) but it is very slow, and I imagine this must have been solved before.</p>
","2795733","","","","","2016-08-17 22:19:33","How to convert a set of features to a count matrix in pandas","<python><pandas><gensim>","2","0","","","","CC BY-SA 3.0"
"26902048","1","","","2014-11-13 05:36:55","","3","797","<p>I know from using scikit learn i could use,</p>

<pre><code>vectorizer = TfidfVectorizer(min_df=2,ngram_range=(1, 2),norm='l2')

corpus = vectorizer.fit_transform(text)
</code></pre>

<p>This piece of code. But how could i do this with gensim?</p>
","3106424","","","","","2020-07-01 15:19:53","how to tokenize a set of documents into unigram + bigram bagofwords using gensim?","<python-2.7><scikit-learn><gensim>","2","0","","","","CC BY-SA 3.0"
"45876711","1","45929180","","2017-08-25 07:51:09","","4","3679","<p>I'm using word2vec on a 1 million abstracts dataset (2 billion words). To find most similar documents, I use the <code>gensim.similarities.WmdSimilarity</code> class. When trying to retrieve the best match using <code>wmd_similarity_index[query]</code>, the calculation spends most of its time building a dictionary. Here is a piece of log:</p>

<pre><code>2017-08-25 09:45:39,441 : INFO : built Dictionary(127 unique tokens: ['empirical', 'model', 'estimating', 'vertical', 'concentration']...) from 2 documents (total 175 corpus positions)                                                        
2017-08-25 09:45:39,445 : INFO : adding document #0 to Dictionary(0 unique tokens: [])          
</code></pre>

<p>What does this part ? Is it dependent on the query ? Is there a way to do these calculations once for all ?</p>

<p><strong>EDIT:</strong> training and scoring phases in my code:</p>

<p>Training and saving to disk:</p>

<pre><code>w2v_size = 300
word2vec = gensim.models.Word2Vec(texts, size=w2v_size, window=9, min_count=5, workers=1, sg=1, hs=1, iter=20) # sg=1 means skip gram is used¬†
word2vec.save(utils.paths.PATH_DATA_GENSIM_WORD2VEC)
corpus_w2v_wmd_index = gensim.similarities.WmdSimilarity(texts, word2vec.wv)
corpus_w2v_wmd_index.save(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)
</code></pre>

<p>Loading and scoring:</p>

<pre><code>w2v = gensim.models.Word2Vec.load(utils.paths.PATH_DATA_GENSIM_WORD2VEC)
words = [t for t in proc_text if t in w2v.wv]
corpus_w2v_wmd_index = gensim.similarities.docsim.Similarity.load(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)
scores_w2v = np.array(corpus_w2v_wmd_index[words])  
</code></pre>
","6084245","","6084245","","2017-08-28 12:24:02","2017-08-29 00:50:55","Gensim word2vec WMD similarity dictionary","<python><nlp><word><gensim><word2vec>","1","2","2","","","CC BY-SA 3.0"
"53610331","1","53610929","","2018-12-04 10:03:24","","0","675","<p>I've got a problem with online updating my Word2Vec model.</p>

<p>I have a document and build model by it. But this document can update with new words, and I need to update vocabulary and model in general.</p>

<p>I know that in gensim 0.13.4.1 we can do this</p>

<p>My code:</p>

<pre><code>model = gensim.models.Word2Vec(size=100, window=10, min_count=5, workers=11, alpha=0.025, min_alpha=0.025, iter=20)
model.build_vocab(sentences, update=False)

model.train(sentences, epochs=model.iter, total_examples=model.corpus_count)

model.save('model.bin')
</code></pre>

<p>And after this I have new words. For e.x.:</p>

<pre><code>sen2 = [['absd', 'jadoih', 'sdohf'], ['asdihf', 'oisdh', 'oiswhefo'], ['a', 'v', 'b', 'c'], ['q', 'q', 'q']]

model.build_vocab(sen2, update=True)
model.train(sen2, epochs=model.iter, total_examples=model.corpus_count)
</code></pre>

<p>What's wrong and how can I solve my problem?</p>
","10208662","","","","","2018-12-04 10:33:13","Online updating Word2Vec","<python><nlp><gensim><word2vec>","1","3","","","","CC BY-SA 4.0"
"16309798","1","","","2013-04-30 22:10:04","","5","1723","<p>I'm using Gensim to calculate the similarity between 2 documents. For some reason the line tfidf[corpus] returns an empty list. I'm not sure why though</p>

<pre><code>    articles = []
#make a corpus by adding each of the top 25 documents to a list
for x in range(0,25):
    articles.append(str(WikiDoc(sorted_links[0]).jsonify()['text']))
#puts all of the top 25 documents into a list
texts = [[word for word in document.lower().split()] for document in articles]
print texts
#load precomputed dictionary
articles_dict = corpora.Dictionary(texts)
articles_dict.save('./articles.dict')
articles_dict = Dictionary.load('./articles.dict')
#articles_corpus = [articles_dict.doc2bow(text) for text in texts]
#corpora.MmCorpus.serialize('./articles.mm', articles_corpus)
corpus = [articles_dict.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('./articles.mm', corpus)
corpus = corpora.MmCorpus('./articles.mm')
#build the tfidf model based on the 25 documents so that we can find similarities 
#with respect to each of these documents
tfidf = models.TfidfModel(corpus)
#get the other document and process to produce dictionary representation
one_doc_bow = WikiDoc('SpongeBob')
one_doc_bow = articles_dict.doc2bow(one_doc_bow.jsonify()['text'].lower().split())
print tfidf[one_doc_bow]
top = tfidf[one_doc_bow]
corpus_tfidf = tfidf[corpus]
</code></pre>

<p>When I print the dictionary I get: Dictionary(2204 unique tokens)
When I print the MmCorpus I get: MmCorpus(25 documents, 2204 features, 55100 non-zero entries)
tfidf[corpus] yield [].
Can anyone diagnose my problem? Thanks a lot!</p>
","2330722","","","","","2013-04-30 22:10:04","Calculating TF-IDF Similarity Between 2 Documents Using Gensim","<python><nlp><similarity><gensim>","0","2","","","","CC BY-SA 3.0"
"44553278","1","44555671","","2017-06-14 19:34:58","","0","604","<p>I am planning to use Multi Layer Perceptron Classifier from Scikit Learn for this purpose.<br>
Output is the Gender of that word which shall be represented in a one-hot encoding like [1,0,0] for male, [0, 1, 0] for female and [0, 0, 1] for female. 
Now one of the inputs is the word vector for the word. Each of these vectors has 20 dimensions.
The other features are it's Part Of Speech Tags and Singularity(0)/Plurality(1) state. 
My question is how do I use the word vector which is an array as a feature in MLPClassifier?</p>
","6503743","","","","","2017-06-14 22:26:18","How do I use the word vector returned by word2vec as features?","<python><scikit-learn><neural-network><gensim><word2vec>","1","1","","","","CC BY-SA 3.0"
"65059959","1","65065084","","2020-11-29 12:16:18","","1","594","<p>I was reading <a href=""https://stackoverflow.com/a/54581599/7339624"">this answer</a> That says about Gensim <code>most_similar</code>:</p>
<blockquote>
<p>it performs vector arithmetic: adding the positive vectors,
subtracting the negative, then from that resulting position, listing
the known-vectors closest to that angle.</p>
</blockquote>
<p>But when I tested it, that is not the case. I trained a Word2Vec with Gensim <code>&quot;text8&quot;</code> dataset and tested these two:</p>
<pre><code>model.most_similar(positive=['woman', 'king'], negative=['man'])

&gt;&gt;&gt; [('queen', 0.7131118178367615), ('prince', 0.6359186768531799),...]
</code></pre>
<hr />
<pre><code>model.wv.most_similar([model[&quot;king&quot;] + model[&quot;woman&quot;] - model[&quot;man&quot;]])

&gt;&gt;&gt; [('king', 0.84305739402771), ('queen', 0.7326322793960571),...]
</code></pre>
<p>They are clearly not the same. even the queen score in the first is <code>0.713</code> and on the second <code>0.732</code> which are not the same.</p>
<p><strong>So</strong> I ask the question again, How does Gensim <code>most_similar</code> work? why the result of the two above are different?</p>
","7339624","","","","","2020-11-29 21:06:34","gensim most_similar with positive and negative, how does it work?","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"56292749","1","56293121","","2019-05-24 12:30:44","","0","49","<p>I want to tokenize text with <code>gensim.utils.tokenize()</code>. And I want to add some phrases that would be recognized as single tokens, for example: <code>'New York', 'Long Island'</code>. </p>

<p>Is it possible with gensim? If not, what other libraries is it possible to use?</p>
","7281684","","7281684","","2019-05-27 08:42:23","2019-05-27 08:42:23","How to specify additional tokens for tokenizator?","<python><nlp><token><tokenize><gensim>","1","0","","","","CC BY-SA 4.0"
"22286488","1","","","2014-03-09 18:47:40","","1","1232","<p>Hello GenSim community. 
I am working with Python 2.7.5, and Sublime Text 2. I need to install Gensim. I have tried Enthought but it does not support NLTK. I need to import GenSim in Sublime Text 2. I tried all the command line instructions, to no success. Can someone please tell me how to get GenSim in ST2? I already have Scipy and Numpy installed. I have tried following instructions on the Radim Rehurek webpage, easy_install, and pip install etc. </p>
","3116297","","","","","2014-03-10 07:18:28","Python with NLTK and GenSim","<python><sublimetext2><nltk><gensim>","1","2","1","","","CC BY-SA 3.0"
"32794087","1","","","2015-09-26 05:48:31","","3","429","<p>Gensim has this document similarity feature which when inputted a query document, it outputs the similarity of that particular document with all the documents it has in its index</p>

<ol>
<li><p>Can this be used like an ""approximate"" version of supervised classification?</p></li>
<li><p>I know gensim's word2vec uses Deep Learning, is this involved during the above step?</p></li>
</ol>
","5378560","","","","","2015-09-26 05:48:31","Gensim's Document similarity can be used as supervised classification?","<machine-learning><nlp><text-processing><gensim>","0","1","","","","CC BY-SA 3.0"
"62140106","1","62161208","","2020-06-01 20:01:45","","1","349","<p>The <code>most_similar</code> method finds the top-N most similar words.</p>

<p>Is there a method or a way to find the N least similar words?</p>
","3480376","","792066","","2020-06-14 13:25:08","2020-06-14 13:25:08","Least Similar with Gensim Doc2Vec","<gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"22433884","1","22561795","","2014-03-16 06:51:25","","33","25708","<p>I've got a trained LDA model and I want to calculate the similarity score between two documents from the corpus I trained my model on.
After studying all the Gensim tutorials and functions, I still can't get my head around it. Can somebody give me a hint? Thanks!</p>
","1808818","","","","","2020-06-10 12:47:30","Python Gensim: how to calculate document similarity using the LDA model?","<python><nlp><lda><gensim>","3","0","17","","","CC BY-SA 3.0"
"62419353","1","62612072","","2020-06-16 23:57:19","","3","215","<p>Would it be possible to look for texts that are within a certain topic (determined by LDA)? </p>

<p>I have a list of 5 topics with 10 words each, found by using lda.</p>

<p>I have analysed the texts in a dataframe‚Äôs column. 
I would like to select/filter rows/texts that are in one specific topic. </p>

<p>If you need more information, I will provide you. </p>

<p>What I am referring to is the step that returns this output:</p>

<pre><code>[(0,
  '0.207*""house"" + 0.137*""apartment"" + 0.118*""sold"" + 0.092*""beach"" + '
  '0.057*""kitchen"" + 0.049*""rent"" + 0.033*""landlord"" + 0.026*""year"" + '
  '0.024*""bedroom"" + 0.023*""home""'),
 (1,
  '0.270*""school"" + 0.138*""homeworks"" + 0.117*""students"" + 0.084*""teacher"" + '
  '0.065*""pen"" + 0.038*""books"" + 0.022*""maths"" + 0.020*""exercise"" + '
  '0.020*""friends"" + 0.020*""college""'),
 ... ]
</code></pre>

<p>created by </p>

<pre><code># LDA Model

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=num_topics, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto', 
                                           # alpha=[0.01]*num_topics,
                                           per_word_topics=True,
                                           eta=[0.01]*len(id2word.keys()))
</code></pre>

<h1>Print the Keyword in the 10 topics</h1>

<pre><code>from pprint import pprint
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]
</code></pre>

<p>The original column with texts that have been analysed is called <code>Texts</code> and it looks like: </p>

<pre><code>Texts 

""Children are happy to go to school...""
""The average price for buying a house is ... ""
""Our children love parks so we should consider to buy an apartment nearby""

etc etc...
</code></pre>

<p>My expected output would be </p>

<pre><code>Texts                                            Topic 
    ""Children are happy to go to school...""         2
    ""The average price for buying a house is ... ""  1
    ""Our children love parks so we should consider to buy an apartment nearby""                                   

      2
</code></pre>

<p>Thanks </p>
","","user12809368","","user12809368","2020-06-17 00:39:12","2020-06-27 15:42:09","Select texts by topic (LDA)","<python><gensim><text-classification><lda>","1","0","1","","","CC BY-SA 4.0"
"46917675","1","","","2017-10-24 18:33:27","","0","1454","<p>I have downloaded dump of Wikipedia files (13.40 GB). It is downloaded in the format <code>enwiki-latest-pages-articles.xml.bz2</code>.</p>

<p>How to load the file in Python &amp; then convert the articles into a plain text file inorder to perform LDA on it?</p>

<p>Was following the instructions fromm <a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a> but data loading into Python is not mentioned.</p>
","8578853","","577167","","2017-10-24 18:35:53","2017-10-24 19:12:38","How to load the Wikipedia dump?","<python><gensim>","1","1","","","","CC BY-SA 3.0"
"32826927","1","","","2015-09-28 15:46:29","","1","103","<p>I have word2vec model and pyspark job in which I am summing up individual word vectors for each document. </p>

<pre><code>model = gensim.models.Word2Vec.load('w2v.mod')
model2 = sc.broadcast(model)

def getVector(article):
    vec = numpy.ndarray(100)
    for word in article:
        if word in model2.value:
            vec += model2.value[word]
    return vec /len(article)

data = sc.textFile('documents.txt').map(lambda doc:doc.split())
vectors=  data.map(lambda doc:(doc,getVector(doc)))
</code></pre>

<p>I am getting strange discrepancy between pyspark results and normal results. </p>

<pre><code>vectors.take(1)
</code></pre>

<p><strong>Results with Spark</strong></p>

<p><a href=""https://i.stack.imgur.com/zgPjC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zgPjC.png"" alt=""Results with spark""></a></p>

<p><strong>Results without spark</strong>
<a href=""https://i.stack.imgur.com/TSx9D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TSx9D.png"" alt=""Results without spark""></a></p>

<p>It just gives me arrays with intensity in dimension of 15 order of magnitude(look at the 7th element above) while computing the same vector locally gives me normal vectors. What is going wrong with using Spark here? Does it mess up answers in communicating the results to Master?</p>
","3413239","","","","","2015-09-28 15:46:29","Discrepancy in results from Spark using Broadcasted varaibles","<python><apache-spark><pyspark><gensim><word2vec>","0","0","","","","CC BY-SA 3.0"
"48313840","1","","","2018-01-18 04:25:36","","1","396","<p>I want to train doc2vec model by gensim but my corpus is too large. 
Is there any method to train every batch of sentences corpus? For example, iterately load some corpus and train the model on it, and reload another batch of corpus....</p>

<p>I don't know if there is any api or method for doing this. Any hints ?</p>
","6407393","","","","","2018-01-18 22:47:38","Training gensim doc2vec occures memory error?","<word2vec><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"32812159","1","","","2015-09-27 19:48:10","","1","75","<p>After I installed the Theano and genesim by pip install xxx --user on server side, I tried to import them in python code, but I had this error: </p>

<p>Exception: Compilation failed (return status=1): /usr/bin/ld: cannot find -lInclude. collect2: ld returned 1 exit status. </p>

<p>In addition, I have no root permission, how can I make it?</p>
","4801125","","","","","2015-09-27 19:48:10","Cannot find -lInclude on server side","<python><compilation><server><theano><gensim>","0","1","","","","CC BY-SA 3.0"
"62200198","1","62206743","","2020-06-04 17:00:29","","0","283","<p>Is there a way to get the topic distribution of an unseen document using a pretrained LDA model without using the LDA_Model[unseenDoc] syntax? I am trying to implement my LDA model into a web application, and if there was a way to use matrix multiplication to get a similar result then I could use the model in javascript.</p>

<p>For example, I tried the following:</p>

<pre><code>import numpy as np
import gensim
from gensim.corpora import Dictionary
from gensim import models
import nltk
from nltk.stem import WordNetLemmatizer, SnowballStemmer
nltk.download('wordnet')


def Preprocesser(text_list):

    smallestWordSize = 3
    processedList = []

    for token in gensim.utils.simple_preprocess(text_list):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; smallestWordSize:
            processedList.append(StemmAndLemmatize(token))

    return processedList

lda_model = models.LdaModel.load('LDAModel\GoldModel')  #Load pretrained LDA model
dictionary = Dictionary.load(""ModelTrain\ManDict"")      #Load dictionary model was trained on

#Sample Unseen Doc to Analyze
doc = ""I am going to write a string about how I can't get my task executor \
to travel properly. I am trying to use the \
AGV navigator, but it doesn't seem to be working network. I have been trying\
to use the AGV Process flow but that isn't working either speed\
trailer offset I am now going to change this so I can see how fast it runs""

termTopicMatrix = lda_model.get_topics()    #Get Term-topic Matrix from pretrained LDA model
cleanDoc = Preprocesser(doc)                #Tokenize, lemmatize, clean and stem words
bowDoc = dictionary.doc2bow(cleanDoc)       #Create bow using dictionary
dictSize = len(termTopicMatrix[0])          #Get length of terms in dictionary
fullDict = np.zeros(dictSize)               #Initialize array which is length of dictionary size
First = [first[0] for first in bowDoc]      #Get index of terms in bag of words
Second = [second[1] for second in bowDoc]   #Get frequency of term in bag of words
fullDict[First] = Second                    #Add word frequency to full dictionary


print('Matrix Multiplication: \n', np.dot(termTopicMatrix,fullDict))
print('Conventional Syntax: \n', lda_model[bowDoc])

Output:
Matrix Multiplication: 
 [0.0283254  0.01574513 0.03669142 0.01671816 0.03742738 0.01989461
 0.01558603 0.0370233  0.04648389 0.02887623 0.00776652 0.02147539
 0.10045133 0.01084273 0.01229849 0.00743788 0.03747379 0.00345913
 0.03086953 0.00628912 0.29406082 0.10656977 0.00618827 0.00406316
 0.08775404 0.00785408 0.02722744 0.09957815 0.01669402 0.00744392
 0.31177135 0.03063149 0.07211428 0.01192056 0.03228589]
Conventional Syntax: 
 [(0, 0.070313625), (2, 0.056414187), (18, 0.2016589), (20, 0.46500313), (24, 0.1589748)]
</code></pre>

<p>In the pretrained model there are 35 topics and 1155 words.</p>

<p>In the ""Conventional Syntax"" output, the first element of each tuple is the index of the topic and the second element is the probability of the topic. In the ""Matrix Multiplication"" version, the probability is the index and the value is the probability. Clearly the two don't match up.</p>

<p>For example, the lda_model[unseenDoc] shows that topic 0 has a 0.07 probability, but the matrix multiplication method says that topic has a 0.028 probability. Am I missing a step here?</p>
","5597306","","","","","2020-06-05 01:36:53","Is there a way to infer topic distributions on unseen document from gensim LDA pre-trained model using matrix multiplication?","<gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"32796485","1","","","2015-09-26 11:05:54","","2","1612","<p>I have a <code>doc2vec</code> model build from my data, now I have a new sentence in run time which does not belong to the trained data set.</p>

<p>How can I build or predict a vector for this sentence from my model?</p>

<p>How should I handle unknown words in this sentence?</p>
","810031","","2808510","","2015-09-26 11:41:38","2015-09-27 03:59:29","Building Vector for a sentence in doc2vec from an untrained data set","<python><machine-learning><nlp><gensim><word2vec>","1","0","1","","","CC BY-SA 3.0"
"9969599","1","13501490","","2012-04-02 00:31:23","","3","2304","<p>I'm trying to retrieve list of topics from a large corpus of news articles, I'm planning to use gensim to extract a topic distribution for each document using LDA. I want to know the format of processed articles required by gensim implementation of lda and how to convert raw articles to that format. I saw this link about using lda on wikipedia dump but I found the corpus to be in a processed state whose format was not mentioned anywhere</p>
","724471","","","","","2012-11-23 16:35:09","How to use gensim for lda on news articles?","<machine-learning><lda><gensim>","2","0","1","","","CC BY-SA 3.0"
"57652804","1","57655071","","2019-08-26 06:31:05","","0","679","<p>i have trained my model with Gensim.now i wanna evaluate my model with simlexx-999 but it gives me error.
my code.</p>

<pre><code>model.wv.evaluate_word_analogies('SimLex-999.txt')
2019-08-25 13:43:22,766 : INFO : Evaluating word analogies for top 300000 words in the model on SimLex-999.txt
</code></pre>

<p>error</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-12-60cb96c45579&gt; in &lt;module&gt;()
----&gt; 1 model.wv.evaluate_word_analogies('SimLex-999.txt')

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in evaluate_word_analogies(self, analogies, restrict_vocab, case_insensitive, dummy4unknown)
   1088             else:
   1089                 if not section:
-&gt; 1090                     raise ValueError(""Missing section header before line #%i in %s"" % (line_no, analogies))
   1091                 try:
   1092                     if case_insensitive:

ValueError: Missing section header before line #0 in SimLex-999.txt
</code></pre>

<p>i have tried</p>

<pre><code>from gensim.test.utils import datapath

similarities = model.evaluate_word_pairs(datapath('SimLex-999.txt'))

print(similarities)
</code></pre>

<p>but it gives me keyError.Please help me to solve the problem.</p>

<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-29-caeb682cb7ff&gt; in &lt;module&gt;()
      1 from gensim.test.utils import datapath
      2 
----&gt; 3 similarities = model.wv.evaluate_word_pairs(datapath('SimLex-999.txt'),dummy4unknown=True)
      4 
      5 print(similarities)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in evaluate_word_pairs(self, pairs, delimiter, restrict_vocab, case_insensitive, dummy4unknown)
   1287 
   1288         """"""
-&gt; 1289         ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
   1290         ok_vocab = {w.upper(): v for w, v in reversed(ok_vocab)} if case_insensitive else dict(ok_vocab)
   1291 

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in &lt;listcomp&gt;(.0)
   1287 
   1288         """"""
-&gt; 1289         ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
   1290         ok_vocab = {w.upper(): v for w, v in reversed(ok_vocab)} if case_insensitive else dict(ok_vocab)
   1291 

KeyError: 'movie'
</code></pre>
","","user10702710","","user10702710","2019-08-27 04:15:31","2019-12-20 11:20:18","evaluating word2vec model using SimLex-999","<python-3.x><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"40524768","1","","","2016-11-10 10:04:40","","2","2804","<p>I applied lda with both sklearn and with gensim. Then i checked perplexity of the held-out data. </p>

<p>I am getting negetive values for perplexity of gensim and positive values of perpleixy for sklearn. How do i compare those values.</p>

<p>sklearn perplexity = 417185.466838</p>

<p>gensim perplexity = -9212485.38144</p>
","5332464","","","","","2016-11-15 17:52:49","Perplexity comparision issue in SKlearn LDA vs Gensim LDA","<python><scikit-learn><nlp><lda><gensim>","1","2","","","","CC BY-SA 3.0"
"47890052","1","47892410","","2017-12-19 15:20:20","","9","5117","<p>I tried to apply doc2vec on 600000 rows of sentences: Code as below:</p>

<pre><code>from gensim import models
model = models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1, workers = 5)
model.build_vocab(res)
token_count = sum([len(sentence) for sentence in res])
token_count

%%time
for epoch in range(100):
    #print ('iteration:'+str(epoch+1))
    #model.train(sentences)
    model.train(res, total_examples = token_count,epochs = model.iter)
    model.alpha -= 0.0001  # decrease the learning rate`
    model.min_alpha = model.alpha  # fix the learning rate, no decay
</code></pre>

<p>I am getting very poor results with the above implementation. 
the change I made apart from what was suggested in the tutorial was change the below line:</p>

<pre><code>  model.train(sentences)
</code></pre>

<p>As:</p>

<pre><code> token_count = sum([len(sentence) for sentence in res])
model.train(res, total_examples = token_count,epochs = model.iter)
</code></pre>
","8382950","","130288","","2017-12-21 05:02:27","2017-12-21 05:02:27","Improving Gensim Doc2vec results","<python><nlp><gensim><doc2vec>","1","0","5","","","CC BY-SA 3.0"
"31975754","1","","","2015-08-12 21:30:08","","1","853","<p>I'm using the LDA algorithm from the gensim package to find topics in a given text.</p>

<p>I've been asked that the resulting topics will include different words for each topic, E.G If topic A has the word 'monkey' in it then no other topic should include the word 'monkey' in its list.</p>

<p><strong>My thoughts so far:</strong> run it multiple times and each time add the previous words to the stop words list.</p>

<p>Since:
A) I'm not even sure of algorithmically/logically it's the right thing to do.
B) I hope there's a built in way to do it that i'm not aware of.
C) This is a large database, and it takes about 20 minutes to run the LDA
each time (using the multi-core version).</p>

<p><strong>Question:</strong> Is there a better way to do it?</p>

<p>Hope to get some help,</p>

<p>Thanks.</p>
","2498849","","","","","2016-10-07 13:41:34","gensim LDA: How can i generate topics with different words for each topic?","<python><algorithm><api><lda><gensim>","2","0","","","","CC BY-SA 3.0"
"40458742","1","40566701","","2016-11-07 06:03:00","","17","4659","<p>In the word2vec model, there are two linear transforms that take a word in vocab space to a hidden layer (the ""in"" vector), and then back to the vocab space (the ""out"" vector). Usually this out vector is discarded after training. I'm wondering if there's an easy way of accessing the out vector in gensim python? Equivalently, how can I access the out matrix?</p>

<p>Motivation: I would like to implement the ideas presented in this recent paper: <a href=""https://arxiv.org/pdf/1602.01137v1.pdf"" rel=""noreferrer"">A Dual Embedding Space Model for Document Ranking</a></p>

<p>Here are more details. From the reference above we have the following word2vec model:</p>

<p><a href=""https://i.stack.imgur.com/OpupG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/OpupG.png"" alt=""enter image description here""></a></p>

<p>Here, the input layer is of size $V$, the vocabulary size, the hidden layer is of size $d$, and an output layer of size $V$. The two matrices are W_{IN} and W_{OUT}. <em>Usually</em>, the word2vec model keeps only the W_IN matrix. This is what is returned where, after training a word2vec model in gensim, you get stuff like:</p>

<blockquote>
  <p>model['potato']=[-0.2,0.5,2,...] </p>
</blockquote>

<p>How can I access, or retain W_{OUT}? This is likely quite computationally expensive, and I'm really hoping for some built in methods in gensim to do this because I'm afraid that if I code this from scratch, it would not give good performance.</p>
","2781958","","2781958","","2016-11-12 18:37:16","2018-09-04 00:23:44","gensim word2vec accessing in/out vectors","<python><gensim>","4","1","4","","","CC BY-SA 3.0"
"40527326","1","","","2016-11-10 12:11:38","","0","902","<p>I would like to extract data from the wikipedia summary page of ""machine learning"" and then use that data to build a word2vec model with gensim library.</p>

<p>So, first I get the wiki summary of ""machine learning"" (Wikipedia API for Python):</p>

<pre><code>sentences = wikipedia.summary(""machine learning"")
</code></pre>

<p>and then I create the model:</p>

<pre><code>model = gensim.models.Word2Vec(sentences, min_count=2, size=50, window=4)
</code></pre>

<p>The problem is that, if I print the vocabulary keys, I get a list of characters rather than a list of words. The following is the code that I use to print the vocabulary keys:</p>

<pre><code>print list(model.vocab.keys())
</code></pre>

<p>Where I am wrong?</p>

<p>Here I pasted the full code:</p>

<pre><code>import wikipedia, gensim.models
sentences = wikipedia.summary(""machine learning"")
model = gensim.models.Word2Vec(sentences, min_count=2, size=50, window=4)
print list(model.vocab.keys())
</code></pre>
","5716568","","","","","2016-11-19 09:25:49","how to create a word2vec model with data extracted from wikipedia summary in python","<python><wikipedia><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"63345527","1","63345663","","2020-08-10 17:55:23","","-1","1412","<p>I am trying to build a Docker application that uses Python's gensim library, version 3.8.3, which is being installed via pip from a requirements.txt file.</p>
<p>However, Docker seems to have trouble while trying to do
RUN pip install -r requirements.txt</p>
<p>My Requirement.txt for reference -</p>
<pre><code>boto==2.49.0
boto3==1.14.33
botocore==1.17.33
certifi==2020.6.20
chardet==3.0.4
click==7.1.2
Cython==0.29.14
docutils==0.15.2
Flask==1.1.2
gensim==3.8.3
idna==2.10
itsdangerous==1.1.0
Jinja2==2.11.2
jmespath==0.10.0
MarkupSafe==1.1.1
numpy==1.19.1
python-dateutil==2.8.1
requests==2.24.0
s3transfer==0.3.3
scipy==1.5.2
six==1.15.0
smart-open==2.1.0
urllib3==1.25.10
Werkzeug==1.0.1
</code></pre>
<p>dockerFile</p>
<pre><code>FROM python:3.8.2-alpine
WORKDIR /project
ADD . /project
RUN set -x &amp;&amp; apk add --no-cache build-base &amp;&amp; apk add --no-cache libexecinfo-dev
RUN pip install --upgrade pip
RUN pip install -r requirements.txt
CMD [&quot;python&quot;,&quot;similarity.py&quot;] 
</code></pre>
<p>error:</p>
<pre><code>(venv) C:\Users\verma\PycharmProjects\flaskTest&gt;docker image build -t similarity-flask-api  .
Sending build context to Docker daemon  302.7MB
Step 1/7 : FROM python:3.8.2-alpine
 ---&gt; 6c32e2504283
Step 2/7 : WORKDIR /project
 ---&gt; Using cache
 ---&gt; 554b6bda89ad
Step 3/7 : ADD . /project
 ---&gt; d085a645ecb1
Step 4/7 : RUN set -x &amp;&amp; apk add --no-cache build-base &amp;&amp; apk add --no-cache libexecinfo-dev
 ---&gt; Running in e7117c1e18ff
+ apk add --no-cache build-base
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/community/x86_64/APKINDEX.tar.gz
(1/18) Installing libgcc (9.2.0-r4)
(2/18) Installing libstdc++ (9.2.0-r4)
(3/18) Installing binutils (2.33.1-r0)
(4/18) Installing libmagic (5.37-r1)
(5/18) Installing file (5.37-r1)
(6/18) Installing gmp (6.1.2-r1)
(7/18) Installing isl (0.18-r0)
(8/18) Installing libgomp (9.2.0-r4)
(9/18) Installing libatomic (9.2.0-r4)
(10/18) Installing mpfr4 (4.0.2-r1)
(11/18) Installing mpc1 (1.1.0-r1)
(12/18) Installing gcc (9.2.0-r4)
(13/18) Installing musl-dev (1.1.24-r2)
(14/18) Installing libc-dev (0.7.2-r0)
(15/18) Installing g++ (9.2.0-r4)
(16/18) Installing make (4.2.1-r2)
(17/18) Installing fortify-headers (1.1-r0)
(18/18) Installing build-base (0.5-r1)
Executing busybox-1.31.1-r9.trigger
OK: 182 MiB in 52 packages
+ apk add --no-cache libexecinfo-dev
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/community/x86_64/APKINDEX.tar.gz
(1/2) Installing libexecinfo (1.1-r1)
(2/2) Installing libexecinfo-dev (1.1-r1)
OK: 183 MiB in 54 packages
Removing intermediate container e7117c1e18ff
 ---&gt; 9e7a97f8bddc
Step 5/7 : RUN pip install --upgrade pip
 ---&gt; Running in 0286591e9e70
Collecting pip
  Downloading pip-20.2.1-py2.py3-none-any.whl (1.5 MB)
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 20.1
    Uninstalling pip-20.1:
      Successfully uninstalled pip-20.1
Successfully installed pip-20.2.1
Removing intermediate container 0286591e9e70
 ---&gt; ca837786d695
Step 6/7 : RUN pip install -r requirements.txt
 ---&gt; Running in 7f124c100c0b
Collecting boto==2.49.0
  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)
Collecting boto3==1.14.33
  Downloading boto3-1.14.33-py2.py3-none-any.whl (129 kB)
Collecting botocore==1.17.33
  Downloading botocore-1.17.33-py2.py3-none-any.whl (6.5 MB)
Collecting certifi==2020.6.20
  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)
Collecting chardet==3.0.4
  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)
Collecting click==7.1.2
  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)
Collecting Cython==0.29.14
  Downloading Cython-0.29.14.tar.gz (2.1 MB)
Collecting docutils==0.15.2
  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)
Collecting Flask==1.1.2
  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)
Collecting gensim==3.8.3
  Downloading gensim-3.8.3.tar.gz (23.4 MB)
Collecting idna==2.10
  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)
Collecting itsdangerous==1.1.0
  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)
Collecting Jinja2==2.11.2
  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)
Collecting jmespath==0.10.0
  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)
Collecting MarkupSafe==1.1.1
  Downloading MarkupSafe-1.1.1.tar.gz (19 kB)
Processing /root/.cache/pip/wheels/df/b2/64/111c431ca7f7d49afb42126b7351fe1a4894803d75026360de/numpy-1.19.1-cp38-cp38-linux_x86_64.whl
Collecting python-dateutil==2.8.1
  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)
Collecting requests==2.24.0
  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)
Collecting s3transfer==0.3.3
  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)
Collecting scipy==1.5.2
  Downloading scipy-1.5.2.tar.gz (25.4 MB)
  Installing build dependencies: started
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
    Preparing wheel metadata: started
    Preparing wheel metadata: finished with status 'error'
    ERROR: Command errored out with exit status 1:
     command: /usr/local/bin/python /usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpoyjzx5wb
         cwd: /tmp/pip-install-r078skp_/scipy
    Complete output (139 lines):
    lapack_opt_info:
    lapack_mkl_info:
    customize UnixCCompiler
      libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    openblas_lapack_info:
    customize UnixCCompiler
    customize UnixCCompiler
      libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    openblas_clapack_info:
    customize UnixCCompiler
    customize UnixCCompiler
      libraries openblas,lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    flame_info:
    customize UnixCCompiler
      libraries flame not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    atlas_3_10_threads_info:
    Setting PTATLAS=ATLAS
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries tatlas,tatlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries tatlas,tatlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries tatlas,tatlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&gt;
      NOT AVAILABLE

    atlas_3_10_info:
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries satlas,satlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries satlas,satlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries satlas,satlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_3_10_info'&gt;
      NOT AVAILABLE

    atlas_threads_info:
    Setting PTATLAS=ATLAS
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries ptf77blas,ptcblas,atlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries ptf77blas,ptcblas,atlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_threads_info'&gt;
      NOT AVAILABLE

    atlas_info:
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries f77blas,cblas,atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries f77blas,cblas,atlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries f77blas,cblas,atlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_info'&gt;
      NOT AVAILABLE

    accelerate_info:
      NOT AVAILABLE

    lapack_info:
    customize UnixCCompiler
      libraries lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    lapack_src_info:
      NOT AVAILABLE

      NOT AVAILABLE

    setup.py:460: UserWarning: Unrecognized setuptools command ('dist_info --egg-base /tmp/pip-modern-metadata-ujofw06w'), proceeding with generating Cython sources
and expanding templates
      warnings.warn(&quot;Unrecognized setuptools command ('{}'), proceeding with &quot;
    Running from SciPy source directory.
    /tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/numpy/distutils/system_info.py:1712: UserWarning:
        Lapack (http://www.netlib.org/lapack/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [lapack]) or by setting
        the LAPACK environment variable.
      if getattr(self, '_calc_info_{}'.format(lapack))():
    /tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/numpy/distutils/system_info.py:1712: UserWarning:
        Lapack (http://www.netlib.org/lapack/) sources not found.
        Directories to search for the sources can be specified in the
        numpy/distutils/site.cfg file (section [lapack_src]) or by setting
        the LAPACK_SRC environment variable.
      if getattr(self, '_calc_info_{}'.format(lapack))():
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py&quot;, line 280, in &lt;module&gt;
        main()
      File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py&quot;, line 263, in main
        json_out['return_val'] = hook(**hook_input['kwargs'])
      File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py&quot;, line 133, in prepare_metadata_for_build_wheel
        return hook(metadata_directory, config_settings)
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 157, in prepare_metadata_for_build_wheel
        self.run_setup()
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 248, in run_setup
        super(_BuildMetaLegacyBackend,
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 142, in run_setup
        exec(compile(code, __file__, 'exec'), locals())
      File &quot;setup.py&quot;, line 583, in &lt;module&gt;
        setup_package()
      File &quot;setup.py&quot;, line 579, in setup_package
        setup(**metadata)
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/numpy/distutils/core.py&quot;, line 137, in setup
        config = configuration()
      File &quot;setup.py&quot;, line 477, in configuration
        raise NotFoundError(msg)
    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.
    ----------------------------------------
ERROR: Command errored out with exit status 1: /usr/local/bin/python /usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_bu
ild_wheel /tmp/tmpoyjzx5wb Check the logs for full command output.
The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
</code></pre>
<p><strong>I tried this thread - <a href=""https://stackoverflow.com/questions/44732303/docker-unable-to-install-numpy-scipy-or-gensim"">Docker unable to install numpy, scipy, or gensim</a>
As suggested I added line 4 and 5 in my dockerFile but it is still not working.</strong></p>
","12138506","","400617","","2020-08-10 23:55:45","2020-08-11 21:39:42","Error when building image from requirement.txt in docker","<python><docker><gensim>","2","0","","","","CC BY-SA 4.0"
"35985951","1","","","2016-03-14 11:12:35","","1","1956","<p>I am using Gensim Library in python for using and training word2vector model. Recently, I was looking at initializing my model weights with some pre-trained word2vec model such as (GoogleNewDataset pretrained model). I have been struggling with it couple of weeks. Now, I just searched out that in gesim there is a function that can help me to initialize the weights of my model with pre-trained model weights. That is mentioned below:</p>

<pre><code>reset_from(other_model)

    Borrow shareable pre-built structures (like vocab) from the other_model. Useful if testing multiple models in parallel on the same corpus.
</code></pre>

<p>I don't know this function can do the same thing or not. Please help!!!</p>
","2235817","","","","","2018-02-15 15:48:25","How to initialize a new word2vec model with pre-trained model weights?","<python><gensim><word2vec><word-embedding>","1","0","1","","","CC BY-SA 3.0"
"66372150","1","","","2021-02-25 16:08:14","","0","98","<p>My question concerns how to perform Latent Dirichlet Allocation filtering my dataset with tf-idf weights. The existent libraries provide filters by absolute frequency (min_df/max_dif on sklearn, filter_extrems() on gensim). I performed the following steps:</p>
<ol>
<li>TF-IDF for all terms in a collection of document with sklearn.</li>
<li>Given the document-term-matrix of above, I removed all words that do not satisfy a tf-idf threshold (i.e. tf-idf: 0.1)</li>
<li>Converted the dtm in Compressed Sparse Row matrix</li>
<li>Create a corpus [with gensim.corpora.Dictionary.fromcorpus()] from the csr matrix</li>
</ol>
<p>Unfortunately, the dictionary created on 1) contains all the words from the original non-filtered documents.</p>
<p>How to filter the vocabulary according to the new filtered dtm and keep the indexes consistent? Please note that I have already managed to create a customized dictionary only including relevant words to give to gensim.Lda_MultiCore(), but I cannot use the feature compactify() because it doesn't work on customized dictionary</p>
<p>Thank you in advance!</p>
","15284356","","15284356","","2021-02-25 16:40:19","2021-02-25 16:40:19","filtering dictionary with TF-IDF rather than absolute counts for LDA in Python","<python><dictionary><gensim><tf-idf><noise-reduction>","0","0","","","","CC BY-SA 4.0"
"50326147","1","50326830","","2018-05-14 08:38:13","","1","3795","<p>As I was just experimenting with NLP then I was working on sarcasm detection but in meanwhile I had put this code. </p>

<p><strong>sarcasmextractor.py</strong></p>

<pre><code># coding: utf-8

# Importing the library

# In[2]:

import io
import sys
import os
import numpy as np
import pandas as pd
import nltk
import gensim
import csv, collections
from textblob import TextBlob
from sklearn.utils import shuffle
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
from sklearn.feature_extraction import DictVectorizer
import pickle
import replace_emoji


# Define a class to load the SentimentWordnet and write methods to calculate the scores

# In[4]:

class load_senti_word_net(object):
    """"""
    constructor to load the file and read the file as CSV
    6 columns - pos, ID, PosScore, NegScore, synsetTerms, gloss
    synsetTerms can have multiple similar words like abducting#1 abducent#1 and will read each one and calculaye the scores
    """"""

    def __init__(self):
        sent_scores = collections.defaultdict(list)
        with io.open(""SentiWordNet_3.0.0_20130122.txt"") as fname:
            file_content = csv.reader(fname, delimiter='\t',quotechar='""')

            for line in file_content:                
                if line[0].startswith('#') :
                    continue                    
                pos, ID, PosScore, NegScore, synsetTerms, gloss = line
                for terms in synsetTerms.split("" ""):
                    term = terms.split(""#"")[0]
                    term = term.replace(""-"","""").replace(""_"","""")
                    key = ""%s/%s""%(pos,term.split(""#"")[0])
                    try:
                        sent_scores[key].append((float(PosScore),float(NegScore)))
                    except:
                        sent_scores[key].append((0,0))

        for key, value in sent_scores.items():
            sent_scores[key] = np.mean(value,axis=0)

        self.sent_scores = sent_scores    

    """"""
    For a word,
    nltk.pos_tag([""Suraj""])
    [('Suraj', 'NN')]
    """"""

    def score_word(self, word):
        pos = nltk.pos_tag([word])[0][1]
        return self.score(word, pos)

    def score(self,word, pos):
        """"""
        Identify the type of POS, get the score from the senti_scores and return the score
        """"""

        if pos[0:2] == 'NN':
            pos_type = 'n'
        elif pos[0:2] == 'JJ':
            pos_type = 'a'
        elif pos[0:2] =='VB':
            pos_type='v'
        elif pos[0:2] =='RB':
            pos_type = 'r'
        else:
            pos_type =  0

        if pos_type != 0 :    
            loc = pos_type+'/'+word
            score = self.sent_scores[loc]
            if len(score)&gt;1:
                return score
            else:
                return np.array([0.0,0.0])
        else:
            return np.array([0.0,0.0])

    """"""
    Repeat the same for a sentence
    nltk.pos_tag(word_tokenize(""My name is Suraj""))
    [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Suraj', 'NNP')]    
    """"""    

    def score_sentencce(self, sentence):
        pos = nltk.pos_tag(sentence)
        print (pos)
        mean_score = np.array([0.0, 0.0])
        for i in range(len(pos)):
            mean_score += self.score(pos[i][0], pos[i][1])

        return mean_score

    def pos_vector(self, sentence):
        pos_tag = nltk.pos_tag(sentence)
        vector = np.zeros(4)

        for i in range(0, len(pos_tag)):
            pos = pos_tag[i][1]
            if pos[0:2]=='NN':
                vector[0] += 1
            elif pos[0:2] =='JJ':
                vector[1] += 1
            elif pos[0:2] =='VB':
                vector[2] += 1
            elif pos[0:2] == 'RB':
                vector[3] += 1

        return vector



# Now let's extract the features
# 
# ###Stemming and Lemmatization

# In[5]:

porter = nltk.PorterStemmer()
sentiments = load_senti_word_net()


# In[7]:

def gram_features(features,sentence):
    sentence_rep = replace_emoji.replace_reg(str(sentence))
    token = nltk.word_tokenize(sentence_rep)
    token = [porter.stem(i.lower()) for i in token]        

    bigrams = nltk.bigrams(token)
    bigrams = [tup[0] + ' ' + tup[1] for tup in bigrams]
    grams = token + bigrams
    #print (grams)
    for t in grams:
        features['contains(%s)'%t]=1.0



# In[8]:

import string
def sentiment_extract(features, sentence):
    sentence_rep = replace_emoji.replace_reg(sentence)
    token = nltk.word_tokenize(sentence_rep)    
    token = [porter.stem(i.lower()) for i in token]   
    mean_sentiment = sentiments.score_sentencce(token)
    features[""Positive Sentiment""] = mean_sentiment[0]
    features[""Negative Sentiment""] = mean_sentiment[1]
    features[""sentiment""] = mean_sentiment[0] - mean_sentiment[1]
    #print(mean_sentiment[0], mean_sentiment[1])

    try:
        text = TextBlob("" "".join([""""+i if i not in string.punctuation and not i.startswith(""'"") else i for i in token]).strip())
        features[""Blob Polarity""] = text.sentiment.polarity
        features[""Blob Subjectivity""] = text.sentiment.subjectivity
        #print (text.sentiment.polarity,text.sentiment.subjectivity )
    except:
        features[""Blob Polarity""] = 0
        features[""Blob Subjectivity""] = 0
        print(""do nothing"")


    first_half = token[0:int(len(token)/2)]    
    mean_sentiment_half = sentiments.score_sentencce(first_half)
    features[""positive Sentiment first half""] = mean_sentiment_half[0]
    features[""negative Sentiment first half""] = mean_sentiment_half[1]
    features[""first half sentiment""] = mean_sentiment_half[0]-mean_sentiment_half[1]
    try:
        text = TextBlob("" "".join([""""+i if i not in string.punctuation and not i.startswith(""'"") else i for i in first_half]).strip())
        features[""first half Blob Polarity""] = text.sentiment.polarity
        features[""first half Blob Subjectivity""] = text.sentiment.subjectivity
        #print (text.sentiment.polarity,text.sentiment.subjectivity )
    except:
        features[""first Blob Polarity""] = 0
        features[""first Blob Subjectivity""] = 0
        print(""do nothing"")

    second_half = token[int(len(token)/2):]
    mean_sentiment_sechalf = sentiments.score_sentencce(second_half)
    features[""positive Sentiment second half""] = mean_sentiment_sechalf[0]
    features[""negative Sentiment second half""] = mean_sentiment_sechalf[1]
    features[""second half sentiment""] = mean_sentiment_sechalf[0]-mean_sentiment_sechalf[1]
    try:
        text = TextBlob("" "".join([""""+i if i not in string.punctuation and not i.startswith(""'"") else i for i in second_half]).strip())
        features[""second half Blob Polarity""] = text.sentiment.polarity
        features[""second half Blob Subjectivity""] = text.sentiment.subjectivity
        #print (text.sentiment.polarity,text.sentiment.subjectivity )
    except:
        features[""second Blob Polarity""] = 0
        features[""second Blob Subjectivity""] = 0
        print(""do nothing"")  





# In[9]:

features = {}
sentiment_extract(features,""a long narrow opening"")


# In[11]:

def pos_features(features,sentence):
    sentence_rep = replace_emoji.replace_reg(sentence)
    token = nltk.word_tokenize(sentence_rep)
    token = [ porter.stem(each.lower()) for each in token]
    pos_vector = sentiments.pos_vector(token)
    for j in range(len(pos_vector)):
        features['POS_'+str(j+1)] = pos_vector[j]
    print (""done"")



# In[12]:

features = {}
pos_features(features,""a long narrow opening"")


# In[13]:

def capitalization(features,sentence):
    count = 0
    for i in range(len(sentence)):
        count += int(sentence[i].isupper())
    features['Capitalization'] = int(count &gt; 3)
    print (count)


# In[14]:

features = {}
capitalization(features,""A LoNg NArrow opening"")


# In[15]:

import topic
topic_mod = topic.topic(nbtopic=200,alpha='symmetric')


# In[16]:

topic_mod = topic.topic(model=os.path.join('topics.tp'),dicttp=os.path.join('topics_dict.tp'))


# In[17]:

def topic_feature(features,sentence,topic_modeler):    
    topics = topic_modeler.transform(sentence)    
    for j in range(len(topics)):
        features['Topic :'] = topics[j][1]



# In[18]:

topic_feature(features,""A LoNg NArrow opening"",topic_mod)


# In[19]:

def get_features(sentence, topic_modeler):
    features = {}
    gram_features(features,sentence)
    pos_features(features,sentence)
    sentiment_extract(features, sentence)
    capitalization(features,sentence)
    topic_feature(features, sentence,topic_modeler)
    return features


# In[20]:

df = pd.DataFrame()
df = pd.read_csv(""dataset_csv.csv"", header=0, sep='\t')
df.head()


# In[17]:

import re

for i in range(0,df.size):
    temp = str(df[""tweets""][i])
    temp = re.sub(r'[^\x00-\x7F]+','',temp)
    featureset.append((get_features(temp,topic_mod), df[""label""][i]))


# In[20]:

c = []
for i in range(0,len(featureset)):
    c.append(pd.DataFrame(featureset[i][0],index=[i]))

result = pd.concat(c)


# In[22]:

result.insert(loc=0,column=""label"",value='0')


# In[23]:

for i in range(0, len(featureset)):
    result[""label""].loc[i] = featureset[i][1]   



# In[25]:

result.to_csv('feature_dataset.csv')


# In[3]:

df = pd.DataFrame()
df = pd.read_csv(""feature_dataset.csv"", header=0)
df.head()


# In[4]:

get_ipython().magic('matplotlib inline')

import matplotlib as matplot 
import seaborn

result = df


# In[5]:

X = result.drop(['label','Unnamed: 0','Topic :'],axis=1).values


# In[6]:

Y = result['label']


# In[7]:

import pickle
import pefile
import sklearn.ensemble as ek
from sklearn import cross_validation, tree, linear_model
from sklearn.feature_selection import SelectFromModel
from sklearn.externals import joblib
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn import preprocessing
from sklearn import svm
from sklearn.linear_model import LinearRegression
import sklearn.linear_model as lm


# In[29]:

model = { ""DecisionTree"":tree.DecisionTreeClassifier(max_depth=10),
         ""RandomForest"":ek.RandomForestClassifier(n_estimators=50),
         ""Adaboost"":ek.AdaBoostClassifier(n_estimators=50),
         ""GradientBoosting"":ek.GradientBoostingClassifier(n_estimators=50),
         ""GNB"":GaussianNB(),
         ""Logistic Regression"":LinearRegression()   
}


# In[8]:

X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, Y ,test_size=0.2)


# In[9]:

X_train = pd.DataFrame(X_train)
X_train = X_train.fillna(X_train.mean())

X_test = pd.DataFrame(X_test)
X_test = X_test.fillna(X_test.mean())


# In[38]:

results_algo = {}
for algo in model:
    clf = model[algo]
    clf.fit(X_train,y_train.astype(int))
    score = clf.score(X_test,y_test.astype(int))
    print (""%s : %s "" %(algo, score))
    results_algo[algo] = score



# In[39]:

winner = max(results_algo, key=results_algo.get)


# In[40]:

clf = model[winner]
res = clf.predict(X_test)
mt = confusion_matrix(y_test, res)
print(""False positive rate : %f %%"" % ((mt[0][1] / float(sum(mt[0])))*100))
print('False negative rate : %f %%' % ( (mt[1][0] / float(sum(mt[1]))*100)))


# In[41]:

from sklearn import metrics
print (metrics.classification_report(y_test, res))


# In[34]:

test_data = ""public meetings are awkard for me as I can insult people but I choose not to and that is something that I find difficult to live with""


# In[101]:

test_data=""I purchased this product 4.47 billion years ago and when I opened it today, it was half empty.""


# In[82]:

test_data=""when people see me eating and ask me are you eating? No no I'm trying to choke myself to death #sarcastic""


# In[102]:

test_feature = []
test_feature.append((get_features(test_data,topic_mod)))


# In[104]:

test_feature


# In[105]:

c = []

c.append(pd.DataFrame(test_feature[0],index=[i]))

test_result = pd.concat(c)
test_result = test_result.drop(['Topic :'],axis=1).values


# In[106]:

res= clf.predict(test_result)
</code></pre>

<p>But it is giving me the following error:</p>

<pre><code>C:\ProgramData\Anaconda3\lib\site-packages\gensim\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
[('a', 'DT'), ('long', 'JJ'), ('narrow', 'JJ'), ('open', 'JJ')]
[('a', 'DT'), ('long', 'JJ')]
[('narrow', 'JJ'), ('open', 'JJ')]
done
5
Traceback (most recent call last):
  File ""C:\shubhamprojectwork\sarcasm detection\SarcasmDetection-master\SarcasmDetection-master\Code\sarcasm-extraction.py"", line 276, in &lt;module&gt;
    topic_feature(features,""A LoNg NArrow opening"",topic_mod)
  File ""C:\shubhamprojectwork\sarcasm detection\SarcasmDetection-master\SarcasmDetection-master\Code\sarcasm-extraction.py"", line 268, in topic_feature
    topics = topic_modeler.transform(sentence)    
  File ""C:\shubhamprojectwork\sarcasm detection\SarcasmDetection-master\SarcasmDetection-master\Code\topic.py"", line 42, in transform
    return self.lda[corpus_sentence]     
  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\ldamodel.py"", line 1160, in __getitem__
    return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
AttributeError: 'LdaModel' object has no attribute 'minimum_phi_value'
</code></pre>

<p>Code for <strong>topic.py</strong>:</p>

<pre><code>from gensim import corpora, models, similarities
import nltk
from nltk.corpus import stopwords
import numpy as np
import pandas as pd
import replace_emoji

class topic(object):
    def __init__(self, nbtopic = 100, alpha=1,model=None,dicttp=None):
        self.nbtopic = nbtopic
        self.alpha = alpha
        self.porter = nltk.PorterStemmer()
        self.stop = stopwords.words('english')+['.','!','?','""','...','\\',""''"",'[',']','~',""'m"",""'s"",';',':','..','$']
        if model!=None and dicttp!=None:
            self.lda = models.ldamodel.LdaModel.load(model)
            self.dictionary =  corpora.Dictionary.load(dicttp)

    def fit(self,documents):

        documents_mod = documents
        tokens = [nltk.word_tokenize(sentence) for sentence in documents_mod]
        tokens = [[self.porter.stem(t.lower()) for t in sentence if t.lower() not in self.stop] for sentence in tokens]        

        self.dictionary = corpora.Dictionary(tokens)
        corpus = [self.dictionary.doc2bow(text) for text in tokens]
        self.lda = models.ldamodel.LdaModel(corpus,id2word=self.dictionary, num_topics=self.nbtopic,alpha=self.alpha)

        self.lda.save('topics.tp')
        self.dictionary.save('topics_dict.tp')

    def get_topic(self,topic_number):

        return self.lda.print_topic(topic_number)

    def transform(self,sentence):

        sentence_mod = sentence
        tokens = nltk.word_tokenize(sentence_mod)
        tokens = [self.porter.stem(t.lower()) for t in tokens if t.lower() not in self.stop] 
        corpus_sentence = self.dictionary.doc2bow(tokens)

        return self.lda[corpus_sentence]     
</code></pre>

<p>The overall code is found here <a href=""https://github.com/surajr/SarcasmDetection"" rel=""nofollow noreferrer"">overall code</a>.</p>
","4786793","","712995","","2018-05-14 13:38:07","2018-05-14 13:38:07","AttributeError: 'LdaModel' object has no attribute 'minimum_phi_value'","<python><tensorflow><nlp><gensim><topic-modeling>","1","2","","","","CC BY-SA 4.0"
"46960119","1","","","2017-10-26 17:11:55","","0","488","<p>I want to load pre-trained word embeddings from google news</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print (model.wv.vocab)
</code></pre>

<p>But the error is showing:</p>

<pre><code>UnicodeEncodeError: 'ascii' codec can't encode character '\u2022' in position 62425: ordinal not in range(128)
</code></pre>

<p>How do I fix this? as I want to list all the words in the word embeddings and do the average for the sentence embedding.</p>
","4921197","","","","","2018-02-01 00:19:28","load pre-trained word embeddings","<python><encoding><word2vec><gensim>","1","2","","","","CC BY-SA 3.0"
"29372611","1","","","2015-03-31 15:36:52","","1","541","<p>I have the following code, to run an LDA analysis on Tweets:</p>

<pre><code>import logging, gensim, bz2
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# load id-&gt;word mapping (the dictionary), one of the results of step 2 above
id2word = 'enams4nieuw.dict'
# load corpus iterator
mm = gensim.corpora.MmCorpus('enams4nieuw.mm')

print(mm)

# extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents)
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)
</code></pre>

<p>When I try to run this script, I receive the following log with error message:</p>

<pre><code>MmCorpus(40152 documents, 13061 features, 384671 non-zero entries)
2015-03-31 16:52:50,246 : INFO : loaded corpus index from enams4nieuw.mm.index
2015-03-31 16:52:50,246 : INFO : initializing corpus reader from enams4nieuw.mm
2015-03-31 16:52:50,246 : INFO : accepted corpus with 40152 documents, 13061 features, 384671 non-zero entries
Traceback (most recent call last):
  File ""C:/Users/gerbuiker/PycharmProjects/twitter-streaming.py/lda.py"", line 15, in &lt;module&gt;
    lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)
  File ""C:\Users\gerbuiker\AppData\Roaming\Python\Python27\site-packages\gensim\models\ldamodel.py"", line 244, in __init__
self.num_terms = 1 + max(self.id2word.keys())
AttributeError: 'str' object has no attribute 'keys'

Process finished with exit code 1
</code></pre>

<p>Anyone got a solution for this?</p>
","4555682","","","","","2017-04-21 12:29:17","Error when running LDA on Tweets using gensim in Python","<python><lda><gensim>","2","0","","","","CC BY-SA 3.0"
"35985851","1","35986123","","2016-03-14 11:07:17","","0","6270","<p>I installed <a href=""http://radimrehurek.com/gensim"" rel=""nofollow"">gensim</a>, Python library.
I executed the command </p>

<pre><code>Import gensim
</code></pre>

<p>It executed without any error. 
Then I tried to import test from gensim using the command </p>

<pre><code>from gensim import test
</code></pre>

<p>and it showed the following error</p>

<blockquote>
  <p>Traceback (most recent call last):
    File """", line 1, in 
      from gensim import test
  ImportError: cannot import name 'test'</p>
</blockquote>

<p>Python site-packages had gensim folder in that. 
Any help would be highly appreciated. </p>
","5625604","","202229","","2020-01-30 00:04:19","2020-01-30 00:04:19","'from gensim import test' is not importing successfully","<python><python-3.4><gensim>","2","3","0","","","CC BY-SA 4.0"
"53616003","1","53620873","","2018-12-04 15:15:05","","0","588","<p>I train my doc2vec model:</p>

<pre><code>data = [""Sentence 1"",
        ""Sentence 2"",
        ""Sentence 3"",
        ""Sentence 4""]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags[str(i)]) 
                              for i, _d in enumerate(data)]
</code></pre>

<p>training part:</p>

<pre><code>model = Doc2Vec(size=100, window=10, min_count=1, workers=11, alpha=0.025, 
                min_alpha=0.025, iter=20)

model.build_vocab(tagged_data, update=False)

model.train(tagged_data,epochs=model.iter,total_examples=model.corpus_count)
</code></pre>

<p>Save model:</p>

<pre><code>model.save(""d2v.model"")
</code></pre>

<p>And it's work. Than I want to add some sentence to my vocabulary and model. E.x.:</p>

<pre><code>new_data = [""Sentence 5"",
            ""Sentence 6"",
            ""Sentence 7""]
new_tagged_data= 
[TaggedDocument(words=word_tokenize(_d.lower()),tags[str(i+len(data))]) 
                for i,_d in enumerate(new_data)]
</code></pre>

<p>And than update model:</p>

<pre><code>model.build_vocab(new_tagged_data, update=True)

model.train(new_tagged_data, 
            epochs=model.iter,total_examples=model.corpus_count)
</code></pre>

<p>But it doesn't work. Jupiter urgently shut down and no answer. I use the same way with word2vec model and it works!</p>

<p>What can be a problem with this?</p>
","10208662","","","","","2018-12-04 20:25:41","Doc2Vec online training","<python><python-3.x><nlp><gensim><doc2vec>","1","1","1","","","CC BY-SA 4.0"
"38200241","1","","","2016-07-05 09:50:01","","1","208","<p>I already have a trained doc2vec model.</p>

<pre><code>model = Doc2Vec(size = 100, window = 10, min_count = 1, workers=4, hashfxn=hash32)
model.build_vocab(doc)
model.train(doc)
model.save(r'C:\Data\Model\hs8000_2016Q2.doc2vec') 
</code></pre>

<p>model only has n_similarity or most_similar as function to check similarity between sentences. However, I want to know top 50 Topics in the model. how do I extract Topics from the model?</p>

<p>Thank you!</p>
","4345535","","4345535","","2016-07-05 10:23:03","2016-07-05 10:23:03","Gensim: how to extract Topics from a trained Doc2Vec model in Gensim?","<python-3.x><gensim>","0","0","","","","CC BY-SA 3.0"
"55492888","1","","","2019-04-03 10:25:59","","0","253","<p>I am working on a document similarity problem. For each document, I retrieve the vectors for each of its words (from a pre-trained word embedding model) and average them to get the document vector. I end up having a dictionary (say, my_dict) that maps each document in my collection to its vector. </p>

<p>I want to feed this dictionary to gensim and for each document, get other documents in 'my_dict' that are closer to it. How could I do that?</p>
","530399","","","","","2019-04-12 14:26:42","How to get similar words from a custom input dictionary of word to vectors in gensim","<python><gensim><cosine-similarity>","1","0","","","","CC BY-SA 4.0"
"36360367","1","36415208","","2016-04-01 15:39:02","","0","678","<p>I am new to Doc2vec use. In case I could get some advice before I start on it, it will save a LOT of time.
My data is an stream of text data (such as tweets) continuously coming in time. For clustering these tweets, I was thinking of using doc2vec to reduce the text content into a fixed size vector and use that to compare between documents. 
So in this case, the text data is getting accumulated over time, can this be still used with Doc2Vec, I may have to learn the model again and again (may be!) or could I use some large corpus such as Wikipedia or a large newscorpus to train the Doc2Vec model.</p>

<p>Any suggestions will help!</p>

<p>Thanks in Advance.</p>
","386384","","","","","2016-04-05 00:37:56","Can doc2vec be used if my text data is incrementally increasing?","<twitter><text><gensim><word2vec>","1","0","1","","","CC BY-SA 3.0"
"53473868","1","","","2018-11-26 01:58:55","","1","478","<p>I have a time series dataset. therefore, for each time period I trained a word2vec model and realigned the models.</p>

<p>However, when I try to load the aligned word2vec models as follows I get the below mentioned error.</p>

<pre><code>#Load model
model = word2vec.Word2Vec.load('model_1970')
</code></pre>

<p>Error:</p>

<pre><code>train_words_pow += wv.vocab[wv.index2word[word_index]].count**power
KeyError: 'ctrx'
</code></pre>

<p>Is there a way to resolve this error? :)</p>

<p>I have attached a sample trained word2vec model that gives error for testing purposes</p>

<p>Link: <a href=""https://drive.google.com/file/d/1IBbUgeAubr2xzNYLKZgPt34xOEsW92bO/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1IBbUgeAubr2xzNYLKZgPt34xOEsW92bO/view?usp=sharing</a></p>

<p>EDIT:
Mention below is the log of my program.</p>

<pre><code>2018-11-30 14:23:43,897 : INFO : loading Word2Vec object from model_1970
2018-11-30 14:23:43,961 : INFO : loading wv recursively from model_1970.wv.* with mmap=None
2018-11-30 14:23:43,965 : INFO : loading vectors from model_1970.wv.vectors.npy with mmap=None
2018-11-30 14:23:44,005 : INFO : setting ignored attribute vectors_norm to None
2018-11-30 14:23:44,009 : INFO : loading vocabulary recursively from model_1970.vocabulary.* with mmap=None
2018-11-30 14:23:44,009 : INFO : loading trainables recursively from model_1970.trainables.* with mmap=None
2018-11-30 14:23:44,009 : INFO : loading syn1neg from model_1970.trainables.syn1neg.npy with mmap=None
2018-11-30 14:23:44,053 : INFO : setting ignored attribute cum_table to None
2018-11-30 14:23:44,053 : INFO : loaded model_1970
Reloaded modules: __mp_main__
Traceback (most recent call last):

  File ""&lt;ipython-input-3-3b9230dacba9&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/Emi/Desktop/code/word2vec_distance.py', wdir='C:/Users/Emi/Desktop/code')

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 668, in runfile
    execfile(filename, namespace)

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 108, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/Emi/Desktop/code/word2vec_distance.py"", line 26, in &lt;module&gt;
    model_1 = word2vec.Word2Vec.load(word2vec_model_name_1)

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 975, in load
    return super(Word2Vec, cls).load(*args, **kwargs)

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py"", line 631, in load
    model.vocabulary.make_cum_table(model.wv)  # rebuild cum_table from vocabulary

  File ""C:\Users\Emi\Anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 1383, in make_cum_table
    train_words_pow += wv.vocab[wv.index2word[word_index]].count**power

KeyError: 'cmnx'
</code></pre>
","10704050","","10704050","","2018-11-30 04:00:39","2018-11-30 04:00:39","How to load an aligned word2vec model in Gensim?","<python><nlp><gensim><word2vec>","0","11","","","","CC BY-SA 4.0"
"62454568","1","","","2020-06-18 16:16:19","","1","52","<p>I need to train a Word2Vec model, which I have done, and then I need to use it to calculate the likelihood of previously unseen data. I'm stuck on how to do this though. I've seen WMD but that's for calculating similarity between 2 sentences, while I'm trying to calculate the likelihood of text with my model.</p>
","13709045","","","","","2020-06-18 16:16:19","How to calculate the likelihood of a sentence using a Word2Vec model?","<python><nlp><gensim>","0","1","","","","CC BY-SA 4.0"
"49380138","1","49380775","","2018-03-20 09:11:53","","4","1938","<p>I made a word embedding with this code:</p>

<pre><code>with open(""text.txt"",'r') as longFile:
        sentences = []
        single= []
        for line in longFile:
            for word in line.split("" ""):
                single.append(word)
            sentences.append(single)
    model = Word2Vec(sentences,workers=4, window=5)
</code></pre>

<p>I want now to calculate the similarity between two word and see what are the neighbours of them.
What is the difference between <code>model[""word""]</code>,<code>model.wv.most_similar()</code>, <code>model.similar_by_vector()</code> and <code>model.similarity()</code>?
Which one should I use?</p>
","8889685","","1323398","","2018-05-30 10:51:19","2018-05-30 10:51:19","Word2Vec Python similarity","<python><similarity><word2vec><gensim><word-embedding>","1","0","","","","CC BY-SA 3.0"
"44944249","1","44944585","","2017-07-06 08:58:52","","0","5003","<p>I'm trying to install <code>pyemd</code> package in Python through <code>pip</code> and getting following error:</p>

<pre><code>C:\Users\dipanwita.neogy&gt;pip install pyemd
Collecting pyemd
  Using cached pyemd-0.4.3.tar.gz
Requirement already satisfied: numpy&lt;2.0.0,&gt;=1.9.0 in c:\users\dipanwita.neogy\a
naconda3\lib\site-packages (from pyemd)
Building wheels for collected packages: pyemd
  Running setup.py bdist_wheel for pyemd ... error
  Complete output from command C:\Users\dipanwita.neogy\Anaconda3\python.exe -u
-c ""import setuptools, tokenize;__file__='C:\\Users\\DIPANW~1.NEO\\AppData\\Loca
l\\Temp\\pip-build-nk13uh5b\\pyemd\\setup.py';f=getattr(tokenize, 'open', open)(
__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __fil
e__, 'exec'))"" bdist_wheel -d C:\Users\DIPANW~1.NEO\AppData\Local\Temp\tmpngn2np
rmpip-wheel- --python-tag cp36:
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win32-3.6
  creating build\lib.win32-3.6\pyemd
  copying pyemd\__about__.py -&gt; build\lib.win32-3.6\pyemd
  copying pyemd\__init__.py -&gt; build\lib.win32-3.6\pyemd
  running build_ext
  building 'pyemd.emd' extension
  error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual C+
+ Build Tools"": http://landinghub.visualstudio.com/visual-cpp-build-tools

  ----------------------------------------
  Failed building wheel for pyemd
  Running setup.py clean for pyemd
Failed to build pyemd
Installing collected packages: pyemd
  Running setup.py install for pyemd ... error
    Complete output from command C:\Users\dipanwita.neogy\Anaconda3\python.exe -
u -c ""import setuptools, tokenize;__file__='C:\\Users\\DIPANW~1.NEO\\AppData\\Lo
cal\\Temp\\pip-build-nk13uh5b\\pyemd\\setup.py';f=getattr(tokenize, 'open', open
)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __f
ile__, 'exec'))"" install --record C:\Users\DIPANW~1.NEO\AppData\Local\Temp\pip-e
rihhtfj-record\install-record.txt --single-version-externally-managed --compile:

    running install
    running build
    running build_py
    creating build
    creating build\lib.win32-3.6
    creating build\lib.win32-3.6\pyemd
    copying pyemd\__about__.py -&gt; build\lib.win32-3.6\pyemd
    copying pyemd\__init__.py -&gt; build\lib.win32-3.6\pyemd
    running build_ext
    building 'pyemd.emd' extension
    error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual
C++ Build Tools"": http://landinghub.visualstudio.com/visual-cpp-build-tools

    ----------------------------------------
Command ""C:\Users\dipanwita.neogy\Anaconda3\python.exe -u -c ""import setuptools,
 tokenize;__file__='C:\\Users\\DIPANW~1.NEO\\AppData\\Local\\Temp\\pip-build-nk1
3uh5b\\pyemd\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read(
).replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install
 --record C:\Users\DIPANW~1.NEO\AppData\Local\Temp\pip-erihhtfj-record\install-r
ecord.txt --single-version-externally-managed --compile"" failed with error code
1 in C:\Users\DIPANW~1.NEO\AppData\Local\Temp\pip-build-nk13uh5b\pyemd\
</code></pre>

<p>I cannot find anything regarding this error. Please suggest me what should I do? </p>
","8214842","","","","","2020-03-03 05:08:50","pip install pyemd error?","<python><nlp><pip><gensim>","2","0","","","","CC BY-SA 3.0"
"45444964","1","45453040","","2017-08-01 18:12:40","","7","12418","<p>I have been struggling to understand the use of <code>size</code> parameter in the <code>gensim.models.Word2Vec</code></p>

<p>From the Gensim documentation, <code>size</code> is the dimensionality of the vector. Now, as far as my knowledge goes, word2vec creates a vector of the probability of closeness with the other words in the sentence for each word. So, suppose if my <code>vocab</code> size is 30 then how does it create a vector with the dimension greater than 30? Can anyone please brief me on the optimal value of <code>Word2Vec</code> size? </p>

<p>Thank you.</p>
","6395618","","","","","2020-12-23 23:18:51","Python: What is the ""size"" parameter in Gensim Word2vec model class","<python><gensim><word2vec>","2","2","2","","","CC BY-SA 3.0"
"13913142","1","","","2012-12-17 11:20:56","","1","3591","<p>I have trained LDA model using gensim on a text_corpus.</p>

<pre><code>&gt;lda_model = gensim.models.ldamodel.LdaModel(text_corpus, 10)
</code></pre>

<p>Now if a new text document text_sparse_vector has to be inferred I have to do </p>

<pre><code>&gt;lda_model[text_sparse_vector]
[(0, 0.036479568280206563), (3, 0.053828073308160099), (7, 0.021936618544365804), (11, 0.017499953446152686), (15, 0.010153090454090822), (16, 0.35967516223499041), (19, 0.098570351997275749), (26, 0.068550060242800928), (27, 0.08371562828754453), (28, 0.14110945630261607), (29, 0.089938130046832571)]
</code></pre>

<p>But how do I get the word distribution for each of the corresponding topics. For example, How do I know top 20 words for topic number 16 ?</p>

<p>The class gensim.models.ldamodel.LdaModel has method called show_topics(topics=10, topn=10, log=False, formatted=True), but the as the documentation says it shows randomly selected list of topics. </p>

<p>Is there a way to link or print I can map the inferred topic numbers to word distributions ?</p>
","933401","","","","","2019-04-01 16:35:17","How do I get topic numbers in LDA model in gensim","<python><nlp><lda><gensim>","3","1","2","","","CC BY-SA 3.0"
"50328915","1","50442918","","2018-05-14 11:13:01","","0","426","<p>I wanna use the LDA in gensim for topic modeling over a few thousand documents.
Therefore I¬¥m using a csv-File as Input in the format of a term-document-matrix.</p>

<p>Currently it occurs an error when running the following code:</p>

<pre><code>from gensim import corpora

import_path =""TDM.csv""

dictionary = corpora.csvcorpus(import_path, labels='true')
</code></pre>

<p>The error is the following:</p>

<pre><code>dictionary = corpora.csvcorpus(import_path, labels='true')

AttributeError: module 'gensim.corpora' has no attribute 'csvcorpus'
</code></pre>

<p>Am I using the module correctly and if so, where is my mistake?</p>

<p>Thanks in advance.</p>
","9751594","","","","","2018-05-21 06:07:51","CSV Input in gensim LDA via corpora.csvcorpus","<python-3.x><csv><gensim><lda><corpus>","1","1","2","","","CC BY-SA 4.0"
"29369317","1","29369606","","2015-03-31 13:01:20","","0","405","<p>I've downloaded Tweets about Amsterdam, in UTF-8 using the Twitter API for python.
Now i'm trying to make a dictionary for LDA, using this code (just a part of the code, but this is the part that causes the error):</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file))
</code></pre>

<p>which always gives me an error, depending on which txt file I choose as input, either: </p>

<pre><code>UnicodeDecodeError: 'utf8' codec can't decode byte 0xf1 in position 2: invalid continuation byte
</code></pre>

<p>or</p>

<pre><code> UnicodeDecodeError: 'utf8' codec can't decode byte xxxx in position 175-176: unexpected end of data
</code></pre>

<p>I expect the reason for this to be characters which are unknown in UTF-8 (some smilies used in Tweets maybe) and after Googling tried to replace the code by:</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, errors='ignore'))
</code></pre>

<p>with error message:</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, errors='ignore'))
TypeError: 'errors' is an invalid keyword argument for this function
</code></pre>

<p>or </p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, 'ignore'))
</code></pre>

<p>with error message:</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, 'ignore'))
ValueError: mode string must begin with one of 'r', 'w', 'a' or 'U', not 'ignore'
</code></pre>

<p>Does anyone have a solution? Thanks</p>
","4555682","","","","","2015-03-31 13:15:27","Tweet analysis, Python error when making dictionary for LDA","<python><dictionary><lda><gensim>","3","0","","","","CC BY-SA 3.0"
"57728181","1","57729709","","2019-08-30 13:59:02","","0","142","<p>I am new for word2vec and I have trained a text file via word2vec for feature extraction than when I look at the words that are trained I found that it is single characters instead of words, what did I miss here? anyone help</p>

<p>I try to feed tokens instead of the raw text into the models</p>

<pre><code>import nltk

from pathlib import Path
data_folder = Path("""")
file_to_open = data_folder / ""test.txt""
#read the file
file = open(file_to_open , ""rt"")
raw_text = file.read()
file.close()

#tokenization
token_list = nltk.word_tokenize(raw_text)

#Remove Punctuation
from nltk.tokenize import punkt
token_list2 = list(filter(lambda token : punkt.PunktToken(token).is_non_punct,token_list))
#upper to lower case
token_list3 = [word.lower() for word in token_list2]
#remove stopwords
from nltk.corpus import stopwords
token_list4 = list(filter(lambda token: token not in stopwords.words(""english""),token_list3))

#lemmatization
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
token_list5 = [lemmatizer.lemmatize(word) for word in token_list4]
print(""Final Tokens are :"")
print(token_list5,""\n"")
print(""Total tokens : "", len(token_list5))

#word Embedding
from gensim.models import Word2Vec
# train model
model = Word2Vec(token_list5, min_count=2)
# summarize the loaded model

    print(""The model is :"")
    print(model,""\n"")`enter code here`

# summarize vocabulary

    words = list(model.wv`enter code here`.vocab)
    print(""The learned vocabulary words are : \n"",words)

Output- ['p', 'o', 't', 'e', 'n', 'i', 'a', 'l', 'r', 'b', 'u', 'm', 'h', 'd', 'c', 's', 'g', 'q', 'f', 'w', '-']
Expected -[ 'potenial', 'xyz','etc']
</code></pre>
","11883451","","","","","2019-08-30 15:40:08","I get 'single' characters as learned vocabulary on word2vec genism as an output","<nlp><gensim><word2vec><feature-extraction><text-classification>","1","0","","","","CC BY-SA 4.0"
"27533977","1","27535351","","2014-12-17 20:09:12","","1","267","<p>For research purposes, I want a large (~100K) set of web pages, though I am only interested in their text. I plan to use them for gensim LDA topic model. CommonCrawler seems like a good place to start, but I am not sure how to do it.
Could someone point the way how to download 100K text files or how to access them (if it's easier than downloading them)?</p>
","3118093","","","","","2014-12-17 21:42:53","How to download subset of Amazon CommonCrawel (only the text (WET files?) is needed)","<download><lda><gensim><common-crawl>","1","0","0","","","CC BY-SA 3.0"
"55755962","1","55761147","","2019-04-19 02:46:54","","1","469","<p>I am trying to build a Fake news classifier and I am quite new in this field. I have a column ""title_1_en"" which has the title for fake news and another column called ""title_2_en"". There are 3 target labels; ""agreed"", ""disagreed"", and ""unrelated"" if the title of the news in column ""title_2_en"" agrees, disagrees or is unrelated to that in the first column. </p>

<p>I have tried calculating basic cosine similarity between the two titles after converting the words of the sentences into vectors. This has resulted in the the cosine similarity score but this needs a lot of improvement as synonyms and semantic relationship has not been considered at all. </p>

<pre><code>def L2(vector):
    norm_value = np.linalg.norm(vector)
    return norm_value

def Cosine(fr1, fr2):
    cos = np.dot(fr1, fr2)/(L2(fr1)*L2(fr2))
    return cos
</code></pre>
","10158279","","","","","2019-04-19 11:29:36","How to train a model that will result in the similarity score between two news titles?","<nlp><classification><gensim><cosine-similarity><sentence-similarity>","1","0","","","","CC BY-SA 4.0"
"49388929","1","49402606","","2018-03-20 16:02:47","","3","842","<p>I am new to stackoverflow and python so please bear with me.
I am trying to run an Latent Dirichlet Analysis on a text corpora with the gensim package in python using PyCharm editor. I prepared the corpora in R and exported it to a csv file using this R command:</p>
<pre><code>write.csv(testdf, &quot;C://...//test.csv&quot;, fileEncoding = &quot;utf-8&quot;) 
</code></pre>
<p>Which creates the following csv structure (though with much longer and already preprocessed texts):</p>
<pre><code>,&quot;datetimestamp&quot;,&quot;id&quot;,&quot;origin&quot;,&quot;text&quot;
1,&quot;1960-01-01&quot;,&quot;id_1&quot;,&quot;Newspaper1&quot;,&quot;Test text one&quot;
2,&quot;1960-01-02&quot;,&quot;id_2&quot;,&quot;Newspaper1&quot;,&quot;Another text&quot;
3,&quot;1960-01-03&quot;,&quot;id_3&quot;,&quot;Newspaper1&quot;,&quot;Yet another text&quot;
4,&quot;1960-01-04&quot;,&quot;id_4&quot;,&quot;Newspaper2&quot;,&quot;Four Five Six&quot;
5,&quot;1960-01-05&quot;,&quot;id_5&quot;,&quot;Newspaper2&quot;,&quot;Alpha Bravo Charly&quot;
6,&quot;1960-01-06&quot;,&quot;id_6&quot;,&quot;Newspaper2&quot;,&quot;Singing Dancing Laughing&quot;
</code></pre>
<p>I then try the following essential python code (based on the <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow noreferrer"">gensim tutorials</a>) to perform simple LDA analysis:</p>
<pre><code>import gensim
from gensim import corpora, models, similarities, parsing
import pandas as pd
from six import iteritems
import os
import pyLDAvis.gensim

class MyCorpus(object):
     def __iter__(self):
             for row in pd.read_csv('//mpifg.local/dfs/home/lu/Meine Daten/Imagined Futures and Greek State Bonds/Topic Modelling/Python/test.csv', index_col=False, header = 0 ,encoding='utf-8')['text']:
                 # assume there's one document per line, tokens separated by whitespace
                 yield dictionary.doc2bow(row.split())

if __name__ == '__main__':
    dictionary = corpora.Dictionary(row.split() for row in pd.read_csv(
        '//.../test.csv', index_col=False, encoding='utf-8')['text'])
    print(dictionary)
    dictionary.save(
        '//.../greekdict.dict')  # store the dictionary, for future reference

    ## create an mmCorpus
    corpora.MmCorpus.serialize('//.../greekcorpus.mm', MyCorpus())
    corpus = corpora.MmCorpus('//.../greekcorpus.mm')

    dictionary = corpora.Dictionary.load('//.../greekdict.dict')
    corpus = corpora.MmCorpus('//.../greekcorpus.mm')

    # train model
    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=50, iterations=1000)
</code></pre>
<p>I get the following error codes and the code exits:</p>
<blockquote>
<p>...\Python\venv\lib\site-packages\setuptools-28.8.0-py3.6.egg\pkg_resources_vendor\pyparsing.py:832: DeprecationWarning: invalid escape sequence \d</p>
<p>\...\Python\venv\lib\site-packages\setuptools-28.8.0-py3.6.egg\pkg_resources_vendor\pyparsing.py:2736: DeprecationWarning: invalid escape sequence \d</p>
<p>\...\Python\venv\lib\site-packages\setuptools-28.8.0-py3.6.egg\pkg_resources_vendor\pyparsing.py:2914: DeprecationWarning: invalid escape sequence \g</p>
<p>\...\Python\venv\lib\site-packages\pyLDAvis_prepare.py:387:
DeprecationWarning:
.ix is deprecated. Please use
.loc for label based indexing or
.iloc for positional indexing</p>
</blockquote>
<p>I cannot find any solution and to be honest neither have any clue where exactly the problem comes from. I spent hours making sure that the encoding of the csv is utf-8 and exported (from R) and imported (in python) correctly.</p>
<p>What am I doing wrong or where else could I look at? Cheers!</p>
","9523233","","-1","","2020-06-20 09:12:55","2018-03-21 09:33:25","Python LDA gensim ""DeprecationWarning: invalid escape sequence""","<r><python-3.x><export-to-csv><gensim><deprecation-warning>","1","0","","","","CC BY-SA 3.0"
"47018088","1","","","2017-10-30 14:44:50","","1","1524","<p>Using gensim word2vec, built a CBOW model with a bunch of litigation files for representation of word as vector in a Named-Entity-recognition problem, but I want to known how to evaluate my representation of words. If I use any other datasets like wordsim353(NLTK) or other online datasets of google, it doesn't work because I built the model specific to my domain dataset of files. How do I evaluate my word2vec's representation of word vectors .I want words belonging to similar context to be closer in vector space.How do I ensure that the build model is doing it ?</p>

<p>I started by using a techniques called <strong>odd one out</strong>. Eg:</p>

 

<pre class=""lang-python prettyprint-override""><code>model.wv.doesnt_match(""breakfast cereal dinner lunch"".split()) --&gt; 'cereal'
</code></pre>

<p>I created my own dataset(for validating) using the words in the training of word2vec .Started evaluating with taking three words of similar context and an odd word out of context.But the accuracy of my model is only 30 % .</p>

<p>Will the above method really helps in evaluating my w2v model ? Or Is there a better way ?</p>

<p>I want to go with word_similarity measure but I need a reference score(Human assessed) to evaluate my model or is there any techniques to do it? Please ,do suggest any ideas or techniques .</p>
","4185468","","4185468","","2017-10-30 15:34:04","2017-10-30 16:01:24","How to evaluate word2vec build on a specific context files","<machine-learning><nltk><word2vec><gensim><feature-engineering>","1","0","","","","CC BY-SA 3.0"
"32978429","1","","","2015-10-06 19:47:47","","4","1116","<p>I have a data pipeline with <code>luigi</code> that works perfectly fine if I put 1 worker to the task. However, if I put > 1 workers, then it dies (unexpectedly with exit code -11) in a stage with 2 dependencies. The code is rather complex, so a minimum example would be difficult to give. The gist of the matter is that I am doing the following things with <code>gensim</code>:</p>

<ol>
<li>Building a dictionary from some texts.</li>
<li>Building a corpus from said texts and the dictionary (requires (1)).</li>
<li>Training an LDA model from the corpus and dictionary (requires (1) and (2)).</li>
</ol>

<p>For some reason, step (3) crashes every time I put more than one worker, even if (1) and (2) are already completed...</p>

<p>Any help would be greatly appreciated!</p>

<p><strong>EDIT:</strong> Here is an example of the logging info. TrainLDA is task (3). There are still two tasks after that that require TrainLDA. All earlier tasks finished correctly. I substituted TrainLDA's arguments for <code>...</code> so that the output would be more readable. The additional info are just <code>print</code> statements we put to help us know what is happening.</p>

<p>DEB</p>

<pre><code>UG: Pending tasks: 3
DEBUG: Asking scheduler for work...
INFO: [pid 28851] Worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825) running   TrainLDA(...)
INFO: Done
INFO: There are no more tasks to run at this time
INFO: TrainLDA(...) is currently run by worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825)
==============================
Corriendo LDA de spanish con nivel de limpieza stopwords
==============================
N√∫mero de t√≥picos: 40
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: TrainLDA(...) is currently run by worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825)
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: TrainLDA(...) is currently run by worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825)
INFO: Worker task TrainLDA(...) died unexpectedly with exit code -11
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: There are 2 pending tasks possibly being run by other workers
INFO: There are 2 pending tasks unique to this worker
INFO: Worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825) was stopped. Shutting down Keep-Alive thread
</code></pre>
","5016012","","5016012","","2015-10-07 23:03:14","2019-05-13 21:00:35","python luigi died unexpectedly with exit code -11","<python><text-mining><gensim><luigi>","0","10","0","","","CC BY-SA 3.0"
"40521982","1","","","2016-11-10 07:21:57","","1","140","<p>I have referred the website <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/tut2.html</a>. I have come across the error UnpicklingError was unhandled by user code : invalid load key,'%'. How do I clear that error? I had referred the other queries and included the klepto package but still that error persists. I am using anacoanda2. This is the code:-</p>

<pre><code>import logging
import xml.etree.cElementTree
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',
level=logging.INFO)
import os
import klepto
from gensim import corpora

documents = [""Human machine interface for lab abc computer applications"",
         ""A survey of user opinion of computer system response time"",
         ""The EPS user interface management system"",
         ""System and human system engineering testing of EPS"",              
         ""Relation of user perceived response time to error measurement"",
         ""The generation of random binary unordered trees"",
         ""The intersection graph of paths in trees"",
         ""Graph minors IV Widths of trees and well quasi ordering"",
         ""Graph minors A survey""]
# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
     for document in documents]

# remove words that appear only once
from collections import defaultdict
frequency = defaultdict(int)
for text in texts:
for token in text:
    frequency[token] += 1

texts = [[token for token in text if frequency[token] &gt; 1] for text in texts]

from pprint import pprint  # pretty-printer
pprint(texts)

dictionary = corpora.Dictionary(texts)
dictionary.save_as_text('/tmp/deerwester.dict')  # store the dictionary, for future reference
print(dictionary)

print(dictionary.token2id)

new_doc = ""Human computer interaction""
new_vec = dictionary.doc2bow(new_doc.lower().split())
print(new_vec)  # the word ""interaction"" does not appear in the dictionary and is ignored

corpus = [dictionary.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('/tmp/deerwester.dict', corpus)  # store to disk, for later use
for c in corpus:
print(c)

class MyCorpus(object):
def __iter__(self):
    for line in open('/datasets/mycorpus.txt'):
        # assume there's one document per line, tokens separated by whitespace
        yield dictionary.doc2bow(line.lower().split())

corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!
print(corpus_memory_friendly)

for vector in corpus_memory_friendly:  # load one vector into memory at a time
print(vector)

from six import iteritems

# collect statistics about all tokens
dictionary = corpora.Dictionary(line.lower().split() for line in open('/datasets/mycorpus.txt'))

# remove stop words and words that appear only once
stop_ids = [dictionary.token2id[stopword] for stopword in stoplist 
        if stopword in dictionary.token2id]
once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]

# remove stop words and words that appear only once
dictionary.filter_tokens(stop_ids + once_ids)

# remove gaps in id sequence after words that were removed
dictionary.compactify()
print(dictionary)

# create a toy corpus of 2 documents, as a plain Python list
corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it

corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)

corpora.SvmLightCorpus.serialize('/tmp/corpus.svmlight', corpus)
corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)
corpora.LowCorpus.serialize('/tmp/corpus.low', corpus)

corpus = corpora.MmCorpus('/tmp/corpus.mm')

print(corpus)

# one way of printing a corpus: load it entirely into memory
print(list(corpus))  # calling list() will convert any sequence to a plain Python list


# another way of doing it: print one document at a time, making use of the streaming interface
for doc in corpus:
print(doc)

corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)

import gensim
import numpy as np
numpy_matrix = np.random.randint(10, size=[5,2])
corpus = gensim.matutils.Dense2Corpus(numpy_matrix)
numpy_matrix_dense = gensim.matutils.corpus2dense(corpus, num_terms=10)

import scipy.sparse
scipy_sparse_matrix = scipy.sparse.random(5,2)
corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)
scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)

from gensim import corpora, models, similarities
if (os.path.exists(""/tmp/deerwester.dict"")):
dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')
corpus = corpora.MmCorpus('/tmp/deerwester.mm')
print(""Used files generated from first tutorial"")
else:
print(""Please run first tutorial to generate data set"")

tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model

doc_bow = [(0, 1), (1, 1)]
print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors

corpus_tfidf = tfidf[corpus]
for doc in corpus_tfidf:
print(doc)

lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation
corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow-&gt;tfidf-&gt;fold-in-lsi

lsi.print_topics(2)

for doc in corpus_lsi: # both bow-&gt;tfidf and tfidf-&gt;lsi transformations are actually executed here, on the fly
print(doc)

lsi.save('/tmp/model.lsi') # same for tfidf, lda, ...
lsi = models.LsiModel.load('/tmp/model.lsi')

model = models.TfidfModel(corpus, normalize=True)

model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)

model.add_documents(another_tfidf_corpus) # now LSI has been trained on tfidf_corpus + another_tfidf_corpus
lsi_vec = model[tfidf_vec] # convert some new document into the LSI space, without affecting the model

model.add_documents(more_documents) # tfidf_corpus + another_tfidf_corpus + more_documents
lsi_vec = model[tfidf_vec]

model = models.RpModel(tfidf_corpus, num_topics=500)

model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)

model = models.HdpModel(corpus, id2word=dictionary)
</code></pre>
","7125665","","","","","2016-11-10 07:21:57","UnpicklingError was unhandled by user code : invalid load key,'%'","<python><dictionary><anaconda><gensim><corpus>","0","0","","","","CC BY-SA 3.0"
"27308118","1","27321102","","2014-12-05 03:10:43","","1","2437","<p>I have a list of bag of words for two classes. Say <strong><em>n</em></strong> items in class <strong><em>A</em></strong> and <strong><em>m</em></strong> items in class <strong><em>B</em></strong>. I want to use the topic modeling with gensim package (for LDA) in python in order to train a model for class A vs class B. Meanwhile I am new to both <strong>Topic Modeling</strong> and <strong>Python</strong>. Does anyone know how should I do this? I mean, should I merge all the bags for each class and the use gensim or should I use bag for each item seperately? Thanks!</p>
","1842211","","","","","2014-12-05 17:06:21","Topic Modeling Using Gensim in Python","<python><machine-learning><nlp><lda><gensim>","1","3","","","","CC BY-SA 3.0"
"45444304","1","","","2017-08-01 17:29:52","","2","370","<p>I am using gensim's <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">lda implementation</a> to create a document topic model distribution. I have a model <code>lda</code> trained</p>

<pre><code>dictionary = corpora.Dictionary(data)
corpus = [dictionary.doc2bow(doc) for doc in data]
num_cores = multiprocessing.cpu_count()
num_topics = 150
lda = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary, workers=num_cores, alpha=1e-4, eta=5e-1,
              minimum_probability=0.0)
</code></pre>

<p>I want to extract a distribution matrix of size N * M where N is the number of documents in the corpus, and M is the number of topics. Each row therefore would represent a document, and each column would represent a topic. These distributions are already available in the <code>lda</code> model variable e.g.</p>

<pre><code>lda
&gt;&gt;&gt; [[(0,0.01),(1,0.23),(2,1e-7)],
    [(0,0.91),(1,0.067),(2,0.38)]]
</code></pre>

<p>where the above represents a simple example if we had 2 documents and 3 topics (0,1,2). So <code>lda</code> is a list of list of tuples, where each list is a document, the first element in the tuple is the topic id and the second element is the topic contribution to that document.</p>

<p>What I have done so far</p>

<pre><code># approach 1
distributions = np.array( [[tup[1] for tup in lst] for lst in lda[corpus]] )
# approach 2 (about 3X slower than approach 1)
distributions = np.array(lda[corpus])[:,:,1]
</code></pre>

<p>The problem is, for a dataset with 300k documents and 150 topics, creating the <code>distributions</code> matrix using approach 1 takes over 10 minutes.</p>

<p>Ultimately, I want to create this matrix to feed into a function which calculates document similarity (comparing the first row / document of the matrix to all other rows / documents). If there is a way to directly feed the learned <code>lda</code> model values into the function, that would be great as it would save time on creating the redundant matrix. If not, is there a more efficient way to create the <code>distributions</code> matrix?</p>

<p>For reference, the document comparison function into which the <code>distributions</code> matrix is fed is</p>

<pre><code>from sklearn.preprocessing import normalize
from scipy.stats import entropy
def jsd(mat):
    mat = normalize(mat, axis=1, norm='l1')
    p = mat[0,None].T # just comparing first row to all other rows
    q = mat[0:].T
    m = 0.5*(p + q)
    return 0.5*(entropy(p,m) + entropy(q,m))
</code></pre>
","4139143","","4139143","","2017-08-01 17:56:21","2017-08-01 17:56:21","Extract LDA model values into distribution matrix efficiently","<python><matrix><gensim><lda>","0","0","1","","","CC BY-SA 3.0"
"55756841","1","55779081","","2019-04-19 05:02:37","","0","174","<p>Am struggling with training wikipedia dump on doc2vec model, not experienced in setting up a server as a local machine is out of question due to the ram it requires to do the training. I couldnt find a pre trained model except outdated copies for python 2.</p>
","4586806","","","","","2019-04-21 01:12:19","Where to find a pretrained doc2vec model on Wikipedia or large article dataset like Google news?","<python><nlp><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"55754594","1","","","2019-04-18 22:47:31","","1","115","<p>I would like to use Gensim and Scikit in the same pipeline. </p>

<p><strong>[Update]</strong>
Corpus is created from a list of lemmatized tokens <code>doc.tokens</code></p>

<pre><code>bowlist = []
for doc in linked_doc_list:
    bowlist.append(doc.tokens)

dictionary = corpora.Dictionary(bowlist)
corpus = [dictionary.doc2bow(line) for line in bowlist]
</code></pre>

<p>This involves transforming the Gensim corpus to a numpy array like so:</p>

<pre><code> numpy_matrix = gensim.matutils.corpus2dense(package.corpus, num_terms=len(package.dict.token2id))
</code></pre>

<p>This seems to work. The sklearn lda runs:</p>

<pre><code>model = LatentDirichletAllocation(n_components=components,
                                          max_iter=maxiter,
                                          learning_method=learningmethod,
                                          learning_offset=learningoffset,
                                          random_state=randomstate,
                                          verbose=verbose).fit(numpy_matrix)
</code></pre>

<p>But now, to read the results, I need to read the actual terms from the gensim dict (otherwise I am stuck with meaningless feature numbers).</p>

<p>However, the results from the following code are clearly meaningless. </p>

<pre><code> def filterAndReportResultsLDA(self, model, gensimdict, n_top_words=10):
     for topic_idx, topic in enumerate(model.components_):
         print(""Topic %d:"" % (topic_idx))
         words = []
         for i in topic.argsort()[:-n_top_words - 1:-1]:
            words.append(gensimdict[i])
         print(words)
</code></pre>

<p>Example result is:</p>

<pre><code>['reporting.', '7:23', 'users?', 'breaking', '5am', 'bell', 'c7n', 'content?', 'functions', 'vi']
</code></pre>

<p>Can anyone tell me what I am doing wrong?</p>
","5082504","","3933052","","2019-04-19 05:29:32","2019-04-19 05:29:32","Python: using Gensim and Scikit in the same pipeline","<python><scikit-learn><gensim>","0","9","","","","CC BY-SA 4.0"
"49384123","1","","","2018-03-20 12:22:12","","2","1410","<p>I have changed the code in word2vec.py to this:</p>

<pre><code>\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\word2vec.py in reset_weights(self, hs, negative, wv)
   1417         for i in xrange(len(wv.vocab)):
   1418             # construct deterministic seed from word AND seed argument
-&gt; 1419             wv.vectors[i] = self.seeded_vector( """".join(str(wv.index2word[i]),str(self.seed)), wv.vector_size)
   1420         if hs:
   1421             self.syn1 = zeros((len(wv.vocab), self.layer1_size), dtype=REAL)
</code></pre>

<p>But I receive the error:</p>

<pre><code>TypeError: unsupported operand type(s) for +: 'int' and 'str'
</code></pre>

<p>On line 1419, where the arrow is pointed. I don't understand how the operand is concatenating an int and str when both are within a str() method?</p>

<p>I am using Anaconda environment if thats matters in python3</p>
","3174096","","","","","2018-09-07 07:12:05","Gensim 3.4.0 word2vec unsupported operand type(s) for +: 'int' and 'str'","<python><word2vec><gensim>","0","2","","","","CC BY-SA 3.0"
"57729961","1","57736743","","2019-08-30 15:59:38","","0","36","<p>I have a set of documents that all fit a pre-defined category and have successfully trained a model off of those documents.</p>

<p>The question is, if I have a novel document, how can I calculate how closely this new document lines up with my trained model?</p>

<p>My current solution:</p>

<pre><code>novel_vector = model.infer_vector(novel_doc_words, steps = 20)
similarity_scores = model.docvecs.most_similar([novel_vector])
average = 0
for score in similarity_scores:
  average += score[1]
overall_similarity = average/len(similarity_scores)
</code></pre>

<p>I was unable to find any convenience methods in the documentation</p>
","2212967","","","","","2019-08-31 09:25:08","Can gensim Doc2Vec be used to compare a novel document to a trained model?","<python><python-3.x><nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"53791972","1","53797066","","2018-12-15 11:34:03","","0","672","<p>I have a pair of word and semantic types of those words. I am trying to compute the relatedness measure between these two words using semantic types, for example: word1=king, type1=man, word2=queen, type2=woman
we can use gensim word_vectors.most_similar to get 'queen' from 'king-man+woman'. However, I am looking for similarity measure between vector represented by 'king-man+woman' and 'queen'.</p>

<p>I am looking for a solution to above (or)
way to calculate vector that is representative of 'king-man+woman' (and)
calculating similarity between two vectors using vector values in gensim (or)
 way to calculate simple mean of the projection weight vectors(i.e king-man+woman)</p>
","9526057","","9526057","","2018-12-15 11:44:35","2018-12-15 20:37:47","Similarity measure using vectors in gensim","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"57730631","1","","","2019-08-30 16:57:26","","6","1004","<p>The <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""noreferrer"">documentation</a> is a bit unclear how to save the fasttext model to disk - how do you specify a path in the argument, I tried doing so and it failed with an error</p>

<p>Example in documentation</p>

<pre><code>&gt;&gt;&gt; from gensim.test.utils import get_tmpfile
&gt;&gt;&gt;
&gt;&gt;&gt; fname = get_tmpfile(""fasttext.model"")
&gt;&gt;&gt;
&gt;&gt;&gt; model.save(fname)
&gt;&gt;&gt; model = FastText.load(fname)
</code></pre>

<p>Furthermore, how can I save the model in text format like can be done with word2vec models?</p>

<pre><code>'word2vecmodel.wv.save_word2vec_format(""D:\w2vmodel.txt"")'
</code></pre>

<p><strong>EDIT</strong></p>

<p>After trying the suggestion to make a file first I keep kgetting the same error as before when I run this code</p>

<pre><code>savepath = os.path.abspath('D:\fasttextmodel.v3.bin');
from gensim.test.utils import get_tmpfile
fname = get_tmpfile(savepath)
fasttext_model.save(fname)
</code></pre>

<blockquote>
  <p>TypeError: file must have a 'write' attribute</p>
</blockquote>
","1462656","","1462656","","2019-08-30 19:33:31","2021-06-14 13:57:02","How to save fasttext model in binary and text formats?","<gensim><fasttext>","2","0","2","","","CC BY-SA 4.0"
"49402113","1","49403004","","2018-03-21 09:08:39","","0","821","<p>I have the first Harry Potter book in txt format. From this, I created two new txt files: in the first, all the occurrencies of <code>Hermione</code> have been replaced with <code>Hermione_1</code>; in the second, all the occurrencies of <code>Hermione</code> have been replaced with <code>Hermione_2</code>. Then I concatenated these 2 text to create one long text and I used this as input for Word2Vec.
This is my code:</p>

<pre><code>import os
from gensim.models import Word2Vec
from gensim.models import KeyedVectors

with open(""HarryPotter1.txt"", 'r') as original, \
        open(""HarryPotter1_1.txt"", 'w') as mod1, \
        open(""HarryPotter1_2.txt"", 'w') as mod2:

    data=original.read()
    data_1 = data.replace(""Hermione"", 'Hermione_1')
    data_2 = data.replace(""Hermione"", 'Hermione_2')
    mod1.write(data_1 + r""\n"")
    mod2.write(data_2 + r""\n"")

with open(""longText.txt"",'w') as longFile:
    with open(""HarryPotter1_1.txt"",'r') as textfile:
        for line in textfile:
            longFile.write(line)
    with open(""HarryPotter1_2.txt"",'r') as textfile:
        for line in textfile:
            longFile.write(line)


model = """"
word_vectors = """"
modelName = ""ModelTest""
vectorName = ""WordVectorsTestst""

answer2 = raw_input(""Overwrite  embeddig? (yes or n)"")
if(answer2 == 'yes'):
    with open(""longText.txt"",'r') as longFile:
        sentences = []
        single= []
        for line in longFile:
            for word in line.split("" ""):
                single.append(word)
            sentences.append(single)

    model = Word2Vec(sentences,workers=4, window=5,min_count=5)

    model.save(modelName)
    model.wv.save_word2vec_format(vectorName+"".bin"",binary=True)
    model.wv.save_word2vec_format(vectorName+"".txt"", binary=False)
    model.wv.save(vectorName)

    word_vectors = model.wv

else:
    model = Word2Vec.load(modelName)
    word_vectors = KeyedVectors.load_word2vec_format(vectorName + "".bin"", binary=True)

    print(model.wv.similarity(""Hermione_1"",""Hermione_2""))
    print(model.wv.distance(""Hermione_1"",""Hermione_2""))
    print(model.wv.most_similar(""Hermione_1""))
    print(model.wv.most_similar(""Hermione_2""))
</code></pre>

<p>How is possible that <code>model.wv.most_similar(""Hermione_1"")</code> and <code>model.wv.most_similar(""Hermione_2"")</code> give me different output? 
Their neighbour are completely different. This is the output of the four print:</p>

<pre><code>0.00799602753634
0.992003972464
[('moments,', 0.3204237222671509), ('rose;', 0.3189219534397125), ('Peering', 0.3185565173625946), ('Express,', 0.31800806522369385), ('no...', 0.31678506731987), ('pushing', 0.3131707012653351), ('triumph,', 0.3116190731525421), ('no', 0.29974159598350525), ('them?""', 0.2927379012107849), ('first.', 0.29270970821380615)]
[('go?', 0.45812922716140747), ('magical', 0.35565727949142456), ('Spells.""', 0.3554503619670868), ('Scabbets', 0.34701400995254517), ('cupboard.""', 0.33982667326927185), ('dreadlocks', 0.3325180113315582), ('sickening', 0.32789379358291626), ('First,', 0.3245708644390106), ('met', 0.3223033547401428), ('built', 0.3218075931072235)]
</code></pre>
","8889685","","8889685","","2018-03-21 09:21:47","2018-03-21 09:51:06","Gensim Word2Vec most similar different result python","<python><string><word2vec><gensim><word-embedding>","1","1","","","","CC BY-SA 3.0"
"57729538","1","","","2019-08-30 15:28:23","","0","723","<p>I wanted to fix topics to include some words, e.g. 
Topic 0 - cloud_computing,hybrid_cloud, ...
Topic 1 - smartphone,mobile, ...</p>

<p>So I can across this blog <a href=""http://scignconsulting.com/2019/03/09/guided-lda/"" rel=""nofollow noreferrer"">http://scignconsulting.com/2019/03/09/guided-lda/</a> which attempts to do just that by settings priors for eta.</p>

<p>But what I've found is that for large collections of documents (10s of 1000s) the seed words are getting downranked, with only 5/104 of the final topics actually including any of the original seed words.</p>

<p>I have a hypothesis for why this is happening. I believe that the alphas for the probability of a topic given a document would also need to be set, otherwise, if the probability for a seeded topic is very low, the seedwords may not matter much at all. </p>

<p>Has anyone had experience in this or any pointers to avoid the seed words being ignored.</p>
","1244945","","","","","2020-01-03 15:51:47","Guided LDA in GenSim with fixed Eta","<python><gensim><lda>","1","1","1","","","CC BY-SA 4.0"
"55884548","1","","","2019-04-27 20:18:21","","2","1198","<p>I am training a <code>doc2vec gensim model</code> with txt file 'full_texts.txt' that contains ~1600 documents. Once I have trained the model, I wish to use similarity methods over words and sentences. </p>

<p>However, since this is my first time using gensim , I am unable to get a solution. If I want to look for similarity by words I try as mentioned below but I get an <strong>error</strong> that the <code>word doesnt exist in the vocabulary</code> and on the other question is how do I check similarity for entire documents? I have read a lot of questions around it, like this <a href=""https://stackoverflow.com/questions/45420466/gensim-keyerror-word-not-in-vocabulary"">one</a> and looked up <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar"" rel=""nofollow noreferrer"">documentation</a> but still not sure what I am doing wrong.</p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedLineDocument
from gensim.models.doc2vec import TaggedDocument

tagdocs = TaggedLineDocument('full_texts.txt')
d2v_mod = Doc2Vec(min_count=3,vector_size = 200, workers = 2, window = 5, epochs = 30,dm=0,dbow_words=1,seed=42)
d2v_mod.build_vocab(tagdocs)
d2v_mod.train(tagdocs,total_examples=d2v_mod.corpus_count,epochs=20)

d2v_mod.wv.similar_by_word('overdraft',topn=10)
KeyError: ""word 'overdraft' not in vocabulary""
</code></pre>
","1885727","","","","","2020-08-15 12:37:22","gensim Doc2Vec word not in vocabulary","<python><nlp><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"64655691","1","","","2020-11-03 01:45:39","","0","36","<p>I'm building a fake news classifier.</p>
<p>I have a typical document term matrix with documents as strings and words as columns.</p>
<p>I'd like to compute the entropy of each word and plot the distribution of the entropy in order to decide a cut off value and remove words with low entropy from the matrix.</p>
","10469275","","","","","2020-11-03 01:45:39","Is there Python code to remove words with low entropy from document term matrix?","<python><nlp><nltk><gensim><entropy>","0","0","","","","CC BY-SA 4.0"
"49431270","1","49589053","","2018-03-22 14:29:51","","3","3684","<p>When building a python gensim word2vec <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">model</a>, is there a way to see a doc-to-word matrix?</p>

<p>With input of <code>sentences = [['first', 'sentence'], ['second', 'sentence']]</code> I'd see something like*:</p>

<pre><code>      first  second  sentence
doc0    1       0        1
doc1    0       1        1
</code></pre>

<p>*I've illustrated 'human readable', but I'm looking for a scipy (or other) matrix, indexed to <code>model.wv.index2word</code>.</p>

<p>And, can that be transformed into a word-to-word matrix (to see co-occurences)? Something like:</p>

<pre><code>          first  second  sentence
first       1       0        1
second      0       1        1  
sentence    1       1        2   
</code></pre>

<p>I've already implemented something like <a href=""https://stackoverflow.com/questions/35562789/word-word-co-occurrence-matrix"">word-word co-occurrence matrix</a> using CountVectorizer. It works well. However, I'm already using gensim in my pipeline and speed/code simplicity matter for my use-case. </p>
","4052455","","","","","2018-03-31 14:54:35","word co-occurrence matrix from gensim","<python><nlp><gensim>","2","0","2","","","CC BY-SA 3.0"
"53800830","1","","","2018-12-16 09:10:49","","7","24536","<p>Here is my code :</p>

<pre><code>data = pd.read_csv('asscsv2.csv', encoding = ""ISO-8859-1"", error_bad_lines=False);
data_text = data[['content']]
data_text['index'] = data_text.index
documents = data_text
</code></pre>

<p>It looks like</p>

<pre><code>print(documents[:2])
                                              content  index
 0  Pretty extensive background in Egyptology and ...      0
 1  Have you guys checked the back end of the Sphi...      1
</code></pre>

<p>And I define a preprocess function by using gensim </p>

<pre><code>stemmer = PorterStemmer()
def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
            result.append(lemmatize_stemming(token))
    return result
</code></pre>

<p>And when I use this function:</p>

<pre><code>processed_docs = documents['content'].map(preprocess)
</code></pre>

<p>It appears </p>

<pre><code>TypeError: decoding to str: need a bytes-like object, float found
</code></pre>

<p>How to encode my csv file to byte-like object or how to avoid this kind of error?</p>
","10595338","","5223757","","2018-12-16 09:13:56","2021-05-01 11:54:34","How to avoid decoding to str: need a bytes-like object error in pandas?","<python><python-3.x><pandas><gensim><topic-modeling>","2","2","2","","","CC BY-SA 4.0"
"16509883","1","","","2013-05-12 17:00:00","","1","477","<p>First, is this the right way to get the topic distributions of the corpus on which LDA was performed?</p>

<pre><code>lda = LdaModel(corpus,  num_topics=500, update_every=0, passes=2)
#get the topics distribution of the corpus
result=lda[corpus]
</code></pre>

<p>Now the issue occurs when I add the alpha parameter to the LDA and try to convert the corpus to a sparse matrix as follows:</p>

<pre><code>  1- lda = LdaModel(corpus,  num_topics=500, update_every=0, passes=2,alpha=0.5)
  2- result=lda[corpus]
  3- gensim.matutils.corpus2csc(result).T
</code></pre>

<p>During the conversion from gensim corpus to the sparse matrix as in line 3, I get the error <code>ValueError: invalid shape</code></p>

<p>I only get this problem when I add the ALPHA parameter!</p>

<p>The complete traceback:</p>

<pre><code>    ---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-788-7fb54d5da9fb&gt; in &lt;module&gt;()
----&gt; 1 xp,xc=issam.lda(c)

C:\Anaconda\lib\issamKit.py in lda(X)
   1745      corpus=gensim.matutils.Sparse2Corpus(X.T)
   1746      lda = LdaModel(corpus,  num_topics=500, update_every=0, passes=2,alpha=1)
-&gt; 1747      return lda,gensim.matutils.corpus2csc(lda[corpus]).T
   1748 def lsi(X):
   1749      import gensim

C:\Anaconda\lib\site-packages\gensim-0.8.6-py2.7.egg\gensim\matutils.pyc in corpus2csc(corpus, num_terms, dtype, num_docs, num_nnz, printprogress)
     97         data = numpy.asarray(data, dtype=dtype)
     98         indices = numpy.asarray(indices)
---&gt; 99         result = scipy.sparse.csc_matrix((data, indices, indptr), shape=(num_terms, num_docs), dtype=dtype)
    100     return result
    101 

C:\Anaconda\lib\site-packages\scipy\sparse\compressed.pyc in __init__(self, arg1, shape, dtype, copy)
     66         # Read matrix dimensions given, if any
     67         if shape is not None:
---&gt; 68             self.shape = shape   # spmatrix will check for errors
     69         else:
     70             if self.shape is None:

C:\Anaconda\lib\site-packages\scipy\sparse\base.pyc in set_shape(self, shape)
     69 
     70         if not (shape[0] &gt;= 1 and shape[1] &gt;= 1):
---&gt; 71             raise ValueError('invalid shape')
     72 
     73         if (self._shape != shape) and (self._shape is not None):

ValueError: invalid shape
</code></pre>
","1248073","","","","","2013-12-04 22:25:18","(Gensim) ValueError: invalid shape, with the alpha parameter","<python><lda><gensim>","1","0","","","","CC BY-SA 3.0"
"47366918","1","","","2017-11-18 14:00:21","","1","197","<p>I have about 200-600k documents of user descriptions.</p>

<p>and I'm using <code>gensim</code> Doc2Vec model.</p>

<p>I wanted to ask what would be the best fit model configuration so I can do a contextual search on my documents? I want to enter a free text query and get the best similar results for this query. Some of my queries including unique words that are important.</p>

<p>For example: ""I need an English speaker for ...""</p>

<p>with my current configuration:</p>

<p><code>Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, sample=1e-4,  workers=cores)</code></p>

<p>I'm getting really good results but, for my example it will find a description with a speaker inside but not an English one, it can be German speaker.</p>

<p>Is there a better configuration of those examples?</p>
","2228884","","","","","2017-11-18 14:00:21","Doc2Vec configuration","<python><similarity><word2vec><gensim><doc2vec>","0","2","","","","CC BY-SA 3.0"
"57672901","1","59786965","","2019-08-27 10:39:27","","1","134","<p>I wanted to successfully run LDAseq model on my very huge corpus. I finally want to extract 100 topics from it.</p>

<p>I am getting an error ""out of memory"" on the step of ldaseq model. This is because I have a huge token and I don't want to truncate it. How to resolve this memory issue?</p>

<ul>
<li>Windows-10-10.0.17763-SP0</li>
<li>Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)]</li>
<li>NumPy 1.17.0</li>
<li>SciPy 1.3.0</li>
<li>gensim 3.8.0</li>
<li>FAST_VERSION 0</li>
</ul>

<p>My expected result is the same as shown in the documentation. I need a topic-term and topic-doc matrix finally.</p>
","7786625","","6464308","","2019-08-27 11:39:20","2020-01-17 12:01:40","Out of memory issue in gensim topic modeling","<gensim><lda><topic-modeling>","1","0","1","","","CC BY-SA 4.0"
"33059671","1","","","2015-10-10 22:35:24","","1","777","<p>I just started to experiment with word2vec form gensim using tutorial provide in <a href=""http://radek&#39;%20s%20tutorial"" rel=""nofollow"">http://rare-technologies.com/word2vec-tutorial/</a>. If we need need the raw output vectors, we write:</p>

<pre><code>model['computer']  
</code></pre>

<p>And the result is:</p>

<pre><code>array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)
</code></pre>

<p>How can I get the word having the array? So if I write:</p>

<pre><code>f=model['computer']
</code></pre>

<p>how can I get the word 'computer' using f?</p>
","1680859","","","","","2015-10-11 07:20:19","Get word from array in word2vec in gensim","<gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"55892073","1","","","2019-04-28 16:04:49","","0","1729","<p>I want to get bigrams for symbols (letters of words). For example, for words ""done"" and ""dog"" I want to be able to find bigram ""do"". </p>

<p>I tried to do it using gensim.Phrases, but it doesn`t work for me. </p>

<p>Here is my code:</p>

<pre><code>from gensim.models import Phrases

documents = [""God"", ""Good"",""happy"",""hangry"",""pypi""]
documents_proc = [list(doc) for doc in documents]

bigram = Phrases(documents_proc, min_count=1)
trigram = Phrases(bigram[documents_proc], min_count=1)

for sent in documents_proc:
    print(sent, bigram[sent])
    bigrams_ = [b for b in bigram[sent] if b.count('_') == 1]
    trigrams_ = [t for t in trigram[bigram[sent]] if t.count('_') == 2]
    print(bigrams_)
    print(trigrams_)
    print()
</code></pre>

<p>I expected the output of
<code>['Go', 'od', 'ha', 'py']</code>, but there are nothing in the output. 
What am I doing wrong? </p>

<p>Thank you.</p>
","7026376","","","","","2019-04-28 17:29:45","Gensim phrases don`t find some bigrams","<python><gensim><phrase>","1","0","1","","","CC BY-SA 4.0"
"24816912","1","","","2014-07-18 03:47:19","","1","1931","<p>I'm using gensim's package to implement LSI on a corpus. My goal is to find out the most frequently occurring distinct topics that appear in the corpus.</p>

<p>If I don't know the number of topics that are in the corpus (I'd estimate anywhere from 5 to 20), what is the best approach in setting the number of topics that LSI should search for? Is it better to look for a large number of topics (20-30), or a small number of topics (~5)? </p>
","3803999","","","","","2014-11-10 06:34:11","Number of Latent Semantic Indexing topics","<topic-modeling><gensim><latent-semantic-indexing>","2","1","1","","","CC BY-SA 3.0"
"31996843","1","32008315","","2015-08-13 19:28:39","","0","3516","<p>I have Windows 7 and WinPython 3.4.3.2; trying to install Gensim from <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/#gensim"" rel=""nofollow"">http://www.lfd.uci.edu/~gohlke/pythonlibs/#gensim</a></p>

<p>I'm getting this error:</p>

<pre><code>C:\Program Files (x86)\PowerCmd&gt;pip install e:\Python\gensim-0.12.1-cp34-none-win_amd64.whl
You are using pip version 6.0.8, however version 7.1.0 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Processing e:\python\gensim-0.12.1-cp34-none-win_amd64.whl
Requirement already satisfied (use --upgrade to upgrade): six&gt;=1.2.0 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages (from gensim==0.12.1)
Requirement already satisfied (use --upgrade to upgrade): numpy&gt;=1.3 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages (from gensim==0.12.1)
Collecting smart-open&gt;=1.2.1 (from gensim==0.12.1)
  Using cached smart_open-1.2.1.tar.gz
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 20, in &lt;module&gt;
  File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 28, in &lt;module&gt;
    long_description = read('README.rst'),
  File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 21, in read
    return open(os.path.join(os.path.dirname(__file__), fname)).read()
  File ""M:\WinPython-64bit-3.4.3.2\python-3.4.3.amd64\lib\encodings\cp1251.py"", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 4345: character maps to &lt;undefined&gt;
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 20, in &lt;module&gt;
      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 28, in &lt;module&gt;
        long_description = read('README.rst'),
      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 21, in read
        return open(os.path.join(os.path.dirname(__file__), fname)).read()
      File ""M:\WinPython-64bit-3.4.3.2\python-3.4.3.amd64\lib\encodings\cp1251.py"", line 23, in decode
        return codecs.charmap_decode(input,self.errors,decoding_table)[0]
    UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 4345: character maps to &lt;undefined&gt;
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

      File ""&lt;string&gt;"", line 20, in &lt;module&gt;

      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 28, in &lt;module&gt;

        long_description = read('README.rst'),

      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 21, in read

        return open(os.path.join(os.path.dirname(__file__), fname)).read()

      File ""M:\WinPython-64bit-3.4.3.2\python-3.4.3.amd64\lib\encodings\cp1251.py"", line 23, in decode

        return codecs.charmap_decode(input,self.errors,decoding_table)[0]

    UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 4345: character maps to &lt;undefined&gt;

    ----------------------------------------
    Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open
</code></pre>
","1849805","","1832942","","2015-08-13 21:29:00","2015-08-15 18:12:00","How to install Gensim on Windows 7","<python><gensim>","2","0","1","","","CC BY-SA 3.0"
"40581010","1","","","2016-11-14 02:17:09","","7","13493","<p>I want to visualize a word2vec created from gensim library. I tried sklearn but it seems I need to install a developer version to get it. I tried installing the developer version but that is not working on my machine . Is it possible to modify this code to visualize a word2vec model ?</p>

<p><a href=""https://lvdmaaten.github.io/tsne/code/tsne_python.zip"" rel=""noreferrer"">tsne_python</a></p>
","825707","","","","","2019-09-04 22:38:26","How to run tsne on word2vec created from gensim?","<scikit-learn><gensim><word2vec>","2","0","5","","","CC BY-SA 3.0"
"40472070","1","41733267","","2016-11-07 18:30:20","","0","471","<p>I am trying to understand relation between word2vec and doc2vec vectors in Gensim's implementation. In my application, I am tagging multiple documents with same label (topic), I am training a doc2vec model on my corpus using dbow_words=1 in order to train word vectors as well. I have been able to obtain similarities between word and document vectors in this fashion which does make a lot of sense
For ex. getting documents labels similar to a word-
doc2vec_model.docvecs.most_similar(positive = [doc2vec_model[""management""]], topn = 50))</p>

<p>My question however is about theoretical interpretation of computing similarity between word2vec and doc2vec  vectors. Would it be safe to assume that when trained on the same corpus with same dimensionality (d = 200), word vectors and document vectors can always be compared to find similar words for a document label or similar document labels for a word. Any suggestion/ideas are most welcome.</p>

<p>Question 2: My other questions is about impact of high/low frequency of a word in final word2vec model. If wordA and wordB have similar contexts in a particular doc label(set) of documents but wordA has much higher frequency than wordB, would wordB have higher similarity score with the corresponding doc label or not. I am trying to train multiple word2vec models by sampling corpus in a temporal fashion and want to know if the hypothesis that as words get more and more frequent, assuming context relatively stays similar, similarity score with a document label would also increase. Am I wrong to make this assumption? Any suggestions/ideas are very welcome.</p>

<p>Thanks,
Manish</p>
","7127620","","","","","2017-01-19 03:35:24","word vector and paragraph vector query","<similarity><gensim><word2vec><temporal><doc2vec>","1","0","","","","CC BY-SA 3.0"
"40596702","1","","","2016-11-14 19:49:05","","1","501","<p>I read the following code to learn a doc2vec model.Each document is defined as a text/line between two lines : </p>

<ul>
<li>clueweb09-en0001-XX-XXXXX</li>
<li>end_clueweb09-en0001-XX-XXXXX</li>
</ul>

<p>This is my code:</p>

<pre><code> path='/home/work/Step2/test-input/html'


alldocs = []  # will hold all docs in original order


for fname in os.listdir(path):
    with open(path+'/'+fname) as alldata:
        for line in alldata:
            docId= line
            print docId
            context= alldata.next()
            #print context
            tokens = gensim.utils.to_unicode(context).split()
            end=alldata.next()
            alldocs.append(LabeledSentence(tokens[:],[docId]))

model = Doc2Vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate
model.build_vocab(alldocs)
for epoch in range(10):
    model.train(alldocs)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model.alpha  # fix the learning rate, no decay

# store the model to mmap-able files
model.save(path+'/my_html_model.doc2vec')
</code></pre>

<p>But I got the error when I wrote <em>model.docvecs['clueweb09-en0001-01-34238']</em> but when I write <em>model.docvecs[0]</em> I got the result.</p>

<p>This is the error I got:</p>

<pre><code>    Traceback (most recent call last):
  File ""getLearingDoc.py"", line 40, in &lt;module&gt;
    print model.docvecs['clueweb09-en0001-01-34238']
  File ""/home/flashkar/anaconda/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 341, in __getitem__
    return self.doctag_syn0[self._int_index(index)]
  File ""/home/flashkar/anaconda/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 315, in _int_index
    return self.max_rawint + 1 + self.doctags[index].offset
KeyError: 'clueweb09-en0001-01-34238'
</code></pre>

<p>I do not have experiences in python and gensim please tell me how can I solve this problem. </p>
","3092781","","","","","2017-01-19 03:28:13","How solve gensim KeyError when I try to have a document's vector?","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"40597319","1","","","2016-11-14 20:32:16","","1","1648","<p>I am training word2vec model in gensim using the sentences in a csv file as follows:</p>

<pre><code>import string
import gensim
import csv
import nltk

path = '/home/neel/Desktop/csci544_proj/test/sample.csv'
translator = str.maketrans({key: None for key in string.punctuation})

class gen(object):

    def __init__(self, path):
        self.path = path

    def __iter__(self):
        with open(path) as infile:
            reader = csv.reader(infile)
            for row in reader:
                rev = row[4]
                l = nltk.sent_tokenize(rev)
                for sent in l:
                    sent = sent.translate(translator)
                    yield sent.lower().split()

sentences = [path]
for p in gen(path):
    model = gensim.models.Word2Vec(p, min_count=1, iter=1)

print(model.vocab.keys())
</code></pre>

<p>I get the following result:
(['b', 'u', 'm', 'h', 'e', 'n', 'r', 'v', 'i', 'a', 't', 's', 'k', 'w', 'o', 'l'])</p>

<p>The result I am get is not words but the characters. Where is the program going wrong?</p>
","3516427","","3516427","","2016-11-15 04:42:01","2017-05-28 07:15:52","Gensim word2vec online training","<python><gensim><word2vec><yield-keyword>","1","1","","","","CC BY-SA 3.0"
"53796806","1","","","2018-12-15 20:04:58","","2","1662","<p>I am using gensim's Doc2vec to learn features from news articles. I can successfully train my documents. However, I struggle to retrieve the document vectors from the model for further processing. </p>

<p>Example code (directly taken <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#usage-examples"" rel=""nofollow noreferrer"">from gensim's documentation</a>):</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from gensim.test.utils import common_texts

documents = [TaggedDocument((doc), [i]) for i, doc in enumerate(common_texts)]
model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)
</code></pre>

<p>This correctly trains without error. </p>

<p>If I try use <code>model.docvecs</code> directly or iterate over it like so:</p>

<pre><code>for vector in model.docvecs:
    print(vector)
</code></pre>

<p>I get this error:</p>

<pre><code>KeyError: ""tag '9' not seen in training corpus/invalid""
</code></pre>

<p>What is the reason for this and how can I fix this?</p>

<p>Thanks in advance!</p>
","7117003","","","","","2018-12-15 20:18:28","Gensim Doc2vec ‚Äì KeyError: ""tag not seen in training corpus/invalid""","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"49414962","1","","","2018-03-21 19:23:05","","2","246","<p>I am attempting to train an encoding for a text using gensim. I run the text over 5000 iterations (Document is ~250,000 words long, and Gensim is training over ~7500 frequent words), and the summed training loss for each iteration reduces from ~800,000 to 4,000 in that time. That said, when plotted, it is clear that training ended early and that further training would reduce the loss. However, when I load back in the saved model and run training:</p>

<pre><code>model = Word2Vec.load(""encodings"")
model.train(lines, total_examples=model.corpus_count, epochs=model.iter, compute_loss=True, callbacks=[logLoss, saveModel])
</code></pre>

<p>Training appears to start over from scratch with the initial training error ~800,000 and reduces back down to 4,000. I am saving the model with <code>model.save(""encodings"")</code>, and the saved model appears to be at least partially trained as word vector similarities seem somewhat reasonable. I want to further train my encoding, but this isn't working. (Note, initially training my encoding for more time ie. <code>iter=10000</code> doesn't improve my loss (it starts at 800,000 and ends at 4,000). Reducing the initial alpha or min_alpha over this extended period does not help either. Initial training is run with:</p>

<pre><code>model = Word2Vec(lines, min_count=2, size=300, workers=8, sg=1, iter=5000, compute_loss=True, callbacks=[logLoss, saveModel])
</code></pre>
","9530726","","","","","2018-03-21 19:23:05","Gensim resume training starting training from scratch","<gensim>","0","1","","","","CC BY-SA 3.0"
"50362506","1","","","2018-05-16 04:32:06","","9","1657","<p>I am using <code>Doc2vec</code> to get vectors from words.
Please see my below code:</p>

<pre><code>from gensim.models.doc2vec import TaggedDocument
f = open('test.txt','r')

trainings = [TaggedDocument(words = data.strip().split("",""),tags = [i]) for i,data in enumerate(f)


model = Doc2Vec(vector_size=5,  epochs=55, seed = 1, dm_concat=1)

model.build_vocab(trainings)
model.train(trainings, total_examples=model.corpus_count, epochs=model.epochs)

model.save(""doc2vec.model"")

model = Doc2Vec.load('doc2vec.model')
for i in range(len(model.docvecs)):
    print(i,model.docvecs[i])
</code></pre>

<p>I have a <code>test.txt</code> file that its content has 2 lines and contents of these 2 lines is the same (they are ""a"")
I trained with doc2vec and got the model, but the problem is although the contents of 2 lines is the same, doc2vec gave me 2 different vectors.</p>

<pre><code>0 [ 0.02730868  0.00393569 -0.08150548 -0.04009786 -0.01400406]
1 [ 0.03916578 -0.06423566 -0.05350181 -0.00726833 -0.08292392]
</code></pre>

<p>I dont know why this happened. I thought that these vectors would be the same.
Can you explain that? And if I want to make the same vectors for the sames words, what should I do in this case?</p>
","9751314","","712995","","2018-05-16 20:08:46","2018-05-16 21:06:02","Why Doc2vec gives 2 different vectors for the same texts","<python><nlp><word2vec><gensim><doc2vec>","2","0","3","","","CC BY-SA 4.0"
"66331105","1","","","2021-02-23 10:11:18","","0","16","<p>I have a list of strings (1.5 million) where list of strings like</p>
<pre><code>['zzh2z24nV5Rl5TMKpSZFGBINFUVq', 'zzDD78WbbuiJmuu39V0opHMzArTU',
   'zz+GR08MrX9sDVH14wK0ql3z7Hh22+mj2IhnxO/69b0=',
   'zz+GR08MrX9sDVH14wK0ql3z7Hh22+mj2IhnxO/69b0=',
   'zytUgOn10HEL2P1nt0JN', 'zytUgOn10HEL2P1nt0JN',
   'zxwQJmJQp0MILZt1vKyhnSg65RgF', 'zxwQJmJQp0MILZt1vKyhnSg65RgF',
   'zxnJy0uha0iIZdRxS6GhA%2BSxpvQdqiguF8fws11Xqcw%3D',
   'zxfea5z0riInF4qMqkXLoZv96k2a', 'zxfea5z0riInF4qMqkXLoZv96k2a',
   'zxJSM5pcPRN8YTz/gm5mf2Y61M3A26biLsUMKlu20OE=',
   'zwgOkuH7AmkDxOUz3FD7xFAkTgvCBd46IVTOsEXZxOM%3D',
   'zvvxBkk9qVyxvMrqZ3xC9aOE9ufKIt6jNbxhUphKkow%3D',
   'zvYYj1FYNsX5EBN8mS+fhTi5bNcJrdp+KnJPf9vG1cg=', ...]
</code></pre>
<p>I want to plot graph of most similar words like
<a href=""https://i.stack.imgur.com/VnKbR.png"" rel=""nofollow noreferrer"">this </a>. At least 1-2 thousand.
And also I need a list of similar vectors/words.
What I do:</p>
<ol>
<li><p>I am considering strings as sentences so any symbol in the string is a word.</p>
</li>
<li><p>Am choosing unique strings and transform them into sentences using split like</p>
<pre><code>list_of_sentences = [['2',  '2',  '1',  '1',  '1',  '%',  '7',  'C',  '8',  '6',  '2', 
    '1',  '9',  '5',  '9',  '%',  '7',  'C',  '9',  'b',  '0',  '5', 
    '8',  '5',  '5',  '2',  'd',  '0',  '8',  'e',  '8',  '9',  'e', 
    '9'], [...], ... ]
</code></pre>
</li>
<li><p>Convert words into vectors</p>
<pre><code>model = Word2Vec(list_of_sentences[:1000]), 
         min_count=2, 
         size=50, 
         workers=4)
</code></pre>
</li>
<li><p>Extract vocabulary of word-vectors from model and collapse to 2 dimension, plot grap</p>
</li>
</ol>
<blockquote>
</blockquote>
<pre><code>def tsne_plot(model):
    labels = []
    tokens = []
    for word in model.wv.vocab:
        tokens.append(model[word])
        labels.append(word)
    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
    new_values = tsne_model.fit_transform(tokens)
    x = []
    y = []
    for value in new_values:
        x.append(value[0])
        y.append(value[1])   
    plt.figure(figsize=(10, 10)) 
    for i in range(len(x)):
        plt.scatter(x[i],y[i])
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.show()
tsne_plot(model)
</code></pre>
<p>But I am getting a <a href=""https://i.stack.imgur.com/oWnzV.png"" rel=""nofollow noreferrer"">graph</a> of the similarity of characters, not strings of symbol similarity</p>
<p><strong>What I do wrong and how I can get list of similar strings?</strong></p>
","15261082","","15261082","","2021-02-23 10:59:36","2021-02-23 10:59:36","Scatterplot of string similarity","<python><nlp><gensim>","0","0","","","","CC BY-SA 4.0"
"49410113","1","","","2018-03-21 15:16:47","","0","8320","<p>I have the following code and I made sure its extension and name are correct. However, I still get the error outputted as seen below.  </p>

<p>I did see another person asked a similar question here on Stack Overflow, and read the answer but it did not help me.</p>

<p><a href=""https://stackoverflow.com/questions/44045881/failed-to-load-a-bin-gz-pre-trained-words2vecx"">Failed to load a .bin.gz pre trained words2vecx</a></p>

<p>Any suggestions how to fix this?</p>

<p>Input:</p>

<pre><code>import gensim
word2vec_path = ""GoogleNews-vectors-negative300.bin.gz""
word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)
</code></pre>

<p>Output:</p>

<pre><code>OSError: Not a gzipped file (b've')
</code></pre>
","8778033","","","","","2020-06-19 12:49:05","OSError: Not a gzipped file (b've') python","<python><word2vec><gensim>","2","4","","","","CC BY-SA 3.0"
"27354912","1","","","2014-12-08 09:26:26","","1","570","<p>last parts of the code:</p>

<pre><code>lda = models.LdaModel(corpus_tfidf, id2word = dic, num_topics = 64)
corpus_lda = lda[corpus_tfidf]
</code></pre>

<p>I am wondering how to save corpus_lda for further use?</p>
","4336601","","","","","2014-12-08 14:04:59","How to save trainset's distribution on a trained LDA models by gensim?","<python><lda><gensim>","1","4","","","","CC BY-SA 3.0"
"49380258","1","","","2018-03-20 09:17:00","","0","329","<p>I tried doing text clustering using LDA, but it isn't giving me distinct clusters. Below is my code</p>

<pre><code>#Import libraries
from gensim import corpora, models
import pandas as pd
from gensim.parsing.preprocessing import STOPWORDS
from itertools import chain

#stop words
stoplist = list(STOPWORDS)
new = ['education','certification','certificate','certified']
stoplist.extend(new)
stoplist.sort()

#read data
dat = pd.read_csv('D:\data_800k.csv',encoding='latin').Certi.tolist()
#remove stop words
texts = [[word for word in document.lower().split() if word not in stoplist] for document in dat]
#dictionary
dictionary = corpora.Dictionary(texts)
#corpus
corpus = [dictionary.doc2bow(text) for text in texts]
#train model
lda = models.LdaMulticore(corpus, id2word=dictionary, num_topics=25, workers=4,minimum_probability=0)
#print topics
lda.print_topics(num_topics=25, num_words=7)
#get corpus
lda_corpus = lda[corpus]
#calculate cutoff score
scores = list(chain(*[[score for topic_id,score in topic] \
                      for topic in [doc for doc in lda_corpus]]))


#threshold
threshold = sum(scores)/len(scores)
threshold
**0.039999999971137644**

#cluster1
cluster1 = [j for i,j in zip(lda_corpus,dat) if i[0][1] &gt; threshold]

#cluster2
cluster2 = [j for i,j in zip(lda_corpus,dat) if i[1][1] &gt; threshold]
</code></pre>

<p>The problem is there are overlapping elements in cluster1, which tend to be present in cluster2 and so on.</p>

<p>I also tried to increase threshold manually to 0.5, however it is giving me the same issue</p>
","4566277","","4566277","","2018-03-21 11:17:54","2018-03-21 11:17:54","Inefficiency of topic modelling for text clustering","<python><cluster-analysis><gensim><lda>","1","0","","","","CC BY-SA 3.0"
"49391597","1","49396614","","2018-03-20 18:22:22","","5","1363","<p>The command model.most_similar(positive=['france'], topn=100) gives the top 100 most similar words to ""france"". However, I would like to know if there is a method which will output the most similar words above a similarity threshold to a given word. Is there a method like the following?:
model.most_similar(positive=['france'], threshold=0.9)</p>
","4729454","","130288","","2018-03-21 00:59:14","2019-11-26 12:43:27","Applying word2vec to find all words above a similarity threshold","<word2vec><gensim>","2","0","","","","CC BY-SA 3.0"
"40564790","1","","","2016-11-12 16:06:36","","2","845","<p>I am trying out Gensim for the first time and have a question now. I have trained a LSI Model with a corpus of prepared documents. My question is, how do i get to know if a new document is similar to my model generated from the corpus of documents. I don¬¥t want to know the similarity for the document to each document in my corpus like MatrixSimilarity does but rather know if the document is similar to my topic/model.</p>
","4566692","","","","","2017-07-09 08:13:30","Doc2Vec Gensim Similarity between Document and Topic","<python><similarity><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"36034454","1","50550236","","2016-03-16 11:31:27","","24","9451","<p>I am using Word2vec through <a href=""https://radimrehurek.com/gensim/"" rel=""noreferrer""><em>gensim</em></a> with Google's pretrained vectors trained on Google News. I have noticed that the word vectors I can access by doing direct index lookups on the <code>Word2Vec</code> object are not unit vectors:</p>

<pre><code>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; w2v = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
&gt;&gt;&gt; king_vector = w2v['king']
&gt;&gt;&gt; numpy.linalg.norm(king_vector)
2.9022589
</code></pre>

<p>However, in the <a href=""https://github.com/piskvorky/gensim/blob/0.12.4/gensim/models/word2vec.py#L1153-L1213"" rel=""noreferrer""><code>most_similar</code></a> method, these non-unit vectors are not used; instead, normalised versions are used from the undocumented <code>.syn0norm</code> property, which contains only unit vectors:</p>

<pre><code>&gt;&gt;&gt; w2v.init_sims()
&gt;&gt;&gt; unit_king_vector = w2v.syn0norm[w2v.vocab['king'].index]
&gt;&gt;&gt; numpy.linalg.norm(unit_king_vector)
0.99999994
</code></pre>

<p>The larger vector is just a scaled up version of the unit vector:</p>

<pre><code>&gt;&gt;&gt; king_vector - numpy.linalg.norm(king_vector) * unit_king_vector
array([  0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,
         0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,
        -7.45058060e-09,   0.00000000e+00,   3.72529030e-09,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
        ... (some lines omitted) ...
        -1.86264515e-09,  -3.72529030e-09,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)
</code></pre>

<p>Given that word similarity comparisons in Word2Vec are done by <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""noreferrer"">cosine similarity</a>, it's not obvious to me what the lengths of the non-normalised vectors mean - although I assume they mean <em>something</em>, since gensim exposes them to me rather than only exposing the unit vectors in <code>.syn0norm</code>.</p>

<p>How are the lengths of these non-normalised Word2vec vectors generated, and what is their meaning? For what calculations does it make sense to use the normalised vectors, and when should I use the non-normalised ones?</p>
","1709587","","","","","2018-05-27 11:38:54","What meaning does the length of a Word2vec vector have?","<python><nlp><gensim><word2vec>","1","1","9","","","CC BY-SA 3.0"
"38484117","1","","","2016-07-20 14:31:00","","3","355","<p>I trained a doc2vec model with Python2 and I would like to use it in Python3.</p>

<p>When I try to load it in Python 3, I get : </p>

<pre><code>Doc2Vec.load('my_doc2vec.pkl')

UnicodeDecodeError: 'ascii' codec can't decode byte 0xb0 in position 0: ordinal not in range(128)
</code></pre>

<p>It seems to be related to a pickle compatibility issue, which I tried to solve by doing :</p>

<pre><code>with open('my_doc2vec.pkl', 'rb') as inf:
    data = pickle.load(inf)
data.save('my_doc2vec_python3.pkl')
</code></pre>

<p>Gensim saved other files which I renamed as well so they can be found when calling</p>

<pre><code>de = Doc2Vec.load('my_doc2vec_python3.pkl')
</code></pre>

<p>The load() does not fail with UnicodeDecodeError but after the inference provides meaningless results.</p>

<p>I can't easily re-train it using Gensim in Python 3 as I used this model to create derived data from it, so I would have to re-run a long and complex pipeline.</p>

<p>How can I make the doc2vec model compatible with Python 3?</p>
","2243972","","2243972","","2016-07-22 06:59:00","2016-07-22 06:59:00","Doc2Vec model Python 3 compatibility","<python><python-3.x><pickle><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"47533772","1","47582012","","2017-11-28 14:18:29","","1","2423","<p>I would like to call <code>model.wv.most_similar_cosmul</code>, on the same copy of <code>model</code> object, using <code>multiple cores</code>, on <code>batches of input pairs</code>.</p>

<p>The <code>multiprocessing</code> module requires multiple copies of <code>model</code>, which will require too much RAM because my <code>model</code> is 30+ GB in RAM.</p>

<p>I have tried to evaluate my query pairs. It took me ~12 hours for the first round. There may be more rounds coming. That's why I am looking for a threading solution. I understand Python has <code>Global Interpreter Lock</code> issue.</p>

<p>Any suggestions?</p>
","2593536","","","","","2021-07-30 21:36:08","Gensim word2vec / doc2vec multi-threading parallel queries","<python><multithreading><word2vec><gensim><doc2vec>","2","4","1","","","CC BY-SA 3.0"
"53621737","1","53624970","","2018-12-04 21:31:56","","6","4919","<p>How is it possible to retrieve the n most frequent words from a Gensim <code>word2vec</code> model? As I understand, the frequency and count are not the same, and I therefore can't use the <code>object.count()</code> method.  </p>

<p>I need to produce a list of the n most frequent words from my <code>word2vec</code> model. </p>

<p>Edit: </p>

<p>I've tried the following: </p>

<pre class=""lang-py prettyprint-override""><code>w2c = dict()
for item in model.wv.vocab:
   w2c[item]=model.wv.vocab[item].count
w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))
w2cSortedList = list(w2cSorted.keys())
</code></pre>

<p>My initial guess was to use code above, but this implements the count method. I'm not sure if this represents the most frequent words.</p>
","8269637","","8430155","","2020-03-05 20:56:33","2021-07-09 15:08:18","Gensim (word2vec) retrieve n most frequent words","<gensim>","1","1","","","","CC BY-SA 4.0"
"36132650","1","","","2016-03-21 13:42:51","","1","119","<p>I have a large corpus of texts represented as list of texts:</p>

<p>[text1, text2, ..., textn]</p>

<p>I also have a list of names of these texts:</p>

<p>[text1_name, text2_name, ..., textn_name]</p>

<p>How do I turn such data into Mallet Corpus?
So that it could be possible to use LDA from gensim?</p>

<pre><code>from gensim.models.ldamulticore import LdaMulticore
corpus = gensim.corpora.MalletCorpus( **what_should_stay_here??** )
lda = LdaMulticore(corpus, workers = -1)
</code></pre>
","5550721","","5550721","","2016-03-21 14:26:16","2016-03-21 14:26:16","How to transform list of texts into Mallet Corpus?","<python><nlp><lda><gensim><mallet>","0","0","","","","CC BY-SA 3.0"
"44957082","1","","","2017-07-06 19:10:13","","1","523","<p>What is Spacy's built in method of creating vector representations?
I performed NLP on my corpus, and then used .similarity (cosine similarity) to map out documents that were ""similar"". However, I am unsure what method spacy uses to create vector representations. To my knowledge, I am thinking that it is probably word2vec skip-gram with negative sampling, however, I would like to be sure!</p>
","8249328","","","","","2017-07-07 10:37:54","What does Spacy use to create vector representations?","<python><nlp><gensim><word2vec><spacy>","1","1","","","","CC BY-SA 3.0"
"50340657","1","","","2018-05-15 00:12:49","","6","5438","<p>is it possible to plot a pyLDAvis with a Mallet implementation of LDA ? I have no troubles with LDA_Model but when I use Mallet I get :</p>

<pre><code>'LdaMallet' object has no attribute 'inference'
</code></pre>

<p>My code :</p>

<pre><code>pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(mallet_model, corpus, id2word)
vis
</code></pre>
","4285400","","","","","2021-04-27 08:36:40","pyLDAvis with Mallet LDA implementation : LdaMallet object has no attribute 'inference'","<gensim><topic-modeling><mallet>","3","0","1","","","CC BY-SA 4.0"
"49429971","1","49468226","","2018-03-22 13:29:27","","2","884","<p>I am new to Gensim, and I am trying to load my given (pre-trained) Word2vec model. I have 2 files: <em>xxxx.model.wv</em> and a bigger one <em>xxxx.model.wv.syn0.npy</em>.</p>

<p>When I call the following line:</p>

<pre><code>gensim.models.Word2Vec.load('xxxx.model.wv')
</code></pre>

<p>I get the following error:</p>

<pre><code>AttributeError: 'EuclideanKeyedVectors' object has no attribute 'negative'
</code></pre>

<p>How can I solve this error?</p>
","9534923","","2675154","","2018-03-22 15:07:41","2020-09-15 01:42:30","How can I load Word2vec with Gensim without getting an AttributeError?","<python><word2vec><gensim><word-embedding>","2","2","","","","CC BY-SA 3.0"
"53618906","1","53621051","","2018-12-04 17:59:28","","0","982","<p>I am trying to apply the word2vec model implemented in the library gensim 3.6 in python 3.7, Windows 10 machine. I have a list of sentences (each sentences is a list of words) as an input to the model after performing preprocessing.</p>

<p>I have computed the results (obtaining 10 most similar words of a given input word using <code>model.wv.most_similar</code>) in <code>Anaconda's Spyder</code> followed by <code>Sublime Text</code> editor. </p>

<p>But, I am getting different results for the same source code executed in two editors.</p>

<p>Which result should <strong>I need to choose and Why</strong>?</p>

<p>I am specifying the screenshot of the results obtained by running the same code in both spyder and sublime text. The input word for which I need to obtain 10 most similar word is <code>#universe#</code></p>

<p>I am really confused how to choose the results, on what basis? Also, I have started learning Word2Vec recently.</p>

<p>Any suggestion is appreciated.</p>

<p>Results Obtained in Spyder:</p>

<p><a href=""https://i.stack.imgur.com/v87AW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v87AW.png"" alt=""enter image description here""></a></p>

<p>Results Obtained using Sublime Text:
<a href=""https://i.stack.imgur.com/JPN0X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JPN0X.png"" alt=""enter image description here""></a></p>
","3966705","","3966705","","2018-12-05 10:39:46","2018-12-05 10:39:46","Different results of Gensim Word2Vec Model in two editors for the same source code in same environment and platform?","<python><python-3.x><gensim><word2vec>","1","2","1","","","CC BY-SA 4.0"
"62339753","1","","","2020-06-12 07:42:45","","0","238","<p>I've trained an lda model using GuidedLDA (<a href=""https://guidedlda.readthedocs.io/"" rel=""nofollow noreferrer"">https://guidedlda.readthedocs.io/</a>). Is it possible to transfer this lda model into the gensim framework to continue working with it?</p>
","10672495","","","","","2021-05-13 09:56:10","Is it possible to load pre-trained lda model into gensim?","<python><nlp><gensim><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"32101795","1","32103815","","2015-08-19 17:17:41","","10","26616","<p>I'm getting an <code>AttributeError</code> while loading the gensim model available at word2vec repository:</p>

<pre><code>from gensim import models
w = models.Word2Vec()
w.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print w[""queen""]

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-3-8219e36ba1f6&gt; in &lt;module&gt;()
----&gt; 1 w[""queen""]

C:\Anaconda64\lib\site-packages\gensim\models\word2vec.pyc in __getitem__(self, word)
    761 
    762         """"""
--&gt; 763         return self.syn0[self.vocab[word].index]
    764 
    765 

AttributeError: 'Word2Vec' object has no attribute 'syn0'
</code></pre>

<p>Is this a known issue ?</p>
","179372","","","","","2019-06-18 10:09:56","Error while loading Word2Vec model in gensim","<python><gensim><word2vec>","3","0","1","","","CC BY-SA 3.0"
"49676060","1","49693289","","2018-04-05 15:21:59","","7","6641","<p>I am creating a chatbot. So, i need word2vec file in binary format.
When i am loading bin file then i am getting this type of error.</p>

<pre><code>import gensim

model = gensim.models.Word2Vec.load('GoogleNews-vectors-negative300.bin')

Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 975, in load
return super(Word2Vec, cls).load(*args, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 629, in load
model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 278, in load
return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 395, in load
obj = unpickle(fname)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 1302, in unpickle
return _pickle.load(f, encoding='latin1')_pickle.

UnpicklingError: invalid load key, '3'.
</code></pre>
","6620822","","4685471","","2018-04-05 15:39:30","2018-04-06 12:53:58","UnpicklingError: invalid load key, '3'","<python-3.x><word2vec><gensim>","1","1","","","","CC BY-SA 3.0"
"49699065","1","","","2018-04-06 18:33:14","","0","586","<p>I am recieving the following error: when calling gensim.models.Word2Vec() on a corpus object that is iteratable.</p>

<pre><code>File ""/anaconda/envs/py36/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 542, in __init__
        self.build_vocab(sentences, trim_rule=trim_rule)
      File ""/anaconda/envs/py36/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 621, in build_vocab
        self.finalize_vocab(update=update)  # build tables &amp; arrays
      File ""/anaconda/envs/py36/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 845, in finalize_vocab
        self.reset_weights()
      File ""/anaconda/envs/py36/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 1270, in reset_weights
        self.wv.syn0[i] = self.seeded_vector(self.wv.index2word[i] + str(self.seed))
    TypeError: can only concatenate tuple (not ""str"") to tuple
</code></pre>

<p>The corpus is defined like so:</p>

<pre><code>class DataSet:
    """"""
    Holds the dataset and the methods associated with it
    """"""

    def __init__(self, dir, verbose, categories):
        self.dir = dir
        self.verbose = verbose
        self.dictionary = None
        self.categories = categories
        self.type = None


    @staticmethod
    def iter_documents():
        """"""
        Generator: iterate over all relevant documents
        :return: yields one document (=list of utf8 tokens) at a time
        """"""
        for root, dirs, files in os.walk(DIR_PROCESSED):
            for fname in filter(lambda fname: fname.endswith('.txt'), files):
                document = open(os.path.join(root, fname)).read()
                yield gensim.utils.tokenize(document, errors='ignore')

    def __iter__(self):
        """"""
        __iter__ is a generator =&gt; Dataset is a streamed iterable
        :return: sparse dictionary
        """"""
        for tokens in DataSet.iter_documents():
            yield self.dictionary.doc2bow(tokens)
</code></pre>

<p>With a subclass XMLDataset: that contains a static method</p>

<p>It is an instance of XMLDataset that is passed as a corpus to <code>gensim.models.Word2Vec()</code></p>

<p>What could the issue be?</p>

<p>EDIT:</p>

<p>Dictionary is updated like so:</p>

<pre><code>self.dictionary = gensim.corpora.Dictionary(DataSet.iter_documents())
</code></pre>

<p>Then gensim is called on the dataset:   </p>

<pre><code>corpus = DataSet() #roughly
model = gensim.models.Word2Vec(corpus, size=dim, window=5, workers=workers)  # mincount
</code></pre>

<p>If I were to iterate through corpus and print each element, I get the output needed for train the model, like so:</p>

<pre><code>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 3), (18, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 2), (28, 1), (29, 2), (30, 1), (31, 2), (32, 2), (33, 2), (34, 5), (35, 1), (36, 2), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 2), (51, 1), (52, 2), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 2), (64, 1), (65, 1), (66, 1), (67, 2), (68, 1), (69, 2), (70, 1), (71, 1), (72, 1), (73, 2), (74, 1), (75, 2), (76, 1), (77, 2), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 2), (87, 1), (88, 2), (89, 1), (90, 3), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 3), (97, 2), (98, 1)]
</code></pre>
","6566268","","6566268","","2018-04-11 23:15:49","2018-04-11 23:15:49","Gensim TypeError: can only concatenate tuple (not ""str"") to tuple","<python><gensim>","0","2","1","","","CC BY-SA 3.0"
"66403628","1","66425772","","2021-02-27 21:26:18","","0","271","<p>I have performed some topic modelling using <em>gensim.models.ldamodel.LdaModel()</em> and I want to label my data, to visualize my findings.</p>
<p><strong>This is what I have so far:</strong></p>
<p>My current dataframe has the following columns:</p>
<pre><code>['text']['date']['gender']['tokens']['topics']['main_topic']
    
</code></pre>
<p>Text is just the pure textdata, date has the form (yyyy-mm-dd), gender is binary with female being 1, tokens is the text after preprocessing, topics is derived from:</p>
<pre><code>df['topics'] = LDA_model.get_document_topics(corpus)
</code></pre>
<p>and main_topic is a little change from the second answer from this <a href=""https://stackoverflow.com/questions/39969919/gensim-lda-topic-assignment"">post</a> and is populated like this:</p>
<pre><code>df['main_topic'] = [int(str(sorted(LDA_model[i],reverse=True,key=lambda x: x[1])[0][0]).zfill(3)) for i in corpus]
</code></pre>
<p>Finally, the first 10 rows of topics and main_topics look like this (notice that num_topics=30):</p>
<pre><code>    topics  main_topic
[(0, 0.051341455), (1, 0.21204428), (2, 0.1145254), (4, 0.055585753), (11, 0.20260869), (29, 0.25616828)]   29
[(0, 0.052005265), (1, 0.21128647), (2, 0.08015486), (3, 0.11465485), (29, 0.4478401)]  29
[(0, 0.05355798), (1, 0.1394092), (2, 0.10734849), (4, 0.32699445), (29, 0.273105)] 4
[(0, 0.053568278), (1, 0.22299954), (2, 0.22616898), (11, 0.0959242), (29, 0.2897638)]  29
[(0, 0.05404401), (1, 0.4482777), (4, 0.141311), (29, 0.24849494)]  1
[(0, 0.054245334), (1, 0.18933308), (2, 0.14567153), (4, 0.11169399), (23, 0.05768766), (29, 0.35825193)]   29
[(0, 0.05449035), (2, 0.114870586), (4, 0.13284092), (11, 0.075592585), (23, 0.13247918), (24, 0.06598773), (29, 0.32016253)]   29
[(0, 0.055871632), (1, 0.23100668), (4, 0.06832383), (29, 0.4730603)]   29
[(0, 0.057746172), (1, 0.057121024), (2, 0.07247137), (3, 0.26388222), (13, 0.07291462), (29, 0.34331965)]  29
[(0, 0.057841185), (1, 0.19891246), (2, 0.09586754), (29, 0.5344914)]   29
</code></pre>
<p><strong>Now what I want is:</strong></p>
<p>I want 30 new columns: &quot;topic 0, topic 1, topic 2,..., topic 29&quot;. And for the first row I want to use df['topics'] and save the values in the new columns so that:</p>
<p>topic 0 in row 1 = 0.0513414, topic 1 in row 1 = 0.21204, topic 2 in row 1 = 0.11452 and topic 3 in row 1 = 0, and so on.</p>
<p>But I dont know how. Can someone help?</p>
","13324356","","","","","2021-03-01 16:20:45","How to change Topic list (from gensim lda get_document_topics()) to a DataFrame format","<python><pandas><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"66919011","1","","","2021-04-02 12:01:09","","0","19","<p>To use WMD we need some word embeddings. For this example a pre-trained embedding provided by Gensim '<strong>word2vec-google-news-300</strong>' is used.</p>
<p>Below is code snippet:</p>
<pre><code>import gensim.downloader as api
model = api.load('word2vec-google-news-300')
distance = model.wmdistance(sent1, sent2)
</code></pre>
<p>How can I use my own customised embedding in place of that ,How can I load that in model?
for ex: Embedding somewhat looks like - {text:1-D NumPy array}</p>
","14588183","","","","","2021-04-02 12:01:09","How can we use our own customised embedding with WordMoverDistance?","<stanford-nlp><gensim><word2vec><wmd>","0","3","","","","CC BY-SA 4.0"
"49768453","1","49771786","","2018-04-11 06:58:06","","0","44","<p>I have names of employees saved in a text file. I processed the file and compared a name that already exist.
When I checked using most_similar method, I found that it returns totally unrelated name even if the exact same name exist in the corpus.</p>

<pre><code>import gensim

training_file='todel.txt'
mylist=list()
with open(training_file, encoding=""iso-8859-1"") as f:
    for i, line in enumerate(f):
        mylist.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]))

model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=55)
model.build_vocab(mylist)

inferred_vector=model.infer_vector(['aakash', 'prakash', 'patel'])

sims = model.docvecs.most_similar([inferred_vector])

' '.join(mylist[sims[0][0]].words)
</code></pre>

<p>How do I correctly train the data to return (closely) matching names?</p>
","139150","","","","","2018-04-11 09:46:18","Using a model to compare name and surname","<machine-learning><gensim>","1","2","","","","CC BY-SA 3.0"
"49761033","1","49877968","","2018-04-10 18:53:50","","1","564","<p>I seem to be getting all the correct results until the very last step. My array of results keeps coming back empty. </p>

<p>I'm trying to follow this tutorial to compare 6 sets of notes:</p>

<p><a href=""https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python"" rel=""nofollow noreferrer"">https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python</a></p>

<p>I have this so far:</p>

<pre><code>#tokenize an array of all text
raw_docs = [Notes_0, Notes_1, Notes_2, Notes_3, Notes_4, Notes_5]
gen_docs = [[w.lower() for w in word_tokenize(text)]
           for text in raw_docs]

#create dictionary
dictionary_interactions = gensim.corpora.Dictionary(gen_docs)
print(""Number of words in dictionary: "", len(dictionary_interactions))
#create a corpus
corpus_interactions = [dictionary_interactions.doc2bow(gen_docs) for gen_docs in gen_docs]
len(corpus_interactions)
#convert to tf-idf model
tf_idf_interactions = gensim.models.TfidfModel(corpus_interactions)
#check for similarities between docs
sims_interactions = gensim.similarities.Similarity('C:/Users/JNproject', tf_idf_interactions[corpus_interactions],
                               num_features = len(dictionary_interactions))

print(sims_interactions)
print(type(sims_interactions))
</code></pre>

<p>with the output:</p>

<pre><code>Number of words in dictionary:  46364
Similarity index with 6 documents in 0 shards (stored under C:/Users/Jeremy Bice/JNprojects/Company/Interactions/sim_interactions)
&lt;class 'gensim.similarities.docsim.Similarity'&gt;
</code></pre>

<p>That seems right so I continue with this:</p>

<pre><code>query_doc = [w.lower() for w in word_tokenize(""client is"")]
print(query_doc)
query_doc_bow = dictionary_interactions.doc2bow(query_doc)
print(query_doc_bow)
query_doc_tf_idf = tf_idf_interactions[query_doc_bow]
print(query_doc_tf_idf)

#check for similarities between docs
sims_interactions[query_doc_tf_idf]
</code></pre>

<p>and my output is this:</p>

<pre><code>['client', 'is']
[(335, 1), (757, 1)]
[]
array([ 0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)
</code></pre>

<p>How do I get an output here?</p>
","8922328","","","","","2018-04-17 12:19:20","gensim.similarities.docsim.Similarity returns empty when queried","<python-3.x><nltk><jupyter-notebook><gensim>","1","0","","","","CC BY-SA 3.0"
"49784513","1","","","2018-04-11 21:09:56","","2","3421","<p>I looked for all suggestion, where everyone says to break the string into tokens by split function. All that has been done already, but still it seems to have same error again and again.</p>

<pre><code>for r in words:
        if not r in stop_words:
            processed_txt+=str(str(ps.stem(r) + "" ""))
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(processed_txt)
    #print(tokens)
    dictionary = corpora.Dictionary(tokens)
    #corpus = [dictionary.doc2bow(text) for text in tokens]
    print(dictionary)
</code></pre>

<p>So now it gives below error.</p>

<pre><code>raise TypeError(""doc2bow expects an array of unicode tokens on input, not a 
single string"")
TypeError: doc2bow expects an array of unicode tokens on input, not a single 
string
</code></pre>

<p>and the output under ""tokens"" variable seems like this as below.</p>

<pre><code>['becom', 'effect', 'willingli', 'without', 'need', 'obtain', 'knowledg', 'other', 'obtain', 'acquir', 'must', 'testamentari','claim', 'ownership', 'task', 'establish', 'endow', 'recept', 'willing', 'willsend', 'anoth', 'given', 'efficaci', 'presuppos']
</code></pre>

<p>Please help.</p>
","2669383","","","","","2018-04-11 21:09:56","TypeError: doc2bow expects an array of unicode tokens on input, not a single string","<python><tokenize><gensim><corpus>","0","4","","2018-04-13 09:56:11","","CC BY-SA 3.0"
"49800622","1","54932180","","2018-04-12 15:36:01","","1","3537","<p>I am running the latest version of Python:</p>

<pre><code>'3.6.5 |Anaconda custom (64-bit)| (default, Mar 29 2018, 13:14:23) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]'
</code></pre>

<p>Upon trying to import gensim like so:</p>

<pre><code>from gensim.corpora import Dictionary
import numpy as np
</code></pre>

<p>I get the following error:</p>

<pre><code>/anaconda/lib/python3.6/site-packages/boto/provider.py in &lt;module&gt;()
     32 
     33 import boto
---&gt; 34 from boto import config
     35 from boto.compat import expanduser
     36 from boto.pyami.config import Config

ImportError: cannot import name 'config'
</code></pre>

<p>I have tried updating Python, all of the packages and their dependencies, and so on.  Nothing seems to be working.</p>

<p>Any thoughts?</p>
","372526","","","","","2019-02-28 18:37:08","Python 3.6: ImportError: cannot import name 'config' when trying to import gensim","<python><jupyter><boto><gensim>","1","2","","","","CC BY-SA 3.0"
"40606524","1","","","2016-11-15 09:42:03","","1","807","<p>I want to find N words with greatest TFIDF value for each document</p>

<p>I'm using gensim with a corpus to compute tfidf values:</p>

<pre><code>tfidf = models.TfidfModel(mmcorpus)
corpus_tfidf = tfidf[mmcorpus]
</code></pre>

<p>Then, I would like to get tf-idf of given words in each document of the corpus
By looking in my corpus_tfidf object, I see a corpus part with an index array of size 417 (which is the size of my corpus), but I doesn't find documentation to help me to get TFIDF of a word in a document of the corpus
Note: the answer of the question ""Getting TF-IDF Scores Of Words Using Gensim"" doesn't really solves the problem; it gives an unique value for each word</p>

<p>===EDIT===</p>

<p>From other sample codes, I've tried with success the following code (surely not as Pythonic than it can be): </p>

<pre><code>for doc in corpus_tfidf: 
    d = {} 
    for id, value in doc: 
        if value&gt;0: 
            d[corpus.dictionary.get(id)] = value
</code></pre>

<p>but I can't explain or document why I can do 'for doc in corpus_tfidf'and get something useful, corpus_tfidf is not clearly iterable and as it is, what define the objects obtained from each iteration?
Then, a subsequent question is: how can I find the source associated with the doc variable? I will create a separate question about that</p>
","5485570","","4141980","","2016-11-15 16:04:53","2016-11-15 16:04:53","Access to tfidf values in gensim","<python><tf-idf><gensim><corpus>","0","0","","","","CC BY-SA 3.0"
"62244610","1","","","2020-06-07 11:28:17","","3","248","<p>I try to use parallelism with word2vec implemented in the gensim library. I notice that, more I increase of threads, more the training is slow and I don't know why.
Are there any settings to make?
I use :
- debian
- python 3.6.9
- cython 
how can i benefit parallelism ?</p>

<p>Thanks for advance</p>
","13699553","","","","","2020-06-08 16:45:22","parallelization not benefit for training word2vec model","<parallel-processing><nlp><gensim><word2vec>","1","2","","","","CC BY-SA 4.0"
"49681525","1","","","2018-04-05 21:06:37","","1","651","<p>I have around a 1000 documents, and I have trained them using gensim's doc2vec class. I need all the 1000 docvecs from the model to perform a kmeans clstering. But the maximum docvecs I'm able to get is 10. Any idea, how to get all of them?. Below is my code snippet.</p>

<p>`</p>

<pre><code>tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()),tags=str(i)) for i, _d in enumerate(data)]

max_epochs = 100
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.025,
                min_count=1,
                dm =1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

X= model.docvecs.doctag_syn0 

print(X)

true_k = 3
km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)
km.fit(X)

clusters = km.labels_.tolist()
</code></pre>

<p>`</p>
","9530913","","","","","2018-04-06 08:57:47","How to get all docvecs of a model","<python><k-means><gensim><doc2vec>","1","0","0","","","CC BY-SA 3.0"
"49811018","1","","","2018-04-13 06:56:51","","0","164","<p>i have two separate data sets, one is resumes and the other is demands, using gensim doc2vec, i created models for each and i am able to query similar words in each data sets, but now, i need to merge these two models into one and query for resumes in demands and attain the similarity or matching between them. My data sets are in plain txt files in which the the two resumes or demands are separated by * . Please find my implementation below, any suggestions would be highly appreciated.
Thanks.</p>

<pre><code>import gensim
import os
import collections
import smart_open
import random


def read_corpus(fname, tokens_only=False):

    with open(fname) as f:
      i=0
      for  line in (f.read().split('&amp;&amp;')):
        if len(line)&gt;1:
            if tokens_only:
                yield gensim.utils.simple_preprocess(line)
            else:
                # For training data, add tags
                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])
            i+=1    


vocabulary = read_corpus('D:\Demand.txt')
train_corpus = list(vocabulary)
print(train_corpus[:2])

model = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2, iter=55)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)
print(model.infer_vector(['trainings', 'certifications', 'analyst', 'unix', 'jdbc','testing']))
model.docvecs.most_similar(positive=[model.infer_vector(['spark', 'sqoop'])])
model.most_similar('unix')
</code></pre>
","7234970","","7234970","","2018-04-13 09:06:24","2018-04-13 09:06:24","matching between two separate documents using gensim doc2vec","<gensim><doc2vec>","0","5","","","","CC BY-SA 3.0"
"32251047","1","","","2015-08-27 13:42:51","","2","1608","<p>Have a hell of a blocker trying to use Gensim's doc2vec.</p>

<p>I import gensim.models.doc2vec.Doc2Vec and successfully train it on a set of tweets. I am able to pull my document vectors fine, using model['DOC_[0123..]''. </p>

<p>My issue now is that I'm trying to get a vector representation for a <em>new, unseen document</em> so that I can feed that vector back into a classifier. As far as I know, the only method that exists to do this with doc2vec is <code>infer_vector()</code>. </p>

<p>HOWEVER, when I try to call this method, I get the following: </p>

<p><em>AttributeError: 'Doc2Vec' object has no attribute 'infer_vector'</em></p>

<p>I'm able to use all the other methods described in the doc2vec documentation: <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">https://radimrehurek.com/gensim/models/doc2vec.html</a></p>

<p>I've tried using different versions of gensim including 0.10.3 (the version released with doc2vec || <a href=""http://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow"">http://rare-technologies.com/doc2vec-tutorial/</a>) and the 0.13.1 (latest version). </p>

<p>PLEASE HELP.</p>
","5273082","","4370109","","2015-11-05 19:59:39","2015-11-05 19:59:39","Gensim doc2vec infer_vector method missing","<python><machine-learning><gensim>","1","0","1","","","CC BY-SA 3.0"
"32276734","1","","","2015-08-28 17:35:07","","3","2925","<p>I am trying to run word2vec (skip-gram model implemented in gensim with a default window size of 5) on a corpus of .txt files. The iterator that I use looks something like this: </p>

<pre><code>class Corpus(object):
    """"""Iterator for feeding sentences to word2vec""""""
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):

        word_tokenizer = TreebankWordTokenizer()
        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
        text = ''

        for root, dirs, files in os.walk(self.dirname):

            for file in files:

                if file.endswith("".txt""):

                    file_path = os.path.join(root, file)


                    with open(file_path, 'r') as f:

                         text = f.read().decode('utf-8')
                         sentences = sent_tokenizer.tokenize(text)

                         for sent in sentences:
                             yield word_tokenizer.tokenize(sent)
</code></pre>

<p>Here I use the punkt tokenizer (which uses an unsupervised algorithm for detecting sentence boundaries) in the nltk package for splitting the text into sentences. However, when I replace this with just a simple <code>line.split()</code> i.e just considering each sentence as one line and splitting the words, I get a time efficiency that is 1.5 times faster than using the nltk parser. The code inside the 'with open' looks something like this: </p>

<pre><code>                 with open(file_path, 'r') as f:
                    for line in f:
                    line.decode('utf-8')
                    yield line.split()
</code></pre>

<p>My question is how important is it for the word2vec algorithm to be fed sentences that are actual sentences (something that I attempt to do with punkt tokenizer)? Is it sufficient for each word in the algorithm to receive a context of the surrounding words that lie on one line  (these words may not necessarily be an actual sentence in the case of a sentence spanning several lines) as opposed to the context of words that the word may have in a sentence spanning several lines. Also, what sort of a part does window size play in this. When a window size is set to 5 for example, does the size of sentences yielded by the Sentences iterator ceases to play a part? Will only window size decide the context words then? In that case should I just use <code>line.split()</code> instead of trying to detect actual sentence boundaries using the punkt tokenizer? </p>

<p>I hope I have been able to describe the issue sufficiently, I would really appreciate any opinions or pointers or help regarding this.</p>
","730314","","","","","2017-01-31 10:01:11","relationship between window size and the actual sentence length in word2vec","<gensim><word2vec>","1","3","0","","","CC BY-SA 3.0"
"49643974","1","","","2018-04-04 06:10:35","","10","19460","<p>I want to perform text classification using word2vec.
I got vectors of words.</p>

<pre><code>ls = []
sentences = lines.split(""."")
for i in sentences:
    ls.append(i.split())
model = Word2Vec(ls, min_count=1, size = 4)
words = list(model.wv.vocab)
print(words)
vectors = []
for word in words:
    vectors.append(model[word].tolist())
data = np.array(vectors)
data
</code></pre>

<p>output:</p>

<pre><code>array([[ 0.00933912,  0.07960335, -0.04559333,  0.10600036],
       [ 0.10576613,  0.07267512, -0.10718666, -0.00804013],
       [ 0.09459028, -0.09901826, -0.07074171, -0.12022413],
       [-0.09893986,  0.01500741, -0.04796079, -0.04447284],
       [ 0.04403428, -0.07966098, -0.06460238, -0.07369237],
       [ 0.09352681, -0.03864434, -0.01743148,  0.11251986],.....])
</code></pre>

<p>How can i perform classification (product &amp; non product)?</p>
","9591443","","","","","2020-10-03 09:56:02","How to do Text classification using word2vec","<python-3.x><word2vec><gensim><text-classification>","2","0","2","","","CC BY-SA 3.0"
"32170261","1","","","2015-08-23 18:52:04","","0","70","<p>I'm new to the world of word2vec and I just start to use gensim's implementation for word2vec.</p>

<p>I use two naive sentences as my first document set, </p>

<pre><code>[['first', 'sentence'], ['second', 'sentence']]
</code></pre>

<p>The vectors I get are like this:</p>

<pre><code>'first', -0.07386458, -0.17405555
'second', 0.0761444 , -0.21217766
'sentence', 0.0545655 , -0.07535963
</code></pre>

<p>However, when I type in another toy document sets:</p>

<pre><code>[['a', 'c'], ['b', 'c']]
</code></pre>

<p>I get the following result:</p>

<pre><code>'a', 0.02936198, -0.05837455
'b', -0.05362414, -0.06813956
'c', 0.11918657, -0.10411404
</code></pre>

<p>Again, I'm new to word2vec but according to my understanding,
my two document sets are structurally identical, so the results of the corresponding word should be the same.
But why I'm getting different results?
Is the algorithm always giving probalistic output or the document sets too small?</p>

<p>The function I used is as the following:</p>

<pre><code>model = word2vec.Word2Vec(sentences, size=2, min_count=1, window=2)
</code></pre>
","4480030","","4480030","","2015-08-23 18:57:54","2015-08-24 12:18:38","word2vec's probalistic output","<gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"49829787","1","49837045","","2018-04-14 09:10:00","","2","202","<p>I created a model from mongodb db news and I tagged the documents by mongo collection id</p>

<pre><code>from gensim.models.doc2vec import TaggedDocument
i=0
docs=[]
for artical in lstcontent:
    doct = TaggedDocument(clean_str(artical), [lstids[i]])
    docs.append(doct)
    i+=1
</code></pre>

<p>after that I created the model by</p>

<pre><code>pretrained_emb='tweet_cbow_300/tweets_cbow_300'
saved_path = ""documentmodel/doc2vec_model.bin""
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, pretrained_emb=pretrained_emb, iter=train_epoch)
model.save(saved_path)
</code></pre>

<p>when I using the model by the code :</p>

<pre><code>import gensim.models as g
import codecs
model=""documentmodel/doc2vec_model.bin""
start_alpha=0.01
infer_epoch=1000
m = g.Doc2Vec.load(model)
sims = m.docvecs.most_similar(['5aa94578094b4051695eeb10'])
sims
</code></pre>

<p>the output is</p>

<pre><code>[('5aa944c1094b4051695eeaef', 0.9255372881889343),
('5aa945c1094b4051695eeb1d', 0.9222575426101685),
('5aa94584094b4051695eeb12', 0.9210859537124634),
('5aa945d2094b4051695eeb20', 0.9083569049835205),
('5aa945c7094b4051695eeb1e', 0.905883252620697),
('5aa9458f094b4051695eeb14', 0.9054019451141357),
('5aa944c7094b4051695eeaf0', 0.9019848108291626),
('5aa94589094b4051695eeb13', 0.9012798070907593),
('5aa945b1094b4051695eeb1a', 0.9000773429870605),
('5aa945bc094b4051695eeb1c', 0.8999895453453064)]
</code></pre>

<p>the ids not related with 5aa94578094b4051695eeb10
where is my proplem !?</p>
","9644854","","","","","2018-04-14 23:14:25","gensim model return ids not related with input doc2vec","<word2vec><gensim><cosine-similarity><doc2vec>","1","1","1","","","CC BY-SA 3.0"
"49710537","1","49802495","","2018-04-07 18:21:06","","45","37913","<p>I want to load a pre-trained word2vec embedding with gensim into a PyTorch embedding layer.</p>

<p>So my question is, how do I get the embedding weights loaded by gensim into the PyTorch embedding layer.</p>

<p>Thanks in Advance!</p>
","7483494","","7483494","","2018-08-10 12:26:52","2020-04-03 08:18:38","PyTorch / Gensim - How to load pre-trained word embeddings","<python><neural-network><pytorch><gensim><embedding>","6","0","19","","","CC BY-SA 3.0"
"66355819","1","66356567","","2021-02-24 17:24:33","","0","21","<p>When training a doc2vec model using a corpus in the <code>TaggedDocument</code> class, you can provide a list of tags. When the doc2vec model is trained it learns a vector representation for the tags. For example you could have one tag representing the document, and another representing some classification that can be shared between documents.</p>
<p>How would one provide additional tags when streaming a corpus using <code>TaggedLineDocument</code>?</p>
","9318767","","","","","2021-02-24 18:13:32","Can you provide additional tags for documents using TaggedLineDocument?","<gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"49750112","1","49750326","","2018-04-10 09:30:19","","9","8218","<p>I have a text file with my precomputed word vectors in the following format (example):</p>

<p><code>word -0.0762464299711 0.0128308048976 ... 0.0712385589283\n‚Äô</code></p>

<p>on each line for every word (with 297 extra floats in place of the <code>...</code>). I am trying to load these with Gensim as KeyedVectors, because I ultimately would like to compute the cosine similarity, find most similar words, etc. Unfortunately I have not worked with Gensim before and from the documentation it's not quite clear to me how to do this. I have tried the following which I found <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""noreferrer"">here</a>:</p>

<p><code>word_vectors = KeyedVectors.load_word2vec_format('/embeddings/word.vectors', binary=False)</code></p>

<p>However this gives the following error:</p>

<p><code>ValueError: invalid literal for int() with base 10: 'the'</code></p>

<p>'the' is the first word in the text file, so I suspect that the loading function is expecting something to be there that is not. But I can't find any information on what should be there. I would highly appreciate a pointer to such information or any other solution to my problem. Thanks!</p>
","7175411","","","","","2018-04-10 09:40:47","Gensim: how to load precomputed word vectors from text file","<python><python-3.x><nlp><gensim>","1","0","1","","","CC BY-SA 3.0"
"57762713","1","57763298","","2019-09-02 21:11:33","","1","933","<p>I have two lists:</p>

<pre><code>list_1 = [['flavor', 'flavors', 'fruity_flavor', 'taste'],
          ['scent', 'scents', 'aroma', 'smell', 'odor'],
          ['mental_illness', 'mental_disorders','bipolar_disorder']
          ['romance', 'romances', 'romantic', 'budding_romance']]

list_2 = [['love', 'eating', 'spicy', 'hand', 'pulled', 'noodles'],
          ['also', 'like', 'buy', 'perfumes'],
          ['suffer', 'from', 'clinical', 'depression'],
          ['really', 'love', 'my', 'wife']]
</code></pre>

<p>I would like to compute the cosine similarity between the two lists above in such a way where the cosine similarity between the first sub-list in list1 and all sublists of list 2 are measured against each other. Then the same thing but with the second sub-list in list 1 and all sub-lists in list 2, etc.</p>

<p>The goal is to create a <strong>len(list_2) by len(list_1) matrix</strong>, and each entry in that matrix is a cosine similarity score. Currently I've done this the following way:</p>

<pre><code>import gensim
import numpy as np
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True) 
similarity_mat = np.zeros([len(list_2), len(list_1)])

for i, L2 in enumerate(list_2):
    for j, L1 in enumerate(list_1):
        similarity_mat[i, j] = model.n_similarity(L2, L1)
</code></pre>

<p>However, I'd like to implement this with matrix multiplication and no for loops. </p>

<p>My two questions are:</p>

<ol>
<li>Is there a way to do some sort of element-wise matrix multiplication but with <code>gensim's n_similiarity() method</code> to generate the required matrix?</li>
<li>Would it be more efficient and faster using the current method or matrix multiplication?</li>
</ol>

<p>I hope my question was clear enough, please let me know if I can clarify even further.</p>
","9431573","","9431573","","2019-09-02 23:02:38","2019-09-02 23:49:10","Perform matrix multiplication with cosine similarity function","<python-3.x><numpy><gensim><word2vec><cosine-similarity>","2","0","","","","CC BY-SA 4.0"
"50361783","1","","","2018-05-16 02:49:54","","0","281","<p>I have a corpus of sentences. Each of them may contain marked compound words. 
For example: </p>

<blockquote>
  <p>This is an example_sentence followed by another awesome_paragraph</p>
</blockquote>

<p>.
I want to get embedding vector for all tokens and compound words</p>

<blockquote>
  <p>(this, is, an, example, sentence, followed, by, another, awesome,
  paragraph, example_sentence, awesome_paragraph)</p>
</blockquote>

<p>Can I do this with gensim or which library should I use?</p>
","9606019","","6121503","","2018-05-16 07:40:09","2018-05-16 07:40:09","Vector representation for token and compound word","<python><machine-learning><word2vec><gensim><doc2vec>","0","3","","","","CC BY-SA 4.0"
"32201795","1","32232681","","2015-08-25 10:38:47","","0","303","<p>how to remove these numbers from output of LDA while using Gensim package?</p>

<p>2015-08-25 15:26:20,439 : INFO : topic #8 (0.100): 0.038*watch + 0.020*water + 0.014*strap + 0.011*analog + 0.011*resistance + 0.010*atm + 0.010*coloured + 0.010*timepiece + 0.010*5 + 0.009*classy</p>

<p>so that output will be  watch,water,strap...etc</p>
","4799994","","","","","2015-08-26 17:11:39","how to remove numbers and symbols from output of LDA while using Gensim package?","<python><lda><gensim>","1","0","","","","CC BY-SA 3.0"
"57695150","1","57697496","","2019-08-28 14:53:10","","1","145","<p>I want to train part of the corpus first and then based on the embeddings train on the whole corpus. Can I achieve this with gensim skipgram?</p>

<p>I haven't found an API that can pass initial embeddings.</p>

<p>what I want is some thing like</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec
sentences = [[""cat"", ""say"", ""meow""], [""dog"", ""say"", ""woof""],
             [""cat2"", ""say2"", ""meow""], [""dog2"", ""say"", ""woof""]]
model = Word2Vec(sentences[:2], min_count=1)
X = #construct a new one
model = Word2Vec(sentences, min_count=1, initial_embedding=X)
</code></pre>
","11989895","","","","","2019-08-28 17:41:32","How can I use a pretrained embedding to gensim skipgram model?","<python><machine-learning><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"49847602","1","","","2018-04-15 22:59:31","","0","1146","<p>I was wondering how can I limited the Google's Word2Vec to my vocabulary.
Google's Word2 vec link:<a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing</a></p>

<p>This is what I have:</p>

<pre><code>import gensim

# Load Google's pre-trained Word2Vec model.
model = gensim.models.Word2Vec.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)

embedding_matrix = np.zeros((len(my_vocabulary), 300))
</code></pre>

<p>where my vocabulary is a list of unique words in my corpus. 
How can I feel the embedding matrix only for words in my_vocabulary?
In addition, I would like to have the flexibility that if my word does not exist in the Google's word2vec to be filled with zeros.</p>

<p>Thanks</p>
","9158409","","","","","2018-04-16 09:24:37","Adjust Google's Word2Vec loaded with Gensim to your vocabulary and then create the embedding vector","<python><word2vec><gensim><embedding>","2","0","","","","CC BY-SA 3.0"
"40865128","1","","","2016-11-29 11:46:08","","4","1746","<pre><code># -*- coding: utf-8 -*- 

'''
ÌïúÍµ≠Ïñ¥ ÌòïÌÉú Î∂ÑÏÑùÎêú ÏûêÎ£åÎ•º word2VecÎ™®Îç∏Î°ú ÎßåÎìúÎäî Î™®Îìà
ÏûÖÎ†•ÏùÄ ÌòïÌÉúÎ∂ÑÏÑùÎêú ÌååÏùºÏùÑ Ï∑®Ìï®
Ìïú Ï§ÑÏóê Ìïú Î¨∏Ïû•Ïî©
Ï∂úÎ†•ÏúºÎ°ú modelÏùÑ ÌòïÏÑ±  
'''

import codecs
import gensim
import multiprocessing
import word2vec

import sys
reload(sys)
sys.setdefaultencoding('utf8')

# Î™®Îç∏ configuration ÏÑ§Ï†ï 
config = {
   'min_count': 15,  # Îì±Ïû• ÌöüÏàòÍ∞Ä 15 Ïù¥ÌïòÏù∏ Îã®Ïñ¥Îäî Î¨¥Ïãú
   'size': 300,  # 300Ï∞®ÏõêÏßúÎ¶¨ Î≤°ÌÑ∞Ïä§ÌéòÏù¥Ïä§Ïóê embedding
   'sg': 1,  # 0Ïù¥Î©¥ CBOW, 1Ïù¥Î©¥ skip-gram
    'batch_words': 10000,  # ÏÇ¨Ï†ÑÏùÑ Íµ¨Ï∂ïÌï†Îïå ÌïúÎ≤àÏóê ÏùΩÏùÑ Îã®Ïñ¥ Ïàò
    'iter': 10,  # Î≥¥ÌÜµ Îî•Îü¨ÎãùÏóêÏÑú ÎßêÌïòÎäî epochÍ≥º ÎπÑÏä∑Ìïú, Î∞òÎ≥µ ÌöüÏàò
    'workers': multiprocessing.cpu_count(),
}

modelTwitterNoStop = gensim.models.Word2Vec(**config)

class SentenceReader:

    def __init__(self, filepath):
        self.filepath = filepath


    def __iter__(self):
         for line in codecs.open(self.filepath, encoding='utf-8'):
             yield line.split(' ')

# ÏÇ¨Ï†ÑÍ≥º ÌïôÏäµÏùÑ ÌòïÌÉúÎ∂ÑÏÑùÎêú ÌååÏùºÏùÑ Í∞ÄÏßÄÍ≥† ÌñâÌï®
sentences_vocab = SentenceReader('corpusAllNewsNoTagNoStop.txt')
sentences_train = SentenceReader('corpusAllNewsNoTagNoStop.txt')

#model = gensim.models.Word2Vec()
modelTwitterNoStop.build_vocab(sentences_vocab)
modelTwitterNoStop.train(sentences_train)

#Ïù¥Î†áÍ≤å ÌïôÏäµÎêú Î™®Îç∏ÏùÑ Ï†ÄÏû•
modelTwitterNoStop.save('modelTwitterNoStop')


### Î™®Îç∏ ÌÖåÏä§Ìä∏ ##########
#Îã§Ïùå ÎùºÏù∏Î∂ÄÌÑ∞Îäî Î∂ÑÎ¶¨ÌïòÏó¨ Î≥ÑÎèÑÎ°ú TestÎ™®Îç∏ ÌîÑÎ°úÍ∑∏Îû®ÏùÑ ÎßåÎì§Ïñ¥ Îã§ÏñëÌïú Í¥ÄÍ≥ÑÎ•º Ïã§ÌóòÌï¥ Î≥º Ïàò ÏûàÏùå

import codecs
import gensim
import multiprocessing

import sys
reload(sys)
sys.setdefaultencoding('utf8')

##ÎèÑ Ìè¨Ìï®


#ÎßåÎì§Ïñ¥ ÎÜìÏùÄ gensim Î™®Îç∏ÏùÑ Î°úÎî©
modelTwitterNoStop = gensim.models.Word2Vec.load('modelTwitterNoStop')


#most similar Test - ÏÉÅÏúÑ 10Í∞úÏùò Ïú†ÏÇ¨Ìïú Î≤°ÌÑ∞Î•º Ï∞æÏùå.. Ïã§Ï†úÎ°ú 'ÏùºÎ≥∏'Ïù¥ Í∞ÄÏû• ÎÜíÏùÄ    Í≤ÉÏúºÎ°ú ÎÇò
print ' '.join([""{}-{}"".format(word, value) for word, value in 
(modelTwitterNoStop.most_similar(positive=[u""ÌïúÍµ≠"", u""ÎèÑÏøÑ""], negative=[u""ÏÑú    Ïö∏""], topn=10))])

print ""\n""

# positiveÎßå ÏÇ¨Ïö©Ìï¥ÏÑúÎèÑ Ìï† Ïàò ÏûàÏúºÎ©∞ cosmulÏùÑ Ïù¥Ïö©
print ' '.join([""{}-{}"".format(word, value) for word, value in 
(modelTwitterNoStop.most_similar_cosmul(positive=[u""ÎπÑÏÑ†"", u""ÏµúÏàúÏã§""], topn=20))])
print ""\n""

#doesn't match Test
print modelTwitterNoStop.doesnt_match(u""Ï†ïÏú§Ìöå ÍπÄÏ¢Ö Î∞ïÍ∑ºÌòú ÏµúÏàúÏã§"".split())

print ""\n""

#similarity Test
print modelTwitterNoStop.similarity(u""ÎπÑÏÑ†"", u""Ï†ïÏú§Ìöå"")
print ""\n""

# no.of vocab.. in this model
print modelTwitterNoStop
print ""\n""
</code></pre>

<p>Warning (from warnings module):
  File ""C:\Python27\lib\site-packages\gensim\utils.py"", line 840
    warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
UserWarning: detected Windows; aliasing chunkize to chunkize_serial</p>
","6880543","","6880543","","2016-12-01 09:12:47","2017-08-10 12:23:28","Problems with gensim install","<python><gensim>","2","2","","","","CC BY-SA 3.0"
"40768132","1","","","2016-11-23 15:18:19","","1","368","<p>I have an already running python code of the <a href=""http://radimrehurek.com/gensim/simserver.html"" rel=""nofollow noreferrer"">document similarity server</a></p>

<p>The code runs fine from the commandline, however when I try to run from Jupyter notebook I get the following error (You can find the code below)</p>

<blockquote>
  <p>AttributeError Traceback (most recent call last)<br>
   in ()<br>
  ----> 1 simServer.queryIndex('National Intergroup Inc said it plans to file a registration statement')</p>
</blockquote>

<pre><code>&lt;ipython-input-2-81df834abc60&gt; in queryIndex(self, queryText)
     58         print ""Querying the INDEX""
     59         doc = {'tokens': utils.simple_preprocess(queryText)}
---&gt; 60         print(self.service.find_similar(doc, min_score=0.4, max_results=50))
</code></pre>

<p>At first I got a different error message where the solution was to install simserver library within jupyter notebook using the command <code>!pip install --upgrade simserver</code> .. but now I do not think there is a missing library that needs to be downloaded</p>

<p>Relevant code from jupyter notebook:</p>

<p>Line where the issue occurs</p>

<pre><code>simServer.queryIndex('National Intergroup Inc said it plans to file a registration statement')

#!/usr/bin/env python
import pickle
import os
import re
import glob
import pprint
import json

from gensim import utils
from simserver import SessionServer

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

class SimilarityServer(object):
    def __init__(self):
        print ""Openning sesesion and setting it to true""
        self.service = SessionServer('tmp/my_server/')
        self.service.open_session()
        self.service.set_autosession(True)

    def indexDocs(self):
        print ""Docs indexing and training""
        #train and index
        print ""Training""
        self.service.session.train(None,method='lsi',clear_buffer=False)
        print ""Indexing""
        self.service.session.index(None)

    def queryIndex(self,queryText):
        print ""Querying the INDEX""
        doc = {'tokens': utils.simple_preprocess(queryText)}
        print(self.service.find_similar(doc, min_score=0.4, max_results=50))

simServer = SimilarityServer()
simServer.queryIndex('National Intergroup Inc said it plans to file a registration statement')
</code></pre>
","1831518","","13302","","2016-12-06 06:33:41","2016-12-06 06:33:41","python code not running on Jupyter notebook","<python><jupyter-notebook><attributeerror><gensim>","0","3","","","","CC BY-SA 3.0"
"40840731","1","40842347","","2016-11-28 09:18:15","","2","4456","<p>Getting this error in python when trying to compute lda for a smaller size of corpus but works fine in other cases.</p>

<p>The size of corpus is 15 and I tried setting the number of topic to 5 then reduced it to 2 but it still gives the same error : <strong>ValueError: cannot compute LDA over an empty collection (no terms)</strong></p>

<p>getting error at this line :     <code>lda = models.LdaModel(corpus, num_topics=topic_number, id2word=dictionary, passes=passes)</code></p>

<p>where corpus is <code>corpus = [dictionary.doc2bow(text) for a, id, text, s_date, e_date, qd, qd_perc in texts]</code></p>

<p>Why is it giving no terms?</p>
","3903762","","3903762","","2018-03-18 07:14:06","2021-04-23 09:38:31","ValueError: cannot compute LDA over an empty collection (no terms)","<python><python-3.x><gensim><lda>","1","9","","","","CC BY-SA 3.0"
"40924185","1","","","2016-12-02 03:14:53","","3","2193","<p>I am trying to use LDA module of GenSim to do the following task</p>

<p>""Train a LDA model with one big document and keep track of 10 latent topics. Given a new, unseen document, predict probability distribution of 10 latent topics"".</p>

<p>As per tutorial here: <a href=""http://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">http://radimrehurek.com/gensim/tut2.html</a>, this seems possible for a document in a corpus, but I am wondering if it it would be possible for an unseen document. </p>

<p>Thank you!  </p>
","4802100","","","","","2016-12-02 19:12:19","Calculating topic distribution of an unseen document on GenSim","<python><nlp><gensim><lda>","1","0","1","","","CC BY-SA 3.0"
"40732313","1","","","2016-11-22 01:20:34","","8","5694","<p>I am trying to install gensim on a google cloud instance using:</p>

<blockquote>
  <p>pip3 install gensim</p>
</blockquote>

<p>and this is the stacktrace when I am trying to import gensim:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python3.4/dist-packages/gensim/__init__.py"", line 6, in &lt;module&gt;
    from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
  File ""/usr/local/lib/python3.4/dist-packages/gensim/models/__init__.py"", line 7, in &lt;module&gt;
    from .coherencemodel import CoherenceModel
  File ""/usr/local/lib/python3.4/dist-packages/gensim/models/coherencemodel.py"", line 30, in &lt;module&gt;
    from gensim.models.wrappers import LdaVowpalWabbit, LdaMallet
  File ""/usr/local/lib/python3.4/dist-packages/gensim/models/wrappers/__init__.py"", line 5, in &lt;module&gt;
    from .ldamallet import LdaMallet
  File ""/usr/local/lib/python3.4/dist-packages/gensim/models/wrappers/ldamallet.py"", line 43, in &lt;module&gt;
    from smart_open import smart_open
  File ""/usr/local/lib/python3.4/dist-packages/smart_open/__init__.py"", line 1, in &lt;module&gt;
    from .smart_open_lib import *
  File ""/usr/local/lib/python3.4/dist-packages/smart_open/smart_open_lib.py"", line 36, in &lt;module&gt;
    import boto.s3.key
  File ""/usr/local/lib/python3.4/dist-packages/boto/s3/key.py"", line 33, in &lt;module&gt;
    import boto.utils
  File ""/usr/local/lib/python3.4/dist-packages/boto/__init__.py"", line 1216, in &lt;module&gt;
    boto.plugin.load_plugins(config)
AttributeError: 'module' object has no attribute 'plugin'
</code></pre>

<p>This is the linux version (output of lsb_release -a):</p>

<pre><code>No LSB modules are available.
Distributor ID: Debian
Description:    Debian GNU/Linux 8.6 (jessie)
Release:    8.6
Codename:   jessie
</code></pre>

<p>and this is the output of</p>

<blockquote>
  <p>pip3 freeze</p>
</blockquote>

<pre><code>Cython==0.25.1
Flask==0.11.1
Jinja2==2.8
MarkupSafe==0.23
Pillow==2.6.1
Werkzeug==0.11.11
beautifulsoup4==4.3.2
boto==2.43.0
bz2file==0.98
chardet==2.3.0
click==6.6
colorama==0.3.2
decorator==3.4.0
gensim==0.13.3
html5lib==0.999
itsdangerous==0.24
lxml==3.4.0
matplotlib==1.4.2
nltk==3.2.1
nose==1.3.4
numexpr==2.4
numpy==1.11.2
pandas==0.14.1
pyparsing==2.0.3
python-apt==0.9.3.12
python-dateutil==2.2
pytz==2012c
requests==2.4.3
scipy==0.14.0
six==1.8.0
smart-open==1.3.5
stop-words==2015.2.23.1
tables==3.1.1
unattended-upgrades==0.1
urllib3==1.9.1
wheel==0.24.0
</code></pre>

<p>Can anybody give me pointers! This is very frustrating.</p>
","518245","","","","","2019-05-10 12:47:24","Gensim installation problems","<python><pip><gensim>","3","2","","","","CC BY-SA 3.0"
"40890226","1","","","2016-11-30 13:55:30","","1","115","<p>I've used gensim to train a Word2Vec model, and would like to query nearby terms. But instead of just getting the words that are closest in all directions:</p>

<pre><code>model = models.Word2Vec.load('MyModel')        # load up my trained model
nearest = model.most_similar(['mushroom'])     # nearby words all around
</code></pre>

<p>I want to move in a particular direction and distance within the vector space and retrieve the nearest word, essentially:</p>

<pre><code>nearest = nearest_by_vector(word, direction_vector)
</code></pre>

<p>My vector math is terrible (ie non-existent), especially with so many dimensions in my model.</p>
","1167783","","","","","2017-03-27 09:36:54","Move in Word2Vec vector space in specific direction","<python><machine-learning><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"41012760","1","","","2016-12-07 08:39:16","","0","441","<p>I am using gensim to construct an LSI corpus and then apply query similarity following gensim tutorials (<a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow noreferrer"">tut1</a>, <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">tut2</a> n <a href=""https://radimrehurek.com/gensim/tut3.html"" rel=""nofollow noreferrer"">tut3</a>) </p>

<p>My issue is that when I try to calcualte query similarity as shown in the code below I get the result in form of (docID, simScore) tuples.</p>

<p>I need to use that <strong>docID</strong> to retrive a <strong>string representation of the document</strong>. (similar to the token2id mapping in the <code>corpora.Dictionary</code>)</p>

<p>Googling that I could not find anything useful</p>

<p><strong>My Code for searching</strong></p>

<pre><code>    def search(self):
    #Load necessary information
    dictionary = corpora.Dictionary.load('dictionary.dict')
    corpus_tfidf = corpora.MmCorpus('corpus.mm') # comes from the first tutorial, ""From strings to vectors""
    #print(corpus_tfidf)

    #Generate LSI model
    #lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
    lsi = LsiModel(corpus_tfidf,num_topics=2)

    #construct index
    index = similarities.MatrixSimilarity(lsi[corpus_tfidf]) # transform corpus to LSI space and index it

    #Construct query vector
    doc = ""Human machine interface for lab abc computer applications""
    vec_bow = dictionary.doc2bow(doc.lower().split())
    vec_lsi = lsi[vec_bow] # convert the query to LSI space

    #Calcualte similarity
    sims = index[vec_lsi] # perform a similarity query against the corpus
    sims = sorted(enumerate(sims), key=lambda item: -item[1])

    print(sims) # print sorted (document number, similarity score) 2-tuples
</code></pre>

<p><strong>Results sample</strong></p>

<pre><code>[(1, 0.9962855), (4, 0.99420911), (2, 0.98064679), (3, 0.97580492), (0, 0.9755646), (8, 0.34740543), (6, 0.1566827), (7, 0.15566549), (5, 0.13825497)]
</code></pre>
","1831518","","","","","2016-12-11 18:58:16","doc2id mapping in gensim","<python><similarity><gensim>","1","0","","","","CC BY-SA 3.0"
"40727093","1","","","2016-11-21 18:33:30","","0","1258","<p>I have trained 26 million tweets with skipgram technique to create word embeddings as follows:</p>

<pre><code>sentences = gensim.models.word2vec.LineSentence('/.../data/tweets_26M.txt')
model = gensim.models.word2vec.Word2Vec(sentences, window=2, sg=1, size=200, iter=20)
model.save_word2vec_format('/.../savedModel/Tweets26M_All.model.bin', binary=True)
</code></pre>

<p>However, I am continuously collecting more tweets in my database. For example, when I have 2 million more tweets, I wanna update my embeddings with also considering this newcoming 2M tweets.</p>

<p>Is it possible to load previously trained model and update weights of embeddings (maybe adding new word embeddings to my model)? Or do I need to  28 (26+2) million tweets from beginning? It takes 5 hours with current parameters and will take longer with a bigger data.</p>

<p>One other question, is it possible to retrieve sentences parameter directly from database (instead of reading it from <em>txt</em>, <em>bz2</em> or <em>gz</em> files)? As our data to be trained is getting bigger, it would be better to bypassing text read/write operations.</p>
","6819658","","","","","2016-11-21 18:33:30","gensim word2vec - updating word embeddings with newcoming data","<gensim><word2vec><word-embedding>","0","5","1","","","CC BY-SA 3.0"
"40910578","1","","","2016-12-01 12:22:28","","2","557","<p>I am trying to build a Phrases model over a big corpus but I keep stumbling over a memory error. 
First I tried to fit my entire corpus into a big generator.
Then, I tried to save the model between each document :</p>

<pre><code>import codecs
import gensim
import os
import random
import string
import sys

def gencorp(file_path):
    with codecs.open(file_path, 'rb',encoding=""utf8"") as doc :
        for sentence in doc:
            yield sentence.split()

out_corpus_dir = ""C:/Users/Administrator/Desktop/word2vec/1billionwords_corpus_preprocessed/""
file_nb = 0
bi_detector = gensim.models.Phrases()
for file in os.listdir(out_corpus_dir):
    file_nb += 1
    file_path = out_corpus_dir+file
    bi_detector.add_vocab(gencorp(file_path))
    bi_detector.save(""generic_EN_bigrams_v%i""%(file_nb/10))
    bi_detector = gensim.models.Phrases.load(""generic_EN_bigrams_v%i""%(file_nb/10))
bi_detector.save(""generic_EN_bigrams"")
</code></pre>

<p>But none of these solutions work. However, generic_EN_bigrams_v0 is generated and saved.
So I am wondering if I can train a Phrases model per document and then find a way to merge them after.</p>

<p>Thanks you for any insight :)</p>
","4424696","","","","","2017-10-10 13:08:24","Merging two gensim Phrases models","<python><gensim>","1","0","","","","CC BY-SA 3.0"
"40966014","1","","","2016-12-05 01:54:42","","20","21179","<p>I found gensim has BM25 ranking function. However, i cannot find the tutorial how to use it.  </p>

<p>In my case, I had one query. a few documents which were retrieved from the search engine. How to use gensim BM 25 ranking to compare the query and documents to find the most similar one?</p>

<p>I am new to gensim. Thanks.</p>

<p>query:</p>

<pre><code>""experimental studies of creep buckling .""
</code></pre>

<p>document 1:</p>

<pre><code>"" the 7 x 7 in . hypersonic wind tunnel at rae farnborough, part 1, design, instrumentation and flow visualization techniques . this is the first of three parts of the calibration report on the r.a.e. some details of the design and lay-out of the plant are given, together with the calculated performance figures, and the major components of the facility are briefly described . the instrumentation provided for the wind-tunnel is described in some detail, including the optical and other methods of flow visualization used in the tunnel . later parts will describe the calibration of the flow in the working-section, including temperature measurements . a discussion of the heater performance will also be included as well as the results of tests to determine starting and running pressure ratios, blockage effects, model starting loads, and humidity of the air flow .""
</code></pre>

<p>document 2:</p>

<pre><code>"" the 7 in. x 7 in. hypersonic wind tunnel at r.a.e. farnborough part ii. heater performance . tests on the storage heater, which is cylindrical in form and mounted horizontally, show that its performance is adequate for operation at m=6.8 and probably adequate for flows at m=8.2 with the existing nozzles . in its present state, the maximum design temperature of 680 degrees centigrade for operation at m=9 cannot be realised in the tunnel because of heat loss to the outlet attachments of the heater and quick-acting valve which form, in effect, a large heat sink . because of this heat loss there is rather poor response of stagnation temperature in the working section at the start of a run . it is hoped to cure this by preheating the heater outlet cone and the quick-acting valve . at pressures greater than about 100 p.s.i.g. free convection through the fibrous thermal insulation surrounding the heated core causes the top of the heater shell to become somewhat hotter than the bottom, which results in /hogging/ distortion of the shell . this free convection cools the heater core and a vertical temperature gradient is set up across it after only a few minutes at high pressure . modifications to be incorporated in the heater to improve its performance are described .""
</code></pre>

<p>document 3:</p>

<pre><code>"" supersonic flow at the surface of a circular cone at angle of attack . formulas for the inviscid flow properties on the surface of a cone at angle of attack are derived for use in conjunction with the m.i.t. cone tables . these formulas are based upon an entropy distribution on the cone surface which is uniform and equal to that of the shocked fluid in the windward meridian plane . they predict values for the flow variables which may differ significantly from the corresponding values obtained directly from the cone tables . the differences in the magnitudes of the flow variables computed by the two methods tend to increase with increasing free-stream mach number, cone angle and angle of attack .""
</code></pre>

<p>document 4:</p>

<pre><code>"" theory of aircraft structural models subjected to aerodynamic heating and external loads . the problem of investigating the simultaneous effects of transient aerodynamic heating and external loads on aircraft structures for the purpose of determining the ability of the structure to withstand flight to supersonic speeds is studied . by dimensional analyses it is shown that .. constructed of the same materials as the aircraft will be thermally similar to the aircraft with respect to the flow of heat through the structure will be similar to those of the aircraft when the structural model is constructed at the same temperature as the aircraft . external loads will be similar to those of the aircraft . subjected to heating and cooling that correctly simulate the aerodynamic heating of the aircraft, except with respect to angular velocities and angular accelerations, without requiring determination of the heat flux at each point on the surface and its variation with time . acting on the aerodynamically heated structural model to those acting on the aircraft is determined for the case of zero angular velocity and zero angular acceleration, so that the structural model may be subjected to the external loads required for simultaneous simulation of stresses and deformations due to external loads .""
</code></pre>
","7159824","","","","","2021-04-28 12:56:53","How to use gensim BM25 ranking in python","<python><ranking><gensim>","4","0","10","","","CC BY-SA 3.0"
"67013822","1","","","2021-04-09 01:18:31","","0","182","<p>I would like to get the number of words or frequency of words in each topic.</p>
<p>The code below is what I have used to generate model and make visualization.</p>
<pre><code>def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values

def find_optimal_number_of_topics(dictionary, corpus, processed_data):
    limit = 40;
    start = 2;
    step = 6;
    
    model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=processed_data, start=start, limit=limit, step=step)
    x = range(start, limit, step)
    plt.plot(x, coherence_values)
    plt.xlabel(&quot;Num Topics&quot;)
    plt.ylabel(&quot;Coherence score&quot;)
    plt.legend((&quot;coherence_values&quot;), loc='best')
    plt.show()

if __name__ == '__main__':
    processed_data = [sent.strip().split(&quot;,&quot;) for sent in tqdm(open('./data/tokenized_data.csv', 'r', encoding='utf-8').readlines())]
    
    dictionary = corpora.Dictionary(processed_data)
    
    dictionary.filter_extremes(no_below=10, no_above=0.05)
    corpus = [dictionary.doc2bow(text) for text in processed_data]
    print('Number of unique tokens: %d' % len(dictionary))
    print('Number of documents: %d' % len(corpus))
    
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    
    find_optimal_number_of_topics(dictionary, corpus, processed_data)

if __name__ == '__main__':
    processed_data = [sent.strip().split(&quot;,&quot;) for sent in tqdm(open('/Users/pc/Downloads/data/tokenized_data.csv', 'r', encoding='utf-8').readlines())]
    
    dictionary = corpora.Dictionary(processed_data)
    
    dictionary.filter_extremes(no_below=10, no_above=0.05)
    corpus = [dictionary.doc2bow(text) for text in processed_data]
    print('Number of unique tokens: %d' % len(dictionary))
    print('Number of documents: %d' % len(corpus))
    
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    
    perplexity_logger = PerplexityMetric(corpus=corpus, logger='shell')
    coherence_logger = CoherenceMetric(corpus=corpus, coherence=&quot;u_mass&quot;, logger='shell')
    
    lda_model = LdaModel(corpus, id2word=dictionary, num_topics=5, passes=30, callbacks=[coherence_logger, perplexity_logger])
    
    topics = lda_model.print_topics(num_words=5)
    for topic in topics:
        print(topic)

    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_data, dictionary=dictionary, coherence='c_v')
    coherence_lda = coherence_model_lda.get_coherence()
    print('\nCoherence Score (c_v): ', coherence_lda)
    
    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_data, dictionary=dictionary, coherence=&quot;u_mass&quot;)
    coherence_lda = coherence_model_lda.get_coherence()
    print('\nCoherence Score (u_mass): ', coherence_lda)
    
    pickle.dump(corpus, open('/Users/pc/Downloads/data/lda_corpus.pkl', 'wb'))
    dictionary.save('/Users/pc/Downloads/data/lda_dictionary.gensim')
    lda_model.save('/Users/pc/Downloads/data/lda_model.gensim')
    
    lda_visualization = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)
    pyLDAvis.save_html(lda_visualization, '/Users/pc/Downloads/data/lda.html')
    pyLDAvis.show(lda_visualization)

</code></pre>
<p>Desirable output would be something as:</p>
<p>Topic 1. word 1: 123 word 2: 323 word 3: 123 word 4: 322</p>
<p>Topic 2. word 1: 133 word 5: 103 word 8: 313 word 4: 232</p>
","12073062","","","","","2021-04-09 01:18:31","Gensim LDA : How to get the number(frequency) of words in each topic?","<python><gensim><lda><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"58666699","1","58666955","","2019-11-01 22:45:31","","0","1175","<p>Using the <code>Word2Vec</code> implementation of the module <code>gensim</code> in order to construct word embeddings for the sentences I do have in a plain text file. Despite the word <code>happy</code> is defined in the vocabulary, getting the error <code>KeyError: ""word 'happy' not in vocabulary""</code>. Tried to apply the given the answers to <a href=""https://stackoverflow.com/questions/41133844/keyerror-word-word-not-in-vocabulary-in-word2vec"">a similar question</a>, but did not work. Hence, posted my own question.</p>

<p>Here is the code:</p>

<pre><code>try:
    data = []
    with open(TXT_PATH, 'r', encoding='utf-8') as txt_file:
        for line in txt_file:
            for part in line.split(' '):
                data.append(part.strip())

    # When I debug, both of the words 'happy' and 'birthday' exist in the variable 'data'
    word2vec = Word2Vec(data, min_count=5, size=10000, window=5, workers=4)

    # Print result
    word_1 = 'happy'
    word_2 = 'birthday'
    print(f'Similarity between {word_1} and {word_2} thru word2vec: {word2vec.similarity(word_1, word_2)}')
except Exception as err:
    print(f'An error happened! Detail: {str(err)}')
</code></pre>
","282855","","","","","2019-11-01 23:27:33","word2vec - KeyError: ""word X not in vocabulary""","<gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"49767270","1","49802374","","2018-04-11 05:37:30","","3","7018","<p>I have a niche corpus of ~12k docs, and I want to test near-duplicate documents with similar meanings across it - think article about the same event covered by different news organisations. </p>

<p>I have tried gensim's Word2Vec, which gives me terrible similarity score(&lt;0.3) even when the test document is <em>within</em> the corpus, and I have tried SpaCy, which gives me >5k documents with similarity > 0.9. I tested SpaCy's most similar documents, and it was mostly useless.</p>

<p>This is the relevant code.                                                              </p>

<pre><code>tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=40)
doc = preprocess(query)
vec_bow = dictionary.doc2bow(doc)
vec_lsi_tfidf = lsi[tfidf[vec_bow]] # convert the query to LSI space
index = similarities.Similarity(corpus = corpus, num_features = len(dictionary), output_prefix = ""pqr"")
sims = index[vec_lsi_tfidf] # perform a similarity query against the corpus
most_similar = sorted(list(enumerate(sims)), key = lambda x:x[1])

for mid in most_similar[-100:]:
    print(mid, file_list[mid[0]])
</code></pre>

<p>Using gensim I have found a decent approach, with some preprocessing, but the similarity score is still quite low. Has anyone faced such a problem, and are there are some resources or suggestions that could be useful?</p>
","6878758","","6878758","","2018-04-11 05:44:36","2018-04-12 17:09:17","Document similarity in Spacy vs Word2Vec","<python-3.x><nlp><gensim><spacy>","1","0","","","","CC BY-SA 3.0"
"58682909","1","","","2019-11-03 17:43:49","","1","2264","<p>I'm working my way through LDA models for text analysis; I've heard that the Mallet implementation is the best. However, it seems to generate very poor results when I compare it with the Gensim version, so I think I may be doing something wrong. Can anyone explain the discrepancy?</p>

<pre><code>import gensim
from gensim.corpora.dictionary import Dictionary
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
import pyLDAvis
import pyLDAvis.gensim  


## Generate a toy corpus:

dog = list(np.repeat('dog', 500)) + list(np.repeat('cat', 20)) + list(np.repeat('bird', 20))
cat = list(np.repeat('dog', 20)) + list(np.repeat('cat', 500)) + list(np.repeat('bird', 20))
bird = list(np.repeat('dog', 20)) + list(np.repeat('cat', 20)) + list(np.repeat('bird', 500))

texts = [dog, cat, bird]

id2word = corpora.Dictionary(texts)

corpus = [id2word.doc2bow(i) for i in texts]

### Gensim model

lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,
                                        id2word=id2word,
                                           num_topics=3, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)

pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)


vis = pyLDAvis.prepared_data_to_html(vis)

with open(""LDA_output.html"", ""w"") as file:
    file.write(vis)
</code></pre>

<p>This gives the following plausible inference of topics:</p>

<p><a href=""https://i.stack.imgur.com/mwenL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mwenL.png"" alt=""Gensim topic inference""></a></p>

<p>However, things work very differently for the Mallet implementation:</p>

<pre><code>mallet_path = '/mallet-2.0.8/bin/mallet'

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=3, iterations=1000, workers = 4, id2word=id2word)

model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)

pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(model, corpus, id2word)

vis = pyLDAvis.prepared_data_to_html(vis)

with open(""LDA_output.html"", ""w"") as file:
    file.write(vis)
</code></pre>

<p>Here, there is very little difference between the topics that the model infers. </p>

<p><a href=""https://i.stack.imgur.com/oG1PT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oG1PT.png"" alt=""Mallet version""></a></p>

<p>Now, it seems to me that I'm making an elementary blunder here--possibly by not specifying a relevant model parameter the correct way. However, I'm baffled as to what that might be. I'd be grateful for any advice!</p>
","6289601","","6289601","","2019-11-03 20:50:00","2019-11-03 20:50:00","Why does Mallet LDA give poor results when then Gensim version doesn't?","<python><nlp><gensim><lda><mallet>","0","5","","","","CC BY-SA 4.0"
"66991154","1","","","2021-04-07 17:29:05","","0","429","<p>How can I print the number of words in the model's vocabulary for gensim Word2Vec?</p>
<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True)
</code></pre>
<p>I have tried</p>
<pre><code>len(model.wv.vocab)
</code></pre>
<p>But I get this error</p>
<pre><code>Traceback (most recent call last):
  File &quot;.\word2vec.py&quot;, line 4, in &lt;module&gt;
    len(model.wv.vocab)
AttributeError: 'KeyedVectors' object has no attribute 'wv'
</code></pre>
<p>And this</p>
<pre><code>len(model.vocab)
</code></pre>
<p>And it gives this error</p>
<pre><code>Traceback (most recent call last):
  File &quot;.\word2vec.py&quot;, line 4, in &lt;module&gt;
    len(model.vocab)
  File &quot;C:\Users\User\AppData\Local\Programs\Python\Python38\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 645, in vocab
    raise AttributeError(
AttributeError: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.
Use KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.
</code></pre>
","15499396","","15499396","","2021-04-07 17:37:32","2021-04-07 17:52:59","Number of words in vocabulary gensim word2vec","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"67003269","1","67004013","","2021-04-08 11:41:54","","1","81","<p>Through the <a href=""https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html"" rel=""nofollow noreferrer"">method</a></p>
<p><code>gensim.models.Word2Vec.most_similar</code></p>
<p>I get the top-N most similar words.</p>
<p>I trained a model with a list of sentences like</p>
<pre><code>list_of_list = [[&quot;i like going to the beach&quot;],
                [&quot;the war is over&quot;], 
                [&quot;we are all made of stars&quot;],  
                         ...
                [&quot;i don't know what to do&quot;]] 
model = gensim.models.Word2Vec(list_of_list, size=100, window=longest_list, min_count=2)

suggestions = model.most_similar(&quot;I don't know what to do&quot;, topn=10)       
</code></pre>
<p>and I wanted to evaluate phrases similarity.</p>
<p>If for example I run</p>
<pre><code>suggestions = model.most_similar(&quot;I don't know what to do&quot;, topn=10)       
</code></pre>
<p>It works correctly.</p>
<p>But if I give a subquery like <code>&quot;to the beach&quot;</code> or <code>&quot;what to do&quot;</code>, it returns an error message because the sub-phrase is not in the vocabulary.</p>
<pre><code> &quot;word 'to the beach' not in vocabulary&quot;
</code></pre>
<p>How can I solve this issue without training again the model?
How can the model identify the most similar phrases based on a new phrase, not necessary a subphrase?</p>
","6290211","","6573902","","2021-04-08 12:50:11","2021-04-08 12:50:11","'word not in the vocabulary' when evaluating similarity using Gensim Word2Vec.most_similar method","<python><nlp><gensim><similarity>","1","0","","","","CC BY-SA 4.0"
"67019956","1","","","2021-04-09 11:05:30","","0","148","<p>How do I train Glove embeddings in gensim from scratch? Can I use gensim for this?</p>
","13969396","","","","","2021-04-09 16:04:07","How do I train Glove embeddings in gensim from scratch?","<stanford-nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"58685936","1","","","2019-11-04 00:41:06","","1","42","<p>With gensim, I can solve the equation ""king + woman - man = queen"" with this line:</p>

<pre class=""lang-py prettyprint-override""><code>model.most_similar_cosmul(positive=['king', 'woman'], negative=['man'])
</code></pre>

<p>But how can I pass in <code>('king', 'queen')</code> and output an interpolation like this:</p>

<pre><code>positive=['woman'], negative=['man']
</code></pre>

<p>It would be awesome to pass in two dissimilar words like ""rain"" and ""mouse"" to see what words need to be added/subtracted to transform one into the other!</p>

<p>If gensim can't do this, are there any tools for this?</p>
","12317831","","","","","2019-11-04 00:41:06","How can I use gensim keyedvectors to find the connecting words between two given words?","<python><nlp><gensim><word2vec>","0","1","","","","CC BY-SA 4.0"
"58671002","1","","","2019-11-02 12:14:43","","0","81","<p>I have around 150,000 rows in csv file and getting 'Memory Error' in the <code>for sims in index</code> statement.<br>
Could you please advise me how to extract values in Similarity object without getting the memory error.</p>

<pre><code>corpus = [dictionary.doc2bow(text) for text in terms_list]
similarity_matrix = []

index = gensim.similarities.Similarity('E:\\cm_test',corpus,len(dictionary))

for sims in index:

    similarity_matrix.append(sims)

similarity_array = np.array(similarity_matrix)
</code></pre>
","11878666","","6707985","","2019-11-15 06:37:10","2019-11-15 06:37:10","Memory error for Similarity matrix for large number of rows (gensim)","<gensim>","0","3","","","","CC BY-SA 4.0"
"58712856","1","58726940","","2019-11-05 13:55:28","","2","1852","<p>Let's say, <strong>word2vec.model</strong> is my trained word2vec model. When a out-of-vocabulary word (<strong>oov_word</strong>) occurs, I compute a vector <strong>vec</strong> using <em>compute_vec(oov_word)</em> method. Now, I want to add/append <strong>oov_word</strong> and its corresponding vector <strong>vec</strong> to my already trained model <strong>word2vec.model</strong>.</p>

<p>I have already checked the below links. But they do not answer my question.</p>

<p><a href=""https://stackoverflow.com/questions/54243797/combining-adding-vectors-from-different-word2vec-models"">Combining/adding vectors from different word2vec models</a></p>

<p><a href=""https://datascience.stackexchange.com/questions/49431/how-to-train-an-existing-word2vec-gensim-model-on-new-words"">https://datascience.stackexchange.com/questions/49431/how-to-train-an-existing-word2vec-gensim-model-on-new-words</a></p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.add"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.add</a></p>
","7150943","","","","","2021-05-26 10:38:20","How to add words and vectors manually to Word2vec gensim?","<gensim><word2vec>","2","2","","","","CC BY-SA 4.0"
"67275684","1","","","2021-04-27 01:10:44","","1","30","<p>I've been trying to fit a topic model using LdaModel() on a large dataset (~17 mil entries). I'm running my code on my University's HPC cluster, and have been running into issues when I try to train the model in distributed mode (distributed = True). Right now I'm just trying to run on 4 cores on a small subset of the data, but will probably need to increase the number of cores when I run on the entire dataset. Here is the error:</p>
<p><a href=""https://i.stack.imgur.com/xiaTV.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/xiaTV.png</a></p>
<p>When I run in serial mode (distributed = False), it seems to work fine. When running distributed, it seems like gensim is trying to train using all 4 cores. The callback log produced the following statement: &quot;<strong>2021-04-26 18:32:05,757:INFO:using distributed version with 4 workers</strong>&quot;. I'm using the most recent versions of Gensim v4.0 and Pyro4 v4.8 and Python v3.7.7. I also tried downgrading gensim to 3.8.3, but that didn't seem to help.</p>
<p>Also worth noting that I'm running the python script from a shell script, in which I've included the following lines of code as suggested by the gensim documentation:</p>
<pre><code>export PYRO_SERIALIZERS_ACCEPTED=pickle
export PYRO_SERIALIZER=pickle  
python -m Pyro4.naming -n 0.0.0.0 &amp;

for i in {1..4}
do
    python -m gensim.models.lda_worker &amp;
    echo &quot;$i&quot;
done
python -m gensim.models.lda_dispatcher &amp;
</code></pre>
<p>Here's most of the python script that I'm trying to run:</p>
<pre><code># Import preprocessed text
df = pd.read_feather('narratives_processed_1.feather')
# Sample down to speed up debugging
df = df.loc[1:4000]

# Create dictionary from text
dictionary = Dictionary(df.FOI_TEXT)
dictionary.filter_extremes(no_below=100, no_above=0.5)
dictionary.compactify()
# Create corpus
corpus = [dictionary.doc2bow(doc) for doc in df.FOI_TEXT]
print('Number of unique tokens: %d' % len(dictionary))
print('Number of documents: %d' % len(corpus))
temp = dictionary[0]
id2word = dictionary.id2token

#Callback logging for coherence using the u_mass metric
coherence_umass_logger = CoherenceMetric(corpus=corpus, logger='shell', coherence = 'u_mass')

filename = &quot;model_callbacks_1.log&quot;
import logging
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)
logging.basicConfig(filename = filename,
                    format=&quot;%(asctime)s:%(levelname)s:%(message)s&quot;,
                    level=logging.INFO)

#Iterate over model parameters
iterations = [10,50,100]
num_topics = [2,3]
passes = 2

all_metrics = pd.DataFrame()

print('Fitting models...')
for iteration in iterations:
    print('Iterations for this model: %d'%(iteration))
    for num_topic in num_topics:
        print('Topics for this model: %d'%(num_topic))
        
        # Create model
        model = LdaModel(corpus=corpus,
                 num_topics=num_topic,
                 id2word=id2word,
                 eval_every=0,
                 passes=passes,
                 iterations=iteration,
                 chunksize=1000,
                 random_state=100,
                 callbacks=[coherence_umass_logger],
                 distributed = True)
            
        df_temp = pd.DataFrame.from_dict(model.metrics)
        df_temp['iterations'] = iteration
        df_temp['topics'] = num_topic
    
        all_metrics = pd.concat([all_metrics, df_temp])
</code></pre>
<p>Any help on this error is much appreciated!</p>
","15772202","","","","","2021-04-27 01:10:44","AssertionError while training distributed LdaModel in Gensim","<python><nlp><gensim><topic-modeling><pyro4>","0","0","","","","CC BY-SA 4.0"
"40936197","1","40944120","","2016-12-02 15:56:07","","2","304","<p>I want to replace the words of my gensim Word2Vec model with a mapping.</p>

<p><strong>Example</strong></p>

<p>My current model has the word <code>'foo'</code> that maps to a vector: </p>

<pre><code>&gt;&gt;&gt; model['foo']
[1.0 0.0]
</code></pre>

<p>I have the mapping: <code>d = {'foo': 'bar', ...}</code></p>

<p>How can I rebuild the model with this new mapping such that </p>

<pre><code>&gt;&gt;&gt; model['bar']  # in place of 'foo'
[1.0 0.0]
</code></pre>
","690430","","690430","","2016-12-03 04:06:25","2016-12-03 18:02:20","Rename gensim Word2Vec words with mapping","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"41064594","1","","","2016-12-09 16:16:05","","1","139","<p>I need to compare two strings, e.g. <em>Highly Toughened Steel - Unbreakable</em> and <em>Highly rugged Steel</em>. If they are the names of two products which are the same, I need to find out how similar the strings are. I also need to check for synonyms in the sentence.</p>

<p>I have already tried using Diff Sequence Matcher and the output is pretty good.</p>

<pre><code>seq=difflib.SequenceMatcher(None, a,b)
d=seq.ratio()*100
</code></pre>

<p>I was trying to look at Genism. Is it a library worth consider for such a small task? 
If so, can anyone provide a reproducible example for using Genism to compare strings?</p>
","4643423","","1575353","","2016-12-09 17:28:24","2016-12-09 17:28:24","Python Based Code To Compare Similarity Between Sentences","<python><gensim>","0","0","","","","CC BY-SA 3.0"
"58714746","1","58715630","","2019-11-05 15:40:56","","0","2183","<p>I've got a question during following the simple gensim tutorial on <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim website</a>,</p>

<pre><code>&gt;&gt;&gt; from gensim.test.utils import common_texts, get_tmpfile
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt;
&gt;&gt;&gt; path = get_tmpfile(""word2vec.model"")
&gt;&gt;&gt;
&gt;&gt;&gt; model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
&gt;&gt;&gt; model.save(""word2vec.model"")
&gt;&gt;&gt; model = Word2Vec.load(""word2vec.model"")
&gt;&gt;&gt; model.train([[""hello"", ""world""]], total_examples=1, epochs=1)

&gt;&gt;&gt; from gensim.models import KeyedVectors
&gt;&gt;&gt;
&gt;&gt;&gt; path = get_tmpfile(""wordvectors.kv"")
&gt;&gt;&gt;
</code></pre>

<p>And when I tried below,</p>

<pre><code>&gt;&gt;&gt; model.wv.save(path)
&gt;&gt;&gt; wv = KeyedVectors.load(""model.wv"", mmap='r')
</code></pre>

<p>I've got a following error :</p>

<pre><code>---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
&lt;ipython-input-81-eee6865b677b&gt; in &lt;module&gt;
      1 path = get_tmpfile('wordvectors.kv')
      2 model.wv.save(path)
----&gt; 3 KeyedVectors.load(""model.wv"",mmap='r')

/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load(cls, fname_or_handle, **kwargs)
    210     @classmethod
    211     def load(cls, fname_or_handle, **kwargs):
--&gt; 212         return super(BaseKeyedVectors, cls).load(fname_or_handle, **kwargs)
    213 
    214     def similarity(self, entity1, entity2):

/anaconda3/lib/python3.7/site-packages/gensim/utils.py in load(cls, fname, mmap)
    420         compress, subname = SaveLoad._adapt_by_suffix(fname)
    421 
--&gt; 422         obj = unpickle(fname)
    423         obj._load_specials(fname, mmap, compress, subname)
    424         logger.info(""loaded %s"", fname)

/anaconda3/lib/python3.7/site-packages/gensim/utils.py in unpickle(fname)
   1356 
   1357     """"""
-&gt; 1358     with smart_open(fname, 'rb') as f:
   1359         # Because of loading from S3 load can't be used (missing readline in smart_open)
   1360         if sys.version_info &gt; (3, 0):

/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py in smart_open(uri, mode, **kw)
    179         raise TypeError('mode should be a string')
    180 
--&gt; 181     fobj = _shortcut_open(uri, mode, **kw)
    182     if fobj is not None:
    183         return fobj

/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py in _shortcut_open(uri, mode, **kw)
    299     #
    300     if six.PY3:
--&gt; 301         return open(parsed_uri.uri_path, mode, buffering=buffering, **open_kwargs)
    302     elif not open_kwargs:
    303         return open(parsed_uri.uri_path, mode, buffering=buffering)

FileNotFoundError: [Errno 2] No such file or directory: 'model.wv'
</code></pre>

<p>Does anyone know the reason for this message? How can I know that I do have 'model.wv' file?</p>

<p>Thank you in advance!</p>
","11977071","","","","","2019-11-05 17:11:31","Gensim -- [Errno 2] No such file or directory: 'model.wv'","<python><model><gensim>","1","0","","","","CC BY-SA 4.0"
"58666807","1","58666998","","2019-11-01 23:02:35","","0","89","<p>I'm using <code>fastText</code> implementation of the module <code>gensim</code>. Despite getting no reasons, my program throws an exception.</p>

<p>Here is the code:</p>

<pre><code>try:
    data = []
    with open(TXT_PATH, 'r', encoding='utf-8') as txt_file:
        for line in txt_file:
            for part in line.split(' '):
                data.append(part.strip())

    fastText = FastText(data, min_count=1, size=10000, window=5, workers=4)

    # Print results
    word_1 = 'happy'
    word_2 = 'birthday'
    print(f'Similarity between {word_1} and {word_2} thru fastText: {fastText.similarity(word_1, word_2)}')
except Exception as err:
    print(f'\n!!!!! An error happened! Detail: {str(err)}')
</code></pre>

<p>The end of the output:</p>

<pre><code>!!!!! An error happened! Detail: 
</code></pre>
","282855","","","","","2019-11-01 23:34:16","fastText - Throws exception without any reasons","<python-3.x><gensim><word-embedding><fasttext>","1","0","","","","CC BY-SA 4.0"
"51287590","1","","","2018-07-11 14:05:07","","3","1395","<p>I wanted to write the code to find the similarity between two sentences and then I ended up writing this code using nltk and gensim. I used tokenization and  gensim.similarities.Similarity to do the work. But it ain't serving my purpose. 
It works fine until I introduce the last line of code. </p>

<pre><code>import gensim
import nltk

raw_documents = [""I'm taking the show on the road."",
             ""My socks are a force multiplier."",
         ""I am the barber who cuts everyone's hair who doesn't cut their 
own."",
         ""Legend has it that the mind is a mad monkey."",
        ""I make my own fun.""]
from nltk.tokenize import word_tokenize
gen_docs = [[w.lower() for w in word_tokenize(text)]
        for text in raw_documents]



dictionary = gensim.corpora.Dictionary(gen_docs)
print(dictionary[5])
print(dictionary.token2id['socks'])
print(""Number of words in dictionary:"",len(dictionary))
for i in range(len(dictionary)):
    print(i, dictionary[i])

corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]
print(corpus)

tf_idf = gensim.models.TfidfModel(corpus)
print(tf_idf)
    s = 0
for i in corpus:
s += len(i)
print(s)

sims = gensim.similarities.Similarity('/usr/workdir/',tf_idf[corpus],
                                  num_features=len(dictionary))
print(sims)
print(type(sims))


query_doc = [w.lower() for w in word_tokenize(""Socks are a force for good."")]
print(query_doc)
query_doc_bow = dictionary.doc2bow(query_doc)
print(query_doc_bow)
query_doc_tf_idf = tf_idf[query_doc_bow]
print(query_doc_tf_idf)

sims[query_doc_tf_idf]
</code></pre>

<p>It throws this error. I couldn't find the answer for this anywhere on the internet.</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\gensim\utils.py"", line 679, in save
_pickle.dump(self, fname_or_handle, protocol=pickle_protocol)
TypeError: file must have a 'write' attribute

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""semantic.py"", line 45, in &lt;module&gt;
    sims[query_doc_tf_idf]
  File ""C:\Python36\lib\site-packages\gensim\similarities\docsim.py"", line 
503, in __getitem__
    self.close_shard()  # no-op if no documents added to index since last 
query
 File ""C:\Python36\lib\site-packages\gensim\similarities\docsim.py"", line 
427, in close_shard
    shard = Shard(self.shardid2filename(shardid), index)
 File ""C:\Python36\lib\site-packages\gensim\similarities\docsim.py"", line 
110, in __init__
    index.save(self.fullname())
  File ""C:\Python36\lib\site-packages\gensim\utils.py"", line 682, in save
    self._smart_save(fname_or_handle, separately, sep_limit, ignore, 
pickle_protocol=pickle_protocol)
  File ""C:\Python36\lib\site-packages\gensim\utils.py"", line 538, in 
_smart_save
    pickle(self, fname, protocol=pickle_protocol)
  File ""C:\Python36\lib\site-packages\gensim\utils.py"", line 1337, in pickle
    with smart_open(fname, 'wb') as fout:  # 'b' for binary, needed on 
Windows
  File ""C:\Python36\lib\site-packages\smart_open\smart_open_lib.py"", line 
181, in smart_open
fobj = _shortcut_open(uri, mode, **kw)
  File ""C:\Python36\lib\site-packages\smart_open\smart_open_lib.py"", line 
287, in _shortcut_open
return io.open(parsed_uri.uri_path, mode, **open_kwargs)
</code></pre>

<p>Please help figure out where the problem is.</p>
","9826763","","","","","2021-03-31 10:36:43","How to use gensim.similarities.Similarity to find similarity between two sentences","<python><python-3.x><nltk><gensim><corpus>","1","0","","","","CC BY-SA 4.0"
"41129933","1","43042699","","2016-12-13 20:23:03","","1","956","<p>I have a trained word2vec models in geinsim with 300 dimensions and would like to cut the dimensions to 100 (simply drop the last 200 dimensions). What is the easiest and most efficient way using python?</p>
","3862262","","","","","2020-03-14 22:47:51","Gensim Word2Vec model: Cut dimensions","<python><python-3.x><gensim><word2vec>","2","0","","","","CC BY-SA 3.0"
"41133844","1","","","2016-12-14 02:28:25","","2","9779","<p>I am using <code>word2vec</code>, wiki corpus I trained, what can I do if the word I input not in vocabulary in <code>word2vec</code>?</p>

<p><strong>Test it a bit:</strong></p>

<pre><code>model = word2vec.Word2Vec.load('model/' + 'wiki_chinese_word2vec.model')    
model['boom']
</code></pre>

<p><strong>Error:</strong></p>

<blockquote>
  <p>KeyError(""word '%s' not in vocabulary"" % word)</p>
</blockquote>
","7294094","","1540468","","2016-12-14 02:29:35","2021-06-17 00:56:38","KeyError: ‚Äúword 'word' not in vocabulary‚Äù in word2vec","<python><gensim><word2vec>","2","2","","","","CC BY-SA 3.0"
"50618993","1","","","2018-05-31 07:31:41","","2","1848","<p>I want to get word embeddings for the words in a corpus. I decide to use pretrained word vectors in <em>GoogleNews</em> by <em>gensim</em> library. But my corpus contains some words that are not in GoogleNews words. for these missing words, I want to use arithmatic mean of n most similar words to it in GoggoleNews words. First I load GoogleNews and check that the word ""to"" is in it?</p>

<pre><code>#Load GoogleNews pretrained word2vec model
model=word2vec.KeyedVectors.Load_word2vec_format(""GoogleNews-vectors-negative33.bin"",binary=True)
print(model[""to""])
</code></pre>

<p>I receive an error: <code>keyError: ""word 'to' not in vocabulary""</code>
is it possible that such a large dataset doesn't have this word? this is true also for some other common word like ""a""!</p>

<p>For adding missing words to word2vec model,first I want to get indices of words that are in GoogleNews. for missing words I have used index 0.</p>

<pre><code>#obtain index of words
word_to_idx=OrderedDict({w:0 for w in corpus_words})
word_to_idx=OrderedDict({w:model.wv.vocab[w].index for w in corpus_words if w in model.wv.vocab})
</code></pre>

<p>then I calculate the mean of embedding vectors of most similar words to each missing word.</p>

<pre><code>missing_embd={}
for key,value in word_to_idx.items():
    if value==0:
        similar_words=model.wv.most_similar(key)
        similar_embeddings=[model.wv[a[0]] for a in similar_words]
        missing_embd[key]=mean(similar_embeddings)
</code></pre>

<p>And then I add these news embeddings to word2vec model by:</p>

<pre><code>for word,embd in missing_embd.items():
    # model.wv.build_vocab(word,update=True)
    model.wv.syn0[model.wv.vocab[word].index]=embd
</code></pre>

<p>There is an un-consistency. When I print missing_embed, it's empty. As if there were not any missing words.
But when I check it by this:</p>

<pre><code>for w in tokens_lower:
    if(w in model.wv.vocab)==False:
        print(w)
        print(""***********"")
</code></pre>

<p>I found a lot of missing words.
Now, I have 3 questions:
1- why <strong><em>missing_embed</em></strong> is empty while there are some missing words?
2- Is it possible that GoogleNews doesn't have words like ""to""?
3- how can I append new embeddings to word2vec model? I used <strong><em>build_vocab</em></strong> and <strong><em>syn0</em></strong>. Thanks.</p>
","7418216","","","","","2018-12-30 05:36:09","add new words to GoogleNews by gensim","<python><word2vec><gensim><google-news>","2","7","1","","","CC BY-SA 4.0"
"41960099","1","41960816","","2017-01-31 14:31:00","","0","1123","<p>I have a textual dataset on which I trained a <code>gensim</code> w2v model. Now I want to use those vectors to recive the tf-idf values for the words and documents in my data set. What is the right way to do it? I tried to followe the <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">tutorial</a> on gensim's site.</p>

<p>I expect something like <code>models.tfidfmodel(model.wv[model.wv.index2word])</code>
 but this fail since </p>

<p><code>File ""&lt;ipython-input-229-7946418f8a82&gt;"", line 1, in &lt;module&gt;
    models.tfidfmodel(model.wv[model.wv.index2word])
TypeError: 'module' object is not callable</code></p>

<p>does what I want makes since? Is BOW the only way to do that?</p>
","4809113","","","","","2017-01-31 15:03:34","how to get tf-id from w2v on gensim","<python-3.x><machine-learning><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"41720864","1","","","2017-01-18 13:35:13","","1","891","<p>I did research on Google also on Gensim Support forum, but I cannot find a good answer. </p>

<p>Basically, I am implementing online learning for Doc2Vec using Gensim, but Gensim keeps throwing me a random error called ""Segmentation </p>

<p>Please take a look at my sample code</p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import LabeledSentence
import random
import logging

if __name__ == ""__main__"":
    logging.basicConfig(level=logging.INFO)

    sentence1 = ""this is a test""
    sentence2 = ""test test 123 test""
    sentence3 = ""qqq zzz""
    sentence4 = ""ppp""

    sentences = [
        LabeledSentence(sentence1.split(), [""p1""]),
        LabeledSentence(sentence2.split(), [""p2""])
    ]
    model = Doc2Vec(min_count=1, window=5, size=400, sample=1e-4, negative=5, workers=1)
    model.build_vocab(sentences)

    for a in range(2):
        random.shuffle(sentences)
        print([s.tags[0] for s in sentences])
        model.train(sentences)
    model.save(""test.d2v"")

    new_model = Doc2Vec.load(""test.d2v"")
    new_sentences = [
        LabeledSentence(sentence1.split(), [""n1""]),
        LabeledSentence(sentence3.split(), [""n2""])
    ]
    new_model.build_vocab(new_sentences, update=True)

    for a in range(4):
        random.shuffle(new_sentences)
        print([s.tags[0] for s in new_sentences])
        new_model.train(new_sentences)
</code></pre>

<p>Here is my error</p>

<pre><code>INFO:gensim.models.word2vec:training model with 1 workers on 7 vocabulary and 400 features, using sg=0 hs=0 sample=0.0001 negative=5 window=5
INFO:gensim.models.word2vec:expecting 2 sentences, matching count from corpus used for vocabulary survey
Segmentation fault
</code></pre>

<p>Can anyone explain to me why? and how to solve this?</p>

<p>Thanks</p>
","3515824","","","","","2017-01-19 00:21:24","Gensim Segmentation Fault","<python><nlp><gensim>","1","0","","","","CC BY-SA 3.0"
"28677350","1","","","2015-02-23 15:34:17","","7","1240","<p>Latent Dirichlet Allocation(LDA) is a topic model to find latent variable (topics) underlying a bunch of documents. I'm using python gensim package and having two problems: </p>

<ol>
<li><p>I printed out the most frequent words for each topic (I tried 10,20,50 topics), and found out that the distribution over words is very ""flat"": meaning even the most frequent word has only 1% probability... </p></li>
<li><p>Most of the topics are similar: meaning the most frequent words for each of the topics overlap a lot and the topics share almost the same set of words for their high frequency words...</p></li>
</ol>

<p>I guess the problem is probably due to my documents: my documents actually belong to a specific category, for example, they are all documents introducing different online games. For my case, will LDA still work, since the documents themselves are quite similar, so a model based on ""bag of words"" may not be a good way to try? </p>

<p>Could anyone give me some suggestions? Thank you!</p>
","4208905","","355715","","2019-03-10 15:17:32","2019-03-10 15:17:32","Using LDA(topic model) : the distrubution of each topic over words are similar and ""flat""","<python><lda><topic-modeling><gensim>","1","0","2","","","CC BY-SA 4.0"
"62742964","1","","","2020-07-05 15:49:26","","2","251","<p>I am currently attempting to record and graph coherence scores for various topic number values in order to determine the number of topics that would be best for my corpus. After several trials using u_mass, the data proved to be inconclusive since the scores don't plateau around a specific topic number. I'm aware that CV ranges from -14 to 14 when using u_mass, however my values range from -2 to -1 and selecting an accurate topic number is not possible. Due to these issues, I attempted to use c_v instead of u_mass but I receive the following error:</p>
<pre><code>    An attempt has been made to start a new process before the
    current process has finished its bootstrapping phase.

    This probably means that you are not using fork to start your
    child processes and you have forgotten to use the proper idiom
    in the main module:
</code></pre>
<p>This is my code for computing the coherence value</p>
<pre><code>cm = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary,coherence='c_v')
      print(&quot;THIS IS THE COHERENCE VALUE &quot;)
      coherence = cm.get_coherence()
      print(coherence)
</code></pre>
<p>If anyone could provide assistance in resolving my issues for either c_v or u_mass, it would be greatly appreciated! Thank you!</p>
","10809003","","","","","2020-07-05 15:49:26","LDA: Coherence Values using u_mass v c_v","<machine-learning><gensim><lda><dirichlet>","0","0","","","","CC BY-SA 4.0"
"40986528","1","","","2016-12-06 01:52:52","","-1","2062","<p>I was running Multi-label classification on text data I noticed TFIDF outperformed LDA by a large margin. TFIDF accuracy was aorund 50% and LDA was around 29%. </p>

<p>Is this expected or should LDA do better than this? </p>
","6013978","","","","","2017-03-15 04:49:14","Classification LDA vs. TFIDF","<machine-learning><gensim><lda><text-classification>","1","0","","","","CC BY-SA 3.0"
"41607976","1","41731826","","2017-01-12 08:07:07","","6","5694","<p>I am using gensim <code>doc2vec</code>. I want know if there is any efficient way to know the vocabulary size from doc2vec. One crude way is to count the total number of words, but if the data is huge(1GB or more) then this won't be an efficient way.</p>
","4555699","","","","","2019-05-07 11:24:24","Is there any way to get the vocabulary size from doc2vec model?","<gensim><word2vec><doc2vec>","2","0","2","","","CC BY-SA 3.0"
"41101424","1","41763588","","2016-12-12 12:55:05","","0","1694","<p>I am using the topic visualization library LDAvis:</p>

<pre><code>## visualization of the topics
import pyLDAvis
import pyLDAvis.gensim
pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
</code></pre>

<p>which produces an image of the Principal Components of the topics unveiled by the LDA (Latent Dirichlet Allocation) model. I will like to download the image but I am stuck. Any help much appreciated it!</p>
","5510540","","","","","2017-01-20 12:21:13","Downloading the image produced by LDAvis library","<python><ipython><gensim><lda>","1","0","","","","CC BY-SA 3.0"
"51292498","1","","","2018-07-11 18:59:38","","2","2834","<p>I have a dataframe with column <code>keywords</code>:</p>

<pre><code>keywords
election
countries
majestic
dollar
....
....
...
</code></pre>

<p>I also have my own pretrained word2vec model using gensim from where I can get 20 dimension vectors for each word using <code>model['anyword']</code>. My question is </p>

<p>i) I want to assign these 20 dimension vectors as columns names (V1 to V20) corresponding to each keyword.</p>

<p>ii) if word is not present in word vocabulary then i want to assign the vectors as array of [0,0,0,,,,,0] corresponding to that word otherwise it will give an <code>error:word not present in vocabulary</code>. for example if word <code>majestic</code> is not present in vocab then <code>df</code> would should like </p>

<pre><code>keyword     V1     V2      V3 ............. V20
election   0.02    0.44    0.32.............0.12
countries  0.33    0.33    0.11............ 0.13
majestic   0       0       0   ............ 0
dollar     0.31    0.77    0.86............ 0.91
.......
.......
</code></pre>

<p>as far what I have done so far:-</p>

<pre><code>for i in df['keywords']:
    vectors=model['i']
</code></pre>

<p>I got array of vector but Im not getting how put it with columns names as <code>V1 V2 V3 V4....V20</code> in <code>df</code> and how to treat missing word as <code>'0'</code></p>
","8568110","","","","","2018-07-16 13:17:20","Add word vectors as columns in pandas dataframe?","<python-3.x><pandas><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"62743531","1","62747179","","2020-07-05 16:39:07","","4","2125","<p>I have trained fasttext model with Gensim over the corpus of very short sentences (up to 10 words). I know that my test set includes words that are not in my train corpus, i.e some of the words in my corpus are like &quot;Oxytocin&quot; &quot;Lexitocin&quot;, &quot;Ematrophin&quot;,'Betaxitocin&quot;</p>
<p>given a new word in the test set, fasttext knows pretty well to generate a vector with high cosine-similarity to the other similar words in the train set by using the characters level n-gram</p>
<p>How do i incorporate the fasttext model inside a LSTM keras network without losing the fasttext model to just a list of vectors in the vocab? because then I won't handle any OOV even when fasttext do it well.</p>
<p>Any idea?</p>
","2509698","","10375049","","2021-01-16 17:16:53","2021-01-16 17:16:53","Using Gensim Fasttext model with LSTM nn in keras","<tensorflow><keras><nlp><gensim><word-embedding>","1","0","4","","","CC BY-SA 4.0"
"56421404","1","56421528","","2019-06-03 05:05:14","","1","177","<p>In understanding what <code>isolation forest</code> really does, I did a sample project as follows using 8 features as follows.</p>

<pre><code>from sklearn.ensemble import IsolationForest    
#features
df_selected = df[[""feature1"", ""feature2"", ""feature3"", ""feature4"", ""feature5"", ""feature6"", ""feature7"", ""feature8""]]
X = np.array(df_selected)

#isolation forest
clf = IsolationForest(max_samples='auto', random_state=42, behaviour=""new"", contamination=.01)
clf.fit(X)
y_pred_train = clf.predict(X)

print(np.where(y_pred_train == -1)[0])
</code></pre>

<p>Now, I want to identify what are the <em>outlier documents</em> using <code>isolation forest</code>. For that I trained a <code>doc2vec</code> model using <code>gensim</code>. Now for each of my document in the dataset I have a <code>300-dimensional vector</code>.</p>

<p>My question is can I straight away use the document vectors in <code>isolation forest</code> as <code>X</code> in the above code to detect outliers? Or do I need to reduce the dimensionality of the vectors before applying them to <code>isolation forest</code>?</p>

<p>I am happy to provide more details if needed.</p>
","10704050","","10704050","","2019-06-03 06:11:28","2019-06-03 06:11:28","How to use document vectors in isolationforest in sklearn","<python><scikit-learn><gensim><outliers><doc2vec>","1","1","","","","CC BY-SA 4.0"
"58710000","1","58714828","","2019-11-05 11:04:03","","0","468","<p>I want to get the list of similar words. Since Spacy doesn't have a built-in support for this I want to convert the spacy model to gensim word2vec and get the list of similar words.</p>

<p>I have tried to use the below method. But it is time consuming.</p>

<pre class=""lang-py prettyprint-override""><code>def most_similar(word):
    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)
    return [w.orth_ for w in by_similarity[:10]]
</code></pre>

<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load('en_core_web_md')
nlp.to_disk(filename)
nlp.vocab.vectors.to_disk(filename)
</code></pre>

<p>This does not save the model to a text file. Hence, I am not able to use the following method.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec

glove_file = datapath('test_glove.txt')
tmp_file = get_tmpfile(""test_word2vec.txt"")

_ = glove2word2vec(glove_file, tmp_file)
</code></pre>
","7150943","","7150943","","2019-11-05 11:15:35","2019-11-05 16:11:16","Is there a way to load spacy trained model into gensim?","<python-3.x><nlp><gensim><spacy><similarity>","1","2","","","","CC BY-SA 4.0"
"65266342","1","65268695","","2020-12-12 14:56:30","","0","119","<p>Using a <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">KeyedVectors</a> object, I can get the W2V vector, given a word, like so.</p>
<pre><code>from gensim.models import KeyedVectors

model = KeyedVectors.load('vectors.kv')
model.get_vector('example')  # output =&gt; [0.12, 0.41, ..., 0.92]
</code></pre>
<p>How can I do the same, for <em>every</em> term (key) contained in the model?</p>
<p><em>Note that this doesn't <em>have</em> to be a KeyedVectors object, it could alternatively be a <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Word2Vec</a> object.</em></p>
<p><strong>EDIT</strong> - thanks to gojomo:</p>
<pre><code>vector_dct = {}
for word in kv_model.index2word: 
    vector_dct[word] = kv_model.get_vector(word)

df = pd.DataFrame(vector_dct).T
</code></pre>
","2205969","","2205969","","2020-12-14 09:48:48","2020-12-14 09:48:48","How to get a dump of all vectors from a gensim W2V model?","<python><vector><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"30583166","1","","","2015-06-01 20:58:47","","2","2164","<p>How can I find the N-nearest words given a word using gensim's word2vec implementation. What is the API for that? I am referring to skip grams here. Maybe I missed something, I read all about finding similar words, finding the odd one out and so on...</p>

<p>In DL4j I have this method called <code>wordsNearest(String A, int n) which gives me the n-nearest words to A</code>. What is the equivalent of this in Gensim?</p>
","741442","","","","","2016-12-02 16:22:05","Gensim word2vec finding nearest words given a word","<gensim><word2vec>","2","3","2","","","CC BY-SA 3.0"
"65333831","1","65335728","","2020-12-17 02:21:44","","0","120","<p>I cannot get the .most_similar() function to work. I have tried both Gensim 3.8.3 version and now am on the beta version 4.0 . I am working right off of the Word2Vec Model tutorial on each documentation version.</p>
<p>The code giving me error and restarting my kernel:</p>
<pre><code>print(wv.most_similar(positive=['car', 'minivan'], topn=5))
</code></pre>
<p>The above code is verbatim in both 3.8.3 documentation and 4.0. Following tutorials verbatim.</p>
<p>As stated in other stack overflow answers I have tried model.wv.most_similar()</p>
<p>I don't think .most_similar() is depreciated.</p>
<p>Additionally the .doesnt_match() function is not working.</p>
<p>EDIT in regards to gojomo:</p>
<p>Right now I am on Genism 3.8.3. I am using the GloVe Model and Word2Vec models, actually just tried it and it worked with the GloVe model, maybe the Word2Vec model is having a memory problem like gojomo suggested my code below:</p>
<p>I am using linx laptop, I-7 core 1065 cpu, memory 7.4 GiB, 64 bit ubuntu</p>
<pre><code>%matplotlib inline

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

import gensim.downloader as api
wv = api.load('word2vec-google-news-300')

for i, word in enumerate(wv.vocab):
    if i == 10:
        break
    print(word)

pairs = [
    ('programming', 'linux'),   
    ('programming', 'bicycle'), 
    ('programming', 'apple'),  
    ('programming', 'cereal'),    
    ('programming', 'capitalism'),
    ('programming', 'computers'), 
    ('programming', 'python'),  
    ('programming', 'algebra'),  
    ('programming', 'logic'),    
    ('programming', 'math'),
]
for w1, w2 in pairs:
    print('%r\t%r\t%.2f' % (w1, w2, wv.similarity(w1, w2)))

print(wv.most_similar(positive=['math'], topn=5))
</code></pre>
","7314286","","7314286","","2020-12-19 02:20:36","2020-12-19 02:20:36","Gensim error with .most_similar(), jupyter kernel restarting","<python-3.x><machine-learning><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"54736839","1","54737050","","2019-02-17 19:29:56","","2","1541","<p>I've trained <code>Doc2Vec</code> model I'm trying to get predictions.</p>

<p>I use</p>

<pre><code>test_data = word_tokenize(""–§–∏–ª–∏–ø –ú–æ—Ä—Ä–∏—Å –ü—Ä–æ–¥–∞–∫—Ç—Å –°.–ê."".lower())
model = Doc2Vec.load(model_path)
v1 = model.infer_vector(test_data)
sims = model.docvecs.most_similar([v1])
print(sims)
</code></pre>

<p>returns</p>

<pre><code>[('624319', 0.7534812092781067), ('566511', 0.7333904504776001), ('517382', 0.7264763116836548), ('523368', 0.7254455089569092), ('494248', 0.7212602496147156), ('382920', 0.7092794179916382), ('530910', 0.7086726427078247), ('513421', 0.6893941760063171), ('196931', 0.6776881814002991), ('196947', 0.6705600023269653)]
</code></pre>

<p>Next I've tried to know, what's text of this number</p>

<pre><code>model.docvecs['624319']
</code></pre>

<p>But it returns me only the vector representation</p>

<pre><code>array([ 0.36298314, -0.8048847 , -1.4890883 , -0.3737898 , -0.00292279,
   -0.6606688 , -0.12611026, -0.14547637,  0.78830665,  0.6172428 ,
   -0.04928801,  0.36754376, -0.54034036,  0.04631123,  0.24066721,
    0.22503968,  0.02870891,  0.28329515,  0.05591608,  0.00457001],
  dtype=float32)
</code></pre>

<p>So, is any way to get text of this label from the model?
Loading train dataset takes a lot of time, so I try to find out another way.</p>
","6840039","","","","","2019-02-17 22:16:00","Doc2Vec: get text of the label","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"63459657","1","","","2020-08-17 22:55:40","","4","1082","<p>So I have multiple text files(around 40). and each file has around 2000 articles (average of 500 words each). And each document is a single line in the text file.</p>
<p>So because of the memory limitations I wanted to use dynamic loading of these text files for training. (Perhaps a iterator class?)</p>
<p>so how do I proceed?</p>
<ul>
<li>train each text file -&gt; save the model -&gt; load the model and rerun on new data?</li>
<li>is there a way with iterator class to do this automatically?</li>
<li>should I give sentence by sentence, article by article or text file by text file as input to model training?</li>
</ul>
","10577402","","","","","2020-08-17 23:15:24","How to load large dataset to gensim word2vec model","<python><iterator><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"65171196","1","","","2020-12-06 17:52:46","","0","124","<p>gensim's <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">KeyedVectors</a> is essentially a mapping between keys and vectors. Each vector is identified by its lookup key, most often a short string token, so this is usually a mapping between {str =&gt; 1D numpy array}.</p>
<p>As well as providing access to this mapping, KeyedVectors also provides a smaller RAM footprint. See a comparison here:</p>
<pre><code>CAPABILITY                          |   KeyedVectors    |   FULL MODEL  |   NOTE
continue training vectors           |   ‚ùå               |   ‚úÖ           |   You need the full model to train or update vectors.
smaller objects                     |   ‚úÖ               |   ‚ùå           |   KeyedVectors are smaller and need less RAM, because they don‚Äôt need to store the model state that enables training.
save/load fasttext/word2vec fmt     |   ‚úÖ               |   ‚ùå           |   Vectors exported by the Facebook and Google tools do not support further training, but you can still load them into KeyedVectors.
append new vectors                  |   ‚úÖ               |   ‚úÖ           |   Add new-vector entries to the mapping dynamically.
concurrency                         |   ‚úÖ               |   ‚úÖ           |   Thread-safe, allows concurrent vector queries.
shared RAM                          |   ‚úÖ               |   ‚úÖ           |   Multiple processes can re-use the same data, keeping only a single copy in RAM using mmap.
fast load                           |   ‚úÖ               |   ‚úÖ           |   Supports mmap to load data from disk instantaneously.
</code></pre>
<p>Is there an equivalent structure in Java's deeplearning4j Word2Vec implementation? I was unable to find something similar in the <a href=""https://deeplearning4j.konduit.ai/language-processing/word2vec"" rel=""nofollow noreferrer"">documentation</a>.</p>
","2205969","","","","","2020-12-06 17:52:46","Equivalent to gensim.models.KeyedVectors in deeplearning4j's Word2Vec implementation?","<java><python><gensim><word2vec><deeplearning4j>","0","0","","","","CC BY-SA 4.0"
"65278189","1","65310769","","2020-12-13 16:53:10","","-1","131","<p>I built a small code to find analogies using word2vec and it runs fine as stand alone application. Here is the working code</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>import numpy as np

# Get the interactive Tools for Matplotlib
%matplotlib notebook


from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
import os
glove_file = os.path.abspath('glove.6B/glove.6B.100d.txt')
word2vec_glove_file = get_tmpfile(""glove.6B.100d.word2vec.txt"")
glove2word2vec(glove_file, word2vec_glove_file)
model = KeyedVectors.load_word2vec_format(word2vec_glove_file)
def analogy(x1, x2, y1):
    result = model.most_similar(positive=[y1, x2], negative=[x1])
    return result[0][0]
analogy('woman', 'queen', 'man')    </code></pre>
</div>
</div>
</p>
<p>Now, I plan to use flask to create a small web application, so that users can find analogies via the webpage. For this I have a basic question</p>
<ol>
<li>I assume I need to save the model and then load it when I start the server. Please correct me  I am I am wrong.</li>
</ol>
<p>Here is the code that using Flask, it is working, but can you please suggest if saving model is required here?
2. Any suggestions to improve this code are welcome!</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>import numpy as np



from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
import os 

from flask import Flask, request


app = Flask(__name__)
@app.route(""/"", methods=['GET'])
def welcome():
    return ""Welcome to our Machine Learning REST API!""
@app.route(""/analogy"", methods=['GET'])
def analogy_route():
    word1 = request.args.get(""word1"")
    word2 = request.args.get(""word2"")
    word3 = request.args.get(""word3"")
    result = model.most_similar(positive=[word3, word2], negative=[word1])
    return str(result[0][0])
if __name__ == ""__main__"":
    glove_file = os.path.abspath('glove.6B/glove.6B.100d.txt')
    word2vec_glove_file = get_tmpfile(""glove.6B.100d.word2vec.txt"")
    glove2word2vec(glove_file, word2vec_glove_file)

    model = KeyedVectors.load_word2vec_format(word2vec_glove_file)
    app.run(host='0.0.0.0', port=5000, debug=True)</code></pre>
</div>
</div>
</p>
<p><a href=""https://i.stack.imgur.com/H5g4j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H5g4j.png"" alt=""enter image description here"" /></a></p>
","649994","","130288","","2020-12-15 17:31:50","2020-12-15 17:31:50","How to load and use word2vec model properly in a web-application via Flask RESTful APIs?","<flask><gensim><word2vec><flask-restful>","1","0","","","","CC BY-SA 4.0"
"65329886","1","","","2020-12-16 19:26:25","","0","119","<p>I have a set of documents that describe different dimensions of corporate culture. Tokenized examples below:</p>
<pre><code>sent1=['innovative','culture','fast','moving','company']
sent2=['manager','micromanage','all','time']
sent3=['slow','response','customer']
</code></pre>
<p>I've already applied Glove and Gensim w2v to the above documents. I'd like to identify documents that have high cosine similarity score to a sets of word, such as
<code>Innovation =['innovate','innovative','fast']</code></p>
<p>How do I calculate the cosine similarities between each document (e.g. sent1, sent2) and <code>Innovation</code> using Gensim?</p>
<p>Ideal Output:</p>
<pre><code>       innovation
sent1  0.98
sent2  0.45
sent3  -0.2
</code></pre>
","13668428","","13668428","","2020-12-16 19:43:16","2020-12-16 19:43:16","Calculate cosine similarity between sets of document and key words (e.g. ""innovate' ""fast"")","<python><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"57125117","1","57127704","","2019-07-20 13:17:11","","0","269","<p>I have about 9000 documents and I am using Gensim's <code>doc2vec</code> to embed my documents. My code is as follows:</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple

dataset = json.load(open(input_file))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')

for description in dataset:
    tags = [description[0]]
    words = description[1]
    docs.append(analyzedDocument(words, tags))

model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4, epochs = 20)
</code></pre>

<p>I would like to get all the documents related to topic ""deep learning"". i.e. the documents that mainly have content related to deep learning. Is it possible to do this in doc2vec model in gensim?</p>

<p>I am happy to provide more details if needed.</p>
","10704050","","","","","2019-07-20 18:56:36","How to get document vectors for a given topic in gensim","<python><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"22469506","1","","","2014-03-18 02:52:52","","6","3624","<p>I'm trying to model twitter stream data with topic models. Gensim, being an easy to use solution, is impressive in it's simplicity. It has a truly online implementation for LSI, but not for LDA. For a changing content stream like twitter, Dynamic Topic Models are ideal. Is there any way, or even a hack - an implementation or even a strategy, using which I can utilize Gensim for this purpose?</p>

<p>Are there any other python implementations which derive (preferably) from Gensim or independent? I am preferring python, since I want to get started asap, but if there is an optimum solution with some work, please mention it.</p>

<p>Thanks.</p>
","215179","","","","","2016-06-02 18:31:50","Are there any efficient python libraries for Dynamic Topic Models, preferably extending Gensim?","<python><lda><text-analysis><topic-modeling><gensim>","3","0","4","","","CC BY-SA 3.0"
"30563361","1","30676559","","2015-05-31 22:25:55","","0","1063","<p>I have two questions related to the usage of <strong>gensim</strong> for LDA. </p>

<p>1) How can I create a model using one corpus, save it and perhaps extend it later on another corpus by training the model on it ? Is it possible ?</p>

<p>2) Can LDA be used to classify an unseen document, or the model needs to be created again by including it in the corpus ? Is there an online way to do it and see the changes on the fly ?</p>

<p>I have a fairly basic understanding of LDA and have used it for Topic modeling on simple corpus using <strong>lda</strong> and <strong>gensim</strong> libraries.  Please point out any conceptual inconsistencies in the question. Thanks !</p>
","2233336","","","","","2017-03-20 07:40:50","Latent Dirichlet Allocation using Gensim on more than one corpus","<python><lda><topic-modeling><gensim>","1","2","0","","","CC BY-SA 3.0"
"65350423","1","65350500","","2020-12-18 00:58:05","","1","435","<p>I am trying to build a Doc2Vec model. I have a list of sentences with their labels, labeled using Gensim‚Äôs LabeledSentence() function. After building the model, I see that they used build_vocab() on the labeled sentences before training the model.</p>
<p>Can someone explain what does build_vocab() do and what happens if I don't use it !?</p>
<p>Please check out the following pictures:</p>
<p><a href=""https://i.stack.imgur.com/1OPNV.png"" rel=""nofollow noreferrer"">labeled sentences</a></p>
<p><a href=""https://i.stack.imgur.com/SsedC.png"" rel=""nofollow noreferrer"">model</a></p>
","14787455","","","","","2020-12-18 17:09:46","What does build_vocab() do exactly?","<python-3.x><nlp><gensim><doc2vec>","1","2","","","","CC BY-SA 4.0"
"57125757","1","57126960","","2019-07-20 14:44:18","","0","927","<p>I need to process a large number of <code>txt</code> files for building a <code>word2vec</code> model.
Now, my txt-files are a bit messy and I need to remove all ¬¥<code>\n</code>¬¥ newlines, read all sentences from my loaded string (txt-file) and then tokenize each sentence for using the word2vec model.</p>

<p>The thing is: I cant read the files line-by-line, cause some sentences do not end after one line. Therefore, I use ¬¥<code>nltk.tokenizer.tokenize()</code>¬¥, which splits the file into sentences.</p>

<blockquote>
  <p>I cant figure out, how to convert a list of strings into a list of list, where each sub-list contains the sentences, while passing it thourgh a generator.</p>
</blockquote>

<p>Or do I actually need to save each sentences into a new file (one sentence per line) to pass it through a generator?</p>

<p>Well, my code looks like this:
¬¥<code>tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')</code></p>

<pre><code># initialize tokenizer for processing sentences

class Raw_Sentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for file in file_loads: ## Note: file_loads includes directory name of files (e.g. 'C:/Users/text-file1.txt')
            with open(file,'r', encoding='utf-8') as t:     
               # print(tokenizer.tokenize(t.read().replace('\n', ' ')))           
                storage = tokenizer.tokenize(t.read().replace('\n', ' '))
# I tried to temporary store the list of sentences to a list for an iteration
                for sentence in storage:
                    print(nltk.word_tokenize(sentence))
                    yield nltk.word_tokenize(sentence)¬¥
</code></pre>

<p>So the goal is: 
load file 1: ¬¥<code>'some messy text here. And another sentence'</code>¬¥
 tokenize into sentences ¬¥<code>['some messy text here','And another sentence']</code>¬¥
and then split each sentence into words ¬¥<code>[['some','messy','text','here'],['And','another','sentence']]</code>¬¥</p>

<p>load file 2: <code>'some other messy text. sentence1. sentence2.'</code>
etc.</p>

<p>and input sentences into word2vec model:
¬¥<code>sentences = Raw_Sentences(directory)</code>¬¥</p>

<p>¬¥<code>model = gensim.models.Word2Vec(sentences)</code>¬¥</p>
","10053244","","","","","2019-07-20 17:16:02","Training word2vec model streaming data from file and tokenize to sentence","<python><streaming><nltk><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"41182372","1","41732973","","2016-12-16 10:33:16","","8","3867","<p>Please help me in understanding the difference between how <code>TaggedDocument</code> and <code>LabeledSentence</code> of <code>gensim</code> works. My ultimate goal is Text Classification using <code>Doc2Vec</code> model and any classifier. I am following this <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""noreferrer"">blog</a>!</p>

<pre><code>class MyLabeledSentences(object):
    def __init__(self, dirname, dataDct={}, sentList=[]):
        self.dirname = dirname
        self.dataDct = {}
        self.sentList = []
    def ToArray(self):       
        for fname in os.listdir(self.dirname):            
            with open(os.path.join(self.dirname, fname)) as fin:
                for item_no, sentence in enumerate(fin):
                    self.sentList.append(LabeledSentence([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no]))
        return sentList


class MyTaggedDocument(object):
    def __init__(self, dirname, dataDct={}, sentList=[]):
        self.dirname = dirname
        self.dataDct = {}
        self.sentList = []
    def ToArray(self):       
        for fname in os.listdir(self.dirname):            
            with open(os.path.join(self.dirname, fname)) as fin:
                for item_no, sentence in enumerate(fin):
                    self.sentList.append(TaggedDocument([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no]))
        return sentList

sentences = MyLabeledSentences(some_dir_name)
model_l = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5,     workers=7)
sentences_l = sentences.ToArray()
model_l.build_vocab(sentences_l )
for epoch in range(15): # 
    random.shuffle(sentences_l )
    model.train(sentences_l )
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model_l.alpha 

sentences = MyTaggedDocument(some_dir_name)
model_t = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5, workers=7)
sentences_t = sentences.ToArray()
model_l.build_vocab(sentences_t)
for epoch in range(15): # 
    random.shuffle(sentences_t)
    model.train(sentences_t)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model_l.alpha
</code></pre>

<p>My question is <code>model_l.docvecs['some_word']</code> is same as <code>model_t.docvecs['some_word']</code>?
Can you provide me weblink of good sources to get a grasp on how <code>TaggedDocument</code> or <code>LabeledSentence</code> works.</p>
","4555699","","","user6613600","2016-12-16 10:37:24","2017-01-19 02:59:31","What is the difference between gensim LabeledSentence and TaggedDocument","<gensim><text-classification><word2vec><doc2vec>","1","0","3","","","CC BY-SA 3.0"
"30628566","1","","","2015-06-03 19:07:00","","0","384","<p>I have trained few million words in <code>Word2Vec</code> of <code>Gensim</code> of Python. I want to update this trained model with new data. 
But from your previous posts and other sources around the web I came to know this is not possible.
So I am trying to create multiple models and dump them. Now I want to merge the models I am dumping. I want to use these dumped results. I got a previous post <a href=""https://stackoverflow.com/questions/30482669/merging-pretrained-models-in-word2vec"">Merging pretrained models in Word2Vec?</a>
but I am not getting how to do it. I came to know there is a library named deepdist, I am trying to see some experiments around:</p>

<pre><code>model = word2vec.Word2Vec.load_word2vec_format('/tmp/vectors.bin', binary=True)
</code></pre>

<ol>
<li>Is there a possible solution?</li>
<li>If any, one may kindly suggest how to do it?</li>
</ol>

<p>I am using Python2.7 on Windows 7 Professional. </p>
","4718092","","-1","","2017-05-23 11:46:18","2015-06-03 19:54:42","Averaging Multiple Models Word2vec Gensim","<python><python-2.7><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"57148357","1","","","2019-07-22 14:38:05","","2","3961","<p>I am building the vocabulary table using Doc2vec, but there is an error ""AttributeError: module 'gensim.utils' has no attribute 'smart_open'"". How do I solve this?</p>

<p>This is for a notebook on Databricks platform, running in Python 3. In the past, I've tried on running the code on a local Jupyter Notebook but the same error occurred.</p>

<p>I've also searched <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a> but could not find anything related to smart_open.</p>

<pre class=""lang-py prettyprint-override""><code>model = Doc2Vec(window=5, min_count=1, size=50, sample=1e-5, negative=5, workers=1)

model.build_vocab(sentences.to_array())
</code></pre>

<p>I ran the above lines separately. The first line worked fine. The second says:
 AttributeError: module 'gensim.utils' has no attribute 'smart_open'</p>
","11343736","","11343736","","2019-07-22 18:14:14","2020-07-25 14:49:35","AttributeError: module 'gensim.utils' has no attribute 'smart_open'","<python><gensim><databricks><doc2vec>","2","1","","","","CC BY-SA 4.0"
"51311240","1","","","2018-07-12 17:16:40","","0","454","<p>Using Word2vec (skip-gram) model in tensorflow , I wrote the code to obtain word embeddings from document-set.
The final embeddings are in numpy.ndarray format</p>

<p>Now to obtain similar documents , I need to use the WMD(Word Movers Distance) algorithm.</p>

<p>(I don't have much knowledge of gensim)
The gensim.similarities.WmdSimilarity() requires the embeddings to be in KeyedVectors data type (seems like) -- 
What can I do to implement WMD in my code.I have a tight deadline and can't give much time to writing the code of WMD from scratch .</p>
","9539527","","","","","2018-07-15 18:03:37","How to use WmdSimilarity function provided in gensim along with word embeddings which are in numpy.ndarray data type","<python-3.6><gensim><word2vec><numpy-ndarray><wmd>","1","1","","","","CC BY-SA 4.0"
"51330246","1","","","2018-07-13 17:35:01","","1","188","<p>I am training multiple word2vec models with Gensim. Each of the word2vec will have the same parameter and dimension, but trained with slightly different data. Then I want to compare how the change in data affected the vector representation of some words.</p>

<p>But every time I train a model, the vector representation of the same word is wildly different. Their similarity among other words remain similar, but the whole vector space seems to be rotated.</p>

<p>Is there any way I can rotate both of the word2vec representation in such way that same words occupy same position in vector space, or at least they are as close as possible.</p>

<p>Thanks in advance.</p>
","5482717","","","","","2018-07-13 18:09:20","How to rotate a word2vec onto another word2vec?","<gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"65865657","1","","","2021-01-23 23:33:46","","0","226","<pre><code>import gensim  
LDA = gensim.models.ldamodel.LdaModel 
dictionnary = corpora.Dictionary(docCleaned) #Error message appears here!!!
doc_term_matrix = [dictionary.doc2bow(doc) for doc in docCleaned]
</code></pre>
<p>Error Message -&gt;</p>
<blockquote>
<p>TypeError: doc2bow expects an array of unicode tokens on input, not a
single string</p>
</blockquote>
","15067598","","6573902","","2021-01-26 07:47:36","2021-01-26 07:56:46","Getting ""doc2bow expects an array of unicode tokens on input, not a single string"" as a try to do nlp using gensim"" Is there a solution?","<python><dictionary><nlp><gensim><lda>","1","1","","","","CC BY-SA 4.0"
"65876755","1","65876794","","2021-01-24 22:37:11","","0","594","<p>I tried to install gensim by <code>pip install gensim</code> but it fails with following pile of error on <strong>macs10.15.7 Catalina</strong> in <strong>python 3.9</strong></p>
<pre><code>      /Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/cmath:649:25: error: no template named 'numeric_limits'
      return _FloatBigger ? numeric_limits&lt;_IntT&gt;::max() :  (numeric_limits&lt;_IntT&gt;::max() &gt;&gt; _Bits &lt;&lt; _Bits);
                            ^
    fatal error: too many errors emitted, stopping now [-ferror-limit=]
    220 warnings and 20 errors generated.
    error: command '/usr/bin/clang' failed with exit code 1
    ----------------------------------------
ERROR: Command errored out with exit status 1: ~/.pyenv/versions/3.9.0/envs/jupyter/bin/python3.9 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/private/var/folders/pg/1drqvjn54tbczc1pl5qd8qwh0000gp/T/pip-install-ndvn1a9y/gensim_2b2eae30f7e140c0af90d98d9e598905/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/private/var/folders/pg/1drqvjn54tbczc1pl5qd8qwh0000gp/T/pip-install-ndvn1a9y/gensim_2b2eae30f7e140c0af90d98d9e598905/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /private/var/folders/pg/1drqvjn54tbczc1pl5qd8qwh0000gp/T/pip-record-_xbpvhdn/install-record.txt --single-version-externally-managed --compile --install-headers ~/.pyenv/versions/3.9.0/envs/jupyter/include/site/python3.9/gensim
</code></pre>
<p>I googled and could not find any solution for this. Could someone share some pointer</p>
","1053496","","","","","2021-01-24 22:41:14","gensim installation error on macs10.15.7 with 3.9.0","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"57157137","1","","","2019-07-23 05:20:00","","0","140","<p>I loaded a gensim continuous skipgram model from <code>http://vectors.nlpl.eu/repository/#</code> built on Google News 2013 with a vocabulary size of 2883863. However, I am getting an error message for any two random words I'm trying to get a similarity on.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format(r'C:\Users\Projects\NLPL\model.txt', binary=False)
model.similarity('president','minister')
</code></pre>

<p>While loading the model I also tried the binary file instead of <code>txt</code>file but that didn't work either.</p>

<pre><code>model = KeyedVectors.load_word2vec_format(r'C:\Users\Projects\NLPL\model.bin', binary=True)
</code></pre>

<p>Also for the similarity score, I tried using unicode characters in the parameters but that didn't work either.</p>

<pre><code>model.similarity(u'president',u'minister')
</code></pre>

<p>I'm pretty sure it is a huge corpus and should have these words and not sure why I'm not getting the results. I also tried some other common words such as <code>weapon</code>, military, car, etc. but a same error message.</p>
","9829523","","7583919","","2019-07-23 05:28:21","2019-07-23 05:28:21","raise KeyError(""word '%s' not in vocabulary"" % word) for gensim model","<python><gensim><word2vec><similarity>","0","4","","","","CC BY-SA 4.0"
"63916338","1","63926576","","2020-09-16 08:43:17","","0","178","<p>I have a &quot;corpus&quot; built from an item-item graph, which means each sentence is a graph walk path and each word is an item. I want to train a word2vec model upon the corpus to obtain items' embedding vectors. The graph is updated everyday so the word2vec model is trained in an increased way (using <code>Word2Vec.save()</code> and <code>Word2Vec.load()</code>) to keep updating the items' vectors.</p>
<p>Unlike words, the items in my corpus have their lifetime and there will be new items added in everyday. In order to prevent the constant growth of the model size, I need to drop items that reached their lifetime while keep the model trainable. I've read the similar question
<a href=""https://stackoverflow.com/questions/48941648/how-to-remove-a-word-completely-from-a-word2vec-model-in-gensim"">here</a>, but this question's answer doesn't related to increased-training and is based on <code>KeyedVectors</code>. I come up with the below code, but I'm not sure if it is correct and proper:</p>
<pre><code>from gensim.models import Word2Vec
import numpy as np

texts = [[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], [&quot;a&quot;, &quot;h&quot;, &quot;b&quot;]]
m = Word2Vec(texts, size=5, window=5, min_count=1, workers=1)

print(m.wv.index2word)
print(m.wv.vectors)

# drop old words
wordsToDrop = [&quot;b&quot;, &quot;c&quot;]
for w in wordsToDrop:
    i = m.wv.index2word.index(w)
    m.wv.index2word.pop(i)
    m.wv.vectors = np.delete(m.wv.vectors, i, axis=0)
    del m.wv.vocab[w]

print(m.wv.index2word)
print(m.wv.vectors)
m.save(&quot;m.model&quot;)
del m

# increased training
new = [[&quot;a&quot;, &quot;e&quot;, &quot;n&quot;], [&quot;r&quot;, &quot;s&quot;]]
m = Word2Vec.load(&quot;m.model&quot;)
m.build_vocab(new, update=True)
m.train(new, total_examples=m.corpus_count, epochs=2)
print(m.wv.index2word)
print(m.wv.vectors)
</code></pre>
<p>After deleting and increased training, is the <code>m.wv.index2word</code> and <code>m.wv.vectors</code> still element-wise corresponding? Is there any side-effect of above code? If my way is not good, could someone give me an example to show how to drop the old &quot;words&quot; properly and keep the model trainable?</p>
","6670282","","","","","2020-09-16 19:11:39","What is the best way to drop old ""words"" from gensim word2vec model?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"31687263","1","","","2015-07-28 21:17:27","","0","453","<p>I've got BOW vectors and I'm wondering if there's a supervised dimensionality reduction algorithm in sklearn or gensim capable of taking high-dimensional, supervised data and projecting it into a lower dimensional space which preserves the variance between these classes.</p>

<p>Actually I'm trying to find a proper metric for the classification/regression, and I believe using dimensionality can help me. I know there's unsupervised methods, but I want to keep the label information along the way.</p>
","4481166","","","","","2016-08-31 08:43:46","supervised dimensionality redunction/topic model using sklearn or gensim","<python><machine-learning><gensim><dimensionality-reduction>","2","0","","","","CC BY-SA 3.0"
"51281241","1","","","2018-07-11 08:56:14","","0","1649","<p>I am using gensim and executed the following code (simplified):</p>

<pre><code>model = gensim.models.Word2Vec(...)
mode.build_vocab(sentences)
model.train(...)
model.save('file_name')
</code></pre>

<p>After days my code finished <code>model.train(...)</code>. However, during saving, I experienced:</p>

<pre><code>Process finished with exit code 137 (interrupted by signal 9: SIGKILL)
</code></pre>

<p>I noticed that there were some npy files generated:</p>

<pre><code>&lt;...&gt;.trainables.syn1neg.npy
&lt;...&gt;.trainables.vectors_lockf.npy
&lt;...&gt;.wv.vectors.npy
</code></pre>

<p>Are those intermediate results I can re-use or do I have to rerun the entire process?</p>
","7125403","","","","","2018-07-11 21:39:18","Gensim Word2Vec Model trained but not saved","<gensim><word2vec>","1","0","1","","","CC BY-SA 4.0"
"36032410","1","","","2016-03-16 10:04:50","","1","2856","<p>When I use PyCharm to install gensim package, the installment failed.
The error messages were listed below:</p>

<p><a href=""https://i.stack.imgur.com/CEaTN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CEaTN.png"" alt=""enter image description here""></a></p>

<pre><code>Collecting gensim
  Using cached gensim-0.12.4-cp27-none-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl
Requirement already satisfied (use --upgrade to upgrade): scipy&gt;=0.7.0 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from gensim)
Collecting smart-open&gt;=1.2.1 (from gensim)
  Using cached smart_open-1.3.2-py2-none-any.whl
Requirement already satisfied (use --upgrade to upgrade): six&gt;=1.5.0 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/six-1.10.0-py2.7.egg (from gensim)
Requirement already satisfied (use --upgrade to upgrade): numpy&gt;=1.3 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from gensim)
Requirement already satisfied (use --upgrade to upgrade): boto&gt;=2.32 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from smart-open&gt;=1.2.1-&gt;gensim)
Requirement already satisfied (use --upgrade to upgrade): bz2file in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from smart-open&gt;=1.2.1-&gt;gensim)
Collecting requests==2.8.1 (from smart-open&gt;=1.2.1-&gt;gensim)
  Using cached requests-2.8.1-py2.py3-none-any.whl
Collecting httpretty==0.8.10 (from smart-open&gt;=1.2.1-&gt;gensim)
  Using cached httpretty-0.8.10-py2-none-any.whl
Installing collected packages: requests, httpretty, smart-open, gensim
  Found existing installation: requests 2.7.0
    Uninstalling requests-2.7.0:

You are using pip version 7.1.0, however version 8.1.0 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Exception:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/basecommand.py"", line 223, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/commands/install.py"", line 299, in run
    root=options.root_path,
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_set.py"", line 640, in install
    requirement.uninstall(auto_confirm=True)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_install.py"", line 726, in uninstall
    paths_to_remove.remove(auto_confirm)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_uninstall.py"", line 125, in remove
    renames(path, new_path)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/utils/__init__.py"", line 314, in renames
    shutil.move(old, new)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py"", line 303, in move
    os.unlink(src)
OSError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/requests-2.7.0.dist-info/DESCRIPTION.rst'
</code></pre>

<hr>

<p>And I also use pip to install, but also failed....
The error messages were listed below:</p>

<pre><code>chenguanyingdeMacBook-Pro:~ ChenGuanYing$ sudo pip install gensim
Password:
The directory '/Users/ChenGuanYing/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
You are using pip version 7.1.0, however version 8.1.0 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
The directory '/Users/ChenGuanYing/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
Collecting gensim
  Downloading gensim-0.12.4-cp27-none-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (2.5MB)
    100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.5MB 178kB/s 
Requirement already satisfied (use --upgrade to upgrade): scipy&gt;=0.7.0 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from gensim)
Collecting smart-open&gt;=1.2.1 (from gensim)
  Downloading smart_open-1.3.2-py2-none-any.whl
Collecting six&gt;=1.5.0 (from gensim)
  Downloading six-1.10.0-py2.py3-none-any.whl
Requirement already satisfied (use --upgrade to upgrade): numpy&gt;=1.3 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from gensim)
Collecting boto&gt;=2.32 (from smart-open&gt;=1.2.1-&gt;gensim)
  Downloading boto-2.39.0-py2.py3-none-any.whl (1.3MB)
    100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3MB 312kB/s 
Collecting bz2file (from smart-open&gt;=1.2.1-&gt;gensim)
  Downloading bz2file-0.98.tar.gz
Collecting requests==2.8.1 (from smart-open&gt;=1.2.1-&gt;gensim)
  Downloading requests-2.8.1-py2.py3-none-any.whl (497kB)
    100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 499kB 948kB/s 
Collecting httpretty==0.8.10 (from smart-open&gt;=1.2.1-&gt;gensim)
  Downloading httpretty-0.8.10-py2-none-any.whl
Installing collected packages: boto, bz2file, requests, httpretty, smart-open, six, gensim
  Running setup.py install for bz2file
  Found existing installation: requests 2.7.0
    Uninstalling requests-2.7.0:
      Successfully uninstalled requests-2.7.0
  Found existing installation: six 1.4.1
    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.
    Uninstalling six-1.4.1:
Exception:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/basecommand.py"", line 223, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/commands/install.py"", line 299, in run
    root=options.root_path,
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_set.py"", line 640, in install
    requirement.uninstall(auto_confirm=True)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_install.py"", line 726, in uninstall
    paths_to_remove.remove(auto_confirm)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/req/req_uninstall.py"", line 125, in remove
    renames(path, new_path)
  File ""/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg/pip/utils/__init__.py"", line 314, in renames
    shutil.move(old, new)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py"", line 302, in move
    copy2(src, real_dst)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py"", line 131, in copy2
    copystat(src, dst)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py"", line 103, in copystat
    os.chflags(dst, st.st_flags)
OSError: [Errno 1] Operation not permitted: '/tmp/pip-vV05uu-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six-1.4.1-py2.7.egg-info'
</code></pre>

<hr>

<p>Thank for your help, and sorry for that my English is very pool.</p>
","5699402","","","","","2016-05-10 02:47:20","python - Can't install gensim in Mac OS","<python><macos><python-2.7><pycharm><gensim>","3","0","","","","CC BY-SA 3.0"
"41162876","1","47925136","","2016-12-15 11:19:11","","14","6845","<p>I am using gensim word2vec package in python.
I would like to retrieve the <code>W</code> and <code>W'</code> weight matrices that have been learn during the skip-gram learning.</p>
<p>It seems to me that <code>model.syn0</code> gives me the first one but I am not sure how I can get the other one. Any idea?</p>
<p>I would actually love to find any exhaustive documentation on models accessible attributes because the official one does not seem to be precise (for instance <code>syn0</code> is not described as an attribute)</p>
","4560470","","6573902","","2021-02-02 07:18:26","2021-02-02 07:18:26","Get weight matrices from gensim word2Vec","<python><machine-learning><nlp><word2vec><gensim>","1","1","4","","","CC BY-SA 4.0"
"57138453","1","57152005","","2019-07-22 02:17:04","","1","168","<p>I am using the doc2vec model as follows to construct my document vectors.</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple

dataset = json.load(open(input_file))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')

for description in dataset:
    tags = [description[0]]
    words = description[1]
    docs.append(analyzedDocument(words, tags))

model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4, epochs = 20)
</code></pre>

<p>I have seen that <strong>gensim doc2vec also includes word vectors</strong>. Suppose I have a word vector created for the word <code>deep learning</code>. My question is; is it possible to get the <code>documents</code> nearest to <code>deep learning</code> word vector in gensim in python?</p>

<p>I am happy to provide more details if needed.</p>
","10704050","","1413687","","2019-07-22 05:35:59","2019-07-22 18:42:34","How to get the nearest documents for a word in gensim in python","<python><gensim><word2vec><doc2vec>","1","2","","","","CC BY-SA 4.0"
"22674660","1","","","2014-03-26 22:47:51","","1","1152","<p>Once I get topics for a document with lda[doc] I can print each topic using lda.print_topic(topic_id).</p>

<p>What's the equivalent way to retrieve topics for HdpModel? </p>

<p>One way I can think of is using hdp_to_lda to create an LdaModel. Is there a more straightforward way?</p>
","48782","","","","","2014-05-11 02:23:58","How to get the specific topics on HDP","<gensim>","1","0","","","","CC BY-SA 3.0"
"47901979","1","47918273","","2017-12-20 08:47:26","","0","274","<p>I am using the Doc2Vec model in gensim python library.</p>

<p>Every time I feeds the model with the same sentences data and set the parameter:seed of Doc2Vec to a fixed number, the model gives different vectors after the model is built.</p>

<p>For tests purpose, I need a determined result every time I gave a unchanged input data. I searched a lot and does not find a way to keep the gensim's result unchanged. </p>

<p>Is there anything wrong in the way I use it? thanks for replying in advance.</p>

<p>Here is my code:</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
model = Doc2Vec(sentences, dm=1, dm_concat=1, size=100, window=5, hs=0, min_count=10, seed=64)
result = model.docvecs
</code></pre>
","5777564","","5777564","","2017-12-20 13:38:00","2017-12-21 05:08:19","gensim doc2vec give non-determined result","<python><nlp><gensim>","1","2","","","","CC BY-SA 3.0"
"47893976","1","","","2017-12-19 19:30:31","","1","1272","<p>Below is the code I used to preprocess the text and apply text rank(I followed the gensim textrank tutorial). Please help me with a method to get better results. My text data is a column from a csv with more than 2000 rows. (each row, a sentence). </p>

<p>Output I get is 18 lines (Each different line, not a paragraph) of text as 
summary, and 20 words as keywords. Will the output be a paragraph of text as summary? Can we control the number of keywords to be displayed </p>

<pre><code>reg_ex = r'[^a-zA-Z]'
replace = ' '
wordnet_lemmatizer = WordNetLemmatizer()
#stop = stopwords.words('english')

comp_df = df['COMMENT'].str.replace(reg_ex, replace).apply(lambda t: ' '.join([wordnet_lemmatizer.lemmatize(w)for w in t.split()])).str.lower()

aa = comp_df.to_string()

import requests

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

from gensim.summarization import summarize
from gensim.summarization import keywords


print ('Summary:')
print (summarize(aa,ratio=0.01))

print ('\nKeywords:')
print (keywords(aa, ratio=0.01))
</code></pre>
","8382950","","","","","2018-07-18 09:44:25","Gensim text rank:","<python><nlp><gensim><summarization>","1","0","","","","CC BY-SA 3.0"
"57157390","1","","","2019-07-23 05:44:44","","2","122","<p>I have a large corpus (~100 million documents, 59GB) in a CSV. I want to create a TF-IDF vector and do some feature engineering on the data, but it's too large to load into memory all at once (I'm working on Google Colab, GPU with 12GB RAM). I imagine there is a way to process the data in chunks and then combine the TF-IDFs at the end but I'm not sure how to proceed. Here's my code so far:</p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

chunks = pd.read_csv(""data.csv.bz2"", 
                     chunksize=1000000,
                     nrows=120000000,
                    )

print(type(chunks))  # &lt;class 'pandas.io.parsers.TextFileReader'&gt;
</code></pre>

<p>Then, after removing stopwords and punctuation, lemmatizing (WordNetLemmatizer()), and stemming (SnowballStemmer('english')):</p>

<pre><code>count_vectorizer = CountVectorizer()
chunk1_counts = count_vectorizer.fit_transform(chunk1.comment)

tfidf_transformer = TfidfTransformer()
chunk1_tfidf = tfidf_transformer.fit_transform(chunk1_counts)
</code></pre>

<p>I can read in a few chunks at a time, but to avoid memory errors I'll probably have to save the results to disk and delete the objects from memory before processing the next set of chunks.</p>

<p>At that point, what's the process to combine the multiple TF-IDFs?</p>
","9677043","","","","","2019-07-23 05:44:44","NLP Combining multiple TF-IDF matrices","<python><scikit-learn><gensim><tf-idf><tfidfvectorizer>","0","1","","","","CC BY-SA 4.0"
"40158051","1","","","2016-10-20 15:01:36","","0","222","<p>I have this sentences iterator that is using <code>yield</code> so it's formally a generator</p>

<pre><code>from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
import json
import os

class LyricsCorpus(object):

    def __init__(self, corpus, tokenize=False, deaccent=False):
        self.corpus = corpus
        self.tokenize = tokenize
        self.deaccent = deaccent

    def __iter__(self):
        for index,fname in enumerate( os.listdir(self.corpus) ):
            with open( os.path.join(self.corpus, fname) ) as data_file:
                data = json.load(data_file)
                for item in data:
                    if ""lyrics"" in item:
                        if ""lyrics_body"" in item[""lyrics""]:
                            if self.tokenize:
                                yield self.tokens( item[""lyrics""][""lyrics_body""] )
                            else:
                                yield item[""lyrics""][""lyrics_body""].split()
    '''
        This lowercases, tokenizes, de-accents (optional). ‚Äì the output are final tokens = unicode strings, that won‚Äôt be processed any further.
    '''
    def tokens(self,text):
        return [token for token in simple_preprocess(text, deacc=self.deaccent, min_len=2, max_len=15) if token not in STOPWORDS]
</code></pre>

<p>when running into <code>Word2vec</code> like</p>

<pre><code>min_count = 1
size = 50
window = 4
model = Word2Vec(corpus_iterator, min_count=min_count, size=size, window=window)
</code></pre>

<p>the iterator does not stop, <em>looping through the files in the corpus folder indefinitively</em>. This does not happen in a ordinary iteration like</p>

<pre><code>from LyricsCorpus import *
it=LyricsCorpus('./corpus')
[item for k in it]
</code></pre>
","758836","","1143894","","2019-05-18 20:25:12","2019-05-18 20:25:12","Gensim word2vec: iterator does not stop with yield","<python><gensim><word2vec>","0","12","","","","CC BY-SA 4.0"
"57373626","1","57383478","","2019-08-06 10:05:48","","0","412","<p>I have around 20k documents with 60 - 150 words. Out of these 20K documents, there are 400 documents for which the similar document are known. These 400 documents serve as my test data.</p>

<p>I am trying to find similar documents for these 400 datasets using gensim doc2vec. The paper ""Distributed Representations of Sentences and Documents"" says that ""The combination of PV-DM and PV-DBOW often work consistently better (7.42% in IMDB) and therefore recommended.""</p>

<p>So I would like to combine the vectors of these two methods and find cosine similarity with all the train documents and select the top 5 with the least cosine distance.</p>

<p>So what's the effective method to combine the vectors of these 2 methods: adding or averaging or any other method ???</p>

<p>After combining these 2 vectors I can normalise each vector and then find the cosine distance.</p>
","5679933","","","","","2019-08-06 20:22:26","How to combine vectors generated by PV-DM and PV-DBOW methods of doc2vec?","<python><nlp><gensim><doc2vec><sentence-similarity>","1","0","","","","CC BY-SA 4.0"
"56408849","1","","","2019-06-01 17:16:13","","3","1907","<p>I have used gensim LDA Topic Modeling to get associated topics from a corpus. Now I want to get the top 20 documents representing each topic: documents that have the highest probability in a topic. And I want to save them in a CSV file with this format: 4 columns for Topic ID, Topic words, probability of each word in the topic, top 20 documents for each topic.</p>

<p>I have tried get_document_topics which I think it is the best approach for this task:</p>

<p>all_topics = lda_model.get_document_topics(corpus, minimum_probability=0.0, per_word_topics=False)</p>

<p>But I am not sure how to get top 20 documents that best represent the topic and add them to the CSV file.</p>

<pre class=""lang-py prettyprint-override""><code>    data_words_nostops = remove_stopwords(processed_docs)
    # Create Dictionary
    id2word = corpora.Dictionary(data_words_nostops)
    # Create Corpus
    texts = data_words_nostops
    # Term Document Frequency
    corpus = [id2word.doc2bow(text) for text in texts]
    # Build LDA model
    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                               id2word=id2word,
                                               num_topics=20,
                                               random_state=100,
                                               update_every=1,
                                               chunksize=100,
                                               passes=10,
                                               alpha='auto',
                                               per_word_topics=True)

    pprint(lda_model.print_topics())
    #save csv
    fn = ""topic_terms5.csv""
    if (os.path.isfile(fn)):
        m = ""a""
    else:
        m = ""w""

    num_topics=20
    # save topic, term, prob data in the file
    with open(fn, m, encoding=""utf8"", newline='') as csvfile:
        fieldnames = [""topic_id"", ""term"", ""prob""]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if (m == ""w""):
            writer.writeheader()

        for topic_id in range(num_topics):
            term_probs = lda_model.show_topic(topic_id, topn=6)
            for term, prob in term_probs:
                row = {}
                row['topic_id'] = topic_id
                row['prob'] = prob
                row['term'] = term
                writer.writerow(row)

</code></pre>

<p>Expected result: CSV file with this format: 4 columns for Topic ID, Topic words, probability of each word, top 20 documents for each topic.</p>
","11517723","","","","","2021-07-20 01:23:27","After applying gensim LDA topic modeling, how to get documents with highest probability for each topic and save them in a csv file?","<python><csv><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"57391750","1","57394562","","2019-08-07 09:59:35","","2","294","<p>I have 250k text documents (tweets and newspaper articles) represented as vectors obtained with a doc2vec model. Now, I want to use a regressor (multiple linear regression) to predict continuous value outputs - in my case the UK Consumer Confidence Index. 
My code runs, since forever. What am I doing wrong?</p>

<p>I imported my data from Excel and splitted it into x_train and x_dev. The data are composed of preprocessed text and CCI continuous values. </p>

<pre><code># Import doc2vec model
dbow = Doc2Vec.load('dbow_extended.d2v')
dmm = Doc2Vec.load('dmm_extended.d2v')
concat = ConcatenatedDoc2Vec([dbow, dmm]) # model uses vector_size 400

def get_vectors(model, input_docs):
    vectors = [model.infer_vector(doc.words) for doc in input_docs]
    return vectors

# Prepare X_train and y_train
train_text = x_train[""preprocessed_text""].tolist()
train_tagged = [TaggedDocument(words=str(_d).split(), tags=[str(i)]) for i, _d in list(enumerate(train_text))]
X_train = get_vectors(concat, train_tagged)
y_train=x_train['CCI_UK']

# Fit regressor 
from sklearn import linear_model
reg = linear_model.LinearRegression()
reg.fit(X_train, y_train)

# Predict and evaluate
prediction=reg.predict(X_dev)
print(classification_report(y_true=y_dev,y_pred=prediction),'\n')
</code></pre>

<p>Since the fitting never completed, I wonder whether I am using a wrong input. However, no error message is shown and the code simply runs forever. What am I doing wrong?</p>

<p>Thank you so much for your help!!</p>
","11660156","","11660156","","2019-08-07 11:01:20","2019-08-07 12:29:43","How to use Sklearn linear regression with doc2vec input","<scikit-learn><linear-regression><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"23270933","1","","","2014-04-24 13:51:10","","4","1182","<p>I am using <code>GENSIM</code> on a corpus of 50000 documents along with a dictionary of around 4000 features. I also have a <code>LSI</code> model already prepared for the same. </p>

<p>Now I want to find the highest matching features for each of the added documents. To find the best features in a particular document, I am running gensim's similarity module for each of the features on all the documents. This gives us a score for each of the feature that we want to use later on. But as you can imagine, this is a costly process as we have to iterate over 50000 indices and run 4000 iterations of similarity on each. </p>

<p>I need a better way of doing this as I run out of 8 GB memory on my system at around 1000 iterations. There's actually no reason for the memory to keep rising as I am only reallocating it during the iterations. Surprisingly the memory starts rising only after around 200 iterations. </p>

<ol>
<li>Why the memory issue? How can it be solved?</li>
<li>Is there a better way of finding the highest scored features in a particular document (not topics)?</li>
</ol>

<p>Here's a snippet of the code that runs out of memory:</p>

<pre><code>dictionary = corpora.Dictionary.load('features-dict.dict')
corpus = corpora.MmCorpus('corpus.mm')
lsi = models.LsiModel.load('model.lsi')
corpus_lsi = lsi[corpus]
index = similarities.MatrixSimilarity(list(corpus_lsi))
newDict = dict()

for feature in dictionary.token2id.keys():
  vec_bow = dictionary.doc2bow([feature])
  vec_lsi = lsi[vec_bow]
  sims = index[vec_lsi]
  li = sorted(enumerate(sims * 100), key=lambda item: -item[1])

  for data in li:
    dict[data[0]] = (feature,data[1]) # Store feature and score for each document


# Do something with the dict created above
</code></pre>

<p>EDIT: </p>

<p>The memory issue was resolved using a memory profiler. There was something else in that loop that caused it to rise drastically.</p>

<p>Let me explain the purpose in detail. Imagine we are dealing with various recipes (each recipe is document) and each item in our dictionary is an ingredient. Find six such recipes below.</p>

<p><code>corpus = [[Olive Oil, Tomato, Brocolli, Oregano], [Garlic, Olive Oil, Bread, Cheese, Oregano], [Avocado, Beans, Cheese, Lime], [Jalepeneo, Lime, Tomato, Tortilla, Sour Cream], [Chili Sauce, Vinegar, Mushrooms, Rice], [Soy Sauce, Noodles, Brocolli, Ginger, Vinegar]]</code></p>

<p>There are thousands of such recipes. What I am trying to achieve is to assign a weight between 0 and 100 to each of the ingredient (where higher weighted ingredient is the most important or most unique). What would be the best way to achieve this.</p>
","1109240","","1109240","","2014-05-09 06:27:16","2014-05-09 06:27:16","Use Gensim for scoring features in each document. Also a Python memory issue","<python><memory-management><dictionary><nlp><gensim>","1","0","2","","","CC BY-SA 3.0"
"65219346","1","","","2020-12-09 15:04:07","","0","172","<p>I'm trying to make topic modeling to my articles dataset. I'm creating a corpus with <code>CountVectorizer</code> and then finding topics with <code>gensim.models.ldamodel</code>.
When I'm sending my corpus for the <code>fit_lda</code> function, I'm getting this error:</p>
<pre><code>IndexError: index 8922 is out of bounds for axis 1 with size 4573 
</code></pre>
<p>I have seen similar questions, for example <a href=""https://stackoverflow.com/questions/50214899/indexerror-when-trying-to-update-gensims-ldamodel"">this</a> or <a href=""https://stackoverflow.com/questions/23853828/python-indexerror-using-gensim-for-lda-topic-modeling"">this</a> but unfortunately, I can't seem to apply these solutions to my problem.</p>
<p>my full code is:</p>
<pre><code>tf_vectorizer_001 = CountVectorizer(max_df=0.001, min_df= 10,  stop_words=get_update_stopwords())
tf_001 = tf_vectorizer_001.fit_transform(articles_df[&quot;article_text&quot;].values.astype('U'))
vocab_001 = tf_vectorizer_001.get_feature_names()

tf_vectorizer_01 = CountVectorizer(max_df=0.01, min_df= 10,  stop_words=get_update_stopwords())
tf_01 = tf_vectorizer_01.fit_transform(articles_df[&quot;article_text&quot;].values.astype('U'))
vocab_01 = tf_vectorizer_01.get_feature_names()

def fit_lda(X, vocab, num_topics=5, passes=20, alpha=0.01, eta=0.01):
    &quot;&quot;&quot; Fit LDA from a scipy CSR matrix (X). &quot;&quot;&quot;
    print_current_time()
    print('fitting lda. topics= ', num_topics, &quot;alpha= &quot;, alpha, &quot;eta= &quot;, eta)
    return LdaModel(matutils.Sparse2Corpus(X), num_topics=num_topics,
                    passes=passes,
                    id2word=dict([(i, s) for i, s in enumerate(vocab)]))


alhpha_values = np.arange(0.001, 0.01, 0.00025).tolist()
eta_values = np.arange(0.001, 0.1, 0.00025).tolist()
topic_amounts = np.arange(10, 100, 5).tolist()

for topic_amount in topic_amounts:
    for alpha in alhpha_values:
        for eta in eta_values:
            print(&quot;max df = 0.001: \n &quot;)
            lda_001 = fit_lda(tf_001, vocab_001, num_topics=topic_amount, alpha=alpha, eta=eta)
            print(&quot;finish&quot;)
            print_topics(lda_001, topic_amount, alpha, eta, 10)
            print(&quot;&quot;)
            del lda_001

            print(&quot;max df = 0.01: \n &quot;)
            lda_01 = fit_lda(tf_01, vocab_01, num_topics=topic_amount, alpha=alpha, eta=eta)
            print(&quot;finish&quot;)
            print_topics(lda_01, topic_amount, alpha, eta, 10)
            print(&quot;&quot;)
            del lda_01
</code></pre>
<p>The full log error is:</p>
<pre><code>&lt;ipython-input-2-9614097c70e5&gt;:1: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.
  runfile('C:/Users/amit_/PycharmProjects/articles/topics_genism.py', wdir='C:/Users/amit_/PycharmProjects/articles')
C:\Users\amit_\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['◊ë◊ê◊ô◊ñ◊ï', '◊ë◊ë◊ê◊ô◊ñ◊ï', '◊ë◊ë◊û◊ß◊ï◊ù', '◊ë◊ë◊©◊¢◊î', '◊ë◊î◊°◊ô◊ë◊î', '◊ë◊ï_◊ê◊™◊û◊ï◊ú', '◊ë◊ï_◊ë◊ô◊ï◊™◊®', '◊ë◊ï_◊î◊ô◊ï◊ù', '◊ë◊ï_◊î◊¢◊®◊ë', '◊ë◊ï_◊ô◊ó◊°◊ô◊™', '◊ë◊ï_◊û◊ê◊ï◊ì', '◊ë◊ï_◊û◊ì◊ô', '◊ë◊ï_◊û◊ü', '◊ë◊ï_◊û◊©◊û◊¢◊ï◊™◊ô', '◊ë◊ï_◊¢◊ú', '◊ë◊ï_◊¢◊ù', '◊ë◊ú◊ê◊ô◊ñ◊ï', '◊ë◊ú◊û◊ß◊ï◊ù', '◊ë◊û◊ê◊ô◊ñ◊ï', '◊î◊ë◊ê◊ô◊ñ◊ï', '◊î◊ë◊û◊ß◊ï◊ù', '◊î◊ë◊©◊¢◊î', '◊î◊î◊°◊ô◊ë◊î', '◊î◊ú◊ê◊ô◊ñ◊ï', '◊î◊ú◊û◊ß◊ï◊ù', '◊î◊û◊ê◊ô◊ñ◊ï', '◊ï◊ë◊ê◊ô◊ñ◊ï', '◊ï◊ë◊û◊ß◊ï◊ù', '◊ï◊ë◊©◊¢◊î', '◊ï◊î◊°◊ô◊ë◊î', '◊ï◊ú◊ê◊ô◊ñ◊ï', '◊ï◊ú◊û◊ß◊ï◊ù', '◊ï◊û◊ê◊ô◊ñ◊ï', '◊õ◊©◊ë◊ê◊ô◊ñ◊ï', '◊õ◊©◊ë◊û◊ß◊ï◊ù', '◊õ◊©◊ë◊©◊¢◊î', '◊õ◊©◊î◊°◊ô◊ë◊î', '◊õ◊©◊ú◊ê◊ô◊ñ◊ï', '◊õ◊©◊ú◊û◊ß◊ï◊ù', '◊õ◊©◊û◊ê◊ô◊ñ◊ï', '◊ú◊ê◊ô◊ñ◊ï', '◊ú◊ë◊ê◊ô◊ñ◊ï', '◊ú◊ë◊û◊ß◊ï◊ù', '◊ú◊ë◊©◊¢◊î', '◊ú◊î◊°◊ô◊ë◊î', '◊ú◊ú◊ê◊ô◊ñ◊ï', '◊ú◊ú◊û◊ß◊ï◊ù', '◊ú◊û◊ê◊ô◊ñ◊ï', '◊û◊ê◊ô◊ñ◊ï', '◊ß◊ô◊ë◊ú◊ë◊ê◊ô◊ñ◊ï', '◊ß◊ô◊ë◊ú◊ë◊û◊ß◊ï◊ù', '◊ß◊ô◊ë◊ú◊ë◊©◊¢◊î', '◊ß◊ô◊ë◊ú◊î◊°◊ô◊ë◊î', '◊ß◊ô◊ë◊ú◊ú◊ê◊ô◊ñ◊ï', '◊ß◊ô◊ë◊ú◊ú◊û◊ß◊ï◊ù', '◊ß◊ô◊ë◊ú◊û◊ê◊ô◊ñ◊ï', '◊©◊ë◊ê◊ô◊ñ◊ï', '◊©◊ë◊í◊ú◊ú◊î', '◊©◊ë◊í◊ú◊ú◊î_◊ê◊™◊û◊ï◊ú', '◊©◊ë◊í◊ú◊ú◊î_◊ë◊ô◊ï◊™◊®', '◊©◊ë◊í◊ú◊ú◊î_◊î◊ô◊ï◊ù', '◊©◊ë◊í◊ú◊ú◊î_◊î◊¢◊®◊ë', '◊©◊ë◊í◊ú◊ú◊î_◊ô◊ó◊°◊ô◊™', '◊©◊ë◊í◊ú◊ú◊î_◊û◊ê◊ï◊ì', '◊©◊ë◊í◊ú◊ú◊î_◊û◊ì◊ô', '◊©◊ë◊í◊ú◊ú◊î_◊û◊ü', '◊©◊ë◊í◊ú◊ú◊î_◊û◊©◊û◊¢◊ï◊™◊ô', '◊©◊ë◊í◊ú◊ú◊î_◊¢◊ú', '◊©◊ë◊í◊ú◊ú◊î_◊¢◊ù', '◊©◊ë◊ï', '◊©◊ë◊ï_◊ê◊™◊û◊ï◊ú', '◊©◊ë◊ï_◊ë◊ô◊ï◊™◊®', '◊©◊ë◊ï_◊î◊ô◊ï◊ù', '◊©◊ë◊ï_◊î◊¢◊®◊ë', '◊©◊ë◊ï_◊ô◊ó◊°◊ô◊™', '◊©◊ë◊ï_◊û◊ê◊ï◊ì', '◊©◊ë◊ï_◊û◊ì◊ô', '◊©◊ë◊ï_◊û◊ü', '◊©◊ë◊ï_◊û◊©◊û◊¢◊ï◊™◊ô', '◊©◊ë◊ï_◊¢◊ú', '◊©◊ë◊ï_◊¢◊ù', '◊©◊ë◊û◊ß◊ï◊ù', '◊©◊ë◊©◊¢◊î', '◊©◊î◊°◊ô◊ë◊î', '◊©◊ú◊ê◊ô◊ñ◊ï', '◊©◊ú◊û◊ß◊ï◊ù', '◊©◊û◊ê◊ô◊ñ◊ï'] not in stop_words.
  warnings.warn('Your stop_words may be inconsistent with '
max df = 0.001: 
 
The current is: 
2020-12-09 16:45:22.145026+02:00
fitting lda. topics=  10 alpha=  0.001 eta=  0.001
Traceback (most recent call last):
  File &quot;C:\Users\amit_\anaconda3\lib\site-packages\IPython\core\interactiveshell.py&quot;, line 3343, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File &quot;&lt;ipython-input-2-9614097c70e5&gt;&quot;, line 1, in &lt;module&gt;
    runfile('C:/Users/amit_/PycharmProjects/articles/topics_genism.py', wdir='C:/Users/amit_/PycharmProjects/articles')
  File &quot;C:\Users\amit_\AppData\Local\JetBrains\PyCharm 2020.2.3\plugins\python\helpers\pydev\_pydev_bundle\pydev_umd.py&quot;, line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File &quot;C:\Users\amit_\AppData\Local\JetBrains\PyCharm 2020.2.3\plugins\python\helpers\pydev\_pydev_imps\_pydev_execfile.py&quot;, line 18, in execfile
    exec(compile(contents+&quot;\n&quot;, file, 'exec'), glob, loc)
  File &quot;C:/Users/amit_/PycharmProjects/articles/topics_genism.py&quot;, line 2057, in &lt;module&gt;
    lda_001 = fit_lda(tf_001, vocab_001, num_topics=topic_amount, alpha=alpha, eta=eta)
  File &quot;C:/Users/amit_/PycharmProjects/articles/topics_genism.py&quot;, line 2033, in fit_lda
    return LdaModel(matutils.Sparse2Corpus(X), num_topics=num_topics,
  File &quot;C:\Users\amit_\anaconda3\lib\site-packages\gensim\models\ldamodel.py&quot;, line 519, in __init__
    self.update(corpus, chunks_as_numpy=use_numpy)
  File &quot;C:\Users\amit_\anaconda3\lib\site-packages\gensim\models\ldamodel.py&quot;, line 980, in update
    gammat = self.do_estep(chunk, other)
  File &quot;C:\Users\amit_\anaconda3\lib\site-packages\gensim\models\ldamodel.py&quot;, line 742, in do_estep
    gamma, sstats = self.inference(chunk, collect_sstats=True)
  File &quot;C:\Users\amit_\anaconda3\lib\site-packages\gensim\models\ldamodel.py&quot;, line 680, in inference
    expElogbetad = self.expElogbeta[:, ids]
IndexError: index 8922 is out of bounds for axis 1 with size 4573
</code></pre>
","7169209","","7169209","","2020-12-09 18:38:20","2020-12-09 18:38:20","IndexError out of bounds, using gensim for LDA Topic Modeling","<python><nlp><gensim><lda><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"56418980","1","56419460","","2019-06-02 21:11:50","","0","135","<p>Suppose, I have a Seq2Seq model. I want to have the Embedding layer in this model.</p>

<p>Based on my research I can do it in three ways:</p>

<ol>
<li><p>train a word embedding separately on my data set or download a pre-trained word embedding, then use the weights of those embedding as the weight of the words in my data set. So here I do not need to have an embedding layer at all, I just load the weights of the already trained words into the words in my data set.</p></li>
<li><p>I create an embedding layer and set the trainable true, so not only I have an embedding, but also, that embedding will be trained based on my task</p></li>
<li><p>I create an Embedding layer, load already trained weights, and set trainable False. in this case, the weights will not get updated.</p></li>
</ol>

<p>(please correct me if Im wrong).</p>

<p>I have used the <code>first approach</code>. I want to know what will be the interpretation of the output of this code:</p>

<pre><code>model_wv = Word2Vec.load(""word2vec_50d_7w"")
embeddings = np.zeros((len(model_wv.wv.vocab), emb_dim))
for i in range(len(model_wv.wv.vocab)):
    # print(i)
    embedding_vector = model_wv.wv[model_wv.wv.index2word[i]]
    if embedding_vector is not None:
        embeddings[i] = embedding_vector

print(embeddings[[1,2,3],[3,4,1]])
</code></pre>

<p>this is the output:</p>

<pre><code>[-0.01566689 -1.36469996  0.59684211]
</code></pre>

<p>consider this <code>[1,2,3],[3,4,1]</code> as two sequence with <code>length=3</code>.</p>

<p>I was thinking we use word embedding in lstm to transform each word of the sequence into an embedding. I expected to see <code>two vectors</code> and <code>three items</code> in each vector.</p>

<p>The embedding is the word2vec in gensim,</p>

<p>Appreciate it if someone shed light on it where I am getting lost?</p>

<p>Thanks~</p>
","7934786","","7934786","","2019-06-02 21:29:54","2019-06-02 23:11:55","word embedding of a lstm sequence","<tensorflow><keras><lstm><gensim><word-embedding>","1","0","","","","CC BY-SA 4.0"
"47898159","1","","","2017-12-20 02:57:57","","1","516","<p>I am trying to build a word2vec  similarity dictionary. I was able to build one dictionary but the similarities are not being populated correctly. Am I missing anything in my code?</p>

<p><strong>Input sample data  Text</strong></p>

<pre><code>TAK PO LUN UNIT 3 15/F WAYSON COMMERCIAL G 28 CONNAUGHT RD WEST SHEUNG WAN
- EDDY SUSANTO YAHYA ROOM 1503-05 WESTERN CENTRE 40-50 DES VOEUX W. SHEUNG WAN
DNA FINANCIAL SYSTEMS INC UNIT 10 19F WAYSON COMMERCIAL 28 CONNAUGHT RD SHEUNG WAN
G/F 60 PO HING FONG SHEUNG WAN
10B CENTRAL MANSION 270 QUEENS RD CENTRAL SHEUNG WAN
AKAMAI INTERNATIONAL BV C/O IADVANTAGE 28/F OF MEGA I-ADVANTAGE 399 CHAI WAN RD CHAI WAN HONG KO HONG KONG
VICTORIA CHAN F/5E 1-3 FLEMING RD WANCHI WAN CHAI
HISTREND 365 5/F FOO TAK BUILDING 365 HENNESSY RD WAN CHAI H WAN CHAI
ROOM 1201 12F CHINACHEM JOHNSO PLAZA 178 186 JOHNSTON RD WAN CHAI
LUEN WO BUILDING 339 HENNESSY RD 9 FLOOR WAN CHAI HONG KONG
</code></pre>

<p><strong>My code:</strong> </p>

<pre><code>import gensim
from gensim import corpora,similarities,models
class AccCorpus(object):

   def __init__(self):
       self.path = ''

   def __iter__(self):
       for sentence in data[""Adj_Addr""]:
           yield [word.lower() for word in sentence.split()]

   def build_corpus():
       model = gensim.models.word2vec.Word2Vec(alpha=0.05, min_alpha=0.05,window=2,sg=1)
       sentences = AccCorpus()
       model.build_vocab(sentences)
       for epoch in range(1):
           model.train(sentences,total_examples=model.corpus_count, epochs=model.iter)
           model.alpha -= 0.002  # decrease the learning rate
           model.min_alpha = model.alpha  # fix the learning rate, no decay

       model_name = ""word2vec_model""
       model.save(model_name)
       return model

model=build_corpus()
</code></pre>

<p><strong>My results:</strong></p>

<pre><code>model.most_similar(""wan"")
[('want', 0.6867533922195435),
 ('puiwan', 0.6323356032371521),
 ('wan.', 0.6132887005805969),
 ('wanstreet', 0.5945449471473694),
 ('aupuiwan', 0.594132661819458),
 ('futan', 0.5883135199546814),
 ('fotan', 0.5817855000495911),
 ('shanmei', 0.5807071924209595),
 ('30-33', 0.5789132118225098),
 ('61-63au', 0.5711270570755005)]
</code></pre>

<p>Here are my expected outputs for the similarity: <strong>sheungwan, wanchai, chaiwan</strong>. I am guessing my skipgrams are not working properly. How can I fix this?</p>
","8793219","","712995","","2017-12-20 15:56:38","2017-12-20 15:56:38","Skip-gram with Word2Vec not working properly","<scikit-learn><neural-network><word2vec><gensim><word-embedding>","1","2","","","","CC BY-SA 3.0"
"56408959","1","56411523","","2019-06-01 17:31:23","","1","73","<p>I already trained a word2vec model with gensim library. For example, my model contains vectors for 2 words: ""new"" and ""york"". However, I also want to train a vector for the word ""new york"", so I transform ""new york"" into ""new_york"" and train a new vector model. Finally, I want to combine 3 vectors: vector of the word ""new"", ""york"" and ""new_york"" into one vector representation for the word ""new york"".</p>

<p>How can I save the new vector value to the model?</p>

<p>I try to assign the new vector to the model but gensim did not allow to assign the new value for vector model.</p>
","5860504","","","","","2019-06-02 01:06:49","Create a new vector model in gensim","<python><vector><gensim><word2vec>","1","0","1","","","CC BY-SA 4.0"
"40160526","1","","","2016-10-20 17:11:33","","1","175","<p>wiki= gensim.corpora.MmCorpus(r'C:\Users\Public\Documents\Python Scripts\wiki_en_vocab200k.mm')</p>

<p>tfidf= gensim.models.TfidfModel.load(r'C:\Users\tfidf_model')</p>

<p>corpus_tfidf = tfidf[wiki]</p>

<p>I have the above steps but now I want to be able to find individual word scores</p>

<p>Many thanks in advance</p>
","6166236","","","","","2016-10-20 17:11:33","How do I get the TF-IDF score for a particular word in Gensim","<python><nlp><gensim>","0","0","","","","CC BY-SA 3.0"
"51376241","1","51387619","","2018-07-17 08:05:51","","0","608","<p>While I trained d2v on a large text corpus I received these 3 files: </p>

<pre><code>doc2vec.model.trainables.syn1neg.npy

doc2vec.model.vocabulary.cum_table.npy

doc2vec.model.wv.vectors.npy
</code></pre>

<p>Bun final model has not saved, because there was not enough free space available on the disk. </p>

<pre><code>OSError: 5516903000 requested and 4427726816 written
</code></pre>

<p>Is there a way to resave my model using these files in a shorter time, than all training time? </p>

<p>Thank you in advance! </p>
","5975112","","5975112","","2018-07-17 08:35:03","2018-07-17 17:55:06","Gensim Doc2vec trained, but not saved","<model><save><gensim><word-embedding><doc2vec>","1","0","0","","","CC BY-SA 4.0"
"31685048","1","31687769","","2015-07-28 19:05:06","","0","696","<p>I am new to Web2Py and Python stack. I need to use a module in my Web2Py application which uses ""gensim"" and ""nltk"" libraries. I tried installing these into my Python 2.7 on a Windows 7 environment but came across several errors due to some issues with ""numpy"" and ""scipy"" installations on Windows 7. Then I ended up resolving those errors by uninstalling Python 2.7 and instead installing Anaconda Python which successfully installed the required ""gensim"" and ""nltk"" libraries. </p>

<p>So, at this stage I am able to see all these ""gensim"" and ""nltk"" libraries resolving properly without any error in ""Spyder"" and ""PyCharm"". However, when I run my application in Web2Py, it still complains about ""gensim"" and gives this error: <code>&lt;type 'exceptions.ImportError'&gt; No module named gensim</code></p>

<p>My guess is if I can configure Web2Py to use the Anaconda Python then this issue would be resolved. </p>

<p>I need to know if it's possible to configure Web2Py to use Anaconda Python and if it is then how do I do that? </p>

<p>Otherwise, if someone knows of some other way resolve that ""gensim"" error in Web2Py kindly share your thoughts.</p>

<p>All your help would be highly appreciated.</p>
","1443527","","","","","2015-07-28 21:52:08","Configure Web2Py to use Anaconda Python","<python-2.7><web2py><anaconda><gensim>","1","2","","","","CC BY-SA 3.0"
"23877375","1","23885178","","2014-05-26 20:35:36","","30","13586","<p>Word2vec seems to be mostly trained on raw corpus data. However, lemmatization is a standard preprocessing for many semantic similarity tasks. I was wondering if anybody had experience in lemmatizing the corpus before training word2vec and if this is a useful preprocessing step to do.</p>
","511005","","6613889","","2018-04-13 14:37:46","2019-04-16 06:55:47","word2vec lemmatization of corpus before training","<nlp><word2vec><gensim><lemmatization>","2","3","5","","","CC BY-SA 3.0"
"41194588","1","","","2016-12-17 01:39:23","","2","767","<p>I am trying to install gensim on EC2 (RedHat - t2-micro) to build a REST API. The overall installation process is really long so I shorten the code to replicate the error I am getting. Here below the shorter version:</p>

<ul>
<li><p>sudo yum update -y (EC2 update)</p></li>
<li><p>sudo yum install nano (nano editor)</p></li>
<li>sudo yum -y install gcc-c++ python27-devel atlas-sse3-devel lapack-devel (various Gensim related packages)</li>
<li>sudo yum -y install httpd (Apache)</li>
<li>sudo yum install mod_wsgi (WSGI)</li>
<li>curl -O <a href=""https://bootstrap.pypa.io/get-pip.py"" rel=""nofollow noreferrer"">https://bootstrap.pypa.io/get-pip.py</a> then sudo python2.7 get-pip.py (PIP)</li>
<li>virtualenv -p python2.7 /tmp/my_app then . /tmp/my_app/bin/activate (virtual env)</li>
</ul>

<p>With my virtual env setup, I then added numpy, scipy, and gensim:
- pip install -U --force numpy
- pip install -U --force scipy
- pip install -U --force </p>

<p>Now if I add a simple file with the following code:</p>

<pre><code># -*- coding: utf-8 -*-

import gensim

def main():
    return ""Hello my AVABBBCCCDDDEEE world!""

if __name__ == ""__main__"":
    main()
</code></pre>

<p>and run ""python file.py"" in the terminal (MACOS), i get the following error:</p>

<p>/tmp/ava_app/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.
  warnings.warn(""Pattern library is not installed, lemmatization won't be available."")
No handlers could be found for logger ""gensim.models.doc2vec""</p>

<p>Any idea? The error results in a 500 error in the browser.</p>

<p>Many thanks, Stephane</p>
","5293067","","","","","2016-12-17 01:39:23","gensim on EC2: installation issue","<amazon-ec2><installation><gensim>","0","0","","","","CC BY-SA 3.0"
"59067555","1","","","2019-11-27 10:01:20","","1","453","<p>I am getting KeyError:0 when running this code in python:</p>

<pre><code>full_pipeline.fit(X_train, y_train)
</code></pre>

<p>Here is the completed code:</p>

<pre><code>from gensim.sklearn_api import D2VTransformer
from sklearn.pipeline import FeatureUnion, Pipeline 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

name_pipeline = Pipeline( steps = [ 
                              ( 'feature_selector', FeatureSelector(['name']) ),
                              ( 'feature_transformer', D2VTransformer() ) ] )

description_pipeline = Pipeline( steps = [ 
                              ( 'feature_selector', FeatureSelector(['description']) ),
                              ( 'feature_transformer', D2VTransformer() ) ] )

X_pipeline = FeatureUnion( transformer_list = [ 
                                                  ( 'name_pipeline', name_pipeline ), 
                                                  ( 'description_pipeline', description_pipeline ) ] )

#Split up the train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)

clf = LogisticRegression(random_state=0, class_weight='balanced', solver='lbfgs', max_iter=1000, multi_class='multinomial')

full_pipeline = Pipeline( steps = 
                         [ ( 'pipeline', X_pipeline),
                          ( 'model', clf ) ] )

full_pipeline.fit(X_train, y_train)
</code></pre>

<p>And here is the error I'm getting:</p>

<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2896             try:
-&gt; 2897                 return self._engine.get_loc(key)
   2898             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 0

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
19 frames
&lt;ipython-input-14-0ddbaedffb67&gt; in &lt;module&gt;()
     25                           ( 'model', clf ) ] )
     26 
---&gt; 27 full_pipeline.fit(X_train, y_train)

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    350             This estimator
    351         """"""
--&gt; 352         Xt, fit_params = self._fit(X, y, **fit_params)
    353         with _print_elapsed_time('Pipeline',
    354                                  self._log_message(len(self.steps) - 1)):

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in _fit(self, X, y, **fit_params)
    315                 message_clsname='Pipeline',
    316                 message=self._log_message(step_idx),
--&gt; 317                 **fit_params_steps[name])
    318             # Replace the transformer of the step with the fitted
    319             # transformer. This is necessary when loading the transformer

/usr/local/lib/python3.6/dist-packages/joblib/memory.py in __call__(self, *args, **kwargs)
    353 
    354     def __call__(self, *args, **kwargs):
--&gt; 355         return self.func(*args, **kwargs)
    356 
    357     def call_and_shelve(self, *args, **kwargs):

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    714     with _print_elapsed_time(message_clsname, message):
    715         if hasattr(transformer, 'fit_transform'):
--&gt; 716             res = transformer.fit_transform(X, y, **fit_params)
    717         else:
    718             res = transformer.fit(X, y, **fit_params).transform(X)

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    910             sum of n_components (output dimension) over transformers.
    911         """"""
--&gt; 912         results = self._parallel_func(X, y, fit_params, _fit_transform_one)
    913         if not results:
    914             # All transformers are None

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in _parallel_func(self, X, y, fit_params, func)
    940             message=self._log_message(name, idx, len(transformers)),
    941             **fit_params) for idx, (name, transformer,
--&gt; 942                                     weight) in enumerate(transformers, 1))
    943 
    944     def transform(self, X):

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in __call__(self, iterable)
   1001             # remaining jobs.
   1002             self._iterating = False
-&gt; 1003             if self.dispatch_one_batch(iterator):
   1004                 self._iterating = self._original_iterator is not None
   1005 

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
    832                 return False
    833             else:
--&gt; 834                 self._dispatch(tasks)
    835                 return True
    836 

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in _dispatch(self, batch)
    751         with self._lock:
    752             job_idx = len(self._jobs)
--&gt; 753             job = self._backend.apply_async(batch, callback=cb)
    754             # A job can complete so quickly than its callback is
    755             # called before we get here, causing self._jobs to

/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py in apply_async(self, func, callback)
    199     def apply_async(self, func, callback=None):
    200         """"""Schedule a func to be run""""""
--&gt; 201         result = ImmediateResult(func)
    202         if callback:
    203             callback(result)

/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py in __init__(self, batch)
    580         # Don't delay the application, to avoid keeping the input
    581         # arguments in memory
--&gt; 582         self.results = batch()
    583 
    584     def get(self):

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in __call__(self)
    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    255             return [func(*args, **kwargs)
--&gt; 256                     for func, args, kwargs in self.items]
    257 
    258     def __len__(self):

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in &lt;listcomp&gt;(.0)
    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    255             return [func(*args, **kwargs)
--&gt; 256                     for func, args, kwargs in self.items]
    257 
    258     def __len__(self):

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    714     with _print_elapsed_time(message_clsname, message):
    715         if hasattr(transformer, 'fit_transform'):
--&gt; 716             res = transformer.fit_transform(X, y, **fit_params)
    717         else:
    718             res = transformer.fit(X, y, **fit_params).transform(X)

/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    391                 return Xt
    392             if hasattr(last_step, 'fit_transform'):
--&gt; 393                 return last_step.fit_transform(Xt, y, **fit_params)
    394             else:
    395                 return last_step.fit(Xt, y, **fit_params).transform(Xt)

/usr/local/lib/python3.6/dist-packages/sklearn/base.py in fit_transform(self, X, y, **fit_params)
    554         else:
    555             # fit method of arity 2 (supervised transformation)
--&gt; 556             return self.fit(X, y, **fit_params).transform(X)
    557 
    558 

/usr/local/lib/python3.6/dist-packages/gensim/sklearn_api/d2vmodel.py in fit(self, X, y)
    158 
    159         """"""
--&gt; 160         if isinstance(X[0], doc2vec.TaggedDocument):
    161             d2v_sentences = X
    162         else:

/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in __getitem__(self, key)
   2993             if self.columns.nlevels &gt; 1:
   2994                 return self._getitem_multilevel(key)
-&gt; 2995             indexer = self.columns.get_loc(key)
   2996             if is_integer(indexer):
   2997                 indexer = [indexer]

/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2897                 return self._engine.get_loc(key)
   2898             except KeyError:
-&gt; 2899                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2900         indexer = self.get_indexer([key], method=method, tolerance=tolerance)
   2901         if indexer.ndim &gt; 1 or indexer.size &gt; 1:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 0
</code></pre>

<p>Does anyone know why might this happen? I think it has to do with D2VTransformer because when I'm running the code below I'm getting the same error:</p>

<pre><code>model = D2VTransformer(min_count=1, size=5)
docvecs = model.fit_transform(X_train) 
</code></pre>

<p>But when trying to select only one column from the dataframe:</p>

<pre><code>docvecs = model.fit_transform(X_train['name']) 
</code></pre>

<p>it doesn't throw an error and that is why when I created the pipelines I've only used one column, but still getting the error.</p>

<p>This is how <a href=""https://i.stack.imgur.com/87d5Q.png"" rel=""nofollow noreferrer"">X_train</a> looks.</p>

<pre><code>name    description
9107    way great entrepreneur push limit help succeed  way great entrepreneur push limit
7706    dit het team week week  dit het team week week
3995    decorate home jewel tone    feel bold colour choice inspire fill home abun...
5220    attic meat district attic meat district
3412    tee apparel choose design item clothe accessory piece inde...
... ... ...
3830    marque web designer mode    marque web designer
3261    design holiday rest bite try lear magazine dai...   design holiday rest bite try lear
2415    hallucinatory house father spirit   music room hold tower season rug produce early...
7223    jacket rise jacket rise
4697    cupcake bake explorer   love love chocolate cupcake top kind easy foll...
</code></pre>

<p>And some more details about X_train:</p>

<pre><code>X_train.shape
(7159, 2)

X_train.dtypes
name           object
description    object
dtype: object
</code></pre>
","12444552","","12444552","","2019-11-28 22:44:34","2019-11-30 18:00:47","Python KeyError: 0 when using D2VTransformer","<python><dataframe><pipeline><gensim><doc2vec>","1","6","","","","CC BY-SA 4.0"
"41223299","1","","","2016-12-19 13:07:29","","0","2412","<p>I am learning <code>Doc2Vec</code> model from <code>gensim</code> library and using it as follows:</p>

<pre><code>class MyTaggedDocument(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            with open(os.path.join(self.dirname, fname),encoding='utf-8') as fin:
                print(fname)
                for item_no, sentence in enumerate(fin):
                    yield LabeledSentence([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no])
sentences = MyTaggedDocument(dirname)
model = Doc2Vec(sentences,min_count=2, window=10, size=300, sample=1e-4, negative=5, workers=7)
</code></pre>

<p>The input <code>dirname</code> is a directory path which has , for the sake of simplicity, only 2 files located with each file containing more than 100 lines. I am getting following Exception.</p>

<p><a href=""https://i.stack.imgur.com/T6INA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T6INA.png"" alt=""Output""></a></p>

<p>Also, with <code>print</code> statement I could see that the iterator iterated over directory 6 times. Why is this so?</p>

<p>Any kind of help would be appreciated.</p>
","4555699","","","","","2017-01-19 02:52:01","Gensim Doc2Vec Exception AttributeError: 'str' object has no attribute 'words'","<python><neural-network><gensim><word2vec><doc2vec>","1","4","0","","","CC BY-SA 3.0"
"40149951","1","","","2016-10-20 09:03:16","","1","1811","<p>I follow <a href=""http://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim"" rel=""nofollow noreferrer"">this</a> tutorial and everything is fine, I preprocess and train my model But when I want to find similarity with following code:</p>

<pre><code>model = gensim.models.Word2Vec.load(""wiki.fa.word2vec.model"")
print model.most_similar(U'ŸÖÿßŸá')
</code></pre>

<p>Vectors are not in proper format:</p>

<pre><code>[(u'\u0631\u0648\u0632', 0.6399222612380981), (u'\u0647\u0641\u062a\u0647', 0.5578583478927612), (u'\u0645\u0627\u0647\u0647\u0627\u06cc', 0.5577661991119385), (u'\u062f\u0631\u0645\u0627\u0647', 0.5260834097862244), (u'\u0634\u0627\u0645\u06af\u0627\u0647', 0.5142802596092224), (u'\u06cc\u06a9\u0645\u0627\u0647', 0.48211610317230225), (u'\u0642\u062f\u06cc\u0631\u0641', 0.4799095690250397), (u'\u06cc\u06a9\u0633\u0627\u0644', 0.47623544931411743), (u'\u0645\u0627\u0647\u0647', 0.46996498107910156), (u'\u062d\u0648\u062a', 0.4551585912704468)]
</code></pre>

<p>Does anyone knows how to fix this issue?</p>
","1462770","","4422856","","2019-03-18 19:57:59","2019-03-18 19:57:59","Arabic/Persian language isn't printed correctly to screen","<python><gensim><word2vec>","1","0","1","","","CC BY-SA 4.0"
"56412272","1","56416972","","2019-06-02 04:51:47","","0","192","<p>I use gensim Doc2Vec package to train doc2vec embeddings. I would expect that two models trained with the identical parameters and data would have very close values of the doc2vec vectors. However, in my experience it is only true with doc2vec trained in the PV-DBOW without training word embedding (dbow_words = 0).
For PV-DM and for PV-DBOW with dbow_words = 1, i.e. every case the word embedding are trained along with doc2vec, the doc2vec embedding vectors for identically trained models are fairly different. </p>

<p>Here is my code</p>

<pre class=""lang-py prettyprint-override""><code>    from sklearn.datasets import fetch_20newsgroups
    from gensim import models
    import scipy.spatial.distance as distance
    import numpy as np
    from nltk.corpus import stopwords
    from string import punctuation
    def clean_text(texts,  min_length = 2):
        clean = []
        #don't remove apostrophes
        translator = str.maketrans(punctuation.replace('\'',' '), ' '*len(punctuation))
        for text in texts:
            text = text.translate(translator)
            tokens = text.split()
            # remove not alphabetic tokens
            tokens = [word.lower() for word in tokens if word.isalpha()]
            # filter out stop words
            stop_words = stopwords.words('english')
            tokens = [w for w in tokens if not w in stop_words]
            # filter out short tokens
            tokens = [word for word in tokens if len(word) &gt;= min_length]
            tokens = ' '.join(tokens)
            clean.append(tokens)
        return clean
    def tag_text(all_text, tag_type =''):
        tagged_text = []
        for i, text in enumerate(all_text):
            tag = tag_type + '_' + str(i)
            tagged_text.append(models.doc2vec.TaggedDocument(text.split(), [tag]))
        return tagged_text

    def train_docvec(dm, dbow_words, min_count, epochs, training_data):
        model = models.Doc2Vec(dm=dm, dbow_words = dbow_words, min_count = min_count)
        model.build_vocab(tagged_data)
        model.train(training_data, total_examples=len(training_data), epochs=epochs)    
        return model

    def compare_vectors(vector1, vector2):
        cos_distances = []
        for i in range(len(vector1)):
            d = distance.cosine(vector1[i], vector2[i])
            cos_distances.append(d)
        print (np.median(cos_distances))
        print (np.std(cos_distances))    

    dataset = fetch_20newsgroups(shuffle=True, random_state=1,remove=('headers', 'footers', 'quotes'))
    n_samples = len(dataset.data)
    data = clean_text(dataset.data)
    tagged_data = tag_text(data)
    data_labels = dataset.target
    data_label_names = dataset.target_names

    model_dbow1 = train_docvec(0, 0, 4, 30, tagged_data)
    model_dbow2 = train_docvec(0, 0, 4, 30, tagged_data)
    model_dbow3 = train_docvec(0, 1, 4, 30, tagged_data)
    model_dbow4 = train_docvec(0, 1, 4, 30, tagged_data)
    model_dm1 = train_docvec(1, 0, 4, 30, tagged_data)
    model_dm2 = train_docvec(1, 0, 4, 30, tagged_data)

    compare_vectors(model_dbow1.docvecs, model_dbow2.docvecs)
    &gt; 0.07795828580856323
    &gt; 0.02610614028793008

    compare_vectors(model_dbow1.docvecs, model_dbow3.docvecs)
    &gt; 0.6476179957389832
    &gt; 0.14797587172616306

    compare_vectors(model_dbow3.docvecs, model_dbow4.docvecs)
    &gt; 0.19878000020980835
    &gt; 0.06362519480831186

    compare_vectors(model_dm1.docvecs, model_dm2.docvecs)
    &gt; 0.13536489009857178
    &gt; 0.045365127475424386

    compare_vectors(model_dbow1.docvecs, model_dm1.docvecs)
    &gt; 0.6358324736356735
    &gt; 0.15150255674571805
</code></pre>

<blockquote>
  <p>UPDATE</p>
</blockquote>

<p>I tried, as suggested by gojomo, to compare the differences between the vectors, and, unfortunately, those are even worse:</p>

<pre class=""lang-py prettyprint-override""><code>def compare_vector_differences(vector1, vector2):
    diff1 = []
    diff2 = []
    for i in range(len(vector1)-1):
        diff1.append( vector1[i+1] - vector1[i])
    for i in range(len(vector2)-1):
        diff2[i].append(vector2[i+1] - vector2[i])
    cos_distances = []
    for i in range(len(diff1)):
        d = distance.cosine(diff1[i], diff2[i])
        cos_distances.append(d)
    print (np.median(cos_distances))
    print (np.std(cos_distances))    

compare_vector_differences(model_dbow1.docvecs, model_dbow2.docvecs)
&gt; 0.1134452223777771
&gt; 0.02676398444178949

compare_vector_differences(model_dbow1.docvecs, model_dbow3.docvecs)
&gt; 0.8464127033948898
&gt; 0.11423789350773429

compare_vector_differences(model_dbow4.docvecs, model_dbow3.docvecs)

&gt; 0.27400463819503784
&gt; 0.05984108730423529
</code></pre>

<blockquote>
  <p>SECOND UPDATE</p>
</blockquote>

<p>This time, after I finally understood gojomo, the things look fine.</p>

<pre class=""lang-py prettyprint-override""><code>def compare_distance_differences(vector1, vector2):
    diff1 = []
    diff2 = []
    for i in range(len(vector1)-1):
        diff1.append( distance.cosine(vector1[i+1], vector1[i]))
    for i in range(len(vector2)-1):
        diff2.append( distance.cosine(vector2[i+1], vector2[i]))
    diff_distances = []
    for i in range(len(diff1)):
        diff_distances.append(abs(diff1[i] - diff2[i]))
    print (np.median(diff_distances))
    print (np.std(diff_distances))    

compare_distance_differences(model_dbow1.docvecs, model_dbow2.docvecs)
&gt;0.017469733953475952
&gt;0.01659284710785352

compare_distance_differences(model_dbow1.docvecs, model_dbow3.docvecs)
&gt;0.0786697268486023
&gt;0.06092163158218411

compare_distance_differences(model_dbow3.docvecs, model_dbow4.docvecs)
&gt;0.02321992814540863
&gt;0.023095123172320778
</code></pre>
","2299692","","2299692","","2019-06-03 17:20:17","2019-06-03 17:20:17","Discrepancies in gensim doc2vec embedding vectors","<gensim><word-embedding><doc2vec>","1","0","0","","","CC BY-SA 4.0"
"67518203","1","67520554","","2021-05-13 11:07:55","","2","142","<p>I'm trying to extract morphs/similar words in Sinhala language using Fasttext.
But FastText takes a 1 second for 2.64 words. How can I increase the speed without changing the model size?</p>
<p>My code looks like this:</p>
<pre><code>import fasttext
fasttext.util.download_model('si', if_exists='ignore')  # Sinhala
ft = fasttext.load_model('cc.si.300.bin')
words_file = open(r'/Datasets/si_words_filtered.txt')
words = words_file.readlines()
words = words[0:300]
synon_dict = dict()
from tqdm import tqdm_notebook
for i in tqdm_notebook(range(len(words))):
    word = words[i].strip()
    synon = ft.get_nearest_neighbors(word)[0][1] ### takes a lot of time
    if is_strictly_sinhala_word(synon):
        synon_dict[word] = synon
import json
with open(&quot;out.json&quot;, &quot;w&quot;, encoding='utf8') as f:
    json.dump(synon_dict, f, ensure_ascii=False)

</code></pre>
","7205625","","7205625","","2021-05-14 05:13:53","2021-05-14 05:13:53","How to run Fasttext get_nearest_neighbors() faster?","<python><machine-learning><nlp><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"41273573","1","","","2016-12-21 23:23:28","","1","10843","<p>I have a bIg data platform. On that I install Anaconda. When I ssh to my account on the platform and open up a Python interpreter in terminal it works fine when I import the gensim library. I had earlier installed this library using </p>

<pre><code>Conda install gensim

$ python
Python 2.7.12 |Anaconda 2.5.0 (64-bit)| (default, Jul  2 2016, 17:42:40) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
&gt;&gt;&gt; import gensim
/home/anaconda2/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.
  warnings.warn(""Pattern library is not installed, lemmatization won't be available."")
</code></pre>

<p>If you see it does import the library (just gives some warning for Pattern library). However when I open up Jupyter notebook and try to import the same library there it gives the following:</p>

<pre><code>In [11]:

import gensim 
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-11-0539d76422c6&gt; in &lt;module&gt;()
----&gt; 1 import gensim

ImportError: No module named gensim
</code></pre>

<p>Am not sure why the same library which is installed is not working in Jupyter. Please note that when I do </p>

<pre><code>pip list
</code></pre>

<p>it shows me all the libraries and it has gensim there. </p>
","2769240","","2769240","","2016-12-21 23:30:53","2020-11-26 19:00:22","Gensim Library not recognized in Jupyter notebook","<python><gensim>","6","0","","","","CC BY-SA 3.0"
"47905576","1","","","2017-12-20 11:59:12","","2","1641","<p>I have created document vectors for a large corpus using Gensim's doc2vec.</p>

<p><code>sentences=gensim.models.doc2vec.TaggedLineDocument('file.csv')</code></p>

<p><code>model = gensim.models.doc2vec.Doc2Vec(sentences,size = 10, window = 800, min_count = 1, workers=40, iter=10, dm=0)</code></p>

<p>Now I am using Gensim's infer_vector() using those document vectors to create document vectors for another sample corpus</p>

<p><code>Eg: model.infer_vector('This is a string')</code></p>

<p>Is there a way to pass the entire DataFrame through infer_vector and get the output vectors for each line in the DataFrame?</p>
","8581716","","","","","2017-12-21 13:11:43","How to use Gensim Doc2vec infer_vector() for large DataFrame?","<python><gensim><doc2vec>","2","3","1","","","CC BY-SA 3.0"
"57391090","1","","","2019-08-07 09:24:38","","1","111","<p>Distributed Online LDA is implemented by <a href=""https://spark.apache.org/docs/latest/ml-clustering.html#latent-dirichlet-allocation-lda"" rel=""nofollow noreferrer"">Spark MLlib</a> and <a href=""https://radimrehurek.com/gensim/dist_lda.html"" rel=""nofollow noreferrer"">Gensim</a>. I would like to choose one of them to do my project.</p>

<p>At present, I already have a hadoop system running with 7 worker nodes.</p>

<p>Could someone having experience with both give a recommendation and point out pros and cons of them, e.g. the difficulty of cluster setup, speed of the modelling process, etc. Thanks.</p>
","9672303","","9672303","","2019-08-07 12:02:36","2019-08-07 12:02:36","Comparison between Distributed Online LDA implemented by PySpark and Gensim","<pyspark><nlp><apache-spark-mllib><gensim><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"24861239","1","24862357","","2014-07-21 09:03:08","","-1","544","<p>I'm using Django for a search engine. Requests are made by POST, the server treats them and answers with JSON format. In order to be fast, I need to have an index file loaded at the beginning (with manage.py runserver) and a way to access it when a view is called.</p>

<p>Does anyone know how to do it ?</p>

<p>Thanks in advance !</p>
","3437221","","","","","2014-07-21 09:59:16","Load file with django manage.py runserver","<python><django><gensim>","1","4","","","","CC BY-SA 3.0"
"41330637","1","","","2016-12-26 11:34:38","","2","1366","<p>It may be very basic problem but i have stuck with this since 2 hours.
I am trying to execute below line of code:</p>

<pre><code>from gensim.models.phrases import Phrases , Phraser
</code></pre>

<p>but i am getting error: <em>cannot import name 'Phraser'</em> as for as i know we get this kind of error when <em>Phraser</em> is neither variable nor function in <em>gensim.models.phrases</em> but i have checked gensim's homepage and found this:</p>

<blockquote>
  <p>class gensim.models.phrases.Phraser(phrases_model)</p>
</blockquote>

<p>I have gensim's latest module 0.13.4 and i am using Python 3.5.2 |Anaconda 4.1.1 (64-bit), on windows 10.</p>
","5468983","","","","","2017-07-09 17:20:28","Gensim: cannot import name 'Phraser'","<python><gensim>","1","3","0","","","CC BY-SA 3.0"
"46141998","1","","","2017-09-10 14:51:35","","0","450","<p>I trying to install gensim using the following command:</p>

<pre><code>sudo pip install gensim
</code></pre>

<p>I got the following error messages:</p>

<pre><code>The directory '/home/woojung/.cache/pip/http' or its parent directory is
not owned by the current user and the cache has been disabled. Please  
check the permissions and owner of that directory. If executing pip with 
sudo, you may want sudo's -H flag.


The directory '/home/woojung/.cache/pip' or its parent directory is not 
owned by the current user and caching wheels has been disabled. check the   
permissions and owner of that directory. If executing pip with sudo, you  
may want sudo's -H flag.
Collecting gensim
Downloading gensim-2.3.0.tar.gz (17.2MB)
99% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17.2MB 422kB/s eta  
0:00:01Exception:
Traceback (most recent call last):
File ""/home/woojung/.local/lib/python2.7/site-packages 
/pip/basecommand.py"", line 215, in main
status = self.run(options, args)
File ""/home/woojung/.local/lib/python2.7/site-packages/pip/commands 
/install.py"", line 324, in run
requirement_set.prepare_files(finder)

....


File ""/home/woojung/.local/lib/python2.7/site-packages/pip/_vendor
/cachecontrol/filewrapper.py"", line 50, in _close
self.__callback(self.__buf.getvalue())
File ""/home/woojung/.local/lib/python2.7/site-packages/pip/_vendor
/cachecontrol/controller.py"", line 275, in cache_response
self.serializer.dumps(request, response, body=body),
File ""/home/woojung/.local/lib/python2.7/site-packages/pip/_vendor
/cachecontrol/serialize.py"", line 87, in dumps
).encode(""utf8""),
MemoryError
</code></pre>

<p>I installed numpy and scipy. How can I fix this problem?</p>
","8587926","","8587926","","2017-09-10 15:15:35","2017-09-10 15:21:15","install gensim error in ubuntu","<python><ubuntu><gensim>","1","2","","","","CC BY-SA 3.0"
"50390455","1","","","2018-05-17 11:35:16","","1","1009","<p>I extracted 145,185,965 sentences (14GB) out of the english wikipedia dump and I want to train a Doc2Vec model based on these sentences. Unfortunately I have 'only' 32GB of RAM and get a <em>MemoryError</em> when trying to train. Even if I set the min_count to 50, gensim tells me that it would need over 150GB of RAM. I don't think that further increasing the min_count would be a good idea, because the resulting model would be not very good (just a guess). But anyways, I will try it with 500 to see if memory is sufficient then.</p>

<p>Are there any possibilities to train such a large model with limited RAM?</p>

<p>Here is my current code:</p>

<pre><code>corpus = TaggedLineDocument(preprocessed_text_file)
model = Doc2Vec(vector_size=300, 
                window=15, 
                min_count=50,  #1
                workers=16, 
                dm=0, 
                alpha=0.75, 
                min_alpha=0.001, 
                sample=0.00001,
                negative=5)
model.build_vocab(corpus)
model.train(corpus, 
            epochs=400, 
            total_examples=model.corpus_count, 
            start_alpha=0.025, 
            end_alpha=0.0001)
</code></pre>

<p>Are there maybe some obvious mistakes I am doing? Using it completely wrong?</p>

<p>I could also try reducing the vector size, but I think this will result in much worse results as most papers use 300D vectors.</p>
","3827381","","","","","2018-05-17 17:04:50","gensim - Doc2Vec: MemoryError when training on english Wikipedia","<python><out-of-memory><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"50390582","1","50397071","","2018-05-17 11:41:46","","4","1411","<p>When reading the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Doc2Vec documentation of gensim</a>, I get a bit confused about some options. For example, the constructor of Doc2Vec has a parameter <em>iter</em>:</p>

<blockquote>
  <p>iter (int) ‚Äì Number of iterations (epochs) over the corpus.</p>
</blockquote>

<p>Why does the train method then also have a similar parameter called <em>epochs</em>?</p>

<blockquote>
  <p>epochs (int) ‚Äì Number of iterations (epochs) over the corpus.</p>
</blockquote>

<p>What is the difference between both? There's one more paragraph on it in the docs:</p>

<blockquote>
  <p>To avoid common mistakes around the model‚Äôs ability to do multiple
  training passes itself, an explicit epochs argument MUST be provided.
  In the common and recommended case, where train() is only called once,
  the model‚Äôs cached iter value should be supplied as epochs value.</p>
</blockquote>

<p>But I do not really understand why the constructor needs a <em>iter</em> parameter and what exactly should be provided for it.</p>

<p><strong>EDIT</strong>:</p>

<p>I just saw that there is also the possibility to specify the corpus directly in the constructor rather than calling train() separately. So I think in this case, <em>iter</em> would be used and otherwise <em>epochs</em>. Is that correct?</p>

<p>If so, what is the difference between specifying the corpus in the constructor and calling train() manually? Why would one choose the one or other?</p>

<p><strong>EDIT 2</strong>:</p>

<p>Although not mentioned in the docs, <em>iter</em> is now depreciated as parameter of Doc2Vec. It was renamed to <em>epochs</em> to be consistent with the parameter of <em>train()</em>. Training seems to work with that, although I struggle with <a href=""https://stackoverflow.com/questions/50390455/gensim-doc2vec-memoryerror-when-training-on-english-wikipedia"">MemoryErrors</a>.</p>
","3827381","","3827381","","2018-05-17 14:04:15","2018-05-17 17:13:01","gensim - Doc2Vec: Difference iter vs. epochs","<python><gensim><doc2vec>","1","2","2","","","CC BY-SA 4.0"
"41430565","1","41433975","","2017-01-02 16:55:15","","1","3141","<p>I'm writing my first app in python to use word2vec model.
Here is my simple code</p>

<pre><code>import gensim, logging
import sys
import warnings
from gensim.models import Word2Vec

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

def main(): 
    ####LOAD MODEL
    model = Word2Vec.load_word2vec_format('models/vec-cbow.txt', binary=False)  
    model.similarity('man', 'women')

if __name__ == '__main__':
    with warnings.catch_warnings():
        warnings.simplefilter(""error"")
        #warnings.simplefilter(""ignore"")
    main()
</code></pre>

<p>I getting this the following error:</p>

<pre><code>UnicodeDecodeError: 'utf8' codec can't decode bytes in position 96-97: invalid continuation byte 
</code></pre>

<p>I tried solving it by adding these two lines, but I'm still getting the error. </p>

<pre><code>reload(sys)  # Reload does the trick!
sys.setdefaultencoding('UTF8') #UTF8 #latin-1
</code></pre>

<p>The w2v model was trained on English sentences.</p>

<p>EDIT: Here is the full stack:</p>

<pre><code>**%run ""...\getSimilarity.py""**
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
**...\getSimilarity.py in &lt;module&gt;()**
     64         warnings.simplefilter(""error"")
     65         #warnings.simplefilter(""ignore"")
---&gt; 66     main()

**...\getSimilarity.py in main()**
     30     ####LOAD MODEL
---&gt; 31     model = Word2Vec.load_word2vec_format('models/vec-cbow.txt', binary=False)  # C binary format
     32     model.similarity('man', 'women')

**...\AppData\Local\Enthought\Canopy\User\lib\site-packages\gensim-0.12.4-py2.7-win-amd64.egg\gensim\models\word2vec.pyc in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors)**
   1090             else:
   1091                 for line_no, line in enumerate(fin):
-&gt; 1092                     parts = utils.to_unicode(line.rstrip(), encoding=encoding, errors=unicode_errors).split("" "")
   1093                     if len(parts) != vector_size + 1:
   1094                         raise ValueError(""invalid vector on line %s (is this really the text format?)"" % (line_no))

**...\AppData\Local\Enthought\Canopy\User\lib\site-packages\gensim-0.12.4-py2.7-win-amd64.egg\gensim\utils.pyc in any2unicode(text, encoding, errors)**
    215     if isinstance(text, unicode):
    216         return text
--&gt; 217     return unicode(text, encoding, errors=errors)
    218 to_unicode = any2unicode
    219 

**...\AppData\Local\Enthought\Canopy\App\appdata\canopy-1.6.2.3262.win-x86_64\lib\encodings\utf_8.pyc in decode(input, errors)**
     14 
     15 def decode(input, errors='strict'):
---&gt; 16     return codecs.utf_8_decode(input, errors, True)
     17 
     18 class IncrementalEncoder(codecs.IncrementalEncoder):

**UnicodeDecodeError: 'utf8' codec can't decode bytes in position 96-97: invalid continuation byte** 
</code></pre>

<p>Any hints how to solve the problem?
Thanks in advance.</p>
","2520616","","2520616","","2017-01-02 17:25:53","2017-12-05 09:17:20","Encoding issue in python while using w2v","<python><gensim><word2vec>","3","5","","","","CC BY-SA 3.0"
"65198394","1","65199653","","2020-12-08 11:52:07","","1","63","<p>With Gensim, there are three functions I use regularly, for example this one:</p>
<pre><code>model = gensim.models.Word2Vec(corpus,size=100,min_count=5)
</code></pre>
<p>The output from gensim, but I cannot understand how to set the size and min_count parameters in the equivalent SciSpacy command of:</p>
<pre><code>model = spacy.load('en_core_web_md')
</code></pre>
<p>(The output is a model of embeddings (too big to add here))).</p>
<p>This is another command I regularly use:</p>
<pre><code>model.most_similar(positive=['car'])
</code></pre>
<p>and this is the output from gensim/Expected output from SciSpacy:</p>
<pre><code>[('vehicle', 0.7857330441474915),
 ('motorbike', 0.7572781443595886),
 ('train', 0.7457204461097717),
 ('honda', 0.7383008003234863),
 ('volkswagen', 0.7298516035079956),
 ('mini', 0.7158907651901245),
 ('drive', 0.7093928456306458),
 ('driving', 0.7084407806396484),
 ('road', 0.7001082897186279),
 ('traffic', 0.6991947889328003)]
</code></pre>
<p>This is the third command I regularly use:</p>
<pre><code>print(model.wv['car'])
</code></pre>
<p>Output from Gensim/Expected output from SciSpacy (in reality this vector is length 100):</p>
<pre><code>    [ 1.0942473   2.5680697  -0.43163642 -1.171171    1.8553845  -0.3164575
  1.3645878  -0.5003705   2.912658    3.099512    2.0184739  -1.2413547
  0.9156444  -0.08406237 -2.2248871   2.0038593   0.8751471   0.8953876
  0.2207374  -0.157277   -1.4984075   0.49289042 -0.01171476 -0.57937795...]
</code></pre>
<p>Could someone show me the equivalent commands for SciSpacy? For example, for 'gensim.models.Word2Vec' I can't find how to specify the length of the vectors (size parameter), or the minimum number of times the word should be in the corpus (min_count) in SciSpacy (e.g. I looked <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/allenai/scispacy"" rel=""nofollow noreferrer"">here</a>), but I'm not sure if I'm missing them?</p>
","8407951","","4317058","","2020-12-08 13:22:35","2020-12-08 19:42:02","SciSpacy equivalent of Gensim's functions/parameters","<python><nlp><spacy><gensim>","1","2","","","","CC BY-SA 4.0"
"56418817","1","","","2019-06-02 20:43:08","","0","148","<p>I am following this blog trying to train doc2vec on wikipedia corpus using gensim. <a href=""https://markroxor.github.io/gensim/static/notebooks/doc2vec-wikipedia.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/doc2vec-wikipedia.html</a>.</p>

<p>I notice that the output is extremely case-sensitive. For example,</p>

<pre><code>string1=model.infer_vector(""machine Learning"".split())
string2=model.infer_vector(""computer Science"".split())
spatial.distance.cosine(string1, string2)
output is 0.25
</code></pre>

<p>If I change the case for the input,</p>

<pre><code>string1=model.infer_vector(""machine learning"".split())
string2=model.infer_vector(""computer science"".split())
spatial.distance.cosine(string1, string2)
output is 1.0535
</code></pre>

<p>I think that I should lower case everything before train the model. However, as shown in the link above, I first read the input directly:</p>

<pre><code>class TaggedWikiDocument(object):
    def __init__(self, wiki):
        self.wiki = wiki
        self.wiki.metadata = True
    def __iter__(self):
        for content, (page_id, title) in self.wiki.get_texts():
            yield TaggedDocument([c for c in content], [title])
wiki = WikiCorpus(""enwiki-latest-pages-articles-sample2.xml.bz2"")
documents = TaggedWikiDocument(wiki)
</code></pre>

<p>Any suggestion on how I can lower all the cases in the wiki document?</p>

<p>By the way, I used a very small file size for now just to test out whether there are errors in running the code, hence it can be the reason that infer_vector is very sensitive to case. Maybe this problem is minimum if I use the actual wiki dataset ?</p>
","3253821","","3253821","","2019-06-12 00:33:39","2019-06-12 00:33:39","How to train wiki with Doc2Vec Wiki with case insensitive features","<python-3.x><mediawiki><gensim><doc2vec>","0","2","","","","CC BY-SA 4.0"
"41440633","1","","","2017-01-03 09:50:20","","4","1714","<pre><code>sentences=gensim.models.doc2vec.TaggedLineDocument(""raw_docs.txt"")
model=gensim.models.Doc2Vec(sentences,min_count=1,iter=100)
sentence=TaggedDocument(words=[u'‰∏∫‰∫Ü'],tags=[u'T1'])
sentences1=[sentence]
model.build_vocab(sentences1,update=True)
model.train(sentences1)
print ""successful!""
</code></pre>

<p>I want to use a big data to train a doc2vec model. And I want to use this pretrained model to train a new text.</p>

<p>I only expect to train the new one with a pretrained model. How can I do that?The code above doesn't work...</p>
","7368736","","5859685","","2017-01-03 10:52:18","2017-01-03 10:52:18","How to train a new text with gensim doc2vec","<gensim><doc2vec>","0","6","","","","CC BY-SA 3.0"
"33200360","1","","","2015-10-18 16:31:42","","0","3076","<p>I just got acquainted with gensim and I tried to install it. I performed  any steps is written in page <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow"">https://radimrehurek.com/gensim/install.html</a> but I could not install it. I have installed python 2.7, scipy, numpy successfully on windows 8.1 64bit, but when I run setup.py in gensim it doesn't run.</p>

<p>Please help me I need to gensim Immediately and tell me installation steps with More details and other software that needs to be installed before it.
thanks</p>
","5459783","","1413133","","2015-10-18 16:43:35","2015-11-13 07:45:53","how to install gensim on windows 8.1","<python><numpy><gensim>","1","0","","","","CC BY-SA 3.0"
"67095698","1","67097448","","2021-04-14 16:35:15","","3","2459","<p>I am going to find the optimal number of topics for LDA. To do this, I used GENSIM as follows :</p>
<pre><code>def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values

    
</code></pre>
<p>But I have an attribute error: I used spyder.</p>
<pre><code>AttributeError: module 'gensim.models' has no attribute 'wrappers'
</code></pre>
","13887688","","","","","2021-06-09 16:07:30","Genism Module attribute error for wrappers","<python><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"58749620","1","","","2019-11-07 13:19:46","","2","1137","<p>There are a lot of examples of LDA Mallet topic modelling however non of them shows how to add dominant topic, percent contribution and topic keywords to the original dataframe. 
Let's assume this is the dataset and my code</p>

<h1>Dataset:</h1>

<pre><code>Document_Id   Text
1             'Here goes one example sentence that is generic'
2             'My car drives really fast and I have no brakes'
3             'Your car is slow and needs no brakes'
4             'Your and my vehicle are both not as fast as the airplane'
</code></pre>

<h1>Code</h1>

<pre><code># Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
import pandas as pd
df = pd.read_csv('data_above.csv')
data = df.Text.values.tolist() 
# Assuming I have done all the preprocessing, lemmatization and so on and ended up with data_lemmatized:

# Create Dictionary
id2word = corpora.Dictionary(data_lemmatized)

# Create Corpus
texts = data_lemmatized

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
model = gensim.models.ldamodel.LdaModel(corpus=corpus, 
        id2word=id2word, 
        num_topics=50,random_state=100, 
        chunksize = 1000, update_every=1, 
        passes=10, alpha='auto', per_word_topics=True)
</code></pre>

<p>I tried something like this but it doesn't work...</p>

<pre><code>def format_topics_sentences(ldamodel, corpus, df):
    # Init output
    sent_topics_df = pd.DataFrame()
    # Get main topic in each document
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # =&gt; dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = "", "".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
    # Add original text to the end of the output
    contents = df
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)
</code></pre>
","7403752","","7403752","","2019-11-07 13:35:59","2020-07-27 06:39:56","How to return dominant topic, percent contribution and topic keywords to original model","<python><gensim><lda><mallet>","1","1","1","","","CC BY-SA 4.0"
"54972481","1","","","2019-03-03 18:52:54","","1","234","<p>I am working with gensim WmdSimilarity. I followed <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">this tutorial</a>. But for a pre-trained model, it took 5-6 seconds for each output. I found <a href=""https://github.com/src-d/wmd-relax"" rel=""nofollow noreferrer"">some other implementation</a> of calculating WMD (Word Mover Distance) but I'm not sure about using it with pre-trained W2V </p>
","6484011","","6484011","","2019-03-04 07:10:03","2019-03-04 07:10:03","Is there any way to calculate gensim WmdSimilarity faster","<nlp><gensim><word2vec><sentence-similarity>","0","4","1","","","CC BY-SA 4.0"
"65172509","1","","","2020-12-06 20:00:33","","0","166","<p>I have a 200000 tokens corpus in Persian language. I want to create Glove word embedding for my corpus. Does the code below works for me? Or I should try something else.</p>
<pre><code>from glove import Corpus, Glove

#Creating a corpus object
corpus = Corpus() 

#Training the corpus to generate the co occurence matrix which is used in GloVe
corpus.fit(lines, window=10)

glove = Glove(no_components=5, learning_rate=0.05) 
glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)
glove.save('glove.model')
</code></pre>
","3319756","","6573902","","2020-12-10 13:12:58","2020-12-10 13:12:58","How to train our Persian(Farsi) corpus on Glove word embedding?","<python><nlp><stanford-nlp><gensim>","0","1","","","","CC BY-SA 4.0"
"57551270","1","","","2019-08-19 06:11:43","","1","215","<p>When using gensim utils to preprocess text for NLP, the library makes a call to numpy and returns (inter alia) this error message</p>

<pre><code>IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the multiarray numpy extension module failed.  Most
likely you are trying to import a failed build of numpy.
Here is how to proceed:
- If you're working with a numpy git repository, try `git clean -xdf`
  (removes all files not under version control) and rebuild numpy.
- If you are simply trying to use the numpy version that you have installed:
  your installation is broken - please reinstall numpy.
- If you have already reinstalled and that did not fix the problem, then:
  1. Check that you are using the Python you expect (you're using /Users/lorajohns/anaconda3/bin/python),
     and that you have no directories in your PATH or PYTHONPATH that can
     interfere with the Python and numpy versions you're trying to use.
  2. If (1) looks fine, you can open a new issue at
     https://github.com/numpy/numpy/issues.  Please include details on:
     - how you installed Python
     - how you installed numpy
     - your operating system
     - whether or not you have multiple versions of Python installed
     - if you built from source, your compiler versions and ideally a build log

     Note: this error has many possible causes, so please don't comment on
     an existing issue about this - open a new one instead.

Original error was: dlopen(/Users/$(USER)/anaconda3/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libopenblas.dylib
  Referenced from: /Users/$(USER)/anaconda3/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-darwin.so
  Reason: image not found
</code></pre>

<p>I already tried fixing the path and my .bash profile, in case it was a case of competing python installations. I am using venvs, so I also uninstalled any additional virtual env extensions that could potentially cause a conflict (pyenv). I reinstalled all packages into a new <code>conda</code> env, and updated <code>conda</code> and <code>anaconda</code>.</p>

<p>Essentially, I followed all the instructions, and nothing succeeded.</p>
","11500447","","11500447","","2019-10-07 16:57:39","2019-10-07 16:57:39","numpy image not found when importing gensim in python 3.7","<numpy><scipy><anaconda><gensim>","1","1","1","","","CC BY-SA 4.0"
"58754450","1","58768247","","2019-11-07 17:50:21","","0","136","<p>I'm using Gensim to train a skip-gram word2vec model. The dataset has 1 million sentences, but the vocabulary is of size 200. I would like to see the model accuracy over iterations, so I used <code>model.wv.similar_by_word</code> in the callback function to see the scores. But the returned values were not updated over iterations.</p>

<p>The <code>iter</code> was set to be <code>100</code>.
I tried to change the values of <code>window</code> and <code>size</code>, but it has no effect.</p>

<p>The model was initialized with callbacks:</p>

<pre class=""lang-py prettyprint-override""><code>Word2Vec(self.train_corpus, workers=multiprocessing.cpu_count(), compute_loss=True, callbacks=[A_CallBack], **word2vec_params)
</code></pre>

<p>In the class <code>A_CallBack</code>, I have something like this:</p>

<pre class=""lang-py prettyprint-override""><code>def on_epoch_end(self, model):
    word, score = model.wv.similar_by_word(word='target_word', topn=1)[0]
    print(word, score)
</code></pre>

<p>The <code>word</code> and <code>score</code> were printed out for every epoch, but the values have never changed.</p>

<p>I was expecting the values of them to be updated over iterations, which should make sense?</p>

<p>I'm new to machine learning and word2vec. Thanks a lot for the help.</p>
","12339035","","","","","2019-11-08 14:15:28","'similar_by_word' did not improve over iterations","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"58730230","1","58733750","","2019-11-06 12:30:19","","0","29","<p>I used gimsm for LSA as per this tutorial
<a href=""https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"" rel=""nofollow noreferrer"">https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python</a></p>

<p>and I got the following output after running it for a list of text</p>

<pre><code>
[(1, '-0.708*""London"" + 0.296*""like"" + 0.294*""go"" + 0.287*""dislike"" + 0.268*""great"" + 0.200*""romantic"" + 0.174*""stress"" + 0.099*""lovely"" + 0.082*""good"" + -0.075*""Tower"" + 0.072*""see"" + 0.063*""nice"" + 0.061*""amazing"" + -0.053*""Palace"" + 0.053*""walk"" + -0.050*""Eye"" + 0.046*""eat"" + -0.042*""Bridge"" + 0.041*""Garden"" + 0.040*""Covent"" + -0.040*""old"" + -0.039*""visit"" + 0.039*""really"" + 0.035*""spend"" + 0.034*""watch"" + 0.034*""get"" + -0.032*""Buckingham"" + 0.032*""Weather"" + -0.032*""Museum"" + -0.032*""Westminster""')]

</code></pre>

<p>What does -0.708 London indicate?</p>
","4470070","","715239","","2019-11-06 16:58:20","2019-11-06 16:58:20","What does the score indicate in topic modelling","<python><nlp><gensim><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"67278439","1","67286965","","2021-04-27 07:10:28","","2","322","<p>Im little confuse i would like use texthero library for some pca analysis. But when i trying run my code :</p>
<pre><code>import texthero as hero
import pandas as pd


df['pca']=(df['clean_tweet'].pipe(hero.clean).pipe(hero.do_tfidf).pipe(hero.do_pca))
hero.scatterplot(df, col='pca', color='topic', title=&quot;PCA BBC Sport news&quot;)
</code></pre>
<p>I get error:</p>
<pre><code>ModuleNotFoundError: No module named 'gensim.sklearn_api
</code></pre>
<p>But when i put !pip show gensim. i got</p>
<pre><code>Name: gensim
Version: 4.0.1
Summary: Python framework for fast Vector Space Modelling
Home-page: http://radimrehurek.com/gensim
</code></pre>
","15735263","","12439119","","2021-04-28 08:12:57","2021-04-28 08:12:57","No module named 'gensim.sklearn_api' how to resolve","<python><pandas><machine-learning><gensim>","1","0","","","","CC BY-SA 4.0"
"58789484","1","","","2019-11-10 14:13:54","","1","391","<p>I am trying to lemmatize documents with the following codes. Lemmatization works. It produces byte string. Therefore, the next part of the codes produces ""cant concan byte to str"" error. Then I have changed tokens as str() as given in below codes. The output of the code is as given below;(I am using Python 3.7 (64 bit))</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-223-cb505389f802&gt; in &lt;module&gt;
      1 #Build a Vocabulary
----&gt; 2 model.build_vocab(train_demo_corpus)

~\Anaconda3\lib\site-packages\gensim\models\doc2vec.py in build_vocab(self, documents, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    727         """"""
    728         total_words, corpus_count = self.vocabulary.scan_vocab(
--&gt; 729             documents, self.docvecs, progress_per=progress_per, trim_rule=trim_rule)
    730         self.corpus_count = corpus_count
    731         report_values = self.vocabulary.prepare_vocab(

~\Anaconda3\lib\site-packages\gensim\models\doc2vec.py in scan_vocab(self, documents, docvecs, progress_per, trim_rule)
    807         for document_no, document in enumerate(documents):
    808             if not checked_string_types:
--&gt; 809                 if isinstance(document.words, string_types):
    810                     logger.warning(
    811                         ""Each 'words' should be a list of words (usually unicode strings). ""

AttributeError: 'str' object has no attribute 'words'
</code></pre>

<p>here is my code;</p>

<pre class=""lang-py prettyprint-override""><code>train_demo_corpus = list(lemmat(lee_train_demo_file))

def lemmat(fname, tokens_only=False):
    with smart_open.smart_open(fname, encoding=""iso-8859-1"") as f:
        for i, line in enumerate(f):
            tokens = gensim.utils.lemmatize(line)
            if tokens_only:
                yield str(tokens)
            else:
                # For training data, add tags
                yield str(gensim.models.doc2vec.TaggedDocument(tokens, [i]))

model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)
model.build_vocab(train_demo_corpus)
</code></pre>

<p>Best regards,</p>
","8141098","","8141098","","2019-11-10 21:15:46","2021-04-08 11:41:27","Gensim Lemmatization Remove Postag b'","<gensim><lemmatization>","1","8","","","","CC BY-SA 4.0"
"41432760","1","","","2017-01-02 20:12:22","","1","1476","<p>I want to use gensim word2vec as input for neural network. I have 2 questions:</p>

<p>1) gensim.models.Word2Vec get as parameter the size. How this parameter is used? and size of what?</p>

<p>2) Once trained what is the output of gensim word2vec? As i could see this is not a probability values (not between 0 and 1). It seems to me for each word vector we get a distance (cosinus) between this word and some other words (but which words exactly?)</p>

<p>Thanks for your response.</p>
","5974525","","","","","2019-08-28 07:58:03","What is the Gensim word2vec output","<gensim><word2vec>","2","0","1","","","CC BY-SA 3.0"
"67096547","1","67097495","","2021-04-14 17:37:56","","0","46","<p>I run the following code and just wonder why the top 3 most similar words for &quot;exposure&quot; don't include &quot;charge&quot; and &quot;lend&quot;?</p>
<pre><code>from gensim.models import Word2Vec
corpus = [['total', 'exposure', 'charge', 'lend'],
          ['customer', 'paydown', 'rate', 'months', 'month']]
gens_mod = Word2Vec(corpus, min_count=1, vector_size=300, window=2, sg=1, workers=1, seed=1)
keyword=&quot;exposure&quot;
gens_mod.wv.most_similar(keyword)

Output:
[('customer', 0.12233059108257294),
 ('month', 0.008674687705934048),
 ('total', -0.011738087050616741),
 ('rate', -0.03600010275840759),
 ('months', -0.04291829466819763),
 ('paydown', -0.044823747128248215),
 ('lend', -0.05356598272919655),
 ('charge', -0.07367636263370514)]
</code></pre>
","6221871","","6221871","","2021-04-14 17:45:12","2021-04-14 18:50:24","How to explain gensim word2vec output?","<python><nlp><gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"58797101","1","58807013","","2019-11-11 07:24:27","","1","436","<p>I'm trying to store data in a global variable inside a Redis Queue (RQ) worker so that this data remains pre-loaded, i.e. it doesn't need to be loaded for every RQ job.</p>

<p>Specifically, I'm working with Word2Vec vectors and loading them using gensim's KeyedVectors.</p>

<p>My app is in Python Flask, running on a Linux server, containerized using Docker.</p>

<p>My goal is to reduce processing time by keeping a handful of large vectors files loaded in memory at all times. </p>

<p>I first tried storing them in global variables in Flask, but then <em>each</em> of my 8 gunicorn workers loads the vectors, which eats up a lot of RAM.</p>

<p>I only need <em>one</em> worker to store a particular vectors file.</p>

<p>I've been told that one solution is to have a set number of RQ workers holding the vectors in a global variable, so that I can control which workers get which vectors files loaded in.</p>

<p>Here is what I have so far:</p>

<p><em>RQ_worker.py</em></p>

<pre><code>from rq import Worker, Connection
from gensim.models.keyedvectors import KeyedVectors
from my_common_methods import get_redis

W2V = KeyedVectors.load_word2vec_format('some_path/vectors.bin', binary=True)

def rq_task(some_args):
    # use some_args and W2V to do some processing, e.g.:
    with open(some_args_filename, 'w') as f_out:
        f_out.write(str(W2V['word']))

if __name__ == '__main__':
    with Connection(get_redis()):
        worker = Worker(['default'])
        worker.work()
</code></pre>

<p><em>app.py</em></p>

<pre><code>from rq import Queue, Connection
from RQ_worker import rq_task

@app.route(""/someroute"", methods=['POST'])
def some_route():
    # test Redis Queue
    with Connection(get_redis()):
        q = Queue()
        task = q.enqueue(rq_task, some_args)
</code></pre>

<p><em>docker-stack.yml</em></p>

<pre><code>version: '3.7'

services:
  nginx:
    image: nginx:mainline-alpine
    deploy: ...
    configs: ...
    networks: ...

  flask:
    image: ...
    deploy: ...
    environment: ...
    networks: ...
    volumes: ...

  worker:
    image: ...
    command: python2.7 RQ_worker.py
    deploy:
      replicas: 1
    networks: ...
    volumes:
      - /some_path/data:/some_path/data

configs:
  nginx.conf:
    external: true
    name: nginx.conf

networks:
  external:
    external: true
  database:
    external: true
</code></pre>

<p>(I redacted a bunch of stuff from Docker, but can provide more details, if relevant.)</p>

<p>The above generally works, <strong>except</strong> that the RQ worker seems to load W2V <strong>from scratch</strong> each time it gets a new job, which defeats the whole purpose. It should keep the vectors stored in W2V as a global variable, so they don't need to be reloaded each time.</p>

<p>Am I missing something? Should I set it up differently? </p>

<p>I've been told that it might be possible to use mmap to load the vectors file into a global variable that the RQ worker sits on, but I'm not sure how that would work with KeyedVectors.</p>

<p>Any advice would be much appreciated!</p>
","12353728","","","","","2019-11-11 18:46:45","How to store gensim's KeyedVectors object in a global variable inside a Redis Queue worker","<docker><flask><redis><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"58786596","1","","","2019-11-10 07:30:53","","0","205","<p>I am struggling with the following problem:</p>

<p>I downloaded a <strong>pre-trained word embedding model</strong> for <strong>Spanish</strong> (over 1 million words 300-dimensional word vectors for Spanish)
I loaded it successfully and I even managed to undertake a couple of experiments, such as most similar words and basic analogies in Spanish (A is to B as C is to what), but when I try the following:</p>

<pre><code> for pais in 'Italia', 'Francia', 'India', 'China':
      print(' is the capital of '  
      (A_is_to_B_as_C_is_to('Alemania','Berl√≠n',pais),pais))
</code></pre>

<p>It raises the error:</p>

<pre><code>KeyError: ""word 'Berl√≠n' not in vocabulary""
</code></pre>

<p>I already checked that the word is actually in the word embedding. I have also eliminated the possibility of an encoding error. </p>

<p>Based on my research, this type of error is produced when the token/word is supposed to be wrapped in a list [], however I don‚Äôt know how to apply that to this specific problem. Besides, this block of code is the same code used in ‚Äú<strong>Deep Learning Cookbook</strong>‚Äù in chapter 3 (Word2vecMath)</p>

<p>This is the complete script:</p>

<pre><code>import os
from keras.utils import get_file
import gensim

from gensim.models.keyedvectors import KeyedVectors

import subprocess
import numpy as np
import matplotlib.pyplot as plt
from IPython.core.pylabtools import figsize


from sklearn.manifold import TSNE
import json
from collections import Counter
from itertools import chain

from keras.models import load_model
path = (""D:\Pretrained_wordEmbeddings_ESP\embeddings-l-model.vec"")


model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=False)


data=model.most_similar(positive=[""muerte""])

print(data[:])


def A_is_to_B_as_C_is_to(a, b, c, topn=1):
    a, b, c = map(lambda x:x if type(x) == list else [x], (a, b, c))
    res = model.most_similar(positive=b + c, negative=a, topn=topn)
    if len(res):
        if topn == 1:
            return res[0][0]
        return [x[0] for x in res]
    return None

A_is_to_B_as_C_is_to('hombre', 'mujer', 'rey')

## for pais in 'Italia', 'Francia', 'India', 'China':
##    print(' is the capital of '  
##          (A_is_to_B_as_C_is_to('Alemania', 'Berl√≠n', pais), pais))
</code></pre>

<p>Thank you from your support</p>
","6849749","","","","","2019-11-10 19:14:34","using a Spanish pretrained model with Gensim causes raise KeyError(""word '%s' not in vocabulary"" % word)","<python><deep-learning><gensim><word-embedding><keyerror>","1","0","","","","CC BY-SA 4.0"
"67523963","1","67524557","","2021-05-13 17:56:41","","0","19","<p>Suppose I have a corpus of short sentences of which the number of words ranges from 1 to around 500 and the average number of words is around 9. If I train a Gensim Word2vec model using window=5(which is the default), should I use all of the sentences? or I should remove sentences with low word count? If so, is there a rule of thumb for the minimum number of words?</p>
","6221871","","","","","2021-05-13 18:45:03","Minimum Number of Words for Each Sentence for Training Gensim Word2vec Model","<nlp><gensim><word2vec><hyperparameters>","1","0","","","","CC BY-SA 4.0"
"51418154","1","51430452","","2018-07-19 08:45:36","","0","686","<p>I'm training doc2vec, and using callbacks trying to see if alpha is decreasing over training time using this code:</p>

<pre><code>class EpochSaver(CallbackAny2Vec):
'''Callback to save model after each epoch.'''

    def __init__(self, path_prefix):
        self.path_prefix = path_prefix
        self.epoch = 0

        os.makedirs(self.path_prefix, exist_ok=True)

    def on_epoch_end(self, model):
        savepath = get_tmpfile(
            '{}_epoch{}.model'.format(self.path_prefix, self.epoch)
        )
        model.save(savepath)
        print(
            ""Model alpha: {}"".format(model.alpha), 
            ""Model min_alpha: {}"".format(model.min_alpha),
            ""Epoch saved: {}"".format(self.epoch + 1),
            ""Start next epoch""
        )
        self.epoch += 1


def train():

    workers = multiprocessing.cpu_count()*4
    model = Doc2Vec(
        DocIter(),
        vec_size=600, alpha=0.03, min_alpha=0.00025, epochs=20,
        min_count=10, dm=1, hs=1, negative=0, workers=workers,
        callbacks=[EpochSaver(""./checkpoints"")]
    )
    print(
        ""HS"", model.hs, ""Negative"", model.negative, ""Epochs"", 
         model.epochs, ""Workers: "", model.workers, ""Model alpha: 
         {}"".format(model.alpha)
    )  
</code></pre>

<p>And while training I see that alpha is not changing over time. On each callback I see alpha = 0.03.<br>
Is it possible to check if alpha is decreasing? Or it really not decreasing at all during training? </p>

<p>One more question: 
How can I benefit from all my cores while training doc2vec?</p>

<p><a href=""https://i.stack.imgur.com/Bo1uh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bo1uh.jpg"" alt=""Loading of cores""></a></p>

<p>As we can see, each core is not loaded more than +-30%. </p>
","5975112","","5975112","","2018-07-20 14:08:41","2018-07-20 14:08:41","How to check via callbacks if alpha is decreasing? + How to load all cores during training?","<callback><gensim><multicore><word-embedding><doc2vec>","1","0","","","","CC BY-SA 4.0"
"41326577","1","","","2016-12-26 04:56:52","","1","1118","<p>I am using python 3.5 on both windows and Linux but get the same error:
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc1 in position 0: ordinal not in range(128)
The error log is the following:
    Reloaded modules: lazylinker_ext
    Traceback (most recent call last):</p>

<pre><code>  File ""&lt;ipython-input-2-d60a2349532e&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/YZC/Google     Drive/sunday/data/RA/data_20100101_20150622/w2v_coherence.py',     wdir='C:/Users/YZC/Google Drive/sunday/data/RA/data_20100101_20150622')

  File ""C:\Users\YZC\Anaconda3\lib\site-    packages\spyderlib\widgets\externalshell\sitecustomize.py"", line 699, in runfile
    execfile(filename, namespace)

  File ""C:\Users\YZC\Anaconda3\lib\site-    packages\spyderlib\widgets\externalshell\sitecustomize.py"", line 88, in execfile
    exec(compile(open(filename, 'rb').read(), filename, 'exec'), namespace)

  File ""C:/Users/YZC/Google     Drive/sunday/data/RA/data_20100101_20150622/w2v_coherence.py"", line 70, in     &lt;module&gt;
    model = gensim.models.Word2Vec.load('model_all_no_lemma')

  File ""C:\Users\YZC\Anaconda3\lib\site-packages\gensim\models\word2vec.py"",     line 1485, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)

  File ""C:\Users\YZC\Anaconda3\lib\site-packages\gensim\utils.py"", line 248,     in load
    obj = unpickle(fname)

  File ""C:\Users\YZC\Anaconda3\lib\site-packages\gensim\utils.py"", line 912, in unpickle
    return _pickle.loads(f.read())

UnicodeDecodeError: 'ascii' codec can't decode byte 0xc1 in position 0:     ordinal not in range(128)
</code></pre>

<p>1.I checked and found the default decode method is utf-8 by:
    import sys
    sys.getdefaultencoding()
Out[2]: 'utf-8'</p>

<ol start=""2"">
<li>when read the file, I also added .decode('utf-8')</li>
<li>I did add shepang line in the beginning and declare utf-8
so I really dont know why python couldnt read the file. Can anybody help me out?</li>
</ol>

<p>Here are the code:</p>

<pre><code># -*- coding: utf-8 -*-
import gensim
import csv
import numpy as np
import math
import string
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob, Word



class SpeechParser(object):

    def __init__(self, filename):
        self.filename = filename
        self.lemmatize = WordNetLemmatizer().lemmatize
        self.cached_stopwords = stopwords.words('english')

    def __iter__(self):

        with open(self.filename, 'rb', encoding='utf-8') as csvfile:
            file_reader = csv.reader(csvfile, delimiter=',', quotechar='|', )
            headers = file_reader.next()
            for row in file_reader:
                parsed_row = self.parse_speech(row[-2])
                yield parsed_row

    def parse_speech(self, row):

        speech_words =  row.replace('\r\n', ' ').strip().lower().translate(None, string.punctuation).decode('utf-8', 'ignore')         

        return speech_words.split()

    # -- source: https://github.com/prateekpg2455/U.S-Presidential-    Speeches/blob/master/speech.py --
    def pos(self, tag):
        if tag.startswith('J'):
            return wordnet.ADJ
        elif tag.startswith('V'):
            return wordnet.VERB
        elif tag.startswith('N'):
            return wordnet.NOUN
        elif tag.startswith('R'):
            return wordnet.ADV
        else:
            return ''

if __name__ == '__main__':

    # instantiate object
    sentences = SpeechParser(""sample.csv"")

    # load an existing model
    model = gensim.models.Word2Vec.load('model_all_no_lemma')



    print('\n-----------------------------------------------------------')
    print('MODEL:\t{0}'.format(model))

    vocab = model.vocab

    # print log-probability of first 10 sentences
    row_count = 0
    print('\n------------- Scores for first 10 documents: -------------')
    for doc in sentences: 
        print(sum(model.score(doc))/len(doc))
        row_count += 1
        if row_count &gt; 10:
            break
    print('\n-----------------------------------------------------------')
</code></pre>
","5439134","","5439134","","2016-12-26 05:45:33","2016-12-28 12:51:12","UnicodeDecodeError: 'ascii' codec can't decode, with gensim, python3.5","<encoding><utf-8><python-3.5><gensim><word2vec>","1","3","","","","CC BY-SA 3.0"
"41360580","1","","","2016-12-28 10:58:21","","1","1519","<p>I'm trying to read my pretrained doc2vec model:</p>

<pre><code>from gensim.models import Doc2Vec
model = Doc2Vec.load('/path/to/pretrained/model')
</code></pre>

<p>However, an error appears during reading process. Could anyone suggest how to deal with this? Here is the error:</p>

<pre><code>AttributeErrorTraceback (most recent call last)
&lt;ipython-input-9-819b254ac835&gt; in &lt;module&gt;()
----&gt; 1 model = Doc2Vec.load('/path/to/pretrained/model')

/opt/jupyter-notebook/.local/lib/python2.7/site-packages/gensim/models/word2vec.pyc in load(cls, *args, **kwargs)
   1682     @classmethod    
   1683     def load(cls, *args, **kwargs):
-&gt; 1684         model = super(Word2Vec, cls).load(*args, **kwargs)
   1685         # update older models
   1686         if hasattr(model, 'table'):

/opt/jupyter-notebook/.local/lib/python2.7/site-packages/gensim/utils.pyc in load(cls, fname, mmap)
    246         compress, subname = SaveLoad._adapt_by_suffix(fname)
    247 
--&gt; 248         obj = unpickle(fname)
    249         obj._load_specials(fname, mmap, compress, subname)
    250         return obj

/opt/jupyter-notebook/.local/lib/python2.7/site-packages/gensim/utils.pyc in unpickle(fname)
    909     with smart_open(fname) as f:
    910         # Because of loading from S3 load can't be used (missing readline in smart_open)
--&gt; 911         return _pickle.loads(f.read())
    912 
    913 

AttributeError: 'module' object has no attribute 'defaultdict'
</code></pre>
","5550721","","","","","2017-05-15 23:02:46","Gensim: how to load pretrained doc2vec model?","<python><model><gensim><word2vec><doc2vec>","1","6","","","","CC BY-SA 3.0"
"67064737","1","","","2021-04-12 19:43:48","","0","49","<p>I'm new with NLP and especially topic modelling and i was curious for a way to find the most suitable number of topics for my dataset. I've found this answer: <a href=""https://stackoverflow.com/questions/32313062/what-is-the-best-way-to-obtain-the-optimal-number-of-topics-for-a-lda-model-usin"">What is the best way to obtain the optimal number of topics for a LDA-Model using Gensim?</a> ---  and the code worked fine until this step:</p>
<pre class=""lang-py prettyprint-override""><code>coh_sta_diffs_2010 = [coherences_2010[i] - mean_stabilities_2010[i] for i in range(num_keywords)[:-1]] # limit topic numbers to the number of keywords
coh_sta_max_2010 = max(coh_sta_diffs_2010)
coh_sta_max_idxs_2010 = [i for i, j in enumerate(coh_sta_diffs_2010) if j == coh_sta_max_2010]
ideal_topic_num_index_2010 = coh_sta_max_idxs_2010[0] # choose less topics in case there's more than one max
ideal_topic_num_2010 = num_topics[ideal_topic_num_index_2010]
</code></pre>
<p>Error message (second-to-last line):</p>
<p><code>IndexError: list index out of range</code></p>
<p>I can't seem to find out why is doing it.</p>
","15614549","","13746389","","2021-04-13 02:53:23","2021-04-13 02:53:23","list index out of range for topic Modelling LDA","<python><nlp><gensim><lda><topic-modeling>","0","1","","","","CC BY-SA 4.0"
"58723979","1","","","2019-11-06 06:04:50","","0","54","<p>I ran Gensim to train Doc2vec of the corpus. I need to extract the vector of each document as input data for Logical regression in spark.    </p>
","2281101","","","","","2019-11-06 17:21:39","Recall Doc2Vec in Spark and input vectors to Logical regression machine learning","<apache-spark><gensim>","1","3","","","","CC BY-SA 4.0"
"51429975","1","67001870","","2018-07-19 18:53:50","","1","1626","<p>I try to use</p>

<pre><code>from gensim.sklearn_api import W2VTransformer
</code></pre>

<p>and get</p>

<pre><code>ImportError: No module named 'gensim.sklearn_api'
</code></pre>

<p>I used </p>

<pre><code>import gensim
import sklearn
from sklearn.base import BaseEstimator, TransformerMixin
</code></pre>

<p>and get the same.
In <a href=""https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html"" rel=""nofollow noreferrer"">sklearn_api.w2vmodel ‚Äì Scikit learn wrapper for word2vec model</a> I could find no advice. 
How to install <code>gensim.sklearn_api</code>?</p>
","5998425","","","","","2021-07-12 15:47:36","No module named 'gensim.sklearn_api'","<python><scikit-learn><gensim>","4","0","","","","CC BY-SA 4.0"
"51426107","1","","","2018-07-19 15:07:55","","6","5540","<p>I'm trying to build a Tf-Idf model that can score bigrams as well as unigrams using <a href=""https://radimrehurek.com/gensim/index.html"" rel=""noreferrer"">gensim</a>. To do this, I build a gensim dictionary and then use that dictionary to create bag-of-word representations of the corpus that I use to build the model. </p>

<p>The step to build the dictionary looks like this:</p>

<pre><code>dict = gensim.corpora.Dictionary(tokens)
</code></pre>

<p>where <code>token</code> is a list of unigrams and bigrams like this:</p>

<pre><code>[('restore',),
 ('diversification',),
 ('made',),
 ('transport',),
 ('The',),
 ('grass',),
 ('But',),
 ('distinguished', 'newspaper'),
 ('came', 'well'),
 ('produced',),
 ('car',),
 ('decided',),
 ('sudden', 'movement'),
 ('looking', 'glasses'),
 ('shapes', 'replaced'),
 ('beauties',),
 ('put',),
 ('college', 'days'),
 ('January',),
 ('sometimes', 'gives')]
</code></pre>

<p>However, when I provide a list such as this to <code>gensim.corpora.Dictionary()</code>, the algorithm reduces all tokens to bigrams, e.g.:</p>

<pre><code>test = gensim.corpora.Dictionary([(('happy', 'dog'))])
[test[id] for id in test]
=&gt; ['dog', 'happy']
</code></pre>

<p>Is there a way to generate a dictionary with gensim that includes bigrams? </p>
","1190586","","","","","2021-07-20 06:07:39","How to build a gensim dictionary that includes bigrams?","<python><nlp><gensim>","2","0","1","","","CC BY-SA 4.0"
"55612440","1","55612601","","2019-04-10 12:21:27","","0","550","<p>I am trying to add pretrained vectors to a training model using fasttext and getting the below error. Code is written in python with fasttext 0.8.3.</p>

<p>I thought with fasttext you could add pre trained vectors to a supervised training model?</p>

<p>TypeError: supervised() got an unexpected keyword argument 'pretrainedVectors'</p>

<pre><code>pretrainedVectors = 'vectorFile.vec'
classifier = ft.supervised(model_data, model_name, pretrainedVectors=pretrainedVectors, label_prefix=label_prefix, lr=lr, epoch=epoch, minn=minn, maxn=maxn, dim=dim, bucket=bucket)
</code></pre>
","6402231","","2084384","","2019-04-10 12:30:34","2019-04-10 12:30:34","fasttext error TypeError: supervised() got an unexpected keyword argument 'pretrainedVectors'","<python><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"55629368","1","","","2019-04-11 09:40:00","","1","651","<p>I am using a word embeddings model (FastText via Gensim library) to expand the terms of a search. 
So, basically if the user write ""operating system"" my goal is to expand that term with very similar terms like ""os"", ""windows"", ""ubuntu"", ""software"" and so on.</p>

<p>The model works very well but now the time has come to improve the model with ""external information"", with ""external information"" i mean OOV (out-of-vocabulary) terms OR terms that do not have good context.</p>

<p>Following the example i wrote above when the user writes <strong>operating system</strong> i would like to expand the query with the ""general"" terms:</p>

<p>Terms built in the FastText model:</p>

<ul>
<li>windows </li>
<li>ubuntu</li>
<li>software</li>
</ul>

<p><strong>AND</strong> </p>

<p>terms that represent (organizations/companies) like ""Microsoft"", ""Apple"" so the complete query will be:</p>

<ul>
<li><strong>term</strong>: operating system</li>
<li><strong>query</strong>: operating system, os, software, windows, ios, Microsoft, Apple</li>
</ul>

<p>My problem is that i DO NOT have companies inside the corpus OR, if present, i do not have to much context to ""link"" Microsoft to ""operating system"".</p>

<p>For example if i extract a piece inside the corpus i can read ""... i have started working at Microsoft in November 2000 with my friend John ..."" so, as you can see, i cannot contextualize ""Microsoft"" word because i do not have good context, indeed.</p>

<p>A small recap:</p>

<ol>
<li>I have a corpus where the companies (terms) have poor context</li>
<li>I have a big database with companies and the description of what they do.</li>
</ol>

<p>What i need to do:</p>

<p>I would like to include the companies in my FastText model and set ""manually"" their words context/cloud of related terms.</p>

<p>Ideas?</p>
","887651","","","","","2019-04-11 13:56:53","How to add OOV terms in a word embeddings model","<python><machine-learning><nlp><gensim>","2","2","","","","CC BY-SA 4.0"
"65221032","1","","","2020-12-09 16:39:55","","0","446","<p>I would like to see the result from the word embeddings model, the vectors results.</p>
<p>When using the following code:</p>
<pre><code>words_embeddings = gensim.models.Word2Vec(all_sentences, size=96, window=5, min_count=1, workers=2, sg=1)
print(words_embeddings) 
</code></pre>
<p>I get the following output:</p>
<pre><code>Word2Vec(vocab=9700, size=96, alpha=0.025)
</code></pre>
<p>I am wondering if the <code>gensim</code> library as a method to display the result form <code>gensim.models.Word2Vec()</code></p>
","9366726","","","","","2020-12-09 16:45:22","How to print the result from gensim.models.Word2Vec","<python><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"51388707","1","52216299","","2018-07-17 19:14:21","","0","668","<p>We have Anaconda 4.3.1 installed on our hosts and recently we have installed several packages for data science use. All the imports were fine except for gensim.</p>

<p>I am getting ""Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so."" and getting out of python shell.</p>

<p>It sounds like a duplicate but the weird part is, when I import tensorflow or seaborn before importing gensim, I am not getting that error and gensim is being imported. I would also like to know if there is any dependency between these packages. And I do have the latest version of numpy which is 1.14.5. I have looked at various solutions proposed about installing few packages and uninstalling few. I would like to know the reason why we should be doing it before actually doing it.</p>
","1707388","","","","","2018-09-07 06:11:24","Intel MKL FATAL ERROR: while trying to import gensim package","<python><tensorflow><anaconda><seaborn><gensim>","1","0","","","","CC BY-SA 4.0"
"50396111","1","","","2018-05-17 16:13:44","","1","556","<p>I load a word2vec-format file and I want to calculate the similarities between vectors, but I don't know what this issue means.</p>

<pre><code>from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import KeyedVectors
import numpy as np

model = KeyedVectors.load_word2vec_format('it-vectors.100.5.50.w2v')

similarities = cosine_similarity(model.vectors)


---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
&lt;ipython-input-54-1d4e62f55ebf&gt; in &lt;module&gt;()
----&gt; 1 similarities = cosine_similarity(model.vectors)

/usr/local/lib/python3.5/dist-packages/sklearn/metrics/pairwise.py in cosine_similarity(X, Y, dense_output)
    923         Y_normalized = normalize(Y, copy=True)
    924 
--&gt; 925     K = safe_sparse_dot(X_normalized, Y_normalized.T, dense_output=dense_output)
    926 
    927     return K

/usr/local/lib/python3.5/dist-packages/sklearn/utils/extmath.py in safe_sparse_dot(a, b, dense_output)
    138         return ret
    139     else:
--&gt; 140         return np.dot(a, b)
    141 
    142 

MemoryError: 
</code></pre>

<p>What it means?
Thank you!</p>
","7625445","","","","","2018-05-17 17:16:41","Cosine similarity with word2vec","<scikit-learn><nlp><word2vec><gensim><cosine-similarity>","1","0","","","","CC BY-SA 4.0"
"58735585","1","58737377","","2019-11-06 17:28:04","","2","537","<p>I am doing my research with fasttext pre-trained model and I need word frequency to do further analysis. Does the .vec or .bin files provided on fasttext website contain the info of word frequency? if yes, how do I get?</p>

<p>I am using load_word2vec_format to load the model tried using model.wv.vocab[word].count, which only gives you the word frequency rank not the original word frequency.</p>
","12333458","","12333458","","2019-11-06 19:54:03","2019-11-06 19:54:03","Gensim: Any chance to get word frequency in Word2Vec format?","<python-3.6><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"51449841","1","51449982","","2018-07-20 20:22:20","","-1","31","<p>All-</p>

<p>I would like to use the gensim library, but unfortunately I can't install it via pip due to the company's firewall. Any advice? Thank you in advance for any help or suggestions you can provide. </p>
","10112826","","","","","2018-07-20 20:34:18","How to install gensim without pip (firewall issues)","<python><installation><firewall><gensim>","1","0","","","","CC BY-SA 4.0"
"64409987","1","","","2020-10-18 05:22:06","","0","880","<blockquote>
<p>I am trying to load the google_news_vecotors.bin file but it gives an
error. Below is my code it is written in the nlp_gen2.py file</p>
</blockquote>
<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('google_news_vectors.bin', binary=True)
</code></pre>
<p><strong>the error I get is:</strong></p>
<pre><code>FileNotFoundError Traceback (most recent call last) in 1 import gensim
----&gt; 2 model = gensim.models.KeyedVectors.load_word2vec_format('google_news_vectors.bin',
binary=True)

C:\Anaconda3\envs\DataScience\lib\site-packages\gensim\models\keyedvectors.py
in load_word2vec_format(cls, fname, fvocab, binary, encoding,
unicode_errors, limit, datatype) 1547 return _load_word2vec_format(
1548 cls, fname, fvocab=fvocab, binary=binary, encoding=encoding,
unicode_errors=unicode_errors, -&gt; 1549 limit=limit, datatype=datatype)
1550 1551 @classmethod

C:\Anaconda3\envs\DataScience\lib\site-packages\gensim\models\utils_any2vec.py
in _load_word2vec_format(cls, fname, fvocab, binary, encoding,
unicode_errors, limit, datatype, binary_chunk_size) 273 274
logger.info(&quot;loading projection weights from %s&quot;, fname) --&gt; 275 with
utils.open(fname, 'rb') as fin: 276 header =
utils.to_unicode(fin.readline(), encoding=encoding) 277 vocab_size,
vector_size = (int(x) for x in header.split()) # throws for invalid
file format

C:\Anaconda3\envs\DataScience\lib\site-packages\smart_open\smart_open_lib.py
in open(uri, mode, buffering, encoding, errors, newline, closefd,
opener, ignore_ext, transport_params) 185 encoding=encoding, 186
errors=errors, --&gt; 187 newline=newline, 188 ) 189 if fobj is not None:

C:\Anaconda3\envs\DataScience\lib\site-packages\smart_open\smart_open_lib.py
in _shortcut_open(uri, mode, ignore_ext, buffering, encoding, errors,
newline) 285 open_kwargs['errors'] = errors 286 --&gt; 287 return
_builtin_open(local_path, mode, buffering=buffering, **open_kwargs) 288 289

FileNotFoundError: [Errno 2] No such file or directory:
'google_news_vectors.bin'
</code></pre>
<p><strong>my file structure is like below:</strong>
<a href=""https://i.stack.imgur.com/ssoD6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ssoD6.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ssoD6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ssoD6.jpg"" alt=""enter image description here"" /></a></p>
<p>how can I solve this?</p>
","10369901","","10369901","","2020-10-19 04:06:53","2020-10-19 16:46:56","No such file or directory: 'google_news_vectors.bin'","<python><gensim><word2vec>","2","3","","","","CC BY-SA 4.0"
"59079173","1","","","2019-11-27 22:16:11","","0","122","<p>I have been training a gensim doc2vec model for a couple days on 10 epochs. It's been running smoothly until it got to 79.29% on the last Epoch, and it suddenly stopped logging anything new. The last message logged was:</p>

<blockquote>
  <p>INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 79.29% examples, 8566 words/s, in_qsize 0, out_qsize 0</p>
</blockquote>

<p>There is a line of code that saves the model after the 10 epochs, but it hasn't saved anything. Any idea what could be the issue?</p>
","12448547","","","","","2019-11-27 22:16:11","Gensim doc2vec training stalled","<python><gensim><word2vec><doc2vec>","0","8","","","","CC BY-SA 4.0"
"51481553","1","","","2018-07-23 14:43:44","","5","2358","<p>I am trying to use the FastText's french pre-trained binary model (downloaded from the official <a href=""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"" rel=""nofollow noreferrer"">FastText's github page</a>). I need the <code>.bin</code> model and not the <code>.vec</code> word-vectors so as to approximate misspelled and out-of-vocabulary words.</p>
<p>However when I try to load said model, using:</p>
<pre><code>from gensim.models import FastText
model = FastText.load_fasttext_format('french_bin_model_path')
</code></pre>
<p>I get the following error:</p>
<pre><code>NotImplementedError: Supervised fastText models are not supported
</code></pre>
<p>What is surprising is that <em>it works just fine</em> when I try to load the <strong>english</strong> binary model.</p>
<p>I am running python 3.6 and gensim 3.5.0.</p>
<p>Any idea as of why it doesn't work with french vectors are welcome!</p>
","7918836","","-1","","2020-06-20 09:12:55","2018-11-19 15:21:51","Error when loading FastText's french pre-trained model with gensim","<python><gensim><pre-trained-model><fasttext><french>","2","0","","","","CC BY-SA 4.0"
"67099706","1","67099950","","2021-04-14 22:06:59","","0","58","<p>Suppose my corpus is reasonably large - having tens-of-thousands of unique words. I can either use it to build a word2vec model directly(Approach #1 in the code below) or initialize a new word2vec model with pre-trained model weights and fine tune it with my own corpus(Approach #2). Is the approach #2 worth consideration? If so, is there a rule of thumb on when I should consider a pre-trained model?</p>
<pre><code># Approach #1
from gensim.models import Word2Vec
model = Word2Vec(my_corpus, vector_size=300, min_count=1)

# Approach #2
model = Word2Vec(vector_size=300, min_count=1)
model.build_vocab(my_corpus)
model.intersect_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True, lockf=1.0)
model.train(my_corpus, total_examples=len(my_corpus))
</code></pre>
","6221871","","","","","2021-04-14 22:34:28","When should I consider to use pretrain-model word2vec model weights?","<python><gensim><word2vec><word-embedding><pre-trained-model>","1","0","","","","CC BY-SA 4.0"
"51414910","1","","","2018-07-19 05:24:32","","2","187","<p>Hey guys I have a pretrained binary file and I want to train it on my corpus. </p>

<p><strong>Approach I tried :</strong></p>

<p>I tried to extract the txt file from the bin file I had and use this as a word2vec file at time of loading and further trained it on my own corpus and saved the model but the model is performing badly for the words which are there in the pre-trained bin file (I used intersect_word2vec_format command for this.) </p>

<p><a href=""https://drive.google.com/open?id=1AdBYWP1lWrg9QsX4BZ63rpUihyR4BpvS"" rel=""nofollow noreferrer"">Here</a> is the script I used.</p>

<p>What should be my approach for my model to perform well on words from both the pre-trained file and my corpus?</p>
","6464277","","","","","2018-07-24 04:33:49","How to train a pretrained binary file on my own corpus using gensim?","<nlp><models><gensim><corpus>","1","0","","","","CC BY-SA 4.0"
"51430560","1","","","2018-07-19 19:36:06","","1","1352","<p>I've been getting some irregular behavior from an LDA topic model program and right now, it seems like my file won't save the lda model it creates... I'm really not sure why.</p>

<p>Here's a code snippet, albeit it's going to take me more time before I could write code that's reproducible since I'm really just trying to load certain files I created beforehand.</p>

<pre><code>def naive_LDA_implementation(name_of_lda, create_dict=False, remove_low_freq=False):

    LDA_MODEL_PATH = ""lda_dir/"" + str(name_of_lda) +""/model_dir/"" # for some reason this location doesn't work entirely... and yes, I have made a directory in a the folder of this name.
    # This ends up saving the .state, .id2word, and .expEblogbeta.npy files... But normally when saving an lda model actually works, a fourth file is included that's to my understanding the model itself.
    # LDA_MODEL_PATH = ""models/"" # This is what I originally had as the location for LDA_MODEL_PATH. I was using a directory called models for multiple lda models. This no longer works.

    doc_df = getCorpus(name_of_lda, cleaned=True) # returns a dataframe containing a row for each text record and an extra column that contains the tokenized version of the text's post/string of words.
    dict_path = ""lda_dir/"" + str(name_of_lda) + ""/dict_of_tokens.dict""
    docs_of_tokens = convert_cleaned_tokens_entries(doc_df['cleaned_tokens'])
    if create_dict != False:
        doc_dict = corpora.Dictionary(docs_of_tokens) :
        if remove_low_freq==True:
            doc_dict.filter_extremes(no_below=5, no_above=0.6)
        doc_dict.save(dict_path)
        print(""Finished saving"") 
    else:
        doc_dict = corpora.Dictionary.load(dict_path)
doc_term_matrix = [doc_dict.doc2bow(doc) for doc in docs_of_tokens] # gives a unique id for each word in corpus_arr

Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(doc_term_matrix, num_topics=15, id2word = doc_dict, passes=20, chunksize=10000)
ldamodel.save(LDA_MODEL_PATH)
</code></pre>

<p>To put it sraightforwardly... I have no clue why permission is being denied when I try to save my lda model to a particular location. Right now even the original <code>models/</code> directory location is giving me ""permission denied"" with this error message. It's seeming like any and all directories I could use just... won't work. This is odd behavior and I can't really find asks that talk about this error in the same context. I have found posts of people getting this error message when they actually tried storing in locations that did not exist. But for me that isn't really a question.</p>

<p>When I first got this error... I actually started to wonder if it was because I had another lda topic model that I named topic_model_1. It was stored in the <code>models/</code> subdirectory. I started to wonder if the name was a potential cause, and changed it to <code>lda_model_topic_1</code> to see if that could change results... but nothing is working.</p>

<p>Even if you can't really figure out what solution applies to my situation (especially since right now I don't have reproducible code, I just have my work)... Can someone tell me what this error message means? When and why does it come up? Maybe that's a start.</p>

<pre><code>      Traceback (most recent call last):
  File ""C:\Users\biney\Miniconda3\lib\site-packages\gensim\utils.py"", line 679,
in save
    _pickle.dump(self, fname_or_handle, protocol=pickle_protocol)
TypeError: file must have a 'write' attribute

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""text_mining.py"", line 461, in &lt;module&gt;
    main()
  File ""text_mining.py"", line 453, in main
    naive_LDA_implementation(name_of_lda=""lda_model_topic_1"", create_dict=True,
remove_low_freq=True)
  File ""text_mining.py"", line 411, in naive_LDA_implementation
    ldamodel.save(LDA_MODEL_PATH)
  File ""C:\Users\biney\Miniconda3\lib\site-packages\gensim\models\ldamodel.py"",
line 1583, in save
    super(LdaModel, self).save(fname, ignore=ignore, separately=separately, *arg
s, **kwargs)
  File ""C:\Users\biney\Miniconda3\lib\site-packages\gensim\utils.py"", line 682,
in save
    self._smart_save(fname_or_handle, separately, sep_limit, ignore, pickle_prot
ocol=pickle_protocol)
  File ""C:\Users\biney\Miniconda3\lib\site-packages\gensim\utils.py"", line 538,
in _smart_save
    pickle(self, fname, protocol=pickle_protocol)
  File ""C:\Users\biney\Miniconda3\lib\site-packages\gensim\utils.py"", line 1337,
 in pickle
    with smart_open(fname, 'wb') as fout:  # 'b' for binary, needed on Windows
  File ""C:\Users\biney\Miniconda3\lib\site-packages\smart_open\smart_open_lib.py
"", line 181, in smart_open
    fobj = _shortcut_open(uri, mode, **kw)
  File ""C:\Users\biney\Miniconda3\lib\site-packages\smart_open\smart_open_lib.py
"", line 287, in _shortcut_open
    return io.open(parsed_uri.uri_path, mode, **open_kwargs)
PermissionError: [Errno 13] Permission denied: 'lda_dir/lda_model_topic_1/model_
dir/'
</code></pre>
","6432861","","","","","2018-07-20 21:41:25","gensim lda permission denied when I try to save my model","<python><text-mining><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"55649311","1","55649667","","2019-04-12 10:10:07","","0","46","<p>I have a function:</p>

<pre><code>def remove_stopwords(text):
     return [[word for word in simple_preprocess(str(doc), min_len = 2) if word not in stop_words] for doc in texts] 
</code></pre>

<p>My input is a list with a tokenized sentence:</p>

<pre><code>input = ['This', 'is', 'an', 'example', 'of', 'my', 'input']
</code></pre>

<p>Assume that <code>stop_words</code> contains the words: 'this', 'is', 'an', 'of' and 'my', then the output I would like to get is:</p>

<pre><code>desired_output = ['example', 'input']
</code></pre>

<p>However, the actual output that I'm getting now is:</p>

<pre><code>actual_output = [[], [], [], ['example'], [], [], ['input']]
</code></pre>

<p>How can I adjust my code, to get this output?</p>
","7714681","","","","","2019-04-12 10:32:24","How to only return actual tokens, rather than empty variables when tokenizing?","<python><apply><tokenize><gensim>","2","0","","","","CC BY-SA 4.0"
"51514825","1","51515395","","2018-07-25 08:57:25","","2","1123","<p>I need to generate word2vec array for a dictionary of words. The dictionary looks something like this </p>

<pre><code>test={0: 'tench, Tinca tinca',
 1: 'goldfish, Carassius auratus',
 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',
 3: 'tiger shark, Galeocerdo cuvieri',
 4: 'hammerhead, hammerhead shark'}
</code></pre>

<p>The loop should go through each line, check if the word exists in the model, if yes then store the vector in an array otherwise check the next word in the line. If none of the words are present in the gensim model, then it should do nothing (array is initialised with zeros)
However if a word doesn't exist in the pre trained model, then it raises this exception:</p>

<blockquote>
  <p>KeyError: ""word 'Galeocerdo cuvieri' not in vocabulary""</p>
</blockquote>

<p>What should be the ideal loop that also has the exception in order to bypass the error raised?
This is my starting code:</p>

<pre><code> import gensim
 model = gensim.models.KeyedVectors.load_word2vec_format('/home/shikhar /Downloads/GoogleNews-vectors-negative300.bin',binary=True) 
 array=np.zeros((4,300)) 
 for i in test:
     synonyms=test[i].split(',')
</code></pre>
","6922785","","","","","2018-07-25 09:47:44","word2vec for dictionary of words","<python><nlp><gensim><word2vec>","1","0","0","","","CC BY-SA 4.0"
"45007902","1","45090080","","2017-07-10 09:05:57","","0","349","<p>I want to know the loss for my w2v model and I upgrade <code>gensim</code> to the latest version, but still can't use the argument <code>compute_loss</code>, am I miss something??  </p>

<p><a href=""https://i.stack.imgur.com/Io83T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Io83T.png"" alt=""enter image description here""></a></p>
","3909384","","","","","2017-07-13 20:10:12","'Word2Vec' object has no attribute 'compute_loss'","<gensim>","1","0","","","","CC BY-SA 3.0"
"51523248","1","","","2018-07-25 16:00:06","","1","2653","<p>I have trained doc2vec model on 4 million records. I want to find most similar sentence to a new sentence i put in from my data but i am getting very bad results.</p>

<p>sample of data:</p>

<pre><code>Xolo Era (Black, 8 GB)(1 GB RAM).
Sugar C6 (White, 16 GB)(2 GB RAM).
Celkon Star 4G+ (Black &amp; Dark Blue, 4 GB)(512 MB RAM).
Panasonic Eluga I2 (Metallic Grey, 16 GB)(2 GB RAM).
Itel IT 5311(Champagne Gold).
Itel A44 Pro (Champagne, 16 GB)(2 GB RAM).
Nokia 2 (Pewter/ Black, 8 GB)(1 GB RAM).
InFocus Snap 4 (Midnight Black, 64 GB)(4 GB RAM).
Panasonic P91 (Black, 16 GB)(1 GB RAM).
</code></pre>

<p>Before passing this data i have done preprocessing which includes
1) Stop words removal.
2) special character and numeric value removal.
3) lowercase the data. 
I have also performed the same steps in testing process.</p>

<p>code which i used for training :</p>

<pre><code>sentences=doc2vec.TaggedLineDocument('training_data.csv') # i have used TaggedLineDocument which can generate label or tags for my data

max_epochs = 100
vec_size = 100
alpha = 0.025

model = doc2vec.Doc2Vec(vector_size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                dm =1,
                min_count=1)
model.build_vocab(sentences)
model.train(sentences, epochs=100, total_examples=model.corpus_count)
model.save('My_model.doc2vec')
</code></pre>

<p>well i am new to gensim and doc2vec so i have followed an example for training my model so please correct me if i have used wrong parameters.</p>

<p>on testing side</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec.load('My_model.doc2vec')
test = 'nokia pewter black gb gb ram'.split()
new_vector = model.infer_vector(test)
similar = model.docvecs.most_similar([new_vector]) 
print(similar) # It returns index of sentence and similarity score
</code></pre>

<p>for testing i have passed same sentences which are present in training data but model does not give related documents as similar document,for example i got ""<strong>lootmela tempered glass guard for micromax canvas juice</strong>"" as a most similar sentence to <strong>""nokia pewter black gb gb ram""</strong> this sentence with 0.80 as a similarity score.</p>

<pre><code>So my questions to you: 
1) Do i need to reconsider parameters for model training?
2) Training process is correct?
3) How to build more accurate model for similarity?
4) Apart from doc2vec what will be your suggestion for similarity (keeping in mind i have very large data so training and testing time should not be much longer)
</code></pre>

<p>Please forgive if question formatting is not good.</p>
","8601437","","8601437","","2018-07-25 16:38:36","2018-07-25 17:20:43","doc2vec inaccurate cosine similarity","<python><machine-learning><gensim><word2vec><doc2vec>","1","3","2","","","CC BY-SA 4.0"
"51527411","1","","","2018-07-25 20:47:34","","1","739","<p>I have documents with over 37M sentences and I'm using Gensim's Doc2Vec to train them. The model training works fine with smaller data sets, say 5M-10M records. However, when training on the full dataset, the process dies mostly at the ""resetting layer weights"" stage. Sometimes, it dies before.</p>

<p>I'm suspecting that it's a memory issue. I have 16GB of RAM with 4 cores. If it's indeed a memory issue, is there any way I can train the model in batches. From reading the documentation, it seems train() is useful in cases where the new documents don't have new vocabularies. But, this is not the case with my documents.</p>

<p>Any suggestions?</p>
","9362235","","","","","2018-07-26 21:25:17","Gensim Doc2Vec training crashes with Killed: 9 error","<gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"55592142","1","55597329","","2019-04-09 11:46:29","","1","454","<p>I don't understand how word vectors are involved at all in the training process with gensim's <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec"" rel=""nofollow noreferrer"">doc2vec</a> in DBOW mode (<code>dm=0</code>). I know that it's disabled by default with <code>dbow_words=0</code>. But what happens when we set <code>dbow_words</code> to 1?</p>

<p>In my understanding of DBOW, the context words are predicted directly from the paragraph vectors. So the only parameters of the model are the <code>N</code> <code>p</code>-dimensional paragraph vectors plus the parameters of the classifier.</p>

<p>But multiple sources hint that it is possible in DBOW mode to co-train word and doc vectors. For instance:</p>

<ul>
<li>section 5 of <a href=""https://www.aclweb.org/anthology/W16-1609"" rel=""nofollow noreferrer"">An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation</a></li>
<li>this SO answer: <a href=""https://stackoverflow.com/questions/27470670/how-to-use-gensim-doc2vec-with-pre-trained-word-vectors/30337118#30337118"">How to use Gensim doc2vec with pre-trained word vectors?</a></li>
</ul>

<p>So, how is this done? <strong>Any clarification would be much appreciated!</strong></p>

<p>Note: for DM, the paragraph vectors are averaged/concatenated with the word vectors to predict the target words. In that case, it's clear that words vectors are trained simultaneously with document vectors. And there are <code>N*p + M*q + classifier</code> parameters (where <code>M</code> is vocab size and <code>q</code> word vector space dim).</p>
","2835597","","2835597","","2019-04-10 12:47:23","2019-04-10 12:47:23","How are word vectors co-trained with paragraph vectors in doc2vec DBOW?","<gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"47080842","1","47706607","","2017-11-02 17:03:00","","2","173","<p>Im using Jython 2.7.1. It is working fine. I need to install gensim. Does this library works in jython?</p>

<p>Thank you</p>
","2869180","","2869180","","2017-11-02 18:30:07","2017-12-08 01:58:13","gensim with jython","<gensim><jython-2.7>","1","0","","","","CC BY-SA 3.0"
"51520655","1","","","2018-07-25 13:52:14","","3","372","<p>I am trying to download gensim pretrained word2vec models behind a proxy. I receive this error.</p>

<blockquote>
  <p>urllib.error.URLError: urlopen error [Errno 11004] getaddrinfo failed </p>
</blockquote>

<p>for the following code </p>

<pre><code>import gensim.downloader as api
api.info() 
</code></pre>

<p>I have already set proxy using </p>

<pre><code>set HTTPS_PROXY=https://username:xxxxxx@myproxy.com 
</code></pre>

<p>and have been successfully downloading packages using pip. Is there a way to add my proxy to gensim?  </p>
","8580190","","","","","2021-02-03 03:59:19","Downloading gensim models behind a proxy","<python><python-3.x><gensim><http-proxy>","1","0","1","","","CC BY-SA 4.0"
"47105869","1","","","2017-11-03 23:47:34","","5","7196","<p>Here is the code:</p>

<pre><code>from pyemd import emd

print(""sentence 1:"")
print(input_document_lower[0])
print(""sentence 2:"")
print(input_document_lower[1])
print(""similarity:"")
model_w2v.wmdistance(input_document_lower[0], input_document_lower[1])
</code></pre>

<p>Here's the error:</p>

<pre><code>sentence 1:
incorrect batch number printed primary label pbn
sentence 2:
unconfirmed oos met vial washing qualification sample per 
similarity:

ImportErrorTraceback (most recent call last)
&lt;ipython-input-201-50af089a2354&gt; in &lt;module&gt;()
      4 print(input_document_lower[1])
      5 print(""similarity:"")
----&gt; 6 model_w2v.wmdistance(input_document_lower[0], input_document_lower[1])

C:\ProgramData\Anaconda2\lib\site-packages\gensim\models\word2vec.pyc in wmdistance(self, document1, document2)
   1308         Refer to the documentation for `gensim.models.KeyedVectors.wmdistance`
   1309         """"""
-&gt; 1310         return self.wv.wmdistance(document1, document2)
   1311 
   1312     def most_similar_cosmul(self, positive=None, negative=None, topn=10):

C:\ProgramData\Anaconda2\lib\site-packages\gensim\models\keyedvectors.pyc in wmdistance(self, document1, document2)
    386 
    387         if not PYEMD_EXT:
--&gt; 388             raise ImportError(""Please install pyemd Python package to compute WMD."")
    389 
    390         # Remove out-of-vocabulary words.

ImportError: Please install pyemd Python package to compute WMD.
</code></pre>

<p>It is being installed properly so I really have no clue as to what is going wrong. Have any of your encountered this?</p>
","7148245","","130288","","2017-11-04 08:27:33","2021-03-10 07:54:13","Getting an error to install pyemd even though I just installed it","<python><installation><word2vec><gensim>","7","1","1","","","CC BY-SA 3.0"
"51438277","1","","","2018-07-20 08:28:02","","0","220","<p>I am trying to train a Doc2Vec word embedding on preprocessed paragraphs. I have removed punctuation, and have carried out tokenization, pos tag and chunking.</p>

<pre><code>import nltk
from nltk import word_tokenize, pos_tag, ne_chunk
from gensim.models.doc2vec import Doc2Vec

ne_tree = ne_chunk(pos_tag(word_tokenize(sent_pun)))

model = Doc2Vec(ne_tree)
</code></pre>

<p>I get the error ""AttributeError: 'Tree' object has no attribute 'words'"" when I run the Doc2Vec model. What should be done to correct this? Thank you.</p>
","9685157","","","","","2018-07-20 09:24:53","AttributeError: 'Tree' object has no attribute 'words'. Doc2Vec error","<model><nltk><gensim><attributeerror><doc2vec>","1","0","","","","CC BY-SA 4.0"
"51438425","1","","","2018-07-20 08:36:25","","1","361","<p><strong>I am currently trying to classify text into 7 classes.</strong> Up to now, I have been able to reach a 90% precision score using Majority Voting (with SVM, Multinomial NB, Random Forest and KNN).</p>

<p><strong>I wanted to try to increase a little more this precision by using word embeddings</strong> and thus getting less dimensions for my samples. I use gensim word2vec to create my model and the NLTK list of stop-words and tokenizer:</p>

<pre><code>with open('data.pkl','r') as f:
    corpus=pickle.load(f)

with open('targets.pkl','r') as f:
    targets=pickle.load(f)

tokenized_corpus=[word_tokenize(anomaly) for anomaly in corpus]
stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', ""you're"", ""you've"", ""you'll"", ""you'd"", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', ""she's"", 'her', 'hers', 'herself', 'it', ""it's"", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', ""that'll"", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', ""don't"", 'should', ""should've"", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', ""aren't"", 'couldn', ""couldn't"", 'didn', ""didn't"", 'doesn', ""doesn't"", 'hadn', ""hadn't"", 'hasn', ""hasn't"", 'haven', ""haven't"", 'isn', ""isn't"", 'ma', 'mightn', ""mightn't"", 'mustn', ""mustn't"", 'needn', ""needn't"", 'shan', ""shan't"", 'shouldn', ""shouldn't"", 'wasn', ""wasn't"", 'weren', ""weren't"", 'won', ""won't"", 'wouldn', ""wouldn't""]
for i,anomaly in enumerate(tokenized_corpus):
    for w in anomaly:
        if w in stop_words:
            tokenized_corpus[i].remove(w)

model = gensim.models.Word2Vec(
        tokenized_corpus,
        window=5,
        size=100)

model.train(tokenized_corpus, total_examples=len(corpus), epochs=10)
</code></pre>

<p>The model seems fine, I get satisfying results when I use similarity between words.</p>

<p>I use this class to get a mean representation of my samples:</p>

<pre><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        if len(word2vec)&gt;0:
            self.dim=len(word2vec[next(iter(word2vec))])
        else:
            self.dim=0

    def fit(self, X, y):
        return self 

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec] 
                    or [np.zeros(self.dim)], axis=0) #moyenne des vecteurs des mots (a.word2vec[w])[ou 0 si il connait pas le mot] composant un √©l√©ment de X
            for words in X
        ])
</code></pre>

<p>and </p>

<pre><code>w2v = dict(zip(model.wv.index2word, model.wv.syn0))
a=MeanEmbeddingVectorizer(w2v)    
X_transformed=a.transform(tokenized_corpus)
</code></pre>

<p>Finally, I build a common sklearn pipeline and let sklearn perform a GridSearchCV on my data:</p>

<pre><code>test_param_n_estimators=[i for i in range(1,50)]
parameters = {'clf2__n_estimators': test_param_n_estimators}

pip=Pipeline([['clf2',RandomForestClassifier()]])

gs_clf2 = GridSearchCV(pip, parameters,verbose=10,n_jobs=2)
gs_clf2 = gs_clf2.fit(X_transformed, targets)

print(gs_clf2.best_score_)
print(gs_clf2.best_params_)
</code></pre>

<p><strong>The issue is that I get random precision scores (around 0.5).</strong></p>

<p>Maybe dimensionality reduction is not always able to increase precision but I think I don't understand something or I did something wrong, <strong>do you have an idea of what is going wrong ?</strong></p>

<p>Thank you in advance</p>
","10071385","","10071385","","2018-07-20 13:05:31","2018-07-20 13:05:31","Word embedding decreasing classification precision","<scikit-learn><nlp><nltk><gensim><text-classification>","1","0","","","","CC BY-SA 4.0"
"62529967","1","","","2020-06-23 07:55:10","","0","37","<p>In order to use the <code>gensim.similarities.docsim.Similarity</code> class to compute similarities between words, one need to provide the corpus and the size of the dictionary.</p>
<p>In my case, the corpus are the word vectors computed using a word2vec model.</p>
<p>I wonder why <code>gensim</code> needs the size of the dictionary? And also, if it needs here the size of the dictionary used to create the word2vec model, or the size of the dictionary of the corpus, for which I want to compute the similarities.</p>
","1711146","","3001761","","2020-06-23 08:17:04","2020-06-23 17:14:06","Why computing similarity with gensim needs the size of the dictionary?","<python><nlp><gensim>","1","1","","","","CC BY-SA 4.0"
"51520031","1","51615039","","2018-07-25 13:23:54","","0","91","<p>I am familiar with the tfidf vectorizer.</p>

<p>However, in gensim it seems like tfidf is treated as a model on itself, just like LDA, LSI and others.</p>

<p>Why is this the case? Can't tfidf not just be used to vectorize and then to input in an LDA model for example?</p>

<p>Link to documentation: <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/tut2.html</a></p>
","9635751","","","","","2019-10-21 07:53:31","Why is TFIDF seen as a model in Gensim","<python><gensim>","1","6","","","","CC BY-SA 4.0"
"64425652","1","","","2020-10-19 10:38:26","","1","686","<p>I want to download the gensim glove-wiki-gigaword-100 dataset. Here's my code</p>
<pre><code>import gensim.downloader as api
model = api.load(&quot;glove-wiki-gigaword-100&quot;)
</code></pre>
<p>But I'm receiving this error</p>
<pre><code>ValueError: unable to read local cache '/Users/xxx/gensim-data/information.json' during fallback, connect to the Internet and retry
</code></pre>
<p>I checked my terminal for the gensim version and got this so I think it's installed</p>
<pre><code>pip3 show gensim
Name: gensim
Version: 3.8.3
Summary: Python framework for fast Vector Space Modelling
Home-page: http://radimrehurek.com/gensim
Author: Radim Rehurek
Author-email: me@radimrehurek.com
License: LGPLv2.1
Location: /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages
Requires: smart-open, scipy, numpy, six
Required-by: 
</code></pre>
<p>I can't figure this out. I turned my laptop on and off and reset the router. I dont think this issue is related to my internet despite the error?</p>
","11303290","","","","","2021-06-27 23:01:42","ValueError when downloading gensim data set","<python><gensim>","2","0","1","","","CC BY-SA 4.0"
"51560126","1","","","2018-07-27 14:30:30","","0","902","<p>I have gensim pretrained  model  and I was trying to find most similar words using <code>model.most_similar('word')</code> So lets say, I have word named as 'politics' so what I have done is</p>
<pre><code>for i,q in model.most_similar('politics'):
    print (i) 
</code></pre>
<p>This gives me output as a list of words. But, same thing when I put it inside function as :</p>
<pre><code>def taxonomy(word):
    for i,q in model.most_similar(word):
        print (i)
</code></pre>
<p>when I used <code>taxonomy('politics')</code>, I get the error: <code>ValueError: cannot compute similarity with no input</code>. Is it something I did wrong?</p>
<h1>edit :</h1>
<p>How I can append the <code>i</code> into blank array with same name as my word. i.e for present array name should be <code>politics</code>, I've done this, but not working</p>
<pre><code>def taxonomy(word):
    word=[]
    for i,q in model.most_similar(word):
        word.append(i)
        return word
</code></pre>
","7319366","","-1","","2020-06-20 09:12:55","2021-01-09 07:30:24","Python for loop not working inside user defined function","<python><python-3.x><function><gensim><cosine-similarity>","2","9","","","","CC BY-SA 4.0"
"51547315","1","51547783","","2018-07-26 20:49:50","","1","1029","<p>I have an issue similar to the one discussed here - <a href=""https://stackoverflow.com/questions/40727093/gensim-word2vec-updating-word-embeddings-with-newcoming-data"">gensim word2vec - updating word embeddings with newcoming data</a></p>

<p>I have the following code that saves a model as <strong>text8_gensim.bin</strong></p>

<pre><code>sentences = word2vec.Text8Corpus('train/text8')
model = word2vec.Word2Vec(sentences, size=200, workers=12, min_count=5,sg=0, window=8, iter=15, sample=1e-4,alpha=0.05,cbow_mean=1,negative=25)
model.save(""./savedModel/text8_gensim.bin"")
</code></pre>

<p>Here is the code that adds more data to the saved model (after loading it)</p>

<pre><code>fname=""savedModel/text8_gensim.bin""
model = word2vec.Word2Vec.load(fname)
model.epochs=15

#Custom words
docs = [""start date"", ""end date"", ""eft date"",""termination date""]
model.build_vocab(docs, update=True)
model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)
model.wv.similarity('start','eft')
</code></pre>

<p>The model loads fine; however when I try to call <strong>model.wv.similarity</strong> function I get the following error</p>

<p><strong>KeyError: ""word 'eft' not in vocabulary""</strong></p>

<p>Am I missing something here?</p>
","2112979","","","","","2018-07-26 21:30:41","gensim word2vec - update model data","<gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"51566257","1","","","2018-07-27 22:21:48","","1","308","<p>I am now working on a project using Gensim.word2vec, and I am a total freshman for this field.</p>

<p>Actually I already got a model. Are there any way that I can get the similarity rank of a word for another word. For example, the top 2 most similar words for the word 'girl' is 'lady' and then 'woman'. Are there any functions I can use if i enter 'lady' is can return 1, if i enter 'woman' it can return 2?</p>

<p>Thanks!</p>
","10146559","","","","","2020-08-31 19:00:35","Return the rank of word in Gensim Word2vec","<python><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"62766787","1","","","2020-07-07 01:30:18","","1","172","<p>First, apologies for being long-winded.</p>
<p>I'm not a mathematician, so I'm hoping there's a &quot;dumbed down&quot; solution to this. In short, I'm attempting to compare two bodies of text to generate recommendations. What you'll see below is a novice attempt at measuring similarity using NLP. I'm open to all feedback. But my primary question: does the method described below serve as an accurate means of finding similarities (in wording, sentiment, etc) in two bodies of text? If not, how would you generate such a recommendation engine (new methods, new data, etc)?</p>
<p>I currently have two dictionaries ‚Äì¬†one with personality data called <strong>personality_feature_dict</strong> that includes the personality type and associated descriptor words: <code>{'Type 1': ['able', 'accepting', 'according', 'accountable'...]}</code> and the other called <strong>book_feature_dict</strong> containing book titles and their own descriptor words, which were extracted using TF-IDF <code>{'Book Title': ['actually', 'administration', 'age', 'allow', 'anti'...]}</code></p>
<p>As it stands, I'm using the following code to calculate the similarity between dictionary values from each to identify % similarity. First, I create a larger corpus using all dictionary items.</p>
<pre><code>book_values = list(book_feature_dict.values())
personality_values = list(personality_feature_dict.values()) 

texts = book_values + personality_values

dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

import numpy as np
np.random.seed(1)
</code></pre>
<p>Then I create an LDA model to identify similarities. My knowledge of LDA modeling is limited, so if you spot an error here, I appreciate you flagging it!</p>
<pre><code>from gensim.models import ldamodel
model = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=4, minimum_probability=1e-8)
</code></pre>
<p>Finally, I iterate through sets of values as bags of words and compare how the first personality type or <code>(personality_feature_dict.values())[personality_num]</code> compares to 99 book descriptions/values by finding the Hellinger distance.</p>
<pre><code>from gensim.matutils import hellinger
personality_num = 0
i = 0

while i &lt; 98:

    s_0 = list(book_feature_dict.values())[i]
    s_0_bow = model.id2word.doc2bow(s_0)
    s_0_lda_bow = model[s_0_bow]

    e_0 = list(personality_feature_dict.values())[personality_num]
    e_0_bow = model.id2word.doc2bow(e_0)
    e_0_lda_bow = model[e_0_bow]

    x = 100 - (hellinger(e_0_lda_bow, s_0_lda_bow)*100)
    i = i+1

</code></pre>
<p>Finally, I print all instances where the LDA model comes back with a high correlation as a percentage.</p>
<pre><code>    if x &gt; 50:
        print (list(personality_feature_dict.keys())[personality_num])
        print('similarity to ', (list(book_feature_dict.keys())[i]), 'is')
        print(x, '%', '\n\n')

</code></pre>
<p>The result looks something like this:</p>
<pre><code>Personality Type 
similarity to  Name of Book 1 is
84.6029228744518 % 


Personality Type 
similarity to  Name of Book 2 is
83.09513184950528 % 


Personality Type 
similarity to  Name of Book 3 is
85.44322295890642 % 

...
</code></pre>
","13099845","","","","","2020-09-30 08:49:45","Is there a Python library or tool that analyzes two bodies of text for similarities in order to provide recommendations?","<python><python-3.x><nlp><gensim><lda>","1","0","1","","","CC BY-SA 4.0"
"67528470","1","","","2021-05-14 02:45:20","","0","80","<p>I am trying to use mallet model using gensim. when i tried to use below code i am getting error. please help me out from this.</p>
<pre><code>import os
os.environ.update({'MALLET_HOME':r'F:\Suneel\Venkat Freelancing\Topic Modeling\new mallet\mallet-2.0.8\'})
mallet_path = 'F:/Suneel/Venkat Freelancing/Topic Modeling/new mallet/mallet-2.0.8/bin/mallet' # update this path
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)
</code></pre>
<p>error:
CalledProcessError: Command 'F:/Suneel/Venkat Freelancing/Topic Modeling/new mallet/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex &quot;\S+&quot; --input C:\Users\hp\AppData\Local\Temp\921f3_corpus.txt --output C:\Users\hp\AppData\Local\Temp\921f3_corpus.mallet' returned non-zero exit status 1.</p>
","10684237","","","","","2021-05-14 02:45:20","How to fix the gensim mallet issue?","<python-3.x><gensim><topic-modeling>","0","2","","","","CC BY-SA 4.0"
"62565772","1","62666889","","2020-06-24 23:52:38","","1","84","<p>I would like to know if it is possible to group together same words included in the LDA's output, i.e. words generated by</p>
<pre><code>doc_lda = lda_model[corpus]
</code></pre>
<p>for example</p>
<pre><code>[(0,
  '0.084*&quot;tourism&quot; + 0.013*&quot;touristic&quot; + 0.013*&quot;Madrid&quot; + '
  '0.010*&quot;travel&quot; + 0.008*&quot;half&quot; + 0.007*&quot;piare&quot; + '
  '0.007*&quot;turism&quot;')]
</code></pre>
<p>I would like to group <code>tourism, touristic</code> and <code>turism</code> (mispelled) together.
Would it be possible?</p>
<p>This is some relevant previous code:</p>
<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=num_topics, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha=[0.01]*num_topics,
                                           per_word_topics=True,
                                           eta=[0.01]*len(id2word.keys())) 
</code></pre>
<p>Thank you</p>
","7788837","","7788837","","2020-06-29 00:52:37","2020-07-03 20:03:52","Grouping words with same meaning. in LDA","<python><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"47117569","1","47118185","","2017-11-05 02:21:00","","12","11841","<p>By doc we can use this to read a word2vec model with genism</p>

<pre><code>model = KeyedVectors.load_word2vec_format('word2vec.50d.txt', binary=False)
</code></pre>

<p>This is an index-to-word mapping, that is, e.g., <code>model.index2word[2]</code>, how to derive an inverted mapping (word-to-index) based on this?</p>
","6733064","","6733064","","2020-02-08 05:09:00","2020-02-08 05:09:00","How to get word2index from gensim","<gensim>","2","0","3","","","CC BY-SA 4.0"
"36371591","1","","","2016-04-02 09:37:58","","3","1530","<p>I'm using Window 10.1 and Python 3.4. I installed the nltk, numpy, scipy and gensim modules using wheel files (URL: <a href=""http://www.lfd.uci.edu/%7Egohlke/pythonlibs/#scipy"" rel=""nofollow noreferrer"">http://www.lfd.uci.edu/~gohlke/pythonlibs/</a>).</p>
<p>Here's a screenshot of the installed modules:</p>
<p><img src=""https://i.stack.imgur.com/fPNxd.jpg"" alt=""Screenshot of installed modules"" /></p>
<p>When I'm running this statement:</p>
<pre><code>from gensim import corpora, models, similarities
</code></pre>
<p>I'm getting this Import error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Python34\sample.py&quot;, line 1, in &lt;module&gt;
    from gensim import corpora, models, similarities
  File &quot;C:\Python34\lib\site-packages\gensim\__init__.py&quot;, line 6, in &lt;module&gt;
    from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
  File &quot;C:\Python34\lib\site-packages\gensim\matutils.py&quot;, line 21, in &lt;module&gt;
    import scipy.linalg
  File &quot;C:\Python34\lib\site-packages\scipy\linalg\__init__.py&quot;, line 174, in &lt;module&gt;
    from .misc import *
  File &quot;C:\Python34\lib\site-packages\scipy\linalg\misc.py&quot;, line 5, in &lt;module&gt;
    from .blas import get_blas_funcs
  File &quot;C:\Python34\lib\site-packages\scipy\linalg\blas.py&quot;, line 155, in &lt;module&gt;
    from scipy.linalg import _fblas
ImportError: DLL load failed: The specified module could not be found.
</code></pre>
<p>How can I fix this?</p>
","6136065","","10388629","","2020-12-18 18:24:49","2020-12-18 18:24:49","""ImportError: DLL load failed: The specified module could not be found"" when importing installed module","<python><installation><scipy><nltk><gensim>","1","2","","","","CC BY-SA 4.0"
"51593971","1","51598911","","2018-07-30 12:27:15","","0","192","<p>I want to label my documents with tags mapped to id attribute in database.
The ids can be for example also like this:</p>

<p>documents[0] is for example</p>

<pre><code>TaggedDocument(words=['blabla', 'request'], tags=[225616076])
</code></pre>

<p>For some reason, it is not able to build_vocabulary. Although I have only 33382 unique ids/tags with higher values, it does not matter, gensim writes that I have '225616077 tags' (in the log).  </p>

<pre><code>2018-07-30 12:07:59,271 : INFO : collecting all words and their counts
2018-07-30 12:07:59,273 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2018-07-30 12:07:59,330 : INFO : PROGRESS: at example #1000, processed 7974 words (314086/s), 1975 word types, 225616077 tags
2018-07-30 12:07:59,343 : INFO : PROGRESS: at example #2000, processed 15882 words (701054/s), 2794 word types, 225616077 tags
...

...  
2018-07-30 12:14:56,454 : INFO : estimated required memory for 6765 words and 20 dimensions: 19793760900 bytes
2018-07-30 12:14:56,457 : INFO : resetting layer weights

---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
in &lt;module&gt;()
----&gt; 1 model.build_vocab(documents)
</code></pre>

<p>How can I solve this problem? I do not want to start from 0 and then map it to the higher numbers (uselessly used compute time). I also tried it to tag it as strings (so the documents[0] is TaggedDocument(words=['blabla', 'request'], tags=['225616076'])) but it does not work either.</p>

<p>I am inspecting gensim's code but can not get to solution on my own.</p>
","5673485","","","","","2018-07-30 16:57:10","Gensim tagging documents with big numbers","<python><gensim><topic-modeling><doc2vec>","1","0","","","","CC BY-SA 4.0"
"51594165","1","51598803","","2018-07-30 12:38:19","","3","321","<p>CBOW word2vec scheme look like this:</p>

<p><a href=""https://i.stack.imgur.com/gmgo6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gmgo6.png"" alt=""enter image description here""></a></p>

<p>How I can extract matrixes WI and WO from <code>gensim.models.word2vec.Word2Vec</code>?
I found only these fields in gensim w2v model:</p>

<p><code>gensim.models.word2vec.Word2Vec.trainables.syn1neg</code></p>

<p>and</p>

<p><code>gensim.models.word2vec.Word2Vec.vw.syn1neg.vectors</code></p>

<p>Can I make an assumption that <code>syn1neg</code> is WI, and WO = <code>vectors</code> - <code>syn1neg</code>?</p>

<p>Why this code</p>

<pre><code>sentences = [['car', 'tree', 'chip2'], ['chip1', 'sugar']]
model = Word2Vec(sentences, min_count=1, size = 5)
</code></pre>

<p>give <code>Word2Vec.trainables.syn1neg</code> matrix with zero elements only?</p>

<p>For 30MB dataset <code>Word2Vec.trainables.syn1neg</code> matrix also contain zero elements only, log is here:</p>

<p><a href=""https://pastebin.com/cKfxv2zz"" rel=""nofollow noreferrer"">gensim log</a></p>
","4399478","","4399478","","2018-08-01 14:55:47","2018-08-01 14:55:47","How I can extract matrixes WI and WO from gensim word2vec?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"55665180","1","55670156","","2019-04-13 11:57:31","","1","287","<p>In gensim I have a trained doc2vec model, if I have a document and either a single word or two-three words, what would be the best way to calculate the similarity of the words to the document? </p>

<p>Do I just do the standard cosine similarity between them as if they were 2 documents? Or is there a better approach for comparing small strings to documents?</p>

<p>On first thought I could get the cosine similarity from each word in the 1-3 word string and every word in the document taking the averages, but I dont know how effective this would be.</p>
","5476045","","5476045","","2019-04-13 12:54:14","2019-04-13 21:48:53","How do I calculate the similarity of a word or couple of words compared to a document using a doc2vec model?","<python><gensim><doc2vec>","1","0","1","","","CC BY-SA 4.0"
"45046058","1","","","2017-07-11 23:34:25","","2","1566","<p>I am new to doc2vec and I wish to classify set of texts using it.</p>

<p>I am confused about TaggedDocument and TaggedLineDocument.</p>

<p>1) What is the difference between two? Is it that TaggedLineDocument is collection of TaggedDocuments?</p>

<p>2) If I have a directory containing all the files, How to generate feature vectors for them? Should I create a new file where each line contains text from the file from the directory?</p>
","8292505","","","","","2017-07-12 01:36:48","Difference between TaggedDocument and TaggedLineDocument in gensim? and How to work with files in a directory?","<nlp><gensim><word2vec><text-classification><doc2vec>","1","1","1","","","CC BY-SA 3.0"
"62581874","1","64819292","","2020-06-25 18:19:05","","4","1994","<p>What is the difference between using <code>gensim.models.LdaMallet</code> and <code>gensim.models.LdaModel</code>? I noticed that the parameters are not all the same and would like to know when one should be used over the other?</p>
","11262404","","","","","2020-11-13 10:27:37","(gensim) LdaMallet vs LdaModel?","<gensim><lda><topic-modeling><mallet>","1","0","","","","CC BY-SA 4.0"
"36374414","1","","","2016-04-02 14:18:38","","0","297","<p>Let me make my question clearer:</p>

<p>I am using python <code>gensim.models.Word2Vec</code> to train a word embedding model. Based on my understanding, the model training is in essence a machine learning issue---to train a neural network via a prediction task. For example, if I select parameters to train a skip-gram model, then the model is trained by predicting context words from target word. Once the model is well-trained, word vectors are just obtained from the model.</p>

<p>If my understanding is correct, so since in fact it is a machine learning process and the training goal is to perform well in the prediction task, there should be a loss function during training and the model is supposed to make the loss as low as possible. So, how to know the <strong>model loss value</strong> for a given set of parameters? Or is there <strong>any other metrics</strong> that we can know to <strong>understand the model itself</strong>?</p>

<p>Hope I have made my question clear. In a word, I <strong>don't</strong> want to evaluate the model by its outputs as in the Google test set <a href=""http://word2vec.googlecode.com/svn/trunk/questions-words.txt"" rel=""nofollow"">http://word2vec.googlecode.com/svn/trunk/questions-words.txt</a>, but I want to understand the model itself as a simple machine learning problem during its training process. Would this be possible?    </p>
","4129364","","4129364","","2016-04-03 15:27:59","2016-04-03 15:27:59","Python Word2Vec: understand the trained model itself in detail","<python><model><gensim><word2vec>","0","2","","","","CC BY-SA 3.0"
"62783726","1","","","2020-07-07 20:50:29","","0","347","<p>I am new in NLU and I am doing a project on document embedding. I want to fine-tune the doc2vec model in gensim on my small dataset to see if it can help for document clustering. I read the tutorial on the website but they did not mention anything about fine-tuning. Where I can find doc2vec pertained on wikipedia or twitter on any big dataset.</p>
","2130515","","","","","2020-07-07 20:50:29","Fine tune doc2vec on gensim","<nlp><gensim><nlu>","0","5","1","","","CC BY-SA 4.0"
"64537121","1","","","2020-10-26 12:28:46","","2","195","<p>my aim is to create document embeddings from the column df[&quot;text&quot;] as a first step and then as a second step plug them along with other variables into a XGBoost Regressor model in order to make predictions. This works very well for the train_df.<br />
I am currently trying to evaluate my trained Doc2Vec model by inferring vectors with infer_vector() on the unseen test_df and then again make predictions with it.However, the results are super bad. I got a very large error (RMSE).
I assume, this means that Doc2Vec is massively overfitting?
I am actually not sure if this is the correct way to evaluate my doc2vec model (by infer_vector)?
What to do to prevent doc2vec from overfitting?</p>
<p>Please find my code below for infering vectors from a model:</p>
<pre><code>vectors_test=[]
for i in range(0, len(test_df)):
    vecs=model.infer_vector(tokenize(test_df[&quot;text&quot;][i]))
    vectors_test.append(vecs)
vectors_test= pd.DataFrame(vectors_test)
test_df = pd.concat([test_df, vectors_test], axis=1)
</code></pre>
<p>I then make predictions with my XGBoost model:</p>
<pre><code>np.random.seed(0)
test_df= test_df.reindex(np.random.permutation(test_df.index))

y = test_df['target'].values
X = test_df.drop(['target'], axis=1).values

y_pred = mod.predict(X)
pred = pd.DataFrame()
pred[&quot;Prediction&quot;] = y_pred
rmse = np.sqrt(mean_squared_error(y,y_pred))
print(rmse)
</code></pre>
<p>Please see also the training of my doc2vec model:</p>
<pre><code>doc_tag = train_df.apply(lambda train_df: TaggedDocument(words=tokenize(train_df[&quot;text&quot;]), tags= [train_df.Tag]), axis = 1)

# initializing model, building a vocabulary 

model = Doc2Vec(dm=0, vector_size=200, min_count=1, window=10, workers= cores) 

model.build_vocab([x for x in tqdm(doc_tag.values)])

# train model for 5 epochs 

for epoch in range(5): 
    model.train(utils.shuffle([x for x in tqdm(doc_tag.values)]), total_examples=len(doc_tag.values), epochs=1)
</code></pre>
","14521644","","","","","2020-10-26 17:12:31","Checking model overfit of doc2vec with infer_vector()","<python><testing><nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"38245739","1","","","2016-07-07 12:41:04","","1","973","<p>I used the <code>MySentences</code> class for extracting sentences from all files in a directory and use this sentences for train a <em>word2vec</em> model.
My dataset is unlabeled.</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

sentences = MySentences('sentences')
model = gensim.models.Word2Vec(sentences)
</code></pre>

<p>Now I want to use that class to make a <em>doc2vec</em> model. I read <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">Doc2Vec</a> reference page. <code>Doc2Vec()</code> function gets sentences as parameter, but it doesn't accept above sentences variable and return error :</p>

<pre><code>AttributeError: 'list' object has no attribute 'words'
</code></pre>

<p>What is the problem? What is the correct type of that parameter?</p>

<p><strong>Update :</strong></p>

<p>I think, unlabeled data is the problem. It seems doc2vec needs labeled data.</p>
","1013249","","2867928","","2016-07-07 12:59:04","2018-10-16 18:48:56","Gensim Doc2Vec - Pass corpus sentences to Doc2Vec function","<python><text-mining><gensim><word2vec><doc2vec>","2","2","","","","CC BY-SA 3.0"
"36397798","1","36437478","","2016-04-04 08:29:35","","1","1443","<p>I have some training sentences generally of warning nature.  Now my goal is to predict weather incoming sentence is a warning message or not. I have gone through <a href=""https://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow"">Sentiment Analysis Using Doc2Vec</a> but according to my understanding it have not considered newly arriving sentence to predict if its positive or negative. </p>

<p>According to my experience I found that the output vector in <code>gensim.doc2vec</code> for each sentence is dependent on other sentences as well, which means we can not directly use the model to generate vector for newly arriving sentence. Please anyone help me with this. Thanks.</p>
","4612998","","","","","2016-04-05 21:33:51","Is it possible to use gensim doc2vec for classification","<nlp><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"64673751","1","","","2020-11-04 03:03:16","","0","234","<p>I am writing code for Gensim Doc2Vec model in Python 3 <br>
This is the snippet I am running:</p>
<pre><code>model1.docvecs.doctag_syn0norm =  (model1.docvecs.doctag_syn0 / sqrt((model1.docvecs.doctag_syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)[d_indices]
</code></pre>
<p>I am getting the following error:</p>
<pre><code>AttributeError: can t set attribute
</code></pre>
<p>The right hand value is getting calculated when I tested separately but is not getting assigned.</p>
<p>I feel that it is a setter issue as I can't set the value to what I want for that class attribute.</p>
<p>Let me know if there's any work-around as I don't want to change gensim's source code.</p>
<p><a href=""https://github.com/satya-tsky/NETL-Automatic-Topic-Labelling-"" rel=""nofollow noreferrer"">Clone my repo</a> and run get_labels.py in model run folder following Readme instructions to reproduce the error.</p>
<p>It's in line 90 of cand_gen.py</p>
<p>The full error stack trace is:</p>
<pre><code>Extracting candidate labels
models loaded
Data Gathered
cand_generation.py:71: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).
  model1.wv.syn0norm = (model1.wv.syn0 / sqrt((model1.wv.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)
cand_generation.py:71: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).
  model1.wv.syn0norm = (model1.wv.syn0 / sqrt((model1.wv.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)
cand_generation.py:89: DeprecationWarning: Call to deprecated `doctag_syn0` (Attribute will be removed in 4.0.0, use docvecs.vectors_docs instead).
  model1.docvecs.doctag_syn0norm =  (model1.docvecs.doctag_syn0 / sqrt((model1.docvecs.doctag_syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)[d_indices]
Traceback (most recent call last):
  File &quot;cand_generation.py&quot;, line 89, in &lt;module&gt;
    model1.docvecs.doctag_syn0norm =  (model1.docvecs.doctag_syn0 / sqrt((model1.docvecs.doctag_syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)[d_indices]
AttributeError: can t set attribute
Executing Unsupervised model
Traceback (most recent call last):
  File &quot;unsupervised_labels.py&quot;, line 33, in &lt;module&gt;
    test_chunk_size = len(label_list[0])
IndexError: list index out of range
Executing Supervised Model
page Rank models loaded
Traceback (most recent call last):
  File &quot;supervised_labels.py&quot;, line 49, in &lt;module&gt;
    test_chunk_size = len(label_list[0])
IndexError: list index out of range

</code></pre>
","10779528","","10779528","","2020-11-04 23:50:57","2020-11-05 23:45:37","Attribute Error for gensim.models.docvecs.doctag_syn0norm","<python><nlp><gensim><word2vec><doc2vec>","1","7","","","","CC BY-SA 4.0"
"62492450","1","","","2020-06-20 22:31:59","","0","96","<p>I am new to Gensim and I am learning Gensim and followed the example here: <a href=""https://www.machinelearningplus.com/nlp/gensim-tutorial/"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/gensim-tutorial/</a></p>
<p>I am not to sure about the last line that creates the corpus from dictionary. When creating the dictionary, we already used simple_preprocess to process the &quot;documents&quot; line by line. I was thinking while creating the corpus using the dictionary, we needed to use simple_preprocess again to process &quot;documents&quot; line by line. Is that redundant?</p>
<pre><code>documents = [&quot;This is the first line&quot;,
         &quot;This is the second sentence&quot;,
         &quot;This third document&quot;]

# Create the Dictionary and Corpus
mydict = corpora.Dictionary([simple_preprocess(line) for line in documents])
# Why need to use simple_preprocess and pass the documents again while
# the last call already created the dictionary using simple_preporcess on documents
corpus = [mydict.doc2bow(simple_preprocess(line)) for line in documents]
</code></pre>
<p>Thanks,</p>
<p>Alex</p>
","1941319","","1941319","","2020-06-22 16:49:07","2020-06-22 16:49:07","Questions on Gensim create corpus from dictionary","<python><gensim>","1","1","","","","CC BY-SA 4.0"
"62543491","1","62543802","","2020-06-23 20:47:35","","0","115","<p>I am trying to understand what is going wrong in the following example.</p>
<p>To train on the 'text8' dataset as described in the docs, one only has to do the following:</p>
<pre><code>import gensim.downloader as api
from gensim.models import Word2Vec

dataset = api.load('text8')
model = Word2Vec(dataset)
</code></pre>
<p>doing this gives very good embedding vectors, as verified by evaluating on a word-similarity task.</p>
<p>However, when loading the same textfile which is used above manually, as in</p>
<pre><code>text_path = '~/gensim-data/text8/text'
text = []
with open(text_path) as file:
    for line in file:
        text.extend(line.split())
text = [text]

model = Word2Vec(test)
</code></pre>
<p>The model still says it's training for the same number of epochs as above (5), but training is much faster, and the resulting vectors have a very, very bad performance on the similarity task.</p>
<p>What is happening here? I suppose it could have to do with the number of 'sentences', but the text8 file seems to have only a single line, so does gensim.downloader split the text8 file into sentences? If yes, of which length?</p>
","9007131","","9007131","","2020-06-23 20:56:06","2020-06-23 21:09:44","Inconsistent results when training gensim model with gensim.downloader vs manual loading","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"36192132","1","","","2016-03-24 02:14:38","","0","5794","<p>I am working on a project where I need to apply topic modelling to a set of documents and I need to create a matrix :</p>

<p>DT , a D √ó T matrix, where D is the number of documents and T is the number of topics. DT(ij) contains the number of times a word in document Di  has been assigned to topic Tj.</p>

<p>So far I have followed this tut: <a href=""https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"" rel=""nofollow"">https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html</a></p>

<p>I am new to gensim  and so far I have
1. created a document list
2. preprocessed and tokenized the documents.
3. Used corpora.Dictionary() to create id-> term dictionary (id2word)
4. convert tokenized documents into a document-term matrix </p>

<p>generated an LDA model.
So now I get the topics.</p>

<p>How can I now get the matrix that I mentioned before.
I will be using this matrix to calculate similarity between 2 documents on topic t as :</p>

<p>sim(a,b) = 1- |DT(a,t) - DT(b, t)|</p>
","1105139","","","","","2017-06-17 01:00:29","Gensim - LDA create a document- topic matrix","<python><lda><gensim><topic-modeling>","2","0","1","","","CC BY-SA 3.0"
"27632404","1","27636142","","2014-12-24 06:17:47","","4","3331","<p>I am trying to use gensim word2vec. I am unable to train the model based on Brown Corpus. Here is my code.</p>

<pre><code>from gensim import models

model = models.Word2Vec([sentence for sentence in models.word2vec.BrownCorpus(""E:\\nltk_data\\"")],workers=4)
model.save(""E:\\data.bin"")
</code></pre>

<p>I downloaded nltk_data using <code>nltk.download()</code>.  I am getting the error below.</p>

<pre><code>C:\Python27\lib\site-packages\gensim-0.10.1-py2.7.egg\gensim\models\word2vec.py:401: UserWarning: Cython compilation failed, training will be slow. Do you have Cython installed? `pip install cython`
  warnings.warn(""Cython compilation failed, training will be slow. Do you have Cython installed? `pip install cython`"")
Traceback (most recent call last):
  File ""E:\eclipse_workspace\Python_files\Test\Test.py"", line 8, in &lt;module&gt;
    model = models.Word2Vec([sentence for sentence in models.word2vec.BrownCorpus(""E:\\nltk_data\\"")],workers=4)
  File ""C:\Python27\lib\site-packages\gensim-0.10.1-py2.7.egg\gensim\models\word2vec.py"", line 276, in __init__
    self.train(sentences)
  File ""C:\Python27\lib\site-packages\gensim-0.10.1-py2.7.egg\gensim\models\word2vec.py"", line 407, in train
    raise RuntimeError(""you must first build vocabulary before training the model"")
RuntimeError: you must first build vocabulary before training the model
</code></pre>

<p>What am I doing wrong?</p>
","2598279","","","","","2018-01-17 17:53:24","How should I train gensim on Brown corpus","<python><gensim>","2","0","1","","","CC BY-SA 3.0"
"55764137","1","55779156","","2019-04-19 15:25:32","","0","522","<p>I have a word list like<code>['like','Python']</code>and I want to load pre-trained Glove word vectors of these words, but the Glove file is too large, is there any fast way to do it? </p>

<p><strong>What I tried</strong></p>

<p>I iterated through each line of the file to see if the word is in the list and add it to a dict if True. But this method is a little slow.</p>

<pre><code>def readWordEmbeddingVector(Wrd):
    f = open('glove.twitter.27B/glove.twitter.27B.200d.txt','r')
    words = []
    a = f.readline()
    while a!= '':
        vector = a.split()
        if vector[0] in Wrd:
            words.append(vector)
            Wrd.remove(vector[0])
        a = f.readline()
    f.close()
    words_vector = pd.DataFrame(words).set_index(0).astype('float')
    return words_vector
</code></pre>

<p>I also tried below, but it loaded the whole file instead of vectors I need</p>

<pre class=""lang-py prettyprint-override""><code>gensim.models.keyedvectors.KeyedVectors.load_word2vec_format('word2vec.twitter.27B.200d.txt')
</code></pre>

<p><strong>What I want</strong></p>

<p>Method like <code>gensim.models.keyedvectors.KeyedVectors.load_word2vec_format</code> but I can set a word list to load.</p>
","11384649","","","","","2021-07-04 06:14:44","Load a part of Glove vectors with gensim","<python><gensim><word-embedding><glove>","1","1","","","","CC BY-SA 4.0"
"47381841","1","","","2017-11-19 20:47:53","","0","137","<p>I am building a chat-bot where every message a user sends needs to be converted to a vector(for other ML related work). I am using a pre-trained Word2Vec model to do this. The Word2Vec model was created using the Gensim library and is saved to disk as a 600MB file and is being used in a Django/Python web-application.</p>

<p>Every time a new message is received as an API request, a function loads the word2Vec model and uses that object to generate a vector of the message. This needs to happen on a real time basis. I am worried that every time a new message is received, the application loads an instance of the Word2Vec model and this would cause a memory problem if there are too many requests coming at the same time(because there will be multiple instance of the Word2Vec model present in the RAM at that time). How do I handle the memory efficiently such that it does not use too much memory?</p>
","2315762","","2315762","","2017-11-21 04:04:07","2017-11-21 04:04:07","Handling a large number of requests that use an ML model","<django><memory-management><machine-learning><word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"47386166","1","","","2017-11-20 06:28:05","","2","1087","<p>I have been given a doc2vec model using gensim which was trained on 20 Million documents. The 20 Million documents it was trained are also given to me but I have no idea how or which order the documents were trained in from the folder. I am supposed to use the test data to find the top 10 match from the training set. The code I use is - </p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec.load(""doc2vec_sample.model"")

test_docs=[""This is the test set I want to test on.""]

def read_corpus(documents, tokens_only=False):
    count=0
    count=count+1
    for line in documents:
        if tokens_only:
            yield gensim.utils.simple_preprocess(line)
        else:
            # For training data, add tags
            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [count])


test_corpus = list(read_corpus(test_docs, tokens_only=True))

doc_id=0

inferred_vector = model.infer_vector(test_corpus[doc_id])
maxx=10
sims = model.docvecs.most_similar([inferred_vector], topn=maxx)

for match in sims:
    print match
</code></pre>

<p>`
The output I get is -</p>

<pre><code>(1913, 0.4589531719684601)
(3250, 0.4300411343574524)
(1741, 0.42669129371643066)
(1, 0.4023148715496063)
(1740, 0.3929900527000427)
(1509, 0.39229822158813477)
(3189, 0.387174129486084)
(3145, 0.3842133581638336)
(1707, 0.3813004493713379)
(3200, 0.3754497170448303)
</code></pre>

<p>How do I get to know which document does document id ""1913"" refer to? How can I access the documents of the trained data set from these 10 job ids? </p>
","5729119","","","","","2018-04-20 18:54:21","How to access document details from Doc2Vec similarity scores in gensim model?","<python><gensim><doc2vec><sentence-similarity>","2","4","1","","","CC BY-SA 3.0"
"54131612","1","54133330","","2019-01-10 15:11:13","","0","586","<p>I am struggling to implement FastText (<a href=""https://github.com/kataev/gensim/blob/969b7178a7c90690f18db33807b390048ef0c83e/gensim/sklearn_api/ftmodel.py"" rel=""nofollow noreferrer"">FTTransformer</a>) into a Pipeline that iterates over different vectorizers. More particular, I can't get cross-validation scores. Following code is used:</p>

<pre><code>%%time
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.pipeline import Pipeline
from gensim.utils import simple_preprocess
from gensim.sklearn_api.ftmodel import FTTransformer
np.random.seed(0)

data = pd.read_csv('https://pastebin.com/raw/dqKFZ12m')
X_train, X_test, y_train, y_test = train_test_split(data.text, data.label, random_state=0)
w2v_texts = [simple_preprocess(doc) for doc in X_train]

models = [FTTransformer(size=10, min_count=0, seed=42)]
classifiers = [LogisticRegression(random_state=0)]

for model in models:

    for classifier in classifiers:

        model.fit(w2v_texts)
        classifier.fit(model.transform(X_train), y_train)

        pipeline = Pipeline([
                ('vec', model),
                ('clf', classifier)
            ])

        print(pipeline.score(X_train, y_train))
        #print(model.gensim_model.wv.most_similar('kirk'))

        cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=5)
</code></pre>

<blockquote>
  <p>KeyError: 'all ngrams for word  ""Machine learning can be useful
  branding sometimes"" absent from model'</p>
</blockquote>

<p><strong>How can the problem be solved?</strong> </p>

<p>Sidenote: My other pipelines with <code>D2VTransformer</code> or <code>TfIdfVectorizer</code> work just fine. Here, I can simply apply <code>pipeline.fit(X_train, y_train)</code> after defining the pipeline, instead of the two fits as shown above. It seems like <a href=""https://github.com/kataev/gensim/blob/969b7178a7c90690f18db33807b390048ef0c83e/gensim/sklearn_api/ftmodel.py"" rel=""nofollow noreferrer"">FTTransformer</a> doesn't integrate so well with other given vectorizers?  </p>
","4697646","","","","","2019-01-10 16:45:52","FastText: Can't get cross_validation","<scikit-learn><cross-validation><gensim>","1","0","","","","CC BY-SA 4.0"
"45571295","1","45575362","","2017-08-08 14:38:45","","1","1548","<p>I am using word embeddings for finding similarity between two sentences. Using word2vec, I also get a similarity measure if one sentence is in English and the other one in Dutch (though not very good). </p>

<p>So I started wondering if it's possible to compute the similarity between two sentences in two different languages (without an explicit translation), especially if the languages have some similarities (Englis/Dutch)?</p>
","1996842","","1996842","","2017-08-08 15:08:50","2020-05-16 10:58:05","Semantic Similarity across multiple languages","<nlp><nltk><gensim><word2vec>","2","2","","","","CC BY-SA 3.0"
"54243797","1","54244794","","2019-01-17 20:27:57","","1","1659","<p>I am using gensim to create Word2Vec models trained on large text corpora. I have some models based on StackExchange data dumps. I also have a model trained on a corpus derived from English Wikipedia. </p>

<p>Assume that a vocabulary term is in both models, and that the models were created with the same parameters to Word2Vec. Is there any way to combine or add the vectors from the two separate models to create a single new model that has the same word vectors that would have resulted if I had combined both corpora initially and trained on this data?</p>

<p>The reason I want to do this is that I want to be able to generate a model with a specific corpus, and then if I process a new corpus later, I want to be able to add this information to an existing model rather than having to combine corpora and retrain everything from scratch (i.e. I want to avoid reprocessing every corpus each time I want to add information to the model). </p>

<p>Are there builtin functions in gensim or elsewhere that will allow me to combine models like this, adding information to existing models instead of retraining?</p>
","176410","","","","","2020-05-16 11:17:12","Combining/adding vectors from different word2vec models","<python><gensim><word2vec><training-data><corpus>","2","0","","","","CC BY-SA 4.0"
"51457515","1","51459252","","2018-07-21 15:29:33","","-1","164","<p>I have several tables that have different column names which are mapped through ETL. There are a total of around 200 tables and 500 attributes, so the set is not massive.</p>

<p>Some column mappings are as follows:</p>

<pre><code>startDate EFT_DATE
startDate START_DATE
startDate entryDate 
</code></pre>

<p>As you can see the same column name can be mapped to different names across different tables. </p>

<p>I'm trying to solve the following problem :</p>

<p>Given two schemas I want to find matches between attribute names. </p>

<p>I was wondering if there is a way to leverage gensim to solve this problem similar to source-words from Google example. The challenge I'm facing is which dataset to use to train the model. Also I am wondering if there is another approach to solve the problem. </p>
","2112979","","7117003","","2018-07-21 19:34:41","2018-07-21 20:27:38","Attribute mapping using Machine learning","<machine-learning><database-design><gensim>","1","0","","","","CC BY-SA 4.0"
"51492778","1","","","2018-07-24 07:23:46","","12","5677","<p>I am trying to build a translation network using embedding and RNN. I have trained a Gensim Word2Vec model and it is learning word associations pretty well. However, I couldn‚Äôt get my head around how to properly add the layer to a Keras model. (And how to do an ‚Äòinverse embedding‚Äô for the output. But that‚Äôs another question that had been answered: by default you can‚Äôt.)</p>

<p>In Word2Vec, when you input a string, e.g. <code>model.wv[‚Äòhello‚Äô]</code>, you get a vector representation of the word. However, I believe that the <code>keras.layers.Embedding</code> layer returned by Word2Vec's <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.get_keras_embedding"" rel=""noreferrer"">get_keras_embedding()</a> takes a one-hot/tokenized input, instead of a string input. But the documentation provides no explanation on what the appropriate input is. <strong>I cannot figure out how to obtain the one-hot/tokenized vector of the vocabulary that has 1-to-1 correspondence with the Embedding layer‚Äôs input.</strong></p>

<p>More elaboration below:</p>

<p>Currently my workaround is to apply the embedding outside Keras before feeding it to the network. Is there any detriment in doing this? I will set the embedding to non-trainable anyway. So far I have noticed that memory use is extremely inefficient (like 50GB even before declaring the Keras model for a collection of 64-word-long sentences) having to load the padded inputs and the weights outside the model. Maybe generator can help.</p>

<p>The following is my code. Inputs are padded to 64-words long. The Word2Vec embedding has 300 dimensions. There are probably a lot of mistakes here due to repeated experimentation trying to make embedding work. Suggestions are welcome.</p>

<pre class=""lang-python prettyprint-override""><code>import gensim
word2vec_model = gensim.models.Word2Vec.load(‚Äúword2vec.model"")
</code></pre>

<pre class=""lang-python prettyprint-override""><code>from keras.models import Sequential
from keras.layers import Embedding, GRU, Input, Flatten, Dense, TimeDistributed, Activation, PReLU, RepeatVector, Bidirectional, Dropout
from keras.optimizers import Adam, Adadelta
from keras.callbacks import ModelCheckpoint
from keras.losses import sparse_categorical_crossentropy, mean_squared_error, cosine_proximity

keras_model = Sequential()
keras_model.add(word2vec_model.wv.get_keras_embedding(train_embeddings=False))
keras_model.add(Bidirectional(GRU(300, return_sequences=True, dropout=0.1, recurrent_dropout=0.1, activation='tanh')))
keras_model.add(TimeDistributed(Dense(600, activation='tanh')))
# keras_model.add(PReLU())
# ^ For some reason I get error when I add Activation ‚Äòoutside‚Äô:
# int() argument must be a string, a bytes-like object or a number, not 'NoneType'
# But keras_model.add(Activation('relu')) works.
keras_model.add(Dense(source_arr.shape[1] * source_arr.shape[2]))
# size = max-output-sentence-length * embedding-dimensions to learn the embedding vector and find the nearest word in word2vec_model.wv.similar_by_vector() afterwards.
# Alternatively one can use Dense(vocab_size) and train the network to output one-hot categorical words instead.
# Remember to change Keras loss to sparse_categorical_crossentropy.
# But this won‚Äôt benefit from Word2Vec. 

keras_model.compile(loss=mean_squared_error,
              optimizer=Adadelta(),
              metrics=['mean_absolute_error'])
keras_model.summary()
</code></pre>

<pre class=""lang-none prettyprint-override""><code>_________________________________________________________________ 
Layer (type)                 Output Shape              Param #   
================================================================= 
embedding_19 (Embedding)     (None, None, 300)         8219700   
_________________________________________________________________ 
bidirectional_17 (Bidirectio (None, None, 600)         1081800   
_________________________________________________________________ 
activation_4 (Activation)    (None, None, 600)         0         
_________________________________________________________________ 
time_distributed_17 (TimeDis (None, None, 600)         360600    
_________________________________________________________________ 
dense_24 (Dense)             (None, None, 19200)       11539200  
================================================================= 
Total params: 21,201,300 
Trainable params: 12,981,600 
Non-trainable params: 8,219,700
_________________________________________________________________
</code></pre>

<pre class=""lang-python prettyprint-override""><code>filepath=""best-weights.hdf5""
checkpoint = ModelCheckpoint(filepath, monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='auto')
callbacks_list = [checkpoint]
keras_model.fit(array_of_word_lists, array_of_word_lists_AFTER_being_transformed_by_word2vec, epochs=100, batch_size=2000, shuffle=True, callbacks=callbacks_list, validation_split=0.2)
</code></pre>

<p>Which throws an error when I try to fit the model with text:</p>

<pre><code>Train on 8000 samples, validate on 2000 samples Epoch 1/100

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-32-865f8b75fbc3&gt; in &lt;module&gt;()
      2 checkpoint = ModelCheckpoint(filepath, monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='auto')
      3 callbacks_list = [checkpoint]
----&gt; 4 keras_model.fit(array_of_word_lists, array_of_word_lists_AFTER_being_transformed_by_word2vec, epochs=100, batch_size=2000, shuffle=True, callbacks=callbacks_list, validation_split=0.2)

~/virtualenv/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1040                                         initial_epoch=initial_epoch,
   1041                                         steps_per_epoch=steps_per_epoch,
-&gt; 1042                                         validation_steps=validation_steps)
   1043 
   1044     def evaluate(self, x=None, y=None,

~/virtualenv/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    197                     ins_batch[i] = ins_batch[i].toarray()
    198 
--&gt; 199                 outs = f(ins_batch)
    200                 if not isinstance(outs, list):
    201                     outs = [outs]

~/virtualenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)
   2659                 return self._legacy_call(inputs)
   2660 
-&gt; 2661             return self._call(inputs)
   2662         else:
   2663             if py_any(is_tensor(x) for x in inputs):

~/virtualenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)
   2612                 array_vals.append(
   2613                     np.asarray(value,
-&gt; 2614                                dtype=tensor.dtype.base_dtype.name))
   2615         if self.feed_dict:
   2616             for key in sorted(self.feed_dict.keys()):

~/virtualenv/lib/python3.6/site-packages/numpy/core/numeric.py in asarray(a, dtype, order)
    490 
    491     """"""
--&gt; 492     return array(a, dtype, copy=False, order=order)
    493 
    494 

ValueError: could not convert string to float: 'hello'
</code></pre>

<p>The following is <a href=""https://rajmak.wordpress.com/2017/12/07/text-classification-classifying-product-titles-using-convolutional-neural-network-and-word2vec-embedding/"" rel=""noreferrer"">an excerpt from Rajmak</a> demonstrating how to use a tokenizer to convert words into the input of a Keras Embedding.</p>

<pre class=""lang-python prettyprint-override""><code>tokenizer = Tokenizer(num_words=MAX_NB_WORDS) 
tokenizer.fit_on_texts(all_texts) 
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index
‚Ä¶‚Ä¶
indices = np.arange(data.shape[0]) # get sequence of row index 
np.random.shuffle(indices) # shuffle the row indexes 
data = data[indices] # shuffle data/product-titles/x-axis
‚Ä¶‚Ä¶
nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0]) 
x_train = data[:-nb_validation_samples]
‚Ä¶‚Ä¶
word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)
</code></pre>

<blockquote>
  <p>Keras embedding layer can be obtained by Gensim Word2Vec‚Äôs¬†word2vec.get_keras_embedding(train_embeddings=False)¬†method or constructed like shown below. The null word embeddings indicate the number of words not found in our pre-trained vectors (In this case Google News). This could possibly be unique words for brands in this context.</p>
</blockquote>

<pre class=""lang-python prettyprint-override""><code>from keras.layers import Embedding
word_index = tokenizer.word_index
nb_words = min(MAX_NB_WORDS, len(word_index))+1

embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))
for word, i in word_index.items():
    if word in word2vec.vocab:
        embedding_matrix[i] = word2vec.word_vec(word)
print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))

embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1
                            embedding_matrix.shape[1], # or EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)

from keras.models import Sequential
from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation

model = Sequential()
model.add(embedding_layer)
model.add(Dropout(0.2))
model.add(Conv1D(300, 3, padding='valid',activation='relu',strides=2))
model.add(Conv1D(150, 3, padding='valid',activation='relu',strides=2))
model.add(Conv1D(75, 3, padding='valid',activation='relu',strides=2))
model.add(Flatten())
model.add(Dropout(0.2))
model.add(Dense(150,activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Dense(3,activation='sigmoid'))

model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])

model.summary()

model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2, batch_size=128)
score = model.evaluate(x_val, y_val, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
</code></pre>

<p>Here the <code>embedding_layer</code> is explicitly created using:</p>

<pre><code>for word, i in word_index.items():
    if word in word2vec.vocab:
        embedding_matrix[i] = word2vec.word_vec(word)
</code></pre>

<p>However, if we use <code>get_keras_embedding()</code>, the embedding matrix <em>is already constructed and fixed.</em> I do not know how each word_index in the Tokenizer can be coerced match the corresponding word in <code>get_keras_embedding()</code>'s Keras embedding input.</p>

<p><strong>So, what is the proper way to use Word2Vec's get_keras_embedding() in Keras?</strong></p>
","2473022","","","","","2018-07-25 16:28:24","How to properly use get_keras_embedding() in Gensim‚Äôs Word2Vec?","<python><keras><gensim><word2vec><word-embedding>","1","1","4","","","CC BY-SA 4.0"
"62534889","1","","","2020-06-23 12:36:29","","0","252","<p>i have trouble with this error while using word2vec from gensim</p>
<pre><code>MemoryError: Unable to allocate 380. MiB for an array with shape (331792, 300) and data type float32
</code></pre>
<p>and this is my code</p>
<pre><code>from gensim.models import Word2Vec

@app.route('/admin/index')
def show_admin():

model_baru = Word2Vec.load('model/idwiki_word2vec_300.model')
tester = model_baru.most_similar(positive=['wanita'],topn=3)
hasil = print(tester)

return render_template('admin/index.html',hasil=hasil)
</code></pre>
<p>what happen with this error?
please help me</p>
<p>thanks for your attention</p>
","13647565","","400617","","2020-06-23 13:25:16","2020-06-23 13:25:16","MemoryError: Unable to allocate 380. MiB for an array with shape (331792, 300) and data type float32","<gensim><word2vec>","0","2","","","","CC BY-SA 4.0"
"53742400","1","53851705","","2018-12-12 11:49:30","","0","356","<p>A couple of years ago, a previous developer for my team wrote the following Python code calling word2vec, passing in a training file and the location of an output file. He worked on Linux. I have been asked to get this running on a Windows machine. Bearing in mind <em>I know next to no Python</em>, I have installed Gensim which I'm guessing implements word2vec now, but do not know how to rewrite the code to use the library rather than the executable which it doesnt seem possible to compile on a Windows box. Could someone help me update this code please?</p>

<pre><code>#!/usr/bin/env python3

import os
import csv
import subprocess
import shutil

from gensim.models import word2vec

def train_word2vec(trainFile, output):
    # run word2vec:
    subprocess.run([""word2vec"", ""-train"", trainFile, ""-output"", output,
                    ""-cbow"", ""0"", ""-window"", ""10"", ""-size"", ""100""],
                   shell=False)
    # Remove some invalid unicode:
    with open(output, 'rb') as input_,\
         open('%s.new' % output, 'w') as new_output:
        for line in input_:
            try:
                print(line.decode('utf-8'), file=new_output, end='')
            except UnicodeDecodeError:
                print(line)
                pass
    shutil.move('%s.new' % output, output)

def main():
    train_word2vec(""c:/temp/wc/test1_BigF.txt"", ""c:/temp/wc/test1_w2v_model.txt"")

if __name__ == '__main__':
    main()
</code></pre>
","2756","","2756","","2018-12-12 14:06:48","2018-12-19 12:54:14","How to run word2vec on Windows using gensim","<python><python-3.x><gensim><word2vec>","2","2","","","","CC BY-SA 4.0"
"51616074","1","51722607","","2018-07-31 14:41:18","","1","744","<p>Following <a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">related question solution</a> I created docker container which loads GoogleNews-vectors-negative300 KeyedVector inside docker container and load it all to memory</p>

<pre><code>KeyedVectors.load(model_path, mmap='r')
word_vectors.most_similar('stuff')
</code></pre>

<p>Also I have another Docker container which provides REST API which loads this model with </p>

<pre><code>KeyedVectors.load(model_path, mmap='r')
</code></pre>

<p>And I observe that fully loaded container takes more than 5GB of memory and each gunicorn worker takes 1.7 GB of memory.</p>

<pre><code>CONTAINER ID        NAME                        CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS
acbfd080ab50        vectorizer_model_loader_1   0.00%               5.141GiB / 15.55GiB   33.07%              24.9kB / 0B         32.9MB / 0B         15
1a9ad3dfdb8d        vectorizer_vectorizer_1     0.94%               1.771GiB / 15.55GiB   11.39%              26.6kB / 0B         277MB / 0B          17
</code></pre>

<p>However, I expect that all this processes share same memory for KeyedVector, so it only takes 5.4 GB shared between all containers.</p>

<p>Have someone tried to achieve that and succeed?</p>

<p>edit: 
I tried following code snippet and it indeed share same memory across different containers.</p>

<pre><code>import mmap
from threading import Semaphore

with open(""data/GoogleNews-vectors-negative300.bin"", ""rb"") as f:
    # memory-map the file, size 0 means whole file
    fileno = f.fileno()
    mm = mmap.mmap(fileno, 0, access=mmap.ACCESS_READ)
    # read whole content
    mm.read()
    Semaphore(0).acquire()
    # close the map
    mm.close()
</code></pre>

<p>So the problem that <code>KeyedVectors.load(model_path, mmap='r')</code> don't share memory</p>

<p>edit2:
Studying gensim's source code I see that <code>np.load(subname(fname, attrib), mmap_mode=mmap)</code> is called to open memmaped file. Following code sample shares memory across multiple container.</p>

<pre><code>from threading import Semaphore

import numpy as np

data = np.load('data/native_format.bin.vectors.npy', mmap_mode='r')
print(data.shape)
# load whole file to memory
print(data.mean())
Semaphore(0).acquire()
</code></pre>
","6663153","","6663153","","2018-08-06 13:38:04","2018-08-07 08:49:54","Sharing memory for gensim's KeyedVectors objects between docker containers","<python><mmap><gensim><word2vec>","2","0","0","","","CC BY-SA 4.0"
"21403839","1","21466752","","2014-01-28 11:04:12","","2","1785","<p>I want to cluster words based on their semantic similarity. Currently I have a list of documents with detected noun phrases in them. I want to make cluster out of these obtained nouns within the documents and unsupervisedly cluster them semantically?</p>

<p>I have looked at wordnet and gensim libraries. Any suggestions as to which can really help in getting the required cluster of words based on their semantic similarity?</p>
","812950","","812950","","2014-01-29 08:57:33","2014-01-30 20:25:14","Unsupervised Clustering of Words in a document semantically","<python><cluster-analysis><semantics><wordnet><gensim>","1","0","","","","CC BY-SA 3.0"
"53837088","1","","","2018-12-18 16:15:30","","3","778","<p>I used gensim to build a word2vec embedding of my corpus.
Currently I'm converting my (padded) input sentences to the word vectors using the gensim model.
This vectors are used as input for the model.</p>

<pre><code>model = Sequential()
model.add(Masking(mask_value=0.0, input_shape=(MAX_SEQUENCE_LENGTH, dim)))
model.add(Bidirectional(
    LSTM(num_lstm, dropout=0.5, recurrent_dropout=0.4, return_sequences=True))
)
...
model.fit(training_sentences_vectors, training_labels, validation_data=validation_data)
</code></pre>

<p>Are there any drawbacks using the word vectors directly without a keras embedding layer?</p>

<p>I'm also currently adding additional (one-hot encoded) tags to the input tokens by concatenating them to each word vector, does this approach make sense?</p>
","410771","","","","","2018-12-20 15:29:05","Embedding vs inserting word vectors directly to input layer","<keras><deep-learning><nlp><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"64839893","1","","","2020-11-14 23:56:48","","0","39","<p>I want to print out topics and their keywords summarized as a table form.<br>
For example (for simplicity just 2 topics and 5 keywords for each are shown),<br></p>
<pre><code>&lt;Desired Output&gt;

topic0  topic1     topic2   ... 
film    school     ...
show    students   ...
music   education  ...
movie   high       ...
play    teach      ...
...     ...        ...
</code></pre>
<p>Right now what I can get is each topic with their keywords and the corresponding keyword probability after using the below code.</p>
<pre><code>&lt;Code&gt;
#Instead of using regular LDA, I use LDA Multicore for the faster process.
lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, chunksize=2000, alpha='symmetric', passes=2, workers=4)

for idx, topic in lda_model_tfidf.print_topics(-1, num_words=20):
    print('Topic: {} Word: {}'.format(idx, topic))

&lt;output&gt;
topic: 0 words: 0.08*film + 0.05*show + 0.04*music + 0.035*movie + 0.02 * play + ...
topic: 1 words: 0.07*school + 0.06*students + 0.055*education + 0.03*high + 0.01*teach + ...
topic: 2 words: ...
...
</code></pre>
","7882846","","7882846","","2020-11-17 19:13:45","2020-11-17 19:13:45","how to print out topics and their keywords as a table using LDA Multicore in Python","<python><gensim><lda><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"51602111","1","51603313","","2018-07-30 20:52:57","","2","1654","<p>I have two pretrained word embeddings: <code>Glove.840b.300.txt</code> and <code>custom_glove.300.txt</code></p>

<p>One is pretrained by Stanford and the other is trained by me.
Both have different sets of vocabulary. To reduce oov, I'd like to add words that don't appear in file1 but do appear in file2 to file1.
How do I do that easily?</p>

<p>This is how I load and save the files in gensim 3.4.0.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

model = KeyedVectors.load_word2vec_format('path/to/thefile')
model.save_word2vec_format('path/to/GoogleNews-vectors-negative300.txt', binary=False)
</code></pre>
","3907250","","3907250","","2018-07-30 21:20:32","2018-07-30 22:53:27","Adding additional words in word2vec or Glove (maybe using gensim)","<nlp><gensim><word2vec><glove>","1","0","","","","CC BY-SA 4.0"
"54362442","1","","","2019-01-25 09:34:43","","0","274","<p>My goal is to read the lines inside a file and to replace all special characters like french characters (√†, √©, √ß, ...) by normal characters (a, e, c, ...)</p>

<p>I work with Python 3 and in the documentation of gensim, the example works with a simple sentence like : deaccent(""√†√©√ß) but not with lines I read from a file
At this time, I just get ""√†√©√ß"" and not ""aec"" with my code</p>

<pre><code>from gensim.utils import deaccent

def getTextFromFile(filename):
    with open(filename) as file:
        text = [line.rstrip() for line in file.readlines()]
    file.close()
    for line in text:
        print(deaccent(line))
    return text
</code></pre>

<p>My file contains : √†√©√ß</p>

<p>I would like to get : aec</p>
","6830324","","","","","2019-01-25 19:33:53","How to use properly deaccent method from gensim?","<python><string><gensim>","1","2","","","","CC BY-SA 4.0"
"59093325","1","","","2019-11-28 16:46:19","","1","34","<p>I am trying to annotate the vocabulary in the corpus.</p>

<ol>
<li><p>I have trained the word2vec model on the corpus</p></li>
<li><p>I have grouped the words which are related based on the score as key as the first word as the key and remaining words as a list of 2-tuple of word and scores with respect to the key </p></li>
</ol>

<p>example:
'coffee'---key 
values are </p>

<pre><code>[('tea', 0.8139282),
 ('latte', 0.76456803),
 ('coffe', 0.7607962),
 ('lattes', 0.756057),
 ('starbucks', 0.7158153),
 ('espresso', 0.71386236),
 ('mocha', 0.69999266),
 ('coffees', 0.6816252),
 ('frappucino', 0.67192864),
 ('cuppa', 0.66720986),
 ('cappucino', 0.6664002),
 ('chai', 0.6623157),
 ('decaf', 0.65980726),
 ('frappuccino', 0.65150374),
 ('venti', 0.6486204),
 ('expresso', 0.6369579),
 ('macchiato', 0.6280453),
 ('scone', 0.62476856),
 ('sippy', 0.6236704),
 ('cappuccino', 0.61718297),
 ('iced', 0.6130485),
 ('hazelnut', 0.6023698),
 ('mug', 0.6004759),
'
'
'
'
'
</code></pre>

<p>as i  know the coffee is releated to latte ,green_tea ,espresso,starbucks.. from the above data
I would like to label each word as below </p>

<p>latte [COHYPO] green_tea [COHYPO] espresso [HYPO] Starbucks [RELATED] tim_horton [RELATED] </p>

<p>COHYPO-<a href=""https://en.wiktionary.org/wiki/cohyponym"" rel=""nofollow noreferrer"">https://en.wiktionary.org/wiki/cohyponym</a></p>

<p>[HYPO] -<a href=""https://en.wiktionary.org/wiki/hyponyme"" rel=""nofollow noreferrer"">https://en.wiktionary.org/wiki/hyponyme</a></p>

<p>[RELATED] -the word is repeated </p>

<p>[MORPHO]-Morphological variant (example :Computer and computers )</p>

<p>[Partof]- indicates that the annotated word is a part of the word of interest</p>

<p>Any suggestion or ideas by which I can approach this problem </p>
","5955533","","5955533","","2019-11-28 23:29:28","2019-11-28 23:29:28","Annotating the vocabulary using Word2vec model","<nlp><gensim><word2vec>","0","3","1","","","CC BY-SA 4.0"
"38665556","1","44579542","","2016-07-29 18:40:54","","21","16211","<p>I have had the <a href=""https://radimrehurek.com/gensim/"" rel=""noreferrer"">gensim</a> <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""noreferrer"">Word2Vec</a> implementation compute some word embeddings for me. Everything went quite fantastically as far as I can tell; now I am clustering the word vectors created, hoping to get some semantic groupings.</p>
<p>As a next step, I would like to look at the words (rather than the vectors) contained in each cluster. I.e. if I have the vector of embeddings <code>[x, y, z]</code>, I would like to find out which actual word this vector represents. I can get the words/Vocab items by calling <code>model.vocab</code> and the word vectors through <code>model.syn0</code>. But I could not find a location where these are explicitly matched.</p>
<p>This was more complicated than I expected and I feel I might be missing the obvious way of doing it. Any help is appreciated!</p>
<h3>Problem:</h3>
<p>Match words to embedding vectors created by <code>Word2Vec ()</code> -- how do I do it?</p>
<h3>My approach:</h3>
<p>After creating the model (code below*), I would now like to match the indexes assigned to each word (during the <code>build_vocab()</code> phase) to the vector matrix outputted as <code>model.syn0</code>.
Thus</p>
<pre><code>for i in range (0, newmod.syn0.shape[0]): #iterate over all words in model
    print i
    word= [k for k in newmod.vocab if newmod.vocab[k].__dict__['index']==i] #get the word out of the internal dicationary by its index
    wordvector= newmod.syn0[i] #get the vector with the corresponding index
    print wordvector == newmod[word] #testing: compare result of looking up the word in the model -- this prints True
</code></pre>
<ul>
<li><p>Is there a better way of doing this, e.g. by feeding the vector into the model to match the word?</p>
</li>
<li><p>Does this even get me correct results?</p>
</li>
</ul>
<p>*My code to create the word vectors:</p>
<pre><code>model = Word2Vec(size=1000, min_count=5, workers=4, sg=1)
        
model.build_vocab(sentencefeeder(folderlist)) #sentencefeeder puts out sentences as lists of strings

model.save(&quot;newmodel&quot;)
</code></pre>
<p>I found <a href=""https://stackoverflow.com/questions/35914287/word2vec-how-to-get-words-from-vectors"">this question</a> which is similar but has not really been answered.</p>
","667913","","-1","","2020-06-20 09:12:55","2017-06-16 02:41:24","Matching words and vectors in gensim Word2Vec model","<python><vector><machine-learning><gensim><word2vec>","4","0","5","","","CC BY-SA 3.0"
"64421012","1","","","2020-10-19 04:03:29","","0","250","<p>I'm working with Gensim to extract keywords from HTML or text.
My environment is <strong>Centos 7 / Python 3.6 / Pip3</strong>.
After checking Gensim source code, I found that Gensim Keywords API in Summarization module depends on pattern.</p>
<p><strong>Keywords.py:</strong></p>
<p><code>def keywords(text, ratio=0.2, words=None, split=False, scores=False, pos_filter=('NN', 'JJ'),lemmatize=False, deacc=True):</code></p>
<p>The pos_filter and lemmatize feature depend on pattern.
If no pattern module avaiable, pos_filter is set to None, no filtering.</p>
<p><strong>textcleaner.py:</strong></p>
<p>HAS_PATTERN is set by pattern.en module.
In my system, only pattern3 is available, <strong>pattern is only available for python2.7.</strong></p>
<p>How to fix this problem?</p>
","10694798","","11742141","","2020-10-19 08:19:18","2020-10-19 23:39:29","How to fix the problem that Gensim not working with pattern3?","<python><design-patterns><gensim>","1","1","","","","CC BY-SA 4.0"
"45037860","1","","","2017-07-11 14:46:45","","1","3090","<p>For gensim(1.0.1) doc2vec, I am trying to load google pre-trained word vectors instead of using <code>Doc2Vec.build_vocab</code> </p>

<pre><code>wordVec_google = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)    
model0 = Doc2Vec(size=300, alpha=0.05, min_alpha=0.05, window=8, min_count=5, workers=4, dm=0, hs=1)    
model0.wv = wordVec_google    
##some other code 
model0.build_vocab(sentences=allEmails, max_vocab_size = 20000)
</code></pre>

<p>but this object <code>model0</code> can not be further trained with ""labeled Docs"", and can't infer vectors for documents. </p>

<p>Anyone knows how to use doc2vec with google pretrained word vectors?<br>
I tried this post:  <a href=""http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/"" rel=""nofollow noreferrer"">http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/</a>
but it does not work to load into <code>gensim.models.Word2Vec</code> object, perhaps it is a different gensim version.</p>
","3527917","","2743206","","2018-10-23 23:51:20","2018-10-23 23:51:20","gensim(1.0.1) Doc2Vec with google pretrained vectors","<gensim>","1","1","","","","CC BY-SA 4.0"
"45069715","1","45072692","","2017-07-13 00:47:01","","0","525","<p>I loaded a word2vec model using Google News dataset. Now I want to get the Word2Vec representations of a list of sentences that I wish to cluster. After going through the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">documentation</a> I found this <code>gensim.models.word2vec.LineSentence</code>but I'm not sure this is what I am looking for. </p>

<p>There should be a way to get word2vec representations of a list of sentences from a pretrained model right? None of the links I searched had anything about it. Any leads would be appreciated. </p>
","2251545","","","","","2017-07-13 06:12:44","After loading a pretrained Word2Vec model, how do I get word2vec representations of new sentences?","<cluster-analysis><gensim><word2vec>","1","1","","","","CC BY-SA 3.0"
"36900356","1","40033291","","2016-04-27 20:35:46","","2","745","<p>I thought this may have been discussed before, but somehow I couldn't find answers, so here it is.</p>

<p>Below are the topics generated using gensim lsi from some customer survey. My questions are:</p>

<ol>
<li>what does the minus and plus signs in front of the words mean?</li>
<li>here I generated 5 topics and I could have generated more. how do I determine what might be the optimal number of topics? for example, maybe statistically after the third topic everything else will just be trivial.</li>
</ol>

<p>Any suggestions are appreciated.</p>

<p>0.527*""interest"" + 0.475*""lower"" + 0.376*""rates"" + 0.338*""rate"" + 0.324*""good"" + 0.257*""service""
0.671*""good"" + 0.586*""service"" + -0.254*""interest"" + -0.251*""lower"" + -0.159*""rate"" + -0.150*""rates""
0.600*""great"" + 0.351*""easy"" + 0.337*""rewards"" + 0.242*""use"" + -0.167*""service"" + 0.160*""like""
-0.503*""rates"" + 0.499*""rate"" + -0.39*""great"" + 0.364*""high"" + -0.289*""lower"" + 0.167*""easy""
-0.608*""great"" + 0.362*""easy"" + -0.303*""rate"" + 0.275*""rates"" + 0.244*""use"" + -0.227*""high""</p>
","5555401","","","","","2016-10-14 00:55:22","How to interpret gensim topics properly?","<gensim>","1","0","2","","","CC BY-SA 3.0"
"45499558","1","","","2017-08-04 06:22:59","","7","6390","<p>I re-install the gensim pkg and Cython but it continusly show this warning,
Does anybody know about this?
I am using Python 3.6,PyCharm Linux Mint.</p>

<p>UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.
  warnings.warn(""C extension not loaded for Word2Vec, training will be slow. ""</p>

<p>And it also show this line when I create or load model.<br>
Slow version of gensim.models.doc2vec is being used</p>
","","user8349292","","","","2017-08-04 11:28:20","C extension not loaded for Word2Vec","<python><python-3.x><gensim><word2vec>","1","0","1","","","CC BY-SA 3.0"
"51634656","1","","","2018-08-01 13:31:40","","1","6012","<p>I am trying to filter out tokens by their frequency using the filter_extremes function in Gensim (<a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/corpora/dictionary.html</a>). Specifically, I am interested in filtering out words that occur in ""Less frequent than no_below documents"" and ""More frequent than no_above documents"". </p>

<pre><code>id2word_ = corpora.Dictionary(texts)
print(len(id2word_))
id2word_.filter_extremes(no_above = 0.600)
print(len(id2word_))
</code></pre>

<p>The first print statement gives 11918 and the second print statement gives 3567. However, if I do the following:</p>

<pre><code>id2word_ = corpora.Dictionary(texts)
print(len(id2word_))
id2word_.filter_extremes(no_below = 0.599)
print(len(id2word_))
</code></pre>

<p>The first print statement gives 11918 (as expected) and the second gives 11406. Shouldn't <code>id2word_.filter_extremes(no_below = 0.599)</code> and <code>id2word_.filter_extremes(no_above = 0.600)</code> add up to the number of total words? However, 11406 +  3567 > 11918, so how come this sum exceeds the number of words in the corpus? That does not make sense since the filters should cover non-overlapping words, based off the explanation in the documentation. </p>

<p>If you have any ideas, I would really appreciate your input! Thanks!</p>
","7375754","","7375754","","2018-08-01 15:58:23","2020-07-05 09:59:51","Filtering tokens by frequency using filter_extremes in Gensim","<python><dictionary><text-processing><gensim><corpus>","4","0","","","","CC BY-SA 4.0"
"51650896","1","","","2018-08-02 10:07:38","","0","203","<p>I am trying to compute the accuracy of a Word2Vec model. This is my code:</p>

<pre><code>import gensim

vectors = gensim.models.KeyedVectors.load(""cbow_vectors.kv"", mmap='r')

questions = ""questions-words.txt""

analogy_scores = vectors.accuracy(questions, restrict_vocab=30000, case_insensitive=True)
</code></pre>

<p>I get the following error:</p>

<blockquote>
  <p>C:\Users\\AppData\Local\Programs\Python\Python36\lib\site-packages\gensim\matutils.py:737:
  FutureWarning: Conversion of the second argument of issubdtype from
  <code>int</code> to <code>np.signedinteger</code> is deprecated. In future, it will be
  treated as <code>np.int32 == np.dtype(int).type</code>.   if
  np.issubdtype(vec.dtype, np.int):</p>
</blockquote>

<p>and then nothing happens :(
Anyone know how to fix this? </p>
","10112248","","4685471","","2018-08-02 11:31:21","2018-08-02 11:31:21","FutureWarning error when calculating accuracy of a Word2Vec model","<python><numpy><nlp><gensim><word2vec>","0","3","","","","CC BY-SA 4.0"
"45813264","1","","","2017-08-22 09:02:54","","4","606","<p>I am training word vectors on particular text corpus using fast text.
Fasttext provides all the necessary mechanics and options for training word vectors and when looked with tsne, the vectors are amazing. I notice gensim has a wrapper for fasttext which is good for accessing vectors. </p>

<p>for my task, I have many text corpuses. I need to use the above trained vectors again with new corpus and use the trained vectors again on new discovered corpuses. fasttext doesnot provide this function. I donot see any package that achieves this or may be I am lost. I see in google <a href=""https://groups.google.com/forum/#!topic/gensim/Y_WmJST9xx8"" rel=""nofollow noreferrer"">forum</a> gensim provides intersect_word2vec_format, but cannot understand or find usage tutorial for this. There is another <a href=""https://stackoverflow.com/questions/41096821/word2vec-model-intersect-word2vec-format"">question</a> open similar to this with no answer.</p>

<p>So apart from gensim, is there any other way to train the models like above.</p>
","471384","","","","","2017-08-22 09:02:54","train word2vec with pretrained vectors","<nlp><gensim><word2vec><fasttext>","0","5","1","","","CC BY-SA 3.0"
"38343475","1","","","2016-07-13 05:42:34","","0","1166","<pre><code>from deepdist import DeepDist

from gensim.models.word2vec import Word2Vec

from pyspark import SparkConf, SparkContext

conf = (SparkConf()
     .setAppName(""Work2Vec"")
)

sc = SparkContext(conf=conf)
corpus = sc.textFile('AllText.txt').map(lambda s: s.split())

def gradient(model, sentences):

    syn0, syn1 = model.syn0.copy(), model.syn1.copy()   # previous weights
    model.train(sentences)
    return {'syn0': model.syn0 - syn01, 'syn1': model.syn1 - syn1}


def descent(model, update):

    model.syn0 += update['syn0']

    model.syn1 += update['syn1']


with DeepDist(Word2Vec(corpus.collect())) as dd:

    dd.train(corpus, gradient, descent)

    dd.model.save(""Model"")
</code></pre>

<p>Please help me, I have a 56Gb text and want to build a word2Vec model but using only gensim is very slow, so i try deepdist and their example code on the web, so I just wondering have anyone seen this kind of error </p>

<p>The output when i run this script:</p>

<p><a href=""https://i.stack.imgur.com/L4SO8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L4SO8.png"" alt=""script output""></a></p>
","6582582","","3223422","","2016-07-13 06:23:01","2016-08-02 23:07:09","Trying Deepdict, run gensim word2vec with pyspark","<python><pyspark><gensim><word2vec>","1","1","","","","CC BY-SA 3.0"
"45091817","1","","","2017-07-13 22:20:01","","1","531","<p>There are 2 arrays containing 30,000 vectors and 50000 vectors respectively.</p>

<pre><code>Item_array = [item1,item2,...,item30000]
User_array = [user1,user2,...,user50000]
</code></pre>

<p>Each vector in array is the tfidf value using Gensim.</p>

<p>For example:  </p>

<pre><code>Item_array[0] = [(0, 0.03663947221807151),(2, 0.09781584692664856),(10, 0.07212302141012294)]
</code></pre>

<p>I'm trying to build a item-user matrix for sorting each user's similar items.
For-loop method cost me lots of time to finish it.</p>

<p>How should i handle this efficiently. Any help should be appreciable..</p>

<p>It's my expected output:</p>

<pre><code>           user1 user2 user3 ... user50000
item1      0.35  0.45  0.86        0.46
item2      0.42  0.32  0.53        0.53          
item3      0.65  0.33  0.45        0.46        
...        ...   ...   ...         ... 
item50000  0.54  0.33  0.00        1.00
</code></pre>
","8000414","","","","","2017-07-13 22:24:34","Python: How to get cosine similarity matrix efficiently","<python><numpy><gensim><cosine-similarity>","0","5","","","","CC BY-SA 3.0"
"53839662","1","","","2018-12-18 19:15:56","","0","354","<p>I am implementing word2vec in gensim, on a corpus with nested lists (collection of tokenized words in sentences of sentences form) with 408226 sentences (lists) and a total of 3150546 words or tokens. </p>

<p>I am getting a meaningful results (in terms of the similarity between two words using model.wv.similarity) with the  chosen values of 200 as size, window as 15, min_count as 5, iter as 10 and alpha as 0.5. All are lemmatized words and these all are input to models with vocabulary as 32716.</p>

<p>The results incurred from default alpha value, size, window and dimensions are meaningless for me based on the used data in computing the similarity values. However higher value of alpha as 0.5 gives me some meaningful results in terms of inducing meaningful similarity scores between two words. However, when I calculate the top n similar words, it's again meaningless. Does I need to change the entire parameters used in the initial training process.</p>

<p>I am still unable to reveal the exact reason, why the model behaves good with such a higher alpha value in computing the similarity between two words of the used corpus, whereas it's meaningless while computing the top n similar words with scores for an input word. Why is this the case?</p>

<p>Does it is diverging towards optimal solution. How to check this?</p>

<p>Any idea why is it the case is deeply appreciated.</p>

<p>Note: I'm using Python 3.7 on Windows machine with anaconda prompt and giving input to the model from a file.</p>

<p>This is what I have tried.</p>

<pre><code>import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
from gensim.models import Word2Vec
import ast

path = ""F:/Folder/""
def load_data():
    global Sentences
    Sentences = []
    for file in ['data_d1.txt','data_d2.txt']:
        with open(path + file, 'r', encoding = 'utf-8') as f1:
           Sentences.extend(ast.literal_eval(*f1.readlines()))
load_data()

def initialize_word_embedding():
    model = Word2Vec(Sentences, size = 200, window = 15, min_count = 5, iter = 10, workers = 4)
    print(model)
    print(len(model.wv.vocab))
    print(model.wv.similarity(w1 = 'structure', w2 = '_structure_'))
    similarities = model.wv.most_similar('system')
    for word, score in similarities:
        print(word , score)

initialize_word_embedding()    
</code></pre>

<p>The example of Sentences list is as follows:</p>

<blockquote>
  <p>[['<em>scientist</em>', '<em>time</em>', '<em>comet</em>', '<em>activity</em>', '<em>sublimation</em>', '<em>carbon</em>', '<em>dioxide</em>', '<em>nears</em>', '<em>ice</em>', '<em>system</em>'], ['<em>inconsistent</em>', '<em>age</em>', '<em>system</em>', '<em>year</em>', '<em>size</em>', '<em>collision</em>'], ['intelligence', 'system'], ['example', 'application', 'filter', 'image', 'motion', 'channel', 'estimation', 'equalization', 'example', 'application', 'filter', 'system']] </p>
</blockquote>

<p>The <code>data_d1.txt</code> and <code>data_d2.txt</code> is a nested list (list of lists of lemmatized tokenized words). I have preprocessed the raw data and save it in a file. Now giving the same as input. For computing the lemmatizing tokens, I have used the popular WordNet lemmatizer. </p>

<p>I need the word-embedding model to calculate the similarity between two words and computing the most_similar words of a given input word. I am getting some meaningful scores for the <code>model.wv.similarity()</code> method, whereas in calculating the <code>most_similar()</code> words of a word (say, <code>system</code> as shown in above). I am not getting the desired results. </p>

<p>I am guessing the model is getting diverged from the global minima, with the use of high alpha values. </p>

<p>I am confused what should be the dimension size, window for inducing some meaningful results, as there is no such rules regarding how to compute the the size and window. </p>

<p>Any suggestion is appreciated. The size of total sentences and words are specified above in the question. </p>

<p>Results what I am getting without setting alpha = 0.5</p>

<blockquote>
  <p>Edit to Recent Comment:</p>
</blockquote>

<p>Results:</p>

<blockquote>
  <p>Word2Vec(vocab=32716, size=200, alpha=0.025)</p>
</blockquote>

<p>The similarity between <code>set</code> and <code>_set_</code> is : <code>0.000269373188960656</code>
which is meaningless for me as it is very very less in terms of accuracy, But, I am a getting 71% by setting alpha as 0.5, which seems to be meaningful for me as the word <code>set</code> is same for both the domains. </p>

<p>Explanation: The word <code>set</code> should be same for both the domains (as I am comparing the data of two domains with same word). Don't get confused with word <code>_set_</code>, this is because the word is same as set, I have injected a character <code>_</code> at start and end to distinguish the same for two different domains. </p>

<p>The top 10 words along with scores of <code>_set_</code> are:</p>

<pre><code>_niche_ 0.6891741752624512
_intermediate_ 0.6883598566055298
_interpretation_ 0.6813371181488037
_printer_ 0.675414502620697
_finer_ 0.6625382900238037
_pertinent_ 0.6620787382125854
_respective_ 0.6619025468826294
_converse_ 0.6610435247421265
_developed_ 0.659270167350769
_tent_ 0.6588765382766724
</code></pre>

<p>Whereas, the top 10 words for set are:</p>

<pre><code>cardinality 0.633270263671875
typereduction 0.6233855485916138
zdzis≈Çaw 0.619156002998352
crisp 0.6165326833724976
equivalenceclass 0.605925977230072
pawlak 0.6058803200721741
straight 0.6045454740524292
culik 0.6040038466453552
rin 0.6038737297058105
multisets 0.6035065650939941
</code></pre>

<p>Why the <code>cosine similarity value</code> is 0.00 for the word <code>set</code> for two different data.</p>
","3966705","","3966705","","2018-12-19 23:37:32","2018-12-19 23:37:32","Some diverging issues of Word2Vec in Gensim using high alpha values","<python-3.x><gensim><word2vec><word-embedding>","0","3","1","","","CC BY-SA 4.0"
"56076298","1","56084792","","2019-05-10 10:59:45","","0","228","<p>I have trained Doc2Vec paragraph embeddings on text documents using the <code>Doc2Vec</code> module in Python's <code>gensim</code> package. Normally each document is tagged with a unique ID, yielding a unique output representation, as follows (see <a href=""https://fzr72725.github.io/2018/01/14/genism-guide.html"" rel=""nofollow noreferrer"">this link</a> for details):</p>

<pre><code>def tag_docs(docs, col):
    tagged = docs.apply(lambda r: TaggedDocument(words=simple_preprocess(r[col]), tags=[r.label]), axis=1)
    return tagged
</code></pre>

<p>However, you can also tag a group of documents with the same tag in order to train class representations, which is what I did here. You can query the number of output representations with the following command:</p>

<pre><code>print(model.docvecs.count)
</code></pre>

<p>My question is as follows: I trained the model of <code>n</code> classes of documents, yielding <code>n</code> document vectors in <code>model.docvecs</code>. Now I want to map each document vector to the corresponding class tag. How can I establish which vector is associated with which tag?</p>
","7554826","","7554826","","2019-05-13 09:16:55","2019-05-13 09:16:55","Mapping doc2vec paragraph representation to its class tag post-training","<python><gensim><word2vec><text-classification><doc2vec>","1","0","","","","CC BY-SA 4.0"
"51707594","1","","","2018-08-06 12:24:26","","0","386","<p>I try to integrate gensim's <code>get_keras_embedding</code> into a Keras model.</p>

<p>First I do text preprocessing</p>

<ol>
<li><code>text_to_word_sequence</code> converts text into token sequence</li>
<li><code>w2v.vocab[tok].index</code> converts words into gensim's word indexes</li>
<li><code>pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, value=999999)</code> padding with some value</li>
</ol>

<p>Then I build a model</p>

<pre><code>embedding_layer = w2v.get_keras_embedding(train_embeddings=False)
...
sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
...
x = Conv1D(100, kernel_size, activation='relu', padding='same', strides=1)(embedded_sequences)
</code></pre>

<p>Apparently the problem is with the padding value. The default value is 0 which is 'the' in gensim word2vec, if I use non-existing value it fails with the following error message. What is the right way to use padding with gensim word2vec?</p>

<pre><code>InvalidArgumentError (see above for traceback): indices[48,0] = 999999 is not in [0, 400000)
     [[Node: embedding_1/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@embedding_1/embeddings""], validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](embedding_1/embeddings/read, _arg_input_1_0_3)]]
</code></pre>

<p>I would appreciate any help.</p>
","811921","","811921","","2018-08-06 12:46:37","2018-08-06 12:46:37","How to use padding value with get_keras_embedding","<keras><gensim><word2vec><word-embedding>","0","3","","","","CC BY-SA 4.0"
"45569142","1","45575074","","2017-08-08 13:03:45","","0","793","<p>I want to compare word2vec and fasttext model based on this comparison tutorial. 
<a href=""https://github.com/jayantj/gensim/blob/fast_text_notebook/docs/notebooks/Word2Vec_FastText_Comparison.ipynb"" rel=""nofollow noreferrer"">https://github.com/jayantj/gensim/blob/fast_text_notebook/docs/notebooks/Word2Vec_FastText_Comparison.ipynb</a></p>

<p>According to this, the semantic accuracy of fastText model increase when we set the max length of char n-grams to zero, such that fastText starts to behave almost like to word2vec. It ignores the ngrams. </p>

<p>However, I can not find any formation on how to set this parameter while loading a fastText model. Any ideas on how to do this?</p>
","1996842","","","","","2017-08-08 17:59:02","Setting max length of char n-grams for fastText","<nlp><nltk><gensim><word2vec><fasttext>","1","0","","","","CC BY-SA 3.0"
"45573181","1","","","2017-08-08 16:08:35","","2","462","<p>I'm using Gensim's excellent library to compute similarity queries on a corpus using LSI. However, I have a distinct feeling that the results could be better, and I'm trying to figure out whether I can adjust the corpus itself in order to improve the results. </p>

<p>I have a certain amount of control over how to split the documents. My original data has a lot of very short documents (mean length is 12 words in a document, but there exist documents that are 1-2 words long...), and there are a few logical ways to concatenate several documents into one. The problem is that I don't know whether it's worth doing this or not (and if so, to what extent). I can't find any material addressing this question, but only regarding the size of the corpus, and the size of the vocabulary. I assume this is because, at the end of the day, the size of a document is bounded by the size of the vocabulary. But I'm sure there are still some general guidelines that could help with this decision.</p>

<p>What is considered a document that is too short? What is too long? (I assume the latter is a function of <code>|V|</code>, but the former could easily be a constant value.)</p>

<p>Does anyone have experience with this? Can anyone point me in the direction of any papers/blog posts/research that address this question? Much appreciated!</p>

<p><em>Edited to add:</em>
Regarding the strategy for grouping documents - each document is a text message sent between two parties. The potential grouping is based on this, where I can also take into consideration the time at which the messages were sent. Meaning, I could group all the messages sent between A and B within a certain hour, or on a certain day, or simply group all the messages between the two. I can also decide on a minimum or maximum number of messages grouped together, but that is exactly what my question is about - how do I know what the ideal length is?</p>
","2986584","","2986584","","2017-08-08 17:29:40","2017-08-09 10:25:32","Optimal Document Size for LSI Similarity Model","<gensim><lsa>","2","0","","","","CC BY-SA 3.0"
"51632344","1","","","2018-08-01 11:39:43","","1","76","<p>I have around 1000 pairs of sentences. Each pair consists of two sentences, one causing high CTR and one LOW. I want to create a mechanism to auto-produce sentences optimized for high CTR. When iterating the pairs, I can get a vector (using Spacy NLP) for each sentence. I take the vectors difference (Sent1.vector - Sent2.Vector) and then mean all the pairs using numpy mean. When I have the ""difference vector"" in hand, I want to add it to any given text and get a new sentence. any Ideas how to obtain this? Gensim most_similar only works on single words... Thanks</p>
","3726128","","","","","2018-08-01 11:39:43","Spacy/Gensim - creating similar sentences","<gensim><spacy>","0","0","","","","CC BY-SA 4.0"
"45917969","1","","","2017-08-28 11:31:30","","0","894","<p>I am using doc2vec to convert the top 100 tweets of my followers in vector representation (say v1.....v100). After that I am using the vector representation to do the K-Means clusters. </p>

<pre><code>model = Doc2Vec(documents=t, size=100, alpha=.035, window=10, workers=4, min_count=2)
</code></pre>

<p>I can see that cluster 0 is dominated by some values (say v10, v12, v23, ....). My question is what does these v10, v12 ... etc represents. Can I deduce that these specific column clusters specific keywords of document.  </p>
","2591096","","","","","2019-04-19 21:35:02","How to intrepret Clusters results after using Doc2vec?","<python><scikit-learn><cluster-analysis><gensim><doc2vec>","3","0","","","","CC BY-SA 3.0"
"56076714","1","","","2019-05-10 11:24:57","","0","756","<p>I have a model trained with Word2Vec. It works well.
I would like to plot <strong>only a list of words</strong> which I have entered in a list.
I have written the function below (and reused some code found) and get the following error message when a vector is added to <strong>arr</strong>: 
<em>'ValueError: all the input arrays must have same number of dimensions'</em></p>

<pre><code>def display_wordlist(model, wordlist):
    vector_dim = model.vector_size
    arr = np.empty((0,vector_dim), dtype='f') #dimension trained by the model
    word_labels = [word]

    # get words from word list and append vector to 'arr'
    for wrd in wordlist:
        word_array = model[wrd]
        arr = np.append(arr,np.array(word_array), axis=0) #This goes wrong

    # Use tsne to reduce to 2 dimensions
    tsne = TSNE(perplexity=65,n_components=2, random_state=0)
    np.set_printoptions(suppress=True)
    Y = tsne.fit_transform(arr)

    x_coords = Y[:, 0]
    y_coords = Y[:, 1]
    # display plot
    plt.figure(figsize=(16, 8)) 
    plt.plot(x_coords, y_coords, 'ro')

    for label, x, y in zip(word_labels, x_coords, y_coords):
        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points')
    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)
    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)
    plt.show()

</code></pre>
","9697213","","","","","2019-05-13 15:39:16","Gensim: Plot list of words from a Word2Vec model","<python><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"42096798","1","","","2017-02-07 18:01:16","","0","470","<p>I have three NumPy arrays saved to disk in <code>.npy</code> format, together totaling about 40 GB (representing text count data from a very large document set). The three arrays represent the <code>data</code>, <code>indices</code>, and <code>indptr</code> attributes of a <a href=""https://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.sparse.csc_matrix.html"" rel=""nofollow noreferrer""><code>scipy.sparse.csc_matrix</code></a> sparse matrix.</p>

<p>I want to use distributed gensim LsiModel on this data, specifically with the <a href=""https://radimrehurek.com/gensim/matutils.html#gensim.matutils.Scipy2Corpus"" rel=""nofollow noreferrer""><code>gensim.matutils.Scipy2Corpus</code></a> function to provide the corpus iterator from the underlying sparse matrix.</p>

<p>However, I do not want to materialize the whole matrix in memory. Instead, how can I tell gensim about the underlying disk data and have gensim stream from disk into the csc matrix as needed to distribute chunks to the worker processes? If I understand correctly, this is what <code>Scipy2Corpus</code> and the <a href=""https://radimrehurek.com/gensim/dist_lsi.html"" rel=""nofollow noreferrer"">distributed LsiModel examples</a> claim to do, but instead they require materializing the arrays into a csc matrix ahead of time.</p>

<p>I have tried loading each of the underlying arrays with <code>mmap_mode='r'</code>, but the function that constructs the materialized csc matrix, <code>scipy.sparse.csc_matrix</code> will pull all of the data in regardless.</p>
","567620","","","","","2017-02-08 00:01:37","Using gensim Scipy2Corpus without materializing sparse matrix in memory","<python><numpy><scipy><sparse-matrix><gensim>","1","8","","","","CC BY-SA 3.0"
"63275259","1","","","2020-08-06 00:33:16","","0","247","<p>Given a query and a document, I would like to compute a similarity score using Gensim doc2vec.
Each document consists of multiple fields (e.g., main title, author, publisher, etc)</p>
<p>For training, is it better to concatenate the document fields and treat each row as a unique document or should I split the fields and use them as different training examples?</p>
<p>For inference, should I treat a query like a document? Meaning, should I call the model (trained over the documents) on the query?</p>
","6855259","","4685471","","2020-08-06 10:22:36","2020-08-06 10:22:36","Query-document similarity with doc2vec","<machine-learning><gensim><word2vec><information-retrieval><doc2vec>","1","2","","","","CC BY-SA 4.0"
"30480027","1","","","2015-05-27 10:36:34","","3","1807","<p>I am trying to use the freebase word embeddings released by Google, but I have a hard time getting the words from the freebase name. </p>

<pre><code>model = gensim.models.Word2Vec.load_word2vec_format('freebase-vectors-skipgram1000.bin',binary=True)
model.vocab.keys()[:10]

Out[22]:
[u'/m/026tg5z',
 u'/m/018jz8',
 u'/m/04klsk',
 u'/m/08gd39',
 u'/m/0kt94',
 u'/m/05mtf0t',
 u'/m/05tjjb',
 u'/m/01m3vn',
 u'/m/0h7p35',
 u'/m/03ggvg3']
</code></pre>

<p>Does anyone know if it exist some kind of table to map the freebase representations into the words they represent ?</p>

<p>Regards,</p>

<p>Hedi</p>
","4944073","","","","","2016-08-10 21:27:07","Using freebase vectors with gensim","<python><freebase><gensim><word2vec>","2","0","1","","","CC BY-SA 3.0"
"67545324","1","","","2021-05-15 09:40:31","","0","251","<p>I'm trying to use the Gensim for soft cosine similarity. I do install gensim using <code>pip install gensim</code>. Also, I have upgraded it.</p>
<p>But, still the error is not resolved.</p>
","15933353","","9136348","","2021-08-02 15:12:57","2021-08-02 15:12:57","ImportError: cannot import name 'softcossim' from 'gensim.matutils' (d:\python\lib\site-packages\gensim\matutils.py)","<python><gensim><cosine-similarity>","1","1","","","","CC BY-SA 4.0"
"42109463","1","","","2017-02-08 09:39:19","","1","399","<p>I am a bit confused regarding an aspect of Doc2Vec. Basically, I am not sure if what I do makes sense. I have the following dataset :</p>

<pre><code>train_doc_0      --&gt; label_0
    ...               ...
train_doc_99     --&gt; label_0
train_doc_100    --&gt; label_1
    ...               ...
train_doc_199    --&gt; label_1
    ...               ...
    ...               ...
train_doc_239999 --&gt; label_2399

eval_doc_0
    ...
eval_doc_29
</code></pre>

<p>Where <code>train_doc_n</code> is a short document, belonging to some label. There are 2400 labels, with 100 training documents per label. <code>eval_doc_0</code> are evaluation documents where I would like to predict their label in the end (using a classifier).</p>

<p>I train a Doc2Vec model with these training documents &amp; labels. Once the model is trained, I reproject each of the original training document as well as my evaluation documents (the ones I would like to classify in the end) into the model's space using <code>infer_vector</code>. </p>

<p>The resulting is a matrix :</p>

<pre><code>X_train (240000,300) # doc2vec vectors for training documents
y_train (240000,)    # corresponding labels
y_eval  (30, 300)    # doc2vec vectors for evaluation documents
</code></pre>

<p>My problem is the following : If I run a simple cross validation on <code>X_train</code> and <code>y_train</code>, I have a decent accuracy. Once I try to classify my evaluation documents (even, using only 50 randomly sampled labels) I have a super bad accuracy, which makes me question my way of approaching this problem.</p>

<p>I followed this <a href=""https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1#.masebkx6n"" rel=""nofollow noreferrer"">tutorial</a> for the training of documents.</p>

<p>Does my approach make sense, especially with reprojecting all the training documents using <code>infer_vector</code> ?</p>
","1874054","","1874054","","2017-02-08 12:59:02","2017-02-08 17:31:37","Doc2Vec: reprojecting training documents into model space","<python><classification><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 3.0"
"51762199","1","","","2018-08-09 08:27:39","","-6","746","<p>help me to correct this or are there any other way to accomplish this task?</p>

<p><img src=""https://i.stack.imgur.com/OVvoK.png"" alt=""Snapshot of code snippet""></p>
","10167429","","123632","","2018-08-09 09:06:56","2018-08-09 17:29:16","add more vocabulary to pretrained word2vec model","<machine-learning><nlp><data-mining><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"63385272","1","63386999","","2020-08-12 22:01:03","","0","289","<p>I am trying to load a pre-trained word2vec model in pkl format taken from <a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">here</a></p>
<p>The line of code I use to load it:</p>
<pre><code>model = gensim.models.KeyedVectors.load('enwiki_20180420_500d.pkl') 
</code></pre>
<p>However, i keep getting the following error (full traceback):</p>
<pre><code>UnpicklingError                           Traceback (most recent call last)
&lt;ipython-input-15-ebd5780b6636&gt; in &lt;module&gt;
     55 
     56 #Load pretrained word2vec
---&gt; 57 model = gensim.models.KeyedVectors.load('enwiki_20180420_500d.pkl',mmap='r')
     58 

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load(cls, fname_or_handle, **kwargs)
   1551     @classmethod
   1552     def load(cls, fname_or_handle, **kwargs):
-&gt; 1553         model = super(WordEmbeddingsKeyedVectors, cls).load(fname_or_handle, **kwargs)
   1554         if isinstance(model, FastTextKeyedVectors):
   1555             if not hasattr(model, 'compatible_hash'):

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load(cls, fname_or_handle, **kwargs)
    226     @classmethod
    227     def load(cls, fname_or_handle, **kwargs):
--&gt; 228         return super(BaseKeyedVectors, cls).load(fname_or_handle, **kwargs)
    229 
    230     def similarity(self, entity1, entity2):

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in load(cls, fname, mmap)
    433         compress, subname = SaveLoad._adapt_by_suffix(fname)
    434 
--&gt; 435         obj = unpickle(fname)
    436         obj._load_specials(fname, mmap, compress, subname)
    437         logger.info(&quot;loaded %s&quot;, fname)

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in unpickle(fname)
   1396         # Because of loading from S3 load can't be used (missing readline in smart_open)
   1397         if sys.version_info &gt; (3, 0):
-&gt; 1398             return _pickle.load(f, encoding='latin1')
   1399         else:
   1400             return _pickle.loads(f.read())

UnpicklingError: invalid load key, ':'.
</code></pre>
<p>I tried loading it with load_word2vec_format, but no luck. Any ideas what might be wrong with it?</p>
","12970944","","","","","2020-08-13 02:02:57","How to fix unpickling key error when loading word2vec (gensim)?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"51742332","1","","","2018-08-08 08:45:59","","1","674","<p>I trained a word2vec model using gensim and I want to randomly select vectors from it, and find the corresponding word.
What is the best what to do so?</p>
","10117402","","10117402","","2018-08-08 09:29:37","2021-04-17 19:26:05","Randomly select vector in gensim word2vec","<python-3.x><nlp><gensim><word2vec><word-embedding>","2","0","","","","CC BY-SA 4.0"
"54568930","1","","","2019-02-07 08:15:14","","2","1361","<p>Im writing a script that takes a website url and downloads it using beautiful soup. It then uses gensim.summarization to summarize the text but I keep getting ValueError(""input must have more than one sentence"") even thought the text has more than one sentence. The first section of the script works that downloads the text but I cant get the second part to summarize the text.</p>

<pre><code>import bs4 as bs
import urllib.request
from gensim.summarization import summarize
from gensim.summarization.textcleaner import split_sentences

#===========================================

print(""(Insert URL)"")
url = input()
sauce = urllib.request.urlopen(url).read()
soup = bs.BeautifulSoup(sauce,'lxml')

#===========================================

print(soup.title.string)
with open (soup.title.string + '.txt', 'wb') as file:
    for paragraph in soup.find_all('p'):
        text = paragraph.text.replace('.', '.\n')
        text = split_sentences(text)
        text = summarize(str(text))
        text = text.encode('utf-8', 'ignore')

#===========================================

        file.write(text+'\n\n'.encode('utf-8'))
</code></pre>

<p>It should create a .txt file with the summarized text in it after the script is run in whatever folder the .py file is located</p>
","11016284","","","","","2019-02-07 09:12:34","How to fix 'ValueError(""input must have more than one sentence"")' Error","<python-3.x><beautifulsoup><gensim>","1","2","","","","CC BY-SA 4.0"
"51754634","1","","","2018-08-08 19:54:03","","0","135","<p>I am currently running LDA on my dataset. The dataset consists of approximately 60k Documents. The documents are approximately as long as a Wikipedia article. 
I ran LDA for 10,20,30...,70 Topics fine. For 80 and more Topics, all coefficients turn to nan. I tried every combination of numpy and gensim version on two different computers but I get the same result on both.
I have absolutely no idea how to solve this problem. Please let me know what info you need.</p>

<p>Since you asked for a code snippet here is the line for the lda:</p>

<p><code>lda = gensim.models.ldamulticore.LdaMulticore(corpus, 
    id2word=dictionary, num_topics=80, chunksize=1800, passes=20, 
    workers=1, eval_every=1, iterations=1000)</code></p>

<p>I changed the number of workers. Normally I used 3 or 4.</p>
","3934598","","3934598","","2018-08-11 14:50:05","2018-08-11 14:50:05","Gensim LDA All coefficients are NaN for more than 70 topics","<python><gensim><lda><topic-modeling>","0","2","","","","CC BY-SA 4.0"
"51785296","1","","","2018-08-10 11:14:48","","1","1008","<p>I am trying to load a saved gensim lda mallet:</p>

<pre><code> ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=n_topics,id2word=id2word)
 ldamallet.save('ldamallet')
</code></pre>

<p>When testing this for a new query (with the original corpus and dictionary), everything seems fine for the first load. </p>

<pre><code>ques_vec = [dictionary.doc2bow(words) for words in data_words_list]
for i, row in enumerate(lda[ques_vec]):
    row = sorted(row, key=lambda x: (x[1]), reverse=True)
</code></pre>

<p>On executing the same code afterward, it is this error that pops up:</p>

<blockquote>
  <p>java.io.FileNotFoundException: /tmp/9f371_corpus.mallet (No such file
  or directory)
          at java.io.FileInputStream.open0(Native Method)
          at java.io.FileInputStream.open(FileInputStream.java:195)
          at java.io.FileInputStream.(FileInputStream.java:138)
          at cc.mallet.types.InstanceList.load(InstanceList.java:787)
          at cc.mallet.classify.tui.Csv2Vectors.main(Csv2Vectors.java:131)
  Exception in thread ""main"" java.lang.IllegalArgumentException:
  Couldn't read InstanceList from file /tmp/9f371_corpus.mallet
          at cc.mallet.types.InstanceList.load(InstanceList.java:794)
          at cc.mallet.classify.tui.Csv2Vectors.main(Csv2Vectors.java:131)
  Traceback (most recent call last):   File ""topic_modeling1.py"", line
  406, in 
      topic = get_label(text, id2word, first, ldamallet)   File ""topic_modeling1.py"", line 237, in get_label
      for i, row in enumerate(lda[ques_vec]):   File ""/home/user/sjha/anaconda3/envs/conda_env/lib/python3.6/site-packages/gensim/models/wrappers/ldamallet.py"", line 308, in <strong>getitem</strong>
      self.convert_input(bow, infer=True)   File ""/home/user/sjha/anaconda3/envs/conda_env/lib/python3.6/site-packages/gensim/models/wrappers/ldamallet.py"", line 256, in convert_input
      check_output(args=cmd, shell=True)   File ""/home/user/sjha/anaconda3/envs/conda_env/lib/python3.6/site-packages/gensim/utils.py"",
  line 1806, in check_output
      raise error subprocess.CalledProcessError: Command '/home/user/sjha/projects/topic_modeling/mallet-2.0.8/bin/mallet
  import-file --preserve-case --keep-sequence --remove-stopwords
  --token-regex ""\S+"" --input /tmp/9f371_corpus.txt --output /tmp/9f371_corpus.mallet.infer --use-pipe-from
  /tmp/9f371_corpus.mallet' returned non-zero exit status 1.</p>
</blockquote>

<p>Contents of my <code>/tmp/</code> directory:</p>

<pre><code>/tmp/9f371_corpus.txt  /tmp/9f371_doctopics.txt /tmp/9f371_doctopics.txt.infer  /tmp/9f371_inferencer.mallet  /tmp/9f371_state.mallet.gz  /tmp/9f371_topickeys.txt
</code></pre>

<p>Also, it seems like the files <code>/tmp/9f371_doctopics.txt.infer</code> and <code>/tmp/9f371_corpus.txt</code> get modified every time I load the model. What could be the possible error source? Or is it some kind of bug in gensim's mallet wrapper?</p>
","5140684","","","","","2019-07-09 20:43:17","Gensim mallet bug? Fails to load the saved model more than once","<python><gensim><lda><topic-modeling><mallet>","1","3","2","","","CC BY-SA 4.0"
"51680023","1","51710877","","2018-08-03 20:59:43","","0","347","<p>When I call <code>docvecs.most_similar</code> on a document, I am getting the error <code>AttributeError: 'list' object has no attribute 'shape'</code>:  </p>

<pre><code># load model from file
from gensim.models.doc2vec import Doc2Vec
model_doc2vec = Doc2Vec.load(""/path_to_file/doc2vec.bin"")

# attempt to get most similar documents from docvec
tokens = ""in space"".split()
new_vector = model_doc2vec.infer_vector(tokens)
sims = model_doc2vec.docvecs.most_similar( positive=[new_vector], topn=10 )
</code></pre>

<p>which yields <code>AttributeError: 'list' object has no attribute 'shape'</code>.</p>

<p>I have a hunch this may have to do with numpy and gensim version compatibility.  I am using Python 3.6, numpy 1.14, and gensim 1.0.1.</p>

<p>Full error:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-37-220db2331e84&gt; in &lt;module&gt;()
----&gt; 1 sims = model_doc2vec.docvecs.most_similar( positive=[new_vector], topn=10 )

~/doc2vec.py in most_similar(self, positive, negative, topn, clip_start, clip_end, indexer)
    436         there was chosen to be significant, such as more popular tag IDs in lower indexes.)
    437         """"""
--&gt; 438         self.init_sims()
    439         clip_end = clip_end or len(self.doctag_syn0norm)
    440 

~/doc2vec.py in init_sims(self, replace)
    419                         mode='w+', shape=self.doctag_syn0.shape)
    420                 else:
--&gt; 421                     self.doctag_syn0norm = empty(self.doctag_syn0.shape, dtype=REAL)
    422                 np_divide(self.doctag_syn0, sqrt((self.doctag_syn0 ** 2).sum(-1))[..., newaxis], self.doctag_syn0norm)
    423 

AttributeError: 'list' object has no attribute 'shape'
</code></pre>
","1673784","","1673784","","2018-08-03 21:25:34","2018-08-06 15:20:10","Gensim: calling docvecs.most_similar yields error","<python><numpy><gensim>","1","3","","","","CC BY-SA 4.0"
"42131107","1","","","2017-02-09 07:59:52","","0","246","<p>I trained a word2vec model on my dataset using the word2vec gensim package. My dataset has about 131,681 unique words but the model outputs a vector matrix of shape (47629,100). So only 47,629 words have vectors associated with them. What about the rest? Why am I not able to get a 100 dimensional vector for every unique word?</p>
","7452543","","4284627","","2017-02-10 16:09:23","2017-02-10 16:09:23","Word2vec model query","<neural-network><deep-learning><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"38968353","1","38985945","","2016-08-16 06:53:20","","0","1010","<p>i want to have phrases in doc2vec and i use gensim.phrases. in doc2vec we need tagged document to train the model and i cannot tag the phrases. how i can do this?</p>

<p>here is my code</p>

<pre><code>text = phrases.Phrases(text)
for i in range(len(text)):
    string1 = ""SENT_"" + str(i)

    sentence = doc2vec.LabeledSentence(tags=string1, words=text[i])
    text[i]=sentence

print ""Training model...""
model = Doc2Vec(text, workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling)
</code></pre>
","4357169","","","","","2016-08-16 23:24:42","How to use doc2vec with phrases?","<python><nlp><gensim><phrases><doc2vec>","1","0","","","","CC BY-SA 3.0"
"42186543","1","42194067","","2017-02-12 10:38:17","","3","998","<p>I am training a word2vec model from the tensorflow tutorial.</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a></p>

<p>After training I get the embedding matrix. I would like to save this and import it as a trained model in gensim.</p>

<p>To load a model in gensim, the command is:</p>

<pre><code>model = Word2Vec.load_word2vec_format(fn, binary=True)
</code></pre>

<p>But how do I generate the <code>fn</code> file from Tensorflow?</p>

<p>Thanks</p>
","5417769","","690430","","2017-02-14 20:17:48","2017-12-24 15:07:14","Training wordvec in Tensorflow, importing to Gensim","<python><machine-learning><tensorflow><gensim>","1","0","1","","","CC BY-SA 3.0"
"42039964","1","42065722","","2017-02-04 11:45:01","","0","950","<p>I'm running a Lubuntu 16.04 Machine with <code>gcc</code> installed. I'm not getting <code>gensim</code> to work with <code>cython</code> because when I train a <code>doc2vec model</code>, it is only ever trained with one worker which is dreadfully slow. </p>

<p>As I said <code>gcc</code> was installed from the start. I then maybe made the mistake and installed <code>gensim</code> before <code>cython</code>. I corrected that by forcing a reinstall of <code>gensim</code> via <code>pip</code>. With no effect still just one worker.</p>

<p>The machine is setup as a <code>spark</code> master and I interface with <code>spark</code> via <code>pyspark</code>.  It works something like this, <code>pyspark</code> uses <code>jupyter</code> and <code>jupyter</code> uses python 3.5. This way I get a <code>jupyter</code> interface to my cluster. Now I have no idea if this is the reason why i cant get <code>gensim</code> to work with <code>cython</code>. I don't execute any gensim code on the cluster, it is just more convenient to fire up <code>jupyter</code> to also do <code>gensim</code>. </p>
","3749379","","","","","2017-02-06 16:17:28","How to get cython and gensim to work with pyspark","<python><python-3.x><pyspark><cython><gensim>","2","6","","","","CC BY-SA 3.0"
"45810954","1","45822511","","2017-08-22 07:03:01","","2","92","<p>I have created two models using gensim word2vec. Now I want to merge these two models in a way that I get the union of these two models.</p>

<p>Eg: </p>

<ul>
1. Model one has following vocabulary
</ul>

<pre><code>{""Hi"", ""Hello"", ""World""}
</code></pre>

<ul>
2. Model two has the following vocabulary
</ul>

<pre><code>{""Hi"", ""King"", ""Hello"", ""Human""}
</code></pre>

<p>Now I want to use these two models and create a new model which will have the following vocabulary</p>

<pre><code>{""Hi"", ""Hello"", ""World"", ""King"", ""Human""}
</code></pre>
","5059870","","6037536","","2017-08-22 08:20:23","2017-10-12 12:46:28","How to create a model using trained models?","<machine-learning><nlp><deep-learning><gensim><word2vec>","3","0","","","","CC BY-SA 3.0"
"54673964","1","","","2019-02-13 15:37:30","","0","167","<p>I have a DataFrame that has a text column. I am splitting the DataFrame into two parts based on the value in another column. One of those parts is indexed into a gensim similarity model. The other part is then fed into the model to find the indexed text that is most similar. This involves a couple of search functions to enumerate over each item in the indexed part. With the toy data, it is fast, but with my real data, it is much too slow using <code>apply</code>. Here is the code example:</p>

<pre><code>import pandas as pd
import gensim
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

d = {'number': [1,2,3,4,5], 'text': ['do you like python', 'do you hate python','do you like apples','who is nelson mandela','i am not interested'], 'answer':['no','yes','no','no','yes']}
df = pd.DataFrame(data=d)

df_yes = df[df['answer']=='yes']

df_no = df[df['answer']=='no']
df_no = df_no.reset_index()

docs = df_no['text'].tolist()
genDocs = [[w.lower() for w in word_tokenize(text)] for text in docs]
dictionary = gensim.corpora.Dictionary(genDocs)
corpus = [dictionary.doc2bow(genDoc) for genDoc in genDocs]
tfidf = gensim.models.TfidfModel(corpus)
sims = gensim.similarities.MatrixSimilarity(tfidf[corpus], num_features=len(dictionary))

def search(row):
    query = [w.lower() for w in word_tokenize(row)]
    query_bag_of_words = dictionary.doc2bow(query)
    query_tfidf = tfidf[query_bag_of_words]
    return query_tfidf

def searchAll(row):
    max_similarity = max(sims[search(row)])
    index = [i for i, j in enumerate(sims[search(row)]) if j == max_similarity]
    return max_similarity, index

df_yes = df_yes.copy()

df_yes['max_similarity'], df_yes['index'] = zip(*df_yes['text'].apply(searchAll))
</code></pre>

<p>I have tried converting the operations to dask dataframes to no avail, as well as python multiprocessing. How would I make these functions more efficient? Is it possible to vectorize some/all of the functions? </p>
","7668467","","7668467","","2019-02-13 16:02:56","2019-02-14 06:29:37","Make Python Gensim Search Functions Efficient","<python><python-3.x><pandas><gensim>","1","3","","","","CC BY-SA 4.0"
"42119237","1","42131236","","2017-02-08 16:58:43","","3","1527","<p>I'm using gensim to extract feature vector from a document.
I've downloaded the pre-trained model from Google named <code>GoogleNews-vectors-negative300.bin</code> and I loaded that model using the following command:</p>

<pre><code>model = models.Doc2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>My purpose is to get a feature vector from a document. For a word, it's very easy to get the corresponding vector:</p>

<pre><code>vector = model[word]
</code></pre>

<p>However, I don't know how to do it for a document. Could you please help?</p>
","2241766","","","","","2019-12-17 09:43:27","load pre-trained word2vec model for doc2vec","<machine-learning><nlp><gensim><word2vec><doc2vec>","1","0","2","","","CC BY-SA 3.0"
"45926917","1","45928851","","2017-08-28 20:31:20","","1","102","<p>I'm working on a document comparison engine / search engine. I'm currently using it as follows...</p>

<pre><code>search_doc = [""test search""]
documents = [""doc 1 text"", ""doc 2 text"", ""doc 3 text"", ""...""]
</code></pre>

<p>And then comparing the results.</p>

<p>What I would like to do (in the simplest terms possible) is have multi-dimensional documents (a document that has multiple dimensions, rather than just the ""document"")... for example..</p>

<pre><code>documents = {
                { ""doc 1 title"", ""doc 1 body"", ""doc 1 tags"" },
                { ""doc 2 title"", ""doc 2 body"", ""doc 2 tags"" },
                { ""doc 3 title"", ""doc 3 body"", ""doc 3 tags"" }
                { ... }
            }
</code></pre>

<p>And also be able to weight the results (for example, title is 0.6, body is 0.4, etc).</p>

<p>My question is... is there a way to do this within Gensim, or do I need to create a separate document for each meta item of the document (for example, comparing to each meta item (title, body, tags) as a separate document, and then combining weights after the fact using the document key/id? </p>

<p>I'm not sure i'm doing a good job of explaining this, but please let me know if I can improve my question.</p>

<p>Thank you.</p>
","634621","","","","","2017-08-28 23:59:21","Multi-dimensional documents with Gensim","<python><machine-learning><tensorflow><gensim><tf-idf>","1","0","","","","CC BY-SA 3.0"
"51791964","1","51792176","","2018-08-10 18:11:40","","4","9151","<p>While implementating Word2Vec in Python 3.7, I am facing an unexpected scenario related to depreciation. My question is what exactly is the depreciation warning with respect to 'most_similar' in word2vec gensim python?</p>
<p>Currently, I am getting the following issue.</p>
<p><strong>DeprecationWarning: Call to deprecated <code>most_similar</code> (Method will be removed in 4.0.0, use self.wv.most_similar() instead).
model.most_similar('hamlet')
FutureWarning: Conversion of the second argument of issubdtype from <code>int</code> to <code>np.signedinteger</code> is deprecated. In future, it will be treated as <code>np.int32 == np.dtype(int).type</code>.
if np.issubdtype(vec.dtype, np.int):</strong></p>
<p>Please help to curb this issue? Any help is appreciated.</p>
<p>The code what, I have tried is as follows.</p>
<pre><code>import re
from gensim.models import Word2Vec
from nltk.corpus import gutenberg

sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   
print('Type of corpus: ', type(sentences))
print('Length of corpus: ', len(sentences))

for i in range(len(sentences)):
    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]
print(sentences[0])    # title, author, and year
print(sentences[1])
print(sentences[10])
model = Word2Vec(sentences=sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
model.init_sims(replace = True)
model.save('word2vec_model')
model = Word2Vec.load('word2vec_model')
model.most_similar('hamlet')
</code></pre>
","3966705","","3966705","","2020-09-29 09:45:14","2020-09-29 09:45:14","DeprecationWarning in Gensim `most_similar`?","<python><python-3.x><gensim><word2vec>","4","0","2","","","CC BY-SA 4.0"
"51798248","1","","","2018-08-11 09:18:16","","3","118","<p>I've trained a Doc2Vec model in order to do a simple binary classification task, but I would also love to see which words or sentences weigh more in terms of contributing to the meaning of a given text. So far I had no luck finding anything relevant or helpful. Any ideas how could I implement this feature? Should I switch from Doc2Vec to more conventional methods like tf-idf?</p>
","8265036","","","","","2019-09-03 21:04:39","How find the most decisive sentences or words in a document via Doc2Vec?","<python><nlp><gensim><word2vec><doc2vec>","1","1","","","","CC BY-SA 4.0"
"51736907","1","","","2018-08-07 23:54:22","","1","268","<p>I am trying to use word2vec for supervised classification of labelled classes. I have data containing sentences with their labels which I would like to train the word2vec model on, and then use word2vec (or possibly random forest classifier) in order to figure out the classes of unseen sentences. Here is what I have tried so far:</p>

<pre><code>import gensim
import numpy as np

class TokenizedSentence(object):

    def __init__(self, doc_list):
            self.doc_list = doc_list

    def __iter__(self):
            for t in (self.doc_list):
                    t = t.decode('utf-8')
                    yield gensim.utils.simple_preprocess(t)



tweets = [""a tweet"", ""another tweet"", ... , ""some tweet""]
labels = [1, 1, ... , 16]

training_data = TokenizedSentence(tweets)

model = gensim.models.Word2Vec(training_data, size=150, window=10, min_count=2, workers=10)
model.train(training_data, total_examples=model.corpus_count, epochs=200)
model.save(""w2vmodel"")
</code></pre>

<p>I am wondering how I need to go about doing this in order to include the labels that go along with each sentence, so that I can tell the model which label a sentence belongs to, and then be able to load the model in a separate file and classify unseen data into categories based on the supervised learning. Any help is appreciated!</p>
","7700994","","","","","2018-08-07 23:54:22","Using word2vec for supervised classification","<python><gensim><word2vec><supervised-learning>","0","1","","","","CC BY-SA 4.0"
"51747613","1","51748393","","2018-08-08 13:09:31","","3","1249","<p>Given I got a word2vec model (by gensim), I want to get the rank similarity between to words.
For example, let's say I have the word ""desk"" and the most similar words to ""desk"" are:</p>

<blockquote>
  <ol>
  <li>table 0.64</li>
  <li>chair 0.61</li>
  <li>book 0.59</li>
  <li>pencil 0.52</li>
  </ol>
</blockquote>

<p>I want to create a function such that:</p>

<blockquote>
  <p>f(desk,book) = 3
  Since book is the 3rd most similar word to desk.
  Does it exists? what is the most efficient way to do this?</p>
</blockquote>
","10117402","","1836483","","2018-08-08 17:54:18","2018-08-08 17:54:18","Word2vec - get rank of similarity","<python><python-3.x><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"56363181","1","","","2019-05-29 14:41:44","","0","1173","<p>I am training a ldamallet model in python and saving it. I am also saving training dictionary that I can use to create corpus for unseen documents later. If I perform every action (i.e. train a model, save trained model, load saved model, infer unseen corpus) within same console, everything works fine. However, I want to use the trained model in different console / computer. </p>

<p>I passed prefix while training to look at the temp files created by the model. Following files are created when the model is trained:</p>

<blockquote>
  <p>'corpus.mallet'</p>
  
  <p>'corpus.txt'</p>
  
  <p>'doctopics'txt'</p>
  
  <p>inferencer.mallet'</p>
  
  <p>'state.mallet.gz'</p>
  
  <p>'topickeys.txt'</p>
</blockquote>

<p>Now when I load the saved model in a different console and infer unseen corpus created using the saved dictionary, I can see no other temp files being created and produces following error:</p>

<pre><code>FileNotFounderror: No such file or directory : 'my_directory\\doctopics.txt.infer'
</code></pre>

<p>For some odd reason, if I load the saved model in same console (console it was trained on) and infer unseen corpus like above, 'corpus.txt' is updated and two new temp files are created:</p>

<blockquote>
  <p>'corpus.mallet.infer'</p>
  
  <p>'doctopics.txt.infer'</p>
</blockquote>

<p>Any idea why I might be having this issue?</p>

<p>I have tried using LdaModel instead of LdaMallet and LdaModel works fine irrespective of whether I perform whole task in same console or different console.</p>

<p>Below is the snippet of the code I am using.</p>

<pre class=""lang-py prettyprint-override""><code>    def find_optimum_model(self):
        lemmatized_words = self.lemmatization()
        id2word = corpora.Dictionary(lemmatized_words)
        all_corpus = [id2word.doc2bow(text) for text in lemmatized_words]

        #For two lines below update with your path to new_mallet
        os.environ['MALLET_HOME'] = r'C:\\users\\axk0er8\\Sentiment_Analysis_Working\\new_mallet\\mallet-2.0.8'
        mallet_path = r'C:\\users\\axk0er8\\Sentiment_Analysis_Working\\new_mallet\\mallet-2.0.8\\bin\\mallet.bat'
        prefix_path = r'C:\\users\\axk0er8\\Sentiment_Analysis_Working\\new_mallet\\mallet_temp\\'

    def compute_coherence_values(dictionary, all_corpus, texts, limit, start=2, step=4):
        coherence_values = []
        model_list = []
        num_topics_list = []


        for num_topics in range(start, limit, step):
            model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=all_corpus, num_topics=num_topics, id2word=dictionary,
                                                random_seed=42)
            #model = gensim.models.ldamodel.LdaModel(corpus=all_corpus,num_topics=num_topics,id2word=dictionary,eval_every=1,
            #                                        alpha='auto',random_state=42)
            model_list.append(model)
            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
            coherence_values.append(coherencemodel.get_coherence())
            num_topics_list.append(num_topics)

        return model_list, coherence_values, num_topics_list

    model_list, coherence_values, num_topics_list = compute_coherence_values(dictionary=id2word,all_corpus=all_corpus,
                                                                                texts=lemmatized_words,start=5,limit=40, step=6)
    model_values_df = pd.DataFrame({'model_list':model_list,'coherence_values':coherence_values,'num_topics':num_topics_list})

    optimal_num_topics = model_values_df.loc[model_values_df['coherence_values'].idxmax()]['num_topics']

    optimal_model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=all_corpus, num_topics=optimal_num_topics, id2word=id2word,
                                                        prefix=prefix_path, random_seed=42)

    #joblib.dump(id2word,'id2word_dictionary_mallet.pkl')
    #joblib.dump(optimal_model,'optimal_ldamallet_model.pkl')
    id2word.save('id2word_dictionary.gensim')
    optimal_model.save('optimal_lda_model.gensim')

    def generate_dominant_topic(self):
        lemmatized_words = self.lemmatization()
        id2word = corpora.Dictionary.load('id2word_dictionary.gensim')
        #id2word = joblib.load('id2word_dictionary_mallet.pkl')
        new_corpus = [id2word.doc2bow(text) for text in lemmatized_words]
        optimal_model = gensim.models.wrappers.LdaMallet.load('optimal_lda_model.gensim')
        #optimal_model = joblib.load('optimal_ldamallet_model.pkl')


        def format_topics_sentences(ldamodel, new_corpus):
            sent_topics_df = pd.DataFrame()
            for i, row in enumerate(ldamodel[new_corpus]):
                row = sorted(row, key=lambda x: (x[1]), reverse=True)
                for j, (topic_num, prop_topic) in enumerate(row):
                    if j == 0:
                        wp = ldamodel.show_topic(topic_num)
                        topic_keywords = "", "".join([word for word, prop in wp])
                        sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]),
                                                               ignore_index=True)
                    else:
                        break
            sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
            return (sent_topics_df)
</code></pre>

<p>My expectation is use <code>find_optimum_model</code> function with the training data and save the optimum model and dictionary. Once saved, use <code>generate_dominant_topic</code> function to load saved model and dictionary, create corpus for unseen text and run the model to get desired topic modeling output.</p>
","11573475","","11573475","","2019-05-30 13:07:25","2020-08-26 06:51:50","Saved Gensim LdaMallet model not working in different console","<python><gensim><lda><mallet>","3","0","","","","CC BY-SA 4.0"
"51800210","1","","","2018-08-11 13:40:14","","1","952","<p>I'm training a Word2vec model using Gensim Word2vec on a well-known Wikipedia dump provided by Tobias Schnabel in the following link:
<a href=""http://www.cs.cornell.edu/~schnabts/eval/index.html"" rel=""nofollow noreferrer"">http://www.cs.cornell.edu/~schnabts/eval/index.html</a> (about 4GB).</p>

<p>I would like to understand how many epochs I should run the model for training until the model will be converged.</p>

<p>I added the following code:</p>

<pre><code> model = Word2Vec(size=self._number_of_dimensions_in_hidden_layer,
                    window=self._window_size,
                    min_count=3,
                    max_vocab_size=self._max_vocabulary_size,
                    sg=self._use_cbow,
                    seed=model_seed,
                    compute_loss=True,
                    iter=self._epochs)
    model.build_vocab(sentences)

    learning_rate = 0.025
    step_size = (learning_rate - 0.001) / self._epochs

    for i in range(self._epochs):
        end_lr = learning_rate - step_size
        trained_word_count, raw_word_count = model.train(sentences, compute_loss=True,
                                                         start_alpha=learning_rate,
                                                         end_alpha=learning_rate,
                                                         total_examples=model.corpus_count,
                                                         epochs=1)
        loss = model.get_latest_training_loss()
        print(""iter={0}, loss={1}, learning_rate={2}"".format(i, loss, learning_rate))
        learning_rate  *= 0.6


    model.save(model_name_path)
</code></pre>

<p>However I cannot see the model is converging:</p>

<pre><code>iter=0, loss=76893000.0, learning_rate=0.025
iter=1, loss=74870528.0, learning_rate=0.015
iter=2, loss=73959232.0, learning_rate=0.009
iter=3, loss=73605400.0, 
learning_rate=0.005399999999999999
iter=4, loss=73224288.0, 
learning_rate=0.0032399999999999994
iter=5, loss=73008048.0, 
learning_rate=0.0019439999999999995
iter=6, loss=72935888.0, 
learning_rate=0.0011663999999999997
iter=7, loss=72774304.0, 
learning_rate=0.0006998399999999999
iter=8, loss=72642072.0, 
learning_rate=0.0004199039999999999
iter=9, loss=72624384.0, 
learning_rate=0.00025194239999999993
iter=10, loss=72700064.0, 
learning_rate=0.00015116543999999996
iter=11, loss=72478656.0, 
learning_rate=9.069926399999997e-05
iter=12, loss=72486744.0, 
learning_rate=5.441955839999998e-05
iter=13, loss=72282776.0, 
learning_rate=3.2651735039999986e-05
iter=14, loss=71841968.0, 
learning_rate=1.9591041023999992e-05
iter=15, loss=72119848.0, 
learning_rate=1.1754624614399995e-05
iter=16, loss=72054544.0, 
learning_rate=7.0527747686399965e-06
iter=17, loss=71958888.0, 
learning_rate=4.2316648611839976e-06
iter=18, loss=71933808.0, 
learning_rate=2.5389989167103985e-06
iter=19, loss=71739256.0, 
learning_rate=1.523399350026239e-06
iter=20, loss=71660288.0, 
learning_rate=9.140396100157433e-07
</code></pre>

<p>I don't undersatnd why the loss function result is not reducing and stay quite constant around 71M. </p>
","1725371","","","","","2018-08-11 17:30:11","Gensim Word2vec model is not converged","<gensim><word2vec><loss-function>","1","0","","","","CC BY-SA 4.0"
"55660598","1","55663209","","2019-04-12 23:25:57","","1","1147","<p>I have trained a doc2vec model on the Wikipedia corpus using gensim and I wish to retrieve vectors from different documents. </p>

<p>I was wondering what text processing the WikiCorpus function did when I used it to train my model e.g. removed punctuation, made all the text lower case, removed stop words etc. </p>

<p>This is important as I wish to perform the same text processing on the documents I am inferring vectors from for greater consistency/accuracy with my model. </p>
","5476045","","","","","2019-04-13 21:03:07","What text processing does WikiCorpus perform in gensim?","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"29751328","1","","","2015-04-20 14:55:22","","1","1053","<p>i am trying to implement Topic Tiling algorithm on my trained lda model.
For the algorithm I need all of the IDs that are assigned to a single word in an unseen document. I will then calculate the most frequent topic id for the given word and assign it as the mode of that word.</p>

<p>I am using the gensim lib so it is very easy to get topic->word dist, where the words are given with their probabilities. However how do I get ""what topic(s) are/were assigned to a single world"", meaning word->topic dists.</p>

<pre><code>Example:
s = ""Banks are closed on Sunday""

Topic -&gt; Word Dist from Gensim:
TopicTag -&gt; Prob*Word
Topic 0 -&gt; 0,3*Bank, 0,2*are
Topic 1 -&gt; 0,2*closed, 0,1*Sunday
Topic 2 -&gt; 0,4*Sunday, 0,3*on

What I want:
word -&gt; TopicTag(Frequency that given word was assigned with the specified topic tag)
Banks -&gt; Topic1(2), Topic2(2)
Closed -&gt; Topic0(1),Topic1 (4)
</code></pre>

<p>Please also note that I am not interested in parsing the Topic -> Word Dist results from Gensim, I am interested in finding an accurate way that my model assigns (numerous) topic(s) to each individual word that will come in an unseen document.</p>

<p>Thanks in advance.</p>
","2539771","","","","","2016-05-16 23:53:36","LDA Gensim Word -> Topic Ids Distribution instead of Topic -> Word Distribution","<python><lda><topic-modeling><gensim>","2","0","0","","","CC BY-SA 3.0"
"28155313","1","","","2015-01-26 17:22:32","","1","3377","<p>I have been trying to install <code>gensim</code> using the following command:</p>

<pre><code>pip install --upgrade gensim
</code></pre>

<p>I got the following error messages:</p>

<pre><code>Downloading/unpacking gensim
Cannot fetch index base URL https://pypi.python.org/simple/
Could not find any downloads that satisfy the requirement gensim
Cleaning up...
No distributions at all found for gensim
Storing debug log for failure in /home/users/.pip/pip.log
</code></pre>

<p>How can I fix this problem?</p>

<p>Here is the version of Ubuntu, which is installed on Windows through VmWare</p>

<pre><code>No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.1 LTS
Release:    14.04
Codename:   trusty
</code></pre>
","288609","","288609","","2015-01-26 18:11:58","2016-02-02 02:54:34","Problems when installing gensim on Ubuntu","<python><ubuntu><gensim>","1","3","","","","CC BY-SA 3.0"
"54472367","1","54472568","","2019-02-01 03:14:39","","0","696","<p>I tried saving a word2vec model that I had trained with gensim like so:</p>

<pre><code>from gensim.models import Word2Vec
model = Word2Vec(sentences, parameters)
model.save('modelfile.model')
</code></pre>

<p>Now when I try <code>Word2Vec.load('modelfile.model')</code>, I get:</p>

<pre><code>ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'
</code></pre>

<p>Can post the full traceback if it helps.</p>
","7542939","","","","","2019-02-01 04:08:23","Can't load saved gensim word2vec model","<python><gensim>","2","0","","","","CC BY-SA 4.0"
"42212423","1","43880304","","2017-02-13 19:54:33","","8","3927","<p>I have a dataset of several thousand rows of text, my target is to calculate the tfidf score and then cosine similarity between documents, this is what I did using gensim in Python followed the tutorial:</p>

<pre><code>dictionary = corpora.Dictionary(dat)
corpus = [dictionary.doc2bow(text) for text in dat]

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
index = similarities.MatrixSimilarity(corpus_tfidf)
</code></pre>

<p>Let's say we have the tfidf matrix and similarity built, when we have a new document come in, I want to query for its most similar document in our existing dataset.</p>

<p>Question: is there any way we can update the tf-idf matrix so that I don't have to append the new text doc to the original dataset and recalculate the whole thing again? </p>
","4757432","","","","","2020-09-04 11:28:52","Python tf-idf: fast way to update the tf-idf matrix","<python><nlp><tf-idf><gensim><cosine-similarity>","2","0","7","","","CC BY-SA 3.0"
"63065232","1","","","2020-07-24 00:53:22","","0","228","<p>I use virtualenv in Python. I use gensim in a script. I get this error
name 'gensim' is not defined
I tried to install genism using pip and conda. I ended up updating conda packages after some suggested solution .
I see there is genism 3.8 after reunnig pip list, but I still have the error !. Could you please tell me what to do
P.S. I take input from a html form in a Flask function. Inside the function, I call the script that has genism. The program show the forms input buttons . After clicking the submit buton, I get the error message.</p>
<pre><code>import re
import numpy as np
import pandas as pd
from pprint import pprint

#database
import db
from db import *

# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric

# spacy for lemmatization
import spacy

# Plotting tools
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
#matplotlib inline

from conn import *
from functions import *

# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)

import warnings
warnings.filterwarnings(&quot;ignore&quot;,category=DeprecationWarning)
</code></pre>
<p>Thanks in advance</p>
","9529190","","","","","2020-07-24 00:53:22","'gensim' is not defined even though it shows in the virtualenv packages","<python-3.x><flask><gensim>","0","4","","","","CC BY-SA 4.0"
"37101671","1","","","2016-05-08 15:42:48","","2","829","<p>I have this piece of code:</p>

<pre><code>import gensim
import random


file = open('../../../dataset/output/interaction_jobroles_titles_tags.txt')

read_data = file.read()

data = read_data.split('\n')

sentences = [line.split() for line in data]
print(len(sentences))
print(sentences[1])

model = gensim.models.Word2Vec(min_count=1, window=10, size=300, negative=5)
model.build_vocab(sentences)

for epoch in range(5):
    shuffled_sentences = random.shuffle(sentences)
    model.train(shuffled_sentences)
    print(epoch)
    print(model)

model.save(""../../../dataset/output/wordvectors_jobroles_titles_300d_10w_wordshuffling"" + '.model')
</code></pre>

<p>If I print a single sentence, then it output is something like this:</p>

<pre><code>['JO_3787672', 'JO_272304', 'JO_2027410', 'TI_2969041', 'TI_2509936', 'TA_954638', 'TA_4321623', 'TA_339347', 'TA_272304', 'TA_3017535', 'TA_494116', 'TA_798840']
</code></pre>

<p>What I need is to shuffle the words before training and then save the model. </p>

<p>I am not sure whether I am coding it in a right way. I end up with exception:</p>

<pre><code>Exception in thread Thread-8:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 747, in job_producer
    for sent_idx, sentence in enumerate(sentences):
  File ""/usr/local/lib/python3.5/site-packages/gensim/utils.py"", line 668, in __iter__
    for document in self.corpus:
TypeError: 'NoneType' object is not iterable
</code></pre>

<p>I would like to ask you how can I shuffle words.</p>
","5368173","","4895725","","2016-05-08 16:51:29","2016-05-09 08:34:03","How to shuffle words in word2vec","<python><gensim><word2vec>","2","1","","","","CC BY-SA 3.0"
"46146241","1","46147083","","2017-09-10 22:45:20","","1","3943","<pre><code>from gensim import corpora, models, similarities

documents = [""This is a book about cars, dinosaurs, and fences""]

# remove common words and tokenize
stoplist = set('for a of the and to in - , is'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# Remove commas
texts[0] = [text.replace(',','') for text in texts[0]]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)

doc = ""I like cars and birds""
vec_bow = dictionary.doc2bow(doc.lower().split())

vec_lsi = lsi[vec_bow] 
index = similarities.MatrixSimilarity(lsi[corpus]) 

sims = index[vec_lsi] # perform a similarity query against the corpus
print(sims)
</code></pre>

<p>In the above code I am comparing how much ""This is a book about cars, dinosaurs, and fences"" is similar to ""I like cars and birds"" using the cosine similarity technique.</p>

<p>The two sentences have effectively 1 words in common, which is ""cars"", however when I run the code I get that they are 100% similar.  This does not make sense to me.</p>

<p>Can someone suggest how to improve my code so that I get a reasonable number?</p>
","5131031","","","","","2017-09-11 01:30:24","Text similarity with gensim and cosine similarity","<python><gensim><cosine-similarity>","1","0","2","","","CC BY-SA 3.0"
"42242521","1","42264559","","2017-02-15 06:55:42","","5","2071","<p>I am just playing around with Doc2Vec from gensim, analysing stackexchange dump to analyze semantic similarity of questions to identify duplicates.</p>

<p>The tutorial on <a href=""https://rare-technologies.com/doc2vec-tutorial/"" rel=""noreferrer"">Doc2Vec-Tutorial</a> seems to describe the input as tagged sentences.</p>

<p>But the original paper: <a href=""https://arxiv.org/pdf/1405.4053.pdf"" rel=""noreferrer"">Doc2Vec-Paper</a> claims that the method can be used to infer fixed length vectors of paragraphs/documents.</p>

<p>Can someone explain the difference between a sentence and a document in this context, and how i would go about inferring paragraph vectors.</p>

<p>Since a question can sometimes span multiple sentences,
I thought, during training i will give sentences arising from the same question the same tags, but then how would i do this to infer_vector on unseen questions? </p>

<p>And this notebook : <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""noreferrer"">Doc2Vec-Notebook</a></p>

<p>seems to be training vectors on TRAIN and TEST docs, can someone explain the rationale behind this and should i do the same?</p>
","4126652","","","","","2017-02-16 03:49:52","Doc2Vec: Differentiate Sentence and Document","<python><gensim><doc2vec>","1","0","3","","","CC BY-SA 3.0"
"46148182","1","46153316","","2017-09-11 04:28:12","","13","6918","<p>I want to get bigrams and trigrams from the example sentences I have mentioned.</p>

<p>My code works fine for bigrams. However, it does not capture trigrams in the data (e.g., human computer interaction, which is mentioned in 5 places of my sentences)</p>

<p><strong>Approach 1</strong> Mentioned below is my code using Phrases in Gensim.</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=1, delimiter=b' ')
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p><strong>Approach 2</strong> I even tried to use Phraser and Phrases both, but it didn't work.</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
bigram_phraser = Phraser(bigram)
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>Please help me to fix this issue of getting trigrams.</p>

<p>I am following the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""noreferrer"">example documentation</a> of Gensim.</p>
","","user8566323","","user8566323","2017-09-11 07:33:33","2017-09-11 15:19:31","Issues in getting trigrams using Gensim","<python><data-mining><text-mining><word2vec><gensim>","1","0","4","","","CC BY-SA 3.0"
"51851193","1","","","2018-08-14 23:55:08","","1","129","<p>I've seen some posts say that the average of the word vectors perform better in some tasks than the doc vectors learned through PV_DBOW. What is the relationship between the document's vector and the average/sum of its words' vectors? Can we say that vector <b><i>d</i></b>
 is approximately equal to the average or sum of its word vectors? 
Thanks!</p>
","1640669","","","","","2018-08-15 03:59:17","Is doc vector learned through PV-DBOW equivalent to the average/sum of the word vectors contained in the doc?","<machine-learning><nlp><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"46147013","1","46147020","","2017-09-11 01:14:35","","0","1363","<p>I want to extract all bigrams and trigrams of the given sentences. </p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""Human Computer Interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]

sentence_stream = [doc.split("" "") for doc in documents]
bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
trigram = Phrases(bigram(sentence_stream, min_count=1, threshold=2, delimiter=b' '))

for sent in sentence_stream:
    #print(sent)
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>The code works fine for bigrams and capture 'new york' and 'machine learning' ad bigrams.</p>

<p>However, I get the following error when I try to insert trigrams.</p>

<pre><code>TypeError: 'Phrases' object is not callable
</code></pre>

<p>Please let me know, how to correct my code.</p>

<p>I am following the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">example documentation</a> of gensim.</p>
","","user8566323","4909087","","2017-09-27 04:11:06","2018-07-30 13:45:23","Error getting trigrams using gensim's Phrases","<python><nlp><data-mining><text-mining><gensim>","1","0","","","","CC BY-SA 3.0"
"56639993","1","","","2019-06-17 23:26:44","","1","1750","<p>According to <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors"" rel=""nofollow noreferrer"">Gensim's page on WordEmbeddingKeyedVectors</a>, you can add a new key-value pair of new word vectors incrementally. However, after initializing WordEmbeddingKeyedVectors with pre-trained vectors and its tags, and adding new unseen model-inferred word vectors to it, the <code>most_similar</code> method could no longer be used. </p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors

test = WordEmbeddingsKeyedVectors(vector_size=3)

test.add(entities=[""1"", ""2""], weights=[np.random.randint(5, size=3), 
                                  np.random.randint(5, size=3)])

test.most_similar(""2"") #THIS WORKS

test.add(entities=['3'], weights=[np.random.randint(5, size=3)])

test.most_similar(""3"") #THIS FAILS
</code></pre>

<p>I expect the output to be a list of vector tags most similar to the input tag, but the output is:</p>

<blockquote>
  <p>IndexError: index 2 is out of bounds for axis 0 with size 2</p>
</blockquote>
","11661492","","2745495","","2019-06-17 23:35:48","2019-06-18 22:25:15","How to add new word vectors to gensim.models.keyedvectors and calculate most_similar","<python><gensim><word2vec><doc2vec>","2","2","1","","","CC BY-SA 4.0"
"56641954","1","56643042","","2019-06-18 04:53:03","","1","1429","<p>I am trying to convert tokens of sentences into integers. But it is giving me floats. </p>

<pre><code>from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

sometext = ""hello how are you doing?""

tokens = word_tokenize(sometext)
model = Word2Vec([tokens], min_count=1, size=1)
</code></pre>

<p>when I do,</p>

<pre><code>print(model[""hello""])
</code></pre>

<p>it gives me,</p>

<pre><code>[-0.3843384]
</code></pre>

<p>I want this to be a positive integer. </p>
","5405358","","","","","2019-06-18 08:12:52","Converting string tokens into integers","<python><python-3.x><nltk><gensim><word2vec>","2","1","","","","CC BY-SA 4.0"
"42289858","1","42303281","","2017-02-17 05:09:35","","3","2215","<p>I have the LDA model and the document-topic probabilities.</p>

<pre><code># build the model on the corpus
ldam = LdaModel(corpus=corpus, num_topics=20, id2word=dictionary) 
# get the document-topic probabilities
theta, _ = ldam.inference(corpus)
</code></pre>

<p>I also need the distribution of words for all the topics i.e. a topic-word probability matrix. Is there a way to extract this information? </p>

<p>Thanks!</p>
","2324298","","","","","2017-02-17 16:49:05","Extract topic word probability matrix in gensim LdaModel","<python><gensim><lda><topic-modeling>","1","0","1","","","CC BY-SA 3.0"
"63509864","1","63510136","","2020-08-20 16:58:37","","1","558","<p>I am using the Word2vec module of Gensim library to train a word embedding, the dataset is 400k sentences with 100k unique words (its not english)</p>
<p>I'm using this code to monitor and calculate the loss :</p>
<pre class=""lang-py prettyprint-override""><code>class MonitorCallback(CallbackAny2Vec):
    def __init__(self, test_words):
        self._test_words = test_words

    def on_epoch_end(self, model):
        print(&quot;Model loss:&quot;, model.get_latest_training_loss())  # print loss
        for word in self._test_words:  # show wv logic changes
            print(model.wv.most_similar(word))


monitor = MonitorCallback([&quot;MyWord&quot;])  # monitor with demo words

w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT  , callbacks=[monitor])

w2v_model.build_vocab(tokenized_corpus)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print(&quot;Vocab size&quot;, vocab_size)

print(&quot;[*] Training...&quot;)

# Train Word Embeddings
w2v_model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=W2V_EPOCH)
</code></pre>
<p>The problem is from epoch 1 the loss is 0 and the vector of the monitored words dont change at all!</p>
<pre><code>[*] Training...
Model loss: 0.0
Model loss: 0.0
Model loss: 0.0
Model loss: 0.0
</code></pre>
<p>so what is the problem here? is this normal?  the tokenized corpus is a list of lists that are something like tokenized_corpus[0] = [ &quot;word1&quot; , &quot;word2&quot; , ...]</p>
<p>I googled and seems like some of the old versions of gensim had problem with calculating loss function, but they are from almost a year ago and it seems like it should be fixed right now?</p>
<p>I tried the code provided in the answer of this question as well but still the loss is 0 :</p>
<p><a href=""https://stackoverflow.com/questions/52038651/loss-does-not-decrease-during-training-word2vec-gensim"">Loss does not decrease during training (Word2Vec, Gensim)</a></p>
<p>EDIT1 : after adding compute_loss=True, the loss shows up, but it keeps going higher and higher, and the top similar words and their similarity doesn't change at all :</p>
<pre><code>Model loss: 2187903.5
Model loss: 3245492.0
Model loss: 4103624.5
Model loss: 4798541.0
Model loss: 5413940.0
Model loss: 5993822.5
Model loss: 6532631.0
Model loss: 7048384.5
Model loss: 7547147.0
</code></pre>
","9557861","","9557861","","2020-08-20 18:23:32","2020-08-20 18:23:32","Gensim's word2vec has a loss of 0 from epoch 1?","<nlp><pytorch><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"42329766","1","42373173","","2017-02-19 16:28:49","","7","4593","<p>I'm currently working on NLP in python. However, in my corpus, there are both British and American English(realize/realise) I'm thinking to convert British to American. However, I did not find a good tool/package to do that. Any suggestions?</p>
","6659095","","","","","2021-03-26 01:07:44","Python NLP British English vs American English","<python><nlp><nltk><gensim><linguistics>","3","0","4","","","CC BY-SA 3.0"
"42357678","1","42420036","","2017-02-21 02:35:31","","8","3407","<p>Word2Vec from gensim 0.13.4.1 to update the word vectors on the fly does not work.</p>

<pre><code>model.build_vocab(sentences, update=False)
</code></pre>

<p>works fine;  however, </p>

<pre><code>model.build_vocab(sentences, update=True)
</code></pre>

<p>does not.</p>

<hr>

<p>I am using <a href=""http://rutumulkar.com/blog/2015/word2vec"" rel=""noreferrer"">this website</a> to try and emulate what they have done; hence I use the following script at some point:</p>

<pre><code>model = gensim.models.Word2Vec()
sentences = gensim.models.word2vec.LineSentence(""./text8/text8"")
model.build_vocab(sentences, keep_raw_vocab=False, trim_rule=None, progress_per=10000, update=False)
model.train(sentences)
</code></pre>

<p>However while this runs with <code>update=False</code>, using <code>update=True</code> gives me the following traceback:</p>

<pre><code>Traceback (most recent call last):
  File ""word2vecAttempt.py"", line 34, in &lt;module&gt;
    model.build_vocab(sentences, progress_per=10000, update=True)
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 535, in build_vocab
    self.finalize_vocab(update=update)  # build tables &amp; arrays
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 708, in finalize_vocab
    self.update_weights()
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 1070, in update_weights
    self.wv.syn0 = vstack([self.wv.syn0, newsyn0])
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/numpy/core/shape_base.py"", line 230, in vstack
    return _nx.concatenate([atleast_2d(_m) for _m in tup], 0)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
</code></pre>
","1486196","","1486196","","2017-02-22 21:26:23","2017-02-23 15:35:04","gensim word2vec - array dimensions in updating with online word embedding","<python><numpy><gensim>","1","0","2","","","CC BY-SA 3.0"
"51903087","1","","","2018-08-17 21:27:50","","1","768","<p>So I have made an AnnoyIndexer and am running some most_similar queries to find the nearest neighbours of some vectors in a 300dimensional vector space. This is the code for it:</p>

<pre><code>def most_similar(self, vector, num_neighbors):
    """"""Find the approximate `num_neighbors` most similar items.
    Parameters
    ----------
    vector : numpy.array
        Vector for word/document.
    num_neighbors : int
        Number of most similar items
    Returns
    -------
    list of (str, float)
        List of most similar items in format [(`item`, `cosine_distance`), ... ]
    """"""

    ids, distances = self.index.get_nns_by_vector(
        vector, num_neighbors, include_distances=True)

    return [(self.labels[ids[i]], 1 - distances[i] / 2) for i in range(len(ids))]
</code></pre>

<p>I am wondering why the returned values for the distances are all taken away from 1 and then divided by 2? Surely after doing that, largest/smallest distances are all messed up?</p>
","10112248","","","","","2018-08-17 22:12:21","Understanding the most_similar method for an AnnoyIndexer in gensim.similarities.index","<python><nlp><gensim><word2vec><annoy>","1","0","","","","CC BY-SA 4.0"
"51945520","1","51947552","","2018-08-21 09:23:24","","1","974","<p>Hello Community Members,</p>

<p>At present, I am implementing the Word2Vec algorithm.</p>

<p>Firstly, I have extracted the data (sentences), break and split the sentences into tokens (words), remove the punctuation marks and store the tokens in a single list. The list basically contain the words. Then I have calculated the frequency of words and then computed it occurrences in terms of frequency. It results a list. </p>

<p>Next, I am trying to load the model using gensim. However, I am facing a problem. The problem is about <code>the word is not in the vocabulary</code>. The code snippet, whatever I have tried is as follows.</p>

<pre><code>import nltk, re, gensim
import string
from collections import Counter
from string import punctuation
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from nltk.corpus import gutenberg, stopwords

def preprocessing():
    raw_data = (gutenberg.raw('shakespeare-hamlet.txt'))
    tokens = word_tokenize(raw_data)
    tokens = [w.lower() for w in tokens]
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    global words
    words = [word for word in stripped if word.isalpha()]
    sw = (stopwords.words('english'))
    sw1= (['.', ',', '""', '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])
    sw2= (['for', 'on', 'ed', 'es', 'ing', 'of', 'd', 'is', 'has', 'have', 'been', 'had', 'was', 'are', 'were', 'a', 'an', 'the', 't', 's', 'than', 'that', 'it', '&amp;', 'and', 'where', 'there', 'he', 'she', 'i', 'and', 'with', 'it', 'to', 'shall', 'why', 'ham'])
    stop=sw+sw1+sw2
    words = [w for w in words if not w in stop]
preprocessing()

def freq_count():
    fd = nltk.FreqDist(words)
    print(fd.most_common())
    freq_count()
def word_embedding():
    for i in range(len(words)):
        model = Word2Vec(words, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
        model.init_sims(replace = True)
        model.save('word2vec_model')
        model = Word2Vec.load('word2vec_model')
        similarities = model.wv.most_similar('hamlet')
        for word, score in similarities:
            print(word , score)
word_embedding()
</code></pre>

<p>Note: I am using Python 3.7 in Windows OS. From the <code>syntax of gensim</code>, it is suggested to use sentences and split into tokens and apply the same to build and train the model. My question is that how to apply the same to a corpus with single list containing only words. I have specified the words also using list, i.e. [words], during the training of the model.       </p>
","3966705","","3966705","","2018-08-21 09:25:28","2018-08-21 17:22:36","'word' not in Vocabulary in a corpus with words shown in a single list only in gensim library","<python-3.x><nltk><gensim><word2vec><nltk-book>","2","0","","","","CC BY-SA 4.0"
"42363897","1","","","2017-02-21 09:49:33","","5","9478","<p>I am trying to implement word2vec model and getting Attribute error </p>

<blockquote>
  <p>AttributeError: type object 'Word2Vec' has no attribute 'load_word2vec_format'</p>
</blockquote>

<p>Below is the code :</p>

<pre><code>wv = Word2Vec.load_word2vec_format(""GoogleNews-vectors-negative300.bin.gz"", binary=True)
wv.init_sims(replace=True)
</code></pre>

<p>Please let me know the issue ?</p>
","7421387","","7505321","","2017-02-21 10:14:37","2017-03-17 09:34:24","AttributeError: type object 'Word2Vec' has no attribute 'load_word2vec_format'","<python><nlp><gensim><word2vec>","2","0","","","","CC BY-SA 3.0"
"63625873","1","","","2020-08-28 00:32:53","","0","211","<p>Well the issue is I have 1000s of the document and I passed all the documents for the training of Gensim model and I successfully trained and saved the model in .model format.</p>
<p>But with the current format, 2 new files have also been generated</p>
<ol>
<li>doc2vec.model</li>
<li>doc2vec.model.trainables.syn1neg.npy</li>
<li>doc2vec.model.wv.vectors.npy</li>
</ol>
<p>Due to the limitation of Hardware I trained and saved the model on Google Colab and Google Driver respectively. When I downloaded the generated models and extra files in my local machine and ran the code it's giving me a File Not Found Error, whereas I have added the particular files where the .py file is or current working directory is.</p>
<p>Well I used below code</p>
<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize

files = readfiles(&quot;CuratedData&quot;)
data = [TaggedDocument(words=word_tokenize(_d.decode('utf-8').strip().lower()), tags=[str(i)]) for i, _d in enumerate(files)]

max_epochs = 100
vec_size = 300
alpha = 0.025

model = Doc2Vec(vector_size=vec_size,
                alpha=alpha,
                min_alpha=0.00025,
                min_count=1,
                dm=1)

model.build_vocab(data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(&quot;doc2vec.model&quot;)
print(&quot;Model Saved&quot;)
</code></pre>
<p>Code for Loading the Model</p>
<pre><code>    webVec = &quot;&quot;
    try:

        path = os.path.join(os.getcwd(), &quot;doc2vec.model&quot;)

        model = Word2Vec.load(path)

        data = word_tokenize(content['htmlResponse'].lower())

        # Webvector
        webVec = model.infer_vector(data)
    except ValueError as ve:
        print(ve)
    except (TypeError, ZeroDivisionError) as ty:
        print(ty)
    except:
        print(&quot;Oops!&quot;, sys.exc_info()[0], &quot;occurred.&quot;)
</code></pre>
<p>Any help would be greatly appreciated. Thanks, Cheers</p>
","6792928","","6792928","","2020-08-28 10:16:56","2020-08-28 10:16:56","Gensim Model : class 'FileNotFoundError'","<python><python-2.7><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"42381902","1","42523598","","2017-02-22 03:00:46","","19","8277","<p>E.g. we train a word2vec model using <code>gensim</code>:</p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models.word2vec import Word2Vec

documents = [""Human machine interface for lab abc computer applications"",
              ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]

texts = [[word for word in document.lower().split()] for document in documents]
w2v_model = Word2Vec(texts, size=500, window=5, min_count=1)
</code></pre>

<p>And when we query the similarity between words, we find negative similarity scores:</p>

<pre><code>&gt;&gt;&gt; w2v_model.similarity('graph', 'computer')
0.046929569156789336
&gt;&gt;&gt; w2v_model.similarity('graph', 'system')
0.063683518562347399
&gt;&gt;&gt; w2v_model.similarity('survey', 'generation')
-0.040026775040430063
&gt;&gt;&gt; w2v_model.similarity('graph', 'trees')
-0.0072684112978664561
</code></pre>

<p><strong>How do we interpret the negative scores?</strong> </p>

<p>If it's a cosine similarity shouldn't the range be <code>[0,1]</code>?</p>

<p><strong>What is the upper bound and lower bound of the <code>Word2Vec.similarity(x,y)</code> function?</strong> There isn't much written in the docs: <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity</a> =(</p>

<p>Looking at the Python wrapper code, there isn't much too: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165</a></p>

<p>(If possible, please do point me to the <code>.pyx</code> code of where the similarity function is implemented.)</p>
","610569","","","","","2017-03-07 03:12:36","Interpreting negative Word2Vec similarity from gensim","<python><nlp><similarity><gensim><word2vec>","2","2","3","","","CC BY-SA 3.0"
"42316431","1","42317702","","2017-02-18 14:30:56","","1","890","<p>I am trying to classify paragraphs based on their sentiments. I have training data of 600 thousand documents. When I convert them to <code>Tf-Idf</code> vector space with words as analyzer and ngram range as 1-2 there are almost 6 million features. So I have to do Singular value decomposition (SVD) to reduce features.</p>

<p>I have tried gensim and sklearn's SVD feature. Both work fine for feature reduction till 100 but as soon as I try for 200 features they throw memory error. </p>

<p>Also I have not used entire document (600 thousand) as training data, I have taken 50000 documents only. So essentially my training matrix is:
50000 * 6 million and want to reduce it to  50000 * (100 to 500) </p>

<p>Is there any other way I can implement it in python, or do I have to implement sparks mllib SVD(written for only java and scala) ? If Yes, how much faster will it be?</p>

<p>System specification: 32 Gb RAM with 4 core processors on ubuntu 14.04</p>
","6552958","","4974175","","2017-02-18 14:38:37","2017-02-18 16:29:06","SVD using Scikit-Learn and Gensim with 6 million features","<python><scikit-learn><gensim><svd>","1","0","","","","CC BY-SA 3.0"
"56686830","1","56696806","","2019-06-20 13:16:11","","0","102","<p>I am trying to get the embeddings for a list of 1043 nodes with word2vec. When I try to build the vocabulary I find that word2vec takes the list of lists with the nodes and treats them as single digits, eg that ""143"" becomes ""1"",""4"",""3"".</p>

<p>I already tried to have all the numbers as single entries and see wether it is an formatting problem and went with a buil_vocab_from_freq instead of build_vocab, but this also just produces errors (object of type 'int' has no len()).</p>

<p>My code is the following:</p>

<pre><code>from gensim.models import Word2Vec

def generateEmbeddings(all_walks,dimension,min_count):
    model = Word2Vec(min_count = min_count, size = dimension)
    mylist = list(range(1,1043))
    corpus = {}
    j=1
    for i in mylist:
      corpus[str(i)] = j
      j=j+1
    #mylist = [str(i) for i in mylist]
    print(corpus)
    model.build_vocab_from_freq(corpus)
    model.train(mylist, total_examples=model.corpus_count, epochs = 30)
    #if it reaches this point it throws the error ""14 not found in vocabulary""
    print(model.wv.most_similar(positive=['14']))
    return model

print(generateEmbeddings(all_walks,128,2))
</code></pre>

<p>I want to get the embedding for eg. the number ""14"" and not ""1"" as it is by now. Thanks for your help!</p>

<p>//Edit</p>

<p>I managed to fix this, if anybody else is having this specific problem:
you have to format the list as mentioned as [[""1"",""102"",""43""],[""54"",""43""]] etc.
You cant change the old list at runtime (or at least it didnt work the way I did it), so you could create a new list at runtime with</p>

<pre><code>new_list = []
    for i in all_walks:
      temp_list = []
      for j in i:
        temp_list.append(str(j))
      new_list.append(temp_list)
</code></pre>
","11474610","","11474610","","2019-06-20 18:00:38","2019-06-21 04:22:47","How to add numbers with more than one digit to the word2vec-vocabulary","<python><gensim><word2vec>","1","4","","","","CC BY-SA 4.0"
"42389748","1","42589705","","2017-02-22 11:16:52","","0","343","<p>Okay, this is a specific question about what data structure is required when providing training data to the Gensim python library. In particular, there must be an implicit understanding of what constitutes a document in any data that it is provided (otherwise it wouldn't, for instance, be able to find the tf-idf).</p>

<p>For a specific example, the wikipedia dump is used in the tutorials for the library for training purposes. The wikipedia dump is provided in XML. What gives gensim an understanding of separate documents? Is this understanding predicated on the nesing of xml elements? </p>
","1075708","","","","","2017-03-16 10:36:23","Gensim data parsing","<python><gensim>","2","0","","","","CC BY-SA 3.0"
"51988701","1","51989150","","2018-08-23 14:56:05","","3","487","<p>We are trying to implement a word vector model for the set of words given below.</p>

<pre><code>stemmed = ['data', 'appli', 'scientist', 'mgr', 'microsoft', 'hire', 'develop', 'mentor', 'team', 'data', 'scientist', 'defin', 'data', 'scienc', 'prioriti', 'deep', 'understand', 'busi', 'goal', 'collabor', 'across', 'multipl', 'group', 'set', 'team', 'shortterm', 'longterm', 'goal', 'act', 'strateg', 'advisor', 'leadership', 'influenc', 'futur', 'direct', 'strategi', 'defin', 'partnership', 'align', 'efficaci', 'broad', 'analyt', 'effort', 'analyticsdata', 'team', 'drive', 'particip', 'data', 'scienc', 'bi', 'commun', 'disciplin', 'microsoftprior', 'experi', 'hire', 'manag', 'run', 'team', 'data', 'scientist', 'busi', 'domain', 'experi', 'use', 'analyt', 'must', 'experi', 'across', 'sever', 'relev', 'busi', 'domain', 'util', 'critic', 'think', 'skill', 'conceptu', 'complex', 'busi', 'problem', 'solut', 'use', 'advanc', 'analyt', 'larg', 'scale', 'realworld', 'busi', 'data', 'set', 'candid', 'must', 'abl', 'independ', 'execut', 'analyt', 'project', 'help', 'intern', 'client', 'understand']
</code></pre>

<p>We are using this code:</p>

<pre><code>import gensim
model = gensim.models.FastText(stemmed, size=100, window=5, min_count=1, workers=4, sg=1)
model.wv.most_similar(positive=['data'])
</code></pre>

<p>However, we are getting this error:</p>

<pre><code>KeyError: 'all ngrams for word data absent from model'
</code></pre>
","9498558","","6347629","","2019-01-25 12:43:36","2019-01-25 12:43:36","Implementing Word to vector model using Gensim","<python-3.x><machine-learning><gensim><fasttext>","2","0","","","","CC BY-SA 4.0"
"30488695","1","30518722","","2015-05-27 16:53:24","","6","3903","<p>I am using the doc2vec model from teh gensim framework to represent a corpus of 15 500 000  short documents (up to 300 words): </p>

<pre><code>gensim.models.Doc2Vec(sentences, size=400, window=10, min_count=1, workers=8 )
</code></pre>

<p>After creating the vectors there are  more than  18 000 000 vectors representing words and documents. </p>

<p>I want to find the most similar items (words or documents) for a given item: </p>

<pre><code> similarities = model.most_similar(‚Äòuid_10693076‚Äô)
</code></pre>

<p>but I get a MemoryError when the similarities are computed:</p>

<pre><code>Traceback (most recent call last):

   File ""article/test_vectors.py"", line 31, in &lt;module&gt; 
    similarities = model.most_similar(item) 
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 639, in most_similar 
    self.init_sims() 
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 827, in init_sims 
    self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL) 
</code></pre>

<p>I have a Ubuntu machine  with 60GB Ram and 70GB swap . I checked the memory allocation (in htop) and I observed that never the memory was completely used. I also set to unlimited the the maximum address space which may be locked in memory in python:</p>

<pre><code>resource.getrlimit(resource.RLIMIT_MEMLOCK)
</code></pre>

<p>Could someone explain the reason for this MemoryError? In my opinion the available memory should be enough for doing this computations. Could be some memory limits in python or OS?</p>

<p>Thanks in advance!</p>
","4576527","","","","","2015-06-17 00:47:03","Doc2vec MemoryError","<python><memory><gensim><word2vec>","1","0","5","","","CC BY-SA 3.0"
"51792916","1","","","2018-08-10 19:23:44","","0","669","<p>I have just been started learning about word embeddings and gensim and I tried this <a href=""https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"" rel=""nofollow noreferrer"">code</a> 
. In this article during the visualisation it says we need PCA to convert high-dimensional vectors into low-dimensions. Now we have a parameter ""size"" in Word2Vec method, so why can't we set that size equals to 2 rather using PCA. 
So, I tried to do this and compare both graphs (one with 100 size and other with 2 as size) and got very different result. Now I am confused that what this ""size"" depicts? How the size of vectors affect this?</p>

<p><a href=""https://i.stack.imgur.com/B2AkA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B2AkA.png"" alt=""enter image description here""></a></p>

<p>This is what I got when I used 100 as size.</p>

<p><a href=""https://i.stack.imgur.com/Wf4vM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wf4vM.png"" alt=""enter image description here""></a></p>

<p>This is what I got when I used 2 as size.</p>
","9355802","","","","","2018-08-10 20:23:23","What is the parameter ""size"" means in gensim.model.Word2Vec(sentence, size)?","<python><nlp><gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"42198720","1","","","2017-02-13 07:32:35","","0","889","<p>With word2vec, to find similarity score/most similar words of a single word can be done by</p>

<pre><code>model.most_similar('man')
model.similarity('man', 'woman')
</code></pre>

<p>However, now i want to find similarity score of a word phrase, such as,</p>

<pre><code>model.most_similar('battery life')
model.similarity('battery life', 'battery')
model.similarity('battery life', 'sound quality')
</code></pre>

<p>which i get the KeyError: ""word 'battery life' not in vocabulary"",
so is it possible to do it with word2vec?</p>
","6086295","","","","","2017-02-14 19:14:12","How to find similarity score of two word phrases with word2vec?","<text-mining><gensim><word2vec>","2","0","","","","CC BY-SA 3.0"
"48044670","1","48080869","","2017-12-31 17:55:45","","2","856","<p>I am trying to get started with <code>word2vec</code> and <code>doc2vec</code> using the excellent tutorials, <a href=""http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow noreferrer"">here</a> and <a href=""https://medium.com/@mishra.thedeepak/doc2vec-in-a-simple-way-fa80bfe81104"" rel=""nofollow noreferrer"">here</a> and trying to use the code samples. I only added in a <code>line_clean()</code> method to remove punctuation, stopwords etc. </p>

<p>But I am having trouble with the <code>line_clean()</code> method called in the training iterations.  I understand the call to the global method is messing it up, but I am not sure how to get past this problem.</p>

<pre><code>Iteration 1
Traceback (most recent call last):
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 96, in &lt;module&gt;
    train()
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 91, in train
    model.train(sentences.sentences_perm(),total_examples=model.corpus_count,epochs=model.iter)
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 61, in sentences_perm
    shuffled = list(self.sentences)
AttributeError: 'TaggedLineSentence' object has no attribute 'sentences'
</code></pre>

<p>My code is below:</p>

<pre><code>import gensim
from gensim import utils
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec
import os
import random
import numpy
from sklearn.linear_model import LogisticRegression
import logging
import sys
from nltk import RegexpTokenizer
from nltk.corpus import stopwords

tokenizer = RegexpTokenizer(r'\w+')
stopword_set = set(stopwords.words('english'))


def clean_line(line):
    new_str = unicode(line, errors='replace').lower() #encoding issues
    dlist = tokenizer.tokenize(new_str)
    dlist = list(set(dlist).difference(stopword_set))
    new_line = ' '.join(dlist)
    return new_line


class TaggedLineSentence(object):
    def __init__(self, sources):
        self.sources = sources

        flipped = {}

        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')

    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    yield TaggedDocument(utils.to_unicode(clean_line(line)).split(), [prefix + '_%s' % item_no])

    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    self.sentences.append(TaggedDocument(utils.to_unicode(clean_line(line)).split(), [prefix + '_%s' % item_no]))
        return(self.sentences)

    def sentences_perm(self):
        shuffled = list(self.sentences)
        random.shuffle(shuffled)
        return(shuffled)


def train():
    #create a list data that stores the content of all text files in order of their names in docLabels
    doc_files = [f for f in os.listdir('./data/') if f.endswith('.csv')]

    sources = {}
    for doc in doc_files:
        doc2 = os.path.join('./data',doc)
        sources[doc2] = doc.replace('.csv','')

    sentences = TaggedLineSentence(sources)


    # #iterator returned over all documents
    model = gensim.models.Doc2Vec(size=300, min_count=2, alpha=0.025, min_alpha=0.025)
    model.build_vocab(sentences)

    #training of model
    for epoch in range(10):
        #random.shuffle(sentences)
        print 'iteration '+str(epoch+1)
        #model.train(it)
        model.alpha -= 0.002
        model.min_alpha = model.alpha
        model.train(sentences.sentences_perm(),total_examples=model.corpus_count,epochs=model.iter)
    #saving the created model
    model.save('reddit.doc2vec')
    print ""model saved"" 

train()
</code></pre>
","1868436","","","","","2018-01-03 16:00:15","doc2vec/gensim - issue with shuffling sentences in the epochs","<python><word2vec><gensim><doc2vec>","1","0","2","","","CC BY-SA 3.0"
"42399565","1","42404566","","2017-02-22 18:33:14","","4","14574","<p>I'm training my own word2vec model using different data. To implement the resulting model into my classifier and compare the results with the original pre-trained Word2vec model I need to save the model in binary extension .bin. Here is my code, <em>sentences</em> is a list of short messages.</p>

<pre><code>import gensim, logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
sentences = gensim.models.word2vec.LineSentence('dati.txt')
model = gensim.models.Word2Vec(
sentences, size=300, window=5, min_count=5, workers=5,
sg=1, hs=1, negative=0
)
model.save_word2vec_format('model.bin', binary=True)
</code></pre>

<p>The last method, save_word2vec_format, gives me this error:</p>

<p><code>
AttributeError: 'Word2Vec' object has no attribute 'save_word2vec_format'
</code></p>

<p>What am I missing here? I've read the documentation of gensim and other forums. This <a href=""https://github.com/devmount/GermanWordEmbeddings/blob/c2b603a07d968146995ee9dde54a25fd0aa8586a/training.py#L56"" rel=""nofollow noreferrer"">repo on github</a> uses almost the same configuration so I cannot understand what's wrong. I've tried to switch from skipgram to cbow and from hierarchical softmax to negative sampling with no results.</p>

<p>Thank you in advance!</p>
","5834799","","","","","2018-12-26 20:37:42","Save gensim Word2vec model in binary format .bin with save_word2vec_format","<python><attributes><nlp><gensim><word2vec>","2","0","","","","CC BY-SA 3.0"
"42269313","1","","","2017-02-16 09:06:14","","20","9949","<p>First let's extract the TF-IDF scores per term per document:</p>

<pre><code>from gensim import corpora, models, similarities
documents = [""Human machine interface for lab abc computer applications"",
              ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
</code></pre>

<p>Printing it out:</p>

<pre><code>for doc in corpus_tfidf:
    print doc
</code></pre>

<p>[out]:</p>

<pre><code>[(0, 0.4301019571350565), (1, 0.4301019571350565), (2, 0.4301019571350565), (3, 0.4301019571350565), (4, 0.2944198962221451), (5, 0.2944198962221451), (6, 0.2944198962221451)]
[(4, 0.3726494271826947), (7, 0.27219160459794917), (8, 0.3726494271826947), (9, 0.27219160459794917), (10, 0.3726494271826947), (11, 0.5443832091958983), (12, 0.3726494271826947)]
[(6, 0.438482464916089), (7, 0.32027755044706185), (9, 0.32027755044706185), (13, 0.6405551008941237), (14, 0.438482464916089)]
[(5, 0.3449874408519962), (7, 0.5039733231394895), (14, 0.3449874408519962), (15, 0.5039733231394895), (16, 0.5039733231394895)]
[(9, 0.21953536176370683), (10, 0.30055933182961736), (12, 0.30055933182961736), (17, 0.43907072352741366), (18, 0.43907072352741366), (19, 0.43907072352741366), (20, 0.43907072352741366)]
[(21, 0.48507125007266594), (22, 0.48507125007266594), (23, 0.48507125007266594), (24, 0.48507125007266594), (25, 0.24253562503633297)]
[(25, 0.31622776601683794), (26, 0.31622776601683794), (27, 0.6324555320336759), (28, 0.6324555320336759)]
[(25, 0.20466057569885868), (26, 0.20466057569885868), (29, 0.2801947048062438), (30, 0.40932115139771735), (31, 0.40932115139771735), (32, 0.40932115139771735), (33, 0.40932115139771735), (34, 0.40932115139771735)]
[(8, 0.6282580468670046), (26, 0.45889394536615247), (29, 0.6282580468670046)]
</code></pre>

<p>If we want to find the ""saliency"" or ""importance"" of the words within this corpus, <strong>can we simple do the sum of the tf-idf scores across all documents and divide it by the number of documents?</strong> I.e. </p>

<pre><code>&gt;&gt;&gt; tfidf_saliency = Counter()
&gt;&gt;&gt; for doc in corpus_tfidf:
...     for word, score in doc:
...         tfidf_saliency[word] += score / len(corpus_tfidf)
... 
&gt;&gt;&gt; tfidf_saliency
Counter({7: 0.12182694202050007, 8: 0.11121194156107769, 26: 0.10886469856464989, 29: 0.10093919463036093, 9: 0.09022272408985754, 14: 0.08705221175200946, 25: 0.08482488519466996, 6: 0.08143359568202602, 10: 0.07480097322359022, 12: 0.07480097322359022, 4: 0.07411881371164887, 13: 0.07117278898823597, 5: 0.07104525967490458, 27: 0.07027283689263066, 28: 0.07027283689263066, 11: 0.060487023243988705, 15: 0.055997035904387725, 16: 0.055997035904387725, 21: 0.05389680556362955, 22: 0.05389680556362955, 23: 0.05389680556362955, 24: 0.05389680556362955, 17: 0.048785635947490406, 18: 0.048785635947490406, 19: 0.048785635947490406, 20: 0.048785635947490406, 0: 0.04778910634833961, 1: 0.04778910634833961, 2: 0.04778910634833961, 3: 0.04778910634833961, 30: 0.045480127933079706, 31: 0.045480127933079706, 32: 0.045480127933079706, 33: 0.045480127933079706, 34: 0.045480127933079706})
</code></pre>

<p>Looking at the output, could we assume that the most ""prominent"" word in the corpus is:</p>

<pre><code>&gt;&gt;&gt; dictionary[7]
u'system'
&gt;&gt;&gt; dictionary[8]
u'survey'
&gt;&gt;&gt; dictionary[26]
u'graph'
</code></pre>

<p>If so, <strong>what is the mathematical interpretation of the sum of TF-IDF scores of words across documents?</strong></p>
","610569","","","","","2019-04-08 15:59:32","Interpreting the sum of TF-IDF scores of words across documents","<python><statistics><nlp><tf-idf><gensim>","5","3","4","","","CC BY-SA 3.0"
"56681210","1","56695049","","2019-06-20 07:38:02","","2","867","<h1>Intro</h1>
<p>Currently I am trying to use dask in concert with gensim to do NLP document computation and I'm running into an issue when converting my corpus into a &quot;<a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument"" rel=""nofollow noreferrer"">TaggedDocument</a>&quot;.</p>
<p>Because I've tried so many different ways to wrangle this problem I'll list my attempts.</p>
<p>Each attempt at dealing with this problem is met with slightly different woes.</p>
<h1>First some initial givens.</h1>
<h2>The Data</h2>
<pre><code>df.info()
&lt;class 'dask.dataframe.core.DataFrame'&gt;
Columns: 5 entries, claim_no to litigation
dtypes: object(2), int64(3)
</code></pre>
<pre><code>  claim_no   claim_txt I                                    CL ICC lit
0 8697278-17 battery comprising interior battery active ele... 106 2 0
</code></pre>
<h2>Desired Output</h2>
<pre><code>&gt;&gt;tagged_document[0]
&gt;&gt;TaggedDocument(words=['battery', 'comprising', 'interior', 'battery', 'active', 'elements', 'battery', 'cell', 'casing', 'said', 'cell', 'casing', 'comprising', 'first', 'casing', 'element', 'first', 'contact', 'surface', 'second', 'casing', 'element', 'second', 'contact', 'surface', 'wherein', 'assembled', 'position', 'first', 'second', 'contact', 'surfaces', 'contact', 'first', 'second', 'casing', 'elements', 'encase', 'active', 'materials', 'battery', 'cell', 'interior', 'space', 'wherein', 'least', 'one', 'gas', 'tight', 'seal', 'layer', 'arranged', 'first', 'second', 'contact', 'surfaces', 'seal', 'interior', 'space', 'characterized', 'one', 'first', 'second', 'contact', 'surfaces', 'comprises', 'electrically', 'insulating', 'void', 'volume', 'layer', 'first', 'second', 'contact', 'surfaces', 'comprises', 'formable', 'material', 'layer', 'fills', 'voids', 'surface', 'void', 'volume', 'layer', 'hermetically', 'assembled', 'position', 'form', 'seal', 'layer'], tags=['8697278-17'])
&gt;&gt;len(tagged_document) == len(df['claim_txt'])
</code></pre>
<h1>Error Number 1 No Generators Allowed</h1>
<pre><code>def read_corpus_tag_sub(df,corp='claim_txt',tags=['claim_no']):
    for i, line in enumerate(df[corp]):
        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), (list(df.loc[i,tags].values)))

tagged_document = df.map_partitions(read_corpus_tag_sub,meta=TaggedDocument)
tagged_document = tagged_document.compute()
</code></pre>
<p>TypeError: Could not serialize object of type generator.</p>
<p>I found no way of getting around this while still using a generator. A fix for this would be great! As this works perfectly fine for regular pandas.</p>
<h1>Error Number 2 Only the first element of each partition</h1>
<pre><code>def read_corpus_tag_sub(df,corp='claim_txt',tags=['claim_no']):
    for i, line in enumerate(df[corp]):
        return gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), (list(df.loc[i,tags].values)))

tagged_document = df.map_partitions(read_corpus_tag_sub,meta=TaggedDocument)
tagged_document = tagged_document.compute()
</code></pre>
<p>This one is a bit dumb as the function won't iterate (I know) but gives the desired format, but only returns the first row in each partition.</p>
<h1>Error Number 3 function call hangs with 100% cpu</h1>
<pre><code>def read_corpus_tag_sub(df,corp='claim_txt',tags=['claim_no']):
    tagged_list = []
    for i, line in enumerate(df[corp]):
        tagged = gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), (list(df.loc[i,tags].values)))
        tagged_list.append(tagged)
    return tagged_list
</code></pre>
<p>Near as I can tell when refactoring the return outside the loop this function hangs builds memory in the dask client and my CPU utilization goes to 100% but no tasks are being computed. Keep in mind I'm calling the function the same way.</p>
<h1>Pandas Solution</h1>
<pre><code>def tag_corp(corp,tag):
    return gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(corp), ([tag]))

tagged_document = [tag_corp(x,y) for x,y in list(zip(df_smple['claim_txt'],df_smple['claim_no']))]
</code></pre>
<p>List comp I haven't time tested this solution</p>
<h1>Other Pandas Solution</h1>
<pre><code>tagged_document = list(read_corpus_tag_sub(df))
</code></pre>
<p>This solution will chug along pretty much for hours. However I don't have enough memory to juggle this thing when it's done.</p>
<h1>Conclusion(?)</h1>
<p>I feel Super lost right now. Here is a list of threads I've looked at. I admit to being really new to dask I've just spent so much time and I feel like I'm on a fools errand.</p>
<ol>
<li><a href=""https://stackoverflow.com/questions/50862165/creating-a-dask-bag-from-a-generator"">Dask Bag from generator</a></li>
<li><a href=""https://medium.com/mindorks/speeding-up-text-pre-processing-using-dask-45cc3ede1366"" rel=""nofollow noreferrer"">Processing Text With Dask</a></li>
<li><a href=""https://gdcoder.com/speed-up-pandas-apply-function-using-dask-or-swifter-tutorial/"" rel=""nofollow noreferrer"">Speed up Pandas apply using Dask</a></li>
<li><a href=""https://stackoverflow.com/questions/45545110/how-do-you-parallelize-apply-on-pandas-dataframes-making-use-of-all-cores-on-o"">How do you parallelize apply() on Pandas Dataframes making use of all cores on one machine?</a></li>
<li><a href=""https://stackoverflow.com/questions/31361721/python-dask-dataframe-support-for-trivially-parallelizable-row-apply"">python dask DataFrame, support for (trivially parallelizable) row apply?</a></li>
<li><a href=""https://stackoverflow.com/questions/39215617/what-is-map-partitions-doing"">What is map_partitions doing?</a></li>
<li><a href=""https://stackoverflow.com/questions/47125665/simple-dask-map-partitions-example"">simple dask map_partitions example</a></li>
<li><a href=""https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions"" rel=""nofollow noreferrer"">The Docs</a></li>
</ol>
","1985173","","-1","","2020-06-20 09:12:55","2019-06-21 18:31:48","Convert a column in a dask dataframe to a TaggedDocument for Doc2Vec","<python><dask><gensim><doc2vec>","2","0","1","","","CC BY-SA 4.0"
"51958011","1","","","2018-08-21 23:30:50","","0","693","<p>Here is my use case: </p>

<p>HR department provide <code>job description</code>(free text) and set of <code>resumes</code>(plain text), and the ask is to come up with salience score based on job description relevance.</p>

<p>The <code>job description</code> consists of skills required and minimum qualification. I was considering <strong>Doc2Vec</strong> but bit confused <strong>how should I train the model</strong>? </p>

<ol>
<li>If I collate all job descriptions, and create corpus, querying profile text will give incorrect results. </li>
<li>Moreover, job requisitions are transient in nature and a profile might match with a expired requisition.</li>
</ol>

<p>Since each job description is exclusive, shall I create a trained model for each job? Or if there's any better framework, please advise.</p>

<p>Please see the code below:</p>

<pre><code>'
import os
from gensim import corpora, models, similarities

path = &lt;working_dir&gt;

os.chdir(path)

with open('Dir int Strategy.txt') as f:
    job_desc_text = f.read()

with open('Jeannine.txt') as f:
    candidate1_text = f.read()

with open('Penny.txt') as t:
    candidate2_text = t.read()

with open('Omar.txt') as z:
    candidate3_text = z.read()


with open('Kyle.txt') as p:
    candidate4_text = p.read()

documents = [candidate1_text, candidate2_text, candidate3_text,candidate4_text]   
stoplist = set('for a of the and to in'.split())   
documents_split_texts = [[word for word in document.lower().split() if  word not in stoplist]
    for document in documents]   
dictionary = corpora.Dictionary(documents_split_texts)   
dictionary.save('/tmp/deerwester.dict')        

corpus = [dictionary.doc2bow(text) for text in documents_split_texts]   
corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)     

tfidf = models.TfidfModel(corpus)      

query_vector = job_desc_text    
query_vector = dictionary.doc2bow(query_vector.lower().split())

index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=699)     

sims = index[tfidf[query_vector]]            
'
</code></pre>
","2087408","","2087408","","2018-08-27 18:48:07","2018-08-27 18:48:07","Using Doc2Vec to find salience score for resumes based on job description","<nlp><gensim><doc2vec><information-extraction>","1","2","1","","","CC BY-SA 4.0"
"46270606","1","","","2017-09-18 02:00:01","","0","647","<p>I'm trying to calculate the cosine similarity between all the values.</p>

<p>The time for 1000*20000 calculations cost me more than 10 mins.</p>

<p>Code:</p>

<pre><code>from gensim import matutils
# array_A contains 1,000 TF-IDF values
# array_B contains 20,000 TF-IDF values 
for x in array_A:
   for y in array_B:
      matutils.cossim(x,y)
</code></pre>

<p>It's necessary to using gensim package to get the tf-idf value and similarity calculation.</p>

<p>Can someone please give me some advice and guidance to speed up time? </p>
","8000414","","365102","","2017-09-18 02:06:24","2019-10-24 11:26:04","How to speed up time when calculate cosine similarity using nested loops in python","<python><gensim><cosine-similarity>","3","1","","","","CC BY-SA 3.0"
"52012579","1","52030823","","2018-08-24 22:49:37","","0","494","<p>Instead of having construct from one single document ('mycorpus.txt'), How can i frame dictionary from multiple documents (Each one going to be 25 MB in file size with 10,000 Files) and please be aware that i am trying ""to construct the dictionary without loading all texts into memory"" via gensim</p>

<pre><code>&gt;&gt;&gt; from gensim import corpora
&gt;&gt;&gt; from six import iteritems
&gt;&gt;&gt; dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))
&gt;&gt;&gt; stop_ids = [dictionary.token2id[stopword] for stopword in stoplist
&gt;&gt;&gt;             if stopword in dictionary.token2id]
&gt;&gt;&gt; once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]
&gt;&gt;&gt; dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once
&gt;&gt;&gt; dictionary.compactify()  # remove gaps in id sequence after words that were removed
&gt;&gt;&gt; print(dictionary)
</code></pre>
","1531248","","","","","2018-08-26 22:51:07","Gensim Contruct the dictionary without loading all texts into memory gensim","<python><bigdata><gensim>","1","0","1","","","CC BY-SA 4.0"
"56625109","1","","","2019-06-17 05:20:10","","0","72","<p>I have trained Gensim's WordToVec on a text corpus,converted it to DocToVec and then used cosine similarity to find the similarity between documents. I need to suggest similar documents. Now suppose among the top 5 suggestions for a particular document, we manually find that 3 of them are not similar.Can this feedback be incorporated in retraining the model?</p>
","8458745","","","","","2019-06-17 18:04:39","Incorporating feedback to retrain WordToVec for finding document similarity","<machine-learning><deep-learning><gensim><cosine-similarity>","1","0","1","","","CC BY-SA 4.0"
"51900688","1","","","2018-08-17 17:54:43","","1","975","<p>Spacy has great parsing capacities and it's API is very intuitive for the most part. Is there any way from the Spacy API to fine tune its word embedding models? In particular, I would like to keep Spacy's tokens and give them a vector when possible.</p>

<p>The only thing I've come across for now is to train the embeddings using gensim (but then I wouldn't know how to load the embeddings from spacy to gensim) and then load then back to spacy, as in: <a href=""https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/"" rel=""nofollow noreferrer"">https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/</a>. This doesn't help for the first part: training on spacy tokens.</p>

<p>Any help appreciated.</p>
","5173924","","5173924","","2018-09-14 21:23:59","2018-09-14 21:23:59","Fine tune spaCy's word embeddings","<python-3.x><gensim><spacy>","1","0","","","","CC BY-SA 4.0"
"42382207","1","42384737","","2017-02-22 03:32:50","","0","1396","<p>The <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">Word2Vec</a> object in <code>gensim</code> has a <code>null_word</code> parameter that isn't explained in the docs. </p>

<blockquote>
  <p>class gensim.models.word2vec.Word2Vec(sentences=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000)</p>
</blockquote>

<p><strong>What is the <code>null_word</code> parameter used for?</strong></p>

<p>Checking the code at <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L680"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L680</a>, it states:</p>

<pre><code>    if self.null_word:
        # create null pseudo-word for padding when using concatenative L1 (run-of-words)
        # this word is only ever input ‚Äì never predicted ‚Äì so count, huffman-point, etc doesn't matter
        word, v = '\0', Vocab(count=1, sample_int=0)
        v.index = len(self.wv.vocab)
        self.wv.index2word.append(word)
        self.wv.vocab[word] = v
</code></pre>

<p><strong>What is ""concatenative L1""?</strong></p>
","7527506","","","","","2017-02-22 07:02:14","What is the `null_word` parameter in gensim Word2Vec?","<python><null><deep-learning><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"52009779","1","52011766","","2018-08-24 18:11:54","","0","127","<p>I found it is not explicit in usage  </p>

<pre><code>from gensim.models import Word2Vec
sentences = [[""cat"", ""say"", ""meow""], [""dog"", ""say"", ""woof""]]

model = Word2Vec(min_count=1)
model.build_vocab(sentences)  # prepare the model vocabulary
model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors
(1, 30)
</code></pre>

<p>the sentences whether should contain the old corpus?</p>
","8026780","","130288","","2018-08-24 20:59:18","2018-08-24 21:10:06","Is it necessary to mix old corpus and new corpus in updating word2vec model?","<gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"63633739","1","","","2020-08-28 12:24:03","","0","577","<p>I 've been some trouble updating a model from gensim. I use the following command to create the model.</p>
<p><code>model = gensim.models.Word2Vec(sentences,size=100, window=20, min_count=10, workers=24, iter=200, callbacks=[epoch_saver])</code>
and to save the model, I used:</p>
<p><code>model.save(type+&quot;/&quot;+&quot;word2vec_&quot;+name+&quot;_&quot;+type+&quot;.&quot;+version)</code></p>
<p>As far as I remember, the first step, when training a model, is to create a vocab. Anyway, I had to stop the training on iter=147. So, now I want to load and continue training. This is how I load the model:</p>
<p><code>model = gensim.models.Word2Vec.load(&quot;encoded_op_op/temporary_model/word2vec.model&quot;)</code></p>
<p>But how do I use train() method to continue update ? I am trying :</p>
<p><code>model = model.train(sentences, epochs=53, callbacks=[epoch_saver])</code></p>
<p>but it gives an error:</p>
<blockquote>
<p>You must specify either total_examples or total_words, for proper job parameters updationand progress
calculations. The usual value is total_examples=model.corpus_count.</p>
</blockquote>
<p>Anyway, where could I define the same parameters used when creating the model: size=100, window=20, min_count=10, workers=24. Ok. i believe size is already defined. but what about workers ?</p>
<p>Thanks in advance</p>
","7088477","","","","","2020-08-28 18:34:26","How to update a trained Word2vec model in gensim with same parameters","<parameters><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"42368713","1","","","2017-02-21 13:31:26","","3","112","<p>I am using word2vec from Gensim and I am feeding sentences to the model with the following iterator:</p>

<pre><code>class SentencesIterator(object):
    def __init__(self, source):
        self.source = source

        if os.path.isdir(self.source):
            self.type_source = 'dir'
        else:
            self.type_source = 'file'

    def __iter__(self):
        if self.type_source == 'dir':
            for fname in os.listdir(self.source):
                with open(os.path.join(self.source, fname)) as f:
                    for line in f:
                        yield line.split()
        else:
            with open(self.source) as f:
                    for line in f:
                        yield line.split()
</code></pre>

<p>Everything is working as expected but I noticed that the performance varies a lot depending on the type of input.</p>

<p>For a single file in input:
<code>INFO : PROGRESS: at 5.43% examples, 73483 words/s</code></p>

<p>For a directory with 1 file: <code>INFO : PROGRESS: at 17.09% examples, 71716 words/s</code></p>

<p>For a directory with 2 files: <code>INFO : PROGRESS: at 11.62% examples, 67678 words/s</code></p>

<p>For a directory with 30 files: <code>INFO : PROGRESS: at 1.19% examples, 54004 words/s</code></p>

<p>I don't understand the decrease in the speed of streaming. To me, all the operations are identical, it is just about opening a file and reading it line by line..</p>

<p>PS: I tried with 4 cores and 1 core and the same behavior is observed!</p>
","5236675","","481584","","2017-02-22 05:17:06","2017-03-04 11:22:47","Performance of data streaming from file vs directory of files","<python><python-3.x><io><gensim>","1","2","","","","CC BY-SA 3.0"
"51985536","1","51985852","","2018-08-23 12:11:20","","1","1438","<p>I am using gensim's doc2vec implementation and I have a few thousand documents tagged with four labels.</p>

<pre><code>yield TaggedDocument(text_tokens, [labels])
</code></pre>

<p>I'm training a Doc2Vec model with a list of these <strong>TaggedDocument</strong>s. However, I'm not sure how to infer the tag for a document that was not seen during training. I see that there is a infer_vector method which returns the embedding vector. But how can I get the most likely label from that?</p>

<p>An idea would be to infer the vectors for every label that I have and then calculate the cosine similarity between these vectors and the vector for the new document I want to classify. Is this the way to go? If so, how can I get the vectors for each of my four labels?</p>
","3827381","","3827381","","2018-08-23 12:18:34","2018-08-23 19:11:05","gensim doc2vec - How to infer label","<python><nlp><gensim><doc2vec>","2","0","1","","","CC BY-SA 4.0"
"52077016","1","52120987","","2018-08-29 12:04:58","","2","3913","<p>I created an LDA model for some text files using gensim package in python. I want to get topic's distributions for the learned model. Is there any method in gensim ldamodel class or a solution to get topic's distributions from the model?
For example, I use the coherence model to find a model with the best cohrence value subject to the number of topics in range 1 to 5. After getting the best model I use get_document_topics method (thanks <a href=""https://stackoverflow.com/users/6256482/kenhbs"">kenhbs</a>) to get topic distribution in the document that used for creating the model.</p>

<pre><code>id2word = corpora.Dictionary(doc_terms)

bow = id2word.doc2bow(doc_terms)

max_coherence = -1
best_lda_model = None

for num_topics in range(1, 6):

    lda_model = gensim.models.ldamodel.LdaModel(corpus=bow, num_topics=num_topics)

    coherence_model = gensim.models.CoherenceModel(model=lda_model, texts=doc_terms,dictionary=id2word)

    coherence_value = coherence_model.get_coherence()

    if coherence_value &gt; max_coherence:
        max_coherence = coherence_value
        best_lda_model = lda_model
</code></pre>

<p>The best has 4 topics</p>

<pre><code>print(best_lda_model.num_topics)

4
</code></pre>

<p>But when I use get_document_topics, I get less than 4 values for document distribution.</p>

<pre><code>topic_ditrs = best_lda_model.get_document_topics(bow)

print(len(topic_ditrs))

3
</code></pre>

<p>My question is: For best lda model with 4 topics (using coherence model) for a document, why get_document_topics returns fewer topics for the same document? why some topics have very small distribution (less than 1-e8)?</p>
","10288731","","10288731","","2018-09-03 10:35:00","2019-12-31 11:21:32","Extracting Topic distribution from gensim LDA model","<gensim><lda><topic-modeling>","3","0","1","","","CC BY-SA 4.0"
"42372346","1","42374263","","2017-02-21 16:07:53","","0","772","<p>My code is running out of memory because of the question I asked in <a href=""https://groups.google.com/forum/#!topic/gensim/g19d2xXJ9VY"" rel=""nofollow noreferrer"">this page</a>. Then, I wrote the second code to have an iterable <code>alldocs</code>, not an all-in-memory <code>alldocs</code>. I changed my code based on the explanation of <a href=""https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/"" rel=""nofollow noreferrer"">this page</a>. I am not familiar with stream concept and I could not solve the error I got.</p>

<p>This code read all files of all folders of given path.The context of each file is consist of a document name and its context in two lines.For instance:</p>

<blockquote>
  <p>clueweb09-en0010-07-00000</p>
  
  <p>dove   gif clipart pigeon  clip    art picture image   hiox    free    birds   india   web icons   clipart add stumble upon    </p>
  
  <p>clueweb09-en0010-07-00001</p>
  
  <p>google bookmarks   yahoo   bookmarks   php script  java    script  jsp script  licensed    scripts html    tutorials   css tutorials</p>
</blockquote>

<p>First code:</p>

<pre><code># coding: utf-8
 import string
 import nltk
 import nltk.tokenize 
 from nltk.corpus import stopwords
 import re
 import os, sys 

 import MySQLRepository

 from gensim import utils
 from gensim.models.doc2vec import Doc2Vec
 import gensim.models.doc2vec
 from gensim.models.doc2vec import LabeledSentence
 from boto.emr.emrobject import KeyValue


 def readAllFiles(path):
    dirs = os.listdir( path )
    for file in dirs:
        if os.path.isfile(path+""/""+file):
           prepareDoc2VecSetting(path+'/'+file)
       else:
           pf=path+""/""+file
           readAllFiles(pf)      

def prepareDoc2VecSetting (fname):
    mapDocName_Id=[]
    keyValues=set()
   with open(fname) as alldata:
        a= alldata.readlines()
        end=len(a)
        label=0
        tokens=[]
        for i in range(0,end):
            if a[i].startswith('clueweb09-en00'):
               mapDocName_Id.insert(label,a[i])
               label=label+1
               alldocs.append(LabeledSentence(tokens[:],[label]))
               keyValues |= set(tokens)
               tokens=[]
           else:
               tokens=tokens+a[i].split()  

   mydb.insertkeyValueData(keyValues) 

   mydb.insertDocId(mapDocName_Id)


   mydb=MySQLRepository.MySQLRepository()

  alldocs = [] 
  pth='/home/flr/Desktop/newInput/tokens'
  readAllFiles(ipth)

  model = Doc2Vec(alldocs, size = 300, window = 5, min_count = 2, workers = 4)
  model.save(pth+'/my_model.doc2vec')
</code></pre>

<p>Second code:(I did not consider parts related to DB)</p>

<pre><code>import gensim
import os


from gensim.models.doc2vec import Doc2Vec
import gensim.models.doc2vec
from gensim.models.doc2vec import LabeledSentence



class prepareAllDocs(object):

    def __init__(self, top_dir):
        self.top_dir = top_dir

    def __iter__(self):
    mapDocName_Id=[]
    label=1
    for root, dirs, files in os.walk(top_directory):
        for fname in files:
            print fname
            inputs=[]
            tokens=[]
            with open(os.path.join(root, fname)) as f:
                for i, line in enumerate(f):          
                    if line.startswith('clueweb09-en00'):
                        mapDocName_Id.append(line)
                        if tokens:
                            yield LabeledSentence(tokens[:],[label])
                            label+=1
                            tokens=[]
                    else:
                        tokens=tokens+line.split()
                yield LabeledSentence(tokens[:],[label])

pth='/home/flashkar/Desktop/newInput/tokens/'
allDocs = prepareAllDocs('/home/flashkar/Desktop/newInput/tokens/')
for doc in allDocs:
    model = Doc2Vec(allDocs, size = 300, window = 5, min_count = 2, workers = 4)
model.save(pth+'/my_model.doc2vec')
</code></pre>

<p>This is the error:</p>

<blockquote>
  <p>Traceback (most recent call last):   File
  ""/home/flashkar/git/doc2vec_annoy/Doc2Vec_Annoy/KNN/testiterator.py"",
  line 44, in 
      model = Doc2Vec(allDocs, size = 300, window = 5, min_count = 2, >workers = 4)   File
  ""/home/flashkar/anaconda/lib/python2.7/site->packages/gensim/models/doc2vec.py"",
  line 618, in <strong>init</strong>
      self.build_vocab(documents, trim_rule=trim_rule)   File >""/home/flashkar/anaconda/lib/python2.7/site->packages/gensim/models/word2vec.py"",
  line 523, in build_vocab
      self.scan_vocab(sentences, progress_per=progress_per, >trim_rule=trim_rule)  # initial survey   File
  ""/home/flashkar/anaconda/lib/python2.7/site->packages/gensim/models/doc2vec.py"",
  line 655, in scan_vocab
      for document_no, document in enumerate(documents):   File >""/home/flashkar/git/doc2vec_annoy/Doc2Vec_Annoy/KNN/testiterator.py"",
  line 40, in <strong>iter</strong>
      yield LabeledSentence(tokens[:],tpl<a href=""https://groups.google.com/forum/#!topic/gensim/g19d2xXJ9VY"" rel=""nofollow noreferrer"">1</a>) IndexError: list index out of range</p>
</blockquote>
","3092781","","3092781","","2017-02-23 17:52:43","2017-02-23 17:52:43","How build Doc2Vec model by useing an 'iterable' object","<python><iterator><gensim><doc2vec>","1","1","","","","CC BY-SA 3.0"
"42376652","1","","","2017-02-21 19:51:13","","1","4745","<p>I tried the three default-options for alpha in gensim's lda implementation and now wonder about the result:
The sum of topic-probabilities over all documents is smaller than the number of documents in the corpus (see below). For example alpha = 'symmetric' yields about 9357 as sum of topic-probabilities, however, the number of topics is 9459. Could one tell me the reason for this unexpected result?</p>

<pre><code>alpha = symmetric
nr_of_docs = 9459
sum_of_topic_probs = 9357.12285605

alpha = asymmetric
nr_of_docs = 9459
sum_of_topic_probs = 9375.29253851

alpha = auto
nr_of_docs = 9459
sum_of_topic_probs = 9396.40123459
</code></pre>
","7540092","","5101926","","2018-12-13 20:03:15","2019-03-23 23:30:44","Gensim LDA alpha-parameter","<gensim><lda>","2","0","1","","","CC BY-SA 4.0"
"42468394","1","","","2017-02-26 12:19:01","","0","448","<p>I'm using PyCharm and loading models trained on words using Word2Vec. I tried to check the similarity between two words, but I get this error :</p>

<pre><code># Loading model trained on words
    model = word2vec.Word2Vec.load('models/text8.model')

    # Loading model enhanced with phrases (2-grams)
    model_phrase = word2vec.Word2Vec.load('models/text8.phrase.model')

    # Words that are similar are close in the sense of the cosine similarity.
    sim = model.similarity('woman', 'man')
    print 'Printing word similarity between ""woman"" and ""man"" : {0}'.format(sim)

Traceback (most recent call last):
File ""C:\Program Files (x86)\JetBrains\PyCharm 2016.3.2\helpers\pydev\pydevd.py"", line 1596, in &lt;module&gt;
globals = debugger.run(setup['file'], None, None, is_module)
File ""C:\Program Files (x86)\JetBrains\PyCharm 2016.3.2\helpers\pydev\pydevd.py"", line 974, in run
pydev_imports.execfile(file, globals, locals)  # execute the script
File ""C:/Users/XXX/Desktop/code/word2vec/embedding_word2vec_students.py"", line 144, in &lt;module&gt;
sim = model.similarity('woman', 'man')
File ""C:\Users\XXX\Anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 1194, in similarity
return self.wv.similarity(w1, w2)
File ""C:\Users\XXX\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 587, in similarity
return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2]))
File ""C:\Users\XXX\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 567, in __getitem__
return self.word_vec(words)
File ""C:\Users\XXX\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 271, in word_vec
return self.syn0[self.vocab[word].index]
IndexError: list index out of range
</code></pre>

<p>When I debug, it seems that the problem comes from this line in the function word_vec :</p>

<pre><code>return self.syn0[self.vocab[word].index]
</code></pre>

<p>However I have no clue why I'm getting this. Thank you very much in advance if you can help me with this.</p>
","7622015","","","","","2017-02-27 12:21:20","Word2Vec similarity function not working","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"52080365","1","56927701","","2018-08-29 14:47:33","","8","4497","<p>I have downloaded a <code>.bin</code> FastText model, and I use it with <code>gensim</code> as follows:</p>

<pre><code>model = FastText.load_fasttext_format(""cc.fr.300.bin"")
</code></pre>

<p>I would like to continue the training of the model to adapt it to my domain. After checking <a href=""https://github.com/facebookresearch/fastText/pull/423"" rel=""noreferrer"">FastText's Github</a> and the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/fasttext.py#L685"" rel=""noreferrer"">Gensim documentation</a> it seems like it is <strong>not</strong> currently feasible appart from using this person's proposed <a href=""https://github.com/ericxsun/fastText"" rel=""noreferrer"">modification</a> (not yet merged). </p>

<p>Am I missing something?</p>
","3867406","","","","","2019-07-08 02:55:06","Continue training a FastText model","<python><gensim><fasttext>","2","0","4","","","CC BY-SA 4.0"
"42459373","1","42482358","","2017-02-25 17:38:06","","8","3761","<p>Loading the complete pre-trained word2vec model by <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"" rel=""noreferrer"">Google</a> is time intensive and tedious, therefore I was wondering if there is a chance to remove words below a certain frequency to bring the <code>vocab</code> count down to e.g. 200k words.</p>

<p>I found Word2Vec methods in the <code>gensim</code> package to determine the word frequency and to re-save the model again, but I am not sure how to <code>pop</code>/<code>remove</code> vocab from the pre-trained model before saving it again. I couldn't find any hint in the <code>KeyedVector class</code> and the <code>Word2Vec class</code> for such an operation?</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py</a>
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py</a></p>

<p><strong>How can I select a subset of the vocabulary of the pre-trained word2vec model?</strong></p>
","729241","","","","","2018-02-16 05:43:22","Reduce Google's Word2Vec model with Gensim","<nlp><gensim><word2vec>","2","1","2","","","CC BY-SA 3.0"
"52061585","1","52067729","","2018-08-28 15:33:05","","0","72","<p>I am trying to train the <code>Gensim Word2Vec</code> model by:</p>

<pre><code>X = train['text']    

model_word2vec = models.Word2Vec(X.values, size=150)
model_word2vec.train(X.values, total_examples=len(X.values), epochs=10)
</code></pre>

<p>after the training, I get a small vocabulary (<code>model_word2vec.wv.vocab</code>) of length <code>74</code> containing only the alphabet's letters.</p>

<p>How could I get the right vocabulary?</p>

<p><strong>Update</strong></p>

<p>I tried this before:</p>

<pre><code>tokenizer = Tokenizer(lower=True)
tokenized_text = tokenizer.fit_on_texts(X)
sequence = tokenizer.texts_to_sequences(X)

model_word2vec.train(sequence, total_examples=len(X.values), epochs=10
</code></pre>

<p>but I got the same wrong vocabulary size.</p>
","7387749","","7387749","","2018-08-29 01:37:35","2018-08-29 01:37:35","Wrong length for Gensim Word2Vec's vocabulary","<nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"52072855","1","","","2018-08-29 08:26:50","","2","685","<p>I found a warning about <code>word2vec.similarity()</code> as follows:</p>

<pre><code>&gt;d:\python\lib\site-packages\gensim\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.
  if np.issubdtype(vec.dtype, np.int):
</code></pre>

<p>what can I do to remove this warning?</p>
","10288763","","6048114","","2018-08-29 08:31:29","2019-04-11 07:36:17","how to remove gensim warning about use Word2vec gensim\matutils.py:737","<python><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"42493143","1","","","2017-02-27 18:26:15","","1","156","<p>I wrote the following code to build a Doc2vec Model iteratively. As I read in <a href=""https://groups.google.com/forum/#!topic/gensim/4xPzZiemH1k"" rel=""nofollow noreferrer"">this page</a>, if the number of tokens is more than 10000 in  a document then we need to split tokens and repeat the label(s) for each segment. </p>

<p>The length of tokens is more than 10000 for most of my documents.I try to split my tokens by writing the following code.But I got error which shows the tokens after 10000 is not considered in my model. </p>

<pre><code>    def iter_documents(top_directory):
        mapDocName_Id=[]
        label=1
        for root, dirs, files in os.walk(top_directory):
            for fname in files:
                print fname
                inputs=[]
                tokens=[]
                with open(os.path.join(root, fname)) as f:
                    for i, line in enumerate(f):          
                        if line.startswith('clueweb09-en00'):
                            if tokens:
                                i=0
                                if len(tokens)&lt;10000:
                                    yield LabeledSentence(tokens[:],[label])
                                else:
                                    tLen=len (tokens)
                                    times= int(math.floor(tLen/10000))
                                    for i in range(0,times):
                                        s=i*10000
                                        e=(i*10000)+9999
                                        yield LabeledSentence(tokens[s:e],[label])
                                    start=times*10000
                                    yield LabeledSentence(tokens[start:tLen],[label])
                                label+=1
                                tokens=[]
                        else:
                            tokens=tokens+line.split()
                    yield LabeledSentence(tokens[:],[label])
class docIterator(object):
    def __init__(self,top_directory):
       self.top_directory = top_directory

    def __iter__(self):
       return iter_documents(self.top_directory)

allDocs = docIterator(inputPath)

model = Doc2Vec(allDocs, size = 300, window = 5, min_count = 2, workers = 4)
model.save('my_model.doc2vec')
</code></pre>

<p>I test my model with the following code then I got this error:</p>

<pre><code>model= Doc2Vec.load('my_model.doc2vec')

#print model['school']
print model['philadelphia']
</code></pre>

<p>I got a vector as result of school but I got this error for philadelphia. philadelphia is in tokens after index 10000. </p>

<pre><code>2017-02-27 13:59:36,751 : INFO : loading Doc2Vec object from /home/fl/Desktop/newInput/tokens/my_model.doc2vec

2017-02-27 13:59:36,765 : INFO : loading docvecs recursively from /home/fl/Desktop/newInput/tokens/my_model.doc2vec.docvecs.* with mmap=None

2017-02-27 13:59:36,765 : INFO : setting ignored attribute syn0norm to None

2017-02-27 13:59:36,765 : INFO : setting ignored attribute cum_table to None
Traceback (most recent call last): 
File ""/home/fl/git/doc2vec_annoy/Doc2Vec_Annoy/KNN/CreateAnnoyIndex.py"",
line 31, in &lt;module&gt;
     print model['philadelphia']   File ""/home/flashkar/anaconda/lib/python2.7/site-packages/gensim/models/word2vec.py"",
 line 1504, in __getitem__  
return self.syn0[self.vocab[words].index] 
KeyError: 'philadelphia'
</code></pre>
","3092781","","","","","2017-03-10 04:21:24","words dos not exist in a doc2vec model when a document is added iteratively to the model","<gensim><doc2vec>","1","3","1","","","CC BY-SA 3.0"
"52038651","1","52067942","","2018-08-27 11:48:18","","6","5012","<p>What can cause loss from <code>model.get_latest_training_loss()</code> increase on each epoch?  </p>

<p>Code, used for training: </p>

<pre><code>class EpochSaver(CallbackAny2Vec):
    '''Callback to save model after each epoch and show training parameters '''

    def __init__(self, savedir):
        self.savedir = savedir
        self.epoch = 0

        os.makedirs(self.savedir, exist_ok=True)

    def on_epoch_end(self, model):
        savepath = os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch))
        model.save(savepath)
        print(
            ""Epoch saved: {}"".format(self.epoch + 1),
            ""Start next epoch ... "", sep=""\n""
            )
        if os.path.isfile(os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch - 1))):
            print(""Previous model deleted "")
            os.remove(os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch - 1))) 
        self.epoch += 1
        print(""Model loss:"", model.get_latest_training_loss())

    def train():

        ### Initialize model ###
        print(""Start training Word2Vec model"")

        workers = multiprocessing.cpu_count()/2

        model = Word2Vec(
            DocIter(),
            size=300, alpha=0.03, min_alpha=0.00025, iter=20,
            min_count=10, hs=0, negative=10, workers=workers,
            window=10, callbacks=[EpochSaver(""./checkpoints"")], 
            compute_loss=True
    )     
</code></pre>

<p>Output: </p>

<p>Losses from epochs (1 to 20): </p>

<pre><code>Model loss: 745896.8125
Model loss: 1403872.0
Model loss: 2022238.875
Model loss: 2552509.0
Model loss: 3065454.0
Model loss: 3549122.0
Model loss: 4096209.75
Model loss: 4615430.0
Model loss: 5103492.5
Model loss: 5570137.5
Model loss: 5955891.0
Model loss: 6395258.0
Model loss: 6845765.0
Model loss: 7260698.5
Model loss: 7712688.0
Model loss: 8144109.5
Model loss: 8542560.0
Model loss: 8903244.0
Model loss: 9280568.0
Model loss: 9676936.0
</code></pre>

<p>What am I doing wrong?</p>

<p>Language arabian. 
As input from DocIter - list with tokens. </p>
","5975112","","7256554","","2019-12-19 00:06:53","2020-11-18 14:07:12","Loss does not decrease during training (Word2Vec, Gensim)","<python><gensim><word2vec><loss>","2","1","2","","","CC BY-SA 4.0"
"63740186","1","","","2020-09-04 11:20:58","","0","172","<p>I have trained FastText model for french language using Gensim library.
Suddenly, this trained model is not getting loaded into memory.</p>
<p>I am using below code :-</p>
<pre><code>from gensim.models import FastText
fname = &quot;filename&quot;
model = FastText.load(fname)
</code></pre>
<p>and it throws following error : -</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.7/site-packages/gensim/models/fasttext.py&quot;, line 1070, in load
    model = super(FastText, cls).load(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
  File &quot;/usr/local/lib/python3.7/site-packages/gensim/utils.py&quot;, line 426, in load
    obj = unpickle(fname)
  File &quot;/usr/local/lib/python3.7/site-packages/gensim/utils.py&quot;, line 1384, in unpickle
    return _pickle.load(f, encoding='latin1')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x86 in position 14072054: invalid start byte
</code></pre>
<p>As this model is trained on large dataset, is there any way to recover/load this model?</p>
","7434281","","","","","2020-09-04 11:25:43","Unable to load gensim Fasttext model - UTF-8 unicode error","<python><encoding><pickle><gensim><fasttext>","1","2","","","","CC BY-SA 4.0"
"52102961","1","","","2018-08-30 18:01:55","","9","6900","<p>I have a question around measuring/calculating topic coherence for LDA models built in scikit-learn. </p>

<p>Topic Coherence is a useful metric for measuring the human interpretability of a given LDA topic model. Gensim's <a href=""https://radimrehurek.com/gensim/models/coherencemodel.html"" rel=""noreferrer"">CoherenceModel</a> allows Topic Coherence to be calculated for a given LDA model (several variants are included). </p>

<p>I am interested in leveraging <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"" rel=""noreferrer"">scikit-learn's LDA</a> rather than <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""noreferrer"">gensim's LDA</a> for ease of use and documentation (<em>note: I would like to avoid using the gensim to scikit-learn wrapper i.e. actually leverage sklearn‚Äôs LDA</em>). From my research, there is seemingly no scikit-learn equivalent to Gensim‚Äôs CoherenceModel. </p>

<p>Is there a way to either:</p>

<p><strong>1</strong> - Feed scikit-learn‚Äôs LDA model into gensim‚Äôs CoherenceModel pipeline, either through manually converting the scikit-learn model into gensim format or through a scikit-learn to gensim wrapper (I have seen the wrapper the other way around) to generate Topic Coherence?</p>

<p>Or</p>

<p><strong>2</strong> - Manually calculate topic coherence from scikit-learn‚Äôs LDA model and CountVectorizer/Tfidf matrices?</p>

<p>I have done quite a bit of research on implementations for this use case online but haven‚Äôt seen any solutions. The only leads I have are the documented equations from scientific literature.</p>

<p>If anyone has any knowledge on any similar implementations, or if you could point me in the right direction for creating a manual method for this, that would be great. Thank you!</p>

<p>*Side note: I understand that perplexity and log-likelihood are available in scikit-learn for performance measurements, but these are not as predictive from what I have read.</p>
","7789046","","7789046","","2018-08-30 19:49:03","2019-04-14 15:45:50","LDA Topic Model Performance - Topic Coherence Implementation for scikit-learn","<scikit-learn><nlp><gensim><lda><topic-modeling>","1","2","2","","","CC BY-SA 4.0"
"51854220","1","51863212","","2018-08-15 06:56:45","","3","1427","<p>I trained a gensim Word2Vec model.
Let's say I have a certain vector and I want the find the word it represents - what is the best way to do so?</p>

<p>Meaning, for a specific vector:</p>

<pre><code>vec = array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)
</code></pre>

<p>I want to get a word:</p>

<pre><code> 'computer' = model.vec2word(vec)
</code></pre>
","10117402","","10117402","","2018-08-15 07:23:55","2020-09-17 00:34:34","word2vec - find a word by a specific vector","<python-3.x><nlp><gensim><word2vec>","2","2","2","","","CC BY-SA 4.0"
"56667348","1","56935003","","2019-06-19 12:11:57","","-1","232","<p>Basically I have installed Gensim 3.7.3 from Python 3.7.1  , but while importing it in Pycharm i got an error: 
""ImportError: DLL load failed: %1 is not a valid Win32 application.""</p>

<p>I want to use Word2Vec model of Gensim but due to this error, I am stuck. I can't Change Python Version too. </p>

<p>Need Help! how I get Gensim Imported in this version of Python using Pycharm </p>

<p>import gensim
from gensim.models import Word2Vec</p>
","11242590","","","","","2019-07-08 12:30:10","Gensim Import Error ""ImportError: DLL load failed: %1 is not a valid Win32 application.""","<python><winapi><dll><pycharm><gensim>","1","5","","","","CC BY-SA 4.0"
"51956153","1","","","2018-08-21 20:21:09","","0","1546","<p>I used this code, <a href=""https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/"" rel=""nofollow noreferrer"">https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/</a>, to find topic coherence for a dataset. When I tried this code with the same number of topics, I got new values after each running. For example, for the number of topics =10, I got the following value after 2 running:</p>

<p>First Run for the number of topics =10
Coherence Score CV_1:  0.31230269562327095</p>

<p>Coherence Score UMASS_1:  -3.3065236823786064</p>

<p>Second Run the number of topics =10
Coherence Score CV_2:  0.277016662550274</p>

<p>Coherence Score UMASS_2:  -3.6146150653617743</p>

<p>What is the reason? In this unstable case, how we can trust this library? The highest coherence value changed as well.  </p>
","9844472","","11350828","","2019-04-14 22:09:30","2019-04-14 22:09:30","Gensim LDA: coherence values not reproducible between runs","<gensim><lda>","1","0","1","","","CC BY-SA 4.0"
"52116717","1","","","2018-08-31 13:38:43","","0","156","<p>I used an LDA over a corpus of 60000 entries and I got a good result. But inserting 200 more lines in this corpus and relaunching the LDA, I have quite different topics. However, 200 lines do not represent 1% of the corpus. Normally, the results should not change. I've looked for something about the sensitivity and stability of the LDA models and I've seen that they're just pretty sensitive on the parameter level ... does anyone know anything about this?</p>
<p>Here this script:</p>
<pre><code>import pandas
import mglearn
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

text = pandas.read_csv('pretraitement_janaina_modif.csv', encoding = 'utf-8')
text_list = text.values.tolist()

vector = CountVectorizer()
X = vector.fit_transform(text_list)

lda_model = LatentDirichletAllocation(n_components = 30, learning_method = &quot;batch&quot;, max_iter = 25, random_state = 0)
document_topics = lda_model.fit_transform(X)
sorting = np.argsort(lda_model.components_, axis = 1)[:, ::-1]
feature_names = np.array(vector.get_feature_names())

topics = mglearn.tools.print_topics(topics = range(30), feature_names = feature_names, sorting = sorting, topics_per_chunk = 5, n_words = 10)

print(topics)
</code></pre>
<p><strong>I have as a first result this list of topics:</strong></p>
<blockquote>
<p>Topic 0: no order mail received back always share refund answer more</p>
<p>Topic 1: cancel order wishes possible wish would wish to advance error items deceived</p>
<p>Topic 2: keep current informed delivery order informed product inform face number</p>
<p>Topic 3: faulty wooden box present box side level painting defects box</p>
</blockquote>
<p><strong>But when I add some few lines in the corpus, the topics change:</strong></p>
<blockquote>
<p>Topic 0: big top metal big size damaged product chandelier canape support</p>
<p>Topic 1: receipt ordered article product misses well order not send back only</p>
<p>Topic 2: faulty wooden box present box side level painting defects box</p>
<p>Topic 3: order payment amount spend transfer made pay bank come well</p>
</blockquote>
<p>What I don't understand is how this algorithm can be so sensitive and not stability and change so much when we add a few lines...</p>
","8683269","","-1","","2020-06-20 09:12:55","2018-08-31 15:37:39","Sensitivity and stability of the LDA models","<python><scikit-learn><gensim><lda>","0","4","","","","CC BY-SA 4.0"
"42626287","1","42929579","","2017-03-06 12:59:32","","2","2095","<p>I downloaded Wikipedia word vectors from <a href=""https://github.com/clips/dutchembeddings"" rel=""nofollow noreferrer"">here</a>. I loaded the vectors with:</p>

<pre><code>model_160 = KeyedVectors.load_word2vec_format(wiki_160_path, binary=False)
</code></pre>

<p>and then want to train them with:</p>

<pre><code>model_160.train()
</code></pre>

<p>I get the error back:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-11-22a9f6312119&gt; in &lt;module&gt;()
----&gt; 1 model.train()

AttributeError: 'KeyedVectors' object has no attribute 'train'
</code></pre>

<p>My question is now:
It seems like KeyedVectors has no train function, but I want to continue training the vectors on my personal sentences, instead of just using the Wikipedia vectors. How is this possible?</p>

<p>Thanks in advance, Jan</p>
","5015382","","","","","2017-03-21 20:30:45","Gensim: KeyedVectors.train()","<python-3.x><gensim>","1","0","2","","","CC BY-SA 3.0"
"52144567","1","","","2018-09-03 06:56:40","","0","83","<p>I have a large corpus of words extracted from the documents. In the corpus are words which might mean the same.
For eg: ""command"" and ""order"" means the same, ""apple"" and ""apply"" which does not mean the same.</p>

<p>I would like to merge the similar words, say ""command"" and ""order"" to ""command"".
I have tried to use word2vec but it doesn't check for semantic similarity of words(it ouputs good similarity for apple and apply since four characters in the words are the same). And when I try using wup similarity, it gives good similarity score if the words have matching synonyms whose results are not that impressive.</p>

<p>What could be the best approach to reduce semantically similar words to get rid of redundant data and merge similar data?</p>
","9901266","","","","","2018-09-03 09:01:12","How to reduce semantically similar words?","<python-2.7><gensim><word2vec><text-analysis><redundancy>","1","0","","","","CC BY-SA 4.0"
"67573416","1","","","2021-05-17 16:13:30","","3","589","<p>I am trying to make my own Fasttext embeddings so I went to official Gensim documentation and <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">implemented this exact code below</a> with exact <code>4.0</code> version.</p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import common_texts

model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
model.build_vocab(sentences=common_texts)
model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)
</code></pre>
<p>And to my surprise it is giving me errors as:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-4-6b2d1de02d90&gt; in &lt;module&gt;
      1 model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
----&gt; 2 model.build_vocab(sentences=common_texts)
      3 model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)

~/anaconda3/lib/python3.8/site-packages/gensim/models/word2vec.py in build_vocab(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    477 
    478         &quot;&quot;&quot;
--&gt; 479         self._check_corpus_sanity(corpus_iterable=corpus_iterable, corpus_file=corpus_file, passes=1)
    480         total_words, corpus_count = self.scan_vocab(
    481             corpus_iterable=corpus_iterable, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)

~/anaconda3/lib/python3.8/site-packages/gensim/models/word2vec.py in _check_corpus_sanity(self, corpus_iterable, corpus_file, passes)
   1484         &quot;&quot;&quot;Checks whether the corpus parameters make sense.&quot;&quot;&quot;
   1485         if corpus_file is None and corpus_iterable is None:
-&gt; 1486             raise TypeError(&quot;Either one of corpus_file or corpus_iterable value must be provided&quot;)
   1487         if corpus_file is not None and corpus_iterable is not None:
   1488             raise TypeError(&quot;Both corpus_file and corpus_iterable must not be provided at the same time&quot;)

TypeError: Either one of corpus_file or corpus_iterable value must be provided
</code></pre>
<p>Can someone please help what is happening here?</p>
","11725056","","","","","2021-05-17 17:58:23","Unable to recreate Gensim docs for training FastText. TypeError: Either one of corpus_file or corpus_iterable value must be provided","<python><nlp><gensim><fasttext>","1","0","","","","CC BY-SA 4.0"
"52139386","1","52139853","","2018-09-02 17:21:53","","1","1335","<p>I have this code and I have list of article as dataset. Each raw has an article. </p>

<p>I run this code:</p>

<pre><code>import gensim    
docgen = TokenGenerator( raw_documents, custom_stop_words )    
# the model has 500 dimensions, the minimum document-term frequency is 20    
w2v_model = gensim.models.Word2Vec(docgen, size=500, min_count=20, sg=1)    
print( ""Model has %d terms"" % len(w2v_model.wv.vocab) )    
w2v_model.save(""w2v-model.bin"")    
# To re-load this model, run    
#w2v_model = gensim.models.Word2Vec.load(""w2v-model.bin"")    
    def calculate_coherence( w2v_model, term_rankings ):
        overall_coherence = 0.0
        for topic_index in range(len(term_rankings)):
            # check each pair of terms
            pair_scores = []
            for pair in combinations(term_rankings[topic_index], 2 ):
                pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )
            # get the mean for all pairs in this topic
            topic_score = sum(pair_scores) / len(pair_scores)
            overall_coherence += topic_score
        # get the mean score across all topics
        return overall_coherence / len(term_rankings)

import numpy as np    
def get_descriptor( all_terms, H, topic_index, top ):    
    # reverse sort the values to sort the indices    
    top_indices = np.argsort( H[topic_index,:] )[::-1]    
    # now get the terms corresponding to the top-ranked indices    
    top_terms = []    
    for term_index in top_indices[0:top]:    
        top_terms.append( all_terms[term_index] )    
    return top_terms    
from itertools import combinations    
k_values = []    
coherences = []    
for (k,W,H) in topic_models:    
    # Get all of the topic descriptors - the term_rankings, based on top 10 terms
    term_rankings = []    
    for topic_index in range(k):
        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )

    # Now calculate the coherence based on our Word2vec model
    k_values.append( k )
    coherences.append( calculate_coherence( w2v_model, term_rankings ) )
    print(""K=%02d: Coherence=%.4f"" % ( k, coherences[-1] ) )
</code></pre>

<p>I face with this error:</p>

<pre><code>raise KeyError(""word '%s' not in vocabulary"" % word)
</code></pre>

<p>KeyError: u""word 'business' not in vocabulary""</p>

<p>The original code works great with their data set. </p>

<p><a href=""https://github.com/derekgreene/topic-model-tutorial"" rel=""nofollow noreferrer"">https://github.com/derekgreene/topic-model-tutorial</a></p>

<p>Could you help what this error is?</p>
","7721773","","7721773","","2018-09-02 19:15:44","2018-09-02 19:15:44","Gensim: raise KeyError(""word '%s' not in vocabulary"" % word)","<python><nlp><gensim><word2vec><topic-modeling>","1","0","1","","","CC BY-SA 4.0"
"67574809","1","68836966","","2021-05-17 17:58:15","","0","67","<p>To my understanding, the output of the skip-gram model must be compared with many training labels (depending on the window size)</p>
<p>My question is: Does the final output of the skip-gram model look like the description in this picture?
<img src=""https://i.stack.imgur.com/njSDy.jpg"" alt=""skip-gram"" /></p>
<p>Ps. the most similar question I can find:[1]<a href=""https://stackoverflow.com/questions/45431179/what-does-the-multiple-outputs-in-skip-gram-mean"">What does the multiple outputs in skip-gram mean?</a></p>
","13874745","","","","","2021-08-18 17:33:32","What would the output of skip-gram model look like?","<python><tensorflow><nlp><nltk><gensim>","1","0","1","","","CC BY-SA 4.0"
"42746007","1","","","2017-03-12 09:58:26","","5","4124","<p>I have tried to train incrementally  word2vec model produced by gensim. But I found that the vocabulary size doesn't increased , only the word2vec model weights are updated . But i need to  update both vocabulary and model size .</p>

<pre><code>#Load data 
sentences = []
....................

#Training 
model = Word2Vec(sentences, size=100)
model.save(""modelbygensim.txt"")
model.save_word2vec_format(""modelbygensim_text.txt"")



#Incremental Training 
model = Word2Vec.load('modelbygensim.txt')
model.train(sentences)
model.save(""modelbygensim_incremental.txt"")
model.save_word2vec_format(""modelbygensim_text_incremental.txt"")
</code></pre>
","1832454","","","","","2017-03-16 03:30:54","Incremental Word2Vec Model Training in gensim","<python><deep-learning><gensim><word2vec>","1","0","6","","","CC BY-SA 3.0"
"42554289","1","42593273","","2017-03-02 11:31:07","","7","6540","<p>I want to use output embedding of word2vec such as in <a href=""http://www2016.net/proceedings/companion/p83.pdf"" rel=""noreferrer"">this paper (Improving document ranking with dual word embeddings)</a>.</p>

<p>I know input vectors are in syn0, output vectors are in syn1 and syn1neg if negative sampling.</p>

<p>But when I calculated most_similar with output vector, I got same result in some ranges because of removing syn1 or syn1neg.</p>

<p>Here is what I got.</p>

<pre><code>IN[1]: model = Word2Vec.load('test_model.model')

IN[2]: model.most_similar([model.syn1neg[0]])

OUT[2]: [('of', -0.04402521997690201),
('has', -0.16387106478214264),
('in', -0.16650712490081787),
('is', -0.18117375671863556),
('by', -0.2527652978897095),
('was', -0.254993200302124),
('from', -0.2659570872783661),
('the', -0.26878535747528076),
('on', -0.27521973848342896),
('his', -0.2930959463119507)]
</code></pre>

<p>but another syn1neg numpy vector is already similar output.</p>

<pre><code>IN[3]: model.most_similar([model.syn1neg[50]])

OUT[3]: [('of', -0.07884830236434937),
('has', -0.16942456364631653),
('the', -0.1771494299173355),
('his', -0.2043554037809372),
('is', -0.23265135288238525),
('in', -0.24725285172462463),
('by', -0.27772971987724304),
('was', -0.2979024648666382),
('time', -0.3547973036766052),
('he', -0.36455872654914856)]
</code></pre>

<p>I want to get output numpy arrays(negative or not) with preserved during training.</p>

<p>Let me know how can I access pure syn1 or syn1neg, or code, or some word2vec module can get output embedding.</p>
","7646279","","","","","2017-03-06 21:58:03","How can I access output embedding(output vector) in gensim word2vec?","<python><numpy><gensim><word2vec>","1","0","6","","","CC BY-SA 3.0"
"42752356","1","42752808","","2017-03-12 19:55:27","","2","1403","<p>I would like to do some text analysis on job descriptions and was going to use nltk. I can build a dictionary and remove the stopwords, which is part of what I want. However in addition to the single words and their frequencies I would like to keep meaningful 'word groups' and count them as well. </p>

<p>For example in job descriptions containing 'machine learning' I don't want to consider 'machine' and 'learning' separately but keep retain the word group in my dictionary if it frequently occurs together. What is the most efficient method to do that? (I think I wont need to go beyond word groups containing 2 or words). And: At which point should I do the stopword removal?</p>

<p>Here is an example:</p>

<pre><code>    text = 'As a Data Scientist, you will focus on machine 
            learning and Natural Language Processing'
</code></pre>

<p>The dictionary I would like to have is:</p>

<pre><code>     dict = ['data scientist', 'machine learning', 'natural language processing', 
             'data', 'scientist', 'focus', 'machine', 'learning', 'natural' 
             'language', 'processing']
</code></pre>
","3550016","","5014455","","2017-03-12 20:09:01","2017-03-20 17:17:37","Create a dictionary with 'word groups'","<python><nltk><gensim>","3","4","","","","CC BY-SA 3.0"
"59129793","1","","","2019-12-01 20:43:42","","1","571","<p>As the title says, I would like to load custom word vectors built from <code>gensim</code> to the <code>SpaCy</code> Vector class. </p>

<p>I have found several other questions where folks have successfully loaded vectors to the <code>nlp</code> object itself, but I have a current project where I would like to have a separate Vectors object. </p>

<p>Specifically, I am using BioWordVec to generate my word vectors which serializes the vectors using methods from <code>gensim.models.Fastext</code>.</p>

<p>On the <code>gensim</code> end I am:</p>

<ul>
<li>calling <code>model.wv.save_word2vec_format(output/bin/path, binary=True)</code></li>
<li>saving the model -> <code>model.save(path/to/model)</code></li>
</ul>

<p>On the <code>SpaCy</code> side:</p>

<ul>
<li>I can either use the <code>from_disk</code> or <code>from_bytes</code> methods to load the word vectors </li>
<li>there is also a <code>from_glove</code> method that expects a vocab.txt file and a binary file (which I already have a binary file</li>
</ul>

<p>Link to <a href=""https://spacy.io/api/vectors"" rel=""nofollow noreferrer"">Vectors Documentation</a></p>

<p>just for reference, here is my code to test the load process:</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.vectors import Vectors 

vecs = Vectors()
path = '/home/medmison690/pyprojects/BioWordVec/pubmed_mesh_test.bin'
dir_path = '/home/medmison690/Desktop/tuned_vecs'


vecs.from_disk(dir_path)


print(vecs.shape)
</code></pre>

<p>I have tried various combinations of <code>from_disk</code> and <code>from_bytes</code> with no success. Any help or advice would be greatly appreciated!</p>
","12266797","","","","","2019-12-02 22:35:52","Load word vectors from Gensim to SpaCy Vectors class","<python><gensim><spacy>","1","3","","","","CC BY-SA 4.0"
"42517435","1","42541662","","2017-02-28 19:43:33","","19","16769","<p>I'm using gensim implementation of Word2Vec. I have the following code snippet:</p>

<pre><code>print('training model')
model = Word2Vec(Sentences(start, end))
print('trained model:', model)
print('vocab:', model.vocab.keys())
</code></pre>

<p>When I run this in python2, it runs as expected. The final print is all the words in the vocabulary.</p>

<p>However, if I run it in python3, I get an error:</p>

<pre><code>trained model: Word2Vec(vocab=102, size=100, alpha=0.025)
Traceback (most recent call last):
  File ""learn.py"", line 58, in &lt;module&gt;
    train(to_datetime('-4h'), to_datetime('now'), 'model.out')
  File ""learn.py"", line 23, in train
    print('vocab:', model.vocab.keys())
AttributeError: 'Word2Vec' object has no attribute 'vocab'
</code></pre>

<p>What is going on? Is gensim word2vec not compatible with python3?</p>
","58109","","","","","2021-10-12 14:55:52","Gensim word2vec in python3 missing vocab","<python><gensim><word2vec>","2","0","2","","","CC BY-SA 3.0"
"42529467","1","42600053","","2017-03-01 10:38:55","","0","667","<p>I am making SMS categorizer. For this I want to classify my messages into different topics. So I want to use gensim for that. 
Can anybody provide me the source of any tutorial that can help me to begin topic modelling using gensim?</p>
","7640692","","","","","2017-03-04 19:00:35","Topic Modelling using gensim","<gensim><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"52131011","1","","","2018-09-01 18:23:27","","0","756","<p>I want to understand how <code>pip install -e &lt;local project path&gt;</code> works. <a href=""https://pip.pypa.io/en/stable/reference/pip_install/#usage"" rel=""nofollow noreferrer"">The references</a> tell the <code>&lt;local project path&gt;</code> should have a setup.py file but doesn't mention anything else. </p>

<p>Does the <code>-e</code> flag executes setup.py file? Or are there some other steps involved?</p>

<p>Also, I see <a href=""https://github.com/RaRe-Technologies/gensim/issues/1519#issuecomment-339700695"" rel=""nofollow noreferrer"">projects</a> using the command like this: <code>pip install -e .[test]</code>. What does <code>.[test]</code> mean? Is it value placeholder that is defined somewhere? Is it a folder name? Or is it specific to how I write my <code>setup.py</code>? I couldn't find such example usage in the <a href=""https://github.com/RaRe-Technologies/gensim/issues/1519#issuecomment-339700695"" rel=""nofollow noreferrer""><code>pip install</code> references</a>.</p>
","3206884","","","","","2018-09-01 18:23:27","pip - Installing dependencies with <local project path>. Using `pip install -e`","<python-3.x><pip><gensim>","0","3","","","","CC BY-SA 4.0"
"67580388","1","67590179","","2021-05-18 05:24:35","","0","118","<p>I have trained a doc2vec (PV-DM) model in <code>gensim</code> on documents which fall into a few classes. I am working in a non-linguistic setting where both the number of documents and the number of unique words are small (~100 documents, ~100 words) for practical reasons. Each document has perhaps 10k tokens. My goal is to show that the doc2vec embeddings are more predictive of document class than simpler statistics and to explain which <em>words</em> (or perhaps word sequences, etc.) in each document are indicative of class.</p>
<p>I have good performance of a (cross-validated) classifier trained on the embeddings compared to one compared on the other statistic, but I am still unsure of how to connect the results of the classifier to any features of a given document. Is there a standard way to do this? My first inclination was to simply pass the co-learned word embeddings through the document classifier in order to see which words inhabited which classifier-partitioned regions of the embedding space. The document classes output on word embeddings are very consistent across cross validation splits, which is encouraging, although I don't know how to turn these effective labels into a statement to the effect of &quot;Document X got label Y because of such and such properties of words A, B and C in the document&quot;.</p>
<p>Another idea is to look at similarities between word vectors and document vectors. The ordering of similar word vectors is pretty stable across random seeds and hyperparameters, but the output of this sort of labeling does not correspond at all to the output from the previous method.</p>
<p>Thanks for help in advance.</p>
<p><strong>Edit</strong>: Here are some clarifying points. The tokens in the &quot;documents&quot; are ordered, and they are measured from a discrete-valued process whose states, I suspect, get their &quot;meaning&quot; from context in the sequence, much like words. There are only a handful of classes, usually between 3 and 5. The documents are given unique tags and the classes are not used for learning the embedding. The embeddings have rather dimension, always &lt; 100, which are learned over many epochs, since I am only worried about overfitting when the classifier is learned, not the embeddings. For now, I'm using a multinomial logistic regressor for classification, but I'm not married to it. On that note, I've also tried using the normalized regressor coefficients as vector in the embedding space to which I can compare words, documents, etc.</p>
","3703379","","3703379","","2021-05-18 22:36:46","2021-05-18 22:36:46","How to interpret doc2vec classifier in terms of words?","<gensim><word2vec><word-embedding><doc2vec>","1","0","","","","CC BY-SA 4.0"
"52125136","1","52125367","","2018-09-01 05:26:57","","0","270","<p>I followed the steps in gensim Python <a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a> to train wikipedia on LDA model, now I want to compare arbitary article from cnn.com for example with the trained data, what do I need to do next? Suppose this article is in txt file? </p>
","4586806","","7117003","","2018-09-01 05:53:43","2018-09-02 16:50:53","Training LDA on Wikipedia corpus to tag arbitary article?","<python><nltk><gensim>","1","0","","","","CC BY-SA 4.0"
"42697499","1","","","2017-03-09 14:08:15","","0","1113","<p>I'm new to python. What I'm trying to do is to read 2 parameters via console. </p>

<ol>
<li>parameter:path to a trained LDA model with gensim. </li>
<li>parameter:the number of most common words per topic which I want to get in return.</li>
</ol>

<p>Now I want to print for all the topics the number of the most common words per topic. Now my question is how to get all the topics.</p>

<p>This is what I have so far:</p>

<pre><code>import sys, getopt
import gensim

def main(argv):
   input_file = argv[0] #LDA Path
   number_of_words = argv[1] #Number of most common word per topic

   ldamodel = gensim.models.ldamodel.LdaModel.load(input_file, mmap=None) #load model
   ldamodel.print_topic(?, number_of_words)



if __name__ == ""__main__"":
   main(sys.argv[1:])
</code></pre>

<p><a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">Gensim doc</a></p>

<p>Thanks</p>
","5152497","","","","","2017-03-09 14:12:06","Load the computed LDA models and print the most common words per topic","<python><gensim><topic-modeling>","1","1","","","","CC BY-SA 3.0"
"42928438","1","","","2017-03-21 13:23:13","","0","317","<p>I know that the creation of LDA models is probabilistic, and that two models trained under the same parameters on the same corpus will not necessarily be identical. However, I'm wondering if the topic distribution of a document fed into an LDA model is also probabilistic. </p>

<p>I have an LDA model as presented here:</p>

<pre><code>lda = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=numTopics,passes=10)
</code></pre>

<p>as well as two documents, Doc1 and Doc2. I want to find the cosine similarity of the two documents in lda space, so that:</p>

<pre><code>x = cossim(lda[Doc1], lda[Doc2]).
</code></pre>

<p>The problem I'm noticing is that when I run this through multiple iterations, the cosine similarity is not always identical. (even when I use the same saved LDA model). The similarity is extremely similar, but it's always a bit off each time. In my actual code I have hundreds of documents, so I'm converting the topic distributions to a dense vector and using numpy to do the calculations in a matrix:</p>

<pre><code>documentsList = np.array(documentsList)
calcMatrix=1-cdist(documentsList, documentsList, metric=self.metric)
</code></pre>

<p>Am I running into a rounding error with numpy (or another bug in my code), or is this behavior I should expect when using lda to find the topic distribution of a document?</p>

<p>Edit: I'm going to run a simple cosine similarity on 2 different documents using my lda model, and plot the spread of results. I will report back with what I find.</p>

<p>Ok, here are the results of running cossine similarity against 2 documents, using the same LDA model. </p>

<p>Here is my code:</p>

<pre><code>def testSpacesTwoDocs(doc1, doc2, dictionary):
    simList = []
    lda = gensim.models.ldamodel.LdaModel.load('LDA_Models/lda_bow_behavior_allFields_t385_p10')
    for i in range(50):
        doc1bow = dictionary.doc2bow(doc1)
        doc2bow = dictionary.doc2bow(doc2)

        vec1 = lda[doc1bow]
        vec2 = lda[doc2bow]

        S = matutils.cossim(vec1, vec2)
        simList.append(S)


    for entry in simList:
        print entry

    sns.set_style(""darkgrid"")
    plt.plot(simList, 'bs--')
    plt.show()


    return
</code></pre>

<p>Here are my results:
    0.0082616863035,
    0.00828413767524,
    0.00826550453411,
    0.00816756826185,
    0.00829832701338,
    0.00828970584276,
    0.00828578705814,
    0.00817109902484,
    0.00817138247141,
    0.00825297374028,
    0.008269435921,
    0.00826470121538,
    0.00818282042634,
    0.00824660449673,
    0.00818087532906,
    0.0081770261766,
    0.00817128310123,
    0.00817643202588,
    0.00827404791376,
    0.00832439428054,
    0.00816643128216,
    0.00828540881955,
    0.00825746652101,
    0.00816793513824,
    0.00828471827526,
    0.00827161219003,
    0.00817773114553,
    0.00826166001503,
    0.00828048713541,
    0.00817435544365,
    0.0082956702812,
    0.00826167470288,
    0.00829873425476,
    0.00825744872634,
    0.00826802120149,
    0.00829604894909,
    0.0081776752236,
    0.00817613482849,
    0.00825839326441,
    0.00817530362838,
    0.0081747561999,
    0.0082597447174,
    0.00828958180101,
    0.00827157760835,
    0.00826939127657,
    0.00826138381094,
    0.00817755590806,
    0.00827135780051,
    0.00827314260067,
    0.00817035250043</p>

<p>Am I correct to assume that the LDA model is infering the topic distribution of both documents at each iteration, and thus that the cosine similarities are stochastic rather than determanistic? Is this much variation a sign that I'm not training my model long enough? Or am I not properly normalizing the vectors? Thanks</p>

<p>Thanks</p>
","6758743","","6758743","","2017-03-21 14:42:13","2017-12-18 10:06:07","Are Topic Distributions of Documents in LDA Space Probabilistic?","<python><numpy><gensim><lda>","1","2","","","","CC BY-SA 3.0"
"42857000","1","","","2017-03-17 11:58:57","","0","463","<p>I have 2 lists of sentences. First list contains different questions, second contains different statements.</p>

<p>Little example:</p>

<pre><code>1st list:
[
    ""What are cultures that are associated with core values?"",
    ""How do bumblebees fly?"",
    ""It is possible that avocado seed can be a tea?"",
    ...
]

2nd list:
[
    ""The population was 388 at the 2010 census."",
    ""Grevillea helmsiae is a tree which is endemic to Queensland in Australia."",
    ""He played youth football for Tynecastle Boys Club."",
    ...
]
</code></pre>

<p>I want to write program which will be able to classify this 2 types of sentences. For this, I can create neural network and train it on my 2 lists. I guess, this must be recurrent neural network.</p>

<p>I have transformed each sentence to array of word2vec vectors. And now I want to set up keras recurrent neural network with LSTM layers. But I don't know how to do that correctly. Can you write keras model for this problem?</p>

<p>UPDATE</p>

<p>the form of above sentences after transforming it by word2vec is like this:</p>

<pre><code>[
    [vector_of_floats_for_word_""what"", vector_of_floats_for_word_""are"", vector_of_floats_for_word_""cultures"", vector_of_floats_for_word_""that"", ...],
    [vector_of_floats_for_word_""how"", vector_of_floats_for_word_""do"", vector_of_floats_for_word_""bumblebees"", ...]
]
</code></pre>

<p>and so on. each vector has 300 dimensions.</p>

<p>here is my model:</p>

<pre><code>X = []
Y = []

for i in range(1000):
    X.append(questions_vectors[i])
    Y.append([1, 0])
    X.append(statements_vectors[i])
    Y.append([0, 1])

model = Sequential()
model.add(LSTM(128, input_shape=(2000, None, 300)))
model.add(Dense(2, activation='softmax'))
model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.01))
</code></pre>

<p>there you can see magic numbers 2000 and 300. 2000 is 1000 questions + 1000 statements, 300 - word vector length</p>

<p>but I'm sure that my model is wrong. also I'm getting the error:</p>

<pre><code>ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=4
</code></pre>
","7691051","","7691051","","2017-03-17 12:32:42","2017-03-17 12:32:42","How to classify sentences using word2vec and keras?","<keras><recurrent-neural-network><gensim><word2vec><keras-layer>","0","3","","","","CC BY-SA 3.0"
"42881749","1","","","2017-03-19 01:05:34","","1","210","<p>I'm having an issue running multicored LDA in gensim (generating 2000 topics, 1 pass using 15 workers). I get the error below, I initially thought it might not have to do with saving the model, but looking at the error (the code still keeps running, at least the process hasn't quit).
Anyone know what I can do to prevent this error from occurring?</p>

<pre><code>python3 run.py --method MultiLDA --ldaparams 2000 1 --workers 15 --path $DATA/gender_spectrum/

Traceback (most recent call last):
   File ""/usr/lib64/python3.5/multiprocessing/queues.py"", line 241, in _feed
   obj = ForkingPickler.dumps(obj)
   File ""/usr/lib64/python3.5/multiprocessing/reduction.py"", line 50, in dumps
   cls(buf, protocol).dump(obj)
OverflowError: cannot serialize a bytes object larger than 4 GiB```
</code></pre>
","2489879","","2489879","","2017-03-19 23:41:56","2017-03-19 23:41:56","Gensim multicore LDA overflow error","<python-3.x><python-multiprocessing><multicore><gensim><lda>","0","0","","","","CC BY-SA 3.0"
"67596945","1","67600164","","2021-05-19 04:28:24","","0","56","<p>I have trained a gensim doc2vec model for an English news recommender system. the model was trained with 40K news data. I am using the code below to recommend the top 5 most similar news for e.g. news_1:</p>
<pre><code>inferred_vector = model.infer_vector(news_1)
sims = model.dv.most_similar([inferred_vector], topn=5)
</code></pre>
<p>The problem is that if I add another 100 news data to the database(so our database will have 40K + 100 news data now) and re-run the same code, the code will only be able to recommend news based on the original 40K(instead of 40K + 100) to me, in another word, the recommended articles will never come from the 100 articles.</p>
<p>how can I address this issue without the need to retrain the model? Thank you in advanced!</p>
<p>Ps: As our APP is for news, so everyday we'll have lots of news data coming into our database, so we won't consider to retrain the model everyday(doing so may crash our backend server).</p>
","12076816","","12076816","","2021-05-19 04:40:42","2021-05-19 09:07:18","How to get similarity score for unseen documents using Gensim Doc2Vec model?","<python><gensim><doc2vec><recommender-systems>","1","1","1","","","CC BY-SA 4.0"
"42836992","1","45444514","","2017-03-16 14:32:11","","2","1582","<p>I installed Python 3.5 using Anaconda and gensim 1.0.1 (supports Python 3) using pip. I got the following error when running gensim:</p>

<pre><code>Exception in thread Thread-61:
Traceback (most recent call last):
  File ""/Users/mac/anaconda/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/Users/mac/anaconda/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/Users/mac/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 838, in job_producer
    sentence_length = self._raw_word_count([sentence])
  File ""/Users/mac/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 755, in _raw_word_count
    return sum(len(sentence) for sentence in job)
  File ""/Users/mac/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 755, in &lt;genexpr&gt;
    return sum(len(sentence) for sentence in job)
TypeError: object of type 'map' has no len()
</code></pre>

<p>The code causing this error is from <a href=""https://github.com/aditya-grover/node2vec"" rel=""nofollow noreferrer"">node2vec</a>. I am porting it to Python 3 but got this error.</p>

<p>I know that in Python 3, len(map) causes error, does it mean Gensim 1.0.1 does not support Python 3 although <a href=""https://pypi.python.org/pypi/gensim"" rel=""nofollow noreferrer"">pip website</a> says it supports? Or are there some hidden settings?</p>

<p>Anyone has any idea what is wrong? Thanks.</p>
","1259561","","1259561","","2017-03-16 15:00:42","2017-08-01 17:42:32","Gensim 1.0.1 on Python 3.5 TypeError: object of type 'map' has no len()?","<python><python-3.x><anaconda><gensim>","1","5","","","","CC BY-SA 3.0"
"52171195","1","","","2018-09-04 17:17:04","","0","129","<p>There are two ways to load in pretrained word embeddings, those who are compiled in C and the other in python. I have self trained embeddings in python which are loaded in with:</p>

<p><code>model = gensim.models.Word2Vec.load('transcript-vectors.pickle')</code></p>

<p>But when I go to load them into a word dictionary ( the same way I would with a pretrained embeddings from a third party ) it errors out since it does not have the same methods as the other load.</p>

<pre><code>embeddings_index = dict()
for word in model.wv.vocab:
    embeddings_index[word] = model.word_vec(word)
print('Loaded %s vectors' % len(embeddings_index))
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-94-c1e5d21d49af&gt; in &lt;module&gt;()
  1 embeddings_index = dict()
  2 for word in model.wv.vocab:
----&gt; 3     embeddings_index[word] = model.word_vec(word)
  4 print('Loaded %s vectors' % len(embeddings_index))

AttributeError: 'Word2Vec' object has no attribute 'word_vec'
</code></pre>
","6919063","","","","","2018-09-04 17:46:25","Gensim Self Trained embedding load","<python><gensim>","1","1","","","","CC BY-SA 4.0"
"42976912","1","42993170","","2017-03-23 13:05:13","","1","1400","<p>After reading <a href=""https://codesachin.wordpress.com/2015/10/09/generating-a-word2vec-model-from-a-block-of-text-using-gensim-python/"" rel=""nofollow noreferrer"">this article</a>, I start to train my own model. The problem is that the author does not make it clear what the <code>sentences</code> in  <code>Word2Vec</code> should be like. </p>

<p>I download the text from a Wikipedia page, as it is written is the article, and I make a list of sentences from it:</p>

<pre><code>sentences = [word for word in wikipage.content.split('.')]
</code></pre>

<p>So, for example, <code>sentences[0]</code> looks like:</p>

<pre><code>'Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed'
</code></pre>

<p>Then I try to train a model with this list:</p>

<pre><code>model = Word2Vec(sentences, min_count=2, size=50, window=10,  workers=4)
</code></pre>

<p>But the dictionary of the model consists of letters! For example, the output of <code>model.wv.vocab.keys()</code> is:</p>

<pre><code>dict_keys([',', 'q', 'D', 'B', 'p', 't', 'o', '(', ')', '0', 'V', ':', 'j', 's', 'R', '{', 'g', '-', 'y', 'c', '9', 'I', '}', '1', 'M', ';', '`', '\n', 'i', 'r', 'a', 'm', '‚Äì', 'v', 'N', 'h', '/', 'P', 'F', '8', '""', '‚Äô', 'W', 'T', 'u', 'U', '?', ' ', 'n', '2', '=', 'w', 'C', 'O', '6', '&amp;', 'd', '4', 'S', 'J', 'E', 'b', 'L', '$', 'l', 'e', 'H', '‚âà', 'f', 'A', ""'"", 'x', '\\', 'K', 'G', '3', '%', 'k', 'z'])
</code></pre>

<p>What am I doing wrong? Thanks in advance!</p>
","6133400","","610569","","2017-03-24 06:57:36","2017-03-24 06:57:36","How to train Word2Vec model on Wikipedia page using gensim?","<python><nlp><gensim><word2vec>","1","0","1","","","CC BY-SA 3.0"
"37539760","1","","","2016-05-31 07:43:15","","0","392","<p>I was making a corpus using the command </p>

<pre><code>background_corpus = TextCorpus('wiki.en.text')
</code></pre>

<p>This is an over 10 GB file so while making this Corpus and it adding to a dictionary it gives this </p>

<pre><code>adding document #820000 to Dictionary(2000000 unique tokens: [u'tripolitan', u'ftdna', u'soestdijk', u'billycorgan', u'olmsville']...)

discarding 31072 tokens: [(u'vnsas', 1), (u'ezequeel', 1), (u'trapeztafel', 1), (u'pubsub', 1), (u'gyvenimas', 1), (u'gilibrand', 1), (u'catfaced', 1), (u'beuningan', 1), (u'moodadi', 1), (u'nocaster', 1)]...

keeping 2000000 tokens which were in no less than 0 and no more than 830000 (=100.0%) documents
</code></pre>

<p>Hence its discarding the new tokens as its maximum size is 2000000. Is there anyway I can not limit on the size of the dictionary?</p>
","3481478","","","","","2017-05-10 11:44:27","How to increase Dictionary size in gensim while making Corpus?","<python><dictionary><gensim>","1","0","","","","CC BY-SA 3.0"
"42673590","1","43165280","","2017-03-08 14:11:29","","2","1717","<p>I  getting memory error, when I use <strong>GoogleNews-vectors-negative300.bin</strong> or try to train a model with Gensim with wikipedia dataset corpus.(1 GB). I have 4GB RAM in my system. Is there any way to bypass this.</p>

<p>Can we host it on cloud service like AWS to get better speed ?</p>
","7421387","","7421387","","2017-11-20 07:37:39","2019-01-14 10:21:20","Gensim - Memory error using GoogleNews-vector model","<nlp><gensim><word2vec>","3","0","","","","CC BY-SA 3.0"
"42827175","1","","","2017-03-16 06:54:27","","18","18984","<p>I'm kinda newbie and not native english so have some trouble understanding <code>Gensim</code>'s <code>word2vec</code> and <code>doc2vec</code>.</p>

<p>I think both give me some words most similar with query word I request, by <code>most_similar()</code>(after training).</p>

<p>How can tell which case I have to use <code>word2vec</code> or <code>doc2vec</code>?</p>

<p>Someone could explain difference in short word, please?</p>

<p>Thanks.</p>
","3595632","","","","","2017-03-16 10:04:35","Gensim: What is difference between word2vec and doc2vec?","<nlp><gensim>","1","2","7","","","CC BY-SA 3.0"
"43019447","1","43074104","","2017-03-25 17:15:18","","1","263","<p>I have a list of sentences, and I follow the instructions at the <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow noreferrer"">tutorial</a> to make a corpora from it:</p>

<pre><code>texts = [[word for word in document.lower().split() if word.isalpha()] for document in documents]
corpus = corpora.Dictionary(texts)
</code></pre>

<p>I want to train a LDA model on this corpora and extract the topics keywords.</p>

<pre><code>lda = models.LdaModel(corpus, num_topics=10)
</code></pre>

<p>However, I receive an error while training: <code>TypeError: 'int' object is not iterable</code>. What am I doing wrong? What the format of a corpus should be?</p>
","6133400","","","","","2017-03-28 15:46:18","Gensim: Unable to train the LDA model","<nlp><gensim><lda><corpus>","1","0","","","","CC BY-SA 3.0"
"43045418","1","","","2017-03-27 11:40:25","","0","987","<p>I used to python 3.5 and Based on gensim samples I created a project and added these codes in my project:</p>

<pre><code>    class MyCorpus(object):
    def __iter__(self):
        for line in open('files/2/mycorpus.txt'):
            # assume there's one document per line, tokens separated by whitespace
            yield dictionary.doc2bow(line.lower().split())


corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!
print(corpus_memory_friendly)
</code></pre>

<p>But after running I have these error in my pycharm console :</p>

<pre><code>    Traceback (most recent call last):
  File ""D:/Python-Workspace(s)/GensimSamples/2.Gensim_CorpusStreaming.py"", line 31, in &lt;module&gt;
    for vector in corpus_memory_friendly:  # load one vector into memory at a time
  File ""D:/Python-Workspace(s)/GensimSamples/2.Gensim_CorpusStreaming.py"", line 17, in __iter__
    yield dictionary.doc2bow(line.lower().split())
AttributeError: module 'gensim.corpora.dictionary' has no attribute 'doc2bow'
</code></pre>

<p>How can I solve this issue?</p>
","1976453","","","","","2017-07-11 02:40:16","gensim Memory-friendly corpora error","<python-3.x><gensim><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"42781292","1","42817402","","2017-03-14 08:43:26","","40","34238","<p>I am trying to build a document retrieval model that returns most documents ordered by their relevancy with respect to a query or a search string. For this I trained a doc2vec model using the <code>Doc2Vec</code> model in gensim. My dataset is in the form of a pandas dataset which has each document stored as a string on each line. This is the code I have so far</p>

<pre><code>import gensim, re
import pandas as pd

# TOKENIZER
def tokenizer(input_string):
    return re.findall(r""[\w']+"", input_string)

# IMPORT DATA
data = pd.read_csv('mp_1002_prepd.txt')
data.columns = ['merged']
data.loc[:, 'tokens'] = data.merged.apply(tokenizer)
sentences= []
for item_no, line in enumerate(data['tokens'].values.tolist()):
    sentences.append(LabeledSentence(line,[item_no]))

# MODEL PARAMETERS
dm = 1 # 1 for distributed memory(default); 0 for dbow 
cores = multiprocessing.cpu_count()
size = 300
context_window = 50
seed = 42
min_count = 1
alpha = 0.5
max_iter = 200

# BUILD MODEL
model = gensim.models.doc2vec.Doc2Vec(documents = sentences,
dm = dm,
alpha = alpha, # initial learning rate
seed = seed,
min_count = min_count, # ignore words with freq less than min_count
max_vocab_size = None, # 
window = context_window, # the number of words before and after to be used as context
size = size, # is the dimensionality of the feature vector
sample = 1e-4, # ?
negative = 5, # ?
workers = cores, # number of cores
iter = max_iter # number of iterations (epochs) over the corpus)

# QUERY BASED DOC RANKING ??
</code></pre>

<p>The part where I am struggling is in finding documents that are most similar/relevant to the query. I used the <code>infer_vector</code> but then realised that it considers the query as a document, updates the model and returns the results. I tried using the <code>most_similar</code> and <code>most_similar_cosmul</code> methods but I get words along with a similarity score(I guess) in return. What I want to do is when I enter a search string(a query), I should get the documents (ids) that are most relevant along with a similarity score(cosine etc). How do I get this part done?</p>
","2324298","","2324298","","2018-06-25 05:41:30","2018-06-25 05:41:30","Doc2Vec Get most similar documents","<python><nlp><gensim><doc2vec>","1","6","21","","","CC BY-SA 4.0"
"52170784","1","","","2018-09-04 16:47:18","","1","287","<p>I have gone through the other threads where its specified that in LDA the memory is proportional to <strong>numberOfTerms*numberOfTopics</strong> . In my case  I have two datasets. 
In dataset A I have 250K Documents and around 500K terms here I am easily able to run for ~ 500 Topics. But in dataset B I have around 2 Million documents and 500K terms(we got here after some filtering) but here I am only able to run till 50 topics above that it throws memory exception.</p>

<p>So just wanted to understand if only number of terms and topics matter for memory why number of documents is causing this problem and is there any quick workaround which can avoid this.</p>

<p><strong>Note :</strong> I know corpus can be wrapped around as an iteratable as specified in <a href=""https://stackoverflow.com/questions/35609171/memory-efficient-lda-training-using-gensim-library"">memory-efficient-lda-training-using-gensim-library</a> but lets assume I already loaded the corpora in memory because of some other restrictions I have of keeping input data in different format so it can be run on different platforms for different algorithms. The point is I am able to run it for some lesser number of Topics after loading whole corpora in memory. So is there any workaround which can help it run for more number of topics. For example I was thinking adjusting chunksize might help but that didn't work.</p>
","4855331","","","","","2018-09-04 16:47:18","LDA Gensim OOM Exception because of large corpus","<python><gensim><lda><topic-modeling>","0","3","","","","CC BY-SA 4.0"
"43051902","1","43066315","","2017-03-27 16:37:53","","3","1178","<p>I'm trying create an algorithm that's capable of show the top n documents similar to a specific document.
For that i used the gensim doc2vec. The code is bellow:</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(size=400, window=8, min_count=5, workers = 11, 
dm=0,alpha = 0.025, min_alpha = 0.025, dbow_words = 1)

model.build_vocab(train_corpus)

for x in xrange(10):
    model.train(train_corpus)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
    model.train(train_corpus)

model.save('model_EN_BigTrain')

sims = model.docvecs.most_similar([408], topn=10)
</code></pre>

<p>The sims var should give me 10 tuples, being the first element the id of the doc and the second the score.
The problem is that some id's do not correspond to any document in my training data.</p>

<p>I've been trying for some time now to make sense out of the ids that aren't in my training data but i don't see any logic.</p>

<p>Ps: This is the code that i used to create my train_corpus</p>

<pre><code>def readData(train_corpus, jData):

print(""The response contains {0} properties"".format(len(jData)))
print(""\n"")
for i in xrange(len(jData)):
    print ""&gt; Reading offers from Aux array""
    if i % 10 == 0: 
        print ""&gt;&gt;"", i, ""offers processed...""

      train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(jData[i][1]), tags=[jData[i][0]]))
print ""&gt; Finished processing offers""
</code></pre>

<p>Being each position of the aux array one array in witch the position 0 is an int (that i want to be the id) and the position 1 a description</p>

<p>Thanks in advance.</p>
","7685576","","","","","2017-12-12 02:50:49","Gensim docvecs.most_similar returns Id's that dont exist","<python><gensim><doc2vec>","2","0","1","","","CC BY-SA 3.0"
"52126539","1","52126699","","2018-09-01 08:53:27","","9","12336","<p>I have trained word2vec in gensim. In Keras, I want to use it to make matrix of sentence using that word embedding. As storing the matrix of all the sentences is very space and memory inefficient. So, I want to make embedding layer in Keras to achieve this so that It can be used in further layers(LSTM). Can you tell me in detail how to do this?</p>

<p>PS: It is different from other questions because I am using gensim for word2vec training instead of keras.</p>
","7290240","","7290240","","2018-09-01 09:25:02","2019-08-06 11:30:57","Using pretrained gensim Word2vec embedding in keras","<python><keras><gensim><word2vec><word-embedding>","3","1","2","","","CC BY-SA 4.0"
"42552733","1","","","2017-03-02 10:23:07","","2","123","<p>I'm trying to load a model of that contains spanish words using gensim-1.0 in python3.5, but when I do <code>gensim.models.KeyedVectors.load_word2vec_format(mymodel)</code> the CLI says this:</p>

<pre><code>Traceback (most recent call last):
  File ""./prueba.py"", line 30, in &lt;module&gt;
    model = KeyedVectors.load_word2vec_format('./data/WikiModelEsp/wiki.size.800.window.5.mincount.50.new.model', binary=True)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/keyedvectors.py"", line 192, in load_word2vec_format
    header = utils.to_unicode(fin.readline(), encoding=encoding)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/utils.py"", line 231, in any2unicode
    return unicode(text, encoding, errors=errors)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>I try to call load function with <code>encoding='latin1'</code> and <code>binary=True</code> but still doesn't work.</p>
","4950291","","","","","2017-05-09 08:58:59","Issue loading a model of Spanish data","<python-3.x><gensim>","1","0","1","","","CC BY-SA 3.0"
"64067272","1","","","2020-09-25 15:39:22","","0","166","<p>I have a pretrained embeddings file, which was quantized, in .ftz format. I need it to look up words, find the nearest neighbours. But I fail to find any toolkits that can do that. FastText can load the embeddings file, yet not able to look up the nearest neighbour, Gensim can lookup the nearest neighbour, but not be able to load the model...</p>
<p>Or it's me not finding the right function?</p>
<p>Thank you!</p>
","14341025","","","","","2020-09-25 15:39:22","Load fasttext quantized model (.ftz), and look up words","<python><gensim><word-embedding><fasttext>","0","2","","","","CC BY-SA 4.0"
"42789612","1","","","2017-03-14 15:07:04","","0","290","<p>I have this:</p>

<pre><code>texts = ['human', 'machine', 'interface']
</code></pre>

<p>When I do this with Gensim:</p>

<pre><code>dictionary = corpora.Dictionary(texts)
</code></pre>

<p>It leads to unicode <code>u'</code>'s being added... How can I suppress this?</p>
","2273626","","","","","2017-03-19 05:11:58","Gensim and unicode in Python","<python><unicode><gensim>","1","5","","","","CC BY-SA 3.0"
"42851859","1","43130140","","2017-03-17 07:41:18","","2","1223","<p>As I understand, if i'm training a LDA model over a corpus where the size of the dictionary is say 1000 and no of topics (K) = 10, for each word in the dictionary I should have a vector of size 10 where each position in the vector is the probability of that word belongs to that particular topic, right? </p>

<p>So my question is given a word, what is the probability of that word belongs to to topic k where k could be from 1 to 10, how do I get this value in the gensim lda model?</p>

<p>I was using <code>get_term_topics</code> method but it doesn't output all the probabilities for all the topics. For eg.,</p>

<pre><code>lda_model1.get_term_topics(""fun"")
[(12, 0.047421702085626238)],
</code></pre>

<p>but I want to see what is the prob that ""fun"" could be in all the other topics as well?</p>
","601357","","","","","2017-03-30 23:46:41","How to get the topic-word probabilities of a given word in gensim LDA?","<gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"52184439","1","","","2018-09-05 11:49:19","","0","501","<p>I have 50,000k files - that have a combined total of 162 million words. I wanted to do topic modelling using Gensim similar to this tutorial <a href=""https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"" rel=""nofollow noreferrer"">here</a></p>
<p>So, LDA requires one to tokenize the documents into words and then create a word frequency dictionary.</p>
<p>So, I have these files read into a pandas dataframe (The 'content' column has the text) and do the following to create a list of the texts.<a href=""https://i.stack.imgur.com/R42dr.png"" rel=""nofollow noreferrer"">image of dataframe attached here</a></p>
<p><code>texts = [[word for word in row[1]['content'].lower().split() if word not in stopwords] for row in df.iterrows()]</code></p>
<p>However, I have been running into a memory error, because of the large word count.</p>
<p>I also tried the TokenVectorizer in Python. I had got a memory error for this too.</p>
<pre class=""lang-py prettyprint-override""><code>def simple_tokenizer(str_input):
    words = re.sub(r&quot;[^A-Za-z0-9\-]&quot;, &quot; &quot;, str_input).lower().split()
    return words
</code></pre>
<pre class=""lang-py prettyprint-override""><code>vectorizer = TfidfVectorizer(use_idf=True, tokenizer=simple_tokenizer, stop_words='english')
X = vectorizer.fit_transform(df['content'])
</code></pre>
<p>How do I handle tokenizing these really long documents in a way it can be processed for LDA Analysis?</p>
<p>I have an i7, 16GB Desktop if that matters.</p>
<p><strong>EDIT</strong></p>
<p>Since Python was unable to store really large lists. I actually rewrote the code, to read each file (originally stored as HTML), convert it to text, create a text vector, append it to a list, and then sent it to the LDA code. It worked!</p>
","10046100","","13542937","","2021-04-21 10:13:40","2021-04-21 10:13:40","Handling Memory Error when dealing with really large number of words (>100 million) for LDA analysis","<python><out-of-memory><gensim><lda>","1","1","","","","CC BY-SA 4.0"
"67609635","1","67624658","","2021-05-19 19:21:58","","0","79","<p>I have a couple of issues regarding Gensim in its Word2Vec model.</p>
<p>The first is what is happening if I set it to train for 0 epochs? Does it just create the random vectors and calls it done. So they have to be random every time, correct?</p>
<p>The second is concerning the WV object in the doc page says:</p>
<pre><code>This object essentially contains the mapping between words and embeddings.
After training, it can be used directly to query those embeddings in various ways.  
See the module level docstring for examples.
</code></pre>
<p>But that is not clear to me, allow me to explain I have my own created word vectors which I have substitute in the</p>
<pre><code>   word2vecObject.wv['word'] = my_own
</code></pre>
<p>Then call the train method with those replacement word vectors. But I would like to know which part am I replacing, is it the input to hidden weight layer or the hidden to input? This is to check if it can be called pre-training or not. Any help? Thank you.</p>
","1423656","","","","","2021-05-20 18:08:41","Inner workings of Gensim Word2Vec","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"43080291","1","53114413","","2017-03-28 21:29:10","","0","1398","<p>Gensim's HDP model for topic modeling (gensim.models.hdpmodel.HdpModel) has a constructor that takes an argument called <code>max_chunks</code>.</p>

<p>On the documentation, it says <code>max_chunks</code> is the number of chunks the model will go over, and if that is larger than the number of chunks in supplied corpus, the training will wrap around the corpus.</p>

<p>Since I was warned by INFO logs that the likelihood function has been decreasing, I figure I may need multiple passes on corpus to converge.</p>

<p>LDA model provides with the <code>passes</code> argument the functionality to train on corpus for multiple iterations. I have difficulty figuring out how <code>max_chunks</code> in HDP maps to <code>passes</code> in LDA.</p>

<p>For example, let say my corpus has 1000000 documents. what <code>max_chunks</code> needs to be exactly in order to train, say, 3 passes on my corpus.</p>

<p>Any suggestion? Many many thanks</p>
","2593536","","","","","2018-11-02 07:36:01","Gensim HDP topic model: How to train on multiple passes of corpus?","<nlp><gensim><lda><topic-modeling>","2","0","1","","","CC BY-SA 3.0"
"43146077","1","43165219","","2017-03-31 17:01:18","","3","4163","<p>I trained a doc2vec (gensim.models.Doc2Vec) model and now I'm using this line:</p>

<pre><code>print(dict([(model.index2word[i], similarity) for i, similarity in enumerate(model.similar_by_word('igdumd32.dll@0x', topn=False))])['igdumd64.dll@0x'])
</code></pre>

<p>but it yields this error:
AttributeError: 'Doc2Vec' object has no attribute 'index2word'</p>

<p>I am using gensim 1.0.1</p>

<p>Can you help?</p>
","3134867","","","","","2017-04-02 06:19:28","Index2word in Gensim's Doc2vec raises an Attribute error","<python><gensim>","1","0","1","","","CC BY-SA 3.0"
"43146420","1","43150523","","2017-03-31 17:24:05","","0","519","<p>I'm loading pretrained Doc2Vec model using:</p>

<pre><code>from gensim.models import Doc2Vec
model = Doc2Vec.load('/path/to/pretrained/model')
</code></pre>

<p>I'm getting the following error:</p>

<blockquote>
  <p>AttributeError: 'module' object has no attribute 'call_on_class_only'</p>
</blockquote>

<p>Does anyone know how to fix it. The model was trained with gensim 0.13.3 and I'm using  gensim 0.12.4. </p>
","4821486","","224671","","2017-03-31 17:26:05","2017-03-31 22:22:24","Gensim: error while loading pretrained doc2vec model?","<python><gensim><doc2vec>","1","4","","","","CC BY-SA 3.0"
"43171573","1","43171638","","2017-04-02 17:55:28","","2","356","<p>word2vec uses either of the model for distributed representation of words. I was checking out the codes of gensim but it is not defined about the model used by gensim .</p>
","7342148","","","","","2017-04-02 17:59:45","can anyone tell me about the model (skipgram/ CBOW ) used by Gensim?","<python><nlp><semantics><gensim><word2vec>","1","1","","","","CC BY-SA 3.0"
"43260074","1","","","2017-04-06 15:52:26","","7","1049","<p>I need to process the topics in the LDA output (lda.show_topics(num_topics=-1, num_words=100...) and then compare what I do with the pyLDAvis graph but the topic numbers are differently numbered. Is there a way I can match them? </p>
","7821933","","","","","2017-10-19 04:22:28","Is there any way to match Gensim LDA output with topics in pyLDAvis graph?","<python-3.x><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"42539384","1","42541261","","2017-03-01 18:34:51","","1","209","<p>I could not load a doc2vec model on my computer and I got the following error. But, when I load that model on other computers, I can use that model.Therefore, I know the model was built correctly.</p>

<p>what should I do.</p>

<p>This is the code:</p>

<pre><code># coding: utf-8
from gensim.models.doc2vec import Doc2Vec
import gensim.models.doc2vec
from gensim.models.doc2vec import LabeledSentence
import os
import pickle
pth='/home/fatemeh/Step2/input-output/model/iterator'
model= Doc2Vec.load(pth+'/my_model.doc2vec')
</code></pre>

<p>This is the error:</p>

<pre><code>    Traceback (most recent call last):
  File ""CreateAnnoyIndex.py"", line 16, in &lt;module&gt;
    model= Doc2Vec.load(pth+'/my_model.doc2vec')
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.13.3-py2.7-linux-x86_64.egg/gensim/models/word2vec.py"", line 1762, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.13.3-py2.7-linux-x86_64.egg/gensim/utils.py"", line 248, in load
    obj = unpickle(fname)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.13.3-py2.7-linux-x86_64.egg/gensim/utils.py"", line 912, in unpickle
    return _pickle.loads(f.read())
EOFError
</code></pre>
","3092781","","3487667","","2017-03-01 19:35:23","2017-03-01 20:19:32","Got EOFError during loading doc2vec model","<python-2.7><pickle><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"42727181","1","","","2017-03-10 20:29:18","","5","4455","<p>I am trying to load the pretrained word vectors from Google using the following code:</p>

<pre><code>from gensim import models
w = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)
</code></pre>

<p>But I am getting an error that tells me </p>

<blockquote>
  <p>File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 197, in load_word2vec_format
      result.syn0 = zeros((vocab_size, vector_size), dtype=datatype)</p>
  
  <p>ValueError: array is too big; <code>arr.size * arr.dtype.itemsize</code> is larger than the maximum possible size.</p>
</blockquote>

<p>Could anyone suggest a possible solution. Thanks in advance.</p>
","7692334","","","","","2017-04-02 06:25:34","ValueError: array is too big when loading GoogleNews-vectors-negative","<python><gensim>","1","0","2","","","CC BY-SA 3.0"
"42753119","1","","","2017-03-12 21:03:02","","2","716","<p>I am calling load like¬†this .</p>

<p>.7/dist-packages/gensim/utils.py"", line 912, in </p>

<pre><code>  model = gensim.models.Word2Vec.load(""F:\\TrialGrounds\\gensimMODEL4\\model4"")¬†

model = super(Word2Vec, cls).load(*args, **kwargs)
¬† File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 248, in load
¬†¬†¬† obj = unpickle(fname)
¬† File ""/usr/local/lib/python2unpickle
¬†¬†¬† return _pickle.loads(f.read())
AttributeError: 'module' object has no attribute 'call_on_class_only'
</code></pre>

<p>The model has split 500mb *2 numpy arrays. Can anyone help me in figuring out this issue</p>
","1516947","","","","","2017-03-16 03:18:20","Error loading Pretrained vectors on gensim 0.12","<gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"43202548","1","","","2017-04-04 08:56:07","","5","3816","<p>Im <code>gensims</code> latest version, loading trained vectors from a file is done using <code>KeyedVectors</code>, and dosent requires instantiating a new Word2Vec object. But now my code is broken because I can't use the <code>model.vector_size</code> property. What is the alternative to that? I mean something better than just <code>kv[kv.index2word[0]].size</code>.</p>
","4809113","","","","","2017-09-04 10:02:10","gensim KeydVectors dimensions","<python-3.x><gensim>","1","0","","","","CC BY-SA 3.0"
"43321492","1","","","2017-04-10 11:03:23","","1","1259","<p>I use <code>Gensim</code> <code>Word2Vec</code> to train word sets in my database.</p>

<p>I have about 400,000 phrase(Each phrase is short. Total 700MB) in my <code>PostgreSQL</code> database.</p>

<p>This is how I train these data using <code>Django ORM</code>: </p>

<pre><code>post_vector_list = []
for post in Post.objects.all():
    post_vector = my_tokenizer(post.category.name)
    post_vector.extend(my_tokenizer(post.title))
    post_vector.extend(my_tokenizer(post.contents))
    post_vector_list.append(post_vector)
word2vec_model = gensim.models.Word2Vec(post_vector_list, window=10, min_count=2, size=300) 
</code></pre>

<p>But this job getting a lot of time and feels like not efficient.</p>

<p>Especially, creating <code>post_vector_list</code> part took a lot of time and space..</p>

<p>I want to improve speed of training but have no idea how to do.</p>

<p>Want to get your advices. Thanks.</p>
","3595632","","","","","2021-09-02 16:27:40","Word2Vec: Any way to train model fastly?","<orm><nlp><gensim><word2vec>","2","0","","","","CC BY-SA 3.0"
"43165724","1","43172119","","2017-04-02 07:25:25","","2","895","<p>I used <code>Gensim</code>'s <code>Word2Vec</code> for training most similar words.</p>

<p>My dataset is all posts from my college community site.</p>

<p>Each dataset consists of like this:</p>

<pre><code>(title) + (contents) + (all comments)  // String
</code></pre>

<p>For example, </p>

<pre><code>data[0] =&gt; ""This is title. Contents is funny. What so funny?. Not funny for me""
</code></pre>

<p>So, I have around 400,000 datas like above and make them as a vector and try to train these data via <code>Word2Vec</code>. </p>

<p>I wonder that whether it is possible to make <code>Word2Vec</code> consider WEIGHT, which means, if I give an weight to certain data vector, <code>Word2Vec</code> train this data in a way that each word in this data vector has more strong relationship(similarity).</p>

<p>For example, If I gave a weight 5 to dataset, <code>I like Pizza, Chicken</code>, the word <code>Pizza</code> and <code>Chicken</code> (or <code>like</code> and <code>Pizza</code> etc) has strong relations than other data vector's words.</p>

<p>Would that be possible?</p>

<p>Sorry for poor explanation but I'm not native english speaker. If need more detailed info, please post comment.</p>
","3595632","","","","","2017-04-02 18:40:31","Word2Vec: Is it possible to train with respect to weight in NLP?","<nlp><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"43166762","1","43172600","","2017-04-02 09:29:02","","2","827","<p>As I know of, <code>tsne</code> is reducing dimension of word vector. </p>

<p><code>Word2vec</code> is generate word embedding model with huge amount of data.</p>

<p>What is the relation between two?</p>

<p>Does <code>Word2vec</code> use <code>tsne</code> inside? </p>

<p>(I use <code>Word2vec</code> from <code>Gensim</code>)</p>
","3595632","","","","","2017-04-02 19:27:03","What is relation between tsne and word2vec?","<nlp><gensim><word2vec>","1","0","1","","","CC BY-SA 3.0"
"67629458","1","67633398","","2021-05-21 00:25:43","","0","165","<p>I'm beginner at nlp and I'm using gensim for the first time.
I noticed that some text it returns a blank summary. For example:</p>
<pre><code>from gensim.summarization import summarize
text =&quot;The continued digitization of most every sector of society and industry means that an ever-growing volume of data will continue to be generated. The ability to gain insights from these vast datasets is one key to addressing an enormous array of issues ‚Äî from identifying and treating diseases more effectively, to fighting cyber criminals, to helping organizations operate more effectively to boost the bottom line.&quot;
summarize(text, 0.6)
</code></pre>
<p>returns:
<code>''</code></p>
<p>When I have equivalent sized paragraphs in other instances it returns a summary, so I know it's not that my ratio is too small. Any insights appreciated!</p>
","11805611","","","","","2021-05-23 12:03:31","why does gensim summarize() return blank sometimes?","<python><nlp><gensim>","2","1","","","","CC BY-SA 4.0"
"52252119","1","52264245","","2018-09-10 06:22:37","","2","1676","<p>For Skip-gram word2vec training samples are obtained as follows:</p>

<pre><code>Sentence: The fox was running across the maple forest
</code></pre>

<p>The word <code>fox</code> give next pairs for training:</p>

<pre><code>fox-run, fox-across, fox-maple, fox-forest
</code></pre>

<p>and etc. for every word. CBOW w2v use reverse approach:</p>

<pre><code>run-fox, across-fox, maple-fox, forest-fox
</code></pre>

<p>or for <code>forest</code> word:</p>

<pre><code>fox-forest, run-forest, across-forest, maple-forest
</code></pre>

<p>So we get all the pairs. What's the difference between Skip-gram word2vec and CBOW w2v during training with gensim library, if we do not specify the target word when training in the CBOW-mode? In both cases all pairs of words are used, or not?</p>
","4399478","","","","","2018-09-10 19:24:58","What's the difference between Skip-gram word2vec and CBOW w2v during training with gensim library?","<python><machine-learning><nlp><gensim><word2vec>","1","0","2","","","CC BY-SA 4.0"
"52277384","1","52286130","","2018-09-11 13:43:28","","1","2279","<p>I build two word embedding (word2vec models) using <code>gensim</code> and save it as (word2vec1 and word2vec2) by using the <code>model.save(model_name)</code> command for two different corpus (the two corpuses are somewhat similar, similar means they are related like part 1 and part 2 of a book). Suppose, the top words (in terms of frequency or occurrence) for the two corpuses is the same word (let's say it as <code>a</code>). </p>

<p>How to compute the degree of similarity (<code>cosine-similarity or similarity</code>) of the extracted top word (say 'a'), for the two word2vec models? Does <code>most_similar()</code> will work in this case efficiently? </p>

<p>I want to know by how much degree of similarity, does the same word (a), is related for two different generated models?</p>

<p>Any idea is deeply appreciated.</p>
","3966705","","3966705","","2018-09-11 16:11:07","2018-09-12 01:15:31","Calculation of Cosine Similarity of a single word in 2 different Word2Vec Models","<python-3.x><gensim><word2vec><word-embedding>","1","0","1","","","CC BY-SA 4.0"
"52220514","1","52225632","","2018-09-07 10:24:05","","1","689","<p>I would like to pass the <code>X_train_word2vec</code> vector as input to <code>Gensim Word2Vec</code> model.
The vector type is <code>numpy.ndarray</code>, at example:</p>

<pre><code>X_train_word2vec[9] = array([   19,     7,     1, 20120,     2,     1,   856,   233,   671,
       1,  1208,  6016,     2,    32,     0,     0,     0,     0, ....)]
</code></pre>

<p>When I run this code:</p>

<pre><code>model_word2vec = models.Word2Vec(X_train_word2vec, size=150, window=9)
model_word2vec.train(X_train_word2vec,total_examples=X_train_word2vec.shape[0], epochs=10)
</code></pre>

<p>I get this error:</p>

<p><code>TypeError: ufunc 'add' did not contain a loop with signature matching types dtype('&lt;U11') dtype('&lt;U11') dtype('&lt;U11')</code></p>

<p>I have read <a href=""https://stackoverflow.com/questions/44527956/python-ufunc-add-did-not-contain-a-loop-with-signature-matching-types-dtype"">this</a> post, where the issue is due to different data types in the input array but, in my case, I have all the data of the same type: <code>int</code>.</p>

<p><strong>Update:</strong>
The code before <code>model_Word2Vec</code>:</p>

<pre><code>tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
sequence = tokenizer.texts_to_sequences(X)

seq_max_len = 50
X_seq = pad_sequences(sequenza, maxlen=seq_max_len,padding='post',truncating='post',dtype=int)

X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(X_seq, y_cat, test_size=0.2, random_state=123)
</code></pre>
","7387749","","7387749","","2018-09-07 10:39:47","2018-09-07 15:29:15","TypeError: ufunc 'add' did not contain a loop with signature matching types dtype","<python><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"43337242","1","","","2017-04-11 05:13:18","","0","1130","<p>I want to use a word2vec module containing tons of Indian characters. The module was trained by Facebook - <a href=""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md</a> .
(I am using Gujarati vectors)</p>

<p>I installed gensim and tries to load the module, but following error occurred:</p>

<pre><code>In [1]: import gensim  

In [2]: from gensim.models.keyedvectors import KeyedVectors
word_vectors = KeyedVectors.load_word2vec_format('wiki.gu/wiki.gu.bin', binary=True,unicode_errors='ignore')

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 32: invalid start byte
</code></pre>

<p>I tried to load the module both in python 2.7 and 3.5, failed in the same way. So how can I load the module in gensim? Thanks.</p>
","4920296","","","","","2018-04-05 20:09:08","'utf8' decode error while loading a word2vec module","<python><utf-8><nlp><gensim><word2vec>","1","4","","","","CC BY-SA 3.0"
"43357247","1","","","2017-04-11 22:29:51","","14","19484","<p>The <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/ldamodel.py"" rel=""noreferrer"">ldamodel</a> in gensim has the two methods: <code>get_document_topics</code> and <code>get_term_topics</code>. </p>

<p>Despite their use in this gensim tutorial <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_methods.ipynb"" rel=""noreferrer"">notebook</a>, I do not fully understand how to interpret the output of <code>get_term_topics</code> and created the self-contained code below to show what I mean:</p>

<pre><code>from gensim import corpora, models

texts = [['human', 'interface', 'computer'],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]

# build the corpus, dict and train the model
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, 
                                 random_state=0, chunksize=2, passes=10)

# show the topics
topics = model.show_topics()
for topic in topics:
    print topic
### (0, u'0.159*""system"" + 0.137*""user"" + 0.102*""response"" + 0.102*""time"" + 0.099*""eps"" + 0.090*""human"" + 0.090*""interface"" + 0.080*""computer"" + 0.052*""survey"" + 0.030*""minors""')
### (1, u'0.267*""graph"" + 0.216*""minors"" + 0.167*""survey"" + 0.163*""trees"" + 0.024*""time"" + 0.024*""response"" + 0.024*""eps"" + 0.023*""user"" + 0.023*""system"" + 0.023*""computer""')

# get_document_topics for a document with a single token 'user'
text = [""user""]
bow = dictionary.doc2bow(text)
print ""get_document_topics"", model.get_document_topics(bow)
### get_document_topics [(0, 0.74568415806946331), (1, 0.25431584193053675)]

# get_term_topics for the token user
print ""get_term_topics: "", model.get_term_topics(""user"", minimum_probability=0.000001)
### get_term_topics:  [(0, 0.1124525558321441), (1, 0.006876306738765027)]
</code></pre>

<p>For <code>get_document_topics</code>, the output makes sense. The two probabilities add up to 1.0, and the topic where <code>user</code> has a higher-probability (from <code>model.show_topics()</code>) has also the higher probability assigned.</p>

<p>But for <code>get_term_topics</code>, there are questions:</p>

<ol>
<li>The probabilities do not add up to 1.0, why?</li>
<li>While numerically, the topic where <code>user</code> has a higher-probability (from <code>model.show_topics()</code>) has also a higher number assigned, what does this number mean?</li>
<li>Why should we use <code>get_term_topics</code> at all, when <code>get_document_topics</code> can provide (seemingly) the same functionality and has meaningful output?</li>
</ol>
","3229995","","3229995","","2017-04-21 19:39:04","2021-03-31 20:05:40","get_document_topics and get_term_topics in gensim","<python><gensim><topic-modeling>","2","0","1","","","CC BY-SA 3.0"
"42995073","1","","","2017-03-24 08:52:07","","0","998","<p>Gensim has a tutorial saying how to, given a document/query string, say what other documents are most similar to it, in descending order:</p>

<p><a href=""http://radimrehurek.com/gensim/tut3.html"" rel=""nofollow noreferrer"">http://radimrehurek.com/gensim/tut3.html</a></p>

<p>It can also display what topics are associated with an entire model <em>at all</em>:</p>

<p><a href=""https://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python"">How to print the LDA topics models from gensim? Python</a></p>

<p>But how do you find what <em>topics</em> are associated with <em>a given document/query string</em>? Ideally with some numeric similarity metric for each topic? I haven't been able to find anything on that.</p>
","45843","","-1","","2017-05-23 12:10:13","2017-06-08 00:41:39","Displaying topics associated with a document/query in Gensim","<python><nlp><gensim><lda><topic-modeling>","1","2","","","","CC BY-SA 3.0"
"43098535","1","43107662","","2017-03-29 15:57:20","","1","1139","<p>I have studied <code>word2vec</code> implementation in gensim, I am aware that input vectors are in <code>syn0</code>, output vectors are in <code>syn1</code> and <code>syn1neg</code> if negative sampling.</p>

<p>I know I can access similarity between input and output embeddings like this:</p>

<pre><code>outv = KeyedVectors()
outv.vocab = model.wv.vocab
outv.index2word = model.wv.index2word  
outv.syn0 = model.syn1neg 
inout_similars = outv.most_similar(positive=[model['cousin']])
</code></pre>

<p>My question is, if it is possible to save output embeddings (from <code>syn1</code> or <code>syn1neg</code> matrix) as final model. For example, when <code>model.save()</code>, so that it outputs output embeddings (or where exactly in the code of <code>word2vec.py</code> I could access and modify that). I need this in order to use these output embeddings as input to classifier. I have done it previously in brute-force approach, so I would like to access output embeddings easily.</p>
","6250500","","6048114","","2017-03-29 16:08:11","2017-03-30 02:52:55","Saving output (context) embeddings in word2vec (gensim implementation) as a final model","<python><gensim><word2vec><word-embedding>","1","0","1","","","CC BY-SA 3.0"
"43213046","1","","","2017-04-04 16:41:20","","0","427","<p>Is it possible to use a gensim Random Projection to train a SVM in sklearn?<br>
I need to use <code>gensim</code>'s tfidf implementation because it's better at dealing with large inputs and then want to put that into a random projection on which I will train my SVM. I'd also be happy to just pass the tfidf model generated by <code>gensim</code> to <code>sklearn</code> and use their random projection, if that makes things easier.<br>
But so far I haven't found a way to get either model out of gensim into sklearn.  </p>

<p>I have tried using <code>gensim.matutils.corpus2csc</code>but of course that doesn't work: neither TfidfModel nor RpModel are corpi, so now I'm clueless at what to try next.</p>
","1866639","","","","","2018-03-12 20:41:17","Use gensim Random Projection in sklearn SVM","<scikit-learn><tf-idf><gensim>","1","0","","","","CC BY-SA 3.0"
"52297260","1","52297543","","2018-09-12 14:12:38","","1","296","<p>PyCharm can't find gensim that is listed in ""anaconda list"". in anaconda list I can see gensim but it does not exist in project interpreter!!!
I'm using paython version 3.7. I have no problem with other libraries, the problem is just in gensim installation </p>

<p><img src=""https://i.stack.imgur.com/PcrFs.png"" alt=""anaconda list""></p>

<p><img src=""https://i.stack.imgur.com/pHN8L.png"" alt=""project interpreter list""></p>

<p>If anyone has any advice, I'd appreciate it! Thanks.</p>
","2358233","","2308683","","2018-09-12 14:31:18","2018-09-12 14:31:18","'gensim' is not recognized in pycharm","<python><python-3.x><pycharm><gensim>","1","4","","","","CC BY-SA 4.0"
"56715394","1","56729625","","2019-06-22 12:17:33","","0","780","<p>I am trying to use the English Wikipedia dump (<a href=""https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2</a>) as my pre-trained word2vec model using <code>Gensim</code>.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

model_path = 'enwiki-latest-pages-articles.xml.bz2'
w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)
</code></pre>

<p>when I do this, I get</p>

<pre><code>   342     with utils.smart_open(fname) as fin:
    343         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 344         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    345         if limit:
    346             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: '&lt;mediawiki'
</code></pre>

<p>Do I have to re-download or something?</p>
","8836582","","","","","2019-06-24 03:50:04","How do I use the wikipedia dump as a Gensim model?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"56707416","1","","","2019-06-21 16:39:13","","4","1328","<p>The code below takes forever to execute. Probably due to the large size of the dictionary. Is there a way to make it faster, by e.g. cropping the visualized data?</p>

<pre><code>vis = pyLDAvis.gensim.prepare(lda, corpus, id2word)
</code></pre>
","672018","","","","","2019-08-19 07:35:49","pyLDAvis prepare() is slow","<python><gensim><lda>","1","2","","","","CC BY-SA 4.0"
"48717970","1","48778511","","2018-02-10 06:32:46","","1","692","<p>I am trying to implement something similar in <a href=""https://arxiv.org/pdf/1603.04259.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1603.04259.pdf</a> using awesome gensim library however I am having trouble improving quality of results when I compare to Collaborative Filtering.</p>

<p>I have two models one built on Apache Spark and other one using gensim Word2Vec on grouplens 20 million ratings dataset. My apache spark model is hosted on AWS <a href=""http://sparkmovierecommender.us-east-1.elasticbeanstalk.com"" rel=""nofollow noreferrer"">http://sparkmovierecommender.us-east-1.elasticbeanstalk.com</a>
and I am running gensim model on my local. However when I compare the results I see superior results with CF model 9 out of 10 times(like below example more similar to searched movie - affinity towards Marvel movies) </p>

<p>e.g.:- If I search for Thor movie I get below results </p>

<p><strong><em>Gensim</em></strong></p>

<ul>
<li>Captain America: The First Avenger (2011) </li>
<li>X-Men: First Class (2011)</li>
<li>Rise of the Planet of the Apes (2011) </li>
<li>Iron Man 2 (2010) </li>
<li>X-Men Origins: Wolverine (2009) </li>
<li>Green Lantern (2011) </li>
<li>Super 8 (2011) </li>
<li>Tron:Legacy (2010) </li>
<li>Transformers: Dark of the Moon (2011)</li>
</ul>

<p><strong><em>CF</em></strong></p>

<ul>
<li>Captain America: The First Avenger</li>
<li>Iron Man 2</li>
<li>Thor: The Dark World</li>
<li>Iron Man</li>
<li>The Avengers</li>
<li>X-Men: First Class</li>
<li>Iron Man 3</li>
<li>Star Trek</li>
<li>Captain America: The Winter Soldier</li>
</ul>

<p>Below is my model configuration, so far I have tried playing with window, min_count and size parameter but not much improvement.</p>

<pre><code>word2vec_model = gensim.models.Word2Vec(
    seed=1,
    size=100, 
    min_count=50, 
    window=30)

word2vec_model.train(movie_list, total_examples=len(movie_list), epochs=10)
</code></pre>

<p>Any help in this regard is appreciated.</p>
","3048081","","3048081","","2018-02-10 06:49:55","2018-02-14 01:53:07","Gensim: Word2Vec Recommender accuracy Improvement","<word2vec><gensim><recommendation-engine>","1","2","","","","CC BY-SA 3.0"
"31425123","1","","","2015-07-15 08:22:03","","1","100","<p>As far as I know, doc2vec computes both embeddings for documents and words. Can I use  a word vector and a document vector  to estimate the similarity of a word to a document or only documents against documents and words against words? Any remark would be helpful.</p>
","4274465","","","","","2016-03-25 20:38:27","Comparing words with documents","<gensim><word2vec>","0","0","","","","CC BY-SA 3.0"
"56757166","1","","","2019-06-25 15:11:35","","1","188","<blockquote>
  <p>I receive an error when trying to upload a pre-trained word2vec file
  (compiled with fasttext) using Gensim. File has '.vec' extension and
  can be found here:
  <a href=""http://89.38.230.23/word_embeddings/we/corola.300.20.vec.zip"" rel=""nofollow noreferrer"">http://89.38.230.23/word_embeddings/we/corola.300.20.vec.zip</a></p>
  
  <p>What I've tried so far: Option 1: KeyedVectors from gensim.models
  Option 2: FastText wrapper</p>
</blockquote>

<pre><code>#Option 1
    from gensim.models import KeyedVectors
    model = KeyedVectors.load_word2vec_format('Word_embeddings/corola.300.20.vec', binary=True)
######

#Option 2
    from gensim.models.wrappers import FastText
    model = FastText.load_word2vec_format('Word_embeddings/corola.300.20.vec')
</code></pre>

<blockquote>
  <p>Error option 1: UnicodeDecodeError: 'utf-8' codec can't decode byte
  0x9b in position 0: invalid start byte</p>
  
  <p>Deprecation Error option 2: DeprecationWarning: Deprecated. Use
  gensim.models.KeyedVectors.load_word2vec_format instead.</p>
  
  <p>I need the correct method to successfully upload the word2vec file,
  using gensim.</p>
  
  <p>Thank you.</p>
</blockquote>
","8490994","","","","","2019-06-26 09:55:10","Word2Vec: Error received at uploading a pre-trained word2vec file using Gensim","<python><nlp><gensim><word2vec><fasttext>","1","1","","","","CC BY-SA 4.0"
"42913090","1","","","2017-03-20 20:12:19","","1","125","<p>Would like to  obtain an identifiable list of most relevant documents (to LDA model) using gensim, i.e. exactly as the OP in the link below</p>

<p><a href=""https://groups.google.com/forum/#!topic/gensim/lHi2MhoNDsY"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/gensim/lHi2MhoNDsY</a> has the relevant answer, but not sure of how to get this to work:</p>

<pre><code>tops = sorted(zip(my_ids, all_documents)), reverse=True, \
key=lambda my_id, doc: abs(dict(doc).get(topic_number, 0.0))) 
</code></pre>

<p>Apart from a parenthesis that seems to be in the wrong place, the two main questions are:</p>

<ol>
<li>How should my_ids be generated (do these map to the documents or to the words)? Something like <code>my_ids = [my_id for my_id in range(len(all_documents))]</code>
throws </li>
</ol>

<p>lambda () missing 1 required positional argument: 'doc'</p>

<ol start=""2"">
<li>What is the reason for this error, and how should <code>key=lambda my_id, doc: abs(dict(doc).get(topic_number, 0.0)))</code> be understood? Would <code>abs(dict(doc).get(topic_number, 0.0))</code> be unpacked as needed if the correct <code>my_ids</code> is supplied, or is some other fix needed?
Also have had a look at <a href=""https://stackoverflow.com/questions/26543349/python-3-map-lambda-method-with-2-inputs"">python 3 map/lambda method with 2 inputs</a>, but still unenlightened so far.</li>
</ol>
","7741934","","-1","","2017-05-23 11:46:21","2017-03-20 20:40:49","Gensim/Python - mapping document ids to documents, in sorted()","<python><lambda><gensim>","0","0","","","","CC BY-SA 3.0"
"43065843","1","43081934","","2017-03-28 09:32:41","","1","1559","<p>I want to make word2vec with gensim. I heard that vocabulary corpus should be unicode so I converted it to unicode.</p>

<pre><code># -*- encoding:utf-8 -*-
# !/usr/bin/env python
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
from gensim.models import Word2Vec
import pprint

with open('parsed_data.txt', 'r') as f:
    corpus = map(unicode, f.read().split('\n'))

model = Word2Vec(size=128, window=5, min_count=5, workers=4)
model.build_vocab(corpus,keep_raw_vocab=False)
model.train(corpus)
model.save('w2v')

pprint.pprint(model.most_similar(u'ÎÑà'))
</code></pre>

<p>Above is my source code. It seems like work well. However there are problem with vocabulary key. I want to make korean word2vec which use unicode. For example word <code>ÏÇ¨Í≥º</code> which means apology in english and it's unicode is <code>\xC0AC\xACFC</code> If I try to find <code>ÏÇ¨Í≥º</code> in word2vec, key error occur...<br>
Instead of <code>\xc0ac\xacfc</code> <code>\xc0ac</code> and <code>\xacfc</code> stores separately. 
What's the reason and how to solve it?</p>
","4929293","","","","","2017-03-28 23:56:15","Python Gensim word2vec vocabulary key","<python><unicode><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"67687962","1","67690719","","2021-05-25 12:30:41","","2","1806","<p>I am trying to build a Word2vec model but when I try to reshape the vector for tokens, I am getting this error. Any idea ?</p>
<pre><code>wordvec_arrays = np.zeros((len(tokenized_tweet), 100)) 
for i in range(len(tokenized_tweet)):
    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 100)
wordvec_df = pd.DataFrame(wordvec_arrays) 
wordvec_df.shape

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-101-71156bf1c4a3&gt; in &lt;module&gt;
      1 wordvec_arrays = np.zeros((len(tokenized_tweet), 100))
      2 for i in range(len(tokenized_tweet)):
----&gt; 3     wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 100)
      4 wordvec_df = pd.DataFrame(wordvec_arrays)
      5 wordvec_df.shape

&lt;ipython-input-100-e3a82e60af93&gt; in word_vector(tokens, size)
      4     for word in tokens:
      5         try:
----&gt; 6             vec += model_w2v[word].reshape((1, size))
      7             count += 1.
      8         except KeyError: # handling the case where the token is not in vocabulary

TypeError: 'Word2Vec' object is not subscriptable
</code></pre>
","8020986","","","","","2021-09-08 16:23:21","TypeError: 'Word2Vec' object is not subscriptable","<python-3.x><jupyter-notebook><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"52286330","1","52300508","","2018-09-12 01:44:58","","2","942","<p>I am working with Gensim library to train some data files using doc2vec,  while trying to test the similarity of one of the files using the method <code>model.docvecs.most_similar(""file"")</code> , I always get all the results above 91% with almost no difference between them (which is not logic), because the files do not have similarities between them. so the results are inaccurate.<br/><br/>
<strong>Here is the code for training the model</strong><br/></p>

<pre><code>model = gensim.models.Doc2Vec(vector_size=300, min_count=0, alpha=0.025, min_alpha=0.00025,dm=1)
model.build_vocab(it)
for epoch in range(100):
    model.train(it,epochs=model.iter, total_examples=model.corpus_count)
    model.alpha -= 0.0002
    model.min_alpha = model.alpha
model.save('doc2vecs.model')
model_d2v = gensim.models.doc2vec.Doc2Vec.load('doc2vecs.model')
sim = model_d2v.docvecs.most_similar('file1.txt')
print sim
</code></pre>

<p><br/> 
<strong>this is the output result</strong></p>

<blockquote>
<p>
[('file2.txt', 0.9279470443725586), ('file6.txt', 0.9258157014846802), ('file3.txt', 0.92499840259552), ('file5.txt', 0.9209873676300049), ('file4.txt', 0.9180108308792114), ('file7.txt', 0.9141069650650024)]
</p>
</blockquote>

<p>what am I doing wrong? how could I improve the accuracy of results?</p>
","7700300","","","","","2018-09-12 17:23:05","Inaccurate similarities results by doc2vec using gensim library","<python><nlp><gensim><doc2vec>","2","0","3","","","CC BY-SA 4.0"
"43343034","1","","","2017-04-11 10:13:58","","2","743","<p>I am able to create the lda model and save it. Now I am trying load the model, and pass a new document</p>

<p><code>lda = LdaModel.load('..\\models\\lda_v0.1.model')
doc_lda = lda[new_doc_term_matrix]
print(doc_lda )</code></p>

<p>On printing the doc_lda I am getting the object. <code>&lt;gensim.interfaces.TransformedCorpus object at 0x000000F82E4BB630&gt;</code>
 However I want to get the topic words associated with it. What is the method I have to use. I was referring to <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">this</a>.</p>
","4145051","","4145051","","2017-04-12 06:41:49","2020-10-28 01:18:45","How to view and interpret the output of lda model using gensim","<python-3.x><gensim><lda><topic-modeling>","2","0","","","","CC BY-SA 3.0"
"56793969","1","56819398","","2019-06-27 15:05:26","","0","734","<p>I have different LDA models (on the same text, but all with different #topics) stored in one list. Now, I want to save this list with all the models in it to my disk. However, I am not sure how this works. Should I treat is as a list or as a LDA model?</p>

<p>On the <a href=""https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.save"" rel=""nofollow noreferrer"">gensim website</a> I found the following code:</p>

<pre><code>from gensim.test.utils import datapath
&gt;&gt;&gt;
&gt;&gt;&gt; # Save model to disk.
&gt;&gt;&gt; temp_file = datapath(""model"")
&gt;&gt;&gt; lda.save(temp_file)
</code></pre>

<p>However, this works for separate LDA models, not for lists with multiple models. What is the best way to save my list of models?</p>
","7714681","","","","","2019-07-05 18:02:20","How to save a list of Gensim LDA models?","<python><list><save><gensim>","2","1","","","","CC BY-SA 4.0"
"59270197","1","","","2019-12-10 15:06:41","","0","263","<p>I got a file regarding pre-trained data from this link: (.bin file)<a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>

<p>and I want to use this file on input layer of neural network(maybe its' forms vector)</p>

<p>I want pre-trained data learn more of training data. ( let' call training data as data.txt.)</p>

<p>and I have loaded pre-trained model using fasttext 
and genism library however I don't know how to train them more.</p>

<pre><code>import fasttext
model = fasttext.load_model('model.bin')
#whatever using data.txt
</code></pre>

<p>or</p>

<pre><code>from gensim.models import FastText
model = FastText.load_fasttext_format('model.bin')
#whatever using data.txt
</code></pre>

<p>please advice.</p>
","12512247","","12512247","","2019-12-13 11:17:10","2019-12-13 11:17:10","How can fastText or Gensim train additional data from pre-trained data (.bin)?","<gensim><bin><fasttext>","0","2","","","","CC BY-SA 4.0"
"56835032","1","","","2019-07-01 11:39:01","","0","373","<p>I would like to find bigrams in a large corpus in text format. As the corpus cannot be loaded at once in memory and its lines are very big, I load it by chunks, each 1 kb</p>

<pre><code>def read_in_chunks(filename, chunk_size=1024):
    """"""Lazy function (generator) to read a file piece by piece.
    Default chunk size: 1k.""""""
    while True:
        data = filename.read(chunk_size)
        if not data:
            break
        yield data

</code></pre>

<p>Then I want to go piece by piece through the corpus and find bigrams and I use the gensim Phrases() and Phraser() functions, but while training, my model constantly loses state. Thus, I tried to save and reload the model after each megabyte that I read and then free the memory, but it still loses state. My code is here:</p>

<pre><code>with open(""./final/corpus.txt"", ""r"", encoding='utf8') as r:
    max_vocab_size=20000000
    phrases = Phrases(max_vocab_size=max_vocab_size)
    i=1
    j=1024
    sentences = """"
    for piece in read_in_chunks(r):   

        if i&lt;=j:
            sentences = sentences + piece

        else:
            phrases.add_vocab(sentences)
            phrases = Phrases(sentences)
            phrases = phrases.save('./final/phrases.txt')
            phrases = Phraser.load('./final/phrases.txt')

            sentences = """"
            j+=1024
        i+=1
print(""Done"")

</code></pre>

<p>Any suggestion?
Thank you.</p>
","11410896","","","","","2019-07-01 17:25:24","Detect bigram collection on a very large text dataset","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"59282572","1","59293099","","2019-12-11 09:29:46","","2","3401","<p>I would like to load pretrained multilingual word embeddings from the fasttext library with gensim; here the link to the embeddings:</p>

<p><a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>

<p>In particular, I would like to load the following word embeddings: </p>

<ul>
<li>cc.de.300.vec (4.4 GB) </li>
<li>cc.de.300.bin (7 GB)</li>
</ul>

<p>Gensim offers the following two options for loading fasttext files:</p>

<ol>
<li><p><code>gensim.models.fasttext.load_facebook_model(path, encoding='utf-8')</code>    </p>

<blockquote>
  <ul>
  <li><em>Load the input-hidden weight matrix from Facebook‚Äôs native fasttext
  .bin output file.</em></li>
  <li><em>load_facebook_model() loads the full model, not just
  word embeddings, and enables you to continue model training.</em></li>
  </ul>
</blockquote></li>
<li><p><code>gensim.models.fasttext.load_facebook_vectors(path, encoding='utf-8')</code></p>

<blockquote>
  <ul>
  <li><em>Load word embeddings from a model saved in Facebook‚Äôs native fasttext .bin format.</em></li>
  <li><em>load_facebook_vectors() loads the word embeddings only. Its faster, but does not enable you to continue training.</em></li>
  </ul>
</blockquote></li>
</ol>

<p>Source Gensim documentation: 
<a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model</a></p>

<p>Since my laptop has only 8 GB RAM, I am continuing to get MemoryErrors or the loading takes a very long time (up to several minutes).</p>

<p>Is there an option to load these large models from disk more memory efficient?</p>
","7256554","","","","","2019-12-11 19:54:18","Memory efficiently loading of pretrained word embeddings from fasttext library with gensim","<python><nlp><gensim><word-embedding><fasttext>","1","0","","","","CC BY-SA 4.0"
"67719858","1","","","2021-05-27 09:59:19","","0","66","<p>I am trying to find words similar to a word I give the code. But somehow everything goes fine till the end, where it says the word doesn't exist. Could someone help me?</p>
<pre><code>from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec

glove_file = datapath('/content/drive/MyDrive/Colab Notebooks/glove.6B.300d.txt')
tmp_file = get_tmpfile('/content/drive/MyDrive/Colab Notebooks/word2vec-glove.6B.300d.txt')

_ = glove2word2vec(glove_file, tmp_file)

model = KeyedVectors.load_word2vec_format(tmp_file)

model.most_similar(positive=[&quot;new&quot;], topn=10)
</code></pre>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-32-0752cfe838d7&gt; in &lt;module&gt;()
     10 model = KeyedVectors.load_word2vec_format(tmp_file)
     11 
---&gt; 12 model.most_similar(positive=[&quot;new&quot;], topn=10)

1 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py in word_vec(self, word, use_norm)
    450             return result
    451         else:
--&gt; 452             raise KeyError(&quot;word '%s' not in vocabulary&quot; % word)
    453 
    454     def get_vector(self, word):

KeyError: &quot;word 'new' not in vocabulary&quot;
</code></pre>
","13799189","","13799189","","2021-05-27 15:53:12","2021-05-27 17:35:39","Finding similar words using word2vec and gensim, throwing error","<python><google-colaboratory><gensim><word2vec><drive>","1","0","","","","CC BY-SA 4.0"
"59268044","1","62651960","","2019-12-10 13:12:54","","1","109","<p>I am trying to measure the Word Mover's Distance between a lot of texts using Gensim's Word2Vec tools in Python. I am comparing each text with all other texts, so I first use itertools to create pairwise combinations like <code>[1,2,3] -&gt; [(1,2), (1,3), (2,3)]</code>. For memory's sake, I don't do the combinations by having all texts repeated in a big dataframe, but instead make a reference dataframe <code>combinations</code> with indices of the texts, which looks like:</p>

<pre><code>    0   1
0   0   1
1   0   2
2   0   3
</code></pre>

<p>And then in the comparison function I use these indices to look up the text in the original dataframe. The solution works fine, but I am wondering whether I would be able to it with big datasets. For instance I have a 300.000 row dataset of texts, which gives me about a 100 year's worth of computation on my laptop:</p>

<pre><code>C2‚Äã(300000) = 300000‚Äã! / (2!(300000‚àí2))!
           = 300000‚ãÖ299999‚Äã / 2 * 1
           = 44999850000 combinations
</code></pre>

<p>Is there any way this could be optimized better?</p>

<p>My code right now:</p>

<pre><code>import multiprocessing
import itertools
import numpy as np
import pandas as pd
import dask.dataframe as dd
from dask.diagnostics import ProgressBar
from gensim.models.word2vec import Word2Vec
from gensim.corpora.wikicorpus import WikiCorpus

def get_distance(row):
    try: 
        sent1 = df.loc[row[0], 'text'].split()
        sent2 = df.loc[row[1], 'text'].split()
        return model.wv.wmdistance(sent1, sent2)  # Compute WMD
    except Exception as e:
        return np.nan

df = pd.read_csv('data.csv')

# I then set up the gensim model, let me know if you need that bit of code too.

# Make pairwise combination of all indices
combinations = pd.DataFrame(itertools.combinations(df.index, 2))

# To dask df and apply function
dcombinations = dd.from_pandas(combinations, npartitions= 2 * multiprocessing.cpu_count())
dcombinations['distance'] = dcombinations.apply(get_distance, axis=1)
with ProgressBar():
    combinations = dcombinations.compute()
</code></pre>
","7891326","","7891326","","2020-07-23 12:45:31","2020-07-23 12:45:31","Can I optimize this Word Mover's Distance look-up function?","<python><python-multiprocessing><dask><gensim><wmd>","1","0","","","","CC BY-SA 4.0"
"48733918","1","","","2018-02-11 16:49:51","","0","2909","<p>I'm having trouble with the most_similar method in Gensim's Doc2Vec model. When I run most_similar I only get the similarity of the first 10 tagged documents (based on their tags-always from 0-9). For this code I have topn=5, but I've used topn=len(documents) and I still only get the similarity for the first 10 documents</p>

<p>Tagged documents:</p>

<pre><code>tokenizer = RegexpTokenizer(r'\w+')
taggeddoc=[]

for index,wod in enumerate(model_data):
    wordslist=[]
    tagslist=[]
    tokens = tokenizer.tokenize(wod)

    td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(tokens))).split(), str(index)) 
    taggeddoc.append(td)

documents=taggeddoc
</code></pre>

<p>Instantiate the model:</p>

<pre><code>model=gensim.models.Doc2Vec(documents, dm=0, dbow_words=1, iter=1, alpha=0.025, min_alpha=0.025, min_count=10)
</code></pre>

<p>Train the model:</p>

<pre><code>for epoch in range(100):
    if epoch % 10 == 0:
        print(""Training epoch {}"".format(epoch))
    model.train(documents, total_examples=model.corpus_count, epochs=model.iter)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
</code></pre>

<p>Problem is here (I think):</p>

<pre><code>new = model_data[100].split()
new_vector = model.infer_vector(new)
sims = model.docvecs.most_similar([new_vector], topn=5)
print(sims)
</code></pre>

<p>Output:</p>

<pre><code>[('3', 0.3732905089855194), ('1', 0.36121609807014465), ('7', 0.35790640115737915), ('9', 0.3569292724132538), ('2', 0.3521473705768585)]
</code></pre>

<p>Length of documents is the same before and after training the model. Not sure why it's only returning similarity for the first 10 documents.</p>

<p>Side question: In anyone's experience, is it better to use Word2Vec or Doc2Vec if the input documents are very short (~50 words) and there are >2,000 documents? Thanks for the help!</p>
","7228012","","","","","2018-02-12 03:46:40","Gensim Doc2Vec Most_Similar","<python><nlp><deep-learning><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"43070656","1","","","2017-03-28 13:16:37","","8","6173","<p>I have a <code>Word2Vec</code> model which is trained in <code>Gensim</code>. How can I use it in <code>Tensorflow</code> for <code>Word Embeddings</code>. I don't want to train Embeddings from scratch in Tensorflow. Can someone tell me how to do it with some example code?</p>
","1215889","","","","","2017-03-28 19:45:55","How to use pretrained Word2Vec model in Tensorflow","<python><tensorflow><gensim><word2vec><word-embedding>","1","0","1","","","CC BY-SA 3.0"
"43249717","1","69514475","","2017-04-06 08:27:18","","8","766","<p>I want to use word2vec with PySpark to process some data.
I was previously using Google trained model <code>GoogleNews-vectors-negative300.bin</code> with <code>gensim</code> in Python.</p>

<p>Is there a way I can load this bin file with <code>mllib.word2vec</code> ?
Or does it make sense to export the data as a dictionary from Python <code>{word : [vector]}</code> (or <code>.csv</code> file) and then load it in <code>PySpark</code>? </p>

<p>Thanks</p>
","5706548","","8805315","","2019-10-09 07:27:58","2021-10-10 10:48:58","Pyspark - Load trained model word2vec","<python><load><pyspark><gensim><word2vec>","1","1","3","","","CC BY-SA 4.0"
"56791240","1","","","2019-06-27 12:39:57","","0","304","<p>I have a structured dataset with columns 'text' and 'topic'. Someone has already conducted a word embedding/topic modeling so each row in 'text' is assigned a topic number (1-200). I would like to create a new data frame with the topic number and the top 5-10 key words that represent that topic.</p>

<p>I've done this before, but I usually start from scratch and run an LDA model. Then use the objects created by the LDA to find keywords per topic. That said, I'm starting from a mid-point that my supervisor gave me, and it's throwing me off.</p>

<p>The data structure looks like below:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'text': ['foo bar baz', 'blah bling', 'foo'], 
               'topic': [1, 2, 1]})
</code></pre>

<p>So would the plan be to create a bag of words, groupby 'topic,' and count the words? Or is there a keywords function and group by a column option that I don't know about in gensim or nltk?</p>
","11599196","","","","","2019-06-27 15:40:06","Extract key words by topic","<python><nlp><nltk><gensim>","2","5","","","","CC BY-SA 4.0"
"67698932","1","","","2021-05-26 05:21:17","","0","13","<p>I just tested the same sets of documents with different orders, and surprisingly, I found that ngrams from the documents were slightly different. Is this right or wrong? I tried to figure out other factors but failed to find any differences in the documents except for the order.</p>
<p>So, I guessed that Gensim's phrase model is a probabilistic model (i.e., a Markov model) and it was affected by the order of documents as updating probabilities based on words in documents.</p>
<p>Am I right? or are there some other reasons for the difference?</p>
","15474691","","","","","2021-05-26 05:21:17","Does the order of documents affect the result of phrase model of Gensim?","<nlp><gensim><phrase>","0","1","","","","CC BY-SA 4.0"
"59292179","1","","","2019-12-11 18:46:09","","0","62","<p><strong>I have this code</strong></p>

<p>from gensim import models
import numpy as np</p>

<p><strong>Create the TF-IDF model</strong>
tfidf = models.TfidfModel(corpus, smartirs= ""ntc)</p>

<p><strong>Show the TF-IDF weights</strong>
for doc in tfidf[corpus]:
    print([[dictionary[id], np.around(freq, decimals=7)] for id, freq in doc])</p>

<p><strong>Then, I have this result</strong> </p>

<p>[['brocolli', 0.5491093], ['brother', 0.1237955], ['but', 0.1237955], ['eat', 0.568619], ['good', 0.3660728], ['like', 0.2843095], ['not', 0.1830364], ['rata', 0.1237955], ['saluut', 0.2843095]]
[['brother', 0.1647724], ['rata', 0.1647724], ['around', 0.3784174], ['basebal', 0.3784174], ['drive', 0.1647724], ['lot', 0.3784174], ['mother', 0.2436224], ['practic', 0.3784174], ['spend', 0.3784174], ['time', 0.3784174]]<code>enter code here</code>
[['rata', 0.1335974], ['drive', 0.1335974], ['and', 0.3068207], ['blood', 0.3068207], ['caus', 0.3068207], ['expert', 0.3068207], ['health', 0.197529], ['increas', 0.3068207], ['may', 0.3068207], ['pressur', 0.197529], ['some', 0.3068207], ['suggest', 0.3068207],['tension',0.3068207], ['that', 0.197529]]</p>

<p>I want to remove all the tuples &lt; 0.2 
For exaple the word ""DRIVE""= 0.1335 I want to remove this tuple. How can I do this?</p>
","11985685","","","","","2019-12-11 18:46:09","How can I remove a tuple from my corpus TF-IDF?","<python><tuples><gensim><tf-idf>","0","2","","","","CC BY-SA 4.0"
"56816261","1","","","2019-06-29 08:50:28","","0","776","<p>I am new to NLP, how to find the similarity between 2 sentences and also how to print scores of each word. And also how to implement the gensim word2Vec model.</p>

<p>Try this code:
here my two sentences :</p>

<pre><code>sentence1=""I am going to India""
sentence2="" I am going to Bharat""
from gensim.models import word2vec
import numpy as np


words1 = sentence1.split(' ')
words2 = sentence2.split(' ')

#The meaning of the sentence can be interpreted as the average of its words
sentence1_meaning = word2vec(words1[0])
count = 1
for w in words1[1:]:
    sentence1_meaning = np.add(sentence1_meaning, word2vec(w))
    count += 1
sentence1_meaning /= count

sentence2_meaning = word2vec(words2[0])
count = 1
for w in words2[1:]:
    sentence2_meaning = np.add(sentence2_meaning, word2vec(w))
    count += 1
sentence2_meaning /= count

#Similarity is the cosine between the vectors
similarity = np.dot(sentence1_meaning, sentence2_meaning)/(np.linalg.norm(sentence1_meaning)*np.linalg.norm(sentence2_meaning))
</code></pre>
","10153905","","","","","2019-06-29 17:02:13","How to find the score for sentence Similarity using Word2Vec","<nlp><gensim><word2vec>","2","1","","","","CC BY-SA 4.0"
"67707418","1","","","2021-05-26 14:51:54","","1","230","<p>Error image1:<br />
<a href=""https://i.stack.imgur.com/pnKVb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pnKVb.png"" alt=""Error image1"" /></a>    Error image2:<br />
<a href=""https://i.stack.imgur.com/vEtXu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vEtXu.png"" alt=""Error image2"" /></a></p>
<p>I am new to study word2vec model. Here is the code, I want to use sentences list to finetune word2vec model (From Gensim 4.1.0)</p>
<pre><code>from gensim.models import Word2Vec, KeyedVectors
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '2'

sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],
            ['this', 'is', 'the', 'second', 'sentence'],
            ['yet', 'another', 'sentence'],
            ['one', 'more', 'sentence'],
            ['and', 'the', 'final', 'sentence']]

# load GoogleNews-vectors-negative300.bin
model = Word2Vec(sentences, vector_size=300, min_count=1, epochs=10)
model.build_vocab(sentences)
total_examples = model.corpus_count
print('total_examples:', total_examples)

model.wv.intersect_word2vec_format(&quot;GoogleNews-vectors**strong text**-negative300.bin&quot;, binary=True, lockf=1.0)

print('success')
model.train(sentences, total_examples=total_examples, epochs=model.epochs)
model.save(&quot;word2vec_model1&quot;)
</code></pre>
<p>I first got error below:</p>
<blockquote>
<p>model.wv.intersect_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;,
binary=True, lockf=1.0) self.vectors_lockf[self.get_index(word)] =
lockf  # lock-factor: 0.0=no changes IndexError: index 12 is out of
bounds for axis 0 with size 1</p>
</blockquote>
<p>Then I look into resource code in keyedvectores.py, code as below:</p>
<pre><code>if word in self.key_to_index:
     overlap_count += 1
     self.vectors[self.get_index(word)] = weights
     self.vectors_lockf[self.get_index(word)] = lockf  # lock-factor: 0.0=no changes
</code></pre>
<p>I use ctrl+right click &quot;vectors_lockf&quot; to see this variance's declaration, but it shows &quot;cannot find declaration to go&quot;...</p>
<p>After that, when debugging, I find vectors_lockf is ndarray{ (1,)}[1.], but I don't know how it's generated... which code generates it.</p>
","11118722","","1839439","","2021-07-06 11:50:47","2021-07-06 11:50:47","Index out of bounds error with Gensim 4.0.1 Word2Vec model","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"56889408","1","56907436","","2019-07-04 13:49:34","","0","349","<p>I'm starting to get familiar with Word2Vec, but I'm struggeling with a problem and coudln't find something similar...
I want to use gensims Word2Vec on an imported PDF document (a book). To import I used PyPDF2 and stored the whole book into a list. Furthermore, I used gensims simple_preprocess in order to preprocess the data. This worked so far, I got the following output:</p>

<pre class=""lang-py prettyprint-override""><code>text=['schottky','diode','semiconductors',...]
</code></pre>

<p>So then I tried to use the Word2Vec:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec
model=Word2Vec(text, size=100, window=5, min_count=5, workers=4)
words=list(model.wv.vocab)

</code></pre>

<p>but the output was like this:</p>

<pre class=""lang-py prettyprint-override""><code>print(words)
['c','h','t','k','d',...]
</code></pre>

<p>I expected also the same words as in the text list and not just some characters. When I tried to find relations between words (e.g. 'schottky' and 'diode') I got the error-message that none of these words is included in the vocabulary.</p>

<p>My first thought was that the import is wrong, but I got the same result with textract instead of PyPDF2.</p>

<p>Does someone know what's the problem? Thanks for your help!</p>

<p>Appendix:</p>

<p>Importing the book</p>

<p>content_text=[]
number_of_inputs=len(os.listdir(path))</p>

<pre><code>    file_to_open=path
open_file=open(file_to_open,'rb')
read_pdf=PyPDF2.PdfFileReader(open_file)
number_of_pages=read_pdf.getNumPages()
page_content=""""
for page_number in range(number_of_pages):
    page = read_pdf.getPage(page_number)
    page_content += page.extractText()
content_text.append(page_content)
</code></pre>
","9811883","","","","","2019-07-05 17:53:21","Gensim Word2Vec Vocabulary: Unclear output","<python><python-3.x><text-mining><gensim><word2vec>","2","1","1","","","CC BY-SA 4.0"
"42986405","1","43067907","","2017-03-23 20:30:58","","24","24135","<p>I'm building a chatbot so I need to vectorize the user's input using Word2Vec. </p>

<p>I'm using a pre-trained model with 3 million words by Google (GoogleNews-vectors-negative300).</p>

<p>So I load the model using Gensim:</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>The problem is that it takes about 2 minutes to load the model. I can't let the user wait that long.</p>

<p>So what can I do to speed up the load time?</p>

<p>I thought about putting each of the 3 million words and their corresponding vector into a MongoDB database. That would certainly speed things up but intuition tells me it's not a good idea.</p>
","4333199","","4333199","","2017-03-29 18:42:16","2019-10-17 10:36:30","How to speed up Gensim Word2vec model load time?","<deep-learning><gensim><word2vec>","4","0","17","","","CC BY-SA 3.0"
"56712365","1","","","2019-06-22 03:33:15","","1","917","<p>I am new to LDA and when I am calculating the coherence score for my LDA model using gensim CoherenceModel, it takes extremely long time to run. However, the training part is relatively fast, and in a reasonable time. I wonder if this is because of my data size(about 250000 long text) and are there any ways to speed up this process? Thanks</p>

<p>This is my code, which is exactly the same as the tutorial</p>

<pre><code>from gensim.models import CoherenceModel
coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=LDA_, dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
</code></pre>
","11683888","","11683888","","2019-06-22 13:17:12","2019-06-29 17:23:46","Why it is so slow when calculating the coherence score for LDA using gensim","<nlp><gensim><lda>","1","1","","","","CC BY-SA 4.0"
"56837259","1","","","2019-07-01 14:15:05","","0","551","<p>I cannot install gensim==3.5.0 in my elastic beanstalk environment (python 3.4). I get an error that gensim needs python >= 3.5 to run.</p>

<p>This was not a problem until a mid-day deployment today, that made only project code changes, nothing related to elastic beanstalk, requirements or settings.</p>

<p>At the same time, I'm succesfully running the same version in another identical environment. That means the same pip, same python version, same required dependencies.</p>

<p>I tried lowering the gensim requirement to gensim==0.13.4 which officially supports python 3.4, but I get the same error.</p>

<p>EDIT: I managed to make things work by installing gensim==0.10.0 and then redeploying with gensim=3.5.0. I still don't know the cause of the issue and the solution is not really a solution, so I'm still interested in insights.</p>
","2882778","","2882778","","2019-07-03 11:34:07","2019-07-03 11:34:07","Can't install gensim with python 3.4","<amazon-web-services><pip><amazon-elastic-beanstalk><gensim>","1","0","","","","CC BY-SA 4.0"
"56897173","1","56907369","","2019-07-05 05:48:32","","1","81","<p>I have a list of 1000 documents, where the first 500 belongs to documents in <code>movies</code> (i.e. list index from <code>0</code> to <code>499</code>) and the remaining 500 belings to documents in <code>tv series</code> (i.e. list index from <code>500</code> to <code>999</code>).</p>

<p>For movies the <code>document tag</code> starts with <code>movie_</code> (e.g., <code>movie_fast_and_furious</code>) and for tv series the document tag starts with <code>tv_series_</code> (e.g., <code>tv_series_the_office</code>)</p>

<p>I use these movies and tv series dataset to build a <code>doc2vec</code> model as follows.</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple

dataset = json.load(open(input_file))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')

for description in dataset:
    tags = [description[0]]
    words = description[1]
    docs.append(analyzedDocument(words, tags))

model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4, epochs = 20)
</code></pre>

<p>Now for each <code>movie</code>, I want to get its nearest 5 <code>tv series</code> along with their cosine similarity.</p>

<p>I know, the function gensim provides <code>model.docvecs.most_similar</code>. However, the results of this include movies as well (which is not my intension). Is it possible to do this in gensim (I assume that the document vectors are creating in the order of the <code>documents list</code> that we provide).</p>

<p>I am happy to provide more details if needed.</p>
","10704050","","10704050","","2019-07-05 10:52:25","2019-07-05 17:46:46","How to identify doc2vec instances seperately in gensim in python","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"56910538","1","56961014","","2019-07-06 01:11:46","","0","2611","<p>I'm trying to install gensim in a specific conda env on my Python 3 only, Windows 10 machine. I've tried 3 different ways based on suggestions in SO and elsewhere, summarized below. Each time it shows as successfully installed and present in the env, but when I try to import it in jupyter notebook I get the <code>ModuleNotFoundError: No module named 'gensim'</code> error. </p>

<p>Note: I closed and relaunched anaconda and jupyter after each install.</p>

<p>SUMMARY: 
3 attempts with 3 install commands:</p>

<pre><code>COMMAND                              CONDA LIST                              IMPORT IN JUPYTER NOTEBOOK
conda install -c anaconda gensim     gensim 3.4.0 py36hfa6e2cd_0 anaconda    ModuleNotFoundError: No module named 'gensim'
pip install -U gensim                gensim 3.7.3 pypi_0 pypi                ModuleNotFoundError: No module named 'gensim'
conda install -c conda-forge gensim  gensim 3.7.3 py36h6538335_0 conda-forge ModuleNotFoundError: No module named 'gensim'
</code></pre>

<pre><code>(base) C:\Users\kb&gt;conda activate SARC
(SARC) C:\Users\kb&gt;conda install -c anaconda gensim
(SARC) C:\Users\kb&gt;conda list
. . .
gensim                    3.4.0            py36hfa6e2cd_0    anaconda
. . .

. . .
</code></pre>

<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-1-e92e291fb8cb&gt; in &lt;module&gt;
      1 import loader
      2 import reader
----&gt; 3 import transformers
      4 import vectorization

~\OneDrive\Documents\ds\courses_books\Applied_Text_Analysis_Python_book_code\atap-master\snippets\ch04\transformers.py in &lt;module&gt;
      3 import os
      4 import nltk
----&gt; 5 import gensim
      6 import unicodedata
      7 

ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>Details of the install commands and output can be seen <a href=""https://github.com/nicolas-ivanov/debug_seq2seq/issues/23#issuecomment-508880177"" rel=""nofollow noreferrer"">here</a>.</p>
","9677043","","9677043","","2019-07-06 03:34:32","2019-07-09 21:59:49","gensim installed in anaconda env but won't import in jupyter notebook","<python><jupyter-notebook><anaconda><conda><gensim>","1","9","","","","CC BY-SA 4.0"
"67730468","1","","","2021-05-27 21:57:14","","-1","43","<p>I think there is something I am missing because I am trying to do a Word2Vec, but the results are so bad I think something is wrong.
Here is my code:</p>
<pre><code># 1.65 million documents of 15-20 words each
documents = [['example','of','input'],
             ['second','sentence','of','input'],
             ['this','is','a','list','of','lists']]

model = gensim.models.Word2Vec(documents, size=100, workers=-1, window=3)
model.train(documents, total_examples=model.corpus_count, epochs=30)
</code></pre>
<p>First, the training runs very fast (I expected long time for training with 30 epochs?)
And then when I look for most similar words the results seems completely random, nothing related.</p>
<p>Is there something I am missing?
Thanks</p>
","12448331","","","","","2021-05-28 16:41:23","Very poor results Word2Vec","<python><nlp><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"43505696","1","","","2017-04-19 20:43:08","","1","1194","<p>I'm fitting a Hierarchical Dirichlet Process (HDP) topic model using the python gensim package on the 20newsgroups dataset, and I discover that my topics are not very informative (the top word probability is very small). </p>

<p>I'm using standard text pre-processing that includes tokenization, stop-words removal, and stemming. I was thinking that reducing dictionary size can help generate more meaningful topics. What are some of ways of reducing the dictionary size in gensim?</p>
","6131248","","","","","2017-04-19 21:12:59","How to reduce dictionary size in gensim?","<python><dictionary><gensim>","1","0","","","","CC BY-SA 3.0"
"56961877","1","","","2019-07-10 00:11:50","","0","370","<p>I am running some code on a Google Platform Compute Engine VM and I get an error when I imported <code>Python boto</code> library.</p>

<p>The first time if I run 'import boto', the error message would be: </p>

<blockquote>
  <p>ModuleNotFoundError: No module named 'urllib2'</p>
</blockquote>

<p>Then I ran it again, a different error message came out:</p>

<blockquote>
  <p>AttributeError: module 'boto' has no attribute 'plugin'</p>
</blockquote>

<p>I tried installing <strong>google-compute-engine</strong> but it didn't work. I also tried different versions of <strong>boto</strong> but failed as well.</p>
","9553934","","4037220","","2019-07-10 11:14:47","2019-07-12 21:56:05","Python3: AttributeError: module 'boto' has no attribute 'plugin'","<google-cloud-platform><boto><gensim>","2","0","","","","CC BY-SA 4.0"
"43186733","1","","","2017-04-03 14:03:06","","0","217","<p>I'm using the Phrases class and want to visualize the vectors in a 2D space. In order to do this with Word2Vec I've used T-SNE and it worked perfectly. When I'm trying to do the same with Phrases it doesn't make any sense (words appear next to irrelevant words). </p>

<p>Any suggestions on how to visualize the Phrases output?</p>
","4583366","","","","","2017-04-12 18:20:55","Visualize Gensim's Phrases' vectors in 2D","<data-visualization><gensim><word2vec><phrases>","1","0","","","","CC BY-SA 3.0"
"43396677","1","","","2017-04-13 15:49:36","","0","1088","<p>Executing this function row by row using a loop works. Executing the same function using pandas.DataFrame.apply returns ValueError: operands could not be broadcast together with shapes. Should the pandas.DataFrame.apply work? If it is one of those things that is not easily explainable, any ideas on how to speed up processing (other than multiprocessing)?</p>

<pre><code>#python 3.6
import pandas as pd # version 0.19.2  
import numpy as np  # 
#gensim version 1.0.1
from gensim import models #https://radimrehurek.com/gensim/models/word2vec.html

df=pd.DataFrame({""q1"":[['how', 'I', 'from', 'iPhone', 'keep', 'them', 'my', 'but', 'delete', 'iCloud', 'photos', 'in', 'can'],
                   ['use', 'are', 'radio', 'What', 'commercial', 'cognitive', 'technology', 'in'],
                   ['how', 'I', 'razor', 'prevent', 'burns', 'the', 'stomach', 'on', 'can']],
             ""q2"":[['Can', 'remove', 'from', 'I', 'iPhone', 'removing', 'them', 'my', 'storage', 'photos', 'iCloud', 'without'],
                  ['radio', 'from', 'Where', 'do', 'come', 'cognitive', 'distinction'],
                   ['how', 'I', 'razor', 'prevent', 'can', 'burn']]})

#using pretrained model https://code.google.com/archive/p/word2vec/
w2v = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) 

#This works
df['w2v_sim']=np.nan
for i in range(len(df)):       
df['w2v_sim'].ix[i]=w2v.n_similarity(df['q1'].ix[i],df['q2'].ix[i])
print(str(df['w2v_sim'].ix[i]))

#this doesn't work
df['w2v_sim']=np.nan
df['w2v_sim']=df.apply(w2v.n_similarity(df['q1'],df['q2']),axis=1)
</code></pre>

<p>ValueError: operands could not be broadcast together with shapes (13,300) (8,300) </p>

<p>Thank you</p>
","7842097","","7842097","","2017-04-13 18:20:11","2017-04-13 18:40:18","pandas.DataFrame.apply ValueError: operands could not be broadcast together with shapes","<python-3.x><pandas><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"56965668","1","","","2019-07-10 07:38:49","","1","467","<p>I am working on a NLP project at the moment. One task is to compare how similar two news articles/titles are. </p>

<p>To do that, i have already trained a doc2vec model using English wikipeida articles on gensim library. Now i want to compare similarities of new text by inferring vectors from the wiki doc2vec model. 
One of the method i have tried is the gensim docvecs's 'similarity_unseen_docs' function. However, the result isn't really intuitive.
May i know are there any other way to get a similarity score with better performance? Or maybe some part of code is wrong?  </p>

<p>Training of the doc2vec model:</p>

<pre class=""lang-py prettyprint-override""><code>models = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=5, min_count=20, epochs =10, sample=0.001, negative=5, workers=cores)
</code></pre>

<p>Comparing similarity:</p>

<pre class=""lang-py prettyprint-override""><code>arg1='China is  the leader in manufacturing electric vehicle'
arg2='The biggest electric car producer is China'

model.docvecs.similarity_unseen_docs(model=wikimodel, doc_words1=tokenize_text(arg1), 
                                         doc_words2=tokenize_text(arg2), alpha=0.025, min_alpha=0.0001, steps=50)
</code></pre>

<p>The output similarity score is only ~0.25 which doesn't seem to be good.</p>

<p>Here's the code for the training: </p>

<pre class=""lang-py prettyprint-override""><code>from gensim.corpora.wikicorpus import WikiCorpus
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from gensim.models import KeyedVectors
from pprint import pprint
import multiprocessing

#read the wiki corpus
wiki = WikiCorpus(""trend_analysis/enwiki-latest-pages-articles.xml.bz2"")

#define the class to convert wikicorpus into suitable form to train Doc2vec
class TaggedWikiDocument(object):
    def __init__(self, wiki):
        self.wiki = wiki
        self.wiki.metadata = True
    def __iter__(self):
        for content, (page_id, title) in self.wiki.get_texts():
            yield TaggedDocument([c for c in content], [title])

documents = TaggedWikiDocument(wiki)


cores = multiprocessing.cpu_count()

#initialize a model and choose training method. (we use DM here)
models = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=5, min_count=20, epochs =10, sample=0.001, negative=5, workers=cores)

#building vocab for the model
models.build_vocab(documents)

#train the Doc2vec model
models.train(documents,total_examples=models.corpus_count, epochs=1)
</code></pre>

<p>Please let me know if the training is having any error that caused the inaccuracy for the inferring of unseen docs.</p>
","11763479","","11763479","","2019-07-11 02:35:10","2019-07-11 02:35:10","How to get sentence/text similarity of new corpus from a WIKI-doc2vec model?","<python><gensim><cosine-similarity><doc2vec><sentence-similarity>","0","6","","","","CC BY-SA 4.0"
"43393248","1","","","2017-04-13 13:09:52","","3","1214","<p>I'm trying to import a downloaded model from Google. I'm doing this using the following code:</p>

<pre><code>import gensim

model= gensim.models.KeyedVectors.load_word2vec_format('C://gensim/model/GoogleNews-vectors-negative300.bin.gz', binary=True)  
</code></pre>

<p>However, when run, I get this error:</p>

<pre><code>File ""C:\Users\Acer\AppData\Local\Programs\Python\Python36-32\lib\site packages\smart_open\smart_open_lib.py"", line 309, in __init__
raise NotImplementedError(""unknown URI scheme %r in %r"" % (self.scheme, uri))
NotImplementedError: unknown URI scheme 'c' in 'C://gensim/model/GoogleNews-vectors-negative300.bin.gz'
</code></pre>

<p>The file path is correct, and name for the model, however I cannot get it to import correctly. I have been using <a href=""http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/"" rel=""nofollow noreferrer"">this</a> guide. </p>

<p>Any suggestions?</p>

<p>Thanks</p>
","7862300","","2612002","","2017-07-01 00:09:03","2017-07-01 00:09:03","Gensim error when importing Word2Vec model","<python><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"56796761","1","56819577","","2019-06-27 18:22:26","","0","284","<p>Thanks for stopping by!  I have a question about the dynamic topic model path: </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&gt;&gt;&gt; from gensim.test.utils import common_corpus, common_dictionary
&gt;&gt;&gt; from gensim.models.wrappers import DtmModel
&gt;&gt;&gt;
&gt;&gt;&gt; path_to_dtm_binary = ""/path/to/dtm/binary""
&gt;&gt;&gt; model = DtmModel(
...     path_to_dtm_binary, corpus=common_corpus, id2word=common_dictionary,
...     time_slices=[1] * len(common_corpus)</code></pre>
</div>
</div>
</p>

<p>what is the path the dynamic topic model binary?  Is that something I need to install or download?  Where can I install or download that?</p>

<p>Thanks!</p>
","11192516","","","","","2019-06-29 17:39:10","Dynamic Topic Model Path","<python><nlp><gensim><lda><topic-modeling>","1","0","1","","","CC BY-SA 4.0"
"59293936","1","","","2019-12-11 20:55:57","","0","181","<p>Is there a standard process that <a href=""https://github.com/rare-technologies/gensim"" rel=""nofollow noreferrer"">gensim</a> uses to compile the cython bits?  I've forked the project and edited the cython code, but it doesn't seem to recompile when I run <code>python3 setup.py install --force</code>.</p>

<p>I'm not getting any errors, it just doesn't seem to even attempt to compile the cython code.  I have a <a href=""https://github.com/rare-technologies/gensim"" rel=""nofollow noreferrer"">fresh clone of the <code>develop</code> branch</a></p>

<p>How do I compile gensim ""from source""?</p>

<p>OS: Ubuntu 16.04<br>
gcc: 5.4.0</p>
","2886575","","2886575","","2019-12-12 19:27:24","2019-12-12 19:27:24","How to compile the cython modules for gensim?","<gensim>","0","3","","","","CC BY-SA 4.0"
"56968915","1","56970510","","2019-07-10 10:37:59","","1","1210","<p>In Gensims word2vec api, I trained a model where I initialized the model with max_final_vocab = 100000 and saved the model using model.save()
(This gives me one .model file, one .model.trainables.syn1neg.npy and one .model.wv.vectors.npy file).</p>

<p>I do not need to train model any further, so I'm fine with using just</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.Word2Vec.load(""train.fr.model"")
kv = model.wv
del model


</code></pre>

<p>the kv variable shown here. I now want to use only the <em>top</em> N (N=40000 in my case) vocabulary items instead of the entire vocabulary. The only way to even attempt cutting down the vocabulary I could find was</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
emb_matrix = np.load(""train.fr.model.wv.vectors.npy"")
emb_matrix.shape
# (100000, 300)
new_emb_matrix = emb_matrix[:40000]
np.save(""train.fr.model.wv.vectors.npy"", new_emb_matrix)
</code></pre>

<p>If I load this model again though, the vocabulary still has length 100000.</p>

<p>I want to reduce the vocabulary of the model or model.wv while retaining a working model. Retraining is not an option.</p>
","11599075","","","","","2019-07-10 12:16:06","In Gensim Word2vec, how to reduce the vocab size of an existing model?","<gensim><word2vec><vocabulary>","1","0","","","","CC BY-SA 4.0"
"56908407","1","56908649","","2019-07-05 19:33:03","","1","1283","<p>I have a target NumPy array with shape (300,) and a set of candidate arrays also of shape (300,). These arrays are Word2Vec representations of words; I'm trying to find the candidate word that is most similar to the target word using their vector representations. What's the best way to find the candidate word that is most similar to the target word?</p>

<p>One way to do this is to sum up the absolute values of the element-wise differences between the target word and the candidate words, then select the candidate word with the lowest overall absolute difference. For example:</p>

<pre><code>candidate_1_difference = np.subtract(target_vector, candidate_vector)
candidate_1_abs_difference = np.absolute(candidate_1_difference)
candidate_1_total_difference = np.sum(candidate_1_abs_difference)
</code></pre>

<p>Yet, this seems clunky and potentially wrong. What's a better way to do this?</p>

<p>Edit to include example vectors:</p>

<pre><code>import numpy as np
import gensim

path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'


def func1(path):
    #Limited to 50K words to reduce load time
    model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True, limit=50000)
    context =  ['computer','mouse','keyboard']
    candidates = ['office','house','winter']
    vectors_to_sum = []
    for word in context:
        vectors_to_sum.append(model.wv[word])
    target_vector = np.sum(vectors_to_sum)

    candidate_vector = candidates[0]
    candidate_1_difference = np.subtract(target_vector, candidate_vector)
    candidate_1_abs_difference = np.absolute(candidate_1_difference)
    candidate_1_total_difference = np.sum(candidate_1_abs_difference)
    return candidate_1_total_difference
</code></pre>
","3893580","","3893580","","2019-07-05 19:53:49","2019-07-05 19:57:52","Comparing NumPy Arrays for Similarity","<python><numpy><gensim>","1","2","","","","CC BY-SA 4.0"
"56909294","1","57464302","","2019-07-05 21:14:14","","2","966","<p><strong>Intro</strong></p>

<p>Currently I am using Gensim in combination with pandas and numpy to run document NLP computation.  I'd like to build a LDA seqential model to track how our topics change over time but am running into errors with the corpus format.</p>

<p>I am trying to figure out how to set time slices for dynamic topic models.  I am using <a href=""https://radimrehurek.com/gensim/models/ldaseqmodel.html"" rel=""nofollow noreferrer"">LdaSeqModel</a> which requires an integer time slice. </p>

<p><strong>The Data</strong></p>

<p>It's a csv:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>data = pd.read_csv('CGA Jan17 - Mar19 Time Slice.csv', encoding = ""ISO-8859-1"");
documents = data[['TextForTopics']]
documents['index'] = documents.index</code></pre>
</div>
</div>
</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>	       Month	Year	Begin Date	TextForTopics	                                      time_slice
0	march	2017	3/23/2017	request: the caller is requesting an appointme...	1</code></pre>
</div>
</div>
</p>

<p>This is then converted into an array of tuples called the bow_corpus:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[[(12, 2), (25, 1), (30, 1)], [(33, 1), (136, 1), (159, 1), (161, 1)], [(165, 1), (247, 2)], (326, 1), (354, 1), (755, 1), (821, 1)]]</code></pre>
</div>
</div>
</p>

<p><strong>Desired Output</strong></p>

<p>It should print one topic allocation for each time slice. If I entered 3 topics and two time slices I should get three topics printed twice showing how the topics evolved over time.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[(0,
  '0.165*""enrol"" + 0.108*""medicar"" + 0.051*""form""),
(1,
  '0.303*""caller"" + 0.290*""inform"" + 0.031*""abl""),
(2,
  '0.208*""date"" + 0.140*""effect"" + 0.060*""medicaid""')]
[(0,
  '0.165*""enrol"" + 0.108*""cats"" + 0.051*""form""),
(1,
  '0.303*""caller"" + 0.290*""puppies"" + 0.031*""abl""),
(2,
  '0.208*""date"" + 0.140*""elephants"" + 0.060*""medicaid""')]</code></pre>
</div>
</div>
</p>

<p><strong>What I've tried</strong></p>

<p>This is the function - the bow corpus is an array of tuples</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>ldaseq = LdaSeqModel(corpus=bow_corpus, time_slice=[], num_topics=15, chunksize=1)</code></pre>
</div>
</div>
</p>

<p>I've tried every version of integer inputs for those time_slices and they all produce errors.  The premise was that the time_slice would represent the number of indicies/rows/documents in each time slice.  For example my data has 1.8 million rows if I wanted two time slices I would order my data by time and enter an integer cutoff like time_slice = [489234, 1310766].  All inputs produce this error:</p>

<p><strong>The Error</strong>
<div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-5-e58059a7fb6f&gt; in &lt;module&gt;
----&gt; 1 ldaseq = LdaSeqModel(corpus=bow_corpus, time_slice=[], num_topics=15, chunksize=1)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in __init__(self, corpus, time_slice, id2word, alphas, num_topics, initialize, sstats, lda_model, obs_variance, chain_variance, passes, random_state, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    186 
    187             # fit DTM
--&gt; 188             self.fit_lda_seq(corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    189 
    190     def init_ldaseq_ss(self, topic_chain_variance, topic_obs_variance, alpha, init_suffstats):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in fit_lda_seq(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    275             # seq model and find the evidence lower bound. This is the E - Step
    276             bound, gammas = \
--&gt; 277                 self.lda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)
    278             self.gammas = gammas
    279 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in lda_seq_infer(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)
    351             bound, gammas = self.inferDTMseq(
    352                 corpus, topic_suffstats, gammas, lhoods, lda,
--&gt; 353                 ldapost, iter_, bound, lda_inference_max_iter, chunksize
    354             )
    355         elif model == ""DIM"":

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in inferDTMseq(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)
    401         time = 0  # current time-slice
    402         doc_num = 0  # doc-index in current time-slice
--&gt; 403         lda = self.make_lda_seq_slice(lda, time)  # create lda_seq slice
    404 
    405         time_slice = np.cumsum(np.array(self.time_slice))

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in make_lda_seq_slice(self, lda, time)
    459         """"""
    460         for k in range(self.num_topics):
--&gt; 461             lda.topics[:, k] = self.topic_chains[k].e_log_prob[:, time]
    462 
    463         lda.alpha = np.copy(self.alphas)

IndexError: index 0 is out of bounds for axis 1 with size 0</code></pre>
</div>
</div>
</p>

<p><strong>Solutions</strong></p>

<p>I tried going back to the documentation and looking at the format of the common_corpus used as an example and the format of my bow_corpus is the same.  I also tried running the code in the documentation to see how it worked but it also produced the same error.  I'm not sure if the problem is my code anymore but I hope it is.</p>

<p>I've also tried messing with the file format by manually dividing my csv into 9 csvs containing my time_slices and creating an iterated corpus out of those, but that didn't work.  I've considered converting each row of my csv into txt files and then creating a corpus out of that like David Beil does, but that sounds pointlessly tedious as I already have an iterated corpus.</p>
","11192516","","11192516","","2019-08-01 17:11:48","2021-04-29 19:17:22","How to set time slices - Dynamic Topic Model","<python-3.x><nlp><gensim><lda><topic-modeling>","2","0","","","","CC BY-SA 4.0"
"43580272","1","","","2017-04-24 05:16:36","","1","671","<p>I went through the word2vec tutorial and was able to train with given example data of Text8Corpus. <a href=""https://rare-technologies.com/word2vec-tutorial"" rel=""nofollow noreferrer"">Tutorial link</a> But when I tried to test on custom data, model is not training. The input are in unicode string format in python list. And min_count is also set to 1. Since it was not training in above input format, I tried to check type of input from the given tutorial, but I receive the format as this: class 'gensim.models.word2vec.Text8Corpus'. So not sure how to train my custom data of unicode string sentences in a list. Can anyone please guide me in right direction ?</p>

<blockquote>
  <p>Example of the input : [[u'SENTENCE_START', u'dont', u'let', u'him', u'treat', u'you', u'like', u'garbage', u'SENTENCE_END']] </p>
</blockquote>

<pre><code>sentences_clean = []
data = pandas.read_excel('from my folder/3_captions.xlsx', parse_cols = ""A"")
sentences = data.iloc[:, 0].tolist()

for line in sentences:
  line = re.sub(r""""""[""?,$!]|'(?!(?&lt;! ')[ts])"""""", """", line)
  line = re.sub(r""\."", """", line) 
  line = line.lower().replace(""'"", """")
  sentences_clean.append(line)

tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences_clean]

import gensim, logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model_word2vec = gensim.models.Word2Vec(sentences_clean, min_count=1, size=300, workers=4)
model_word2vec.save('/Users/rajesh/Documents/word2vec')
print (model_word2vec.similarity('freudian','slip'))
</code></pre>

<p>that's all there is to train to word2vec I understand from tutorial and example ? But this below tutorial works.    </p>

<pre><code>sentences = word2vec.Text8Corpus('/Users/rajesh/Downloads/text8')
model_word2vec = gensim.models.Word2Vec(sentences, min_count=1, size=20)
print (model_word2vec.similarity('queen','woman'))
&gt;&gt;&gt; 0.659536897647
</code></pre>
","5505224","","5505224","","2017-04-24 06:05:32","2017-05-16 05:41:59","Word2Vec model is not training, input sentences tried with both sequence of sentence and tokenized words in list","<python><gensim><word2vec>","1","8","","","","CC BY-SA 3.0"
"43546165","1","","","2017-04-21 15:08:58","","2","462","<p>I've just installed Gensim and have run a test. 484 tests run successfully however I hit an error:</p>

<pre><code>======================================================================
ERROR: testAddMorphemesToEmbeddings (gensim.test.test_varembed_wrapper.TestVarembed)
Test add morphemes to Embeddings
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/franciskim/Downloads/gensim-2.0.0/gensim/test/test_varembed_wrapper.py"", line 56, in testAddMorphemesToEmbeddings
    vectors=varembed_model_vector_file, morfessor_model=varembed_model_morfessor_file)
  File ""/Users/franciskim/Downloads/gensim-2.0.0/gensim/models/wrappers/varembed.py"", line 69, in load_varembed_format
    morfessor_model = morfessor.MorfessorIO().read_binary_model_file(morfessor_model)
  File ""/Users/franciskim/anaconda/lib/python3.6/site-packages/morfessor/io.py"", line 179, in read_binary_model_file
    model = pickle.load(fobj)
AttributeError: Can't get attribute 'FixedCorpusWeight' on &lt;module 'morfessor.baseline' from '/Users/franciskim/anaconda/lib/python3.6/site-packages/morfessor/baseline.py'&gt;

----------------------------------------------------------------------
</code></pre>

<p>I can only find 1 other instance of this happening on the internet, but there are no answers to this yet.</p>
","960201","","","","","2017-04-21 15:08:58","Gensim - AttributeError: Can't get attribute 'FixedCorpusWeight'","<python><gensim><word2vec>","0","0","","","","CC BY-SA 3.0"
"59312001","1","59312860","","2019-12-12 20:08:37","","0","565","<p>Getting <code>TypeError: unhashable type: 'list'</code> and <code>AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found</code> errors when I create my model based on <code>Word2Vec</code> implementation of the <code>gensim</code> module.</p>

<p><strong>Each entry has three parts</strong> which are presented within a list. And, <strong>the model contains three entries</strong> for the sake of demonstration.</p>

<p>Here is what I have tried:</p>

<pre><code>model = Word2Vec(sentences=features, size=100, sg=1, window=3, min_count=1, iter=10, workers=Pool()._processes)

model.build_vocab(features)

model.train(features)
</code></pre>

<p>The value of the <code>features</code> is: </p>

<pre><code>  [
    [
      ['permission.ACCESS_WIFI_STATE', 'permission.ACCESS_NETWORK_STATE', 'permission.READ_PHONE_STATE', 'permission.INTERNET', 'permission.CHANGE_WIFI_STATE'],
       ['intent.action.MAIN', 'intent.action.BATTERY_CHANGED_ACTION', 'intent.action.SIG_STR', 'intent.action.BOOT_COMPLETED'],
       []
    ],
    [
      ['permission.WRITE_EXTERNAL_STORAGE', 'permission.ACCESS_NETWORK_STATE', 'permission.READ_PHONE_STATE', 'permission.INTERNET', 'permission.INSTALL_PACKAGES', 'permission.SEND_SMS', 'permission.DELETE_PACKAGES'],
      ['intent.action.BOOT_COMPLETED', 'intent.action.USER_PRESENT', 'intent.action.PHONE_STATE', 'intent.action.MAIN'],
      []
    ], 
    [
      ['permission.WRITE_EXTERNAL_STORAGE', 'permission.ACCESS_FINE_LOCATION', 'permission.INTERNET', 'permission.READ_PHONE_STATE', 'permission.ACCESS_COARSE_LOCATION', 'permission.CALL_PHONE', 'permission.READ_CONTACTS', 'permission.READ_SMS'], 
      ['intent.action.PHONE_STATE', 'intent.action.MAIN'], 
      []
    ]
  ]
</code></pre>

<p><strong>Edit:</strong> The error stack trace after correcting the form of the feature vector according to the comment of @gojomo.</p>

<pre><code>Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 3 more threads
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 2 more threads
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 1 more threads
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 0 more threads
2019-12-13 12:24:34,520:gensim.models.base_any2vec:INFO - EPOCH - 10 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,520:gensim.models.base_any2vec:INFO - training on a 60 raw words (2 effective words) took 0.1s, 21 effective words/s
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,520:gensim.models.base_any2vec:WARNING - under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
2019-12-13 12:24:34,521:gensim.utils:INFO - saving Word2Vec object under model/word2vec_model, separately None
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
    debugger.enable_tracing(apply_to_all_threads=True)  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads

  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
      File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
2019-12-13 12:24:34,521:gensim.utils:INFO - not storing attribute vectors_norm
        func = self.__getitem__(name)func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__

2019-12-13 12:24:34,522:gensim.utils:INFO - not storing attribute cum_table
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
    func = self._FuncPtr((name_or_ordinal, self))AttributeError: dlsym(0x7fed18f247e0, AttachDebuggerTracing): symbol not found

AttributeError: dlsym(0x7fed18f247e0, AttachDebuggerTracing): symbol not found
func = self._FuncPtr((name_or_ordinal, self))
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
AttributeError: dlsym(0x7fed18d2ff70, AttachDebuggerTracing): symbol not found
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,524:gensim.utils:INFO - saved model/word2vec_model
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18a163b0, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
        pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
result = lib.AttachDebuggerTracing(  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads

  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed15fd5850, AttachDebuggerTracing): symbol not found
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18a163b0, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18b09320, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed15fd5850, AttachDebuggerTracing): symbol not found
</code></pre>
","282855","","282855","","2019-12-13 09:29:17","2019-12-13 09:29:17","Word2Vec - How to rid of ""TypeError: unhashable type: 'list'"" and ""AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found""?","<gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"48928739","1","","","2018-02-22 13:33:38","","3","1337","<p>I have a set of documents in a df. I am transforming those documents to vectors with <code>gensim</code> <code>Doc2Vec</code>:</p>

<pre><code>def read_corpus(documents):
    for i, plot in enumerate(documents):
        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(plot, max_len=30), [i])

train_corpus = list(read_corpus(df.note))

model = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2, iter=55)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)
</code></pre>

<p>I then save the model and convert the .w2v file to tsv files. Finally, I overwrite the metadata tsv file so that it is meaningful:</p>

<pre><code>model.save_word2vec_format('doc_tensor.w2v', doctag_vec=True, word_vec=False)

%run word2vec2tensor.py -i doc_tensor.w2v -o notes 

with open('notes_metadata.tsv','w') as w:
    w.write('created_by\tnote_type\n')
    for i,j in zip(df.created_by, df.note_type):
        w.write(""%s\t%s\n"" % (i,j))
</code></pre>

<p>At this point, I have two tsv files: one contains the vectors and the other contains metadata about the vectors. I got to this point by following this tutorial: <a href=""http://nbviewer.jupyter.org/github/RaRe-Technologies/gensim/blob/8f7c9ff4c546f84d42c220dcf28543500747c171/docs/notebooks/Tensorboard_visualizations.ipynb#Training-the-Doc2Vec-Model"" rel=""nofollow noreferrer"">http://nbviewer.jupyter.org/github/RaRe-Technologies/gensim/blob/8f7c9ff4c546f84d42c220dcf28543500747c171/docs/notebooks/Tensorboard_visualizations.ipynb#Training-the-Doc2Vec-Model</a>.</p>

<p>Now I would like to embed this model and the tsv files in a local Tensorboard. I tried this:</p>

<pre><code># load model
embedding = model.docvecs.vectors_docs

# setup a TensorFlow session
tf.reset_default_graph()
sess = tf.InteractiveSession()
X = tf.Variable([0.0], name='embedding')
place = tf.placeholder(tf.float32, shape=embedding.shape)
set_x = tf.assign(X, place, validate_shape=False)
sess.run(tf.global_variables_initializer())
sess.run(set_x, feed_dict={place: embedding})

# create a TensorFlow summary writer
summary_writer = tf.summary.FileWriter('log', sess.graph)
config = projector.ProjectorConfig()
embedding_conf = config.embeddings.add()
embedding_conf.tensor_name = 'embedding:0'
embedding_conf.metadata_path = os.path.join('log', 'metadata.tsv')
projector.visualize_embeddings(summary_writer, config)
</code></pre>

<p>This code ran without error, but when I type <code>tensorboard --logdir=log</code> and go to the localhost location, it looks like this:</p>

<p><a href=""https://i.stack.imgur.com/dQWmi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dQWmi.png"" alt=""enter image description here""></a></p>

<p>My folder structure looks like this:</p>

<pre><code>project
   - jupyter_notebook_from_which_I_run_my_code.ipynb
   - log
        - events.out.tfevents.1519305293.COMPUTERNAME
        - notes_metadata.tsv
        - notes_tensor.tsv
        - projector_config.pbtxt
</code></pre>

<p>If I click ""Choose File"" in the TensorBoard projector and choose my notes_tensor.tsv file, it says ""Graph visualization failed: The graph is empty. Make sure that the graph is passed to the tf.summary.FileWriter after the graph is defined.</p>

<p>How do I get the tsv files to show up in the projector for tSNE and PCA visualizations like in the tutorial I linked to earlier?</p>

<p><strong>Update:</strong> I tried adding these two lines:</p>

<pre><code>saver = tf.train.Saver([X])
saver.save(sess, os.path.join('log', 'model2.ckpt'), 1)
</code></pre>

<p>This added these files to <code>log</code>:</p>

<pre><code>checkpoint
model2.ckpt-1.data-00000-of-00001
model2.ckpt-1.index
model2.ckpt-1.meta
</code></pre>

<p>It also gave TensorBoard the <code>Projector</code> tab!</p>

<p><a href=""https://i.stack.imgur.com/vx1nA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vx1nA.png"" alt=""enter image description here""></a></p>

<p>However, there is an error fetching metadata.tsv. This is because it doesn't exist. It's also looking in /log/log instead of just /log. When I dismiss that error, click ""Load"", and choose notes_metadata.tsv, nothing happens.</p>
","7668467","","7668467","","2018-02-22 13:58:37","2018-02-22 13:58:37","Embedding Gensim Doc2Vec Tensorboard","<python><tensorflow><gensim><tensorboard><doc2vec>","0","2","1","","","CC BY-SA 3.0"
"57283636","1","","","2019-07-31 05:14:40","","1","2695","<p>I have around 20k documents with 60 - 150 words. Out of these 20K documents, there are 400 documents for which the similar document are known. These 400 documents serve as my test data.</p>

<p>At present I am removing those 400 documents and using remaining 19600 documents for training the doc2vec. Then I extract the vectors of train and test data. Now for each test data document, I find it's cosine distance with all the 19600 train documents and select the top 5 with least cosine distance. If the similar document marked is present in these top 5 then take it to be accurate. Accuracy% = No. of Accurate records / Total number of Records.</p>

<p>The other way I find similar documents is by using the doc2Vec most similiar method. Then calculate accuracy using the above formula.</p>

<p>The above two accuracy doesn't match. With each epoch one increases other decreases.</p>

<p>I am using the code given here: <a href=""https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e"" rel=""nofollow noreferrer"">https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e</a>. For training the Doc2Vec.</p>

<p>I would like to know how to tune the hyperparameters so that I can get making accuracy by using above-mentioned formula. Should I use cosine distance to find the most similar documents or shall I use the gensim's most similar function?</p>
","5679933","","","","","2019-07-31 21:58:09","How to effectively tune the hyper-parameters of Gensim Doc2Vec to achieve maximum accuracy in Document Similarity problem?","<python><nlp><gensim><doc2vec><sentence-similarity>","1","0","","","","CC BY-SA 4.0"
"48934154","1","48934419","","2018-02-22 17:53:38","","0","88","<p>I have two different corpus and what i want is to train the model with both and to do it it I thought that it could be something like this:</p>

<pre><code>model.build_vocab(sentencesCorpus1)
model.build_vocab(sentencesCorpus2)
</code></pre>

<p>Would it be right?</p>
","9025222","","","","","2018-02-22 18:17:06","Can i build vocaburay in twice with gensim word2vec or doc2vec?","<python><word2vec><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"59281409","1","59281516","","2019-12-11 08:17:32","","0","508","<p>I'm trying to execute simple code to lemmatize string, but there's an error about iteration.
I have found some solutions which are about reinstalling web.py, but this not worked for me.</p>

<p>python code</p>

<pre><code>from gensim.utils import lemmatize
lemmatize(""gone"")
</code></pre>

<p>error is</p>

<pre><code>---------------------------------------------------------------------------
StopIteration                             Traceback (most recent call last)
I:\Anaconda\lib\site-packages\pattern\text\__init__.py in _read(path, encoding, comment)
    608             yield line
--&gt; 609     raise StopIteration
    610 

StopIteration: 

The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-4-9daceee1900f&gt; in &lt;module&gt;
      1 from gensim.utils import lemmatize
----&gt; 2 lemmatize(""gone"")

-------------------------------------------------------------------------------------

I:\Anaconda\lib\site-packages\pattern\text\__init__.py in &lt;genexpr&gt;(.0)
    623     def load(self):
    624         # Arnold NNP x
--&gt; 625         dict.update(self, (x.split("" "")[:2] for x in _read(self._path) if len(x.split("" "")) &gt; 1))
    626 
    627 #--- FREQUENCY -------------------------------------------------------------------------------------

RuntimeError: generator raised StopIteration
</code></pre>
","","user11466918","","","","2019-12-11 08:25:50","gensim lemmatize error generator raised StopIteration","<python><nlp><gensim><lemmatization>","1","0","","","","CC BY-SA 4.0"
"67738987","1","","","2021-05-28 12:37:33","","0","637","<p>I trying to import gensim.</p>
<p>But I got this error.</p>
<pre><code>ImportError: cannot import name 'has_pattern' from 'gensim.utils' (C:\Users\admin\anaconda3\envs\tf-gpu\lib\site-packages\gensim\utils.py)
</code></pre>
<p>I will be grateful for any help.</p>
","11589824","","","","","2021-07-14 05:02:21","ImportError: cannot import name 'has_pattern' from 'gensim.utils'","<python><gensim>","2","1","","","","CC BY-SA 4.0"
"43543762","1","43993195","","2017-04-21 13:16:57","","6","4642","<p>I'm using the Doc2Vec tags as an unique identifier for my documents, each document has a different tag and no semantic meaning. I'm using the tags to find specific documents so I can calculate the similarity between them. </p>

<p>Do the tags influence the results of my model? </p>

<p>In this <a href=""https://rare-technologies.com/doc2vec-tutorial/"" rel=""noreferrer"">tutorial</a> they talk about a parameter <code>train_lbls=false</code>, with this set to false there are no representations learned for the labels (tags). </p>

<p>That tutorial is somewhat dated and I guess the parameter does no longer exist, how does Doc2Vec handle tags? </p>
","3296266","","3296266","","2017-04-24 07:32:21","2017-05-16 05:30:47","Does Doc2Vec learn representations for the tags?","<gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"59348206","1","59349662","","2019-12-15 21:27:14","","0","406","<p>I'm a complete novice to NLP and would like to load a zipped XLM file of the Hungarian Wikipedia corpus (807 MB). I downloaded the dumpfile and started parsing it in Python with Gensim, but after 4 hours my laptop crashed, complaining that I had run out of RAM. I have a fairly old laptop (4GB RAM) and was wondering whether there is any way I could solve this problem by </p>

<ul>
<li>(1) either  tinkering with my code, e.g, by reducing the corpus by taking, say, a 1/10th random sample of it; </li>
<li>(2) or using some cloud platform to enhance my CPU power. I read in <a href=""https://stackoverflow.com/questions/32543235/python-gensim-memory-error"">this SO post</a> that AWS can be used for such puposes, but I am unsure which service I should select (Amazon EC2?). I also checked Google Colab, but got confused that it lists hardware acceleration options (GPU and CPU) in the context of Tensorflow, and I am not sure if that is suitable for NLP. I didn't find any posts about that. </li>
</ul>

<p>Here's my Jupyter Notebook code that I've tried after downloading the wikipedia dumps from <a href=""https://dumps.wikimedia.org/huwiki/latest/huwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">here</a>: </p>

<pre><code>! pip install gensim 
from nltk.stem import SnowballStemmer
from gensim.corpora import WikiCorpus
from gensim.models.word2vec import Word2Vec

hun_stem = SnowballStemmer(language='hungarian')

%%time
hun_wiki = WikiCorpus(r'huwiki-latest-pages-articles.xml.bz2')
hun_articles = list(hun_wiki.get_texts())
len(hun_articles)
</code></pre>

<p>Any guidance would be much appreciated. </p>
","6534207","","6534207","","2019-12-15 22:57:23","2019-12-16 01:47:50","Loading Wikipedia XML files into Gensim","<python><nlp><gensim>","1","3","1","","","CC BY-SA 4.0"
"56874230","1","","","2019-07-03 16:09:41","","0","896","<p>I have a binary word2vec file and I am using <code>gensim</code> to load it.</p>

<p>While there is function to get <code>similarity</code> between 2 words in gensim but no function to calculate and return difference vector.</p>

<p>How can I use two vectors and get there difference vector?</p>

<p>And also I am trying to use these difference vectors as feature in document classification.Calculating diff vector between each word and each class.Is this right approach?</p>

<p>For example if classes are <code>sport</code> and <code>politics</code></p>

<pre><code>sport = [0.4,0.456,45,...] #wordvector of class
politics = [0.23,0.56...] #wordvector of class
</code></pre>

<p>And my word is <code>football</code></p>

<pre><code>football = [0.2,0.6,0.45,...] #wordvector of football
</code></pre>

<p>I want to calculate diff vector</p>

<pre><code>(sport - football) = [some vector] # this as a feature for classification
</code></pre>
","7867539","","7867539","","2019-07-03 16:23:51","2019-07-05 18:01:15","How to calculate difference vector in word2vec","<python><gensim><word2vec><calculation><document-classification>","2","0","","","","CC BY-SA 4.0"
"56883959","1","56888761","","2019-07-04 08:45:50","","0","628","<p>I am having a data-set consisting of faculty id and the feedback of students regarding the respective faculty. There are multiple comments for each faculty and therefore the comments regarding each faculty are present in the form of a list. I want to apply gensim summarization on the ""comments"" column of the data-set to generate the summary of faculty performance according to the student feedback.</p>

<p>Just for a trial I tried to summarize the feedbacks corresponding to the first faculty id. There are 8 distinct comments (sentences) in that particular feedback, still gensim throws an error ValueError: input must have more than one sentence. </p>

<pre><code>df_test.head()
    csf_id  comments
0   9   [' good subject knowledge.', ' he has good kn...
1   10  [' good knowledge of subject. ', ' good subjec...
2   11  [' good at clearing the concepts interactive w...
3   12  [' clears concepts very nicely interactive wit...
4   13  [' good teaching ability.', ' subject knowledg...
from gensim.summarization import summarize
text = df_test[""comments""][0]
print(""Text"")
print(text)
print(""Summary"")
print(summarize(text))
</code></pre>

<blockquote>
  <p>ValueError: input must have more than one sentence  </p>
</blockquote>

<p>what changes shold i make so that the summarizer reads all the sentenses and summarizes them.</p>
","11112726","","9698684","","2019-07-04 08:46:32","2019-07-05 08:58:07","Columnwise Summarize multiple sentences present in a list using the gensim summarizer","<python><nlp><gensim>","2","0","","","","CC BY-SA 4.0"
"43540857","1","","","2017-04-21 10:53:43","","4","1562","<p>Trying to load up a file in gensim with this line of code :    </p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format(r""C:/Users/dan/txt_sentoken/pos/cv000_29590.tx"", binary=False)
</code></pre>

<p>However, I am getting this error: </p>

<pre><code>ValueError: invalid literal for int() with base 10:'films'
</code></pre>

<p>Help how do I solve this error ?</p>
","5511290","","","","","2017-05-08 04:23:54","Gensim - Trying to load a text file in gensim","<python-3.x><jupyter-notebook><gensim>","1","0","","","","CC BY-SA 3.0"
"59358828","1","","","2019-12-16 14:39:13","","2","956","<p>I am getting this error in the following code</p>

<pre><code>from sklearn.externals import joblib
from gensim.models import Word2Vec 


filename2 = 'Models/mpl_model.sav'
loaded_model2 = joblib.load(filename2)
modell= Word2Vec.load(""Models/cbow.model"")
</code></pre>

<p>I am trying to load model from Models folder. I keep getting the error</p>

<pre><code>TypeError: 'errstate' object is not callable
</code></pre>

<p>Traceback</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/mr_toot/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py"", line 605, in load
    obj = _unpickle(fobj, filename, mmap_mode)
  File ""/home/mr_toot/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py"", line 529, in _unpickle
    obj = unpickler.load()
  File ""/home/mr_toot/anaconda3/lib/python3.7/pickle.py"", line 1088, in load
    dispatch[key[0]](self)
  File ""/home/mr_toot/anaconda3/lib/python3.7/pickle.py"", line 1376, in load_global
    klass = self.find_class(module, name)
  File ""/home/mr_toot/anaconda3/lib/python3.7/pickle.py"", line 1426, in find_class
    __import__(module, level=0)
  File ""/home/mr_toot/anaconda3/lib/python3.7/site-packages/numpy/random/_pickle.py"", line 2, in &lt;module&gt;
    from .philox import Philox
  File ""philox.pyx"", line 1, in init numpy.random.philox
  File ""bit_generator.pyx"", line 397, in init numpy.random.bit_generator
TypeError: 'errstate' object is not callable
</code></pre>

<p>Update :</p>

<p>The error was solved by upgrading numpy. Model was dumped using a new version and 
I was loading it using an older version.</p>
","8388143","","8388143","","2019-12-16 15:57:31","2019-12-16 15:57:31","Python Error : TypeError: 'errstate' object is not callable","<python><scikit-learn><gensim>","0","2","","","","CC BY-SA 4.0"
"43476869","1","","","2017-04-18 15:53:34","","5","5504","<p>I have multiple documents that contain multiple sentences. I want to use <strong>doc2vec</strong> to cluster (e.g. k-means) the sentence vectors by using <strong>sklearn</strong>. </p>

<p>As such, the idea is that similar sentences are grouped together in several clusters. However, it is not clear to me if I have to train every single document separately and then use a clustering algorithm on the sentence vectors. Or, if I could infer a sentence vector from doc2vec without training every new sentence.</p>

<p>Right now this is a snippet of my code:</p>

<pre><code>sentenceLabeled = []
for sentenceID, sentence in enumerate(example_sentences):
    sentenceL = TaggedDocument(words=sentence.split(), tags = ['SENT_%s' %sentenceID])
    sentenceLabeled.append(sentenceL)

model = Doc2Vec(size=300, window=10, min_count=0, workers=11, alpha=0.025, 
min_alpha=0.025)
model.build_vocab(sentenceLabeled)
for epoch in range(20):
    model.train(sentenceLabeled)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model.alpha  # fix the learning rate, no decay
textVect = model.docvecs.doctag_syn0

## K-means ##
num_clusters = 3
km = KMeans(n_clusters=num_clusters)
km.fit(textVect)
clusters = km.labels_.tolist()

## Print Sentence Clusters ##
cluster_info = {'sentence': example_sentences, 'cluster' : clusters}
sentenceDF = pd.DataFrame(cluster_info, index=[clusters], columns = ['sentence','cluster'])

for num in range(num_clusters):
     print()
     print(""Sentence cluster %d: "" %int(num+1), end='')
     print()
     for sentence in sentenceDF.ix[num]['sentence'].values.tolist():
        print(' %s ' %sentence, end='')
        print()
    print()
</code></pre>

<p>Basically, what I am doing right now is training on every labeled sentence in the document. However, if have the idea that this could be done in a simpler way.</p>

<p>Eventually, the sentences that contain similar words should be clustered together and be printed. At this point training every document separately, does not clearly reveal any logic within the clusters.</p>

<p>Hopefully someone can steer me in the right direction.
Thanks.</p>
","7097040","","","","","2017-04-19 17:09:51","Doc2Vec Sentence Clustering","<python><scikit-learn><text-mining><gensim><doc2vec>","2","1","2","","","CC BY-SA 3.0"
"59322409","1","","","2019-12-13 12:14:22","","2","520","<p>I am using PyLDAvis to visualise the results of the LDA from Mallet. </p>

<p>Before I can do that, I need the wrapper of the gensim library:</p>

<pre><code>model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(model_list[8])
</code></pre>

<p>When I print the found topics, they are ordered from 0-10.</p>

<p>However when I am using the pyLDAvis to visualise the Topics, the Topic order (0-10), does not align with printed topics.</p>

<p>Example:  </p>

<pre><code>(5,
  '0.042*""euro"" + 0.030*""smartpho"" + 0.022*""camera"" + 0.020*""display"" + '
  '0.018*""model"" + 0.016*""picture"" + 0.012*""price"" + 0.010*""android""')
</code></pre>

<p>As you can see this topic is about smartphones.</p>

<p>However when I visualise the model with pyLDAvis, Topic 5 is not about smartphones, but about another Topic (cars for example). The smartphone topic is not 5 anymore but topic 1.</p>

<p>Example1: </p>

<p><a href=""https://i.stack.imgur.com/XeS6s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XeS6s.png"" alt=""enter image description here""></a></p>

<p>Example2:
<a href=""https://i.stack.imgur.com/p1zUU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p1zUU.png"" alt=""enter image description here""></a></p>

<p>Is this a known error or is this the normal? 
Somebody can help?</p>
","12195215","","","","","2020-06-06 06:53:51","PyLDAvis visualisation does not align with generated topics","<python><gensim><lda><topic-modeling><mallet>","1","0","","","","CC BY-SA 4.0"
"23176061","1","23379750","","2014-04-19 21:58:42","","1","483","<p>I have a python code that uses sklearn and gensim libraries for tf-idf and LDA(Latent Dirichlet Allocation). Now that I want to migrate to Google app engine I can't use any of these two libraries because they are not supported yet. Is there any service already included in Google app engine that I can use instead of these two libraries to do tf-idf and LDA?</p>
","2118596","","","","","2014-04-30 04:33:58","tf-idf and LDA on Google App Engine","<google-app-engine><scikit-learn><tf-idf><lda><gensim>","1","1","1","","","CC BY-SA 3.0"
"48941648","1","","","2018-02-23 05:26:07","","18","5332","<p>Given a model, e.g.</p>

<pre><code>from gensim.models.word2vec import Word2Vec


documents = [""Human machine interface for lab abc computer applications"",
""A survey of user opinion of computer system response time"",
""The EPS user interface management system"",
""System and human system engineering testing of EPS"",
""Relation of user perceived response time to error measurement"",
""The generation of random binary unordered trees"",
""The intersection graph of paths in trees"",
""Graph minors IV Widths of trees and well quasi ordering"",
""Graph minors A survey""]

texts = [d.lower().split() for d in documents]

w2v_model = Word2Vec(texts, size=5, window=5, min_count=1, workers=10)
</code></pre>

<p>It's possible to remove the word from the w2v vocabulary, e.g.</p>

<pre><code># Originally, it's there.
&gt;&gt;&gt; print(w2v_model['graph'])
[-0.00401433  0.08862179  0.08601206  0.05281207 -0.00673626]

&gt;&gt;&gt; print(w2v_model.wv.vocab['graph'])
Vocab(count:3, index:5, sample_int:750148289)

# Find most similar words.
&gt;&gt;&gt; print(w2v_model.most_similar('graph'))
[('binary', 0.6781558990478516), ('a', 0.6284914612770081), ('unordered', 0.5971308350563049), ('perceived', 0.5612867474555969), ('iv', 0.5470727682113647), ('error', 0.5346164703369141), ('machine', 0.480206698179245), ('quasi', 0.256790429353714), ('relation', 0.2496253103017807), ('trees', 0.2276223599910736)]

# We can delete it from the dictionary
&gt;&gt;&gt; del w2v_model.wv.vocab['graph']
&gt;&gt;&gt; print(w2v_model['graph'])
KeyError: ""word 'graph' not in vocabulary""
</code></pre>

<p>But when we do a similarity on other words after deleting <code>graph</code>, we see the word <code>graph</code> popping up, e.g.</p>

<pre><code>&gt;&gt;&gt; w2v_model.most_similar('binary')
[('unordered', 0.8710334300994873), ('ordering', 0.8463168144226074), ('perceived', 0.7764195203781128), ('error', 0.7316686511039734), ('graph', 0.6781558990478516), ('generation', 0.5770125389099121), ('computer', 0.40017056465148926), ('a', 0.2762695848941803), ('testing', 0.26335978507995605), ('trees', 0.1948457509279251)]
</code></pre>

<p><strong>How to remove a word completely from a Word2Vec model in gensim?</strong> </p>

<hr>

<h1>Updated</h1>

<p>To answer @vumaasha's comment:</p>

<blockquote>
  <p>could you give some details as to why you want to delete a word</p>
</blockquote>

<ul>
<li><p>Lets say my universe of words in all words in the corpus to learn the dense relations between all words. </p></li>
<li><p>But when I want to generate the similar words, it should only come from a subset of domain specific word.</p></li>
<li><p>It's possible to generate more than enough from <code>.most_similar()</code> then filter the words but lets say the space of the specific domain is small, I might be looking for a word that's ranked 1000th most similar which is inefficient. </p></li>
<li><p>It would be better if the word is totally removed from the word vectors then the <code>.most_similar()</code> words won't return words outside of the specific domain.</p></li>
</ul>
","610569","","610569","","2018-02-23 06:17:40","2019-08-14 03:55:22","How to remove a word completely from a Word2Vec model in gensim?","<python><dictionary><word2vec><gensim><del>","4","4","3","","","CC BY-SA 3.0"
"48949166","1","48949430","","2018-02-23 13:40:21","","1","1088","<p>I am using <code>gensim</code> to train a <code>Doc2Vec</code> model on documents assigned to particular people. There are 10 million documents and 8,000 people. I don't care about all 8,000 people. I care about a specific group of people (say anywhere from 1 to 500). </p>

<p>The people I'm interested in could change day-to-day, but I will never need to look at the full population. The end goal is to have the resulting vectors of the people I am interested in. I am currently training the model each time on the documents assigned to the specific people.</p>

<p>Should I train the model on all 10 million documents? Or should I train the model on only the documents assigned to the people I'm interested in? If it's important to train it on all 10 million documents, how would I then get the vectors only for the people I'm interested in?</p>
","7668467","","","","","2018-02-23 13:54:25","Gensim Doc2Vec Training","<python><gensim><doc2vec>","1","4","","","","CC BY-SA 3.0"
"57298877","1","","","2019-07-31 21:10:19","","1","750","<p>i want to do in python 3.7.4:</p>

<p>and getting this Error:</p>

<p>i already tried: </p>

<p>using <code>conda</code> and <code>pip</code></p>

<p>using local windows and windows server </p>

<p>multiple reinstallments of diffenent versions of packages (e.g. <code>numpy</code> and <code>scipy</code>)</p>

<pre><code>from gensim.models import Word2Vec 
</code></pre>

<blockquote>
  <p>Traceback (most recent call last):
    File ""c:/Users/Administrator/Documents/GitHub/contract-criteria-identifier-on-aws/schnelltest.py"", line 1, in 
      import gensim
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim__init__.py"", line 5, in 
      from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\parsing__init__.py"", line 4, in 
      from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\parsing\preprocessing.py"", line 42, in 
      from gensim import utils
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\utils.py"", line 40, in 
      import scipy.sparse
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse__init__.py"", line 230, in 
      from .csr import *
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse\csr.py"", line 13, in 
      from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,
  ImportError: DLL load failed: The specified module could not be found.
  PS C:\Users\Administrator\Documents\GitHub\contract-criteria-identifier-on-aws> &amp; C:/Users/Administrator/AppData/Local/Programs/Python/Python37/python.exe c:/Users/Administrator/Documents/GitHub/contract-criteria-identifier-on-aws/schnelltest.py
  Traceback (most recent call last):
    File ""c:/Users/Administrator/Documents/GitHub/contract-criteria-identifier-on-aws/schnelltest.py"", line 1, in 
      import gensim
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim__init__.py"", line 5, in 
      from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\parsing__init__.py"", line 4, in 
      from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\parsing\preprocessing.py"", line 42, in 
      from gensim import utils
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\gensim\utils.py"", line 40, in 
      import scipy.sparse
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse__init__.py"", line 230, in 
      from .csr import *
    File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse\csr.py"", line 13, in 
      from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,
  ImportError: DLL load failed: The specified module could not be found.</p>
</blockquote>
","11866120","","7758804","","2019-07-31 21:22:23","2019-07-31 22:18:52","How to install gensim and run package in python?","<python><machine-learning><pip><gensim><word2vec>","1","4","","","","CC BY-SA 4.0"
"48968688","1","","","2018-02-24 23:25:23","","0","587","<p>I have trained a <code>gensim</code> Doc2Vec model on five million documents, and those documents are tagged with a unique ID (IDNO). Now I am loading the model, and accessing a specific subset of the vectors based on IDNO. First, I load a <code>pandas</code> df from a database and it looks like this:</p>

<pre><code>IDNO    author   document
123XYZ  john     the cat sat
234FGH  jane     the dog ran
345RTY  jane     the hippo ate
</code></pre>

<p>Then I load the model:</p>

<pre><code>model = Doc2Vec.load('documents_doc2vec_vectorsize100_mincount2_epochs50.model')
</code></pre>

<p>Then I access the three vectors that are in my df:</p>

<pre><code>row_id_list = list(df.row_id)
vectors_tuple = itemgetter(*row_id_list)(model.docvecs)
embedding = np.asarray(vectors_tuple)
</code></pre>

<p>Then I create all the necessary <code>TensorBoard</code> files:</p>

<pre><code>tf.reset_default_graph()
sess = tf.InteractiveSession()

X = tf.Variable([0.0], name='embedding')
place = tf.placeholder(tf.float32, shape=embedding.shape)
set_x = tf.assign(X, place, validate_shape=False)

sess.run(tf.global_variables_initializer())
sess.run(set_x, feed_dict={place: embedding})

summary_writer = tf.summary.FileWriter('log', sess.graph)

config = projector.ProjectorConfig()
embedding_conf = config.embeddings.add()
embedding_conf.tensor_name = 'embedding:0'
embedding_conf.metadata_path = os.path.join('metadata','metadata.tsv')
projector.visualize_embeddings(summary_writer, config)

saver = tf.train.Saver([X])
saver.save(sess, os.path.join('log', 'model1.ckpt'), 1)
</code></pre>

<p>When I run <code>tensorboard --logdir=log</code>, <code>TensorBoard</code> loads, but it says points and dimensions are loading. When I enter Chrome Developer Tools, I get this error:</p>

<pre><code>Uncaught TypeError: Cannot read property 'length' of undefined
    at (index):147401
    at Array.filter (&lt;anonymous&gt;)
    at (index):147399
    at XMLHttpRequest.xhr.onload ((index):143698)
</code></pre>

<p>I had <code>TensorBoard</code> working before, but that was when I set <code>embedding</code> to <code>model.docvecs.vectors_docs</code> rather than accessing specific vectors and pushing them into a <code>numpy</code> array.</p>

<p>Why is this happening? </p>
","7668467","","","","","2018-03-16 20:23:05","TensorBoard UncaughtTypeError: Cannot read property 'length' of undefined","<python><tensorflow><gensim><tensorboard>","1","0","","","","CC BY-SA 3.0"
"56976941","1","","","2019-07-10 18:50:07","","0","1443","<p>I am working with text data and at the moment I have put my data into a term document matrix and calculated the TF, term frequency and TF-IDF, term frequency inverse document frequency. From here my matrix looks like:</p>

<p>columns = document names</p>

<p>rownames = words</p>

<p>filled with their TF and TF-IDF scores. </p>

<p>I have been using the <code>tm</code> package in <code>R</code> for much of my current analysis but to take it further I have started playing around with the <code>gensim</code> library in Python.</p>

<p>Its not clear to me if I have the word embeddings as in the TF and TF-IDF. I am hopeing to use Word2Vec/Doc2Vec and obtain a matrix similar to what I currently have and then calculate the cosine similarity between document. Is this one of the outputs of the models?</p>

<p>I basically have about 6000 documents I want to calculate the cosine similarity between them and then rank these cosine similarity scores.</p>
","8959427","","","","","2019-07-10 20:33:51","Can I obtain Word2Vec and Doc2Vec matrices to calculate a cosine similarity?","<python><gensim><word2vec><doc2vec>","2","0","2","","","CC BY-SA 4.0"
"57297194","1","57299472","","2019-07-31 18:49:40","","0","363","<p>I'm new to NLP and gensim, currently trying to solve some NLP problems with gensim word2vec module. I my current understanding of word2vec, the result vectors/matrix should have all entries between -1 and 1. However, trying a simple one results into a vector which has entries greater than 1. I'm not sure which part is wrong, could anyone give some suggestions, please?</p>

<p>I've used gensim utils.simple_preprocess to generate a list of list of token. The list looks like: </p>

<pre><code>[['buffer', 'overflow', 'in', 'client', 'mysql', 'cc', 'in', 'oracle', 'mysql', 'and', 'mariadb', 'before', 'allows', 'remote', 'database', 'servers', 'to', 'cause', 'denial', 'of', 'service', 'crash', 'and', 'possibly', 'execute', 'arbitrary', 'code', 'via', 'long', 'server', 'version', 'string'], ['the', 'xslt', 'component', 'in', 'apache', 'camel', 'before', 'and', 'before', 'allows', 'remote', 'attackers', 'to', 'read', 'arbitrary', 'files', 'and', 'possibly', 'have', 'other', 'unspecified', 'impact', 'via', 'an', 'xml', 'document', 'containing', 'an', 'external', 'entity', 'declaration', 'in', 'conjunction', 'with', 'an', 'entity', 'reference', 'related', 'to', 'an', 'xml', 'external', 'entity', 'xxe', 'issue']]
</code></pre>

<p>I believe this is the correct input format for gensim word2vec.</p>

<pre><code>word2vec = models.word2vec.Word2Vec(sentences, size=50, window=5, min_count=1, workers=3, sg=1)
vector = word2vec['overflow']
print(vector)
</code></pre>

<p>I expect the output to be a vector containing probabilities (i.e., all between -1 and 1), but it actually turned out to be the following:</p>

<pre><code>[ 0.12800379 -0.7405527  -0.85575     0.25480416 -0.2535793   0.142656
 -0.6361196  -0.13117172  1.1251501   0.5350017   0.05962601 -0.58876884
  0.02858278  0.46106443 -0.22623934  1.6473309   0.5096218  -0.06609935
 -0.70007527  1.0663376  -0.5668168   0.96070313 -1.180383   -0.58649933
 -0.09380565 -0.22683378  0.71361005  0.01779896  0.19778453  0.74370056
 -0.62354785  0.11807996 -0.54997736  0.10106519  0.23364201 -0.11299669
 -0.28960565 -0.54400533  0.10737313  0.3354464  -0.5992898   0.57183135
 -0.67273194  0.6867607   0.2173506   0.15364875  0.7696457  -0.24330224
  0.46414775  0.98163396]
</code></pre>

<p>You can see there are <code>1.6473309</code> and <code>-1.180383</code> in the above vector.</p>
","11865626","","11086680","","2019-07-31 18:55:19","2019-07-31 22:12:07","gensim word2vec entry greater than 1","<python><nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"66021131","1","66022276","","2021-02-03 04:08:58","","0","1284","<p>I am trying to load a pre-trained glove as a word2vec model in gensim. I have downloaded the glove file from <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">here</a>. I am using the following script:</p>
<pre><code>from gensim import models
model = models.KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=True)
</code></pre>
<p>but get the following error</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-38-e0b48b51f433&gt; in &lt;module&gt;()
      1 from gensim import models
----&gt; 2 model = models.KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=True)

2 frames
/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py in &lt;genexpr&gt;(.0)
    171     with utils.smart_open(fname) as fin:
    172         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 173         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    174         if limit:
    175             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: 'the'
</code></pre>
<p>What is the underlying problem? Does gensim need a specific format to be able to load it?</p>
","9916514","","","","","2021-02-03 06:23:33","How to load pre-trained glove model with gensim load_word2vec_format?","<stanford-nlp><gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"43554048","1","","","2017-04-22 00:55:21","","0","1527","<p>When I load doc2vec model from pkl file, I get this error.</p>

<pre><code>    ---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-45-5ed9dc71f3a6&gt; in &lt;module&gt;()
----&gt; 1 temp_docky.infer_vector(['paypal'])

C:\Users\Laxmikant\Anaconda\lib\site-packages\gensim\models\doc2vec.pyc in infer_vector(self, doc_words, alpha, min_alpha, steps)
    750                 train_document_dm(self, doc_words, doctag_indexes, alpha, work, neu1,
    751                                   learn_words=False, learn_hidden=False,
--&gt; 752                                   doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
    753             alpha = ((alpha - min_alpha) / (steps - i)) + min_alpha
    754 

C:\Users\Laxmikant\Anaconda\lib\site-packages\gensim\models\doc2vec_inner.pyx in gensim.models.doc2vec_inner.train_document_dm (./gensim/models/doc2vec_inner.c:5165)()
    406     # default vectors, locks from syn0/doctag_syn0
    407     if word_vectors is None:
--&gt; 408        word_vectors = model.wv.syn0
    409     _word_vectors = &lt;REAL_t *&gt;(np.PyArray_DATA(word_vectors))
    410     if doctag_vectors is None:

AttributeError: 'Doc2Vec' object has no attribute 'wv'
</code></pre>

<p>Can you please help with the error?</p>
","1275877","","","","","2017-05-15 22:54:31","'Doc2Vec' object has no attribute 'wv'","<python><nlp><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 3.0"
"59318935","1","59339940","","2019-12-13 08:45:43","","1","1315","<p>I already have a training model for fastText with gensim, and<br>
I can get the distance between two sentence as described below,  </p>

<pre><code>sentence_1 = ""Today is very cold.""  
sentence_2 = ""I'd like something to drink.""    

print(model.wv.wmdistance(sentence_1.split("" ""), sentence_2.split("" "")))
# 0.8446287678977793  # for example
</code></pre>

<p>but how does <code>vmdistance</code> calculate this value?<br>
I'd like to know the formula.  </p>

<p>API documents: <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Doc2VecKeyedVectors.distance"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Doc2VecKeyedVectors.distance</a></p>
","9432064","","","","","2019-12-14 23:08:40","How does gensim.models.FatText.wv.wmdistance calculate between two documents?","<python-3.x><gensim><fasttext>","1","0","1","","","CC BY-SA 4.0"
"57528271","1","","","2019-08-16 16:41:01","","2","247","<p>I would like to import a pre-trained <code>word2vec</code> dictionary (in binary format) into <code>spacy</code> for vectorizing some text</p>

<p>I am able to import the vectors with <code>gensim</code> through:</p>

<pre class=""lang-py prettyprint-override""><code>import gensim 
model = gensim.models.KeyedVectors.load_word2vec_format('PubMed- 
shuffle-win-2.bin', binary=True)
</code></pre>

<p>Then I initialize a blank spacy nlp object and get the words associated with each index:</p>

<pre class=""lang-py prettyprint-override""><code>nlp = spacy.blank('en')
keys = []
for idx in range(len(model.index2word)):
keys.append(model.index2word[idx])`
</code></pre>

<p>Then set the vectors for the nlp object:</p>

<pre class=""lang-py prettyprint-override""><code>nlp.vocab.vectors = spacy.vocab.Vectors(data=model.syn0, keys=keys)
</code></pre>

<p>I am able to get to this stage without any problems. However, I was wondering <strong><em>how to save this nlp object and load it again into spacy</em></strong> to vectorize new text as efficiently as possible</p>
","10462788","","","","","2019-09-08 06:17:21","Import word2vec vectors in binary format into spacy","<python><nlp><gensim><spacy>","0","1","","","","CC BY-SA 4.0"
"57532018","1","57532194","","2019-08-16 23:07:02","","8","1198","<p>I am using 24 cores virtual CPU and 100G memory to training Doc2Vec with Gensim, but the usage of CPU always is around 200% whatever to modify the number of cores.</p>
<pre><code>top
</code></pre>
<p><a href=""https://i.stack.imgur.com/1FgE9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1FgE9.png"" alt=""enter image description here"" /></a></p>
<pre><code>htop
</code></pre>
<p><a href=""https://i.stack.imgur.com/6jgCK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6jgCK.png"" alt=""enter image description here"" /></a></p>
<p>The above two pictures showed the percentage of cpu usage, this pointed out that cpu wasn't used efficiently.</p>
<pre><code>cores = multiprocessing.cpu_count()
assert gensim.models.doc2vec.FAST_VERSION &gt; -1, &quot;This will be painfully slow otherwise&quot;

simple_models = [
    # PV-DBOW plain
    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, 
            epochs=20, workers=cores),
    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes
    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, 
            epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),
    # PV-DM w/ concatenation - big, slow, experimental mode
    # window=5 (both sides) approximates paper's apparent 10-word total window size
    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, 
            epochs=20, workers=cores),
]

for model in simple_models:
    model.build_vocab(all_x_w2v)
    print(&quot;%s vocabulary scanned &amp; state initialized&quot; % model)

models_by_name = OrderedDict((str(model), model) for model in simple_models)
</code></pre>
<p>Edit:</p>
<p>I tried to use parameter corpus_file instead of documents, and resolved above problem. but, I need to adjust the code and convert all_x_w2v to file, and all_x_w2v didn't directly do this.</p>
","2281101","","-1","","2020-06-20 09:12:55","2019-08-17 00:06:53","Not efficiently to use multi-Core CPU for training Doc2vec with gensim","<gensim>","1","0","1","","","CC BY-SA 4.0"
"66021967","1","","","2021-02-03 05:49:52","","1","15","<p>Gensim currently only offers Python API. After I create a LDA model using gensim, is it possible to use this model in Java environment?</p>
","6221871","","","","","2021-02-03 05:49:52","Run Gensim LDA model in Java environment","<java><gensim>","0","0","","","","CC BY-SA 4.0"
"40315446","1","","","2016-10-29 02:21:12","","1","1351","<p>I am trying to implement doc2vec from gensim but having some errors and theres not enough documentation or help on the web.
Here is part of my working code:</p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import LabeledSentence

class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename
    def __iter__(self):
        with open(self.filename, 'r') as f:
            for uid, line in enumerate(f):
                print LabeledSentence(line.split(), tags=['TXT_%s' % uid])
                yield LabeledSentence(words=line.split(), tags=['TXT_%s' % uid])

sentences = LabeledLineSentence('myfile.txt')
</code></pre>

<p>what my txt file looks like:</p>

<pre><code>  1 hi how are you
  2 hi how are you
  3 hi how are you
  4 its such a great day
  5 its such a great day
  6 its such a great day
  7 i like dogs
  8 i like cats
  9 i like snakes
 10 the ice cream was yummy
 11 the cake was awesome  
</code></pre>

<h1>init the model</h1>

<pre><code>model = Doc2Vec(alpha=0.025, min_alpha=0.025, size=50, window=5, min_count=5,
                dm=1, workers=8, sample=1e-5)       
</code></pre>

<h1>example print output:</h1>

<pre><code>LabeledSentence(['hi', 'how', 'are', 'you'], ['TXT_0'])
LabeledSentence(['hi', 'how', 'are', 'you'], ['TXT_1'])
LabeledSentence(['hi', 'how', 'are', 'you'], ['TXT_2'])
LabeledSentence(['its', 'such', 'a', 'great', 'day'], ['TXT_3'])
LabeledSentence(['its', 'such', 'a', 'great', 'day'], ['TXT_4'])
</code></pre>

<p>This is where the error is:</p>

<pre><code>for epoch in range(500):
    try:
        print 'epoch %d' % (epoch)
        model.train(sentences)
        model.alpha *= 0.99
        model.min_alpha = model.alpha
    except (KeyboardInterrupt, SystemExit):
        break

RuntimeError: you must first build vocabulary before training the model
</code></pre>

<p>Any idea why? </p>
","2800939","","2800939","","2016-10-29 07:49:11","2016-10-30 10:08:23","Python simple implementation of doc2vec?","<python><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 3.0"
"49929066","1","49935903","","2018-04-19 19:44:29","","0","145","<p>I am looking for a good approach using python libraries to tackle the following problem:</p>

<p>I have a dataset with a column that has product description. The values in this column can be very messy and would have a lot of other words that are not related to the product. I want to know which rows are about the same product, so I would need to tag each description sentence with its main topics. For example, if I have the following: 
""500 units shoe green sport tennis import oversea plastic"", I would like the tags to be something like: ""shoe"", ""sport"". So I am looking to build an approach for semantic tagging of sentences, not part of speech tagging. Assume I don't have labeled (tagged) data for training. </p>

<p>Any help would be appreciated.</p>
","2187233","","","","","2018-04-20 07:03:35","Clear approach for assigning semantic tags to each sentence (or short documents) in python","<python-2.7><nlp><nltk><gensim><semantic-analysis>","1","0","","","","CC BY-SA 3.0"
"49929170","1","","","2018-04-19 19:52:08","","2","1767","<p>The following line works fine:</p>

<pre><code>import gensim
</code></pre>

<p>while the following line generates error:</p>

<pre><code>import gensim.test.utils
</code></pre>

<p>Error:</p>

<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-98-49e2788ad1a0&gt; in &lt;module&gt;()
----&gt; 1 import gensim.test.utils

ImportError: No module named 'gensim.test.utils'
</code></pre>

<p>I was working on converting GloVe to word2vec format:
<a href=""https://radimrehurek.com/gensim/scripts/glove2word2vec.html"" rel=""nofollow noreferrer"">glove2word2vec</a></p>
","1700890","","","","","2018-04-19 19:52:08","import gensim vs import gensim.test.utils","<python><gensim><word-embedding>","0","3","1","","","CC BY-SA 3.0"
"49867648","1","","","2018-04-16 23:29:15","","0","597","<p>I am trying to use Gensim.phrases library to identify phrases in a text. </p>

<p>I used the following: </p>

<pre><code>bigram = models.Phrases(txt_to_words,min_count=min_count, threshold=threshold,common_terms=common_terms)
</code></pre>

<p>And I get the error: </p>

<pre><code>&lt;ipython-input-13-1c8b06a0b078&gt; in words_to_phrases(txt_to_words, min_count, threshold)
     33     common_terms=[""of"", ""with"", ""without"", ""and"", ""or"", ""the"", ""a"",""in"",""to"",""is"",""but""]
     34 
---&gt; 35     bigram = models.Phrases(txt_to_words,min_count=min_count, threshold=threshold,common_terms=common_terms)
     36 
     37     # trigram

TypeError: __init__() got an unexpected keyword argument 'common_terms'
</code></pre>

<p>I have the latest gensim package 2.0+ </p>

<p>Any idea why it is not recognizing the common_terms parameter?</p>
","2769240","","","","","2018-04-17 00:04:43","Genism Phrase library not accepting common_terms","<python><gensim>","1","0","","","","CC BY-SA 3.0"
"49891527","1","","","2018-04-18 05:02:29","","2","2157","<p>I was having trouble with the ""most_similar"" call in a FastText model, from my understanding, Fasttext should be able to obtain results for words that aren't in the vocabulary, but I'm getting a ""Not in Vocabulary"" error, even when prior to saving and loading, the call was perfectly fine.</p>

<p>Here's the code from juypter.</p>

<pre><code>import gensim as gensim

model = gensim.models.FastText(my_sentences, size=100, window=5, min_count=3, workers=4, sg=1)
model.wv.most_similar(positive=['iPhone 6'])
</code></pre>

<p>Returns</p>

<pre><code>[('iPhone7', 0.942690372467041),
('iPhone7.', 0.9395840764045715),
('iPhone5s', 0.9379133582115173),
('iPhone6s', 0.9338586330413818),
('iPhone5S', 0.9335439801216125),
('iPhone5.', 0.9318809509277344),
('iPhone¬Æ', 0.9314558506011963),
('iPhone6', 0.9268479347229004),
('iPhone4s', 0.9223971366882324),
('iPhone5', 0.9212019443511963)]
</code></pre>

<p>So far so good, now I save the model.</p>

<pre><code>model.wv.save_word2vec_format(""example_fasttext.txt"", binary=False)
</code></pre>

<p>Then load it up again:</p>

<pre><code>from gensim.models import KeyedVectors
new_model = KeyedVectors.load_word2vec_format('example_fasttext.txt', binary=False, limit=50000)
</code></pre>

<p>Then I do the exact most_similar call from the model I just loaded:</p>

<pre><code>new_model.most_similar(positive=['iPhone 6'])
</code></pre>

<p>But results now are:</p>

<pre><code>KeyError: ""word 'iPhone 6' not in vocabulary""
</code></pre>

<p>Any idea what I did wrong?</p>
","9018429","","","","","2018-07-23 13:47:21","Gensim FastText - KeyError: ""word not in vocabulary""","<gensim><fasttext>","2","0","","","","CC BY-SA 3.0"
"41467115","1","","","2017-01-04 15:13:46","","2","600","<p>To extract the embedding representations of input data, the tensorflow documentation says we can use the following:</p>

<pre><code>embed = tf.nn.embedding_lookup(embeddings, input_data)
</code></pre>

<p>Accdg to the <a href=""https://www.tensorflow.org/api_docs/python/nn/embeddings#embedding_lookup"" rel=""nofollow noreferrer"">TF documentation</a>, the 2nd parameter of the function tf.nn.embedding_lookup is a tensor of ids:</p>

<blockquote>
  <p>ids: A Tensor with type int32 or int64 containing the ids to be looked up in params.</p>
</blockquote>

<p>My question is: Given a sentence, say, </p>

<blockquote>
  <p>""Welcome to the world""</p>
</blockquote>

<p>how can I represent and transform it into <code>ids</code>? In the code below, how can I transform my sentence into <code>input_data</code>. </p>

<pre><code>from gensim import models
embedding_path = ""../embeddings/GoogleNews-vectors-negative300.bin""
w = models.Word2Vec.load_word2vec_format(embedding_path, binary=True)
X = w.syn0
W = tf.Variable(tf.constant(0.0, shape=X.shape),trainable=False, name=""W"")
embedding_placeholder = tf.placeholder(tf.float32, X.shape)
embedding_init = W.assign(embedding_placeholder)
embed = tf.nn.embedding_lookup(embedding_init, input_data)
sess = tf.Session()
sess.run(embed, feed_dict={embedding_placeholder: X})
</code></pre>
","3009947","","3009947","","2017-01-04 15:39:21","2017-01-04 16:58:49","Using word2vec pretrained vectors, how to generate ids of a sentence as input to tf.nn.embedding_lookup function in tensorflow?","<python><tensorflow><gensim><word2vec>","1","3","","","","CC BY-SA 3.0"
"59297344","1","","","2019-12-12 03:34:19","","0","59","<p>My aim here is text summarization, not sure if I'm doing it correctly but here's the plan. I've got a dataframe called train_data. Each cell in every row contains messages. Now, I am looking to iterate over each cell or each message in the dataframe column to get the keywords from each message, using the gensim.summarization.keyword package. </p>

<p>I understand that the keyword function takes text as an input and I can't pass the whole df column inside so tried to iterate each cell over the keyword function as text but it doesn't seem to work. What am I missing here? Here's my code.</p>

<pre><code>cols = train_data.new_msg
for col in cols:
    cols

train_data['keywords'] = keywords(col)

</code></pre>

<p>I then plan to count the length of original vs new message(ie keyword column) to get the compression rate/ratio. </p>
","12388978","","","","","2019-12-12 07:52:08","Getting keywords from messages","<python><nlp><nltk><gensim><text-classification>","1","1","","","","CC BY-SA 4.0"
"43500996","1","","","2017-04-19 16:14:54","","5","1393","<p>I want to train a word2vec model on the english wikipedia using python with gensim. I closely followed <a href=""https://groups.google.com/forum/#!topic/gensim/MJWrDw_IvXw"" rel=""noreferrer"">https://groups.google.com/forum/#!topic/gensim/MJWrDw_IvXw</a> for that.</p>

<p>It works for me but what I don't like about the resulting word2vec model is that named entities are split which makes the model unusable for my specific application. The model I need has to represent named entities as a single vector. </p>

<p>Thats why I planned to parse the wikipedia articles with spacy and merge entities like ""north carolina"" into ""north_carolina"", so that word2vec would represent them as a single vector. So far so good.</p>

<p>The spacy parsing has to be part of the preprocessing, which I originally did as recommended in the linked discussion using:</p>

<pre><code>...
wiki = WikiCorpus(wiki_bz2_file, dictionary={})
for text in wiki.get_texts():
    article = "" "".join(text) + ""\n""
    output.write(article)
...
</code></pre>

<p>This removes punctuation, stop words, numbers and capitalization and saves  each article in a separate line in the resulting output file. The problem is that spacy's NER doesn't really work on this preprocessed text, since I  guess it relies on punctuation and capitalization for NER (?).</p>

<p><strong>Does anyone know if I can ""disable"" gensim's preprocessing so that it doesn't remove punctuation etc. but still parses the wikipedia articles to text directly from the compressed wikipedia dump? Or does someone know a better way to accomplish this? Thanks in advance!</strong></p>
","7890777","","","","","2019-10-07 14:14:32","Disabling Gensim's removal of punctuation etc. when parsing a wiki corpus","<python><nlp><gensim><word2vec><spacy>","2","3","","","","CC BY-SA 3.0"
"50408740","1","50418988","","2018-05-18 10:00:20","","1","710","<p>I have the following code and I think I am getting the vectors in a wrong way, because for example the vectors of two documents that are 100% identical are not the same.</p>

<pre><code>def getDocs(corpusPath):
    """"""Function for processings documents as TaggedDocument""""""
    # Loop over all the files in corpus
    for file in glob.glob(os.path.join(corpusPath, '*.csv')):
        # getWords is a function that gets the words from the provided directory
        # os.path.basename(file) takes the filename from the complete path
        yield TaggedDocument(words=getWords(file), tags=[os.path.basename(file)])

def getModel(corpusPath, outputName):
    # Get documents words from path
    documents = getDocs(corpusPath)

    cores = multiprocessing.cpu_count()

    # Initialize the model
    model = models.doc2vec.Doc2Vec(vector_size=100, epochs=10, min_count=1, max_vocab_size=None, alpha=0.025, min_alpha=0.01, workers=cores)

    # Build Vocabulary
    model.build_vocab(documents)

    # Train the model
    model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)

    # Save the model as shown below
    model.save_word2vec_format(outputName, doctag_vec=True, word_vec=False, prefix="""")
</code></pre>

<p>And the output has to be like this:</p>

<pre><code>12571 100
134602.csv 0.00691074 0.157398 0.0921498 0.126362 0.158668 -0.0753151 -0.164655 0.0883756 0.0407546 0.15239 -0.0145177 0.061617 -0.0891562 -0.0417054 -0.0858589 0.00102948 0.0161595 2.13553e-05 -0.0668119 0.0450828 0.117537 -0.0729031 -0.0580456 -0.00258632 -0.104359 0.136366 -0.144994 -0.12065 -0.121757 0.0830929 -0.16462 -0.0151503 0.0399056 0.160027 -0.0787732 -0.00789994 -0.094897 0.00608254 -0.0661624 0.129721 0.163127 -0.0793746 -0.0964145 0.0606208 0.0875067 0.0161015 -0.132051 -0.0491245 -0.154828 0.133222 -0.0687664 0.120808 -0.111705 -0.053042 -0.0912231 -0.111089 0.0443708 -0.139493 0.0607425 -0.161168 0.0786498 0.150048 0.146688 -0.0837242 -0.0553738 -0.117545 0.0986267 -0.0923841 0.098877 -0.12193 -0.062616 -0.0845228 -0.0636123 0.0823107 -0.0826875 0.139011 -0.0923962 0.0288433 0.137355 0.121588 -0.145517 0.160373 0.0628389 -0.0764258 -0.107213 0.0421445 0.137447 -0.0658571 0.0424128 0.0672861 0.109817 -0.126953 -0.0453275 0.0834503 0.0974179 0.00825522 -0.165445 -0.0213084 -0.0292943 -0.162938
125202.csv 0.106642 0.167441 -0.0275412 0.130408 -0.107533 0.091452 0.0103496 -0.0214623 0.0873943 -0.0465384 -0.165227 -0.0540914 -0.00923723 0.175378 -0.051865 0.0107003 -0.179349 0.0683971 -0.159605 0.0644916 0.136338 0.111336 -0.0805002 0.00214934 -0.0490576 0.151279 -0.0397022 0.075442 -0.0278023 -0.0636982 0.174473 0.087985 -0.0714066 -0.0800442 -0.103995 -0.0228613 0.157171 -0.0678672 -0.161953 0.0839289 -0.155191 -0.00721683 0.0586751 -0.0474399 -0.122106 0.170611 0.157929 0.075531 -0.13505 0.093849 -0.119415 0.0386302 0.0139714 0.0756701 -0.0810199 -0.111754 0.112905 0.130293 -0.126257 -0.00654255 -0.0369909 -0.072449 0.0257127 0.0716955 0.103714 -0.0842208 -0.0534867 -0.095218 0.127797 -0.029322 0.161806 -0.177695 -0.0684089 0.0623551 0.06396 0.0828089 -0.0590939 0.0180832 -0.0591218 0.136139 -0.153984 0.108085 -0.127018 -0.0847872 -0.167081 0.0199622 0.0209045 0.0320618 0.0591803 0.0809688 0.0799196 0.15632 -0.0519707 0.0270171 -0.163197 -0.0846849 -0.176135 -0.0120047 -0.0697305 0.014441
116200.csv -0.0182099 -0.130409 -0.138414 -0.0310527 -0.0274882 -0.0711805 -0.0628653 -0.144249 -0.166021 -0.0242265 -0.130593 -0.141916 0.0119525 0.0500143 -0.147568 -0.036778 0.110357 0.0439302 -0.132496 -0.105203 0.0356234 0.0982645 0.134903 -0.0648039 -0.0566216 0.138991 -0.0467151 -0.140643 0.139711 0.0943256 0.0576583 0.0644239 0.00136725 -0.0296913 0.0612566 0.148131 0.067239 0.100442 0.0665155 0.104861 -0.0498524 0.0995954 -0.115922 -0.00524584 0.0491675 0.159028 0.132554 0.0479373 0.141164 0.173129 0.022317 -0.000446397 0.0867293 -0.155649 -0.0675728 -0.0981307 -0.0806008 -0.0107237 -0.103454 -0.0753868 -0.0551634 0.170743 0.0495554 0.11536 -0.0294355 0.061617 0.126016 -0.04804 -0.0315217 -0.169522 -0.0892494 -0.025444 0.0672556 0.166157 0.0647261 0.0944827 -0.0792354 0.0182105 0.118192 0.000124603 -0.10565 -0.155033 0.107355 0.150469 -0.104327 -0.162604 -0.0218357 0.145972 -0.145784 -0.00176559 0.153054 -0.16377 -0.11736 0.0892985 -0.0212026 0.0511168 -0.146278 -0.0134697 -0.0540684 0.0791529
148597.csv -0.15473 0.0955252 0.0432369 -0.0945614 0.136283 -0.102851 0.0847211 -0.0396431 -0.0467567 0.17154 0.153097 0.0693114 0.163837 0.135897 0.146128 -0.167215 -0.152268 -0.11602 0.0282252 -0.0779752 -0.0829204 0.018318 0.00621094 0.0707405 0.0968831 0.00652018 -0.0568833 0.0916579 -0.0400151 -0.0391421 -0.0548217 -0.173926 -0.110223 -0.0317329 -0.02952 -0.129147 0.0698902 -0.154276 -0.157658 -0.14261 0.032107 -0.0385964 -0.0587693 0.0212146 0.143626 0.142041 -0.0530896 -0.133748 0.131452 0.13672 0.148338 0.160325 -0.113424 0.0678939 -0.0229337 -0.170486 -0.156904 0.0710402 0.00277802 0.120395 0.0360002 -0.0593753 0.155915 -0.0620641 -0.112055 0.0153659 0.147731 -0.0249911 0.0360584 -0.0402479 0.022273 0.00174414 -0.0178126 -0.116679 0.0191754 -0.0089874 0.083151 -0.168562 -0.160357 -0.0659622 0.0248376 0.045583 0.127733 -0.0675122 -0.0734585 0.113653 0.166756 0.0723445 0.0554671 -0.0751338 0.0481711 -0.00127609 0.0560728 0.124651 -0.0495638 0.0985305 -0.110315 0.0672438 0.096637 0.104245
166916.csv 0.168698 0.0629846 0.0248923 -0.105248 0.172408 -0.0322083 0.174124 -0.113572 -0.0104922 0.0429484 -0.0306917 0.022368 -0.0584265 0.0337984 -0.0225754 0.143456 -0.121288 -0.133673 0.0677091 0.0583681 0.0390327 -0.141176 0.0694527 -0.0290526 -0.129707 -0.0765447 0.071578 0.146411 -0.112526 0.103688 -0.110703 0.0781341 0.0318269 0.105218 0.0177797 0.123248 0.158062 0.0370042 -0.137394 0.0246147 0.00653834 0.166063 -0.100149 -0.0479191 -0.0702838 0.0690037 0.114349 -0.0274343 0.014801 -0.0421596 0.0694873 0.0662955 -0.12477 -0.0088994 0.104959 0.149459 0.16611 0.0265376 -0.134808 0.101123 0.0431258 0.0584757 -0.0315779 0.121671 -0.0380923 -0.0897689 -0.0237933 0.110452 -0.0039647 0.106183 -0.165717 -0.16557 0.136988 0.121843 0.0722612 -0.00844494 0.175932 -0.0751714 0.152611 -0.0646956 0.105122 -0.108245 0.0583691 0.113012 0.171521 -0.0258976 0.0851889 -0.0941529 0.153386 0.0455267 -0.0259182 -0.0437207 -0.150415 0.132313 -0.143572 -0.0281547 -0.00231613 -0.00760185 -0.147233 -0.167408
148291.csv 0.00976907 0.168438 -0.0919878 -0.164332 -0.138181 -0.149775 -0.0394723 0.027946 0.0662307 -0.00850593 0.12174 0.106023 -0.11512 0.0694538 0.128228 0.066019 0.0805346 0.00220964 -0.0465066 0.0923588 0.121286 0.168551 0.0462572 0.0221805 -0.119831 0.00797117 -0.00709804 -0.0222688 0.0938169 0.100695 0.133902 0.15964 0.0544278 -0.0504766 -0.0539783 -0.0158389 0.0280565 -0.10531 0.112356 -0.0349924 0.155673 0.0491142 0.171533 -0.044268 0.0560867 -0.135758 0.114202 -0.120608 0.0373457 -0.0847815 0.0285375 -0.0101114 0.0169282 -0.00141743 -0.028344 -0.00979434 -0.0599551 0.0554465 -0.0583942 -0.169627 0.167471 -0.00661054 0.114252 -0.00489984 0.167312 0.144928 0.0376684 -0.118885 0.0426739 0.169052 0.00265325 0.146609 0.163534 -0.100965 -0.101386 0.127619 0.148285 -0.0881821 -0.100448 -0.044064 0.106071 0.0239426 0.0733384 -0.0962991 0.0939341 0.0659483 0.122844 -0.140426 -0.0485195 0.0645185 0.037179 0.0963829 -0.109955 -0.151168 -0.0413991 -0.0556731 -0.173456 -0.167728 -0.128145 0.150923
...
</code></pre>

<p>Where the first word of each line is the name of each file, and what follows is the corresponding vector for that file. I need to save the vectors in this way to use an external software.</p>
","9025222","","","","","2018-05-18 20:49:12","Gensim Doc2Vec: I'm gettting different vectors from documents that are identical","<python><gensim><word-embedding><doc2vec>","1","0","1","","","CC BY-SA 4.0"
"25272500","1","","","2014-08-12 19:26:56","","1","123","<p>If I try to move my gensim database though windows explorer and access it from the new location I get an error because it says there is a sharding error?</p>

<p>Also I know the underlying SQLite is in gensim can I port that to the SQLite in a django model?</p>
","1840592","","","","","2014-08-12 19:26:56","Gensim sharding in python when moving database","<python><django><gensim>","0","0","","","","CC BY-SA 3.0"
"67143926","1","67184381","","2021-04-17 23:38:52","","0","55","<p>I preprocess my docs, trained my model, and saved it by following the guidelines given here: <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html</a></p>
<p>After a period of time, I want to re-train my model with different parameters. However, I don't want to preprocess docs and create &quot;train corpus&quot; again because it takes nearly 3 days. Is there a solution to easily load saved model, change parameters and train the model with these new parameters for the following codes:</p>
<pre><code>model = Doc2Vec.load(myPath/myModel.doc2vec)
model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=40, dm=1, window=8)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>
<p>Best.</p>
","8141098","","","","","2021-04-20 18:24:46","How to change parameters of saved model without training docs in Gensim Doc2Vec?","<parameters><model><gensim><doc2vec>","1","2","1","","","CC BY-SA 4.0"
"48962171","1","","","2018-02-24 11:10:57","","23","20025","<p>I tried to follow <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">this.</a><br>
But some how I wasted a lot of time ending up with nothing useful.<br>
I just want to train a <code>GloVe</code> model on my own corpus (~900Mb corpus.txt file).
I downloaded the files provided in the link above and compiled it using <code>cygwin</code> (after editing the demo.sh file and changed it to <code>VOCAB_FILE=corpus.txt</code> . should I leave <code>CORPUS=text8</code> unchanged?)
the output was:  </p>

<ol>
<li>cooccurrence.bin </li>
<li>cooccurrence.shuf.bin  </li>
<li>text8</li>
<li>corpus.txt</li>
<li>vectors.txt</li>
</ol>

<p>How can I used those files to load it as a <code>GloVe</code> model on python?</p>
","9160882","","6796042","","2020-01-27 06:21:12","2021-09-10 02:19:02","How to Train GloVe algorithm on my own corpus","<nlp><stanford-nlp><gensim><word2vec><glove>","4","0","7","","","CC BY-SA 4.0"
"40318719","1","40319083","","2016-10-29 11:42:13","","1","332","<p>I have made a sample program for getting topic distribution per document after doing LDA using gensim</p>

<pre><code>documents = [""Apple is releasing a new product"", 
             ""Amazon sells many things"",
             ""Microsoft announces Nokia acquisition""]   

stoplist=[""is"",""are"",""am"",""were"",""a"",""me"",""I""]

texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, update_every=1, chunksize=10000, passes=1)
lda.print_topics(2)
</code></pre>

<p>But the program is not printing anything.. Any changes required?</p>
","6354442","","","","","2016-10-29 12:26:36","Printing topic distribution after LDA using gensim","<python><lda><gensim>","1","0","","","","CC BY-SA 3.0"
"49926774","1","","","2018-04-19 17:19:54","","-1","615","<p>I am using python3.5 and trying to repeat the code implemented in this video moment:</p>

<p><a href=""https://youtu.be/BkeQzJt0f5A?t=73"" rel=""nofollow noreferrer"">https://youtu.be/BkeQzJt0f5A?t=73</a>
<a href=""https://youtu.be/BkeQzJt0f5A?t=73"" rel=""nofollow noreferrer""></a></p>

<p>In the piece of code below ""TypeError: slice indices must be integers or None or have an index method"" is happening:</p>

<pre><code>with open(""metadata.tsv"", ""w+"") as file_metadata:
    for i,word in enumerate(model.wv.index2word[:max]):
        w2v[i] = model.wv[word]
        file_metadata.write(word + ""\n"")
</code></pre>

<p>How can I fix this to get the output from w2v like the video?</p>
","4104459","","4104459","","2018-04-19 17:56:53","2018-04-19 18:06:58","gensim word2vec model.wv.index2word ""TypeError: slice indices must be integers or None or have an __index__ method"" in enumerate","<python-3.x><word2vec><gensim><tensorboard><enumerate>","1","0","","","","CC BY-SA 3.0"
"67143593","1","","","2021-04-17 22:33:21","","0","517","<p>I am trying to install gensim through because I am working on Topic Modelling with Python on Windows 10.
I did:</p>
<pre><code>pip install --upgrade gensim 
</code></pre>
<p>and I keep encountering error:</p>
<pre><code>  ERROR: Command errored out with exit status 1:
 command: 'c:\users\swat_\appdata\local\programs\python\python39\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'C:\\Users\\swat_\\AppData\\Local\\Temp\\pip-install-jq6jmo27\\gensim_b5ab7f463fbc44d1850e29dba11294b4\\setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'C:\\Users\\swat_\\AppData\\Local\\Temp\\pip-install-jq6jmo27\\gensim_b5ab7f463fbc44d1850e29dba11294b4\\setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record 'C:\Users\swat_\AppData\Local\Temp\pip-record-090khyko\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\swat_\appdata\local\programs\python\python39\Include\gensim'
     cwd: C:\Users\swat_\AppData\Local\Temp\pip-install-jq6jmo27\gensim_b5ab7f463fbc44d1850e29dba11294b4\
Complete output (382 lines):
</code></pre>
<p>then an output of 382 lines that all resemble:</p>
<blockquote>
<p>running install
running build
running build_py
creating build
creating build\lib.win-amd64-3.9
creating build\lib.win-amd64-3.9\gensim
copying gensim\downloader.py -&gt; build\lib.win-amd64-3.9\gensim
copying gensim\interfaces.py -&gt; build\lib.win-amd64-3.9\gensim
copying gensim\matutils.py -&gt; build\lib.win-amd64-3.9\gensim
copying gensim\nosy.py -&gt; build\lib.win-amd64-3.9\gensim
copying gensim\utils.py -&gt; build\lib.win-amd64-3.9\gensim
copying gensim_<em>init</em>_.py -&gt; build\lib.win-amd64-3.9\gensim
creating build\lib.win-amd64-3.9\gensim\corpora
copying gensim\corpora\bleicorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
copying gensim\corpora\csvcorpus.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
copying gensim\corpora\dictionary.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
copying gensim\corpora\hashdictionary.py -&gt; build\lib.win-amd64-3.9\gensim\corpora
....</p>
</blockquote>
<p>then ends with this.</p>
<blockquote>
<p>ERROR: Command errored out with exit status 1: 'c:\users\swat_\appdata\local\programs\python\python39\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'C:\Users\swat_\AppData\Local\Temp\pip-install-42rc4g6q\gensim_9e111ba4f41b4925a9f3fc8cdd0170f7\setup.py'&quot;'&quot;'; <strong>file</strong>='&quot;'&quot;'C:\Users\swat_\AppData\Local\Temp\pip-install-42rc4g6q\gensim_9e111ba4f41b4925a9f3fc8cdd0170f7\setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(<strong>file</strong>);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, <strong>file</strong>, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record 'C:\Users\swat_\AppData\Local\Temp\pip-record-3djkmh1b\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\swat_\appdata\local\programs\python\python39\Include\gensim' Check the logs for full command output.</p>
</blockquote>
<p>I have tried to update setuptools by:</p>
<pre><code>pip install --upgrade setuptools
</code></pre>
<p>Didn't help.. anybody know what to do?</p>
","11234734","","11138259","","2021-04-19 21:46:15","2021-04-19 21:46:15","ERROR: Command errored out with exit status 1: when pip install --upgrade gensim","<python><pip><gensim>","1","2","","","","CC BY-SA 4.0"
"59331278","1","","","2019-12-14 00:17:18","","1","737","<p>In python, I'm building ngrams with gensim and passing the words into spacy for lemmatization. I'm finding that spacy is not working very well as it's keeping many words as plurals that shouldn't be.</p>

<p>It looks like this is mostly happening when it's mistakenly tagging nouns as proper nouns.</p>

<pre><code>import spacy
nlp = spacy.load('en', disable=['parser','ner'])

doc = nlp(u""bed_bugs bed bug beds bedbug bugs bed_bug nymph nymphs nintendo"")

for token in doc:
    print(""original: {}, Lemma: {}, POS: {}"".format(token, token.lemma_, token.pos_))
</code></pre>

<p>output:</p>

<pre><code>original: bed_bugs, Lemma: bed_bugs, POS: PROPN
original: bed, Lemma: bed, POS: NOUN
original: bug, Lemma: bug, POS: NOUN
original: beds, Lemma: bed, POS: VERB
original: bedbug, Lemma: bedbug, POS: PROPN
original: bugs, Lemma: bugs, POS: PROPN
original: bed_bug, Lemma: bed_bug, POS: X
original: nymph, Lemma: nymph, POS: PROPN
original: nymphs, Lemma: nymphs, POS: PROPN
original: nintendo, Lemma: nintendo, POS: PROPN
</code></pre>

<p>My preferred output would have these changes -</p>

<pre><code>bed_bugs -&gt; bed_bug
nymphs -&gt; nymph
bugs -&gt; bug
</code></pre>

<p>Is there a way to accomplish this with spacy or some other tool?</p>
","1171899","","","","","2019-12-16 19:15:30","Improve spacy lemmatization with bigrams, proper nouns, and plurals?","<python><nlp><gensim><spacy>","1","0","1","","","CC BY-SA 4.0"
"59368232","1","59368454","","2019-12-17 05:48:21","","0","283","<p>I wonder what does <code>.build_vocab_from_freq()</code> function from gensim actually do? What is the difference when I'm not using it? Thank you!</p>
","11991961","","","","","2019-12-17 06:08:36","Gensim Word2Vec or FastText build vocab from frequency","<python><gensim><word2vec><fasttext>","1","0","","","","CC BY-SA 4.0"
"32321375","1","","","2015-08-31 23:25:19","","1","658","<p>I am getting segmentation fault when I multiply a scipy sparse matrix by its transpose. I've searched all over the Internet but could not find any answer. Any help is appreciated.</p>

<pre><code>&gt;&gt;&gt; import cPickle
&gt;&gt;&gt; fs = open('vec.pickle', 'rb')
&gt;&gt;&gt; vec = cPickle.load(fs)
&gt;&gt;&gt; vec
&lt;3020x512 sparse matrix of type '&lt;type 'numpy.float64'&gt;' with 26008 stored elements in Compressed Sparse Column format&gt;
&gt;&gt;&gt; vec.max()
10.0
&gt;&gt;&gt; vec.min()
0.0
&gt;&gt;&gt; vec * vec.T
Segmentation fault: 11
</code></pre>

<p>I do not think this is memory issue since the dimension is small. The vec object is created by gensim, if that information helps.</p>

<p>I also do not think this is overflow issues since the range of element is [0.0, 10.0]</p>

<p>The pickle object is here:
<a href=""https://drive.google.com/open?id=0B3DJbsn85XMvdmFYT0MzZVFjOVU"" rel=""nofollow"">https://drive.google.com/open?id=0B3DJbsn85XMvdmFYT0MzZVFjOVU</a></p>
","5286416","","5286416","","2015-08-31 23:37:02","2015-09-01 05:55:07","sparse matrix python segmentation fault","<segmentation-fault><scipy><sparse-matrix><gensim>","1","4","","","","CC BY-SA 3.0"
"50627026","1","50722119","","2018-05-31 14:45:48","","1","145","<p>I want to generate a Topic to Topic Matrix in order to find similar topic to generate internal clusters with the function <code>gensim.models.ldamodel.diff</code> from gensim LDA.
How can I save my generated data into a csv with topics over topics and the distances (in this case hellinger distance) in the cells?
This code is not working for me:</p>

<pre><code>from gensim import models
import pandas

dateiname_model1 = ""lda.model""
model1 =  models.LdaModel.load(dateiname_model1)

topic_over_topic = model1.diff(model1, annotation=True)

topic_over_topic_speicherpfad = ""topic_over_topic_similarity.csv""
pandas.DataFrame(topic_over_topic).to_csv(topic_over_topic_speicherpfad, sep=';')
</code></pre>
","9751594","","","","","2018-06-06 13:47:24","Topic Similarity in one model to csv Matrix","<python-3.x><export-to-csv><gensim>","1","3","1","","","CC BY-SA 4.0"
"41729287","1","41740066","","2017-01-18 20:56:50","","0","607","<p>I am using gensim for topic modeling. I've created a corpus using </p>

<pre><code>wordDict = corpora.Dictionary(trimmedTextTokens)

gsCorpus = [wordDict.doc2bow(text) for text in trimmedTextTokens]
</code></pre>

<p>where trimmedTextTokens are the result of removing stop words. Now I want to filter out the terms from the corpus that are not in a list of a restricted or constructed vocabulary. Any ideas? Thank you!!</p>
","1801125","","","","","2017-01-19 10:56:02","How to filter out words in a corpus from a constrained vocabulary with gensim?","<python><nlp><gensim><topic-modeling>","1","0","","","","CC BY-SA 3.0"
"67116370","1","67121201","","2021-04-15 21:40:48","","0","32","<p>I use gensim 4.0.1 and follow tutorial <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">1</a> and <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html"" rel=""nofollow noreferrer"">2</a>:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

texts = [
    &quot;Human machine interface for lab abc computer applications&quot;,
    &quot;A survey of user opinion of computer system response time&quot;,
    &quot;The EPS user interface management system&quot;,
    &quot;System and human system engineering testing of EPS&quot;,
    &quot;Relation of user perceived response time to error measurement&quot;,
    &quot;The generation of random binary unordered trees&quot;,
    &quot;The intersection graph of paths in trees&quot;,
    &quot;Graph minors IV Widths of trees and well quasi ordering&quot;,
    &quot;Graph minors A survey&quot;,
]

texts = [t.lower().split() for t in texts]

documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(texts)]
model = Doc2Vec(documents, epochs=50, vector_size=5, window=2, min_count=2, workers=4)

new_vector = model.infer_vector(&quot;human machine interface&quot;.split())


for rank,(doc_id,score) in enumerate(model.dv.most_similar_cosmul(positive=[new_vector])):
        print('{}. {:.5f} [{}] {}'.format(rank, score, doc_id, ' '.join(documents[doc_id].words)))


1. 0.56613 [7] graph minors iv widths of trees and well quasi ordering
2. 0.55941 [6] the intersection graph of paths in trees
3. 0.55061 [2] the eps user interface management system
4. 0.54981 [1] a survey of user opinion of computer system response time
5. 0.52249 [4] relation of user perceived response time to error measurement
6. 0.52240 [8] graph minors a survey
7. 0.49214 [0] human machine interface for lab abc computer applications
8. 0.49016 [3] system and human system engineering testing of eps
9. 0.47899 [5] the generation of random binary unordered trees
‚Äã
</code></pre>
<p>Why the document[0] containing &quot;human machine interface&quot; has such a low (position 7) ranking? Is it a result of semantic generalization or the model needs to be tuned? Is larger corpus tutorial available to get repeatable results?</p>
","7206879","","","","","2021-04-16 07:51:50","Understanding Gensim Doc2vec ranking","<gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"58804099","1","58806607","","2019-11-11 15:18:43","","0","122","<p>Going through the gensim source, I noticed the <code>simple_preprocess</code> utility function clears all punctuations except those with words starting with an underscore, <code>_</code>. Is there a reason for this?</p>

<pre class=""lang-py prettyprint-override""><code>def simple_preprocess(doc, deacc=False, min_len=2, max_len=15):
    tokens = [
        token for token in tokenize(doc, lower=True, deacc=deacc, errors='ignore')
        if min_len &lt;= len(token) &lt;= max_len and not token.startswith('_')
    ]
    return tokens

</code></pre>
","4673839","","","","","2019-11-11 18:15:07","Why does gensim ignore underscores during preprocessing?","<nltk><gensim>","1","0","","","","CC BY-SA 4.0"
"64102023","1","64106705","","2020-09-28 12:15:48","","1","33","<p>I've created an artificial corpus (with 52624 documents). Each document is a list of objects (there are 461 of them).</p>
<p>So one possibility could be: <code>['chair', 'chair', 'chair', 'chair', 'chair', 'table', 'table']</code></p>
<p>Here's a bar plot (log-scale) of the vocabulary.</p>
<p><a href=""https://i.stack.imgur.com/B1CP3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B1CP3.png"" alt=""enter image description here"" /></a></p>
<p>And this is how I defined the model:</p>
<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=8, workers=4, min_count=1,epochs=40, dm=0)
</code></pre>
<p>Observing at:
<code>model.wv.most_similar_cosmul(positive = [&quot;chair&quot;])</code></p>
<p>I see non related words</p>
<p>And it seems to me that the following works poorly as well:</p>
<pre><code>inferred_vector = model.infer_vector([&quot;chair&quot;])
model.docvecs.most_similar([inferred_vector])
</code></pre>
<p>Where has my model failed?</p>
<p><strong>UPDATE</strong></p>
<p>There is the data (JSON file):</p>
<p><a href=""https://gofile.io/d/bZDcPX"" rel=""nofollow noreferrer"">https://gofile.io/d/bZDcPX</a></p>
","10255450","","10255450","","2020-09-28 13:29:30","2020-09-28 17:15:08","Can doc2vec work on an artificial ""text""?","<machine-learning><nlp><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"66448514","1","66467946","","2021-03-02 23:21:20","","0","116","<p>I am a beginner with gensim word2vec, and I am encountering a memory error when preparing text for training the model. I am using Python 3.8.8. I have about 900,000 text files in 12 different folders. I was thinking I should send all text documents through gensim.utils.simple_preprocess, and then I'd have a list of lists for the model. After going through about 150,000 documents, I received a memory error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;word2vec_part1.py&quot;, line 58, in &lt;module&gt;
    documents = list(read_input(paths))
  File &quot;word2vec_part1.py&quot;, line 39, in read_input
    myfile = infile.read()
MemoryError
</code></pre>
<p>Is there a way to fix this memory issue? I included the code I am using below. I am new to Python, word2vec, and stackoverflow, so I apologize if my question is poorly worded or if this is a dumb question! Thank you for your time!</p>
<pre><code># imports and logging

import gensim 
import logging
import os
import os.path
import glob


logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# define function

def read_input(inputs):

    # logging info
    logging.info(&quot;reading files&quot;)

    # set working directories and load files into a list
    for path in inputs:
        os.chdir(path)
        read_files=glob.glob(&quot;*.txt&quot;) 
        # preprocess and counting
        for i, file in enumerate(read_files):
            if(i%10000==0):
                logging.info(&quot;read {0} reviews&quot;.format(i))
            # preprocessing and return a list of words
            with open(file, &quot;rb&quot;) as infile:
                myfile = infile.read()
                yield gensim.utils.simple_preprocess(myfile)


# create a list of all file paths
paths = [#here is a list of file paths]

# call function
documents = list(read_input(paths))
logging.info(&quot;done reading files!!&quot;)
print(len(documents))
print(documents[1])

# training word2vec model
model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2)
model.train(documents,total_examples=len(documents),epochs=10)
model.save(&quot;word2vec.model&quot;)

# look up top 6 words similar to 'law'
w1 = [&quot;law&quot;]
model.wv.most_similar (positive=w1,topn=6)

logging.info(&quot;done!!!&quot;)
</code></pre>
","14598255","","","","","2021-03-04 02:59:02","Memory Error in Python using gensim.utils.simple_preprocess","<python><memory><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"67118171","1","","","2021-04-16 02:06:28","","0","63","<p>For Gensim 3.8, I can use the following script to initialize a model with pre-train weights and then train it on my own corpus. How to do this in Gensim 4.0?</p>
<pre><code>mod = Word2Vec(size=300, min_count=5, workers=1, sg=1, seed=1)
mod.build_vocab(my_corpus)
mod.min_count = 0
pret_mod = KeyedVectors.load_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True)
mod.build_vocab([list(pret_mod.vocab.keys())], update=True)
mod.intersect_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True, lockf=1.0)
mod.train(my_corpus, total_examples=mod.corpus_count, epochs=mod.iter)
</code></pre>
","6221871","","","","","2021-04-16 02:06:28","For Gensim 4.0, how to use a pre-trained model and further train it with my own corpus?","<nlp><gensim><word2vec><word-embedding><pre-trained-model>","0","3","","","","CC BY-SA 4.0"
"64125039","1","64128017","","2020-09-29 17:54:57","","0","72","<p>I have a df with 2 columns and 5 million rows, all text (customer reviews of businesses).
<code>df.head()</code> produces:
<a href=""https://i.stack.imgur.com/JXtXR.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JXtXR.jpg"" alt=""enter image description here"" /></a></p>
<p><code>df.info()</code> shows that memory usage is only 120.3+ MB</p>
<p>I am trying to do topic modelling of <code>df['text']</code> using the gensim library. I attempt to create a document-term matrix (dtm) first and then perform latent Dirichlet allocation (LDA) as follow:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from gensim import matutils, models
import scipy.sparse

cv = CountVectorizer(stop_words='english')
data_cv = cv.fit_transform(df.text)
data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names()) #LINE THROWING MemoryError

data_dtm.index = df.index

tdm = data_dtm.transpose()

sparse_counts = scipy.sparse.csr_matrix(tdm)
corpus = matutils.Sparse2Corpus(sparse_counts)

id2word = dict((v, k) for k, v in cv.vocabulary_.items())
lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)
lda.print_topics()
</code></pre>
<p><strong>Issue</strong>: But the 7th line (pd.DataFrame()) throws the MemoryError  while I still have 60% of the machine memory free. Even when I repeat the operation on the first 100,000 rows of df, I get the same MemoryError.</p>
<p>Since this is topic modeling, I would rather analyze all the rows together, or at least analyze them in a few batches.</p>
<p><strong>Question</strong> What is making Python run out of memory when converting <code>data_cv</code> to dataframe? How can I get past it?</p>
","4701426","","","","","2020-09-29 21:40:17","MemoryError in pd.DataFrame()","<pandas><gensim><lda><topic-modeling>","1","0","","","","CC BY-SA 4.0"
"57795240","1","","","2019-09-04 20:26:48","","0","28","<p>I am doing some <code>topic modelling</code>work. If I understand things correctly, a <code>LDA</code> model is entirely defined by its word/topic distribution (which can be obtained with <code>model.get_topics()</code> in <code>gensim</code>. I am wondering if it is possible to create a <code>gensim</code> model by specifying this distribution, notably in order to use gensim's functions, such <code>CoherenceModel</code>, and topic prediction.  </p>
","10011330","","","","","2019-09-04 20:26:48","Create a Gensim model by specifying the word/topics distributions","<gensim><lda><topic-modeling>","0","2","","","","CC BY-SA 4.0"
"58816895","1","58817060","","2019-11-12 10:38:53","","0","95","<p>I am trying to train a Doc2Vec model using gensim.</p>

<p>The dataset i am using is the 20 newsgroups dataset [1] which is included in sklearn's datasets module.</p>

<p>I have used the example in the gensim documentation to create the model.</p>

<pre class=""lang-py prettyprint-override""><code>docs = newsgroups_train['data']
enumerated_docs = enumerate(docs)
documnets= [TaggedDocument(doc.split(),i) for i, doc in enumerated_docs]
model = Doc2Vec(documnets, vector_size=20, window=2, min_count=30, workers=4)
</code></pre>

<p>I checked every line of code, all seems to be working up to the line which initializes the model.</p>

<p>I  get a type error:
<code>TypeError: 'int' object is not iterable</code></p>

<p>[1] <a href=""https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html</a></p>
","4114945","","","","","2019-11-12 10:47:20","Type error when trying to create a doc2vec model in gensim","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"20349958","1","20350229","","2013-12-03 11:31:04","","29","30024","<p>I am trying to understand how gensim package in Python implements Latent Dirichlet Allocation. I am doing the following:</p>

<p>Define the dataset</p>

<pre><code>documents = [""Apple is releasing a new product"", 
             ""Amazon sells many things"",
             ""Microsoft announces Nokia acquisition""]             
</code></pre>

<p>After removing stopwords, I create the dictionary and the corpus:</p>

<pre><code>texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
</code></pre>

<p>Then I define the LDA model.</p>

<pre><code>lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, update_every=1, chunksize=10000, passes=1)
</code></pre>

<p>Then I print the topics:</p>

<pre><code>&gt;&gt;&gt; lda.print_topics(5)
['0.181*things + 0.181*amazon + 0.181*many + 0.181*sells + 0.031*nokia + 0.031*microsoft + 0.031*apple + 0.031*announces + 0.031*acquisition + 0.031*product', '0.077*nokia + 0.077*announces + 0.077*acquisition + 0.077*apple + 0.077*many + 0.077*amazon + 0.077*sells + 0.077*microsoft + 0.077*things + 0.077*new', '0.181*microsoft + 0.181*announces + 0.181*acquisition + 0.181*nokia + 0.031*many + 0.031*sells + 0.031*amazon + 0.031*apple + 0.031*new + 0.031*is', '0.077*acquisition + 0.077*announces + 0.077*sells + 0.077*amazon + 0.077*many + 0.077*nokia + 0.077*microsoft + 0.077*releasing + 0.077*apple + 0.077*new', '0.158*releasing + 0.158*is + 0.158*product + 0.158*new + 0.157*apple + 0.027*sells + 0.027*nokia + 0.027*announces + 0.027*acquisition + 0.027*microsoft']
2013-12-03 13:26:21,878 : INFO : topic #0: 0.181*things + 0.181*amazon + 0.181*many + 0.181*sells + 0.031*nokia + 0.031*microsoft + 0.031*apple + 0.031*announces + 0.031*acquisition + 0.031*product
2013-12-03 13:26:21,880 : INFO : topic #1: 0.077*nokia + 0.077*announces + 0.077*acquisition + 0.077*apple + 0.077*many + 0.077*amazon + 0.077*sells + 0.077*microsoft + 0.077*things + 0.077*new
2013-12-03 13:26:21,880 : INFO : topic #2: 0.181*microsoft + 0.181*announces + 0.181*acquisition + 0.181*nokia + 0.031*many + 0.031*sells + 0.031*amazon + 0.031*apple + 0.031*new + 0.031*is
2013-12-03 13:26:21,881 : INFO : topic #3: 0.077*acquisition + 0.077*announces + 0.077*sells + 0.077*amazon + 0.077*many + 0.077*nokia + 0.077*microsoft + 0.077*releasing + 0.077*apple + 0.077*new
2013-12-03 13:26:21,881 : INFO : topic #4: 0.158*releasing + 0.158*is + 0.158*product + 0.158*new + 0.157*apple + 0.027*sells + 0.027*nokia + 0.027*announces + 0.027*acquisition + 0.027*microsoft
&gt;&gt;&gt; 
</code></pre>

<p>I'm not able to understand much out of this result. Is it providing with a probability of the occurrence of each word? Also, what's the meaning of topic #1, topic #2 etc? I was expecting something more or less like the most important keywords.</p>

<p>I already checked the <a href=""http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""noreferrer"">gensim tutorial</a> but it didn't really help much.</p>

<p>Thanks.</p>
","295338","","6573902","","2021-01-25 14:57:27","2021-01-25 14:57:27","Understanding LDA implementation using gensim","<python><gensim><lda><topic-modeling><dirichlet>","5","2","20","","","CC BY-SA 3.0"
"48953871","1","48954904","","2018-02-23 18:08:41","","1","1513","<p>I have three documents in a df:</p>

<pre><code>id    author    document
12X   john      the cat sat
12Y   jane      the dog ran
12Z   jane      the hippo ate
</code></pre>

<p>These documents are converted into a corpus of <code>TaggedDocuments</code> with the tags being the typical practice of semantically meaningless ints:</p>

<pre><code>def read_corpus(documents):
    for i, plot in enumerate(documents):
        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(plot, max_len=30), [i])

train_corpus = list(read_corpus(df.document))
</code></pre>

<p>This corpus is then used to train my <code>Doc2Vec</code> model:</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=55)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>

<p>The resulting vectors of the model are accessed like this:</p>

<pre><code>model.docvecs.vectors_docs
</code></pre>

<p>How would I tie the original df to the resulting vectors? Now that all the documents are trained and vectors are identified for each one, I want to query the set of vectors by author. For example, if I want to return a set of vectors only for Jane, how would I do so?</p>

<p>I think the basic idea is to identify the int tags that correspond to Jane and then do something like this to access them:</p>

<pre><code>from operator import itemgetter 
a = model.docvecs.vectors_docs
b = [1, 2]
itemgetter(*b)(a)
</code></pre>

<p>How would I identify the tags though? They are only meaningful to the model and the tagged documents, so they don't join back to my original df.</p>
","7668467","","7668467","","2018-02-23 19:01:44","2018-02-23 19:49:19","Gensim Doc2Vec Access Vectors by Document Author","<python><gensim><doc2vec>","1","0","","","","CC BY-SA 3.0"
"57316991","1","","","2019-08-01 21:07:17","","1","209","<pre><code>model.similar_by_vector(model['king'] - model['man'] + model['woman'], topn=1)[0]
</code></pre>

<p>Results in </p>

<pre><code>('king', 0.8551837205886841)
</code></pre>

<p>Whereas </p>

<pre><code>model.most_similar(positive=['king', 'queen'], negative=['man'], topn=1)[0]
</code></pre>

<p>Gives a different answer (the one you'd expect)</p>

<p><code>('monarch', 0.6350384950637817)</code></p>

<p>But I'd expect both of these to return the same thing. Am I misunderstanding how vector math should be performed on these vectors?</p>
","5009004","","","","","2019-08-01 22:02:28","Why do passing 'positive' and 'negative' parameters into gensim's most_similar function not return the same as the vector math results?","<nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"49210010","1","","","2018-03-10 14:10:15","","5","753","<p>I want to add new words into a trained gensim word2vec model using a new text dataset. However, I want to preserve the old word embeddings and just add the new words from the dataset into the existing model. This means simple retraining of the old model with the new text dataset isn't an option as it will readjust the vectors of the previous word embeddings that are also in the new text dataset. Can you give any suggestions regarding this task? I would like something like Gensim's doc2vec infer feature where you feed the model some text input and it gives a vector as an output. Thanks.</p>
","8841203","","","","","2019-09-03 20:53:51","How to infer new word vectors from a gensim word2vec model?","<neural-network><word2vec><gensim>","1","2","1","","","CC BY-SA 3.0"
"64174071","1","64178002","","2020-10-02 15:15:43","","1","227","<p>I've got a dataset of job postings with about 40 000 records. I extracted skills from descriptions using NER with about 30 000 skills in the dictionary. Every skill is represented as an unique identificator.</p>
<p>The distribution of skills number for a posting looks like that:</p>
<p>mean        15.12 |
std         11.22 |
min          1.00 |
25%          7.00 |
50%         13.00 |
75%         20.00 |</p>
<p>I've trained a word2vec model using only skill ids and it works more or less fine. I can find most similar skills to a given one and the result looks okay.</p>
<p>But when it comes to a doc2vec model I'm not satisfied with the result.</p>
<p>I have about 3200 unique job titles, most of them have only few entries and there are quite a few of them being from the same field ('front end developer', 'senior javascript developer', 'front end engineer'). I delibirately went for a variety of job titles which I use as tags in doc2vec.TaggedDocument(). My goal is to see a number of relevant job titles when I input a vector of skills into docvecs.most_similar().</p>
<p>After training a model (I've tried different number of epochs (100,500,1000) and vector sizes (40 and 100)) sometimes it works correctly, but most of the time it doens't. For example for a skills set like [numpy, postgresql, pandas, xgboost, python, pytorch] I get the most similar job title with a skill set like [family court, acting, advising, social work].</p>
<p>Can it be a problem with the size of my dataset? Or the size of docs (I consider that I have short texts)? I also think that I misunderstand something about doc2vec mechanism and just ignore it. I'd also like to ask if you know any other, maybe more advanced, ideas how I can get relevant job titles from a skill set and compare two skill set vectors if they are close or far.</p>
<p>UPD:</p>
<p>Job titles from my data are 'tags' and skills are 'words'. Each text has a single tag. There are 40 000 documents with 3200 repeating tags. 7881 unique skill ids appear in the documents. The average number of skill words per document is 15.</p>
<p>My data example:</p>
<pre><code>         job_titles                                             skills
1  business manager                 12 13 873 4811 482 2384 48 293 48
2    java developer      48 2838 291 37 484 192 92 485 17 23 299 23...
3    data scientist      383 48 587 475 2394 5716 293 585 1923 494 3
</code></pre>
<p>The example of my code:</p>
<pre><code>def tagged_document(df):
    #tagging documents
    for index, row in df.iterrows():
        yield gensim.models.doc2vec.TaggedDocument(row['skills'].split(), [row['job_title']])


data_for_training = list(tagged_document(job_data[['job_titles', 'skills']])

model_d2v = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)

model_d2v.train(data_for_training, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)

#the skill set contains close skills which represent a front end developer
skillset_ids = '12 34 556 453 1934'.split()                                                  
new_vector = model_d2v.infer_vector(skillset_ids, epochs=100)
model_d2v.docvecs.most_similar(positive=[new_vector], topn=30)
</code></pre>
<p>I've been experimenting recently and noticed that it performs a little better if I filter out documents with less than 10 skills. Still, there are some irrelevant job titles coming out.</p>
","11896174","","11896174","","2020-10-03 12:01:29","2020-10-03 12:01:29","Doc2Vec most similar vectors don't match an input vector","<python><nlp><gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"55086734","1","","","2019-03-10 10:31:00","","0","1239","<p>I have a large txt file(150MG) like this</p>

<pre><code>'intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one', 'better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', ...
</code></pre>

<p>I  wanna train word2vec model  model using that file but it gives me RAM problem.i dont know how to feed txt file to word2vec model.this is my code.i know that my code has problem but i don't know where is it.</p>

<pre><code>import gensim 


f = open('your_file1.txt')
for line in f:
    b=line
   model = gensim.models.Word2Vec([b],min_count=1,size=32)

w1 = ""bad""
model.wv.most_similar (positive=w1)
</code></pre>
","","user10702710","","user10702710","2019-03-10 10:37:09","2019-03-10 19:00:33","train Gensim word2vec using large txt file","<python-3.x><gensim><word2vec>","1","6","","","","CC BY-SA 4.0"
"64151977","1","64159869","","2020-10-01 08:52:41","","0","124","<p>Questions like <a href=""https://stackoverflow.com/questions/37190989/how-to-get-vocabulary-word-count-from-gensim-word2vec"">1</a> and <a href=""https://stackoverflow.com/questions/55657062/how-can-i-count-word-frequencies-in-word2vecs-training-model"">2</a> give answers for retrieving vocabulary frequencies from gensim word2vec models.</p>
<p>For some reason, they actually just give a deprecating counter from n (size of vocab) to 0, alongside the most frequent tokens, ordered.</p>
<p>For example:</p>
<pre><code>for idx, w in enumerate(model.vocab):
    print(idx, w, model.vocab[w].count)
</code></pre>
<p>Gives:</p>
<pre><code>0 &lt;/s&gt; 111051
1 . 111050
2 , 111049
3 the 111048
4 of 111047
...
111050 tokiwa 2
111051 muzorewa 1
</code></pre>
<p>Why is it doing this? How can I extract term frequencies from the model, given a word?</p>
","2205969","","","","","2020-10-01 17:23:33","Extract token frequencies from gensim model","<python><gensim>","1","1","","","","CC BY-SA 4.0"
"64187447","1","","","2020-10-03 18:10:53","","0","667","<p>For a list of words I want to get their fasttext vectors and save them to a file in the same &quot;word2vec&quot; .txt format (word+space+vector in txt format).</p>
<p>This is what I did:</p>
<pre><code>dict = open(&quot;word_list.txt&quot;,&quot;r&quot;) #the list of words I have

path = &quot;cc.en.300.bin&quot; 

model = load_facebook_model(path)

vectors = []

words =[] 

for word in dict: 
    vectors.append(model[word])
    words.append(word)

vectors_array = np.array(vectors)


</code></pre>
<p>*I want to take the list &quot;words&quot; and nd.array &quot;vectors_array&quot; and save in the original .txt format.</p>
<p>I try to use the function from gensim &quot;_save_word2vec_format&quot;:</p>
<pre><code>def _save_word2vec_format(fname, vocab, vectors, fvocab=None, binary=False, total_vec=None):
    &quot;&quot;&quot;Store the input-hidden weight matrix in the same format used by the original
    C word2vec-tool, for compatibility.
    Parameters
    ----------
    fname : str
        The file path used to save the vectors in.
    vocab : dict
        The vocabulary of words.
    vectors : numpy.array
        The vectors to be stored.
    fvocab : str, optional
        File path used to save the vocabulary.
    binary : bool, optional
        If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.
    total_vec : int, optional
        Explicitly specify total number of vectors
        (in case word vectors are appended with document vectors afterwards).
    &quot;&quot;&quot;
    if not (vocab or vectors):
        raise RuntimeError(&quot;no input&quot;)
    if total_vec is None:
        total_vec = len(vocab)
    vector_size = vectors.shape[1]
    if fvocab is not None:
        logger.info(&quot;storing vocabulary in %s&quot;, fvocab)
        with utils.open(fvocab, 'wb') as vout:
            for word, vocab_ in sorted(iteritems(vocab), key=lambda item: -item[1].count):
                vout.write(utils.to_utf8(&quot;%s %s\n&quot; % (word, vocab_.count)))
    logger.info(&quot;storing %sx%s projection weights into %s&quot;, total_vec, vector_size, fname)
    assert (len(vocab), vector_size) == vectors.shape
    with utils.open(fname, 'wb') as fout:
        fout.write(utils.to_utf8(&quot;%s %s\n&quot; % (total_vec, vector_size)))
        # store in sorted order: most frequent words at the top
        for word, vocab_ in sorted(iteritems(vocab), key=lambda item: -item[1].count):
            row = vectors[vocab_.index]
            if binary:
                row = row.astype(REAL)
                fout.write(utils.to_utf8(word) + b&quot; &quot; + row.tostring())
            else:
                fout.write(utils.to_utf8(&quot;%s %s\n&quot; % (word, ' '.join(repr(val) for val in row))))
</code></pre>
<p>but I get the error:</p>
<pre><code>INFO:gensim.models._fasttext_bin:loading 2000000 words for fastText model from cc.en.300.bin
INFO:gensim.models.word2vec:resetting layer weights
INFO:gensim.models.word2vec:Updating model with new vocabulary
INFO:gensim.models.word2vec:New added 2000000 unique words (50% of original 4000000) and increased the count of 2000000 pre-existing words (50% of original 4000000)
INFO:gensim.models.word2vec:deleting the raw counts dictionary of 2000000 items
INFO:gensim.models.word2vec:sample=1e-05 downsamples 6996 most-common words
INFO:gensim.models.word2vec:downsampling leaves estimated 390315457935 word corpus (70.7% of prior 552001338161)
INFO:gensim.models.fasttext:loaded (4000000, 300) weight matrix for fastText model from cc.en.300.bin
trials.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).
  vectors.append(model[word])
INFO:__main__:storing 8664x300 projection weights into arrays_to_txt_oct3.txt
loading the model for: en
finish loading the model for: en
len(vectors): 8664
len(words):  8664
shape of vectors_array (8664, 300)
mission launched!
Traceback (most recent call last):
  File &quot;trials.py&quot;, line 102, in &lt;module&gt;
    _save_word2vec_format(YOUR_VEC_FILE_PATH, words, vectors_array, fvocab=None, binary=False, total_vec=None)
  File &quot;trials.py&quot;, line 89, in _save_word2vec_format
    for word, vocab_ in sorted(iteritems(vocab), key=lambda item: -item[1].count):
  File &quot;/cs/snapless/oabend/tailin/transdiv/lib/python3.7/site-packages/six.py&quot;, line 589, in iteritems
    return iter(d.items(**kw))
AttributeError: 'list' object has no attribute 'items'
</code></pre>
<p>I understand that it has to do with the second argument in the function, but I don't understand how should I make a list of words into a dictionary object?</p>
<p>I tried doing that with:</p>
<pre><code>#convert list of words into a dictionary
words_dict = {i:x for i,x in enumerate(words)}
</code></pre>
<p>But still got the error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;trials.py&quot;, line 99, in &lt;module&gt;
    _save_word2vec_format(YOUR_VEC_FILE_PATH, dict, vectors_array, fvocab=None, binary=False, total_vec=None)
  File &quot;trials.py&quot;, line 77, in _save_word2vec_format
    total_vec = len(vocab)
TypeError: object of type '_io.TextIOWrapper' has no len()
</code></pre>
<p>I don't understand how to insert the word list in the right format...</p>
","14385885","","","","","2020-10-04 17:55:08","problem saving pre-trained fasttext vectors in ""word2vec"" format with _save_word2vec_format()","<nlp><gensim><word2vec><word-embedding><fasttext>","1","2","","","","CC BY-SA 4.0"
"64540488","1","64541198","","2020-10-26 15:48:02","","0","215","<p>I am training a word2vec model, using about 700 text files as my corpus. But, when I start reading the files after the preprocessing step, I get the mentioned error. The code is as follows</p>
<pre><code>class MyCorpus(object):
    def __iter__(self):
        for i in ceo_path:                              /// ceo_path contains abs path of all text files
            file = open(i, 'r', encoding='utf-8')
            text = file.read()

            ###########                                        
            ###########                                 /// text preprocessing steps
            ###########
            
            yield final_text                            /// returns preprocessed text


sentences = MyCorpus()
logging.basicConfig(format=&quot;%(levelname)s - %(asctime)s: %(message)s&quot;, datefmt= '%H:%M:%S', level=logging.INFO)

# training the model
cores = multiprocessing.cpu_count()
w2v_model = Word2Vec(min_count=5,
                     iter=30,
                     window=3,
                     size=200,
                     sample=6e-5,
                     alpha=0.025,
                     min_alpha=0.0001,
                     negative=20,
                     workers=cores-1,
                     sg=1)
w2v_model.build_vocab(sentences)
w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)
w2v_model.save('ceo1.model')
</code></pre>
<p>The error that I am getting is:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:/Users/name/PycharmProjects/prac2/hbs_word2vec.py&quot;, line 131, in &lt;module&gt;
    w2v_model.build_vocab(sentences)
  File &quot;C:\Users\name\PycharmProjects\prac1\venv\lib\site-packages\gensim\models\base_any2vec.py&quot;, line 921, in build_vocab
    total_words, corpus_count = self.vocabulary.scan_vocab(
  File &quot;C:\Users\name\PycharmProjects\prac1\venv\lib\site-packages\gensim\models\word2vec.py&quot;, line 1403, in scan_vocab
    total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)
  File &quot;C:\Users\name\PycharmProjects\prac1\venv\lib\site-packages\gensim\models\word2vec.py&quot;, line 1372, in _scan_vocab
    for sentence_no, sentence in enumerate(sentences):
  File &quot;C:/Users/name/PycharmProjects/prac2/hbs_word2vec.py&quot;, line 65, in __iter__
    text = file.read()
  File &quot;C:\Users\name\AppData\Local\Programs\Python\Python38-32\lib\codecs.py&quot;, line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
<p>I am not able to understand the error as I am new to this. I was not getting the error in reading the text files when I wasn't using the <strong>iter</strong> function and sending the data in chunks as I am doing currently.</p>
","13490239","","","","","2020-10-26 16:30:14","UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte while reading a text file","<python-3.x><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"20362993","1","20363116","","2013-12-03 22:25:56","","16","11402","<p>I am trying to use the <a href=""http://radimrehurek.com/gensim/models/word2vec.html""><code>word2vec</code></a> module from <code>gensim</code> natural language processing library in Python.</p>

<p>The docs say to initialize the model:</p>

<pre class=""lang-python prettyprint-override""><code>from gensim.models import word2vec
model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>What format does <code>gensim</code> expect for the input sentences?  I have raw text</p>

<pre><code>""the quick brown fox jumps over the lazy dogs""
""Then a cop quizzed Mick Jagger's ex-wives briefly.""
etc.
</code></pre>

<p>What additional processing do I need to post into <code>word2fec</code>?</p>

<hr>

<p><strong>UPDATE:</strong> Here is what I have tried.  When it loads the sentences, I get nothing.</p>

<pre><code>&gt;&gt;&gt; sentences = ['the quick brown fox jumps over the lazy dogs',
             ""Then a cop quizzed Mick Jagger's ex-wives briefly.""]
&gt;&gt;&gt; x = word2vec.Word2Vec()
&gt;&gt;&gt; x.build_vocab([s.encode('utf-8').split( ) for s in sentences])
&gt;&gt;&gt; x.vocab
{}
</code></pre>
","737051","","737051","","2013-12-03 23:33:41","2017-03-31 09:18:06","How to load sentences into Python gensim?","<python><nlp><gensim>","2","0","6","","","CC BY-SA 3.0"
"55923298","1","55941693","","2019-04-30 14:37:51","","1","41","<p>I'm a newbie to <code>gensim</code> and trying to understand the <code>Word2Vec</code> model it generates.</p>

<p>Here is a simple example:- </p>

<pre><code>sentences = [['first', 'sentence', 'for', 'word2vec']]
model = Word2Vec(sentences, min_count=1)
print(model)
print(model['first'])
</code></pre>

<p>Output:- </p>

<pre><code>Word2Vec(vocab=4, size=100, alpha=0.025)

[-3.2170122e-03 -2.9626938e-03 -4.0412871e-03 -5.9279817e-04
  2.5436375e-03  4.5433347e-03 -3.3862963e-03 -4.2654946e-03
  3.8285875e-03  4.3016393e-03  2.3948429e-03  8.1989179e-05
  3.6110645e-03  1.8498371e-03 -2.4455690e-04  4.1978257e-03
  2.9471173e-04  4.9666679e-03 -2.0676558e-03 -1.2046038e-03
 -4.3298928e-03  2.7839688e-03 -2.9434622e-03  4.0511941e-03
 -1.3770841e-03 -8.9504482e-04 -3.1494466e-03 -4.6084630e-03
 -3.3623597e-03  1.6870942e-04 -7.1172835e-04 -4.1482532e-03
  3.7355758e-03  2.3343530e-03 -6.3678029e-04 -1.9861995e-03
 -2.3025211e-03  1.5102652e-03 -2.8942723e-03 -3.0406206e-03
 -7.7123288e-04 -2.1534185e-03  4.0353332e-03 -2.0982060e-03
 -5.1215116e-04 -4.9524521e-03  3.9109741e-03  3.6507500e-03
  5.0717179e-04 -1.2909769e-03  1.7484331e-03  1.8906737e-03
 -2.5824555e-03 -3.3213641e-03  1.3024095e-03  4.8507750e-03
  3.5359471e-03  4.5252368e-03  2.1690773e-03  3.8934432e-03
  4.8941034e-03 -4.3265051e-03  1.2478753e-03  4.8012529e-03
  3.6689214e-04 -3.5324714e-03 -8.2519173e-04  4.6989080e-03
 -4.3403171e-03 -3.2295308e-03 -4.3292320e-03  1.4541810e-03
  2.6360361e-03  4.7351457e-03 -1.1666205e-03  4.0232311e-03
  2.3259546e-03 -4.5906431e-03 -2.3466926e-03 -1.4690498e-03
  4.9304329e-03  3.4869314e-04  1.7118681e-03 -3.9177295e-03
 -1.9519962e-03  4.0137409e-03  1.6459676e-03 -2.6613632e-03
 -3.4537977e-03  1.0973522e-03  1.9739978e-03  4.3450715e-03
  2.8814776e-03 -4.9455655e-03 -1.4207339e-03 -2.8513866e-03
 -3.7962969e-03 -2.7314643e-03 -6.0791872e-04 -5.9866998e-04]
</code></pre>

<p>The size of the model is defaulted to 100, what does each item in the size array represent?</p>

<p>For example:- first element is  <code>-3.2170122e-03</code></p>
","1050619","","6347629","","2019-05-01 19:36:36","2019-05-01 19:36:36","Understanding gensim model inference output","<nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"55939511","1","55941468","","2019-05-01 16:37:26","","0","179","<p>I trained a gensim's Doc2Vec model with default word2vec training (dm=1). I can get the word vectors from the global model in model.wv.vectors.
But the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">documentation</a> says that the same word (""leaves"" in the example) won't have the same vector depending of the document context where it appear.</p>

<p>So I'm a bit confused : in the model.wv.vectors, will the word ""leaves"" by example, have the same vector for all the documents used to train the model (that may be contradictory with what I understand from the documentation) ? If not, how to get the word vectors from a particular document ?</p>
","11133272","","","","","2019-05-01 19:17:18","Word vectors from a whole doc2vec model vs. word vectors from a particular document","<gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"38507935","1","38535846","","2016-07-21 15:09:39","","0","1260","<p>I have trained a GloVe with ~5M <strong>spanish</strong> articles. I know how to load this GloVe in gensim and use it as if it was a word2vec model.
Now I am facing  the problem of topic modelling and keywords extraction from news articles (also in spanish) so I was wondering how could I use the trained model to do so.</p>

<p>How could I do it?</p>
","1309231","","","","","2016-07-22 22:00:11","How can I use a trained GloVe/word2vec model to extract keywords from articles?","<nlp><gensim><word2vec>","1","0","","","","CC BY-SA 3.0"
"21440132","1","21467032","","2014-01-29 18:57:53","","2","610","<p>I am learning Latent semantic analysis (LSA) and I am able to construct term-document matrix and find its SVD decomposition. How can I get the topics from that decomposition?</p>

<p>For example, in gensim:</p>

<pre><code>topic #0(332.762): 0.425*""utc"" + 0.299*""talk"" + 0.293*""page"" + 0.226*""article"" + 0.224*""delete"" + 0.216*""discussion"" + 0.205*""deletion"" + 0.198*""should"" + 0.146*""debate"" + 0.132*""be""
topic #1(201.852): 0.282*""link"" + 0.209*""he"" + 0.145*""com"" + 0.139*""his"" + -0.137*""page"" + -0.118*""delete"" + 0.114*""blacklist"" + -0.108*""deletion"" + -0.105*""discussion"" + 0.100*""diff""
topic #2(191.991): -0.565*""link"" + -0.241*""com"" + -0.238*""blacklist"" + -0.202*""diff"" + -0.193*""additions"" + -0.182*""users"" + -0.158*""coibot"" + -0.136*""user"" + 0.133*""he"" + -0.130*""resolves""
</code></pre>
","1617352","","665954","","2014-01-30 20:48:42","2014-01-31 12:45:54","Latent semantic analysis in finding topics","<algorithm><svd><gensim>","1","0","1","","","CC BY-SA 3.0"
"56167224","1","","","2019-05-16 11:12:27","","0","122","<p>I really accept every hint on the following problem, because all what i want is to obtain that embedding from that dataset, I will write my all solution because (hopefully) the problem is just in some parts that i didn't consider.</p>
<p>I'm working with an annotated corpus, such that i have disambiguate words in a given sentence thanks to WordNet synsets id, that i will call tags. For example:</p>
<h3>Dataset</h3>
<pre class=""lang-xml prettyprint-override""><code>&lt;sentence&gt;
  &lt;text&gt;word1 word2 word3&lt;/text&gt;
  &lt;annotations&gt;
    &lt;annotation anchor=word1 lemma=lemma1&gt;tag1&lt;/annotation&gt;
    &lt;annotation anchor=word2 lemma=lemma2&gt;tag2&lt;/annotation&gt;
    &lt;annotation anchor=word3 lemma=lemma3&gt;tag3&lt;/annotation&gt;
  &lt;annotations&gt;
&lt;/sentence&gt;
</code></pre>
<p>Starting from this, given an embedding dimension that i will call n, i would like to build an embedding like this:</p>
<h3>Embedding</h3>
<pre><code>lemma1_tag1 dim 1 dim 2 dim 3 ... dim n
lemma2_tag2 dim 1 dim 2 dim 3 ... dim n
lemma3_tag3 dim 1 dim 2 dim 3 ... dim n
</code></pre>
<p>I thought to generate a corpus for Word2Vec starting from each text of each sentence, and replace each <code>anchor</code> with the respective <code>lemma1_tag1</code> (some words can contain more underscore, because i replaced space in lemmas with underscores). Since not every single word is annotated, after a simple preprocessing performed to remove stopwords and other punctuation, in the end i have something like the following example:</p>
<h3>Corpus Example</h3>
<pre><code>let just list most_recent_01730444a headline_06344461n
</code></pre>
<p>Since I'm just interested in annotated words, I also generated a predefined vocabulary to use it as Word2Vec vocabulary. This file contains on each row entries like:</p>
<h3>Vocabulary Example</h3>
<pre><code>lemma1_tag1
lemma2_tag2
</code></pre>
<p>So, after having defined a corpus and a vocabulary, I used them in Word2Vec toolkit:</p>
<h3>Terminal emulation</h3>
<pre><code>./word2vec -train data/test.txt -output data/embeddings.vec -size 300 -window 7 -sample 1e-3 -hs 1 -negative 0 -iter 10 -min-count 1 -read-vocab data/dictionary.txt -cbow 1
</code></pre>
<h3>Output</h3>
<pre><code>Starting training using file data/test.txt
Vocab size: 80
Words in train file: 20811
</code></pre>
<p>The problem is that the number of words in the corpus is 32000000+ and the number of words in the predefined vocabulary file is about 80000. I even tried in Python with Gensim, but (of course) I had the very same output. I think that the problem is that Word2Vec doesn't consider words in the format <code>lemma1_tag1</code> because of the underscore, and i don't know how to solve this problem. Any hint is appreciated, thank you in advance!</p>
","7781936","","-1","","2020-06-20 09:12:55","2019-05-16 19:36:33","Use Word2Vec to build a sense embedding","<python><gensim><word2vec><word-embedding>","1","0","","","","CC BY-SA 4.0"
"32313062","1","","","2015-08-31 13:58:08","","11","10101","<p>I am trying to obtain the optimal number of topics for an LDA-model within Gensim. One method I found is to calculate the log likelihood for each model and compare each against each other, e.g. at <a href=""https://stats.stackexchange.com/questions/25113/the-input-parameters-for-using-latent-dirichlet-allocation"">The input parameters for using latent Dirichlet allocation</a></p>

<p>Hence I looked into calculating the log likelihood of a LDA-model with Gensim and came across following post: <a href=""https://stats.stackexchange.com/questions/126268/how-do-you-estimate-alpha-parameter-of-a-latent-dirichlet-allocation-model"">How do you estimate Œ± parameter of a latent dirichlet allocation model?</a></p>

<p>which basically states that the update_alpha() method implements the method decribed in <em>Huang, Jonathan. Maximum likelihood estimation of Dirichlet distribution parameters</em>. Still I don't know how to obtain this parameter using the libary without changing the code.</p>

<p>How can I obtain log likelihood from an LDA model with Gensim?</p>

<p>Is there a better way to obtain optimal number of topics with Gensim?</p>
","5114291","","","","","2020-12-06 11:19:04","What is the best way to obtain the optimal number of topics for a LDA-Model using Gensim?","<python><text-mining><lda><gensim><topic-modeling>","2","2","3","","","CC BY-SA 3.0"
"57796091","1","","","2019-09-04 21:56:09","","0","1524","<p>As a part of the assignment, I am asked to do topic modeling using LDA and visualize the words that come under the top 3 topics as shown in the below screenshot <a href=""https://i.stack.imgur.com/e4Buq.png"" rel=""nofollow noreferrer"">1</a>. However, even after searching a lot I am not able to find any helpful resource that would help me achieve my goal. All resources about text visualization are pointed towards the word cloud, but my goal is not to use word cloud visualizations.
<a href=""https://i.stack.imgur.com/e4Buq.png"" rel=""nofollow noreferrer"">Required LDA topic visulization</a></p>
<p>Any help will be greatly appreciated.</p>
","10987027","","-1","","2020-06-20 09:12:55","2021-02-18 12:32:01","How to visualize results of LDA topic modelling as shown below","<matplotlib><plotly><seaborn><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"55084361","1","","","2019-03-10 04:13:39","","0","84","<p>Are these <code>model.wv.vectors</code> and <code>model.trainables.syn1neg</code>? And will they be similar after enough runs?</p>
","8845716","","","","","2019-03-10 06:20:59","How to access W and W' matrices in Gensim Word2Vec in negative sampling setting?","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"55924378","1","55927699","","2019-04-30 15:36:33","","0","1519","<p>I am trying to train a doc2vec model using training data, then finding the similarity of every document in the <strong>test data</strong> for a specific document in the <strong>test data</strong> using the trained doc2vec model. However, I am unable to determine how to do this.</p>

<p>I currently using <code>model.docvecs.most_similar(...)</code>. However, this function only finds the similarity of every document in the <strong>training data</strong> for a specific document in the <strong>test data</strong>. </p>

<p>I have tried manually comparing the inferred vector of a specific document in the test data with the inferred vectors of every other document in the test data using <code>model.docvecs.n_similarity(inferred_vector.tolist(), testvectors[i].tolist())</code> but this returns <code>KeyError: ""tag '-0.3502606451511383' not seen in training corpus/invalid""</code> as there are vectors not in the dictionary.</p>
","5388182","","","","","2019-04-30 19:40:10","Doc2Vec - Finding document similarity in test data","<python><machine-learning><gensim><doc2vec>","2","0","","","","CC BY-SA 4.0"
"56170250","1","","","2019-05-16 13:55:25","","0","169","<p>I'm on Windows OS10, using python 2.7.15 | Anaconda. Whenever I run </p>

<pre><code>mymodel=gensim.models.Word2Vec.load (pretrain)
mymodel.min_count = mincount
sentences =gensim.models.word2vec.LineSentence('ontology_corpus.lst')
mymodel.build_vocab(sentences, update=True) # ERROR HERE ****
</code></pre>

<p>I get this error:</p>

<pre><code>Traceback (most recent call last):
  File ""runWord2Vec.py"", line 23, in &lt;module&gt;
    mymodel.build_vocab(sentences, update=True)
  File ""C:xxxx\lib\site-packages\gensim\models\ba
se_any2vec.py"", line 936, in build_vocab
    sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, tri
m_rule=trim_rule)
  File ""C:xxxx\lib\site-packages\gensim\models\wo
rd2vec.py"", line 1591, in scan_vocab
    total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_r
ule)
  File ""C:xxxxx\lib\site-packages\gensim\models\wo
rd2vec.py"", line 1560, in _scan_vocab
    for sentence_no, sentence in enumerate(sentences):
  File ""C:xxxx\lib\site-packages\gensim\models\wo
rd2vec.py"", line 1442, in __iter__
    line = utils.to_unicode(line).split()
  File ""C:xxxx\lib\site-packages\gensim\utils.py""
, line 359, in any2unicode
    return unicode(text, encoding, errors=errors)
  File ""C:xxxxx\lib\encodings\utf_8.py"", line 16,
in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0xe6 in position 124: invalid
 continuation byte
</code></pre>

<p>Now this traces back to this LineSentence Class</p>

<pre><code>class LineSentence(object):

def __init__(self, source, max_sentence_length=MAX_WORDS_IN_BATCH, limit=None):

    self.source = source
    self.max_sentence_length = max_sentence_length
    self.limit = limit

def __iter__(self):
    """"""Iterate through the lines in the source.""""""
    try:
        # Assume it is a file-like object and try treating it as such
        # Things that don't have seek will trigger an exception
        self.source.seek(0)
        for line in itertools.islice(self.source, self.limit):
            line = utils.to_unicode(line).split()
            i = 0
            while i &lt; len(line):
                yield line[i: i + self.max_sentence_length]
                i += self.max_sentence_length
    except AttributeError:
        # If it didn't work like a file, use it as a string filename
        with utils.smart_open(self.source) as fin:
            for line in itertools.islice(fin, self.limit):
                line = utils.to_unicode(line).split() # ERROR HERE *************
                i = 0
                while i &lt; len(line):
                    yield line[i: i + self.max_sentence_length]
                    i += self.max_sentence_length
</code></pre>

<p>In the last return that can be seen from the error, I can just change the error parameter to be error='ignore' or change this line:</p>

<pre><code> utils.to_unicode(line).split()
</code></pre>

<p>to this:</p>

<pre><code> line.split()
</code></pre>

<p>ontology_corpus.lst file sample: </p>

<pre><code>&lt;http://purl.obolibrary.org/obo/GO_0090141&gt; EquivalentTo &lt;http://purl.obolibrary.org/obo/GO_0065007&gt; and  &lt;http://purl.obolibrary.org/obo/RO_0002213&gt; some &lt;http://purl.obolibrary.org/obo/GO_0000266&gt; 
&lt;http://purl.obolibrary.org/obo/GO_0090141&gt; SubClassOf &lt;http://purl.obolibrary.org/obo/GO_0065007&gt;
</code></pre>

<p>The problem is that it's working but I'm afraid that the results will be flawed due to the encoding error ignored! Is there a solution to this or would my approach will be just fine?</p>
","7409519","","7409519","","2019-05-16 15:28:48","2019-05-16 18:18:23","Gensim sentences from ontology corpus Unicode error","<python><unicode><gensim><word2vec>","1","6","1","","","CC BY-SA 4.0"
"46536132","1","46538289","","2017-10-03 01:58:24","","8","4935","<p>I built LDA model using Gensim and I want to get the topic words only How can I get the words of the topics only no probabilities and no IDs.words only </p>

<p>I tried print_topics() and show_topics() functions in gensim but I can't get clean words ! </p>

<p>This is the code I used</p>

<pre><code>dictionary = corpora.Dictionary(doc_clean)
doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(doc_term_matrix, num_topics=12, id2word = dictionary, passes = 100, alpha='auto', update_every=5)
x = ldamodel.print_topics(num_topics=12, num_words=5)
for i in x:
    print(i[1])
    #print('\n' + str(i))

0.045*ÿ™ÿπÿ±ÿ∂ + 0.045*ÿßŸÑŸÖÿßÿ∂Ÿäÿ© + 0.045*ÿßŸÑÿ≥ŸÜŸàÿßÿ™ + 0.045*Ÿàÿ°ÿ≥ÿ±ÿ™Ÿá + 0.045*ŸÑÿ°ÿ≠ŸÖÿØ
0.021*ŸÖÿµÿ± + 0.021*ÿßŸÑÿØŸäŸÖŸÇÿ±ÿßÿ∑Ÿäÿ© + 0.021*ÿ≠ÿ±Ÿäÿ© + 0.021*ÿ®ÿßÿ≥ŸÖ + 0.021*ÿßŸÑÿ≠ŸÉŸàŸÖÿ©
0.068*ÿßŸÑŸÖŸàÿßÿ∑ŸÜÿ© + 0.068*ÿßŸÑÿ∑ÿßÿ°ŸÅŸäÿ© + 0.068*ŸàÿßŸÜŸáŸäÿßÿ±ÿßÿ™ + 0.068*ÿ±ÿßÿ®ÿ∑ÿ© + 0.005*ÿ∑ÿ®ŸàŸÑ
0.033*ÿπÿ±ÿ®Ÿäÿ© + 0.033*ÿßŸÜŸÉÿ≥ÿßÿ±ÿßÿ™ + 0.033*ÿ±Ÿáÿßÿ®ŸäŸäŸÜ + 0.033*ÿ®ÿ≠ŸÇŸàŸÇ + 0.033*ŸÑ
0.007*Ÿàÿ≠ÿ±Ÿäÿßÿ™ + 0.007*ŸÖŸÖŸÜŸáÿ¨ + 0.007*ŸÇŸàÿßÿ°ŸÖ + 0.007*ÿßŸÑŸÜÿßÿ≥ + 0.007*ÿØÿ±ÿßÿ¨
0.116*ÿ∑ÿ®ŸàŸÑ + 0.116*ÿßŸÑŸàÿ∑ŸÜŸäÿ© + 0.060*ŸäŸÉÿ™ÿ® + 0.060*ŸÖÿµÿ± + 0.005*ÿπÿ±ÿ®Ÿäÿ©
0.064*ŸÇŸäŸÖ + 0.064*ŸàŸáŸÜ + 0.064*ÿπÿ±ÿ®Ÿäÿß + 0.064*ŸàÿßŸÑÿ™ÿπÿØÿØŸäÿ© + 0.064*ÿßŸÑÿØŸäŸÖŸÇÿ±ÿßÿ∑Ÿäÿ©
0.036*ÿ™ÿ∂ÿßŸÖŸÜÿß + 0.036*ÿßŸÑÿ¥ÿÆÿµŸäÿ© + 0.036*ŸÖÿπ + 0.036*ÿßŸÑÿ™ŸÅÿ™Ÿäÿ¥ + 0.036*ÿßŸÑÿ°ÿÆŸÑÿßŸÇ
0.052*ÿ™ÿ∂ÿßŸÖŸÜÿß + 0.052*ŸÉŸÑ + 0.052*ŸÖÿ≠ŸÖÿØ + 0.052*ÿßŸÑÿÆŸÑŸàŸÇ + 0.052*ŸÖÿ∏ŸÑŸàŸÖ
0.034*ÿ®ŸÖŸàÿßÿ∑ŸÜŸäŸÜ + 0.034*ÿ±Ÿáÿßÿ®Ÿäÿ© + 0.034*ŸÑŸÖ + 0.034*ÿπŸÑŸäŸáŸÖ + 0.034*Ÿäÿ´ÿ®ÿ™
0.035*ŸÖÿπ + 0.035*ŸàŸÖÿ≥ÿ™ÿ¥ÿßÿ± + 0.035*Ÿäÿ≥ÿ™ÿπŸäÿØÿß + 0.035*ÿ°ÿ±ŸáŸÇŸáŸÖÿß + 0.035*ÿ≠ÿ±Ÿäÿ™ŸáŸÖÿß
0.064*ŸÑŸÑŸÇŸÖÿπ + 0.064*ŸÇÿ±Ÿäÿ®ÿ© + 0.064*ŸÑÿß + 0.064*ŸÜŸáÿßŸäÿ© + 0.064*ŸÖÿµÿ±
</code></pre>

<p>I tried show_topics and it gave the same output</p>

<pre><code>y = np.array(ldamodel.show_topics(num_topics=12, num_words=5))
for i in y[:,1]:
    #if i != '%d':
    #print([str(word) for word in i])
    print(i)
</code></pre>

<p>If I have the topic ID how can I access its words and other informations </p>

<p>Thanks in Advance</p>
","4676565","","","","","2021-07-28 16:56:36","How to access topic words only in gensim","<python><nlp><gensim><lda><topic-modeling>","4","0","4","","","CC BY-SA 3.0"
"64702780","1","","","2020-11-05 17:57:31","","0","34","<p>I've trained an <code>ldaseqmodel</code>, and now I'm trying to get the topic distribution for each time_slice so I can see whether the distributions changed over time.</p>
<p>How do I get this output?</p>
<p><strong>Desired Output</strong></p>
<p>A TSV:</p>
<pre><code>Year1 prop_topic1 prop_topic2 ‚Ä¶ prop_topic_20
Year2 prop_topic1 prop_topic2 ‚Ä¶ prop_topic_20
</code></pre>
<p>Is there an existing function that I'm missing for generating this?</p>
<p>LdaSeqModel docs: <a href=""https://radimrehurek.com/gensim/models/ldaseqmodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldaseqmodel.html</a></p>
","579397","","579397","","2020-11-05 18:09:09","2020-11-05 18:09:09","Topic distributions per time_slice in ldaseqmodel","<python><gensim><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"56367510","1","","","2019-05-29 19:45:41","","1","939","<p>I'm doing arabic dialect text classification and I've used Word2Vec to train the model, I got this so far:</p>

<pre><code> def read_input(input_file):

    with open (input_file, 'rb') as f:
        for i, line in enumerate (f): 
            yield gensim.utils.simple_preprocess (line)

documents = list (read_input (data_file))
logging.info (""Done reading data file"")

model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)
model.train(documents,total_examples=len(documents),epochs=10)
</code></pre>

<p>What do I do now to predict a new text if it's of any of the 5 dialects I have?<br>
Also, I looked around and found this code: </p>

<pre><code># load the pre-trained word-embedding vectors 
embeddings_index = {}
for i, line in enumerate(open('w2vmodel.vec',encoding='utf-8')):
    values = line.split()
    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')

# create a tokenizer 
token = text.Tokenizer()
token.fit_on_texts(trainDF['text'])
word_index = token.word_index

# convert text to sequence of tokens and pad them to ensure equal length vectors 
train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)
valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)

# create token-embedding mapping
embedding_matrix = numpy.zeros((len(word_index) + 1, 300))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
</code></pre>

<p>But it gives me this error when I run it and load my trained word2vec model:</p>

<pre><code>ValueError: could not convert string to float: '\x00\x00\x00callbacksq\x04)X\x04\x00\x00\x00loadq\x05cgensim.utils'
</code></pre>

<h2>Note:</h2>

<p>Actually, there's another code that I didn't post here, I wanted to use word2vec with neural networks, I have the code for neural network, but I don't know how to make the features I got from word2vec to be as an input to the neural net and with labels as output. Is it possible to connect word2vec to a deep neural net and how?</p>
","4227564","","4227564","","2019-05-31 23:20:34","2019-05-31 23:20:34","How to predict with Word2Vec?","<python><gensim><word2vec><text-classification>","1","4","","","","CC BY-SA 4.0"
"55774197","1","55779141","","2019-04-20 13:54:20","","1","303","<p>I'm facing a Gensim training problem using Word2Vec. 
model.wv.vocab is not getting any further word from the trained corpus 
the only words in are from the ones from initialization instruction ! </p>

<p>In fact, after many times trying on my own code, even the official site's example didn't work !  </p>

<p>I tried saving model at many spots in my code 
I even tried saving and reloading the corpus alongside train instruction</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec

path = get_tmpfile(""word2vec.model"")

model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
model.save(""word2vec.model"")

print(len(model.wv.vocab))

model.train([[""hello"", ""world""]], total_examples=1, epochs=1)
model.save(""word2vec.model"")

print(len(model.wv.vocab))

</code></pre>

<p>first print statement gives 12 which is right </p>

<p>second 12 when it's supposed to give 14 (len(vocab + 'hello' + 'world'))</p>
","8815856","","","","","2019-04-21 01:27:02","Gensim's Word2Vec not training provided documents","<python-3.x><gensim><google-colaboratory>","1","0","","","","CC BY-SA 4.0"
"64695470","1","","","2020-11-05 10:39:38","","0","182","<p>I am trying to install gensim with the command :</p>
<pre><code>conda install -c anaconda gensim
</code></pre>
<p>but I am gating the following:
UnsatisfiableError: The following specifications were found
to be incompatible with the existing python installation in your environment:</p>
<p>Specifications:</p>
<ul>
<li>gensim -&gt; python[version='&gt;=2.7,&lt;2.8.0a0|&gt;=3.6,&lt;3.7.0a0|&gt;=3.7,&lt;3.8.0a0|&gt;=3.5,&lt;3.6.0a0']</li>
</ul>
<p>Your python: python=3.8</p>
<p>If python is on the left-most side of the chain, that's the version you've asked for.
When python appears to the right, that indicates that the thing on the left is somehow
not available for the python version you are constrained to. Note that conda will not
change your python version to a different minor version unless you explicitly specify
that.
my environment is shown in the pic :
<a href=""https://i.stack.imgur.com/VgvDz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VgvDz.png"" alt=""enter image description here"" /></a></p>
","14581466","","","","","2020-11-05 10:39:38","can't install gensim with anaconda","<python><anaconda><gensim>","0","2","","","","CC BY-SA 4.0"
"55936182","1","","","2019-05-01 12:19:34","","0","686","<p>I have installed the gensim module for MAC by passing the following command in my Terminal:</p>

<pre><code>pip3 install gensim
</code></pre>

<p>I already have many other modules such as pandas and numpy that have been installed but I am able to import the same to my Jupyter Notebook without any issues. </p>

<p>This is how I am importing gensim:</p>

<pre><code>from gensim.models import Word2Vec
from gensim.models import KeyedVectors
</code></pre>

<p>So I checked the path where the 2 modules have been installed through the terminal, these being the same for a module I am able to import such as pandas as well as for gensim. </p>

<pre><code>pip3 show pandas
pip3 show gensim
</code></pre>

<p>In both the cases I get the same output: </p>

<blockquote>
  <p>/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages    </p>
</blockquote>

<p>Can anyone tell me what could be the issue in this case?</p>
","4738168","","8873143","","2019-05-01 12:28:34","2019-05-01 12:28:34","Why am I unable to import the gensim module that has definitely been installed?","<python><python-3.x><machine-learning><jupyter-notebook><gensim>","0","2","","","","CC BY-SA 4.0"
"47859739","1","","","2017-12-17 21:40:27","","0","648","<p>I am using <a href=""https://radimrehurek.com/gensim/index.html"" rel=""nofollow noreferrer"">gensim</a> to analyze document similarity in a large corpus. Each document has a ""title"", or more specifically, a unique ID string, along with the content text.</p>

<p>After looking through several <a href=""https://radimrehurek.com/gensim/tut3.html"" rel=""nofollow noreferrer"">tutorials</a> about <a href=""https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"" rel=""nofollow noreferrer"">top modeling</a>, <a href=""https://radimrehurek.com/topic_modeling_tutorial/3%20-%20Indexing%20and%20Retrieval.html"" rel=""nofollow noreferrer"">indexing and retrieval</a>, and Wikipedia, what is still not clear to me is how to get interpretable results getting building the LSI model, and querying the index for some search vector. After I see the top N most similar document indexes and their similarity scores, how do I lookup the <strong>titles</strong> of those documents?</p>

<p>For example, in this <a href=""https://radimrehurek.com/topic_modeling_tutorial/3%20-%20Indexing%20and%20Retrieval.html"" rel=""nofollow noreferrer"">code</a>:</p>

<pre><code>index.num_best = 10
print(index[query_lsi])
INFO:gensim.utils:loading MatrixSimilarity object from ./data/wiki_index.0
INFO:gensim.utils:loading MatrixSimilarity object from ./data/wiki_index.1
INFO:gensim.utils:loading MatrixSimilarity object from ./data/wiki_index.2

[(4028, 0.82495784759521484), (52384, 0.82495784759521484), (13582, 0.8166358470916748), (61938, 0.8166358470916748), (0, 0.80658835172653198), (48356, 0.80658835172653198), (85, 0.8048851490020752), (48441, 0.8048851490020752), (115, 0.79446637630462646), (48471, 0.79446637630462646)]
</code></pre>

<p>How would I lookup the title of, for example, document #61938 that came back in the most similar results?</p>

<p>In the <a href=""https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"" rel=""nofollow noreferrer"">previous part to that tutorial</a>, the <code>iter_wiki()</code> function yielded a tuple of the (title, tokens). That <code>title</code> is what I want.</p>
","2491761","","","","","2017-12-19 16:25:28","gensim document similarity: how to get document titles from most similar results?","<python><nlp><similarity><gensim><lsa>","1","0","","","","CC BY-SA 3.0"
"48051767","1","","","2018-01-01 17:30:03","","2","690","<p>I am trying to built a predictive model using address data. I have transformed the address data into a bigram model using <strong>Gensim Phrases</strong>  but I am facing  issues while transforming the address data into the corresponding bigrams and attaching as a separate column which can be further used for countvectorization.</p>

<p>My code</p>

<p><strong>Gensim Bigram Phrases model</strong></p>

<pre><code>from gensim.models import Phrases
adj_addr=data['Clean_addr'].values.tolist()
sentence_stream = [doc.split("" "") for doc in adj_addr]
bigram = Phrases(sentence_stream, min_count=100, threshold=2)
</code></pre>

<hr>

<p><strong>Sample input from column of dataframe</strong></p>

<pre><code>SUITE 7001 70/F INTERNATIONAL FINANCE CENTRE TWO  8 FINANCE ST CENTRAL HONG HONG KONG
QUALCOMM INTERNATIONAL INC. 9 QUEENS RD CENTRAL 27TH FLOOR HONG KONG
SAMUEL CHEN COMPANY LIMITED 25 CHIU LUNG ST CHIU LUNG BUILDING 4TH FLOOR CENTRAL HONG KONG
</code></pre>

<p><strong>Expected output(New data after passing the gensim phrase model)</strong></p>

<pre><code>SUITE 7001 70/F INTERNATIONAL_FINANCE CENTRE TWO 8_FINANCE ST CENTRAL_HONG HONG_KONG
QUALCOMM INTERNATIONAL INC. 9 QUEENS_RD CENTRAL 27TH FLOOR HONG_KONG
SAMUEL CHEN COMPANY_LIMITED 25 CHIU LUNG_ST CHIU LUNG BUILDING 4TH_FLOOR CENTRAL_HONG KONG
</code></pre>

<p><strong>I am not able to replace the corresponding addresses with the respective bigrams from gensim 
 phraseses model iteratively  .</strong> <strong>My expected output is to replace all the old addresses with newly generated bigram phrases .  So that i can pass it to a countvectorizer</strong> </p>

<p>Any help is appreciated.</p>
","9161067","","9161067","","2018-01-02 03:04:35","2018-01-02 03:04:35","Creating Bigrams Phrases of a Column in Pandas using Gensim and attaching it to the to same dataframe","<pandas><nlp><word2vec><gensim><word-embedding>","0","2","1","","","CC BY-SA 3.0"
"30851588","1","","","2015-06-15 17:42:10","","0","88","<p>So, I am relatively new using Gensim and LDA in general. The problem right now is that when I run LDA on my corpus, the topics' tokens' weights are all 0:</p>

<p>2015-06-15 12:21:12,439 : INFO : topic diff=0.082235, rho=0.250000</p>

<p>2015-06-15 12:21:12,454 : <strong>INFO : topic #0</strong> (0.100): 0.000*sundayes + 0.000*nowe + 0.000*easter + 0.000*iniunctions + 0.000*eyther + 0.000*christ, + 0.000*authoritie + 0.000*sir + 0.000*saint + 0.000*thinge</p>

<p>2015-06-15 12:21:12,468 : <strong>INFO : topic #1</strong> (0.100): 0.000*eu'n + 0.000*ioseph + 0.000*pharohs + 0.000*pharoh + 0.000*iosephs + 0.000*lo! + 0.000*egypts + 0.000*iacob + 0.000*ioseph, + 0.000*beniamin</p>

<p>2015-06-15 12:21:12,482 : <strong>INFO : topic #2</strong> (0.100): 0.000*agreeable + 0.000*creede, + 0.000*fourme + 0.000*conteined + 0.000*apostolike, + 0.000*vicars, + 0.000*sacrament + 0.000*contrarywise + 0.000*parsons, + 0.000*propitiatorie</p>

<p>2015-06-15 12:21:12,495 : <strong>INFO : topic #3</strong> (0.100): 0.000*yf + 0.000*suche + 0.000*lyke + 0.000*shoulde + 0.000*moste + 0.000*youre + 0.000*oure + 0.000*lyfe, + 0.000*anye + 0.000*thinges</p>

<p>2015-06-15 12:21:12,507 : <strong>INFO : topic #4</strong> (0.100): 0.000*heau'nly + 0.000*eu'n + 0.000*heau'n + 0.000*sweet + 0.000*peace + 0.000*eu'ry + 0.000*constance + 0.000*constant + 0.000*doth + 0.000*oh</p>

<p>2015-06-15 12:21:12,521 : <strong>INFO : topic #5</strong> (0.100): 0.000*eu'n + 0.000*ioseph + 0.000*pharohs + 0.000*pharoh + 0.000*vel + 0.000*iosephs + 0.000*heau'n + 0.000*lo! + 0.000*ac + 0.000*seu'n</p>

<p>2015-06-15 12:21:12,534 : <strong>INFO : topic #6</strong> (0.100): 0.000*thou + 0.000*would + 0.000*love + 0.000*king + 0.000*sir, + 0.000*doe + 0.000*thee + 0.000*1. + 0.000*never + 0.000*2.</p>

<p>2015-06-15 12:21:12,546 : <strong>INFO : topic #7</strong> (0.100): 0.000*quae + 0.000*vt + 0.000*qui + 0.000*ij + 0.000*non + 0.000*ad + 0.000*si + 0.000*vel + 0.000*atque + 0.000*cum</p>

<p>2015-06-15 12:21:12,558 : <strong>INFO : topic #8</strong> (0.100): 0.000*suspected + 0.000*supersticious + 0.000*squire + 0.000*parsons + 0.000*ordinarie + 0.000*vsed, + 0.000*english, + 0.000*fortnight + 0.000*squire, + 0.000*offenders</p>

<p>2015-06-15 12:21:12,572 : <strong>INFO : topic #9</strong> (0.100): 0.001*/ + 0.001*ile + 0.000*y^e + 0.000*che + 0.000*much + 0.000*tis + 0.000*could + 0.000*oh + 0.000*neuer + 0.000*heart</p>

<p>I have 307 documents and I'm running my LDA with the following code after removing the stopwords:</p>

<p>texts = [[token for token in text if frequency[token] > 3 ] for text in texts]</p>

<p>dictionary = corpora.Dictionary(texts)</p>

<p>corpus = [dictionary.doc2bow(text) for text in texts]</p>

<p>tfidf = models.TfidfModel(corpus)
tfidf_corpus = tfidf[corpus]</p>

<p>lda = models.LdaModel(tfidf_corpus, id2word = dictionary, update_every=1, chunksize= 20, num_topics = 10, passes = 1)</p>

<p>lda[tfidf_corpus]</p>

<p>lda.print_topics(10)</p>

<p>I am not sure what is wrong but everytime I run this, the token weights are 0. What might be causing this and how could I correct this? </p>
","4975564","","4975564","","2015-06-15 22:00:02","2015-06-15 22:00:02","LDA Results Errors","<machine-learning><nlp><lda><topic-modeling><gensim>","0","4","","","","CC BY-SA 3.0"
"56082233","1","","","2019-05-10 17:23:33","","1","1847","<pre class=""lang-py prettyprint-override""><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from random import shuffle
import logging

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

tagged_data = []
clas = ['type1', 'type2', 'type3']
for cla in clas:
  with open(f'../data/jieba/{cla}train.txt', 'r', encoding='UTF-8')as f:
    i = 0
    lines = f.readlines()
    for line in lines:
      tagged_data.append(TaggedDocument(words=line.split(' ')[:-1], tags=[cla + str(i)]))
      i += 1

num_doc = len(tagged_data)
shuffle(tagged_data)

model = Doc2Vec(dm=1, vector_size=128, window=5, alpha=0.01, min_alpha=0.0001, max_vocab_size=100000, sample=1e-5, workers=4, epochs=3, hs=1, dm_mean=1)
model.build_vocab(tagged_data)
model.train(documents=tagged_data, epochs=model.epochs, total_examples=num_doc)
model.save(""d2v.model"")
</code></pre>

<p>The above is my code and the output is like</p>

<pre><code>2019-05-11 01:11:48,177 : INFO : EPOCH 1 - PROGRESS: at 3.64% examples, 307751 words/s, in_qsize 7, out_qsize 0
2019-05-11 01:11:49,195 : INFO : EPOCH 1 - PROGRESS: at 7.63% examples, 316010 words/s, in_qsize 7, out_qsize 0
2019-05-11 01:11:50,196 : INFO : EPOCH 1 - PROGRESS: at 11.44% examples, 316465 words/s, in_qsize 8, out_qsize 0
</code></pre>

<p>How to get the value of loss function in each step so I can visualize it?</p>
","7813311","","","","","2020-05-19 16:21:26","Gensim doc2vec, how to get the value of loss function in each step","<nlp><gensim><doc2vec>","1","2","1","","","CC BY-SA 4.0"
"48049619","1","","","2018-01-01 12:22:42","","1","5712","<p>I am trying to solve a nlp problem where i have a dict of words like :</p>

<pre><code>list_1={'phone':'android','chair':'netflit','charger':'macbook','laptop','sony'}
</code></pre>

<p>Now if input is 'phone' i can easily use 'in' operator to get the description of phone and its data by key but problem is if input is something like  'phones' or 'Phones' .</p>

<p>I want if i input  'phone' then i get words like </p>

<pre><code>'phone' ==&gt; 'Phones','phones','Phone','Phone's','phone's' 
</code></pre>

<p>I don't know which word2vec i can use and which nlp module can provide solution like this.</p>

<p>second issue is if i give a word 'Dog' can i get words like 'Puppy','Kitty','Dog','dog' etc ?</p>

<p>I tried something like this but its giving synonyms :</p>

<pre><code>from nltk.corpus import wordnet as wn
for ss in wn.synsets('phone'): # Each synset represents a diff concept.
    print(ss)
</code></pre>

<p>but its returning :</p>

<pre><code>Synset('telephone.n.01')
Synset('phone.n.02')
Synset('earphone.n.01')
Synset('call.v.03')
</code></pre>

<p>Instead i wanted :</p>

<pre><code>'phone' ==&gt; 'Phones','phones','Phone','Phone's','phone's' 
</code></pre>
","","user9158931","5378816","","2018-01-01 14:14:28","2018-01-01 14:14:28","How to get similar words related to one word?","<python><nlp><nltk><gensim><spacy>","1","0","","","","CC BY-SA 3.0"
"48059145","1","48080563","","2018-01-02 10:19:07","","6","2250","<p>I have been using <strong>gensim's</strong> libraries to train a doc2Vec model. After experimenting with different datasets for training, I am fairly confused about what should be an ideal training data size for doc2Vec model?</p>

<p>I will be sharing my understanding here. Please feel free to correct me/suggest changes-</p>

<ol>
<li><strong>Training on a general purpose dataset-</strong> If I want to use a model trained on a general purpose dataset, in a specific use case, I need to train on a lot of data.</li>
<li><strong>Training on the context related dataset-</strong> If I want to train it on the data having the same context as my use case, usually the training data size can have a smaller size.</li>
</ol>

<p><em>But what are the number of words used for training, in both these cases?</em></p>

<p>On a general note, we stop training a ML model, when the error graph reaches an ""elbow point"", where further training won't help significantly in decreasing error. Has any study being done in this direction- where doc2Vec model's training is stopped after reaching an elbow ?</p>
","9160820","","","","","2018-01-03 15:40:45","How much data is actually required to train a doc2Vec model?","<neural-network><gensim><doc2vec>","1","0","1","","","CC BY-SA 3.0"
"65549685","1","65552877","","2021-01-03 12:10:01","","0","77","<p>I want to train a Word2Vec model using &quot;gensim&quot;. I want to determine the initial rating rate. However, it is written that both &quot;alpha&quot; and &quot;start_alpha&quot; parameters can be used to do so. What is the difference between them? Are they the same?</p>
","14251114","","","","","2021-01-03 18:49:11","Difference between ""alpha"" and ""start_alpha""","<python><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"48060401","1","","","2018-01-02 11:46:40","","0","750","<p>as an output for creating a Word2Vec model on ~1GB of corpus I got 3 files as an output:</p>

<ul>
<li>word2vec_model</li>
<li>word2vec_model.syn1neg.npy</li>
<li>word2vec_model.wv.syn0.npy</li>
</ul>

<p>I used to have only the first file on (when training a smaller corpus).</p>

<p>how should I treat the last 2 files when loading the model?
Should I load only the first one and run queries on it as usual?</p>
","9160882","","466862","","2018-01-02 11:49:25","2018-01-03 15:30:29","syn1neg & syn0 created as output","<word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"48073718","1","","","2018-01-03 08:28:06","","2","1337","<p>I have created a word2vec  dictionary using Gensim .I want replace my text corpus with the root word .
Is there a way by which we can replace the text data corpus by the root word .</p>

<p><strong>Eg. Building is my root word and i have similarity of it in my dictionary. I want replace all similar words to building that have similarity above.6 in my orginal text corpus</strong>.</p>

<p><strong>Sample data from column of the dataframe</strong></p>

<pre><code>canara bank aon china bldng queens rd centeal central
des voeux rd west hk unit f kwan yick bldng phase central western
formula growth asia limited suite chinachem tower connaught rd central
bangkok bank public company limited central district branch des voeux rd central cenrta
</code></pre>

<p><strong>Similarities</strong></p>

<pre><code>  model.most_similar(""building"")
    [('bu', 0.762892484664917),
     ('bldg', 0.7351159453392029),
     ('bl', 0.7237456440925598),
     ('building.', 0.7153196334838867),
     ('buliding', 0.6988817453384399),
     ('bld', 0.6966143846511841),
     ('bldng', 0.663501501083374),
     ('bdg', 0.6504702568054199),
     ('bd', 0.6480772495269775),
     ('blog', 0.6432161331176758)]

model.most_similar(""ltd"")
[('limited', 0.7886955142021179),
 ('limi', 0.6512018442153931),
 ('limite', 0.6031635999679565),
 ('wilford', 0.5938706994056702),
 ('lt', 0.583463728427887),
 ('lighttech', 0.5828145146369934),
 ('rmc', 0.5821658372879028),
 ('tomoike', 0.5752800703048706),
 ('jd', 0.5751883387565613),
 ('nxp', 0.5725069046020508)]
</code></pre>

<p><strong>Dictionary</strong></p>

<pre><code>import gensim
from gensim import corpora,similarities,models
class AccCorpus(object):

    def __init__(self):
        self.path = ''


    def __iter__(self):
        for sentence in data[""Adj_Addr""]:
            yield [word.lower() for word in sentence.split()]


def build_corpus():
    model = gensim.models.word2vec.Word2Vec(alpha=0.025, min_alpha=0.025,window=2,sg=2)
    sentences = AccCorpus()
    model.build_vocab(sentences)
    for epoch in range(1):
        model.train(sentences,total_examples=model.corpus_count, epochs=model.iter)
        model.alpha -= 0.002  # decrease the learning rate
        model.min_alpha = model.alpha  # fix the learning rate, no decay

    model_name = ""word2vec_model""
    model.save(model_name)
    return model
model=build_corpus()
</code></pre>
","8793219","","8793219","","2018-01-03 17:43:41","2018-01-17 13:15:33","Word Replacement in text corpus using word2vec similarity dictionary in a pandas dataframe","<python><pandas><string-matching><word2vec><gensim>","1","2","","","","CC BY-SA 3.0"
"48082018","1","48082199","","2018-01-03 17:11:12","","2","148","<p>I have a word2vec dictionary which gives a top similar words  to given word.</p>

<p>I want to pass the list of words for which similarity  needs to calculated  from  a file or list</p>

<p><strong>Input</strong> </p>

<pre><code>word_list =['wan,'floor','street']
</code></pre>

<p>Similarity of these words should be checked against the word2vec dictionary and  similar words to the input word_list must found and written to a dataframe in the below shown format.</p>

<pre><code>model.most_similar(""wan"")

[('wan.', 0.7509685754776001),
 ('want', 0.7326164245605469),
 ('aupuiwan', 0.7161564230918884),
 ('puiwan', 0.7119397521018982),
 ('wanstreet', 0.7096157073974609),
 ('woshing', 0.7046518921852112),
 ('futan', 0.6979573369026184),
 ('won', 0.696295440196991),
 ('fota', 0.6961145401000977),
 ('pul', 0.6921802759170532)]
</code></pre>

<p>I want create a dataframe with  two columns Word and Similar words. </p>

<p><strong>Output Dataframe</strong></p>

<pre><code>Word    Similar Words
wan     ('wan.', 'want','aupuiwan','puiwan','wanstreet')
floor   ('fl','flooor','flor','flr','gf')
street  ('st','rosestreet','stret','strt','str')
</code></pre>

<p>Any help is appreciated.</p>
","8783789","","712995","","2018-01-03 17:58:41","2018-01-03 17:58:41","How to create dataframe of top 5 close words to a particular word lists from a dictionary in pandas","<python><string><pandas><word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"55091094","1","","","2019-03-10 18:43:37","","3","1336","<p>In my project, I use the Python library <a href=""https://radimrehurek.com/gensim/models/wrappers/ldamallet.html"" rel=""nofollow noreferrer"">gensim</a> for topic modeling/extraction of text.
I try to load my trained LdaMallet model to classify new unseen texts.</p>

<p>The first part is loading the model.</p>

<pre><code>import os

dirname = os.path.dirname(__file__)
filename = os.path.join(dirname, 'mallet-2.0.8/bin/mallet')

# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip
os.environ['MALLET_HOME'] = # path to mallet

ldaMallet = gensim.models.wrappers.LdaMallet.load('lda_malletoutputCommentsAndMethods.model)
ldaModel = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldaMallet)
</code></pre>

<p>I am not sure about the last line which converts the ldaMallet to LdaModel. It was the only way to get some result.</p>

<p>Then the second part is preparing the new data and classify it.</p>

<pre><code>from gensim.test.utils import common_dictionary
other_texts = [['new', 'document', 'to', 'classify', 'as', 'array']]
other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]
vector = ldaModel[other_corpus[0]]

# sorts the result by probability and not by topic ID
print(sorted(vector, key=lambda x: x[1], reverse=True))
</code></pre>

<p>Then the result looks something like this:</p>

<pre><code>[(16, 0.143), (17, 0.08), (9, 0.0653),...]
</code></pre>

<p>No matter which text I use in the <code>other_texts</code> array, this result isn't changing, but it should. </p>
","4167701","","4167701","","2019-03-10 18:56:35","2019-03-10 18:56:35","Correct way to load LdaMallet model with gensim and classify unseen documents","<python><gensim><lda><mallet>","0","3","1","","","CC BY-SA 4.0"
"58822292","1","58822726","","2019-11-12 15:56:58","","1","753","<p><strong>Context</strong></p>

<p>There exists severals questions about how to train <code>Word2Vec</code> using <code>gensim</code> with streamed data. Anyhow, these questions don't deal with the issue that streaming cannot use multiple workers since there is no array to split between threads.</p>

<p>Hence I wanted to create a generator providing such functionality for gensim. My results look like:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec as w2v

#The data is stored in a python-list and unsplitted.
#It's too much data to store it splitted, so I have to do the split while streaming.
data = ['this is document one', 'this is document two', ...]

#Now the generator-class
import threading

class dataGenerator:
    """"""
    Generator for batch-tokenization.
    """"""

    def __init__(self, data: list, batch_size:int = 40):
        """"""Initialize generator and pass data.""""""

        self.data = data
        self.batch_size = batch_size
        self.lock = threading.Lock()


    def __len__(self):
        """"""Get total number of batches.""""""
        return int(np.ceil(len(self.data) / float(self.batch_size)))


    def __iter__(self) -&gt; list([]):
        """"""
        Iterator-wrapper for generator-functionality (since generators cannot be used directly).
        Allows for data-streaming.
        """"""
        for idx in range(len(self)):
            yield self[idx]


    def __getitem__(self, idx):

        #Make multithreading thread-safe
        with self.lock:

            # Returns current batch by slicing data.
            return [arr.split("" "") for arr in self.data[idx * self.batch_size : (idx + 1) * self.batch_size]]


#And now do the training
model = w2v(
             sentences=dataGenerator(data),
             size=300,
             window=5,
             min_count=1,
             workers=4
            )
</code></pre>

<p>This results in the error </p>

<blockquote>
  <p>TypeError: unhashable type: 'list'</p>
</blockquote>

<p>Since <code>dataGenerator(data)</code> would work if I'd just yield a single splitted document, I assume that gensims <code>word2vec</code> wraps the generator within an extra list. In this case the <code>__iter__</code> would look like:</p>

<pre class=""lang-py prettyprint-override""><code>def __iter__(self) -&gt; list:
    """"""
    Iterator-wrapper for generator-functionality (since generators cannot be used directly.
    Allows for data-streaming.
    """"""
    for text in self.data:
        yield text.split("" "")
</code></pre>

<p>Hence, my batch would also be wrapped resulting in something like <code>[[['this', '...'], ['this', '...']], [[...], [...]]]</code> (=> list of list of list) which cannot be processed by gensim.</p>

<p><br>
<br>
<br>
<strong>My question:</strong></p>

<p><em>Can I ""stream""-pass batches in order to use multiple workers?
How can I change my code accordingly?</em></p>
","3757672","","3757672","","2020-01-09 22:18:00","2020-07-17 23:38:48","Batch-train word2vec in gensim with support of multiple workers","<python><nlp><batch-processing><gensim><word2vec>","2","0","2","","","CC BY-SA 4.0"
"48090426","1","48248532","","2018-01-04 07:15:54","","0","5191","<p>While training using <code>doc2vec</code>, I got this error:</p>

<pre><code>AttributeError: 'list' object has no attribute 'words' in python gensim module
</code></pre>

<p>This is my code:</p>

<pre><code># Extracting titles from csv to list
with open(query+'_titles.csv', 'rb') as f:
    reader = csv.reader(f)
    titlelist = list(reader)
# build
model = doc2vec.Doc2Vec(size=30, window=1, alpha=0.01, min_count=2, sample=1e-5, workers=100)
model.build_vocab(titlelist)
titlearray = np.asarray(titlelist)
print 'Training Model...'
</code></pre>

<p>I use python <em>2.7.11</em> and gensim version is <em>3.2.0</em> if that helps. There must be something I am really missing. </p>
","8472377","","8472377","","2018-06-09 04:56:25","2018-06-09 04:56:25","AttributeError: 'list' object has no attribute 'words' in python gensim module","<python><machine-learning><nlp><gensim><doc2vec>","1","2","","","","CC BY-SA 4.0"
"33525173","1","","","2015-11-04 15:05:25","","0","1253","<p><strong>Problem:</strong> I want to convert a list of list into a dataframe.</p>

<p><strong>Setup:</strong> I have the following list:</p>

<pre><code>data = [[(1,0.8),(2,0.2)],
       [(0,0.1),(1,0.3),(2,0.6)],
       [(0,0.05),(1,0.05),(2,0.3),(3,0.4),(4,0.2)]]
</code></pre>

<p>This is an LDA Document-Topic Probability List from <code>gensim</code> in which each list is a document and each tuple is one of five topic probabilities. (See an earlier question I posted on Stack Overflow <a href=""https://stackoverflow.com/questions/33002480/convert-get-documents-topics-to-data-frame"">here</a>). The first element in the tuple represents the topic number, the second element is the probability that the topic probability for the document.</p>

<p>Note that while some documents (like the 3rd list) can have up to five tuples (topic probabilities), gensim LDA does not output probabilities for topics with less 0.01 probabilities. Therefore, examples like document 1 and document 2 have less than five tuples.</p>

<p><strong>Goal:</strong> Use for loops to create a Document-Topic Probability matrix such that:</p>

<pre><code>ProbMatrix = [(0,0.8,0.2,0,0),
        (0.1,0.3,0.6,0,0),
        (0.05,0.05,0.3,0.4,0.2)]
</code></pre>

<p>As noted above, for ""missing"" tuples (topics), zero's need to be plugged in. Once I get this list, I figure I can use pandas dataframe function to produce my final output (df) such that</p>

<pre><code>df = pd.DataFrame(ProbMatrix)
</code></pre>

<p><strong>My (Failed) Attempt:</strong></p>

<pre><code>ProbMatrix = []
for i in data:      #each document i
    for j in i:     #each topic j
        if j[0] == 0:
            ProbMatrix[i,0].append(j[1])
        elif j[0]  == 1:
            ProbMatrix[i,1].append(j[1])
        elif j[0]  == 2:
            ProbMatrix[i,2].append(j[1])   
        elif j[0]  == 3:
            ProbMatrix[i,3].append(j[1])   
        elif j[0]  == 4:
            ProbMatrix[i,4].append(j[1])  
</code></pre>

<p>The problem is how I'm referencing ProbMatrix because I'm receiving the following error:</p>

<pre><code>TypeError: list indices must be integers, not tuple
</code></pre>

<p>Thank you for your help!</p>

<p><strong>Bonus (that is, it'd be even better if you can help):</strong></p>

<p>One problem I've found with gensim LDA is that, as mentioned, it does not output probabilities less than 0.01, even if <code>minimum_probability = None</code>. For example, see this earlier <a href=""https://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda/32651946#32651946"">post</a>. The example above is illustrative in that the topic probabilities sum to 1 for each document. However, in reality the output looks more like this:</p>

<pre><code>data = [[(1,0.79),(2,0.2)],  # topic 1 probability 0.79 from 0.8
       [(0,0.09),(1,0.3),(2,0.6)], # topic 0 probability 0.09 from 0.1
       [(0,0.05),(1,0.05),(2,0.3),(3,0.4),(4,0.2)]]
</code></pre>

<p>What I'm looking for is instead of putting zero into unknown topic probabilities, instead make the remaining missing topics an even probability such that topic probabilities for each document equal 1. For example, this would result in a ProbMatrix:</p>

<pre><code>ProbMatrix = [(0.0033,0.79,0.2,0.0033,0.0033),
        (0.09,0.3,0.6,0.005,0.005),
        (0.05,0.05,0.3,0.4,0.2)]
</code></pre>
","5420391","","-1","","2017-05-23 11:50:42","2015-11-05 15:37:54","python - convert list of list to dataframe","<python><list><pandas><dataframe><gensim>","4","0","1","","","CC BY-SA 3.0"
"58836322","1","58839437","","2019-11-13 11:40:57","","0","190","<p>I am trying to implement a semantic search to retrieve similar documents from a dataset of unstructured French documents.</p>

<ul>
<li>These documents are not categorized and are templates with 300 - 3000 words per document.</li>
<li>I am using doc2vec using gensim to find the paragraph embeddings with 300 dimensions and a window of 5 of the dataset.</li>
<li>I am then converting the search query which is a maximum of 5 words to the vector with 300 dimensions and comparing the cosine distance to find the document close to the search queries.</li>
</ul>

<p>I am not getting good results. Please suggest some strategies to do the semantic search. I was trying to reduce the number of words in my dataset by doing rake keyword extraction.</p>
","12366393","","5652313","","2019-11-13 15:46:33","2019-11-25 20:17:33","How to do language representation on huge documents of 3000-4000 word for query-based retrieval?","<search><nlp><gensim><cosine-similarity><doc2vec>","2","6","1","","","CC BY-SA 4.0"
"48115965","1","66546180","","2018-01-05 14:50:58","","2","1232","<p>I want to compute Cosine Similarity between LDA topics. In fact, gensim function .matutils.cossim can do it but I dont know  which parameter (vector ) I can use for this function?</p>

<p>Here is a snap of  code :</p>

<pre><code>import numpy as np
import lda
from sklearn.feature_extraction.text import CountVectorizer

cvectorizer = CountVectorizer(min_df=4, max_features=10000, stop_words='english')
cvz = cvectorizer.fit_transform(tweet_texts_processed)

n_topics = 8
n_iter = 500
lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)
X_topics = lda_model.fit_transform(cvz)

n_top_words = 6
topic_summaries = []

topic_word = lda_model.topic_word_  # get the topic words
vocab = cvectorizer.get_feature_names()
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
    topic_summaries.append(' '.join(topic_words))
    print('Topic {}: {}'.format(i, ' '.join(topic_words)))

doc_topic = lda_model.doc_topic_
lda_keys = []
for i, tweet in enumerate(tweets):
    lda_keys += [X_topics[i].argmax()]

import gensim
from gensim import corpora, models, similarities
#Cosine Similarity between LDA topics
 **sim = gensim.matutils.cossim(LDA_topic[1], LDA_topic[2])** 
</code></pre>
","8103894","","3938208","","2018-05-23 08:51:56","2021-03-09 11:44:15","Cosine Similarity and LDA topics","<python><nlp><gensim><lda>","1","1","3","","","CC BY-SA 3.0"
"33596082","1","","","2015-11-08 16:20:09","","2","1723","<p>I have a gensim Word2Vec model computed in Python 2 like that:</p>

<pre><code>from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

model = Word2Vec(LineSentence('enwiki.txt'), size=100, 
                 window=5, min_count=5, workers=15)
model.save('w2v.model')
</code></pre>

<p>However, I need to use it in Python 3. If I try to load it, </p>

<pre><code>import gensim
from gensim.models import Word2Vec
model = Word2Vec.load('w2v.model')
</code></pre>

<p>it results in an error:</p>

<pre><code>UnicodeDecodeError: 'ascii' codec can't decode byte 0xf9 in position 0: ordinal not in range(128)
</code></pre>

<p>I suppose the problem is in differences in encoding between Python2 and Python3. Also it seems like gensim is using pickle to save/load models.</p>

<p>Is there a way to set encoding/pickle options so that the model loads properly? Or maybe use some external tool to convert the model file?</p>

<p>Recomputing it in Python 3 is not an option: it takes way too much time.</p>
","3642151","","","","","2016-04-21 18:25:56","Load gensim Word2Vec computed in Python 2, in Python 3","<python><python-3.x><encoding><gensim><word2vec>","1","2","2","","","CC BY-SA 3.0"
"50643196","1","","","2018-06-01 12:13:08","","8","739","<p>I am training a <code>fastText</code> model using <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""noreferrer""><code>gensim.models.fasttext</code></a>. However, I can't seem to find a method to compute the loss of the iteration for logging purposes. If I look at <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer""><code>gensim.models.word2vec</code></a>, it has the <code>get_latest_training_loss</code> method which allows you to print the training loss. Are there any alternatives or it is simply impossible?</p>
","7250730","","","","","2021-08-19 09:10:28","Gensim FastText compute Training Loss","<python><nlp><word2vec><gensim><fasttext>","1","5","1","","","CC BY-SA 4.0"
"65564423","1","","","2021-01-04 14:33:04","","0","152","<p>I have a Wiki Dump as xml.bz2 file and want to convert it to txt for later processing with BERT. The goal is to have each separate sentence in a new line and an emptly line between articles (requirements of BERT Training)</p>
<p>I tried to follow this (<a href=""https://stackoverflow.com/questions/50697092/how-to-get-the-wikipedia-corpus-text-with-punctuation-by-using-gensim-wikicorpus"">How to get the wikipedia corpus text with punctuation by using gensim wikicorpus?</a>) Post and did a lot of research of my own. This is what i got so far:</p>
<pre><code>from __future__ import print_function
import sys
from gensim.corpora import WikiCorpus
from wikicorpus import *
import six

def tokenize(content):
    #override original method in wikicorpus.py
    return [token.encode('utf8') for token in content.split() 
           if len(token) &lt;= 15 and not token.startswith('_')]

def process_article(args):
   # override original method in wikicorpus.py
    text, lemmatize, title, pageid = args
    text = filter_wiki(text)
    if lemmatize:
        result = utils.lemmatize(text)
    else:
        result = tokenize(text)
    return result, title, pageid


class MyWikiCorpus(WikiCorpus):
    def __init__(self, fname, processes=None, lemmatize=utils.has_pattern(), dictionary=None, filter_namespaces=('0',)):
        WikiCorpus.__init__(self, fname, processes, lemmatize, dictionary, filter_namespaces)

        def get_texts(self):
            articles, articles_all = 0, 0
            positions, positions_all = 0, 0
            texts = ((text, self.lemmatize, title, pageid) for title, text, pageid in extract_pages(bz2.BZ2File(self.fname), self.filter_namespaces))
            pool = multiprocessing.Pool(self.processes)
            for group in utils.chunkize(texts, chunksize=10 * self.processes, maxsize=1):
                for tokens, title, pageid in pool.imap(process_article, group):  # chunksize=10):
                    articles_all += 1
                    positions_all += len(tokens)
                if len(tokens) &lt; ARTICLE_MIN_WORDS or any(title.startswith(ignore + ':') for ignore in IGNORED_NAMESPACES):
                    continue
                articles += 1
                positions += len(tokens)
                if self.metadata:
                    yield (tokens, (pageid, title))
                else:
                    yield tokens
            pool.terminate()

            print(
                &quot;finished iterating over Wikipedia corpus of %i documents with %i positions&quot;
                &quot; (total %i articles, %i positions before pruning articles shorter than %i words)&quot;,
                articles, positions, articles_all, positions_all, ARTICLE_MIN_WORDS)
            self.length = articles  # cache corpus length
</code></pre>
<p>I used the Post above to override the functions and finally called the class like so:</p>
<pre><code>def make_corpus2(inp, outp):
    space = &quot; &quot;
    i = 0
    output = open(outp, 'w')
    wiki = MyWikiCorpus(inp, lemmatize=False, dictionary={})
    for text in wiki.get_texts():
        if six.PY3:
            output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
        else:
            output.write(space.join(text) + &quot;\n&quot;)
        i = i + 1
        if (i % 10000 == 0):
            print(&quot;Saved &quot; + str(i) + &quot; articles&quot;)

    output.close()
    print(&quot;Finished Saved &quot; + str(i) + &quot; articles&quot;)
</code></pre>
<p>and called it with <code>make_corpus2(&quot;./Wiki_dump_gross.xml.bz2&quot;, &quot;./pretrain/wiki_dump_sentences.txt&quot;)</code></p>
<p>There is no Error and it filles the output file, but punctuations are still missing. I felt like i incorprated the given solution from the previous post, so i was wondering where my mistake could be. To clarify: I used Jupyter Notebook for this.</p>
<p>Example of the Output i get:</p>
<pre><code>der begriff heilkunde bezeichnet die gesamtheit der menschlichen kenntnisse und f√§higkeiten √ºber die
entstehung heilung und verhinderung pr√§vention von krankheiten er wird als synonym f√ºr medizin im
allgemeinen aber auch innerhalb der der volksheilkunde und jeder form der psychotherapie verwendet
aus√ºbung einer heilkunde die aus√ºbung einer heilkunde genannt auch heilkunst ist in deutschland
√∂sterreich und der schweiz rechtlich unterschiedlich geregelt 
</code></pre>
<p>Also i was wondering if it is possible to keep the casing for the text, since in German this is a relevant part of the language.</p>
","12009172","","","","","2021-01-04 14:33:04","Keep punctuation and casing in gensim wikicorpus text","<python><nlp><gensim>","0","0","","","","CC BY-SA 4.0"
"48089141","1","48089358","","2018-01-04 05:24:03","","0","60","<p>I have a word2vec dictionary which has a list of similar words to given word.</p>

<p><strong>Example</strong></p>

<pre><code>model.most_similar(""ltd"")
[('limited', 0.7886955142021179),
 ('limi', 0.6512018442153931),
 ('limite', 0.6031635999679565),
 ('wilford', 0.5938706994056702),
 ('lt', 0.583463728427887),
 ('lighttech', 0.5828145146369934),
 ('rmc', 0.5821658372879028),
 ('tomoike', 0.5752800703048706),
 ('jd', 0.5751883387565613),
 ('nxp', 0.5725069046020508)]
</code></pre>

<p>I want to create  dataframe containing root and similar_words(having similarity above .6)</p>

<p>Currently I am able to write all the  similar words corresponding to root word</p>

<pre><code>words = y
similar = [[item[0] for item in model.most_similar(word)[:6]] for word in words]
similarity_matrix = pd.DataFrame({'Root_Word': words, 'Similar_Words': similar})
</code></pre>

<p><strong>Current Output</strong></p>

<pre><code>Root_Word    Similar_word
[st]         [st., sreet, rd;, yop, tseun, tsven] 
[limited]    [ltd, lt, wt, serial, (h.k., dk] 
[centre]     [cent, ct, cte, entre, ctr., ce]
</code></pre>

<p><strong>Expected output is have only Similar words which having similarity above .6.</strong></p>

<p>How can this be done</p>
","9161067","","9161067","","2018-01-04 06:05:32","2018-01-04 06:05:32","How to write words having similarity above .6 to a specific word from a dictionary to a dataframe in pandas","<python><string><pandas><word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"48096066","1","","","2018-01-04 13:09:28","","5","339","<p>I am trying to use pre-trained word embeddings taking into account phrases. Popular pre-trained embeddings like <code>GoogleNews-vectors-negative300.bin.gz</code> have separate embeddings for phrases as well as unigrams e.g., embeddings for <code>New_York</code> and the two unigrams <code>New</code> and <code>York</code>. Naive word tokenization and dictionary look-up ignore the bigram embedding.     </p>

<p>Gensim provides a nice <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""noreferrer"">Phrase</a> model, where given a text sequence it can learn compact phrases e.g., <code>New_York</code> instead of two unigrams <code>New</code> and <code>York</code>. This is done by aggregating and comparing count statistics between the  unigrams and the bigram. 1. Is it possible to use <code>Phrase</code> with pre-trained embeddings without estimating the count statistics elsewhere? </p>

<ol>
<li>Is it possible to use <code>Phrase</code> with pre-trained embeddings without estimating the count statistics elsewhere? </li>
<li>If not, is there an efficient way to use these bigrams? I can imagine a way using a loop, but I believe it is ugly (Below).</li>
</ol>

<p>Here is the ugly code.</p>

<pre><code>from ntlk import word_tokenize
last_added = False
sentence = 'I love New York.'
tokens =  [""&lt;s&gt;""]+ word_tokenize(sentence) +""&lt;\s&gt;""]
vectors = []
for index, token in enumerate(tokens):
    if last_added:
        last_added=False
        continue
    if ""%s_%s""%(tokens[index-1], token) in model:
        vectors.append(""%s_%s""%(tokens[index-1], token))
        last_added = True
    else:
        vectors.append(tokens[index-1])
        lase_added = False
</code></pre>
","1881878","","","","","2018-01-04 13:09:28","Using gensim's Phraser with pre-trained vectors","<python><machine-learning><gensim><phrase>","0","0","1","","","CC BY-SA 3.0"
"33638915","1","33640218","","2015-11-10 20:24:43","","0","294","<p>I am struggling to create an iterator from a query from sqlalchemy. </p>

<p>Here is what I tried so far </p>

<p><strong>create a table</strong> </p>

<pre><code>from sqlalchemy import create_engine, Column, MetaData, Table , Integer, String
engine = create_engine('sqlite:///test90.db')
conn = engine.connect()
metadata = MetaData()
myTable = Table('myTable', metadata,
     Column('Doc_id', Integer, primary_key=True),
     Column('Doc_Text', String))
metadata.create_all(engine)

conn.execute(myTable.insert(), [{'Doc_id': 1, 'Doc_Text' : 'first sentence'},
          {'Doc_id': 2, 'Doc_Text' : 'second sentence'},
          {'Doc_id': 3, 'Doc_Text' : 'third sentence'},
          {'Doc_id': 4, 'Doc_Text' : 'fourth sentence'}
          ])
</code></pre>

<p>I read everything I could on iterator but do not get it. 
Here the class I created to get an iterator but it does not work 
(it overflows although I specify a break) </p>

<pre><code>from sqlalchemy import create_engine

class RecordsIterator:
def __init__(self, xDB, xSQL):
    self.engine = create_engine(xDB)
    self.conn = self.engine.connect()
    self.xResultCollection = self.conn.execute(xSQL)
def __iter__(self):
    return self 
def next (self):
    while self.xResultCollection.closed is False:
        xText = (self.xResultCollection.fetchone())[1]
        xText = xText.encode('utf-8')
        yield xText.split()
        if not self.xResultCollection:
            break


x1 = RecordsIterator(xDB = 'sqlite:///test91.db', xSQL = 'select * from myTable')
</code></pre>

<p>In case you are wondering why I am not just using a <strong>generator</strong> . 
I need to feed the iterator in gensim.Word2Vec and unfortunately, it does not take a generator </p>

<pre><code>   import gensim
   gensim.models.Word2Vec(x1)
</code></pre>

<p>Thanks in advance  </p>
","1043144","","1043144","","2015-11-10 21:09:22","2015-11-11 10:52:51","results from an sqlachemy query as iterator","<python><iterator><sqlalchemy><gensim>","2","5","","","","CC BY-SA 3.0"
"67287740","1","","","2021-04-27 17:15:17","","0","7","<p>I am analyzing text with topic modeling and using Gensim and pyLDAvis. I would like to create a zip file which includes multiple pyLDAvis interactive graphs (with topic models of differing number of topics e.g., one topic model with 8 topics, one with 10 topics, etc.) to share with colleagues. Not that familiar with working with zip files or html files, so any help would be appreciated.</p>
<p>So far I've tried:</p>
<pre><code>zipObj = ZipFile('models.zip', 'w')
zipObj.write('filename.html', pyLDAvis.prepared_data_to_html(vis))
zipObj.close()
</code></pre>
<p>But when I try to access the zip folder, I get that access is denied.</p>
<p>Does anyone have a solution? Thanks.</p>
","15778079","","15778079","","2021-04-27 17:23:55","2021-04-27 17:23:55","Create ZipFile of pyLDAvis","<python><gensim><zipfile><topic-modeling><pyldavis>","0","0","","","","CC BY-SA 4.0"
"65612062","1","65617717","","2021-01-07 12:03:11","","0","223","<p>I need to <strong>add and subtract word vectors</strong>, for a project in which I use <strong><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">gensim.models.KeyedVectors</a></strong> (from the <code>word2vec-google-news-300</code> model)</p>
<p>Unfortunately, I've tried but can't manage to do it correctly.</p>
<p>Let's look at the poular example <em>queen ~= king - man + woman</em>.<br />
When I want to subtract <em>man</em> from <em>king</em> and add <em>woman</em>,<br />
I can do this with gensim by</p>
<pre class=""lang-py prettyprint-override""><code># model is loaded using gensim.models.KeyedVectors.load()
model.wv.most_similar(positive=[&quot;king&quot;, &quot;woman&quot;], negative=[&quot;man&quot;])[0]
</code></pre>
<p>which, as expected, returns <code>('queen', 0.7118192911148071)</code> for the model I use.</p>
<p>Now, to achieve the same with adding and subtracting vectors (all of them are unit-normed), I've tried the following code:</p>
<pre class=""lang-py prettyprint-override""><code> vec1, vec2, vec3 = model.wv[&quot;king&quot;], model.wv[&quot;man&quot;], model.wv[&quot;woman&quot;]
 result = model.similar_by_vector(vec1 - vec2 + vec3)[0]
</code></pre>
<p><code>result</code> in the code above is <code>('king', 0.7992597222328186)</code> which is not what I'd expect.</p>
<p><em>What is my mistake?</em></p>
","13776689","","13776689","","2021-01-08 08:45:37","2021-01-08 19:07:54","How do I subtract and add vectors with gensim KeyedVectors?","<python><nlp><gensim><word2vec><vector-space>","1","0","","","","CC BY-SA 4.0"
"33702450","1","33702676","","2015-11-13 21:59:05","","0","1005","<p>I'm running Anaconda Python 2.7 on Windows. I've installed gensim and pyLDAvis to do some topic modeling. (Note installing pyLDAvis on python 2.7 in windows is a little tricky as you have to make sure you are not using scikit-bio which doesn't appear to compile on Windows 2.7... I think I have a workaround for this, but I can't try it because of reasons to be outlined below!)</p>

<p>So I got pyLDAvis to install. However when running, it seems to have a problem with an import statement. </p>

<p>pyLDAvis is installed in this folder....</p>

<pre><code>C:\Anaconda2\Lib\site-packages\pyLDAvis-1.3.2-py2.7.egg\pyLDAvis
</code></pre>

<p><code>sys.path</code> returns this:</p>

<pre><code>['',
'C:\\Anaconda2\\lib\\site-packages\\pyldavis-1.3.2-py2.7.egg',
'C:\\Anaconda2\\lib\\site-packages\\joblib-0.9.3-py2.7.egg',
'C:\\Anaconda2\\python27.zip',
'C:\\Anaconda2\\DLLs',
'C:\\Anaconda2\\lib',
'C:\\Anaconda2\\lib\\plat-win',
'C:\\Anaconda2\\lib\\lib-tk',
'C:\\Anaconda2',
'C:\\Anaconda2\\Library\\bin',
'c:\\anaconda2\\lib\\site-packages\\sphinx-1.3.1-py2.7.egg',
'c:\\anaconda2\\lib\\site-packages\\setuptools-18.4-py2.7.egg',
'C:\\Anaconda2\\lib\\site-packages',
'C:\\Anaconda2\\lib\\site-packages\\cryptography-1.0.2-py2.7-win-amd64.egg',
'C:\\Anaconda2\\lib\\site-packages\\win32',
'C:\\Anaconda2\\lib\\site-packages\\win32\\lib',
'C:\\Anaconda2\\lib\\site-packages\\Pythonwin',
'C:\\Anaconda2\\lib\\site-packages\\IPython\\extensions']
</code></pre>

<p>What is happening is that when I try to run <code>pyLDAvis</code>, the library calls <code>import gensim</code>. However, <code>gensim</code> is both a folder in the <code>site-packages</code> and a file (<code>gensim.py</code>) inside <code>pyLDAvis</code>. </p>

<p>So when python tries to <code>import gensim</code> inside the <code>pyLDAvis</code> module, it imports the <code>gensim.py</code> file within the <code>pyLDAvis</code> module, not the ``gensim<code>folder inside</code>site-packages`. </p>

<p>How do I go about fixing this? </p>

<p>Thanks. </p>
","3502161","","","","","2015-11-13 22:22:51","import gensim imports a file in an active module, not the root site-packages folder","<python><python-2.7><gensim>","1","0","","","","CC BY-SA 3.0"
"58839049","1","58864397","","2019-11-13 14:12:14","","0","144","<p>So, I have a keyword list lowercase. Let's say </p>

<pre><code>keywords = ['machine learning', 'data science', 'artificial intelligence']
</code></pre>

<p>and a list of texts in lowercase. Let's say</p>

<pre><code>texts = [
  'the new machine learning model built by google is revolutionary for the current state of artificial intelligence. it may change the way we are thinking', 
  'data science and artificial intelligence are two different fields, although they are interconnected. scientists from harvard are explaining it in a detailed presentation that could be found on our page.'
]
</code></pre>

<p>I need to transform the texts into:</p>

<pre><code>[[['the', 'new',
   'machine_learning',
   'model',
   'built',
   'by',
   'google',
   'is',
   'revolutionary',
   'for',
   'the',
   'current',
   'state',
   'of',
   'artificial_intelligence'],
  ['it', 'may', 'change', 'the', 'way', 'we', 'are', 'thinking']],
 [['data_science',
   'and',
   'artificial_intelligence',
   'are',
   'two',
   'different',
   'fields',
   'although',
   'they',
   'are',
   'interconnected'],
  ['scientists',
   'from',
   'harvard',
   'are',
   'explaining',
   'it',
   'in',
   'a',
   'detailed',
   'presentation',
   'that',
   'could',
   'be',
   'found',
   'on',
   'our',
   'page']]]
</code></pre>

<p>What I do right now is checking if the keywords are in a text and replace them with the keywords with _. But this is of complexity m*n and it is really slow when you have 700 long texts and 2M keywords as in my case.</p>

<p>I was trying to use Phraser, but I can't manage to build one with only my keywords.</p>

<p>Could someone suggest me a more optimized way of doing it?</p>
","7185821","","130288","","2019-11-13 22:18:55","2019-11-14 19:27:32","Python connect composed keywords in texts","<python><data-science><gensim>","2","3","","","","CC BY-SA 4.0"
"48137617","1","","","2018-01-07 13:31:36","","1","431","<p>I have a word2vec model trained on twitter. I imported it into gensim using</p>



<pre class=""lang-py prettyprint-override""><code>from gensim.models.keyedvectors import KeyedVectors
word_vectors = KeyedVectors.load_word2vec_format('./twitter.txt', binary=False)  
</code></pre>

<p>I would like to use a function similar to this one:</p>

<pre class=""lang-py prettyprint-override""><code>word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])
</code></pre>

<p>to show the most similar words, but I want to restrict the results to words that start with a hashtag.
Can somebody please give explain how I can accomplish this?</p>
","9184244","","712995","","2018-01-07 16:24:38","2018-01-07 16:24:38","Gensim word2vec most_similar filtering by # prefix","<python><machine-learning><nlp><word2vec><gensim>","1","0","","","","CC BY-SA 3.0"
"31003021","1","","","2015-06-23 12:37:07","","0","730","<p>I am trying to run exactly the same code, once at my macbook pro and once at Ubuntu machine at AWS.</p>

<p>My code looks just like this (It uses MultinomialNB() from scikit learn):</p>

<pre><code>clf = MultinomialNB()
clf.fit(vectorized_data, labels)
</code></pre>

<p>On my macbook model training goes well, but on the Ubuntu machine I am getting:</p>

<pre><code>&lt;ipython-input-5-c52751e2119e&gt; in &lt;module&gt;()
----&gt; 1 m.train_models()

/home/ubuntu/topic_modeling/classification.pyc in train_models(self, minimal)
133                 continue
134             bm = BinaryModel(label)
--&gt; 135         bm.train_models(self.vectorizer, self.data)
136             self.models.append(bm)
137             logger.info(""Successfully trained model for the %s tag"", label)

/home/ubuntu/topic_modeling/classification.pyc in train_models(self, vectorizer, data)
 92             # TODO some more complex grid search should be here
 93             clf = MultinomialNB()
---&gt; 94         clf.fit(vectorized_data, labels)
 95             self.models.append(clf)
 96
/home/ubuntu/.virtualenvs/topics/local/lib/python2.7/site-packages/sklearn/naive_bayes.pyc in fit(self, X, y, sample_weight)
472             Returns self.
473         """"""
--&gt; 474     X, y = check_X_y(X, y, 'csr')
475         _, n_features = X.shape
476

/home/ubuntu/.virtualenvs/topics/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric)
442     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,
443                     ensure_2d, allow_nd, ensure_min_samples,
--&gt; 444                 ensure_min_features)
445     if multi_output:
446         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,

/home/ubuntu/.virtualenvs/topics/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)
342             else:
343                 dtype = None
--&gt; 344     array = np.array(array, dtype=dtype, order=order, copy=copy)
345         # make sure we actually converted to numeric:
346         if dtype_numeric and array.dtype.kind == ""O"":

ValueError: setting an array element with a sequence.
</code></pre>

<p>I'd like to run the training on the Ubuntu machine to be able to run it in screen.</p>

<p>When I try <code>pip freeze</code> than on both machines it looks exactly the same. 
Does anyone has some idea what can be possibly wrong?</p>

<p><strong>EDIT</strong></p>

<p><code>labels</code> is just list of 0 and 1, eg. <code>[0, 1, 0, 0, 0, 1]</code></p>

<p><code>vectorized_data</code> is obtained using gensim framework. First tokenizing text, then converting it to bow by:</p>

<pre><code>bow_text = self.dictionary.doc2bow(tokenized_text)
self.tfidf = models.TfidfModel(dictionary=self.dictionary)
gensim.matutils.sparse2full(self.tfidf[bow_text], self.tfidf.num_nnz)
</code></pre>
","1499038","","1499038","","2015-06-24 08:01:12","2015-06-24 08:01:12","ValueError: setting an array element with a sequence. Scikit learn","<python><ubuntu><scikit-learn><gensim>","0","2","","","","CC BY-SA 3.0"
"22079418","1","","","2014-02-27 20:18:06","","3","5871","<p>The gensim dictionary object has a very nice filtering function to remove tokens that appear in fewer than a set amount of documents. However, I am looking to remove tokens that occur exactly once in the <em>corpus</em>. Does anyone know of a quick and easy way to do this?</p>
","2993589","","","","","2017-02-07 21:33:53","filter out tokens that occur exactly once in a gensim dictionary","<python-2.7><gensim>","4","0","","","","CC BY-SA 3.0"
"65573173","1","65573827","","2021-01-05 04:14:57","","0","181","<p>I have around 82 gzipped files (around 180MB each and 14GB total) where each file contains new line separated sentences. I am thinking of using <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.PathLineSentences"" rel=""nofollow noreferrer"">PathLineSentences</a> from gensim Word2Vec to train word2vec model on the vocabularies. In that way <a href=""https://stackoverflow.com/questions/58925659/how-to-incrementally-train-a-word2vec-model-with-new-vocabularies"">I do not have to worry about taking all the sentences</a> list into the RAM.</p>
<p>Now I also wanted to get the embedding to include multiword phrases. But from the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#embeddings-with-multiword-ngrams"" rel=""nofollow noreferrer"">documentation</a>, it seems that I need to have an already trained phrase detector an all the sentences I have e.g.</p>
<pre><code>from gensim.models import Phrases
# Train a bigram detector.
bigram_transformer = Phrases(all_sentences)
# Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.
model = Word2Vec(bigram_transformer[all_sentences], min_count=1)
</code></pre>
<p>Now, I have two questions:</p>
<ol>
<li>Is there any way I can do the Phrase Detection while running the Word2Vec on top of each of the individual files in a streaming manner?</li>
<li>If not, is there any way I can do the initial phrase detection in the similar fashion of PathLineSentences, as in doing the phrase detection in a streaming manner?</li>
</ol>
","2529269","","2529269","","2021-01-05 04:27:08","2021-01-05 05:44:17","Embedding multiword ngram phrases with PathLineSentences in gensim word2vec","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"67297183","1","67303785","","2021-04-28 09:09:09","","1","60","<p>What is the difference between <strong>vectorSize</strong> in <strong>Word2Vec</strong> and <strong>numFeatures</strong> in <strong>HashingTF</strong>? I refer to class <em>Word2Vec</em> and <em>HashingTF</em> in pyspark:</p>
<p><strong>WORD2VEC</strong>: class pyspark.ml.feature.Word2Vec(*, <strong>vectorSize=100</strong>, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000)</p>
<p><strong>HashingTF</strong>:  class pyspark.ml.feature.HashingTF(*, <strong>numFeatures=262144</strong>, binary=False, inputCol=None, outputCol=None)</p>
","13715723","","13715723","","2021-04-28 10:22:39","2021-04-28 16:00:14","Difference between VectorSize in word2Vec and numFeatures in TF-IDF","<python><gensim><word2vec><tf-idf>","1","0","0","","","CC BY-SA 4.0"
"30971935","1","31551863","","2015-06-22 03:55:48","","0","1796","<p>I seem to be one of the many people struggling to install gensim on windows. I have trawled through countless forums but the errors poster there never appear to match my errors. So hopefully someone can point me in the right direction! </p>

<p>I am running Windows Server 2012 R2 Standard 64-bit. I have installed MinGW &amp; Anaconda 2.2.0 (64-bit), which comes with Python 2.7.9. </p>

<p>I have added a file distutils.cfg into C:\Users\Sam\Anaconda\Lib\distutils  with the contents:</p>

<pre><code>[build]
compiler=mingw32
</code></pre>

<p>I have added C:\MinGW\bin to my Environment variables. </p>

<p>If I install gensim using pip I do not get any errors, until I try to run Word2Vec when I get the error:</p>

<pre><code>C:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\lib\site-packages\gensim\models\word2vec.py:459: UserWarning: C extension com
pilation failed, training will be slow. Install a C compiler and reinstall gensim for fast training.
</code></pre>

<p>So I have uninstalled gensim and tried to re-install using the mingw32 compiler, but this gives me this error:</p>

<pre><code>python setup.py build --compiler=mingw32
c:\users\sam.passmore\appdata\local\continuum\anaconda\lib\site-packages\setuptools-14.3-py2.7.egg\setuptools\dist.py:282: UserWarni
ng: Normalizing '0.11.1-1' to '0.11.1.post1'
running build
running build_ext
building 'gensim.models.word2vec_inner' extension
C:\MinGW\bin\gcc.exe -DMS_WIN64 -mdll -O -Wall -Igensim\models -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\include -IC:
\Users\sam.passmore\AppData\Local\Continuum\Anaconda\PC -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\lib\site-packages\n
umpy\core\include -c ./gensim/models/word2vec_inner.c -o build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o
gcc: error: ./gensim/models/word2vec_inner.c: No such file or directory
gcc: fatal error: no input files
compilation terminated.
command 'C:\\MinGW\\bin\\gcc.exe' failed with exit status 1
setup.py:82: UserWarning:
********************************************************************
WARNING: %s could not
be compiled. No C extensions are essential for gensim to run,
although they do result in significant speed improvements for some modules.
%s

Here are some hints for popular operating systems:

If you are seeing this message on Linux you probably need to
install GCC and/or the Python development package for your
version of Python.

Debian and Ubuntu users should issue the following command:

    $ sudo apt-get install build-essential python-dev

RedHat, CentOS, and Fedora users should issue the following command:

    $ sudo yum install gcc python-devel

If you are seeing this message on OSX please read the documentation
here:

http://api.mongodb.org/python/current/installation.html#osx
********************************************************************
The gensim.models.word2vec_inner extension moduleThe output above this warning shows how the compilation failed.
  ""The output above this warning shows how the compilation failed."")
building 'gensim.models.doc2vec_inner' extension
C:\MinGW\bin\gcc.exe -DMS_WIN64 -mdll -O -Wall -Igensim\models -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\include -IC:
\Users\sam.passmore\AppData\Local\Continuum\Anaconda\PC -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\lib\site-packages\n
umpy\core\include -c ./gensim/models/doc2vec_inner.c -o build\temp.win-amd64-2.7\Release\.\gensim\models\doc2vec_inner.o
gcc: error: ./gensim/models/doc2vec_inner.c: No such file or directory
gcc: fatal error: no input files
compilation terminated.
command 'C:\\MinGW\\bin\\gcc.exe' failed with exit status 1
setup.py:82: UserWarning:
********************************************************************
WARNING: %s could not
be compiled. No C extensions are essential for gensim to run,
although they do result in significant speed improvements for some modules.
%s

Here are some hints for popular operating systems:

If you are seeing this message on Linux you probably need to
install GCC and/or the Python development package for your
version of Python.

Debian and Ubuntu users should issue the following command:

    $ sudo apt-get install build-essential python-dev

RedHat, CentOS, and Fedora users should issue the following command:

    $ sudo yum install gcc python-devel

If you are seeing this message on OSX please read the documentation
here:

http://api.mongodb.org/python/current/installation.html#osx
********************************************************************
The gensim.models.doc2vec_inner extension moduleThe output above this warning shows how the compilation failed.
  ""The output above this warning shows how the compilation failed.""
</code></pre>

<p>I have exhausted all options I can think of or find, so if anyone could give some advice it would be much appreciated. </p>
","1544746","","1544746","","2015-06-22 04:01:21","2015-07-22 00:24:30","Gensim with MinGW","<python><windows><python-2.7><gensim>","1","3","2","","","CC BY-SA 3.0"
"31011061","1","31011175","","2015-06-23 18:48:58","","2","3096","<p>I am working with the gensim dictionary. For example, you can print <code>print(dictionary.token2id)</code>, as shown here <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">https://radimrehurek.com/gensim/tut1.html</a>. I can also <code>print dictionary</code>:</p>

<p><code>Dictionary(7 unique tokens: [u'nra', u'canon', u'deuterium', u'ion', u'facebook']...)</code></p>

<p>How do I access the key value pairs of the dictionary object, however? </p>
","2795733","","","","","2017-11-02 17:32:24","Access key value pairs in gensim dictionary","<python><gensim>","1","0","","","","CC BY-SA 3.0"
"66591464","1","66687384","","2021-03-11 22:25:55","","0","32","<p>I am working with gensim LDA model for a project. I cant seem to find a proper number of topics. My question is, just to be sure, every time I train the model it re-starts, right?
For example, I try it out with 47 topics, terrible results; so then I go back to the cell and change 47 to 80 topics and run it again. It completely starts a new training and erases what it has learned with the 47 topics, right?</p>
<p>I am having terrible results with LDA, similarity comes to 100% or 0% and I am having trouble parameter tuning. LSI has given me excellent results.
Thanks!</p>
","12448331","","","","","2021-03-18 08:33:32","gensim LDA training","<python><nlp><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"47235153","1","47268964","","2017-11-11 06:39:27","","0","30","<p>When I execute the below code</p>

<pre><code>sim_model = gensim.similarities.MatrixSimilarity(corp)
sim_model.save(""sim_model.pkl"")
</code></pre>

<p>Instead of getting ""sim_model.pkl"" I get two files ""sim_model.pkl.index.npy"" and ""sim_model.pkl"" why is this behavior.</p>
","2478236","","130288","","2017-11-13 16:24:06","2017-11-13 16:24:06","Suffixes being added to extra model files during save","<python-2.7><gensim>","1","0","","","","CC BY-SA 3.0"
"56369258","1","","","2019-05-29 22:17:35","","0","64","<p>Gensim Word2Vec that I've trained lacks vectors for some words. That is, although I have a word ""yuval"" as an input, the model lacks a vector for it. What is the cause?</p>
","9220786","","","","","2019-05-30 07:03:25","Gensim Word2Vec lacks vectors for input words","<gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"48225845","1","59000722","","2018-01-12 11:56:34","","2","1158","<p>I have the following problem:</p>

<p>In English language my code generates successful word embeddings with Gensim, and similar phrases are close to each other considering cosine distance:</p>

<p>The angle between ""Response time and error measurement"" and ""Relation of user perceived response time to error measurement"" is very small, thus they are the most similar phrases in the set.</p>

<p><a href=""https://i.stack.imgur.com/Seuzy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Seuzy.png"" alt=""enter image description here""></a></p>

<p>However, when I use the same phrases in Portuguese, it doesn't work:</p>

<p><a href=""https://i.stack.imgur.com/0l2Sz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0l2Sz.png"" alt=""enter image description here""></a></p>

<p>My code as follows:</p>

<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
import matplotlib.pyplot as plt
from gensim import corpora
documents = [""Interface m√°quina humana para aplica√ß√µes computacionais de laborat√≥rio abc"",
          ""Um levantamento da opini√£o do usu√°rio sobre o tempo de resposta do sistema inform√°tico"",
           ""O sistema de gerenciamento de interface do usu√°rio EPS"",
           ""Sistema e testes de engenharia de sistemas humanos de EPS"",
           ""Rela√ß√£o do tempo de resposta percebido pelo usu√°rio para a medi√ß√£o de erro"",
           ""A gera√ß√£o de √°rvores n√£o ordenadas bin√°rias aleat√≥rias"",
           ""O gr√°fico de interse√ß√£o dos caminhos nas √°rvores"",
           ""Gr√°fico de menores IV Largura de √°rvores e bem quase encomendado"",
           ""Gr√°ficos menores Uma pesquisa""]

stoplist = set('for a of the and to in on'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
for document in documents]
texts

from collections import defaultdict
frequency = defaultdict(int)

for text in texts:
    for token in text:
        frequency[token] += 1
frequency

from nltk import tokenize  
texts=[tokenize.word_tokenize(documents[i], language='portuguese') for i in range(0,len(documents))]

from pprint import pprint
pprint(texts)

dictionary = corpora.Dictionary(texts)
dictionary.save('/tmp/deerwester.dict')
print(dictionary)

print(dictionary.token2id)


# VECTOR
new_doc = ""Tempo de resposta e medi√ß√£o de erro""
new_vec = dictionary.doc2bow(new_doc.lower().split())
print(new_vec)

## VETOR OF PHRASES
corpus = [dictionary.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  
print(corpus)

from gensim import corpora, models, similarities
tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model

### PHRASE COORDINATES
frase=tfidf[new_vec]
print(frase)

corpus_tfidf = tfidf[corpus]
for doc in corpus_tfidf:
    print(doc)

lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)
corpus_lsi = lsi[corpus_tfidf]

lsi.print_topics(2)

## TEXT COORDINATES
todas=[]
for doc in corpus_lsi:
    todas.append(doc)
todas

from gensim import corpora, models, similarities
dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')
corpus = corpora.MmCorpus('/tmp/deerwester.mm')
print(corpus)

lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)

doc = new_doc
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lsi = lsi[vec_bow]
print(vec_lsi)

p=[]
for i in range(0,len(documents)):
    doc1 = documents[i]
    vec_bow2 = dictionary.doc2bow(doc1.lower().split())
    vec_lsi2 = lsi[vec_bow2]
    p.append(vec_lsi2)

p

index = similarities.MatrixSimilarity(lsi[corpus])

index.save('/tmp/deerwester.index')
index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')

sims = index[vec_lsi]
print(list(enumerate(sims)))

sims = sorted(enumerate(sims), key=lambda item: -item[1])
print(sims) 

#################

import gensim
import numpy as np
import matplotlib.colors as colors
import matplotlib.cm as cmx
import matplotlib as mpl

matrix1 = gensim.matutils.corpus2dense(p, num_terms=2)
matrix3=matrix1.T
matrix3[0]
ss=[]
for i in range(0,9):
    ss.append(np.insert(matrix3[i],0,[0,0]))
matrix4=ss
matrix4

matrix2 = gensim.matutils.corpus2dense([vec_lsi], num_terms=2)
matrix2=np.insert(matrix2,0,[0,0])
matrix2

DATA=np.insert(matrix4,0,matrix2)
DATA=DATA.reshape(10,4)
DATA

names=np.array(documents)
names=np.insert(names,0,new_doc)
new_doc
cmap = plt.cm.jet

cNorm  = colors.Normalize(vmin=np.min(DATA[:,3])+.2, vmax=np.max(DATA[:,3]))

scalarMap = cmx.ScalarMappable(norm=cNorm,cmap=cmap)
len(DATA[:,1])

plt.subplots()
plt.figure(figsize=(12,9))
plt.scatter(matrix1[0],matrix1[1],s=60)
plt.scatter(matrix2[2],matrix2[3],color='r',s=95)
for idx in range(0,len(DATA[:,1])):
    colorVal = scalarMap.to_rgba(DATA[idx,3])
    plt.arrow(DATA[idx,0],
          DATA[idx,1], 
          DATA[idx,2], 
          DATA[idx,3], 
          color=colorVal,head_width=0.002, head_length=0.001)
for i,names in enumerate (names):
    plt.annotate(names, (DATA[i][2],DATA[i][3]),va='top')
plt.title(""PHRASE SIMILARITY - WORD2VEC with GENSIM library"")
plt.xlim(min(DATA[:,2]-.2),max(DATA[:,2]+1))
plt.ylim(min(DATA[:,3]-.2),max(DATA[:,3]+.3))
plt.show()
</code></pre>

<p>My question is: is there any additional set up for Gensim to generate proper word embeddings in Portuguese language or Gensim does not support this language?</p>
","6901690","","6901690","","2018-01-12 12:11:19","2019-11-22 19:38:39","How to generate word embeddings in Portuguese using Gensim?","<python><nlp><nltk><gensim>","1","0","","","","CC BY-SA 3.0"
"50530747","1","50557739","","2018-05-25 13:50:44","","2","982","<p>In Gensim's doc2vec implementation, <code>gensim.models.keyedvectors.Doc2VecKeyedVectors.most_similar</code> returns the tags and cosine similarity of the documents most similar to the query document. What if I want the actual documents themselves and not the tags? Is there a way to do that directly without searching for the document associated with the tag returned by <code>most_similar</code>?</p>

<p>Also, is there documentation on this? I can't seem to find the documentation for half of Gensim's classes.</p>
","2980717","","","","","2018-05-28 00:31:24","Gensim doc2vec most_similar equivalent to get full documents","<python-3.x><nlp><text-mining><gensim><doc2vec>","1","0","1","","","CC BY-SA 4.0"
"58961983","1","58966447","","2019-11-20 19:30:20","","3","3470","<p>In Gensim's <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html"" rel=""nofollow noreferrer"">documentation</a>, it says:</p>

<blockquote>
  <p>You can save trained models to disk and later load them back, either to continue training on new training documents or to transform new documents.</p>
</blockquote>

<p>I would like to do this with a dictionary, corpus and tf.idf model. However, the documentation seems to say that it is possible, without explaining how to save these things and load them back up again.</p>

<p>How do you do this?</p>

<hr>

<p>I've been using Pickle, but don't know if this is right...</p>

<pre><code>import pickle
pickle.dump(tfidf, open(""tfidf.p"", ""wb""))
tfidf_reloaded = pickle.load(open(""tfidf.p"", ""rb""))
</code></pre>
","6515530","","6515530","","2019-11-20 19:38:13","2020-04-29 20:17:51","How do you save a model, dictionary and corpus to disk in Gensim, and then load them again?","<python><nlp><gensim>","3","0","2","","","CC BY-SA 4.0"
"66599405","1","","","2021-03-12 11:52:24","","0","73","<p>I am working on a text generation using seq2seq model where GloVe embedding is being used. I want to use a custom Word2Vec (CBOW/Gensim) embedding in this code. Can anyone please help to use my custom embedding instead of GloVe?</p>
<pre><code>    def initialize_embeddings(self):
        &quot;&quot;&quot;Reads the GloVe word-embeddings and creates embedding matrix and word to index and index to word mapping.&quot;&quot;&quot;
        
        # load the word embeddings
        self.word2vec = {}
        with open(glove_path%self.EMBEDDING_DIM, 'r') as file:
            for line in file:
                vectors = line.split()
                self.word2vec[vectors[0]] = np.asarray(vectors[1:], dtype=&quot;float32&quot;)```

                
        ```# get the embeddings matrix
        self.num_words = min(self.MAX_VOCAB_SIZE, len(self.word2idx)+1)
        self.embeddings_matrix = np.zeros((self.num_words, self.EMBEDDING_DIM))
        
        for word, idx in self.word2idx.items():
            if idx &lt;= self.num_words:
                word_embeddings = self.word2vec.get(word)
                if word_embeddings is not None:
                    self.embeddings_matrix[idx] = word_embeddings
                    
        self.idx2word = {v:k for k,v in self.word2idx.items()}
</code></pre>
<p>This code is for GloVe embedding which is transformed to Word2Vec. I want to load my own Word2Vec embedding.</p>
","15377660","","15377660","","2021-03-13 12:12:25","2021-09-01 17:51:01","Use custom Word2Vec embedding instead of GloVe","<keras><stanford-nlp><gensim><word2vec><seq2seq>","1","1","","","","CC BY-SA 4.0"
"67290144","1","","","2021-04-27 20:19:58","","0","23","<p>How to get insights into my created word or document embeddings?</p>
<p>For example if I extract features with the <code>TF-IDF Vectorizer</code>, I can output the top n best features. Is there a similar approach where I can gain knowledge about the model?</p>
","11469656","","","","","2021-04-27 20:19:58","Insights into Word/Document Embeddings","<gensim><word2vec><word-embedding><doc2vec>","0","9","","","","CC BY-SA 4.0"
"50635465","1","","","2018-06-01 02:53:05","","0","336","<p>When Word2Vec model is trained, there are three outputs created.</p>

<ul>
<li>model</li>
<li>model.wv.syn0</li>
<li>model.syn1neg</li>
</ul>

<p>I have a couple of questions regarding these models.</p>

<ol>
<li><p>How are these outputs essentially different from each other?</p></li>
<li><p>Which model to look at if I want to access trained results? </p></li>
</ol>

<p>Thanks in advance !</p>
","9002358","","130288","","2018-06-01 22:23:26","2018-06-01 22:23:26","Word2Vec model output types","<word2vec><gensim>","1","0","","","","CC BY-SA 4.0"
"48164954","1","48171610","","2018-01-09 09:25:03","","1","117","<p>I have a dataset with documents separated into different years, and my objective is to train an embedding model for each year's data, while at the same time, the same word appearing in different years will have similar vector representations. Like this: for word 'compute', its vector in year 1 is</p>

<pre><code>[0.22, 0.33, 0.20]
</code></pre>

<p>and in year 2 it's something around:</p>

<pre><code>[0.20, 0.35, 0.18]
</code></pre>

<p>Is there a way to accomplish this? For example, train the model of year 2 with both initial values (if the word is trained already in year 1, modify its vector) and randomness (if this is a new word for the corpus).</p>
","3455398","","712995","","2018-01-09 17:36:21","2018-01-09 17:36:21","Gensim word embedding training with initial values","<machine-learning><nlp><word2vec><gensim><word-embedding>","1","0","","","","CC BY-SA 3.0"
"30973503","1","30985492","","2015-06-22 06:41:44","","0","13874","<p>I am trying to perform tfidf on a matrix. I would like to use gensim, but <code>models.TfidfModel()</code> only works on a corpus and therefore returns a list of lists of varying lengths (I want a matrix).</p>

<p>The options are to somehow fill in the missing values of the list of lists, or just convert the corpus to a matrix </p>

<pre><code>numpy_matrix = gensim.matutils.corpus2dense(corpus, num_terms=number_of_corpus_features)
</code></pre>

<p>Choosing the latter, I then try to convert this count matrix to a tf-idf weighted matrix:</p>

<pre><code>def TFIDF(m):
    #import numpy
    WordsPerDoc = numpy.sum(m, axis=0)
    DocsPerWord = numpy.sum(numpy.asarray(m &gt; 0, 'i'), axis=1)
    rows, cols = m.shape
    for i in range(rows):
        for j in range(cols):
            amatrix[i,j] = (amatrix[i,j] / WordsPerDoc[j]) * log(float(cols) /     DocsPerWord[i])
</code></pre>

<p>But, I get the error <code>AttributeError: 'numpy.ndarray' object has no attribute 'A'</code></p>

<p>I copied the function above from another script. It was:</p>

<pre><code>def TFIDF(self):
    WordsPerDoc = sum(self.A, axis=0)        
    DocsPerWord = sum(asarray(self.A &gt; 0, 'i'), axis=1)
    rows, cols = self.A.shape
    for i in range(rows):
       for j in range(cols):
          self.A[i,j] = (self.A[i,j] / WordsPerDoc[j]) * log(float(cols) / DocsPerWord[i])
</code></pre>

<p>Which I believe is where it's getting the <code>A</code> from. However, I re-imported the function. </p>

<p>Why is this happening?</p>
","2795733","","3355825","","2015-06-22 06:46:47","2015-06-22 16:47:29","AttributeError: 'numpy.ndarray' object has no attribute 'A'","<python><numpy><matrix><gensim>","1","2","","","","CC BY-SA 3.0"
"67203029","1","","","2021-04-21 20:27:53","","0","25","<p>I have some technical documents that I need to extract the text from regarding a specific set of procedures.  Is there an easy off-the-shelf way to 'show' a language model the text to be extracted and each of the documents and then have it extract that text programmatically?</p>
<p>I was thinking of taking each paragraph and taking like a mean of all the word embeddings within the paragraph to create a 'paragraph embedding' and essentially comparing those to the 'paragraph embeddings' of the training set extracted text but I didn't know if there was a more robust way of doing that.</p>
","12008028","","","","","2021-04-21 20:27:53","Is there an NLP framework for extracting text from larger content","<python><nlp><spacy><gensim><nlu>","0","4","","","","CC BY-SA 4.0"
"65690944","1","","","2021-01-12 19:42:11","","0","64","<p>I am trying to implement Gensim's <code>most_similar</code> function by hand but calculate the similarity between the query word and just one other word (avoiding the time to calculate it for the query word with <em>all</em> other words). So far I use</p>
<pre><code>cossim = (np.dot(a, b)
                   / np.linalg.norm(a)
                   / np.linalg.norm(b))
</code></pre>
<p>and this is the same as the similarity result between <code>a</code> and <code>b</code>. I find this works almost exactly but that some precision is lost, for example</p>
<pre><code>from gensim.models.word2vec import Word2Vec
import gensim.downloader as api

model_gigaword = api.load(&quot;glove-wiki-gigaword-300&quot;)

a = 'france'
b = 'chirac'

cossim1 = model_gigaword.most_similar(a)
import numpy as np
cossim2 = (np.dot(model_gigaword[a], model_gigaword[b])
                   / np.linalg.norm(model_gigaword[a])
                   / np.linalg.norm(model_gigaword[b]))
print(cossim1)
print(cossim2)
</code></pre>
<p>Output:</p>
<pre><code>[('french', 0.7344760894775391), ('paris', 0.6580672264099121), ('belgium', 0.620672345161438), ('spain', 0.573593258857727), ('italy', 0.5643460154533386), ('germany', 0.5567398071289062), ('prohertrib', 0.5564222931861877), ('britain', 0.5553334355354309), ('chirac', 0.5362644195556641), ('switzerland', 0.5320892333984375)]
0.53626436
</code></pre>
<p>So the most_similar function gives 0.53626441955... (rounds to 0.53626442) and the calculation with numpy gives 0.53626436. Similarly, you can see differences between the values for 'paris' and 'italy' (in similarity compared to 'france'). These differences suggest that the calculation is not being done to full precision (but it is in Gensim). How can I fix it and get the output for a single similarity to higher precision, exactly as it comes from most_similar?</p>
<p>TL/DR - I want to use function('france', 'chirac') and get 0.536264<strong>4195556641</strong>, not 0.536264<strong>36</strong>.</p>
<p>Any idea what's going on?</p>
<hr />
<p>UPDATE: I should clarify, I want to know and replicate how most_similar does the computation, but for only one (a,b) pair. That's my priority, rather than finding out how to improve the precision of my cossim calculation above. I just assumed the two were equivalent.</p>
","12384851","","12384851","","2021-01-13 16:20:08","2021-01-13 19:36:21","Word vector similarity precision","<python><numpy><nlp><precision><gensim>","2","3","","","","CC BY-SA 4.0"
"50535278","1","","","2018-05-25 18:51:58","","3","1287","<p>I'm trying to replicate Mikolov's work of PV-DM + PV-DBOW. He says that both algorithms should be used in order to get better results. For this reason I'm trying to train the model and then give the document tags to t-SNE.
Using Gensim's Doc2Vec I can get the document tags with <code>docvecs.vectors_docs</code>, but the concatenated structure doesn't appear to have the document tags of the joint model. It is still treating the models as separate entities.
(This I can see from the variable explorer)</p>

<p>I'm also using the <code>ConcatenatedDoc2Vec</code> from gensim.</p>

<p>Can anyone help me? Is there a way I can get the document tags from the concatenated new entity and not the individual ones?</p>
","9848457","","9758922","","2018-05-25 19:18:07","2018-05-28 00:57:50","Gensim Doc2Vec getting the doc tags from the Concatenated model","<python><model><gensim><doc2vec>","1","0","1","","","CC BY-SA 4.0"
"67419932","1","67421704","","2021-05-06 14:13:42","","-1","58","<p>I'm trying to use pretrained word2vec in Google Colab. Previously I downloaded model onto my C:/, and then uploaded in my Google Drive. However, I get this error I can't seem to find anywhere.</p>
<p>My code is:</p>
<pre><code>from gensim.models import word2vec
import urllib.request

urllib.request.urlretrieve(&quot;https://drive.google.com/file/d/1lgCddPxJC__QA-qGtYTdNNoHRiYWyOpQ/view?usp=sharing/GoogleNews-vectors-negative300.bin&quot;, &quot;GoogleNews-vectors-negative300.bin&quot;)

word2vec_path = 'GoogleNews-vectors-negative300.bin'
word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)
</code></pre>
<p>Error Message:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-354-492ef9dcbbcc&gt; in &lt;module&gt;()
      1 word2vec_path = 'GoogleNews-vectors-negative300.bin'
----&gt; 2 word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)

2 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py in &lt;genexpr&gt;(.0)
    171     with utils.smart_open(fname) as fin:
    172         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 173         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    174         if limit:
    175             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: '&lt;!DOCTYPE'
</code></pre>
","15853961","","","","","2021-05-06 15:52:24","invalid literal for int() with base 10: '<!DOCTYPE","<python><gensim><word2vec>","1","2","","","","CC BY-SA 4.0"
"41758755","1","","","2017-01-20 08:04:02","","1","846","<p>I am currently going through <strong>Gensim</strong> tutorial on <strong>Corpora and Vector Spaces</strong> in that I am right now trying to understand  <a href=""http://radimrehurek.com/gensim/tut1.html#corpus-streaming-one-document-at-a-time"" rel=""nofollow noreferrer""><strong>Corpus Streaming ‚Äì One Document at a Time</strong></a>.</p>

<p>After I compiled these lines of codes referring the above link in python3:</p>

<pre><code>class MyCorpus(object):
    def __iter__(self):
        for line in open('mycorpus.txt'):
            # assume there's one document per line, tokens separated by whitespace
            yield dictionary.doc2bow(line.lower().split())

corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!

print(corpus_memory_friendly)

for vector in corpus_memory_friendly:  # load one vector into memory at a time
    print(vector)
</code></pre>

<p>I am getting this error: </p>

<pre><code>&lt;__main__.MyCorpus object at 0x7f2e37e17d68&gt;

Traceback (most recent call last):
  File ""&lt;pyshell#46&gt;"", line 1, in &lt;module&gt;
    for vector in corpus_memory_friendly:  # load one vector into memory at a time
  File ""&lt;pyshell#41&gt;"", line 3, in __iter__
    for line in open('mycorpus.txt'):
FileNotFoundError: [Errno 2] No such file or directory: 'mycorpus.txt'
</code></pre>

<p>I have already downloaded <strong><em>mycorpus.txt</em></strong> still I am getting this error. Where should I store mycorpus.txt file.</p>

<p>Thanks for your help.</p>
","6782093","","","","","2017-01-20 08:04:02","(Gensim)Python : FileNotFoundError: [Errno 2] No such file or directory: 'mycorpus.txt'","<python><error-handling><gensim><corpus>","0","9","1","","","CC BY-SA 3.0"
"57839264","1","57846100","","2019-09-08 04:47:17","","0","2010","<p>I am using gensim to train a word2Vec model. Here I am passing one sentence at a time to the gensim.models.Word2Vec() method from my corpus to gradually train the model on my whole corpus. But I am confused what should the value of iter parameter be as I'm not sure whether it iterates over the passed sentence n times or the whole corpus.    </p>

<p>I have tried checking the documentation of gensim. it states the definition as follows:<br></p>

<blockquote>
  <p>iter (int, optional) ‚Äì Number of iterations (epochs) over the corpus.</p>
</blockquote>

<p>But I am confused as I am not passing the whole corpus but only a single sentence on each iteration.</p>

<p>My line in the code that trains the model looks like this:<br>
<code>model = gensim.models.Word2Vec(data, min_count=2, window=arg.window_size, size=arg.dim_size, workers=4, sg=0, hs=0, negative=10, ns_exponent=0.75, alpha=0.025, iter=1)</code>
<br>Here ""data"" represents a single sentence passed at a time from a generator. </p>

<p>Suppose I have a corpus of 2 sentences. ""X is a variable. Y is a variable too."". The model receives data = ""X is a variable."" first and data = ""Y is a variable too."" in 2nd iteration. 
Now to clarify, my question is,<b> whether iter = 50 will train my model iterating though ""X is a variable."" 50 times &amp; ""Y is a variable too."" 50 times or will it iterating though ""X is a variable. Y is a variable too."" (my whole corpus) 50 times. 
</b></p>
","8613245","","8613245","","2019-09-08 04:52:37","2019-09-08 22:06:27","Does the ""iter"" parameter of gensim.models.Word2Vec method iterate over the whole corpus or the sentence passed to it at a time?","<python><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"57921755","1","","","2019-09-13 10:19:32","","1","238","<p>I would like to investigate how the TF-IDF score of particular words in document depends on the number of documents on which IDF is based. Unfortunately, the list of results that I receive vary in length and yet the number of words in the document is fixed... How to get TF-IDF results for all words in a document, regardless of the number of modeling documents?</p>

<p>I use the Gensim library to calculate the TF-IDF ratio. Here is my approach:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim import corpora
from gensim import models
from gensim.models import TfidfModel

# Suppose I have a list of words with a random distribution:
docs = [
    ['dog', 'cat', 'panda', 'deer', 'dog', 'elephant', 'panda', 'mouse', 'dog', 'panda', 'dog', 'python', 'penguin', 'lion', 'mouse'],
    ['cat', 'panda', 'rhino', 'lynx', 'panda', 'panda', 'panda', 'koala', 'mammoth', 'hamster', 'cat', 'koala', 'bear', 'fright'],
    ['dog', 'cat', 'elephant', 'panda', 'deer', 'deer', 'baloonfish', 'pig', 'owl', 'dove', 'camel', 'camel', 'camel'],
    ['dog', 'panda', 'mammoth', 'snake', 'lizard', 'elephant', 'partridge', 'alpaca', 'dog', 'dog', 'lizard', 'dog'],
    ['dog', 'owl', 'ostrich', 'porcupine', 'mouse', 'baloonfish', 'croc', 'lion', 'chimp',  'camel', 'doe']
]

# Each document has a certain number of tokens and unique types:
print([len(doc) for doc in docs])  # [15, 14, 13, 12, 11]
print([len(set(doc)) for doc in docs])  # [9, 9, 10, 8, 11]

# I create a dictionary that has 30 unique tokens...
dictionary = corpora.Dictionary(docs)  # ['cat', 'deer', 'dog', 'elephant', 'lion', ...]
# ...and corpus containing individual instances of 5 documents
corpus = [dictionary.doc2bow(doc) for doc in docs]  # [[(0, 1), (1, 1), (2, 4), (3, 1), ...], ...]

# now I'm training tfidf model and applying this model to all corpus documents then I check their length:
model = TfidfModel(corpus)
vector = model[corpus]
print([len(v) for v in vector])  # [9, 9, 10, 8, 11]
</code></pre>

<p>So far so good, but now I would like to compare these results with the results obtained for a smaller number of documents based on which the model is built. In order to do this I do the following:</p>

<pre class=""lang-py prettyprint-override""><code># now I'm training my tfidf model based only on first four documents in corpus:
new_model = TfidfModel(corpus[:4])
new_vector = new_model[corpus]
print([len(v) for v in new_vector])  # [8, 8, 9, 7, 6]

# based on first three:
new_model = TfidfModel(corpus[:3])
new_vector = new_model[corpus]
print([len(v) for v in new_vector])  # [7, 7, 8, 3, 6]

# based on first two:
new_model = TfidfModel(corpus[:2])
new_vector = new_model[corpus]
print([len(v) for v in new_vector])  # [7, 7, 3, 3, 3]
</code></pre>

<p>Can somebody explain to me why the number of results is decreasing? For example, the number of unique tokens in the first document is 9; but when the model is trained on fewer documents, the number of tokens suddenly drops to 8, 7, etc... And yet this document contains a fixed number of tokens. Why not all of them are included in the results? How to include them? Maybe I'm doing something wrong ... I'll be grateful for your help.</p>
","8018223","","8018223","","2019-09-13 10:39:38","2019-09-13 10:39:38","Evaluation of TF-IDF effectiveness in Gensim. Why are the results lists incomplete?","<python><keyword><gensim><tf-idf>","0","0","0","","","CC BY-SA 4.0"
"50651861","1","50653843","","2018-06-01 22:50:23","","3","3337","<p>I am trying to use <code>word2vec</code> in a scikit-learn pipeline.</p>

<pre><code>from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np

class ItemSelector(BaseEstimator, TransformerMixin):
    def __init__(self, key):
        self.key = key

    def fit(self, x, y=None):
        return self

    def transform(self, data_dict):
        return data_dict[self.key]


from sklearn.pipeline import Pipeline
from gensim.sklearn_api import W2VTransformer
pipeline_word2vec = Pipeline([
                ('selector', ItemSelector(key='X')),
                ('w2v', W2VTransformer()),
            ])

pipeline_word2vec.fit(pd.DataFrame({'X':['hello world','is amazing']}), np.array([1,0]))
</code></pre>

<p>this gives me </p>

<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-11-9e2dd309d07c&gt; in &lt;module&gt;()
     23                 ('w2v', W2VTransformer()),
     24             ])
---&gt; 25 pipeline_word2vec.fit(pd.DataFrame({'X':['hello world','is amazing']}), np.array([1,0]))

/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    248         Xt, fit_params = self._fit(X, y, **fit_params)
    249         if self._final_estimator is not None:
--&gt; 250             self._final_estimator.fit(Xt, y, **fit_params)
    251         return self
    252 

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/sklearn_api/w2vmodel.py in fit(self, X, y)
     62             sg=self.sg, hs=self.hs, negative=self.negative, cbow_mean=self.cbow_mean,
     63             hashfxn=self.hashfxn, iter=self.iter, null_word=self.null_word, trim_rule=self.trim_rule,
---&gt; 64             sorted_vocab=self.sorted_vocab, batch_words=self.batch_words
     65         )
     66         return self

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py in __init__(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)
    525             batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window, seed=seed,
    526             hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,
--&gt; 527             fast_version=FAST_VERSION)
    528 
    529     def _do_train_job(self, sentences, alpha, inits):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in __init__(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)
    336             self.train(
    337                 sentences, total_examples=self.corpus_count, epochs=self.epochs, start_alpha=self.alpha,
--&gt; 338                 end_alpha=self.min_alpha, compute_loss=compute_loss)
    339         else:
    340             if trim_rule is not None:

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py in train(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)
    609             sentences, total_examples=total_examples, total_words=total_words,
    610             epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,
--&gt; 611             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)
    612 
    613     def score(self, sentences, total_sentences=int(1e6), chunksize=100, queue_factor=2, report_delay=1):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in train(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)
    567             sentences, total_examples=total_examples, total_words=total_words,
    568             epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,
--&gt; 569             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)
    570 
    571     def _get_job_params(self, cur_epoch):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in train(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)
    239             epochs=epochs,
    240             total_examples=total_examples,
--&gt; 241             total_words=total_words, **kwargs)
    242 
    243         for callback in self.callbacks:

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in _check_training_sanity(self, epochs, total_examples, total_words, **kwargs)
    599 
    600         if not self.wv.vocab:  # should be set by `build_vocab`
--&gt; 601             raise RuntimeError(""you must first build vocabulary before training the model"")
    602         if not len(self.wv.vectors):
    603             raise RuntimeError(""you must initialize vectors before training the model"")

RuntimeError: you must first build vocabulary before training the model
</code></pre>

<p>in a jupyter notebook. Instead I seek a trained model. How can I fix this?</p>
","4099925","","4099925","","2018-06-02 00:25:21","2018-06-02 05:37:18","Using gensim word2vec in scikit-learn pipeline","<python><scikit-learn><word2vec><gensim>","1","0","2","","","CC BY-SA 4.0"
"50655405","1","50792761","","2018-06-02 09:26:26","","10","4846","<p>I have a question related to gensim. I like to know whether it is recommended or necessary to use pickle while saving or loading a model (or multiple models), as I find scripts on GitHub that do either.    </p>

<pre><code>mymodel = Doc2Vec(documents, size=100, window=8, min_count=5, workers=4)
      mymodel.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)
</code></pre>

<p>See <a href=""https://radimrehurek.com/gensim/models/deprecated/doc2vec.html"" rel=""noreferrer"">here</a></p>

<p><strong>Variant 1:</strong></p>

<pre><code>import pickle
# Save
mymodel.save(""mymodel.pkl"")  # Stores *.pkl file
# Load
mymodel = pickle.load(""mymodel.pkl"")
</code></pre>

<p><strong>Variant 2:</strong></p>

<pre><code># Save
model.save(mymodel) # Stores *.model file
# Load
model = Doc2Vec.load(mymodel)
</code></pre>

<p>In <code>gensim.utils</code>, it appears to me that there is a pickle function embedded: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/utils.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/utils.py</a></p>

<p>def save 
  ...
  try:
              _pickle.dump(self, fname_or_handle,   protocol=pickle_protocol)
  ...</p>

<p><strong>Goal of my question:</strong>
I would be glad to learn 1) whether I need pickle (for better memory management) and 2) in case, why it's better than loading *.model files.</p>

<p>Thank you!</p>
","4697646","","6613710","","2018-06-04 16:38:42","2018-06-12 15:27:28","gensim: pickle or not?","<memory><model><pickle><gensim>","2","1","4","","","CC BY-SA 4.0"
"66588108","1","","","2021-03-11 18:07:05","","0","61","<p>I am trying to generate text for which I used custom Gensim Word2Vec embedding. I am trying to fit it instead of GloVe embedding.</p>
<p>Code :</p>
<pre><code>glove_path = &quot;/content/drive/MyDrive/Dataset/Bangla AI/custom_bangla_embedding.txt&quot;

</code></pre>
<pre><code>BATCH_SIZE = 64 # number of data points to consider to train at a single point of time
LATENT_DIM = 200 # the size of the hidden state/vector
EMBEDDING_DIM = 1000 # size of the word embeddings - comes into various sizes 50, 100 or 200
MAX_VOCAB_SIZE = 30000 # the maximum number of words to consider
VALIDATION_SPLIT = 0.2 # % of validation dataset```

</code></pre>
<p>class SequenceGenerator():</p>
<pre><code>def __init__(self, input_lines, target_lines, max_seq_len=None, max_vocab_size=10000, embedding_dim=200):        
    self.input_lines = input_lines
    self.target_lines = target_lines
    
    self.MAX_SEQ_LEN = max_seq_len
    self.MAX_VOCAB_SIZE = max_vocab_size
    self.EMBEDDING_DIM = embedding_dim


def initialize_embeddings(self):
    
    # load the word embeddings
    self.word2vec = {}
    with open(glove_path%self.EMBEDDING_DIM, 'r') as file:
        for line in file:
            vectors = line.split()
            self.word2vec[vectors[0]] = np.asarray(vectors[1:], dtype=&quot;float32&quot;)

            
    # get the embeddings matrix
    self.num_words = min(self.MAX_VOCAB_SIZE, len(self.word2idx)+1)
    self.embeddings_matrix = np.zeros((self.num_words, self.EMBEDDING_DIM))
    
    for word, idx in self.word2idx.items():
        if idx &lt;= self.num_words:
            word_embeddings = self.word2vec.get(word)
            if word_embeddings is not None:
                self.embeddings_matrix[idx] = word_embeddings
                
    self.idx2word = {v:k for k,v in self.word2idx.items()}


def prepare_sequences(self, filters=''):
    
    # train the tokenizer
    self.tokenizer = Tokenizer(num_words=self.MAX_VOCAB_SIZE, filters='')
    self.tokenizer.fit_on_texts(self.input_lines+self.target_lines)
    
    # get the word-index mapping and initialize embeddings
    self.word2idx = self.tokenizer.word_index
    self.initialize_embeddings()
    
    # tokenize the input and target lines
    self.input_sequences = self.tokenizer.texts_to_sequences(self.input_lines)
    self.target_sequences = self.tokenizer.texts_to_sequences(self.target_lines)
    
    # get the max sequence len from the data
    max_seq_len = max(list(map(len, self.input_lines+self.target_lines)))
    if self.MAX_SEQ_LEN:
        self.MAX_SEQ_LEN = min(self.MAX_SEQ_LEN, max_seq_len)
    else:
        self.MAX_SEQ_LEN = max_seq_len
        
    # pad the sequences
    self.input_sequences = pad_sequences(self.input_sequences, maxlen=self.MAX_SEQ_LEN, padding=&quot;post&quot;)
    self.target_sequences = pad_sequences(self.target_sequences, maxlen=self.MAX_SEQ_LEN, padding=&quot;post&quot;)
    
    print(&quot;1st input sequence: &quot;, self.input_sequences[0])
    print(&quot;1st target sequence: &quot;, self.target_sequences[0])
    
    
def one_hot_encoding(self):
    &quot;Creates the One-hot encoding for the target sequence.&quot;
    
    # it will be a 3 dimensional array where
    # first-dim is the number of target lines
    # second-dim is the size of the sequences
    # third-dim is the number of words in the dataset
    self.one_hot_targets = np.zeros((len(self.target_sequences), self.MAX_SEQ_LEN, self.num_words))
    
    for seq_idx, seq in enumerate(self.target_sequences):
        for word_idx, word_id in enumerate(self.target_sequences[seq_idx]):
            if word_id &gt; 0:
                self.one_hot_targets[seq_idx, word_idx, word_id] = 1


def get_closest_word(self, word_vec):
    &quot;&quot;&quot;
        Find the nearest word to the provided vector. The distance between the vectors is 
        calculated using the cosine-distance.
        
        Parameters:
            word_vec (np.array): a vector of size EMBEDDING_DIM
            
        Returns:
            Str: the closest word to the provided vector
    &quot;&quot;&quot;
    
    max_dist = 9999999999
    closest_word = &quot;NULL&quot;
    
    # iterate overall the words and find the closest one
    for word, vec in self.word2vec.items():
        
        # get the cosine distance between the words
        dist = spatial.distance.cosine(word_vec, vec)
        
        # compare the distance and keep the minimum
        if dist &lt; max_dist:
            max_dist = dist
            closest_word = word
    
    return closest_word```
</code></pre>
<h1>create an object of the class</h1>
<pre><code>                           max_vocab_size=MAX_VOCAB_SIZE, embedding_dim=EMBEDDING_DIM)```

# prepare the input &amp; target sequences
```sg_obj.prepare_sequences()```
# create the One-hot encoding on the target sequences
```sg_obj.one_hot_encoding()```

# make sure the tokenized words contains &lt;sos&gt; &amp; &lt;eos&gt;
```assert '&lt;sos&gt;' in sg_obj.word2idx
assert '&lt;eos&gt;' in sg_obj.word2idx```




But getting the following error:

</code></pre>
<p>TypeError                                 Traceback (most recent call last)</p>
<pre><code>```&lt;ipython-input-36-3edfcb198239&gt; in &lt;module&gt;()
    133 
    134 # prepare the input &amp; target sequences
--&gt; 135 sg_obj.prepare_sequences()
    136 # create the One-hot encoding on the target sequences
    137 sg_obj.one_hot_encoding()```

```1 frames

&lt;ipython-input-36-3edfcb198239&gt; in initialize_embeddings(self)
     29         # load the word embeddings
     30         self.word2vec = {}
---&gt; 31         with open(glove_path%self.EMBEDDING_DIM, 'r') as file:
     32             for line in file:
     33                 vectors = line.split()```

```TypeError: not all arguments converted during string formatting
</code></pre>
<p>Looking for kind help. Thanks in advance.</p>
","15377660","","15377660","","2021-03-12 04:03:57","2021-03-12 18:50:21","Gensim Word2Vec Embedding instead of GloVe TypeError: not all arguments converted during string formatting","<python><nlp><gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"58965192","1","","","2019-11-20 23:55:57","","0","538","<p>If I have some documents like this:</p>

<pre><code>doc1 = ""hello hello this is a document""
doc2 = ""this text is very interesting""
documents = [doc1, doc2]
</code></pre>

<p>And I compute a TF-IDF matrix for this in Gensim like this:</p>

<pre><code># create dictionary
dictionary = corpora.Dictionary([simple_preprocess(line) for line in documents])
# create bow corpus
corpus = [dictionary.doc2bow(simple_preprocess(line)) for line in documents]
# create the tf.idf matrix
tfidf = models.TfidfModel(corpus, smartirs='ntc')
</code></pre>

<p>Then for each document, I get a TF-IDF like this:</p>

<pre><code>Doc1: [(""hello"", 0.5), (""a"", 0.25), (""document"", 0.25)]
Doc2: [(""text"", 0.333), (""very"", 0.333), (""interesting"", 0.333)]
</code></pre>

<p>But I want the TF-IDF vector for each document to include words with 0 TF-IDF values (i.e. include every word mentioned in the corpus):</p>

<pre><code>Doc1: [(""hello"", 0.5), (""this"", 0), (""is"", 0), (""a"", 0.25), (""document"", 0.25), (""text"", 0), (""very"", 0), (""interesting"", 0)]
Doc2: [(""hello"", 0), (""this"", 0), (""is"", 0), (""a"", 0), (""document"", 0), (""text"", 0.333), (""very"", 0.333), (""interesting"", 0.333)]
</code></pre>

<p>How can I do this in Gensim? Or maybe there is some other library that can compute a TF-IDF matrix in this fashion (although like Gensim, it needs to be able to handle very large data sets, e.g. I achieved this result in Sci-kit on a small data set, but Sci-kit has memory problems on a large data set).</p>
","6515530","","","","","2019-11-22 20:13:49","How do you include all words from the corpus in a Gensim TF-IDF?","<python><nlp><gensim><text-classification><tf-idf>","1","0","","","","CC BY-SA 4.0"
"67424093","1","","","2021-05-06 18:35:58","","0","37","<p>I use gensim  to count the frequency of words in a given note.</p>
<p>After applying the following code:</p>
<pre><code>from gensim import corpora
dictionary = corpora.Dictionary(sentences) 
corpus = [dictionary.doc2bow(text) for text in sentences]
</code></pre>
<p>Obtains a corpus such as:
[(0, 1), (1, 5), (3, 1) ...]</p>
<p>I would like corpus such as:
[(word_1, 1), (word_2, 5), (word_3, 1) ...]</p>
<p>So I want to get the word instead of id  word in corpus.</p>
<p>Can someone help me how I can get this and then save such a corpus as an excel file?</p>
","11939331","","","","","2021-05-06 19:33:36","Python frequency of words using gensim: How to get the word instead of id word in corpus","<python><text-mining><gensim>","1","0","","","","CC BY-SA 4.0"
"67426039","1","67428904","","2021-05-06 21:14:54","","0","268","<p><strong>This is my code below and the error I have is beneath it but I cant figure out why this is happening. Please share your thoughts: I checked here <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4</a>  but I wasn't able to figure out</strong></p>
<pre><code>word_vec_unpack_idx = [(word, idx.index) for word, idx in \
                   word_vec.wv.key_to_index.items()]
# unpacking vecs tpo create singulrized dataframe 
tokens, indexes = zip(*word_vec_unpack)

word_vec_df = pd.DataFrame(word_vec.wv.syn0[indexes, :], index=tokens)

tokenized_array = np.array(tokenized)
model_array = np.array([word_vec_df.loc[doc].mean(axis=0) for doc in tokenized_array])
model_df = pd.DataFrame(model_array)
# manually adding the label 
model_df[&quot;label&quot;] = df_final[&quot;Classification&quot;]

display(model_df.head())

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-64-8de619ecbc5b&gt; in &lt;module&gt;
----&gt; 1 word_vec_unpack = [(word, idx.index) for word, idx in \
      2                    word_vec.wv.key_to_index.items()]
      3 # unpacking vecs tpo create singulrized dataframe
      4 tokens, indexes = zip(*word_vec_unpack)
      5 

&lt;ipython-input-64-8de619ecbc5b&gt; in &lt;listcomp&gt;(.0)
----&gt; 1 word_vec_unpack = [(word, idx.index) for word, idx in \
      2                    word_vec.wv.key_to_index.items()]
      3 # unpacking vecs tpo create singulrized dataframe
      4 tokens, indexes = zip(*word_vec_unpack)
      5 

AttributeError: 'int' object has no attribute 'index'
</code></pre>
<p><strong>I broke the code down and removed idx so the first part of the code is:</strong>
now the error is gone.</p>
<pre><code>word_vec_unpack = [(word, index) for word, index in \
                   word_vec.wv.key_to_index.items()]
# unpacking vecs tpo create singulrized dataframe 
tokens, indexes = zip(*word_vec_unpack)
</code></pre>
<p><strong>Now I get another error for the second part of the code</strong></p>
<pre><code>word_vec_df = pd.DataFrame(word_vec.wv.syn0[indexes, :], index=tokens)

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-80-a185de0b1b16&gt; in &lt;module&gt;
----&gt; 1 word_vec_df = pd.DataFrame(word_vec.wv.syn0[indexes, :], index=tokens)

AttributeError: 'KeyedVectors' object has no attribute 'syn0'
</code></pre>
","15849966","","15849966","","2021-05-06 21:36:30","2021-05-07 23:36:51","Gensim update to W2vec:AttributeError: 'int' object has no attribute 'index'","<python><gensim>","1","0","","","","CC BY-SA 4.0"
"67210860","1","","","2021-04-22 09:48:26","","0","52","<p>I am comparing an input text with texts in a text file. I use the Gensim package for this. Within that package there are several ways to compare an input string with different strings in a text file: First, by looking at the occurrence of words. Only the frequency of words is considered, and not the semantics. The language of the text is therefore not important in that case. The result, however, is that &quot;How old are you?&quot; and &quot;What is your age?&quot; no resemblance, because those sentences consist of only different words. I want those two sentences to overlap, because they do have the same semantics.</p>
<p>I use <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html"" rel=""nofollow noreferrer"">this example</a> for this. The <code>corpora</code> and <code>similarities</code> modules of the <code>gensim</code> package are used for this. However, the input text and the texts in the database file are in Dutch language, not in English.</p>
<p>I have not been able to find anywhere that you can use a non-English language with these modules, but I have also not read anywhere that you cannot. Can someone tell me if this is possible? And if not, does anyone know a good alternative where that is possible?</p>
","7146705","","","","","2021-04-22 16:47:07","Is it possible to use the Gensim package for non-English languages in Python?","<python><gensim><similarity>","1","0","","","","CC BY-SA 4.0"
"67300475","1","","","2021-04-28 12:42:15","","1","1370","<p>I am new to the Gensim package, and I am trying to get a little familiar with it. I am now trying to import an existing, trained model. I am following exactly the example from <a href=""https://www.youtube.com/watch?v=Z1VsHYcNXDI&amp;t=330s"" rel=""nofollow noreferrer"">this video</a> (this section starts at 5:30). When I run the code from that video I get an error:</p>
<p>Code:</p>
<pre><code>from gensim.models import Word2Vec, KeyedVectors
import pandas as pd
import nltk

model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=100000)

model.wv.most_similar('man')
</code></pre>
<p>Error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:....&quot;, line 7, in &lt;module&gt;
    model.wv.most_similar('man')
AttributeError: 'KeyedVectors' object has no attribute 'wv'
</code></pre>
<p>In a previous example I also ran into a similar problem when training an own model (namely with the name of the <code>size</code> parameter, which turned out to be changed to <code>vector_size</code>, because the package is deprecated I read). That probably only happened quite recently, because in the comments of the YouTube video I do not read about such problems yet. And that fix about that <code>size</code> parameter that was renamed was also from <a href=""https://stackoverflow.com/questions/53195906/getting-init-got-an-unexpected-keyword-argument-document-this-error-in/67080756#67080756"">a post here on stackoverflow</a> that is only 2 weeks old.</p>
<p>Does anyone know how I can still use this <code>wv</code> attribute, so that I can just continue with the example from the video?</p>
","7146705","","","","","2021-09-24 18:43:08","How can I still use the 'wv' attribute of this (probably deprecated?) package/module in Python?","<python><python-3.x><nltk><gensim><word2vec>","2","1","","","","CC BY-SA 4.0"
"48205412","1","","","2018-01-11 10:56:23","","0","28","<p>The gensim Word2Vec allows us to train a model using text. As far as I know, it creates an interim <strong>nx|V|</strong> (<strong>n</strong> is number of relevant concepts, <strong>V</strong> is Vector size, user specified) matrix that stores the information which is later used to create the wordspace. How to access and work with this particular data structure?</p>
","5069280","","","","","2018-01-11 10:56:23","How can I access the interim model created by CBOW of gensim Word2Vec?","<python><word2vec><gensim>","0","2","","","","CC BY-SA 3.0"
"50783082","1","","","2018-06-10 11:13:35","","-1","886","<p>I've already built my own Skip-Gram model by using gensim word2vec. I know that I can get the similarity score between two words, e.g. <code>model.wv.similarity('car', 'truck') -&gt; 0.75</code>. Now I want to know why they are such ""similars"".</p>

<p>Since Skip-Gram has been trained with the context words, I suppose that there is a way to get the most frequent context words shared between <code>car</code> and <code>truck</code>. Another example: if I have the following sentences, I'd like to get the word <code>slow</code> as ""most frequent context"":</p>

<ul>
<li><code>the car is slow</code></li>
<li><code>the truck is slow</code></li>
<li><code>the car is red</code></li>
</ul>

<p>Notice that <code>red</code> isn't appear with <code>truck</code>, so it shouldn't be returned as ""most frequent context"".</p>

<p>Is there any way to do this?</p>
","6556024","","","","","2018-06-13 14:16:21","Get most frequent contexts between two words in word2vec","<machine-learning><nlp><word2vec><gensim>","2","0","","","","CC BY-SA 4.0"
"58986684","1","58991943","","2019-11-22 02:38:46","","0","54","<p>I have been doing clustering of a certain corpus, and obtaining results that group sentences together by obtaining their <em>tf-idf</em>, checking similarity weights > a certain threshold value from the gensim model. </p>

<pre class=""lang-py prettyprint-override""><code>tfidf_dic = DocSim.get_tf_idf()
ds = DocSim(model,stopwords=stopwords, tfidf_dict=tfidf_dic)
sim_scores = ds.calculate_similarity(source_doc, target_docs)
</code></pre>

<p>The problem is that despite putting high threshold values, sentences of similar topics but <strong>opposite polarities</strong> get clustered together as such:</p>

<p><img src=""https://i.stack.imgur.com/K1uIn.png"" alt=""cluster results.""></p>

<blockquote>
  <p>Here is an example of the similarity weights obtained between ""don't like it"" &amp; ""i like it""</p>
</blockquote>

<p><img src=""https://i.stack.imgur.com/8sFvK.png"" alt=""similarity results."">
Are there any other methods, libraries or alternative models that can differentiate the polarities effectively by assigning them very low similarities or opposite vectors?</p>

<p>This is so that the outputs ""i like it"" and ""dont like it"" are in separate clusters.</p>

<p>PS: Pardon me if there are any conceptual errors as I am rather new to NLP. Thank you in advance!</p>
","12413817","","12413817","","2019-11-25 07:08:49","2019-11-25 07:08:49","Text representations : How to differentiate between strings of similar topic but opposite polarities?","<nlp><cluster-analysis><gensim><similarity>","2","0","0","","","CC BY-SA 4.0"
"33615029","1","","","2015-11-09 17:46:45","","0","284","<p>Thanks for reading and taking the time to think about and respond to this.</p>

<p>I am using Gensim's wrapper for Mallet (<a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/wrappers/ldamallet.py"" rel=""nofollow"">ldamallet.py</a>), and it works like a charm.  I need to get the topic proportions for my corpus (over all my documents) and I do not know how to do that.  model.alpha is not it as it is not normalized to 1.  Plus, alpha contains my Dirichlet parameters, and not the topic proportions.  Am I correct?</p>

<p>Any help is much appreciated.</p>
","4602327","","","","","2015-11-10 08:44:01","topic proportions in my corpus?","<lda><gensim><topic-modeling><mallet>","1","0","","","","CC BY-SA 3.0"
"50658576","1","50659147","","2018-06-02 15:47:53","","0","216","<p>I am trying to execute in parallel <a href=""https://github.com/amirouche/wikimark/"" rel=""nofollow noreferrer"">some machine learning algorithm</a>.</p>

<p>When I use multiprocessing, it's slower than without. My wild guess is that the <code>pickle</code> serialization of the models I use slowing down the whole process. So the question is: <em>how can I initialize the pool's worker with an initial state so that I don't need to serialize/deserialize for every single call the models?</em></p>

<p>Here is my current code:</p>

<pre><code>import pickle
from pathlib import Path
from collections import Counter
from multiprocessing import Pool

from gensim.models.doc2vec import Doc2Vec

from wikimark import html2paragraph
from wikimark import tokenize


def process(args):
    doc2vec, regressions, filepath = args
    with filepath.open('r') as f:
        string = f.read()
    subcategories = Counter()
    for index, paragraph in enumerate(html2paragraph(string)):
        tokens = tokenize(paragraph)
        vector = doc2vec.infer_vector(tokens)
        for subcategory, model in regressions.items():
            prediction = model.predict([vector])[0]
            subcategories[subcategory] += prediction
    # compute the mean score for each subcategory
    for subcategory, prediction in subcategories.items():
        subcategories[subcategory] = prediction / (index + 1)
    # keep only the main category
    subcategory = subcategories.most_common(1)[0]
    return (filepath, subcategory)


def main():
    input = Path('./build')
    doc2vec = Doc2Vec.load(str(input / 'model.doc2vec.gz'))
    regressions = dict()
    for filepath in input.glob('./*/*/*.model'):
        with filepath.open('rb') as f:
            model = pickle.load(f)
        regressions[filepath.parent] = model

    examples = list(input.glob('../data/wikipedia/english/*'))

    with Pool() as pool:
        iterable = zip(
            [doc2vec] * len(examples),  # XXX!
            [regressions] * len(examples),  # XXX!
            examples
        )
        for filepath, subcategory in pool.imap_unordered(process, iterable):
            print('* {} -&gt; {}'.format(filepath, subcategory))


if __name__ == '__main__':
    main()
</code></pre>

<p>The lines marked with <code>XXX!</code> point to the data that serialized when I call <code>pool.imap_unodered</code>. There at least 200MB of data that is serialized.</p>

<p>How can I avoid serialization?</p>
","140837","","","","","2018-06-02 16:52:37","How to initialize a pool of python multiprocessing workers with a shared state?","<python><scikit-learn><nlp><data-science><gensim>","1","0","","","","CC BY-SA 4.0"
"34721984","1","34737150","","2016-01-11 12:49:10","","21","16538","<p>I have been trying word2vec for a while now using the gensim's word2vec library. My question is do I have to remove stopwords from my input text?  Because, based on my initial experimental results, I could see words like 'of', 'when'.. (stopwords) popping up when I do a <code>model.most_similar('someword')</code>..?</p>

<p>But I didn't see anywhere referring that stop word removal is necessary with word2vec? Does the word2vec is supposed to handle stop words even if you don't remove them?</p>

<p>What are the must do pre processing things (like for topic modeling, it's almost a must that you should do stopword removal)?</p>
","601357","","327694","","2016-01-12 06:25:02","2019-01-06 08:08:43","stopword removing when using the word2vec","<nlp><gensim><word2vec>","2","6","12","","","CC BY-SA 3.0"
"58975407","1","","","2019-11-21 12:44:46","","2","2078","<p>Good day, fellow humans (?).</p>

<p>I have a methodological question that is confused by a deep research in a tiny amount of time.</p>

<p>The question arises from the following problem(s): I need to apply semi-supervised or unsupervised clustering on documents. I have ~300 documents classified with multi-labels and approximately 3400 documents not classified. The number of unsupervised documents could become ~10'000 in the next days.</p>

<p>The main idea is that of applying semi-supervised clustering based on the labels at hands. Alternatively, that of going fully unsupervised for soft clustering.</p>

<p>We thought of creating embeddings for the whole documents, but here lies the confusion: which library is the best for such a task? </p>

<p>I guess the utmost importance needs to lie in the context of the whole document. As far as I know, BERT and FastText provide context-dependent word embedding, but not whole document embedding. On the other hand, Gensim's Doc2Vec is context-agnostic, right?</p>

<p>I think I saw a way to train sentence embeddings with BERT, via the HuggingFace API, and was wondering whether it could be useful to consider the whole document as a single sentence.</p>

<p>Do you have any suggestion? I'm probably exposing my utter ignorance and confusion on the matter, but my brain is melted.</p>

<p>Thank you very much for your time.</p>

<p>Viva!</p>

<p>Edit to answer to @gojomo:</p>

<p>My documents are on average ~180 words. The original task was that of multi-label text classification, i.e. each document can have from 1 to N labels, with the number of labels now being N=18.  They are highly imbalanced.
Having only 330 labeled documents so far due to several issues, we asked the documents' provider to give also unlabeled data, that should reach the order of the 10k.
I used FastText classification mode, but the result is obviously atrocious. I also run a K-NN with Doc2Vec document embedding, but the result is obviously still atrocious.
I was going to use biomedical BERT-based models (like BioBERT and SciBERT) to produce a NER tagging (trained on domain-specific datasets) on the documents to later apply a classifier.
Now that we have unlabeled documents at disposal, we wanted to adventure into semi-supervised classification or unsupervised clustering, just to explore possibilities. I have to say that this is just a master thesis.</p>
","10781203","","10781203","","2019-11-21 21:27:00","2019-11-21 21:27:00","NLP - Best document embedding library","<nlp><document><gensim><embedding><bert-language-model>","0","7","0","","","CC BY-SA 4.0"
"57943303","1","","","2019-09-15 10:36:08","","1","719","<p>I am trying to compare Glove, Fasttext, Bert ,Elmo on basis on similarity between 2 words using pre-trained models of Wiki. Glove and Fasttext had pretrained models which could easily be used with gensim word2vec in python. Does Elmo and Bert have any such models ?</p>
","7540011","","7540011","","2019-09-16 07:25:33","2019-09-20 12:32:52","Get similarity score between 2 words using Pre trained Bert, Elmo","<nlp><gensim><word2vec><word-embedding><elmo>","1","0","","","","CC BY-SA 4.0"
"57946734","1","","","2019-09-15 17:51:58","","0","375","<p>this is my code</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there the hill have eyes"",""the_hill have_eyes new york mayor was present""]

sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1)
sent = ['the', 'mayor', 'of', 'new_york', 'was', 'there', 'the_hill', 'have_eyes']
print(bigram[sent])
</code></pre>

<p>i want it detects ""the_hill_have_eyes"" but the output is</p>

<pre><code>['the', 'mayor', 'of', 'new_york', 'was', 'there', 'the_hill', 'have_eyes']
</code></pre>
","","user10732053","","","","2019-09-17 04:20:40","find bigram using gensim","<python-3.x><gensim><phrase>","2","0","","","","CC BY-SA 4.0"
"50805556","1","50811902","","2018-06-11 20:30:01","","9","13599","<p>I am using <code>gensim.models.ldamodel.LdaModel</code> to perform LDA, but I do not understand some of the parameters and cannot find explanations in the documentation. If someone has experience working with this, I would love further details of what these parameters signify.
Specifically, I do not understand:</p>

<ul>
<li><code>random_state</code></li>
<li><code>update_every</code></li>
<li><code>chunksize</code></li>
<li><code>passes</code></li>
<li><code>alpha</code></li>
<li><code>per_word_topics</code></li>
</ul>

<p>I am working with a corpus of 500 documents which are roughly around 3-5 pages each (I unfortunately cannot share a snapshot of the data because of confidentiality reasons). Currently I have set </p>

<ul>
<li><code>num_topics = 10</code></li>
<li><code>random_state = 100</code></li>
<li><code>update_every = 1</code></li>
<li><code>chunksize = 50</code></li>
<li><code>passes = 10</code></li>
<li><code>alpha = 'auto'</code></li>
<li><code>per_word_topics = True</code></li>
</ul>

<p>but this is solely based off of an example I saw and I am not sure how generalizable that is to my data.</p>
","7375754","","6808881","","2018-06-12 07:15:02","2018-06-12 07:48:19","Understanding parameters in Gensim LDA Model","<python><parameters><gensim><lda>","1","0","2","","","CC BY-SA 4.0"
"50531181","1","","","2018-05-25 14:13:42","","1","338","<p>i'm trying to cluster some documents with word2vec and numpy.</p>

<p><code>w2v = W2VTransformer()
X_train = w2v.fit_transform(X_train)</code></p>

<p>When I run the fit or fit_transform I get this error:</p>

<blockquote>
  <p>Exception in thread Thread-8:
  Traceback (most recent call last):
    File ""C:\Users\lperona\AppData\Local\Continuum\anaconda3\lib\threading.py"", line 916, in _bootstrap_inner
      self.run()
    File ""C:\Users\lperona\AppData\Local\Continuum\anaconda3\lib\threading.py"", line 864, in run
      self._target(*self._args, **self._kwargs)
    File ""C:\Users\lperona\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\base_any2vec.py"", line 99, in _worker_loop
      tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)
    File ""C:\Users\lperona\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\word2vec.py"", line 539, in _do_train_job
      tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)
    File ""gensim/models/word2vec_inner.pyx"", line 458, in gensim.models.word2vec_inner.train_batch_cbow
  ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</p>
</blockquote>

<p>(X_train is a 2D numpy array of strings)</p>

<p>Does anyone know a solution?
Thank you</p>
","9847278","","","","","2018-08-29 13:30:06","Fit method of gensim.sklearn_api.w2vmodel.W2VTransformer throws error when inputed 2-dimensional array of strings","<python><arrays><python-3.6><word2vec><gensim>","1","0","","","","CC BY-SA 4.0"
"50825058","1","","","2018-06-12 20:05:08","","0","293","<p>I'm trying to load a pretrained word2vec model using gensim. Although the model is tagged, so every word has a tag, which tells what part of speech is this word represents.</p>

<p>For example:</p>

<p><code>big::adj 0.041660 0.045049 -0.204449 0.102298 0.045326 -0.172079 0.197417 -0.012363 0.127003 0.040437 -0.003397 0.048288 0.072291 0.044205 -0.055407 -0.075357 -0.154024 0.021732 0.224021 -0.243452 -0.048776 -0.002823 0.110283 -0.052014 0.104335 -0.108122 -0.033678 -0.098096 -0.012307 0.086673 -0.028013 0.005308 -0.196080 0.002180 -0.004461 0.021646 -0.051721 -0.123485 -0.230521 0.106092 -0.206776 0.137945 0.020572 0.071123 0.042434 0.123633 -0.001925 -0.172347 -0.040973 0.135886 0.057297 -0.027319 0.066697 0.138673 -0.028331 -0.094053 -0.160371 0.158397 0.053368 -0.002126 -0.111501 0.030450 -0.054284 -0.004832 -0.065144 0.030546 -0.011896 -0.103835 -0.007947 0.120997 0.178889 -0.155029 -0.054059 -0.313675 0.061776 -0.060536 0.038848 -0.097532 -0.038358 -0.032634 0.108534 0.067584 0.044829 0.003414 0.028115 -0.010523 0.131776 0.071750 0.045095 0.046262 0.001212 -0.005994 -0.022401 -0.036971 -0.024755 0.096701 -0.026736 -0.029698 -0.107293 -0.038610</code></p>

<p>Can anyone point me out, how to load such a model, so I can ask for model['big']? Right now, it just doesn't work, when I try KeyedVectors.load().</p>
","4354552","","","","","2018-06-18 01:35:55","Gensim - how to deal with model word::tag","<load><gensim><corpus>","1","2","","","","CC BY-SA 4.0"
"25829768","1","","","2014-09-14 03:21:13","","0","777","<p>I have to admit I am not a programmer, but I am in charge of deployment. Now I met a big problem. 
In our production environment, the loading of corpus dictionary always fails and the error is </p>

<pre><code>  File ""/django/rcxue/osqa/rcxue/QuestMatchV2.py"", line 121, in loadCorpus
    corpus = [dictionary.doc2bow(text) for text in corpus_m] 

  AttributeError: 'NoneType' object has no attribute 'doc2bow'
</code></pre>

<p>However, the same codes work fine in both our test environment and in developer's local environment. 
I make test server connect to production database and the loading works, which means the database is ok.</p>

<p>I checked every setting file in every directory.The files required are all there and all 'path' are ok. I re-installed all the dependencies frozen from test environment. But I am unable to find the root cause. </p>

<p>May anyone give me some advice how should I proceed my troubleshooting.</p>
","2538599","","1628832","","2014-09-14 03:41:48","2014-09-14 03:41:48","python corpus: error reported when loading dictionary : 'NoneType' object has no attribute 'doc2bow'","<python><django><dictionary><corpus><gensim>","0","2","","","","CC BY-SA 3.0"
"41568556","1","","","2017-01-10 12:10:53","","1","807","<p>I am currently working on gensim doc2vec model to implement sentence similarity. </p>

<p>I came across this <a href=""https://williambert.online/2012/05/relatively-quick-and-easy-gensim-example-code/"" rel=""nofollow noreferrer"">sample code</a> by William Bert where he has mentioned that to train this model I need to provide my own background corpus. The code is copied below for convenience:</p>

<pre><code>import logging, sys, pprint

logging.basicConfig(stream=sys.stdout, level=logging.INFO)

### Generating a training/background corpus from your own source of documents
from gensim.corpora import TextCorpus, MmCorpus, Dictionary

# gensim docs: ""Provide a filename or a file-like object as input and TextCorpus will be initialized with a
# dictionary in `self.dictionary`and will support the `iter` corpus method. For other kinds of corpora, you only
# need to override `get_texts` and provide your own implementation.""
background_corpus = TextCorpus(input=YOUR_CORPUS)

# Important -- save the dictionary generated by the corpus, or future operations will not be able to map results
# back to original words.
background_corpus.dictionary.save(
    ""my_dict.dict"")

MmCorpus.serialize(""background_corpus.mm"",
    background_corpus)  #  Uses numpy to persist wiki corpus in Matrix Market format. File will be several GBs.

### Generating a large training/background corpus using Wikipedia
from gensim.corpora import WikiCorpus, wikicorpus

articles = ""enwiki-latest-pages-articles.xml.bz2""  # available from http://en.wikipedia.org/wiki/Wikipedia:Database_download

# This will take many hours! Output is Wikipedia in bucket-of-words (BOW) sparse matrix.
wiki_corpus = WikiCorpus(articles)
wiki_corpus.dictionary.save(""wiki_dict.dict"")

MmCorpus.serialize(""wiki_corpus.mm"", wiki_corpus)  #  File will be several GBs.

### Working with persisted corpus and dictionary
bow_corpus = MmCorpus(""wiki_corpus.mm"")  # Revive a corpus

dictionary = Dictionary.load(""wiki_dict.dict"")  # Load a dictionary

### Transformations among vector spaces
from gensim.models import LsiModel, LogEntropyModel

logent_transformation = LogEntropyModel(wiki_corpus,
    id2word=dictionary)  # Log Entropy weights frequencies of all document features in the corpus

tokenize_func = wikicorpus.tokenize  # The tokenizer used to create the Wikipedia corpus
document = ""Some text to be transformed.""
# First, tokenize document using the same tokenization as was used on the background corpus, and then convert it to
# BOW representation using the dictionary created when generating the background corpus.
bow_document = dictionary.doc2bow(tokenize_func(
    document))
# converts a single document to log entropy representation. document must be in the same vector space as corpus.
logent_document = logent_transformation[[
    bow_document]]

# Transform arbitrary documents by getting them into the same BOW vector space created by your training corpus
documents = [""Some iterable"", ""containing multiple"", ""documents"", ""...""]
bow_documents = (dictionary.doc2bow(
    tokenize_func(document)) for document in documents)  # use a generator expression because...
logent_documents = logent_transformation[
                   bow_documents]  # ...transformation is done during iteration of documents using generators, so this uses constant memory

### Chained transformations
# This builds a new corpus from iterating over documents of bow_corpus as transformed to log entropy representation.
# Will also take many hours if bow_corpus is the Wikipedia corpus created above.
logent_corpus = MmCorpus(corpus=logent_transformation[bow_corpus])

# Creates LSI transformation model from log entropy corpus representation. Takes several hours with Wikipedia corpus.
lsi_transformation = LsiModel(corpus=logent_corpus, id2word=dictionary,
    num_features=400)

# Alternative way of performing same operation as above, but with implicit chaining
# lsi_transformation = LsiModel(corpus=logent_transformation[bow_corpus], id2word=dictionary,
#    num_features=400)

# Can persist transformation models, too.
logent_transformation.save(""logent.model"")
lsi_transformation.save(""lsi.model"")

### Similarities (the best part)
from gensim.similarities import Similarity

# This index corpus consists of what you want to compare future queries against
index_documents = [""A bear walked in the dark forest."",
             ""Tall trees have many more leaves than short bushes."",
             ""A starship may someday travel across vast reaches of space to other stars."",
             ""Difference is the concept of how two or more entities are not the same.""]
# A corpus can be anything, as long as iterating over it produces a representation of the corpus documents as vectors.
corpus = (dictionary.doc2bow(tokenize_func(document)) for document in index_documents)

index = Similarity(corpus=lsi_transformation[logent_transformation[corpus]], num_features=400, output_prefix=""shard"")

print ""Index corpus:""
pprint.pprint(documents)

print ""Similarities of index corpus documents to one another:""
pprint.pprint([s for s in index])

query = ""In the face of ambiguity, refuse the temptation to guess.""
sims_to_query = index[lsi_transformation[logent_transformation[dictionary.doc2bow(tokenize_func(query))]]]
print ""Similarities of index corpus documents to '%s'"" % query
pprint.pprint(sims_to_query)

best_score = max(sims_to_query)
index = sims_to_query.tolist().index(best_score)
most_similar_doc = documents[index]
print ""The document most similar to the query is '%s' with a score of %.2f."" % (most_similar_doc, best_score)
</code></pre>

<p>Where and how should I provide my own corpus in the code?</p>

<p>Thanks in advance for your help.</p>
","","user7399214","1636276","","2017-01-10 12:44:19","2017-01-10 12:44:19","How to call a corpus file in python?","<python><machine-learning><gensim><corpus><doc2vec>","0","4","1","","","CC BY-SA 3.0"
"66665981","1","66666896","","2021-03-17 02:04:41","","0","28","<p>I am building a Doc2Vec model with 1000 documents using Gensim.
Each document has consisted of several sentences which include multiple words.</p>
<p>Example)</p>
<p>Doc1: [[word1, word2, word3], [word4, word5, word6, word7],[word8, word9, word10]]</p>
<p>Doc2: [[word7, word3, word1, word2], [word1, word5, word6, word10]]</p>
<p>Initially, to train the Doc2Vec, I first split sentences and tag each sentence with the same document tag using  &quot;TaggedDocument&quot;. As a result, I got the final training input for Doc2Vec as follows:</p>
<p>TaggedDocument(words=[word1, word2, word3], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word4, word5, word6, word7], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word8, word9, word10], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word7, word3, word1, word2], tags=['Doc2'])</p>
<p>TaggedDocument(words=[word1, word5, word6, word10], tags=['Doc2'])</p>
<p>However, would it be okay to train the model with the document as a whole without splitting sentences?</p>
<p>TaggedDocument(words=[word1, word2, word3,word4, word5, word6, word7,word8, word9, word10], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word4, word5, word6, word7,word1, word5, word6, word10], tags=['Doc2'])</p>
<p>Thank you in advance :)</p>
","15411718","","15411718","","2021-03-17 02:12:20","2021-03-17 04:11:55","Should I split sentences in a document for Doc2Vec?","<gensim><word2vec><doc2vec>","1","0","","","","CC BY-SA 4.0"
"25803267","1","25955302","","2014-09-12 07:46:07","","5","5278","<p><strong>Situation:</strong></p>

<p>I have a numpy term-document matrix 
example: [[0,1,0,0....],....[......0,0,0,0]].</p>

<p>I have plugged in the above matrix to the ldamodel method of the gensim. And it is working fine with the lad method <code>lda = LdaModel(corpus, num_topics=10)</code>. 
<code>corpus</code> is my term-document matrix mentioned above.
I needed two intermediate matrices( <strong>topic-word array &amp; document-topic array</strong>) for research purpose.</p>

<blockquote>
  <p>1) per document-topic probability matrix (p_d_t)</p>
  
  <p>2) per topic-word probability matrix (p_w_t)</p>
</blockquote>

<p><strong>Question:</strong></p>

<p>How to get those array from the gensim <code>LdaModel()</code> function.? Kindly help me with getting those matrices.</p>
","3495723","","3495723","","2014-09-12 22:24:29","2014-09-21 02:45:02","retrieve topic-word array & document-topic array from lda gensim","<lda><gensim>","1","1","3","","","CC BY-SA 3.0"
"57856393","1","","","2019-09-09 14:47:55","","1","304","<p>I'd like to calculate <a href=""http://proceedings.mlr.press/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">Word Mover's Distance</a> with <a href=""https://tfhub.dev/google/universal-sentence-encoder/2"" rel=""nofollow noreferrer"">Universal Sentence Encoder on TensorFlow Hub</a> embedding. </p>

<p>I have tried the example on <a href=""https://spacy.io/universe/project/wmd-relax"" rel=""nofollow noreferrer"">spaCy for WMD-relax</a>, which loads 'en' model from spaCy, but I couldn't find another way to feed other embeddings. </p>

<p>In <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb"" rel=""nofollow noreferrer"">gensim</a>, it seems that it only accepts <code>load_word2vec_format</code> file (<code>file.bin</code>) or <code>load</code> file (<code>file.vec</code>). </p>

<p>As I know, someone has written a <a href=""https://github.com/Kyubyong/bert-token-embeddings"" rel=""nofollow noreferrer"">Bert to token embeddings based on pytorch</a>, but it's not generalized to other models on tf-hub.</p>

<p>Is there any other approach to transfer pretrained models on tf-hub to spaCy format or word2vec format?</p>
","3832330","","","","","2020-06-29 16:54:38","Load pretrained model on TF-Hub to calculate Word Mover's Distance (WMD) on Gensim or spaCy","<tensorflow><nlp><gensim><spacy><tensorflow-hub>","2","0","","","","CC BY-SA 4.0"
"50857544","1","55674904","","2018-06-14 12:26:38","","0","758","<p>is there a possibility to evaluate the dynamic model (ldaseqmodel) like the ""normal"" lda model in values of perplexity and topic coherence?
I know that these values are printed into the logging.INFO, so another method would be to save the logging.INFO into a text file to search for these evaluation values after the simulation.
If method 1 (code to evaluate ldaseqmodel) doesnt exist, is it possible to save the logging.INFO into a text file?
Here is my code to generate the ldaseqmodel:</p>

<pre><code>from gensim import models, corpora
import csv
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

Anzahl_Topics1      = 10                

Zeitabschnitte      = [16, 19, 44, 51, 84, 122, 216, 290, 385, 441, 477, 375, 390, 408, 428, 192, 38]

TDM_dateipfad = './1gramm/TDM_1gramm_1998_2014.csv'

dateiname_corpus = ""./1gramm/corpus_DTM_1gramm.mm""

dateiname1_dtm  = ""./1gramm/DTM_1gramm_10.model""

ids = {} 
corpus = [] 

with open(TDM_dateipfad, newline='') as csvfile:
    reader = csv.reader(csvfile, delimiter=';', quotechar='|') 
    for rownumber, row in enumerate(reader): 
        for index, field in enumerate(row):
            if index == 0:
                if rownumber &gt; 0:
                    ids[rownumber-1] = field 
            else:
                if rownumber == 0:
                    corpus.append([])
                else:
                    corpus[index-1].append((rownumber-1, int(field))) 

corpora.MmCorpus.serialize(dateiname_corpus, corpus)

dtm1 = models.ldaseqmodel.LdaSeqModel(corpus=corpus, time_slice = Zeitabschnitte, id2word=ids, num_topics = Anzahl_Topics1, passes=1, chunksize=10000) 
dtm1.save(dateiname1_dtm)
</code></pre>
","9751594","","","","","2019-04-14 11:46:07","Evaluation of ldaseqmodel in gensim","<python-3.x><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"50859540","1","50993572","","2018-06-14 14:10:47","","0","1095","<p>I am using gensim's tdidf model like so:</p>

<pre><code>from gensim import corpora, models

dictionary = corpora.Dictionary(some_corpus)
mapped_corpus = [dictionary.doc2bow(text)
                 for text in some_corpus]

tfidf = models.TfidfModel(mapped_corpus)
</code></pre>

<p>Now I'd like to apply thresholds to remove terms that appear too frequently (max_df) and too infrequently (min_df).  I know that scikit's CountVectorizer allows you to do this, but I can't seem to find how to set these thresholds in gensim's tfidf.  Could someone please help? </p>
","3490622","","","","","2018-06-22 18:14:32","Is there a way to set min_df and max_df in gensim's tfidf model?","<gensim><tf-idf>","1","0","1","","","CC BY-SA 4.0"
"58034474","1","","","2019-09-20 19:56:48","","1","483","<p>I'm trying to modify an example from <a href=""http://dsgeek.com/2018/02/19/tfidf_vectors.html"" rel=""nofollow noreferrer"">this post</a>
that applies tf-idf. </p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
from gensim.corpora import Dictionary
from gensim.models.tfidfmodel import TfidfModel
from gensim.matutils import sparse2full
import numpy as np
import spacy

nlp  = spacy.load('en_core_web_md')


def keep_token(t):
    return (t.is_alpha and 
            not (t.is_space or t.is_punct or 
                 t.is_stop or t.like_num))

def lemmatize_doc(doc):
    return [ t.lemma_ for t in doc if keep_token(t)]

sentences = ['Pro USB and Analogue Microphone']
docs = [lemmatize_doc(nlp(doc)) for doc in sentences]
docs_dict = Dictionary(docs)
docs_dict.filter_extremes(no_below=20, no_above=0.2)
docs_dict.compactify()
docs_corpus = [docs_dict.doc2bow(doc) for doc in docs]
model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict)
docs_tfidf  = model_tfidf[docs_corpus]
docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])
tfidf_emb_vecs = np.vstack([nlp(docs_dict[i]).vector for i in range(len(docs_dict))])
docs_emb = np.dot(docs_vecs, tfidf_emb_vecs) 


But I'm getting this error: 

   282     _warn_for_nonsequence(tup)
--&gt; 283     return _nx.concatenate([atleast_2d(_m) for _m in tup], 0)
    284 
    285 

ValueError: need at least one array to concatenate
</code></pre>

<p>The reason is that this line is retuning an empty list:</p>

<pre><code>docs_corpus = [docs_dict.doc2bow(doc) for doc in docs]
docs_corpus
</code></pre>

<p>This is because the dictionary is empty:</p>

<p><a href=""https://i.stack.imgur.com/bbblE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bbblE.png"" alt=""enter image description here""></a></p>

<p>But I'm feeding the dic with a non empty list</p>

<p><a href=""https://i.stack.imgur.com/MbMxY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MbMxY.png"" alt=""enter image description here""></a></p>

<p>That's the part I'm not finding the reason for which it fails</p>
","4544413","","","","","2019-09-23 07:07:49","Sentence returns empty dictionary from gensim.corpora","<python><nlp><gensim><spacy>","1","0","","","","CC BY-SA 4.0"
"50477192","1","","","2018-05-22 22:16:44","","2","2142","<p>I have persisted a Word2Vec model to a binary file. I am trying to load it into a serverless API adapted from this <a href=""https://medium.com/@patrickmichelberger/how-to-deploy-a-serverless-machine-learning-microservice-with-aws-lambda-aws-api-gateway-and-d5b8cbead846"" rel=""nofollow noreferrer"">blog</a> and using <a href=""https://ryan-cranfill.github.io/gensim-aws-lambda/"" rel=""nofollow noreferrer"">this</a> as a basis.</p>

<p>This works fine locally:</p>

<pre><code>self.model = KeyedVectors.load_word2vec_format('./models/models.bin', binary=True)
</code></pre>

<p>but when calling the file from S3 it errors with:</p>

<blockquote>
  <p>'IOError: [Errno 2] No such file or directory: '46659 100\n,|PUNCT
  \xec>\xd8>\xaf\xa8\x95'</p>
</blockquote>

<pre><code>def load_model(key):
response = S3.get_object(Bucket=BUCKET_NAME, Key=key)
model_str = response['Body'].read()

model = KeyedVectors.load_word2vec_format(model_str, binary=True)
return model
</code></pre>
","2968596","","","","","2018-09-25 12:29:21","Loading Word2Vec binary model from S3 into Gensim fails","<python><amazon-s3><gensim><serverless>","2","0","2","","","CC BY-SA 4.0"
"57961188","1","57966365","","2019-09-16 16:45:21","","0","25","<p>I would like take word ""book"" (for example) get its vector representation, call it v_1 and find all words whose vector representation is within ball of radius r of v_1 i.e. ||v_1 - v_i||&lt;=r, for some real number r.</p>

<p>I know gensim has <code>most_similar</code> function, which allows to state number of top vectors to return, but it is not quite what I need. I surely can use brute force search and get the answer, but it will be to slow. </p>
","1700890","","","","","2019-09-17 01:53:18","Gensim find vectors/words in ball of radius r","<python><gensim><word-embedding>","1","3","","","","CC BY-SA 4.0"
"50849640","1","","","2018-06-14 04:30:20","","0","443","<p>I'm following <a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""nofollow noreferrer"">this tutorial</a> </p>

<p>The result I'm getting has <code>nan</code> for the coefficients. My data set has has two columns: tweets and ingestion dates. I have copied the code exactly and just made a few substitutions like tweet-prepreocessor. Any thoughts? Does the original file need the target and target names column like in the tutorial?</p>

<pre><code># Build LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=20, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)
# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

[(0,
  'nan*""fortnite"" + nan*""louis"" + nan*""yvr"" + nan*""knowhttps"" + '
  'nan*""problematic"" + nan*""zellepay"" + nan*""ritual"" + nan*""underway"" + '
  'nan*""mot"" + nan*""tsb""'),
 (1,
  'nan*""fortnite"" + nan*""louis"" + nan*""yvr"" + nan*""knowhttps"" + '
  'nan*""problematic"" + nan*""zellepay"" + nan*""ritual"" + nan*""underway"" + '
  'nan*""mot"" + nan*""tsb""'),
 (2,
  'nan*""fortnite"" + nan*""louis"" + nan*""yvr"" + nan*""knowhttps"" + '
  'nan*""problematic"" + nan*""zellepay"" + nan*""ritual"" + nan*""underway"" + '
  'nan*""mot"" + nan*""tsb""'),
 (3,
  'nan*""fortnite"" + nan*""louis"" + nan*""yvr"" + nan*""knowhttps"" + '
  'nan*""problematic"" + nan*""zellepay"" + nan*""ritual"" + nan*""underway"" + '
  'nan*""mot"" + nan*""tsb""'),
 (4,...
</code></pre>
","9692246","","9692246","","2018-06-14 13:56:42","2018-08-22 18:06:02","Gensim coefficients are nan and all the same","<python><machine-learning><gensim><lda>","2","0","","","","CC BY-SA 4.0"
"58999509","1","59035508","","2019-11-22 18:01:00","","0","403","<p>Is there any way I can map generated topic from LDA to the list of documents and identify to which topic it belongs to ? I am interested in clustering documents using unsupervised learning and segregating it into appropriate cluster. </p>

<p>Example, I have 10 topics after running LDA model with the best hyperparameter. So, it should return a number of Topic is already defined withe pre-trained LDA model with new sentence or document that user input. </p>

<p>I am waiting you guys good solution. :)</p>

<p>Ps. I am using Gensim for NLP.</p>
","3191225","","","","","2019-11-25 15:49:10","How to map topic to a document after topic modeling is done with LDA?","<nlp><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"58069421","1","58072232","","2019-09-23 20:00:29","","0","1576","<p>I have a pre-trained word2vec bin file by using skipgram. The file is pretty big (vector dimension of 200 ), over 2GB. I am thinking some methods to make the file size smaller. This bin file contains vectors for punctuation, some stop words. So, I want to know what are the options to decrease the file size for this word2vec. Is it safe to delete those punctuation and stop words rows and what would be the most effective way ?</p>
","3280146","","","","","2019-09-24 02:20:48","gensim word2vec extremely big and what are the methods to make file size smaller?","<python><gensim><word2vec>","2","0","","","","CC BY-SA 4.0"
"58069724","1","58072313","","2019-09-23 20:24:45","","2","1065","<p>I wonder how to deploy a doc2vec model in production to create word vectors as input features to a classifier. To be specific, let say, a doc2vec model is trained on a corpus as follows.</p>

<pre><code>dataset['tagged_descriptions'] = datasetf.apply(lambda x: doc2vec.TaggedDocument(
            words=x['text_columns'], tags=[str(x.ID)]), axis=1)

model = doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=150, workers=cores,
                                window=5, hs=0, negative=5, sample=1e-5, dm_concat=1)

corpus = dataset['tagged_descriptions'].tolist()

model.build_vocab(corpus)

model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)

</code></pre>

<p>and then it is dumped into a pickle file. The word vectors are used to train a classifier such as random forests to predict movies sentiment. </p>

<p>Now suppose that in production, there is a document entailing some totally new vocabularies. That being said, they were not among the ones present during the training of the doc2vec model. I wonder how to tackle such a case. </p>

<p>As a side note, I am aware of <a href=""https://stackoverflow.com/questions/47775557/updating-training-documents-for-gensim-doc2vec-model"">Updating training documents for gensim Doc2Vec model</a> and <a href=""https://stackoverflow.com/questions/39252207/gensim-how-to-retrain-doc2vec-model-using-previous-word2vec-model"">Gensim: how to retrain doc2vec model using previous word2vec model</a>. However, I would appreciate more lights to be shed on this matter. </p>
","3000538","","3000538","","2019-09-24 12:46:09","2019-09-24 12:46:09","How to use doc2vec model in production?","<python><nlp><gensim><doc2vec>","1","0","","","","CC BY-SA 4.0"
"57969707","1","57980615","","2019-09-17 07:53:15","","0","159","<p>In gensim's word2vec python, I want to get the list of cosine similarity for ""price"".</p>

<p>I read the document of gensim word2vec, but document it describes <code>most_similar</code> and <code>n_similarity</code> function)()</p>

<p>I want the whole list of similarity between price and all others.</p>
","11746299","","130288","","2019-09-17 19:05:43","2019-09-17 19:08:20","Python3, word2vec, How can I get the list of similarity rank about ""price"" in my model","<python><gensim><word2vec><similarity><cosine-similarity>","1","4","","","","CC BY-SA 4.0"
"50848942","1","","","2018-06-14 02:53:16","","1","111","<p>Below is a gensim's example, but whenever I execute it,
it show different result, so I couldn't believe gensim works well.</p>

<pre><code>from gensim import corpora, models, similarities
from collections import defaultdict

documents = [""Human machine interface for lab abc computer applications"",          # 0
             ""A survey of user opinion of computer system response time"",          # 1
             ""The EPS user interface management system"",                           # 2
             ""System and human system engineering testing of EPS"",                 # 3
             ""Relation of user perceived response time to error measurement"",      # 4
             ""The generation of random binary unordered trees"",                    # 5
             ""The intersection graph of paths in trees"",                           # 6
             ""Graph minors IV Widths of trees and well quasi ordering"",            # 7 
             ""Graph minors A survey""]                                              # 8


stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

frequency = defaultdict(int)
for text in texts:
    for token in text:
        frequency[token] += 1
texts = [[token for token in text if frequency[token] &gt; 1]
         for text in texts]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
lda = models.LdaModel(corpus, id2word=dictionary, num_topics=2)
index = similarities.MatrixSimilarity(lda[corpus])


doc = ""Human computer interaction""
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lda = lda[vec_bow]
sims = index[vec_lda]
sims = sorted(enumerate(sims), key=lambda item: -item[1])
print(sims)

print(lda.get_document_topics(vec_bow))
</code></pre>

<p>result</p>

<p>[(<strong>0</strong>, 0.9986434), (4, 0.99792993), (2, 0.99722278), (3, 0.99651831), (1, 0.99158639), (5, 0.53059661), (6, 0.4146674), (8, 0.38019019), (7, 0.36143348)]
[(0, 0.18366596), (1, 0.81633401)]</p>

<p>[(<strong>1</strong>, 0.999605), (4, 0.9991864), (0, 0.998689), (5, 0.62957084), (6, 0.48837978), (8, 0.48152202), (3, 0.4541581), (7, 0.41751832), (2, 0.40637407)]
[(0, 0.80285221), (1, 0.19714773)]</p>

<p>[(<strong>7</strong>, 0.99957085), (8, 0.99660784), (0, 0.99202132), (5, 0.78449017), (6, 0.77530348), (2, 0.56972337), (3, 0.47117239), (4, 0.47092015), (1, 0.4172135)]
[(0, 0.25292286), (1, 0.74707717)]</p>

<p>Result 7 doesn't look simiar with ""Human computer interaction"" at all.
Thanks.</p>
","7474000","","","","","2019-01-17 03:49:27","different result executing gensim examples","<python><document><similarity><gensim><cosine-similarity>","0","1","1","","","CC BY-SA 4.0"
"50895571","1","","","2018-06-17 10:01:56","","2","4372","<p>Hi I am using Gensim Word2Vec for word embedding in python.</p>

<pre><code>from gensim.models import Word2Vec, KeyedVectors
</code></pre>

<p>But i am getting error like:</p>

<pre><code>from gensim import utils
# cannot import whole gensim.corpora, because that imports wikicorpus...
from gensim.corpora.dictionary import Dictionary
</code></pre>

<p>ImportError: cannot import name utils. Thank you</p>
","9738131","","","","","2019-05-15 13:44:59","Python module Gensim error ""cannot import name utils""","<python><pip><gensim><word-embedding>","1","2","","","","CC BY-SA 4.0"
"66739453","1","","","2021-03-22 02:02:54","","0","18","<p>For example, if Topic A has the following distribution: word A: 0.6, word B: 0.3, word C: 0.1. And a document has been classified as Topic A based on the fact that it has word C in it. It has no other word (neither from topic A nor any other topic). Is there any metric that tells you that even though it has been assigned topic A, it is a weak assignment</p>
","4936133","","","","","2021-03-23 15:38:34","In gensim LDA, Is there a way to construct a document wise measure of how well a topic fits it","<gensim><lda>","1","0","","","","CC BY-SA 4.0"
"50828314","1","","","2018-06-13 02:33:19","","10","9717","<p>I am using gensim to load pre-trained fasttext model. I downloaded the English wikipedia trained model from fasttext <a href=""https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md"" rel=""noreferrer"">website</a>. </p>

<p>here is the code I wrote to load the pre-trained model: </p>

<pre><code>from gensim.models import FastText as ft
model=ft.load_fasttext_format(""wiki.en.bin"")
</code></pre>

<p>I try to check if the following phrase exists in the vocal(which rare chance it would as these are pre-trained model). </p>

<pre><code>print(""internal executive"" in model.wv.vocab)
print(""internal executive"" in model.wv)

False
True
</code></pre>

<p>So the phrase ""internal executive"" is not present in the vocabulary but we still have the word vector corresponding to that. </p>

<pre><code>model.wv[""internal executive""]
Out[46]:
array([ 0.0210917 , -0.15233646, -0.1173932 , -0.06210957, -0.07288644,
       -0.06304111,  0.07833624, -0.17026938, -0.21922196,  0.01146349,
       -0.13639058,  0.17283678, -0.09251394, -0.17875175,  0.01339212,
       -0.26683623,  0.05487974, -0.11843193, -0.01982722,  0.37037706,
       -0.24370994,  0.14269598, -0.16363597,  0.00328478, -0.16560239,
       -0.1450972 , -0.24787527, -0.01318423,  0.03277111,  0.16175713,
       -0.19367714,  0.16955379,  0.1972683 ,  0.09044111,  0.01731548,
       -0.0034324 , -0.04834719,  0.14321515,  0.01422525, -0.08803893,
       -0.29411593, -0.1033244 ,  0.06278021,  0.16452256,  0.0650492 ,
        0.1506474 , -0.14194389,  0.10778475,  0.16008648, -0.07853138,
        0.2183501 , -0.25451994, -0.0345991 , -0.28843886,  0.19964759,
       -0.10923116,  0.26665714, -0.02544454,  0.30637854,  0.04568949,
       -0.04798719, -0.05769338,  0.25762403, -0.05158515, -0.04426906,
       -0.19901046,  0.00894193, -0.17269588, -0.24747233, -0.19061406,
        0.14322804, -0.10804397,  0.4002605 ,  0.01409482, -0.04675362,
        0.10039093,  0.07260711, -0.0938239 , -0.20434211,  0.05741301,
        0.07592541, -0.02921724,  0.21137556, -0.23188967, -0.23164661,
       -0.4569614 ,  0.07434579,  0.10841205, -0.06514647,  0.01220404,
        0.02679767,  0.11840229,  0.2247431 , -0.1946325 , -0.0990666 ,
       -0.02524677,  0.0801085 ,  0.02437297,  0.00674876,  0.02088535,
        0.21464555, -0.16240154,  0.20670174, -0.21640894,  0.03900698,
        0.21772243,  0.01954809,  0.04541844,  0.18990673,  0.11806394,
       -0.21336791, -0.10871669, -0.02197789, -0.13249406, -0.20440844,
        0.1967368 ,  0.09804545,  0.1440366 , -0.08401451, -0.03715726,
        0.27826542, -0.25195453, -0.16737154,  0.3561183 , -0.15756823,
        0.06724873, -0.295487  ,  0.28395334, -0.04908851,  0.09448399,
        0.10877471, -0.05020981, -0.24595442, -0.02822314,  0.17862654,
        0.06452435, -0.15105674, -0.31911567,  0.08166212,  0.2634299 ,
        0.17043628,  0.10063848,  0.0687021 , -0.12210461,  0.10803893,
        0.13644943,  0.10755012, -0.09816817,  0.11873955, -0.03881042,
        0.18548298, -0.04769253, -0.01511982, -0.08552645, -0.05218676,
        0.05387992,  0.0497043 ,  0.06922272, -0.0089245 ,  0.24790663,
        0.27209425, -0.04925154, -0.08621719,  0.15918174,  0.25831223,
        0.01654229, -0.03617229, -0.13490392,  0.08033483,  0.34922174,
       -0.01744722, -0.16894792, -0.10506647,  0.21708378, -0.22582002,
        0.15625793, -0.10860757, -0.06058934, -0.25798836, -0.20142137,
       -0.06613475, -0.08779443, -0.10732629,  0.05967236, -0.02455976,
        0.2229451 , -0.19476262, -0.2720119 ,  0.03687386, -0.01220259,
        0.07704347, -0.1674307 ,  0.2400516 ,  0.07338555, -0.2000631 ,
        0.13897157, -0.04637206, -0.00874449, -0.32827383, -0.03435039,
        0.41587186,  0.04643605,  0.03352945, -0.13700874,  0.16430037,
       -0.13630766, -0.18546128, -0.04692861,  0.37308362, -0.30846512,
        0.5535561 , -0.11573419,  0.2332801 , -0.07236694, -0.01018955,
        0.05936847,  0.25877884, -0.2959846 , -0.13610311,  0.10905041,
       -0.18220575,  0.06902339, -0.10624941,  0.33002165, -0.12087796,
        0.06742091,  0.20762768, -0.34141317,  0.0884434 ,  0.11247049,
        0.14748637,  0.13261876, -0.07357208, -0.11968047, -0.22124515,
        0.12290633,  0.16602683,  0.01055585,  0.04445777, -0.11142147,
        0.00004863,  0.22543314, -0.14342701, -0.23209116, -0.00003538,
        0.19272381, -0.13767233,  0.04850799, -0.281997  ,  0.10343244,
        0.16510887,  0.08671653, -0.24125539,  0.01201926,  0.0995285 ,
        0.09807415, -0.06764816, -0.0206733 ,  0.04697794,  0.02000999,
        0.05817033,  0.10478792,  0.0974884 , -0.01756372, -0.2466861 ,
        0.02877498,  0.02499748, -0.00370895, -0.04728201,  0.00107118,
       -0.21848503,  0.2033032 , -0.00076264,  0.03828803, -0.2929495 ,
       -0.18218371,  0.00628893,  0.20586628,  0.2410889 ,  0.02364616,
       -0.05220835, -0.07040054, -0.03744286, -0.06718048,  0.19264086,
       -0.06490505,  0.27364203,  0.05527219, -0.27494466,  0.22256687,
        0.10330909, -0.3076979 ,  0.04852265,  0.07411488,  0.23980476,
        0.1590279 , -0.26712465,  0.07580928,  0.05644221, -0.18824042],
</code></pre>

<p>Now my confusion is that Fastext creates vectors for character ngrams of a word too. So for a word ""internal"" it will create vectors for all its character ngrams including the full word and then the final word vector for the word is the sum of its character ngrams. </p>

<p>However, how it is still able to give me vector of a word or even the whole sentence? Isn't fastext vector is for a word and its ngram? So what are these vector I am seeing for the phrase when its clearly two words?</p>
","2769240","","355715","","2018-06-14 06:40:43","2019-03-29 22:36:40","How does the Gensim Fasttext pre-trained model get vectors for out-of-vocabulary words?","<python><nlp><gensim><fasttext>","1","0","4","","","CC BY-SA 4.0"
"50860649","1","","","2018-06-14 15:07:58","","2","288","<p>I'm using Google's Word2vec and I'm wondering how to get the top words that are predicted by a skipgram model that is trained using hierarchical softmax, given an input word?</p>

<p>For instance, when using negative sampling, one can simply multiply an input word's embedding (from the input matrix) with each of the vectors in the output matrix and take the one with the top value. However, in hierarchical softmax, there are multiple output vectors that correspond to each input word, due to the use of the Huffman tree. </p>

<p>How do we compute the likelihood value/probability of an output word given an input word in this case?</p>
","2925234","","","","","2018-06-15 02:37:32","How to predict word using trained skipgram model?","<python><c++><nlp><word2vec><gensim>","1","0","","","","CC BY-SA 4.0"
"50557993","1","","","2018-05-28 01:17:01","","1","546","<p>In the Python code:</p>
<pre><code>tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
</code></pre>
<p>I want to find a way to fill the values of the <code>corpus_tfidf</code> manually as I already have a list of lists of tfidfs for each document in the corpus, calculated using specific equations.</p>
<p>So, how to use them to fill the <code>corpus_tfidf</code> instead of recalculating them using <code>gensim</code> calculations.</p>
<p>I want to use my values to be passed for the <code>gensim</code> LSI and LDA models.</p>
","1658160","","10534633","","2020-10-07 02:56:40","2020-10-07 02:56:40","How to set the values of Tfidf Model in gensim manually","<python><gensim><tf-idf>","1","0","","","","CC BY-SA 4.0"
"66718853","1","","","2021-03-20 06:27:56","","1","43","<p>How to compute the centroid of given 5 words from the word-embedding and then find the most similar words from that centroid. (In gensim)</p>
","10720610","","","","","2021-05-12 15:54:36","Gensim Compute centroid from list of words","<nlp><gensim><word2vec>","1","0","","","","CC BY-SA 4.0"
"25936354","1","","","2014-09-19 14:32:16","","1","648","<p>I'm trying to install <code>scipy</code>, <code>scikit-learn</code>, and <code>gensim</code> on Windows 7 with Python 3.3.
If I try any of these:
pip install sci</p>

<pre><code>pip install scipy
pip install scikit-learn
pip install gensim
</code></pre>

<p>I end up with an ImportError similar to:</p>

<pre><code>ImportError: no module named numpy
</code></pre>

<p>And yes, I have installed <code>numpy</code> - it works fine if I try to import it in Python. I've managed to install <code>scipy</code> and <code>scikit-learn</code> by downloading executable installers, but <code>gensim</code> doesn't have one...
I've also tried using <code>easy_install</code> for all three, but that doesn't work either.</p>

<p>Is it something to do with the Python installation? Any ideas? Thanks a lot in advance!</p>
","817296","","","","","2014-09-19 14:32:16","Python pip not working for scipy, scikit-learn and gensim","<windows-7><pip><python-3.3><gensim>","0","4","","","","CC BY-SA 3.0"
"66743945","1","","","2021-03-22 10:06:52","","0","73","<p>I'm trying to find out how similar are 2 sentences.
For doing it i'm using gensim word mover distance and since what i'm trying to find it's a similarity i do like it follow:</p>
<pre><code>sim = 1 - wv.wmdistance(sentence_obama, sentence_president)
</code></pre>
<p>What i give as an input are 2 strings:</p>
<pre><code>    sentence_obama = 'Obama speaks to the media in Illinois'
    sentence_president = 'The president greets the press in Chicago'
</code></pre>
<p>The model i'm using is the one that you can find on the web: word2vec-google-news-300
I load it with this code:</p>
<pre><code>wv = api.load(&quot;word2vec-google-news-300&quot;)
</code></pre>
<p>It give me reasonable results.
Here it's where the problem starts.
For what i can read from the documentation <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html"" rel=""nofollow noreferrer"">here</a> it seems the wmd take as input a list of string and not a string like  i do!</p>
<pre><code>def preprocess(sentence):
   return [w for w in sentence.lower().split() if w not in stop_words]

sentence_obama = preprocess(sentence_obama)
sentence_president = preprocess(sentence_president)
sim = 1 - wv.wmdistance(sentence_obama, sentence_president)
</code></pre>
<p>When i follow the documentation i get results really different:</p>
<pre><code>wmd using string as input: 0.5562025871542842
wmd using list of string as input: -0.0174646259300113
</code></pre>
<p>I'm really confused. Why is it working with string as input and it works better than when i give what the documentation is asking for?</p>
","15450796","","","","","2021-03-22 20:24:40","Gensim: word mover distance with string as input instead of list of string","<python><nlp><gensim>","1","0","0","","","CC BY-SA 4.0"
"66621708","1","","","2021-03-14 06:26:22","","0","102","<p>I just started to work with the Gensim module. I applied the filter_extreme to my processed documents.</p>
<pre><code>dictionary = gensim.corpora.Dictionary(processed_docs)
print(len(dictionary))
</code></pre>
<p>the output is <code>91436</code></p>
<p>Then I applied <code>filter_extreme</code>,</p>
<pre><code>dictionary.filter_extremes(keep_n=None)
print(len(dictionary))
</code></pre>
<p>the output is <code>20687</code>.</p>
<p>From the Gensim documentation, <code>keep_n=None</code> will keep all the tokens, however, in my case, the tokens are reduced by 70000. Do I misunderstand the function of <code>filter_extremes</code>?</p>
","14102014","","","","","2021-07-14 13:16:57","filter_extreme in Gensim","<token><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"50463415","1","","","2018-05-22 08:45:36","","2","456","<p>I did LDA over a corpus of documents with topic_number=5. As a result, I have five vectors of words, each word associates with a weight or degree of importance, like this:</p>
<pre><code>Topic_A = {(word_A1,weight_A1), (word_A2, weight_A2), ... ,(word_Ak, weight_Ak)}
Topic_B = {(word_B1,weight_B1), (word_B2, weight_B2), ... ,(word_Bk, weight_Bk)}
.
.
Topic_E = {(word_E1,weight_E1), (word_E2, weight_E2), ... ,(word_Ek, weight_Ek)}
</code></pre>
<p>Some of the words are common between documents. Now, I want to know, how I can calculate the similarity between these vectors. I can calculate cosine similarity (and other similarity measures) by programming from scratch, but I was thinking, there might be an easier way to do it. Any help would be appreciated. Thank you in advance for spending time on this.</p>
<blockquote>
<ul>
<li><p>I am programming with Python 3.6 and gensim library (but I am open to any other library)</p>
</li>
<li><p>I know someone else has asked similar question (<a href=""https://stackoverflow.com/questions/48115965/cosine-similarity-and-lda-topics"">Cosine Similarity and LDA topics</a>) but becasue he didn't get the answer, I ask it again</p>
</li>
</ul>
</blockquote>
","667355","","-1","","2020-06-20 09:12:55","2018-05-22 12:59:01","Calculating the similarity between two vectors","<python-3.x><nlp><gensim><lda><spacy>","1","0","","","","CC BY-SA 4.0"
"50466643","1","53193973","","2018-05-22 11:32:20","","16","9744","<p>I have trained my own word2vec model in gensim and I am trying to load that model in spacy. First, I need to save it in my disk and then try to load an init-model in spacy but unable to figure out exactly how.</p>

<pre><code>gensimmodel
Out[252]:
&lt;gensim.models.word2vec.Word2Vec at 0x110b24b70&gt;

import spacy
spacy.load(gensimmodel)

OSError: [E050] Can't find model 'Word2Vec(vocab=250, size=1000, alpha=0.025)'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
</code></pre>
","9540174","","","","","2021-09-28 11:24:40","In spacy, how to use your own word2vec model created in gensim?","<model><word2vec><gensim><spacy>","3","1","5","","","CC BY-SA 4.0"
"58055415","1","","","2019-09-23 03:47:51","","0","463","<p>I have been trying to use Bio2Vec for a medical word embedding project using gensim. I have downloaded ""BioWordVec_PubMed_MIMICIII_d200.bin"" from the web however, i am unable to load it. This is the error message:</p>

<blockquote>
  <p>invalid literal for int() with base 10: '¬∫\x16O/'""</p>
</blockquote>

<p>I understand that there is some invalid character in the bin file because of which I am not able to load this. However, I am not sure how to correct it.</p>

<p>I am not able to open the bin file and edit anything. Can someone help?</p>

<p>This is the code that I am using:</p>

<pre class=""lang-py prettyprint-override""><code>model = KeyedVectors.load_word2vec_format(
    datapath('BioWordVec_PubMed_MIMICIII_d200.bin'),
    encoding='windows-1252', binary=True)
</code></pre>
","12104868","","5652313","","2019-09-23 12:49:49","2020-01-15 16:45:01","How to load Bio2Vec in gensim?","<python-3.x><nlp><gensim>","2","2","","","","CC BY-SA 4.0"
"58099559","1","58106916","","2019-09-25 13:28:56","","2","203","<p>Imagine I have a fasttext model that had been trained thanks to the Wikipedia articles (like explained on the official website).
Would it be possible to train it again with another corpus (scientific documents) that could add new / more pertinent links between words? especially for the scientific ones ?</p>

<p>To summarize, I would need the classic links that exist between all the English words coming from Wikipedia. But I would like to enhance this model with new documents about specific sectors. Is there a way to do that ? And if yes, is there a way to maybe 'ponderate' the trainings so relations coming from my custom documents would be 'more important'.</p>

<p>My final wish is to compute cosine similarity between documents that can be very scientific (that's why to have better results I thought about adding more scientific documents)</p>
","12119311","","13860","","2019-09-25 13:29:36","2019-09-25 21:42:46","Training a model from multiple corpus","<python><artificial-intelligence><gensim><training-data><fasttext>","1","0","","","","CC BY-SA 4.0"
"66755354","1","","","2021-03-22 23:48:30","","0","186","<p>to make topic modelling, I used this code, but <code>BrokenPipeError: [Errno 32] Broken pipe</code> error occurs.
how to solve this problem.</p>
<pre><code>from gensim.models import CoherenceModel
from multiprocessing import Process, freeze_support

coherence_score18=[]
def model_func(corpus18, id2word18, bigram_document18, a):
    for i in range(2,10):
        model18 = gensim.models.ldamodel.LdaModel(corpus=corpus18, id2word=id2word18, num_topics=i)
        coherence_model18=CoherenceModel(model18, texts=bigram_document18, dictionary=id2word18, coherence=a)
        coherence_lda18=coherence_model18.get_coherence()
        print('n=',i,'Score:',coherence_lda18)
        coherence_score18.append(coherence_lda18)
        

    if __name__ == '__main__':
        freeze_support()
</code></pre>
","14298073","","14298073","","2021-03-23 06:46:11","2021-03-23 06:46:11","Is it possible to solve [Errno 32] Broken pipe in using get_coherence()?","<python><python-3.x><gensim><topic-modeling><broken-pipe>","0","2","","","","CC BY-SA 4.0"
"66761515","1","","","2021-03-23 10:39:07","","2","114","<p>I want topic distribution for my documents. However, Gensim's HDP's <code>show_topic()</code> <strong>returns 20 topics by default</strong>. And I suppose they are not supposed to be the best. After digging deeper, I found out there are total 150 topics, as the truncation level in the code is set to 150 by default <a href=""https://tedboy.github.io/nlps/_modules/gensim/models/hdpmodel.html"" rel=""nofollow noreferrer"">code</a>.</p>
<p>I came across another post <a href=""https://stackoverflow.com/questions/31543542/hierarchical-dirichlet-process-gensim-topic-number-independent-of-corpus-size"">post</a>, which offers ways to select optimum number of topics. But, even if we identify top topics, <strong>how do we represent new documents in terms of identified topics</strong>? Because <code>hdp[doc]</code> again gives the distribution amongst 150 topics.</p>
<p>HDP is supposed to select optimum number of topics itself unlike LDA where we decide the number of topics. However, I am not able to achieve it in Gensim's implementation.</p>
","8215148","","8215148","","2021-03-24 06:39:01","2021-03-24 06:39:01","Gensim HDP - Top Topics' distribution for document","<python><nlp><gensim><lda><topic-modeling>","0","0","","","","CC BY-SA 4.0"
"50567108","1","","","2018-05-28 13:00:20","","2","458","<p>I am trying to generate the summary of a large text file using Gensim Summarizer. 
I am getting memory error. Have been facing this issue since sometime, any help
would be really appreciated. feel free to ask for more details.</p>

<pre><code>from gensim.summarization.summarizer import summarize

file_read =open(""xxxxx.txt"",'r')
Content= file_read.read()


def Summary_gen(content):
    print(len(Content))
    summary_r=summarize(Content,ratio=0.02)
    print(summary_r)


Summary_gen(Content)
</code></pre>

<p>The length of the document is:</p>

<pre><code>365042
</code></pre>

<p>Error messsage:</p>

<pre><code>    ---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
&lt;ipython-input-6-a91bd71076d1&gt; in &lt;module&gt;()
     10 
     11 
---&gt; 12 Summary_gen(Content)

&lt;ipython-input-6-a91bd71076d1&gt; in Summary_gen(content)
      6 def Summary_gen(content):
      7     print(len(Content))
----&gt; 8     summary_r=summarize(Content,ratio=0.02)
      9     print(summary_r)
     10 

c:\python3.6\lib\site-packages\gensim\summarization\summarizer.py in summarize(text, ratio, word_count, split)
    428     corpus = _build_corpus(sentences)
    429 
--&gt; 430     most_important_docs = summarize_corpus(corpus, ratio=ratio if word_count is None else 1)
    431 
    432     # If couldn't get important docs, the algorithm ends.

c:\python3.6\lib\site-packages\gensim\summarization\summarizer.py in summarize_corpus(corpus, ratio)
    367         return []
    368 
--&gt; 369     pagerank_scores = _pagerank(graph)
    370 
    371     hashable_corpus.sort(key=lambda doc: pagerank_scores.get(doc, 0), reverse=True)

c:\python3.6\lib\site-packages\gensim\summarization\pagerank_weighted.py in pagerank_weighted(graph, damping)
     57 
     58     """"""
---&gt; 59     adjacency_matrix = build_adjacency_matrix(graph)
     60     probability_matrix = build_probability_matrix(graph)
     61 

c:\python3.6\lib\site-packages\gensim\summarization\pagerank_weighted.py in build_adjacency_matrix(graph)
     92         neighbors_sum = sum(graph.edge_weight((current_node, neighbor)) for neighbor in graph.neighbors(current_node))
     93         for j in xrange(length):
---&gt; 94             edge_weight = float(graph.edge_weight((current_node, nodes[j])))
     95             if i != j and edge_weight != 0.0:
     96                 row.append(i)

c:\python3.6\lib\site-packages\gensim\summarization\graph.py in edge_weight(self, edge)
    255 
    256         """"""
--&gt; 257         return self.get_edge_properties(edge).setdefault(self.WEIGHT_ATTRIBUTE_NAME, self.DEFAULT_WEIGHT)
    258 
    259     def neighbors(self, node):

c:\python3.6\lib\site-packages\gensim\summarization\graph.py in get_edge_properties(self, edge)
    404 
    405         """"""
--&gt; 406         return self.edge_properties.setdefault(edge, {})
    407 
    408     def add_edge_attributes(self, edge, attrs):

MemoryError: 
</code></pre>

<p>I have tried looking up for this error on the internet, but, couldn't find a workable solution to this. </p>
","9271870","","9271870","","2018-05-29 15:37:08","2018-05-29 15:37:08","Gensim Summarizer throws MemoryError, Any Solution?","<python><nlp><gensim>","2","0","1","","","CC BY-SA 4.0"
"50945820","1","50958055","","2018-06-20 10:17:47","","3","918","<p>I am using deeplearning4j java library to build paragraph vector model (doc2vec) of dimension 100. I am using a text file. It has around 17 million lines, and size of the file is 330 MB. 
I can train the model and calculate paragraph vector which gives reasonably good results.</p>

<p>The problem is that when I try to save  the model (by writing to disk) with WordVectorSerializer.writeParagraphVectors (dl4j method) it takes around 20 GB of space.  And around 30GB when I use native java serializer. </p>

<p>I'm thinking may be the model is size is too big for that much data. Is the model size 20GB reasonable for the text data of 300 MB?  </p>

<p>Comments are also welcome from people who have used doc2vec/paragraph vector in other library/language. </p>

<p>Thank you!</p>
","1478061","","1478061","","2018-06-21 00:14:16","2018-06-21 00:14:16","Paragraph Vector or Doc2vec model size","<nlp><gensim><word-embedding><doc2vec><deeplearning4j>","1","0","1","","","CC BY-SA 4.0"
"67229763","1","","","2021-04-23 12:15:13","","3","793","<p>I really need some help, as I have gone through all the posts and nothing has worked. I get this error when importing <em>gensim</em> and not numpy (numpy is before and works fine). All I want to do is import gensim and numpy to then run my analysis.</p>
<p>Here is the full error message:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-2-3f0b98039a34&gt; in &lt;module&gt;()
      1 # pip install --user numpy==1.16.4
      2 
----&gt; 3 import gensim
      4 #import numpy
      5 

~/.local/lib64/python3.6/site-packages/gensim/__init__.py in &lt;module&gt;()
      9 import logging
     10 
---&gt; 11 from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:F401
     12 
     13 

~/.local/lib64/python3.6/site-packages/gensim/corpora/__init__.py in &lt;module&gt;()
      4 
      5 # bring corpus classes directly into package namespace, to save some typing
----&gt; 6 from .indexedcorpus import IndexedCorpus  # noqa:F401 must appear before the other classes
      7 
      8 from .mmcorpus import MmCorpus  # noqa:F401

~/.local/lib64/python3.6/site-packages/gensim/corpora/indexedcorpus.py in &lt;module&gt;()
     12 import numpy
     13 
---&gt; 14 from gensim import interfaces, utils
     15 
     16 logger = logging.getLogger(__name__)

~/.local/lib64/python3.6/site-packages/gensim/interfaces.py in &lt;module&gt;()
     17 import logging
     18 
---&gt; 19 from gensim import utils, matutils
     20 
     21 

~/.local/lib64/python3.6/site-packages/gensim/matutils.py in &lt;module&gt;()
     17 import numpy as np
     18 import scipy.sparse
---&gt; 19 from scipy.stats import entropy
     20 import scipy.linalg
     21 from scipy.linalg.lapack import get_lapack_funcs

/cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages/scipy/stats/__init__.py in &lt;module&gt;()
    343 from .stats import *
    344 from .distributions import *
--&gt; 345 from .morestats import *
    346 from ._binned_statistic import *
    347 from .kde import gaussian_kde

/cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages/scipy/stats/morestats.py in &lt;module&gt;()
     10                    sqrt, ceil, floor, array, compress,
     11                    pi, exp, ravel, count_nonzero, sin, cos, arctan2, hypot)
---&gt; 12 from numpy.testing.decorators import setastest
     13 
     14 from scipy._lib.six import string_types

ModuleNotFoundError: No module named 'numpy.testing.decorators'
</code></pre>
<p>What I have tried:
I have tried using the latest version of numpy, and also forcing it to the version 1.16.4 &quot;pip install --user numpy==1.16.4&quot;. I have also tried uninstalling and reinstalling both gensim and numpy, but its not working. I am using Python 3.6 and on a cluster. I don't really understand why numpy has no problem being imported, but that this is a problem of gensim.</p>
<p>Can anyone kindly help me out with this?</p>
<p>Thanks a lot!</p>
<p>Best,
Sandra</p>
","15739947","","6573902","","2021-04-29 17:20:52","2021-04-29 17:20:52","ModuleNotFoundError: No module named 'numpy.testing.decorators'","<python><numpy><installation><python-3.6><gensim>","0","3","","","","CC BY-SA 4.0"
"58993767","1","","","2019-11-22 11:54:50","","0","136","<p>Can we make gensim lda model to use pre-determined topic distribution, while determining topics for new docs?</p>

<p>Ex:</p>

<pre><code>import gensim
texts=[['a','a','a'], ['b','b','b'],['c','c','c']]
d = gensim.corpora.Dictionary(texts)
bow = [d.doc2bow(doc) for doc in texts]
import numpy as np
user_topics=np.array([[1, 0, 0],[0, 1, 0],[0,0,1]])
model = gensim.models.LdaModel(corpus=bow, id2word=d, num_topics=3,  random_state=1,eta=user_topics)
model.get_topics()

</code></pre>

<p>Returns</p>

<pre><code>array([[0.9728407 , 0.01204113, 0.01511812],
       [0.01140388, 0.9742677 , 0.01432837],
       [0.02468761, 0.00788806, 0.9674243 ]], dtype=float32)

</code></pre>

<p>Is it possible for <code>model.get_topics()</code> to return same distribution as 'eta'?</p>

<p>i.e. </p>

<pre><code>[[1, 0, 0],[0, 1, 0],[0,0,1]]

</code></pre>
","12414781","","","","","2020-01-14 03:47:38","Initialize Gensim LDA model with pre-determined topic distribution","<python><gensim><lda><topic-modeling>","1","0","1","","","CC BY-SA 4.0"
"50866996","1","","","2018-06-14 22:53:55","","-1","655","<p>I am using python package Gensim for clustering, I first created a dictionary from tokenizing and lemmatizing sentences of the given text and then using this dictionary created corpus using following code:</p>

<pre><code> mydict = corpora.Dictionary(LemWords)
 corpus = [mydict.doc2bow(text) for text in LemWords]
</code></pre>

<p>I understand corpus would contain id of the words along with their frequency in each document. I wish to know the frequency of a given word in the whole corpus to find top terms in the corpus. I am wondering if there is any method available that return frequency of the term in the entire corpus </p>
","2838082","","1060350","","2018-06-15 07:15:23","2018-06-15 07:15:23","top terms in corpus gensim","<python><gensim><counting><corpus>","1","1","1","","","CC BY-SA 4.0"
"50478046","1","","","2018-05-23 00:02:54","","5","6090","<p>I am using gensim library for loading pre-trained word vectors from GoogleNews dataset. this dataset contains 3000000 word vectors each of 300 dimensions. when I want to load GoogleNews dataset, I receive a memory error. I have tried this code before without memory error and I don't know why I receive this error now.
I have checked a lot of sites for solving this issue but I cant understand.
this is my code for loading GoogleNews:</p>

<pre><code>import gensim.models.keyedvectors as word2vec
model=word2vec.KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin"",binary=True)
</code></pre>

<p>and this is the error I received:</p>

<pre><code>File ""/home/mahsa/PycharmProjects/tensor_env_project/word_embedding_DUC2007/inspect_word2vec-master/word_embeddings_GoogleNews.py"", line 8, in &lt;module&gt;
    model=word2vec.KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin"",binary=True)
  File ""/home/mahsa/anaconda3/envs/tensorflow_env/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 212, in load_word2vec_format
    result.syn0 = zeros((vocab_size, vector_size), dtype=datatype)
MemoryError
</code></pre>

<p>can anybody help me? thanks.</p>
","7418216","","","","","2020-12-07 10:24:19","memory error when using gensim for loading word2vec","<python><word2vec><gensim><word-embedding><google-news>","4","3","0","","","CC BY-SA 4.0"
"66759852","1","","","2021-03-23 08:48:15","","9","18333","<p>I can't import pyLDAvis.</p>
<p>It is installed but for some reason, I can not import it.</p>
<p>I tried</p>
<pre><code>conda update anaconda

pip install --upgrade pip

pip install --upgrade jupyter notebook

pip install pyLDAvis
</code></pre>
<p>Installing pyLDAvis returns the message 'requirement already satisfied'. So I tried uninstalling and reinstalled the package but still doesn't work. This never happened with any other packages.</p>
<p>How can I solve this problem?</p>
","12073062","","","","","2021-08-20 12:02:14","No module named pyLDAvis","<python><import><gensim><pyldavis>","4","1","","","","CC BY-SA 4.0"
"59009670","1","59039022","","2019-11-23 16:19:38","","0","308","<p>I am trying to reimplement wor2vec in pytorch. I implemented subsamping according to the <a href=""https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L407"" rel=""nofollow noreferrer"">code</a> of the original paper. However, I am trying to understand how subsampling is implemented in Gensim. I looked at the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">source code</a>, but I did not manage to grasp how it reconnects to the original paper.</p>

<p>Thanks a lot in advance.</p>
","9824768","","","","","2019-11-25 19:42:23","How does Gensim implement subsampling in Word2Vec?","<gensim><word2vec><subsampling>","1","0","","","","CC BY-SA 4.0"
"50914729","1","55725093","","2018-06-18 17:32:32","","8","1842","<p>I have a large pretrained Word2Vec model in gensim from which I want to use the pretrained word vectors for an embedding layer in my Keras model. </p>

<p>The problem is that the embedding size is enormous and I don't need most of the word vectors (because I know which words can occure as Input). So I want to get rid of them to reduce the size of my embedding layer.</p>

<p>Is there a way to just keep desired wordvectors (including the coresponding indices!), based on a whitelist of words?</p>
","6321155","","7120016","","2018-06-18 20:40:39","2021-02-15 15:12:27","Gensim Word2Vec select minor set of word vectors from pretrained model","<python><keras><word2vec><gensim><word-embedding>","3","0","2","","","CC BY-SA 4.0"
"58134062","1","","","2019-09-27 11:54:40","","2","366","<p>I am training a skipgram model using gensim word2vec. I would like to exit the training before reaching the number of epochs passed in the parameters based on a specific accuracy test in a different set of data in order to avoid the overfitting of the model.</p>

<p>Is there a way in gensim to interrupt the train of word2vec from a callback function?</p>
","12130416","","6573902","","2020-02-12 15:51:03","2020-02-12 15:51:03","How to break the Word2vec training from a callback function?","<python><callback><gensim><word2vec><early-stopping>","1","0","","","","CC BY-SA 4.0"
"58155131","1","","","2019-09-29 12:16:48","","0","202","<p>I am wondering which steps I have to execute on my corpus to pre-process it the same style like google did for their massive, pre-trained word2vec model (<a href=""https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/"" rel=""nofollow noreferrer"">https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/</a> )</p>

<p>According to the website they did the following:</p>

<ul>
<li>bigrams/ trigrams</li>
<li>removal of some stop words (only common ones like: a, and, of)</li>
<li>removal of some numbers (only without surrounding letters)</li>
</ul>

<p>Is there any source which details all steps?</p>

<p>Did they also  e.g. ...</p>

<ul>
<li>remove some punctuation</li>
<li>lowercase some letters</li>
<li>stem or lemmatize
?</li>
</ul>
","11181726","","","","","2019-09-29 12:16:48","How to pre-process texts to match Googles pre-trained word2vec model?","<nlp><gensim><word2vec>","0","3","","","","CC BY-SA 4.0"
"66773529","1","","","2021-03-24 01:29:11","","0","99","<p>When I use CoherenceModel, BrokenPipeError: [Errno 32] Broken pipe error occcurs.
Especially, in coherence='c_v', this error takes place.
When I choose parameter as 'u_mass', this doesn't occur
Is it possible to solve BrokenPipeError: [Errno 32] Broken pipe in LDA model?</p>
<pre><code>model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics) 
model_list.append(model) 
coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v') 
coherence_values.append(coherencemodel.get_coherence()) 

BrokenPipeError                           Traceback (most recent call last)
&lt;ipython-input-8-33bbdf4a0b18&gt; in &lt;module&gt;
     57     logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
     58     # optimize
---&gt; 59     find_optimal_number_of_topics(dictionary, corpus, processed_data)
     60 

&lt;ipython-input-8-33bbdf4a0b18&gt; in find_optimal_number_of_topics(dictionary, corpus, processed_data)
     37     step = 6;
     38 
---&gt; 39     model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=processed_data, start=start, limit=limit, step=step)
     40     x = range(start, limit, step)
     41     plt.plot(x, coherence_values)

&lt;ipython-input-8-33bbdf4a0b18&gt; in compute_coherence_values(dictionary, corpus, texts, limit, start, step)
     29         model_list.append(model)
     30         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
---&gt; 31         coherence_values.append(coherencemodel.get_coherence())
     32         return model_list, coherence_values
     33 

.........


D:\choi\lib\multiprocessing\context.py in _Popen(process_obj)
    320         def _Popen(process_obj):
    321             from .popen_spawn_win32 import Popen
--&gt; 322             return Popen(process_obj)
    323 
    324     class SpawnContext(BaseContext):

D:\choi\lib\multiprocessing\popen_spawn_win32.py in __init__(self, process_obj)
     87             try:
     88                 reduction.dump(prep_data, to_child)
---&gt; 89                 reduction.dump(process_obj, to_child)
     90             finally:
     91                 set_spawning_popen(None)

D:\choi\lib\multiprocessing\reduction.py in dump(obj, file, protocol)
     58 def dump(obj, file, protocol=None):
     59     '''Replacement for pickle.dump() using ForkingPickler.'''
---&gt; 60     ForkingPickler(file, protocol).dump(obj)
     61 
     62 #

BrokenPipeError: [Errno 32] Broken pipe
</code></pre>
","14298073","","","","","2021-03-24 01:29:11","Is it possible to solve BrokenPipeError: [Errno 32] Broken pipe in LDA model?","<python><python-3.x><gensim><topic-modeling><broken-pipe>","0","0","","","","CC BY-SA 4.0"
"66736599","1","","","2021-03-21 19:23:59","","1","180","<p>I want to use Fasttext in my program, but that error prevent me to do it. I want to create embedding matrix for my program, with following code:</p>
<pre><code>model = gensim.models.fasttext.load_facebook_model(EMBEDDING_FILE)

EMBEDDING_DIM = 300
nb_words = min(MAX_NB_WORDS, len(word_index)) + 1
embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))
for word, i in word_index.items():
    if word in model.vocab:
        embedding_matrix[i] = model.word_vec(word)
print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))

</code></pre>
","12968709","","5147259","","2021-04-02 08:47:09","2021-04-02 08:47:09","'FastText' object has no attribute 'vocab'","<python><nlp><gensim><text-processing><fasttext>","0","1","1","","","CC BY-SA 4.0"
"50910287","1","50918206","","2018-06-18 13:08:57","","4","2450","<p>I am using gensim to load the fasttext's pre-trained word embedding</p>

<p><code>de_model = KeyedVectors.load_word2vec_format('wiki.de\wiki.de.vec')</code></p>

<p>But this gives me a memory error.</p>

<p>Is there any way I can load it?</p>
","7852644","","5235392","","2018-06-18 13:16:13","2018-06-18 22:20:29","loading of fasttext pre trained german word embedding's .vec file throwing out of memory error","<nlp><gensim><word-embedding><fasttext>","1","0","1","","","CC BY-SA 4.0"
"58153359","1","","","2019-09-29 07:57:23","","-1","293","<p>I've created a dictionary with the document-topic probabilities from a Gensim LDA model. Each iteration over the dictionary (even with the same exact code) produces slightly different values. Why is this? (Note, when the same code is copied and pasted in another jupyter cell)</p>

<pre><code>for r in doc_topics[:2]:
    print(r)

</code></pre>

<p>First time produces:</p>

<pre><code>[(5, 0.46771166), (8, 0.09964698), (12, 0.08084056), (55, 0.16801219), (58, 0.07947531), (97, 0.04642806)]
[(8, 0.7273078), (69, 0.06939292), (78, 0.062151615), (101, 0.119957164)]
</code></pre>

<p>Second run produces:</p>

<pre><code>[(5, 0.47463417), (8, 0.105600394), (12, 0.06531593), (55, 0.16066092), (58, 0.06662597), (97, 0.054465853)]
[(8, 0.7306167), (69, 0.054978732), (78, 0.06831972), (84, 0.025588958), (101, 0.10244013)]
</code></pre>

<p>Third:</p>

<pre><code>[(5, 0.4771855), (8, 0.09988891), (12, 0.088423), (55, 0.15682992), (58, 0.058175407), (97, 0.053951494)]
[(8, 0.75193375), (69, 0.059308972), (78, 0.0622621), (84, 0.020040851), (101, 0.09659243)]
</code></pre>

<p>And so on...</p>
","10672495","","","","","2019-09-29 08:06:02","Why do different runs of the same iteration produce different results?","<python><pandas><loops><gensim><lda>","2","2","","","","CC BY-SA 4.0"
"50569110","1","50569617","","2018-05-28 14:59:16","","0","542","<p>I tried creating a simple Doc2Vec model:</p>

<pre><code> sentences = []
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'rosse', u'con', u'tacco'], tags=[1]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'blu'], tags=[2]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarponcini', u'Emporio', u'Armani'], tags=[3]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'marca', u'italiana'], tags=[4]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'bianche', u'senza', u'tacco'], tags=[5]))

 model = Doc2Vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate
 model.build_vocab(sentences)  
</code></pre>

<p>But I end up with an empty vocabulary. With some debugging I saw that inside the build_vocab() function a dictionary is actually created by the vocabulary.scan_vocab() function, but it's being deleted by the following vocabulary.prepare_vocab() function. More deeply, this is the function that causes the problem:</p>

<pre><code>def keep_vocab_item(word, count, min_count, trim_rule=None):
    """"""Check that should we keep `word` in vocab or remove.

    Parameters
    ----------
    word : str
        Input word.
    count : int
        Number of times that word contains in corpus.
    min_count : int
        Frequency threshold for `word`.
    trim_rule : function, optional
        Function for trimming entities from vocab, default behaviour is `vocab[w] &lt;= min_reduce`.

    Returns
    -------
    bool
        True if `word` should stay, False otherwise.

    """"""
    default_res = count &gt;= min_count

    if trim_rule is None:
        return default_res # &lt;-- ALWAYS RETURNS FALSE
    else:
        rule_res = trim_rule(word, count, min_count)
        if rule_res == RULE_KEEP:
            return True
        elif rule_res == RULE_DISCARD:
            return False
        else:
            return default_res  
</code></pre>

<p>Does somebody understand the problem?</p>
","6408518","","","","","2018-05-28 15:33:57","Gensim DOC2VEC trims and delete the vocabulary","<python><gensim><doc2vec><vocabulary>","1","1","","","","CC BY-SA 4.0"
"41658568","1","41951301","","2017-01-15 06:43:50","","15","23258","<p>I have installed gensim (through pip) in Python. After the installation is over I get the following warning:</p>

<blockquote>
  <p><strong>C:\Python27\lib\site-packages\gensim\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")</strong></p>
</blockquote>

<p>How can I rectify this? </p>

<p>I am unable to import word2vec from gensim.models due to this warning.</p>

<p>I have the following configurations: Python 2.7, gensim-0.13.4.1, numpy-1.11.3, scipy-0.18.1, pattern-2.6.</p>
","7420652","","7420652","","2017-01-15 07:05:26","2018-10-19 18:01:51","Chunkize warning while installing gensim","<python><gensim>","2","0","7","","","CC BY-SA 3.0"
"67229373","1","67301221","","2021-04-23 11:44:02","","0","190","<p>I have te same error as this thread : <a href=""https://stackoverflow.com/questions/40840731/valueerror-cannot-compute-lda-over-an-empty-collection-no-terms"">ValueError: cannot compute LDA over an empty collection (no terms)</a> but the solution needed isn't the same.</p>
<p>I'm working on a notebook with Sklearn, and I've done an LDA and a NMF.</p>
<p>I'm now trying to do the same using Gensim: <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.htm"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.htm</a></p>
<p>Here is a piece of code (in Python) from my notebook of what I'm trying to do :</p>
<pre><code>dic = gensim.corpora.Dictionary(texts_lem)
dic.filter_extremes(no_below=10, no_above=0.8)
corpus = [dic.doc2bow(doc) for doc in texts_lem]

model = gensim.models.LdaModel(
    corpus=corpus,
    id2word=dic.id2token,
    num_topics=10,
)
</code></pre>
<p>I'm using the existing texts_lem list from another section of my notebook to do the Gensim LDA.
I'm following the guide : Creating a dictionary, filtering extremes, creating a corpus and sending it to LdaModel().</p>
<p>Unfortunately, it doesn't work, and commenting filter_extremes's row doesn't help (This is the answer of the other thread with same error).</p>
<p>texts_lem is the list of list of words like the following :</p>
<pre><code>[
 ['word', 'word', 'word', 'word'],
 ['word', 'word', 'word', 'word'],
 ['word', 'word', 'word', 'word'],
]
</code></pre>
<p>My error is :</p>
<pre><code>ValueError: cannot compute LDA over an empty collection (no terms)
</code></pre>
<p>Many thanks for your help.</p>
","7453264","","","","","2021-04-30 11:30:58","Gensim LDA : error cannot compute LDA over an empty collection (no terms)","<python><nlp><gensim><lda>","2","0","","","","CC BY-SA 4.0"
"50953272","1","","","2018-06-20 16:45:07","","0","565","<p>How do I use gensim to find out Jaccard index in vectors in the corpus?</p>
","8092265","","","","","2018-06-21 04:30:48","Jaccard index in python for a corpus using gensim","<python-2.7><nlp><gensim>","1","0","","","","CC BY-SA 4.0"
"26010645","1","26013072","","2014-09-24 07:08:51","","2","2883","<pre><code>def n_similarity(self, ws1, ws2):
    v1 = [self[word] for word in ws1]
    v2 = [self[word] for word in ws2]
    return dot(matutils.unitvec(array(v1).mean(axis=0)), matutils.unitvec(array(v2).mean(axis=0)))
</code></pre>

<p>This is the code I excerpt from gensim.word2Vec, I know that two single words' similarity can be calculated by cosine distances, but what about two word sets? The code seems to use the mean of each wordvec and then calculated on the two mean vectors' cosine distance. I know few in word2vec, is there some foundations of such process?</p>
","3618972","","","","","2014-09-24 09:16:38","Why the similarity beteween two bag-of-words in gensim.word2vec calculated this way?","<nlp><gensim><word2vec>","1","0","1","","","CC BY-SA 3.0"
"66780051","1","66780420","","2021-03-24 11:26:44","","0","219","<p>I need to upgrade to the newest Gensim version. I ran <code>pip install --upgrade gensim</code> and I get <code>Requirement already satisfied.</code></p>
<p>Then:</p>
<pre><code>Python 3.8.1 (default, Jan  8 2020, 22:29:32) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import gensim
&gt;&gt;&gt; print(gensim.__version__)
3.8.3
</code></pre>
<p>I am running the pip install inside my conda environment. Any ideas?</p>
","5016028","","","","","2021-03-24 11:49:27","Can't upgrade Gensim library to version 4 in anaconda","<python><python-3.x><anaconda><gensim>","1","0","","","","CC BY-SA 4.0"
"58171114","1","","","2019-09-30 15:36:53","","0","172","<p>I'm trying to implement a topic detection function with the HdpModel of Gensim. I choose the HdpModel since it is a model that does not require to know, a priori, the number of topics to detect. That's really cool. </p>

<p>The problem is that the method to generate the topics (print_topics) receives an argument to indicate the number of topics. The docs say that the num_topics param (which is optional) will indicate the number of topics to be selected, and that passing the -1 value will result on all topics be retrieved by significance. </p>

<p>But when I set -1 it retrieves no topics. If I try not defining the parameter, just calling print_topics(), the default number of topics is always returned (20 topics). So, how can I retrieve all the possible events? This is supposed to be the main contribution of Hdp.</p>

<p>Thanks!</p>
","1401235","","","","","2020-11-23 15:52:37","Get all the possible topics with Gensim HdpModel","<gensim><topic-modeling>","0","2","1","","","CC BY-SA 4.0"
"58074914","1","","","2019-09-24 07:13:10","","0","144","<p>i have code </p>

<pre><code>import time
import multiprocessing
from datetime import timedelta
from gensim.models import word2vec
start_time = time.time()
print('Training Word2Vec Model...')
sentences = word2vec.LineSentence('data/data_text.txt')
id_w2v = word2vec.Word2Vec(sentences, size=300, workers=multiprocessing.cpu_count()-1)
id_w2v.save('model_terbaru/word2vec_300.model')
</code></pre>

<p>when i make model, i have an error</p>

<pre><code>Traceback (most recent call last):

File""&lt;ipython-input-10-fc7016864a34&gt;"", line 1, in &lt;module&gt;

        runfile('F:/pa reza/model.py', wdir='F:/pa reza')

File ""C:\ProgramData\Anaconda\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 704, in runfile
    execfile(filename, namespace)

File ""C:\ProgramData\Anaconda\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 108, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

File ""F:/pa reza/model.py"", line 13, in &lt;module&gt;
    iter=10)
</code></pre>

<p>File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\models\word2vec.py"", line 527, in <strong>init</strong>
    fast_version=FAST_VERSION)</p>

<pre><code> File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\models\base_any2vec.py"", line 335, in __init__
        self.build_vocab(sentences, trim_rule=trim_rule)

File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\models\base_any2vec.py"", line 480, in build_vocab
    sentences, progress_per=progress_per, trim_rule=trim_rule)

File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\models\word2vec.py"", line 1151, in scan_vocab
    for sentence_no, sentence in enumerate(sentences):

File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\models\word2vec.py"", line 1073, in __iter__
    line = utils.to_unicode(line).split()

File ""C:\ProgramData\Anaconda\lib\site-packages\gensim\utils.py"", line 359, in any2unicode

return unicode(text, encoding, errors=errors)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe3 in position 87: invalid continuation byte
</code></pre>

<p>help me....</p>
","11701715","","11701715","","2019-09-24 07:26:18","2019-09-24 07:26:18","'utf-8' codec can't decode byte 0xe3 in position 87 word2vec gensim","<python><gensim><word2vec>","0","2","","","","CC BY-SA 4.0"
"50937881","1","50940065","","2018-06-19 22:39:53","","0","352","<p>I am not only interested in the final W0 and W1 (also, to some known as W and W'), but all the variations of these two matrices during the learning.</p>

<p>For now, I am using the gensim implementation, but compared to sklearn, gensim's API is not very well organized in my mind. Hence, I am open to moving to tf if need be, given that getting access to these values would be possible/easier.</p>

<p>I know I can hack the main code; my question is whether there already is a function/variable for it.</p>
","2467183","","4099593","","2019-05-26 11:04:37","2019-05-26 11:04:37","How to get all the weight updates from Word2Vec","<tensorflow><gensim><word2vec>","1","0","","2019-05-26 11:04:40","","CC BY-SA 4.0"
"67458203","1","67467103","","2021-05-09 13:21:30","","0","105","<p>I have used gensim.utils.simple_preprocess(str(sentence) to create a dictionary of words that I want to use for topic modelling. However, this is also filtering important numbers (house resolutions, bill no, etc) that I really need. How did I overcome this? Possibly by replacing digits with their word form. How do i go about it, though?</p>
","12066890","","12066890","","2021-05-09 13:46:11","2021-05-10 08:21:43","How do i retain numbers while preprocessing data using gensim in python?","<nlp><gensim><preprocessor><lda><latent-semantic-analysis>","1","0","","","","CC BY-SA 4.0"
"25915441","1","","","2014-09-18 14:28:04","","0","1140","<p>I am using the gensim library to apply LDA to a set of documents. Using gensim I can apply LDA to a corpus  whatever the term weights are: binary, tf, tf-idf...</p>

<p>My question is, what is the term weighting that should be used for the original <a href=""http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"" rel=""nofollow"">LDA</a>? If I have understood correctly the weights should be term frequencies, but I am not sure.</p>
","446185","","","","","2014-09-21 03:20:12","Term weighting for original LDA in gensim","<python><lda><topic-modeling><gensim>","1","0","","","","CC BY-SA 3.0"
"50919655","1","","","2018-06-19 01:59:24","","1","52","<p>I'm wondering why isn't the number of features the same as the number of unique tokens, but rather, in my case, they differ by one (1236 v.s. 1235)</p>
<blockquote>
<p>2018-06-19 04:54:45,158 : INFO : adding document #0 to Dictionary(0 unique tokens: [])</p>
<p>2018-06-19 04:54:45,182 : INFO : built Dictionary(1236 unique tokens: ['.', ':', .....]...) from 98 documents (total 10007 corpus positions)</p>
<p>2018-06-19 04:54:45,214 : INFO : collecting document frequencies</p>
<p>2018-06-19 04:54:45,215 : INFO : PROGRESS: processing document #0</p>
<p>2018-06-19 04:54:45,219 : INFO : calculating IDF weights for 98 documents and 1235 features (6993 matrix non-zeros)</p>
</blockquote>
","1509695","","-1","","2020-06-20 09:12:55","2018-06-19 02:09:44","gensim tfidf number of unique tokens v.s. number of features","<gensim>","0","2","","","","CC BY-SA 4.0"
"50960914","1","","","2018-06-21 05:24:02","","0","60","<p>While forming the Gensim LDA model, I got dictionary for my Data using following command</p>

<pre><code>    from gensim.corpora import Dictionary
    dictionary1 = Dictionary(docs)
    dictionary1.filter_extremes(no_below=10, no_above=0.75, keep_n = 1000)
</code></pre>

<p>Out of these 1000 most frequent tokens I manually removed 500 tokens so that the remaining tokens would be directly related to the topics I want to generate.
How can i further form corpus document out of this new dictionary formed which is of type dict. In which form should I use it as to train my LDA model?</p>
","8578853","","","","","2018-06-21 05:43:25","Can I form corpus document for LDA model out of dictionary of type dict?","<python><dictionary><gensim><lda>","1","0","","","","CC BY-SA 4.0"
"58123189","1","58125346","","2019-09-26 19:01:59","","1","734","<p>I am extracting the word embeddings vector from a word2vec model using model.wv. What is the range of values for each element in this vector?</p>

<pre><code>import gensim

word2vec_model = gensim.models.Word2Vec.load(""testModel"")
word2vec_model.wv[""increase""] #What is range of values for each vector element?
</code></pre>

<p>Can't seem to find this information in the documentation.</p>
","1462664","","1462664","","2019-09-26 20:31:05","2019-09-26 22:07:46","Range for vector values in gensim model","<gensim><word2vec>","1","1","","","","CC BY-SA 4.0"
"50933591","1","","","2018-06-19 17:06:00","","5","2180","<p>I am using Gensim for vector space model. after creating a dictionary and corpus from Gensim I calculated the (Term frequency*Inverse document  Frequency)TFIDF  using the following line</p>

<pre><code>Term_IDF  = TfidfModel(corpus)
corpus_tfidf = Term_IDF[corpus]
</code></pre>

<p>The corpus_tfidf contain list of the list having Terms ids and corresponding TFIDF. then I separated the TFIDF from ids using following lines:</p>

<pre><code> for doc in corpus_tfidf:
     for ids,tfidf in doc:    
         IDS.append(ids)
         tfidfmtx.append(tfidf)    
         IDS=[]
</code></pre>

<p>now I want to use k-means clustering so I want to perform cosine similarities of tfidf matrix the problem is Gensim does not produce square matrix so when I run following line it generates an error. I wonder how can I get the square matrix from Gensim to calculate the similarities of all the documents in vector space model. Also how to convert tfidf matrix (which in this case is a list of lists) into 2D NumPy array. any comments are much appreciated.</p>

<p>dumydist = 1 - cosine_similarity(tfidfmtx)</p>
","2838082","","2838082","","2018-06-19 17:27:54","2019-04-15 15:25:36","How to perform kmean clustering from Gensim TFIDF values","<numpy><k-means><gensim><tf-idf><corpus>","2","0","","","","CC BY-SA 4.0"