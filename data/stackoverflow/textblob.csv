Id,PostTypeId,AcceptedAnswerId,ParentId,CreationDate,DeletionDate,Score,ViewCount,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditorDisplayName,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate,CommunityOwnedDate,ContentLicense
"68893037","1","","","2021-08-23 12:49:36","","0","12","<p>I am trying to analyze open-ended questions with Polarity and subjectivity. So what I want to achieve is to upload the CSV file, then add new columns one for polarity, subjectively, negative or positive column and here what I did:</p>
<pre><code>from textblob import TextBlob
import pandas as pd 
import numpy as np

# Load the data
from google.colab import files
uploaded = files.upload()


text = open(uploaded) // *this did not work so I just replaced uploaded with the name of the file and the path... this is not what I want. I hoped to get the file name here once uploaded in the first step and refer it to the file name in this line.* //
text = text.read()
blob = TextBlob(text)

with open('text.csv', 'r') as read_obj:
    # pass the file object to reader() to get the reader object
    csv_reader = reader(read_obj)
    # Iterate over each row in the csv using reader object
    for row in csv_reader:
        # row variable is a list that represents a row in csv
        data = data.append(row[row], blob.polarity,blob.subjectivity)
        print(data) 
</code></pre>
<p>And I want to print the data in an external file. but could not figure that out. how can I do it, and thank you in advance.</p>
","16733141","","","","","2021-08-23 12:49:36","Polarity and subjectively from text","<python><dataframe><textblob>","0","0","","","","CC BY-SA 4.0"
"68988554","1","","","2021-08-30 18:10:43","","0","7","<p>I am trying to use textblob for sentiment analysis. This is data I want to use textblob to analyze sentiment:</p>
<p><a href=""https://i.stack.imgur.com/1UrNf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1UrNf.png"" alt=""Data"" /></a></p>
<p>When I use textblob to analyze my data, the following error code occurs:</p>
<pre><code>TypeError: The `text` argument passed to `__init__(text)` must be a string, not &lt;class 'list'&gt;
</code></pre>
<p>Can someone help me understand whats going on and how should I solve it?</p>
","14739940","","3157428","","2021-09-02 12:57:44","2021-09-02 12:57:44","Analyzing Data with TEXTBLOB","<python><nlp><type-conversion><textblob>","0","0","","","","CC BY-SA 4.0"
"69173608","1","69174206","","2021-09-14 07:28:44","","2","60","<p>I am experiencing some problems using the TextBlob library. I'm trying to run a very simple piece of code like this:</p>
<pre><code>from textblob import TextBlob
text = 'this is just a test'
blob = TextBlob(text)
blob.detect_language()
</code></pre>
<p>And it continually gives me this error:</p>
<pre><code>/usr/lib/python3.7/urllib/request.py in http_error_default(self, req, fp, code, msg, hdrs)
    647 class HTTPDefaultErrorHandler(BaseHandler):
    648     def http_error_default(self, req, fp, code, msg, hdrs):
--&gt; 649         raise HTTPError(req.full_url, code, msg, hdrs, fp)
    650 
    651 class HTTPRedirectHandler(BaseHandler):

HTTPError: HTTP Error 404: Not Found
</code></pre>
<p>What is the problem? I have tried it on several devices and it gives me the same error everytime.</p>
<p>Thanks!</p>
","15157581","","","","","2021-09-14 08:13:20","Why do I get an HTTP error when using TextBlob?","<python><http><textblob>","1","0","","","","CC BY-SA 4.0"
"28509490","1","","","2015-02-13 22:00:25","","6","2802","<p>I installed textblob using pip as given <a href=""http://textblob.readthedocs.org/en/dev/install.html"" rel=""noreferrer"">here</a>.</p>

<p>Now, when I try to import this in python3.4 in terminal then it says </p>

<pre><code>ImportError: No module named 'textblob'
</code></pre>

<p>Whereas, in python2.7 it imports happily. I have tried reinstalling it. I have even reinstalled pip. What is the problem here?</p>
","2650427","","","","","2015-02-13 22:50:05","Running TextBlob in Python3","<python><python-3.4><textblob>","1","4","1","","","CC BY-SA 3.0"
"69242475","1","","","2021-09-19 10:39:33","","0","8","<p>I've just built a text analysis tool that's based on <code>TextBlob</code>. I've heavily modified the <code>en-sentiment.xml</code> file to include a number of words that we commonly use in our department. We work in a Compliance forum and therefore the nature of text fed into the text analysis tool is very formal and structured.</p>
<p>The issue I have is with 'double negatives'. The statement - &quot;From our review, no material exception was noted.&quot; - is an overall positive statement, given that we're effectively saying that there's nothing wrong following our review. However, the <code>TextBlob</code> <code>polarity</code> score returned is <code>-0.02</code>, which isn't right. I would expect this to be very close to 1.</p>
<p>The words <code>no</code>, <code>material</code> and <code>exception</code> are classed as negative, neutral and negative words respectively within the <code>en-sentiment.xml</code> lexicon file, and I've already considered the impact of stopwords.</p>
<p>Can anyone please suggest an approach to remediate this? Effectively what I'm asking is how I can boost the polarity score in such cases. Thanks!</p>
<p>P.S. I only experimented with <code>TextBlob</code> as a starting point; I'd love to hear more from anyone who has successfully adapted natural language processing using other NLP tools / packages / libraries in Python for formal / structured text analysis.</p>
","13461439","","","","","2021-09-19 10:39:33","How can I refine my TextBlob-based text analysis model?","<nlp><nltk><textblob>","0","0","","","","CC BY-SA 4.0"
"69138663","1","69138717","","2021-09-10 22:47:17","","1","21","<p>I have 3 columns. Review, Date and Review Rating. I want to split reviews into sentences and add the sentences under a new column, but the other rows should repeat based on the number of sentences.</p>
<p>for example</p>
<pre><code>Date          Review_Rating           Review                      Sentence
12-02-2021        5          ram is good. ram is intelligent       ram is good.
12-02-2021        5          ram is good. ram is intelligent       ram is intelligent
</code></pre>
","16882321","","16882321","","2021-09-10 22:51:15","2021-09-10 22:55:01","Extract Sentences from review column and adding it in a new column, repeating the other rows for each new sentence","<python><pandas><textblob>","1","0","","","","CC BY-SA 4.0"
"69207838","1","","","2021-09-16 11:44:57","","0","87","<p>So I have been trying out coding and am currently finding some language detection packages and found out about textblob, but I am having some sort of proble.
This is my code:</p>
<pre><code># - *- coding: utf- 8 - *-
from textblob import TextBlob

blob = TextBlob(&quot;Comment vas-tu?&quot;)

print(blob.detect_language())

print(blob.translate(to='es'))
print(blob.translate(to='en'))
print(blob.translate(to='zh'))
</code></pre>
<p>and this error shows:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\*****\PycharmProjects\pythonProject\main.py&quot;, line 6, in &lt;module&gt;
    print(blob.detect_language())
  File &quot;C:\Users\*****\AppData\Local\Programs\Python\Python39\lib\site-packages\textblob\blob.py&quot;, line 568, in detect_language
    return self.translator.detect(self.raw)
  File &quot;C:\Users\*****\AppData\Local\Programs\Python\Python39\lib\site-packages\textblob\translate.py&quot;, line 72, in detect
    response = self._request(url, host=host, type_=type_, data=data)
  File &quot;C:\Users\*****\AppData\Local\Programs\Python\Python39\lib\site-packages\textblob\translate.py&quot;, line 92, in _request
    resp = request.urlopen(req)
  File &quot;C:\Users\*****\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&quot;, line 214, in urlopen
    return opener.open(url, data, timeout)
  File &quot;C:\Users\*****\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&quot;, line 523, in open
    response = meth(req, response)
  File &quot;C:\Users\*****\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&quot;, line 632, in http_response
    response = self.parent.error(
  File &quot;C:\Users\*****\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&quot;, line 561, in error
    return self._call_chain(*args)
  File &quot;C:\Users\*****\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&quot;, line 494, in _call_chain
    result = func(*args)
  File &quot;C:\Users\*****\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&quot;, line 641, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found

Process finished with exit code 1
</code></pre>
<p>I am still a little bit of a beginner in programming... Can I ask what I can do to solve this?</p>
","16927819","","1654381","","2021-09-16 13:48:12","2021-09-28 20:01:12","textblob .detect_language() function not working","<python><textblob><language-detection>","1","1","","","","CC BY-SA 4.0"
"69338699","1","69367776","","2021-09-26 20:27:02","","3","465","<p>When I try to use translate function in TextBlob library in jupyter notebook, I get:</p>
<pre><code>HTTPError: HTTP Error 404: Not Found
</code></pre>
<p>I have posted my code and screenshot of error message for reference here. This code worked well 5-6 days ago when I ran exactly the same code first time but after that whenever I run this code it gives me the same error message. I have been trying to run this code since last 4-5 days but it never worked again.</p>
<p><strong>My code:</strong></p>
<pre><code>from textblob import TextBlob

en_blob = TextBlob('Simplilearn is one of the world’s leading certification training providers.')

en_blob.translate(to='es')  
</code></pre>
<p>I am new to stackoverflow and asking my first question on this plateform so please pardon me if I my question is not following rules of this platform.</p>
","13505048","","7043805","","2021-09-26 20:36:07","2021-10-16 07:49:59","""HTTPError: HTTP Error 404: Not Found"" while using translation function in TextBlob","<python><http-status-code-404><translation><textblob>","1","2","1","","","CC BY-SA 4.0"
"69394277","1","","","2021-09-30 14:44:20","","1","14","<p>I have a manually labelled set of ~120K tweets. If I use VADER's compound score it only matches the manual labelling for ~24% of the records, textblob matches ~35% of the manually labelled record. If I take Vaders compound score and textblobs score and add then together and divide by 2 the resulting sentiment result matches the manual labelling ~70% of the time. Is there any reason for why its more accurate or is it just coincidence?</p>
","15759045","","","","","2021-10-03 00:03:40","Using an average of VADER and textBlob's sentiment polarity gives me a more accurate result, why?","<python><nlp><sentiment-analysis><textblob><vader>","1","0","","","","CC BY-SA 4.0"
"69465727","1","","","2021-10-06 12:28:38","","0","22","<p>I want to do a translate my code is:</p>
<pre class=""lang-py prettyprint-override""><code>from textblob import TextBlob
t=TextBlob(&quot;Los telefonos samsung tienen gran capacidad&quot;)
ten=t.translate(to=&quot;en&quot;)
print(ten)
</code></pre>
<p>But spyder show me the next error</p>
<pre><code>File &quot;C:\ProgramData\Anaconda3\lib\urllib\request.py&quot;, line 649, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)

HTTPError: Not Found
</code></pre>
","9116041","","15993687","","2021-10-06 12:40:12","2021-10-06 12:40:12","error when using tranlate library with textblob","<python><spyder><translate><textblob>","0","1","","","","CC BY-SA 4.0"
"60763498","1","","","2020-03-19 19:01:13","","0","164","<p>I have installed NLTK and TextBlob on Python3 on WSL. No matter what I do, I always obtain the same error. Indeed, trying to install shows that all packages are installed</p>

<pre><code>$ sudo pip3 install -U textblob
The directory '/home/jlchulilla/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/home/jlchulilla/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
Requirement already up-to-date: textblob in /usr/local/lib/python3.6/dist-packages
Requirement already up-to-date: nltk&gt;=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob)
Requirement already up-to-date: six in /usr/local/lib/python3.6/dist-packages (from nltk&gt;=3.1-&gt;textblob)
</code></pre>

<p>But when I try to update corpora or use <code>from textblob import TextBlob</code>, this is the error message:</p>

<pre><code>$ python3 -m textblob.download_corpora
Traceback (most recent call last):
  File ""/usr/lib/python3.6/subprocess.py"", line 140, in &lt;module&gt;
    import threading
  File ""/usr/lib/python3.6/threading.py"", line 7, in &lt;module&gt;
    from traceback import format_exc as _format_exc
  File ""/usr/lib/python3.6/traceback.py"", line 5, in &lt;module&gt;
    import linecache
  File ""/usr/lib/python3.6/linecache.py"", line 11, in &lt;module&gt;
    import tokenize
  File ""/mnt/c/Users/jlchu/Dropbox/oando/COVID-19/Supermercados online y covid_csv/prácticas NLTK/tokenize.py"", line 1, in &lt;module&gt;
    from textblob import textblob
ImportError: cannot import name 'textblob'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""/usr/lib/python3.6/runpy.py"", line 109, in _get_module_details
    __import__(pkg_name)
  File ""/usr/local/lib/python3.6/dist-packages/textblob/__init__.py"", line 2, in &lt;module&gt;
    from .blob import TextBlob, Word, Sentence, Blobber, WordList
  File ""/usr/local/lib/python3.6/dist-packages/textblob/blob.py"", line 28, in &lt;module&gt;
    import nltk
  File ""/usr/local/lib/python3.6/dist-packages/nltk/__init__.py"", line 99, in &lt;module&gt;
    from nltk.internals import config_java
  File ""/usr/local/lib/python3.6/dist-packages/nltk/internals.py"", line 11, in &lt;module&gt;
    import subprocess
  File ""/usr/lib/python3.6/subprocess.py"", line 142, in &lt;module&gt;
    import dummy_threading as threading
  File ""/usr/lib/python3.6/dummy_threading.py"", line 45, in &lt;module&gt;
    import threading
  File ""/usr/lib/python3.6/threading.py"", line 7, in &lt;module&gt;
    from traceback import format_exc as _format_exc
  File ""/usr/lib/python3.6/traceback.py"", line 5, in &lt;module&gt;
    import linecache
  File ""/usr/lib/python3.6/linecache.py"", line 11, in &lt;module&gt;
    import tokenize
  File ""/mnt/c/Users/jlchu/Dropbox/oando/COVID-19/Supermercados online y covid_csv/prácticas NLTK/tokenize.py"", line 1, in &lt;module&gt;     
    from textblob import textblob
ImportError: cannot import name 'textblob'
Error in sys.excepthook:
Traceback (most recent call last):
  File ""/usr/lib/python3.6/subprocess.py"", line 140, in &lt;module&gt;
    import threading
  File ""/usr/lib/python3.6/threading.py"", line 7, in &lt;module&gt;
    from traceback import format_exc as _format_exc
ImportError: cannot import name 'format_exc'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 62, in apport_excepthook
    import re, traceback
  File ""/usr/lib/python3.6/traceback.py"", line 5, in &lt;module&gt;
    import linecache
  File ""/usr/lib/python3.6/linecache.py"", line 11, in &lt;module&gt;
    import tokenize
  File ""/mnt/c/Users/jlchu/Dropbox/oando/COVID-19/Supermercados online y covid_csv/prácticas NLTK/tokenize.py"", line 1, in &lt;module&gt;     
    from textblob import textblob
  File ""/usr/local/lib/python3.6/dist-packages/textblob/__init__.py"", line 2, in &lt;module&gt;
    from .blob import TextBlob, Word, Sentence, Blobber, WordList
  File ""/usr/local/lib/python3.6/dist-packages/textblob/blob.py"", line 28, in &lt;module&gt;
    import nltk
  File ""/usr/local/lib/python3.6/dist-packages/nltk/__init__.py"", line 99, in &lt;module&gt;
    from nltk.internals import config_java
  File ""/usr/local/lib/python3.6/dist-packages/nltk/internals.py"", line 11, in &lt;module&gt;
    import subprocess
  File ""/usr/lib/python3.6/subprocess.py"", line 142, in &lt;module&gt;
    import dummy_threading as threading
  File ""/usr/lib/python3.6/dummy_threading.py"", line 45, in &lt;module&gt;
    import threading
  File ""/usr/lib/python3.6/threading.py"", line 7, in &lt;module&gt;
    from traceback import format_exc as _format_exc
ImportError: cannot import name 'format_exc'

Original exception was:
Traceback (most recent call last):
  File ""/usr/lib/python3.6/subprocess.py"", line 140, in &lt;module&gt;
    import threading
  File ""/usr/lib/python3.6/threading.py"", line 7, in &lt;module&gt;
    from traceback import format_exc as _format_exc
  File ""/usr/lib/python3.6/traceback.py"", line 5, in &lt;module&gt;
    import linecache
  File ""/usr/lib/python3.6/linecache.py"", line 11, in &lt;module&gt;
    import tokenize
  File ""/mnt/c/Users/jlchu/Dropbox/oando/COVID-19/Supermercados online y covid_csv/prácticas NLTK/tokenize.py"", line 1, in &lt;module&gt;     
    from textblob import textblob
ImportError: cannot import name 'textblob'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""/usr/lib/python3.6/runpy.py"", line 109, in _get_module_details
    __import__(pkg_name)
  File ""/usr/local/lib/python3.6/dist-packages/textblob/__init__.py"", line 2, in &lt;module&gt;
    from .blob import TextBlob, Word, Sentence, Blobber, WordList
  File ""/usr/local/lib/python3.6/dist-packages/textblob/blob.py"", line 28, in &lt;module&gt;
    import nltk
  File ""/usr/local/lib/python3.6/dist-packages/nltk/__init__.py"", line 99, in &lt;module&gt;
    from nltk.internals import config_java
  File ""/usr/local/lib/python3.6/dist-packages/nltk/internals.py"", line 11, in &lt;module&gt;
    import subprocess
  File ""/usr/lib/python3.6/subprocess.py"", line 142, in &lt;module&gt;
    import dummy_threading as threading
  File ""/usr/lib/python3.6/dummy_threading.py"", line 45, in &lt;module&gt;
    import threading
  File ""/usr/lib/python3.6/threading.py"", line 7, in &lt;module&gt;
    from traceback import format_exc as _format_exc
  File ""/usr/lib/python3.6/traceback.py"", line 5, in &lt;module&gt;
    import linecache
  File ""/usr/lib/python3.6/linecache.py"", line 11, in &lt;module&gt;
    import tokenize
  File ""/mnt/c/Users/jlchu/Dropbox/oando/COVID-19/Supermercados online y covid_csv/prácticas NLTK/tokenize.py"", line 1, in &lt;module&gt;     
    from textblob import textblob
ImportError: cannot import name 'textblob'
</code></pre>

<p>I cannot understand where is the problem. Any help would be really appreciated</p>

<p>Edit: When I try to import any other module of NLTK library, same error happens. It's like if previous calling to 'tokenize' would have blocked the module or something</p>

<pre><code>$ python3
Python 3.6.7 (default, Oct 22 2018, 11:32:17) 
[GCC 8.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from nltk.stem.porter import PorterStemmer
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python3.6/dist-packages/nltk/__init__.py"", line 99, in &lt;module&gt;
    from nltk.internals import config_java
  File ""/usr/local/lib/python3.6/dist-packages/nltk/internals.py"", line 11, in &lt;module&gt;
    import subprocess
  File ""/usr/lib/python3.6/subprocess.py"", line 140, in &lt;module&gt;
    import threading
  File ""/usr/lib/python3.6/threading.py"", line 7, in &lt;module&gt;
    from traceback import format_exc as _format_exc
  File ""/usr/lib/python3.6/traceback.py"", line 5, in &lt;module&gt;
    import linecache
  File ""/usr/lib/python3.6/linecache.py"", line 11, in &lt;module&gt;
    import tokenize
  File ""/mnt/c/Users/jlchu/Dropbox/oando/COVID-19/Supermercados online y covid_csv/prácticas NLTK/tokenize.py"", line 1, in &lt;module&gt;
    from textblob import TextBlob
  File ""/usr/local/lib/python3.6/dist-packages/textblob/__init__.py"", line 2, in &lt;module&gt;
    from .blob import TextBlob, Word, Sentence, Blobber, WordList
  File ""/usr/local/lib/python3.6/dist-packages/textblob/blob.py"", line 35, in &lt;module&gt;
    from textblob.base import (BaseNPExtractor, BaseTagger, BaseTokenizer,
  File ""/usr/local/lib/python3.6/dist-packages/textblob/base.py"", line 44, in &lt;module&gt;
    class BaseTokenizer(with_metaclass(ABCMeta), nltk.tokenize.api.TokenizerI):
AttributeError: module 'nltk' has no attribute 'tokenize'
Error in sys.excepthook:
Traceback (most recent call last):
  File ""/usr/lib/python3.6/subprocess.py"", line 140, in &lt;module&gt;
    import threading
  File ""/usr/lib/python3.6/threading.py"", line 7, in &lt;module&gt;
    from traceback import format_exc as _format_exc
ImportError: cannot import name 'format_exc'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 62, in apport_excepthook
    import re, traceback
  File ""/usr/lib/python3.6/traceback.py"", line 5, in &lt;module&gt;
    import linecache
  File ""/usr/lib/python3.6/linecache.py"", line 11, in &lt;module&gt;
    import tokenize
  File ""/mnt/c/Users/jlchu/Dropbox/oando/COVID-19/Supermercados online y covid_csv/prácticas NLTK/tokenize.py"", line 1, in &lt;module&gt;     
    from textblob import TextBlob
  File ""/usr/local/lib/python3.6/dist-packages/textblob/__init__.py"", line 2, in &lt;module&gt;
    from .blob import TextBlob, Word, Sentence, Blobber, WordList
  File ""/usr/local/lib/python3.6/dist-packages/textblob/blob.py"", line 28, in &lt;module&gt;
    import nltk
  File ""/usr/local/lib/python3.6/dist-packages/nltk/__init__.py"", line 99, in &lt;module&gt;
    from nltk.internals import config_java
  File ""/usr/local/lib/python3.6/dist-packages/nltk/internals.py"", line 11, in &lt;module&gt;
    import subprocess
  File ""/usr/lib/python3.6/subprocess.py"", line 142, in &lt;module&gt;
    import dummy_threading as threading
  File ""/usr/lib/python3.6/dummy_threading.py"", line 45, in &lt;module&gt;
    import threading
  File ""/usr/lib/python3.6/threading.py"", line 7, in &lt;module&gt;
    from traceback import format_exc as _format_exc
ImportError: cannot import name 'format_exc'

Original exception was:
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python3.6/dist-packages/nltk/__init__.py"", line 99, in &lt;module&gt;
    from nltk.internals import config_java
  File ""/usr/local/lib/python3.6/dist-packages/nltk/internals.py"", line 11, in &lt;module&gt;
    import subprocess
  File ""/usr/lib/python3.6/subprocess.py"", line 140, in &lt;module&gt;
    import threading
  File ""/usr/lib/python3.6/threading.py"", line 7, in &lt;module&gt;
    from traceback import format_exc as _format_exc
  File ""/usr/lib/python3.6/traceback.py"", line 5, in &lt;module&gt;
    import linecache
  File ""/usr/lib/python3.6/linecache.py"", line 11, in &lt;module&gt;
    import tokenize
  File ""/mnt/c/Users/jlchu/Dropbox/oando/COVID-19/Supermercados online y covid_csv/prácticas NLTK/tokenize.py"", line 1, in &lt;module&gt;     
    from textblob import TextBlob
  File ""/usr/local/lib/python3.6/dist-packages/textblob/__init__.py"", line 2, in &lt;module&gt;
    from .blob import TextBlob, Word, Sentence, Blobber, WordList
  File ""/usr/local/lib/python3.6/dist-packages/textblob/blob.py"", line 35, in &lt;module&gt;
    from textblob.base import (BaseNPExtractor, BaseTagger, BaseTokenizer,
  File ""/usr/local/lib/python3.6/dist-packages/textblob/base.py"", line 44, in &lt;module&gt;
    class BaseTokenizer(with_metaclass(ABCMeta), nltk.tokenize.api.TokenizerI):
AttributeError: module 'nltk' has no attribute 'tokenize'
</code></pre>
","4337253","","4337253","","2020-03-19 19:05:24","2020-03-19 19:05:24","WSL: ImportError: cannot import name 'textblob'","<python><nltk><textblob>","0","3","","","","CC BY-SA 4.0"
"60843424","1","","","2020-03-25 05:28:05","","0","293","<p>I am newbie in that area. Recently doing a sentiment analysis on twitter data. And came across with two approaches: 
1)TextBlob - python library for processing textual data
2) BOW (Bag Of Words)</p>

<p>What is the difference between them? Is the two difference approach or have any similarity?</p>
","1642103","","","","","2020-03-25 06:43:54","Difference between BOW(Bag Of Words) and TextBlob","<python><machine-learning><data-science><textblob>","1","0","","","","CC BY-SA 4.0"
"61298049","1","","","2020-04-19 00:26:17","","1","80","<p>I'm using Textblob MaxEntClassifier for sentiment analysis, just the positive and negative classes. On a fairly small training set, 3,800 short Tweets, the training takes way too long, approximately an hour and 15 mins. TextBlob goes  through ""Training (100 iterations)"" even when the Log Likelihood and Accuracy stopped changing after the 32 iteration. (On a side note, I have 12gb of RAM and 3.4ghz processor with not much else running in the background). </p>

<p>I see that the Textblob MaxEntClassifier is a wrapper around the NLTK. I found in the <a href=""https://textblob.readthedocs.io/en/dev/api_reference.html#textblob.classifiers.NLTKClassifier.train"" rel=""nofollow noreferrer"">Textblob docs</a> that mentioned ""train(*args, **kwargs) - Train the classifier with a labeled feature set and return the classifier. Takes the same arguments as the wrapped NLTK class...""</p>

<p>Since the TextBlob MaxEntClassifier train takes the same args as the wrapped NLTK class, I reviewed the NLTK docs for a param that might shorten the training time. In the <a href=""https://www.nltk.org/api/nltk.classify.html"" rel=""nofollow noreferrer"">NLTK.classify docs</a> under the MaxEnt Classifier I found the param ""min_lldelta=v: Terminate if a single iteration improves log likelihood by less than v."". I was hopeful that I could pass this param through TextBlob to the NLTK classifier to shorten the training time, however it seems to be ignored. </p>

<p>Is it possible to pass max_iter, min_ll, or min_lldelta through Textblob MaxEntClassifier to the NLTK to shorten the training time? If not, are there other ways to shorten this training time? </p>

<p>My snippet is below:</p>

<pre><code># Most code below from: https://stevenloria.com/simple-text-classification/

from textblob.classifiers import MaxEntClassifier

train = [
    ('I love this sandwich.', 'pos'),
    ('This is an amazing place!', 'pos'),
    ('I feel very good about these beers.', 'pos'),
    ('This is my best work.', 'pos'),
    (""What an awesome view"", 'pos'),
    ('I do not like this restaurant', 'neg'),
    ('I am tired of this stuff.', 'neg'),
    (""I can't deal with this"", 'neg'),
    ('He is my sworn enemy!', 'neg'),
    ('My boss is horrible.', 'neg')
]
test = [
    ('The beer was good.', 'pos'),
    ('I do not enjoy my job', 'neg'),
    (""I ain't feeling dandy today."", 'neg'),
    (""I feel amazing!"", 'pos'),
    ('Gary is a friend of mine.', 'pos'),
    (""I can't believe I'm doing this."", 'neg')
]

print(""\nTraining..."")
classifier = MaxEntClassifier(train, min_lldelta = 0.5)
print(""\nkwargs: "", classifier.format_kwargs)

print(""\nTesting..."")
results = classifier.accuracy(test)
</code></pre>
","4513967","","4513967","","2020-04-21 13:59:11","2020-04-21 13:59:11","How to train TextBlob MaxEntClassifier more quickly?","<python-3.x><nltk><sentiment-analysis><textblob>","0","0","","","","CC BY-SA 4.0"
"68773622","1","","","2021-08-13 14:00:22","","0","48","<p>I'm working on a script that will run through a column of article titles (read in from a .csv) in a data frame and calculate term frequency across the rows in this column using TF-IDF, list these terms with their TF-IDF values in a new df and then calculate an average share rate based on 2 other columns from the original data frame ((sum of all pageveiws for the articles where the keyword was found)/ (sum of all shares for the articles where the keyword was found) * 100).</p>
<p>Here is where I've reached:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from textblob import TextBlob, Word

vect = TfidfVectorizer()
dtm = vect.fit_transform(article_sentiment.title)
features = vect.get_feature_names()

word_scores = {}
for word in TextBlob(article_sentiment.title).words:
    word = word.lower()
    if word in features:
        word_scores[word] = dtm[features.index(word)]
</code></pre>
<p>But I receive the following error:</p>
<pre><code>TypeError: The `text` argument passed to `__init__(text)` must be a string, not &lt;class 'pandas.core.series.Series'&gt;
</code></pre>
<p>UPDATE:</p>
<p>Here is some progress but I've generated another error:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from textblob import TextBlob, Word
from sklearn.naive_bayes import MultinomialNB

vect = TfidfVectorizer()
dtm = vect.fit_transform(article_sentiment.title)
features = vect.get_feature_names()

article_titles = ' '.join(article_sentiment['title']).lower()

blob = TextBlob(article_titles).words

for word in blob:
    if word in features:
        word_scores[word] = dtm[features.index(word)]
scores = sorted(list(word_scores.items()), key=lambda x: x[1])

for word, score in scores:
    print(word, score)
</code></pre>
<p>The new error:</p>
<pre><code>IndexError: row index (994) out of range
</code></pre>
<p>And an example of the data:</p>
<pre><code>article_sentiment = {'title':['there’s always hope to be found','when women move forward, the world moves with them','a new collaboration to fight covid-19'], 'total_shares':[27, 53, 98], 'page_views':[2440.0, 5321.0, 4564.0]}
</code></pre>
","9338592","","9338592","","2021-08-13 19:11:58","2021-08-13 19:11:58","Find the top terms based off on TF-IDF scores across an entire column in a data frame","<python><pandas><scikit-learn><tf-idf><textblob>","0","6","","","","CC BY-SA 4.0"
"69276457","1","","","2021-09-21 23:07:32","","0","28","<p>I faced some issues when I tried to use textblob to analyze <em>10million data</em>.</p>
<p>For the data, I've also used Vader packages to handle the same data, it worked well.</p>
<p>My original code was:</p>
<pre><code>def textblob_polarity(text):
    polarity = []
    for mess in text:
        mood = TextBlob(mess)
        polarity.append(mood.sentiment.polarity)
    return polarity

textblob_polarity = textblob_polarity(data['body'])
data['textblob_polarity'] = textblob_polarity
</code></pre>
<p>However I've got TypeError: The <code>text</code> argument passed to <code>__init__(text)</code> must be a string, not &lt;class 'float'&gt;</p>
<p>I have tried:</p>
<pre><code>def remove_floats(row):
    if isinstance(row, str):
        return row
    else:
        return None

for i in data['body']:
    data['body'].apply(lambda i: remove_floats(i))
</code></pre>
<p>But it takes forever and didn't work out.</p>
<p>I also tried:</p>
<pre><code>for i in data['body']:
    i = str(i)
</code></pre>
<p>It doesn't work as well. I'll get the same error as I stated in the title of this question.</p>
<p>This is a small sample of the data.
<a href=""https://i.stack.imgur.com/eHHjI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eHHjI.png"" alt=""enter image description here"" /></a></p>
<p>Do you have any ideas? Thank you so much in advance!</p>
","11214170","","5946921","","2021-09-21 23:11:36","2021-09-22 14:52:43","TextBlob --- The `text` argument passed to `__init__(text)` must be a string, not <class 'float'>","<python><typeerror><sentiment-analysis><textblob>","1","2","","","","CC BY-SA 4.0"
"43724280","1","","","2017-05-01 18:24:24","","-5","165","<p>I want to use the Naive Bayes algorithm available on Text blob package in python.
Does it classify ""I love terrorism and I hate peace"" and ""I love peace and hate terrorism"" the same way?</p>
","7038677","","","","","2017-05-01 19:29:20","Does Naive Bayes algorithm classify words or phrases?","<python><machine-learning><text-classification><naivebayes><textblob>","1","2","","","","CC BY-SA 3.0"
"69377961","1","","","2021-09-29 14:04:09","","1","20","<p>I have reviews for products in a dataframe 'dfa' from customers in the format below. I want to add another column with score/flag on how 'angry' the review sounds.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Service_id</th>
<th style=""text-align: left;"">Review</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">a1</td>
<td style=""text-align: left;"">Pathetic service, waste of money</td>
</tr>
<tr>
<td style=""text-align: left;"">b2</td>
<td style=""text-align: left;"">The service was average and the cleanliness could have been better</td>
</tr>
<tr>
<td style=""text-align: left;"">v2</td>
<td style=""text-align: left;"">satisfied</td>
</tr>
</tbody>
</table>
</div>
<p>In the example above, positivity and anger will be treated differently. b2 has a negative comment but it should not be considered angry.</p>
<p>Example (output)</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Service_id</th>
<th style=""text-align: left;"">Review</th>
<th style=""text-align: left;"">Anger_flag</th>
<th style=""text-align: left;"">Anger_score</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">a1</td>
<td style=""text-align: left;"">Pathetic service, waste of money</td>
<td style=""text-align: left;"">Y</td>
<td style=""text-align: left;"">0.9</td>
</tr>
<tr>
<td style=""text-align: left;"">b2</td>
<td style=""text-align: left;"">The service was average and the cleanliness could have been better</td>
<td style=""text-align: left;"">N</td>
<td style=""text-align: left;"">0.2</td>
</tr>
<tr>
<td style=""text-align: left;"">v2</td>
<td style=""text-align: left;"">satisfied</td>
<td style=""text-align: left;"">N</td>
<td style=""text-align: left;"">0.0</td>
</tr>
</tbody>
</table>
</div>
<p>Also like the profanity library, does python have a list of words to detect anger ?</p>
","17034129","","17034129","","2021-09-29 14:11:46","2021-10-01 11:28:11","Get score on fileds that detect angry behaviour - Python Sentiment Analysis","<python><nlp><sentiment-analysis><textblob>","1","1","","","","CC BY-SA 4.0"
"54588807","1","54589943","","2019-02-08 08:53:19","","0","378","<p>I have 47 news-articles that I want to extract the sentiment from. They are JSON format (Date, title and body of the article). All I want is to obtain a list with the sentiment using TextBlob. So far I am doing the following:</p>

<pre><code>import json
import pandas
from textblob import TextBlob

appended_data = []

for i in range(1,47):
    df0 = pandas.DataFrame([json.loads(l) for l in open('News_%d.json' % i)])
    appended_data.append(df0)


appended_data = pandas.concat(appended_data)

doc_set = appended_data.body
docs_TextBlob = TextBlob(doc_set)


for i in docs_TextBlob:
    print(docs_TextBlob.sentiment)
</code></pre>

<p>Obvioulsy, I get the following error: <code>TypeError: The text argument passed to __init__(text) must be a string, not &lt;class 'pandas.core.series.Series'&gt;</code> Any idea on how to create a list with the sentiment measure?</p>
","5510540","","5510540","","2019-02-08 09:50:43","2019-02-08 10:02:54","Loop to retrieve sentiment analysis in pandas.core.series.Series","<python><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 4.0"
"44346768","1","44346790","","2017-06-03 17:31:13","","3","77","<p>I'm trying to install textblob on my system. By following the documentation, I had run the command </p>

<pre><code>pip3 install -U textblob
</code></pre>

<p>But while running the command, I'm getting an error like this.</p>

<p><img src=""https://i.stack.imgur.com/WUhRa.png"" alt=""Error message while installing textblob""></p>
","5986489","","2422776","","2017-06-03 17:32:04","2017-06-03 17:33:09","Error while installing textblob on ubuntu 14.04","<python-3.x><ubuntu-14.04><textblob>","1","0","","","","CC BY-SA 3.0"
"61101846","1","","","2020-04-08 13:27:11","","0","70","<p>def sentiment_analysis(polarity):</p>
<pre><code>if x &lt; 0:
    print(&quot;neg&quot;)
elif x &gt;0:
        print(&quot;pos&quot;)
else:
            print(&quot;neutral&quot;)


    
#analysis the text sentiment
text = processed_file.get('1.0',tk.END)
new_text = TextBlob(text)
processed_text = new_text.sentiment
result = '\nSubjectivity:{}, Polarity
                {}'.format(new_text.sentiment.subjectivity,new_text.sentiment.polarity)
x= result.sentiment.polarity
return sentiment_analysis(x)
show.insert(tk.END,x)
                     
</code></pre>
<h1>Can someone explain why I am getting an typeerror</h1>
","9883635","","-1","","2020-06-20 09:12:55","2020-04-08 14:40:31","TypeError: sentiment_analysis() missing 1 required positional argument: 'polarity'?","<python><nlp><textblob>","1","0","","","","CC BY-SA 4.0"
"69194625","1","69242389","","2021-09-15 14:00:06","","2","501","<p>I have a 5 line simple program to translate a language to English via OCR Textblob.
But for some reason, it throws 404 error!!!</p>
<pre><code>from textblob import TextBlob

text = u&quot;おはようございます。&quot;
tb = TextBlob(text)
translated = tb.translate(to=&quot;en&quot;)
print(translated)
</code></pre>
<p><a href=""https://i.stack.imgur.com/0O9i7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0O9i7.png"" alt=""enter image description here"" /></a></p>
<p>The Textblob is installed and the version is 0.15.3</p>
<pre><code>$ pip install -U textblob
$ python -m textblob.download_corpora
</code></pre>
<p>Thank you</p>
","12487489","","","","","2021-09-19 11:03:28","Textblob OCR throws 404 error when trying to translate to another language","<ocr><textblob>","1","0","","","","CC BY-SA 4.0"
"69314231","1","","","2021-09-24 11:25:02","","0","7","<p><code>TextBlob</code> is great, but its lemmatization seems to be context dependent. Unfortunately:</p>
<pre class=""lang-py prettyprint-override""><code>TextBlob(&quot;can't this {example} text [keep] braces (even if that makes no sense).&quot;).words
</code></pre>
<p>yields</p>
<pre class=""lang-py prettyprint-override""><code>WordList(['ca', &quot;n't&quot;, 'this', 'example', 'text', 'keep', 'braces', 'even', 'if', 'that', 'makes', 'no', 'sense'])
</code></pre>
<p>when in this instance I would like:</p>
<pre class=""lang-py prettyprint-override""><code>WordList([
    'ca', &quot;n't&quot;, 'this', '{example}', 'text', 
    '[keep]', 'braces', '(', 'even', 'if', 
    'that', 'makes', 'no', 'sense', ')', '.'
])
</code></pre>
<p>How can I configure <code>TextBlob</code> to handle this scenario?</p>
<p>Question:
What if contract is in theses braces?
Then ignore it e.g.</p>
<pre class=""lang-py prettyprint-override""><code>[x] &quot;{can't}&quot; -&gt; &quot;{can't}&quot; # this
[ ] &quot;{can't}&quot; -&gt; &quot;{ca&quot;, &quot;n't}&quot; # not this
</code></pre>
","5623899","","","","","2021-09-24 11:25:02","Python 3.7+ How to update TextBlob processing to not process words in braces e.g. {}[]()?","<python><nlp><textblob>","0","0","","","","CC BY-SA 4.0"
"28741590","1","","","2015-02-26 11:44:36","","2","627","<p>How does textblob calculate polarity in sentiment analysis? What logic does it follow and can we change the logic?</p>

<p>Thank you.</p>
","4162573","","","","","2015-05-18 07:11:10","Logic behind the polarity score calculated by TEXTBLOB?","<python-2.7><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 3.0"
"43660638","1","43723701","","2017-04-27 14:27:05","","0","557","<p>I am making a classifier using the NaiveBayes model to classify a user's requests for location, weather, etc...</p>

<p>The classifications I am returning look like this:</p>

<pre><code>(""What is the weather in Copenhagen"", ""weather"")
</code></pre>

<p>These requests are then trained and compared against a test set, which return the accuracy. This a works well.</p>

<pre><code>c = NaiveBayesClassifier(train_set)
self.classifier = Blobber(analyzer=NaiveBayesAnalyzer(), classifier=c)
print(c.accuracy(test_set))
</code></pre>

<p>I am running this method to classify new phrases</p>

<pre><code>    def classify_phrase(self, tb_phrase):
        return self.classifier(tb_phrase).classify()
</code></pre>

<p>However, when I try to classify a new phrase that doesn't fall into one of my classification or is an error from the user it still tries to classify it as a request. Example below:</p>

<pre><code>(""Where is Bob"", ""location"")
</code></pre>

<p>This should return an error, but it does not. Is there a way to get the accuracy from the Textblob on a newly entered phrase? So that when I enter a phrase, it will tell me the accuracy of that phrase. I am using the Textblob Package and Python 3. If more information is needed, the full code is under the NaturalLanguage.py file on my <a href=""https://github.com/brandenk514/hal"" rel=""nofollow noreferrer"">GitHub</a>. Thank you in advance.</p>
","4926845","","","","","2017-05-01 17:43:34","How to get the accuracy of new phrases with TextBlob?","<python><nltk><textblob>","1","0","","","","CC BY-SA 3.0"
"43871019","1","43912328","","2017-05-09 13:20:19","","1","1033","<p>How is the polarity of a word in a sentence calculated using PatternAnalyser of Text Blob? </p>
","7617315","","","","","2017-05-11 10:03:29","Polarity calculation in Sentiment Analysis using TextBlob","<sentiment-analysis><textblob>","1","0","","","","CC BY-SA 3.0"
"52804635","1","","","2018-10-14 16:14:33","","1","48","<p>I want to detect the correct email address but my code is giving me the tag with major probability in the dataset and obviously it don't work like i expected. </p>

<p>Here is the code:</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
files = [
  (""data_train/email_positive.txt"", ""yes""), 
  (""data_train/email_negative.txt"", ""no"")
]
train = []; cl = None

for file_txt in files:   
    email_train_raw = []        
    with open(file_txt[0]) as f: 
        email_train_raw = f.readlines()

    for email in email_train_raw:
        e = email.replace(""\n"", """")
        train.append( (e, file_txt[1]) )

cl = NaiveBayesClassifier(train)
print cl.classify(""wrong_email@2x.png"")
# Output: yes 
# it would be: ""no""
</code></pre>

<p>Some correct email dataset: </p>

<pre><code>hello@3commerceinc.com
sales@ablefreight.com
dispatchwaycross@absolutewl.com
ops@absolutewl.com
tol@absolutewl.com
email@gmail.com
email@hotmail.com
. . . 
</code></pre>

<p>Some incorrect email dataset: </p>

<pre><code>pause@2x.png
video@2x.png
right@2x.png
play@2x.png
circle-hover@2x.png
preloader@2x.gif
left@2x.png
circle@2x.png
. . . 
</code></pre>
","10503212","","","","","2018-10-14 16:14:33","How to detect correct email address with machine learning using textblob?","<python><machine-learning><textblob>","0","2","","","","CC BY-SA 4.0"
"27210178","1","","","2014-11-30 04:57:28","","0","692","<p>The link below shows a similar issue except that his was solved downloading the packages and I already downloaded the packages...
<a href=""https://stackoverflow.com/questions/26570944/resource-utokenizers-punkt-english-pickle-not-found"">Resource u&#39;tokenizers/punkt/english.pickle&#39; not found</a></p>

<p>But The weird thing is that I am able to run this on the terminal with no errors, but I have a js file doing a ajax call to this .py file and when it tries to execute it. It returns that error.. but Im not sure why</p>

<pre><code>Errors more 
Resource u'tokenizers/punkt/english.pickle' not found.  Please
    use the NLTK Downloader to obtain the resource:  &gt;&gt;&gt;
    nltk.download()
    Searched in:
    - '/var/www/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - u'



Traceback (most recent call last):
  File ""/var/www/CSCE-470-Anime-Recommender/py/app.py"", line 40, in &amp;lt;module&amp;gt;
    cl = NaiveBayesClassifier(Functions.classify(UserData))
  File ""/usr/local/lib/python2.7/dist-packages/textblob/classifiers.py"", line 192, in __init__
    self.train_features = [(self.extract_features(d), c) for d, c in self.train_set]
  File ""/usr/local/lib/python2.7/dist-packages/textblob/classifiers.py"", line 169, in extract_features
    return self.feature_extractor(text, self.train_set)
  File ""/usr/local/lib/python2.7/dist-packages/textblob/classifiers.py"", line 81, in basic_extractor
    word_features = _get_words_from_dataset(train_set)
  File ""/usr/local/lib/python2.7/dist-packages/textblob/classifiers.py"", line 63, in _get_words_from_dataset
    return set(all_words)
  File ""/usr/local/lib/python2.7/dist-packages/textblob/classifiers.py"", line 62, in &amp;lt;genexpr&amp;gt;
    all_words = chain.from_iterable(tokenize(words) for words, _ in dataset)
  File ""/usr/local/lib/python2.7/dist-packages/textblob/classifiers.py"", line 59, in tokenize
    return word_tokenize(words, include_punc=False)
  File ""/usr/local/lib/python2.7/dist-packages/textblob/tokenizers.py"", line 72, in word_tokenize
    for sentence in sent_tokenize(text))
  File ""/usr/local/lib/python2.7/dist-packages/textblob/base.py"", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File ""/usr/local/lib/python2.7/dist-packages/textblob/decorators.py"", line 38, in decorated
    raise MissingCorpusError()
MissingCorpusError: 
Looks like you are missing some required data for this feature.

To download the necessary data, simply run

    python -m textblob.download_corpora

or use the NLTK downloader to download the missing data: http://nltk.org/data.html
If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
</code></pre>
","3537288","","-1","","2017-05-23 12:27:28","2014-12-01 01:29:56","resource not found python textblob Resource u'tokenizers/punkt/english.pickle' not found","<python><ajax><textblob>","1","0","","","","CC BY-SA 3.0"
"54916010","1","","","2019-02-27 23:16:01","","1","624","<p>I was looking into TextBlob to calculate sentiment scores (polarity, subjectivity) for a list of articles on an excel sheet I've compiled.</p>

<p>Below is an example of the sheet:</p>

<blockquote>
  <p>11/03/2004 04:03 At least 60 people were killed in three bomb attacks
  on crowded Madrid trains in Spain's worst-ever terrorist attack, said
  Efe newswire and other media. Red Cross said at least 200 people were
  injured. ``This is a massacre,'' said Socialist party leader Jose Luis
  Rodriguez Zapatero, who blamed Basque terrorist group ETA.</p>
  
  <p>07/07/2005 04:41 London closed its subway system and evacuated all
  stations after emergency services were called to explosions in and
  around the financial district.</p>
  
  <p>01/12/2009 04:00 American International Group, Inc. (AIG) today
  announced that it has closed two previously announced transactions
  with the Federal Reserve Bank of New York (FRBNY) that have reduced
  the debt AIG owes the FRBNY by $25 billion in exchange for the FRBNY’s
  acquisition of preferred equity interests in certain newly formed
  subsidiaries.</p>
  
  <p>22/08/2013 11:38 NASDAQ shuts down for 3 hours due to a computer
  problem</p>
</blockquote>

<p>I've been able to use textblob the most simples way by doing each line individually as so:</p>

<pre><code>analysis = TextBlob(""NASDAQ shuts down for 3 hours due to a computer problem"")
print(analysis.sentiment)
</code></pre>

<p>What I'd like import my excel file containing date &amp; time and the articles in two columns and move on to loop over each row to calculate polarity and subjectivity scores and save it in the file.</p>

<p>I've tried modifying a code on Thomson Reuters News Analytics this way:</p>

<pre><code>import pandas as pd
import numpy as np
from textblob import TextBlob

path_to_file = ""C:/Users/Parvesh/Desktop/New Project/Sentiment Analysis/events.csv""
df = pd.read_csv(path_to_file, encoding='latin-1')
df.head()

df['Polarity'] = np.nan
df['Subjectivity'] = np.nan

pd.options.mode.chained_assignment = None

for idx, articles in enumerate(df['articles'].values):  # for each row in our df dataframe
    sentA = TextBlob(""articles"")  # pass the text only article to TextBlob to analyze
    df['Polarity'].iloc[idx] = sentA.sentiment.polarity  # write sentiment polarity back to df
    df['Subjectivity'].iloc[idx] = sentA.sentiment.subjectivity  # write sentiment subjectivity score back to df
df.head()

df.to_csv(""out.csv"", index=False)
</code></pre>

<p>The code is not working though...I'm not getting any scores.</p>

<p>Any advice on how I can this done?</p>

<p>I am a complete newbie to Python (I am using Pycharm). I code mostly on Stata and Matlab.</p>

<p>Please help!</p>
","11127817","","","","","2019-03-22 22:47:52","TextBlob - Loop over articles to calculate polarity & subjectivity scores","<python><textblob>","2","0","1","","","CC BY-SA 4.0"
"44234887","1","","","2017-05-29 05:00:06","","0","991","<pre><code>from textblob import TextBlob as tb
from sqlalchemy import create_engine
import pandas as pd
</code></pre>

<p>first i had created engine using sqlalchemy as <code>engine=create_engine(""mysql+mysqldb://root:ja@localhost:3306/listing"")</code></p>

<p>Then i used pandas read_sql command to read the data from database.</p>

<p><code>df=pd.read_sql('select locationId,text from location_reviews',engine)</code></p>

<p>I am getting this error when trying to convert the text column from string to textblob <code>UnicodeDecodeError: 'ascii' codec can't decode byte 0x93 in position 284: ordinal not in range(128)</code>
I am using sqlalchemy and <code>df = pd.read_sql(query,engine)</code> for reading the data from sql.
Then i tried to convert the text column in textblob using </p>

<pre><code>df['text']=df.text.apply(lambda x: tb(x))
</code></pre>

<p>and getting the above error.</p>
","4654001","","4654001","","2017-05-29 19:16:47","2017-05-29 19:16:47","UnicodeDecodeError: 'ascii' codec can't decode byte 0x93 in position 284: ordinal not in range(128)","<python><nltk><textblob>","1","1","","2017-05-29 14:00:32","","CC BY-SA 3.0"
"62051560","1","","","2020-05-27 19:42:46","","0","241","<p>I'm trying to apply spelling correction to some tweets. </p>

<p>I have tried the following code: 
<code>train.text.apply(lambda x: "" "".join([TextBlob(i).correct() for i in x.split()]))</code></p>

<p>However, I get an error. Could you please advise? Thanks in advance!</p>
","11888944","","","","","2020-05-28 16:50:17","How to use TextBlob.correct() for a column in a dataframe","<python><nlp><textblob><spelling>","1","0","","","","CC BY-SA 4.0"
"69645220","1","69646185","","2021-10-20 11:33:23","","0","27","<p>Is it possible to make a particular blob non delete able? If so, how?  I want one blob to stay forever and no one should be able to delete that.</p>
","2526892","","","","","2021-10-20 12:42:26","How to prevent deletion of a blob?","<blob><azure-blob-storage><textblob>","1","0","","","","CC BY-SA 4.0"
"69655526","1","","","2021-10-21 03:30:23","","0","20","<p>Hi I am seriously confused with how to conduct a NLP.
My goal is to conduct sentiment analysis on a 'Review' of a product.</p>
<pre><code>data = pd.read_excel('ProductData.xlsx')
print(data.dtypes)

Clothing ID                 int64
Age                         int64
Review Text                object
</code></pre>
<p>the ['Review Text'] column is an <strong>object</strong></p>
<pre><code>from textblob import TextBlob
blob= TextBlob(data['Review Text'])
</code></pre>
<pre><code>TypeError: The `text` argument passed to `__init__(text)` must be a string, not &lt;class 'pandas.core.series.Series'&gt;
</code></pre>
<p>must be a <strong>string</strong></p>
<pre><code>data['Review Text'] = str(data['Review Text']) 
print(data['Review Text'].dtype)

= object
</code></pre>
<p>Could someone please help me tackle this problem.
How do I approach the task on performing a sentiment analysis on my data feature?</p>
<p>thankyou</p>
","15579379","","","","","2021-10-21 04:16:31","NLP on a xlsx worksheet - single column","<python><pandas><string><nlp><textblob>","1","4","","","","CC BY-SA 4.0"
"26551232","1","26585654","","2014-10-24 15:31:39","","0","1584","<p>I am trying to implement function for text preprocessing in PySpark. I have amazon EMR where I am installing Python dependencies from the bootstrap script. One of these dependencies is textblob ""python -m textblob.download_corpora"". Then I am trying to use it locally on all the machines without any problem.</p>

<p>But when I am trying to run it from Spark then I am getting following error:</p>

<pre><code>INFO: File ""/home/hadoop/spark/python/pyspark/rdd.py"", line 1324, in saveAsTextFile
INFO: keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)
INFO: File ""/home/hadoop/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
INFO: File ""/home/hadoop/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
INFO: py4j.protocol.Py4JJavaError: An error occurred while calling o54.saveAsTextFile.
INFO: : org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 4 times, most recent failure: Lost task 8.3 in stage 1.0 (TID 40, ip-172-31-3-125.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
INFO: File ""/home/hadoop/spark/python/pyspark/worker.py"", line 79, in main
INFO: serializer.dump_stream(func(split_index, iterator), outfile)
INFO: File ""/home/hadoop/spark/python/pyspark/serializers.py"", line 127, in dump_stream
INFO: for obj in iterator:
INFO: File ""/home/hadoop/spark/python/pyspark/rdd.py"", line 1316, in func
INFO: for x in iterator:
INFO: File ""/home/hadoop/pyckage/package_topics/package_topics/preprocessor.py"", line 40, in make_tokens
INFO: File ""./package_topics.zip/package_topics/data_utils.py"", line 76, in preprocess_text
INFO: for noun_phrase in TextBlob(' '.join(tokens)).noun_phrases
INFO: File ""/usr/lib/python2.6/site-packages/textblob/decorators.py"", line 24, in __get__
INFO: value = obj.__dict__[self.func.__name__] = self.func(obj)
INFO: File ""/usr/lib/python2.6/site-packages/textblob/blob.py"", line 431, in noun_phrases
INFO: for phrase in self.np_extractor.extract(self.raw)
INFO: File ""/usr/lib/python2.6/site-packages/textblob/en/np_extractors.py"", line 138, in extract
INFO: self.train()
INFO: File ""/usr/lib/python2.6/site-packages/textblob/decorators.py"", line 38, in decorated
INFO: raise MissingCorpusError()
INFO: MissingCorpusError:
INFO: Looks like you are missing some required data for this feature.
INFO: 
INFO: To download the necessary data, simply run
INFO: 
INFO: python -m textblob.download_corpora
INFO: 
INFO: or use the NLTK downloader to download the missing data: http://nltk.org/data.html
INFO: If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
INFO: 
INFO: 
INFO: org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:124)
INFO: org.apache.spark.api.python.PythonRDD$$anon$1.&lt;init&gt;(PythonRDD.scala:154)
INFO: org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)
INFO: org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
INFO: org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
INFO: org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
INFO: org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
INFO: org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
INFO: org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
INFO: org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
INFO: org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
INFO: org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
INFO: org.apache.spark.scheduler.Task.run(Task.scala:54)
INFO: org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
INFO: java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
INFO: java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
INFO: java.lang.Thread.run(Thread.java:745)
INFO: Driver stacktrace:
INFO: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
INFO: at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
INFO: at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
INFO: at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
INFO: at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
INFO: at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
INFO: at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
INFO: at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
INFO: at scala.Option.foreach(Option.scala:236) 
INFO: at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
INFO: at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
INFO: at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
INFO: at akka.actor.ActorCell.invoke(ActorCell.scala:456)
INFO: at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
INFO: at akka.dispatch.Mailbox.run(Mailbox.scala:219)
INFO: at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
INFO: at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
INFO: at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
INFO: at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
INFO: at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) 
</code></pre>

<p>I am trying to run both Spark and single node scripts under same user.
Does anybody has some idea what can possibly be wrong?</p>
","1499038","","1243590","","2015-02-05 00:26:34","2015-02-05 00:26:34","PySpark textblob from NLTK used in map MissingCorpusError","<apache-spark><nltk><emr><textblob><pyspark>","1","0","","","","CC BY-SA 3.0"
"26393044","1","","","2014-10-15 22:14:28","","0","823","<p>I have a CSV file I am trying to train on a classifier. I am using TextBlob. This is my code that does it.</p>

<pre><code>with open('train.csv', 'r') as fp:
    cl = NaiveBayesClassifier(fp, format='csv', feature_extractor= get_features)
</code></pre>

<p>It however does not seem to work. Is this the correct way to train a classifier using a CSV file?</p>
","3078335","","","user2555451","2014-11-18 16:17:33","2015-05-25 08:18:03","Train data from csv python textblob","<python><csv><nlp><training-data><textblob>","1","3","2","","","CC BY-SA 3.0"
"18174482","1","18213506","","2013-08-11 17:08:19","","4","14447","<p>This morning I set out to install the TextBlob module found at <a href=""https://textblob.readthedocs.org/en/latest/index.html"" rel=""nofollow"">https://textblob.readthedocs.org/en/latest/index.html</a></p>

<p>Per the installation documentation I first ran:</p>

<p><code>pip install -U textblob</code></p>

<p>Now if I run that command I get the following:</p>

<pre><code>Requirement already up-to-date: textblob in /Library/Python/2.7/site-packages/textblob-0.5.0-py2.7.egg

Requirement already up-to-date: PyYAML in /Library/Python/2.7/site-packages (from textblob)

Cleaning up...
</code></pre>

<p>While it would appear to be installed, when I try to run a one line file (wherein the only line is <code>from text.blob import TextBlob</code> I am told <code>ImportError: No module named blob</code></p>

<p>So then I tried to install from git, first I cloned the repository and then I ran both of the following snippets</p>

<p><code>sudo python setup.py install</code></p>

<p>and</p>

<p><code>sudo python2.7 setup.py install</code></p>

<p>Unfortunately neither one of those commands solved my issue. So now im stuck. I suppose the best course of action is to start over but im pretty much lost.</p>
","1672804","","","user2555451","2014-11-15 17:23:05","2021-02-20 20:21:33","Unable to get up and running with TextBlob","<python><pip><textblob>","2","2","1","","","CC BY-SA 3.0"
"53346440","1","","","2018-11-16 22:54:13","","0","16104","<p>I installed text blob with the line below on my PC:</p>

<pre><code>$ git clone https://github.com/sloria/TextBlob.git
</code></pre>

<p>This then happened:</p>

<pre><code>pcarrera@LAP-JYT456465 ~/Python
$ git clone https://github.com/sloria/TextBlob.git
Cloning into 'TextBlob'...
remote: Enumerating objects: 7, done.
remote: Counting objects: 100% (7/7), done.
remote: Compressing objects: 100% (6/6), done.
remote: Total 3729 (delta 1), reused 0 (delta 0), pack-reused 3722
Receiving objects: 100% (3729/3729), 7.96 MiB | 10.79 MiB/s, done.
Resolving deltas: 100% (2054/2054), done.
</code></pre>

<p>I wanted to test it with the simple script below:</p>

<pre><code>from textblob import textblob

text = ''' The movie was great. The movie was bad. The movie was really bad 
'''

blob = textblob(text)

for sentence in blob.sentences:
    print(sentence.sentiment.polarity)
</code></pre>

<p>But I got this error and I do not know how to move forward:</p>

<pre><code>Traceback (most recent call last):
  File ""textblob_install.py"", line 1, in &lt;module&gt;
    from textblob import textblob
ImportError: No module named textblob
</code></pre>

<p>Please help (new to Python)</p>
","10665207","","6454387","","2018-11-17 00:41:13","2021-06-13 11:45:47","No Module named textblob","<python><textblob>","5","1","","","","CC BY-SA 4.0"
"35559199","1","","","2016-02-22 16:48:37","","2","15055","<p>I have a csv file with around 50 rows of sentences. I'm using the textblob sentiment analysis tool. To test the polarity of a sentence, the example shows you write a sentence and the polarity and subjectivity is shown. However, it only works on a single sentence, I want it to work for the csv file that I have, as I can't put in each row and test them individually as it would take too long. How would I go about doing this? </p>

<p>TextBlob show this example, when I type in a sentence, the polarity shows, you can't input two sentences at one time, it doesn't let you. How would i input my csv file into the example below to give me the polarity for all rows?</p>

<pre><code>&gt;&gt;&gt; testimonial = TextBlob(""Textblob is amazingly simple to use. What great fun!"")
&gt;&gt;&gt; testimonial.sentiment
Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)
&gt;&gt;&gt; testimonial.sentiment.polarity
0.39166666666666666
</code></pre>

<p>edited chishaku solution and it worked for me. Solution:</p>

<pre><code>import csv
from textblob import TextBlob

infile = 'xxx.csv'

with open(infile, 'r') as csvfile:
    rows = csv.reader(csvfile)
    for row in rows:
        sentence = row[0]
        blob = TextBlob(sentence)
        print blob.sentiment
</code></pre>
","4705117","","1060350","","2016-02-22 21:10:48","2018-03-08 18:03:12","Textblob sentiment analysis on a csv file","<python><sentiment-analysis><textblob>","2","5","4","","","CC BY-SA 3.0"
"35647205","1","","","2016-02-26 08:48:36","","0","259","<p>I have about 300 participants who generated 45 short situations each (though many participants wrote the same situation) and rated the degree that they would feel anxious in that situation (1-5 likert scale).</p>

<p>Sample data:</p>

<pre><code>train = [('being at a sports game', '1'), ('selecting group members for a group project', '2'), ('interacting with overly dressed people', '1'), ('partnering up with a stranger for a class', '3'), ('having your presentation criticized by an audience member', '4'), ('being in a situation you cannot control', '3')]
</code></pre>

<p>I was wondering if I might be able to use textblob in python to determine if there are any common words that would lead to someone scoring higher on this single item likert scale.</p>

<p>What I have so far is the simple category classification system.</p>

<pre><code>#Import libraries
from textblob.classifiers import NaiveBayesClassifier

#Train the NBC with training data
cl = NaiveBayesClassifier(train)

#Examine Training Accuracy
cl.accuracy(train)

#Generates a list of informative features
cl.show_informative_features(50)
</code></pre>

<ol>
<li><p>How could I get this to take into account that this is interval (or some would say ordinal) data rather than nominal? (e.g., the inclusion of the word ""stranger"" leads to an X% increase in anxiety.)</p></li>
<li><p>If this is impossible (or even if it isn't), is there any way to print the percent probability of a given situation being labeled in each likert category? (e.g., 1 = 0.08; 2 = 0.23, 3 = 0.44, 4 = 0.19, 5 = 0.06)</p></li>
</ol>

<p>Any recommendations would be much appreciated.</p>
","1544858","","1544858","","2016-02-27 03:42:51","2016-02-27 03:42:51","Can I use textblob to predict a likert scale outcome?","<python><scikit-learn><nltk><textblob>","0","4","","","","CC BY-SA 3.0"
"28975339","1","","","2015-03-10 22:31:43","","1","723","<p>I am trying to train a classifier with Textblob by loading my training set from a CSV file. The text in the CSV should be in UTF-8.
When I try to run my code:</p>

<pre><code># -*- coding: utf-8 -*-
from textblob.classifiers import NaiveBayesClassifier
with open('trainingset.csv', 'r') as fp:
   cl = NaiveBayesClassifier(fp, format=""csv"")
</code></pre>

<p>I get the following error:</p>

<pre><code>UnicodeDecodeError: 'utf8' codec can't decode byte 0x92 in position 31:    invalid start byte
</code></pre>

<p>However, it seems that Texblob is using UTF-8 to encode the CSV (I took a look at the source code for the CSV opener, which can be found <a href=""http://textblob.readthedocs.org/en/latest/_modules/textblob/formats.html"" rel=""nofollow"">here)</a>.</p>

<p>So I really can't understand why I am getting this error. Any help to get around this?</p>
","2447387","","2447387","","2015-03-10 22:43:45","2015-03-10 22:43:45","Training a classifier with Textblob from csv in python - encoding issues","<python><csv><text><encoding><textblob>","1","0","","","","CC BY-SA 3.0"
"54927713","1","","","2019-02-28 14:15:49","","1","50","<p>I’m currently using TextBlob to make a chatbot, and I’ve so far been extracting named entities using noun phrase extraction and finding the pos tag NNP. When entering a test user question such as ‘Will Smith’s latest single?’, I am correctly retrieving ‘Will Smith’. But I want to be able to search not only ‘will smith’ but ‘william smith’ ‘bill smith’ ‘willie smith’ ‘billy smith’ - basically other popularly known variations of the name in English language. I am using the Spotipy API as I am trying to retrieve Spotify artists. What I'm currently doing in PyCharm:</p>

<pre><code>while True:
    response = input()
    searchQuery = TextBlob(response)
    who = []
    for item, tag in searchQuery.tags:
        if tag == ""NNP"":
            for nounPhrase in searchQuery.noun_phrases:
                np = TextBlob(nounPhrase)
                if item.lower() in np.words:
                    if nounPhrase not in who:
                        who.append(nounPhrase)

    print(who)
        if who:
            for name in who:
                if spotifyObject.search(name, 50, 0, 'artist', None):
                    searchResults = spotifyObject.search(name, 50, 0, 'artist', None)
                    artists = searchResults['artists']['items']
                    for a in artists:
                        print(a['name'])
</code></pre>
","8119860","","8119860","","2019-02-28 14:53:24","2019-02-28 22:08:59","How do I create a search using NLP techniques which searches an inputted named entity as well as any potential name variations it may have?","<search><nlp><nltk><textblob><natural-language-processing>","1","2","0","","","CC BY-SA 4.0"
"55168908","1","55169383","","2019-03-14 17:41:26","","2","1568","<p>I am doing a quick sentiment analysis console application with Python, TextBlob and NLTK.</p>

<p>Currently i am using a link to a wiki article in spanish, so i don't need to translate it and i can use the nltk spanish stopword list, but what if i wanted to make this code work for different language links?</p>

<p>If i use the line <code>TextFinal=TextFinal.translate(to=""es"")</code> below <code>textFinal=TextBlob(texto)</code>  (code below) i get an error since it can't translate spanish into spanish. </p>

<p>Could i prevent this just by using a try/catch? Is there a way to make the code try to translate to different languages (as well as using different stopword list) depending on the language of the links im feeding to the application?</p>

<pre><code>import nltk
nltk.download('stopwords')
from nltk import  word_tokenize
from nltk.corpus import stopwords
import string
from textblob import TextBlob, Word
import urllib.request
from bs4 import BeautifulSoup

response = urllib.request.urlopen('https://es.wikipedia.org/wiki/Valencia')
html = response.read()

soup = BeautifulSoup(html,'html5lib')
text = soup.get_text(strip = True)


tokens = word_tokenize(text)
tokens = [w.lower() for w in tokens]

table = str.maketrans('', '', string.punctuation)
stripped = [w.translate(table) for w in tokens]
words = [word for word in stripped if word.isalpha()]

stop_words = set(stopwords.words('spanish'))

words = [w for w in words if not w in stop_words]

with open('palabras.txt', 'w') as f:
    for word in words:
        f.write("" "" + word)

with open('palabras.txt', 'r') as myfile:
    texto=myfile.read().replace('\n', '')


textFinal=TextBlob(texto)

print (textFinal.sentiment)

freq = nltk.FreqDist(words)

freq.plot(20, cumulative=False)
</code></pre>
","8954991","","","","","2019-03-14 18:28:35","Python TextBlob translate issue","<python><nltk><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 4.0"
"65029917","1","","","2020-11-26 22:24:05","","0","99","<p>I have news articles, I want to find certain words such as &quot;inflation&quot;, &quot;covid19&quot;, &quot;deficit, and &quot;central bank&quot; simultaneously. I have given below the sample codes and output. Everything works fine except for &quot;central bank&quot; which consist of two words and other are single words. <strong>I am unable to add a search for the combination of words such as a central bank. Thanks a lot in advance!</strong></p>
<pre><code>import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from textblob import TextBlob
data = pd.read_csv(&quot;/content/OC2020.csv&quot;,parse_dates=[0], infer_datetime_format=True)
data.columns = ['date','title','body_text' ]
data['body_text']= data['body_text'].astype(str)
data['title']= data['title'].astype(str)
df =data[['date','title','body_text']]
df.head(2)
</code></pre>
<p><a href=""https://i.stack.imgur.com/U7Nm4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U7Nm4.png"" alt=""enter image description here"" /></a></p>
<pre><code>re_data = df[['title', 'body_text' ]]
re_data.index = df['date']

inflation = [0]*re_data.shape[0]
covid19 = [0]*re_data.shape[0]
central_bank = [0]*re_data[0]
    
for i in range(re_data.shape[0]):
    words = TextBlob(re_data['body_text'][i]).words
    for word in words:
       
        if word == &quot;inflation&quot;: inflation[i]=1
        if word == &quot;covid19 &quot;: covid19 [i]=1
        if word == &quot;central bank&quot;: central_bank[i]=1
       
keywords = pd.DataFrame({'content':re_data['body_text'],
                         'title':re_data['title'],
                         'inflation':inflation,
                         'central bank':central_bank,
                         'covid19':covid19},
                        index=re_data.index)
keywords.head(2)
</code></pre>
<p><a href=""https://i.stack.imgur.com/BhUf7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BhUf7.png"" alt=""enter image description here"" /></a></p>
","11489179","","","","","2020-11-26 22:24:05","Find word frequency with TextBlob python","<python><textblob>","0","0","","","","CC BY-SA 4.0"
"29580056","1","","","2015-04-11 16:12:55","","2","646","<p>I have been trying to install Python TextBlob, but I am getting this error:</p>

<blockquote>
  <p>Now downloading textblob packages<br>
  [localhost] run: python -m textblob.download_corpora<br>
  [localhost] out: /home/naren/VirtualEnvironment/bin/python: No module named textblob<br>
  [localhost] out:   </p>
  
  <p>Fatal error: run() received nonzero return code 1 while executing!</p>
  
  <p>Requested: python -m textblob.download_corpora<br>
  Executed: /bin/bash -l -c ""cd /home/naren/VirtualEnvironment &amp;&amp; source bin/activate &amp;&amp; python -m textblob.download_corpora""</p>
  
  <p>Aborting.<br>
  Disconnecting from localhost... done.<br>
  run() received nonzero return code 1 while executing!  </p>
</blockquote>
","4777464","","2615940","","2015-04-12 18:34:53","2015-04-12 18:34:53","Nonzero return code error while installing TextBlob","<python><textblob>","1","1","","","","CC BY-SA 3.0"
"55799693","1","","","2019-04-22 19:02:52","","1","135","<p>I'm a beginner Python programmer I am finding it hard to figure out a simple Tweepy Streaming api.</p>

<p>Basically I am trying to do the below.</p>

<ol>
<li><p>Stream tweets in Portuguese language.</p></li>
<li><p>Show the sentiment of each tweets.</p></li>
</ol>

<p>I am unable to stream language tweets.
Could someone please help me in figuring out what is it that I am doing wrong.</p>

<pre><code>import tweepy
from textblob import TextBlob
### I have the keys updated on those veriables

auth = tweepy.OAuthHandler(CONSUMER_KEY,CONSUMER_SECRET)
auth.set_access_token(ACCESS_TOKEN,ACCESS_TOKEN_SECRET)
API = tweepy.API(auth)


class MyStreamListener(tweepy.StreamListener):

    def on_status(self, status):
        print(""--------------------"")
        print(status.text)
        analysis = TextBlob(status.text)

        if analysis.sentiment.polarity &gt; 0:
            print(""sentiment is positiv"")
        elif analysis.sentiment.polarity == 0:
            print(""sentiment is Neutral"")
        else:
            print(""sentiment is Negative"")
        print(""--------------------\n"")


myStreamListener = MyStreamListener()
myStream = tweepy.Stream(auth = API.auth, listener=myStreamListener, tweet_mode='extended', lang='pt')

myStream.filter(track=['trump'])
</code></pre>

<p>The example o/p is </p>

<pre><code>RT @SAGEOceanTweets: Innovation Hack Week 2019: @nesta_uk is exploring the possibility of holding a hack week in 2019, focused on state-of-�
</code></pre>

<p>However it stops after few tweets and I get this error</p>

<pre><code>      return codecs.charmap_encode(input,self.errors,encoding_table)[0]
      UnicodeEncodeError: 'charmap' codec can't encode 
      character '\U0001f4ca' in position 76: character maps to &lt;undefined&gt;
      [Finished in 85.488s]
</code></pre>

<p>And also the tweets are not in Portuguese.
How can I stream continuously and also get tweets which are in portuguese and perform a Sentiment analysis</p>

<p>Could you folks please also guide me on how to even stream language tweets and then analyze the sentiment using textblob.</p>

<p>Thank you</p>
","11396265","","11396265","","2019-04-22 22:39:44","2019-04-22 22:39:44","Issues with streaming tweets using tweepy and Sentiment analysis","<python><machine-learning><tweepy><sentiment-analysis><textblob>","1","0","1","","","CC BY-SA 4.0"
"55301331","1","","","2019-03-22 14:03:43","","1","131","<p>for an internship project I'm developping an app with Xamarin that will allow users to scan barcodes and create sheets of labels and purchases.</p>

<p>To prevent the scanned codes from being lost in case of crash etc, I've implemented SQLite to create a local backup that we could restore. </p>

<p>The structure is as follows : a <code>ListOfLabelLines</code> contains several <code>LabelLine</code> which each contain a <code>Product</code> and different other informations (such as packaging, quantity etc).</p>

<p>ListOfLabelLines.cs : </p>

<pre><code>    [Table(""ListOfLabelLines"")] // Indique le nom de la table qui sera générée par SQLite
    public class ListOfLabelLines : BaseItem
    {
        private string _name { get; set; }
        [TextBlob(""LabelLinesBlob"")]
        public ObservableCollection&lt;LabelLine&gt; lines { get; set; }
        [TextBlob(""ListBlob"")]
        public List&lt;String&gt; TestList { get; set; }

        public string LabelLinesBlob { get; set; } // serialized LabelLines
        public string ListBlob { get; set; } // serialized TestList


        public ListOfLabelLines()
        {

        }

        public ListOfLabelLines(string name)
        {
            this._name = name;
            lines = new ObservableCollection&lt;LabelLine&gt;();
            TestList = new List&lt;String&gt;();
            TestList.Add(""Test1"");
            TestList.Add(""Test2"");
        }

        public string Name
        {
            get { return _name; }
            set
            {
                _name = value;
            }
        }
    }

}
</code></pre>

<p>These objects <code>ListOfLabelLines</code> contain an <code>ObservableCollection&lt;LabelLine&gt;</code> which I'm serializing by using the TextBlob property from SQLite-net-extensions.</p>

<p>However, when I retrieve the <code>ListOfLabelLines</code> I've stored, the ObservableCollection appears as null :</p>

<p><a href=""https://i.stack.imgur.com/LRmPw.png"" rel=""nofollow noreferrer"">Example of null collections</a></p>

<p>Here are the methods I use to store the objects in SQlite : </p>

<pre><code>        public void SaveListOfLabelLines(ListOfLabelLines ShelfLabelInstance)
        {
            var query = from label in database.Table&lt;ListOfLabelLines&gt;()
                        where label.Name == ShelfLabelInstance.Name
                        select label;
            var res = query.FirstOrDefault();
            if (res != null)
            {
                database.UpdateWithChildren(ShelfLabelInstance);
                Console.WriteLine(""Label "" + ShelfLabelInstance.Name + "" updated"");
            }
            else
            {
                database.InsertWithChildren(ShelfLabelInstance);
                Console.WriteLine(""Label "" + ShelfLabelInstance.Name + "" created"");
            }
        }
</code></pre>

<p>and to retrieve them :</p>

<pre><code>        public void CheckProductsInLabelLine(string n)
        {
            var query = from LOLL in database.Table&lt;ListOfLabelLines&gt;()
                        where LOLL.Name == n
                        select LOLL;
            ListOfLabelLines res = query.FirstOrDefault();
</code></pre>

<p>The string property linked to the TextBlob, however, contains the JSON object I need.</p>

<p>I thought the ObservableCollection would be obtainable when getting the object in DB since TextBlob is supposed to serialize AND deserialize.</p>

<p>Could anybody help ? </p>

<p>Thanks a lot !</p>
","11243067","","11243067","","2019-03-22 14:13:36","2019-03-22 14:13:36","Object value is null after deserialization (Xamarin with SQLite)","<sqlite><xamarin><deserialization><textblob>","0","7","","","","CC BY-SA 4.0"
"26480393","1","","","2014-10-21 06:37:51","","0","1187","<p>In my construct below, I am trying to pass a JSON object through my web service. As a new requirement I have to pass a dictionary object which is <code>sent</code> in the code below. Can you please guide me how to add the dictionary to JSON object?</p>

<pre><code>if plain_text is not None:
        blob = TextBlob(plain_text)
        sentiment = TextBlob(plain_text)
        sent = {}
        for sentence in blob.sentences:
            sent[sentence] =sentence.sentiment.polarity
        print sent
        return json.dumps(
            {'input' : plain_text, 
             'Polarity': sentiment.polarity,                 
             #'sent': json.dumps(sent) # this is where I am stuck as this 'sent' is a dict
             },
            indent=4)
</code></pre>

<p>If I uncomment the line I get the below error:</p>

<pre><code>Exception:

TypeError('keys must be a string',)
Traceback:
Traceback (most recent call last):
  File ""C:\Python27\lib\site-packages\bottle-0.12.7-py2.7.egg\bottle.py"", line 862, in _handle
    return route.call(**args)
  File ""C:\Python27\lib\site-packages\bottle-0.12.7-py2.7.egg\bottle.py"", line 1729, in wrapper
    rv = callback(*a, **ka)
  File ""C:\Users\hp\Desktop\RealPy\WebServices\bottle\server_bckup.py"", line 53, in sentimentEngine
    'sent': json.dumps(sent),
  File ""C:\Python27\lib\json\__init__.py"", line 231, in dumps
    return _default_encoder.encode(obj)
  File ""C:\Python27\lib\json\encoder.py"", line 201, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""C:\Python27\lib\json\encoder.py"", line 264, in iterencode
    return _iterencode(o, 0)
TypeError: keys must be a string
</code></pre>
","2867374","","","user2555451","2014-11-15 17:51:27","2014-11-15 18:10:57","How to add dictionary to json object","<python><json><python-2.7><dictionary><textblob>","1","4","","","","CC BY-SA 3.0"
"61509951","1","","","2020-04-29 19:19:43","","0","224","<p>I am still new to python and learning and one of my courses expects me to use TextBlob and Pandas for sentiment analysis on cvs file. What I did so far I will attach here:</p>

<pre><code>Import csv
from textblob import TextBlob
import pandas as pd

df = pd.read_csv('Movie_reviews.csv', delimiter='\t', header=None)

Movie_review_texts = df[2]
Movie_review_texts

for intex, review_text in enumerate (Movie_review_texts):
    blob = TextBlob(review_text)
    print('Analysing review\t', review_text)
    for sentence in blob.sentences: 
        print('--------SENTIMENT OF SENTENCE--------')
        print(sentence, '\t', sentence.sentiment.polarity)
        print('-------END-------')
</code></pre>

<p>however what I need to do now is I need to aggregate the sentiment scores of the constituent sentences and then convert the aggregate score into a boolean value. Which I am really really struggling, I am ready to give up at this point!</p>
","13435800","","","","","2020-05-08 17:22:31","Sentiment Analysis in Python - TextBlob","<python><pandas><analysis><textblob>","1","2","","","","CC BY-SA 4.0"
"55215748","1","55216876","","2019-03-18 06:33:44","","0","169","<p>My code produced the following error:</p>

<blockquote>
  <p>AttributeError: 'function' object has no attribute 'translate'</p>
</blockquote>

<p>More detail:</p>

<p><a href=""https://i.stack.imgur.com/uKYoJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uKYoJ.png"" alt=""error""></a></p>

<p>What is wrong with my code?</p>

<pre><code>import pandas as pd
import numpy as np
from textblob import TextBlob

df_file2= df_file['Repair Details']. apply.translate(from_lang='zh-CN',to ='en')
</code></pre>
","11218985","","5405967","","2019-03-18 07:14:58","2019-03-18 08:02:29","How to perform language translation of a column (excel file) to english using Textblob?","<python><pandas><nlp><textblob>","2","0","","","","CC BY-SA 4.0"
"55738386","1","","","2019-04-18 02:40:03","","0","51","<p>I am new to pandas. The following is a sub_set of a dataframe named news:
Id is the id of news and the text column includes the news:</p>

<pre><code>Id             text
1              the news is really bad.
2              I do not have any courses.
3              Asthma is very prevalent.
4              depression causes disability.
</code></pre>

<p>I am going to calculate sentiment for each news in the ""text"" column.
I need to create a column to include the result of sentiment analysis. </p>

<p>This is my code:</p>

<pre><code>    from textblob import TextBlob
    review = TextBlob(news.loc[0,'text'])
    print (review.sentiment.polarity)
</code></pre>

<p>This code works for just one of the news in the text column. </p>

<p>I also wrote this function:</p>

<pre><code>    def detect_sentiment(text):

        blob = TextBlob(text)
        return blob.sentiment.polarity

news['sentiment'] = news.text.apply(detect_sentiment)
</code></pre>

<p>But it has the following error:</p>

<pre><code>The `text` argument passed to `__init__(text)` must be a string, not &lt;class 'float'&gt;
</code></pre>

<p>Any solution?</p>
","6341182","","7954504","","2019-04-18 04:14:17","2019-04-19 00:46:15","Apply method in Pandas can not handle a function","<python><pandas><textblob>","1","4","","","","CC BY-SA 4.0"
"36720700","1","","","2016-04-19 14:02:23","","4","485","<p>Using windows 10
I've install textblob using ""py -m pip install textblob"".
I can import textblob, or from textblob import blob,word
But i cant: from textblobl import Textblob.
The error i get is:
Traceback (most recent call last):
  File """", line 1, in 
    from textblob import Textblob
ImportError: cannot import name 'Textblob'</p>

<p>Thanks.</p>
","5999869","","","","","2019-03-23 14:49:18","Python: Error importing textblob lib","<python><textblob>","1","1","","","","CC BY-SA 3.0"
"49126249","1","","","2018-03-06 08:21:51","","0","860","<p>I have written code for lemmatize  sentence in Python using text blob but I am not getting the expected result: </p>

<pre><code>def get_lemmatize_text(transcript):
    transcript = transcript.strip()
    blob = TextBlob(transcript)
    for word in blob:
        expected_str = Word(word)
        expected_str = expected_str.lemmatize()
    return expected_str
print(get_lemmatize_text(""he had not received the four letters we d sent him as he had been travelling for the whole of august and hadn t received any call or text from us . he has just arrived today and has called us straight away . he has also just of his account when he had asked for it to be cancelled before it switched from the first additions datestr . he says he received contact from us that we were looking into this but doesnot have that to hand""))
</code></pre>

<p>I get following as output: 
<code>d</code></p>

<p>What has gone wrong? Can anyone help me or correct me?</p>
","9013085","","3729797","","2018-03-06 08:51:44","2018-03-06 08:51:44","Lemmatize using Textblob in python","<python><lemmatization><textblob>","1","2","","","","CC BY-SA 3.0"
"50066735","1","50071398","","2018-04-27 16:31:04","","1","103","<p>I'm new to flask and python and am creating a project that pulls Twitter data to perform sentiment analyses on search terms using TextBlob to display some visual statistics. I've saved some of the stats as variables  (percentages of positive, negative and neutral tweets) and am trying to pass them into a function to generate a pie chart. This function is decorated so that it will pass the resulting PNG file to an HTML page. I seem to be having issues passing these variables into my plotting function. </p>

<p>Here is part of my sentiment analyses:</p>

<pre><code>def sentiment(userinput):
# creating object of TwitterClient Class
api = TwitterClient(userinput)
# calling function to get tweets
#searchterm = input(""Enter query: "")
tweets = api.get_tweets(query=api.searchterm, count=10)

# picking positive tweets from tweets
ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive']
# percentage of positive tweets
ptweet_analyses_pie = 100 * len(ptweets) / len(tweets)

# picking negative tweets from tweets
ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative']
# percentage of negative tweets
ntweets_analyses_pie = (100 * len(ntweets) / len(tweets))

# percentage of neutral tweets
#leftoverTweets = tweets - ntweets - ptweets
nut_tweet_analyses_pie = (100 * (len(tweets) - len(ntweets) - len(ptweets)) / len(tweets))

pie_chart_img = pie_chart(ptweet_analyses_pie, nut_tweet_analyses_pie, ntweets_analyses_pie)

return ptweets, ntweets, ptweet_analyses, ntweets_analyses, nut_tweet_analyses, pie_chart_img
</code></pre>

<p>Here is where the sentiment() is being called:</p>

<pre><code>@app.route('/render_Data', methods = ['GET', 'POST'])
def render_Data():
    if request.method == 'POST':
        tweets=request.form['tweets']
        ptweets, ntweets, ptweet_analyses, ntweets_analyses, nut_tweet_analyses, pie_chart_img = sentiment(tweets)

    return render_template('render_Data.html', ptweets = ptweets, ntweets = ntweets, ptweet_analyses = ptweet_analyses,
ntweets_analyses = ntweets_analyses, nut_tweet_analyses = nut_tweet_analyses, pie_chart_img = pie_chart_img)
</code></pre>

<p>Here is where I make the pie_chart.png and give it an url to be rendered as html without saving to static.</p>

<pre><code>@app.route('/pie_chart.png')
def pie_chart(x,y,z):
    labels = 'Positive', 'Negative', 'Neutral'
    sizes = [x,y,z]
    colors = ['gold', 'pink', 'lightskyblue']
    explode = (0, 0, 0)  # explode 1st slice
    # Plot
    plt.axis('equal')
    plt.pie(sizes, explode=explode, labels=labels, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=140)
    img = BytesIO()
    plt.savefig(img)
    response = make_response(img.getvalue())
    response.mimetype = 'image/png'
    return response
</code></pre>

<p>And finally the HTML:</p>

<pre><code>&lt;img src='{{url_for('pie_chart')}}'&gt;
</code></pre>

<p>Unfortunately I get an error when I try to render the results: pie_chart() missing 3 required positional arguments: 'x', 'y', and 'z' </p>

<p>Thanks in advance! I know my code is a little elementary as I'm still in the fledging phases of coding, so please go easy in your response!</p>
","9671588","","9671588","","2018-04-27 16:51:47","2018-04-27 23:24:18","Having trouble passing arguments to decorated function in flask","<python-3.x><flask><tweepy><textblob>","1","2","","","","CC BY-SA 3.0"
"53952604","1","","","2018-12-28 01:07:54","","0","1084","<p>I have a pandas dataframe with tweets in portuguese. I want to translate them in a new column of the dataframe using textblob.</p>

<pre><code>df_pt['Traduccion'] = df_pt['text'].apply(TextBlob.translate(from_lang=""pt"",to='en'))
</code></pre>

<p>This is the error I get:</p>

<blockquote>
  <p>TypeError: translate() missing 1 required positional argument: 'self'</p>
</blockquote>

<p>This is a sample of what I have in df_pt['text']:</p>

<blockquote>
  <p>Acabou de publicar uma foto em Penha Circular, Rio De Janeiro, Brazil</p>
</blockquote>
","5109109","","5109109","","2018-12-28 01:22:24","2018-12-28 01:41:00","Applying translate() to a pandas dataframe","<pandas><translate><textblob>","1","3","","","","CC BY-SA 4.0"
"55275528","1","","","2019-03-21 07:27:00","","2","305","<p>I'm using noun_chunks in spacy and np_extractor in textblob to find all phrases in some articles.
Their are some technical terms parsed wrong. ex: ""ANOVA is also called analysis of variance"" and the result shows that noun phrases are ""ANOVA"", ""analysis"", ""variance"" but I think the correct noun phrases are ""ANOVA"", ""analysis of variance"".
I already have a phrase list contaning some technical phrases and I think it can help parsing. How can I use this list to retrain or improve the noun phrase extractor?</p>
","11236092","","","","","2019-03-21 13:06:12","How to add some noun phrases that I already known when doing noun_chunks in spacy (or np_extractor in textblob)?","<python><spacy><textblob>","1","0","","","","CC BY-SA 4.0"
"57454797","1","","","2019-08-12 01:30:35","","2","105","<p>I need to use the library TextBlob (PYTHON) for processing textual data, but by default it uses the WordNet dictionary, and I need to exchange it for a dictionary adapted to the Portuguese in the financial market domain for my research. It's possible? How?</p>
","10554379","","","","","2019-08-12 01:30:35","Is it possible to change the Wordnet dictionary in TextBlob?","<python><wordnet><textblob>","0","0","","","","CC BY-SA 4.0"
"46462587","1","46462767","","2017-09-28 06:43:58","","1","407","<p>I was banging my head with the python's TextBlob package that </p>

<ul>
<li>identifies sentences from paragraphs </li>
<li>identifies words from sentences</li>
<li>determines POS(Part of Speech) tags for those words, etc...</li>
</ul>

<p>Everything was going well until I found out a possible issue, if I am not wrong. It is explained below with sample code snippet.</p>

<pre><code>from textblob import TextBlob
sample = '''This is greater than that by 5%.''' #Sample Sentence
blob = TextBlob(sample)                         #Passing it to TextBlob package.
Words = blob.words                              #Splitting the Sentence into words.
Tags = blob.tags                                #Determining POS tag for each words in the sentence

print(Tags)
[('This', 'DT'), ('is', 'VBZ'), ('greater', 'JJR'), ('than', 'IN'), ('that', 'DT'), ('by', 'IN'), ('5', 'CD'), ('%', 'NN')]

print(Words)
['This', 'is', 'greater', 'than', 'that', 'by', '5']
</code></pre>

<p>As seen above, blob.tags function is treating '%' symbol as a separate word and determines POS tag as well.</p>

<p>Whereas blob.words function is not even printing '%' symbol either alone or together with its previous word. </p>

<p>I am creating a data frame with the output of both the functions. So it is not getting created due to length mismatch issue.</p>

<p>Here are my questions. 
<strong>Is this possible issue in TextBlob package by any chance ? 
And is there any way to identify '%' in the Words list ?</strong></p>
","7853947","","7853947","","2017-09-28 10:03:38","2017-09-28 12:15:23","Python TextBlob Package - Determines POS tag for '%' symbol but do not print it as a word","<python><textblob>","2","0","","","","CC BY-SA 3.0"
"55840360","1","","","2019-04-25 00:48:58","","1","106","<p>im working on text identifying project i use python 3 and libraries</p>

<ul>
<li>GingerIt </li>
<li>NLTK</li>
<li>TextBlob</li>
</ul>

<p>I used the below code as a sample</p>

<pre><code>from textblob import TextBlob
import nltk
from gingerit.gingerit import GingerIt

blob = TextBlob(""my name is john"")
data = blob.sentences
a=[]
parser = GingerIt()
for sentence in blob.sentences:
    strVal = str(sentence)
    text = parser.parse(strVal.lstrip())['result'] 
    a.append(text)

fullStrVal =(' '.join(a))
print (fullStrVal)
</code></pre>

<p>then i can get a output like this </p>

<p><a href=""https://i.stack.imgur.com/Nu3k9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nu3k9.png"" alt=""enter image description here""></a></p>

<p>after i changes blob text sentence to uncommon name it give a error </p>

<pre><code>blob = TextBlob(""my name is asasasa"")
</code></pre>

<p><a href=""https://i.stack.imgur.com/prA1V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/prA1V.png"" alt=""enter image description here""></a></p>

<p>is there any way to skip those words or handle this</p>
","7871014","","","","","2019-04-25 00:48:58","Ignore name from Ginger It python","<python><python-3.x><nlp><nltk><textblob>","0","2","0","","","CC BY-SA 4.0"
"55326993","1","55344010","","2019-03-24 18:19:11","","0","197","<p>So far, I have this code below</p>

<pre class=""lang-py prettyprint-override""><code>from textblob import TextBlob
class BrinBot:

    def __init__(self, message): #Accepts the message from the user as the argument
        parse(message)

class parse:
    def __init__(self, message):
        self.message = message
        blob = TextBlob(self.message)
        print(blob.tags)

BrinBot(""Handsome Bob's dog is a beautiful Chihuahua"")
</code></pre>

<p>This is the output:</p>

<pre class=""lang-py prettyprint-override""><code>[('Handsome', 'NNP'), ('Bob', 'NNP'), (""'s"", 'POS'), ('dog', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('Chihuahua', 'NNP')]
</code></pre>

<p>My question is that apparently TextBlob thinks ""Handsome"" is a singular proper noun, which is not correct as ""Handsome"" is supposed to be an adjective. Is there a way to fix that, I tried this on NLTK also but got the same results.</p>
","10383123","","","","","2019-03-25 18:05:58","TextBlob and NLTK POS tagging accuracy","<python><python-3.x><nlp><nltk><textblob>","1","0","","","","CC BY-SA 4.0"
"49526009","1","","","2018-03-28 04:16:35","","1","1048","<p>I am trying to perform POS tagging to my text which are present in the dataframe. I tried using TextBlob, but I am not getting the desired result. My desired result is ""a new column should be created with all the taggings"".
Eg:""I like stackoverflow"" and my new column POS_tagged should have [('I', 'PRP'), ('like', 'VBP'), ('stackoverflow', 'JJ')]</p>

<p>I tried using Textblob but it is working for one sentence. It is not working for series of sentences.</p>

<pre><code>def postag(sentence1):
blob=TextBlob(sentence1)
return blob.tags

aspect_new[""POS""]=aspect_new['tweets'].apply(postag)
</code></pre>

<p>I am getting below error</p>

<pre><code>TypeError: The `text` argument passed to `__init__(text)` must be a string, not &lt;class 'float'&gt;
</code></pre>

<p>Could you please help me in achieving the same in textblob?</p>
","9510800","","","","","2018-03-28 07:39:44","POS Tagging in Dataframe pandas-Textblog","<python><text-mining><textblob>","1","0","","","","CC BY-SA 3.0"
"43828097","1","","","2017-05-07 05:19:05","","0","1218","<p>I am still somewhat new to python so I am stuck on a problem that I don't know how to solve this particular problem in it. </p>

<p>So we have a string like ""ThisThingIsCool"" or ""thisthingiscool""</p>

<p>Now I need to somehow make a list like [This,Thing,Is,Cool] or [this,thing,is,cool]</p>

<p>Currently, I am using textblob but I am not sure if they even have such a way to do such a thing. </p>

<p>I mean I downloaded the corpus (I am guessing that it's a list of words), but did not see any function to recognize a word in a garbled string and extract words. Leaving a list as an output.</p>

<p>So I want to settle with at least being able to split the one with a Capitalized letter. However I have no clue how to go about that in python. </p>

<p>So the question is</p>

<ol>
<li><p>How do I recognize capitalized letters?</p></li>
<li><p>How do i split it without having the delimiter consumed?</p></li>
<li><p>Is there something in textblob that already does this?</p></li>
</ol>

<p>Thank You</p>
","3915322","","","","","2017-05-07 05:30:08","How to extract words out of a string with no spaces in python?","<python><string><split><tweepy><textblob>","2","2","","","","CC BY-SA 3.0"
"24837811","1","","","2014-07-19 07:17:59","","0","1384","<p>I have checked all the other trails and used few of the solutions. I am facing a challenge in using port stemmer. I am trying to eliminate the affixs however port stemmer reduces the words into some weird forms like languages becomes languag and strengthening becomes strengthn which is spelled incorrect. </p>

<p>I have to search the sentences using word for which I am using TextBlob. below is my code am using. I pulled the text from the link: <a href=""http://www.nltk.org/book/ch03.html"" rel=""nofollow"">http://www.nltk.org/book/ch03.html</a>.
and I did a search for language using porterstemmer and wordnetlemmatizer. Wordnetlemma only reduces the plurals to singular values.</p>

<pre><code>url = 'http://www.nltk.org/book/ch03.html'
a = urllib.urlopen(url)
br = mechanize.Browser()
br.set_handle_robots(False)
br.addheaders = [('User-agent','Chrome')]
html = br.open(url).read()
raw = nltk.clean_html(html)
tokens = nltk.wordpunct_tokenize(raw)
t = [lmtzr.lemmatize(t) for t in tokens] 
text = nltk.Text(t)
sents = ' '.join([s.lower() for s in Text])
blob = TextBlob(sents)
matches = [str(s) for s in blob.sentences if search_words &amp; set(s.words)]
</code></pre>
","3849485","","","user2555451","2014-11-15 17:42:18","2014-11-15 17:42:18","Stemming, lemmatization in python","<python><stemming><textblob>","0","6","","","","CC BY-SA 3.0"
"53942081","1","","","2018-12-27 08:40:46","","0","1962","<p>I installed textblob using the command <code>pip install</code>. But now I am trying to import it and I get the following error:</p>

<blockquote>
  <p>ModuleNotFoundError: No module named 'textblob'</p>
</blockquote>

<p>I am using Spyder in a windows 10 system</p>

<pre><code>from textblob import TextBlob
</code></pre>

<blockquote>
  <p>C:\Users\Diego>pip install textblob Requirement already satisfied: textblob in c:\users\diego\appdata\local\programs\python\python36-32\lib\site-packages (0.15.2) Requirement already satisfied: nltk>=3.1 in c:\users\diego\appdata\local\programs\python\python36-32\lib\site-packages (from textblob) (3.4) Requirement already satisfied: six in c:\users\diego\appdata\local\programs\python\python36-32\lib\site-packages (from nltk>=3.1->textblob) (1.11.0)
  Requirement already satisfied: singledispatch in c:\users\diego\appdata\local\programs\python\python36-32\lib\site-packages (from nltk>=3.1->textblob) (3.4.0.3)</p>
</blockquote>
","5109109","","10294689","","2018-12-28 00:53:45","2018-12-28 00:53:45","I cant import textblob package","<python><pip><spyder><textblob>","1","6","","","","CC BY-SA 4.0"
"37368795","1","","","2016-05-21 22:54:56","","3","2079","<p>I'm trying to write a little web app that returns the sentiment of a news article involving a keyword.</p>

<p>I used the TextBlob and Newspaper3K python 3 packages. I tried to make the url string for Newspaper3K the result of a search query on Google News but the newspaper package just seems to redirect to the ""main page"" of Google News.</p>

<p>Is there any way to get a list of newspaper articles that contain a certain keyword? In addition, is it possible for newspaper to iterate through pages?</p>

<p>The following is my code:</p>

<pre><code>from textblob import TextBlob
import newspaper

#keyword = input(""Please enter the keyword: "")
keyword = ""Apple"" #for testing only
keyword_lowercase = keyword.lower()

search_string = """" # only for google news
split_keyword = keyword.split()
for i in range(len(split_keyword)):
    search_string += split_keyword[i]
    if i != len(split_keyword)-1:
        search_string += '+'

def google_news_site(search_query):
    prefix = 'http://news.google.com/news?q='
    return prefix+search_string

#Currently for news.google.com only
url_string = google_news_site(search_string)
paper = newspaper.build(url_string, memoize_articles=False)

def sentiment(text):
    return TextBlob(text).sentiment.polarity

current_sum = 0.0
relevant_article_count = 0
for article in paper.articles:
    print(article.url)
    article_text = article.text
    article_text_lowercase = article_text.lower()
    if keyword_lowercase in article_text_lowercase:
        current_sum += sentiment(article_text)

print(""Article count is"", str(relevant_article_count)+""."")

rating = current_sum/max(relevant_article_count, 1)
print(""The rating for"", keyword, ""is"", str(rating)+""."")
</code></pre>
","5869848","","1000551","","2017-07-28 07:56:04","2019-09-22 15:37:01","Python 3: How can I get news articles that contain a certain keyword","<python><python-3.x><search-engine><textblob><python-newspaper>","1","2","2","","","CC BY-SA 3.0"
"61562196","1","61562546","","2020-05-02 15:52:38","","0","23","<p>I'm building a configuration file web editor that lets the user edit settings in a textarea, converts the contents to a Blob file, and then POST the data to a remote API. For some reason, it's appending a random callback parameter and I have no idea where it's coming from...</p>

<pre><code>http://ipaddr:8080/compile?callback=jQuery341029448751790349491588432312011&amp;=1588432312012
</code></pre>

<p>Here is what the code looks like. If anyone can point me in the right direction, I would greatly appreciate it.</p>

<pre><code>&lt;script&gt;
    $(document).ready(function() {

        $('#btnCompile').click(function(event) {

            // Convert TextArea contents to a Blob file
            var configText = $('#configuration').val();
            configText = configText.replace(/\n/g, ""\r\n""); // retain line breaks

            var configFile = new Blob([configText], { type: ""text/plain"" });

            var documentData = new FormData();
            documentData.append('file', configFile, ""configuration.cpp"");

            $.ajax({
                url: ""http://ipaddr:8080/compile"",
                method: ""POST"",
                data: documentData,
                dataType: 'jsonp',
                crossDomain: true,
                cache: false,
                contentType: false,
                processData: false,
                success: function(data, textStatus, jqXHR)
                {
                    alert('success: ' + textStatus);
                },
                error: function(jqXHR, textStatus, errorThrown)
                {
                    alert('error status: ' + textStatus + ' error message: ' + errorThrown);
                }
            });
        });
    });

&lt;/script&gt;
</code></pre>
","5327119","","","","","2020-05-02 16:15:38","POST file data with AJAX is appending unknown jquery callback string","<javascript><jquery><ajax><blob><textblob>","1","0","","","","CC BY-SA 4.0"
"24431449","1","24431545","","2014-06-26 13:08:54","","4","3068","<p>I am using TextBlob's <code>NaiveBayesclassifier</code> for text analysis according to the given themes that I have chosen.</p>

<p>The data is huge(about 3000 entries).</p>

<p>Though I was able to get a result, I'm not able to save it for future use without calling that function again and waiting hours till the processing gets complete.</p>

<p>I tried pickling by the following method</p>

<pre><code>ab = NaiveBayesClassifier(data)

import pickle

object = ab
file = open('f.obj','w') #tried to use 'a' in place of 'w' ie. append
pickle.dump(object,file)
</code></pre>

<p>and I got an error, which is as follows:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python27\lib\pickle.py"", line 1370, in dump
    Pickler(file, protocol).dump(obj)
  File ""C:\Python27\lib\pickle.py"", line 224, in dump
    self.save(obj)
  File ""C:\Python27\lib\pickle.py"", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python27\lib\pickle.py"", line 419, in save_reduce
    save(state)
  File ""C:\Python27\lib\pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python27\lib\pickle.py"", line 649, in save_dict
    self._batch_setitems(obj.iteritems())
  File ""C:\Python27\lib\pickle.py"", line 663, in _batch_setitems
    save(v)
  File ""C:\Python27\lib\pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python27\lib\pickle.py"", line 600, in save_list
    self._batch_appends(iter(obj))
  File ""C:\Python27\lib\pickle.py"", line 615, in _batch_appends
    save(x)
  File ""C:\Python27\lib\pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python27\lib\pickle.py"", line 562, in save_tuple
    save(element)
  File ""C:\Python27\lib\pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python27\lib\pickle.py"", line 649, in save_dict
    self._batch_setitems(obj.iteritems())
  File ""C:\Python27\lib\pickle.py"", line 662, in _batch_setitems
    save(k)
  File ""C:\Python27\lib\pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python27\lib\pickle.py"", line 501, in save_unicode
    self.memoize(obj)
  File ""C:\Python27\lib\pickle.py"", line 247, in memoize
    self.memo[id(obj)] = memo_len, obj
MemoryError
</code></pre>

<p>I also tried with sPickle but it also resulted in errors such as:</p>

<pre><code>#saving object with function sPickle.s_dump
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python27\lib\site-packages\sPickle.py"", line 22, in s_dump
    for elt in iterable_to_pickle:
TypeError: 'NaiveBayesClassifier' object is not iterable

#saving object with function sPickle.s_dump_elt
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python27\lib\site-packages\sPickle.py"", line 28, in s_dump_elt
    pickled_elt_str = dumps(elt_to_pickle)
MemoryError: out of memory
</code></pre>

<p>Can anyone tell me what I have to do to save the object?</p>

<p>Or is there anyway by which is save the results of the classifier for future use?</p>
","3538502","","","user2555451","2014-11-15 17:26:47","2019-05-27 14:02:32","How to save the result of classifier textblob NaiveBayesClassifier?","<python><classification><pickle><sentiment-analysis><textblob>","3","3","1","","","CC BY-SA 3.0"
"53155803","1","","","2018-11-05 13:55:15","","0","474","<p>I'm hoping someone can help me. I am working with a relatively large dataframe (1.4m rows) containing repair ticket text.</p>

<p>An example:</p>

<pre><code>""water tank in loft area may be leaking according to op that went out for roofing issue as damp patches on ceilings all upstairs""
</code></pre>

<p>I have been able to apply TextBlob to extract the noun phrases. Each row has a different job description. There is a Noun_Phrase column with the extracted noun phrases, which will vary in number depending on the job description. There may be no noun phrases up to several.</p>

<p>E.g.</p>

<p>Noun Phrases from above job description would be:</p>

<pre><code>['water tank','loft area','damp patches']
</code></pre>

<p>I want to look at the frequency of how often each noun phrase occurs across the 1.4 million observations. To start with I need to get them into a single list of phrases (not spread across several columns).</p>

<p>I've tried:</p>

<pre><code>df2 = pd.DataFrame(df['Noun_Phrases'].values.tolist())
</code></pre>

<p>Intended output:</p>

<p>df2:</p>

<pre><code>Index  |  Noun_Phrase
0      |  water tank
1      |  loft area
2      |  damp patches
</code></pre>

<p>However this produces the following error:</p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File """", line 1, in 
      df2 = pd.DataFrame(df['Noun_Phrases'].values.tolist())</p>
  
  <p>File
  ""C:\Users[Redacted]\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py"",
  line 387, in <strong>init</strong>
      arrays, columns = _to_arrays(data, columns, dtype=dtype)</p>
  
  <p>File
  ""C:\Users[Redacted]\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py"",
  line 7434, in _to_arrays
      dtype=dtype)</p>
  
  <p>File
  ""C:\Users[Redacted]\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py"",
  line 7511, in _list_to_arrays
      content = list(lib.to_object_array(data).T)</p>
  
  <p>File ""pandas/_libs/src\inference.pyx"", line 1524, in
  pandas._libs.lib.to_object_array</p>
  
  <p>TypeError: Expected list, got WordList</p>
</blockquote>

<p>I'm new to Python and NLP, so figuring things out on the fly. There doesn't seem to be many posts concerning how to deal with WordLists. Perhaps there is a better approach that hasn't occurred to me.</p>
","9312160","","646942","","2018-11-05 14:02:22","2018-11-05 15:29:56","TextBlob For Extracting Noun Phrases: WordList issue","<python><pandas><nlp><textblob>","0","8","","","","CC BY-SA 4.0"
"54596076","1","","","2019-02-08 15:56:09","","0","379","<p>Hi guys I'm trying to trigger the while loop in my code that starts speech recognition whenever the hotword ""virgo"" is said. The problem is that snowboy detects the hotword but I don't know how to execute the ""while"" loop once the hotword is triggered. Any help please? this may sound stupid and should be relatively easy but my brain is on fire right now. thank you! </p>

<pre><code>import speech_recognition as sr
from textblob import TextBlob
import snowboydecoder

recognizer_instance = sr.Recognizer()

def detected_callback():
    print (""tell me!"")

detector = snowboydecoder.HotwordDetector(""virgo.pmdl"",sensitivity=0.5)

detector.start(detected_callback=snowboydecoder.play_audio_file,sleep_time=0.03)
detector.terminate()


while True:
    with sr.Microphone() as source:
        recognizer_instance.adjust_for_ambient_noise(source)
        print(""Listening..."")
        audio = recognizer_instance.listen(source)
        print(""copy that!"")

    try:
        text = recognizer_instance.recognize_google(audio, language = ""it-IT"")
        print(""you said:\n"", text) 

    except Exception as e:
        break
</code></pre>
","8006546","","","","","2019-02-08 20:12:41","SnowBoy hotword detection issue","<python><speech-recognition><speech-to-text><textblob><snowboy>","1","0","","","","CC BY-SA 4.0"
"62905252","1","","","2020-07-14 23:04:05","","1","299","<p>I am trying to use Textblob to perform sentiment analysis on abstracts retrieved from the New York Times APIs. Eventually, I want to extract this data into an excel file using Pandas. How would I go about performing sentiment analysis on all 20 abstracts at once?</p>
<p>This is what I have so far:</p>
<pre class=""lang-py prettyprint-override""><code>import requests

url = f'https://api.nytimes.com/svc/mostpopular/v2/viewed/1.json?api-key={topkey}'
data = requests.get(url)
import json
data = data.json()

# Prints all abstracts for top 20 articles on the NYT
for i in range(0,19):
    print(data['results'][i]['abstract'])

import textblob
</code></pre>
","13932109","","6501141","","2020-07-14 23:39:08","2020-07-14 23:49:02","How to Loop through sentiment analysis with Textblob","<python><sentiment-analysis><textblob>","1","4","","","","CC BY-SA 4.0"
"33980444","1","","","2015-11-29 07:25:29","","4","1698","<p>I'm a complete newbie in Machine Learning, NLP, Data Analysis but I'm very motivated to understand it better. I'm reading couple of books on NLTK, scikit-learn etc. I discovered a python module ""TextBlob"" and found it to be super easy to get started with it. Hence I have created a sample demo python script which is hosted at: <a href=""https://gist.github.com/dpnishant/367cef57a8033138eb0a"" rel=""nofollow"">https://gist.github.com/dpnishant/367cef57a8033138eb0a</a>. I'm trying to figure out the best suited algorithm for sentiment analysis and text classification. My questions are as follows:</p>

<ol>
<li><p>Why is the sentiment analysis in the NaiveBayesClassifier slow even on such a small training set? Is this time constant or is it going to increase even more with more training data? And also the sentiment analysis is incorrect (refer the script output, it says ""negative"" for the input text ""sandwich is good""). What am I doing wrong?</p></li>
<li><p>I read in the TextBlob's documentation that the NaiveBayesClassifier is trained on the movie_review corpus. Is there any api where I can change it to something else, nps_chat maybe? Something that is not very clear to me is what is the role of a corpus? I mean, we are training the classifier with our own sample training data then how would more specific corpus e.g. nps_chat, product_reviews, moview_review etc. would help?</p></li>
<li><p>I understand that I need to train a classifier for it to work on a unlabelled data. But if the training data gets huge, what is the best way to handle it? Should the program build the model from the training data every time or is there way where we can save the model to a file (something like pickle) and read it from there? Is it possible with TextBlob and will there be any performance improvements with this methodology?</p></li>
<li><p>In my script, in the last block I'm trying to evaluate the SklearnClassifier via the NLTKClassifier module but I'm having no luck there. It throws some cryptic error messages. Can you please help me in resolving it? And also may I request you to, if possible, show some examples regarding the usage of algorithms/classifiers available in the nltk.classify package on the TextBlob's  documentation website e.g. the Megam, LogisticRegression, SVM, BernoulliNB, GaussianNB etc. An use-case for understanding the applicability of the each algorithm would clear a lot of doubts in beginners like me.</p></li>
</ol>
","1607737","","","","","2017-04-24 07:40:04","TextClassification with TextBlob","<machine-learning><nltk><sentiment-analysis><text-classification><textblob>","1","2","1","","","CC BY-SA 3.0"
"44384131","1","","","2017-06-06 07:43:33","","0","75","<p>So am trying to extract nouns from a csv file. 
Checking each value whether its a noun or not using Textblob package. 
The nouns found are appended to an empty list x. 
Then finally when the looping is finished, I print the final list expecting all nouns but Nothing happens..(The data set is huge).
Some one help me out.</p>

<pre><code>from nltk import FreqDist
from textblob import TextBlob
import pandas as p

x = list()


data = p.DataFrame.from_csv('hl.csv', encoding = ""ISO-8859-1"")
data = data.reset_index()

for column in data.columns.values:
    for value in data[column]:
        blob = TextBlob(value)
##        print(blob.noun_phrases)    #this print statement gives nouns
        x.append(blob.noun_phrases)   #so I append the results to an empty list

print(x)      #When printing the final list.. NOTHING HAPPENS no empty list nothing. Python just ignores it why ???
</code></pre>
","7361335","","7361335","","2017-06-06 07:50:28","2017-06-06 08:32:49",".append() in python just gets ignored?","<python><nlp><append><nltk><textblob>","1","11","","","","CC BY-SA 3.0"
"33789317","1","33789474","","2015-11-18 20:04:10","","1","1131","<p>I can successfully load the TextBlob module from a simple script, but not from a Flask app.  I'll show you the code and the error.</p>

<p>Loading in a simple script works:</p>

<pre><code>from textblob import TextBlob

text = 'purfect kitten'

blob = TextBlob(text)
print blob.correct()
</code></pre>

<p>Loading from Flask app throws error:</p>

<pre><code>from textblob import TextBlob
from flask import (
    Flask,
    request
)

app = Flask(__name__)

@app.route('/parse', methods=['GET', 'POST'])
def parse():
    b = TextBlob(request.json['text'])
    b.correct()
    return b

if __name__ == '__main__':
    print 'running app on port 5000'
    app.debug = True
    app.run()
</code></pre>

<p>The error:</p>

<pre>
127.0.0.1 - - [18/Nov/2015 14:54:25] ""POST /parse HTTP/1.1"" 500 -
Traceback (most recent call last):
  File ""/Users/peter/if/root/venv/lib/python2.7/site-packages/flask/app.py"", line 1836, in __call__
    return self.wsgi_app(environ, start_response)
  File ""/Users/peter/if/root/venv/lib/python2.7/site-packages/flask/app.py"", line 1820, in wsgi_app
    response = self.make_response(self.handle_exception(e))
  File ""/Users/peter/if/root/venv/lib/python2.7/site-packages/flask/app.py"", line 1403, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""/Users/peter/if/root/venv/lib/python2.7/site-packages/flask/app.py"", line 1817, in wsgi_app
    response = self.full_dispatch_request()
  File ""/Users/peter/if/root/venv/lib/python2.7/site-packages/flask/app.py"", line 1478, in full_dispatch_request
    response = self.make_response(rv)
  File ""/Users/peter/if/root/venv/lib/python2.7/site-packages/flask/app.py"", line 1577, in make_response
    rv = self.response_class.force_type(rv, request.environ)
  File ""/Users/peter/if/root/venv/lib/python2.7/site-packages/werkzeug/wrappers.py"", line 841, in force_type
    response = BaseResponse(*_run_wsgi_app(response, environ))
  File ""/Users/peter/if/root/venv/lib/python2.7/site-packages/werkzeug/wrappers.py"", line 57, in _run_wsgi_app
    return _run_wsgi_app(*args)
  File ""/Users/peter/if/root/venv/lib/python2.7/site-packages/werkzeug/test.py"", line 867, in run_wsgi_app
    app_rv = app(environ, start_response)
TypeError: 'TextBlob' object is not callable
</pre>

<p>I'm using textblob version 0.11.0 installed with pip.</p>
","2738039","","","","","2015-11-18 20:13:34","TypeError: 'TextBlob' object is not callable","<python><flask><textblob>","1","0","","","","CC BY-SA 3.0"
"49942698","1","49944021","","2018-04-20 13:24:06","","0","2568","<p>I want to do some language detection using the python package textblob: I created a new column in a pandas df which should contain the detected language:</p>

<pre><code>from textblob import TextBlob
posts['Language']=posts['Caption'].apply(TextBlob.detect_language)
</code></pre>

<p>This code works. However, with one df it interrupts and throws an exeception ('TranslatorError') where the respective row contains less then 3 character. Therefore, I'd like write a function which ensures that the 'TextBlob.detect_language' function gets applied to the full df even when an exception occurs. </p>

<p>I thought about something like that:</p>

<pre><code>def get_language(r):
    try:
        return r.TextBlob.detect_language()
    # except (r.TextBlob.detect_language==TranslatorError):
        return np.nan # where textblob was not able to detect language -&gt; nan
</code></pre>

<p>However, I don't know what to write after the (outcommented) ""except"" clause. Any help?</p>

<p>The current function applied (with the except not outcommented)</p>

<pre><code>posts['Language']=posts['Caption'].apply(get_language)
</code></pre>

<p>returns</p>

<blockquote>
  <p>AttributeError: 'TextBlob' object has no attribute 'TextBlob'</p>
</blockquote>

<p>if I try</p>

<pre><code>def get_language(r):
    try:
        return r.TextBlob.detect_language()
    except:
        pass # (or np.nan)
</code></pre>

<p>it just passes all the rows, i.e. doesn't detect the language for any row...</p>

<p>Thanks for help guys!</p>
","9590783","","9590783","","2018-04-20 14:09:15","2018-04-20 14:30:03","Exception handling when applying function to pandas df","<python-3.x><pandas><function><exception-handling><textblob>","1","0","","","","CC BY-SA 3.0"
"54512265","1","54512654","","2019-02-04 08:15:28","","-1","73","<p>I am using <code>TextBlob</code> i am training my <code>classifier</code> on a training set after that i am successfully able to get the classified out put</p>

<p>Bit how can i get the score of a particular text in terms of positive or negativity should i  put scores of sentiments in my training data</p>

<p>here is what i have tried </p>

<pre><code>from textblob import TextBlob
from textblob.classifiers import NaiveBayesClassifier
train = [
     ('I love this sandwich.', 'pos'),
     ('This is an amazing place!', 'pos'),
     ('I feel very good about these beers.', 'pos'),
     ('I do not like this restaurant', 'neg'),
     ('I am tired of this stuff.', 'neg'),
     (""I can't deal with this"", 'neg'),
     (""My boss is horrible."", ""neg"")
 ]
cl = NaiveBayesClassifier(train)
 cl.classify(""I feel amazing!"")
</code></pre>

<p>Here is the output</p>

<pre><code>'pos'
</code></pre>

<p>How can i get the score of this like pos .7 or in any other format</p>
","7025362","","7025362","","2019-02-05 01:55:25","2019-02-05 01:55:25","How to get the score of sentiments?","<python><nltk><sentiment-analysis><textblob>","2","0","","","","CC BY-SA 4.0"
"33965021","1","33965199","","2015-11-27 21:31:26","","16","1933","<p>I'm using pickle for saving on disk my NLP classifier built with the TextBlob library.</p>

<p>I'm using pickle after a lot of searches related to <a href=""https://stackoverflow.com/questions/33883976/python-textblob-and-text-classification?"">this question</a>. At the moment I'm working locally and I have no problem loading the pickle file (which is 1.5Gb) with my i7 and 16gb RAM machine. But the idea is that my program, in the future, has to run on my server which only has 512Mb RAM installed.</p>

<p>Can pickle handle such a large file or  will I face memory issues?</p>

<p>On my server I've got Python 3.5 installed and it is a Linux server (not sure which distribution).</p>

<p>I'm asking because at the moment I can't access my server, so I can't just try and find out what happens, but at the same time I'm doubtful if I can keep this approach or I have to find other solutions.</p>
","2418529","","-1","","2017-05-23 11:51:38","2015-12-20 01:19:32","Can Pickle handle files larger than the RAM installed on my machine?","<python><python-3.x><pickle><textblob>","2","5","4","","","CC BY-SA 3.0"
"66459560","1","","","2021-03-03 14:57:56","","1","112","<p>I've done some sentiment analysis using the the spacytextblob library, getting the polarity of the text, but I want to apply incremental learning to the model. I've been looking at creme or riverml.xyz to develop this incremental model but I realized textblob does not use a ml model for the analysis. It uses this instead <a href=""https://github.com/sloria/TextBlob/blob/dev/textblob/en/en-sentiment.xml"" rel=""nofollow noreferrer"">https://github.com/sloria/TextBlob/blob/dev/textblob/en/en-sentiment.xml</a> . How do I go about applying incremental learning to get an accurate depiction of sentiment?</p>
<p>(spacy 2.3.0)</p>
<p><strong>Spacy Code:</strong> (from <a href=""https://spacy.io/universe/project/spacy-textblob"" rel=""nofollow noreferrer"">https://spacy.io/universe/project/spacy-textblob</a>)</p>
<pre><code>import spacy
from spacytextblob.spacytextblob import SpacyTextBlob

nlp = spacy.load('en_core_web_sm')
spacy_text_blob = SpacyTextBlob()
nlp.add_pipe(spacy_text_blob)
text = 'But every now and then I have a really good day that makes me happy.'
doc = nlp(text)
print(f'Polarity: {doc._.sentiment.polarity}')
print(f'Subjectivity: {doc._.sentiment.subjectivity}')
</code></pre>
<p>In terms of <strong>creme</strong>, this is a very simple application: (from <a href=""https://gokhang1327.medium.com/how-to-create-a-text-classifier-online-incremental-learning-with-creme-ml-6aac9d869e5c"" rel=""nofollow noreferrer"">https://gokhang1327.medium.com/how-to-create-a-text-classifier-online-incremental-learning-with-creme-ml-6aac9d869e5c</a>)</p>
<pre><code>import pandas as pd
from creme import compose
from creme import feature_extraction
from creme import naive_bayes
# Read dataset
df = pd.read_csv(&quot;7allV03.csv&quot;)
# Convert dataframe to list of tuples
docs = df.to_records(index=False)
# Creating the pipeline
# 1st function is creating the bag of words
# 2nd function is the naive bayes predictor
model = compose.Pipeline(
    ('tokenize', feature_extraction.BagOfWords(lowercase=False)),
    ('nb', naive_bayes.MultinomialNB(alpha=1))
)
# Training the model row by row
for sentence, label in docs:
    model = model.fit_one(sentence, label)

#Make predictions
model.predict_one(&quot;text to predict category&quot;)

#Increment Model
model = model.fit_one(&quot;new text to increment model&quot;, &quot;label for new text&quot;)
</code></pre>
<p>Let me know how I can approach this problem.</p>
","9909277","","4685471","","2021-03-03 14:59:22","2021-03-03 14:59:22","TextBlob incremental/continuous/online learning using spacy","<python><machine-learning><spacy><textblob><creme>","0","0","1","","","CC BY-SA 4.0"
"57537013","1","57540477","","2019-08-17 14:05:21","","0","71","<p>So basically I want to print a set number of tweets related to a topic that user enters but when I run the following code after giving in the input nothing happens, I see no output after that. I would be really grateful if you could tell me why :-)</p>

<p>I tried regenerating the access token keys and then again copy pasting it but the problem still persists</p>

<pre><code>import tweepy
consumerKey = ""Sgdz0quGjDDTtGbFAxWQ02E5M""
consumerSecret = ""alphanumeric""
accessToken = ""980878168180609024-nggEvf3WSLb1IcmmHfoCMhDNvZjbMid""
accessTokenSecret = ""alpha numeric""

auth = tweepy.OAuthHandler(consumer_key=consumerKey, 
consumer_secret=consumerSecret)
auth.set_access_token(accessToken, accessTokenSecret)
api = tweepy.API(auth)

searchTerm = input(""Enter keyword/hashtag to search about : "")
number = int(input(""How many tweets do you wanna print :  ""))
tweets = tweepy.Cursor(api.search, q=searchTerm, lang= ""English"").items(number)

for tweet in tweets:
    print(tweet.text)
</code></pre>

<p>this is what my console is showing after execution</p>

<pre><code>runfile('C:/users/acer/.spyder-py3/temp.py', wdir='C:/users/acer/.spyder-py3')

Enter keyword/hastag to search about : bts

How many tweets do you wanna print :  5

In [14]:
</code></pre>

<p>(It does not print the tweets)</p>
","11630330","","4661875","","2019-08-17 22:01:46","2019-08-17 23:16:41","Is there a reason why the following code does not execute (print the tweets) after taking input?","<python-3.x><twitter-oauth><tweepy><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 4.0"
"58904074","1","","","2019-11-17 18:59:04","","1","508","<p>I am looking for algorithms that could tell the language of the text to me(e.g. Hello - English, Bonjour - French, Servicio - Spanish) and also correct typos of the words in english. I have already explored Google's TextBlob, it is very relevant but it got ""Too many requests"" error as soon as my code starts executing. I also started exploring Polyglot but I am facing a lot of issues to download the library on Windows.</p>

<p>Code for TextBlob</p>

<pre><code>*import pandas as pd
from tkinter import filedialog
from textblob import TextBlob
import time
from time import sleep
colnames = ['Word']
x=filedialog.askopenfilename(title='Select the word list')
print(""Data to be checked: "" + x)
df = pd.read_excel(x,sheet_name='Sheet1',header=0,names=colnames,na_values='?',dtype=str)
words = df['Word']
i=0
Language_detector=pd.DataFrame(columns=['Word','Language','corrected_word','translated_word'])
for word in words:

        b = TextBlob(word)
        language_word=b.detect_language()
        time.sleep(0.5)

        if language_word in ['en','EN']:
            corrected_word=b.correct()
            time.sleep(0.5)
            Language_detector.loc[i, ['corrected_word']]=corrected_word
        else:
             translated_word=b.translate(to='en')
             time.sleep(0.5)

        Language_detector.loc[i, ['Word']]=word
        Language_detector.loc[i, ['Language']]=language_word
        Language_detector.loc[i, ['translated_word']]=translated_word

        i=i+1

filename=""Language detector test v 1.xlsx""
Language_detector.to_excel(filename,sheet_name='Sheet1')
print(""Languages identified for the word list"")**
</code></pre>
","11364969","","","","","2019-11-18 08:48:53","What are the best algorithms to determine the language of text and to correct typos in python?","<python><nlp><nltk><textblob><polyglot>","3","0","","","","CC BY-SA 4.0"
"34201413","1","34202204","","2015-12-10 12:02:43","","0","1314","<p>I am trying to adapt this code (source found <a href=""http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/"" rel=""nofollow noreferrer"">here</a>)to iterate through a directory of files, instead of having the input hard-coded. </p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

from __future__ import division, unicode_literals
import math
from textblob import TextBlob as tb

def tf(word, blob):
    return blob.words.count(word) / len(blob.words)

def n_containing(word, bloblist):
    return sum(1 for blob in bloblist if word in blob)

def idf(word, bloblist):
    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))

def tfidf(word, blob, bloblist):
    return tf(word, blob) * idf(word, bloblist)


document1 = tb(""""""Today, the weather is 30 degrees in Celcius. It is really hot"""""")

document2 = tb(""""""I can't believe the traffic headed to the beach. It is really a circus out there.'"""""")

document3 = tb(""""""There are so many tolls on this road. I recommend taking the interstate."""""")

bloblist = [document1, document2, document3]
for i, blob in enumerate(bloblist):
    print(""Document {}"".format(i + 1))
    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}
    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    for word, score in sorted_words:
        score_weight = score * 100 
        print(""\t{}, {}"".format(word, round(score_weight, 5)))
</code></pre>

<p>I would like to use an an input txt files in a directory, rather than each hard-coded <code>document</code>.</p>

<p>For instance, imagine I had a directory <code>foo</code> which contains three files <code>file1</code>, <code>file2</code>, <code>file3</code>.</p>

<p>File 1 contains the contents that <code>document1</code> contains, i.e.</p>

<p>file1:</p>

<pre><code>Today, the weather is 30 degrees in Celcius. It is really hot
</code></pre>

<p>File 2 contains the contents that <code>document2</code> contains, i.e.</p>

<pre><code>I can't believe the traffic headed to the beach. It is really a circus out there.
</code></pre>

<p>File 3 contains the contents that <code>document3</code> contains, i.e.</p>

<pre><code>There are so many tolls on this road. I recommend taking the interstate.
</code></pre>

<p>I have though to use <code>glob</code> to achieve my desired result, and I have come up with the following code adapation, which correctly identifies the files, but does not process them individually, as the original code does:</p>

<pre><code>file_names = glob.glob(""/path/to/foo/*"")
files =  map(open,file_names)
documents = [file.read() for file in files]
[file.close() for file in files]


bloblist = [documents]
for i, blob in enumerate(bloblist):
    print(""Document {}"".format(i + 1))
    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}
    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    for word, score in sorted_words:
        score_weight = score * 100 
        print(""\t{}, {}"".format(word, round(score_weight, 5)))
</code></pre>

<p>How can I maintain the scores for each individual file using <code>glob</code>?</p>

<p>The desired result after using the files in a directory as input would be the same as the original code [results truncuated to top 3 for space]:</p>

<pre><code>Document 1
    Celcius, 3.37888
    30, 3.37888
    hot, 3.37888
Document 2
    there, 2.38509
    out, 2.38509
    headed, 2.38509
Document 3
    on, 3.11896
    this, 3.11896
    many, 3.11896
</code></pre>

<p>A similar question <a href=""https://stackoverflow.com/questions/22434092/compute-tf-idf-with-corpus"">here</a> did not fully solve the problem. I was wondering how I can call the files to calculate the <code>idf</code> but maintain them separately for calculate the full <code>tf-idf</code>?</p>
","5316384","","-1","","2017-05-23 11:44:41","2017-12-08 23:04:05","Using directory as input for tf-idf with python `textblob`","<python><glob><textblob>","3","0","","","","CC BY-SA 3.0"
"33883976","1","33884239","","2015-11-24 01:35:30","","2","2063","<p>I'm trying do build a text classification model with python and <a href=""https://textblob.readthedocs.org/en/dev/index.html"" rel=""nofollow noreferrer"">textblob</a>, the script is runing on my server and in the future the idea is that users will be able to submit their text and it will be classified.
i'm loading the training set from csv :</p>
<pre><code># -*- coding: utf-8 -*-
import sys
import codecs
sys.stdout = open('yyyyyyyyy.txt',&quot;w&quot;);
from nltk.tokenize import word_tokenize
from textblob.classifiers import NaiveBayesClassifier
with open('file.csv', 'r', encoding='latin-1') as fp:
    cl = NaiveBayesClassifier(fp, format=&quot;csv&quot;)  

print(cl.classify(&quot;some text&quot;))
</code></pre>
<p>csv is about 500 lines long (with string between 10 and 100 chars), and NaiveBayesclassifier needs about 2 minutes for training and then be able to classify my text(not sure if is normal that it need so much time, maybe is my server slow with only 512mb ram).</p>
<p>example of csv line :</p>
<pre><code>&quot;Oggi alla Camera con la Fondazione Italia-Usa abbiamo consegnato a 140 studenti laureati con 110 e 110 lode i diplomi del Master in Marketing Comunicazione e Made in Italy.&quot;,FI-PDL
</code></pre>
<p>what is not clear to me, and i cant find an answer on textblob documentation, is if there is a way to 'save' my trained classifier (so save a lot of time), because by now everytime i run the script it will train again the classifier.
I'm new to text classification and machine learing so my apologize if it is a dumb question.</p>
<p>Thanks in advance.</p>
","2418529","","-1","","2020-06-20 09:12:55","2015-11-24 12:39:16","python textblob and text classification","<python><nlp><nltk><text-classification><textblob>","1","7","4","","","CC BY-SA 3.0"
"34027586","1","","","2015-12-01 18:34:14","","1","388","<p>I started using textblob (0.11) for my sentiment analysis program with python (2.7).
I am fetching tweets from twitter and decoding the json to extract text from tweets, this text is then fed to sentiment analysis code.
The code works fine but it always outputs sentiment as ""pos"", everytime. For many texts which are clearly negative, the program shows sentiment as ""Pos""</p>

<pre><code>import tweepy
import json
import csv
import textblob
import unicodedata
from tweepy import Stream
from tweepy.streaming import StreamListener
from textblob.sentiments import NaiveBayesAnalyzer
import sys
consumer_key = ''
consumer_secret = ''
access_token = ''
access_token_secret = ''

class listener(StreamListener):

def on_data(self,data):
    decoded = json.loads(data)
   decoded=decoded['text']
    print decoded

   blob = textblob.TextBlob(decoded, analyzer=NaiveBayesAnalyzer())
    print (blob.sentiment)


def on_error(self,status):
    print status


if __name__ == '__main__':

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)

api = tweepy.API(auth)

def on_error(self,status):
    print status


twitterStream = Stream(auth,listener())

twitterStream.filter(track=[""#Trend""])
</code></pre>

<p>Kindly help</p>
","5626960","","","","","2015-12-01 18:34:14","textblob.sentiments gives only positive sentiment","<python><twitter><sentiment-analysis><textblob>","0","0","","","","CC BY-SA 3.0"
"67361805","1","","","2021-05-02 22:55:29","","-1","39","<p>I am trying to create a program that will replace the adjectives from one text file, which I am using Pride and Prejudice (pride.txt) and replace them with the adjectives from another text file, which I am using Moby Dick (moby.txt). I am very new to textblob and would appreciate any guidance on how to start.</p>
","15508837","","","","","2021-05-06 00:48:29","Replacing adjectives from one book into another using textblob","<python><string><text><replace><textblob>","1","0","0","","","CC BY-SA 4.0"
"59610076","1","","","2020-01-06 10:04:10","","1","487","<p>I am new to Pandas and Python.</p>

<p>My dataframe:</p>

<p><strong>df</strong></p>

<pre><code>Text
Best tv in 2020
utilizar un servicio sms gratuito
utiliser un tv pour netflix
</code></pre>

<p><strong>My desired output</strong></p>

<pre><code>Text                                    Language
Best tv in 2020                         en
utilizar un servicio sms gratuito       es
utiliser un tv pour netflix             fr
</code></pre>

<p><strong>What I am using:</strong></p>

<pre><code>from textblob import TextBlob

b = TextBlob(""utilizar un servicio sms gratuito"")
print(b.detect_language())

&gt;&gt;es
</code></pre>

<p>I am not sure how I could integrate this method to fill my Pandas Dataframe.</p>

<p><strong>I have tried:</strong></p>

<pre><code>df['Language'] = TextBlob(df['Text']).detect_language()
</code></pre>

<p>But I am getting an error:</p>

<pre><code>TypeError: The `text` argument passed to `__init__(text)` must be a string, not &lt;class 'pandas.core.series.Series'&gt;
</code></pre>

<p>I understand what it means, that I need to pass a string rather than pandas DataFrame Series, so my question is how would I loop the entire Series to detect language per row in column <code>text</code>?</p>

<p>Thank you for your suggestions.</p>
","8913983","","6900127","","2020-01-06 10:07:21","2020-01-06 10:10:54","Determining what language a string contains in a pandas DataFrame","<python><pandas><textblob>","1","0","","","","CC BY-SA 4.0"
"26184593","1","26187818","","2014-10-03 18:25:17","","1","164","<p>I have this code. I have two features. How do I train the two features together?</p>

<pre><code>from textblob import TextBlob, Word, Blobber
from textblob.classifiers import NaiveBayesClassifier
from textblob.taggers import NLTKTagger
import re
import nltk



def get_word_before_you_feature(mystring):
    keyword = 'you'
    before_keyword, keyword, after_keyword = mystring.partition(keyword)
    before_keyword = before_keyword.rsplit(None, 1)[-1]
    return {'word_after_you': before_keyword}


def get_word_after_you_feature(mystring):
    keyword = 'you'
    before_keyword, keyword, after_keyword = mystring.partition(keyword)
    after_keyword = after_keyword.split(None, 1)[0]
    return {'word_after_you': after_keyword}
    classifier = nltk.NaiveBayesClassifier.train(train)



lang_detector = NaiveBayesClassifier(train, feature_extractor=get_word_after_you_feature)
lang_detector = NaiveBayesClassifier(train, feature_extractor=get_word_before_you_feature)


print(lang_detector.accuracy(test))
print(lang_detector.show_informative_features(5))
</code></pre>

<p>This is the output I get.</p>

<blockquote>
  <p>word_before_you = 'do'           refere : generi =      2.2 : 1.0
  <br />
     word_before_you = 'when'         generi : refere =      1.1 : 1.0</p>
</blockquote>

<p>It only seems to get the last feature. How do I get the classifier to train both features instead of one.</p>
","3078335","","","user2555451","2014-11-15 17:40:36","2014-11-15 17:40:36","Train two features instead of one","<python><machine-learning><nlp><nltk><textblob>","1","0","2","","","CC BY-SA 3.0"
"67145482","1","","","2021-04-18 05:32:55","","0","62","<p>I'm using textblob to determine the sentiment of twitter text but some results have no polarity and no subjectivity (making them neutral sentiment -
<a href=""https://i.stack.imgur.com/8VKgL.png"" rel=""nofollow noreferrer"">tweets cleaned and placed in pd.dataframe</a>)</p>
<p><a href=""https://i.stack.imgur.com/7KCd5.png"" rel=""nofollow noreferrer"">This is a chart of the overall contrasts between sentiment (showing way more neutral)</a></p>
<p>My code is below</p>
<pre><code># Create a function to get the subjectivity
def getSubjectivity(text):
   return TextBlob(text).sentiment.subjectivity

# Create a function to get the polarity
def getPolarity(text):
   return  TextBlob(text).sentiment.polarity


# Create two new columns 'Subjectivity' &amp; 'Polarity'
df['Subjectivity'] = df['Tweets'].apply(getSubjectivity)
df['Polarity'] = df['Tweets'].apply(getPolarity)

# Show the new dataframe with columns 'Subjectivity' &amp; 'Polarity'
df

# Subjectivity &lt; 1  but &gt; 0 is more factual, &gt; 1 is very opinionated (0 and +1 are min/max)
# Polarity &lt; 0 is more negative, &gt; 0 is more positive (-1 and +1 are the min/max)
</code></pre>
","15207164","","4685471","","2021-04-18 12:07:29","2021-04-18 12:07:29","Textblob is over-generalizing text and classifying it as neutral","<python><nlp><sentiment-analysis><textblob>","0","2","","","","CC BY-SA 4.0"
"50726470","1","50765113","","2018-06-06 17:42:18","","-1","422","<p>I am currently streaming tweets and want to apply sentimental analysis for each tweet, mapping both the tweet and the value. I keep getting an error: ""NoneType' object has no attribute 'limit'"". Not sure what I am doing wrong.</p>

<pre><code>from textblob import TextBlob
from textblob import Blobber
from textblob.sentiments import NaiveBayesAnalyzer

tb = Blobber(analyzer=NaiveBayesAnalyzer())
def tweet_sentiment(tweet):
'''function for polarity'''
  sentiment = tb(tweet)
  if analysis.sentiment.polarity &gt; .5:
      return 1
  elif analysis.sentiment.polarity &lt; .5:
      return -1
  else:
      return 0


(lines.flatMap(lambda x: (x, tweet_sentiment))
.map(lambda rec: Tweet(rec[0], rec[1]))
.foreachRDD(lambda rdd: rdd.toDF())
.limit(20).registerTempTable(""tweets""))


    ------------------------------------------------------------------- 
    --------
    AttributeError                            Traceback (most recent 
    call last)
    &lt;ipython-input-8-d939b88ef526&gt; in &lt;module&gt;()
          2 (lines.flatMap(lambda x: (x, tweet_sentiment))
          3 .map(lambda rec: Tweet(rec[0], rec[1]))
    ----&gt; 4 .foreachRDD(lambda rdd: rdd.toDF())
          5 .limit(20).registerTempTable(""tweets""))

    AttributeError: 'NoneType' object has no attribute 'limit'
</code></pre>
","9787221","","","","","2018-06-08 16:43:06","Applying TextBlob sentimental analysis to Twitter Stream","<twitter><pyspark><nlp><streaming><textblob>","1","1","0","","","CC BY-SA 4.0"
"34036158","1","","","2015-12-02 06:21:34","","2","662","<p>I have installed textblob and I want to perform simple translation.</p>

<pre><code>&gt;&gt;&gt; text=""Hello""
&gt;&gt;&gt; blob=TextBlob(text)
&gt;&gt;&gt; blob.translate(to=""es"")
</code></pre>

<p>The problem is, I dont know where to specify the proxy authentication. Can you tell me where to specify the username, password and proxy address so that I can get it working?</p>
","1358676","","26095","","2016-06-03 09:49:07","2016-06-03 10:19:07","Setting proxy for textblob","<python><textblob>","1","0","","","","CC BY-SA 3.0"
"59533218","1","","","2019-12-30 15:33:46","","2","382","<p>I've found various tools to extract verbal and noun phrases in English, including in some questions here in stackoverflow. Yet, the techniques I've found only seem to work for English texts. I've tried spacy and textblob but they won't return anything for Portuguese texts (works perfectly in English).</p>

<p>Here is what I've tried for Portuguese:
<a href=""https://stackoverflow.com/questions/44661200/spacy-to-extract-specific-noun-phrase"">Spacy to extract specific noun phrase</a>
The chunk in doc.noun_chunks works perfectly for English, but does anyone knows an already existent technique for Portuguese? I'm searching everywhere I know.</p>
","12626046","","","","","2019-12-30 16:38:29","How do I extract noun/ verbal phrases for portuguese?","<python><nlp><text-mining><spacy><textblob>","1","0","","","","CC BY-SA 4.0"
"58920075","1","","","2019-11-18 17:33:09","","0","256","<p>I want to calculate the polarity and subjectivity for some headlines that I have.
My code works fine, it does not gives any error but for some rows it gives result 0.00000 for polarity and subjectivity. Do you know why?</p>

<p>You can download the data form here:</p>

<p><a href=""https://www.sendspace.com/file/e8w4tw"" rel=""nofollow noreferrer"">https://www.sendspace.com/file/e8w4tw</a></p>

<p>Am I doing something wrong?
This is the code:</p>

<pre><code>import pandas as pd
from textblob import TextBlob

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

df = pd.read_excel('coca cola news.xlsx', encoding='utf8')

df = df.dropna().reset_index(drop = True)
df = df.drop_duplicates().reset_index(drop = True)
print(df)

head_sentiment = []
head_subj = []

par_sentiment = []
par_subj = []


df['Headline Sentiment'] =  df['Headline'].apply(lambda text: TextBlob(text).sentiment.polarity).round(4)
df['Headline Subjectivity'] =  df['Headline'].apply(lambda text: TextBlob(text).sentiment.subjectivity).round(4)

df['Paragraph Sentiment'] =  df['Paragraph'].apply(lambda text: TextBlob(text).sentiment.polarity).round(4)
df['Paragraph Subjectivity'] =  df['Paragraph'].apply(lambda text: TextBlob(text).sentiment.subjectivity).round(4)

print(df)

print(df[df.columns[-4:]])
</code></pre>

<p>I mean, I know that 0 is possible result, but Im getting 0.0000 in 40%-50% of rows, thats a lot, not even 0.00001, that seams strange to me.</p>

<p>Can you help me?</p>
","9749124","","9749124","","2019-11-18 20:26:44","2019-11-21 18:23:21","How to do sentiment analysis of headlines with TextBlob and Python","<python><pandas><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 4.0"
"51072007","1","","","2018-06-27 21:58:12","","0","10459","<p>I am using Atom and just getting started with Python. 
When I try to run this code:</p>

<pre><code>from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analysis = TextBlob(""TextBlob sure looks like it has some interesting 
features"")

print(analysis.tags)
</code></pre>

<p>I get this:</p>

<pre><code>Traceback (most recent call last):
File ""/Users/bethwalsh/Documents/GitHub/automated-personas/py/test_001.py"", 
line 1, in &lt;module&gt;
from textblob import TextBlob
ImportError: No module named textblob
</code></pre>

<p>I have tried to install textblob but still no change:</p>

<pre><code>$ pip3 install textblob
Requirement already satisfied: textblob in 
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages 
(0.15.1)
</code></pre>

<p>Any ideas?</p>
","7303062","","","","","2021-10-14 11:05:42","ImportError: No module named textblob","<python><atom-editor><textblob>","1","2","","","","CC BY-SA 4.0"
"59498416","1","","","2019-12-27 09:10:51","","0","251","<p>I am doing a small project on sentiment analysis using TextBlob. I understand there are are 2 ways to check the sentiment of tweet:</p>

<ol>
<li>Tweet polarity: Using it I can tell whether the tweet is positive, negative or neutral</li>
<li>Training a classifier: I am using this method where I am training a TextBlob Naive Bayes classifier on positive and negative tweets and using the classifier to classify tweet either as 'positive' or 'negative'.</li>
</ol>

<p>My question is, using the Naive bayes classifier, can I also classify the tweet as 'neutral' ? In other words, can the 'sentiment polarity' defined in option 1 can somehow be used in option 2 ?</p>
","8020986","","","","","2020-01-10 09:11:46","TextBlob Naive Bayes classifier for neutral tweets","<python><nltk><sentiment-analysis><naivebayes><textblob>","1","0","","","","CC BY-SA 4.0"
"41630689","1","41641445","","2017-01-13 08:54:10","","1","967","<p>How can I use Google cloud NL api for sentiment analysis for tweets from Twitter with topic(Keyword) that I choose?</p>

<p>I can write python script that uses Twitter(Twitter api)that how people are feeling about a topic that I choose using python’s NL library “TextBlob”</p>

<pre><code> import tweepy from textblob import TextBlob

# Step 1 - Authenticate
consumer_key= 'CONSUMER_KEY_HERE'
consumer_secret= 'CONSUMER_SECRET_HERE'

access_token='ACCESS_TOKEN_HERE'
access_token_secret='ACCESS_TOKEN_SECRET_HERE'

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)

api = tweepy.API(auth)

#Step 3 - Retrieve Tweets
public_tweets = api.search('Trump')



#CHALLENGE - Instead of printing out each tweet, save each Tweet to a CSV file
#and label each one as either 'positive' or 'negative', depending on the sentiment 
#You can decide the sentiment polarity threshold yourself


for tweet in public_tweets:
    print(tweet.text)

    #Step 4 Perform Sentiment Analysis on Tweets
    analysis = TextBlob(tweet.text)
    print(analysis.sentiment)
    print("""")
</code></pre>
","4150709","","1578940","","2017-01-13 10:06:05","2017-01-13 18:46:57","How can I use Google Cloud NL api for sentiment analysis?","<python><twitter><textblob><google-cloud-nl>","1","0","1","","","CC BY-SA 3.0"
"66514823","1","66514868","","2021-03-07 09:10:41","","-1","63","<p>My <strong>pandas dataframe</strong> (<code>df.tweet</code>) consits of one column with <strong>german tweets</strong>, I already did the data cleaning and dropped the columns I don´t need. Now I want to <strong>word_tokenize</strong> the tweets in the pandas dataframe.
With TextBlob it only works for strings and I´m only able to tokenize the dataframe string by string (see code below). I used <strong>textblob-de</strong> because it tokenizes german text.</p>
<p>Is there an opportunity to getting the tokenization done for the whole dataframe with a for loop? I´m new to Python and NLP and really stack at that point. Some help would be great!</p>
<p>This is what I have:</p>
<pre><code>pip install -U textblob-de
from textblob_de import TextBlobDE as TextBlob
TextBlob(df.tweet [1]).words
</code></pre>
","15214551","","","","","2021-03-07 09:21:10","How to word_tokenize pandas dataframe","<python><dataframe><nlp><tokenize><textblob>","1","0","","","","CC BY-SA 4.0"
"51077926","1","","","2018-06-28 08:15:34","","2","1866","<p>I am trying to run some python code, using textblob.
This is the following code:</p>

<pre><code>#!/Library/Frameworks/Python.framework/Versions/3.6/bin/python3
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analysis = TextBlob(""TextBlob sure looks like it has some interesting         
features"")

print(analysis.tags)
</code></pre>

<p>On running it, it told me i was missing this so I ran it...</p>

<pre><code>Looks like you are missing some required data for this feature.

To download the necessary data, simply run

python -m textblob.download_corpora

or use the NLTK downloader to download the missing data:         
http://nltk.org/data.html
If this doesn't fix the problem, file an issue at 
https://github.com/sloria/TextBlob/issues.
</code></pre>

<p>So when I ran that it gave me this failed attempt. I can't figure out how to correct the error. </p>

<pre><code>$ python -m textblob.download_corpora
[nltk_data] Error loading brown: &lt;urlopen error [SSL:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:833)&gt;
[nltk_data] Error loading punkt: &lt;urlopen error [SSL:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:833)&gt;
[nltk_data] Error loading wordnet: &lt;urlopen error [SSL:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:833)&gt;
[nltk_data] Error loading averaged_perceptron_tagger: &lt;urlopen error
[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify
[nltk_data]     failed (_ssl.c:833)&gt;
[nltk_data] Error loading conll2000: &lt;urlopen error [SSL:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:833)&gt;
[nltk_data] Error loading movie_reviews: &lt;urlopen error [SSL:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:833)&gt;
Finished.
</code></pre>

<p>Any ideas on the cause?</p>
","7303062","","","","","2018-06-28 21:11:50","python -m textblob.download_corpora - CERTIFICATE_VERIFY_FAILED","<python><textblob>","1","0","1","","","CC BY-SA 4.0"
"51125160","1","","","2018-07-01 16:42:21","","1","19","<p>I am trying to install TextBlob on Python 3.6.5. Everything is going just fine until I used this command line:
<code>python3 -m textblob.download_corpora</code>
and it started downloading package brown and package punkt. While the package brown was done and unzipped, the punkt one has been freezing for over an hour now. How can I solve this problem guys? </p>

<pre><code>Macbooks-Air:~ macbook$ python3 -m textblob.download_corpora
[nltk_data] Downloading package brown to /Users/macbook/nltk_data...
[nltk_data]   Unzipping corpora/brown.zip.
[nltk_data] Downloading package punkt to /Users/macbook/nltk_data...
</code></pre>
","10018118","","10018118","","2018-07-01 16:51:36","2018-07-01 16:51:36","Downloading package punkt to /Users/macbook/nltk_data... freezes","<python><nltk><freeze><textblob>","0","2","0","2018-07-01 17:11:19","","CC BY-SA 4.0"
"58442401","1","58442546","","2019-10-18 00:32:18","","2","862","<p>I'm having a bit of difficulty when installing TextBlob in the command line on Windows 10 using pip.</p>

<p>According to their docs, you need to run two commands in succession:</p>

<pre><code>pip install -U textblob
python -m textblob.download_corpora
</code></pre>

<p>Upon trying the first command, I get an error I have never seen before when trying to install a package:</p>

<pre><code>C:\Users\phys&gt;pip install -U textblob
Traceback (most recent call last):
  File ""c:\program files (x86)\python37-32\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\program files (x86)\python37-32\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Program Files (x86)\Python37-32\Scripts\pip.exe\__main__.py"", line 9, in &lt;module&gt;
TypeError: 'module' object is not callable
</code></pre>

<p>I'm not exactly a Windows 10 whiz and I don't really know what's going on here - Could someone please point me in the right direction?</p>

<p>Thanks!</p>
","11882406","","","","","2019-10-18 00:53:54","Trouble installing TextBlob with pip","<python><pip><textblob>","1","1","1","","","CC BY-SA 4.0"
"68289012","1","","","2021-07-07 15:35:57","","1","43","<p>I am trying to run a few hundred Trip Advisor reviews through a Sentiment Analysis program (via textblob), so that it will read each review and provide a sentiment for it. The program already works in that you can type a sentence in and it will return the sentiment (i.e. positive, negative, neutral). I would like to run a excel document through the program without having to manually type in each review. Ideally, the program returns a sentiment on each review.....How do I do this?</p>
<p>This is the code I already have...</p>
<pre><code>from textblob import TextBlob
import string

z = 10

poscounter = 0

negcounter = 0

neucounter = 0

totalsentences = 0

while z &gt; 0:
    
y = input(&quot;Type your sentence &quot;).lower()  
    y = y.translate(str.maketrans('', '', string.punctuation))
    y1 = TextBlob(y)
    sentCheck = y1.sentiment.polarity

    if y == &quot;stop&quot;:
        
            print(f&quot;Positive Sentiment: {(poscounter/totalsentences) * 100} %&quot;)
            print(f&quot;Negative Sentiment: {(negcounter/totalsentences) * 100} %&quot;)
            print(f&quot;Neutral Sentiment: {(neucounter/totalsentences) * 100} %&quot;)
            exit()
            
        
    xplitIt = y.split(&quot; &quot;)
    
    for something in xplitIt:
        if something == &quot;crowded&quot; or something == &quot;busy&quot; or something == &quot;crowds&quot; or something == &quot;hate&quot; or something == &quot;hated&quot;:
            print(&quot;negative&quot;)
            sentCheck = -0.1
            break
            
            

    if sentCheck==0:
        print(&quot;neutral&quot;)
        neucounter+=1

    elif sentCheck&gt;0 and sentCheck &lt;=1:
        print(&quot;positive&quot;)
        poscounter+=1

    elif sentCheck == -0.1 or sentCheck &lt; 0:
        negcounter+=1
        if sentCheck != -0.1:
            print(&quot;negative&quot;)
          
    totalsentences = totalsentences + 1

   
</code></pre>
","16343300","","16343300","","2021-07-12 15:18:48","2021-07-12 15:18:48","How to run a Excel document through a Python program","<python><excel><text-files><textblob>","1","1","1","","","CC BY-SA 4.0"
"68085401","1","","","2021-06-22 14:27:42","","0","19","<p>I'm working with longblob column in a MySql table. This is the first time to see such a string structure</p>
<pre><code>b'a:1:{s:13:&quot;mobile_number&quot;;s:13:&quot;+XXXXXXXXXXXX&quot;;}'
</code></pre>
<p>All I need is to have those column values to python dict</p>
<p>NOTE.. If im not mistaken, this bytes string is a form values.</p>
","1055069","","","","","2021-06-22 14:27:42","MySql longblob to python dict","<python><mysql><textblob>","0","0","","","","CC BY-SA 4.0"
"53851411","1","","","2018-12-19 12:35:12","","0","780","<pre><code>    from tweepy.streaming import StreamListener
    from tweepy import OAuthHandler
    from tweepy import Stream
    import tweepy
    import textblob
    import re
    from textblob import TextBlob
    import pandas as pd     
    import numpy as np      

    ACCESS_TOKEN=""XXXX""
    ACCESS_SECRET=""XXXX""
    CONSUMER_KEY=""XXXX""
    CONSUMER_SECRET=""XXXX""

    def twitter_setup():
        """"""
        Utility function to setup the Twitter's API
        with our access keys provided.
        """"""
        # Authentication and access using keys:
        auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)
        auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)

        # Return API with authentication:
        api = tweepy.API(auth)
        return api
        extractor = twitter_setup()


    tweets = extractor.user_timeline(screen_name=""realDonaldTrump"", count=200)

    data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])

data['len']  = np.array([len(tweet.text) for tweet in tweets])
data['ID']   = np.array([tweet.id for tweet in tweets])
data['Date'] = np.array([tweet.created_at for tweet in tweets])
data['Source'] = np.array([tweet.source for tweet in tweets])
data['Likes']  = np.array([tweet.favorite_count for tweet in tweets])
data['RTs']    = np.array([tweet.retweet_count for tweet in tweets])

def clean_tweet(tweet):
    '''
    Utility function to clean the text in a tweet by removing 
    links and special characters using regex.
    '''
    return ' '.join(re.sub(""(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"", "" "", tweet).split())

def analize_sentiment(tweet):
    '''
    Utility function to classify the polarity of a tweet
    using textblob.
    '''
    analysis = TextBlob(clean_tweet(tweet))
    #print(analysis.sentiment.polarity)
    if analysis.sentiment.polarity &gt; 0:
        return 1
    elif analysis.sentiment.polarity == 0:
        return 0
    else:
        return -1

data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])


display(data.head(200))
</code></pre>

<p>I am working on a Project, in this project we are extracting tweets of some of the world leaders and then we will try to compare their relationships with other countries based on their twitter comment. So far we have extracted the tweets from Donald Trump Account  We have categorized the tweets into positive and negative but what problem I am facing is how we can separate the tweets country-wise, Is their any way by which only those tweets are extracted in which he/she has tweeted about some country and the rest of the tweets are ignored so that we can only get the tweets related to the country.    </p>
","8415060","","10671690","","2018-12-19 13:06:54","2018-12-19 13:25:22","How to extract tweets related to a particular country?","<python-3.x><twitter><tweepy><sentiment-analysis><textblob>","2","0","","","","CC BY-SA 4.0"
"51166149","1","51184993","","2018-07-04 05:10:56","","1","434","<p>I am trying to run a python flask application with some text analytics(using TextBlob) feature on IBM cloud/Bluemix.I get the following error after deploying the application via cf push command(see below).According to documentation on TextBlob site,this Exception is thrown when a user tries to use a feature that requires a dataset or model that the user does not have on their system.</p>

<pre><code>error:
Error while running the app:
textblob.exceptions.MissingCorpusError
MissingCorpusError: 
Looks like you are missing some required data for this feature.

To download the necessary data, simply run

python -m textblob.download_corpora
or use the NLTK downloader to download the missing data: 
http://nltk.org/data.html
If this doesn't fix the problem, file an issue at 
https://github.com/sloria/TextBlob/issues.
</code></pre>

<p>Now my question is I have added Flask,Textblob and NLTK in my requirement.txt like shown below.Please suggest how can I run  python -m textblob.download_corpora command to make this missing dataset/model available to bluemix environment.If not running command mentioned above,is there any other way we can make this work.
Note:This app works perfectly on local system.</p>

<pre><code>requirement.txt content:
Flask==0.12.2
cloudant==2.4.0
textblob==0.15.1
nltk==3.3
</code></pre>

<p>This is error/warning I get while the application is getting deployed through push command</p>

<pre><code>        -----&gt; Downloading NLTK corpora...
!     nltk.txt not found, not downloading any corpora
</code></pre>

<p>Edit asked by Henrik:
When I run command python -m textblob.download_corpora below corporas are being downloaded on my system.I am mentioning the same list in the nltk.txt file</p>

<pre><code>`[nltk_data] Downloading package brown to
 [nltk_data]     C:\Users\MohanaKrishnaV\AppData\Roaming\nltk_data...
 [nltk_data]   Package brown is already up-to-date!
 [nltk_data] Downloading package punkt to
 [nltk_data]     C:\Users\MohanaKrishnaV\AppData\Roaming\nltk_data...
 [nltk_data]   Package punkt is already up-to-date!
 [nltk_data] Downloading package wordnet to
 [nltk_data]     C:\Users\MohanaKrishnaV\AppData\Roaming\nltk_data...
 [nltk_data]   Package wordnet is already up-to-date!
 [nltk_data] Downloading package averaged_perceptron_tagger to
 [nltk_data]     C:\Users\MohanaKrishnaV\AppData\Roaming\nltk_data...
 [nltk_data]   Package averaged_perceptron_tagger is already up-to-
 [nltk_data]       date!
 [nltk_data] Downloading package conll2000 to
 [nltk_data]     C:\Users\MohanaKrishnaV\AppData\Roaming\nltk_data...
 [nltk_data]   Package conll2000 is already up-to-date!
 [nltk_data] Downloading package movie_reviews to
 [nltk_data]     C:\Users\MohanaKrishnaV\AppData\Roaming\nltk_data...
 [nltk_data]   Package movie_reviews is already up-to-date!
 Finished.
</code></pre>

<p>And this is how my nltk.txt looks like</p>

<pre><code>brown wordnet
averaged_perceptron_tagger
brown
sentence_polarity
sentiwordnet
subjectivity
words
punkt
maxent_treebank_pos_tagger
movie_reviews
conll2000
</code></pre>

<p>I have added additional corpora in my nltk.txt like below ,hope that's not a problem</p>

<pre><code> sentence_polarity
 sentiwordnet
 subjectivity
 words
</code></pre>

<p>This is how the error log looks like:</p>

<pre><code>   -------&gt; Buildpack version 1.5.22
   -----&gt; Installing pip-pop (0.1.1)
   Downloaded [https://buildpacks.cloudfoundry.org/dependencies/manual- 
   binaries/pip-pop/pip-pop-0.1.1-d410583a.tar.gz]
   -----&gt; Installing pipenv (4.0.1)
   Downloaded [https://buildpacks.cloudfoundry.org/dependencies/manual- 
   binaries/pipenv/pipenv-4.0.1-148f753f.tar.gz]
    $ pip install -r requirements.txt
   You are using pip version 9.0.1, however version 10.0.1 is available.
   You should consider upgrading via the 'pip install --upgrade pip' command.
   You are using pip version 9.0.1, however version 10.0.1 is available.
   You should consider upgrading via the 'pip install --upgrade pip' command.
   -----&gt; Downloading NLTK corpora...
   -----&gt; Downloading NLTK packages: brown wordnet
   averaged_perceptron_tagger
   brown
   sentence_polarity
   sentiwordnet
   subjectivity
   words
   punkt
   maxent_treebank_pos_tagger
   movie_reviews
      [nltk_data] Downloading package brown to
      [nltk_data]     /tmp/contents525031002/deps/0/python/nltk_data...
      [nltk_data]   Package brown is already up-to-date!
      [nltk_data] Error loading wordnet : Package 'wordnet\r' not found in
      [nltk_data]     index
      Error installing package. Retry? [n/y/e]
    Traceback (most recent call last):
    File ""/tmp/contents525031002/deps/0/python/lib/python2.7/runpy.py"", line 
    174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
    File ""/tmp/contents525031002/deps/0/python/lib/python2.7/runpy.py"", line 
    72, in _run_code
    exec code in run_globals
    File ""/tmp/contents525031002/deps/0/python/lib/python2.7/site- 
    packages/nltk/downloader.py"", line 2272, in &lt;module&gt;
    halt_on_error=options.halt_on_error)
    File ""/tmp/contents525031002/deps/0/python/lib/python2.7/site- 
    packages/nltk/downloader.py"", line 681, in download
    choice = input().strip()
    EOFError: EOF when reading a line
    Exit status 0
    Staging complete
    Uploading droplet, build artifacts cache...
    Uploading build artifacts cache...
    Uploading droplet...
    Uploaded build artifacts cache (64.3M)
    Uploaded droplet (105.6M)
    Uploading complete
    Stopping instance 6cbf3cbc-aef1-4a73-a7ab-d562a606fe5b
    Destroying container
    Successfully destroyed container
</code></pre>

<p>This is how I push my app:
cf login >> [I supply my login details] >>cf push</p>
","2507547","","2507547","","2018-07-06 15:05:28","2018-07-06 15:46:52","Python Flask Application on IBM cloud/bluemix with Textblob library throwing exception - textblob.exceptions.MissingCorpusError","<python><flask><ibm-cloud><nltk><textblob>","1","0","1","","","CC BY-SA 4.0"
"30230380","1","","","2015-05-14 06:04:55","","2","5454","<p>I'm using Python34.
I want to get frequency of words from CSV file but it show an error.
Here is my code.Anyone help me to solve this problem.</p>

<pre><code>from textblob import TextBlob as tb
import math

words={}
def tfidf(word, blob, bloblist):
    return tf(word, blob) * idf(word, bloblist)

def tf(word, blob):
    return blob.words.count(word) / len(blob.words)

def n_containing(word, bloblist):
    return sum(1 for blob in bloblist if word in blob)

def idf(word, bloblist):
    return math.log(len(bloblist) / (1 + n_containing(words, bloblist)))

bloblist = open('afterstopwords.csv', 'r').read()

for i, blob in enumerate(bloblist):
     print(""Top words in document {}"".format(i + 1))
     scores = {word: tfidf(word, blob, bloblist) for word in blob.words}
     sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)
     for word, score in sorted_words[:3]:
         print(""\tWord: {}, TF-IDF: {}"".format(word, round(score, 5)))
</code></pre>

<p>And the error is:</p>

<pre><code> Top words in document 1
 Traceback (most recent call last):
 File ""D:\Python34\tfidf.py"", line 45, in &lt;module&gt;
    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}
 AttributeError: 'str' object has no attribute 'words'
</code></pre>
","4326228","","","","","2016-07-20 22:12:43","AttributeError: 'str' object has no attribute 'words'","<python-3.x><textblob>","1","5","1","","","CC BY-SA 3.0"
"65982871","1","","","2021-01-31 18:38:43","","0","13","<p>I'm trying to analyze news and extracting sentiment data related to words.</p>
<p>I know how to get the general sentiment of a news, but I can't find a way on how to get sentiments by words.</p>
<p>Basically what I want to do is analyze a bunch of news and consolidate a sentiment for a set of words, for instance China, USA, Russia, Brazil. Meaning I wand the polarity and subjectivity related to those words in the news.</p>
<p>I've googled for the last couple of days and couldn't find a solution. But I'm new to sentiment analysis and TextBlob so I might not be looking the right way.</p>
<p>I've already an idea on how to implement this myself (start splitting the news in sentences, getting the sentiment per sentence and looking into each sentence for the words and synomyms I'm interested in). But again, as I'm new to this, I would rather first make sure there is not a solution for this already.</p>
<p>Does anyone knows how to implement what I'm looking for?</p>
","1814529","","","","","2021-01-31 18:38:43","Getting sentiments by word using TextBlob","<python><sentiment-analysis><textblob>","0","0","","","","CC BY-SA 4.0"
"35070452","1","35070548","","2016-01-28 19:35:49","","4","7176","<p>Using the <a href=""http://textblob.readthedocs.org/en/dev/quickstart.html#spelling-correction"" rel=""nofollow"">TextBlob</a> library it is possible to improve the spelling of strings by defining them as TextBlob objects first and then using the <code>correct</code> method. </p>

<p>Example:</p>

<pre><code>from textblob import TextBlob
data = TextBlob('Two raods diverrged in a yullow waod and surry I culd not travl bouth')
print (data.correct())
Two roads diverged in a yellow wood and sorry I could not travel both
</code></pre>

<p>Is it possible to do this to strings in a Pandas DataFrame series such as this one:</p>

<pre><code>data = [{'one': '3', 'two': 'two raods'}, 
         {'one': '7', 'two': 'diverrged in a yullow'}, 
        {'one': '8', 'two': 'waod and surry I'}, 
        {'one': '9', 'two': 'culd not travl bouth'}]
df = pd.DataFrame(data)
df

    one   two
0   3     Two raods
1   7     diverrged in a yullow
2   8     waod and surry I
3   9     culd not travl bouth
</code></pre>

<p>To return this:</p>

<pre><code>    one   two
0   3     Two roads
1   7     diverged in a yellow
2   8     wood and sorry I
3   9     could not travel both
</code></pre>

<p>Either using TextBlob or some other method. </p>
","4061070","","4061070","","2016-01-28 20:33:35","2018-03-15 05:52:02","How to correct spelling in a Pandas DataFrame","<python><pandas><nlp><textblob>","2","0","3","","","CC BY-SA 3.0"
"60086675","1","","","2020-02-06 01:57:55","","0","79","<p>I am working on a sentiment analysis problem in twitter. My goal is to collect data in a csv file, in three columns based on the sentiment.
After some attempts, the script is running (partially). It seems like the sentiment is looking at only 1 character at a time</p>

<p>In addition, after looping for some time, it breaks and show an error</p>

<pre><code>import tweepy
from textblob import TextBlob
import pandas as pd
from plotly import __version__
import cufflinks as cf
from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot
init_notebook_mode(connected=True)
cf.go_offline()
import matplotlib.pyplot as plt
from tweepy import Stream,StreamListener
import json,re,csv

consumer_key = 'xxxxxxxxxxxxxxxx'
consumer_key_secret = 'xxxxxxxxxxxxx'

access_token = 'xxxxxxxxxxxxxxxxxxx'
access_token_secret = 'xxxxxxxxxxxxxx'

auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)

auth.set_access_token(access_token, access_token_secret)

api = tweepy.API(auth)

    pos1, neg1, neu1 = 0, 0, 0
    header=['Positive','Negative','Neutral']
    #Create an empty csv file which has three headers: 'Positive','Negative','Neutra'
    with open('List.csv','w') as file:
        write=csv.DictWriter(file,fieldnames=header)
        write.writeheader()


    class Listener(StreamListener):

        def on_data(self, data):
            raw_t=json.loads(data)

            data=raw_t['text']
            #four lines below will clear the tweets by removing: mentions, hash tag etc.
            data = re.sub('@[A-Za-z0–9]+', '',data) #Removing @mentions
            data = re.sub('#', '', data) # Removing '#' hash tag
            data = re.sub('RT[\s]+', '', data) # Removing RT
            data = re.sub('https?:\/\/\S+', '', data) # Removing hyperlink

            global pos1 
            global neg1 
            global neu1
            pos, neg, neu = 0, 0, 0

            for tweet in data:
                print(tweet)
                analysis = TextBlob(tweet)
                print(analysis.sentiment)

                #the below if statement will count the number of tweets based on their sentiment('Positive','Negative','Neutra')
                if analysis.sentiment[0]&gt;0:
                    pos+=1
                elif analysis.sentiment[0]&lt;0:
                    neg+=1
                else:
                    neu+=1
            pos1=pos1+pos
            neg1=neg1+neg
            neu1=neu1+neu

            #write the result from counting to the csv file ""List.csv""
            with open('List.csv', 'a') as file:
                writer = csv.DictWriter(file, fieldnames=header)
                info={
                    'Positive':pos1,
                    'Negative':neg1,
                    'Neutral':neu1
                      }
                writer.writerow(info)

            print(data)
            return True


        def on_error(self, status):
            print(status)


    l = Listener()
        stream = Stream(auth, l)
        stream.filter(track=['trump'])
</code></pre>

error message:

<pre><code> &lt;ipython-input-3-607cdfdcdd9b&gt; in on_data(self, data)
         12         raw_t=json.loads(data)
         13 
    ---&gt; 14         data=raw_t['text']
         15         #four lines below will clear the tweets by removing: metions, has tag etc.
         16         data = re.sub('@[A-Za-z0–9]+', '',data) #Removing @mentions

    KeyError: 'text'
</code></pre>
","10896502","","10896502","","2020-03-03 19:58:51","2020-03-03 19:58:51","For loop is not appending data properly to csv file","<python><for-loop><tweepy><sentiment-analysis><textblob>","0","3","","","","CC BY-SA 4.0"
"68476792","1","68476884","","2021-07-21 21:42:28","","0","27","<p>I'm trying TextBlob lately and wrote a code to correct a sentence with misspelt words.</p>
<p>The program will return the corrected sentence and also return the list of misspelt words.</p>
<p>Here is the code;</p>
<pre><code>from textblob import TextBlob as tb

x=[]
corrected= []
wrng = []
inp='Helllo wrld! Mi name isz Tom'
word = inp.split(' ')

for i in word:
    x.append(tb(i))

for i in x:
    w=i.correct()
    corrected.append(w)
sentence = (' '.join(map(str,corrected)))
print(sentence)

for i in range(0,len(x)):
    if(x[i]!=corrected[i]):
        wrng.append(corrected[i])
print(wrng)
</code></pre>
<p>The Output is;</p>
<pre><code>Hello world! I name is Tom
[TextBlob(&quot;Hello&quot;), TextBlob(&quot;world!&quot;), TextBlob(&quot;I&quot;), TextBlob(&quot;is&quot;)]
</code></pre>
<p>Now I want to remove the <code>TextBlob(&quot;...&quot;)</code> from the list.</p>
<p>Is there any possible way to do that?</p>
","10010234","","","","","2021-07-21 21:53:06","How to remove ""TextBlob"" from my output list","<python><textblob>","1","1","","","","CC BY-SA 4.0"
"62117003","1","","","2020-05-31 13:33:29","","0","276","<p>I am using TextBlob with python 3 to create sentiment values for a larger corpus of documents. I reviewed the distribution of polarity and subjectivity values and noticed a big share of values equal ""0"", see the distribution in the image (for polarity values on 1300 documents). I thought this might just be because TextBlob returns 0 as a default value if it wasnt able to calculate the sentiments right or for some other reason. I didnt find any docu on that, but maybe one of you can tell me where the high amount of zeros might origin in.</p>

<p>Best, Nero 
<a href=""https://i.stack.imgur.com/bp91C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bp91C.png"" alt=""enter image description here""></a></p>
","","user13567633","","","","2020-06-22 11:44:07","TextBlob 0-Values for Polarity and Subjectivity","<nltk><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 4.0"
"18987117","1","19103259","","2013-09-24 16:28:41","","3","1662","<p>I'm trying to run through the TextBlob tutorial in Windows (using Git Bash shell) with Python 3.3.</p>

<p>I've installed <code>textblob</code> and <code>nltk</code> as well as any dependencies.</p>

<p>The Python code is:</p>

<pre><code>from text.blob import TextBlob

wiki = TextBlob(""Python is a high-level, general-purpose programming language."")
tags = wiki.tags
</code></pre>

<p>I'm getting the following error</p>

<pre><code>Traceback (most recent call last):
File ""textblob.py"", line 4, in &lt;module&gt; 
  tags = wiki.tags
File ""c:\Python33\lib\site-packages\text\decorators.py"", line 18, in __get__ 
  value = obj.__dict__[self.func.__name__] = self.func(obj)
File ""c:\Python33\lib\site-packages\text\blob.py"", line 357, in pos_tags 
  for word, t in self.pos_tagger.tag(self.raw)
File ""c:\Python33\lib\site-packages\text\taggers.py"", line 40, in tag
  return pattern_tag(sentence, tokenize)
File ""c:\Python33\lib\site-packages\text\en.py"", line 115, in tag
  for sentence in parse(s, tokenize, True, False, False, False, encoding).split():
File ""c:\Python33\lib\site-packages\text\en.py"", line 99, in parse
  return parser.parse(unicode(s), *args, **kwargs)
File ""c:\Python33\lib\site-packages\text\text.py"", line 1213, in parse
  s[i] = self.find_tags(s[i], **kwargs)
File ""c:\Python33\lib\site-packages\text\en.py"", line 49, in find_tags
  return _Parser.find_tags(self, tokens, **kwargs)
File ""c:\Python33\lib\site-packages\text\text.py"", line 1161, in find_tags
  map = kwargs.get(     ""map"", None))
File ""c:\Python33\lib\site-packages\text\text.py"", line 967, in find_tags
  tagged.append([token, lexicon.get(token, i==0 and lexicon.get(token.lower()) or   None)])
File ""c:\Python33\lib\site-packages\text\text.py"", line 98, in get
  return self._lazy(""get"", *args)
File ""c:\Python33\lib\site-packages\text\text.py"", line 79, in _lazy
  self.load()
File ""c:\Python33\lib\site-packages\text\text.py"", line 367, in load
  dict.update(self, (x.split("" "")[:2] for x in _read(self._path) if x.strip()))
File ""c:\Python33\lib\site-packages\text\text.py"", line 367, in &lt;genexpr&gt;
  dict.update(self, (x.split("" "")[:2] for x in _read(self._path) if x.strip()))
File ""c:\Python33\lib\site-packages\text\text.py"", line 346, in _read
  for line in f:
File ""c:\Python33\lib\encodings\cp1252.py"", line 23, in decode
  return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 16: character maps to &lt;undefined&gt;
</code></pre>

<p>Any idea what is wrong here?  Adding a <code>'u'</code> before the string didn't help.</p>
","2696605","","","user2555451","2014-11-15 17:17:41","2018-01-10 06:28:11","UnicodeDecodeError in textblob tutorial","<python><nltk><textblob>","1","10","1","","","CC BY-SA 3.0"
"34107362","1","","","2015-12-05 15:45:32","","0","816","<p>I use text blob library of python, and the Naive bayes classifier of text blob. I have learned that it uses nltk naive bayes classifier. Here is the question: My input sentences are non-english (Turkish). Will it be possible? I don't know how it works. But I tried 10 training data, and it seems to work. I wonder how it works, this naive babes classifier of nltk, on non-English data. What are the disadvantages?</p>
","2231498","","699305","","2016-08-28 21:26:10","2016-08-28 21:26:10","Machine learning with naive bayes on non english words","<python><nltk><naivebayes><textblob>","1","2","1","","","CC BY-SA 3.0"
"67977030","1","67977101","","2021-06-14 20:48:18","","0","65","<p>I know that TextBlob ignore the words that it doesn’t know, and it will consider words and phrases that it can assign polarity to and averages to get the final score.</p>
<p>Are there any other problems and defects that I don't know about?
Also, I would like to know how it is possible to fix them.</p>
<p>Considering that we can use TextBlob both with a dictionary and through machine learning, I think a solution could be to use a larger dictionary and improve the train set.
Are my intuitions right?</p>
","14937001","","","","","2021-06-14 20:55:20","What are the cons and potenzial problems of using TextBlob to perform sentiment analysis? How could they be solved?","<python><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 4.0"
"34377815","1","","","2015-12-20 03:57:35","","0","40","<p>I have a small little loop running through the oxford dictionary looking for and printing out any word who's definition has one of the words in <code>search_list</code></p>

<p>But when I run it with for example the list: `['hello', 'next', 'from'] it only runs through the loop the first time. Does anyone know why this is happening?</p>

<pre><code>print(search_list)
Oxford = open(""oxford_dictionary.txt"")
for word in search_list:
    for line in Oxford:
        if word in line: 
            print(line.split(' ', 1)[0])

Oxford.close()
</code></pre>
","5244410","","","","","2015-12-20 03:57:35","Why is Python only performing an action on the first iteration of a loop","<python><loops><textblob>","0","3","","2015-12-20 04:03:11","","CC BY-SA 3.0"
"67471754","1","67475471","","2021-05-10 13:52:11","","1","124","<p>I am using Python with TextBlob for sentiment analysis. I want to deploy my app (build in Plotly Dash) to Google Cloud Run with Google Cloud Build (without using Docker). When using locally on my virtual environment all goes fine, but after deploying it on the cloud the corpora is not downloaded. Looking at the requriements.txt file, there was also no reference to this corpora.</p>
<p>I have tried to add <code>python -m textblob.download_corpora</code> to my requriements.txt file but it doesn't download when I deploy it. I have also tried to add</p>
<pre><code>import textblob
import subprocess
cmd = ['python','-m','textblob.download_corpora']
subprocess.run(cmd)
</code></pre>
<p>and</p>
<pre><code>import nltk
nltk.download('movie_reviews')
</code></pre>
<p>to my script (callbacks.py, I am using Plotly Dash to make my app), all without success.</p>
<p>Is there a way to add this corpus to my requirements.txt file? Or is there another workaround to download this corpus? How can I fix this?</p>
<p>Thanks in advance!</p>
<p>Vijay</p>
","9384467","","","","","2021-05-10 18:00:00","Is there a way to download TextBlob corpora to Google Cloud Run?","<python><nlp><nltk><google-cloud-run><textblob>","1","0","","","","CC BY-SA 4.0"
"53900541","1","","","2018-12-23 01:27:45","","1","3485","<p>I am trying to translate non-english texts into english via textblob. I read documentation and trying to handle possible exceptions as below:</p>

<pre><code>txt="" ""
for word in text.split():
    try:
        w=TextBlob(word)
        w=w.translate(to='en')

    except TranslatorError(TextBlobError):
        word="" ""  #replace word with space
        txt=txt+word
    except NotTranslated(TextBlobError):
         txt=txt+word+"" ""
    else:
         txt=txt+w+"" ""
print(txt)  
</code></pre>

<p>I am getting the following errors:</p>

<pre><code>except TranslatorError(TextBlobError): 
NameError: name 'TranslatorError' is not defined  

raise NotTranslated('Translation API returned the input string unchanged.')
textblob.exceptions.NotTranslated: Translation API returned the input string unchanged.
</code></pre>

<p>I referred to the following link:
<a href=""https://textblob.readthedocs.io/en/dev/api_reference.html#textblob.exceptions.TextBlobError"" rel=""nofollow noreferrer"">https://textblob.readthedocs.io/en/dev/api_reference.html#textblob.exceptions.TextBlobError</a></p>

<p>I am not able to resolve these errors. Please help!</p>
","1992989","","10404329","","2018-12-23 09:15:54","2021-02-19 10:34:18","textblob.exceptions.NotTranslated: Translation API returned the input string unchanged","<textblob>","3","0","","","","CC BY-SA 4.0"
"54276075","1","","","2019-01-20 11:42:38","","1","1192","<p>The main objective is to display the sentiment analysis values positive, negative and neutral of any user input in a pie chart. While, the code has no error, the pie chart only displays the neutral value as 100% of the entire chart and classifies the input as neutral even after a negative or positive is fed in as the input. </p>

<p>I have tried modifying the conditional statements and taking passing the main input variable itself in Textblob. However, the expected results have not been generated. </p>

<pre><code>from textblob import TextBlob
import matplotlib.pyplot as plt

def percentage(part,whole):
    return 100*float(part)/float(whole)


inp = input(""Enter something:"")

positive = 0
negative = 0
neutral = 0
polarity = 0


for word in inp:
    analyzer = TextBlob(word)
    polarity += analyzer.sentiment.polarity
    if analyzer.sentiment.polarity &gt; 0:
    positive += 1
    elif analyzer.sentiment.polarity &lt; 0:
    negative += 1
    elif analyzer.sentiment.polarity == 0:
    neutral += 1

positive = percentage(positive,(positive + negative + neutral))
negative = percentage(negative,(positive + negative + neutral))
neutral = percentage(neutral,(positive + negative + neutral))

positive = format(positive,'.2f')
negative = format(negative,'.2f')
neutral = format(neutral,'.2f')

if (polarity &gt; 0):
    print(""Positive"")
elif (polarity &lt; 0):
    print(""Negative"")
elif (polarity == 0):
    print(""Neutral"")

labels = ['Positive ['+str(positive)+'%]', 'Negative ['+str(negative)+'%]', 
'Neutral ['+str(neutral)+'%]']
sizes = [positive, negative, neutral]
colors = ['blue','red','yellow']
patches, texts = plt.pie(sizes, colors=colors, startangle=90)
plt.legend(patches,labels,loc=""best"")
plt.title(""Polarity Pie Chart"")
plt.axis('equal')
plt.tight_layout()
plt.show()
</code></pre>

<p>The Expected output is to correctly classify and display the positive, negative and neutral in the pie chart. But the output, regardless of the input's context, only classifies as neutral and pie chart also only displays neutral.</p>
","10137549","","","","","2019-04-16 09:48:08","How to display the sentiment analysis values in a pie chart using matplotlib in python 3.6?","<python-3.x><matplotlib><nlp><sentiment-analysis><textblob>","1","6","","","","CC BY-SA 4.0"
"63121456","1","","","2020-07-27 18:32:27","","0","228","<pre><code> Unnamed: 0  rating                                             review
0              0       4  Biggest disappointement ever. It was supposed ...
1              1       4  Destiny is not innovative. This game was alrea...
2              2       2  I was able to play the beta with a couple frie...
3              3       4  I liked the intro, sadly it was all downhill f...
4              4       2  Embrace yourself for Activision $500 million w...
...          ...     ...                                                ...
1765        1765       2  It tried so hard and it got so far. Destiny is...
1766        1766       4  Extremely over hyped. It left me feeling meh. ...
1767        1767       5  The positive reviews here are basically trying...
1768        1768       6  You can absolutely tell that this game is from...
1769        1769       0  Woulda gave this an honest 5, but Bungie score...
</code></pre>
<p>The above dataframe, as well as other dataframes need to have the reviews converted from multiple languages to English. Through looking online I was told to use the following code:</p>
<pre><code>from time import sleep
from textblob import TextBlob
from textblob.exceptions import NotTranslated    

def translate_comment(x):
    try:
        # Try to translate the string version of the comment
        return TextBlob(str(x)).translate(to='en')
    except NotTranslated:
        # If the output is the same as the input just return the TextBlob version of the input
        return TextBlob(str(x))

for i in range(len(df2['review'])):
    # Translate one comment at a time
    df2['review'].iloc[i] = translate_comment(df2['review'].iloc[i])

    # Sleep for a quarter of second
    sleep(0.25)
</code></pre>
<p>However, this presented me with the following error:</p>
<pre><code>HTTPError: HTTP Error 429: Too Many Requests
</code></pre>
<p>Through looking online I have seen that sleep time can change this, but I have been able to change it and think it's unlikely that the method will work. Does anyone have a fix to this? Thanks.</p>
","12182676","","","","","2020-07-28 12:02:57","How do I translate dataframe column with multiple languages to English without getting: HTTP Error 429: Too Many Requests","<python><dataframe><google-colaboratory><google-translate><textblob>","1","0","","","","CC BY-SA 4.0"
"58181908","1","58182673","","2019-10-01 09:31:20","","0","680","<p>I'm working in a code for extract wrong words in a text, I'm using python with ""textblob"" library. In this library there is a function <code>correction()</code>, but it just returns the correct phrase based on the wrong phrase, for example: </p>

<pre><code>in: b = TextBlob(""I havv goood speling!"")
in: print(b.correct())
out: I have good spelling!
</code></pre>

<p>I would like calculate the accuracy of the correction, i.e. obtain the percentage of the correction based on the original text or just obtain the quantity of wrong words in the text. </p>

<p>Someone can help me with that?</p>
","11306809","","461847","","2019-10-01 12:29:12","2019-10-01 12:29:12","How can I calculate the accuracy of spelling correction?","<python><nlp><textblob>","2","1","","","","CC BY-SA 4.0"
"59826124","1","","","2020-01-20 15:12:45","","0","39","<p>I am conducting a sentiment analysis for research purposes using TextBlob in Python 3. 
I used this code to retrieve the sentiment per line of feedback (around 4000 lines):</p>

<pre><code># Handle the file and import the textblob package for sentiment analysis
bestand = open(""kolom_o_english.txt"")
from textblob import TextBlob
from statistics import mean

# Create a ""for"" loop in which you first strip the lines and then print the analyses per line
for line in bestand:
    feedback = line.rstrip()
    blob1 = TextBlob(feedback)
    sentiment_score = blob1.sentiment
    print(feedback)
    print(sentiment_score)
</code></pre>

<p>Some examples of my output:</p>

<pre><code>Nice
Sentiment(polarity=0.6, subjectivity=1.0)
Mediocre
Sentiment(polarity=-0.5, subjectivity=1.0)
I have already retired and I think the questions are much more about people who are still working. It's good that this research is being done.
Sentiment(polarity=0.6, subjectivity=0.55)
as a pensioner not that interesting for me
Sentiment(polarity=0.5, subjectivity=0.5)
Not really suitable for people who are already retired.
Sentiment(polarity=-0.275, subjectivity=0.75)
fine
Sentiment(polarity=0.4166666666666667, subjectivity=0.5)
</code></pre>

<p>Even though I can now analyze the sentiment of every line, what methods exist to make sense out of the data as a whole (e.g. mean, distribution etc.) and what lines of codes can I use for this?</p>

<p>Thanks in advance!</p>
","12661268","","12661268","","2020-01-20 15:24:51","2020-01-20 15:24:51","How to print descriptive statistics on sentiment analyses using Python's TextBlob?","<python><analytics><sentiment-analysis><textblob>","0","4","","","","CC BY-SA 4.0"
"56217453","1","","","2019-05-20 08:56:21","","0","636","<p>If Polarity is 0.0 in TextBlob, whether the sentence is purely negative or it means no output.</p>

<pre class=""lang-py prettyprint-override""><code>wiki = TextBlob (""Python is a high-level, general-purpose programming language."")

wiki.sentiment
</code></pre>

<p>output: Sentiment(polarity=0.0, subjectivity=0.0)</p>

<p>Please explain what the output is actually telling to us?</p>
","10853789","","","","","2019-05-20 09:51:34","Sentiment Analysis - polarity","<nlp><textblob>","1","0","1","","","CC BY-SA 4.0"
"68498350","1","","","2021-07-23 11:19:17","","0","45","<p>I have used textblob to assign polarity score to english tweets.Can textblob be used to assign polarity score to Hinglish tweets?
If yes how?</p>
<p>Thankyou</p>
","16509896","","1097780","","2021-07-23 11:22:39","2021-07-23 11:22:39","Natural language processing for hinglish tweets","<nlp><textblob>","0","9","","","","CC BY-SA 4.0"
"30708128","1","","","2015-06-08 11:36:04","","1","60","<p>I am trying to use TextBlob library for information extraction from text. I stumbled upon this error which caused me a lot of problem. Kindly tell me why this is happening and Is there any way to avoid it or I will have to ignore such words in the text and move on. </p>

<pre><code>&gt; from textblob import TextBlob as tb
&gt; temp = tb('gonna')
&gt; print(temp.words)
['gon', 'na']
</code></pre>

<p>The same issue occurs with words like ""wanna"", ""let's"" etc.</p>
","2026332","","1706564","","2015-06-08 16:00:54","2015-06-08 16:00:54","Is this a bug in the TextBlob library in Python when parsing contractions like 'gonna'?","<python><textblob>","0","0","","","","CC BY-SA 3.0"
"57188631","1","57206209","","2019-07-24 17:49:52","","0","365","<p>I want to change the lexicon for Textblob by adding several new words with scores, and by slightly adjusting the score of the words that are already there. What is the best way to approach this?</p>

<p>In Vader sentiment it's done like this:</p>

<pre><code> SIA.lexicon.update(new_words)
</code></pre>

<p>Is there a similar command for Textblob?</p>
","9088317","","","","","2019-07-25 16:08:33","How to update the sentiment scores for some words in textblob?","<python><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 4.0"
"48794659","1","","","2018-02-14 19:18:56","","0","69","<p>I am trying to perform a sentiment analysis on Donalds Trump tweets.
I already collected about 100 tweets that seems to be very possitive or negative.
Such as</p>
<blockquote>
<p>POS: Great trip to Mexico today  - wonderful leadership and high quality people! Look forward to our next meeting.</p>
<p>NEG: Crooked Hillary Clinton is spending a fortune on ads against me. I am the one person she doesn't want to run against. Will be such fun!</p>
</blockquote>
<p>I trained NB classifier using TextBlob framework. Unfortunatelly I get about 40-50% accuracy and the most informative features are:</p>
<pre><code>&gt;              contains(I) = True              pos : neg    =      6.1 : 1.0
&gt;           contains(been) = True              pos : neg    =      2.8 : 1.0
&gt;            contains(has) = True              pos : neg    =      2.8 : 1.0
&gt;            contains(you) = True              pos : neg    =      2.8 : 1.0
&gt;             contains(is) = True              neg : pos    =      2.6 : 1.0
&gt;              contains(I) = False             neg : pos    =      2.3 : 1.0
&gt;            contains(not) = True              neg : pos    =      2.2 : 1.0
&gt;             contains(in) = True              neg : pos    =      2.0 : 1.0
&gt;             contains(of) = True              pos : neg    =      1.7 : 1.0
&gt;             contains(in) = False             pos : neg    =      1.7 : 1.0
</code></pre>
<p>I think that I am doing something wrong because it decides what is the polaritiy basing on simple words such as &quot;I, am, of, in&quot; instead of &quot;sad, good, tax, jail, god bless, than you&quot;.</p>
<p>What I have done is just run the code from the docs</p>
<pre><code>test_set, train_set = dataset[30:], dataset[:70]
cl = NaiveBayesClassifier(train_set)
print(cl.accuracy(test_set))
cl.show_informative_features(10)
</code></pre>
<p>Is it a proper approach?
Could anyone help, please?</p>
","7207058","","-1","","2020-06-20 09:12:55","2018-03-08 18:00:13","How to select tweets to do twitter sentiment analysis","<python><nltk><sentiment-analysis><naivebayes><textblob>","1","0","","","","CC BY-SA 3.0"
"45328318","1","","","2017-07-26 13:23:51","","3","497","<p>I am trying to install <code>TextBlob</code>. As per the official doc, i did this.</p>
<pre><code>&gt; pip install -U textblob
&gt; python -m textblob.download_corpora
</code></pre>
<p>The <code>pip</code> command is doing its job. But the other command is giving following error.</p>
<blockquote>
<p>[nltk_data] Error loading brown: HTTP Error 405: Not allowed.</p>
<p>[nltk_data] Error loading punkt: HTTP Error 405: Not allowed.</p>
<p>[nltk_data] Error loading wordnet: HTTP Error 405: Not allowed.</p>
<p>[nltk_data] Error loading averaged_perceptron_tagger: HTTP Error 405:</p>
<p>[nltk_data]     Not allowed. [nltk_data] Error loading conll2000: HTTP</p>
<p>Error 405: Not allowed. [nltk_data] Error loading movie_reviews: HTTP</p>
<p>Error 405: Not allowed. Finished.</p>
</blockquote>
<p>Any help is appreciated. Thank you.</p>
","5585424","","-1","","2020-06-20 09:12:55","2017-07-28 12:43:35","Error while installing TextBlob","<python><textblob>","1","0","","","","CC BY-SA 3.0"
"31732793","1","","","2015-07-30 19:49:58","","2","2791","<p>The exsiting questions on textblob do not talk about conda so trying with a new question pls.</p>

<p>I tried doanloading / installing TextBlob
<a href=""https://textblob.readthedocs.org/en/dev/install.html#with-conda"" rel=""nofollow"">https://textblob.readthedocs.org/en/dev/install.html#with-conda</a></p>

<p>but <a href=""https://conda.binstar.org/sloria"" rel=""nofollow"">https://conda.binstar.org/sloria</a> has only for OSX</p>

<p>so tried this instead <a href=""https://binstar.org/sursma/textblob"" rel=""nofollow"">https://binstar.org/sursma/textblob</a> and still not much luck</p>

<p>C:\Users\kumar\AppData\Local\Continuum\Anaconda3>conda install -c <a href=""https://conda.binsta.org/sursma"" rel=""nofollow"">https://conda.binsta.org/sursma</a> textblob
Fetching package metadata: ...Could not connect to <a href=""https://conda.binsta.org/sursma/noarch/"" rel=""nofollow"">https://conda.binsta.org/sursma/noarch/</a>
Could not connect to <a href=""https://conda.binsta.org/sursma/win-64/"" rel=""nofollow"">https://conda.binsta.org/sursma/win-64/</a>
Error: Could not find URL: <a href=""https://conda.binstar.org/sursma"" rel=""nofollow"">https://conda.binstar.org/sursma</a> /win-64/</p>

<p>Thanks !</p>
","5103806","","","","","2015-07-31 00:57:17","Installing TextBlob with Conda on Windows","<python-3.x><anaconda><textblob>","1","1","","","","CC BY-SA 3.0"
"47026298","1","","","2017-10-31 00:46:47","","0","2381","<p>I have been trying to install textblob on my mac but I keep getting errors. </p>

<p>I followed the instructions here <a href=""https://github.com/sloria/TextBlob"" rel=""nofollow noreferrer"">https://github.com/sloria/TextBlob</a></p>

<p>for installing the packaged 
I ran the lines </p>

<pre><code>$ pip install -U text blob
</code></pre>

<p>in which my terminal returned </p>

<pre><code>Requirement already up-to-date: textblob in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages
Requirement already up-to-date: nltk&gt;=3.1 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from textblob)
Requirement already up-to-date: six in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from nltk&gt;=3.1-&gt;textblob
</code></pre>

<p>)</p>

<p>Then following I ran </p>

<pre><code>$ python3 -m textblob.download_corpora
</code></pre>

<p>in which my computer returned </p>

<pre><code>/Library/Frameworks/Python.framework/Versions/3.6/bin/python3: Error while finding module specification for 'textblob.download_corpora' (ModuleNotFoundError: No module named 'textblob')
</code></pre>

<p>I looked at this post <a href=""https://stackoverflow.com/questions/18174482/unable-to-get-up-and-running-with-textblob"">Unable to get up and running with TextBlob</a> and made sure i had no files named text or textblob so now i am at a wall not knowing what else to do. Can someone help me ? </p>
","8679555","","","","","2018-10-19 09:05:38","no module named textblob (on a mac)","<python><python-3.x><textblob>","2","0","1","","","CC BY-SA 3.0"
"51256297","1","","","2018-07-10 02:03:53","","3","1167","<p>This is my code to read text from a CSV file and convert all the words in a column of it into singular form from plural:</p>

<pre><code>import pandas as pd
from textblob import TextBlob as tb
data = pd.read_csv(r'path\to\data.csv')

for i in range(len(data)):
    blob = tb(data['word'][i])
    singular = blob.words.singularize()  # This makes singular a list
    data['word'][i] = ''.join(singular)  # Converting the list back to a string
</code></pre>

<p>But this code has been running for minutes now (and possibly keep running for hours, if I don't stop it?)! Why is that? When I checked for few words individually, the conversion happens instantly - doesn't take any time at all. There are only 1060 rows (words to convert) in the file.</p>

<p><strong>EDIT:</strong> It finished running in about 10-12 minutes.</p>

<p>Here's some sample data:</p>

<p>Input:</p>

<pre><code>word
development
investment
funds
slow
company
commit
pay
claim
finances
customers
claimed
insurance
comment
rapid
bureaucratic
affairs
reports
policyholders
detailed
</code></pre>

<p>Output:</p>

<pre><code>word
development
investment
fund
slow
company
commit
pay
claim
finance
customer
claimed
insurance
comment
rapid
bureaucratic
affair
report
policyholder
detailed
</code></pre>
","5305512","","5305512","","2018-07-10 02:25:54","2018-07-12 11:30:00","Why is converting words into singular from plural in a for loop taking so long (Python 3)?","<python><pandas><for-loop><nlp><textblob>","1","5","1","","","CC BY-SA 4.0"
"60027877","1","","","2020-02-02 15:57:31","","0","37","<p>I have .txt files IMDB text based movie review. I want to compare this review with star based review from IMDB.
I want to calculate the review's NLTK score to something like 0 to 10 scores (example: 7.5/10) through python NLTK (either textblob or vader). 
how can I calculate the score from NLTK to 0 to 10 score? </p>
","11106115","","11106115","","2020-02-03 01:20:47","2020-02-03 01:20:47","Python NLTK scores from 0 to 10","<nltk><sentiment-analysis><textblob><vader>","0","4","","","","CC BY-SA 4.0"
"68447567","1","","","2021-07-19 22:16:20","","0","21","<p>Can anyone please explain me the polarity and subjectivity in the TextBlob. What they signify and what their values tell us.</p>
","15414501","","","","","2021-07-19 22:16:20","Subjectivity and Polarity in Sentimental Analysis TextBlob","<sentiment-analysis><textblob>","0","0","","","","CC BY-SA 4.0"
"49091835","1","49091961","","2018-03-04 03:58:52","","1","373","<p>I have a pandas dataframe with a column <code>allTexts</code> which stores a bunch of text information for each row. I am trying to apply a custom function which returns 3 values given the input text. I then want to store these 3 output values in a new dataframe column -  ideally as a numpy array for each row. I do it with <code>apply()</code>, the code completes successfully but it doesn't actually change values.</p>

<pre><code>#stub for creating a dataframe
df = pd.DataFrame({'allText':['Hateful text. This is bad', 'Text about great stuff', ' ']})

#set a placeholder - just 3 zeros for each record
df['Sentiments'] = df['allText'].apply(lambda x: np.zeros(3))

#function definition. It is a textblob library function, which gives me back sentiment scores for each text
def getTextSentiments(text):
    blob = TextBlob(text)
    pos = 0
    neg = 0
    neutral = 0
    count = 0
    for sentence in blob.sentences:
        sentiment = sentence.sentiment.polarity
        if sentiment &gt; 0.1:
            pos +=1
        elif sentiment &gt; -0.1:
            neutral +=1
        else:
            neg +=1
        count+=1
    if count == 0:
        count = 1
    return numpy.array([pos/count, neutral/count, neg/count])

#apply function only for non-empty texts and override 3 zeros in sentiments column with real 3 values
df[df[""allText""]!="" ""]['Sentiments'] = df[df[""allText""]!="" ""][""allText""].apply(getTextSentiments)
</code></pre>

<p>After this code completes without any error I still end up with same value of all zeros in my Sentiments column.</p>

<p>MVP to demonstrate it doesn't work even with single record:</p>

<pre><code>df[df[""allText""]!="" ""].iloc[0]['Sentiments']
array([ 0.,  0.,  0.])
test = getTextSentiments(df[df[""allText""]!="" ""].iloc[0]['allText'])

test
Out[64]: (0.4166666666666667, 0.5, 0.08333333333333333)
df[df[""allText""]!="" ""].iloc[0]['Sentiments'] = test

df[df[""allText""]!="" ""].iloc[0]['Sentiments']
Out[75]: array([ 0.,  0.,  0.])
</code></pre>

<p>Any advice on what am I doing wrong?</p>
","3633250","","3633250","","2018-03-04 04:42:41","2018-03-04 04:42:41","Pandas - store numpy array in a dataframe column which is a result of function","<python><pandas><sentiment-analysis><textblob>","1","4","","","","CC BY-SA 3.0"
"33241842","1","33360383","","2015-10-20 16:23:10","","10","5079","<p>I'm using TextBlob for python to do some sentiment analysis on tweets. The default analyzer in TextBlob is the PatternAnalyzer which works resonably well and is appreciably fast.</p>

<pre><code>sent = TextBlob(tweet.decode('utf-8')).sentiment
</code></pre>

<p>I have now tried to switch to the NaiveBayesAnalyzer and found the runtime to be impractical for my needs. (Approaching 5 seconds per tweet.)</p>

<pre><code>sent = TextBlob(tweet.decode('utf-8'), analyzer=NaiveBayesAnalyzer()).sentiment
</code></pre>

<p>I have used the scikit learn implementation of the Naive Bayes Classifier before and did not find it to be this slow, so I'm wondering if I'm using it right in this case. </p>

<p>I am assuming the analyzer is pretrained, at least <a href=""http://Naive%20Bayes%20analyzer%20that%20is%20trained%20on%20a%20dataset%20of%20movie%20reviews."" rel=""noreferrer"">the documentation</a> states ""Naive Bayes analyzer that is trained on a dataset of movie reviews."" But then it also has a function train() which is described as ""Train the Naive Bayes classifier on the movie review corpus."" Does it internally train the analyzer before each run? I hope not.</p>

<p>Does anyone know of a way to speed this up?</p>
","3174668","","","","","2021-01-22 22:21:51","TextBlob NaiveBayesAnalyzer extremely slow (compared to Pattern)","<python><naivebayes><textblob>","2","0","3","","","CC BY-SA 3.0"
"68410592","1","","","2021-07-16 14:16:25","","1","23","<p>I want to extract noun phrases from sentence using BERT. There are some available libraries like <code>TextBlob</code> that allows us to extract noun phrases like this:</p>
<pre class=""lang-py prettyprint-override""><code>from textblob import TextBlob

line = &quot;Out business could be hurt by increased labor costs or labor shortages&quot;

blob = TextBlob(line)
blob.noun_phrases

&gt;&gt;&gt; WordList(['labor costs', 'labor shortages'])

</code></pre>
<p>The output of this seems pretty good. However, this is not able to capture longer noun phrases. Consider following example:</p>
<pre class=""lang-py prettyprint-override""><code>from textblob import TextBlob

line = &quot;The Company’s Image Activation program may not positively affect sales at company-owned and participating &quot; 
&quot;franchised restaurants or improve our results of operations&quot;

blob = TextBlob(line)
blob.noun_phrases

&gt;&gt;&gt; WordList(['company ’ s', 'image activation'])

</code></pre>
<p>However, the ideal answer here could be a longer phrase, like : <code>company-owned and participating franchised restaurants</code>. Since BERT is proven to be state-of-the-art in many NLP tasks, it should perform at least better than this approach.</p>
<p>However, I could not find any relevant resource to use BERT for this task. Is it possible to solve this task using pre-trained BERT?</p>
","9734336","","","","","2021-07-16 14:16:25","How to extract noun phrases from a sentence using pre-trained BERT?","<deep-learning><bert-language-model><named-entity-recognition><textblob>","0","0","","","","CC BY-SA 4.0"
"25057071","1","25078357","","2014-07-31 10:52:28","","0","759","<p>My goal is to create a system that will be able to take any random text, extract sentences, remove punctuations, and then, on the bare sentence (one of them), to randomly replace NN or VB tagged words with their meronym, holonym or synonim as well as with a similar word from a WordNet synset. There is a lot of work ahead, but I have a problem at the very beginning.</p>

<p>For this I use pattern and TextBlob packages. This is what I have done so far...</p>

<pre><code>from pattern.web import URL, plaintext
from pattern.text import tokenize
from pattern.text.en import wordnet
from textblob import TextBlob
import string

s = URL('http://www.fangraphs.com/blogs/the-fringe-five-baseballs-most-compelling-fringe-prospects-35/#more-157570').download()
s = plaintext(s, keep=[])
secam = (tokenize(s, punctuation=""""))
simica = secam[15].strip(string.punctuation)
simica = simica.replace("","", """")

simica = TextBlob(simica)
simicaTg = simica.words

synsimica = wordnet.synsets(simicaTg[3])[0]
djidja = synsimica.hyponyms()
</code></pre>

<p>Now everything works the way I want but when I try to extract the i.e. hyponym from this <code>djidja</code> variable it proves to be impossible since it is a <code>Synset</code> object, and I can't manipulate it anyhow. </p>

<p>Any idea how to extract a the very word that is reported in hyponyms list (i.e. <code>print(djidja[2])</code> displays <code>Synset(u'bowler')</code>...so how to extract only <code>'bowler'</code> from this)?</p>
","952712","","","user2555451","2014-11-15 17:37:38","2014-11-15 17:37:38","Replacement by synsets in Python pattern packatge","<python><nlp><wordnet><textblob>","1","0","","","","CC BY-SA 3.0"
"45504940","1","","","2017-08-04 10:51:44","","0","204","<p>I dont have admin rights so I downloaded Textblob and set the path for it in my prog , Now I am getting error
""Looks like you are missing some required data for this feature.</p>

<p>To download the necessary data, simply run</p>

<pre><code>python -m textblob.download_corpora""
</code></pre>

<p>I cant download this as I dont have admin rights , Any other way to use textblob?</p>
","5099056","","","","","2017-08-04 11:12:13","How to install Textblob for Python without admin right?","<python><textblob><text-analytics-api>","1","0","","","","CC BY-SA 3.0"
"36783759","1","","","2016-04-22 02:30:52","","4","191","<p>I want to use the majority of the out-of-the-box classifier than TextBlob offers, but I also wanted to add my own small set of training data. This is because the text I am analyzing has some niche words I want to make sure make it into the training set.</p>

<p>So, in TextBlob, they say you can augment an existing classifier like this</p>

<pre><code>&gt;&gt;&gt; new_data = [('She is my best friend.', 'pos'),
        (""I'm happy to have a new friend."", 'pos'),
        (""Stay thirsty, my friend."", 'pos'),
        (""He ain't from around here."", 'neg')]
&gt;&gt;&gt; cl.update(new_data)
True
&gt;&gt;&gt; cl.accuracy(test)
1.0
</code></pre>

<p>However, it doesn't say anything about adding this data to the default classifier. Does anyone know if this is possible?</p>

<p><strong>EDIT</strong></p>

<p>Alternatively, is there a place where I can get enough training data so that I can training my classifier the other way around?</p>
","2497586","","2497586","","2016-04-22 03:07:42","2016-04-22 03:07:42","Is it possible to augment the existing TextBlob classifier?","<python><nlp><sentiment-analysis><textblob>","0","0","1","","","CC BY-SA 3.0"
"45232230","1","","","2017-07-21 08:00:41","","0","2948","<p>Firstly I must admit that I am a newbie to Python or R. </p>

<p>Here I am trying to create a file with the list of bi-grams / 2-grams along with their POS tags (NN, VB, etc...). This is used to easily identify meaningful bi-grams and their POS tag combinations. </p>

<p>For example: the bigram - 'Gross' 'Profit' has the POS tag combination of JJ &amp; NN. But the bigram - 'quarter' 'of' has the POS tag combination of NN &amp; IN. With this I can find meaningful POS combinations. It may not be accurate. That is fine. Just want to research with it.</p>

<p><a href=""https://datascience.stackexchange.com/questions/5316/general-approach-to-extract-key-text-from-sentence-nlp"">For Reference please check the section ""2-gram Results"" in this page.</a>My requirement is something like that. But it was done in R. So it was not useful to me.</p>

<p>As I have come across in Python, POS Tagging and creation of bi-grams can be done using NLTK or TextBlob package. But I am unable to find a logic to assign POS tags for the bi-grams generated in Python. Please see below for the code and relevant output.</p>

<pre><code>import nltk
from textblob import TextBlob
from nltk import word_tokenize
from nltk import bigrams

################# Code snippet using TextBlob Package #######################
text1 = """"""This is an example for using TextBlob Package""""""
blobs = TextBlob(text1)             ### Converting str to textblob object
blob_tags = blobs.tags              ### Assigning POS tags to the word blobs
print(blob_tags)
blob_bigrams = blobs.ngrams(n=2)    ### Creating bi-grams from word blobs
print(blob_bigrams)

################# Code snippet using NLTK Package #######################
text2 = """"""This is an example for using NLTK Package""""""
tokens = word_tokenize(text2)       ### Converting str object to List object                        
nltk_tags = nltk.pos_tag(tokens)    ### Assigning POS tags to the word tokens
print(nltk_tags)
nltk_bigrams = bigrams(tokens)      ### Creating bi-grams from word tokens
print(list(nltk_bigrams))
</code></pre>

<p>Any help is much appreciated. Thanks in advance.</p>
","7853947","","","","","2017-07-21 08:00:41","How to do POS tagging for Bigrams in Python","<python><blob><nltk><textblob><part-of-speech>","0","2","1","","","CC BY-SA 3.0"
"54080763","1","54081715","","2019-01-07 19:41:26","","0","57","<p>I am trying to split a message into its individual words, and trying to tokenized those message.</p>

<pre><code>def split_into_tokens(message):
    message = unicode(message, 'utf8')  # convert bytes into proper unicode
    return TextBlob(message).words

messages.message.head().apply(split_into_tokens)
</code></pre>

<p>if show nameError: name ""unicode"" is not defined</p>

<pre><code>  &lt;ipython-input-16-98e123c365b4&gt; in &lt;module&gt;()
----&gt; 1 messages.title.head().apply(split_into_tokens)

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\series.py in 
apply(self, func, convert_dtype, args, **kwds)
  3192             else:
  3193                 values = self.astype(object).values
-&gt;3194                 mapped = lib.map_infer(values, f, 
convert=convert_dtype)
   3195 
   3196         if len(mapped) and isinstance(mapped[0], Series):

pandas/_libs/src\inference.pyx in pandas._libs.lib.map_infer()

&lt;ipython-input-14-281c1d080655&gt; in split_into_tokens(title)
      1 def split_into_tokens(title):
----&gt; 2 title = unicode(title, utf8)  # convert bytes into proper 
      unicode
      3     return TextBlob(title).words

NameError: name 'unicode' is not defined
</code></pre>

<p>at the end it show unicode not defined, im trying to change the python version also remain the same issue. Did i need to replace the unicode by str in python plugin directory ?</p>
","9793130","","","","","2019-01-07 20:52:34","bag-of-words approach to split message into its individual words","<python><textblob>","1","2","","","","CC BY-SA 4.0"
"56189054","1","","","2019-05-17 15:00:44","","12","9507","<p>I am having a dataframe of which one column has a list of strings at each row.</p>

<p>On average, each list has 150 words of about 6 characters each.</p>

<p>Each of the 700 rows of the dataframe is about a document and each string is a word of this document; so basically I have tokenised the words of the document.</p>

<p>I want to detect the language of each of these documents and to do this I firstly try to detect the language of each word of the document.</p>

<p>For this reason I do the following:</p>

<pre><code>from textblob import TextBlob

def lang_detect(document):

    lang_count = {}
    for word in document:

        if len(word) &gt;= 4:

            word_textblob = TextBlob(word)
            lang_result = word_textblob.detect_language()

            response = lang_count.get(lang_result)

            if response is None:  
                lang_count[f""{lang_result}""] = 1
            else:
                lang_count[f""{lang_result}""] += 1

    return lang_count

df_per_doc['languages_count'] = df_per_doc['complete_text'].apply(lambda x: lang_detect(x))
</code></pre>

<p>When I do this then I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
&lt;ipython-input-42-772df3809bcb&gt; in &lt;module&gt;
     25 
---&gt; 27 df_per_doc['languages_count'] = df_per_doc['complete_text'].apply(lambda x: lang_detect(x))
     28 
     29 
.
.
.

    647 class HTTPDefaultErrorHandler(BaseHandler):
    648     def http_error_default(self, req, fp, code, msg, hdrs):
--&gt; 649         raise HTTPError(req.full_url, code, msg, hdrs, fp)
    650 
    651 class HTTPRedirectHandler(BaseHandler):

HTTPError: HTTP Error 429: Too Many Requests
</code></pre>

<p>The error is much longer and I have omitted the rest of it at the middle.</p>

<p>Now,I am getting the same error even if I try to do this for only two documents/rows.</p>

<p>Is there any way that I can get a response from <code>textblob</code> for more words &amp; documents?</p>
","9024698","","","","","2020-06-24 09:28:00","Textblob - HTTPError: HTTP Error 429: Too Many Requests","<python><textblob><language-detection>","2","6","","","","CC BY-SA 4.0"
"60159143","1","","","2020-02-10 22:04:57","","0","52","<p>I'm trying to install textblob but I always get this error. My Python version is 2.7.10</p>

<p><a href=""https://i.stack.imgur.com/2OKdu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2OKdu.png"" alt=""enter image description here""></a></p>
","12193088","","4502035","","2020-02-11 03:25:02","2020-02-11 03:25:02","Error while installing Textblob for python","<python><textblob>","0","3","","","","CC BY-SA 4.0"
"63035921","1","","","2020-07-22 13:56:26","","0","41","<p>I tried</p>
<pre><code>$ conda install -c conda-forge textblob
</code></pre>
<p>and</p>
<pre><code>$ python -m textblob.download_corpora
</code></pre>
<p>following <a href=""https://textblob.readthedocs.io/en/dev/install.html"" rel=""nofollow noreferrer"">this link</a>, but I am getting the following error-</p>
<pre><code>Error while finding module specification for 'textblob.download_corpora' (AttributeError: module 'textblob' has no attribute '__path__')
</code></pre>
<p>Can someone please help me with it?</p>
","6945019","","","","","2020-07-28 19:17:42","Error while installing textblob through conda on Ubuntu 18.04","<python-3.x><conda><ubuntu-18.04><textblob>","1","0","","","","CC BY-SA 4.0"
"36982000","1","","","2016-05-02 12:03:44","","0","3124","<p>I've been trying to use the correct function of Textblob on an entire file and have the following code.</p>

<pre><code>import codecs
import os

f = codecs.open(""Source"", ""r"", encoding=""utf-8"")
lines = f.readlines()

from textblob import TextBlob

tweet1 = TextBlob(lines[0])
tweet2 = TextBlob(lines[-1])


print(tweet1.correct())
print(tweet2.correct())
</code></pre>

<p>But i can't figure out how to make it correct each line of the file ?</p>

<p>Thank you for your help, 
Hal</p>
","6281075","","","","","2017-07-17 07:34:42","Spelling correction with Textblob for whole text file","<python><codec><textblob>","1","0","1","","","CC BY-SA 3.0"
"51209514","1","51310935","","2018-07-06 11:32:41","","3","9338","<p>how does TextBlob calculate an empirical value for the sentiment polarity. I have used naive bayes but it just predicts whether it is positive or negative. How could I calculate a value for the sentiment like TextBlob does?</p>
","8787425","","","","","2019-02-18 16:10:08","How does TextBlob calculate sentiment polarity? How can I calculate a value for sentiment with machine learning classifier?","<python><python-3.x><machine-learning><sentiment-analysis><textblob>","2","0","","","","CC BY-SA 4.0"
"53920108","1","","","2018-12-25 07:04:58","","1","94","<p>I want to pluralize verbs in TextBlob, for example: introduces -> introduce.</p>

<pre><code>from textblob.en.inflect import singularize, pluralize
from textblob import Word
Word('introduces').pluralize() # returns 'introducess'
pluralize(Word('introduces'), pos='VB') # returns 'introducess'
singularize(Word('introduce'), pos='VB') # returns 'introduce' instead of 'introduces'
pluralize(Word('was'), pos='VB') # returns 'wass'
</code></pre>

<p>It seems that it treats verbs as nouns. What I am doing wrong? </p>

<p>Is there any other libs that can do it correctly? 
I already tried <a href=""https://pypi.org/project/inflect/0.2.4/"" rel=""nofollow noreferrer"">inflect</a>
which can correctly pluralize verbs but not singularize. </p>
","9544766","","","","","2018-12-25 07:04:58","Textblob cannot pluralize/singularize verbs correctly","<python><nlp><textblob>","0","1","","","","CC BY-SA 4.0"
"56063193","1","","","2019-05-09 15:46:26","","-1","479","<p>I have a data frame called Comments_Final which has one column - ""Comments""</p>

<p>this column has different reviews like for example</p>

<p>1.Fit good fast shipping</p>

<p>2.Product as described and functioned perfectly.</p>

<p>3.this product doesn't fit my Remington rm1415 it is way to long and much larger chain..... looks like it would be a pain to return to Canada to sender</p>

<p>4.Would have given it 5 stars but it is not a sealed battery</p>

<p>5.Was not told I needed to sign to receive item missed delivery, made contact with carrier , then received item next day!</p>

<p>6.Quick delivery. Part as expected</p>

<p>so first i want a column called sentiment which will show that review is negative or positive?</p>

<p>and second i want third column as emotion that will tell you that review defines anger, sad,joy,disgust etc. emotions in that.</p>
","8186267","","","","","2019-05-09 17:02:13","How to do sentiment analysis using the textblob in python(Pandas)","<python><sentiment-analysis><textblob>","1","1","1","","","CC BY-SA 4.0"
"44497435","1","44668002","","2017-06-12 10:41:56","","0","301","<p>I am building a simple classifier that determines sentences whether they are positive. this is how i train the classifier using textblob.</p>

<pre><code>train = [
     'i love your website', 'pos',
     'i really like your site', 'pos',
     'i dont like your website', 'neg',
     'i dislike your site', 'neg
]

cl.NaiveBayesClassifier(train)

#im clasifying text from twitter using tweepy and it goes like this and 
stored into the databse and using the django to save me doing all the hassle 
of  the backend

class StdOutListener(StreamListener)
def __init__(self)
    self.raw_tweets = []
    self.raw_teets.append(jsin.loads(data)
def on_data(self, data):
    tweets = Htweets() # connection to the database
    for x in self.raw_data:
        tweets.tweet_text = x['text']

        cl.classify(x['text'])

        if classify(x['text]) == 'pos'
            tweets.verdict = 'pos'
        elif classify(x['text]) == 'neg':
             tweets.verdict = 'neg'
        else:
             tweets.verdict = 'normal'
</code></pre>

<p>the logic seem pretty straightforward but when i trained the classifier which one is positive or negative it should saved the verdict along with the tweet into the database. </p>

<p>But this doesnt seem the case and i have been altering the logic in many ways and still unsuccesful. The problem is if the tweet is positive or negative yes the algorithm does recognise that they are.</p>

<p>However i want it to save 'normal' if they are not and it is not doing this. I recognise the classifier only recognises two things positive or negative, but surely it should also identify if a text does not falls within this category.</p>

<p>How is this possible when using textblob. sample alternative logic and advise would be great  thanks. </p>
","4784603","","","","","2017-06-21 06:06:56","Textblob logic help. NaiveBayesClassifier","<django><python-2.7><naivebayes><textblob>","1","3","","","","CC BY-SA 3.0"
"65712854","1","65713495","","2021-01-14 03:07:50","","1","174","<p>I'm trying to convert non-English languages to English using TextBlob translate function.
My data set is based on Pandas data frame.</p>
<p>I understood that it worked in non-Pandas data frame context.
For example,</p>
<pre><code>what=TextBlob(&quot;El apartamento de Evan esta muy bien situado, con fcil acceso al cualquier punto de Manhattan gracias al metro.&quot;)
whatt=what.translate(to= 'en')
print (whatt)
</code></pre>
<p>But based on Pandas data frame, TextBlob translate wouldn't work properly.
<br> I searched for a way to address this and found the code but gave me a different error message. Could anyone help me with this?</p>
<pre><code>data[&quot;comments&quot;] = data[&quot;comments&quot;].str.encode('ISO 8859-1', 'ignore').apply(lambda x: TextBlob(x.strip()).translate(to='en'))

TypeError: cannot use a string pattern on a bytes-like object
</code></pre>
","7882846","","","","","2021-01-15 03:25:21","issue with using TextBlob translate function for Pandas Dataframe","<python-3.x><pandas><dataframe><textblob>","1","0","","","","CC BY-SA 4.0"
"62074547","1","","","2020-05-28 21:07:39","","0","134","<p>I'm scraping tweets from Twitter and I'd like to gather a list of all of the nouns from all of the tweets I'm scraping so I can figure out which nouns occur the most frequently.</p>

<pre><code>def sentiment_script():


        for tweet in tweepy.Cursor(api.search, q=hashtag_phrase + ' -filter:retweets', lang=""en"", tweet_mode='extended').items(7):

                text = tweet.full_text

                text = ' '.join(re.sub(""(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"", "" "", text).split())


                blob = TextBlob(text)

                nouns = (blob.noun_phrases)

                print(nouns)

</code></pre>

<p>The output is this:</p>

<pre><code>['covid', 'richmitch']
['uk', 'england', 'uk', 'johnson', 's approach']
['peoria']
['pa', 'surely', 'secretly trying', 'infect', 'covid', 'never wonkette']
['don t', 'full lockdown', 'cancer etc don t', 'full recovery', 'death rate', 'aren t', 'full lockdown']
['datascience team', 'weekly report', 'new data', 'covid', 'may', 'report sheds light', 'business impacts', 'covid', 'read', 'capraplus']
['osdbu', 'small businesses', 'linked', 'covid']
</code></pre>

<p>I'm not sure where to proceed next, as when I do this:</p>

<pre><code>print(type(nouns))

</code></pre>

<p>the result is</p>

<pre><code>&lt;class 'textblob.blob.WordList'&gt;
&lt;class 'textblob.blob.WordList'&gt;
&lt;class 'textblob.blob.WordList'&gt;
&lt;class 'textblob.blob.WordList'&gt;
&lt;class 'textblob.blob.WordList'&gt;
&lt;class 'textblob.blob.WordList'&gt;
&lt;class 'textblob.blob.WordList'&gt;
</code></pre>
","13637598","","","","","2021-03-18 22:06:07","How to iterate through a TextBlob WordList and find the most common nouns?","<python><nlp><tweepy><textblob><word-list>","1","0","","","","CC BY-SA 4.0"
"62611786","1","62612341","","2020-06-27 15:14:26","","0","1906","<p><strong>Defining a function to get sentiment out of tweets:</strong> (written by me)</p>
<pre><code>def get_tweet_sentiment(tweet): 
    '''Utility function to classify sentiment of passed tweet using textblob's sentiment method'''
    # create TextBlob object of passed tweet text 
    analysis = TextBlob(tweet) 
    
    # set sentiment 
    if analysis.sentiment.polarity &gt; 0:
        return 'positive'
    elif analysis.sentiment.polarity == 0: 
        return 'neutral'
    else: 
        return 'negative'
</code></pre>
<p><strong>Calling function to get sentiment:</strong></p>
<pre><code>from textblob import TextBlob 
get_tweet_sentiment(df['tweet_content'])
</code></pre>
<p><strong>Error:</strong></p>
<pre><code>TypeError: The `text` argument passed to `__init__(text)` must be a string, not &lt;class 'pandas.core.series.Series'&gt;
</code></pre>
<p><strong>My df is of the type below:'</strong></p>
<pre><code>tweet_content                   Col2    Col3
[Hi, I , really, like , you]    89      90
[You, are , too , sweet]       777      990   and so on. There are 30,000 such rows.
</code></pre>
<p>Please help how to either change the function or the way to call the function</p>
","11760970","","11760970","","2020-06-27 15:29:56","2020-06-27 16:02:51","Textblob tweets : TypeError: The `text` argument passed to `__init__(text)` must be a string, not <class 'pandas.core.series.Series'> , rows are lists","<python><pandas><dataframe><nlp><textblob>","1","9","","","","CC BY-SA 4.0"
"30762865","1","","","2015-06-10 17:04:57","","2","11678","<p>I was trying to import TextBlob into Python.</p>

<p>When I run the command directly inside shell, it works pretty well:
   from textblob import TextBlob</p>

<p>However, when I put it into the py file and run it, it doesn't work anymore, it says:</p>

<pre><code>ImportError: cannot import name 'TextBlob'
</code></pre>

<p>Please help me out with it, so desperate right now...Thanks a lot </p>
","4995943","","3715328","","2016-12-05 11:50:21","2021-10-20 10:32:32","python.cannot import name 'TextBlob'","<python><textblob>","7","1","2","","","CC BY-SA 3.0"
"47987881","1","48007672","","2017-12-27 07:09:40","","0","420","<p>I am trying to create a .exe file using pyinstaller and execute it 
it is not fetching any result from
<code>b = TextBlob(ar)
score = b.sentiment.polarity</code></p>

<p>it returns proper value when executed on console
but return 0 when executed with .exe </p>

<pre><code>def start_dmo():
 print (""Importing all the required packages..."")
 from textblob import TextBlob
 input (""press enter to continue"")
 print(""All Necessary Packages imported"")
 input(""press enter to continue"")
 ar=""I cant be more happy with this""
 print(ar)
 b = TextBlob (ar)
 score = b.sentiment.polarity
 print (b.sentiment.polarity)
 input(""press enter to continue"")
 score = round (score, 2)
 if score &gt; 0.0:
    senti = ""Positive""
elif score &lt; 0.0:
    senti = ""Negative""
else:
    senti = ""Neutral""
 print(""Score""+str(score)+""sentiment""+senti)
 input(""press enter to continue"")

start_dmo()
</code></pre>

<p><a href=""https://i.stack.imgur.com/7S3YP.png"" rel=""nofollow noreferrer"">this is the output when the above code is executed on console</a></p>

<p><a href=""https://i.stack.imgur.com/O1AVn.png"" rel=""nofollow noreferrer"">this is the output when the above code is executed on .exe of the same code which is created using pyinstaller</a></p>
","6833872","","6833872","","2018-05-18 10:19:24","2019-01-16 19:49:02","How to add a hook for textblob in pyinstaller","<hook><pyinstaller><textblob>","4","1","1","","","CC BY-SA 4.0"
"62621528","1","62624090","","2020-06-28 10:55:11","","1","95","<p>I have a dataframe <code>train</code>, with a column <code>tweet_content</code>. There is a column <code>sentiment</code> which tells the overall sentiment of tweet. Now there are lot of words which are common in tweets of neutral, positive and negative sentiments. I want to find the words which are unique to each specific sentiment</p>
<p>train</p>
<pre><code>tweet_content                                sentiment 
[PM, you, rock, man]                         Positive
[PM, you, are, a, total, idiot, man]         Negative
[PM, I, have, no, opinion, about, you, dear] Neutral and so on..There are 30,000 rows
</code></pre>
<p><strong>P.S. Note that each tweet or row is a list of words for the column tweet_content.</strong></p>
<p><strong>Expected output for the above tweets:</strong> (unique_positive, unique_negative etc is for all the tweets in the df. There are 30,000 rows. So unique positive will be the list of words which are unique for positive sentiment for all 30,000 rows combined. Here I have just taken 3 tweets as random eg</p>
<pre><code>unique_positive = [rock] #you and PM occur in Negative and Neutral tweets, man occurs in negative tweet
unique_negative = [are , an, idiot] #you and PM occur in Positive and Neutral tweets, man occurs in positive tweet 
unique_positive = [I, have, no, opinion, about, dear] #you and PM occur in Negative and Neutral tweets
</code></pre>
<p>where</p>
<pre><code>raw_text = [word for word_list in train['content'] for word in word_list] #list of all words
unique_Positive= words_unique('positive', 20, raw_text) #find 20 unique words which are only in positive sentiment from list of all words 
</code></pre>
<p><strong>Problem:</strong>
The <strong>below function runs perfectly</strong> and finds unique words for positive, neutral and negative sentiments. <strong>But</strong> the problem is it is <strong>taking 30 minutes</strong> to run. <strong>Is there a way to optimise this function and run it faster?</strong>.</p>
<p><strong>Function to find out the unique words for each sentiment:</strong></p>
<pre><code>def words_unique(sentiment,numwords,raw_words):
    '''
    Input:
        segment - Segment category (ex. 'Neutral');
        numwords - how many specific words do you want to see in the final result; 
        raw_words - list  for item in train_data[train_data.segments == segments]['tweet_content']:
    Output: 
        dataframe giving information about the numwords number of unique words in a particular sentiment (in descending order based on their counts)..

    '''
    allother = []
    for item in train[train.sentiment != sentiment]['tweet_content']:
        for word in item:
            allother .append(word)
    allother  = list(set(allother ))
    
    specificnonly = [x for x in raw_text if x not in allother]
    
    mycounter = Counter()
    
    for item in train[train.sentiment == sentiment]['tweet_content']:
        for word in item:
            mycounter[word] += 1
    keep = list(specificnonly)
    
    for word in list(mycounter):
        if word not in keep:
            del mycounter[word]
    
    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])
    
    return Unique_words
</code></pre>
","11760970","","11760970","","2020-06-28 11:17:33","2020-06-28 15:37:04","Tweets analysis: Get unique positive, unique negative and unique neutral words : Optimised solution:Natural Language processing:","<python><pandas><twitter><nlp><textblob>","1","0","","","","CC BY-SA 4.0"
"48486757","1","","","2018-01-28 13:15:44","","0","198","<p>I am trying to create a Python user defined scalar (UDF) function in an AWS Redshift DB. The UDF wraps the following Python code:</p>

<pre><code>CREATE or replace library nltk language plpythonu from 's3://xxx/dev/python-libraries/nltk-3.2.1.zip'
credentials 'aws_access_key_id=xxx;aws_secret_access_key=yyy' region as 'eu-west-1';

CREATE or replace library textblob language plpythonu from 's3://xxx/dev/python-libraries/textblob-0.15.1-py2.py3-none-any.zip'
credentials 'aws_access_key_id=xxx;aws_secret_access_key=yyy' region as 'eu-west-1';

CREATE or replace FUNCTION f_sentiment_polarity (comment varchar(1000)) RETURNS float IMMUTABLE as $$
from textblob import TextBlob
return TextBlob(comment).sentiment.polarity
$$ LANGUAGE plpythonu;

SELECT f_sentiment_polarity('this would be very useful if the corpora were loaded');

f_sentiment_polarity
--------------------
                   0
</code></pre>

<p>The result of the <code>select</code> statement gives me <code>0</code></p>

<p>When I run the same Python code in a local environment (Python 2.7 on Windows with NLTK v3.2.5, I get <code>0.39</code>:  </p>

<pre><code>Python 2.7.10 (default, May 23 2015, 09:44:00) [MSC v.1500 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from textblob import TextBlob
&gt;&gt;&gt; TextBlob('this would be very useful if the corpora were loaded').sentiment.polarity
0.39
&gt;&gt;&gt;
</code></pre>

<p>I presume that this is because the various NLTK Corpora have not been loaded in the AWS Redshift Python environment.  Creating another Redshift UDF as follows seems to bear this out:</p>

<pre><code>CREATE or replace FUNCTION f_num_brown_words () RETURNS int IMMUTABLE as $$
from nltk.corpus import brown
return len(brown.words())
$$ LANGUAGE plpythonu;

select f_num_brown_words();

ERROR: XX000: LookupError: 
**********************************************************************
  Resource u'corpora/brown' not found.  Please use the NLTK
  Downloader to obtain the resource:  &gt;&gt;&gt; nltk.download()
  Searched in:
    - ""'/'/nltk_data""
    - '/usr/shar
</code></pre>

<p>Question: Is there a way of loading the NLTK Corpora in the AWS Redshift Python environment so that my UDF will function correctly? </p>
","5546477","","","","","2018-01-28 17:22:27","Can NLTK Data be installed in AWS Redshift environment?","<python><amazon-web-services><nltk><amazon-redshift><textblob>","1","2","","","","CC BY-SA 3.0"
"20562768","1","20564786","","2013-12-13 09:12:31","","2","3749","<p>I am new to programming, and I'm trying to install the TextBlob library for Python to help me do some stuff. Sadly, I'm having trouble installing TextBlob, let alone use it. I am using Windows, which seems to make things more difficult. I wish I could just run the Linux commands or whatever they are that everybody uses. Anyway</p>

<p>Here is what I have done so far:</p>

<ol>
<li>Forked the Textblob program from <a href=""https://github.com/sloria/TextBlob"" rel=""nofollow"">here.</a></li>
<li>Copied the entire repository to my desktop, and opened the folder up.</li>
<li>Using Command Prompt, ran ""Python C:\Users...\setup.py install""</li>
</ol>

<p>Command Prompt spits back-</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\Sam\Desktop\TextBlob\setup.py"", line 56, in &lt;module&gt;
    __version__ = find_version(""textblob/__init__.py"")
  File ""C:\Users\Sam\Desktop\TextBlob\setup.py"", line 45, in find_version
    with open(fname, 'r') as fp:
IOError: [Errno 2] No such file or directory: 'textblob/__init__.py'
</code></pre>

<p>And I have no idea what to do or how to fix this. Shouldn't this fresh download of TextBlob have the ability to install? What am I messing up?</p>
","3092966","","","user2555451","2014-11-15 17:19:10","2014-11-15 17:19:10","Trouble installing TextBlob for Python","<python><textblob>","1","4","","","","CC BY-SA 3.0"
"26893417","1","26902543","","2014-11-12 17:57:12","","2","340","<p>I am trying to run noun phrase analysis in a Flask app running on Ubuntu, served through gunicorn and nginx. I am getting an error 500 with no (apparent) logging of the error occurring either in nginx, supervisor, or unicorn error logs. Nor does 'supervisorctl tail app' shed any light. </p>

<p>My sites-available nginx.conf:</p>

<pre><code>server {
    listen 80;
    server_name [domain redacted];
    charset utf-8;
    client_max_body_size 75M;

    access_log /var/log/nginx/nginx_access.log;
    error_log /var/log/nginx/nginx_error.log;

    location / { try_files $uri @app; }

    location @app {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
</code></pre>

<p>My supervisor app.conf</p>

<pre><code>[program:app]
command = gunicorn app:app -b localhost:8000
directory = /home/www/app
user = admin
</code></pre>

<p>I am running my app in app.py with the following (issue experienced with DEBUG = False and True in config.py)</p>

<pre><code>app = Flask(__name__, static_folder='static', static_url_path='/static')
app.config.from_pyfile('config.py')

if __name__ == '__main__':
        app.run()
        if not app.debug:
            stream_handler = logging.StreamHandler()
            stream_handler.setLevel(logging.INFO)
            app.logger.addHandler(stream_handler)
</code></pre>

<p>Config.py is simply</p>

<pre><code>DEBUG = False
ALLOWED_HOSTS=['*']
</code></pre>

<p>The noun phrases function I am calling</p>

<pre><code>from textblob import TextBlob

def generateNounPhrases(input):
    blob = TextBlob(input)
    np = blob.noun_phrases

    return np
</code></pre>

<p>The app.py flask route for the page, passing the output of generateNounPhrases()</p>

<pre><code>@app.route('/thread', methods=['GET'])
def thread():
    ...
    nounphrases = generateNounPhrases(text_to_analyze)   
    ...

    return render_template(""Thread.html"", nounphrases=nounphrases)
</code></pre>

<p>I am absolutely lost and am an absolute novice at this. Any guidance would be tremendous!</p>
","3399542","","","user2555451","2014-11-15 17:29:16","2014-11-15 17:29:16","NLTK+TextBlob in flask/nginx/gunicorn on Ubuntu 500 error","<ubuntu><flask><nltk><textblob>","1","2","","","","CC BY-SA 3.0"
"66959012","1","","","2021-04-05 19:55:26","","0","27","<p>I have a df structured as follows:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>text</th>
<th>sentiment</th>
</tr>
</thead>
<tbody>
<tr>
<td>XXXXX</td>
<td>yes</td>
</tr>
<tr>
<td>YYYYYY</td>
<td>no</td>
</tr>
</tbody>
</table>
</div>
<p>I'm trying to check the accuracy manually, according to this code ... however, I can't apply it to my DF. and I have the following error: ValueError: too many values to unpack (expected 2)</p>
<pre><code>    for text, sentiment in df:
    classification = TextBlob(text).sentiment.polarity

if (classification &lt;=0 and sentiment == &quot;no&quot;):
    Sum_classification +=1
elif (classification &gt;=0 and sentiment ==&quot;yes&quot;):
    Sum_classification +=1  

print(&quot;the Accuracy is &quot;, Sum_classification/len(df))
</code></pre>
<p>So, I decided to transform this to the list using the following code, which makes the code run without error, but I don't have the expected result, once I get</p>
<blockquote>
<p>the Accuracy is 0.0</p>
</blockquote>
<pre><code>list = list(zip(df_test['text'], df_test['sentiment']))
</code></pre>
","14612064","","","","","2021-04-05 21:05:34","For using a dataframe in Python","<python><for-loop><text-mining><textblob>","2","0","","","","CC BY-SA 4.0"
"43688542","1","43802866","","2017-04-28 20:38:29","","4","6654","<p>Does anyone know how textblob sentiment is working? I know it is working based on Pattern but I could not find any article or document explain how pattern assigns polarity value to a sentence.  </p>
","5115749","","5115749","","2017-04-28 20:43:45","2020-05-22 22:56:58","Textblob sentiment algorithm","<sentiment-analysis><textblob>","2","2","4","","","CC BY-SA 3.0"
"52573331","1","52639902","","2018-09-29 23:22:37","","-1","199","<p>I have a large csv with thousands of comments from my blog that I'd like to do sentiment analysis on using textblob and nltk.</p>

<p>I'm using the python script from <a href=""https://wafawaheedas.gitbooks.io/twitter-sentiment-analysis-visualization-tutorial/sentiment-analysis-using-textblob.html"" rel=""nofollow noreferrer"">https://wafawaheedas.gitbooks.io/twitter-sentiment-analysis-visualization-tutorial/sentiment-analysis-using-textblob.html</a>, but modified for Python3.</p>

<pre><code>'''
uses TextBlob to obtain sentiment for unique tweets
'''

from importlib import reload
import csv
from textblob import TextBlob
import sys

# to force utf-8 encoding on entire program
#sys.setdefaultencoding('utf8')

alltweets = csv.reader(open(""/path/to/file.csv"", 'r', encoding=""utf8"", newline=''))
sntTweets = csv.writer(open(""/path/to/outputfile.csv"", ""w"", newline=''))

for row in alltweets:
    blob = TextBlob(row[2])
    print (blob.sentiment.polarity)
    if blob.sentiment.polarity &gt; 0:
        sntTweets.writerow([row[0], row[1], row[2], row[3], blob.sentiment.polarity, ""positive""])
    elif blob.sentiment.polarity &lt; 0:
        sntTweets.writerow([row[0], row[1], row[2], row[3], blob.sentiment.polarity, ""negative""])
    elif blob.sentment.polarity == 0.0:
        sntTweets.writerow([row[0], row[1], row[2], row[3], blob.sentiment.polarity, ""neutral""])
</code></pre>

<p>However, when I run this, I continually get</p>

<pre><code>    $ python3 sentiment.py
Traceback (most recent call last):
  File ""sentiment.py"", line 17, in &lt;module&gt;
    blob = TextBlob(row[2])
IndexError: list index out of range
</code></pre>

<p>I know what the error means, but I'm not sure what I need to do to fix.</p>

<p>Any thoughts on what I'm missing?  Thanks!</p>
","1725872","","","","","2018-10-04 05:55:09","list index out of range error with TextBlob to csv","<python><csv><nlp><textblob>","1","5","","","","CC BY-SA 4.0"
"35759673","1","35981202","","2016-03-02 22:42:16","","0","488","<p>I am interested in building a text classifier using textBlob but from my research does not look like after you train the classifier to return neutral tags. Does anyone know a way to implement this ? Or is there a similar library which provides neutral classification ? Thank you.</p>
","3918551","","","","","2016-03-14 06:51:59","Is there any feature of TextBlob to obtain neutral classification?","<python><text-mining><textblob>","1","0","","","","CC BY-SA 3.0"
"35420602","1","","","2016-02-15 22:52:53","","4","6030","<p>I have been using <code>textblob</code> in Python 2.7.10 on Windows for quite some time, and unexpectedly, it stopped working. Testing with two independent virtual machines as well as on OS X produces the same error. </p>

<p>Testing a simple snippet from the <a href=""http://textblob.readthedocs.org/en/dev/quickstart.html#translation-and-language-detection"" rel=""nofollow"">docs</a>:</p>

<pre><code>    from textblob import TextBlob
    en_blob = TextBlob(u'Simple is better than complex.')
    print(en_blob.translate(to='es'))
</code></pre>

<p>produces an error:</p>

<blockquote>
<pre><code>File ""test.py"", line 3, in &lt;module&gt; print(en_blob.translate(to='es'))

File ""C:\Python27\lib\site-packages\textblob\blob.py"", line 509, in translate
from_lang=from_lang, to_lang=to))

File ""C:\Python27\lib\site-packages\textblob\translate.py"", line 45, in translate
raise NotTranslated('Translation API returned the input string unchanged.')

textblob.exceptions.NotTranslated: Translation API returned the input string 
unchanged.
</code></pre>
</blockquote>

<p>How can I debug this error?</p>
","5695866","","2320823","","2016-02-16 08:24:20","2020-10-11 10:30:30","Python textblob Translation API Error","<python><translation><textblob>","4","0","","","","CC BY-SA 3.0"
"62112793","1","","","2020-05-31 07:10:17","","0","31","<p>I am using TextBlob for spell check. Here is my code:</p>

<pre><code>corrected_words = []

for i in data_words:
    for j in i:
        corrected_words.append((TextBlob(j).correct()))
</code></pre>

<p>The result that I get is as follows:</p>

<pre><code>[TextBlob(""me""),
 TextBlob(""correct""),
 TextBlob(""table""),
 TextBlob(""breakfast"")]
</code></pre>

<p>How do I simply output the text inside the quotations?
Have also tried using .text against the correct() function, but no success.</p>

<p>TIA</p>
","13600660","","","","","2020-06-21 08:44:42","How to get the text inside the TextBlob result?","<spell-checking><textblob>","1","0","","","","CC BY-SA 4.0"
"44322645","1","","","2017-06-02 06:55:53","","1","37","<p>I am training a sentiment analysis model using tweepy and textBlob. I came across an interesting instance in which the sentiment of a statement can be affected by the position of stakeholder. Wondering how sentiment analysis works in such case</p>
<blockquote>
<p>RT @Pehla_Trade: #Rupee strengthened by 14 paise to 64.34 against the US #dollar in early trade at the Interbank Foreign Exchange market</p>
<p>Sentiment(polarity= -0.012499999999999997, subjectivity=0.2125)</p>
</blockquote>
<p>In the above statement the polarity of sentiment is -ve, but as a reader, I feel the value should be positive instead</p>
","4372824","","-1","","2020-06-20 09:12:55","2017-06-02 19:42:32","How perspective of two competing stakeholders are captured in sentiment analysis","<machine-learning><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 3.0"
"55362335","1","","","2019-03-26 16:48:48","","0","214","<p>I've started to use the TextBlob library; for sentiment analysis. 
I have run a few tests on a few phrases and I have the polarity and subjectivity score - fine. </p>

<p>What sentence would return the highest <em>polarity</em> value within TextBlob?</p>

<p>For instance </p>

<p>""I really, really, really love and admire your beauty, my good friend"" </p>

<p>returns a polarity score of 0.6. </p>

<p>I understand that +1.0 is the highest score (-1.0) is the least.</p>

<p>What sentence, have you found which returns a score closer to +1.0?</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>TextBlob(""I really, really, really love and admire your beauty my good friend"").sentiment
      Sentiment(<em>polarity=0.6</em>, subjectivity=0.6000000000000001)</p>
      
      <p>TextBlob(""I really, really, really love my place of work"").sentiment 
      Sentiment(<em>polarity=0.5</em>, subjectivity=0.6)</p>
      
      <p>TextBlob(""I really love my place of work"").sentiment
      Sentiment(<em>polarity=0.5</em>, subjectivity=0.6)</p>
    </blockquote>
  </blockquote>
</blockquote>

<hr>

<p>I expect that the ""really"" should increase the sentiment score, at least a bit.  (i.e. really, really like = at least 0.9)</p>

<p>I expect that the score overall, without the really (I really like my work) should return a score closer to 1.0.</p>
","6912460","","","","","2019-09-26 20:11:07","Highest Polarity Score (Sentiment Analysis) using the TextBlob library","<python><textblob>","1","3","","","","CC BY-SA 4.0"
"37094411","1","","","2016-05-07 22:36:18","","0","528","<p>**Issue Resolved in <a href=""https://stackoverflow.com/questions/37150205/python-2-7-and-textblob-typeerror-the-text-argument-passed-to-init-tex"">this post</a>**I am using a Windows 10 PC and trying to scrape and analyze a website forum.  My solution uses Scrapy and Textblob, and I'm running Python 2.7.  The scraping generates the desired output (which I save as either a .csv or .json).  However, when I use this file in the Python script that integrates TextBlob, I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\Marcus\Documents\Blog\Python\Scripts\Brooks\textblob_sentiment.py"", line 14, in &lt;module&gt;
print blob
  File ""C:\Python27\lib\site-packages\textblob\compat.py"", line 30, in &lt;lambda&gt;
cls.__str__ = lambda x: x.__unicode__().encode('utf-8')
UnicodeDecodeError: 'ascii' codec can't decode byte 0xf0 in position 425: ordinal not in range(128)
</code></pre>

<p>Script that generates this error is:</p>

<pre><code># from __future__ import division, unicode_literals (This was recommended     for Python 2.x, but didn't help in my case.)

import csv

from textblob import TextBlob


infile = 'items.csv'

with open(infile, 'r') as scrape_file:
    comments = csv.reader(scrape_file)
    for comment in comments:
        sentence = comment[0]
        blob = TextBlob(sentence)
        print blob
</code></pre>

<p>Structure of the code is similar to another thread I found on SO, and I have also tried to integrate encode/decode methods into this script based on other threads I found on SO.  But perhaps I didn't do so properly  (I am not a developer).  I also tried to open the json file, thinking perhaps the problem was the way that the .csv was encoded.
I can print the desired content (e.g., ""print sentence"" or ""print comments"", it is only when I try to use TextBlob that I receive the error.<br>
Might you have a solution that unblocks this error?
And since I'd like to use these libraries again, how can I avoid similar headaches?</p>

<p>thanks so much for your help on this...</p>
","5588205","","-1","","2017-05-23 12:23:57","2016-05-11 23:57:26","UnicodeDecodeError using TextBlob and Python 2.7","<python-2.7><scrapy><textblob>","1","3","","","","CC BY-SA 3.0"
"61055154","1","61056020","","2020-04-06 07:48:03","","0","183","<p>I need to delete all the proper noun from the text. 
result is the Dataframe.
I'm using text blob. Below is the code.  </p>

<pre><code>from textblob import TextBlob

          strings = []
            for col in result:
                for i in range(result.shape[0]):
                    text = result[col][i]
                    Txtblob = TextBlob(text)

                    for word, pos in Txtblob.noun_phrases:
                        print (word, pos)
                        if tag != 'NNP'
                           print(' '.join(edited_sentence))
</code></pre>

<p>It just recognizes one NNP</p>
","12736061","","12736061","","2020-04-06 08:03:23","2020-04-07 11:26:31","Deleting all the noun phrases from text using Textblob","<python><textblob>","1","9","","","","CC BY-SA 4.0"
"28004731","1","28004853","","2015-01-17 21:50:43","","1","4544","<p>I'm trying to find if a sentence contains the phrase ""go * to"", for example ""go over to"", ""go up to"", etc.  I'm using Textblob, and I know I can just use below:</p>

<pre><code>search_go_to = set([""go"", ""to""])
go_to_blob = TextBlob(var)
matches = [str(s) for s in go_to_blob.sentences if search_go_to &amp; set(s.words)]
print(matches)
</code></pre>

<p>but that would also return sentences like ""go over there and bring this to him"", which I don't want. Anyone know how I can do something like text.find(""go * to"")?</p>
","2246547","","","","","2015-01-18 13:07:30","Python searching for two words regex","<python><regex><search><nltk><textblob>","4","0","1","","","CC BY-SA 3.0"
"61664854","1","","","2020-05-07 18:14:08","","2","101","<p><strong>When I execute this code it terminates without any error but there is no any output. I have attached the right twitter authentication credentials.what would be the issue</strong></p>

<h1>Twitter-Sentiment-Analysis in python using tweepy and Textblob</h1>

<pre><code>from textblob import TextBlob
import sys, tweepy
import matplotlib.pyplot as plt

def percentage(part, whole):
    return 100* float(part)/float(whole)

consumerKey = ""xxxxx""
consumerSecret = ""xxxx""
accessToken = ""xxxx""
accessTokenSecret = ""xxxx""

auth = tweepy.OAuthHandler(consumerKey, consumerSecret)
auth.set_access_token(accessToken, accessTokenSecret)
api = tweepy.API(auth)

searchTerm = input(""Enter keyword/ hashtag you want to search : "")
noOfSearchTerm = int(input(""Enter how many tweets to analyze : ""))

tweets = tweepy.Cursor(api.search, q=searchTerm, lang=""English"").items(noOfSearchTerm)


positive = 0
negative = 0
neutral = 0
polarity = 0
</code></pre>

<h1>make textblob variable called analysis</h1>

<pre><code>for tweet in tweets:
    analysis = TextBlob(tweet.text)
    polarity += analysis.sentiment.polarity

    if (analysis.sentiment.polarity == 0):
        neutral += 1

    elif (analysis.sentiment.polarity &lt; 0.00):
        negative += 1

    if (analysis.sentiment.polarity &gt; 0.00):
        positive += 1

positive = percentage(positive, noOfSearchTerm)
negative = percentage(negative, noOfSearchTerm)
neutral = percentage(neutral, noOfSearchTerm)

positive = format(positive, '.2f')
neutral = format(neutral, '.2f')
negative = format(negative, '.2f')

print(""How people are reacting on "" + searchTerm + ""by analysing "" + str(noOfSearchTerm) + "" Tweets."")

if(polarity == 0):
    print(""Neutral"")
elif (polarity &lt; 0):
    print(""Negative"")
elif (polarity &gt; 0):
    print(""Positive"")

labels = ['Positive ['+str(positive)+ '%]', 'Neutral [' + str(neutral) + '% ]', 'Negative [' + str(negative) + '%]']
sizes = [positive, neutral, negative]
colors = ['yellowgreen', 'gold', 'red']
patches, texts = plt.pie(sizes, colors=colors, startangle=90)
plt.legend(patches, labels, loc=""best"")
plt.title(""How people are reacting on "" + searchTerm + ""by analysing "" + str(noOfSearchTerm) + "" Tweets."")
plt.axis('equal')
plt.tight_layout()
plt.show()
</code></pre>
","7902840","","","","","2020-05-07 18:14:08","Twitter-Sentiment-Analysis in python 3 using tweepy and Textblob","<python-3.x><tweepy><sentiment-analysis><textblob>","0","3","1","","","CC BY-SA 4.0"
"18426127","1","18476685","","2013-08-25 05:44:24","","0","830","<p>I'm trying to use Tolkein's Silmarillion as a practice text for learning some NLP with nltk.</p>

<p>I am having trouble getting started because I'm running into text encoding issues.</p>

<p>I'm using the TextBlob wrapper (<a href=""https://github.com/sloria/TextBlob"" rel=""nofollow"">https://github.com/sloria/TextBlob</a>) around NLTK because it's a lot easier.  TextBlog is available at:</p>

<p>The sentence that I can't parse is:</p>

<pre><code>""But Húrin did not answer, and they sat beside the stone, and did not speak again"".
</code></pre>

<p>I believe it's the special character in Hurin causing the issue.</p>

<p>My code:</p>

<pre><code>from text.blob import TextBlob
b = TextBlob( 'But Húrin did not answer, and they sat beside the stone, and did not speak again' )
b.noun_phrases

UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 1: ordinal not in range(128)
</code></pre>

<p>As this is just a for-fun project, I just want to be able to use this text and extracting some attributes and do some basic processing.</p>

<p>How can I convert this text to ASCII when I don't know what the initial encoding is?  I tried to decode from UTF8, then re-encode into ASCII:</p>

<pre><code>&gt;&gt;&gt; asc = unicode_text.decode('utf-8')
&gt;&gt;&gt; asc = unicode_text.encode('ascii')

UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 10: ordinal not in range(128)
</code></pre>

<p>But even that doesn't worry.  Any suggestions are appreciated -- I'm fine with losing the special characters, as long as it's done consistently across the document.</p>

<p>I'm using python 2.6.8 with the required modules also correctly installed.</p>
","40707","","","user2555451","2014-11-15 17:38:50","2014-11-15 17:38:50","Can't use NLTK on text pulled from the Silmarillion","<python><character-encoding><nltk><textblob>","1","6","","","","CC BY-SA 3.0"
"62643461","1","","","2020-06-29 17:20:10","","1","34","<p>I am working on categorize texts using TextBlob on Python and NaiveBayesClassifier.</p>
<p>The goal is to do continuous learning, which is made possible on NaiveBayesClassifier by using the update method.</p>
<p>But there's a problem : the classifier grow and grow and grow without limit !</p>
<p>How I can set a limit or &quot;clean&quot; the classifier from useless data after update ?</p>
<p>Looking in the sources in NLTK ( <a href=""https://www.nltk.org/_modules/nltk/classify/naivebayes.html"" rel=""nofollow noreferrer"">https://www.nltk.org/_modules/nltk/classify/naivebayes.html</a> ) I didn't saw anything useful.</p>
","13835439","","","","","2020-06-29 17:20:10","On Python TextBlob : limit size of model when update","<python><machine-learning><nltk><textblob>","0","0","","","","CC BY-SA 4.0"
"36560284","1","","","2016-04-11 22:19:20","","1","632","<p>I'm trying to implement the Naive Bayes classifier on tweets using TextBlob in python. I have been able to train the dataset and can successfully classify individual tweets using:</p>

<pre><code>print cl.classify(""text"")
</code></pre>

<p>Now I want to open a csv file and classify all the tweets in that file. Any suggestions on how I can achieve this? My code is as below:</p>

<pre><code>import csv
from textblob import TextBlob

with open(test_path, 'rU') as csvfile:
    lineReader = csv.reader(csvfile,delimiter=',',quotechar=""\"""")
    lineReader = csv.reader(csvfile,delimiter=',')

    test = []
    for row in lineReader:
      blob = (row[0]) 
      blob = TextBlob(blob)
      test.append([blob])

      print (test.classify())
</code></pre>

<p>AttributeError: 'list' object has no attribute 'classify'</p>
","5627769","","","","","2016-07-17 17:57:14","TextBlob Naive Bayes text classification","<python><text-classification><textblob>","1","3","1","","","CC BY-SA 3.0"
"30115917","1","30116148","","2015-05-08 04:44:49","","1","2991","<p>I am trying to use textBlob with a text file input.</p>

<p>All examples I found online were of input in this sense:</p>

<pre><code>wiki = TextBlob(""Python is a high-level, general-purpose programming language."")
wiki.tage
</code></pre>

<p>I tried this:</p>

<pre><code>from textblob import TextBlob
file=open(""1.txt"");
t=file.read();
print(type(t))
bobo = TextBlob(t)
bobo.tags
</code></pre>

<p>The code I tried did not work.</p>
","4877542","","-1","","2018-06-23 16:40:23","2018-06-23 16:40:23","Open text file as input to textblob","<python><readfile><textblob>","3","3","2","","","CC BY-SA 4.0"
"61262509","1","","","2020-04-17 01:03:17","","1","32","<p>I want to pass more than one parameter to perform lemmatization, like 'verb', 'none' using textblob</p>

<p>here is my code which I grabbed from here :<a href=""https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/lemmatization-examples-python/</a></p>

<p>My question is how I can modify my code to pass both 'verb','none' to the function </p>

<pre><code>def lemm_text(text):
    text = str(text)
    sent = TextBlob(text)
    tag_dict = {""J"": 'a', 
                ""N"": 'n', 
                ""V"": 'v', 
                ""R"": 'r'}
    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    
    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]
    return "" "".join(lemmatized_list)

data_words_nostops = map(lemm_text, data_words_nostops)
list(data_words_nostops)
</code></pre>

<p>I tried to pass  or tuple but it didn't work and I got below error :</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-35-1fd4bee599c2&gt; in &lt;module&gt;
----&gt; 1 list(data_words_nostops)

&lt;ipython-input-30-062f187cdad0&gt; in lemm_text(text)
      7                 ""R"": 'r'}
      8     words_and_tags = [(w, tag_dict.get(pos[0], ['n','v'])) for w, pos in sent.tags]
----&gt; 9     lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]
     10     return "" "".join(lemmatized_list)

&lt;ipython-input-30-062f187cdad0&gt; in &lt;listcomp&gt;(.0)
      7                 ""R"": 'r'}
      8     words_and_tags = [(w, tag_dict.get(pos[0], ['n','v'])) for w, pos in sent.tags]
----&gt; 9     lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]
     10     return "" "".join(lemmatized_list)

C:\ProgramData\Anaconda3\lib\site-packages\textblob\decorators.py in decorated(*args, **kwargs)
     33     def decorated(*args, **kwargs):
     34         try:
---&gt; 35             return func(*args, **kwargs)
     36         except LookupError as err:
     37             print(err)

C:\ProgramData\Anaconda3\lib\site-packages\textblob\blob.py in lemmatize(self, pos)
    145         if pos is None:
    146             tag = _wordnet.NOUN
--&gt; 147         elif pos in _wordnet._FILEMAP.keys():
    148             tag = pos
    149         else:

TypeError: unhashable type: 'list'
</code></pre>
","8043312","","","","","2020-04-17 02:31:47","How to pass more than one tag when Lemmatizing using text blob","<python><lemmatization><textblob>","1","3","","","","CC BY-SA 4.0"
"64810721","1","","","2020-11-12 19:45:01","","0","112","<p>I have coded an application that performs sentiment analysis on a topic and exports the tweets to a CSV file but it only exports a few tweets using Tweepy and TextBlob. I want to insert it into a while loop so it continues to run and add to the CSV file until I stop the program. Any help would be appreciated. Or any different way of doing it.</p>
<pre><code>import tweepy
import csv
from urlextract import URLExtract
from textblob import TextBlob

consumer_key = 'KEY'
consumer_secret = 'KEY'
access_token = 'KEY'
access_token_secret = 'KEY'

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)

api = tweepy.API(auth)

public_tweets = api.search('Trump')

with open('twitter_sentiment_analysis.csv', 'w', encoding=&quot;utf-8&quot;, newline='') as output:
    fileOut = csv.writer(output)
    data = [['Tweets', 'Polarity', 'Subjectivity', 'URL']]

    fileOut.writerows(data)

    for tweet in public_tweets:
        analysis = TextBlob(tweet.text)
        polarity = analysis.sentiment.polarity
        subjectivity = analysis.sentiment.subjectivity


        url = None

        words = tweet.text.split()

        link = URLExtract()

        urls = link.find_urls(tweet.text)

        for word in words:
            # print (word)
            if 'http' in word:
                url = word

        fileOut.writerow([tweet.text, polarity, subjectivity, url])

        print(tweet.text)
        print('Polarity: ', polarity)
        print('Subjectivity:', subjectivity)
</code></pre>
","14628307","","","","","2020-11-12 19:45:01","Using Tweepy & TextBlob To Export Analysis To CSV In A Loop","<python><tweepy><sentiment-analysis><textblob>","0","2","","","","CC BY-SA 4.0"
"62716522","1","","","2020-07-03 13:32:26","","0","44","<p>I have been trying to cythonize the following loop, however I can't seem to <code>cdef</code> the <code>np.array([],str)</code>. When compiling, array is not recognised as being part of numpy.</p>
<pre><code>import numpy as np
from textblob import TextBlob

def arr_blob_corr(str_arr1):
    
    ph1=np.array([],str)
    
    for n1 in range(0,str_arr1.shape[0]):
        ph2=TextBlob(str_arr1[n1].lower()).correct().raw
        ph1=np.append(ph1,ph2)
        
    return ph1

</code></pre>
<p>I have tried</p>
<pre><code>cdef np.ndarray arr2=np.ndarray(arr1,str)
</code></pre>
<p>and</p>
<pre><code>cdef np.nrray arr2=np.array(arr1,str)
</code></pre>
<p>To no avail.</p>
","10005867","","","","","2020-07-03 13:32:26","Cythonizing a textblob autocorrect loop of a string array","<python><cython><textblob><autocorrect>","0","5","","","","CC BY-SA 4.0"
"64620918","1","","","2020-10-31 10:51:23","","0","45","<pre><code>                detection = TextBlob('The African Development Bank hereafterreferred).detect_language()
</code></pre>
<blockquote>
<p>this code gives error as:</p>
</blockquote>
<pre><code>TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond
</code></pre>
<blockquote>
<p>how can I solve it.  my internet speed is stable and good.</p>
</blockquote>
","10369901","","","","","2020-10-31 10:51:23","why TextBlob().detect_language gives me timeout error","<python><textblob>","0","0","","","","CC BY-SA 4.0"
"66830855","1","","","2021-03-27 11:33:43","","0","32","<p>I'm using TextBlob to correct a piece of text. However, I'm not getting very good results, and i'm wondering if there's anything that can be done to improve the accuracy. After checking the documentation, I've found this in the Tectblob official website:</p>
<pre><code>&gt;&gt;&gt; from textblob import Word
&gt;&gt;&gt; w = Word('falibility')
&gt;&gt;&gt; w.spellcheck()
[('fallibility', 1.0)]

Word objects have a spellcheck() Word.spellcheck() method that returns a list of (word, confidence) tuples with spelling suggestions.
</code></pre>
<p>I'm wondering if there's a way to change the <code>spellecheck</code> function so no corrections are made for words that are below a specific confidence. Additionally, if there's a way to ignore words that begin with capital letters from being corrected. For example, the following sentence (which is correct) is <strong>corrected</strong> again like this:</p>
<pre><code>original = &quot;Alonso lost the 2010 world championship in his Ferrari stuck behind Vitaly Petrov's Renault for most of the race.&quot;
corrected = &quot;Alonso lost the 2010 world championship in his Errare stuck behind Italy Petrov's Default for most of the race.&quot;
</code></pre>
<p>I think that changing the CI of the outputted word, and ignoring words that begin with capital letters, will increase the overall accuracy significantly.
Please let me know if someone knows how to do that.</p>
","14857189","","","","","2021-03-27 11:33:43","TextBlob - Ignore names and alter output confidence","<python><textblob>","0","0","","","","CC BY-SA 4.0"
"57603939","1","57604208","","2019-08-22 07:17:33","","0","63","<p>I created a list with lists from a text document with WordBlob. Now I want to create a list with the greatest difference within each list and I am only interested in the polarity. I thought of appending the highest and lowest numbers to another list and then substracting them from each other. But how can I refer to the numbers in 'polarity' at all? This is my nested list:</p>

<pre class=""lang-py prettyprint-override""><code>[[Sentiment(polarity=0.35, subjectivity=0.65),
  Sentiment(polarity=0.0, subjectivity=0.0),
  Sentiment(polarity=0.0, subjectivity=0.0),
  Sentiment(polarity=0.6, subjectivity=0.87),
  Sentiment(polarity=0.0, subjectivity=0.0),
  Sentiment(polarity=0.0, subjectivity=0.0)],
 [Sentiment(polarity=0.0, subjectivity=0.0),
  Sentiment(polarity=0.5, subjectivity=0.8),
  Sentiment(polarity=0.0, subjectivity=0.0),
  Sentiment(polarity=-0.29, subjectivity=0.54),
  Sentiment(polarity=0.0, subjectivity=0.0),
  Sentiment(polarity=0.25, subjectivity=1.0)],
  [Sentiment(polarity=0.5, subjectivity=0.8),
  Sentiment(polarity=0.0, subjectivity=0.0)]]
</code></pre>

<p>Does someone has an idea? Thanks for help.</p>
","11451699","","11451699","","2019-08-22 10:21:08","2019-08-22 10:22:29","Calculate the greatest difference in a nested list","<python><nested-lists><textblob>","2","0","","","","CC BY-SA 4.0"
"18138022","1","","","2013-08-08 23:39:25","","1","595","<p>I recently started using the TextBlob package for Python (version 0.3.7). According to the documentation, the <code>.sentiment</code> method returns a tuple of sentiment polarity and subjectivity, with the defined range for polarity being between <code>-1</code> and <code>+1</code>.</p>

<p>However, this code gives me a polarity value of <code>-1.24</code>. </p>

<p>Is this a known bug or is there anything wrong with my usage of the code?</p>

<pre><code>from text.blob import TextBlob
MyText = '''
tired of nbc universal. got 2 of the 3 periods of tonight's game and 62 of the 78 laps of the f1 race in monaco. stop the pregame crap and fill it with the event you morons!!!!!!
'''
text = TextBlob(MyText)
print text.sentiment
</code></pre>
","2361008","","","user2555451","2014-11-15 17:30:07","2014-11-15 17:30:07","Why does TextBlob 0.3.7 sometimes generate sentiment polarity of less than -1","<python><python-2.7><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 3.0"
"54522998","1","54523284","","2019-02-04 19:24:56","","1","1106","<p>I need a language detection script. I tried Textblob library which right now give me the two letter abbreviation of the language. How can I get the complete language expansion?</p>

<p>This detects the language with two letter abbreviation of the language</p>

<pre><code>from textblob import TextBlob
b = TextBlob(""cómo estás"")
language = b.detect_language()
print(language)
</code></pre>

<p>Actual Results : es  <br>
Expected Results : Spanish</p>

<p>I have the list of language and their abbreviation from this link <br>
<a href=""https://developers.google.com/admin-sdk/directory/v1/languages"" rel=""nofollow noreferrer"">https://developers.google.com/admin-sdk/directory/v1/languages</a> </p>
","8333005","","8333005","","2019-02-04 19:43:57","2019-02-04 19:45:09","How to get language expansion in Textblob python language detection library","<python><textblob>","1","0","1","","","CC BY-SA 4.0"
"54724551","1","54725788","","2019-02-16 15:19:13","","2","512","<p>The following code runs on my friend's windows laptop perfectly but on mine, it spits out complete garbage. I think it might have something to do with the libraries but I can't say for sure? Can you guys let me know if it runs for you or if you have ever had this happen before?</p>

<pre><code>import pytesseract
from PIL import Image, ImageEnhance, ImageFilter
from textblob import TextBlob

originalImage = Image.open(""pictures\hp.png"")
workingImage = originalImage.copy()
workingImage = originalImage.filter(ImageFilter.MedianFilter())

enhanceSharpness = ImageEnhance.Sharpness(workingImage)
enhanceColor = ImageEnhance.Color(workingImage)

workingImage = enhanceSharpness.enhance(2)
workingImage = enhanceColor.enhance(0)
workingImage = workingImage.convert('1')

text = pytesseract.image_to_string(workingImage)
correctedText = TextBlob(text).correct()

print(correctedText)
</code></pre>

<p>This is the picture we are feeding it: <a href=""https://i.stack.imgur.com/VLj9g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VLj9g.png"" alt=""enter image description here""></a></p>

<p>Rather than printing the text in the photo as it does on my friend's computer, mine prints the following:</p>

<pre><code>of was an am: came: mm: mmm mm bu noma of arm 523w at""
mmmhmg peaches? w 3 cm {mamma a map m: K wm} Or Mummy
didn‘c realm when Am had cm then he mmm Am: Ma mad m

Am!» adam Them was 3 W317); Ll them on arm mmm a‘met
Turn mm ""mm wasn't a mm: m 313% Ham sum Am have. term mmm
M‘) It must had: {mm 3 mm ﬁftht‘ High: Or Him‘cy [asked and
hated :31 me cm of Taxed back Of I Mummy time mound! m:

come and up in: may M maxim} tin: ca: m Am mm‘m h Am raw
made m: mm mm mid and Run“ w Of iwkmg at me 5 Is
cmricbﬂ mad may or $$ka Or [)unity gave hrmsuifa hut: wake and
put 11% cm my m“ Am Am} Is he rim mere own M {high W
msihmg except &amp; Large may of'dry‘b M. M h my m E“ that thy
</code></pre>
","9067955","","5320906","","2019-02-16 16:47:42","2019-02-16 17:29:30","Pytesseract output is completely different from the text in the image","<python><python-imaging-library><textblob><python-tesseract>","1","4","","","","CC BY-SA 4.0"
"37576649","1","","","2016-06-01 18:53:56","","1","3070","<p>From what I can tell ""compat"" is some component of NLTK, TextBlob is built on. Strangely, I've imported TextBlob in the past, so this must be something recent. I've updated both TextBlob and NLTK to their most recent versions, so that's not what the problem is.</p>

<p>Similar questions such as <a href=""https://stackoverflow.com/questions/35100967/nltk-cant-using-importerror-cannot-import-name-compat"">this one</a> remain basically unresolved, since the recommendation is basically ""don't import all of NLTK"".</p>

<p>Edit: My code is as follows</p>

<p><code>from textblob import TextBlob</code></p>
","5472184","","-1","","2017-05-23 12:09:30","2016-06-01 19:11:35","When I try to import TextBlob I get ""ImportError: cannot import name compat"". What does this mean? How can I fix it?","<python><nlp><nltk><importerror><textblob>","1","2","","","","CC BY-SA 3.0"
"24575100","1","","","2014-07-04 13:07:28","","4","17487","<p>I have followed the instruction in <a href=""https://stackoverflow.com/questions/20562768/trouble-installing-textblob-for-python"">Trouble installing TextBlob for Python</a> for TextBlob installation in the Windows 7.
It got installed but when I go to Python Idle and type <code>import TextBlob</code> it says</p>

<blockquote>
  <p>No module named TextBlob</p>
</blockquote>

<p>How to solve this problem?</p>

<p>Or can I directly place the libraries associated with the package in the Python Lib folder and try to import it in the program? If it is advisable please tell the procedure to do that.
Will it work?</p>

<p>Any help will be highly appreciated. </p>
","1791170","","-1","","2017-05-23 12:00:16","2019-12-04 21:23:25","TextBlob installation in windows","<python><windows><python-2.7><textblob>","5","2","1","","","CC BY-SA 3.0"
"64252876","1","","","2020-10-07 21:57:32","","1","293","<p>Many people use text blob for sentiment analysis on text. I am sure that I am missing something in understanding the approach and how to use it, but there is something that does not work at all with the results I am getting from my analysis.</p>
<p>This is an example of data that I have:</p>
<pre><code>Top                                                     Text                                                   label    sentiment   polarity
51  CVD-Grown Carbon Nanotube Branches on Black Si...   silicon-carbon nanotube (bSi-CNT) hybrid struc...         -1    (-0.16666666666666666, 0.43333333333333335) -0.166667
69  Navy postpones its largest-ever Milan exercise...   Navy on Tuesday postponed a multi-nation mega ...           -1  (-0.125, 0.375) -0.125000
81 Malaysia rings alarm bell on fake Covid...   The United Nations International Children's Em...                   -1  (-0.5, 1.0) -0.500000
82  Poison Not Transmitted By Air...    it falls on the fabric remains 9 hours, so was...                   -1  (-0.2, 0.0) -0.200000
87  A WhatsApp rumor is spreading that is allegedl...   strict about unsourced speculation than other ...        -1 (-0.1, 0.1) -0.100000
90  Dumb Whatsapp Forwards - Page 2 - Cricket Web   as the ones that say like or share this pictur...          -1   (-0.375, 0.5)   -0.375000
144 malaysia | Unicef Malaysia rings alarm b... such messages claiming to be from us,” #Milan...                -1  (-0.5, 1.0) -0.500000
134 False and unverified claims are being...    Soccer was not issued by the U...                               -1  (-0.4000000000000001, 0.6)  -0.400000
123 Truth behind the Viral message about Co...  number of stories ever since the wave of misin...               -1  (-0.4, 0.7) -0.400000
166 In India, Fake WhatsApp Forwards on Coronaviru...   of confirmed cases of rises rapidl...                   -1  (-0.5, 1.0) -0.500000
</code></pre>
<p>I used the following algorithm:</p>
<pre><code>df['sentiment'] = df['Top'].apply(lambda Tweet: TextBlob(Tweet).sentiment)

df1=pd.DataFrame(df['sentiment'].tolist(), index= df.index)

df_new = df
df_new['polarity'] = df1['polarity']
df_new.polarity = df1.polarity.astype(float)
df_new['subjectivity'] = df1['subjectivity']
df_new.subjectivity = df1.polarity.astype(float)
# print(df_new)

conditionList = [
    df_new['polarity'] == 0,
    df_new['polarity'] &gt; 0,
    df_new['polarity'] &lt; 0]
choiceList = ['neutral', 'not_fake', 'fake']
df_new['label'] = np.select(conditionList, choiceList, default='no_label')
</code></pre>
<p>but as you can see the all these messages come from fact checking sources, so they are not fake.
How could I improve the results, maybe removing some specific words?
I can see that if the text contains false, unverified, viral, fake, it is tagged as negative and this makes results even worst.</p>
","","user12809368","","","","2020-10-18 13:07:46","Textblob and sentiment analysis: how to refine a dictionary?","<python><sentiment-analysis><textblob>","1","2","","","","CC BY-SA 4.0"
"37969425","1","38040580","","2016-06-22 13:23:27","","1","312","<p>So i am using textblob python library, but the performance is lacking.</p>

<p>I already serialize it and load it before the loop( using pickle ).</p>

<p>It currently takes ~ 0.1( for small training data ) and ~ 0.3 on 33'000 test data. I need to make it faster, is it even possible ?</p>

<h1><strong>Some code:</strong></h1>

<pre><code># Pass trainings before loop, so we can make performance a lot better
trained_text_classifiers = load_serialized_classifier_trainings(config[""ALL_CLASSIFICATORS""])

# Specify witch classifiers are used by witch classes
filter_classifiers = get_classifiers_by_resource_names(trained_text_classifiers, config[""FILTER_CLASSIFICATORS""])
signal_classifiers = get_classifiers_by_resource_names(trained_text_classifiers, config[""SIGNAL_CLASSIFICATORS""])

for (url, headers, body) in iter_warc_records(warc_file, **warc_filters):
    start_time = time.time()
    body_text = strip_html(body);

    # Check if url body passess filters, if yes, index, if no, ignore
    if Filter.is_valid(body_text, filter_classifiers):
        print ""Indexing"", url.url
        resp = indexer.index_document(body, body_text, signal_classifiers, url=url, headers=headers, links=bool(args.save_linkgraph_domains))
    else:
        print ""\n""
        print ""Filtered out"", url.url
        print ""\n""
        resp = 0
</code></pre>

<p>This is the loop witch performs check on each of the warc file's body and metadata.</p>

<p>there are 2 text classification checks here.</p>

<p>1) In Filter( very small training data ):</p>

<pre><code>if trained_text_classifiers.classify(body_text) == ""True"":
        return True
    else:
        return False
</code></pre>

<p>2) In index_document( 33'000 training data ):</p>

<pre><code>prob_dist = trained_text_classifier.prob_classify(body)
        prob_dist.max()

        # Return the propability of spam
        return round(prob_dist.prob(""spam""), 2)
</code></pre>

<p>The classify and prob_classify are the methods that take the tool on performance.</p>
","2831700","","2831700","","2016-06-22 13:46:25","2016-06-26 16:40:58","Text classification performance","<python><performance><machine-learning><textblob>","1","6","1","","","CC BY-SA 3.0"
"30429160","1","","","2015-05-24 22:50:33","","3","1319","<p>I'm using NaiveBayesClassifier function of TextBlob to classify some phrases and it works fine at the moment.</p>

<p>My problem is, I need explain how the function works with table test. How NaiveBayesClassifier of TextBlob works to classify one phrase and how to get the probabilistic number that the ""prob_classify(text)"" function generates?</p>

<p>I used <a href=""http://textblob.readthedocs.org/en/latest/classifiers.html"" rel=""nofollow"">http://textblob.readthedocs.org/en/latest/classifiers.html</a> to learning about this function.</p>
","4935032","","4935032","","2015-05-24 23:47:56","2015-05-24 23:47:56","How NaiveBayesClassifier of TextBlob works?","<python-3.x><nltk><naivebayes><textblob>","0","2","","","","CC BY-SA 3.0"
"46419683","1","46419980","","2017-09-26 06:57:13","","0","352","<p>I am getting ValueError: too many values to unpack error . here is code sample</p>

<pre><code>import numpy as np
import pandas as pd
from textblob.classifiers import NaiveBayesClassifier

sms_raw = pd.read_csv('text.csv')
# training dataset 70%
# test dataset 30 %
sms_raw['split'] = np.random.randn(sms_raw.shape[0], 1)
fltr = np.random.rand(len(sms_raw)) &lt;= 0.7
train = sms_raw[fltr]
test = sms_raw[~fltr]
cl = NaiveBayesClassifier(sms_raw)
</code></pre>
","2499914","","","","","2017-09-26 07:15:11","Getting error while training NaiveBayesClassifier with csv dataset","<python><pandas><naivebayes><textblob>","1","2","","","","CC BY-SA 3.0"
"62299922","1","62300065","","2020-06-10 09:15:44","","2","572","<p>For a current project, I am planning to perform a sentiment analysis for a number of word combinations with TextBlob.</p>

<p>When running the sentiment analysis line <code>polarity = common_words.sentiment.polarity</code> and calling the results with print(i, word, freq, polarity), I am receiving the following error message:</p>

<pre><code>polarity = common_words.sentiment.polarity
AttributeError: 'list' object has no attribute 'sentiment'
</code></pre>

<p>Is there any smart tweak to get this running? The corresponding code section looks like this:</p>

<pre><code>for i in ['Text_Pro','Text_Con','Text_Main']:
    common_words = get_top_n_trigram(df[i], 150)
    polarity = common_words.sentiment.polarity
    for word, freq in common_words:
        print(i, word, freq, polarity)
</code></pre>

<p>Edit: Please find below the full solution for the situation (in accordance with discussions with user leopardxpreload):</p>

<pre><code>for i in ['Text_Pro','Text_Con','Text_Main']:
    common_words = str(get_top_n_trigram(df[i], 150))
    polarity_list = str([TextBlob(i).sentiment.polarity for i in common_words])
    for element in polarity_list:
        print(i, element)
    for word, freq in common_words:
        print(i, word, freq)
</code></pre>
","10671481","","10671481","","2020-06-10 11:58:44","2020-06-10 11:58:44","NLP sentiment analysis: 'list' object has no attribute 'sentiment'","<python><nlp><sentiment-analysis><textblob>","1","1","","","","CC BY-SA 4.0"
"40359272","1","40363704","","2016-11-01 11:35:41","","0","605","<p><strong>Python version:</strong> 2.7</p>

<p><strong>Windows version</strong>: Windows 7 64-bit</p>

<p><strong>Language of the system</strong>: Russian</p>

<p>I have a problem which has not been solved in the internet yet.</p>

<p>Here is my code:</p>

<pre><code> import textblob

 text = ""I love people""

 text = TextBlob(text)
 print text.sentiment
</code></pre>

<p>I get the following error connected with the <code>nltk</code> method:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\Александр\Desktop\TextBlob.py"", line 1, in &lt;module&gt;
    import textblob
  File ""C:\Python27\lib\site-packages\textblob\__init__.py"", line 9, in &lt;module&gt;
   from .blob import TextBlob, Word, Sentence, Blobber, WordList
   File ""C:\Python27\lib\site-packages\textblob\blob.py"", line 28, in &lt;module&gt;
    import nltk
  File ""C:\Python27\lib\site-packages\nltk\__init__.py"", line 128, in &lt;module&gt;
    from nltk.chunk import *
  File ""C:\Python27\lib\site-packages\nltk\chunk\__init__.py"", line 155, in &lt;module&gt;
   from nltk.data import load
  File ""C:\Python27\lib\site-packages\nltk\data.py"", line 77, in &lt;module&gt;
    if 'APPENGINE_RUNTIME' not in os.environ and os.path.expanduser('~/') != '~/':
  File ""C:\Python27\lib\ntpath.py"", line 311, in expanduser
    return userhome + path[i:]
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc0 in position 9: ordinal not in range(128)
</code></pre>

<p>As far as I understood from answers from Google and Stackoverflow, the problem is related to language problems of <code>ntpath.py</code>. </p>

<p>I tried the following issues, and they did not work:</p>

<ol>
<li><p>Using <code>sys.setdefaultencoding('utf8')</code> <a href=""https://stackoverflow.com/questions/21129020/how-to-fix-unicodedecodeerror-ascii-codec-cant-decode-byte"">How to fix: &quot;UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte&quot;</a></p></li>
<li><p>Using <code>sys.setdefaultencoding('Cp1252')</code> It eliminated the error. However, the output of my programme disappeared too.</p></li>
<li><p>Using <code>import io</code>. <a href=""https://stackoverflow.com/questions/25493720/python-nltk-unicodedecodeerror-ascii-codec-cant-decode-byte"">Python (nltk) - UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte</a></p></li>
<li><p>Using <code>unicode().decode()</code> in <code>ntpath.py</code> (I do not remember a link where I found this solution).</p></li>
</ol>

<p><strong>UPD:</strong> I have found a solution.</p>

<p>I tried to insert this part into <code>ntpath.py</code>:</p>

<pre><code>reload(sys)
sys.setdefaultencoding('Cp1252')
</code></pre>

<p>So, here is the part of the code in this file:</p>

<pre><code>import os
import sys
import stat
import genericpath
import warnings

#another way
reload(sys)
sys.setdefaultencoding('Cp1252')
</code></pre>

<p>It works perfectly. If you have another language in your system settings, ""play"" with them and replace <code>Cp1252</code>.</p>
","6335499","","-1","","2017-05-23 12:16:40","2016-11-01 15:53:22","nltk UnicodeDecodeError - connected with the ntpath.py file","<python><python-2.7><unicode><nltk><textblob>","1","8","","","","CC BY-SA 3.0"
"53467684","1","","","2018-11-25 12:59:24","","0","222","<p>I am opening each text file and assigning it a label -- pos or neg as per for training NaiveBayes classifier.The data set contains about 12,000 txt files. I am using TextBlob library for sentiment analysis</p>

<pre><code>train = [('I dont like this movie','neg')]
path  = 'C://TextDemo//senti//aclImdb//train//neg//*.txt'
for f in glob.glob(path):
with open(f, ""r"", encoding=""UTF-8"") as read_file:
    for line in read_file:
        train.append(((line.replace(""&lt;br /&gt;"","""")),'pos'))

cl = NaiveBayesClassifier(train)
</code></pre>
","10646468","","","","","2018-11-25 12:59:24","Textblob giving memory error while using NaiveBayesAnalyzer on large dataset","<python><textblob>","0","2","","","","CC BY-SA 4.0"
"61871465","1","61872684","","2020-05-18 14:07:25","","0","104","<p>I have a dataframe generated through an sql query.</p>

<p>The dataframe contains only text,description more specifically both in English and in German.</p>

<p>I want to drop the German descriptions or to create a new dataframe only with the English ones.</p>

<p>The descriptions vary from 150-1000 words.</p>

<p>I am using TextBloB module as it show bellow:</p>

<pre><code>from textblob import TextBlob

test = reindexed_data.head()

for adv in test:

    x = TextBlob(adv)
    print(x.detect_language())
</code></pre>

<p>Where i get the desired values:</p>

<pre><code>pl
de
en
de
de
</code></pre>

<p>I am pretty novice in the pandas framework and i am not sure how to drop the rows <strong>or</strong> create a new dataframe with the English only descriptions.</p>

<p>I also noticed that for 5 rows the TextBlop requires around 6 seconds,there are 5000 rows so if there is a different library or approach please suggest me.</p>

<p>To sum up:How can i drop rows that are on German and if there another library that can make the process faster?</p>

<p>Any help is highly appreciated!
Thank you in advance.</p>

<p><strong>DataFrame Structure</strong></p>

<pre><code>    0    zzCHzz
    1    Über Campusjäger GmbH Als Recruiter verbindet ...
    2    ALPADIA Language Schools is specialized in org...
    3    IT-KONTOR ist einer der führenden IT-Dienstlei...
    4    Kennziffer 59476PL       Unser Kunde ist ein d...

Name: JobAdd, dtype: object
</code></pre>
","12170459","","12170459","","2020-05-18 14:51:27","2020-05-18 15:07:31","Drop a whole row of the dataframe if the text its not in English","<python><pandas><nlp><textblob>","1","2","","","","CC BY-SA 4.0"
"22152533","1","22175344","","2014-03-03 16:53:54","","3","890","<p>I am classifying documents as positive and negative labels using Naive Bayes model. It seems working fine for small balanced dataset size around 72 documents. But when I add more negative labeled documents, the classifier is predicting everything as negative. </p>

<p>I am splitting my dataset into 80% training and 20% test set. Adding more negatively labeled documents definitely makes the dataset skewed. Could it be the skewness that makes the classifier predict every test document as negative? I am using TextBlob/nltk implementation of Navive Bayes modle.</p>

<p>Any idea? </p>
","2161903","","","user2555451","2014-11-16 17:12:24","2014-11-16 17:12:24","Naive Bayes text classification using TextBlob: every instance predicted as negative when adding more sample size","<python><machine-learning><classification><nltk><textblob>","1","0","4","","","CC BY-SA 3.0"
"45889395","1","45889824","","2017-08-25 21:04:20","","0","1883","<p>I'm using <a href=""https://github.com/cjhutto/vaderSentiment"" rel=""nofollow noreferrer"">Vader</a> and <a href=""https://textblob.readthedocs.io/en/dev/"" rel=""nofollow noreferrer"">TextBlob</a> to analyse the sentiment of news headlines with mixed results: many headlines I would consider slightly negative are scored as neutral. Here are a few examples:</p>

<pre><code>Who wants to live in an artificially intelligent future?
# Vader: {'compound': 0.4588, 'pos': 0.273, 'neu': 0.727, 'neg': 0.0}
# TextBlob: Sentiment(polarity=0.2840909090909091, subjectivity=0.40625)

The internet and social media provide huge opportunities for the coming generation, but there’s a dark side from which it must be protected.
# Vader: {'compound': 0.743, 'pos': 0.278, 'neu': 0.722, 'neg': 0.0}
# TextBlob: Sentiment(polarity=0.09444444444444448, subjectivity=0.45555555555555555)

For three months I’ve lived without tech and now realise we need to question its ever-encroaching invasion – before we end up in bed with a sex robot.
# Vader {'compound': 0.0, 'pos': 0.0, 'neu': 1.0, 'neg': 0.0}
# TextBlob Sentiment(polarity=0.0, subjectivity=0.0)
</code></pre>

<p>I think the first sentence could be read either way, but the second two definitely have negative elements to them: ""there’s a dark side"" and ""its ever-encroaching invasion"", so I'm surprised to see Vader give both a negative sore of 0 and TextBlob to give a polarity of 0 or above.</p>

<p>Are these kind of texts just fundamentally difficult for sentiment analysis algorithms, or is there another approach I could consider?</p>

<p>The attraction of the libraries I mentioned is that I don't have to make my own classification dataset, but I might consider it if I was likely to get better results.</p>
","2950747","","","","","2017-08-25 21:49:19","How could I improve the accuracy of sentiment analysis of news headlines?","<python><sentiment-analysis><textblob><vader>","1","0","1","2017-08-25 22:35:30","","CC BY-SA 3.0"
"28894566","1","","","2015-03-06 08:10:16","","1","65","<p>I'm writing a Python command line utility that involves converting a string into a <a href=""http://textblob.readthedocs.org/en/dev/"" rel=""nofollow"">TextBlob</a>, which is part of a natural language processing module. Importing the module is very slow, ~300 ms on my system. For speediness, I created a memoized function that converts text to a TextBlob only the first time the function is called. Importantly, if I run my script over the same text twice, I want to avoid reimporting TextBlob and recomputing the blob, instead pulling it from the cache. That's all done and works fine, except, for some reason, the function is still very slow. In fact, it's as slow as it was before. I think this must be because the module is getting reimported even though the function is memoized and the import statement happens inside the memoized function.</p>

<p>The goal here is to fix the following code so that the memoized runs are as speedy as they ought to be, given that the result does not need to be recomputed.</p>

<p>Here's a minimal example of the core code:</p>

<pre><code>@memoize
def make_blob(text):
     import textblob
     return textblob.TextBlob(text)


if __name__ == '__main__':
    make_blob(""hello"")
</code></pre>

<p>And here's the memoization decorator:</p>

<pre><code>import os
import shelve
import functools
import inspect


def memoize(f):
    """"""Cache results of computations on disk in a directory called 'cache'.""""""
    path_of_this_file = os.path.dirname(os.path.realpath(__file__))
    cache_dirname = os.path.join(path_of_this_file, ""cache"")

    if not os.path.isdir(cache_dirname):
        os.mkdir(cache_dirname)

    cache_filename = f.__module__ + ""."" + f.__name__
    cachepath = os.path.join(cache_dirname, cache_filename)

    try:
        cache = shelve.open(cachepath, protocol=2)
    except:
        print 'Could not open cache file %s, maybe name collision' % cachepath
        cache = None

    @functools.wraps(f)
    def wrapped(*args, **kwargs):
        argdict = {}

        # handle instance methods
        if hasattr(f, '__self__'):
            args = args[1:]

        tempargdict = inspect.getcallargs(f, *args, **kwargs)

        for k, v in tempargdict.iteritems():
            argdict[k] = v

        key = str(hash(frozenset(argdict.items())))

        try:
            return cache[key]
        except KeyError:
            value = f(*args, **kwargs)
            cache[key] = value
            cache.sync()
            return value
        except TypeError:
            call_to = f.__module__ + '.' + f.__name__
            print ['Warning: could not disk cache call to ',
                   '%s; it probably has unhashable args'] % (call_to)
            return f(*args, **kwargs)

    return wrapped
</code></pre>

<p>And here's a demonstration that the memoization doesn't currently save any time:</p>

<pre><code>❯ time python test.py
python test.py  0.33s user 0.11s system 100% cpu 0.437 total

~/Desktop
❯ time python test.py
python test.py  0.33s user 0.11s system 100% cpu 0.436 total
</code></pre>

<p>This is happening even though the function is correctly being memoized (print statements put inside the memoized function only give output the first time the script is run).</p>

<p>I've put everything together into a GitHub <a href=""https://gist.github.com/suchow/e75d4efdb5c86dd18ed6"" rel=""nofollow"">Gist</a> in case it's helpful.</p>
","1816933","","","","","2015-03-06 08:34:18","Module seemingly reimported in memoized Python function","<python><performance><python-import><memoization><textblob>","1","7","1","","","CC BY-SA 3.0"
"28944279","1","28970322","","2015-03-09 14:20:17","","2","1462","<p>Hi I have created a python script using tweepy to stream tweets based on a keyword array into a mongodb collection based on the name of the element in the array that it was filtered by via pymongo ie (apple tweets saved to an apple collection). This script saves them in a JSON format and now I want to perform sentiment analysis on these saved tweets.</p>

<p>I have been reading a few tutorials on this and have decided to use the NaiveBayesClassifier built into the TextBlob module. I have created some train data and passed it into the classifier (just a normal text array with the sentiment at the end of each element) but I am unsure of how to apply this classifier to my already saved tweets. I think its like as below but this does not work as it throws an error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Philip/PycharmProjects/FinalYearProject/TrainingClassification.py"", line 25, in &lt;module&gt;
    cl = NaiveBayesClassifier(train)
  File ""C:\Python27\lib\site-packages\textblob\classifiers.py"", line 192, in __init__
    self.train_features = [(self.extract_features(d), c) for d, c in self.train_set]
ValueError: too many values to unpack
</code></pre>

<p>Here is my code so far:</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
import pymongo

train = [
    'I love this sandwich.', 'pos',
    'I feel very good about these beers.', 'pos',
    'This is my best work.', 'pos',
    'What an awesome view"", 'pos',
    'I do not like this restaurant', 'neg',
    'I am tired of this stuff.', 'neg',
    'I can't deal with this', 'neg',
    'He is my sworn enemy!', 'neg',
    'My boss is horrible.', 'neg'
]

cl = NaiveBayesClassifier(train)
conn = pymongo.MongoClient('localhost', 27017)
db = conn.TwitterDB

appleSentiment = cl.classify(db.Apple)
print (""Sentiment of Tweets about Apple is "" + appleSentiment)
</code></pre>

<p>Any help would be greatly appreciated.</p>
","2773052","","3100115","","2015-03-10 17:17:13","2017-12-03 01:10:49","Performing sentiment analysis on a mongodb collection containing JSON elements (tweets) in Python","<python-2.7><pymongo><textblob>","2","0","","","","CC BY-SA 3.0"
"61872385","1","","","2020-05-18 14:53:18","","0","429","<p>I`m trying to extract text from a pdf using Pypdf2 and translate with Textblob.</p>

<pre><code>import PyPDF2 as pdf
from docx import Document
from textblob import TextBlob

Arquivo = 'teste.pdf'
lgout = input('\nPara qual língua traduzir? ex: pt, en, es: ')
lgin = input('\nQual língua é o documento? ex: pt, en, es: ')

with open(Arquivo, mode='rb') as f:
    reader = pdf.PdfFileReader(f)
    npages = int(reader.numPages) -1

    ret = 0
    while ret &lt;= npages:
        page = reader.getPage(ret)
        pagext = str(page.extractText())
        blob = TextBlob(pagext)
        text_trans = (blob.translate(from_lang=lgin,to = lgout))
        doc = Document()
        doc.add_paragraph(str(text_trans))
        doc.save('Doc teste' + str(ret) + '.docx')
        ret +=1
    else:
        print(""Documento convertido"")
</code></pre>

<p>But when I run the script I get the erro</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/Pedrovhz/Desktop/Estudos/Python/Python Translator/tradutor_pdf.py"", line 18, in &lt;module&gt;
    text_trans = (blob.translate(from_lang=lginout,to = lgoutpu))
  File ""/anaconda3/lib/python3.7/site-packages/textblob/blob.py"", line 547, in translate
    from_lang=from_lang, to_lang=to))
  File ""/anaconda3/lib/python3.7/site-packages/textblob/translate.py"", line 61, in translate
    self._validate_translation(source, result)
  File ""/anaconda3/lib/python3.7/site-packages/textblob/translate.py"", line 85, in _validate_translation
    raise NotTranslated('Translation API returned the input string unchanged.')
textblob.exceptions.NotTranslated: Translation API returned the input string unchanged.
</code></pre>

<p>I don't know what I'm doing wrong, thx for the help!</p>
","8341300","","","","","2020-06-25 08:07:15","Problem to translate with textblob in python - textblob.exceptions","<python><anaconda><pypdf2><textblob>","1","0","","","","CC BY-SA 4.0"
"61916424","1","","","2020-05-20 15:03:59","","0","135","<p>For a sentiment analysis project, I'm trying to add stopwords while using TextBlob. I've tried to combine NTLK scripts as well with no luck. Here is my code prior to attempting to add the stopwords.</p>

<pre><code>import csv
from textblob import TextBlob

infile = 'File Path'

with open(infile, 'r') as csvfile:
    rows = csv.reader(csvfile)
    for row in rows:
        sentence = row[0]
        blob = TextBlob(sentence)        
        print (blob.sentiment_assessments)
</code></pre>
","13582913","","","","","2020-06-24 19:03:53","Adding Stopwords while Using TextBlob","<python><stop-words><textblob>","1","0","","","","CC BY-SA 4.0"
"55418941","1","55419177","","2019-03-29 13:51:49","","0","786","<p>I want to do spelling for text in Italian language using textblob, but I find just the code for English language. how can do it?
this is the code for English
    from textblob import TextBlob
    text = ""I am gonig to schol""
    text = TextBlob(text)
    print(text.correct())
    I am going to school</p>
","6930725","","","","","2019-03-29 14:04:54","Spelling text in italian language using textblob","<python><nlp><textblob>","1","0","","","","CC BY-SA 4.0"
"48671265","1","48679657","","2018-02-07 18:53:27","","0","664","<p>I have a data of amazon user reviews in JSON format which i am importing to pandas dataframe and using it to train a  model for text classification. I am trying to preprocess the user review text before training a model with that data. I have two questions here:</p>

<p>1) I have written a code to detect it's language using Textblob library in Python which is working fine but consuming a lot of time. Please tell me if there can be a optimal approach.I am using Textblob library in python and the code is:</p>

<pre><code>    from textblob import TextBlob
    def detect_language(text):
        if len(text)&gt;3:
            r=TextBlob(text)
            lang = r.detect_language()
            return lang
    dataset['language']=dataset.reviewText.apply(lambda x: detect_language(x))
</code></pre>

<p>2) I want to lemmatize my words before training the model. But as lemmatization in NLTK will work properly if the we have parts-of-speech tagged with the words, I am trying it as follows but getting some error:</p>

<pre><code>    from nltk import pos_tag
    from nltk.stem import WordNetLemmatizer
    text='my name is shubham'
    text=pos_tag(text.split())
    wl=WordNetLemmatizer()
    for i in text:
        print(wl.lemmatize(i))
</code></pre>

<p>Here i am getting pos tagged as:</p>

<pre><code>    [('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('shubham', 'JJ')]
</code></pre>

<p>and while doing lemmatization i am getting error as:</p>

<pre><code>    AttributeError: 'tuple' object has no attribute 'endswith'
</code></pre>

<p>Can you please suggest an efficient way to perform lemmatization.
Here is my sample data on which i am performing language detection and lemmatization:</p>

<pre><code>    overall reviewText
        5   Not much to write about here, but it does exac...
        5   The product does exactly as it should and is q...
        5   The primary job of this device is to block the...
        5   Nice windscreen protects my MXL mic and preven...
        5   This pop filter is great. It looks and perform...
</code></pre>
","8269816","","","","","2018-02-08 07:15:25","Optimizing Language Detection code and Lemmatization in Python","<python><pandas><nltk><lemmatization><textblob>","1","4","","","","CC BY-SA 3.0"
"52636398","1","","","2018-10-03 22:12:23","","0","225","<p>I want to check the spelling of an nltk corpus I created</p>

<p>When i pass raw text from an nltk corpus through textblob's spellcheck() I get the following error <code>Object has no len()</code> and the code section below. </p>

<p>Is there a way to get the list of incorrect spellings detected in my text without having them corrected first</p>

<pre><code>site-packages\textblob\_text.py in suggest(self, w)
   1386         if len(self) == 0:
   1387             self.load()
-&gt; 1388         if len(w) == 1:
   1389             return [(w, 1.0)] # I
   1390         if w in PUNCTUATION:

TypeError: object of type 'method' has no len()
</code></pre>
","2645252","","","","","2018-10-13 16:52:31","How to use python and TextBlob to get list of incorrectly spelled words in my nltk corpus","<python><spell-checking><textblob>","1","0","","","","CC BY-SA 4.0"
"37150205","1","37152048","","2016-05-10 22:50:26","","0","6561","<p><strong>Update:  Issue resolved.</strong> (see comment section below.)  Ultimately, the following two lines were required to transform my .csv to unicode and utilize TextBlob: row = [cell.decode('utf-8') for cell in row], and text = ' '.join(row).</p>

<p>Original question:
I am trying to use a Python library called Textblob to analyze text from a .csv file.  Error I receive when I call Textblob in my code is:</p>

<blockquote>
  <p>Traceback (most recent call last):   File
  ""C:\Users\Marcus\Documents\Blog\Python\Scripts\Brooks\textblob_sentiment.py"",
  line 30, in 
      blob = TextBlob(row)   File ""C:\Python27\lib\site-packages\textblob\blob.py"", line 344, in
  <strong>init</strong>
      'must be a string, not {0}'.format(type(text)))TypeError: The <code>text</code> argument passed to <code>__init__(text)</code> must be a string, not </p>
</blockquote>

<p>My code is:</p>

<pre><code>#from __future__ import division, unicode_literals #(This was recommended for Python 2.x, but didn't help in my case.)

#-*- coding: utf-8 -*-
import csv
from textblob import TextBlob
with open(u'items.csv', 'rb') as scrape_file:
reader = csv.reader(scrape_file, delimiter=',', quotechar='""')
for row in reader:
    row = [unicode(cell, 'utf-8') for cell in row]
    print row
    blob = TextBlob(row)
    print type(blob)
</code></pre>

<p>I have been working through UTF/unicode issues.  I'd originally had a different subject which I posed to <a href=""https://stackoverflow.com/questions/ask?title=Python%202.7%20and%20Textblob%20-%20TypeError%3A%20The%20%60text%60%20argument%20passed%20to%20%60__init__(text)%60%20must%20be%20a%20string%2C%20not%20%3Ctype%20%27list%27%3E"">this thread</a>. (Since my code and the error have changed, I'm posting to a new thread.) Print statements indicate that the variable ""row"" is of type=str, which I thought indicated that the reader object had been transformed as required by Textblob. The source .csv file is saved as UTF-8.  Can anyone provide feedback as to how I can get unblocked on this, and the flaws in my code?<br>
Thanks so much for the help.</p>
","5588205","","-1","","2017-05-23 12:07:59","2016-05-11 21:33:56","Python 2.7 and Textblob - TypeError: The `text` argument passed to `__init__(text)` must be a string, not <type 'list'>","<python-2.7><csv><textblob>","1","0","0","","","CC BY-SA 3.0"
"48691087","1","","","2018-02-08 17:09:31","","0","3649","<p>In Python 3, we can use re.compile(), nltk.tokenize() and TextBlob.words() to tokenize a given text. I think there may be other methods too, but I am unaware of them.</p>

<p>Which of these methods or other unmentioned methods tokenizes a given text the fastest?</p>

<p>Thank you in advance.</p>
","7616544","","","","","2018-02-09 04:44:26","Which is the fastest tokenization function in Python 3?","<regex><python-3.x><nltk><tokenize><textblob>","1","4","","","","CC BY-SA 3.0"
"65963173","1","","","2021-01-29 23:43:14","","0","34","<p>I am creating a text classification model but for some reason it is taking a very long time for my code to run. I have looked at it in detail and found that the model loads relatively quickly, but the first classification itself takes much longer. The model loads in only 3 seconds, and when I classify for the first time, it takes 13 seconds to classify the text, but then it takes only 0.01. I was wondering if anyone know a way to reduce the time it takes to classify. My code is listed below.</p>
<pre><code>iimport pickle
import time
from textblob import TextBlob
t1 = time.time()
cl = pickle.load( open( &quot;classifier.pickle&quot;, &quot;rb&quot; ) )
print(&quot;Loading took: &quot;,time.time()-t1)
t1 = time.time()
blob = TextBlob(&quot;while x is 1:&quot;, classifier=cl)
print(blob.classify())
print(&quot;Classifying took: &quot;,time.time()-t1)
t1 = time.time()
blob = TextBlob(&quot;x=4&quot;, classifier=cl)
print(blob.classify())
print(&quot;Classifying took: &quot;,time.time()-t1)
t1 = time.time()
blob = TextBlob(&quot;name = 'hello'&quot;, classifier=cl)
print(blob.classify())
print(&quot;Classifying took: &quot;,time.time()-t1)
</code></pre>
<p>This outputs:
<img src=""https://user-images.githubusercontent.com/48738128/106210789-60096700-617c-11eb-8987-b18f33963e8b.png"" alt=""image"" /></p>
<p>my model code:</p>
<pre><code>with open('model.json', 'r') as fp:
        cl = NaiveBayesClassifier(fp, format=&quot;json&quot;)
    object = cl
    file = open('classifier.pickle','wb') 
    pickle.dump(object,file)
</code></pre>
<p>As you can see, the first classifying attempt takes too long, so is there a way to not have it take that long?</p>
<p>Thanks!</p>
","13464486","","","","","2021-01-29 23:43:14","TextBlob takes too long to classify for the first time","<python><machine-learning><naivebayes><textblob>","0","0","","","","CC BY-SA 4.0"
"31354171","1","","","2015-07-11 06:27:52","","-1","114","<p>I am using Textblob for processing textual data. </p>

<p>My code is:</p>

<pre><code>from textblob import TextBlob
wiki = TextBlob(""Python is a high-level, general-purpose programming language."")
wiki.tags
</code></pre>

<p>I am getting output as:</p>

<pre><code>[(u'Python', u'NNP'), (u'is', u'VBZ'), (u'a', u'DT'), (u'high-level', u'JJ'), (u'general-purpose', u'JJ'), (u'programming', u'NN'), (u'language', u'NN')]
</code></pre>

<p>instead of:</p>

<pre><code>[('Python', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('high-level', 'JJ'), ('general-purpose', 'JJ'), ('programming', 'NN'), ('language', 'NN')]
</code></pre>

<p>What might be reason for the letter 'u' getting prepended to each word?</p>

<p>I'm working on Ubuntu 14.04.2 with Python 2.7.6 version.</p>
","3530596","","","","","2015-07-11 06:30:49","'u' is prepended to all text in TextBlob","<python><textblob>","1","4","","2015-07-11 06:36:15","","CC BY-SA 3.0"
"48647548","1","","","2018-02-06 16:26:13","","1","773","<p>I am using textblob for sentiment analysis and find this error. Help me to sort out this error.
<code>
from textblob import TextBlob</code></p>
","6644397","","","","","2018-02-06 16:26:13","ImportError: cannot import name 'TextBlob'","<python-3.x><textblob>","0","6","","","","CC BY-SA 3.0"
"39293768","1","","","2016-09-02 13:34:42","","1","88","<p>If you go to the directories containing the models of spaCy you find files like data\en-1.1.0\deps\model with 433.881 KB, data\en-1.1.0\ner\model with 35.698 KB, data\en-1.1.0\pos\model with 11.524 KB and in data\en-1.1.0\vocab two binary files lexemes.bin with 81.096 KB and vec.bin with 206.562 KB, and strings.json with 18.371 KB. And of course several smaller files summing up to 812.694 KB in total. When loaded (which takes some time) main memory consumption increases by about 2.4 GB. In TextBlob the whole directory for english is about 2.299 MB. Why is this enormous difference of almost three orders of magnitude? </p>
","5634841","","5634841","","2016-09-02 17:23:24","2016-09-02 17:23:24","Why are models of spaCy more than 500MB compared to the lexicons used in TextBlob which are about 1MB?","<model><textblob><spacy>","0","0","1","","","CC BY-SA 3.0"
"39704220","1","40695788","","2016-09-26 13:28:40","","9","6016","<p>i have searched the web about normalizing tf grades on cases when the documents' lengths are very different
(for example, having the documents lengths vary from 500 words to 2500 words)</p>

<p>the only normalizing i've found talk about dividing the term frequency in the length of the document, hence causing the length of the document to not have any meaning.</p>

<p>this method though is a really bad one for normalizing tf. if any, it causes the tf grades for each document to have a very large bias (unless all documents are constructed from pretty much the same dictionary, which is not the case when using tf-idf)</p>

<p>for example lets take 2 documents - one consisting of 100 unique words, and the other of 1000 unique words. each word in doc1 will have a tf of 0.01 while in doc2 each word will have a tf of 0.001</p>

<p>this causes tf-idf grades to automatically be bigger when matching words with doc1 than doc2</p>

<p>have anyone got any suggustion of a more suitable normalizing formula?</p>

<p>thank you</p>

<p><strong><em>edit</em></strong>
i also saw a method stating we should divide the term frequency with the maximum term frequency of the doc for each doc
this also isnt solving my problem</p>

<p>what i was thinking, is calculating the maximum term frequency from all the documents and then normalizing all of the terms by dividing each term frequency with the maximum</p>

<p>would love to know what you think</p>
","4472627","","3768871","","2017-11-18 17:35:36","2017-11-18 17:35:36","tf-idf documents of different length","<python><normalization><tf-idf><textblob>","1","0","2","","","CC BY-SA 3.0"
"50130571","1","","","2018-05-02 08:47:17","","1","90","<p><a href=""https://i.stack.imgur.com/nsOuy.png"" rel=""nofollow noreferrer"">I have added a picture of what the tagged data looks like</a></p>

<p>I am trying to analyse text from a list of FB posts I have in dataframe format. </p>

<p>I have managed to extract noun_prases and tags with the following codes: </p>

<pre><code>data_tags = data['text'].apply(lambda post: TextBlob(post).tags)
data_noun_phrases = data['text'].apply(lambda post: TextBlob(post).noun_phrases)
</code></pre>

<p>Now I need to: </p>

<ol>
<li><p>Extract the most common nouns and adjectives</p></li>
<li><p>extract the most common noun_phrases. </p></li>
</ol>

<p>Is there a way I can do this? </p>
","9729279","","9729279","","2018-05-02 08:52:33","2018-05-02 08:52:33","Textblob: filter words by tags","<python><textblob>","0","0","","","","CC BY-SA 4.0"
"48539378","1","48539541","","2018-01-31 10:02:46","","-2","33","<pre><code>my_stng = "" Einstein found out (apple) (fruit) which is (red)(green) in colour""
</code></pre>

<h3>Requirement:</h3>

<blockquote>
  <p>in the above string, count the number of times the parenthesis occurs
  and print the whole string that many times. if the count of
  parenthesis is 3, i need to print the above string 3 times.</p>
</blockquote>
","7993248","","1033581","","2019-03-13 15:26:15","2019-03-13 15:26:15","print a particular string based on the count of parenthesis occurs","<python><python-3.x><nlp><textblob>","1","4","","","","CC BY-SA 4.0"
"48585813","1","","","2018-02-02 15:24:30","","0","117","<p>I want to extract questions from question paper.I'm labeling each 
question as q and other sentences as i in dataset.</p>

<p>e.g.</p>

<pre><code>Why is this sector becoming important in India,q
Describe any five public facilities needed for the development of a country,q
CBSE 10th English 2017 Unsolved Paper,i
THE MARKS ARE MENTIONED ON EACH QUESTION,i
</code></pre>

<p>Is it suitable to use textblob naivebayes classifier for this or should I use sklearn with nltk?</p>
","9181906","","","","","2018-02-02 15:24:30","can I use textblob for classification besides sentimental analysis?","<python-2.7><machine-learning><nltk><textblob><python-textprocessing>","0","6","","","","CC BY-SA 3.0"
"31577397","1","","","2015-07-23 02:52:37","","1","407","<pre><code>from textblob import TextBlob
import nltk
array=(""i have a bunch of grapes"",""i like to eat apple"",""this is a laptop"")
array2=[]



for i in array:

    c=TextBlob(i)
    array2.append(c.words)

print array2
</code></pre>

<p>the result printed out will be:</p>

<blockquote>
  <blockquote>
    <p>[WordList(['i', 'have', 'a', 'bunch', 'of', 'grapes']), WordList(['i', 'like', 'to', 'eat', 'apple']), WordList(['this', 'is', 'a', 'laptop'])]</p>
  </blockquote>
</blockquote>

<p>how can i extract from the WordList so that my array2 will be printed as:</p>

<blockquote>
  <blockquote>
    <p>[['i', 'have', 'a', 'bunch', 'of', 'grapes'],['i', 'like', 'to', 'eat', 'apple'],[""this is a laptop""]]</p>
  </blockquote>
</blockquote>
","4891559","","","","","2020-01-16 19:28:48","Textblob word tokenization into array","<arrays><tokenize><textblob>","2","0","","","","CC BY-SA 3.0"
"32577420","1","","","2015-09-15 03:53:31","","4","1684","<p>I am using Python 2.7, Django 1.8 and my server is Apache on Linux Ubuntu. I have a JSON file with 23000 tweets in it. I want to classify the tweets according to predefined categories. But when I run the code, it throws <code>MissingCorpusError at /</code> and suggests:</p>

<p>To download the necessary data, simply run</p>

<pre><code>python -m textblob.download_corpora
</code></pre>

<p>I already have the latest corpora for TextBlob. Still, I get the error.</p>

<p>My views.py is as follows:</p>

<pre><code>def get_tweets(request):
    retweet = 0
    category = ''
    sentiment = ''
    tweets_data_path = STATIC_PATH+'/stream.json'
    tweets_data = []
    tweets_file = open(tweets_data_path, ""r"")
    for line in tweets_file:
        try:
            tweet = json.loads(line)
            tweets_data.append(tweet)
        except:
            continue
    subs = []
    for l in tweets_data:
        s = re.sub(""http[\w+]{0,4}://t.co/[\w]+"","""",l)
        subs.append(s)
    for t in subs:
        i = 0
        while i &lt; len(t):
            text = t[i]['tweet_text']
            senti = TextBlob(text)
            category = cl.classify(text)
            if senti.sentiment.polarity &gt; 0:
                sentimen = 'positive'
            elif senti.sentiment.polarity &lt; 0:
                sentimen = 'negative'
            else:
                sentimen = 'neutral'
            if text.startswith('RT'):
                retweet = 1
            else:
                retweet = 0
            twe = Tweet(text=text,category=category,
                sentiment=sentimen, retweet= retweet)
            twe.save()
            i = i+1
    return HttpResponse(""done"")
</code></pre>
","5315166","","445131","","2019-08-31 04:22:53","2019-08-31 04:22:53","missing corpus error in textblob using django","<python><django><textblob>","2","2","","","","CC BY-SA 4.0"
"31959668","1","31960321","","2015-08-12 08:16:15","","1","551","<p>I followed this <a href=""http://blog.christianperone.com/?p=1589"" rel=""nofollow"">tutorial</a> to search the relevant words in my documents. My code:</p>

<pre><code>&gt;&gt;&gt; for i, blob in enumerate(bloblist):
print i+1
scores = {word: tfidf(word, blob, bloblist) for word in blob.words}
sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)
for word, score in sorted_words[:10]:
    print(""\t{}, score {}"".format(word, round(score, 5)))

1
 k555ld-xx1014h, score 0.19706
 fuera, score 0.03111
 dentro, score 0.01258
 i5, score 0.0051
 1tb, score 0.00438
 sorprende, score 0.00358
 8gb, score 0.0031
 asus, score 0.00228
 ordenador, score 0.00171
 duro, score 0.00157 
2
 frentes, score 0.07007
 write, score 0.05733
 acceleration, score 0.05255
 aprovechando, score 0.05255
 . . . 
</code></pre>

<p>Here's my problem, I would like to export a data frame with the following information: index, 10 top words (separated with commas). Something that i can save with pandas dataframe. 
Example:</p>

<pre><code>TOPWORDS = pd.DataFrame(topwords.items(), columns=['ID', 'TAGS'])
</code></pre>

<p>Thank you all in advance. </p>
","4532964","","","","","2015-08-12 08:49:35","Exporting relevant words TF-IDF TextBlob python","<python><text-mining><tf-idf><textblob>","2","0","3","","","CC BY-SA 3.0"
"31578300","1","","","2015-07-23 04:41:52","","2","494","<p>For my project at work I am tasked with going through a bunch of user generated text, and in some of that text are reasons for cancelling their internet service, as well as how often that reason is occurring. It could be they are moving, just don't like it, or bad service, etc.</p>

<p>While this may not necessarily be a Python question, I am wondering if there is some way I can use NLTK or Textblob in some way to determine reasons for cancellation. I highly doubt there is anything automated for such a specialized task and I realize that I may have to build a neural net, but any suggestions on how to tackle this problem would be appreciated.</p>

<p>This is what I have thought about so far:
1) Use stemming and tokenization and tally up most frequent words. Easy method, not that accurate.
2) n-grams. Computationally intensive, but may hold some promise.
3) POS tagging and chunking, maybe find words which follow conjunctions such as ""because"".
4) Go through all text fields manually and keep a note of reasons for cancellation. Not efficient, defeats the whole purpose of finding some algorithm.
5) NN, have absolutely no idea, and I have no idea if it is feasible.</p>

<p>I would really appreciate any advice on this.</p>
","","user4797334","","","","2015-07-25 13:11:47","Use NLTK to find reasons within text","<python><python-2.7><neural-network><nltk><textblob>","2","0","1","","","CC BY-SA 3.0"
"24975499","1","24986059","","2014-07-26 20:58:31","","2","1746","<p>I'm using Python and nltk + Textblob for some text analysis. It's interesting that you can add a POS for wordnet to make your search for synonyms more specific, but unfortunately the tagging in both nltk and Textblob aren't ""compatible"" with the kind of input that wordnet expects for it's synset class. </p>

<p><strong>Example</strong>
Wordnet.synsets() requires that the POS you give it is one of n,v,a,r, like so</p>

<pre><code>wn.synsets(""dog"", POS=""n,v,a,r"")
</code></pre>

<p>But a standard POS tagging from upenn_treebank looks like </p>

<pre><code>JJ, VBD, VBZ, etc.
</code></pre>

<p>So I'm looking for a good way to convert between the two.</p>

<p>Does anyone know of a good way to make this conversion happen, besides brute force?</p>
","2625778","","","user2555451","2014-11-15 17:28:28","2021-08-09 21:16:55","Converting POS tags from TextBlob into Wordnet compatible inputs","<python><tags><nlp><nltk><textblob>","2","3","1","","","CC BY-SA 3.0"
"35780684","1","35781071","","2016-03-03 18:58:36","","0","1144","<p>I had the textblob library working fine for a while, but decided to install (using easy_install) an additional library (<a href=""http://stevenloria.com/tutorial-state-of-the-art-part-of-speech-tagging-in-textblob/"" rel=""nofollow"">page here</a>) claiming faster and more accurate tagging.</p>

<p>I couldn't get it working so I uninstalled it, but it seems to have messed with the tagging function in TextBlob. I've uninstalled and reinstalled both nltk and TextBlob numerous times with both pip and easy_install, and made sure they're up to date. </p>

<p>Here is an example of a simple script which generates the error:</p>

<pre><code>from textblob import TextBlob

blob = TextBlob(""This is a sentence"")
print repr(blob.tags)
</code></pre>

<p>and the error printed:</p>

<pre><code>    Traceback (most recent call last):
  File ""tesst.py"", line 5, in &lt;module&gt;
    print repr(blob.tags)
  File ""C:\Users\Emmet\Anaconda\lib\site-packages\textblob\decorators.py"", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File ""C:\Users\Emmet\Anaconda\lib\site-packages\textblob\blob.py"", line 445, in pos_tags
    for word, t in self.pos_tagger.tag(self.raw)
  File ""C:\Users\Emmet\Anaconda\lib\site-packages\textblob\decorators.py"", line 35, in decorated
    return func(*args, **kwargs)
  File ""C:\Users\Emmet\Anaconda\lib\site-packages\textblob\en\taggers.py"", line 34, in tag
    tagged = nltk.tag.pos_tag(text)
  File ""C:\Users\Emmet\Anaconda\lib\site-packages\nltk\tag\__init__.py"", line 110, in pos_tag
    tagger = PerceptronTagger()
  File ""C:\Users\Emmet\Anaconda\lib\site-packages\nltk\tag\perceptron.py"", line 141, in __init__
    self.load(AP_MODEL_LOC)
  File ""C:\Users\Emmet\Anaconda\lib\site-packages\nltk\tag\perceptron.py"", line 209, in load
    self.model.weights, self.tagdict, self.classes = load(loc)
  File ""C:\Users\Emmet\Anaconda\lib\site-packages\nltk\data.py"", line 801, in load
    opened_resource = _open(resource_url)
  File ""C:\Users\Emmet\Anaconda\lib\site-packages\nltk\data.py"", line 924, in _open
    return urlopen(resource_url)
  File ""C:\Users\Emmet\Anaconda\lib\urllib2.py"", line 154, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Users\Emmet\Anaconda\lib\urllib2.py"", line 431, in open
    response = self._open(req, data)
  File ""C:\Users\Emmet\Anaconda\lib\urllib2.py"", line 454, in _open
    'unknown_open', req)
  File ""C:\Users\Emmet\Anaconda\lib\urllib2.py"", line 409, in _call_chain
    result = func(*args)
  File ""C:\Users\Emmet\Anaconda\lib\urllib2.py"", line 1265, in unknown_open
    raise URLError('unknown url type: %s' % type)
urllib2.URLError: &lt;urlopen error unknown url type: c&gt;
</code></pre>

<p>You can see that the error actually mentions the perceptron tagger. Is there any way to more thoroughly remove any references there may be to the alternate tagger?</p>

<p>Also note that only the ""tags"" function has been affected.</p>
","4731782","","","","","2016-03-16 13:57:23","Error when using python textblob library tagger","<python><python-2.7><nltk><textblob>","2","0","1","","","CC BY-SA 3.0"
"57253827","1","","","2019-07-29 12:24:00","","0","45","<p>Team,</p>

<p>I installed python 3.4.1 version along with Anaconda. I am installed TextBlob, but it showing me error as,</p>

<blockquote>
  <p>ModuleNotFoundError                       Traceback (most recent call last)
   in ()
  ----> 1 import textBlob
  ModuleNotFoundError: No module named 'textBlob'</p>
</blockquote>

<p>Help appreciated for this !!</p>
","1374736","","","","","2019-07-29 12:24:00","Installation of TextBlob for Python 3.4.1","<python><python-3.x><anaconda><textblob>","0","2","","","","CC BY-SA 4.0"
"46867215","1","","","2017-10-21 19:30:36","","2","727","<p>I looked at <a href=""https://stackoverflow.com/questions/38219889/how-to-convert-result-of-tweepy-search-to-a-json-form"" title=""this answer"">this answer</a> and <a href=""https://stackoverflow.com/questions/40347743/typeerrorrepro-is-not-json-serializable-odoo-v8"">this one</a>, but could not find something relevant to my issue. Pretty sure it's a general JSON-related question - not specific to TextBlob.</p>

<p>I'm trying to use TextBlob to analyse text parsed from a JSON response to an API. Here's a response that just does not seem to pass:</p>

<pre><code>""""Haiti r D, yes, due* no _ t iii me:nor.y;? 
In The 
in$i $ - 
active"" E$icon-c arithms 
are applied to the rriinim ofverylar. database. 
""
</code></pre>

<p>This is just one of many. Does it have something to do with the characters in it - $ sign perhaps? </p>

<p>This is the error I keep getting:</p>

<pre><code>    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: TextBlob(""""Haiti r D, yes, due* no _ t iii me:nor.y;? 
In The 
in$i $ - 
active"" E$icon-c arithms 
are applied to the rriinim ofverylar. database. 
"") is not JSON serializable
</code></pre>

<p>Any push in the right direction will be appreciated. </p>
","4685767","","","","","2017-10-21 19:30:36","TypeError(repr(o) + "" is not JSON serializable"") for a string with TextBlob","<python><arrays><json><serialization><textblob>","0","0","","","","CC BY-SA 3.0"
"50260158","1","50266210","","2018-05-09 18:45:47","","0","441","<p>I am having a problem when running a Python script within my Laravel project. I do not get this problem when running the python script using virtualenv and without. I am using Win 10 64-bit.</p>

<p>When running the python script within Laravel using Symfony/Process, I am getting this error:</p>

<pre><code>""""""
The command ""C:\xampp\htdocs\projects\laravel-project\scripts\sentiment\env\Scripts\activate &amp;&amp; py C:\xampp\htdocs\projects\laravel-project\scripts\sentiment\main.py"" failed.\n ◀
\n
Exit Code: 1(General error)\n
\n
Working directory: C:\xampp\htdocs\projects\laravel-project\public\n
\n
Output:\n
================\n
\n
\n
Error Output:\n
================\n
Traceback (most recent call last):\r\n
  File ""C:\xampp\htdocs\projects\laravel-project\scripts\sentiment\main.py"", line 10, in &lt;module&gt;\r\n
    from textblob import TextBlob\r\n
  File ""C:\xampp\htdocs\projects\laravel-project\scripts\sentiment\env\lib\site-packages\textblob\__init__.py"", line 2, in &lt;module&gt;\r\n ◀
    from .blob import TextBlob, Word, Sentence, Blobber, WordList\r\n
  File ""C:\xampp\htdocs\projects\laravel-project\scripts\sentiment\env\lib\site-packages\textblob\blob.py"", line 28, in &lt;module&gt;\r\n
    import nltk\r\n
  File ""C:\xampp\htdocs\projects\laravel-project\scripts\sentiment\env\lib\site-packages\nltk\__init__.py"", line 160, in &lt;module&gt;\r\n
    from nltk.downloader import download, download_shell\r\n
  File ""C:\xampp\htdocs\projects\laravel-project\scripts\sentiment\env\lib\site-packages\nltk\downloader.py"", line 2237, in &lt;module&gt;\r\n ◀
    _downloader = Downloader()\r\n
  File ""C:\xampp\htdocs\projects\laravel-project\scripts\sentiment\env\lib\site-packages\nltk\downloader.py"", line 443, in __init__\r\n ◀
    self._download_dir = self.default_download_dir()\r\n
  File ""C:\xampp\htdocs\projects\laravel-project\scripts\sentiment\env\lib\site-packages\nltk\downloader.py"", line 954, in default_download_dir\r\n ◀
    raise ValueError(""Could not find a default download directory"")\r\n
ValueError: Could not find a default download directory\r\n
""""""
</code></pre>

<p>From what I understand is that nltk is a textblob dependency, however nltk/downloader.py is returning false at default_download_dir (but it should be true as I test the if condition in a python shell):</p>

<pre><code>    # On Windows, use %APPDATA%
    if sys.platform == 'win32' and 'APPDATA' in os.environ:
        homedir = os.environ['APPDATA']

    # Otherwise, install in the user's home directory.
    else:
        homedir = os.path.expanduser('~/')
        if homedir == '~/':
            raise ValueError(""Could not find a default download directory"")
</code></pre>

<p>This is how textblob is declared on my main.py</p>

<pre><code>from textblob import TextBlob
</code></pre>

<p>My other imports are not having any problem besides this specific package. I'm at a roadblock at this point so all help will be appreciated.</p>
","3960941","","","","","2018-05-10 05:40:55","Error from importing TextBlob","<python><python-3.x><laravel><nltk><textblob>","1","0","","","","CC BY-SA 4.0"
"26948326","1","","","2014-11-15 17:02:42","","1","198","<p>I've recently come across TextBlob, which seems like a very neat Natural Language Processing library. <a href=""http://textblob.readthedocs.org/en/dev/quickstart.html"" rel=""nofollow"">http://textblob.readthedocs.org/en/dev/quickstart.html</a></p>

<p>However, I am concerned because it seems to act as a regular Python string. I have a very large text file, and for example, calling <code>blob.correct()</code> for a very modest text amount takes a very long time. Any feedback on the scale of TextBlob or any alternatives for natural language parsing?</p>
","2649452","","","user2555451","2014-11-15 17:13:29","2017-02-22 22:47:46","Is TextBlob scalable?","<python><nlp><textblob>","0","0","","","","CC BY-SA 3.0"
"37634016","1","37634362","","2016-06-04 19:08:33","","4","2469","<p>I am using TextBlob to perform a sentiment analysis task. I have noticed that TextBlob is able to detect the negation in some cases while in other cases not. </p>

<p>Here are two simple examples</p>

<pre><code>&gt;&gt;&gt; from textblob.sentiments import PatternAnalyzer

&gt;&gt;&gt; sentiment_analyzer = PatternAnalyzer()
# example 1
&gt;&gt;&gt; sentiment_analyzer.analyze('This is good')
Sentiment(polarity=0.7, subjectivity=0.6000000000000001)

&gt;&gt;&gt; sentiment_analyzer.analyze('This is not good')
Sentiment(polarity=-0.35, subjectivity=0.6000000000000001)

# example 2
&gt;&gt;&gt; sentiment_analyzer.analyze('I am the best')
Sentiment(polarity=1.0, subjectivity=0.3)

&gt;&gt;&gt; sentiment_analyzer.analyze('I am not the best')  
Sentiment(polarity=1.0, subjectivity=0.3)
</code></pre>

<p>As you can see in the second example when using the adjective <code>best</code> the polarity is not changing. I suspect that has to do with the fact that the adjective <code>best</code> is a very strong indicator, but doesn't seem right because the negation should have reversed the polarity (in my understanding).</p>

<p>Can anyone explain a little bit what's going? Is textblob using some negation mechanism at all or is it just that the word <code>not</code> is adding negative sentiment to the sentence? In either case, why does the second example has exactly the same sentiment in both cases? Is there any suggestion about how to overcome such obstacles?</p>
","4068678","","4068678","","2016-06-04 19:23:40","2016-06-05 23:37:49","Why is not TextBlob using / detecting the negation?","<python><sentiment-analysis><textblob>","1","0","1","","","CC BY-SA 3.0"
"62083152","1","","","2020-05-29 09:50:01","","-2","64","<p>I have url of multiple websites in an xlsx file. I ran a loop on the xlsx file and passed the urls as an argument to the following sentiment analysis code.
Now the code is providing me with the analysis of the whole website (the websites only contain text and numbers) but the problem is that I want to run the analysis only on the paragraph that starts with ""Managerial function"". How may I do the same?
Here's my code:</p>

<pre><code>article = Article(j)
article.download()
article.parse()
#nltk.download('punkt')
article.nlp()
text = article.summary
obj = TextBlob(text)
sentiment = obj.sentiment.polarity
print(round(sentiment,2))
if sentiment==0:
    print(""neutral"")
elif sentiment&gt;0:
    print(""positive"")
elif sentiment&lt;0:
    print(""negative"")
</code></pre>
","10854214","","10854214","","2020-05-29 11:47:09","2020-05-29 12:15:17","Sentiment analysis of a certain paragraph from a website","<python><scope><sentiment-analysis><article><textblob>","1","4","","","","CC BY-SA 4.0"
"48860422","1","","","2018-02-19 06:13:16","","-1","446","<p>I am using textblob lib for classification using naive bayes , I have a train set and wants to check if I pass a word it should check in the train and classify accordingly and if the word is not present in the train it should not suggest any classification.</p>

<p>example : kartik is not in the train set , however it is classifying it as '1', and same for any other words which are not present in the training set.</p>

<p>is there any way if I suggest some word which is not in train it should not give '1'.</p>

<pre><code>from textblob import TextBlob
from textblob.classifiers import NaiveBayesClassifier


train = [
 ('System is working fine', '1'),
 ('Issue Resolved ', '1'),
 ('Working Fine ', '1'),
 ('running smoothly', '1'),
 (""server is working fine "", '1'),
 ('software installed properly', '1'),
 ('Ticket resolved ', '1'),
 (""Laptop is not working "", '-1'),
 ('laptop issue', '-1'),
 ('upgrade laptop', '-1'),
 ('software not working','-1'),
 ('fix the issue','-1'),
 ('WIFI is not working','-1'),
 ('server is down','-1'),
 ('system is not working','-1')


]

c1 = NaiveBayesClassifier(train)
c1.classify(""kartik"")
</code></pre>
","9218849","","","","","2018-02-19 06:51:26","Text Blob Naive Bayes classification","<python><naivebayes><textblob>","1","0","","","","CC BY-SA 3.0"
"66846209","1","66846408","","2021-03-28 21:05:44","","1","47","<p>I am trying to run the following code, but I have gotten an error that are too many values to unpack</p>
<p>The code is:</p>
<pre><code>import csv
import json
import pandas as pd

df = pd.read_csv(&quot;job/my_data_frame_test.csv&quot;, encoding=&quot;utf-8&quot;)

df.info()
print(df)
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>TEXT</th>
<th>text recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td>ABC</td>
<td>yes</td>
</tr>
<tr>
<td>DEF</td>
<td>no</td>
</tr>
</tbody>
</table>
</div>
<pre><code>from textblob import TextBlob
    
from textblob.classifiers import NaiveBayesClassifier
    
cl = NaiveBayesClassifier(df)
</code></pre>
<p>After running this code, I have the following error (in full)</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-7-3d683b8c482a&gt; in &lt;module&gt;
----&gt; 1 cl = NaiveBayesClassifier(df)

/usr/local/lib/python3.8/dist-packages/textblob/classifiers.py in __init__(self, train_set, feature_extractor, format, **kwargs)
    203     def __init__(self, train_set,
    204                  feature_extractor=basic_extractor, format=None, **kwargs):
--&gt; 205         super(NLTKClassifier, self).__init__(train_set, feature_extractor, format, **kwargs)
    206         self.train_features = [(self.extract_features(d), c) for d, c in self.train_set]
    207 

/usr/local/lib/python3.8/dist-packages/textblob/classifiers.py in __init__(self, train_set, feature_extractor, format, **kwargs)
    137         else:  # train_set is a list of tuples
    138             self.train_set = train_set
--&gt; 139         self._word_set = _get_words_from_dataset(self.train_set)  # Keep a hidden set of unique words.
    140         self.train_features = None
    141 

/usr/local/lib/python3.8/dist-packages/textblob/classifiers.py in _get_words_from_dataset(dataset)
     61             return words
     62     all_words = chain.from_iterable(tokenize(words) for words, _ in dataset)
---&gt; 63     return set(all_words)
     64 
     65 def _get_document_tokens(document):

/usr/local/lib/python3.8/dist-packages/textblob/classifiers.py in &lt;genexpr&gt;(.0)
     60         else:
     61             return words
---&gt; 62     all_words = chain.from_iterable(tokenize(words) for words, _ in dataset)
     63     return set(all_words)
     64 

ValueError: too many values to unpack (expected 2)
</code></pre>
","14612064","","","","","2021-03-28 21:38:49","TextBlob error: too many values to unpack","<python><pandas><dataframe><text-mining><textblob>","1","0","","","","CC BY-SA 4.0"
"62406660","1","62407459","","2020-06-16 10:46:05","","2","98","<p>I am getting the following error while translating a column from spanish to English:</p>

<pre><code>JSONDecodeError: Expecting value: line 1 column 1 (char 0)
</code></pre>

<p>My data frame looks like the following:</p>

<pre><code>case_id      es                                             fr
1234         -                                              -
2345         Hola como estas? Encantada de conocerte        comment vas-tu aujourd'hui     

3456         Hola como estas? Encantada de conocerte        -
123321       -                                              comment vas-tu aujourd'hui
</code></pre>

<p>'-' is something that shows that there are no comments. My data frame has a blank strings as well apart from comments so I have replaced the blanks with a '-'</p>

<p>I am using the following code:</p>

<pre><code>import googletrans
from googletrans import Translator
translator = Translator()
df['es_en'] = df['es'].apply(lambda x: translator.translate(x, src='es',dest='en').text)
df['fr_en'] = df['fr'].apply(lambda x: translator.translate(x, src='fr',dest='en').text)
</code></pre>

<p>What is wrong here? Why I am getting this error? </p>
","12226377","","12226377","","2020-06-16 12:19:02","2020-06-16 12:24:56","JSONDecodeError: Expecting value: line 1 column 1 (char 0) while translating text","<python-3.x><neural-network><google-translate><translate><textblob>","1","0","","","","CC BY-SA 4.0"
"65053768","1","","","2020-11-28 19:39:20","","1","40","<p>I am working on my thesis for economics and I am trying to scrape tweets between two dates for a list of users. Unfortunately, my program, which works fine for a single user breaks and throws this error when I try to loop it for the followers of an influencer. Anyone have suggestions?</p>
<p>Also once I get that fixed I will need to sort between two dates (I was just going to download a massive amount and then sort later using SPSS, but there must be a better way). Does anyone know a way to do this I tried this: <a href=""https://stackoverflow.com/questions/49731259/tweepy-get-tweets-between-two-dates"">tweepy get tweets between two dates</a>
but it didn't work and gave me super irregular results. Also if anyone knows how to make this not trip rate limits that would be great because I think that will be the next problem. :)</p>
<p>Sorry if the code is a little messy it is my first time coding.</p>
<p><strong>The error (I am working in spyder so its a bit long):</strong></p>
<p>Traceback (most recent call last):</p>
<p>File &quot;C:\Users\XPS.ipython\OG + BUILD UP FROM SCRACH.py&quot;, line 91, in 
extract_followers(user)</p>
<p>File &quot;C:\Users\XPS.ipython\OG + BUILD UP FROM SCRACH.py&quot;, line 66, in extract_followers
posts = api.user_timeline(screen_name = user, count = 100, language = &quot;en&quot;, tweet_mode=&quot;extended&quot;, include_rts = True)</p>
<p>File &quot;C:\Users\XPS\Python\lib\site-packages\tweepy\binder.py&quot;, line 252, in _call
return method.execute()</p>
<p>File &quot;C:\Users\XPS\Python\lib\site-packages\tweepy\binder.py&quot;, line 238, in execute
result = self.parser.parse(self, resp.text, return_cursors=self.return_cursors)</p>
<p>File &quot;C:\Users\XPS\Python\lib\site-packages\tweepy\parsers.py&quot;, line 98, in parse
result = model.parse_list(method.api, json)</p>
<p>File &quot;C:\Users\XPS\Python\lib\site-packages\tweepy\models.py&quot;, line 75, in parse_list
results.append(cls.parse(api, obj))</p>
<p><strong>File &quot;C:\Users\XPS\Python\lib\site-packages\tweepy\models.py&quot;, line 89, in parse
<strong>for k, v in json.items():</strong>
<strong>AttributeError: 'str' object has no attribute 'items'</strong></strong></p>
<p><strong>My Code</strong></p>
<h1>Import the libraries</h1>
<pre><code>import tweepy
from textblob import TextBlob 
import pandas as pd
import re
import matplotlib.pyplot as plt
import csv


plt.style.use('fivethirtyeight')
</code></pre>
<h1>Twitter API Credentials</h1>
<pre><code>consumerkey =  ('a')
consumersecret = ('a')
bearer = ('a')
token = ('a')
tokensecret = ('a')
</code></pre>
<h1>Create the authentication object</h1>
<pre><code>authenticate = tweepy.OAuthHandler(consumerkey, consumersecret)
</code></pre>
<p>#Set the access token</p>
<pre><code>authenticate.set_access_token(token, tokensecret)
</code></pre>
<p>#create the API object while passing in the auth info</p>
<pre><code>api = tweepy.API(authenticate, wait_on_rate_limit= True, wait_on_rate_limit_notify=True)
</code></pre>
<h1>Create a function to clean the tweets</h1>
<pre><code>def cleanTxt(text):
 text = re.sub('@[A-Za-z0–9]+', '', text) #Removing @mentions
 text = re.sub('#', '', text) # Removing '#' hash tag
 text = re.sub('RT[\s]+', '', text) # Removing RT
 text = re.sub('https?:\/\/\S+', '', text) # Removing hyperlink
 text = re.sub('https?:\/\/\S+', '', text) # Removing hyperlink

 return text
</code></pre>
<h1>Create a function to get the subjectivity</h1>
<pre><code>def getSubjectivity(text):
   return TextBlob(text).sentiment.subjectivity
</code></pre>
<h1>Create a function to get the polarity (how positive or negative the txt is)</h1>
<pre><code>def getPolarity(text):
   return  TextBlob(text).sentiment.polarity
</code></pre>
<p>#list of followers</p>
<pre><code>name_list = ['_prashantnair','urxnlc', 'Gurmeet1018', 'arpit8691yahooc', 'bnirmaljain', 'anoldschoolboy', 'rrpatange']
</code></pre>
<p>In some versions I just call an excel - The full list is a few thousand per influencer</p>
<h1>Create function to extract 100 tweets from the influencer with dates</h1>
<pre><code>def extract_followers (user):
    results = []
    posts = api.user_timeline(screen_name = user, count = 100, language = &quot;en&quot;, tweet_mode=&quot;extended&quot;, include_rts = True)
    for tweet in posts:
            data = (
            tweet.full_text,
            tweet.created_at,
            tweet.user.screen_name)
            results.append(data)
        
        
    cols = &quot;Tweets Date screen_name&quot;.split()
    global df
    df = pd.DataFrame(results, columns=cols)
    
    print(&quot;df original&quot;)
    print (df)
    
    for tweet in posts:
            cleaned_text = cleanTxt(tweet.full_text)
            with open('influencer.csv', 'a', newline= '') as f:    
                worksheet = csv.writer(f)
                worksheet.writerow([str(tweet.user.screen_name), str(tweet.created_at), str(getSubjectivity(cleaned_text)), str(getPolarity(cleaned_text))]) 
                print(&quot;Tweet Added&quot;)
</code></pre>
<h1>Call extract tweets function</h1>
<pre><code>for user in name_list: 
    extract_followers(user)
</code></pre>
<h1>Clean the tweets</h1>
<pre><code>df['Tweets'] = df['Tweets'].apply(cleanTxt)
</code></pre>
<h1>Show the cleaned tweets</h1>
<pre><code>print('df cleaned')
print (df)
</code></pre>
<h1>Create two new columns 'Subjectivity' &amp; 'Polarity'</h1>
<pre><code>df['Subjectivity'] = df['Tweets'].apply(getSubjectivity)
df['Polarity'] = df['Tweets'].apply(getPolarity)
</code></pre>
<h1>Show the new dataframe with columns 'Subjectivity' &amp; 'Polarity'</h1>
<pre><code>print (&quot;df with subjectivity&quot;)
print (df)
</code></pre>
","14726118","","","","","2020-11-28 19:39:20","Why do I receive the error message when downloading a user timeline using Tweepy","<python><twitter><tweepy><textblob><twitterapi-python>","0","0","","","","CC BY-SA 4.0"
"20827741","1","20827919","","2013-12-29 17:00:44","","22","36134","<p>I am training the <code>NaiveBayesClassifier</code> in Python using sentences, and it gives me the error below. I do not understand what the error might be, and any help would be good. </p>

<p>I have tried many other input formats, but the error remains. The code given below:</p>

<pre><code>from text.classifiers import NaiveBayesClassifier
from text.blob import TextBlob
train = [('I love this sandwich.', 'pos'),
         ('This is an amazing place!', 'pos'),
         ('I feel very good about these beers.', 'pos'),
         ('This is my best work.', 'pos'),
         (""What an awesome view"", 'pos'),
         ('I do not like this restaurant', 'neg'),
         ('I am tired of this stuff.', 'neg'),
         (""I can't deal with this"", 'neg'),
         ('He is my sworn enemy!', 'neg'),
         ('My boss is horrible.', 'neg') ]

test = [('The beer was good.', 'pos'),
        ('I do not enjoy my job', 'neg'),
        (""I ain't feeling dandy today."", 'neg'),
        (""I feel amazing!"", 'pos'),
        ('Gary is a friend of mine.', 'pos'),
        (""I can't believe I'm doing this."", 'neg') ]
classifier = nltk.NaiveBayesClassifier.train(train)
</code></pre>

<p>I am including the traceback below.</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\5460\Desktop\train01.py"", line 15, in &lt;module&gt;
    all_words = set(word.lower() for passage in train for word in word_tokenize(passage[0]))
  File ""C:\Users\5460\Desktop\train01.py"", line 15, in &lt;genexpr&gt;
    all_words = set(word.lower() for passage in train for word in word_tokenize(passage[0]))
  File ""C:\Python27\lib\site-packages\nltk\tokenize\__init__.py"", line 87, in word_tokenize
    return _word_tokenize(text)
  File ""C:\Python27\lib\site-packages\nltk\tokenize\treebank.py"", line 67, in tokenize
    text = re.sub(r'^\""', r'``', text)
  File ""C:\Python27\lib\re.py"", line 151, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or buffer
</code></pre>
","2916950","","","user2555451","2014-11-15 17:59:45","2019-04-24 17:52:31","nltk NaiveBayesClassifier training for sentiment analysis","<python><nlp><nltk><sentiment-analysis><textblob>","3","0","24","","","CC BY-SA 3.0"
"27500159","1","","","2014-12-16 08:09:26","","5","240","<p>I have AWS linux running with apache and django. I have installed textbob extension and it works well through the python shell. However, when I use it in views.py it always gives me MissingCorpusError. </p>

<p>I have downloaded the corpus already (<strong>python -m textblob.download_corpora</strong>) and the module works fine from the python shell. What can be the problem? Have been stuck on this thing since a couple of days now.</p>

<p>Below is the error message I get,</p>

<blockquote>
  <p>Looks like you are missing some required data for this feature.To
  download the necessary data, simply run    python -m
  textblob.download_corporaor use the NLTK downloader to download the
  missing data: <a href=""http://nltk.org/data.htmlIf"" rel=""noreferrer"">http://nltk.org/data.htmlIf</a> this doesn't fix the
  problem, file an issue at <a href=""https://github.com/sloria/TextBlob/issues"" rel=""noreferrer"">https://github.com/sloria/TextBlob/issues</a>.</p>
</blockquote>
","3996787","","","","","2014-12-16 08:09:26","textblob on django + apache on aws gives MissingCorpusError","<python><django><apache><nltk><textblob>","0","0","","","","CC BY-SA 3.0"
"49377319","1","","","2018-03-20 06:06:40","","1","1651","<p>I had been trying to implement a function that would correct spellings in multiple documents. I tried two methods viz <code>TextBlob</code> and <code>autocorrect</code>.</p>

<p><strong>Using TextBlob</strong> </p>

<pre><code>def spell_correct(word_list):
    try:
        corrected = []
        for word in word_list:
            w = Word(word)
            corrected.append(w.correct())
        return corrected
    except UnicodeDecodeError:
        return None
</code></pre>

<p><strong>Using autocorrect</strong></p>

<pre><code>def spell_correct(word_list):
     try:
         corrected = []
         for word in word_list:
             corrected.append(spell(word))
         return corrected
     except UnicodeDecodeError:
         return None
</code></pre>

<p>Both of them work quite well on single list of words provided as input. However, when I work with multiple documents in a pandas <code>DataFrame</code> of 13k rows, it is taking so much time that I usually <code>KeyboardInterrupt</code>. Am I impatient or is there a faster method to spell correct?</p>

<p><strong>Update</strong>
This is how I apply these functions on multiple documents in a pandas <code>DataFrame</code>,</p>

<pre><code>df['corrected_words'] = df.words.apply(lambda x: spell_correct(x))
</code></pre>
","1849007","","1849007","","2018-03-21 06:04:55","2020-04-17 10:18:45","Spell Correction using TextBlob, autocorrect","<python><pandas><nlp><textblob><autocorrect>","1","7","1","","","CC BY-SA 3.0"
"48287316","1","48288384","","2018-01-16 18:03:36","","1","2245","<p>I used sentiment analysis on a <code>CSV</code> file and the output prints the polarity and subjectivity of a sentence. How can I get the output in a table format along with the classification of the sentence as positive or negative or neutral added to it?</p>

<pre><code>    import csv
    from textblob import TextBlob

    infile = 'sentence.csv'

    with open(infile, 'r') as csvfile:
        rows = csv.reader(csvfile)
    for row in rows:
        sentence = row[0]
        blob = TextBlob(sentence)
        print (sentence)
        print (blob.sentiment.polarity, blob.sentiment.subjectivity)
</code></pre>

<p>the output for my code is :</p>

<pre><code>    i am very happy
    1.0 1.0
    its very sad
    -0.65 1.0
    they are bad
    -0.6999999999999998 0.6666666666666666
    hate the life
    -0.8 0.9
    she is so fantastic
    0.4 0.9
</code></pre>

<p>Thanks in advance.</p>
","9222111","","6388994","","2018-01-16 19:58:54","2018-01-16 19:58:54","Sentiment analysis on a csv file using textblob","<python><csv><sentiment-analysis><textblob>","1","3","","","","CC BY-SA 3.0"
"49361038","1","","","2018-03-19 10:45:51","","0","1115","<pre><code>tweet = textblob(tweet)

TypeError: 'module' object is not callable
</code></pre>

<p>I have this problem while trying to run a sentiment analysis script. I have installed textblob with the following commands:</p>

<pre><code>$ pip install -U textblob
$ python -m textblob.download_corpora
</code></pre>

<p>the code is the following:</p>

<pre><code>import json
import csv
from textblob import TextBlob


#set the input and outputing file
input_file= ""tweets.json""
output_file= ""results.csv""

#store all json data
tweets_novartis = []

with open (input_file) as input_novartis:
    for line in input_novartis:
        tweets_novartis.append(json.loads(line))

#open output file to store the results
with open(output_file, ""w"") as output_novartis:
    writer = csv.writer(output_novartis)

    #iterate through all the tweets
    for tweets_novartis in tweets_novartis:
        tweet = tweets_novartis[""full_text""]

        #TextBlob to calculate sentiment
tweet = Textblob(tweet)
tweet = tweet.replace(""\n"" , "" "")
tweet = tweet.replace(""\r"" , "" "")

sentiment = [[tweet.sentiment.polarity]]
writer.writerows(sentiment)
</code></pre>
","9516031","","9516031","","2018-03-19 11:07:57","2019-09-10 04:44:09","tweet = textblob(tweet) TypeError: 'module' object is not callable","<python><sentiment-analysis><textblob>","3","1","","","","CC BY-SA 3.0"
"62433131","1","","","2020-06-17 15:52:41","","0","267","<p>I am using the sentiment analysis tool in the TextBlob package on Python 3.7. I am familiar with it and <a href=""https://planspace.org/20150607-textblob_sentiment/"" rel=""nofollow noreferrer"">understand that it works on a basis of 3 values</a>: polarity, subjectivity, and intensity. Polarity and subjectivity are standard output from <code>TextBlob('string').sentiment</code>, however <code>TextBlob('string').intensity</code> was sadly not successful. Any clues on this? </p>
","11311537","","","","","2020-06-19 12:49:18","How to find intensity from TextBlob sentiment analysis","<python><python-3.x><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 4.0"
"36074867","1","","","2016-03-18 02:03:14","","0","1529","<pre><code>Traceback (most recent call last):
  File ""F:/intership2/jsonTest/blobtest.py"", line 4, in &lt;module&gt;
    print blob.tags
  File ""C:\Anaconda\lib\site-packages\textblob\decorators.py"", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File ""C:\Anaconda\lib\site-packages\textblob\blob.py"", line 445, in pos_tags
    for word, t in self.pos_tagger.tag(self.raw)
  File ""C:\Anaconda\lib\site-packages\textblob\decorators.py"", line 35, in decorated
    return func(*args, **kwargs)
  File ""C:\Anaconda\lib\site-packages\textblob\en\taggers.py"", line 34, in tag
    tagged = nltk.tag.pos_tag(text)
  File ""C:\Anaconda\lib\site-packages\nltk\tag\__init__.py"", line 110, in pos_tag
    tagger = PerceptronTagger()
  File ""C:\Anaconda\lib\site-packages\nltk\tag\perceptron.py"", line 141, in __init__
    self.load(AP_MODEL_LOC)
  File ""C:\Anaconda\lib\site-packages\nltk\tag\perceptron.py"", line 209, in load
    self.model.weights, self.tagdict, self.classes = load(loc)
  File ""C:\Anaconda\lib\site-packages\nltk\data.py"", line 801, in load
    opened_resource = _open(resource_url)
  File ""C:\Anaconda\lib\site-packages\nltk\data.py"", line 924, in _open
    return urlopen(resource_url)
  File ""C:\Anaconda\lib\urllib2.py"", line 154, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Anaconda\lib\urllib2.py"", line 431, in open
    response = self._open(req, data)
  File ""C:\Anaconda\lib\urllib2.py"", line 454, in _open
    'unknown_open', req)
  File ""C:\Anaconda\lib\urllib2.py"", line 409, in _call_chain
    result = func(*args)
  File ""C:\Anaconda\lib\urllib2.py"", line 1265, in unknown_open
    raise URLError('unknown url type: %s' % type)
urllib2.URLError: &lt;urlopen error unknown url type: &gt;
</code></pre>

<p>when I use textblob, I have the problem urllib2.URLError, how I can handle it.
   my code is as follows:
    from textblob import TextBlob
    text = 'The titular threat of The Blob has always struck me as the ultimate movie'
    blob = TextBlob(text)
    print blob.tags</p>
","6080158","","","","","2016-03-18 23:55:37","urllib2.URLError: <urlopen error unknown url type: c> textblob","<textblob>","1","0","","","","CC BY-SA 3.0"
"32953360","1","","","2015-10-05 16:26:59","","-2","201","<p>I need to build a classifier which will classify any tweet string into a specific category. I've around 15 or so categories. I've the training dataset with me. Any ideas how should I go ahead with this? Using Python or Java for this.</p>

<p>I've been looking into Textblob and Stanford NLP classifiers. I'm just not sure which one to pick to get better results.</p>
","3188761","","","","","2015-10-05 16:42:59","Build a text classifier","<python><nltk><stanford-nlp><textblob>","1","0","1","","","CC BY-SA 3.0"
"49434980","1","","","2018-03-22 17:30:19","","0","655","<p>I am working on Aspect Based Sentiment Analysis.In this project we collected data from twitter. After collecting data we performed text cleaning methods and create a corpus. After that we used this corpus to find the aspects using noun_phrases in python.It gives me the list of noun phrases. From this list i want to select only those aspects which contain only two words. How can i do that?</p>

<p>Here is my code and generated output:</p>

<pre><code>from textblob import Word
comments = TextBlob(' '.join(corpus))
comments.noun_phrases
cleaned = list()
for phrase in comments.noun_phrases:
    count = 0
    for w in phrase.split():
        # Count the number of small words and words without an English definition
        if len(w) &lt;= 2 or (not Word(w).definitions):
            count += 1
    # Only if the 'nonsensical' or short words DO NOT make up more than 40% (arbitrary) of the phrase add
    # it to the cleaned list, effectively pruning the ones not added.
    if count &lt; len(phrase.split())*0.4:
        cleaned.append(phrase)       
print(""After compactness pruning:\nFeature Size:"")
print(cleaned)
</code></pre>

<p>Output:
['worth free food k retweet pleas', 'specif waiter job', 'red blend', 'old idea suddenli', 'global focus', 'local issu lot', 'africa food', 'food truck', 'space avail netbal woman footbal amp squash', 'week world cup', 'minor sign confess', 'french fri coupl day', 'great stuff ban plastic straw serv local produc ta xe x xa b differ food home food school home', 'stale croissant', 'thing time', 'great time saver bc', 'clean chop alreadi', 'fake news unit alreadi', 'sure food amp cosmet', 'long food', 'dog china american', 'trade china till', 'warm color', 'yellow orang', 'fast food restaur', 'yellow orang', 'emerg food parcel', 'junk food label parti size', 'share water check systemsthink', 'earth food', 'care chihuahua yappi requir food sleep', 'new cloth', 'dose moron', 'afraid poor rise peopl', 'friend feed', 'wrong shit', 'good guy', 'good bad guy', 'food pension livelihood', 'food fur babi fun stay']</p>

<p>From this we want to select only those noun phrases which contain only two words such as 'red blend','food truck','stale croissant',etc. How can i do that?</p>
","9138062","","","","","2018-03-22 18:03:59","Aspect Based Sentiment Analysis using python","<python><text-mining><sentiment-analysis><textblob>","2","1","","","","CC BY-SA 3.0"
"49765834","1","","","2018-04-11 03:03:34","","-4","429","<p>I am trying to run sentiment analysis on a selection of a data set, but every time I do I get this error: <code>KeyError: 0</code></p>

<p>For reference, this is the code I am working with:</p>

<pre><code>OC = df[df[""text""].str.contains(""Obamacare"")]

from textblob import TextBlob
import re

def clean_tweet(tweet):
    return "" "".join(re.sub(""(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"", "" "", tweet).split())
def analize_sentiment(tweet):
    analysis = TextBlob(clean_tweet(tweet))
    if analysis.sentiment.polarity &gt; 0:
        return 1
    elif analysis.sentiment.polarity == 0:
        return 0
    else:
        return -1

df[""sentiment""] = np.array([ analize_sentiment(tweet) for tweet in df[""text""]])
pos_tweets = [tweet for index, tweet in enumerate(OC['text']) if OC['sentiment'][index] &gt; 0]
neu_tweets = [ tweet for index, tweet in enumerate(OC['text']) if OC['sentiment'][index] == 0]
neg_tweets = [ tweet for index, tweet in enumerate(OC['text']) if OC['sentiment'][index] &lt; 0]
</code></pre>

<p>It's after I try to run the <code>pos_tweets</code>, <code>neu_tweets</code>, <code>neg_tweets</code> that I keep getting <code>Key Error: 0</code></p>
","9628141","","8005315","","2018-04-11 03:46:04","2018-04-11 22:39:35","key error 0 sentiment analysis","<python><pandas><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 3.0"
"67273372","1","","","2021-04-26 20:27:25","","0","25","<p>I am working on a project for a data science class that scrapes popular news sites based on a user-defined search term and returns how much each site leans conservative/liberal. I've run into an issue using TextBlob where my NaiveBayesClassifier always returns 1.</p>
<p>My training set includes 28 articles labeled 'conservative', 26 labeled 'liberal'. After instantiating my classifier (cl), I run</p>
<pre><code>print(cl)
print(cl.show_informative_features(10))
</code></pre>
<p>which returns</p>
<p><a href=""https://i.stack.imgur.com/1f81g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1f81g.png"" alt=""enter image description here"" /></a></p>
<p>Since there is such a large bias in the use of the term &quot;restrictions&quot;, I use it as a simple test:</p>
<pre><code>print(cl.prob_classify('restrictions').prob('conservative')  # 1.0
</code></pre>
<p>To my surprise, the term is categorized as 'conservative' with a 100% probability! In fact, every word, phrase, article, etc. I try to classify returns with a 100% probability of being conservative.</p>
<p>Could anyone shed some light on what I'm doing wrong? There is a &quot;None&quot; returned by cl.show_informative_features - does this have any meaning?</p>
<p>EDIT:
I'm trying to make a smaller example, but for now my GitHub repo for this is at <a href=""https://github.com/jpbic/news_scraper"" rel=""nofollow noreferrer"">https://github.com/jpbic/news_scraper</a>. Just run &quot;text_analysis.py&quot;. The CSV data used to train the classifier is in &quot;data/news_scraper_data.csv&quot;.</p>
","1339382","","1339382","","2021-04-26 22:34:51","2021-04-26 22:34:51","TextBlob NaiveBayesClassifier Always Returns 1","<python><data-science><textblob>","0","3","","","","CC BY-SA 4.0"
"62745497","1","62745740","","2020-07-05 19:34:48","","0","93","<p>I retrieved a dataset from a news API in JSON format. I want to extract the news description from the JSON data.</p>
<p>This is my code:-</p>
<pre><code>import requests
import json
url = ('http://newsapi.org/v2/top-headlines?'
       'country=us&amp;'
       'apiKey=608bf565c67f4d99994c08d74db82f54')
response = requests.get(url)
di=response.json()
di = json.dumps(di)
for di['articles'] in di:
  print(article['title']) 
</code></pre>
<p>The dataset looks like this:-</p>
<pre><code>{'status': 'ok', 
 'totalResults': 38, 
 'articles': [
              {'source': 
                {'id': 'the-washington-post', 
                 'name': 'The Washington Post'}, 
               'author': 'Derek Hawkins, Marisa Iati', 
               'title': 'Coronavirus updates: Texas, Florida and Arizona officials say early reopenings fueled an explosion of cases - The Washington Post', 
               'description': 'Local officials in states with surging coronavirus cases issued dire warnings Sunday about the spread of infections, saying the virus was rapidly outpacing containment efforts.', 
               'url': 'https://www.washingtonpost.com/nation/2020/07/05/coronavirus-update-us/', 
               'urlToImage': 'https://www.washingtonpost.com/wp-apps/imrs.php?src=https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/K3UMAKF6OMI6VF6BNTYRN77CNQ.jpg&amp;w=1440', 
               'publishedAt': '2020-07-05T18:32:44Z', 
               'content': 'Here are some significant developments:\r\n&lt;ul&gt;&lt;li&gt;The rolling seven-day average for daily new cases in the United States reached a record high for the 27th day in a row, climbing to 48,606 on Sunday, … [+5333 chars]'}])
</code></pre>
<p>Please guide me with this!</p>
","11589463","","11606728","","2020-07-06 01:52:19","2020-07-06 02:28:06","How to retrieve data from a json","<json><python-3.x><api><data-analysis><textblob>","2","3","","","","CC BY-SA 4.0"
"63470743","1","63472071","","2020-08-18 14:25:23","","0","87","<p>so I want to use Textblob sentiment to calculate the sentiment of my data. But, before calculating the sentiment, I translated the data from Indonesian to English.</p>
<p>Here are my codes</p>
<pre><code>import pandas as pd
df = pd.read_csv('file.csv', encoding=&quot;utf-16&quot;)
from googletrans import Translator
translator = Translator()
df['english'] = df['Comment'].apply(translator.translate, src='id', dest='en')
#print(df)
#print(df['english'])
from textblob import TextBlob
def sentiment_calc(text):
    try:
        return TextBlob(text).sentiment
    except:
        return None
    
df['sentiment']=df['english'].apply(lambda text: TextBlob(text).sentiment)
print(df['sentiment'])
</code></pre>
<p>But then I got this error</p>
<pre><code>TypeError: The `text` argument passed to `__init__(text)` must be a string, not &lt;class 'googletrans.models.Translated'&gt;
</code></pre>
<p>Any Solution? By the way, the translation results were fine.</p>
","12873432","","","","","2020-08-18 15:42:14","Textblob sentiment error after using googletrans","<python><pandas><google-translate><textblob>","1","0","","","","CC BY-SA 4.0"
"50309786","1","","","2018-05-12 19:05:37","","1","248","<p>I want to use textblob for sentiment analysis for financial news. But when using
<code>TextBlob(""news text"", analyzer=NaiveBayesAnalyzer()))</code>. </p>

<p>I am very new to python, but according to some references, the above line uses a pretrained Naive Bayes Classifier trained on movie reviews corpus. There are many other corpus available at <a href=""http://www.nltk.org/nltk_data/"" rel=""nofollow noreferrer"">http://www.nltk.org/nltk_data/</a>. I feel may be movie reviews corpus may not be suitable for analyzing sentiment of financial news. Some words may have different polarity and weightage when used in different context. If I want to use Reuters corpus, what should I do? Is there any pretrained model already available or some other way I can use it as the above one uses movie reviews corpus?  </p>

<p>If no, how do I train one and how much time does it take? I am new to python and have no idea how much time/memory/processing power will it take.</p>
","6623277","","","","","2019-07-26 03:04:38","How to train Naive Bayes classifier in nltk on different corpus?","<python-3.x><nltk><sentiment-analysis><textblob>","0","1","","","","CC BY-SA 4.0"
"53669956","1","53670003","","2018-12-07 12:52:20","","0","95","<p>As shown in the <a href=""https://textblob.readthedocs.io/en/dev/quickstart.html"" rel=""nofollow noreferrer"">qickstart</a></p>

<p>i have a list of list of words to be searched programitically , But .word_counts['ekki'] (as shown in quickstart) . is giving me error </p>

<pre><code>&gt;&gt;&gt; import textblob
&gt;&gt;&gt; str = textblob.TextBlob(""hello im programmer"")
&gt;&gt;&gt; lis = [""hi"",""hello""]
&gt;&gt;&gt; str.word_counts[i for i in lis]
  File ""&lt;stdin&gt;"", line 1
    str.word_counts[i for i in lis]
                        ^
SyntaxError: invalid syntax
</code></pre>

<p>Code snippets or helpful links appreciated</p>
","8794168","","9835872","","2021-02-05 16:00:01","2021-02-05 16:00:01","Why I'm unable to pass list to word_counts in textblob?","<python><python-3.x><nltk><textblob>","1","1","","","","CC BY-SA 4.0"
"49388488","1","","","2018-03-20 15:41:47","","1","943","<p>I am sentiment analysing comments with TextBlob (which uses NLTK).
However, I want to add custom negative and positive words but I can't find a location of a positive-word and negative-word location locally.</p>

<p>I am running iPython trough Anaconda.</p>

<p>Can someone tell me the location of the negative and positive words, or if this is even possible?</p>

<p>Thanks!</p>
","7699091","","","","","2018-03-20 20:54:16","NLTK add custom negative and positive words","<nlp><nltk><corpus><textblob>","1","0","","","","CC BY-SA 3.0"
"65299862","1","","","2020-12-15 04:08:23","","0","74","<p>Here is the code that is causing problems</p>
<pre><code>import pandas as pd
from textblob import TextBlob
df = pd.read_csv('C:/Users/prana/Downloads/Sample tweet.csv') # path to csv file
df['english'] = df['tweet'].str.encode('ascii', 'ignore').apply(lambda x:TextBlob(x.strip()).translate(to='en'))
df.to_csv(&quot;C:/Users/prana/Downloads/test.csv&quot;) # your documents folder
</code></pre>
<p>and the error I get is</p>
<pre><code>TypeError: cannot use a string pattern on a bytes-like object'
</code></pre>
<p>Does anyone have any idea of how to go forward?</p>
","14827571","","5478373","","2020-12-15 04:47:56","2020-12-15 04:47:56","I am trying to use texblob translator to translate a .csv file into english but I am getting the following error","<python-3.x><pandas><dataframe><translate><textblob>","0","4","","","","CC BY-SA 4.0"
"65170483","1","","","2020-12-06 16:39:56","","0","26","<p>I am trying to translate a very large number of files for machine learning purposes, however I got the following error: HTTP Error 429: Too Many Requests for this line <code>reshaped_text = arabic_reshaper.reshape(str(blob.translate(to='ar')))</code>, but I am not able to retry my code and continue.P.S.: when I got this error I was not using <code>time.sleep()</code>.
this is my code:</p>
<pre><code>train_pos_data_dir ='/Users/User/Desktop/aclImdb/train/pos'

train_pos_data = os.listdir(train_pos_data_dir)


for i,data in enumerate(train_pos_data, 236):
    time.sleep(5)
    f = open('/Users/User/Desktop/aclImdb/train/pos/'+data, 'r')
    contents = f.read()
    blob = TextBlob(contents)
    reshaped_text = arabic_reshaper.reshape(str(blob.translate(to='ar')))
    reshaped_text_dir = get_display(reshaped_text)

    arabic_f = open('/Users/User/Desktop/Arabic_aclImdb/train/pos/'+'0.'+str(i)+'.txt', '+w',encoding='utf-8')
    arabic_f.write(reshaped_text_dir)
    arabic_f.close()
</code></pre>
<p>I added time.sleep(5) to avoid this error next time.
Any idea on how to resume my work, and if time.sleep() is sufficient to avoid this problem the next time?
Thank you.</p>
","13184263","","","","","2020-12-06 16:39:56","How to get back on track after an HTTP Error 429: Too Many Requests","<python><http><textblob><http-status-code-429>","0","1","","","","CC BY-SA 4.0"
"22211721","1","","","2014-03-05 23:33:50","","1","2876","<p>I am trying to implement a Naive Bayes algorithm to read tweets from a csv file and classify them into categories i define (for example: tech, science, politics)</p>

<p>I want to use NLTK's naive bayes classification algorithm but the example is not anywhere close to what i need to do. </p>

<p>One of my biggest confusion is how do we improve the classification accuracy of NB?</p>

<p>*<strong>*I am hoping to get some guidance on the detailed steps i need take to do the classification.</strong> </p>

<ul>
<li>do i have to create separate csv files for each category where i
manually put the tweets in there?</li>
<li>How do i train the algorithm if i do the above and how does the algorithm test?**</li>
</ul>

<p>I have been researching online and found some brief examples like TextBlob which makes use if NLTK's NB algorithm to do sentiment classification of Tweets. it is simple to understand but difficult to tweak for beginners.</p>

<p><a href=""http://stevenloria.com/how-to-build-a-text-classification-system-with-python-and-textblob/"" rel=""nofollow"">http://stevenloria.com/how-to-build-a-text-classification-system-with-python-and-textblob/</a></p>

<p>In his example from the link above, how does he implement the test when he already put the sentiment next to the tweets? I thought to test, we should hide the second argument. </p>

<pre><code>train = [
    ('I love this sandwich.', 'pos'),
    ('This is an amazing place!', 'pos'),
    ('I feel very good about these beers.', 'pos'),
    ('This is my best work.', 'pos'),
    (""What an awesome view"", 'pos'),
    ('I do not like this restaurant', 'neg'),
    ('I am tired of this stuff.', 'neg'),
    (""I can't deal with this"", 'neg'),
    ('He is my sworn enemy!', 'neg'),
    ('My boss is horrible.', 'neg')
]
test = [
    ('The beer was good.', 'pos'),
    ('I do not enjoy my job', 'neg'),
    (""I ain't feeling dandy today."", 'neg'),
    (""I feel amazing!"", 'pos'),
    ('Gary is a friend of mine.', 'pos'),
    (""I can't believe I'm doing this."", 'neg')
]
</code></pre>
","2800939","","","user2555451","2014-11-15 17:50:33","2014-11-15 18:11:01","Python Naive Bayes Classification of tweets into categories. Methods","<python><machine-learning><nltk><bayesian><textblob>","1","1","4","","","CC BY-SA 3.0"
"21835987","1","21836097","","2014-02-17 17:51:51","","1","1097","<p>I am trying to install this one: <a href=""https://pypi.python.org/pypi/textblob-aptagger"" rel=""nofollow"">https://pypi.python.org/pypi/textblob-aptagger</a> and it says to use this code  - but I do not know where to use it (command line and Python console do not work): </p>

<pre><code>$ pip install -U textblob-aptagger
</code></pre>

<p>I installed easy_install and pip using exe files from 
    <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/"" rel=""nofollow"">http://www.lfd.uci.edu/~gohlke/pythonlibs/</a></p>

<p>So when I use the command:</p>

<pre><code>$ pip install -U textblob-aptagger
</code></pre>

<p>in the Python console I get this error:</p>

<pre><code>  File ""&lt;console&gt;"", line 1
  $ pip install -U textblob-aptagger
  ^
  SyntaxError: invalid syntax  
</code></pre>

<p>Where should I use this installation command?  </p>
","2951230","","2213647","","2014-11-15 19:03:30","2014-11-15 19:03:30","Cannot install anything using Pip","<python-2.7><pip><easy-install><part-of-speech><textblob>","1","0","","","","CC BY-SA 3.0"
"46959492","1","","","2017-10-26 16:33:58","","4","3734","<p>I'm processing a textblob and one of the steps is stopwords removal. Textblobs are immutable, so I'm turning one into a list to do the job:</p>

<pre><code>blob = tb(tekst)
lista = [word for word in blob.words if word not in stopwords.words('english')]
tekst = ' '.join(lista)
blob = tb(tekst)
</code></pre>

<p>Is there a simpler / more elegant solution for the problem?</p>
","8839065","","","","","2021-09-29 04:28:23","Removing stopwords from a textblob","<python><textblob>","1","4","2","","","CC BY-SA 3.0"
"29091959","1","","","2015-03-17 05:33:38","","3","821","<pre><code>&gt;&gt;&gt; train = [
   ('I love this sandwich.', 'pos'),
   ('this is an amazing place!', 'pos'),
   ('I feel very good about these beers.', 'pos'),
   ('this is my best work.', 'pos'),
   (""what an awesome view"", 'pos'),
   ('I do not like this restaurant', 'neg'),
   ('I am tired of this stuff.', 'neg'),
   (""I can't deal with this"", 'neg'),
   ('he is my sworn enemy!', 'neg'),
   ('my boss is horrible.', 'neg')
   ]
&gt;&gt;&gt; test = [
    ('the beer was good.', 'pos'),
    ('I do not enjoy my job', 'neg'),
    (""I ain't feeling dandy today."", 'neg'),
    (""I feel amazing!"", 'pos'),
    ('Gary is a friend of mine.', 'pos'),
    (""I can't believe I'm doing this."", 'neg')
    ]
&gt;&gt;&gt; from textblob.classifiers import NaiveBayesClassifier
&gt;&gt;&gt; cl = NaiveBayesClassifier(train)
&gt;&gt;&gt; cl.classify(""This is an amazing library!"")
'pos'
</code></pre>

<p>The above code is for classifying a text using Python by using NaiveBayesClassifier. Similarly i have used MaxEntClassifier, DecisionTreeClassifier. Now i want to know are there any classifeirs other than the ones which i have mentioned for classifying in python. please let me know!!!</p>
","3821301","","","","","2015-03-17 05:33:38","can we classify text using SVM classifier in python-TextBlob?","<python><svm><text-classification><textblob>","0","0","3","","","CC BY-SA 3.0"
"38897710","1","","","2016-08-11 13:26:21","","0","252","<p>I am using pickle to save classified model with bayes theorem, I have saved a file with 2.1 GB after classification with 5600 records. but when i loading that file it is taking nearly 2 minutes but for classifying some text it is taking 5.5 minutes. I am using following code to load it and classify.   </p>

<pre><code>classifierPickle = pickle.load(open( ""classifier.pickle"", ""rb"" ) )
   classifierPickle.classify(""want to go some beatifull work place""))
</code></pre>

<p>First line for loading pickle object and second one for classifying text it results which topic(Category) it is. I am using following code to save model.</p>

<pre><code>file = open('C:/burberry_model/classifier.pickle','wb')
pickle.dump(object,file,-1)
</code></pre>

<p>Every thing i am using from textblob.Environment is  Windows,28GB RAM,four core CPU's . It would very help full if any one can resolve this issue.</p>
","6671611","","6671611","","2016-08-17 05:15:49","2016-11-30 00:14:05","How to reduce topic classification time in textblob naive bayes classifier","<python><text-classification><naivebayes><textblob>","1","0","","","","CC BY-SA 3.0"
"32957708","1","32957860","","2015-10-05 20:53:53","","109","76300","<p>I'm trying to do some text classification using Textblob. I'm first training the model and serializing it using pickle as shown below. </p>

<pre><code>import pickle
from textblob.classifiers import NaiveBayesClassifier

with open('sample.csv', 'r') as fp:
     cl = NaiveBayesClassifier(fp, format=""csv"")

f = open('sample_classifier.pickle', 'wb')
pickle.dump(cl, f)
f.close()
</code></pre>

<p>And when I try to run this file:</p>

<pre><code>import pickle
f = open('sample_classifier.pickle', encoding=""utf8"")
cl = pickle.load(f)    
f.close()
</code></pre>

<p>I get this error:</p>

<blockquote>
  <p>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position
  0: invalid start byte</p>
</blockquote>

<p>Following are the content of my sample.csv:</p>

<blockquote>
  <p>My SQL is not working correctly at all. This was a wrong choice, SQL</p>
  
  <p>I've issues. Please respond immediately, Support</p>
</blockquote>

<p>Where am I going wrong here? Please help.</p>
","3188761","","","","","2021-05-18 13:33:00","Python pickle error: UnicodeDecodeError","<python><pickle><textblob>","5","1","6","","","CC BY-SA 3.0"
"22555611","1","","","2014-03-21 10:02:51","","8","8484","<p>I have been using TextBlob, a package for Python (<a href=""https://pypi.python.org/pypi/textblob"" rel=""noreferrer"">https://pypi.python.org/pypi/textblob</a>) for translating articles to different language . </p>

<p>After reading their docs, I got to know that TextBlob makes use of Google Translate. Since google translate is not a free service, I wanted to know whether there is any usage limit on translating articles using TextBlob services? </p>
","2124470","","","user2555451","2014-11-15 17:16:10","2020-02-09 14:10:43","Is there a limit on TextBlob translation?","<python><google-translate><textblob>","3","0","1","","","CC BY-SA 3.0"
"65187277","1","","","2020-12-07 18:23:34","","0","72","<p>I'm trying to translate a certain number of files to arabic using TextBlob, this is my code:</p>
<pre><code># import pandas as pd
from textblob import TextBlob
import arabic_reshaper
from bidi.algorithm import get_display
import os
import time
# import requests
train_pos_data_dir = '/Users/User/Downloads/train_pos/train_pos_1'
train_pos_data = os.listdir(train_pos_data_dir)
for i, data in enumerate(train_pos_data,2):
    time.sleep(1)
    # contents = &quot;&quot;
    f = open('/Users/User/Downloads/train_pos/train_pos_1/' + data, 'r')
    contents = f.read()
    f.close()
    blob = TextBlob(contents)
    reshaped_text = arabic_reshaper.reshape(str(blob.translate(to='ar')))
    reshaped_text = str(blob.translate(to='ar'))
    # print(str(blob.translate(to='ar')))
    #     print(reshaped_text)
    reshaped_text_dir = get_display(reshaped_text)
    arabic_f = open('/Users/User/Desktop/Arabic_aclImdb/train/' + '0.' + str(i + 238) + '.txt', '+w', encoding='utf-8')
    arabic_f.write(reshaped_text_dir)
    arabic_f.close()
</code></pre>
<p>however I'm getting in the file the original text in english appended to the translated text in arabic.
I just want to have the translated text in arabic in each file, but I cannot see the problem in my code
Thank you!</p>
","13184263","","","","","2020-12-07 18:23:34","Translating from english into arabic using TextBlob: getting two texts in english and arabic in the same file","<python><textblob>","0","1","","","","CC BY-SA 4.0"
"62417947","1","","","2020-06-16 21:33:53","","0","241","<pre><code>This is my Data
d = {'col1': [""hola chica"", ""hello girl""], 'col2': [2, 4]}
df = pd.DataFrame(data=d)
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/ktFqT.png"" rel=""nofollow noreferrer"">see my Dataframe</a></p>

<p>Then I created this function</p>

<pre><code>def translate(x):
    blob =TextBlob(x)
    return blob.translate(to='fr')
</code></pre>

<p>I called it here </p>

<pre><code>df['translated'] = df['col1'].apply(lambda x:translate(x))
</code></pre>

<p>When I print my dataframe to see the translated column I get results with letter by letter followed by comma. I cant figure out why it doesnt show the sentence</p>

<p><a href=""https://i.stack.imgur.com/PFKru.png"" rel=""nofollow noreferrer"">see how my column translated looks like</a></p>

<p>Any ideas why this is happening?</p>

<p>I need to be able to have the sentence without the commas thanks</p>
","2480825","","","","","2020-06-16 22:10:04","Textblob Translation issue into a pandas dataframe","<python><pandas><translation><textblob>","1","0","","","","CC BY-SA 4.0"
"55756374","1","","","2019-04-19 03:52:13","","0","85","<p>I am using POS tagging, parse chunking and deep parsing in one step using TextBlob. I want to write the output in a txt file. The error I get is 
""TypeError: a bytes-like object is required, not 'TaggedString'"" </p>

<p>Following is the code I am using. The out variable contains information like this. 
 Technique:Plain/NNP/B-NP/O and/CC/I-NP/O enhanced-MPR/JJ/I-NP/O CT/NN/I-NP/O chest/NN/I-NP/O is/VBZ/B-VP/O
<a href=""https://i.stack.imgur.com/zU3n9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zU3n9.png"" alt=""enter image description here""></a></p>

<pre><code> from textblob import TextBlob
 with open('report1to8_1.txt', 'r') as myfile:
    report=myfile.read().replace('\n', '')

 out = TextBlob(report).parse()
 tagS = 'taggedop.txt'
 f = open('taggedop.txt', 'wb')
 f.write(out)
</code></pre>

<p>I also want to split these tags into dataframe in this way.
<a href=""https://i.stack.imgur.com/AcRi4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AcRi4.png"" alt=""enter image description here""></a></p>
","2287486","","2287486","","2019-04-19 04:04:35","2019-04-19 04:04:35","How to write tagged string into text file in python?","<python-3.x><nlp><textblob>","0","2","","","","CC BY-SA 4.0"
"41310885","1","","","2016-12-24 06:20:19","","1","933","<p>I am running Python 2.7 on a Macbook Air 10.5 Yosemite. I am running into this problem with installing textblob. What to do?</p>

<pre><code>$ pip install -U textblob //no errors
$ python -m textblob.download_corpora
[nltk_data] Error loading brown: &lt;urlopen error [SSL:
[nltk_data]dsaffdsa     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:661)&gt;
[nltk_data] Error loading punkt: &lt;urlopen error [SSL:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:661)&gt;
[nltk_data] Error loading wordnet: &lt;urlopen error [SSL:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:661)&gt;
[nltk_data] Error loading averaged_perceptron_tagger: &lt;urlopen error
[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify
[nltk_data]     failed (_ssl.c:661)&gt;
[nltk_data] Error loading conll2000: &lt;urlopen error [SSL:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:661)&gt;
[nltk_data] Error loading movie_reviews: &lt;urlopen error [SSL:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:661)&gt;
Finished.
</code></pre>
","5755239","","","","","2017-10-22 19:23:43","Error downloading textblob certificate verify failed","<python><macos><python-2.7><textblob>","1","1","","","","CC BY-SA 3.0"
"67049479","1","","","2021-04-11 19:43:00","","0","23","<p>Im trying build some sentimental model using Textblob. Below is my df. I get result after senimental function.</p>
<pre><code>Text    Tweet_tokenized Tweet_nonstop   Tweet_stemmed   sentiment
0   0   RT @bennyjohnson: 🚨 BOMBSHELL🚨 \n\nVeteran &amp;am... [rt, bennyjohnson, bombshell, veteran, amp, bu...   [rt, bennyjohnson, bombshell, veteran, amp, bu...   [rt, bennyjohnson, bombshel, veteran, amp, bu```s...    (0.0, 0.07142857142857142)
</code></pre>
<p>My code:</p>
<pre><code>df['sentiment'] = df['Text'].apply(lambda tweet: TextBlob(tweet).sentiment)
</code></pre>
<p>But i would like for each position using classification like a below:</p>
<pre><code>&gt;&gt;&gt; from textblob import TextBlob
&gt;&gt;&gt; blob = TextBlob(&quot;The beer is good. But the hangover is horrible.&quot;, classifier=cl)
&gt;&gt;&gt; blob.classify()
pos
</code></pre>
<p>Somebody have idea how to do that ? Thank for all support
'pos'</p>
","15605720","","","","","2021-04-11 19:43:00","How to create classification for each row on dataframe","<python><nlp><sentiment-analysis><textblob>","0","0","","","","CC BY-SA 4.0"
"57147426","1","","","2019-07-22 13:45:54","","0","229","<p>I'm trying to detect and return the language of each tweet in a list of tweets. I'm using textblob's detect_language function. </p>

<p>I guess that I'd need to use a for loop to iterate over each tweet in the list. But I'm stuck on how to do this. Do I need to use a TextBlob object in the loop, or something else?</p>

<pre><code>from textblob import TextBlob
df = pd.read_csv(""christian_sex_tweets.csv"")
text = df[""Message""]

# join all tweets into a big text
text = "" "".join(review for review in df[""Message""])
print (""There are {} words in the combination of all review."".format(len(text)))

sex_tweets_blob = TextBlob(text)

# detect languages in tweets 
tweets_langs = sex_tweets_blob.detect_language()

for lang in tweets_langs: 
    print(lang)

e
s

</code></pre>

<p>It just returns ""e s"". Can anyone help me fix the loop so it returns a list of languages for each tweet?</p>

<p>Thanks</p>
","6204076","","6204076","","2019-07-22 14:00:49","2019-07-22 14:00:49","Detect and return language for each tweet in a list of tweets","<python><textblob>","0","2","","","","CC BY-SA 4.0"
"65181192","1","","","2020-12-07 11:47:56","","1","384","<p><a href=""https://i.stack.imgur.com/qmKSm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qmKSm.png"" alt=""enter image description here"" /></a>I have a problem where , I need to calculate sentiment analysis of two columns present in the excel file and after calculation of polarity of those two columns, I need to update those polarity values in two other columns which are already present in the same excel input file. Any how I have achieved by calculating polarity of single text sentence . Need suggestions to calculate polarity of entire column present in the excel file.
I am using pandas for excel processing.</p>
<pre><code>from textblob import TextBlob
import pandas as pd
Input_file='filepath'
df = pd.read_excel(Input_file, 
sheet_name='Sheet1')
col1 = pd['video_title'].tolist()
# col2 = pd['description'].tolist()
blob = TextBlob(col1)
# blob1 = Texxtblob(col2)
polarity_score = blob.sentiment.polarity
polarity_rounded = round(polarity_score, 6)
print(polarity_rounded)
</code></pre>
<p>As i posted in the above image, here i need to replace the values 'None' in the column 'title_sentiment' to the calculated polarity values. Likewise, i have to update the 'description_sentiment' column to the calculated polarity values.</p>
<p>Desired output:
<a href=""https://i.stack.imgur.com/EWRFu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EWRFu.png"" alt=""enter image description here"" /></a></p>
","3363539","","3363539","","2020-12-07 12:55:27","2020-12-07 12:58:59","Sentiment analysis using python textblob on a excel file data","<python><excel><pandas><sentiment-analysis><textblob>","1","3","","","","CC BY-SA 4.0"
"30550411","1","30550865","","2015-05-30 19:23:13","","2","720","<p>Is there a way to tell #textblob <em>not</em> to split contractions like <code>let's</code> into <code>let</code> &amp; <code>'s</code> when creating ngrams? I know they are technically two separate words, but I'd like to maintain them as one.</p>
","2767173","","","","","2015-05-30 20:09:35","Preserving contractions with textblob ngrams","<python><nlp><textblob>","1","0","1","","","CC BY-SA 3.0"
"58736287","1","","","2019-11-06 18:17:37","","0","210","<p>I am trying to install and use textblob in my Anaconda environment.  I am following the directions <a href=""https://textblob.readthedocs.io/en/dev/install.html"" rel=""nofollow noreferrer"">HERE</a> I am successfully able to pip install <code>pip install -U textblob</code> </p>

<p>On step 2 running <code>python -m textblob.download_corpora</code> I am presented with multiple errors. One being a Windows popup with the error message: </p>

<blockquote>
  <p>the ordinal 242 could not be located in the dynamic link library
  libiomp5md.dll</p>
</blockquote>

<p>I am also presented with 2 errors in the terminal:</p>

<ol>
<li><blockquote>
  <p>INTEL MKL ERROR: The operating system cannot run %1.
  mkl_intel_thread.dll.</p>
</blockquote></li>
<li><blockquote>
  <p>Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.</p>
</blockquote></li>
</ol>
","11996885","","","","","2019-11-06 18:33:43","Error installing textblob.download_corpora in Anaconda environment","<python><anaconda><textblob>","1","0","","","","CC BY-SA 4.0"
"49731478","1","","","2018-04-09 11:05:56","","5","4974","<p>I have a text blob in which i am classifying the text as positive if polarity is > 0, neutral if = 0, and negative if &lt; 0.
How can i get the words based on which it is classifying as positive, negative or neutral?</p>
","3578462","","","","","2021-01-24 18:12:22","Get the positive and negative words from a Textblob based on its polarity in Python (Sentimental analysis)","<python><python-3.x><machine-learning><sentiment-analysis><textblob>","2","1","1","","","CC BY-SA 3.0"
"58694762","1","","","2019-11-04 13:47:44","","3","3891","<p>I would like to correct the misspelled words of a text in french, it seems that spacy is the most accurate and faster package to do it, but it's to complex, 
i tried with textblob, but i didnt' manage to do it  with french words</p>

<p>it works perfectly in english, but when I try to do the same in french I get the same misspelled words </p>

<pre class=""lang-py prettyprint-override""><code>#english words 
from textblob import TextBlob
misspelled=[""hapenning"", ""mornin"", ""windoow"", ""jaket""]
[str(TextBlob(word).correct()) for word in misspelled]

#french words
misspelled2=[""resaissir"", ""matinnée"", ""plonbier"", ""tecnicien""]
[str(TextBlob(word).correct()) for word in misspelled2]
</code></pre>

<p>i get this :</p>

<p>english:
['happening', 'morning', 'window', 'jacket']</p>

<p>french:
['resaissir', 'matinnée', 'plonbier', 'tecnicien']</p>
","11306372","","5612363","","2019-11-07 20:30:06","2019-11-07 20:30:06","Using textblob or spacy for correction spelling in french","<python><nlp><spacy><textblob>","1","0","3","","","CC BY-SA 4.0"
"57382169","1","","","2019-08-06 18:40:43","","1","489","<p>I have a <code>dataframe</code> in which there is a comments column. The comments are written in different languages including English. I wanted to translate all the comments into English language.</p>

<p>I used the code:</p>

<pre><code>df['comments'] = df['comments'].apply(lambda x: x.translate(to = 'en'))
</code></pre>

<p>The error shown is:</p>

<pre class=""lang-none prettyprint-override""><code>TypeError: translate() takes no keyword arguments
</code></pre>
","11848227","","355230","","2019-08-06 18:52:09","2019-08-06 18:52:09","How to translate different languages in a column in a dataframe to English using textblob?","<python><dataframe><textblob>","0","12","","","","CC BY-SA 4.0"
"67038596","1","67038988","","2021-04-10 19:48:22","","2","43","<p>I'm trying to insert a new dataframe column with only the 'positive' or 'negative' string according to TextBlob classification ex: for the 1st line of my df the result is (<strong>pos</strong>, 0.75, 0.2499999999999997) and I would like to have '<strong>positive</strong>' in a new column named 'algo_sentiment', I've been trying with this code:</p>
<pre class=""lang-py prettyprint-override""><code>def sentiment_algo(text):
    try:
        if TextBlob (text, analyzer=NaiveBayesAnalyzer()).sentiment == neg:
          return 'negative'
        return 'positive'
    except:
        return None

df_test['algo_sentiment'] = df_test['cleaned_tweets'].apply(sentiment_algo)
</code></pre>
<p>The new colum is indeed created, but returns always either everything positive or everything negative. I've runned some tests and cannot find a solution.</p>
","13479074","","10035985","","2021-04-10 20:00:00","2021-04-10 20:43:52","Return a TextBlob positive, negative or netutral classification in a new column in dataframe","<python><pandas><textblob>","1","0","","","","CC BY-SA 4.0"
"58766599","1","","","2019-11-08 12:27:51","","0","677","<p>I've been trying to install matplotlib and textblob on my system using</p>

<pre><code>pip3 install matplotlib
</code></pre>

<p>or </p>

<pre><code>pip3 install textblob
</code></pre>

<p>It says it's installed successfully. However, when I try to import in in Sublime Text it says </p>

<pre><code>ModuleNotFoundError: No module named 'matplotlib'
</code></pre>

<p>or </p>

<pre><code>ModuleNotFoundError: No module named 'textblob'
</code></pre>

<p>I've set up the build system in sublime text for python 3. A lot of modules that I've installed and are using the same way works fine (BeautifulSoup, requests and tweepy for example). I do not understand why some modules work and others do not. I've been trying to find the solution to this problem but after hours of research without results, I see this question as my last resort. Very thankful for all help.</p>

<p>shell_cmd: /usr/bin/env python3</p>
","12164431","","12164431","","2019-11-22 13:18:25","2019-11-22 13:18:25","Python3.7: No module named","<macos><matplotlib><sublimetext3><python-3.7><textblob>","0","4","","","","CC BY-SA 4.0"
"55578939","1","","","2019-04-08 17:41:58","","2","486","<p>I am trying to tokenize a sentence in a pandas dataframe but I am having some trouble</p>

<p>I know this code works to covert just one row</p>

<pre><code>TextBlob(df['H'][0]).words
</code></pre>

<p>But when I tried to apply it in a for loop I got an error</p>

<pre><code>for i, row in df.H():
ifor_val = TextBlob(df['H'][i]).words
df.at[i,'ifor'] = H
</code></pre>

<p>Error message:
TypeError: 'Series' object is not callable</p>

<p>Edit:</p>

<pre><code>data = {'H':['the quick brown fox jumps over the road', 'the weather is nice 
today'], 'marks':[99, 98]} 
df = pd.DataFrame(data) 
</code></pre>

<p>desired </p>

<pre><code>H                                  marks
['the','quick','brown', 'fox'....]   99
['the','weather','is', 'nice'....]   98
</code></pre>

<p>SOLUTION:</p>

<p>df['H']=df['H'].apply(word_tokenize)
df['H'].head()</p>
","7679796","","7679796","","2019-04-08 18:29:32","2019-04-08 18:29:32","Tokenize each row in a dataframe - for loop not working","<python><pandas><for-loop><textblob>","3","3","","","","CC BY-SA 4.0"
"62525780","1","","","2020-06-23 01:08:17","","1","772","<p>I am trying to analyze twitter data using textblob. Most commonly used Bigrams of my twitter text and their respective frequencies are retrieved and stored in a list variable 'l' as shown below.</p>
<pre><code>from textblob import TextBlob
blob = TextBlob(text)

import nltk, re, string, collections
from nltk.util import ngrams

'first get individual words'
tokenized = blob.split()

'and get a list of all the bi-grams'
Bigrams = ngrams(tokenized, 2)
Bigrams

'get the frequency of each bigram '
BigramFreq = collections.Counter(Bigrams)
BigramFreq

' what are the ten most popular bigrams '
l = BigramFreq.most_common(10)
l
</code></pre>
<p>Here the output of 'l' is a list containg bigrams and frequencies of each bigram shown as below after running the above code:</p>
<pre><code>  [(('@UniverCurious:', 'The'), 39),
 (('The', 'underside'), 38),
 (('underside', 'of'), 38),
 (('of', 'Jupiter.'), 38),
 (('Jupiter.', 'Credit:'), 38),
 (('Credit:', 'NASA/JPL/JUNO'), 38),
 (('to', 'the'), 25),
 (('just', '100'), 15),
 (('20', 'years'), 14)]
</code></pre>
<p>Now I am able to create a table from the most common bigrams. But i need help with creation of wordcloud from the given code above.</p>
<p>My question is how to create a  wordcloud from this list 'l'?</p>
","13795464","","13795464","","2020-06-25 22:44:47","2020-06-25 22:44:47","How to create wordcloud showing most common bigrams in a text using Python?","<python><textblob>","2","0","","","","CC BY-SA 4.0"
"45037610","1","45077186","","2017-07-11 14:35:42","","1","502","<h1>How to persist model results for next update later in TextBlob?</h1>

<p>The documentation reference can be found here  <a href=""https://github.com/sloria/TextBlob"" rel=""nofollow noreferrer"">https://github.com/sloria/TextBlob</a></p>

<p>I noticed the documentation specified how to update the training data but I did not see a method or way to save data from a last session.</p>

<p><strong>how to update:</strong>
<a href=""https://textblob.readthedocs.io/en/dev/classifiers.html#updating-classifiers-with-new-data"" rel=""nofollow noreferrer"">https://textblob.readthedocs.io/en/dev/classifiers.html#updating-classifiers-with-new-data</a></p>

<p>In particular I'm referring to classifying text. I do feel I am dumb in this particular topic as I always find it difficult to know where these training sessions are being persisted in any AI examples.</p>

<p>You don't want to run the whole thing again right? You want to start where you left off and keep improving it iteratively. </p>

<p>I want to do this:</p>

<ol>
<li>If past training results exists, load them into the model </li>
<li>Update or run new training session</li>
<li>Save training session</li>
<li>Repeat at a later time as needed</li>
</ol>
","775516","","","","","2017-07-13 09:52:57","How to persist model for Python TextBlob?","<python><textblob>","1","2","","","","CC BY-SA 3.0"
"30188801","1","30296994","","2015-05-12 10:56:46","","0","320","<p>I'm building an application that analyze sentiment for news-related tweets in different domains, such as sports, disaster and technology, I'm using Textblob with the default mode (PatternAnalyzer). Does that provide a good sentiment even though domains are different? And how can I evaluate its performance? Or is it better to provide my own training data for each domain and train a classifier? </p>
","4890990","","","","","2015-05-18 07:05:53","The use of Textblob for multiple domain tweets for sentiment","<twitter><textblob>","1","0","1","","","CC BY-SA 3.0"
"29169732","1","29179773","","2015-03-20 15:07:07","","5","5682","<p>I want to analyze sentiment of texts that are written in German. I found a lot of tutorials on how to do this with English, but I found none on how to apply it to different languages.</p>

<p>I have an idea to use the <code>TextBlob</code> Python library to first translate the sentences into English and then to do sentiment analysis, but I am not sure whether or not it is the best way to solve this task.</p>

<p>Or are there any other possible ways to solve this task?</p>
","1606150","","3001761","","2015-03-20 15:10:25","2021-08-13 10:26:03","Sentiment analysis of non-English texts","<python><machine-learning><nlp><sentiment-analysis><textblob>","4","5","2","","","CC BY-SA 3.0"
"35981117","1","","","2016-03-14 06:46:17","","0","726","<p>I am testing textblob module for Positive and Negative words.
But some of the results are not good.
for example : </p>

<p>code :</p>

<pre><code>from textblob.sentiments import NaiveBayesAnalyzer
from textblob import TextBlob

message = ""Fraud""
blob = TextBlob(message, analyzer=NaiveBayesAnalyzer())
a = (blob.sentiment)
print(a)
</code></pre>

<h1>Result</h1>

<pre><code>Sentiment(classification='pos', p_pos=0.6428571428571429, p_neg=0.3571428571428571)
</code></pre>

<p>it gives 90% correct answers but for some words it returns false result!!!</p>

<p>like :
    message = ""like this""
    Sentiment(classification='neg', p_pos=0.4794333489299875, p_neg=0.5205666510700125)</p>

<pre><code>message = ""good habits""
Sentiment(classification='neg', p_pos=0.41318402216578204, p_neg=0.5868159778342183)
</code></pre>

<p>=====================================</p>

<pre><code>""fraud"" = pos
""like this"" = neg
""good habits"" = neg
</code></pre>
","5208649","","","","","2016-04-04 23:32:35","TextBlob Python3 - ""Neg Tag for Positive words""","<python><python-2.7><python-3.x><nltk><textblob>","1","1","","","","CC BY-SA 3.0"
"49743234","1","49928792","","2018-04-09 23:44:30","","0","899","<p>This my first post on StackOverflow, so please be forgiving with any faux pas I may be making. I'm also new to Python, so any and all tips are welcome. My questions is simple, but no matter what I've tried I can't seem to figure it out. Here is my code:</p>

<pre class=""lang-py3 prettyprint-override""><code>import os
from bs4 import BeautifulSoup
import string
import nltk
from nltk import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk import FreqDist

# For TF-IDF calculations
import math
from textblob import TextBlob as tb

def tf(word, blob):
    return blob.words.count(word) / len(blob.words)

def n_containing(word, bloblist):
    return sum(1 for blob in bloblist if word in blob.words)

def idf(word, bloblist):
    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))

def tfidf(word, blob, bloblist):
    return tf(word, blob) * idf(word, bloblist)

rootDir ='D:\rootDir'
testPath = r'D:\testPath'
trainPath = r'D:\trainPath'

data = []
lemmatizer = WordNetLemmatizer()
stop = stopwords.words(""english"")
stop += ['also']

bloblist_train = [tb('')]  #Before EDIT: bloblist_train = tb('')
bloblist_test = [tb('')]   #Before EDIT: bloblist_test = tb('')

for currentDirPath, subDirs, files in os.walk(rootDir):
    for file in files:
        with open(os.path.join(currentDirPath, file)) as dataFile:
            inFile = dataFile.read()
            html = BeautifulSoup(inFile, ""html.parser"")
            text = html.get_text()
            text_no_punc = text.translate(str.maketrans("""", """", string.punctuation))
            if testPath in currentDirPath:
                bloblist_test += (tb(text_no_punc))
            elif trainPath in currentDirPath:
                bloblist_train += tb(text_no_punc)
            words = text_no_punc.split()
            data = data + words
</code></pre>

<p>I'm iterating over a larger directory of files with HTML documents and am parsing them and then further trying to find the TF-IDF for each word. I'm using a mix of packages and classes for this including BeautifulSoup, NLTK, and TextBlob. I'm using TextBlob to find the TF-IDF, but have run into the issue of creating a list of TextBlobs. The specific lines I'm having issues with are these:</p>

<pre><code>if testPath in currentDirPath:
    bloblist_test += tb(text_no_punc)
elif trainPath in currentDirPath:
    bloblist_train += tb(text_no_punc)
</code></pre>

<p>The code presently creates just one giant TextBlob with all the documents concatenated as one TextBlob. I would like a TextBlob for each document. I have tried the following approach as well</p>

<pre><code>if testPath in currentDirPath:
    bloblist_test.append(tb(text_no_punc))
elif trainPath in currentDirPath:
    bloblist_train.append(tb(text_no_punc))
</code></pre>

<p>which gives the error:</p>

<pre><code>AttributeError: 'TextBlob' object has no attribute 'append'
</code></pre>

<p>What am I missing? Append is the method I have been using to create lists python strings like so:</p>

<pre><code>s1 = [1,2,3]
s2 = [4,5]
s1.append(s2)
# Output: [[1,2,3], [4,5]]
</code></pre>

<p>But TextBlobs apparently don't support this.</p>

<p>So how do I go about creating a list of these Textblobs?</p>

<p><strong>EDIT:</strong></p>

<p>So I made some progress on my own, but am still having trouble formatting the list. Instead of initializing <code>bloblist_train</code> and <code>bloblist_test</code> to <code>tb('')</code>, I set them equal to <code>[tb('')]</code> because like my question says, they're suppose to hold a LIST of TextBlobs, not just TextBlobs. So now it would seem...it works! There's just one thing I still can't seem to get right: The way it is now creates a list with one empty TextBlob as the very first item (e.g. <code>[TextBlob(""""), TextBlob(""one two three"")]</code>).</p>

<p><em>I realize this is a slightly different question than what I started with, so if someone thinks I need to close this question and start a separate one, please let me know. Again, I'm new.</em></p>

<p>If not, I feel there is a simple keyword or syntactical solution that I'm missing and would greatly appreciate some input.</p>
","9407186","","9407186","","2018-04-11 01:49:48","2018-04-19 19:26:43","How to create a list of TextBlobs?","<python><python-3.x><list><syntax><textblob>","1","3","","","","CC BY-SA 3.0"
"48714328","1","","","2018-02-09 21:27:48","","0","269","<p>I'm trying to test out loading data from a csv file into TextBlob to create a classifier, which I'll then test with a training set. I've started with a small csv just to test it and ensure it works, before I write hundreds of lines for the real thing. However, I've run into an issue. I'm using the example code listed on the TextBlob website:</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier

with open('testerdict.csv',) as fp:
    cl = NaiveBayesClassifier(fp, format=""csv"")

test = [
    ('The beer was good.', 'pos'),
    ('I do not enjoy my job', 'neg'),
    (""I ain't feeling dandy today."", 'neg'),
    (""I feel amazing!"", 'pos'),
    ('Gary is a friend of mine.', 'pos'),
    (""I can't believe I'm doing this."", 'neg')
]
cl = NaiveBayesClassifier(fp)
print(cl.classify(""This is an amazing library!""))
print(cl.accuracy(test))
</code></pre>

<p>The errors I'm receiving vary depending on how I format my csv file, so I assume the issue lies there. If I format it like this:</p>

<p>!<a href=""https://ibb.co/nx3ttx"" rel=""nofollow noreferrer"">https://ibb.co/nx3ttx</a></p>

<p>I get this error: ValueError: I/O operation on closed file.</p>

<p>If I format it like this:</p>

<p>!<a href=""https://ibb.co/i6LHLc"" rel=""nofollow noreferrer"">https://ibb.co/i6LHLc</a></p>

<p>I get this error: ValueError: not enough values to unpack (expected 2, got 1)</p>

<p>Like I said, I've just made a very quick and basic csv so I can understand how to properly format it and load it into TextBlob for training data for the classifier. Does anyone have any ideas as to what could be wrong?</p>

<p>Thanks in advance.</p>

<p>Edit: full traceback as requested:</p>

<pre><code>  File ""C:\Users\Ver\AppData\Local\Programs\Python\Python36-32\SA Project\Work\tester.py"", line 14, in &lt;module&gt;
    cl = NaiveBayesClassifier(fp)
  File ""C:\Users\Ver\AppData\Local\Programs\Python\Python36-32\lib\site-packages\textblob\classifiers.py"", line 205, in __init__
    super(NLTKClassifier, self).__init__(train_set, feature_extractor, format, **kwargs)
  File ""C:\Users\Ver\AppData\Local\Programs\Python\Python36-32\lib\site-packages\textblob\classifiers.py"", line 136, in __init__
    self.train_set = self._read_data(train_set, format)
  File ""C:\Users\Ver\AppData\Local\Programs\Python\Python36-32\lib\site-packages\textblob\classifiers.py"", line 148, in _read_data
    format_class = formats.detect(dataset)
  File ""C:\Users\Ver\AppData\Local\Programs\Python\Python36-32\lib\site-packages\textblob\formats.py"", line 145, in detect
    if Format.detect(fp.read(max_read)):
ValueError: I/O operation on closed file.
</code></pre>
","4424587","","4424587","","2018-02-09 21:49:49","2018-02-09 21:55:36","Python TextBlob: Can't load classifer csv for training","<python><csv><textblob>","1","6","","","","CC BY-SA 3.0"
"24865739","1","24867745","","2014-07-21 12:59:26","","2","856","<p>I am working on a text search project, and using text blob to search for sentences from text.
TextBlob pulls all the sentences with the keywords efficiently. However for effective research i also want to pull out one sentence before and one after which I am unable to figure. </p>

<p>Below is the code I am using:</p>

<pre><code>def extraxt_sents(Text,word):
    search_words = set(word.split(','))
        sents = ''.join([s.lower() for s in Text])
        blob = TextBlob(sents)
    matches = [str(s) for s in blob.sentences if search_words &amp; set(s.words)]
    print search_words
    print(matches)
</code></pre>
","3849485","","","user2555451","2014-11-15 17:41:24","2016-04-07 21:32:32","Text search using python","<python><textblob>","1","9","","","","CC BY-SA 3.0"
"47113841","1","","","2017-11-04 18:02:14","","2","1626","<pre><code>from textblob import TextBlob

def sentiment_calc(text):
    try:
        return TextBlob(text).sentiment
    except:
        return None

test_df['sentiment score'] = test_df['text'].apply(sentiment_calc)
test_df
</code></pre>

<p>I recently ran a code on my dataset to implement sentiment analysis using the TextBlob package. After running that, my sentiment column has the following output below (I did an example table with dummy numbers below).</p>

<pre><code> text   | sentiment score
 ------------------------
 nice   | (0.45, 4.33)
 good   | (0.45, 4.33)
 ok     | (0.45, 4.33)
</code></pre>

<p>And the output I would like to get is this, where I split the sentiment column into two columns, but add those columns onto the current dataframe.</p>

<pre><code>text | polarity | subjectivity
------------------------------
nice |0.45      | 0.433
good |0.45      | 0.433
ok   |0.45      | 0.433
</code></pre>

<p>Is there a way to do this in Python 2.7?   </p>
","5758721","","5758721","","2017-11-04 19:13:05","2017-11-04 19:13:05","Splitting TextBlob sentiment analysis results into two separate columns - Python Pandas","<python><pandas><textblob>","1","3","","","","CC BY-SA 3.0"
"42449485","1","","","2017-02-24 22:39:28","","1","1293","<p>I want to do sentiment analysis of a list of tweets fetched based on a particular keyword. The tweets coming in are mostly in dutch language and TextBlob needs them converted to English in order to compute the tweet's polarity and subjectivity value. How can I convert the tweet to English language? I basically need a FREE API to do the translation. Having trouble using the MS Bing Translator. I have tried using <code>goslate</code> , <code>langdetect</code> , <code>translate</code> and <code>translation</code> libraries but none of them worked. Here's the code that I am using :</p>

<pre><code>#!/usr/bin/env python
import tweepy
import goslate
from langdetect import detect
from translation import baidu, google, youdao, iciba
from translate import Translator
import os
import time
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
t=time.time()
#karan's api keys 
consumer_key = 'xxx'
consumer_secret = 'xxx'
access_key = 'xxx'
access_secret = 'xxx'

gs=goslate.Goslate()
translator= Translator(to_lang=""en"")
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_key, access_secret)
api = tweepy.API(auth)
search_results = api.search(q=""football"", count=2, geocode=""52.132633,5.2912659999999505,300km"")
f=open('tweets_football.txt','wb')

for i in range(0,len(search_results)):
    try:
        print search_results[i].text        
        print search_results[i].id
        print search_results[i].user.screen_name
        trans=search_results[i].text
        #print(gs.translate(trans,'en'))
        print(translator.translate(trans))
        if search_results[i].text not in search_results:
            f.write(search_results[i].text)
            f.write(""\n"")
            print ""Written to file!""
    except Exception as e:
         print str(e)



f.close()
print time.time()-t
</code></pre>

<p>Please point me in the right direction. If there's an easier method for this process, please suggest that as well. Thanks in advance.</p>
","6535460","","","","","2019-12-23 08:15:25","Sentiment analysis using TextBlob for Dutch language","<python-2.7><twitter><translation><sentiment-analysis><textblob>","1","2","","","","CC BY-SA 3.0"
"62866213","1","62866342","","2020-07-12 20:36:04","","1","2067","<p>I'm trying to use <code>TextBlobDE.correct()</code> method. In each run of the script below i get the error below:</p>
<pre><code>/usr/local/lib/python3.8/site-packages/textblob_de/blob.py&quot;, line 523, in correct
    raise NotImplementedError
NotImplementedError
</code></pre>
<p>My script :</p>
<pre><code>import pandas as pd
from textblob_de import TextBlobDE as TextBlob

Text_Attribute = %{textAttribute}

def spellingCorrection(text) :   
  b = TextBlob(text)
  return b.correct()

def rm_main(data):
  data['corrected_text'] = data[Text_Attribute].apply(spellingCorrection)
  return data
</code></pre>
<p>Can someone lead me to the error and suggest a fix.</p>
","13908095","","8748472","","2020-07-12 21:17:25","2020-07-12 21:17:25","How to fix NotImplementedError?","<python><textblob><python-textprocessing>","1","0","","","","CC BY-SA 4.0"
"43018030","1","43023503","","2017-03-25 15:11:07","","13","11283","<p>I am using python to clean a given sentence. Suppose that my sentence is:</p>

<pre><code>What's the best way to ensure this?
</code></pre>

<p>I want to convert:</p>

<pre><code>What's -&gt; What is
</code></pre>

<p>Similarly,</p>

<pre><code> must've -&gt; must have
</code></pre>

<p>Also, verbs to original form,</p>

<pre><code>told -&gt; tell
</code></pre>

<p>Singular to plural, and so on.</p>

<p>I am currently exploring textblob. But not all of the above is possible using it. </p>
","3397298","","1033581","","2018-11-03 20:25:47","2021-08-08 06:15:58","Replace apostrophe/short words in python","<python><nlp><textblob>","4","1","4","","","CC BY-SA 4.0"
"43354696","1","","","2017-04-11 19:31:56","","0","554","<p>I have a script that searches Twitter for a certain term and then prints out a number of attributes for the returned results.</p>

<p>I'm trying to Just a blank array is returned. Any ideas why?</p>

<pre><code>public_tweets = api.search(""Trump"")

tweets_array = np.empty((0,3))

for tweet in public_tweets:

    userid = api.get_user(tweet.user.id)
    username = userid.screen_name
    location = tweet.user.location
    tweetText = tweet.text
    analysis = TextBlob(tweet.text)
    polarity = analysis.sentiment.polarity

    np.append(tweets_array, [[username, location, tweetText]], axis=0)

print(tweets_array)
</code></pre>

<p>The behavior I am trying to achieve is something like..</p>

<pre><code>array = []
array.append([item1, item2, item3])
array.append([item4,item5, item6])
</code></pre>

<p><code>array</code> is now <code>[item1, item2, item3],[item4, item5, item6]</code>.</p>

<p>But in Numpy :)</p>
","1130864","","","","","2017-04-12 02:18:52","Trying to append content to numpy array","<arrays><loops><numpy><tweepy><textblob>","2","1","","","","CC BY-SA 3.0"
"56750656","1","","","2019-06-25 09:18:11","","0","635","<p>Textblob-de is the German extension of Textblob and is documented under <a href=""https://textblob-de.readthedocs.io/en/latest/#"" rel=""nofollow noreferrer"">https://textblob-de.readthedocs.io/en/latest/#</a></p>

<p>I want to lemmatize German words like hast -> haben, Häuser -> Haus with Textblob-de </p>

<p>I found this post from J. Schneider about different lemmatization methods in Python, which is tailored for English, which also includes a part about Textblob (English version): <a href=""https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/lemmatization-examples-python/</a></p>

<p>I wonder if there is the possibility of doing lemmatization in the German extension too.</p>

<p>Following the text from J. Schneider the lemmatization should be possible with the following lines</p>

<pre><code>from textblob_de import TextBlobDE, Word
word = 'hast Häuser'
w = Word(word)
w.lemmatize()
</code></pre>

<p>I receive the following NotImplementedError:</p>

<pre><code>NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-5-16cfb68c1c58&gt; in &lt;module&gt;
      2 word = 'hast Häuser'
      3 w = Word(word)
----&gt; 4 w.lemmatize()

/anaconda3/lib/python3.7/site-packages/textblob/decorators.py in decorated(*args, **kwargs)
     33     def decorated(*args, **kwargs):
     34         try:
---&gt; 35             return func(*args, **kwargs)
     36         except LookupError as err:
     37             print(err)

/anaconda3/lib/python3.7/site-packages/textblob_de/blob.py in lemmatize(self, pos)
    161         #lemmatizer = nltk.stem.WordNetLemmatizer()
    162         # return lemmatizer.lemmatize(self.string, pos)
--&gt; 163         raise NotImplementedError
    164 
    165     @cached_property

NotImplementedError: 
</code></pre>

<p>I expect output like:</p>

<pre><code>haben Haus
</code></pre>
","11113801","","","","","2019-06-25 09:53:47","Lemmatizing German Texts with Textblob-de","<python><lemmatization><textblob>","1","0","","","","CC BY-SA 4.0"
"52048757","1","52086965","","2018-08-28 01:10:33","","0","370","<p>I can successfully split a sentence into its individual words and take of every average of the polarity score of every word using this code. It works great. </p>

<pre><code>import statistics as s
from textblob import TextBlob

a = TextBlob(""""""Thanks, I'll have a read!"""""")
print(a)

    c=[]
    for i in a.words: 
        c.append(a.sentiment.polarity)
        d = s.mean(c)


d = 0.25
a.words = WordList(['Thanks', 'I', ""'ll"", 'have', 'a', 'read'])
</code></pre>

<p>How do I transfer the above code to a df that looks like this?: </p>

<p>df</p>

<pre><code>     text
1    Thanks, I’ll have a read!
</code></pre>

<p>but take the average of every polarity per word? </p>

<p>The closet is I can apply polarity to every sentence for every sentence in df: </p>

<pre><code>def sentiment_calc(text):
    try:
        return TextBlob(text).sentiment.polarity
    except:
        return None

df_sentences['sentiment'] = df_sentences['text'].apply(sentiment_calc)
</code></pre>
","3277133","","3277133","","2018-08-28 08:42:15","2018-08-29 22:33:02","How to split every sentence into individual words and average polarity score per sentence and append into new column in dataframe?","<python><pandas><dataframe><nlp><textblob>","1","6","","","","CC BY-SA 4.0"
"52055442","1","","","2018-08-28 10:16:21","","0","193","<p>I have a df that looks like this: </p>

<pre><code>       text
0   Thanks, I’ll have a read!
1   Am I too late
</code></pre>

<p>How do I apply TextBlob tokenization to every word in sentence and average the polarity scores of every word in each sentence? </p>

<p>for example, I can do this with a single sentence in a variable: </p>

<pre><code>from textblob import TextBlob
import import statistics as s

#tokenize word in sentence
a = TextBlob(""""""Thanks, I'll have a read!"""""")
print a.words

    WordList(['Thanks', 'I', ""'ll"", 'have', 'a', 'read'])

#get polarity of every word
    for i in a.words:
        print( a.sentiment.polarity)

    0.25
    0.25
    0.25
    0.25
    0.25
    0.25


#calculating the mean of the scores
c=[]
for i in a.words: 
    c.append(a.sentiment.polarity)
    d = s.mean(c)
    print (d)

0.25
</code></pre>

<p>How do I apply the <code>a.words</code> to every row of dataframe column for sentence? </p>

<p>New df: </p>

<pre><code>      text                        score
0   Thanks, I’ll have a read!      0.25
1   Am I too late                  0.24
</code></pre>

<p>closet I come is that I can get polarity of every sentence using this function on the dataframe:</p>

<pre><code>def sentiment_calc(text):
    try:
        return TextBlob(text).sentiment.polarity
    except:
        return None

df_sentences['sentiment'] = df_sentences['text'].apply(sentiment_calc)
</code></pre>

<p>Thank you in advance. </p>
","3277133","","","","","2018-08-28 10:16:21","How to tokenize every sentence into indivdual words in row of dataframe and average the polarity for every word in sentence?","<python><pandas><nlp><nltk><textblob>","0","9","","","","CC BY-SA 4.0"
"56757622","1","","","2019-06-25 15:38:59","","0","144","<p>I am having difficulties printing out the text from this page as BeautifulSoup is not picking up the span class or section class tags. I would like to pull in the text from Motley Fool and then parse by sentence. </p>

<p><a href=""https://www.fool.com/earnings/call-transcripts/2019/04/26/exxon-mobil-corp-xom-q1-2019-earnings-conference-c.aspx"" rel=""nofollow noreferrer"">https://www.fool.com/earnings/call-transcripts/2019/04/26/exxon-mobil-corp-xom-q1-2019-earnings-conference-c.aspx</a></p>

<p>So far when it occasionally does pull in the text the sentence parsing works, however, beautiful soup only occasionally pulls in the text.</p>

<pre><code>from textblob import TextBlob
from html.parser import HTMLParser
import re
def news(): 
    # the target we want to open     
    url = dataframe_url

    #open with GET method 
    resp=requests.get(url) 

    #http_respone 200 means OK status 
    if resp.status_code==200: 

        soup = BeautifulSoup(resp.text,""html.parser"")

        #l = soup.find(""span"",attrs={'class':""article-content""})
        l = soup.find(""section"",attrs={'class':""usmf-new article-body""})

        #print ('\n-----\n'.join(tokenizer.tokenize(l.text)))
        textlist.extend(tokenizer.tokenize(l.text))

    else: 
        print(""Error"")
</code></pre>
","11698513","","","","","2019-06-28 19:07:05","BeautifulSoup not picking up text from span class or section class tags","<python><beautifulsoup><tags><html-parsing><textblob>","1","2","","","","CC BY-SA 4.0"
"32264399","1","","","2015-08-28 06:10:36","","1","641","<p>I am currently working on a Java project with python integration.
For that purpose I am using the following packages and ide:</p>

<p>Eclipse luna
Jython 2.7.0
TextBlob v0.10.0-dev</p>

<p>Here is my java code-snippet:</p>

<pre><code>        PythonInterpreter interpreter = new PythonInterpreter();
        interpreter.execfile(""AnSoMiaPy/analyser/test1.py"");
        PyObject translated_text = interpreter.get(""translated_text"");
        interpreter.close();
        System.out.println(""translated_text: "" + translated_text.toString());
</code></pre>

<p>The python code is also fairly simply.</p>

<pre><code>        #!/usr/bin/python
        # -*- coding: UTF-8 -*-
        from textblob import TextBlob
        import sys
        reload(sys)
        sys.setdefaultencoding(""utf-8"")

        text = '''
        The titular threat of The Blob has always struck me as the ultimate             movie
        monster: an insatiably hungry, amoeba-like mass able to penetrate
        virtually any safeguard, capable of--as a doomed doctor chillingly
        describes it--""assimilating flesh on contact.
        Snide comparisons to gelatin be damned, it's a concept with the most
        devastating of potential consequences, not unlike the grey goo scenario
        proposed by technological theorists fearful of
        artificial intelligence run rampant.
        '''
        blob = TextBlob(text)
        blob.noun_phrases   # WordList(['titular threat', 'blob',
                #            'ultimate movie monster',
                #            'amoeba-like mass', ...])

        for sentence in blob.sentences:
        print(sentence.sentiment.polarity)
        # 0.060
        # -0.341

        translated_text = blob.translate(to=""de"")
        print(translated_text)
        print blob.sentiment.polarity
</code></pre>

<p>When I am executing the python code I get what I expect:</p>

<blockquote>
  <p>0.06
  -0.341666666667
  Der Titular Bedrohung des BLOB hat mich als die ultimative Film immer geschlagen
  Monster: ein unersättlich hungrig, amöbenartige Masse in der Lage, zu durchdringen
  praktisch jede Sicherung, in der Lage ist - als Untergang geweihten Arzt unterkühlt
  es beschreibt - ""assimilieren Fleisch auf Kontakt.
  Abfällige Vergleiche zu Gelatine verdammt sein, es ist ein Konzept, mit dem die meisten
  verheerende der möglichen Folgen, nicht anders als die graue Schmiere-Szenario
  durch technologische Theoretiker fürchten vorgeschlagen
  künstliche Intelligenz wuchern.
  -0.159090909091</p>
</blockquote>

<p>However, when I call the python-script with jython I am getting the following result:</p>

<blockquote>
  <p>0.0
  0.0
  Der Titular Bedrohung des BLOB hat mich als die ultimative Film immer geschlagen
  Monster: ein unersättlich hungrig, amöbenartige Masse in der Lage, zu durchdringen
  praktisch jede Sicherung, in der Lage ist - als Untergang geweihten Arzt unterkühlt
  es beschreibt - ""assimilieren Fleisch auf Kontakt.
  Abfällige Vergleiche zu Gelatine verdammt sein, es ist ein Konzept, mit dem die meisten
  verheerende der möglichen Folgen, nicht anders als die graue Schmiere-Szenario
  durch technologische Theoretiker fürchten vorgeschlagen
  künstliche Intelligenz wuchern.
  0.0
  translated_text: TextBlob(""Der Titular Bedrohung des BLOB hat mich als die ultimative Film immer geschlagen
  Monster: ein unersÃ¤ttlich hungrig, amÃ¶benartige Masse in der Lage, zu durchdringen
  praktisch jede Sicherung, in der Lage ist - als Untergang geweihten Arzt unterkÃ¼hlt
  es beschreibt - ""assimilieren Fleisch auf Kontakt.
  AbfÃ¤llige Vergleiche zu Gelatine verdammt sein, es ist ein Konzept, mit dem die meisten
  verheerende der mÃ¶glichen Folgen, nicht anders als die graue Schmiere-Szenario
  durch technologische Theoretiker fÃ¼rchten vorgeschlagen
  kÃ¼nstliche Intelligenz wuchern."")</p>
</blockquote>

<p>The sentiment.polarity is suddenly 0 and after accessing the translated variable in java and printing it the ascii encoding looks a bit weird.</p>

<p>Has anyone experienced a similar problem?</p>
","5275593","","2572285","","2015-08-28 06:34:33","2015-08-28 06:34:33","Jython for Java with TextBlob","<java><ascii><jython><textblob>","0","1","1","","","CC BY-SA 3.0"
"51886641","1","","","2018-08-16 23:37:38","","2","158","<p>On my locat host , if i run simple print statement it works fine. but i want to use textblob for sentiment analysis. For this purpose, I am trying to run below code on my localhost but it's showing error.</p>

<p>Code:</p>

<pre><code>#!C:\fname lname\AppData\Local\Programs\Python\Python37-32\python.exe
from textblob.classifiers import NaiveBayesClassifier
from textblob import TextBlob

train = [
    ('I love this sandwich.', 'pos'),
    ('This is an amazing place!', 'pos'),
    ('I feel very good about these beers.', 'pos'),
    ('This is my best work.', 'pos'),
    (""What an awesome view"", 'pos'),
    ('I do not like this restaurant', 'neg'),
    ('I am tired of this stuff.', 'neg'),
    (""I can't deal with this"", 'neg'),
    ('He is my sworn enemy!', 'neg'),
    ('My boss is horrible.', 'neg')
]
test = [
    ('The beer was good.', 'pos'),
    ('I do not enjoy my job', 'neg'),
    (""I ain't feeling dandy today."", 'neg'),
    (""I feel amazing!"", 'pos'),
    ('Gary is a friend of mine.', 'pos'),
    (""I can't believe I'm doing this."", 'neg')
]

cl = NaiveBayesClassifier(train)

# Classify some text
print(cl.classify(""Their burgers are amazing.""))  # ""pos""
print(cl.classify(""I don't like their pizza.""))   # ""neg""

# Classify a TextBlob
blob = TextBlob(""The beer was amazing. But the hangover was horrible. ""
                ""My boss was not pleased."", classifier=cl)
print(blob)
print(blob.classify())

for sentence in blob.sentences:
    print(sentence)
    print(sentence.classify())

# Compute accuracy
print(""Accuracy: {0}"".format(cl.accuracy(test)))

# Show 5 most informative features
cl.show_informative_features(5)
</code></pre>

<p>I feel problem is these statements. Not sure how to fix it. Not sure if i need to import any library and should place it location where script is placed.</p>

<pre><code> from textblob.classifiers import NaiveBayesClassifier
    from textblob import TextBlob
</code></pre>

<p>Error appeared as :</p>

<pre><code>Server error!
The server encountered an internal error and was unable to complete your request.

Error message: 
End of script output before headers: nltk.py

If you think this is a server error, please contact the webmaster.

Error 500
localhost
Apache/2.4.33 (Win32) OpenSSL/1.1.0h PHP/7.2.
</code></pre>

<p>7</p>
","3196663","","10406601","","2020-07-26 17:07:14","2020-07-26 17:07:14","how to run textblob on localhost?","<python><localhost><textblob>","0","5","","","","CC BY-SA 4.0"
"52005532","1","","","2018-08-24 13:27:27","","0","141","<p>I'm trying to install textblob but I always get an error. I installed nltk and spacy using pip and it worked fine but this just throws the same error over and over again.
I even tried using conda-forge but I'm getting the same error.</p>

<p>Please help!</p>

<p>Terminal SS:</p>

<p><a href=""https://i.stack.imgur.com/RAVIo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RAVIo.png"" alt=""enter image description here""></a></p>
","","user9566995","4896949","","2018-08-24 14:26:13","2018-08-24 14:26:13","Syntax Error while installing Textblob","<python><nlp><textblob>","0","3","","","","CC BY-SA 4.0"
"52280334","1","","","2018-09-11 16:24:07","","1","1692","<p>I am new to <strong>NLP</strong>. My requirement is to parse meaning from sentences. </p>

<p><strong>Example</strong> </p>

<ol>
<li>""Perpetually Drifting is haunting in all the best ways.""  </li>
<li>""When The Fog Rolls In is a fantastic song</li>
</ol>

<p>From above sentences, I need to extract the following sentences </p>

<ol>
<li>""haunting in all the best ways."" </li>
<li>""fantastic song""</li>
</ol>

<p>Is it possible to achieve this in <code>spacy</code>?</p>
","8231731","","10321971","","2018-09-11 16:38:15","2018-11-17 00:03:29","How to Grab meaning of sentence using NLP?","<python><nlp><nltk><spacy><textblob>","2","0","1","","","CC BY-SA 4.0"
"43416645","1","","","2017-04-14 17:51:38","","1","359","<p>As training data, have reviews of restaurants in XML, with associated target expression a sentiment is being expressed toward, a category which is a discrete label this belongs to and the polarity expressed toward this:</p>

<pre><code>&lt;text&gt;With the great variety on the menu , I eat here often and never get bored .&lt;/text&gt;
     &lt;Opinions&gt;
         &lt;Opinion target=""menu"" category=""FOOD#STYLE_OPTIONS"" polarity=""positive"" from=""30"" to=""34""/&gt;
     &lt;/Opinions&gt;
</code></pre>

<p>I have used the TextBlob NB classifier to train targets terms to associated categories.</p>

<p>For test data, my aim is to predict the target expression, given a sentence and the category. I have first extracted nouns and noun phrases from the sentence, assuming the expression will be a subset of these. For the sentence:</p>

<p><code>""what may be interesting to most is the worst sevice attitude come from the owner of this establishment</code>"", these are <code>['sevice attitude', 'owner', 'establishment']</code>. </p>

<p>I would like to know which of these is most likely given the category, which in this case is <code>SERVICE#GENERAL</code>. How could I go about this?</p>
","3058703","","3058703","","2017-04-18 14:38:50","2017-04-22 23:04:13","TextBlob Naive Bayes. Choosing highest likelihood","<python><machine-learning><nltk><naivebayes><textblob>","1","4","","","","CC BY-SA 3.0"
"52063689","1","52067640","","2018-08-28 17:48:53","","2","314","<p>I have a dataframe of sentences that looks like this: </p>

<pre><code>             text
0  this is great!
1  how dare you?!
</code></pre>

<p>I can succesfully use <code>TextBlob.words</code> (<a href=""https://textblob.readthedocs.io/en/dev/quickstart.html#tokenization"" rel=""nofollow noreferrer"">https://textblob.readthedocs.io/en/dev/quickstart.html#tokenization</a>) to break each sentence into its individual words. </p>

<p>An example would be</p>

<pre><code>a = TextBlob('moon is big')
print(a)

WordList(['moon','is','big'])
</code></pre>

<p><code>WordList</code> creates  a list type <code>blob.Wordlist</code> that saves each word. </p>

<p>I can break the sentences in the dataframe into individual words and save it it in a variable using this code: </p>

<pre><code>for i in df.text:
    d = TextBlob(i)
    words_list=d.words 
</code></pre>

<p>To get the sentiment of every word, I need to reapply TextBlob to every word. I can do this with the below code and append the polarity score in a list. </p>

<pre><code>lst=[]
for i in text.text:
    d = TextBlob(i)
    words_list=d.words
    for i in words_list:
        f = TextBlob(i)
        print(f.sentiment)
        lst.append(f.sentiment.polarity)
</code></pre>

<p>At this point, I dont know which polarity score belongs to which sentence, because my goal is that I want to average the polarity score of every word per row of dataframe and generate a new column <code>score</code>. Is there anyway I can pass an index per <code>blob.Wordlist</code> so I can match the average back to the dataframe?</p>

<p>code so far: </p>

<pre><code>from textblob import TextBlob
import pandas as pd
import statistics as s

df = pd.DataFrame({'text':['this is great!','how dare you?!']})

lst=[]
for i in text.text:
    d = TextBlob(i)
    words_list=d.words
    for i in words_list:
        f = TextBlob(i)
        print(f.sentiment)
        lst.append(f.sentiment.polarity)
        for i in lst:
            z = s.mean(lst)
            df['score'] = z
</code></pre>

<p>New df should look like this:</p>

<pre><code>             text     score
0  this is great!  0.2
1  how dare you?!  0.3
</code></pre>

<p>NOT </p>

<pre><code>             text     score
0  this is great!  0.133333
1  how dare you?!  0.133333
</code></pre>

<p>Thank you in advance. </p>

<p>edit:</p>

<p>@kevin here is your code with the proper df names</p>

<pre><code>from textblob import TextBlob
import pandas as pd
import statistics as s

df = pd.DataFrame({'text':['this is great!','how dare you?!']})
df['score'] = 0

for j in range(len(df.text)):
    lst=[]
    i = df.text[j]
    d = TextBlob(i)
    words_list=d.words
    for i in words_list:
        f = TextBlob(i)
        print(f.sentiment)
        lst.append(f.sentiment.polarity)
    z = s.mean(lst)
    df['score'][j] = z
</code></pre>
","3277133","","3277133","","2018-08-28 23:10:18","2018-08-29 00:03:01","How to maintain index when splitting sentences into words and reapplying sentiment polarity to each word?","<python><pandas><indexing><nlp><textblob>","2","7","","","","CC BY-SA 4.0"
"59289404","1","","","2019-12-11 15:47:02","","2","403","<p>I am trying to read the contents (blog) from a text file using Python (SpaCy/Textacy/Textblob) but it has been in vain, so far. Following is the code that I have recently tried: </p>

<pre class=""lang-py prettyprint-override""><code>import content as content
import pattern as pattern
import textacy
import spacy
nlp = spacy.load('en')
verb_clause_pattern = r'&lt;VERB&gt;&lt;ADV&gt;&lt;PART&gt;&lt;VERB&gt;+&lt;PART&gt;'
doc = textacy.Doc.content, lang = 'en'
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
</code></pre>

<p>And I am getting following error: </p>

<pre><code>    ""E:\TWP\TWP\venv\Scripts\python.exe E:/TWP/TWP/VerbPhrases.py
    Traceback (most recent call last):
      File ""E:/TWP/TWP/VerbPhrases.py"", line 5, in &lt;module&gt;
        nlp = spacy.load('en')
      File ""E:\TWP\TWP\venv\lib\site-packages\spacy\__init__.py"", line 30, in load
        return util.load_model(name, **overrides)
      File ""E:\TWP\TWP\venv\lib\site-packages\spacy\util.py"", line 169, in load_model
        raise IOError(Errors.E050.format(name=name))
    OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.""
</code></pre>
","3438153","","12240283","","2019-12-11 18:16:38","2019-12-20 18:07:37","Spacy/Textacy not reading file contents from .txt (text) file","<python><text><spacy><textblob><textacy>","1","3","","","","CC BY-SA 4.0"
"42058396","1","","","2017-02-05 23:10:27","","5","9094","<p>I'm using NLTK and TextBlob to find nouns and noun phrases in a text:</p>

<pre><code>from textblob import TextBlob 
import nltk

blob = TextBlob(text)
print(blob.noun_phrases)
tokenized = nltk.word_tokenize(text)
nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)]
print(nouns)
</code></pre>

<p>This works fine if my text is in english but it's not good anymore if my text is in french. </p>

<p>I was unable to find how to adapt this code for french language, how do I do that?</p>

<p>And is there a list somewhere of all the languages that are possible to parse?</p>
","1967110","","","","","2021-06-02 20:59:04","Python: NLTK and TextBlob in french","<python><nltk><textblob>","2","2","","","","CC BY-SA 3.0"
"51859091","1","51859441","","2018-08-15 12:52:36","","0","83","<p>I am leaning NLP and noticed that TextBlob classification based in Naive Bayes (textblob is Build on top of NLTK) <a href=""https://textblob.readthedocs.io/en/dev/classifiers.html"" rel=""nofollow noreferrer"">https://textblob.readthedocs.io/en/dev/classifiers.html</a> works fine when training data is list of sentences and does not work at all when training data are individual words (where each word and assigned classification).</p>

<p>Why?</p>
","183685","","","","","2018-08-15 13:13:33","Text classification with Naive Bayes","<python><nlp><nltk><text-classification><textblob>","1","0","","","","CC BY-SA 4.0"
"42346637","1","42352072","","2017-02-20 14:02:48","","1","1381","<p>I'm trying to implement Naive Bayes algorithm for sentiment analysis of News Paper headlines. I'm using TextBlob for this purpose and I'm finding it difficult to remove stop words such as 'a', 'the', 'in' etc. Below is the snippet of my code in python:</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
from textblob import TextBlob

test = [
(""11 bonded labourers saved from shoe firm"", ""pos""),
(""Scientists greet Abdul Kalam after the successful launch of Agni on May 22, 1989"",""pos""),
(""Heavy Winter Snow Storm Lashes Out In Northeast US"", ""neg""),
(""Apparent Strike On Gaza Tunnels Kills 2 Palestinians"", ""neg"")
       ]

with open('input.json', 'r') as fp:
cl = NaiveBayesClassifier(fp, format=""json"")

print(cl.classify(""Oil ends year with biggest gain since 2009""))  # ""pos""
print(cl.classify(""25 dead in Baghdad blasts""))  # ""neg""
</code></pre>
","5842134","","","","","2019-03-15 08:26:04","Which is the efficient way to remove stop words in textblob for sentiment analysis of text?","<python><sentiment-analysis><text-classification><textblob>","2","0","0","","","CC BY-SA 3.0"
"65841360","1","","","2021-01-22 08:05:51","","0","11","<p>If someone perform sentiment analysis using textblob which approach is adopted internally? Bagofwords or text mining</p>
","13013353","","","","","2021-01-22 08:05:51","Which approach textblob uses in sentiment analysis? Bag of words or text mining?","<textblob>","0","0","","","","CC BY-SA 4.0"
"31442693","1","31444183","","2015-07-15 23:02:14","","1","602","<p>For a project for my lab, I'm analyzing Twitter data. The tweets we've captured all have the word 'sex' in them, that's the keyword we filtered the TwitterStreamer to capture based on.</p>

<p>I converted the CSV where all of the tweet data (json metatags) is housed into a pandas DB and saved the 'text' column to isolate the tweet text. </p>

<pre><code>    import pandas as pd
    import csv
    df = pd.read_csv('tweets_hiv.csv')
    saved_column4 = df.text
    print saved_column4
</code></pre>

<p>Out comes the correct output:</p>

<pre><code>    0                                Some example tweet text
    1                 Oh hey look more tweet text @things I hate #stuff
    ...a bunch more lines
    Name: text, Length: 8540, dtype: object
</code></pre>

<p>But, when I try this</p>

<pre><code>    from textblob import TextBlob
    tweetstr = str(saved_column4)
    tweets = TextBlob(tweetstr).upper()
    print tweets.words.count('sex', case_sensitive=False)
</code></pre>

<p>My output is <code>22</code>.</p>

<p>There should be AT LEAST as many incidences of the word 'sex' as there are lines in the CSV, and likely more. I can't figure out what's happening here. Is TextBlob not configuring right around a dtype:object? </p>
","4455771","","2548721","","2015-07-16 01:55:52","2015-07-22 00:25:52","TextBlob not returning the correct number of instances of string in Pandas dataframe","<python><pandas><textblob>","2","0","1","","","CC BY-SA 3.0"
"62578890","1","","","2020-06-25 15:26:29","","0","57","<p>I am trying to run a sql query on some data I generated in databricks, but I am getting an error when running the SQL query that is referencing a library I used for the sentiment analysis.</p>
<p>This is the query <code>%sql select sentiment, count(*) as Demo from tweets_parsed group by sentiment</code></p>
<p>This is the block of code prior using the python library</p>
<pre><code>def get_sentiment(text):
    from textblob import TextBlob
    tweet = TextBlob(text)
    if tweet.sentiment.polarity &lt; 0:
      sentiment = &quot;negative&quot;
    elif tweet.sentiment.polarity == 0:
        sentiment = &quot;neutral&quot;
    else:
        sentiment = &quot;positive&quot;
    return sentiment
  
# Define your function
getSentiment = UserDefinedFunction(lambda x: get_sentiment(x), StringType())

# Apply the UDF using withColumn
tweets = tweets.withColumn('sentiment', getSentiment(col(&quot;tweet&quot;)))
</code></pre>
<p>And this is the error I get when trying to run the SQL query</p>
<pre><code>Error in SQL statement: SparkException: Job aborted due to stage failure: Task 3 in stage 3761.0 failed 4 times, most recent failure: Lost task 3.3 in stage 3761.0 (TID 5402, 10.81.233.238, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 480, in main
    process()
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 472, in process
    serializer.dump_stream(out_iter, outfile)
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 460, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 150, in dump_stream
    for obj in iterator:
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 449, in _batched
    for item in iterator:
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 87, in &lt;lambda&gt;
    return lambda *a: f(*a)
  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 99, in wrapper
    return f(*args, **kwargs)
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 79, in &lt;lambda&gt;
    return lambda *a: g(f(*a))
  File &quot;&lt;command-3730768793397314&gt;&quot;, line 13, in &lt;lambda&gt;
  File &quot;&lt;command-3730768793397314&gt;&quot;, line 2, in get_sentiment
ModuleNotFoundError: No module named 'textblob'

    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)
    at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)
    at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)
    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)
    at org.apache.spark.scheduler.Task.run(Task.scala:113)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
</code></pre>
<p>the textblob library is installed and working just when I run this query is when I get an error referencing that library.</p>
<p>Does anyone know what may be the issue?</p>
","10297524","","","","","2020-06-25 15:26:29","Why is this python library causing an error in my sql query?","<python><apache-spark><databricks><textblob>","0","4","","","","CC BY-SA 4.0"
"64679967","1","","","2020-11-04 12:20:43","","0","11","<p>I have a model that can generate embeddings for sentences. I want to do some preprocessing using textblob for every example I get at run time (as user input). Will it be possible to build a tensorflow pipeline that can use textblob to perform the preprocessing and then generate the embedding using the model? Also, will I be able to serve that model with the pipeline using tensorflow serving?</p>
","10715700","","","","","2020-11-04 12:20:43","Using TFX pipelines to load data at runtime and perform preprocessing using TextBlob","<tensorflow><text><tensorflow-serving><textblob><tfx>","0","0","","","","CC BY-SA 4.0"
"59124971","1","","","2019-12-01 11:15:53","","1","1785","<p>I want to do sentiment analysis of some sentences with <code>Python</code> and <code>TextBlob</code> lib.
I know how to use that,  but Is there any way to set <code>n-grams</code> to that?
Basically, I do not want to analyze word by word, but I want to analyze 2 words, 3 words, because phrases can carry much more meaning and sentiment.</p>

<p>For example, this is what I have done (it works):</p>

<pre><code>from textblob import TextBlob

my_string = ""This product is very good, you should try it""

my_string = TextBlob(my_string)

sentiment = my_string.sentiment.polarity
subjectivity = my_string.sentiment.subjectivity

print(sentiment)
print(subjectivity)
</code></pre>

<p>But how can I apply, for example n-grams = 2, n-grams = 3 etc?
Is it possible to do that with <code>TextBlob</code>, or <code>VaderSentiment</code> lib?</p>
","9749124","","9749124","","2019-12-03 10:43:04","2019-12-08 07:32:35","Setting n-grams for sentiment analysis with Python and TextBlob","<python><sentiment-analysis><textblob>","2","4","","","","CC BY-SA 4.0"
"43187884","1","","","2017-04-03 14:54:33","","2","335","<p>I am using the library TextBlob to analyse law texts. I really like TextBlob but I am struggling with some features.  When I create a <code>TextBlob</code> object, it automatically lowercases my entire text.</p>

<pre><code>from textblob import TextBlob    
text_blob = TextBlob(input_text)
</code></pre>

<p>This is inconvenient for me because I want to use the Entity Recognition method to find entries in the law texts, and many of them are capitalized (e.g. LEI, ISO, Member State, etc...). I want to leave those as they are, capitalized. Any other way Entity Recognizer won't find all possible entities (or I would have to lowercase everything, which is not my plan).</p>

<p>Is there a way to NOT lowercase the entire text with TextBlob, like passing a parameter:</p>

<pre><code>from textblob import TextBlob    
text_blob = TextBlob(input_text, lower=False)
</code></pre>

<p>I have tried a couple of things but without good results. I haven't found an answer in the documentation of TextBlob either. Maybe it is a very simple task.</p>
","692728","","5764553","","2017-07-12 22:11:51","2017-07-12 22:11:51","How to NOT lowercase a text in TextBlob","<python><textblob>","0","2","","","","CC BY-SA 3.0"
"55085959","1","55087825","","2019-03-10 08:46:56","","0","87","<p>My main string is in dataframe and substrings are stored in lists. My desired output is to find the matched substring. Here is the code I am using.</p>

<pre><code>sentence2 = ""Previous study: 03/03/2018 (other hospital)  Findings:   Lung parenchyma: The study reveals evidence of apicoposterior segmentectomy of LUL showing soft tissue thickening adjacent surgical bed at LUL, possibly post operation."" 
blob_sentence = TextBlob(sentence2)
noun = blob_sentence.noun_phrases
df1 = pd.DataFrame(noun)
comorbidity_keywords = [""segmentectomy"",""lobectomy""]
matches =[]
for comorbidity_keywords[0] in df1:
    if comorbidity_keywords[0] in df1 and comorbidity_keywords[0] not in matches:
       matches.append(comorbidity_keywords)
</code></pre>

<p>This gives me the result as the string that is not an actual match. The output should be ""segmentectomy"". But I get [0,'lobectomy']. Please Help!!. I have tried to take help from the answer posted here.  <a href=""https://stackoverflow.com/questions/3389574/check-if-multiple-strings-exist-in-another-string"">Check if multiple strings exist in another string</a> Please help to find out what am I doing incorrectly?</p>
","2287486","","5821988","","2019-03-10 13:33:55","2019-03-10 13:33:55","How to check if a string contains substring when both are stored in lists in python?","<python><string><pandas><textblob>","2","2","","","","CC BY-SA 4.0"
"59255019","1","","","2019-12-09 18:57:19","","0","163","<p>I'm new to sentiment analysis and I'm exploring with TextBlob.</p>

<p>My data is pre-processed Twitter data. It's in a series and each tweet has been cleaned and tokenized:</p>

<pre><code>    0   [new, leaked, treasury, document, full, sugges...
    1   [tommy, robinson, endorsing, conservative, for...
    2   [thanks, already, watched, catch, tv, morning, ]
    3   [treasury, document, check, today, check, cons...
    4   [utterly, stunning, video, hoped, prayed, woul...
    ... ...
    307370  [trump, disciple, copycat]
    307373  [disgusting]
    307389  [wonder, people, vote, racist, homophobe, like...
    307391  [gary, neville, slam, fuelling, racism, manche...
    307393  [brexit, fault, excuseforeverything]
</code></pre>

<p>When I run textblob sentiment (using help from <a href=""https://stackoverflow.com/questions/43485469/apply-textblob-in-for-each-row-of-a-dataframe"">Apply textblob in for each row of a dataframe</a>), my result is a column of nan values:</p>

<pre><code>    # Create sentiment column using textblob
    # Source: https://stackoverflow.com/questions/43485469/apply-textblob-in-for-each-row-of-a-dataframe

    def sentiment_calc(text):
try:
    return TextBlob(text).sentiment
except:
    return None

    boris_data['sentiment'] = boris_data['text'].apply(sentiment_calc)

         text   sentiment
    0   [new, leaked, treasury, document, full, sugges...   None
    1   [tommy, robinson, endorsing, conservative, for...   None
    2   [thanks, already, watched, catch, tv, morning, ]    None
    3   [treasury, document, check, today, check, cons...   None
    4   [utterly, stunning, video, hoped, prayed, woul...   None
       ...  ... ...
    307370  [trump, disciple, copycat]  None
    307373  [disgusting]    None
    307389  [wonder, people, vote, racist, homophobe, like...   None
    307391  [gary, neville, slam, fuelling, racism, manche...   None
    307393  [brexit, fault, excuseforeverything]    None
</code></pre>
","12447841","","","","","2019-12-09 18:57:19","TextBlob sentiment analysis: nan values","<python><nlp><sentiment-analysis><textblob>","0","5","","","","CC BY-SA 4.0"
"43485469","1","43487565","","2017-04-19 02:47:38","","10","12013","<p>i have a data frame with a col which has text. I want to apply textblob and calculate sentiment value for each row.</p>

<pre><code>text                sentiment
</code></pre>

<p>this is great<br>
great movie 
great story </p>

<p>When i execute the below code:</p>

<p><code>df['sentiment'] = list(map(lambda tweet: TextBlob(tweet), df['text']))</code></p>

<p>I get the error:</p>

<pre><code>TypeError: The `text` argument passed to `__init__(text)` must be a string, not &lt;class 'float'&gt;
</code></pre>

<p>How do you apply textBLob to each row of a col in a dataframe to get the sentiment value?</p>
","2585048","","7788355","","2017-04-19 03:18:40","2017-04-20 01:41:25","Apply textblob in for each row of a dataframe","<python><pandas><textblob>","1","0","4","","","CC BY-SA 3.0"
"33691590","1","","","2015-11-13 11:21:24","","2","3888","<p>I'm trying to read a csv and translate one column that is written in French to English with the TextBlob package in Python (2.7.10 Mac OS X Yosemite).</p>

<p>However, Python throws the following error message at me:</p>

<pre><code>AttributeError: 'Series' object has no attribute 'translate'
</code></pre>

<p>My Python code:</p>

<pre><code>import pandas as pd
import numpy as np
from textblob import TextBlob

df = pd.read_csv('france_content.csv')
df2 = df[['HEADLINE', 'AUTHOR', 'CONTENT']]

TextBlob = df2['CONTENT'].str.strip()
TextBlob.translate(to=""es"")
</code></pre>

<p>On second thought, I actually think I don't need numpy here. But how can I make pandas to read the content field and have textblob translate this to English. Preferably placing this in a column named 'English'</p>

<p>EDIT:
Changed to:</p>

<pre><code>import pandas as pd
import numpy as np
from textblob import TextBlob

df = pd.read_csv('france_content.csv')

df['English'] = df['CONTENT'].str.encode('ascii', 'ignore').apply(lambda x:    TextBlob(x.strip()).translate(to='en'))
</code></pre>

<p>Data is very basic with in column 1 Author name and in column 2 ('CONTENT') the French text.</p>

<p>I still have the following error:</p>

<pre><code>UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 9: ordinal not in range(128)
</code></pre>
","4644691","","4644691","","2015-11-13 12:09:59","2016-12-08 08:49:43","Translate pandas column with TextBlob","<python><pandas><textblob>","1","10","2","","","CC BY-SA 3.0"
"51883947","1","61297899","","2018-08-16 19:13:10","","2","749","<p>I have a dataframe that looks like this: </p>

<pre><code>     Text
0    this is amazing
1    nan
2    wow you are great
</code></pre>

<p>I want to iterate every word in a cell of the dataframe into textblob to get the polarity in a new column. However many rows have <code>nan</code> in them. </p>

<p>I think this is causing TextBlob to implement scores of 0.0 for polarity in the new column for all rows even those with text in them. </p>

<p>How do I run TextBlob.sentiment.polarity over every text in my column and create a new column with the polarity scores? </p>

<p>New df should look like this: </p>

<pre><code>     Text                 sentiment
0    this is amazing      0.9
1    nan                  0.0
2    wow you are great    0.8
</code></pre>

<p>I dont care about the <code>nan</code> so the sentiment value can be <code>nan</code> or 0. </p>

<p>Current code that is not working: </p>

<pre><code>for text in df.columns:
    a = TextBlob(text)
    df['sentiment']=a.sentiment.polarity
    print(df.value)
</code></pre>

<p>Thank you in advance. </p>

<p>edit: </p>

<p>To add, not sure if this makes a difference, the index on the df is not reset, for the fact that other parts of df are grouped together by the same index number. </p>
","3277133","","","","","2020-04-19 00:10:21","How to apply TextBlob if value in columns are missing for some rows?","<python><python-3.x><pandas><textblob>","3","0","","","","CC BY-SA 4.0"
"55920945","1","","","2019-04-30 12:25:45","","0","141","<p>I am facing an issue with textblob sentimental analysis. I have written code in pyspark that runs on aws glue spark job. The issue is it returns neutral result for negative comments. I have tried the same code on lambda(with python2.7,python3.6, and python 3.7 ), aws glue pythonshell job, and on my local machine, library works fine on them. but giving neutral results on aws glue spark job.</p>

<p>this code will get the return the result against comments</p>

<pre><code>import sys
import boto3
import json
import logging
import sys
import uuid
import json
import imp
import sys
sys.modules[""sqlite""] = imp.new_module(""sqlite"")
sys.modules[""sqlite3.dbapi2""] = imp.new_module(""sqlite.dbapi2"")
from textblob import TextBlob 
import re
import datetime

from awsglue.utils import getResolvedOptions
from awsglue.transforms import *
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import *
from pyspark.sql import SparkSession
from pyspark.sql import Row
from collections import OrderedDict



boto3.setup_default_session(region_name='us-east-1')

def clean_tweet(tweet): 
''' 
Utility function to clean tweet text by removing links, special 
characters 
using simple regex statements. 
'''
return ' '.join(re.sub(""(@[A-Za-z0-9]+)|([^0-9A-Za-z \t]) |(\w+:\/\/\S+)"", "" "", tweet).split()) 


def get_tweet_sentiment(tweet): 
''' 
Utility function to classify sentiment of passed tweet 
using textblob's sentiment method

print(""s"",tweet)
analysis = TextBlob(clean_tweet(tweet)) 
print(analysis.sentiment.polarity)
if analysis.sentiment.polarity &gt; 0: 
    return 'positive'
elif analysis.sentiment.polarity == 0: 
    return 'neutral'
else: 
    return 'negative'
</code></pre>
","4097499","","704848","","2019-04-30 13:35:36","2019-04-30 13:35:36","Textblob return neutral result for all negative comments, using aws glue pyspark job","<python><pyspark><aws-lambda><aws-glue><textblob>","0","4","","","","CC BY-SA 4.0"
"41860478","1","","","2017-01-25 19:54:22","","0","314","<p>I'm using TextBlob for Sentiment analysis and observed this --</p>

<p>The positive sentence is: 
<strong>She is that good at this.</strong></p>

<p>Its sentiment analysis is:
<strong>polarity=0.7</strong>, subjectivity=0.6000000000000001</p>

<p>The negative sentence is:
<strong>She isn't that good at this.</strong></p>

<p>Its sentiment analysis is:
<strong>polarity=0.7</strong>, subjectivity=0.6000000000000001</p>

<p>I understand Subjectivity could be same(or almost same) but how could polarity be the same? Isn't the second sentence negative?</p>

<p>Here's the code as requested:</p>

<pre><code>analysis = TextBlob(""She isn't that good at this"")
print (analysis.sentiment)
</code></pre>
","4944856","","4944856","","2017-01-25 20:26:09","2017-01-25 20:26:09","Why negative sentences have the same polarity as a positive sentences in Sentiment analysis with TextBlob?","<nlp><nltk><sentiment-analysis><textblob>","0","5","","","","CC BY-SA 3.0"
"66205218","1","","","2021-02-15 09:04:31","","0","54","<p>I am using a textbob for calculate the accuracy  of the words.</p>
<pre><code>text = 'thank you calling on product support talking to caafic k y please helping with your a digit employ number   ing'
orig_words = text.split()
fixed_words = TextBlob(text).correct().split()
words=(len([(x,y) for x,y in zip(orig_words, fixed_words) if x != y])) # =&gt; 3
total_word_len=len(orig_words)
correct_words=total_word_len-words
accuraccy=correct_words/total_word_len
</code></pre>
<p>but the problem is it is considering the  'caafic' ,'k', 'y' ,'ing' as the correct word. Ideally these should not be considered as correct words.</p>
","13488497","","4685471","","2021-02-15 09:36:59","2021-02-15 09:36:59","How can I calculate the accuracy of words?","<python><python-3.x><machine-learning><nlp><textblob>","0","5","","","","CC BY-SA 4.0"
"64441812","1","","","2020-10-20 08:59:31","","0","34","<p>I'm trying to extract verbs from German sentences. The problem is, for example in this sentence</p>
<p><code>Ich rufe noch einmal an.</code></p>
<p>Im getting rufe as the verb but its anrufe. I'm using textBlob and dont really know anything about linguistic. and using textblob I came accross POS tags. It tagged <code>an</code> as &quot;RP&quot;(doesnt know what that means) and <code>rufe</code> as &quot;VB&quot;. I could just glue all &quot;RP&quot; and &quot;VB&quot; together but then again there could more than one verb in a sentence.</p>
<p>What is the right way of doing this?</p>
","14310222","","","","","2020-10-20 14:58:45","Extracting verb from german sentenceces","<python><nlp><nltk><textblob><linguistics>","1","2","","","","CC BY-SA 4.0"
"65649916","1","","","2021-01-10 03:38:15","","0","86","<p>so I pip installed textblob in my terminal but when I try to import it in my vscode it says module not found. I'm new to this and trying to mess around. Any help would be appreciated and apologize in advance if this is a redundant question. Thanks!</p>
<pre><code>import pandas as pd
from textblob import TextBlob
import os


absolute_path = os.path.abspath(os.path.dirname('*.csv'))
df = pd.read_csv(absolute_path + '/data/*.csv')

exception has occurred: ModuleNotFoundError
No module named 'textblob'
</code></pre>
<p>I already tried pip install -U textblob but it says already satisfied. Basically have tried everything you can find on stackoverflow about textblob.</p>
","14767080","","14767080","","2021-01-10 06:04:29","2021-01-10 06:04:29","Textblob module in vs code help? Mac vscode version 1.52.1","<python><pandas><dataframe><libraries><textblob>","0","0","","","","CC BY-SA 4.0"
"41652692","1","","","2017-01-14 17:08:53","","0","263","<p>I am using textblob nltk to do some action for a given sentence.I am very new to nltk and text processing.I am able to decide the action for the given sentence but I have no idea how to extract the subject</p>

<p>Here is the code </p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier


trainingSet = [
('Get me juice','get'),
('Get me orange', 'get'),
('I want juice', 'get'),
('I dont want orange', 'take'),
('I dont like apple', 'take'),
('I detest watermelon', 'take')
]

cl = NaiveBayesClassifier(trainingSet)

print(cl.classify('I hate pumpkin'))
</code></pre>

<p>This gives me the label 'take' but how do I train it to extract the subject too?
That is I want the result to be 'take', 'pumpkin'</p>
","7419170","","","","","2017-01-14 17:08:53","Extract subject using textblob nltk","<nlp><nltk><textblob><nltk-trainer>","0","2","","","","CC BY-SA 3.0"
"25776598","1","","","2014-09-10 23:17:38","","2","338","<p>It seems to ignore the <code>if self.i &gt;5:</code> statement (I have removed my keys). The tweets should stop streaming after a few tweets but continuously stream until the program has been stopped. I have tried <code>sys.exit()</code> and return worked at the end.</p>

<pre><code>from bottle import route, default_app, get, post, request, run
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import smaclient
import MySQLdb
import sys
from textblob import TextBlob

ckey = ''
csecret = ''
atoken = ''
asecret = ''
license_key = '


@route('/')
def hello():
    return '''
    &lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;
    &lt;h1 align=""center""&gt;Gavin's Twitter Sentiment Analysis&lt;/h1&gt;
    &lt;p align=""center""&gt;Enter a word into the search box below&lt;/p&gt;
        &lt;form action=""/page2"" method=""post"" align=""center""&gt;
            Search: &lt;input name=""search"" type=""text"" /&gt;
            &lt;input value=""Search"" type=""submit"" /&gt;
        &lt;/form&gt;
    '''


@route('/page2', method='POST')
def result():
    search = request.forms.get('search')
    result = analyze(search)
    return result


def analyze(subject):
    auth = OAuthHandler(ckey, csecret)
    auth.set_access_token(atoken, asecret)
    twitterStream = Stream(auth, listener())
    return twitterStream.filter(track=[subject])


class listener(StreamListener):
    i = 0

    def on_data(self, data):
        print data
        tweet = data.split(',""text"":""')[1].split('"",""source')[0]
        print tweet
        db = MySQLdb.connect(
            host='localhost',
            user='root',
            passwd='',
            db='sentiment'
        )
        cursor = db.cursor()
        no = 2
        cursor.execute('INSERT INTO tweets (Tweet, no) VALUES (%s, %s)', (tweet, no))
        db.commit()
        positive = 0
        negative = 0
        self.i += 1
        if self.i &gt; 5:
            cursor = db.cursor()
            cursor.execute(""SELECT * FROM tweets"")
            rows = cursor.fetchall()
            for eachRow in rows:
                tweet = eachRow[0]
                blob = TextBlob(tweet)
                blob.language = 'en'
                polarity = blob.sentiment.polarity
                if (polarity &lt; - 0.10):
                    negative += 1
                elif (polarity &gt; 0.10):
                    negative += 1

            cursor.execute('DELETE FROM tweets WHERE no = 2')
            db.commit()
            if positive &gt; negative:
                return 'positive'
            else:
                return 'negative'
        sys.exit()


run(host='localhost', port=7000, debug=True)
</code></pre>
","4028822","","","user2555451","2014-11-15 17:43:53","2014-11-15 17:43:53","How to stop tweets streaming?","<python><bottle><tweepy><sentiment-analysis><textblob>","0","0","","","","CC BY-SA 3.0"
"43590397","1","","","2017-04-24 14:11:15","","1","2303","<p>Looking at responses from a recent survey we did. I do not think this respondent is all that happy. Here, TextBlob would have me believe his sentiment has reached a positive ceiling. If I remove the word 'best' from the string sentiment score turns to '0'. </p>

<p>Would you help to re-instill my trust is TextBlob, what am I doing wrong in this very simple application? </p>

<pre><code>a = ""Follow on rounds for the best prospects. Some choke to death now.""
b = TextBlob(a)
print b.sentiment 
</code></pre>

<p><em>Sentiment(polarity=1.0, subjectivity=0.3)</em></p>

<p>Thanks, </p>
","4595201","","","","","2018-05-12 14:42:01","TextBlob, totally inaccurate","<python><python-2.7><nltk><textblob>","1","2","","","","CC BY-SA 3.0"
"65140332","1","","","2020-12-04 08:38:31","","0","245","<p>I'm brand new to Python and coding so please bear with me.</p>
<p>I installed the TextBlob plugin to my IDE and it works like a charm when detecting the language of a string. See below code and output at the bottom.</p>
<p>What I need to do is have it detect the language of a text file, not just a string I have typed out. So essentially I need replace the lines of text with text files of whatever languages and add code to open/read the files and have TextBlob do its thing.</p>
<p>Any ideas?</p>
<pre><code>from textblob import TextBlob

text1 = TextBlob('I looked for Mary and Samantha at the bus station')
a = text1.detect_language()
print(a)

text2 = TextBlob('Appliquer un nom , une dénomination , un mot , une phrase à une personne , à une chose')
b = text2.detect_language()
print(b)

text3 = TextBlob('Escribe un ejemplo para mostrar el significado de la palabra de vocabulario.')
c = text3.detect_language()
print(c)


&gt;&gt;&gt; %Run 'NLP TextBlob.py'
en
fr
es
&gt;&gt;&gt;
</code></pre>
","14761854","","14761854","","2020-12-04 09:16:10","2020-12-04 12:16:20","Python: Using TextBlob NLTK to read a text file and detect the language","<python><text-files><textblob><language-detection>","1","0","","","","CC BY-SA 4.0"
"30948150","1","30948316","","2015-06-19 22:26:41","","0","328","<p>I am trying to run text from twitter api through sentiment analysis from textblob library, When I run my code, the code prints one or two sentiment values and then errors out, to the following error:</p>

<pre><code>UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 31: ordinal not in range(128)
</code></pre>

<p>I do not understand why this is an issue for the code to handle if it is only analyzing text. I have tried to code the script to UTF-8. Here is the code:</p>

<pre><code>from tweepy.streaming import StreamListener
from tweepy import OAuthHandler
from tweepy import Stream
import json
import sys
import csv
from textblob import TextBlob

# Variables that contains the user credentials to access Twitter API
access_token = """"
access_token_secret = """"
consumer_key = """"
consumer_secret = """"


# This is a basic listener that just prints received tweets to stdout.
class StdOutListener(StreamListener):
    def on_data(self, data):
        json_load = json.loads(data)
        texts = json_load['text']
        coded = texts.encode('utf-8')
        s = str(coded)
        content = s.decode('utf-8')
        #print(s[2:-1])
        wiki = TextBlob(s[2:-1])

        r = wiki.sentiment.polarity

        print r

        return True

    def on_error(self, status):
        print(status)

auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
stream = Stream(auth, StdOutListener())

# This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'
stream.filter(track=['dollar', 'euro' ], languages=['en'])
</code></pre>

<p>Can someone please help me with this situtation?</p>

<p>Thank you in advance.</p>
","3277133","","","","","2015-06-22 00:19:13","How to decode ascii from stream for analysis","<python-2.7><twitter><stream><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 3.0"
"66714100","1","","","2021-03-19 18:52:30","","2","74","<p>im trying to translate some sentences by using textblob from arabic to english,it was working when suddenly i started getting this errors</p>
<pre><code>from textblob import TextBlob

en_blob = TextBlob(u'انا سعيد') 

print(en_blob.translate(to='en'))
</code></pre>
<p>getting this error</p>
<pre><code>
  File &quot;C:\Users\USER\Anaconda3\lib\site-packages\textblob\translate.py&quot;, line 61, in translate
    self._validate_translation(source, result)

  File &quot;C:\Users\USER\Anaconda3\lib\site-packages\textblob\translate.py&quot;, line 84, in _validate_translation
    if result.strip() == source.strip():

AttributeError: 'list' object has no attribute 'strip'
</code></pre>
","10658897","","","","","2021-03-19 19:08:02","python translating from arabic to english using TextBlob","<python><translate><textblob>","0","3","","","","CC BY-SA 4.0"
"56167331","1","","","2019-05-16 11:18:10","","0","89","<p>I have installed python ""TextBlob"" package and it is available in the below location.</p>

<pre><code>c:\users\XYZ\appdata\local\continuum\anaconda3\lib\site-packages
</code></pre>

<p>(checked through runing command (pip show TextBlob)</p>

<p>When i am trying to run the command ""<code>Import TextBlob</code>"" is showing up error as</p>

<blockquote>
  <p>No module named 'TextBlob'.</p>
</blockquote>

<p>I have changed the current directory to <code>""c:\users\XYZ\appdata\local\continuum\anaconda3\lib\site-packages"".</code>
still no luck.</p>

<p>Could anyone reply on this ?</p>
","5316973","","6109920","","2019-05-16 11:18:46","2019-05-16 15:35:11","Error message while importing ""TextBlob"" package","<python><textblob>","1","2","","","","CC BY-SA 4.0"
"48181836","1","","","2018-01-10 06:53:52","","0","142","<p>I have a file which i m using as my input. But it has some special characters which I need to convert it into plain text. How can I do that?</p>
","9060284","","","","","2018-01-10 06:53:52","Converting text into utf-8 in textblob","<python><unicode><utf-8><textblob>","0","4","1","","","CC BY-SA 3.0"
"50828262","1","51331196","","2018-06-13 02:25:12","","6","1132","<p>The built-in classifier in textblob is pretty dumb. It's trained on movie reviews, so I created a huge set of examples in my context (57,000 stories, categorized as positive or negative) and then trained it using <code>nltk.</code> I tried using textblob to train it but it always failed:</p>

<pre><code>with open('train.json', 'r') as fp:
    cl = NaiveBayesClassifier(fp, format=""json"")
</code></pre>

<p>That would run for hours and end in a memory error. </p>

<p>I looked at the source and found it was just using nltk and wrapping that, so I used that instead, and it worked.</p>

<p>The structure for nltk training set needed to be a list of tuples, with the first part was a Counter of words in the text and frequency of appearance. The second part of tuple was 'pos' or 'neg' for sentiment.</p>

<pre><code>&gt;&gt;&gt; train_set = [(Counter(i[""text""].split()),i[""label""]) for i in data[200:]]
&gt;&gt;&gt; test_set = [(Counter(i[""text""].split()),i[""label""]) for i in data[:200]] # withholding 200 examples for testing later

&gt;&gt;&gt; cl = nltk.NaiveBayesClassifier.train(train_set) # &lt;-- this is the same thing textblob was using

&gt;&gt;&gt; print(""Classifier accuracy percent:"",(nltk.classify.accuracy(cl, test_set))*100)
('Classifier accuracy percent:', 66.5)
&gt;&gt;&gt;&gt;cl.show_most_informative_features(75)
</code></pre>

<p>Then I pickled it.</p>

<pre><code>with open('storybayes.pickle','wb') as f:
    pickle.dump(cl,f)
</code></pre>

<p>Now... I took this pickled file, and re opened it to get the nltk.classifier 'nltk.classify.naivebayes.NaiveBayesClassifier'&gt; -- and tried to feed it into textblob. Instead of </p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
blob = TextBlob(""I love this library"", analyzer=NaiveBayesAnalyzer())
</code></pre>

<p>I tried:</p>

<pre><code>blob = TextBlob(""I love this library"", analyzer=myclassifier)
Traceback (most recent call last):
  File ""&lt;pyshell#116&gt;"", line 1, in &lt;module&gt;
    blob = TextBlob(""I love this library"", analyzer=cl4)
  File ""C:\python\lib\site-packages\textblob\blob.py"", line 369, in __init__
    parser, classifier)
  File ""C:\python\lib\site-packages\textblob\blob.py"", line 323, in 
_initialize_models
    BaseSentimentAnalyzer, BaseBlob.analyzer)
  File ""C:\python\lib\site-packages\textblob\blob.py"", line 305, in 
_validated_param
    .format(name=name, cls=base_class_name))
ValueError: analyzer must be an instance of BaseSentimentAnalyzer
</code></pre>

<p>what now? I looked at the source and both are classes, but not quite exactly the same. </p>
","536538","","","","","2019-10-14 19:12:53","After training my own classifier with nltk, how do I load it in textblob?","<python><nltk><naivebayes><textblob>","3","1","1","","","CC BY-SA 4.0"
"24952690","1","","","2014-07-25 09:43:25","","0","386","<p>How can I get different word forms using Python. I want to create a list like the following.</p>

<pre><code>Work=['Work','Working','Works']
</code></pre>

<p>My code:</p>

<pre><code>raw = nltk.clean_html(html)
cleaned = re.sub(r'&amp; ?(ld|rd)quo ?[;\]]', '\""', raw)
tokens = nltk.wordpunct_tokenize(cleaned)
stemmer = PorterStemmer()
t = [stemmer.stem(t) if t in Words else t for t in tokens]
text = nltk.Text(t)
word = words(n)
Words = [stemmer.stem(e) for e in word]
find = ' '.join(str(e) for e in Words)
search_words = set(find.split(' '))
sents = ' '.join([s.lower() for s in text])
blob = TextBlob(sents.decode('ascii','ignore'))
matches = [map(str, blob.sentences[i-1:i+2])     # from prev to after next
            for i, s in enumerate(blob.sentences) # i is index, e is element
            if search_words &amp; set(s.words)]
    #return list(itertools.chain(' '.join (str(y).replace('&amp; rdquo','').replace('&amp; rsquo','') for y in matches))
return list(itertools.chain(*matches))
</code></pre>
","3849485","","","user2555451","2014-11-15 17:46:44","2014-11-15 17:46:44","Create wordforms using python","<python><nlp><nltk><textblob>","1","8","1","","","CC BY-SA 3.0"
"66621561","1","","","2021-03-14 05:59:58","","0","34","<p>Here is my code of python where I want to correct the spelling of 'WORDS_WITHOUT_SW' column. and storing in 'CORRECT_SPELL' column. But when I give range 2 then it takes less than 1 sec. but when I give 100 then it goes in infinite. Please help me how can I solve this problem.</p>
<pre><code>for i in range(2):
     text = df['WORDS_WITHOUT_SW'][i]
     df['CORRECT_SPELL'][i] = str(TextBlob(text).correct())
df.to_csv(str_file_name, index=False)
</code></pre>
","15312008","","","","","2021-03-14 05:59:58","How to reduce execution time in python when applying textblob correct method on csv data?","<python><loops><execution><textblob>","0","4","","","","CC BY-SA 4.0"
"66658438","1","","","2021-03-16 15:35:45","","0","41","<p>I've been playing around with the Streamlit module and wanted to make a very basic Sentiment analysis project but whenever my sentiment score is equal to 0 it skips the else statement, it's driving me mad.</p>
<p>Would appreciate some help thank you!</p>
<pre><code>def get_polarity(phrase):

    sentiment_sentence = TextBlob(phrase)
    sentiment = sentiment_sentence.sentiment.polarity

    return sentiment


st.title('Sentiment Analysis in Python')

sentence = st.text_input('Enter a sentence: ')

result = get_polarity(sentence)

if result:

    if result &gt; 0:
        st.image('positive.png')
        st.write('Your sentiment score: ', result)

    elif result &lt; 0:
        st.image('negative.png')
        st.write('Your sentiment score: ', result)

    else:
        st.image('neutral.png')
        st.write('Your sentiment score: ', result)

</code></pre>
","12399554","","15213553","","2021-03-17 02:34:39","2021-03-17 02:34:39","Why is it skipping the else statement and not passing the correct image?","<python><textblob>","1","5","","","","CC BY-SA 4.0"
"48954496","1","","","2018-02-23 18:53:52","","2","93","<p>I followed this tutorial:
<a href=""http://textblob.readthedocs.io/en/dev/classifiers.html#feature-extractors"" rel=""nofollow noreferrer"">http://textblob.readthedocs.io/en/dev/classifiers.html#feature-extractors</a>
And I wrote an extractor:</p>

<pre><code>def phrases_extractor(document):
    r.extract_keywords_from_text(document)
    words = r.get_ranked_phrases()
    feats = {}
    for w in words:
        feats[""contains({0})"".format(w)] = True
    return feats
</code></pre>

<p>But I don't know why the docs says that there should be ""contains... = False""
How can I do that? Should I firstly collect all the possible words and add them to the dictionary with False value?</p>
","7207058","","","","","2020-03-23 21:31:13","How to specify my own feature extractor in TextBlob?","<python><nlp><nltk><textblob>","1","0","","","","CC BY-SA 3.0"
"55766626","1","","","2019-04-19 19:04:47","","0","741","<p>I'm doing sentiment analysis for several languages. my code runs successfully but it's extremely slow (10mn for just 11K records). Here is my code:</p>

<pre><code># Spanish Classifier - from https://github.com/aylliote/senti-py
clf = SentimentClassifier()

# Italian Classifier - Also for Russian
from polyglot.text import Text as T

# Germany Classifier
from textblob_de import TextBlobDE as TextBlob_d

# English
from textblob import TextBlob

# French
from textblob_fr import PatternTagger, PatternAnalyzer

def Flag(row):
    try:
        if row['lang'] == 'es':
            txt=clf.predict(row['rev'])
            return txt
        elif row['lang'] == 'it':
            txt=T(row['rev'])
            return txt.polarity
        elif row['lang'] == 'de':
            txt=TextBlob_d(row['rev'])
            return txt.sentiment
        elif row['lang'] == 'en':
            txt=TextBlob(row['rev'])
            return txt.sentiment.polarity
        elif row['lang'] == 'fr':
            txt=TextBlob(row['rev'], pos_tagger=PatternTagger(), 
            analyzer=PatternAnalyzer())
            return txt.sentiment[0]
        elif row['lang'] == 'ru':
            txt=T(row['rev'])
            return txt.polarity
        else:
            return """"
    except:
        return """"

df['sent']=df.apply(Flag,axis=1)
</code></pre>

<p>I've checked other posts about textblob.sentiments import NaiveBayesAnalyzer being very slow but I don't think it's the same situation i'm facing here?</p>

<p>Thank you</p>
","5503494","","","","","2019-04-19 19:04:47","how can I speed up my sentiment analysis?","<python><nlp><classification><sentiment-analysis><textblob>","0","5","","","","CC BY-SA 4.0"
"41864938","1","","","2017-01-26 02:15:36","","0","589","<p>Is there a way I can conduct sentiment analysis with TextBlob, as NLP, using Sentiwordnet, as it's corpora database?</p>

<p>Currently, I believe TextBlob is using other corpora databases to do sentiment analysis from the download_corpora.py, but I do not know which ones are actually being used for the analysis.</p>

<p>(I know I can use NLTK, but I find TextBlob more powerful, as it calls NLTK.)</p>
","7462513","","","","","2017-04-06 22:53:34","Using Sentiwordnet with TextBlob","<python><nlp><sentiment-analysis><textblob><senti-wordnet>","1","1","","","","CC BY-SA 3.0"
"25804995","1","","","2014-09-12 09:26:13","","1","2185","<p>I am working on pyspark for NLP processing etc.  I am using TextBlob Python library.  </p>

<p>Normally, in standalone mode, it is easy to install the external Python libraries.  In cluster mode I am facing problem to install these libraries on worker nodes remotely.  I cannot access each worker machine to install these libs in Python path. </p>

<p>I tried to use Sparkcontext pyfiles option to ship <code>.zip</code> files...but the problem is these Python packages need to be installed on worker machines.</p>

<p>Are there different ways of doing it so that this lib-Textblob could be available in Python path?</p>
","3544282","","-1","","2016-01-02 22:18:18","2016-01-02 22:18:18","Installing external libraries on worker nodes in Pyspark-Cluster mode","<python><hadoop><nlp><apache-spark><textblob>","1","0","1","","","CC BY-SA 3.0"
"66712090","1","","","2021-03-19 16:32:42","","1","73","<p>I am using textblob detecct_language, which worked two weeks ago. Just basic usage.</p>
<pre><code>from textblob import TextBlob
text = &quot;C𝚊𝚕𝚕 𝚘𝚏 𝙳𝚞𝚝𝚢®: 𝙱𝚕𝚊𝚌𝚔 𝙾𝚙𝚜 𝙲𝚘𝚕𝚍 𝚆𝚊𝚛 - 𝚁𝚎𝚟𝚎𝚊𝚕 𝚃𝚛𝚊𝚒𝚕𝚎𝚛&quot;
lang = TextBlob(text).detect_language()
</code></pre>
<p>Now it raises ValueError.</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-27-bad6e63440b4&gt; in &lt;module&gt;
      3 text = &quot;C𝚊𝚕𝚕 𝚘𝚏 𝙳𝚞𝚝𝚢®: 𝙱𝚕𝚊𝚌𝚔 𝙾𝚙𝚜 𝙲𝚘𝚕𝚍 𝚆𝚊𝚛 - 𝚁𝚎𝚟𝚎𝚊𝚕 𝚃𝚛𝚊𝚒𝚕𝚎𝚛&quot;
      4 lang = TextBlob(text)
----&gt; 5 lang.detect_language()

~\anaconda3\lib\site-packages\textblob\blob.py in detect_language(self)
    566         :rtype: str
    567         &quot;&quot;&quot;
--&gt; 568         return self.translator.detect(self.raw)
    569 
    570     def correct(self):

~\anaconda3\lib\site-packages\textblob\translate.py in detect(self, source, host, type_)
     71         url = u'{url}&amp;sl=auto&amp;tk={tk}'.format(url=self.url, tk=_calculate_tk(source))
     72         response = self._request(url, host=host, type_=type_, data=data)
---&gt; 73         result, language = json.loads(response)
     74         return language
     75 

ValueError: not enough values to unpack (expected 2, got 1)
</code></pre>
<p>Totally have no idea what's causing the error.</p>
","14374705","","","","","2021-03-20 15:48:45","Python3 Textblob detection - ValueError","<python-3.x><textblob>","1","1","","","","CC BY-SA 4.0"
"33290217","1","33456121","","2015-10-22 20:32:54","","2","160","<p>I am trying to use <a href=""https://textblob.readthedocs.org/en/dev/quickstart.html#spelling-correction"" rel=""nofollow noreferrer"">TextBlob's spelling correction</a>, but correct() returns with an empty object for every call.
The following shows the method call on the terminal:</p>

<pre><code>&gt;&gt;&gt; from textblob import TextBlob
&gt;&gt;&gt; b = TextBlob(""I havv goood speling!"")
&gt;&gt;&gt; b.correct()
TextBlob("""")
&gt;&gt;&gt; print(b.correct())

&gt;&gt;&gt; 
</code></pre>

<p>I am running Python 2.7.6 on Linux.</p>
","3832088","","1033581","","2019-05-03 10:11:13","2019-05-03 10:11:13","TextBloB: correct() method returning empty object","<python><python-2.7><nlp><textblob>","1","3","","","","CC BY-SA 4.0"
"58097154","1","58097242","","2019-09-25 11:17:05","","0","58","<p>I am using Python to assign labels to results returned from TextBlob.
My very basic code looks like this:</p>

<pre><code>from textblob import TextBlob

def sentLabel(blob):
    label = blob.sentiment.polarity 

    if(label == 0.0):
        print('Neutral')
    elif(label &gt; 0.0):
        print('Positive')
    else:
        print('Negative')

    Feedback1 = ""The food in the canteen was awesome""
    Feedback2 = ""The food in the canteen was awful""
    Feedback3 = ""The canteen has food""


    b1 = TextBlob(Feedback1)
    b2 = TextBlob(Feedback2)
    b3 = TextBlob(Feedback3)

    print(b1.sentiment_assessments)
    print(sentLabel(b1))
    print(b2.sentiment_assessments)
    print(sentLabel(b2))
    print(b3.sentiment_assessments)
    print(sentLabel(b3))
</code></pre>

<p>This prints out the sentiment correctly but it also prints out ""None"" as shown below:</p>

<pre><code>Sentiment(polarity=1.0, subjectivity=1.0, assessments=[(['awesome'], 1.0, 1.0, None)])

Positive

None

...
</code></pre>

<p>Is there any way to suppress ""None"" from being printed?</p>

<p>Thanks for any help or pointers.</p>
","9007492","","7347631","","2019-09-25 12:18:41","2019-09-25 12:18:41","how can I suppress some output from the Python Library TextBlob sentiment.polarity","<python><output><textblob><suppress>","1","0","","","","CC BY-SA 4.0"
"67442929","1","67443308","","2021-05-08 00:10:58","","0","66","<p>I need your help as i tried every method but not able to perform sentiment analysis on my noun phrases, extracted from tweets in dataframe, using TextBlob. Also i think TextBlob.noun_phrases function is not producing the correct results. See for yourself in the image below. I am really new to Python, please help!!</p>
<p>So my code for extracting the Noun phrase from dataframe is:</p>
<pre><code>from textblob import TextBlob
nltk.download('wordnet')
nltk.download('brown')
nltk.download('punkt')

def blob(text):
  return TextBlob(text).noun_phrases

df['Noun_Phrases'] = df['Tweets'].apply(blob)

df
</code></pre>
<p><a href=""https://i.stack.imgur.com/wkIBu.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Next, my code for sentiment analysis is below, and i get the error as shown in below image:</p>
<pre><code>def getsubjectivity(text):
return TextBlob(text).sentiment.subjectivity 

df['Subjectivity'] = df['Noun_Phrases'].apply(getsubjectivity)
</code></pre>
<p>Error : TypeError: The <code>text</code> argument passed to <code>__init__(text)</code> must be a string, not &lt;class 'textblob.blob.WordList'&gt;</p>
<p><a href=""https://i.stack.imgur.com/gYWZO.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","15472212","","","","","2021-05-08 01:32:37","How to perform Sentiment Analysis on Noun Phrases in Pandas?","<python><pandas><twitter><sentiment-analysis><textblob>","1","0","","","","CC BY-SA 4.0"
"66688914","1","66689845","","2021-03-18 10:10:33","","0","30","<p><a href=""https://i.stack.imgur.com/woron.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/woron.png"" alt=""1"" /></a></p>
<p>I'm using TextBlob to translate some text , it worked fine till I tested it today ...
I'm using python 3
textblob Version: 0.15.3.
Is there a new update or something like that ?</p>
","11743970","","","","","2021-03-29 10:25:56","textblob error 'list' object has no attribute 'strip'","<python-3.x><translate><textblob>","2","0","","","","CC BY-SA 4.0"
"43512525","1","","","2017-04-20 07:07:09","","0","496","<p>I am trying to implement tf-idf without using sklearn and similar packages. Can someone help me convert values in a DataFrame to a list of blob objects?</p>

<p>I have a DataFrame with one column- ""Text"" and I want a bloblist as [TextBlob(Text1),TextBlob(Text2),...,TextBlob(Textn)]
[find image here]<a href=""https://i.stack.imgur.com/2eWpC.png"" rel=""nofollow noreferrer"">1</a></p>

<p>I tried:
bloblist=TextBlob(str(df[""text""].values))
but this gives me just one list like    TextBlob(0  Text1,0  Text2,...0  Textn)</p>

<p>Is there a way to eliminate these preceding 0's...or is there a better way</p>

<p>Could someone please point out where I am wrong.</p>
","2059294","","2059294","","2017-04-21 22:51:49","2017-04-21 22:51:49","tf-idf on pandas DataFrame","<python><dataframe><textblob>","1","4","0","","","CC BY-SA 3.0"
"64130925","1","64133442","","2020-09-30 04:33:53","","1","159","<p>So i have a comma delimited file that im saving to a blob.
Im using the latest Chrome based Edge browser.
This particular code (typescript) that I have has not changed for many months now.
But suddenly, i noticed that if i save the file with a particular datetime string in it, then i get a weird output for that. Basically, i see the weird text instead of the datetime string.</p>
<p>Here is the datetime string im saving (and fully expect to see in the saved file):</p>
<pre><code>‎9‎/‎26‎/‎2020‎ ‎7‎:‎00‎:‎00‎ ‎AM
</code></pre>
<p>Here is the weird text that appears instead:</p>
<pre><code>â€Ž9â€Ž/â€Ž26â€Ž/â€Ž2020â€Ž â€Ž7â€Ž:â€Ž00â€Ž:â€Ž00â€Ž â€ŽAM
</code></pre>
<p>Now judging by the fact that i couldn't simply copy &amp; paste this weird string into this edit window (it thinks im trying to paste an image), im guessing it is binary. Which is probably a huge hint, but it's not ringing any bells for me.</p>
<p>So question is: why is this binary when im certain im writing out a string?</p>
<p>After some digging around I was able to determine that there seems to be an encoding issue. Still not sure why. In addition, upon closer inspection of the weird string, the date is actually in there. It just looks strange because each component is padded with this weird string &quot;â€Ž&quot;.</p>
","158103","","158103","","2020-09-30 05:36:05","2020-09-30 08:09:54","Saving an HTML Blob file produces weird text inside","<javascript><html><blob><textblob>","1","3","","","","CC BY-SA 4.0"
"33285194","1","33286153","","2015-10-22 15:47:24","","2","1612","<p>I need to extract <code>ngrams</code> from <code>text</code>. I'm using:</p>

<pre><code>from textblob import TextBlob
text = TextBlob('me king of python')
print(text.ngrams(n=3)
</code></pre>

<p>to split the text (me king of python) in trigrams, and it gives:</p>

<pre><code>[WordList(['me', 'king', 'of']), WordList(['king', 'of', 'python'])]
</code></pre>

<p>now i need to join the items of each WordList with:</p>

<pre><code>x = {word for word in ' '.join(text.ngrams(n=3)) }
print x
</code></pre>

<p>And it gives me the following error:</p>

<pre><code>TypeError: sequence item 0: expected string or Unicode, WordList found
</code></pre>

<p>I know the solution is silly but i'm not good in python and I don't understand <code>wordlists</code>.</p>
","2598997","","2839786","","2015-10-22 17:48:46","2017-09-06 03:17:12","Joining WordLists in python","<python><textblob>","1","6","","","","CC BY-SA 3.0"